{
    "paper_title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
    "authors": [
        "Xiaolong Wang",
        "Lixiang Ru",
        "Ziyuan Huang",
        "Kaixiang Ji",
        "Dandan Zheng",
        "Jingdong Chen",
        "Jun Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 0 8 0 2 . 0 1 5 2 : r ARGenSeg: Image Segmentation with Autoregressive Image Generation Model Xiaolong Wang, Lixiang Ru, Ziyuan Huang, Kaixiang Ji, Dandan Zheng Jingdong Chen, Jun Zhou Ant Group {xiaowang.wxl, rulixiang.rlx, pishi.hzy, kaixiang.jkx, yuandan.zdd}@antgroup.com {jingdongchen.cjd, jun.zhoujun}@antgroup.com Figure 1: ARGenSeg is unified framework for visual understanding, segmentation, and generation. It supports semantic, instance, interactive, and zero-shot reasoning segmentation, as well as anomaly detection, by leveraging strong visual understanding capabilities."
        },
        {
            "title": "Abstract",
            "content": "We propose novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent 39th Conference on Neural Information Processing Systems (NeurIPS 2025). on the pixel-level understanding of the MLLM. To reduce inference latency, we employ next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with remarkable boost in inference speed, while maintaining strong understanding capabilities."
        },
        {
            "title": "Introduction",
            "content": "The emergence of large language models (LLMs) [8, 17, 52] has significantly accelerated the development of artificial general intelligence (AGI) [9]. Breakthroughs like ChatGPT [40] enable the transformer-based [54] autoregressive framework to unify diverse tasks of natural language processing [1, 4]. As for multimodal task, LLaVA [34] employs visual adaptor to map visual features into the embedding space of LLMs, establishing universal paradigm for multimodal large language models [33, 5, 15, 56]. Recent studies [48, 64, 49, 67, 51, 61, 2, 3, 22] explore the unified framework for multimodal understanding and generation. However, integrating fundamental visual perception tasks into unified AGI framework remains an open challenge. While sparse-output tasks such as visual grounding can be directly addressed via text expression [14], tasks requiring dense outpus like image segmentation are inherently difficult to represent through natural language. Previous methods that incorporate image segmentation into MLLMs typically fall into two categories. The first discretizes dense masks into boundary point sequences [11, 42, 58], which inevitably leads to incomplete segmentation masks and unnatural object boundaries. The second achieves segmentation through downstream dedicated decoders (e.g., SAM [27], Mask2Former [16]), which are conditioned on either textual prompts [12] or hidden states [29, 45, 75] generated by MLLMs. This not only results in complex model architectures, but also leads to insufficient understanding of pixel-level information for LLM due to its reliance on specialized task head. To address the above challenges, we propose ARGenSeg, which leverages the image generationbased paradigm to integrate image segmentation into unified MLLM framework. To retain the strong understanding capability of MLLMs, we use continuous image features as the input. For the generation output, we train the model to directly predict quantized image tokens, aligning with the next-token autoregressive prediction mechanism of language models. We use pre-trained VQ-VAE as image tokenizer to quantize and detokenize images, with its visual tokens added to the codebook of MLLM. By leveraging the understanding ability of MLLM, ARGenSeg is capable of additional complex reasoning segmentation [29], anomaly detection [7, 6] and other image segmentation tasks [75] as shown in Fig. 1. The image tokenizer is kept frozen throughout training, thereby avoiding the dependence of LLM on subsequent decoders when learning pixel-level information. In real-world application, image segmentation often requires fast response times. For this purpose, we adopt next-scale prediction strategy for image generation. On one hand, the multi-scale mask generation process aligns with the intuitive process of object segmentation, which typically involves coarse localization followed by fine-grained boundary refinement. On the other hand, generating visual tokens in parallel provides significant efficiency advantage, achieving over 4 speedup compared to sequential generation methods [19, 59]. Some methods also propose to use image generation for image segmentation. UniGS [43] uses diffusion model [21, 46] to achieve image segmentation. However, its U-Net structure causes lack of understanding ability. HiMTok [57] proposes an innovative mask tokenizer that enables decoding discrete outputs from the MLLM into binary masks via image generation. However, the task-specific tokenizer limits its generality and extensibility. Moreover, both of these methods suffer from significant disadvantages in inference speed. Extensive experiments demonstrate that the proposed ARGenSeg outperforms existing MLLM-based segmentation methods, while also achieving significantly faster inference. Notably, our method achieves superior performance using substantially less segmentation data compared to prior stateof-the-art approach [57]. In addition, the use of general-purpose visual tokenizer provides the flexibility to extend the framework to additional tasks. As demonstration, by fine-tuning on small amount of image generation data, we successfully unlock the image generation capability of our framework, as illustrated in Fig. 1. The main contributions of this paper include: 2 We propose novel image segmentation framework based on unified multimodal understanding and generation paradigm. To our knowledge, we are the first to show that unified MLLMs can achieve SOTA segmentation results without any extra segmentation heads. We leverage universal image tokenizer, allowing segmentation to fully rely on the pixellevel visual understanding of the MLLM. We further show that direct image token prediction by the MLLM is important for achieving high segmentation accuracy. We propose to use next-scale prediction to speed up inference. And we observe that the coarse-to-fine multi-scale mask generation process also boosts segmentation robustness."
        },
        {
            "title": "2 Related Work",
            "content": "Integrating image segmentation into MLLMs not only equips them with fine-grained visual perception, but also enables more complex reasoning-based segmentation tasks by leveraging understanding capabilities. However, representing segmentation masks within the MLLM framework remains significant challenge. PolyFormer [35] and VistaLLM [42] represent masks as polygons using point sequences, which are easy to express but struggle with complex shapes. LISA [29] aggregates segmentation information using special tokens and predicts masks through SAM [27] decoder. Subsequent works such as GLaMM [44], PixelLM [45], GSVA [65], and PSALM [75] build upon this paradigm, and still rely on special tokens and dedicated segmentation decoders. These methods essentially aim to extract semantic embeddings of target objects and then obtain dense segmentation masks by computing similarity with image features. Such representations tend to emphasize high-level semantics rather than true pixel-level understanding. HiMTok [57] explores an alternative that removes the reliance on special tokens and SAM-like decoders. However, it still depends on dedicated mask tokenizer trained on binary masks. Moreover, the expressiveness of the tokenizer is limited and cannot be extended to support other tasks such as image generation. This suggests that segmentation representation in MLLMs remains an open challenge, which we think can be effectively addressed through autoregressive image generation. Unified multimodal understanding and generation models have recently attracted increasing attention for their ability to seamlessly perform both understanding and generation tasks within single framework. Several works [48, 20, 63, 51] leverage diffusion models for image generation by regressing visual embeddings from MLLM outputs and using them as conditional inputs. TransFusion [77] and Show-O [67] unify next-token prediction and diffusion-based generation within single transformer framework. Chameleon [49] and Emu3 [59] adopt shared discrete visual embedding space for both understanding and generation, decoding images through VQ-based tokenizers [19, 71]. Janus [61] decouples the encoder for multimodal understanding and generation, using discrete visual tokens for generation while retaining continuous visual features for better understanding accuracy. VARGPT [78] proposes next-token prediction for understanding and next-scale prediction for image generation, but relies on an additional transformer-based visual decoder. Image tokenization enables discrete outputs from autoregressive models to be reconstructed into images. VQ-VAE [53] encodes images into downsampled latent space and quantizes the features into discrete token IDs, simplifying the learning process for generative models. VQGAN [19] improves reconstruction quality and training efficiency through adversarial training. TiTok [72] significantly reduces the number of tokens required for image representation, improving generation speed, and further shows that increasing the number of latent tokens consistently enhances reconstruction quality. VAR [50] reformulates visual autoregressive generation as next-scale prediction task, achieving high efficiency while maintaining relatively large number of visual tokens."
        },
        {
            "title": "3 Method",
            "content": "In this paper, we propose novel image segmentation framework based on autoregressive image generation model, using Vector-Quantized (VQ) autoencoder [53, 19] to tokenize images into discrete tokens and reconstruct them from generated outputs. To address the unique challenges of segmentation, we introduce two key designs. (1) The MLLM is trained to directly output image tokens, which is crucial for achieving high pixel-level accuracy. (2) We utilize multi-scale generation process that performs coarse-to-fine refinement. This not only enhances segmentation robustness but also improves inference efficiency. This section first presents the background of the 3 image tokenizer (Sec. 3.1), then details the architecture (Sec. 3.2), training procedure (Sec. 3.3), and inference process (Sec. 3.4) of our proposed model. 3.1 Preliminary Vector-Quantized Autoencoder The standard VQ model learns to encode images into latent space and reconstruct them from discrete tokens. Given an input image RHW 3, the encoder maps it to latent feature space: = E(I), H D, (1) where is the spatial downsampling factor and denotes the feature dimesion. The latent features are then quantized by vector quantizer into discrete token indices [V ] W : = Q(f ), q(i,j) = arg min (i,j) cv2, v[V ] (2) where cv is the v-th embedding vector in the visual codebook RV D, and [V ] denotes the set of codebook indices {1, 2, . . . , }. The reconstruction of the image can be interpreted as detokenizing discrete visual tokens into an image. In this procedure, the quantized indices are used to index the corresponding embedding from the visual codebook C, producing the estimated latent feature map ˆf . The estimated feature map is then passed through the decoder to generate the reconstructed image ˆI: ˆf = lookup(C, q), ˆI = D( ˆf ). (3) Multi-Scale VQ Autoencoder When using VQ-VAE for autoregressive image generation, the inference process typically requires O(n2) steps. To address this inefficiency, VAR[50] introduces next-scale prediction paradigm for visual token generation. Specifically, the feature map is quantized into multi-scale token maps (r1, r2, . . . , rK) , where each map corresponds to different resolution. At each inference step, the model generates all hk wk tokens required for the current scale rk in parallel, repeating this process until rK reaches the target resolution of . Moreover, the coarseto-fine predictions can enhance the generation quality. Based on this paradigm, an image of resolution 256 256 can be represented using 680 visual tokens, while requiring just autoregressive inference steps, significantly improving generation efficiency. Given the fast response requirements of image segmentation tasks, we adopts this paradigm to enable efficient autoregressive image generation. Figure 2: The architecture of ARGenSeg and its training and inference procedures. Left: ARGenSeg integrates image segmentation into the MLLM via an autoregressive image generation paradigm. unified classification prediction head is used to generate both text and visual tokens. Right: Visual tokens are generated in parallel using the next-scale prediction strategy. During training, VAE encoder is used to construct supervision for cross-entropy loss. During inference, the VAE decoder reconstructs the image from the predicted visual tokens. [S]/[E] denotes <gen_start>/<gen_end>. 4 3.2 Architecture Multimodal Understanding ARGenSeg uses unified autoregression framework for image understanding and generation as shown in Fig. 2. Our framework employs the built-in tokenizer of the LLM to convert text input into discrete token IDs and corresponding embeddings. For image input, vision encoder is used to extract features, which are then mapped to the LLMs embedding space via vision projector. After the concatenated embeddings are fed into the LLM, the model performs next-token prediction to sequentially generate token embeddings. These embeddings are then passed through classification head to sample discrete token IDs, which are subsequently detokenized into meaningful text. For multimodal understanding tasks, decoupling the framework from image generation preserves the native understanding capabilities of the LLM. Image Generation To integrate image generation into the framework, we introduce special tokens <gen_start> and <gen_end> to mark the beginning and end of the generation process. Additionally, the visual token IDs from the visual tokenizer are added to the LLMs vocabulary in the form of <visual_token_ID>. When image generation is required, the framework autonomously determines whether to initiate generation based on the input instruction. Upon encountering the <gen_start> token, multi-scale image generation begins, where visual tokens for each scale are predicted in parallel. At k-th scale, the visual feature corresponding to the visual token map from the previous scale is retrieved by looking up the visual codebook and then upsampled to match the resolution of the current scale. lightweight linear layer, referred to as the generation projector, maps these upsampled visual features into the embedding space of LLM, serving as input for the next scale. This design allows one-step parallel inference to obtain all visual tokens at the current scale. Importantly, the unified prediction head is used to generate visual tokens, which are then directly converted to the corresponding index IDs in the codebook C. Once all visual tokens across scales are generated, they are detokenized by the visual tokenizer to reconstruct the final image. 3.3 Training Procedure Training Strategy In our framework, the vision encoder, large language model, vision projector and classification prediction head are initialized using InternVL 2.5[13], while the multi-scale visual tokenizer is initialized from VAR [50]. During training, the vision encoder and visual tokenizer are kept frozen to reduce the models reliance on dedicated decoders for pixel-level understanding. By leveraging pre-trained multimodal understanding, the framework converges rapidly when training on image segmentation data. Thus, we employ single-stage supervised finetuning (SFT) strategy, jointly optimizing both image segmentation and multimodal understanding data. For image generation, we further finetune the pre-trained ARGenSeg model using image generation data to unlock its text-to-image generation capabilities. Training Objective Since our framework unifies both text and image generation outputs within the LLM codebook, the entire training process is directly supervised using cross-entropy loss, as shown in Fig. 2. During supervision construction, the <gen_start> token is added as marker before image generation begins. The model is expected to learn both when to initiate image generation and how to generate all the required visual tokens. The ground-truth visual tokens are obtained using the encoder and quantizer of the VQ-VAE. When constructing input embeddings, the visual tokens for the first scale are obtained by using the <gen_start> token as the query. For each subsequent scale, the input embeddings are derived by upsampling the visual token map rk1 of the previous scale to match the size of the current scale. Finally, the <gen_end> token is added to ensure the proper progression of subsequent predictions. 3.4 Inference During inference, our model follows next-token prediction strategy, generating outputs sequentially until the <gen_start> token is produced. This token then serves as query to initiate the generation of visual tokens for the first scale. For the subsequent K1 scales, query embeddings of size hk wk are obtained by upsampling and projecting the visual token map ˆrk1 predicted at the previous scale, enabling parallel generation of all visual tokens at the current scale. Since the upsampling process determines the number of queries, our framework naturally ensures alignment between the number of generated tokens and the input size required by the VQ-VAE decoder. Once the visual tokens for 5 Table 1: Performance comparison with state-of-the-art methods on three referring image segmentation benchmarks using cIoU. (ft) indicates models further finetuned on RefCOCO/+/g after mixed training. Paradigm Method Boundary Point-based PolyFormer-B [42] VistaLLM-7B [42] LISA-7B(ft) [29] PixelLM-7B [45] GSVA-7B [65] GSVA-7B(ft) LaSagnA-7B [60] VisionLLM v2 [62] OMG-LLAVA [73] OMG-LLAVA(ft) GLaMM [44] u-LLAVA [68] PSALM [75] GroundHog-7B [74] SAM4MLLM-8B [42] LMMHiMTok-8B [57] LMMHiMTok-8B(ft) ARGenSeg ARGenSeg (ft) Dedicated Segmentation Head-based Generation based RefCOCO RefCOCO+ val testA testB val testA testB RefCOCOg test val 74.8 74.5 74.9 73.0 76.4 77.2 76.8 76.6 75.6 78.0 79.5 83.0 83.6 78.5 79.8 81.1 85. 82.2 86.3 76.6 76.0 79.1 76.5 77.4 78.9 78.7 79.3 77.7 80.3 83.2 85.1 84.7 79.9 82.7 81.2 85.2 84.0 87.5 71.1 72.7 72.3 68.2 72.8 73.5 73.8 74.3 71.2 74.1 76.9 80.5 81.6 75.7 74.7 79.2 83. 80.1 82.7 67.6 69.1 65.1 66.3 64.5 65.9 66.4 64.5 65.6 69.1 72.6 77.1 72.9 70.5 74.6 77.1 79.7 77.9 82.3 72.9 73.7 70.8 71.7 67.7 69.6 70.6 69.8 69.7 73.1 78.7 81.7 75.5 75.0 80.0 78.8 82. 81.8 85.8 59.3 64.0 58.1 58.3 58.6 59.8 60.1 61.5 58.9 63.0 64.6 70.6 70.1 64.9 67.2 71.5 76.0 73.3 77.0 67.8 69.0 67.9 69.3 71.1 72.7 70.6 70.7 70.7 72.9 74.2 77.1 73.8 74.1 75.5 75.8 80. 78.4 81.7 69.1 70.9 70.6 70.5 72.0 73.3 71.9 71.2 70.2 72.9 74.9 78.0 74.4 74.6 76.4 76.7 80.6 79.6 83.5 all scales are obtained, the VAR tokenizer decodes them into the final image. To ensure smooth progression of subsequent inference, the <gen_end> token is manually added."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets As described in Sec. 3.3, we perform single-stage supervised finetuning to jointly train on both image segmentation and multimodal understanding data. Details of all datasets used are provided in Appendix A. The training of ARGenSeg relies entirely on publicly available external datasets. Specifically, we use 402K image segmentation samples, which are significantly fewer than the 2.91M samples used by HiMTok[57] and constitute strict subset of their data. For multimodal understanding, we use 1.25M samples derived from the open-source dataset of InternVL 1.2 [14]. Implementation Details Our model accepts input images of arbitrary resolutions, while the output images are generated at the resolution of 256 256. The image tokenizer uses downsampling ratio = 16, with feature dimension = 32 and visual codebook size = 4096. The model operates with = 10 scales. During training, we use the AdamW [36] optimizer with maximum learning rate of 4 105 and employ cosine learning rate scheduling. The batch size is set to 128. 4.2 Referring Segmentation Referring Expression Segmentation Recent works have increasingly focused on equipping multimodal large language models with image segmentation capabilities, aiming to leverage their strong language understanding for more complex segmentation tasks. Referring Expression Segmentation (RES) requires models to segment target objects in an image based on natural language descriptions. We evaluate our approach on standard RES benchmarks RefCOCO/+/g [37, 70]. Following prior works [29, 57], we assess two versions of our model: one trained on the mixed dataset, and another further finetuned on the in-domain training sets of RefCOCO/+/g. As shown in Tab. 1, our method consistently outperforms the previous state-of-the-art, HiMTok [57], across both versions, despite training on fewer segmentation data. It is worth noting that, our approach achieves superior results without relying on dedicated segmentation head, demonstrating the effectiveness of our unified multimodal understanding and generation framework. Table 2: Performance comparison with state-of-the-art methods on generalized referring expression segmentation. * indicates zero-shot performance. Method val testA cIoU gIoU cIoU gIoU cIoU gIoU testB LISA-7B [29] LISA-7B(ft) GSVA-7B [65] GSVA-7B(ft) LaSagnA* [60] PSALM* [75] GroundHog-7B [74] SAM4MLLM-8B [42] LMMHiMTok-8B [57] ARGenSeg 38.7 61.8 61.7 63.3 38.1 42.0 - 67.8 66.8 72.2 32.2 61.6 63.3 66.5 32.4 43.3 66.7 71.9 68.7 74.7 52.6 68.5 69.2 69.9 50.4 52.4 - 72.2 68.6 73. 48.5 66.3 70.1 71.1 47.3 54.5 - 74.2 67.6 73.7 44.8 60.6 60.3 60.5 42.1 50.6 - 63.4 65.8 70.0 39.7 58.8 61.3 62.2 38.9 52.5 - 65.3 64.1 70. Average 42.8 62.9 64.3 65.6 41.5 49.2 66.7 69.1 66.9 72.4 Figure 3: Multi-scale generation process of the segmentation mask. The model first localizes the target object and then progressively refines its boundaries. Fig. 3 illustrates the multi-scale mask generation process of ARGenSeg. The model first locates the target object and then progressively refines the segmentation boundaries. This coarse-to-fine reasoning process aligns with human intuition and enhances the robustness of image segmentation. Generalized Referring Expression Segmentation We further evaluate our model on the more challenging gRefCOCO benchmark [32], where segmentation instructions may refer to multiple objects or none at all. As shown in Tab. 2, our method outperforms all prior approaches that rely on dedicated segmentation heads, highlighting the strong understanding and segmentation capabilities of our unified framework. 4.3 Multumodal Understanding Our model adopts InternVL 2.5 [13] as the underlying MLLM and is finetuned on both understanding and segmentation data. To fairly assess the effect of adding segmentation supervision on the models understanding capability, we finetune baseline using only understanding data. We evaluate the models understanding performance using two tasks. The first task is visual grounding, where we use the RefCOCO/+/g datasets for referring expression comprehension (REC). As shown in Tab. 3, our model successfully retains and even slightly enhances its grounding ability while acquiring segmentation capabilities. The second task evaluates object hallucination in MLLMs using POPE [30] as the benchmark. Results in Tab. 3 also demonstrate performance improvement of our model compared to the baseline. These results highlight the effectiveness of our proposed framework in unifying understanding and segmentation tasks. further discussion on the understanding performance is provided in Appendix C.1. 7 Table 3: Multimodal understanding performance compared with the baseline. * indicates further finetuning on understanding data. Method RefCOCO RefCOCO+ val testA testB val testA testB RefCOCOg test val POPE InternVL2.5-8B* [13] ARGenSeg 89.0 89.6 92.6 92.8 84.3 84.4 83.4 83. 89.1 88.8 76.5 76.5 83.5 86.1 85.0 85.6 86.73 87.57 4.4 Function Extension Interactive Segmentation Interactive segmentation allows users to provide diverse input prompts during segmentation tasks to meet varying application needs. We finetune ARGenSeg on the COCOInteractive dataset [75] to unlock its interactive segmentation capabilities. During training, various forms of interactive prompts are used, including points, scribbles, and bounding boxes. Bounding boxes are provided as textual input to the MLLM, while points and scribbles are represented as binary masks and fed in as additional visual inputs. We observe that, building upon pre-trained segmentation capabilities, the model quickly adapts to interactive segmentation tasks. Qualitative results are shown in the top portion of Fig. 4, while the quantitative evaluation can be found in the Appendix C.2. Image Generation Our model leverages universal image tokenizer, enabling the potential for image generation. We finetune ARGenSeg on 1.28M class-based samples from the ImageNetInstruct-class dataset [78], using batch size of 512 for 20k iterations. This successfully enables class-conditional image generation, as illustrated in Fig. 1. We then continue training for an additional 30k iterations with batch size of 256 on the ImageNet-Instruct1270K dataset [78], which is based on instruction-conditioned generation. The results of instruction-based image generation are shown in the bottom of Fig. 4. Notably, our model achieves these results without relying on pre-trained generation model, using only small amount of data and training iterations. Figure 4: Top: Visualization of interactive segmentation. Points and scribbles are provided as visual prompts, while bounding boxes are input via text. Bottom: Visualization results of instruction-based image generation. The model is trained on image generation data for only 50k iterations. 4.5 Efficiency Analysis We compare ARGenSeg with previous autoregressive generation models and MLLM-based segmentation methods in terms of inference time required to generate 256 256 image or mask. All experiments are conducted using official implementations on an NVIDIA A100 GPU. Segmentation performance is evaluated using cIoU on RefCOCO-val. Detailed results are provided in Tab. 4. Compared to sequential token generation approaches such as Emu3 [59], our parallel inference achieves more than 10 speedup. While VARGPT [78] also employs VAR as its visual tokenizer, 8 Table 4: Computational efficiency comparison. \"Num.\" represents the number of required tokens. Time is tested by seconds per image. Table 5: Ablation study on the impact of understanding capability, pretraining-stage, and generation projector on segmentation performance. Method Paradigm Num. cIoU Time Experiment Ref. Ref.+ Ref.g Average VQ-GAN[19] Emu3 [59] VARGPT [78] VAR + Vis.Dec PixelLM [45] Query + Seg.Dec HiMTok [57] Mask Tokenizer VAR Tokenizer ARGenSeg 1024 680 6 32 - - 73.0 81.1 82.2 59.4 2.64 0.91 1.89 1.28 Baseline Only-Seg. Gen Projetctor Pretrain 82.2 80.5 80.5 80.8 77.9 73.8 73.7 74.9 78.4 73.2 73.4 74. 79.5 75.8 75.9 76.6 Table 6: Ablation study on the visual tokenizer. All results are reported on the val splits, using gIoU (per-sample IoU averaged over the dataset) as the segmentation metric. Tokenizer Type Prediction RefCOCO RefCOCO+ RefCOCOg Average Time Single-scale Multi-scale next-token next-scale 82.1 80.5 71.8 76.7 65.8 70.4 73.23 75.87 5.50 1.28 our method is approximately 2 more efficient, due to its simplified architecture. In contrast to VARGPT, our model directly uses the classification head to predict token IDs from the VAR codebook, eliminating the need for an additional transformer-based visual decoder. PixelLM [45], identifierbased approach, uses only six tokens and dedicated segmentation decoder, making it slightly faster than ARGenSeg. However, its segmentation performance is significantly lower. While HiMTok [57] employs dedicated mask tokenizer to achieve notable segmentation performance using only 32 visual tokens for efficiency, our method achieves superior performance while offering clear advantage in inference speed. 4.6 Ablation Study Ablation on Understanding Data We compare our baseline, fine-tuned on both understanding and segmentation data, against counterpart trained solely on segmentation data. As shown in Tab. 5, incorporating understanding data significantly improves performance on reasoning-based segmentation, particularly on the semantically challenging RefCOCO+/g dataset. This highlights the value of unifying segmentation with multimodal large language model. Ablation on Model Architecture and Training Strategy We analyze the effects of model architecture and training strategy. First, to ablate the architecture, we replace our default single-layer generation projector with two-layer variant. Results indicate that the simpler design is sufficient. Second, to assess the training strategy, we introduce pre-training phase where only the generation projector is trained, followed by full fine-tuning stage. As shown in Tab. 5, this two-stage approach offers only marginal gains on RefCOCO+/g and little impact on RefCOCO, while increasing training complexity. Therefore, for efficiency, our final model adopts direct, single-stage fine-tuning strategy. Ablation on Visual Tokenizer We ablate our multi-scale visual tokenizer by comparing it against single-scale tokenizer, for which we adopt the pre-trained VQ-GAN [59] from Janus [61]. As shown in Tab. 6, using multi-scale scheme not only demonstrates clear speed advantage but also improves robustness through its inherent coarse-to-fine refinement process. Further ablations, including an analysis of using semantic embeddings instead of visual tokens, are provided in Appendix D."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present ARGenSeg, unified framework that integrates image segmentation into multimodal large language models through an image generation paradigm. To address the unique challenges of segmentation, we design the framework so that the MLLM directly outputs image tokens for pixel-level accuracy and utilizes multi-scale image generation for high responsiveness and robustness through coarse-to-fine refinement. Our experiment results are the first to show that unified MLLM models can perform state-of-the-art segmentation without any extra task-specific segmentation heads, providing an effective technical pathway for unified AGI."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Inclusion AI, Biao Gong, Cheng Zou, Chuanyang Zheng, Chunluan Zhou, Canxiang Yan, Chunxiang Jin, Chunjie Shen, Dandan Zheng, Fudong Wang, et al. Ming-omni: unified multimodal model for perception and generation. arXiv preprint arXiv:2506.09344, 2025. [3] Inclusion AI, Biao Gong, Cheng Zou, Dandan Zheng, Hu Yu, Jingdong Chen, Jianxin Sun, Junbo Zhao, Jun Zhou, Kaixiang Ji, et al. Ming-lite-uni: Advancements in unified architecture for natural multimodal interaction. arXiv preprint arXiv:2505.02471, 2025. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [6] Paul Bergmann, Kilian Batzner, Michael Fauser, David Sattlegger, and Carsten Steger. The mvtec anomaly detection dataset: comprehensive real-world dataset for unsupervised anomaly detection. International Journal of Computer Vision, 129(4):10381059, 2021. [7] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ada comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 95929600, 2019. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [9] Sébastien Bubeck, Varun Chadrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4, 2023. [10] Jie Cao and Jing Xiao. An augmented benchmark dataset for geometric question answering through dual parallel text encoding. In Proceedings of the 29th international conference on computational linguistics, pages 15111520, 2022. [11] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David Fleet, and Geoffrey Hinton. unified sequence interface for vision tasks. Advances in Neural Information Processing Systems, 35:3133331346, 2022. [12] Yi-Chia Chen, Wei-Hua Li, Cheng Sun, Yu-Chiang Frank Wang, and Chu-Song Chen. Sam4mllm: Enhance multi-modal large language model for referring expression segmentation. In European Conference on Computer Vision, pages 323340. Springer, 2024. [13] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [14] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [15] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. 10 [16] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1 113, 2023. [18] Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension. arXiv preprint arXiv:1710.10723, 2017. [19] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [20] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. arXiv preprint arXiv:2310.01218, 2023. [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [22] Ziyuan Huang, DanDan Zheng, Cheng Zou, Rui Liu, Xiaolong Wang, Kaixiang Ji, Weilong Chai, Jianxin Sun, Libin Wang, Yongjie Lv, et al. Ming-univision: Joint image understanding and generation with unified continuous tokenizer. arXiv preprint arXiv:2510.06590, 2025. [23] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [24] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. [25] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [26] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer. In European Conference on Computer Vision, pages 498517. Springer, 2022. [27] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [28] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [29] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. [30] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [32] Chang Liu, Henghui Ding, and Xudong Jiang. GRES: Generalized referring expression segmentation. In CVPR, 2023. [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [35] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and Manmatha. Polyformer: Referring image segmentation as sequential polygon generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1865318663, 2023. [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [37] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. [38] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [39] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. [40] OpenAI. Chatgpt. https://chat.openai.com/, 2023. Accessed: 2023. [41] Xichen Pan, Satya Narayan Shukla, Aashu Singh, Zhuokai Zhao, Shlok Kumar Mishra, Jialiang Wang, Zhiyang Xu, Jiuhai Chen, Kunpeng Li, Felix Juefei-Xu, et al. Transfer between modalities with metaqueries. arXiv preprint arXiv:2504.06256, 2025. [42] Shraman Pramanick, Guangxing Han, Rui Hou, Sayan Nag, Ser-Nam Lim, Nicolas Ballas, Qifan Wang, Rama Chellappa, and Amjad Almahairi. Jack of all tasks master of many: Designing general-purpose coarse-to-fine vision-language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1407614088, 2024. [43] Lu Qi, Lehan Yang, Weidong Guo, Yu Xu, Bo Du, Varun Jampani, and Ming-Hsuan Yang. Unigs: Unified representation for image generation and segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 63056315, 2024. [44] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. [45] Zhongwei Ren, Zhicheng Huang, Yunchao Wei, Yao Zhao, Dongmei Fu, Jiashi Feng, and Xiaojie Jin. Pixellm: Pixel reasoning with large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2637426383, 2024. [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [47] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [48] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Emu: Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. 12 [49] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [50] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [51] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. [52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [53] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [55] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. arXiv preprint arXiv:1601.07140, 2016. [56] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [57] Tao Wang, Changxu Cheng, Lingfeng Wang, Senda Chen, and Wuyue Zhao. Himtok: Learning hierarchical mask tokens for image segmentation with large multimodal model. arXiv preprint arXiv:2503.13026, 2025. [58] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric tasks. Advances in Neural Information Processing Systems, 36:6150161513, 2023. [59] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [60] Cong Wei, Haoxian Tan, Yujie Zhong, Yujiu Yang, and Lin Ma. Lasagna: Language-based segmentation assistant for complex queries. arXiv preprint arXiv:2404.08506, 2024. [61] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [62] Jiannan Wu, Muyan Zhong, Sen Xing, Zeqiang Lai, Zhaoyang Liu, Zhe Chen, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, et al. Visionllm v2: An end-to-end generalist multimodal large language model for hundreds of vision-language tasks. Advances in Neural Information Processing Systems, 37:6992569975, 2024. [63] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. [64] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [65] Zhuofan Xia, Dongchen Han, Yizeng Han, Xuran Pan, Shiji Song, and Gao Huang. Gsva: In Proceedings of the Generalized segmentation via multimodal large language models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 38583869, 2024. 13 [66] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer, 2025. [67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [68] Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Fanyi Wang, Yanchun Xie, Yi-Jie Huang, and Yaqian Li. u-llava: Unifying multi-modal tasks via large language model. In ECAI 2024, pages 618625. IOS Press, 2024. [69] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. An improved baseline for reasoning segmentation with large language model. CoRR, 2023. [70] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. [71] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. [72] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940128966, 2024. [73] Tao Zhang, Xiangtai Li, Hao Fei, Haobo Yuan, Shengqiong Wu, Shunping Ji, Chen Change Loy, and Shuicheng Yan. Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. Advances in Neural Information Processing Systems, 37:7173771767, 2024. [74] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. GroundIn Proceedings of the hog: Grounding large language models to holistic segmentation. IEEE/CVF conference on computer vision and pattern recognition, pages 1422714238, 2024. [75] Zheng Zhang, Yeyao Ma, Enming Zhang, and Xiang Bai. Psalm: Pixelwise segmentation with large multi-modal model. In European Conference on Computer Vision, pages 7491. Springer, 2024. [76] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 633641, 2017. [77] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [78] Xianwei Zhuang, Yuxin Xie, Yufan Deng, Liming Liang, Jinghan Ru, Yuguo Yin, and Yuexian Zou. Vargpt: Unified understanding and generation in visual autoregressive multimodal large language model. arXiv preprint arXiv:2501.12327, 2025."
        },
        {
            "title": "A Implementation Details",
            "content": "Datasets The datasets used for image segmentation, multimodal understanding, and image generation are listed in Tab. 7. To ensure fair comparison, we exclusively use subsets of the data employed by the previous state-of-the-art method, HiMTok [57]. Specifically, we train on 402K segmentation samples compared to HiMToks 2.91M, and 1.25M multimodal understanding samples compared to HiMToks 4.2M. Image generation data are used only in the optional function-extension stage. Table 7: Training data used in our experiments. Task Datasets Image Segmentation ADE20K(20K) [76], COCO-Panoptic(118K) [31], gRefCOCO (79K) [32], RefCOCO/+/g(127K) [37, 70], LISA++ Inst.Seg(58K) [69] Multimodal Understanding AI2D [25], ChartQA[38], COCO-Text[55], DocVQA[18], LLaVA-150K[34], GQA[23], DVQA[24], OCR-VQA[39], TextVQA[47], SynthDoG-EN [26], InternVL-SA1B-Caption [14], VisualGenome [28], GeoQA+[10] Image Generation ImageNet-Instruct-class [78], ImageNet-Instruct1270K [78] Inference Details During inference, we get visual outputs exclusively from the logits corresponding to visual tokens in the MLLM codebook. This constraint ensures compatibility with the visual tokenizer and enables successful reconstruction of the image. For image segmentation tasks, we adopt deterministic argmax sampling strategy to obtain the predicted visual tokens. For image generation tasks, we apply classifier-free guidance (CFG) to compute the output distribution over visual tokens, followed by top-k sampling to enhance the diversity and quality of generated images."
        },
        {
            "title": "B Additional Qualitative Results",
            "content": "Multi-scale Image Generation We provide visualization of segmenting similar objects in the same image using different instructions, as shown in Fig.5. From the multi-scale mask generation process, it is evident that our model can correctly understand and localize the target based on the given instructions. The ability to correctly follow distinct segmentation commands indicates that ARGenSeg possesses robust understanding of both spatial positions and semantic relationships. Comparison with Single-scale Generation We compare our method with HiMTok [57], treating it as representative single-scale generative segmentation approach. We conducted thorough evaluation on the test set and visualized cases where ARGenSeg succeeds while HiMTok fails. As shown in Fig. 6, these cases reveal two primary advantages of our coarse-to-fine, multi-scale generation scheme: (1) Robust Target Identification in Multi-object Scenarios. The initial coarse localization stage effectively identifies the target object even when multiple similar objects are present. (2) Enhanced Mask Quality through Progressive Refinement. Following target identification, the multi-scale refinement process progressively improves mask precision for higher-quality segmentation. For instance, in the case of partially occluded teddy bear, both HiMTok and our coarse localization stage initially segment only visible part. However, our models subsequent fine-grained refinement successfully reconstructs the entire object while correctly excluding the occluder."
        },
        {
            "title": "C Additional Quantitative Results",
            "content": "C.1 Performance on Multimodal Understanding We further assess the multimodal understanding capabilities of ARGenSeg. As shown in Tab. 8, the inclusion of segmentation data does not cause the model to lose its reasoning capability. While we 15 Figure 5: Visualization of using different segmentation instructions in the same image. Figure 6: Comparison between multi-scale and single-scale generative segmentation approach. The examples highlight scenarios where the multi-scale approach excels. observe slight performance drops on some benchmarks, we attribute this minor degradation not to the segmentation task itself, but to the significantly smaller and lower-quality understanding corpus used for fine-tuning (1.25M vs. the 16.3M samples used for InternVL-2.5 [13]). To validate this hypothesis, we conducted control experiment: fine-tuning InternVL-2.5 solely on the same understanding data for an increasing number of steps. The performance declined monotonically, mirroring the trend observed with joint segmentation training and thus confirming our attribution. Table 8: Multimodal understanding results across benchmarks. Method POPE TextVQA VQAv2 MMMU-val AI2D InternVL2.5-ft-1ep InternVL2.5-ft-4ep ARGenSeg 86.73 86.01 87. 63.54 59.73 56.98 80.40 79.28 77.87 43.7 36.8 33.4 78.7 74.8 69.6 C.2 Results on Interactive Segmentation To ensure fair comparison with HiMTok, which was not trained on interactive-segmentation data, we omitted this task from our main experiments. Here, we evaluate our model on the COCO-Interactive benchmark [75], reporting the cIoU metric. It is worth noting that while PSALM [75] was fine-tuned for 10 epochs according to its official implementation, our model is fine-tuned for only single epoch due to computational constraints. As shown in Tab. 9, ARGenSeg significantly outperforms SAM [27] in interactive segmentation. Moreover, it achieves performance comparable to PSALM with substantially less fine-tuning, which underscores the strong generalization capabilities of our model."
        },
        {
            "title": "D Additional Ablation Studies",
            "content": "Table 9: Quantitative results on interactive segmentation. The results for SAM and PSALM are sourced directly from the PSALM paper. Method Point Scribble Box SAM-B [27] SAM-L [27] PSALM [75] ARGenSeg 33.6 37.7 74.0 65.6 80.0 68.6 68.7 71.6 80.9 79. Table 10: Ablation study of MLLM backbones and image generation strategies. The segmentation performance is measured in cIoU. Method Backbone Generation Strategy RefCOCO RefCOCO+ RefCOCOg HiMTok [57] ARGenSeg-LLaVA ARGenSeg-InternVL ARGenSeg-DiT Single-scale VQ InternVL-2.5 Multi-scale VQ LLaVA1.5 InternVL-2.5 Multi-scale VQ InternVL-2.5 Diffusion Head 81.1 72.7 82.2 59.0 77.1 68.3 77.9 62.7 75.8 69.1 78.4 64.1 D.1 Ablation on MLLM Backbone Our approach, which integrates VQVAE codebook into the MLLMs token space, is designed to be model-agnostic. To demonstrate this portability, we replaced the default InternVL-2.5 backbone with LLaVA-1.5 [33], LLaMA-2-based MLLM. As shown in Tab. 10, our pipeline successfully imparts segmentation capabilities to LLaVA-1.5. As established in Sec. 4.6, referring segmentation performance is highly correlated with the MLLMs underlying understanding ability. Consequently, given LLaVA-1.5s weaker understanding capabilities compared to InternVL-2.5, the resulting segmentation performance is expectedly lower. Nevertheless, Tab. 10 shows that with the same powerful InternVL-2.5 backbone, our method outperforms HiMTok. This confirms that our performance gains are inherent to our approach and not merely byproduct of stronger backbone. 17 Figure 7: Comparison between direct visual token generation and DiT-based generation. The DiT-based approach, which uses semantic embeddings from the MLLM, struggles with pixel-level accuracy, leading to artifacts like spatial shifts and imprecise boundaries. D.2 Ablation on Image Generation Strategy To further validate our choice of generation strategy, we explore an alternative approach where the MLLM outputs semantic embeddings to separate diffusion head (DiT) for segmentation, inspired by MetaQuery [41]. Specifically, we configure the MLLM to generate learnable queries, which are then mapped to the feature space of the pre-trained SANA-1.5 1.6B [66] via connector module. This alternative strategy, labeled as ARGenSeg-DiT in Tab. 10, led to severe performance degradation. As show in Fig. 7, while the model could roughly localize the target region, the generated masks suffered from significant artifacts, such as spatial shifts and inflation, indicating poor pixel-level accuracy. This experiment underscores the importance of the MLLM directly generating discrete image tokens to maintain the high pixel-level precision crucial for segmentation tasks."
        },
        {
            "title": "E Limitations",
            "content": "This paper proposes novel image segmentation paradigm based on autoregressive image generation, integrating multimodal understanding, generation, and image segmentation into unified framework. Our model demonstrates strong performance across range of segmentation tasks, and further shows the potential to extend to more complex scenarios, such as interactive segmentation and text-to-image generation. The unified framework also shows promise for expanding to broader tasks, such as image editing and depth estimation. However, due to resource constraints, exploring these extensions is beyond the scope of this work, and we consider them as promising directions for future research."
        },
        {
            "title": "F Broader Impacts",
            "content": "This work contributes to the development of unified multimodal frameworks by integrating dense image segmentation into the unified multimodal understanding and generation models. The proposed framework may inspire future research toward more generalizable, modular, and efficient visuallanguage models that require fewer task-specific components. Potential applications include humanrobot interaction, assistive vision systems, and real-world visual understanding under low supervision. However, like most large-scale models, ARGenSeg may inherit biases from pre-trained components or datasets. Care should be taken to evaluate fairness and robustness when deploying it in real-world scenarios, especially in sensitive domains such as healthcare or surveillance."
        }
    ],
    "affiliations": [
        "Ant Group"
    ]
}