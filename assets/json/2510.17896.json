{
    "paper_title": "Long-Context Attention Benchmark: From Kernel Efficiency to Distributed Context Parallelism",
    "authors": [
        "Tao Bu",
        "Qiangang Wang",
        "Bowen Zeng",
        "Hanwen Sun",
        "Yunpeng Huang",
        "Chun Cao",
        "Jingwei Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing a major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose a unified benchmark that integrates representative attention kernels and context parallel mechanisms with a modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights method-specific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 6 9 8 7 1 . 0 1 5 2 : r LONG-CONTEXT ATTENTION BENCHMARK: FROM KERNEL EFFICIENCY TO DISTRIBUTED CONTEXT PARALLELISM Tao Bu 1 Qiangang Wang 1 Bowen Zeng2 Hanwen Sun3 Yunpeng Huang1 Chun Cao1 Jingwei Xu 1 1State Key Laboratory for Novel Software Technology, Nanjing University, China 2Zhejiang University, China 3Peking University, China {butao,qgwang}@smail.nju.edu.cn, jingweix@nju.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Transformer-based large language models (LLMs) have achieved remarkable success, yet their standard attention mechanism incurs quadratic computation and memory costs with respect to sequence length, posing major bottleneck for long-context training. Prior work tackles this challenge along two directions: (1) kernel-level optimizations, which accelerate dense and sparse attention operators; and (2) module-level strategies, often referred to as distributed attention or context parallel training, which scale attention across multiple devices. However, systematic evaluation still remains limited: operator-level comparisons are often incomplete, while context parallel strategies are typically framework-specific, with unclear performance analysis across contexts. To address these gaps, we propose unified benchmark that integrates representative attention kernels and context parallel mechanisms with modular and extensible interface for evaluation. The benchmark evaluates methods along two critical dimensions: (1) attention mask patterns, which strongly affect efficiency, scalability, and usability, and (2) sequence length and distributed scale, which determine performance under extreme long-context training. Through comprehensive experiments on the cluster of up to 96 GPUs, our benchmark enables reproducible comparisons, highlights methodspecific trade-offs, and provides practical guidance for designing and deploying attention mechanisms in long-context LLM training."
        },
        {
            "title": "INTRODUCTION",
            "content": "The Transformer architecture, powered by the attention mechanism, has become the foundation of large language models (LLMs) (Achiam et al., 2023; Team et al., 2023; Dubey et al., 2024). With the guidance of the scaling law (Kaplan et al., 2020; Tay et al., 2022), current state-of-the-art LLMs, such as GPT (Agarwal et al., 2025), Gemini (Team et al., 2024), and DeepSeek (Liu et al., 2024), contain billions of parameters and are trained on trillions of data using large-scale distributed GPU clusters. However, as model size and training data continue to grow, the computational and memory costs of conventional attention scale quadratically with sequence length, posing fundamental efficiency bottleneck for large-scale LLM training (Dai et al., 2024). Although the context window of LLMs has expanded dramatically from 4K tokens to 128K (Grattafiori et al., 2024), 1M (Yang et al., 2025a), and even 10M (Team et al., 2024) tokens, the design and performance characteristics of long-context attention mechanisms at these scales in distributed training remain insufficiently understood (Gao et al., 2024b). Recent research on efficient long-context attention at scale has progressed along two main directions. The first focuses on kernel-level optimizations (Zhang et al.), such as dense and sparse kernels, Equal contribution. Corresponding Author 1 reducing attention complexity on single GPU. The second emphasizes module-level designs, or context parallelism (Duan et al., 2024), which partition long sequences (e.g., 32K128K tokens) across multiple GPUs with tailored communication and scheduling for scalability. Despite these advances, comprehensive analyses of long-context attention mechanisms remain lacking. Attention operators differ significantly in their support for mask patterns, and even the same operator can exhibit substantial performance variation across masks. Currently, no unified evaluation has been established. Furthermore, existing context parallel attention mechanisms are often tightly integrated with specific training frameworks (e.g., DeepSpeed (Rasley et al., 2020) and InternEvo (Chen et al., 2024)), which limits reusability and hinders systematic comparison. As result, researchers lack clear understanding of the trade-offs between methods, and practitioners have no reliable benchmark or reference to guide the selection of attention mechanisms in long-context training. To address these issues, we collect representative attention operators and context parallel mechanisms, and design unified framework to systematically benchmark their capabilities, limitations, and potential risks in ultra-long context training. In our framework, we establish unified data preparation interface that supports both non-distributed kernels and context parallel attention mechanisms, enabling fair evaluation across methods. Specifically, for non-distributed kernel scenarios, we integrate variety of dense and sparse attention kernels, implementing standardized interfaces that eliminate inconsistencies in data representation and ensure comparability under the benchmark. For distributed scenarios, we reconstruct and optimize representative context parallel attention mechanisms within the unified framework, providing efficient, scalable implementations with modular interfaces. Building on the foundation, we conduct large-scale experiments and in-depth analyses along two critical dimensions: (1) attention mask patterns (up to 14 mask patterns), which are often overlooked but have significant impact on efficiency, scalability, and usability, and (2) context length and distributed scale, where we systematically evaluate performance trends and capability limits as both the input length and distributed scale grow, reaching up to 512K on 96 GPUs. We hope our results offer valuable insights for research on long-context training of large models, as well as for the design and development of next-generation distributed attention mechanisms. Our contributions are threefold: 1. Unified benchmarking: we provide standardized framework with consistent data preparation for fair evaluation of attention mechanisms across diverse long-context scenarios. 2. Modular components: we unify dense and sparse kernels under high-level modular interface, and provide optimized distributed attention in terms of context parallelism. 3. In-depth analysis: we conduct extensive experiments across dense long-context scenarios to identify key factors affecting attention efficiency and scalability, providing valuable guidance for ultra-long context training and development."
        },
        {
            "title": "2 LONGCA-BENCH",
            "content": "LongCA-bench is benchmark designed to evaluate the efficiency of long-context attention across both single-device kernels and distributed context parallel mechanisms. The benchmark consists of three core components: (1) unified data preparation interface that standardizes preprocessing, (2) unified input representation interface that supports 7 dense and 5 sparse attention kernels, and (3) an optimized context parallelism framework that incorporates 5 distributed attention mechanisms. Together, these components provide systematic and extensible platform for analyzing long-context attention, enabling fair comparisons across operator-level efficiency and distributed scalability. 2.1 DATA PREPARATION We first describe the data preparation process in the benchmark. To generate inputs practical for long-context attention benchmarking, we introduce dedicated data preparation interface. Rather than directly using the downstream datasets, our interface combines diverse mask patterns with variable lengths of sequence sampling, ensuring that the evaluation data accurately reflects the characteristics and challenges of long-context training. 2 Figure 1: The architecture of LongCA benchmark Figure 2: Attention mask patterns 2.1.1 INPUT MASK PATTERNS Different tasks require specific mask types based on the training scenario. In LongCA-bench, we categorize total of 14 mask patterns into two major classes (see Figure 2): 12 static masks (6 regular and 6 heterogeneous), and 2 dynamic masks. The key distinction lies in whether the mask can be predetermined before training or must be generated adaptively during the training process. Static regular mask. The FULL and CAUSAL masks are the most widely used in training (Vaswani et al., 2017). Considering the document-level variants, FULL DOCUMENT and CAUSAL DOCUMENT are employed for efficient sequence packing and in-batch/in-token processing (Krell et al., 2021; Dehghani et al., 2023). In addition, by applying the sliding window variants, FULL SLIDING WINDOW and CAUSAL SLIDING WINDOW can leverage sparsity to balance computational cost and token coverage (Beltagy et al., 2020). Static heterogeneous mask. SHARED QUESTION mask used in reward models allows multiple answers to share the same question (Ouyang et al., 2022). GLOBAL SLIDING mask is designed to effectively capture both global context and local details (Zaheer et al., 2020). CAUSAL BLOCKWISE mask, which is widely adopted in in-context learning, restricts demonstrations to local blocks while letting the test example attend globally, supporting long-context evaluation (Bertsch et al., 2024). The PREFIX LM CAUSAL and PREFIX DOCUMENT masks are specifically tailored to introduce prefix for language modeling tasks (Raffel et al., 2020). BLOCK CAUSAL DOCUMENT mask combines the block and document concepts and is widely used in multimodal model training (Zewei & Yunpeng, 2025). Dynamic mask. In long-context scenarios, block sparse masks reduce computational latency and memory usage by restricting attention computation to the most salient blocks of the input. Since 3 the selected blocks depend on the contextual input, the mask pattern varies across examples. Block sparse masks have been widely adopted in both natural language processing (Lu et al., 2025; Yuan et al., 2025; Xu et al., 2025; Guo et al., 2024; Ye et al., 2025) and visual generation (Zhang et al., 2025d; Zewei & Yunpeng, 2025; Yang et al., 2025b). We categorize block sparse masks into two types: uniform and variable (Figure 2). The uniform block mask applies attention blocks of fixed size (e.g., 6464) across the entire attention map and computes the selected blocks during attention. In contrast, the variable block mask provides greater flexibility by allowing blocks of different sizes, offering more efficient and expressive representation of sparse attention patterns. 2.1."
        },
        {
            "title": "INPUT DATA SAMPLING",
            "content": "Dense data sampling. The mask specifies the area of attention interactions within context window. As the context window expands (e.g., from 8K to 512K), input data sampling becomes crucial. To ensure that the benchmark data reflects realistic training scenarios, we analyzed several widely used public pretraining datasets, including Pile (Gao et al., 2020), ProLong64K (Gao et al., 2024a), ProLong512K (Gao et al., 2024a), Slimpajama-Per-Source-Length-Upsample (Yaofu, 2024), OpenWebText (Gokaslan et al., 2019), and C4 (Raffel et al., 2020) (see Appendix A.1 for details). Following prior findings (Fu et al., 2024; Gao et al., 2024b), we note that: (1) language model training typically requires datasets from diverse sources; (2) extending the context length requires maintaining domain diversity while upsampling long-sequence samples; and (3) mixing long-context sources (e.g., code repositories and books) with high-quality short-text data improves long-context modeling without sacrificing overall performance. In our benchmark, the data sampling method therefore uses the Pile dataset for samples up to 8K, ProLong64K for long-context samples up to 64K, and ProLong512K for ultra-long samples up to 512K. This combination ensures that evaluation data reflects realistic training scenarios across different context scales. Sparse data sampling. Mask generation for block sparse attention in our benchmark follows standard methodology (Xia et al., 2025; Lu et al., 2025; Yuan et al., 2025; Zhang et al., 2025d). The attention matrix is first partitioned into two-dimensional grid of blocks, with either uniform or variable pre-defined block sizes. An importance score is then computed for each block in the grid, which in multi-head attention may be assigned on per-head or per-group basis. Guided by target sparsity ratio, top-K selection is performed for each query block to identify the most salient key blocks for attention computation. In our benchmark, however, we simplify the process to specifically evaluate kernel performance under varying sparsity levels (e.g., 0.2, 0.5, and 0.8). Instead of computing explicit important scores, we simulate the scoring and selection process by randomly generating block masks to achieve the desired sparsity. To create real-world workloads, we evaluate sequence lengths from 32K to 128K, sampled at 32K intervals. This range is derived from analyzing prominent benchmarks for block sparse attentions primary applications in video generation (e.g., VBench (Huang et al., 2024)) and LLMs (e.g., RULER (Hsieh et al., 2024)). Our evaluation covers both MHA and GQA using uniform block sizes of 6464 and 128128 (see Appendix A.2 for full results). 2.2 ATTENTION KERNEL Efficient attention kernels aim to reduce time and memory complexity without compromising expressiveness. The most straightforward approach is hardware acceleration, which speeds up computation without altering the original attention logic. Another common strategy leverages the inherent sparsity of attention, skipping unnecessary computations, often guided by dynamic sparse mask. Dense attention kernel. We integrate seven dense attention kernels and categorize their support for different mask types (see Table 1). Since kernels often apply for different requirements on data structure and mask formats, we implement dedicated adapter interfaces for each. These interfaces generate kernel-specific input representations from unified data format, eliminating inconsistencies in data expression across kernels. This design simplifies input preparation for diverse mask scenarios, ensures comparability within the benchmark, and provides unified solution for future kernel extensions. As baselines in our benchmark, we include both the step-by-step naıve attention and PyTorchs fused scaled dot product attention (SDPA) (PyTorch Contributors, 2024b), both construct full 2D masks and theoretically support arbitrary masking patterns. For hardware-optimized kernels, we 4 Table 1: Dense kernel support across mask patterns Dense Kernels Mask Type FULL CAUSAL FULL SLIDING WINDOW CAUSAL SLIDING WINDOW FULL DOCUMENT CAUSAL DOCUMENT SHARE QUESTION CAUSAL BLOCKWISE GLOBAL SLIDING PREFIX LM CAUSAL PREFIX LM DOCUMENT BLOCK CAUSAL DOCUMENT Naive-Torch SDPA FA2 FA3 cuDNN-Fused-Attn FlexAttn FlashMask Table 2: Characteristics of sparse kernels Sparse Kernels Characteristics VSA Triton VSA FA2 Sparse FlexAttention FlashInfer Uniform/Variable Masks Uniform only Uniform only Uniform only Forward only Forward/Backward 128 only Block Size GQA Support sm80 GPU Support Performance Medium Memory Overhead Low Both 64 only sm80 Medium Low Both 64 only sm90 High Low Both Both Arbitrary sm80 Low High Both Forward only Arbitrary sm80 Medium Medium integrate the FlashAttention series, including FA (Dao et al., 2022), FA2 (Dao, 2023), FA3 (Shah et al., 2024), as well as cuDNN fused kernels (NVIDIA Corporation, 2025). These kernels employ advanced techniques such as shared memory, block-wise partitioning, warp scheduling, FP8, and asynchronous processing. For flexible kernels, we integrate FlexAttention (Flex) (Dong et al., 2024), general fused operator with memory complexity close to O(S2), where denotes the sequence length, that generates specialized kernels based on per-position boolean functions and enables compatibility with arbitrary masks. We also include FlashMask (Wang et al., 2025a), which introduces column-wise representation to optimize heterogeneous computation. Sparse attention kernel. Sparse attention significantly reduces the computational complexity of attention for long sequences. Due to its versatile mask representation, block sparse attention is widely used in state-of-the-art sparse attention methods (Lu et al., 2025; Zhang et al., 2025b; Yuan et al., 2025; Zhang et al., 2025d; Xu et al., 2025). Therefore, our benchmark incorporates five block sparse attention kernels to evaluate long-context sparse attention. We categorize these kernels into two main types. The first type consists of dedicated block sparse attention kernels, which are highly optimized for sparse patterns with uniform block sizes (e.g., 6464). Representative implementations include VSA (Zhang et al., 2025d), its Triton-based version (Triton VSA), and the FlashAttention-2-based block sparse attention (FA2 Sparse) (Guo et al., 2024). The second type comprises general-purpose sparse attention kernels, which offer greater flexibility and support arbitrary block structures. They are compatible with both uniform and variable block sparse masks. This category includes FlexAttention (Dong et al., 2024) and FlashInfer (Ye et al., 2025). These kernels exhibit different characteristics, as summarized in Table 2 (refer to Appendix A.3 for details). We evaluate performance through comparisons using two mask types: uniform block mask and variable block mask. Note that backward computation in training is supported by only limited set of block-sparse attention kernels. For comprehensiveness, we select FA2 Sparse and FlashInfer, two inference-side methods, for comparisons in our benchmark. 2.3 DISTRIBUTED ATTENTION MECHANISM In our benchmark, we reproduce and optimize 5 representative distributed attention mechanisms under unified framework, including Ulysess, Ring P2P, Ring All-Gather, USP, and LoongTrain. We establish unified infrastructure that standardizes distributed setup and sequence partitioning, ensuring consistent invocation protocol across all methods. The integrated distributed attention mechanisms can be categorized into three architectural designs: 5 All-to-all based design. DeepSpeeds Ulysses (Jacobs et al., 2023) partitions both the sequence and head dimensions in multi-head attention, using All-to-All communication to switch parallel dimensions. This approach is simple, general, and numerically precise, but the scalability is constrained by the number of attention heads, particularly under GQA, MQA, or tensor parallelism. Ring P2P based design. Ring P2P (Liu et al., 2023) uses multi-round ring-structured point-to-point communication, while Ring All-Gather (NVIDIA, 2025) performs single all-gather of key-value tensors, relying on ring topologies. These approaches exhibit strong scalability and naturally overlap computation with communication via pipelining. However, they suffer from lower efficiency and potential numerical error accumulation. Hybrid design. USP (Fang & Zhao, 2024) and LoongTrain (Gu et al., 2024) extend Ulysses and ring-based designs into two-dimensional scheme. An inner layer applies Ulysses with All-toAll for intra-node bandwidth, while an outer layer uses ring-based attention to enhance scalability and enable computecommunication overlap. LoongTrain further proposes DoubleRing Attention, enhancing Ring P2P with two-level sliding window to improve communication efficiency. In our reproduction and optimization, we draw inspiration from TransformerEngine (NVIDIA, 2025), achieving perfect load balancing through double-parallel partitioning combined with headto-tail reordering (zhuzilin, 2024). We also incorporate optimizations such as double buffering and multi-stream overlap of computation. For each method, we implement backend support for both Flash Attention v3 and cuDNN Fused Attention operators. We extend the input layout to variablelength (varlen) format, allowing multiple sequences of different lengths to be concatenated along the sequence dimension while handling padding tokens. This ensures the flexibility and usability of varlen inputs under different distributed scales. Since varlen inputs can introduce substantial synchronization and waiting overhead across devices, we precompute the necessary meta-information for all distributed strategies as one-time preprocessing step, thereby minimizing distributionrelated performance degradation. Despite these extensions and optimizations, our benchmark remains constrained by the underlying distributed attention designs, thus currently supporting only FULL, CAUSAL, FULL/CAUSAL DOCUMENT masks."
        },
        {
            "title": "3 EVALUATION",
            "content": "In this section, we present experiments evaluating the speed and memory efficiency of different attention methods under long-context scenarios. The speed is measured in TFLOPs/s metric, and peak memory usage is reported in gigabytes (GB). Kernel performance is evaluated on single GPU, while the performance of distributed context parallel attention is assessed across multi-GPU clusters of varying scales. All experiments are conducted on NVIDIA H100 GPUs with 80GB HBM3 memory. The code is publicly available for the community1. 3.1 DENSE ATTENTION KERNEL PERFORMANCE We evaluate dense kernels across 12 static mask configurations to assess both expressiveness and efficiency. Sequence lengths range from 1K to 48K, with BFloat16 precision, hidden dimension of 128, and two head settings: GQA (64:8) and MHA (64:64)2. We record forward and backward throughput as well as peak memory usage (see Appendix A.4 for full results). Figure 3 and 7 report TFLOPs at 8K sequence length under GQA (64:8), where denotes unsupported configurations. The six groups on the left correspond to static regular masks, and the remaining on the right show static heterogeneous masks. Note that the FA series and cuDNN fused kernels do not support heterogeneous masks. In particular, cuDNN fused kernel does not support the FULL SLIDING WINDOW mask with GQA (64:8), though other configurations are recommended. Although the naıve implementation and Torch SDPA theoretically support arbitrary masks, their quadratic complexity leads to severe efficiency degradation and excessive memory overhead, making them impractical in long-context settings. Under computation-intensive dense settings (e.g., FULL or CAUSAL), SDPA achieves performance comparable to general fused operators such as 1The implementation is accessible at: https://github.com/NJUDeepEngine/LongCA-bench 2headq : headkv Figure 3: Forward TFLOPs of dense kernels with different masks (8K length) FlexAttention. These results provide baseline for fused operators without hardware-specific optimizations. FlashMask, another generic fused operator, leverages column-wise mask representation to mitigate computational sparsity. While optimized for heterogeneous masks, its column-wise representation cannot cover all scenarios, making it less general than FlexAttention. For regular scenarios, FA series and cuDNN fused attention are all hardware-optimized kernels. On H100 GPUs, FA3, specifically optimized for the Hopper architecture, achieves the best performance. cuDNN fused attention supports multiple architectures but imposes stricter constraints on data patterns (e.g., GQA (64:8) with FULL SLIDING WINDOW). While some of these limitations can be circumvented by preprocessing techniques such as padding, doing so introduces extra overhead. Note that although FA2 and cuDNN fused attention yield lower performance, kernel selection should be guided by the target hardware architecture. 3.2 SPARSE ATTENTION KERNEL PERFORMANCE To comprehensively evaluate the functionality and computational efficiency of various sparse kernels, we include VSA (Zhang et al., 2025d), Triton VSA, FA2 sparse (Guo et al., 2024) and FlashInfer (Ye et al., 2025) in our evaluation (see Appendix A.5 for full results). We perform kernel-level evaluations across two kinds of block sizes (64 and 128), both forward and backward computation, two attention variants (MHA (64:64) and GQA (64:8)), and sequence lengths ranging from 32K to 128K. Note that FlexAttention (Dong et al., 2024) is excluded due to severe out-of-memory (OOM) issues originating from its mask representations. From functionality perspective, comparisons in Figure 4 (a) and (b) reveal that FA2 sparse does not support block size of 64, while FlashInfer lacks backward computation. As shown in Figure 4 (a), (c), and (d), VSA does not support block size of 128. Due to its specific design for the MHA architecture in video diffusion models, VSA does not currently support GQA. In contrast, FlashInfer is prone to OOM errors at longer sequence lengths and smaller block sizes, stemming from the substantial metadata storage it requires. These limitations highlight the need for further engineering optimizations in block-sparse kernels. Backward computation is essential for trainable sparse attention, particularly in GQA and MHA. Flexibility across block sizes is required to support diverse sparse attention designs, and memory challenges in block-sparse mask representations also need to be addressed. From performance perspective, Figure 4 (a) and (b) show that VSA outperforms both Triton VSA and FlashInfer, while Figures 4 (c) and (d) indicate that FlashInfer outperforms fa2 sparse. Across all kernels, the forward pass consistently achieves higher percentage of theoretical TFLOPs than the backward pass (with theoretical TFLOPs taken from FA3 in Figure 7). Additionally, Figure 4 (c) and (d) show minimal performance differences between MHA and GQA, though GQA achieves better GPU memory efficiency. By comparing Figures 4 (a) and (c), we observe that FlashInfer performs significantly better with block size of 128 than with 64, suggesting that larger block sizes are more effective for achieving higher performance. 7 Figure 4: Performance results (TFLOPs) of sparse kernels with 50% sparsity ratio Figure 5: Forward TFLOPs of Context Parallel Attention on FULL DOCUMENT. Overall, these results demonstrate that performance in block sparse attention is significantly improved through specialization, where kernels tailored to particular parameters (e.g., block size or hardware architecture) consistently outperform general implementations. Meanwhile, the backward pass remains major bottleneck, underscoring an urgent need for optimization. key future direction is the development of more flexible, comprehensive kernels that deliver high performance across wide range of block sizes. Achieving this goal requires moving beyond single-parameter tuning toward deeper, hardware-level optimizations. 3.3 CONTEXT PARALLEL ATTENTION PERFORMANCE We evaluate four mask patterns: FULL, CAUSAL, FULL DOCUMENT, and CAUSAL DOCUMENT. The per-device sequence length is fixed at 8K with hidden size 128, validated under the GQA (64:8) setting. Experiments are conducted on NVIDIA H100 GPUs, scaling from 8 to 96 GPUs across 12 servers, with total context windows from 64K to 512K. Since Ulysses requires divisibility constraints, GQA is converted to MHA by replicating KV heads ( indicates divisibility failure). Performance under FULL DOCUMENT is shown in Figure 5 and 23, with additional details provided in Appendix A.63. Context parallel attention inevitably involves distributed communication, raising two major concerns: (1) whether communication effectively overlaps with computation, and if not, how efficient the communication is; and (2) whether workload balance is achieved in terms of both data volume and computation across devices. Ideally, context parallel strategy should behave close to non-distributed setting. Inter-node communication constitutes the dominant bottleneck compared to computation and intra-node communication. In our experiments, we fix the large-load AllToAll groups within the mixed architecture to 8 per node, while the small-load P2P groups are placed across nodes and scale with the number of nodes. For the secondary P2P communication groups in LoongTrain, we adopt balanced configuration (e.g., 12 = 34) to maximize inter-node bandwidth utilization. All experiments are performed on the FA3 backend for consistency. 3Due to resource constraints, we temporarily omit the experimental results for Ring All-Gather. 8 Ulysses AllToAll communication is entirely exposed outside the computation. Thanks to its collective communication pattern (NVIDIA Corporation, 2020) with low communication overhead and its head-sharded computation pattern leading to perfectly balanced workloads, Ulysses still delivers solid performance. However, its scalability is bounded by the number of attention heads. loadbalanced Ring P2P ensures that each GPU processes the same amount of computation and communication per iteration. However, Ring P2P communication is mask-independent, always transferring in fixed ratio of D/N , where is the total data and is the world size, meaning performance depends entirely on the amount of computation workload. Ring P2P performs optimally in the FULL scenario. However, in the DOCUMENT scenario, variable-length padding depends on scale and sampling, leading to noticeable per-GPU computation variation and performance fluctuations. The hybrid architecture alleviates the above issues. While the intra-node AllToAll communication group still remains exposed outside computation, its per-communication volume is reduced from (N 1)/N in Ulysses to (8 1)/N per group (one-way). Meanwhile, the inter-node Ring P2P computation volume increases from D/N in pure Ring P2P to D/K, enabling USP and LoongTrain to achieve optimal performance improvements, where denotes the size of the internode communication group. Additionally, LoongTrain introduces secondary P2P architecture to further improve inter-node bandwidth utilization, providing modest forward speedups compared to USP. However, because the secondary architecture involves extra window synchronization, the Ring backward pass cannot directly continue from the forward state, negating the overall performance gains. Overall, the experimental results demonstrate that fully leveraging MHA by first partitioning the heads yields significant performance benefits."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Long context language modeling. Models such as BERT (Devlin et al., 2019) and GPT (Brown et al., 2020) can process thousands of tokens, supporting document and dialogue level tasks, with full document understanding and long range retrieval emerging as key challenges. Recently, model context windows have expanded dramatically, from 4K tokens to 128K (Dubey et al., 2024), 1M (Yang et al., 2025a), and even 10M tokens (Team et al., 2024). The ability to model ultra-long contexts enables continuous reference, reasoning, and summarization over extended input sequences. This enhances advanced capabilities such as long-text reasoning (Guo et al., 2025; Muennighoff et al., 2025), improved in-context learning (Li et al., 2025; Team et al., 2024), efficient information compression (Lee et al., 2024; Wang et al., 2024), and multimodal understanding (Weng et al., 2024). Attention kernels. Attention is the core component in Transformers with the time complexity of O(n2) in terms of context length. Hardware-efficient attention leverages hardware features to reduce the time and memory costs. Dao et al. (2022); Dao (2023); Shah et al. (2024) employs matrix tiling and kernel fusion. Zhang et al. (2024b;a; 2025a) uses quantization to leverage low-bit Tensor Cores. Sparse Kernels use the inherent sparsity (Child et al., 2019; Zhang et al., 2025b) of the attention map = Softmax(QK / d) to accelerate computation. Other directions include KV cache compression (Zhao et al., 2023a) via weight sharing (Ainslie et al., 2023) or low-rank decomposition (Liu et al., 2024) to reduce memory overhead without extra computation. Parallelism for distributed training. Various parallel paradigms have been developed to tackle resource challenges in large-scale distributed model training. Data parallelism (PyTorch Contributors, 2024a; Rajbhandari et al., 2020; Zhao et al., 2023b) partitions data along the batch dimension. Tensor parallelism (Shoeybi et al., 2019; Xu & You, 2023; Wang et al., 2022), pipeline parallelism (Huang et al., 2019; Li & Hoefler, 2021; Narayanan et al., 2019; Qi et al., 2024) [3942], and expert parallelism (Gale et al., 2023; Hwang et al., 2023; Li et al., 2023; Liu et al., 2024) partition model parameters along different dimensions. Hybrid parallel strategies (Smith et al., 2022; Ge et al., 2025; Wang et al., 2025b) are used to meet diverse needs and balance computation and memory. However, these strategies cannot fully address activation memory overhead from ultra-long sequences. Context parallelism (Korthikanti et al., 2023; Li et al., 2021) partitions data by sequence, but faces challenges in computationcommunication overlap, balancing, scalability, and usability; many designs remain underexplored, and near-linear scalability is still difficult."
        },
        {
            "title": "5 CONCLUSION",
            "content": "The complexity of distributed environments is far greater than that of single-device settings. In ultra-long context training, selecting or developing appropriate kernels and context parallel strategies poses significant challenges and requires substantial effort and resources. To address this, we present fair and unified benchmark for attention mechanisms in ultra-long context training, covering the spectrum from single-device kernels to large-scale distributed context parallel methods. Although our work has limitations, it aims to improve fairness in comparing different approaches, expose their performance trade-offs and constraints, and provide objective references to guide future research and development in ultra-long context training."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Sand. ai, Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, W. Q. Zhang, Weifeng Luo, Xiaoyang Kang, Yuchen Sun, Yue Cao, Yunpeng Huang, Yutong Lin, Yuxin Fang, Zewei Tao, Zheng Zhang, Zhongshu Wang, Zixun Liu, Dai Shi, Guoli Su, Hanwen Sun, Hong Pan, Jie Wang, Jiexin Sheng, Min Cui, Min Hu, Ming Yan, Shucheng Yin, Siran Zhang, Tingting Liu, Xianping Yin, Xiaoyu Yang, Xin Song, Xuan Hu, Yankai Zhang, and Yuqiao Li. Magi-1: Autoregressive video generation at scale, 2025. URL https://arxiv.org/abs/2505.13211. Joshua Ainslie, James Lee-Thorp, Michiel De Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. arXiv preprint arXiv:2305.13245, 2023. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Qiaoling Chen, Diandian Gu, Guoteng Wang, Xun Chen, YingTong Xiong, Ting Huang, QingInternevo: Efficient long-sequence hao Hu, Xin Jin, Yonggang Wen, Tianwei Zhang, et al. large language model training via hybrid parallelism and redundant sharding. arXiv preprint arXiv:2401.09149, 2024. Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Liuyao Dai, Hao Qi, Weicong Chen, and Xiaoyi Lu. High-speed data communication with advanced networks in large language model training. IEEE Micro, 44(2):3140, 2024. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. Flex attention: programming model for generating optimized attention kernels. arXiv preprint arXiv:2412.05496, 2024. Jiangfei Duan, Shuo Zhang, Zerui Wang, Lijuan Jiang, Wenwen Qu, Qinghao Hu, Guoteng Wang, Qizhen Weng, Hang Yan, Xingcheng Zhang, et al. Efficient training of large language models on distributed infrastructures: survey. arXiv preprint arXiv:2407.20018, 2024. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Jiarui Fang and Shangchun Zhao. Usp: unified sequence parallelism approach for long context generative ai. arXiv preprint arXiv:2405.07719, 2024. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128k context. arXiv preprint arXiv:2402.10171, 2024. Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, and Xiangyu Zhao. Sliding window attention training for efficient large language models. arXiv preprint arXiv:2502.18845, 2025. Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. Proceedings of Machine Learning and Systems, 5:288304, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. Enabling large language models to generate text with citations. 2024a. Tianyu Gao, Alexander Wettig, Howard Yen, and Danqi Chen. How to train long-context language models (effectively). arXiv preprint arXiv:2410.02660, 2024b. Hao Ge, Junda Feng, Qi Huang, Fangcheng Fu, Xiaonan Nie, Lei Zuo, Haibin Lin, Bin Cui, and Xin Liu. Bytescale: Efficient scaling of llm training with 2048k context length on more than 12,000 gpus. arXiv preprint arXiv:2502.21231, 2025. Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. Openwebtext corpus. http: //Skylion007.github.io/OpenWebTextCorpus, 2019. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, 11 Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzman, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur, Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur elebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vıtor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy 12 Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan, Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Diandian Gu, Peng Sun, Qinghao Hu, Ting Huang, Xun Chen, Yingtong Xiong, Guoteng Wang, Qiaoling Chen, Shangchun Zhao, Jiarui Fang, et al. Loongtrain: Efficient training of longsequence llms with head-context parallelism. arXiv preprint arXiv:2406.18485, 2024. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Junxian Guo, Haotian Tang, Shang Yang, Zhekai Zhang, Zhijian Liu, and Song Han. Block Sparse https://github.com/mit-han-lab/Block-Sparse-Attention, Attention. 2024. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism. Advances in neural information processing systems, 32, 2019. Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2180721818, 2024. 13 Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, et al. Tutel: Adaptive mixture-of-experts at scale. Proceedings of Machine Learning and Systems, 5:269287, 2023. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Shuaiwen Leon Song, Samyam Rajbhandari, and Yuxiong He. Deepspeed ulysses: System optimizations for enabling training of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Alec Gray, Scott ..and Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer models. Proceedings of Machine Learning and Systems, 5:341353, 2023. Mario Michael Krell, Matej Kosec, Sergio Perez, and Andrew Fitzgibbon. Efficient sequence packing without cross-contamination: Accelerating large language models without impacting performance. arXiv preprint arXiv:2107.02027, 2021. Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sebastien MR Arnold, Vincent Perot, Siddharth Dalmia, et al. Can long-context language models subsume retrieval, rag, sql, and more? arXiv preprint arXiv:2406.13121, 2024. Aonian Li, Bangwei Gong, Bo Yang, Boji Shan, Chang Liu, Cheng Zhu, Chunhao Zhang, Congchao Guo, Da Chen, Dong Li, et al. Minimax-01: Scaling foundation models with lightning attention. arXiv preprint arXiv:2501.08313, 2025. Jiamin Li, Yimin Jiang, Yibo Zhu, Cong Wang, and Hong Xu. Accelerating distributed {MoE} training and inference with lina. In 2023 USENIX Annual Technical Conference (USENIX ATC 23), pp. 945959, 2023. Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, and Yang You. Sequence parallelism: Long sequence training from system perspective. arXiv preprint arXiv:2105.13120, 2021. Shigang Li and Torsten Hoefler. Chimera: efficiently training large-scale neural networks with In Proceedings of the International Conference for High Performance bidirectional pipelines. Computing, Networking, Storage and Analysis, pp. 114, 2021. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context. arXiv preprint arXiv:2310.01889, 2023. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil Devanur, Gregory Ganger, Phillip Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parIn Proceedings of the 27th ACM symposium on operating systems allelism for dnn training. principles, pp. 115, 2019. NVIDIA. Transformer engine. https://github.com/NVIDIA/TransformerEngine, 2025. Accessed: 2025-09-23. NVIDIA Corporation. Collective communication functions. https://docs.nvidia.com/ deeplearning/nccl/user-guide/docs/api/colls.html, 2020. Accessed: 202509-23. NVIDIA Corporation. fused attn.h. https://docs.nvidia.com/deeplearning/ transformer-engine-releases/release-2.3/user-guide/api/c/fused_ attn.html, 2025. Accessed: 2025-09-23. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35: 2773027744, 2022. PyTorch Contributors. Distributed data parallel. https://docs.pytorch.org/docs/ stable/notes/ddp.html, 2024a. PyTorch Contributors. torch.nn.functional.scaled dot product attention. https://docs. pytorch.org/docs/2.6/generated/torch.nn.functional.scaled_dot_ product_attention.html, 2024b. Penghui Qi, Xinyi Wan, Guangxing Huang, and Min Lin. Zero bubble (almost) pipeline parallelism. In The Twelfth International Conference on Learning Representations, 2024. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining, pp. 35053506, 2020. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {Zero-offload}: Democratizing {billion-scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551564, 2021. Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems, 37:6865868685, 2024. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022. Daria Soboleva, Faisal Al-Khateeb, and Nolan Dey. Hestness, deduplicated slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama, 2023. URL https://huggingface.co/datasets/cerebras/SlimPajama-627B. SlimPajama: RedPajama. cleaned version of Jacob Steeves, Joel and https://cerebras.ai/blog/ 627B token Robert Myers, Benjamin Spector, Simran Arora, Aaryan Singhal, Daniel Fu, and Christopher Re. Thunderkittens: Simple, fast, and adorable ai kernels. arXiv preprint arXiv:2410.20399, 2024. 15 Yi Tay, Jason Wei, Hyung Won Chung, Vinh Tran, David So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, et al. Transcending scaling laws with 0.1% extra compute. arXiv preprint arXiv:2210.11399, 2022. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Boxiang Wang, Qifan Xu, Zhengda Bian, and Yang You. Tesseract: Parallelize the tensor parallelism efficiently. In Proceedings of the 51st International Conference on Parallel Processing, pp. 111, 2022. Guoxia Wang, Jinle Zeng, Xiyuan Xiao, Siming Wu, Jiabin Yang, Lujing Zheng, Zeyu Chen, Jiang Bian, Dianhai Yu, and Haifeng Wang. Flashmask: Efficient and rich mask extension of flashattention, 2025a. URL https://arxiv.org/abs/2410.01359. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Large search model: Redefining search stack in the era of llms. In ACM SIGIR Forum, volume 57, pp. 116. ACM New York, NY, USA, 2024. Zheng Wang, Anna Cai, Xinfeng Xie, Zaifeng Pan, Yue Guan, Weiwei Chu, Jie Wang, Shikai Li, Jianyu Huang, Chris Cai, et al. Wlb-llm: Workload-balanced 4d parallelism for large language model training. arXiv preprint arXiv:2503.17924, 2025b. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient In European Conference on Computer long video understanding via large language models. Vision, pp. 453470. Springer, 2024. Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, and Bin Cui. Training-free and adaptive sparse attention for efficient long video generation. arXiv preprint arXiv:2502.21079, 2025. Qifan Xu and Yang You. An efficient 2d method for training super-large deep learning models. In 2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS), pp. 222232. IEEE, 2023. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. arXiv preprint arXiv:2503.16428, 2025. An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383, 2025a. Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. arXiv preprint arXiv:2505.18875, 2025b. Yaofu. Slimpajama per source length upsample, 2024. URL https://huggingface.co/ datasets/yaofu/slimpajama-per-source-length-upsample. Accessed: 202509-15. Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005, 2025. 16 Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Y. X. Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention, 2025. URL https://arxiv.org/abs/2502.11089. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Tao Zewei and Huang Yunpeng. Magiattention: distributed attention towards linear scalability for ultra-long context, heterogeneous mask training. https://github.com/SandAI-org/ MagiAttention/, 2025. Jintao Zhang, Rundong Su, Chunyu Liu, Jia Wei, Ziteng Wang, Pengle Zhang, Haoxu Wang, Huiqiang Jiang, Haofeng Huang, Chendong Xiang, et al. survey of efficient attention methods: Hardware-efficient, sparse, compact, and linear attention. Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. arXiv preprint arXiv:2411.10958, 2024a. Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, and Jianfei Chen. SageatarXiv preprint tention: Accurate 8-bit attention for plug-and-play inference acceleration. arXiv:2410.02367, 2024b. Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025a. Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. arXiv preprint arXiv:2502.18137, 2025b. Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhengzhong Liu, and Hao Zhang. Fast video generation with sliding tile attention. arXiv preprint arXiv:2502.04507, 2025c. Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. arXiv preprint arXiv:2505.13389, 2025d. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023a. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023b. zhuzilin. [feature request] balancing computation with zigzag blocking. https://github. com/zhuzilin/ring-flash-attention/issues/2, Feb 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DETAILS OF DENSE DATA SAMPLING In large language model construction, the quality and diversity of the training data are crucial for enhancing model performance (Gunasekar et al., 2023). Numerous studies have explored various methods to improve data quality. Our benchmark builds upon these efforts by systematically analyzing several publicly available, high-quality, and widely used English datasets. Full results are presented in Figure 6. We mainly analyze the Pile and SlimPajama (Soboleva et al., 2023) datasets to study their effects on the models short-context modeling capabilities. The Pile dataset is large-scale, diverse English text corpus designed for training large language models, with total size of 825 GB. It consists of 22 high-quality subsets, many drawn from academic or professional sources, including Common Crawl, Wikipedia, OpenWebText, ArXiv, and PubMed. Such diversity across multiple domains and topics substantially increases the richness and variety of the training data. SlimPajama is an open-source dataset obtained from the original RedPajama corpus through multiple preprocessing steps such as NFC normalization, cleaning, deduplication, and document interleaving, comprising total of 627B tokens. Compared to Pile, SlimPajama contains less web data and more content from Books, ArXiv, and Wikipedia. These are high-quality long-form text sources that help improve the models long-context modeling capabilities. Owing to its large scale, SlimPajama is not fully sampled; instead, our benchmark samples sequences of up to 8k tokens from Pile, which is sufficient to represent realistic short-context modeling scenarios. The above data cleaning mainly focused on limited context window (e.g., 8k). To extend the models context window, recent studies have begun exploring data mixing strategies for long contexts. We follow the findings of (Fu et al., 2024) and (Gao et al., 2024b): (1) continual pretraining on long-context data can significantly improve the models ability to accurately retrieve information in long contexts; (2) when extending the context length, oversampling long sequences while preserving the original domain diversity of the pretraining dataset is crucial; (3) mixing high-quality long-context sources with high-quality short-context sources is essential for enhancing long-context modeling capability while maintaining performance on short contexts. In our benchmark, we collected statistics on the publicly available long-text upsampled dataset slimpajama-per-source-lengthupsample (referred to as Upsampled SlimPajama) (Yaofu, 2024) from (Fu et al., 2024), as well as the datasets prolong-data-64K (ProLong64K) (Gao et al., 2024a) and prolong-data-512K (ProLong512K) (Gao et al., 2024a) from (Gao et al., 2024b), which are used to extend the models context window to 64K and 512K tokens, respectively. Considering the significant differences resulting from various tokenization methods, we directly split English samples by spaces in our statistics (approximately reflecting tokenized lengths). All statistics are shown in Figure 6, with short-context and long-context distributions arranged side by side. It is worth noting that datasets are generally expressed in terms of the number of tokens rather than the number of samples. Although long-text tokens in datasets such as ProLong64 and ProLong512K account for up to 60%, their prominence in the length distribution may still be limited. 19 (a) C4 Dataset Distribution (b) Upsampled SlimPajama Dataset Distribution Figure 6: Pretraining Dataset Length Distributions. 20 A.2 DETAILS OF SPARSE DATA SAMPLING Referencing common block sparse mask generation methods, our block sparse mask generation process is as follows: 1. Block Partitioning: For task with an input sequence length of seqlen, we can conceptualize seqlen seqlen attention map. Based on the provided block lists and block lists, this two-dimensional attention map is partitioned into len(q block lists) len(k block lists) blocks. For uniform block mask, the block sizes within block lists and block lists are the same. In contrast, for variable block mask, these block sizes can differ. 2. Score Calculation: Common block sparse methods typically calculate score for each block in some manner (e.g., by applying mean pooling over each block) and generate score matrix which represents the importance of the block. Blocks with higher importance are more likely to be selected. It is important to note that scores may vary across different attention heads. Generally, distinct mask is generated for each KV head. This means for Multi-Head Attention, the score matrix can be unique for each head. For Grouped-Query Attention, masks are the same within group but can differ between groups. In our experiments, we abstract away the specifics of score calculation and use randomly generated numbers, thereby focusing on the final block sparse attention computation. 3. Top-k Selection: For each block in the query dimension (q), we select the top-k blocks from the key dimension (k) for computation. The overall degree of sparsity can be expressed as the fraction len(k block lists) . We define sparsity ratio to represent this degree of sparsity, which has direct conversion relationship with top-k and is essentially equivalent. To observe the kernels performance under different sparsity levels, we select 0.2, 0.5, and 0.8 as representative sparsity ratios. A.3 DETAILS OF SPARSE ATTENTION KERNELS VSA (Zhang et al., 2025d) is trainable block sparse attention implementation designed specifically for video diffusion models. It employs two-stage methodology. In the coarse-grained stage, it applies token rearrangement strategy from STA (Zhang et al., 2025c) to increase computational density. It then calculates inter-block scores via cube partition and mean-pooling on the QK matrix, which selects the top-k blocks for each block. Subsequently, in the fine-grained stage, it utilizes ThunderKittens (Spector et al., 2024) to develop high-performance block sparse attention kernel. This kernel is customized for Hopper GPUs to maximize hardware utilization. According to VSAs analysis, the fine-grained stage accounts for over 80% of latency at context lengths of 32K or more. This finding highlights the critical need to optimize the block sparse attention kernel, and our performance benchmarks also focus on this kernel for fair comparisons. As variant, Triton VSA implements the algorithm in the Triton language. This approach aims to enhance cross-hardware compatibility but results in some performance degradation. However, both implementations are specifically optimized for video models and they do not support common LLM attention modes like Grouped-Query Attention (GQA). FA2 Sparse Guo et al. (2024) is an open-source implementation based on the FlashAttention-2 (Dao, 2023) codebase. It enables block sparse functionality by modifying the computation logic, allowing each block to traverse only its designated KV blocks. The primary limitation of this implementation is its lack of support for the backward pass and without optimization for advanced GPUs like NVIDIA Hopper GPUs. FlexAttention Dong et al. (2024) leverages compiler technology to introduce more flexible mask description method. This approach enables it to support sparse attention in the form of block mask. However, the representation for block masks is relatively complex. Its compilation technique can therefore degenerate to instantiating full O(S2) mask, which causes significant memory overhead. FlashInfer Ye et al. (2025) is general-purpose kernel library oriented toward LLM inference. It designs block sparse matrix structure as unified format for the KV cache. This design allows block sparse attention input to be converted into paged attention format where page size equals to 1. This process enables the reuse of its efficient attention kernel and supports arbitrary block sizes. Due to its positioning as an inference library, it does not support the backward pass. 21 A.4 DETAILS OF DENSE KERNEL PERFORMANCE For single-sequence samples such as FULL/CAUSAL and SLIDING WINDOW, we conduct 2 runs of sampling, with each run followed by 5 warm-up steps and 20 kernel computation steps. For multisequence data such as DOCUMENT and SHARE QUESTION, considering the sparsity differences introduced by sampling, we perform 30 runs with independent sampling in each run, also followed by 5 warm-up steps and 20 kernel computation steps. We record the median values of FLOPS and peak memory. It is worth noting that our results represent the expected outcomes under these specific settings, and occasional large deviations in individual kernel runs are considered normal. A.4.1 PERFORMANCE METRIC: FLOPS Figure 7: Backward TFLOPs of dense kernels with different masks (8K length) FULL and CAUSAL are the most common masks used in language model pretraining, as shown in Figure 8. SDPA and Flex approximately represent the baselines of fused operators without hardware-specific optimization. In general, increasing sequence length improves kernel performance, which typically stabilizes around 16K. The results highlight the importance of hardwareaware optimization: on the H100 Hopper architecture, FA3 achieve significant performance gains. FULL/CAUSAL DOCUMENT is primarily designed for concatenating variable-length input sequences to reduce unnecessary padding while preserving full or causal connectivity. It is important to note that concatenating variable-length sequences can introduce computational instability, which becomes particularly pronounced when the data contains many small fragmented chunks, as shown in Figure 9. In our experiments, we fixed the sliding window size to 1024, as shown in Figure 10, though we recommend evaluating with other window sizes as well (Fu et al., 2025). Overall, in heterogeneous mask scenarios, Flex and FlashMask show varying performance gains depending on the context. PREFIX LM and PREFIX LM DOCUMENT extend the standard language model regular mask by introducing prefix, allowing the prefix to attend to all tokens. In our experiments, prefix is randomly generated for each run, and the median across multiple runs is reported to reflect expected performance in realistic scenarios with varying prefixes, as shown in Figure 11. Models trained with prefix demonstrate advantages in handling long-text and multi-turn dialogue tasks. This approach enables the model to better leverage contextual information, improving performance in generation tasks (Raffel et al., 2020). SHARE QUESTION and CAUSAL BLOCKWISE can be viewed as variants of DOCUMENT, as shown in Figure 12. SHARE QUESTION allows all query tokens to share the key tokens from the first document and is commonly used in Reward Models (RM) as shared-question mask, enabling multiple answers to reference the same question (Ouyang et al., 2022). This eliminates redundant computation and accelerates training. CAUSAL BLOCKWISE, on the other hand, allows all key tokens to share the query tokens from the last document and is typically applied in demonstrationtest tasks, where test examples can attend to all demonstrations. This facilitates studying model performance improvements in long-context tasks (Bertsch et al., 2024). 22 (a) FULL GQA Fwd TFLOPS (b) FULL GQA Bwd TFLOPS (c) FULL MHA Fwd TFLOPS (d) FULL MHA Bwd TFLOPS In each run, we GLOBAL SLIDING combines global attention with sliding-window attention. randomly sample window size, treating the leftmost window size tokens in the Query and Key as global tokens, which attend to all Key and Query tokens, respectively. Due to the increased sparsity of the mask, the performance of Flex and FlashMask correspondingly decreases, as shown in Figure 13. 23 (e) CAUSAL GQA Fwd TFLOPS (f) CAUSAL GQA Bwd TFLOPS (g) CAUSAL MHA Fwd TFLOPS (h) CAUSAL MHA Bwd TFLOPS Figure 8: TFLOPS of FULL and CAUSAL 24 (a) FULL DOCUMENT GQA Fwd TFLOPS (b) FULL DOCUMENT GQA Bwd TFLOPS (c) FULL DOCUMENT MHA Fwd TFLOPS (d) FULL DOCUMENT MHA Bwd TFLOPS BLOCK CAUSAL DOCUMENT can also be viewed as variant of DOCUMENT, elevating computation from the token level to the block level. In our experiments, we fix block size = 1024, as shown in Figure 14. This approach is commonly used in training autoregressive multimodal large models (ai et al., 2025). 25 (e) CAUSAL DOCUMENT GQA Fwd TFLOPS (f) CAUSAL DOCUMENT GQA Bwd TFLOPS (g) CAUSAL DOCUMENT MHA Fwd TFLOPS (h) CAUSAL DOCUMENT MHA Bwd TFLOPS Figure 9: TFLOPS of FULL/CAUSAL DOCUMENT 26 (a) FULL SLIDING WINDOW GQA Fwd TFLOPS (b) FULL SLIDING WINDOW GQA Bwd TFLOPS (c) FULL SLIDING WINDOW MHA Fwd TFLOPS (d) FULL SLIDING WINDOW MHA Bwd TFLOPS (e) CAUSAL SLIDING WINDOW GQA Fwd TFLOPS (f) CAUSAL SLIDING WINDOW GQA Bwd TFLOPS (g) CAUSAL SLIDING WINDOW MHA Fwd TFLOPS (h) CAUSAL SLIDING WINDOW MHA Bwd TFLOPS Figure 10: TFLOPS of FULL/CAUSAL SLIDING WINDOW (a) PREFIX LM GQA Fwd TFLOPS (b) PREFIX LM GQA Bwd TFLOPS (c) PREFIX LM MHA Fwd TFLOPS (d) PREFIX LM MHA Bwd TFLOPS 29 (e) PREFIX LM DOCUMENT GQA Fwd TFLOPS (f) PREFIX LM DOCUMENT GQA Bwd TFLOPS (g) PREFIX LM DOCUMENT MHA Fwd TFLOPS (h) PREFIX LM DOCUMENT MHA Bwd TFLOPS Figure 11: TFLOPS of PREFIX LM and PREFIX LM DOCUMENT 30 (a) SHARE QUESTION GQA Fwd TFLOPS (b) SHARE QUESTION GQA Bwd TFLOPS (c) SHARE QUESTION MHA Fwd TFLOPS (d) SHARE QUESTION MHA Bwd TFLOPS 31 (e) CAUSAL BLOCKWISE GQA Fwd TFLOPS (f) CAUSAL BLOCKWISE GQA Bwd TFLOPS (g) CAUSAL BLOCKWISE MHA Fwd TFLOPS (h) CAUSAL BLOCKWISE MHA Bwd TFLOPS Figure 12: TFLOPS of SHARE QUESTION and CAUSAL BLOCKWISE 32 (a) GLOBAL SLIDING GQA Fwd TFLOPS (b) GLOBAL SLIDING GQA Bwd TFLOPS (c) GLOBAL SLIDING MHA Fwd TFLOPS (d) GLOBAL SLIDING MHA Bwd TFLOPS Figure 13: TFLOPS of GLOBAL SLIDING 33 (a) BLOCK CAUSAL DOCUMENT GQA Fwd TFLOPS (b) BLOCK CAUSAL DOCUMENT GQA Bwd TFLOPS (c) BLOCK CAUSAL DOCUMENT MHA Fwd TFLOPS (d) BLOCK CAUSAL DOCUMENT MHA Bwd TFLOPS Figure 14: TFLOPS of BLOCK CAUSAL DOCUMENT 34 A.4.2 PERFORMANCE METRIC: PEAK MEMORY USAGE We report the peak memory usage of the kernel as reference. The memory plots (Figure 16, Figure 17) clearly illustrate the detrimental impact of quadratic storage complexity on training: Naive Attention and SDPA scale only up to around 16K in the forward pass and about 8K in the backward pass. We truncate the plots at certain points and annotate the corresponding values. Under the same mask setting, different kernels exhibit similar peak memory in the forward pass, but show noticeable differences in the backward pass. Overall, the trend of peak memory scaling with sequence length across different kernels aligns well with intuition. Since sequence length is extended on single GPU, model parameters remain fixed, and the growth in peak memory is entirely determined by activations. In the standard attention module, activation memory is computed as 11bshd + 5bhs2 + 2bshd, where is the batch size, the sequence length, the number of heads, and the hidden dimension (see Figure 15). We do not consider any gradient checkpointing (Chen et al., 2016) or offloading (Ren et al., 2021) techniques. Although different kernels may employ various strategies to optimize memory usage, the overall growth trend of activations still approximately follows quadratic curve, which is confirmed by our experimental results. At the same time, recording peak memory further highlights the performance bottlenecks of the attention mechanism when handling ultra-long contexts. Due to the presence of activations, other distributed strategies such as tensor parallelism and data parallelism are insufficient to alleviate peak memory usage. Only context parallelism, which balances the workload across devices along the sequence dimension, can effectively address this issue. Figure 15: Full Activations in Attention Module 35 (a) FULL GQA Fwd Peak Memory (b) FULL GQA Bwd Peak Memory (c) FULL MHA Fwd Peak Memory (d) FULL MHA Fwd Peak Memory 36 (e) CAUSAL GQA Fwd Peak Memory (f) CAUSAL GQA Bwd Peak Memory (g) CAUSAL MHA Fwd Peak Memory (h) CAUSAL MHA Fwd Peak Memory (i) FULL DOCUMENT GQA Fwd Peak Memory (j) FULL DOCUMENT GQA Bwd Peak Memory (k) FULL DOCUMENT MHA Fwd Peak Memory (l) FULL DOCUMENT MHA Fwd Peak Memory 38 (m) CAUSAL DOCUMENT GQA Fwd Peak Memory (n) CAUSAL DOCUMENT GQA Bwd Peak Memory (o) CAUSAL DOCUMENT MHA Fwd Peak Memory (p) CAUSAL DOCUMENT MHA Fwd Peak Memory 39 (q) FULL SLIDING WINDOW GQA Fwd Peak Memory (r) FULL SLIDING WINDOW GQA Bwd Peak Memory (s) FULL SLIDING WINDOW MHA Fwd Peak Memory (t) FULL SLIDING WINDOW MHA Bwd Peak Memory 40 (u) CAUSAL SLIDING WINDOW GQA Fwd Peak Memory (v) CAUSAL SLIDING WINDOW GQA Fwd Peak Memory (w) CAUSAL SLIDING WINDOW MHA Fwd Peak Memory (x) CAUSAL SLIDING WINDOW MHA Bwd Peak Memory Figure 16: Peak Memory of Static Regular Masks 41 (a) PREFIX LM GQA Fwd Peak Memory (b) PREFIX LM GQA Bwd Peak Memory (c) PREFIX LM MHA Fwd Peak Memory (d) PREFIX LM MHA Bwd Peak Memory 42 (e) PREFIX LM DOCUMENT GQA Fwd Peak Memory (f) PREFIX LM DOCUMENT GQA Bwd Peak Memory (g) PREFIX LM DOCUMENT MHA Fwd Peak Memory (h) PREFIX LM DOCUMENT MHA Bwd Peak Memory 43 (i) SHARE QUESTION GQA Fwd Peak Memory (j) SHARE QUESTION GQA Bwd Peak Memory (k) SHARE QUESTION MHA Fwd Peak Memory (l) SHARE QUESTION MHA Bwd Peak Memory (m) CAUSAL BLOCKWISE GQA Fwd Peak Memory (n) CAUSAL BLOCKWISE GQA Bwd Peak Memory (o) CAUSAL BLOCKWISE MHA Fwd Peak Memory (p) CAUSAL BLOCKWISE MHA Bwd Peak Memory 45 (q) GLOBAL SLIDING GQA Fwd Peak Memory (r) GLOBAL SLIDING GQA Bwd Peak Memory (s) GLOBAL SLIDING MHA Fwd Peak Memory (t) GLOBAL SLIDING MHA Bwd Peak Memory 46 (u) BLOCK CAUSAL DOCUMENT GQA Fwd Peak Memory (v) BLOCK CAUSAL DOCUMENT GQA Bwd Peak Memory (w) BLOCK CAUSAL DOCUMENT MHA Fwd Peak Memory (x) BLOCK CAUSAL DOCUMENT MHA Bwd Peak Memory Figure 17: Peak Memory of Static Heterogeneous Masks 47 A.5 DETAILS OF SPARSE KERNEL PERFORMANCE In this section, we present the detailed comparisons for sparse kernels, including TFLOPS performance and peak memory usage. There are only partial baselines in each figure or table because current block sparse kernels have such limitations in functionality: VSA and Triton VSA do not support GQA and 128 block size, FA2 Sparse does not support 64 block size, FA2 Sparse and FlashInfer do not support backward computation, and FlexAttention faces severe memory issues so we discuss it separately. A.5.1 PERFORMANCE METRIC: FLOPS TFLOPS of MHA with 64 block size. We report the performance of block sparse attention kernels with MHA and 64 block size in Figure 18. Only VSA, Triton VSA and FlashInfer support this setting. The left side shows the foward FLOPS with different sparsity ratios while the right side shows the backward FLOPS. Our findings reveal that VSA performs stably across different sparsity ratios, showing their robustness with adaptive computation. FlashInfer also performs stably but suffers from OOM issues with higher sparsity ratios because there are more blocks to compute, causing memory overhead with its metadata. However, we find that the performance of VSA reduces with the increase of the context length, especially for backward computation. This indicates that there may exists optimization opportunities for larger context lengths and backward computation for training scenarios. TFLOPS of MHA with 128 block size. We report the performance of block sparse attention kernels with MHA and 128 block size with forward computation in Table 4. Only FlashInfer and FA2 Sparse support this setting. Comparing with the performance of 64 block size in Figure 18, the TFLOPS of FlashInfer in 128 block size increases about 2x, proportional to the block size increase, showing the scalability of FlashInfer block sparse kernels. Its OOM issues also decrease compared with 64 block size, because larger block size means smaller number of blocks, leading to smaller metadata storage. For the TFLOPS of FA2 Sparse, it demonstrates robustness across different sparsity ratios and context lengths. Its average performance is about 300+ FLOPS because it is not optimized for NVIDIA Hopper GPUs. There exists opportunities for tailored optimizations for specific hardware platforms to unleash the hardware performance. Separate explanation of FlexAttention TFLOPS. FlexAttention is separately discussed because it is hard to generate the dynamic block sparse block through compilation, causing severe memory overhead with O(S2) block mask representations. So we only test its TFLOPS performance with 4 heads and 16K context length in Table 5. It performs bad in 64 block size due to the lack of optimizations with small block size. While in 128 block size, it is comparable to TFLOPS of FA2 Sparse in Table 4. The reason behind is that FlexAttention uses 128 as its default block size, so the kernels it generates demonstrate relative good TFLOPS. It also supports backward computation with the similar performance. A.5.2 PERFORMANCE METRIC: PEAK MEMORY USAGE Peak memory of MHA with 64 block size. We report the peak memory of block sparse attention kernels with MHA and 64 block sizes in Figure 19. Only VSA, Triton VSA and FlashInfer support this setting. The left side window shows the forward pass with different sparsity ratio while the right side window shows the backward pass. VSA and VSA trition share the same memory usage while flashinfer exhibit higher GPU memory consumption which may because of its metadata representation for block sparse. It is also clear that The GPU memory consumption of VSA and VSA trition shows almost no correlation with the sparsity ratio. In contrast, the memory footprint of FlashInfer grows in proportion to the increase in the sparsity ratio. Peak memory in GQA scenarios. As shown in Table 6, in GQA scenarios, FlashInfers GPU memory usage is significantly lower than in MHA scenarios, for both block sizes of 64 and 128. This is because the metadata required to represent the block sparse structure is greatly reduced. As for FA2 Sparse, its memory consumption is greatly lower than flashinfer, which indicates flashinfers poor performance in terms of GPU memory usage. Peak memory of MHA with 128 block size. We report the peak memory of block sparse attention kernels with MHA and 64 block sizes in Table 7, compared with MHA with 64 block size, flashinfer consumes less GPU memory. However, FA2 Sparse utilizes more GPU memory compared with GQA scenerio. Separate explanation of FlexAttention peak memory. FlexAttention 48 (a) MHA Fwd TFLOPS with 0.2 sparsity ratio (b) MHA Bwd TFLOPS with 0.2 sparsity ratio (c) MHA Fwd TFLOPS with 0.5 sparsity ratio (d) MHA Bwd TFLOPS with 0.5 sparsity ratio (e) MHA Fwd TFLOPS with 0.8 sparsity ratio (f) MHA Bwd TFLOPS with 0.8 sparsity ratio Figure 18: TFLOPS of block sparse attention kernels with 64 block size and MHA is separately discussed because it is hard to generate the dynamic block sparse block through compilation, causing severe memory overhead with O(S2) block mask representations. So we only test its TFLOPS performance with 4 heads and 16K context length in Table 5, most of FlexAttentions GPU memory is consumed during mask creation, while its runtime memory usage is not high. In block sparse scenarios, an efficient mask representation is crucial for GPU memory consumption. 49 Table 3: TFLOPs of Sparse Kernels for GQA (64:8) Forward Note: Seqlen = Sequence Length, SR = Sparsity Ratio, = Not Supported. BlockSize 64 BlockSize 128 Kernels Seqlen SR 0.2 SR 0. SR 0.8 SR 0.2 SR 0.5 SR 0.8 FlashInfer FA2 Sparse 16k 32k 48k 64k 80k 96k 112k 128k 16k 32k 48k 64k 80k 96k 112k 128k 211.78 232.81 211.50 190.73 201.71 202.03 198.14 196.47 236.30 243.77 218.30 210.82 198.39 197.48 196.82 195.61 247.83 203.54 215.47 204.16 206.43 201.42 199.41 197.93 387.63 422.95 413.11 414.25 351.87 393.16 393.41 389. 312.54 327.46 331.99 328.84 293.43 328.38 298.22 317.48 449.79 455.97 434.61 427.77 393.34 387.89 389.33 396.90 326.62 333.45 281.27 329.88 330.62 318.18 316.92 317.11 472.52 485.06 432.16 399.22 410.88 398.73 397.02 393.80 332.00 339.16 331.24 329.99 319.52 321.05 322.47 321.04 Table 4: Forward TFLOPs of Sparse MHA Kernels (64:64, Block Size = 128) Note: Seqlen = Sequence Length, SR = Sparsity Ratio, = Not Supported. Kernels Seqlen SR 0.2 SR 0.5 SR 0. FlashInfer FA2 Sparse 16k 32k 48k 64k 80k 96k 112k 128k 16k 32k 48k 64k 80k 96k 112k 128k 414.93 425.38 407.67 398.01 342.30 388.09 380.67 384.05 315.12 327.63 331.79 325.86 295.20 325.09 301.61 308. 447.76 416.37 401.60 394.79 384.46 OOM OOM OOM 326.09 333.32 282.22 330.73 328.64 317.42 318.13 317.80 484.21 425.88 402.69 392.05 OOM OOM OOM OOM 331.27 337.07 335.95 329.45 318.44 320.26 322.56 321.67 Table 5: TFLOPs of Sparse FlexAttention (SeqLen = 16K) Note: Seqlen = Sequence Length, SR = Sparsity Ratio. BlockSize 64 BlockSize 128 Mode SR Forward Backward Forward Backward GQA (4:1) MHA (4:4) 0.2 0.5 0.8 0.2 0.5 0.8 29.05 48.21 99.80 28.75 48.76 101.58 57.22 95.33 170. 63.14 103.37 186.44 339.74 352.09 358.55 341.55 345.84 352.18 299.43 312.76 325.52 305.37 337.60 349.00 (a) MHA Fwd peak memory with 0.2 sparsity ratio (b) MHA Bwd peak memory with 0.2 sparsity ratio (c) MHA Fwd peak memory with 0.5 sparsity ratio (d) MHA Bwd peak memory with 0.5 sparsity ratio (e) MHA Fwd peak memory with 0.8 sparsity ratio (f) MHA Bwd peak memory with 0.8 sparsity ratio Figure 19: Peak memory of block sparse attention kernels with 64 block size and MHA Table 6: Peak Memory (GB) of Sparse Kernels for GQA (64:8) Forward Note: Seqlen = Sequence Length, SR = Sparsity Ratio, = Not Supported. BlockSize 64 BlockSize 128 Kernels Seqlen SR 0.2 SR 0.5 SR 0.8 SR 0.2 SR 0. SR 0.8 FlashInfer FA2 Sparse 16k 32k 48k 64k 80k 96k 112k 128k 16k 32k 48k 64k 80k 96k 112k 128k 5.468 6.620 7.837 9.115 10.455 11.857 13.322 14.846 5.502 6.770 8.174 9.715 11.393 13.207 15.159 17.247 51 5.540 6.920 8.512 10.317 12.330 14.557 16.997 19.647 5.448 6.552 7.684 8.845 10.034 11.250 12.493 13.766 0.810 1.393 1.986 2.590 3.205 3.830 4.466 5.113 5.466 6.627 7.853 9.145 10.502 11.923 13.413 14. 0.810 1.393 1.986 2.590 3.205 3.830 4.466 5.113 5.485 6.702 8.022 9.444 10.970 12.599 14.331 16.165 0.810 1.393 1.986 2.590 3.205 3.830 4.466 5.113 Table 7: Forward Peak Memory of Sparse MHA Kernels (64:64, Block Size = 128) Note: Seqlen = Sequence Length, SR = Sparsity Ratio, = Not Supported. Kernels Seqlen SR 0.2 SR 0.5 SR 0.8 FlashInfer FA2 Sparse 16k 32k 48k 64k 80k 96k 112k 128k 16k 32k 48k 64k 80k 96k 112k 128k 5.477 6.791 8.316 10.051 11.994 14.148 16.514 19.085 1.251 2.281 3.329 4.395 5.478 6.587 7.696 8.832 5.627 7.392 9.666 12.452 15.746 OOM OOM OOM 1.251 2.281 3.329 4.395 5.478 6.578 7.696 8. 5.776 7.991 11.015 14.849 OOM OOM OOM OOM 1.251 2.281 3.329 4.395 5.478 6.578 7.696 8.832 Table 8: Peak Memory (GB) of Sparse FlexAttention (SeqLen = 16K) Note: Seqlen = Sequence Length, SR = Sparsity Ratio. BlockSize 64 BlockSize Mode SR Forward Backward Forward Backward GQA (4:1) MHA (4:4) 0.2 0.5 0. 0.2 0.5 0.8 0.285 0.287 0.288 0.304 0.306 0.307 0.383 0.384 0.386 0.449 0.450 0.452 0.294 0.296 0. 0.313 0.315 0.316 0.392 0.394 0.395 0.458 0.460 0.461 52 A.6 DETAILS OF CONTEXT PARALLEL ATTENTION PERFORMANCE Performance of ring P2P. The main limitation of ring P2P is that its communication volume cannot be adjusted. In each iteration, while computing with the KV pairs for the current stage, each device simultaneously sends its own KV to the next device in the topology and receives the KV needed for the next stage. In multi-node, multi-GPU setup, all GPUs are connected in ring-based P2P topology. The communication bottleneck is determined by the slowest inter-node link, forcing intranode communication to synchronize with the inter-node transfers, which leads to substantial overall bandwidth underutilization. Ultimately, the overall efficiency of Ring P2P is determined by the actual per-GPU computation, which manifests in whether communication can be overlapped with computation. The FULL scenario represents the optimal performance case for Ring P2P. As shown in the Figure 20, in this scenario, each GPU executes the largest per-kernel computation. When inter-node communication can also be effectively overlapped with computation, further scaling the distributed setup does not significantly change the balance between computation and communication efficiency, so the overall computational efficiency remains essentially constant. In the CAUSAL scenario, the per-GPU communication volume of Ring P2P remains the same as in the FULL scenario. Even after load balancing, the per-stage computation per GPU is roughly half of that in the FULL scenario (except for the first stage, which is 3/4). The reduced computation may no longer fully overlap with communication, leading to performance drop in Ring P2P under CAUSAL, as shown in the Figure 21. During the forward pass, when scaling to 8 nodes, we observe further performance degradation. We attribute this mainly to machine instability: even if only single GPU underperforming in the ring, for example from sudden drop in computation efficiency, can stall the entire topology and greatly reduce overall efficiency. In large-scale distributed settings, such effects are inevitable, and we report the experimental results faithfully. For the DOCUMENT scenario, as shown in the Figure 22, Ring P2P exhibits similar trends in both the CAUSAL DOCUMENT and FULL DOCUMENT settings. Unlike the FULL/CAUSAL scenarios, it does not maintain relatively constant trend, which is expected. First, when handling variable-length data, each segment must be padded according to its specific scale, leading to significant variation in per-GPU computation per iteration, while communication volume remains constant. Second, sampling of variable-length data introduces differences in computational sparsity across iterations, resulting in fluctuations in the overall trend. Hybrid Design. USP and LoongTrain share very similar overall architectures, using the Ulysses design intra-node and Ring P2P inter-node. Overall, they achieve significant and stable performance improvements in the FULL and CAUSAL scenarios, as shown in Figure 20 and 21. In the FULL DOCUMENT and CAUSAL DOCUMENT scenarios, performance gradually decreases due to reduced overall computation, which aligns with expectations. Additionally, in the DOCUMENT scenario, as shown in Figure 23 and 22, both architectures demonstrate improved stability, mitigating the limitations of using only Ulysses or Ring P2P. Here, we primarily explain why LoongTrain generally outperforms USP during the forward pass, yet performs on par with or slightly worse than USP during the backward pass. In our benchmark reproduction and optimization, USP and Ring P2P both use the same RingAttn class. In the ring-topology iterations, the forward and backward data flows are essentially opposite. For example, during the forward pass, GPUi receives data from GPUi1 and sends data to GPUi+1; in the backward pass, GPUi receives from GPUi+1 and sends to GPUi1. This design is both necessary and reasonable. At the end of the forward pass, GPU0 actually holds the initial-stage KV data of GPU1, and so on, with GPUN 1 finally holding GPU0s initial data. If the backward pass rotated data in the same direction as the forward pass, it would require either an additional P2P communication or storing the initial-stage KV on each GPU in advance, both of which incur extra overhead. Instead, we exploit the time difference between KV and gradient generation: the backward pass directly continues from the forward-pass KV states in reverse rotation, while gradients computed in the current stage are sent during the next stage. After completing the same number of rotations, each 53 GPU receives exactly its corresponding gradient (e.g., GPU0 receives the gradient for KV0, and so on). For LoongTrains DoubleRingAttn, the forward pass is consistent with USP. In addition, LoongTrain leverages heterarchical P2P architecture to implement two-level sliding window, decomposing the full ring topology into intra-window and inter-window groups. The intra-window group is identical to the RingAttn class, but in the first stage, an additional P2P communication is performed for the inter-window group to prefetch the initial data for each GPU after the next inter-window rotation. This design fully utilizes inter-node bandwidth, resulting in superior forward-pass performance compared to USP. However, LoongTrains backward pass cannot directly leverage the forward-pass end states. While this poses no issue for the last inter-window, it alters the initial state for each subsequent interwindow, and the final states differ depending on the specific intraand inter-window configuration. As result, LoongTrain performs forward and backward passes using the same rotation order. Each GPU additionally stores the initial KV data, and to ensure correct gradient propagation, an extra P2P communication and synchronization of gradients is performed at the end of each inter-window. This guarantees that in the next inter-window, each GPU receives the corresponding KV data and gradients. Consequently, LoongTrain gains no significant backward-pass advantage from the heterarchical architecture, yet overall maintains performance comparable to USP. The observed fluctuations in trends are similarly attributed to the instability introduced by the additional P2P communications. Ulysess. For Ulysess, the results are straightforward: different sampling patterns naturally lead to variations in computation, and we recommend evaluating performance based on the specific application scenario. Figure 20: Forward and Backward TFLOPs of Context Parallel Attention on FULL 54 Figure 21: Forward and Backward TFLOPs of Context Parallel Attention on CAUSAL Figure 22: Forward and Backward TFLOPs of Context Parallel Attention on CAUSAL DOCUMENT Figure 23: Backward TFLOPs of Context Parallel Attention on FULL DOCUMENT"
        }
    ],
    "affiliations": [
        "Peking University, China",
        "State Key Laboratory for Novel Software Technology, Nanjing University, China",
        "Zhejiang University, China"
    ]
}