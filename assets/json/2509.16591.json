{
    "paper_title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature",
    "authors": [
        "Zheng Liu",
        "Mengjie Liu",
        "Siwei Wen",
        "Mengzhang Cai",
        "Bin Cui",
        "Conghui He",
        "Wentao Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 1 9 5 6 1 . 9 0 5 2 : r From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Tokens Nature Zheng Liu1,2, Mengjie Liu1,2, Siwei Wen3, Mengzhang Cai2, Bin Cui1, Conghui He2, Wentao Zhang1 1Peking University, 2Shanghai AI Laboratory, 3Beihang University Abstract Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewardsadjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO. (a) Qwen2.5-Math-1.5B (b) Qwen2.5-Math-7B (c) Qwen3-8B Figure 1: AIME24 Results (a) Qwen2.5-Math-1.5B (b) Qwen2.5-Math-7B (c) Qwen3-8B Figure 2: AIME25 Results Equal Contribution, Email: lz030515123@gmail.com Corresponding Author"
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) [18] has emerged as fundamental technique for aligning Large Language Models (LLMs)[6; 17; 3] with human preferences and enhancing their reasoning capabilities. State-of-the-art models including OpenAI o1[16], Claude3.5[2], DeepSeek-R1 [5], Seed-1.5-Thinking [23], and the Qwen3 series [29] have achieved remarkable performance gains on complex reasoning benchmarks. These achievements underscore how carefully designed reinforcement learning frameworks can empower LLMs to tackle tasks requiring multi-step reasoning, logical consistency, and systematic problem solving. Despite these successes, existing algorithms[21; 24; 30] share fundamental limitation: they employ uniform optimization strategy across all tokens. From rollout sampling to advantage calculation to clipping loss computation, these algorithms fail to distinguish between tokens that represent critical reasoning junctures versus those that merely serve as syntactic connectives or routine patterns. This uniform treatment fundamentally conflicts with the heterogeneous nature of language generation, where tokens serve dramatically different functional roles in the reasoning process. Recent works have recognized the importance of token heterogeneity. DAPO with forking tokens [27] proves that only minority of high-entropy tokens guide the optimization process. Archer [26] reduces token to binary high/low entropy groups and encouraging exploration for high-entropy tokens by relaxing their clipping bounds. Nevertheless, such coarse discretization imposes sharply different strategies across arbitrary boundaries, causing tokens with marginally different entropy values to receive vastly different updates. EDGE-GRPO [33] uses the entropy of sequences to modify advantages and assigns higher advantages to responses that are confident. However, this modification operates at the sequence-level, lacking fine-grained assignment between tokens within sequences. More importantly, as demonstrated in Section 3 and Section 4, many of these strategies contradict core optimization algorithms, as they overlook training dynamics and lack token-level analysis. To address these limitations, We first conduct systematic empirical analysis of entropy and learning dynamics, characterizing the distinct entropy-RL interactions that govern each stage: Rollout Generation: Exploration-Exploitation Imbalance. Our analysis reveals that high-entropy tokens encoding critical reasoning decisions are rare and inversely correlated with sampling probability. We investigate using temperature to control the occurrence of critical tokens and discover an irreconcilable dilemma: low temperatures maintain accuracy but suppress critical tokens, starving the model of exploration opportunities; high temperatures increase their generation but introduce excessive noise. Standard rollout fails to capture sufficient high-quality exploratory signals. Advantage Calculation: Reward Attribution Granularity. Standard advantage computation distributes sequence-level rewards uniformly across all tokens. This coarse-grained attribution proves especially problematic given DAPOs token-mean averaging, which allows longer negative samples to dominate gradients. Moreover, our importance ratio analysis reveals that tokens require vastly different update magnitudescritical high-entropy decisions need strong reinforcement while neutral tokens need minimal adjustment. Yet uniform reward distribution and entropy-only approaches fails to identify which specific token-level decisions actually determine reasoning success. Clipping Loss Computation: Clipping Constraint Mismatch. Our clipping analysis reveals paradoxical patterns: low-entropy tokens predominantly hit left boundaries, preventing probability reduction, while high-entropy tokens hit right boundaries, limiting exploration. Semantic analysis reveals that left-clipped low-entropy tokens are mostly irrelevant artifacts like formatting symbols, while right-clipped high-entropy tokens include crucial reasoning elements. Uniform clipping thus protects noise while constraining exploration, directly opposing optimal learning dynamics. These findings reveal that effective optimization requires token-level adaptations tailored to each stages unique challenges. We introduce Heterogeneous Adaptive Policy Optimization (HAPO), comprehensive framework that systematically addresses the discovered entropy-RL interactions. Rather than using entropy for coarse categorization, HAPO leverages entropy as continuous signal throughout the pipeline, enabling fine-grained optimization that respects token heterogeneity. Our approach comprises four key innovations: Adaptive Temperature Sampling: To address the critical token scarcity in rollout generation, we dynamically adjust sampling temperature based on token entropyincreasing temperature for high-entropy tokens to promote exploration at reasoning branch points, while 2 Figure 3: HAPO overall framework: four entropy-aware components address token heterogeneity. Adaptive Temperature Sampling adjusts temperature by token entropy. Token-Level Group Average normalizes advantages across tokens. Differential Advantage Redistribution modulates advantages using entropy and importance ratios. Asymmetric Adaptive Clipping applies entropy-conditioned boundaries for targeted noise suppression and exploration. reducing temperature for low-entropy tokens to maintain semantic coherence. This resolves the irreconcilable trade-off between accuracy and exploration, enriching the representation of critical tokens in the training corpus. Token-Level Group Average Advantage: To counter DAPOs length-induced gradient bias where longer negative samples dominate optimization, we normalize advantages at the token level across the entire group rather than at the sequence level. This ensures balanced optimization between positive and negative samples while preserving DAPOs benefits for long-sequence learning, creating stable foundation for subsequent advantage redistribution. Differential Advantage Redistribution: To address the heterogeneous update patterns revealed by importance ratios, we jointly leverage entropy and ratio information to modulate advantages within sequences. High-entropy tokens with ratios far from 1.0 receive amplified advantages, while low-entropy tokens near 1.0 receive suppressed advantages. This enables fine-grained advantage attribution aligned with each tokens actual optimization needs. Asymmetric Adaptive Clipping: To correct the inverted constraint priorities where noise is protected while exploration is restricted, we implement entropy-conditioned reversed asymmetric boundaries. Low-entropy tokens receive expanded left boundaries to enable aggressive noise suppression, while high-entropy tokens receive expanded right boundaries to facilitate exploration at critical decision points. Our work demonstrates that recognizing and leveraging token heterogeneity is crucial for advancing RLHF. By introducing systematic, token-aware optimizations throughout the learning pipeline, HAPO achieves substantial improvements across mathematical reasoning benchmarks. Our principled design ensures generalizability to other domains. We believe this work will inspire further research into heterogeneous optimization that more accurately reflect the complex, multi-scale nature of language generation and reasoning. The rest of this paper is organized as follows. Section 2 provides background on existing RLHF methods. Section 3 presents empirical evidence for token heterogeneity, demonstrating the necessity of token-aware optimization. Section 4 analyzes entropy-RL interactions across the three pipeline stages using binary categorization for analytical clarity. Section 5 presents HAPO, extending the binary analysis to continuous entropy-based functions. We evaluate HAPO against state-of-the-art DAPO baselines across different model scales."
        },
        {
            "title": "2 Background",
            "content": "We first establish the reinforcement learning framework for language models. Consider language model policy πθ parameterized by neural network weights θ. In the RL setting for language generation, states consist of the prompt and previously generated tokens o<t = (o1, o2, ..., ot1), actions are tokens ot selected from vocabulary V, and the policy πθ(otq, o<t) defines the probability of selecting token ot given the current state. Rewards R(o) are typically provided at the sequence level for the complete response o. Proximal Policy Optimization. PPO[21] constrains policy updates to remain within trust region of the previous policy πθold through the following objective: (cid:88) LPPO(θ) = E(q,o)D (cid:16) min rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17) (1) t=1 where rt(θ) = πθ(otq,o<t) πθold (otq,o<t) denotes the importance sampling ratio. The advantage ˆAt is computed using Generalized Advantage Estimation (GAE)[22]: ˆAGAE(γ,λ) = (cid:88) l=0 (γλ)lδV t+l (2) where δV γ [0, 1] and bias-variance parameter λ [0, 1]. = rt + γV (st+1) (st) represents the temporal difference error, with discount factor Group Relative Policy Optimization. GRPO[24] eliminates the value function dependency by employing group-based advantage normalization. For each prompt q, GRPO samples responses {oi}G i=1 and optimizes: LGRPO(θ) = EqP (Q) {oi}G i=1πθold (q) 1 (cid:88) i=1 1 oi oi (cid:88) t=1 LGRPO i,t (3) The per-token loss incorporates both the clipped objective and KL regularization: LGRPO i,t = min (cid:16) ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) βDKL [πθ(q, oi,<t)πref(q, oi,<t)] (4) The advantage is normalized within the group as ˆAi,t = , which corresponds to GAE with λ = 1 combined with group normalization. The KL divergence term DKL[πθπref] prevents excessive deviation from the reference policy. Rimean({Rj }G std({Rj }G j=1) j=1) Decoupled Clip and Dynamic Ampling Policy Optimization. DAPO [30] modifies GRPO with token-mean normalizationaveraging over total tokens rather than sequencesto preserve gradient contributions from longer sequences. It also introduces the clip-higher mechanism with asymmetric bounds: LDAPO(θ) = E(q,a)D 1 i=1 oi (cid:80)G (cid:88) oi (cid:88) (cid:16) min i=1 t=1 ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵlow, 1 + ϵhigh) ˆAi,t (cid:17) (5) The asymmetric bounds ϵhigh > ϵlow (typically ϵhigh = 0.28, ϵlow = 0.2) allow larger probability increases for exploration while maintaining conservative probability decreases. DAPO with Forking Tokens. Recent work[27] demonstrates that only minority of tokens exhibit high entropy and function as critical decision points in reasoning paths. This observation motivates selective optimization focusing on high-entropy tokens: LEntropy(θ) = E(q,o)D (cid:88) I[Ht τρ] min (cid:16) rt(θ) ˆAt, clip(rt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17) (6) where Ht = (cid:80) the ρ-th percentile entropy threshold (e.g., ρ = 80 for [27]), and I[] is the indicator function. vV πθ(vq, o<t) log πθ(vq, o<t) represents the entropy at position t, τρ denotes t=1 4 (a) AIME (b) AIME2025 (c) MATH Figure 4: Performance of Different Entropy-Based Selection Methods (a) Average Length Statistics (b) Dataset Distribution (c) Solved Problem Distributions Figure 5: Distribution Analysis and Problem-Solving Pattern Characterization"
        },
        {
            "title": "3 Token Heterogeneity in RLHF: Empirical Necessity and Foundations",
            "content": "Recent advances in RLHF suggest that tokens should not be weighted uniformly, yet systematic empirical evidence remains limited. In this section, we conduct comprehensive investigation into the necessity of heterogeneous token treatment. Through entropy-based selective training experiments and distributional analyses, we reveal the limitations of uniform weighting and expose the intricate dual-entropy phenomenon, which challenges both naive uniform weighting and simplistic entropy-thresholding strategies. All experiments use DAPO in verl (see Appendix for details). Selective Training Under Entropy Thresholds To understand whether heterogeneous token treatment is necessary, we conduct ablation studies examining the impact of selective token training based on entropy thresholds. We systematically evaluate models trained exclusively on different entropy ranges to quantify the contribution of various token subsets. Experiment 1: Entropy-Based Token Selection We investigate the necessity of token heterogeneity through selective training experiments using Qwen2.5-Math-7B. Our experimental configuration includes: Training Configuration: We employ four distinct training strategies - training only on top 20% highest-entropy tokens, top 80% highest-entropy tokens, bottom 80% lowest-entropy tokens, and bottom 20% lowest-entropy tokens. Analysis: We evaluate model accuracy on AIME2024, AIME2025 and MATH. The results are shown in Figure 4. The token entropy selection strategies are detailed in Experiment 1. The results in Figure 4 highlight nuanced relationship between entropy-based selection and downstream performance. Training on the top 20% highest-entropy tokens performs well on AIME2024, while low-entropy tokens (bottom 20% or 80%) consistently underperform on AIME2024. Notably, this high-entropy advantage disappears on the more challenging AIME2025. Interestingly, broader token selection (top/bottom 5 Figure 6: Entropy Divergence of Top-20 Most Uncertain Tokens Figure 7: Dual-Entropy Token Frequency-Entropy Landscape 80%) improves MATH performance compared to the concentrated top-20% strategy, suggesting that mixing lower-entropy tokens provides regularization benefits for better generalization. To investigate the underlying causes of this phenomenon, we conducted detailed analysis of the distribution patterns in both training and evaluation datasets in Observation 1. Observation 1: Distribution and Problem-Solving Pattern Analysis We analyze both the distributional similarity and problem-solving patterns across different entropy selection strategies. Our analysis includes: Distribution Analysis: We perform length analysis and clustering analysis on question-answer patterns to quantify the similarity between DAPO training data and evaluation datasets (AIME2024, AIME2025, AMC, MATH, and Olympiad Bench). Problem-Solving Patterns: We visualize clusters of correctly solved problems on test sets for models of different entropy selection strategies. The visualization results are shown in Figure 5. Through analysis in Figure 5, we uncover the cause of the discrepancy: DAPO training data aligns with AIME but differs from MATH. Models training on top 20% high-entropy tokens quickly overfit to training data, while models using top 80% high-entropy tokens maintain slightly more diverse answer distributions, demonstrating that lower-entropy tokens provide essential regularization. The Dual-Entropy Phenomenon To further understand why low-entropy tokens provide essential regularization and help maintain the models existing capabilities, we conducted comprehensive analysis of tokens generated during the training process to examine the intricate relationship between token forms and their entropy distributions in Observation 2. As shown in Figure 6 and Figure 7, we uncover the \"Dual-Entropy Phenomenon\": high-entropy tokens frequently have \"twin siblings\" in the low-entropy regiontokens with identical word stems that manifest with drastically different entropy values. This pattern spans multiple semantic categories and frequency ranges, explaining why low-entropy tokens are crucial: they contain important reasoning tokens that differ from high-entropy ones only in syntactic context. This finding reveals limitations of both entropy-based selection and uniform 6 (a) Entropy Distribution (b) Entropy vs Probability (c) Top 100 High Entropy Tokens Figure 8: Token Probability and Entropy Analysis (a) Accuracy (b) Critic Token Counts (c) Critic Token Entropy Figure 9: Temperature Effects on Model Performance and Critic Token Behavior treatment, necessitating nuanced approach that preserves low-entropy tokens stabilizing influence without letting them dominate the learning signal. Observation 2: Token-Level Entropy Distribution Analysis We analyze the relationship between token forms and their entropy values across the entire token distribution. Our analysis configuration includes: Datasets: We examine AIME2024, AIME2025, AMC, Olympiad Bench, and MATH. Scale: We generated approximately 107 tokens across diverse problem types. Analysis: We categorize tokens by their semantic types and examine their entropy divergence and entropy-frequency landscape. The visualization results are shown in Figure 6 and Figure 7. We present concrete examples of dual-entropy tokens in Figure 22, 23, 24, 25, 26, 27."
        },
        {
            "title": "4 Binary Differentiation: Functional Roles of Entropy-Based Categories",
            "content": "Instead of treating all tokens uniformly, we explore heterogeneous approaches tailored to different token characteristics. As first step, we investigate binary differentiationseparating high-entropy and low-entropy tokensacross three critical RL stages: rollout generation, advantage computation, and clipping loss calculation. This analysis reveals how leveraging token heterogeneity enables more nuanced optimization, laying the groundwork for our fine-grained approach. All experiments use DAPO in verl (see Appendix for details). 4.1 Rollout Generation: Exploration-Exploitation Imbalance We first examine rollout generation, stage neglected by previous work. Standard rollout often fail to generate sufficient high-entropy tokens for exploration and learning, creating an exploration7 exploitation imbalance. Through systematic investigation, we identify temperature as the key factor controlling both the frequency of critical tokens and accuracy. This leads us to develop dynamic temperature scheduling method that enhances exploratory token generation while maintaining output quality, establishing balanced foundation for subsequent heterogeneous token treatment. Characterizing Tokens in Rollout Generation To understand the generation dynamics during rollout, we conduct comprehensive analysis of token properties. We examine probability distributions, occurrence frequencies, and entropy values in Observation 3. Observation 3: Token Property Analysis We analyze token characteristics using Qwen2.5-Math-7B on challenging mathematical reasoning datasets. Our experimental setup includes: Datasets: We examine AIME2024, AIME2025, AMC, Olympiad Bench, and MATH. Scale: We generated approximately 107 tokens across diverse problem types. Analysis: We examined entropy distributions, probability-entropy relationships, and characteristics of high-entropy tokens. The visualization results are shown in Figure 8. As shown in Figure 8, most tokens cluster near zero entropy, with frequency decreasing sharply as entropy increaseshigh-entropy tokens are exceptionally rare. We find an inverse relationship between entropy and sampling probability. Among the top 100 highest-entropy tokens, most represent critical decision points in mathematical reasoning, creating paradox: the tokens most valuable for learning are systematically under-sampled during standard rollout generation. Temperature Effects on Critic Token Generation This paradox motivates our investigation into how to control the occurrence frequency of critical tokens and their corresponding entropy during the sampling process. We turn our attention to temperaturethe primary hyperparameter governing sampling randomness in language models. We systematically evaluate rollout generation across various temperature settings in Observation 4. Observation 4: Temperature Impact Analysis We conduct comprehensive experiments on Qwen2.5-Math-7B to understand how temperature affects important token generation during rollout. Our experimental configuration includes: Temperature Range: Systematically varied from 0.0 to 1.5 in increments of 0.1. Scale: We selected the top 20% highest-entropy tokens from Observation 3 (approximately 1,452 tokens) as critical tokens. We generated approximately 107 tokens across diverse problem types. Analysis: We examined model accuracy, critical token count and ratio, average entropy of critical tokens. The experimental results are visualized in Figure 9. Figure 9 reveals complex temperature-rollout relationship. While model accuracy decreases with increasing temperature, the occurrence of critical tokens shows an overall increasing. The entropy grows exponentially with temperature. Through concrete output analysis in Figure 25, 26, 27, we find that high-entropy tokens typically represent critical reasoning decisions. Conversely, low-entropy tokens involve formula calculations, where low temperature ensures computational stability. This reveals fundamental trade-off: low temperatures maintain accuracy but suppress critical token generation, while high temperatures increase entropy and generate more meaningful tokens, but degrade accuracyproducing noise rather than useful diversity. Adaptive Temperature Sampling The observed trade-off between token diversity and model accuracy suggests that no single temperature value can optimally serve all positions in sequence. Different stages of mathematical reasoning require varying degrees of explorationdeterministic 8 (a) AIME2024 (b) AIME (c) Response Length Figure 10: Validation of Entropy-Guided Adaptive Temperature Sampling Strategy calculations demand low temperatures for precision, while strategic decisions benefit from higher temperatures to explore alternative approaches. This heterogeneity motivates us to develop an adaptive temperature strategy that dynamically adjusts sampling randomness based on token characteristics. In our approach, we use entropy as the primary signal to guide temperature adaptation. Method 1: Adaptive Temperature Sampling Objective: Dynamically adjust sampling temperature based on current token entropy. Temperature Schedule: Ti,t = (cid:26)Thigh Tlow if Hi,t > θ if Hi,t θ (high-entropy tokens) (low-entropy tokens) (7) where: Hi,t = (cid:80) log is the entropy at position i, θ is the entropy threshold Thigh > 1.0 enhances exploration at uncertain positions Tlow < 1.0 maintains coherence at confident positions Experiment 2: Adaptive Temperature Sampling Validation We evaluate the effectiveness of our entropy-guided temperature strategy using Qwen2.5Math-7B on mathematical reasoning tasks. Our experimental setup includes: Training Configuration: We set θ = 0.5, Thigh = 1.1, Tlow = 0.8 empirically. Baselines: We conduct baseline experiments with: Fixed temperatures {0.5, 0.8, 1.0, 1.1} Random temperature sampling U(0.8, 1.1) Analysis: We compare model accuracy, output length, and average entropy. The results are shown in Figure 9 and Figure 10. As shown in Method 1, we apply high temperature to high-entropy tokens to encourage exploration at critical decision points, while using low temperature for low-entropy tokens to maintain structural coherence. Since rollout generation proceeds token-by-token without sequence-level context, we use hard entropy threshold to determine temperature assignment for each token independently. Detailed configurations are in Experiment 2. Figure 9 shows our adaptive temperature sampling achieves dual benefits: improved sequence correctness and increased critical token proportion with higher entropy. As illustrated in Figure 10, model trained with our method shows higher accuracy and response length. Low-temperature models suffer from insufficient exploration and late-stage degradation, while high-temperature models exhibit training instability. Interestingly, the random temperature model shows partial performance gains, suggesting that even unguided temperature variation can provide some benefit over fixed temperature strategies. 9 (a) Average Advantage (b) Average Length Differences (c) Sum Length Differences Figure 11: Analysis of Advantage Dynamics And Response Length Disparities (a) AIME2024 (b) Response Length (c) Average Entropy Figure 12: Analysis of Model Performance on Different Clipping Bounds 4.2 Advantage Calculation: Reward Attribution Granularity We next examine advantage calculation. Through systematic analysis of advantage dynamics, we discover that sequence-level advantage computation creates severe imbalance between positive and negative tokens. We address this through token-level advantage averaging, establishing foundation for entropy-based advantage redistribution. Additionally, we introduce importance ratiosa novel approach that directly captures each tokens update dynamics to guide advantage modulation, effectively aligning optimization with both token uncertainty and learning progress. Observation 5: Advantage Distribution Analysis We conduct comprehensive analysis of advantage dynamics during DAPO training with Qwen2.5-Math-7B. Our analysis configuration includes: Analysis: We track average advantage values during training, record mean advantage ranges, and compute average and sum length differences between positive and negative samples. Scaling Analysis: We additionally present visualization results of mean advantage ranges with random scaling factors α U(0.75, 1.25) applied to advantages. The experimental results are visualized in Figure 11. Advantage Dynamics Analysis To understand the behavior of advantage values in DAPO training, we systematically analyze the distribution patterns and characteristics of advantages during the optimization process in Observation 5. As illustrated in Figure 11, we identify fundamental imbalance in advantage distribution that biases toward negative reward sequences.While GRPO normalizes by sequence length in Equation 3, DAPOs token-mean loss in Equation 5 preserves individual token gradients but creates bias: negative samples generate longer responses that dominate gradient computation. Our analysis in Figure 11b, 11c reveals that negative samples systematically generate longer responses, contributing proportionally more tokens to the gradient computation, dominating the optimization process. 10 (a) AIME2024 (b) AIME2025 (c) Response Length Figure 13: Validation of Token-Level Group Average Advantage Experiment 3: Entropy Collapse with Relaxed Left Clipping We investigate the impact of relaxing left clipping boundaries. Our experimental configuration includes: Training Configuration: We test different left clipping boundaries: 0.75, 0.70, compared to baseline 0.80. The right clipping boundary is fixed at 1.28 for all configurations. Analysis: We evaluate model accuracy on AIME2024, model response length, and average entropy throughout training. The results are shown in Figure 12. This mechanism severely impacts token-level optimization. First, as demonstrated in Figure 11a, when we apply minor perturbations to advantages, token-mean loss amplifies these minor advantage fluctuations into substantial optimization deviations, greatly affecting the stability of advantage redistribution methods. Second, it undermines Archer [26]s clipping strategies that relax left bounds for high-entropy tokens. We try different left clipping bounds in Experiment 3. As shown in Figure 12, negative advantage dominance with relaxed left boundaries causes rapid probability degradation for high-entropy tokens. Since these tokens mark critical reasoning points, their suppression triggers cascade: the model becomes conservative, generating shorter responses with lower diversity and eventual entropy collapse. This explains why DAPOs clip higher only modifies the right boundary. Token-Level Group Average Advantage Computation DAPOs bias toward negative advantages and its sensitivity to advantage fluctuations pose significant challenges for advantage redistribution. We propose computing group advantages at the token level rather than the sequence level. Our method is illustrated in Method 2. The key distinction lies in the granularity of advantage computation. We first distribute rewards to individual tokens, then apply normalization across all tokens. This ensures (cid:80) (i,t)T Ai,t = 0, resolving the gradient bias problem inherent in DAPOs token-mean framework. Moreover, our token-level group average method preserves length-dependent gradient scaling within reward categories. Formally, for sequences within the same reward class (positive or negative), longer sequences contribute proportionally larger gradientsmaintaining DAPOs effective treatment of complex, multi-step reasoning. Simultaneously, the cross-category normalization guarantees that (cid:80) iD , preventing systematic bias toward either positive or negative samples. This design elegantly reconciles DAPOs length-aware optimization with GRPOs balanced gradient distribution. i+D+ i+ (cid:80) We validate the effectiveness of token-level group average advantage in Experiment 4. The results in Figure 11a demonstrate that our method successfully addresses the imbalance issue. Actually, token-level group advantage computation can be viewed as form of advantage modulation between positive and negative tokens. When positive tokens are scarce, our method effectively amplifies the advantages of positive tokens while diminishing those of negative tokens, resulting in more targeted gradient updates. Notably, as shown in Figure 13, this leads to faster learning and superior performance on difficult tasks such as AIME2025, demonstrating enhanced adaptation capabilities. 11 (a) Token Distribution: Entropy-Dependent Ratio Spread (b) Ratio-Advantage Heatmap Figure 14: Token Update Patterns via Importance Ratios Furthermore, another key advantage of token-level group average computation is its ability to balance advantages across tokens, which stabilizes subsequent intra-sequence advantage redistribution. As shown in in Figure 11a, token-level normalization provides stable foundation that maintains overall balance even under aggressive modulation strategies, ensuring training stability while enabling fine-grained control over individual token contributions. Method 2: Token-Level Group Average Advantage Computation Objective: Balance positive and negative gradient contributions by normalizing advantages across all tokens in the group. Advantage Formulation: where: Ai,t = ai,t µtok σtok (8) ai,t = ri {0, 1} is the token-level reward inherited from sequence µtok = 1 (cid:113) 1 (i,t)T ai,t is the mean across all tokens in the group (cid:80) (i,t)T (ai,t µtok)2 is the standard deviation σtok = (cid:80) = {(i, t)} represents all token positions across all sequences in the group Experiment 4: Token-Level Group Average Advantage Validation We evaluate the effectiveness of our token-level group advantage computation strategy using Qwen2.5-Math-7B on mathematical reasoning tasks: Baselines: We compared against standard DAPO with sequence-level advantages. Analysis: We compared model accuracy and output length. We also tracked advantage dynamics including average advantage and max advantage. The results are shown in Figure 11 and Figure 13. Analyzing Token Update Patterns Existing method [33] uniformly modulates advantages based on sequence entropy, lacking token-level granularity and ignoring actual update dynamics. To address this, we explore intra-sequence advantage redistribution. Since advantages directly determine the update magnitude for each token, we need principled guidance for this redistribution. We first leverage importance ratios rt(θ) = πθ(atst) πref(atst) to analyze the update patterns of different tokens in Observation 6, as these ratios provide direct insight into the models optimization tendencies. As shown in Figure 14, we discover substantial heterogeneity in update magnitudes. High-entropy tokens have broader ratio distributions, with most high-entropy tokens deviating significantly from 1.0 as the model preferentially updates them. However, high-entropy tokens near 1.0 show update uncertainty. For the top 20% high-entropy tokens in Figure 14b, we observe strong correlation between advantages and ratiostokens with high advantages predominantly exhibit high ratios, 12 indicating clear update preferences by the model. Meanwhile, we find that low-entropy tokens ratios mostly cluster around 1.0, suggesting no clear update direction. However, similar to high-entropy tokens, those with large deviations are actually important for updates despite their low entropy. This reveals that entropy alone fails to capture actual update dynamicstokens with similar entropy have vastly different optimization needs based on importance ratios. Observation 6: Analyzing Token Update Patterns via Importance Ratios We analyze the relationship between importance ratios and token characteristics during DAPO training with Qwen2.5-Math-7B: Scale: We tracked over 107 tokens during the training process. Analysis: We examine ratio distributions across different entropy levels. Additionally, for the top 20% highest-entropy tokens, we quantified their distribution across entropy-ratio ranges to analyze ratio-advantage correlations We excluded the first half mini-batches from each batch, as these tokens consistently have ratio approximately equal to 1. The visualization results are shown in Figure 14. Method 3: Differential Advantage Redistribution Objective: Redistribute advantages within sequences based on both token entropy and importance ratios to achieve fine-grained optimization control. Joint Entropy-Ratio Modulation Function: ˆAi,t = (cid:26)λH (Ai,t, ri,t) Ai,t λL(Ai,t, ri,t) Ai,t if Hi,t top 20% of batch if Hi,t bottom 80% of batch (high-entropy tokens) (low-entropy tokens) where the modulation factors are defined as: λH (Ai,t, ri,t) = λL(Ai,t, ri,t) = 1 (cid:26)αH if ri,t / [γlow, γhigh] otherwise (cid:26)αL if ri,t [γlow, γhigh] otherwise 1 (9) (10) (11) where: [γlow, γhigh] defines the neutral zone αH > 1 is the amplification factor for high-entropy tokens αL < 1 is the suppression factor for low-entropy tokens The entropy percentile is computed across all tokens within each training batch Differential Advantage Redistribution Building on these observations, we propose novel advantage redistribution method. Unlike previous approaches that rely solely on entropy for advantage scaling, we incorporate importance ratios to capture the models actual update intentions within each mini-batch. This dual-signal approach enables us to align advantage modulation with the models natural optimization trajectory. Our Differential Advantage Redistribution is presented in Method 3. We first define neutral zone, typically centered around ratio = 1. High-entropy tokens receive amplification only when their importance ratio falls outside the neutral zone, reflecting clear update trends. We enhance them accordingly. For low-entropy tokens, we apply suppression within the neutral zone to reduce the influence of tokens lacking update tendency. This aligns with our analysis, where ratios far from 1 typically represent relatively important tokens, and we assign them relatively larger advantages. We validate the effectiveness of Differential Advantage Redistribution in Experiment 5. Figure 15 reveals the superior performance of our Joint Entropy-Ratio Advantage modulation. Compared with advantage scaling without ratio, our approach achieves more significant performance improvements, 13 (a) AIME2024 (b) AIME2025 (c) Response Length Figure 15: Validation of Differential Advantage Redistribution as we only amplify tokens with clear update directions. In contrast, random advantage scaling fails to bring any performance gains, confirming that targeted modulation based on both entropy and directional alignment is crucial for effective optimization. Experiment 5: Differential Advantage Redistribution Validation We evaluate the effectiveness of our joint entropy-ratio advantage modulation strategy using Qwen2.5-Math-7B on mathematical reasoning tasks: Training Configuration: We test entropy-ratio advantage modulation on tokenlevel group advantages with αH = 1.25, αL = 0.75, and neutral zone [γlow, γhigh] = [1 ϵl/2, 1 + ϵr/2] = [0.9, 1.14], half of the clipping-allowed importance ratio range. Baselines: We compare against: DAPO with token-level group advantages Advantage scaling without ratio (amplify advantage for high-entropy tokens while suppress advantage for low-entropy tokens) Random advantage scaling where each tokens advantage is multiplied by random factor λ U(0.75, 1.25) Analysis: We compare model accuracy and output length. Our results are shown in Figure 15. 4.3 Clipping Loss Computation: Clipping Constraint Mismatch We finally examine clipping loss computation. We discover significant asymmetry between highentropy and low-entropy tokens in their clipping patternsthese token categories exhibit fundamentally different sensitivities to left and right clipping boundaries. Based on this phenomenon, we design Asymmetric Adaptive Clipping that respects these differential sensitivities. Analyzing Clipped Token Properties During Training To understand how clipping affects our heterogeneous optimization framework, we systematically track and analyze every token that hits clipping boundaries during DAPO training. We categorize these tokens based on their entropy values and examine their clipping patterns in Observation 7. Figures 16 and 17 reveal distinct clipping patterns between high and low-entropy tokens. For left-side clipping, low-entropy tokens are clipped far more frequentlythese are typically unimportant tokens like formatting symbols or code elements. For right-side clipping, high-entropy tokens dominate, and while some are noisy, many are reasoning-critical tokens. This creates fundamental contradiction: uniform clipping constrains useless low-entropy tokens, preventing them from participating in subsequent gradient updates and their probabilities from decreasing further. Meanwhile, it restricts high-entropy tokens that require exploration freedom from further exploration. 14 Figure 16: Relationship Between Token Entropy Percentiles and Clipping Behavior (a) Right-Clipped Tokens (b) Left-Clipped Tokens (c) Critical Token Clipping Figure 17: Token-Level Clipping Patterns and Word Cloud Visualization Observation 7: Differential Clipping Sensitivities We analyze clipping patterns during DAPO training with Qwen2.5-Math-7B to understand how different token categories interact with clipping boundaries. Our experimental configuration includes: Scale: We tracked over 106 clipped tokens during training process. Analysis: We compare left and right clipping patterns across different entropy levels to discover the characteristics of different directions. Additionally, we examine the most frequently left clipped and right clipped tokens. We further analyze the entropy distribution of right-clipped critical tokens in Observation 3. Our results are shown in Figure 16 and Figure 17. Asymmetric Adaptive Clipping Based on the above findings, we propose Asymmetric Adaptive Clipping. Our method is presented in Method 4. By lowering the left boundary for low-entropy tokens, we allow less useful tokens to decrease more substantially. By raising the right boundary for high-entropy tokens, we encourage exploration at critical decision points. Our method differs from existing approach Archer [26] that completely relax clipping boundaries for high-entropy tokens while tightening them for low-entropy tokens. Instead, we apply asymmetric adjustments that respect the natural update tendencies of each token category. We validate the effectiveness of our asymmetric adaptive clipping in Experiment 6. As shown in Figure 18, asymmetric clipping achieves significant performance improvements and longer response lengths. We compare against reversed asymmetric clipping, which gives high-entropy tokens wider boundaries on both sides while imposing stricter constraints on low-entropy tokens. We find this reversed approach rapidly leads to entropy collapse with no performance gains. In essence, our method extends the clip-higher concept in DAPO but differs fundamentally by using entropy to determine whether to apply clip-higher or clip-lower. For high-entropy tokens, we apply stronger clip-higher on the right boundary, allowing greater exploration. Simultaneously, we keep the left boundary to prevent the probabilities of these critical tokens from decreasing too much. For (a) AIME2024 (b) AIME2025 (c) Response Length Figure 18: Validation of Asymmetric Adaptive Clipping low-entropy tokens, we apply clip-lower on the left boundary, enabling more aggressive probability reduction for noisy tokens. Method 4: Asymmetric Adaptive Clipping Objective: Design entropy-specific clipping boundaries that respect the differential sensitivities of tokens to optimization constraints. Asymmetric clipping loss: LCLIP(θ) = Et [min (ri,t(θ)Ai,t, clip(ri,t(θ), 1 ϵL(i, t), 1 + ϵR(i, t))Ai,t)] (12) where the boundary functions are defined as: ϵL(i, t) = ϵR(i, t) = (cid:40) ϵhigh ϵlow (cid:40) ϵhigh ϵlow where: if Hi,t top 20% of batch if Hi,t bottom 80% of batch (high-entropy tokens) (low-entropy tokens) if Hi,t top 20% of batch if Hi,t bottom 80% of batch (high-entropy tokens) (low-entropy tokens) (13) (14) > ϵhigh , ϵhigh > ϵlow creates asymmetric lower boundaries ϵlow Lower boundary: ϵlow allows aggressive reduction for noisy tokens. Upper boundary: ϵhigh enables exploration at decision points. The entropy percentile is computed across all tokens within each training batch Experiment 6: Asymmetric Adaptive Clipping Validation We evaluate the effectiveness of our asymmetric adaptive clipping strategy using Qwen2.5Math-7B: Training Configuration: We set ϵlow = 0.2 for low-entropy tokens and = 0.35 for high-entropy tokens. This results in clipping boundaries ϵhigh = 0.2, ϵhigh of [0.65, 1.2] for low-entropy tokens and [0.8, 1.35] for high-entropy tokens. = 0.35, ϵlow Baselines: We compare against: Standard DAPO with clip-higher enabled Reversed asymmetric clipping with high-entropy tokens using [0.65, 1.35] and low-entropy tokens using [0.8, 1.2] Analysis: We compared model accuracy and output length. Our results are shown in Figure 18."
        },
        {
            "title": "5 Continuous Differentiation: Heterogeneous Adaptive Policy Optimization",
            "content": "In Section 4, we explored token heterogeneity through binary differentiation, demonstrating the effectiveness of treating high-entropy and low-entropy tokens distinctly. However, this discontinuous approach presents critical limitation: coarse discretization artificially partitions tokens with similar entropy values into disparate categories, subjecting them to fundamentally different optimization strategies. The Dual-Entropy Phenomenon further amplifies this instability. Moreover, even within the same category, tokens exhibit significant entropy variations, reflecting their distinct roles and mechanisms in the reasoning process, and should not be treated uniformly. more sophisticated approach would use entropy as continuous signal to modulate token treatment dynamically. In this section, we formalize the insights from binary differentiation into continuous framework. We extend each component of our heterogeneous treatmenttemperature scheduling, advantage redistribution, and adaptive clippingfrom discrete categories to continuous functions, while maintaining principle of simplicity to ensure computational efficiency and interpretability. 5.1 Fine-Grained Heterogeneity Modeling Continuous Adaptive Temperature Sampling For token-by-token generation during rollout, we dynamically adjust temperature based on current entropy. Since generation proceeds without sequence-level context, we compute entropy normalization on-the-fly. As shown in Section 4.1, the distribution of entropy exhibits significant variance, therefore we apply logarithmic smoothing: (cid:32) Ti,t = Tbase 1 + log(Hi,t) ρinit σinit log(H) log(H) (cid:33) τ (15) where Hi,t = (cid:80) log is the entropy at position i, t. ρinit of entropy and serves as the threshold direction of temperature adjustments. σinit variance, and τ determines the maximum bounds of temperature adjustment. log(H) denotes the estimated ρ-th quantile log(H) is the estimated Continuous Differential Advantage Redistribution For advantage redistribution, we followed [27] to regulate entropy at the batch level. To ensure scores are bounded in [1, 1], we apply asymmetric scaling: hi,t = log(Hi,t) Qρ(log(H)) σ(log(H)) , hi,t = (cid:26)hi,t/hmax hi,t/hmin if hi,t > 0 if hi,t 0 (16) where Qρ denotes the ρ-th quantile, hmax = max(hi,thi,t > 0) and hmin = min(hi,thi,t < 0). Advantages are then redistribution based on both entropy and importance ratios. We directly use ht as both the direction and magnitude for advantage redistribution. ˆAi,t = Ai,t λ(Ai,t, hi,t, ri,t) (17) The redistribution factor adapts continuously based on entropy and ratio: if C(hi,t, ri,t) otherwise λ(Ai,t, hi,t, ri,t) = (cid:26)1 + hi,t 1 with the condition function is defined as: C(hi,t, ri,t) = (cid:26)ri,t / [γL, γU ] ri,t [γL, γU ] if hi,t > 0 if hi,t 0 (high-entropy) (low-entropy) (18) (19) where [γL, γU ] defines the neutral zone. This formulation ensures that high-entropy tokens receive stronger updates when their importance ratios fall outside the neutral zone. Low-entropy tokens follow the same principle but are modulated within the neutral zone. Continuous Asymmetric Adaptive Clipping The clipping boundaries adapt continuously based on entropy. Consistent with Continuous Differential Advantage Redistribution, We reuse ht to determine both the direction and magnitude of clipping adjustments, maintaining unified control mechanism across all components: ϵL(i, t) = (cid:26)ϵbase (1 hi,t) ϵbase if hi,t 0 if hi,t > 0 (low-entropy) (high-entropy) (20) 17 ϵR(i, t) = if hi,t 0 if hi,t > 0 are the base clipping bounds. This allows low-entropy tokens to decrease more (cid:26)ϵbase (1 + hi,t) ϵbase (low-entropy) (high-entropy) (21) where ϵbase aggressively while high-entropy tokens can increase more freely. , ϵbase These continuous formulations unify our heterogeneous treatment strategy, with all modulations smoothly varying as functions of the normalized entropy. This ensures concise implementation that is easy to modify. 5.2 Unified Framework We present the complete Heterogeneous Adaptive Policy Optimization algorithm in Algorithm 1. The overall architecture can be found in Figure 3. Algorithm 1 Heterogeneous Adaptive Policy Optimization Require: Policy πθ, dataset Require: Entropy quantile: ρ Require: Temperature params: Tbase, ρinit Ensure: Updated policy πθ log(H), σinit log(H), τ Clipping params: base bounds ϵbase , ϵbase Sampling Rollout Sequences Continuous Adaptive Temperature Sampling πθ(vsi,t) log πθ(vsi,t) 1 + log(Hi,t)ρlog(H) σlog(H) (cid:17) τ (cid:16) 1: for each token position i, do 2: Compute entropy: Hi,t (cid:80) Adaptive temperature: Ti,t Tbase 3: 4: end for 5: Assign rewards to tokens: ai,t ri, {(i, t)} 6: Compute token-level statistics: µtok 1 7: Normalize advantages: Ai,t (ai,t µtok)/σtok 8: Qρ(log(H)) Quantileρ({log(Hi,t) : (i, t) }) 9: σ(log(H)) 10: for each mini batch do 11: 12: for each token i, in do (cid:113) 1 (cid:80) (cid:80) (i,t)T (log(Hi,t) Qρ(log(H)))2 Compute log-entropies: hi,t = log(Hi,t)Qρ(log(H)) σ(log(H)) Asymmetric scaling: hi,t (cid:26)hi,t/hmax hi,t/hmin if hi,t > 0 if hi,t 0 Token-Level Group Average Advantage (i,t) ai,t, σtok (cid:113) 1 (cid:80) (i,t)(ai,t µtok) Compute global entropy statistics Training with Heterogeneous Treatment Continuous Asymmetric Adaptive Clipping - Compute First if hi,t 0 then ϵL(i, t) ϵbase else ϵL(i, t) ϵbase end if γL 1 ϵL(i,t) 2 (1 hi,t), ϵR(i, t) ϵbase (1 + hi,t) , ϵR(i, t) ϵbase , γU 1 + ϵR(i,t) 2 Neutral Zone based on Dynamic Clipping Continuous Differential Advantage Redistribution Compute importance ratio: ri,t πθ(ai,tsi,t)/πθold(ai,tsi,t) if (hi,t > 0 and ri,t / [γL, γU ]) or (hi,t 0 and ri,t [γL, γU ])) then ˆAi,t Ai,t (1 + hi,t) else ˆAi,t Ai,t end if end for (cid:104) LHAPO(θ) = Update parameters: θ θ + η θLHAPO(θ) t=1 min 1 i=1 oi (cid:80)G i=1 (cid:80)G (cid:80)oi (cid:16) 28: 29: end for 30: ρinit log(H) Qρ(log(Ht)), σinit log(H) σ(log(Ht)) Update entropy statistics for next step ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵL(i, t), 1 + ϵR(i, t)) ˆAi,t (cid:17)(cid:105) Compute HAPO loss 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: Table 1: Comparison between vanilla DAPO using all tokens, DAPO with forking tokens), Archer, EDGE-GRPO, and HAPO, evaluated on the Qwen-Math-1.5B Base, Qwen-Math-7B Base, and Qwen3-8B Base models. Method Avg AIME24(30) AIME25(30) AMC(83) Math(500) OlympiadBench(675) Minerva (272) RLVR from the Qwen2.5-Math-1.5B Base Model Vanilla DAPO 38. DAPO w/ Forking Tokens 38.83 Archer EDGE-GRPO HAPO(ours) 37.06 38. 40.62 21.73 22.51 19.62 22.88 25. 17.11 18.56 17.24 17.86 20.12 72. 74.80 72.17 74.48 75.69 67.33 65. 64.21 66.94 66.83 RLVR from the Qwen2.5-Math-7B Base Model Vanilla DAPO 46. DAPO w/ Forking Tokens 47.43 Archer EDGE-GRPO HAPO(ours) 45.63 45. 50.04 37.24 38.45 35.25 38.85 41. 20.24 21.90 19.19 21.83 24.34 81. 80.55 82.53 83.23 85.47 76.72 75. 72.16 69.78 78.45 RLVR from the Qwen3-8B Base Model Vanilla DAPO 50. DAPO w/ Forking Tokens 50.21 Archer EDGE-GRPO HAPO(ours) 49.17 50. 51.97 35.84 36.22 34.76 35.15 39. 25.24 25.85 23.33 24.12 26.83 78. 80.26 78.47 80.47 81.77 83.54 82. 82.57 83.33 84.23 31.47 32.81 29. 30.67 33.86 34.35 35.52 33.04 32. 36.94 44.87 46.27 42.69 43.35 45. 19.52 18.74 19.14 19.93 21.91 31. 32.79 31.61 29.24 33.73 32.13 29. 33.22 33.91 34.74 5.3 Experiments Experimental Setup We incorporate our token-level strategy into DAPO[30] within the verl[25] framework. Experimental setups leverage the core components of DAPO, such as clip-higher, dynamic sampling, token-level policy gradient loss, and overlong reward shaping. To ensure reproducibility, we maintain DAPOs recommended hyperparameter settings: clip-higher parameters of ϵhigh = 0.28 and ϵlow = 0.2; overlong reward shaping with 10240-token maximum generation length and 4096-token cache. Furthermore, we use training batch size of 512 and mini-batch size of 32. We sample 16 responses for each training prompt. Training proceeds with learning rate of 106 and 10-step warmup period. Crucially, we excludes both KL divergence loss and entropy loss. For HAPOs token-level strategies, we follow [27] and set the entropy quantile ρ to 80%, encouraging exploration to the top 20% highest-entropy tokens. For Adaptive Temperature Sampling, we compute ρlog(H) and σlog(H) based on all tokens entropy in the previous step. We set Tbase = 1.0 and τ = 0.05. For Differential Advantage Redistribution, we determine the neutral zone using the clipping ratios that correspond to each tokens actual dynamics, setting it to [1 ϵL 2 ]. For Asymmetric Adaptive Clipping, we set ϵbase = 0.2 and ϵbase = 0.28, same as DAPO. Importantly, we leverage entropy for regulation throughout, introducing virtually no additional hyperparameters. 2 , 1 + ϵR To evaluate the scaling ability of our methods, we perform RLVR experiments across the Qwen2.5Math-1.5B,Qwen2.5-Math-7B[28] base models and Qwen3-8B[29] base model, using DAPO-Math17K[30] as the training dataset. We trained all models on 4 nodes with 32 Nvidia A100 GPUs equipped with 128-core CPU and 1024GB of memory. Evaluation We evaluate all the models on mathematical reasoning benchmarks: AIME24, AIME25, AMC23, Minerva [12], MATH500 [10], and OlympiadBench [9]. All evaluations are conducted in zero-shot setting. For each question, we generate 8 independent responses under decoding temperature = 0.5, and report the average accuracy. Main Results We compare HAPO against vanilla DAPO, DAPO with Forking Tokens, Archer, and EDGE-GRPO in Table 1. Our method consistently outperforms all baselines across different model scales and benchmarks, demonstrating how carefully designed token-level heterogeneous 19 (a) Qwen2.5-Math-1.5B (b) Qwen2.5-Math-7B (c) Qwen3-8B Figure 19: Overall Response Length (a) Qwen2.5-Math-1.5B (b) Qwen2.5-Math-7B (c) Qwen3-8B Figure 20: Overall Entropy Table 2: Component ablation study of HAPO. We denote Adaptive Temperature Sampling, Tokenlevel Group Average, Differential Advantage Redistribution, and Asymmetric Adaptive Clipping as A, B, C, and respectively for convenience. D Avg (1560) AIME24 (30) AIME25 (30) AMC (83) Math (500) OlympiadBench(675) Minerva (272) 46.97 48.85 48.56 48.28 48.02 48.74 49.42 50.04 37.24 39.77 39.04 38.63 38.38 39.19 40.17 41. 20.24 23.01 23.77 22.07 20.73 22.55 23.62 24.34 81.99 84.20 83.59 84.19 82.45 84.43 84.41 85.47 76.72 76.88 77.19 76.58 78.56 77.72 79.10 78. 34.35 37.34 35.02 36.00 36.49 36.13 35.96 36.94 31.28 31.92 32.76 32.19 31.51 32.44 33.28 33.73 treatment can significantly boost model performance. Specifically, on Qwen2.5-Math-7B, HAPO surpasses DAPO w/ Forking Tokens by 2.86 and 2.44 points on AIME24 and AIME25 respectively. Furthermore, HAPO demonstrates substantial improvements over vanilla DAPO across all model scales, achieving average accuracy gains ranging from 1.97 to 3.07 points. When compared to existing entropy-based approaches, our method exhibits consistent superiority: HAPO outperforms Archer by 2.80-4.41 points and EDGE-GRPO by 1.83-4.15 points across the three evaluated models, highlighting the effectiveness of our fine-grained heterogeneous treatment over coarse-grained entropy-based strategies. We visualize the training dynamics in Figures 1, 2 and Figure 19, 20, comparing HAPO and DAPO throughout the training process. Our method consistently maintains longer response lengths and higher entropy, indicating that HAPO successfully preserves model exploration capabilities while achieving better task performance. Moreover, compared to vanilla DAPO, HAPO introduces negligible computational overhead, as entropy values are already computed within the verl framework. Our method simply leverages these existing computations for adaptive optimization, making it practical and efficient enhancement that can be readily integrated into existing RLHF pipelines. Component-wise contributions We examine each components contribution on Qwen2.5-Math7B in Table 2. All components contribute meaningfully to HAPOs performance, with adaptive temperature proving particularly crucial as it governs token category distribution and entropy. Tokenlevel group averaging significantly impacts advantage redistribution and clipping, corroborating 20 our analysis. Notably, both Asymmetric Clipping and Differential Advantage Redistribution show limited effectiveness when used individually, yet their combination yields substantial performance gains. In fact, all the HAPO components form an organic unity: Adaptive Temperature ensures sufficient high-entropy critical tokens, Differential Advantage Redistribution optimizes them with larger updates, and Asymmetric Clipping encourages exploration. This synergy enables effective leverage of token heterogeneity throughout optimization."
        },
        {
            "title": "6 Related Work",
            "content": "6.1 Reinforcement Learning from Human Feedback The evolution of reinforcement learning in language models represents trajectory from basic preference alignment toward sophisticated reasoning capabilities. Foundational work in constrained optimization emerged through TRPO [20] and PPO [21], establishing principles for stable policy updates. Subsequent algorithmic innovations eliminated computational bottlenecksGRPO [24] and REINFORCE++ [11] removed value network dependencies via group-based advantage computation. The offline optimization paradigm gained prominence through DPO [19], KTO [7], and SimPO [15], which circumvent reward model training entirely. fundamental transformation occurred with the advent of reasoning-centric models. OpenAIs o1 [16] pioneered effective multi-step reasoning through reinforcement learning at scale, catalyzing widespread development. DeepSeek-R1 [5] demonstrated reasoning emergence without supervised fine-tuning, while Claude3.5 [2], Qwen3 [29], and Seed-1.5-Thinking [23] expanded the reasoning frontier. Beyond these state-of-the-art models, various works explored complementary optimization strategies, including SimpleRLZoo [32] and Open-Reasoner-Zero, which investigate alternative training paradigms. Meanwhile, algorithmic refinements continue to enhance existing methodsDAPO [30] introduced token-mean averaging and asymmetric clipping for extended sequences, while VAPO [31] developed variance-aware optimization strategies to improve training stability and convergence. 6.2 Entropy-Based Optimization in LLMs Entropy serves as crucial signal for model uncertainty and optimization guidance. [4] explored how entropy dynamics affect learning in reasoning models. Entropy-based optimization strategies have diversified significantly. [1; 8] showed entropy minimization alone improves performance without labeled data. Step entropy methods [13] identify superfluous reasoning steps by aggregating token-level entropy. Recent studies have leveraged entropy as key metric for identifying and treating critical tokens in reasoning tasks. [27] established the \"80/20 rule,\" demonstrating that 20% of high-entropy \"forking tokens\" fundamentally control reasoning diversity. Building upon token identification, subsequent works have developed entropy-based differential treatment strategies. Archer [26] employs entropy to determine token-specific clipping boundaries, encouraging exploration for high-entropy tokens while maintaining stability for low-entropy ones. Similarly, ETTRL [14] and EDGE-GRPO [26] modulate advantages based on token entropy, amplifying updates for high-entropy tokens and dampening those for low-entropy tokens to achieve an optimal exploration-exploitation balance."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce Heterogeneous Adaptive Policy Optimization (HAPO), which addresses the fundamental limitation of uniform token treatment in existing RLHF methods. Through systematic analysis, we identified that tokens with different entropy characteristics require different optimization strategies. HAPO implements this insight through four componentsAdaptive Temperature Sampling, Token-Level Group Average, Differential Advantage Redistribution, and Asymmetric Adaptive Clippingthat leverage entropy as continuous signal for fine-grained optimization. Experimental results demonstrate consistent improvements over state-of-the-art DAPO baselines across multiple model scales with negligible computational overhead. By establishing that effective RLHF requires recognizing and adapting to token heterogeneity, HAPO opens new directions for developing more capable reasoning models that align optimization with the intrinsic structure of language generation."
        },
        {
            "title": "References",
            "content": "[1] Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning, 2025. [2] Anthropic. Claude 3.5 sonnet, 2024. [3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [4] Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, Zhiyuan Liu, Hao Peng, Lei Bai, Wanli Ouyang, Yu Cheng, Bowen Zhou, and Ning Ding. The entropy mechanism of reinforcement learning for reasoning language models, 2025. [5] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [6] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu, Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, Y. K. Li, Y. Q. Wang, Y. X. Wei, Y. X. Zhu, Yang Zhang, Yanhong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu, Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan Xiong, Yunxian Ma, Yuting 22 Yan, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhigang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu, Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi Gao, and Zizheng Pan. Deepseek-v3 technical report, 2025. [7] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization, 2024. [8] Zitian Gao, Lynx Chen, Joey Zhou, and Bryan Dai. One-shot entropy minimization, 2025. [9] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. [10] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. [11] Jian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with robustness to both prompt and reward models, 2025. [12] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022. [13] Zeju Li, Jianyuan Zhong, Ziyang Zheng, Xiangyu Wen, Zhijian Xu, Yingying Cheng, Fan Zhang, and Qiang Xu. Compressing chain-of-thought in llms via step entropy, 2025. [14] Jia Liu, ChangYi He, YingQiao Lin, MingMin Yang, FeiYang Shen, and ShaoGuo Liu. Ettrl: Balancing exploration and exploitation in llm test-time reinforcement learning via entropy mechanism, 2025. [15] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward, 2024. [16] OpenAI. Learning to reason with llms, 2024. [17] OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen OKeefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul 23 Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report, 2024. [18] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. [19] Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model, 2024. [20] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization, 2017. [21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. [22] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation, 2018. [23] ByteDance Seed, :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, and Lei Zuo. Seed1.5-thinking: Advancing superb reasoning models with reinforcement learning, 2025. 24 [24] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. [25] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, page 12791297. ACM, 2025. [26] Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, and Guorui Zhou. Stabilizing knowledge, promoting reasoning: Dual-token constraints for rlvr, 2025. [27] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning, 2025. [28] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. [29] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. [30] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale, 2025. [31] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, Xiangpeng Wei, Xiangyu Yu, Gaohong Liu, Juncai Liu, Lingjun Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Ru Zhang, Xin Liu, Mingxuan Wang, Yonghui Wu, and Lin Yan. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks, 2025. [32] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. [33] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Edge-grpo: Entropy-driven grpo with guided error correction for advantage diversity, 2025."
        },
        {
            "title": "Appendix",
            "content": "(a) AIME2024 (b) Response Length (c) Max Advantage Figure 21: Comparison Between Pre-Norm and Post-Norm Advantage Redistribution"
        },
        {
            "title": "A Training Configuration for Analysis",
            "content": "For all the experiments in Section 3 and Section 4, we leverage the core components of DAPO, such as clip-higher, dynamic sampling, token-level policy gradient loss, and overlong reward shaping. To ensure reproducibility, we maintain DAPOs recommended hyperparameter settings: clip-higher parameters of ϵhigh = 0.28 and ϵlow = 0.2; overlong reward shaping with 8192-token maximum generation length and 4096-token cache. Furthermore, we use training batch size of 512 and mini-batch size of 32. We sample 8 responses for each training prompt. Training proceeds with learning rate of 106 and 10-step warmup period. We excludes both KL divergence loss and entropy loss. For each experiment, we conduct training 3 times to reduce variance and ensure the reliability of our results. We trained all models during analysis on 8 Nvidia A100 GPUs equipped with 128-core CPU and 1024GB of memory. Example of Dual-Entropy Phenomenon We present concrete examples of the dual-entropy phenomenon in Figure 22, 23, 24, 25, 26, 27. We use blue and red to represent the low-entropy and high-entropy parts of dual-entropy, respectively. These visualizations demonstrate how tokens with identical semantic meanings but different surface forms exhibit drastically different entropy values during model generation."
        },
        {
            "title": "C Discussion about the Order of Differential Advantage Redistribution",
            "content": "As we introduced Token-Level Group Average, an alternative implementation of Differential Advantage Redistribution involves applying differential scaling to rewards before performing group averaging across all tokens. We describe this alternative approach in Discussion 1. We categorize Differential Advantage Redistribution into two variants based on the operation order: pre-norm and post-norm (ours), which apply differential scaling before and after group averaging, respectively. The pre-norm approach enables more fine-grained token-level optimization by preserving individual token reward signals before normalization. We conducted comparative experiments in Experiment 7. However, we discovered that while prenorm provides more fine-grained token representations, applying differential scaling before averaging introduces variance into the optimization process. As shown in Figure 21, the pre-norm approach exhibits advantage variance that is substantial higher than post-norm across training iterations. The instability arises because differential scaling before normalization amplifies the natural variation in rewards, preventing the group average from effectively stabilizing the optimization signal. Therefore, we adopt the post-norm approach. 26 Discussion 1: Order of Differential Advantage Redistribution Pre-normalization Modulation: ai,t = ai,t α(Hi,t), ˆAi,t = ai,t µtok σtok Post-normalization Modulation: Ai,t = ai,t µtok σtok , ˆAi,t = ˆAi,t β(Hi,t) where: ai,t = ri {0, 1} is the original token-level reward α(Hi,t) and β(Hi,t) are entropy-based scaling functions: (22) (23) α(Hi,t) = β(Hi,t) = (cid:26)λhigh λlow if Hi,t top 20% of batch if Hi,t bottom 80% of batch (high-entropy tokens) (low-entropy tokens) (24) λhigh > 1.0 amplifies advantages for exploration. λlow < 1.0 reduces advantages for stability. Experiment 7: Pre-norm and Post-norm comparison We evaluate the effectiveness of our entropy-based advantage modulation strategie using Qwen2.5-Math-7B on mathematical reasoning tasks: Training Configuration: We test both pre-normalization and post-normalization modulation with λhigh = 1.25, λlow = 0.75 empirically. Baselines: We compare against DAPO with token-level group advantages Analysis: We compare model accuracy, output length, and max advantage. Our results are shown in Figure 21. 27 Figure 22: Dual-Entropy Example 1-part1 28 Figure 23: Dual-Entropy Example 1-part2 Figure 24: Dual-Entropy Example 1-part3 30 Figure 25: Dual-Entropy Example 2-part1 31 Figure 26: Dual-Entropy Example 2-part2 Figure 27: Dual-Entropy Example 2-part"
        }
    ],
    "affiliations": [
        "Beihang University",
        "Peking University",
        "Shanghai AI Laboratory"
    ]
}