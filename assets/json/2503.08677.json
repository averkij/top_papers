{
    "paper_title": "OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting",
    "authors": [
        "Yongsheng Yu",
        "Ziyun Zeng",
        "Haitian Zheng",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, a unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging a pre-trained diffusion prior along with a progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers a robust, reference-free evaluation of context consistency and object hallucination, establishing a new benchmark for high-fidelity image editing. Project page: https://yeates.github.io/OmniPaint-Page/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 1 ] . [ 2 7 7 6 8 0 . 3 0 5 2 : r OmniPaint: Mastering Object-Oriented Editing via Disentangled Insertion-Removal Inpainting Yongsheng Yu1 *, Ziyun Zeng1 *, Haitian Zheng2, Jiebo Luo1 1University of Rochester, 2Adobe Research {yyu90,zzeng24}@ur.rochester.edu, hazheng@adobe.com, jluo@cs.rochester.edu Illustration of OmniPaint for object-oriented editing, including realistic object removal (left) and generative object insertion Figure 1. (right). Masked regions are shown as semi-transparent overlays. In removal cases, the marks the target object and its physical effects, such as reflections, with the right column showing the results. In insertion cases, the reference object (inset) is placed into the scene, indicated by green arrow. Note that for model input, masked regions are fully removed rather than semi-transparent."
        },
        {
            "title": "Abstract",
            "content": "https://yeates.github.io/OmniPaint-Page/. Diffusion-based generative models have revolutionized object-oriented image editing, yet their deployment in realistic object removal and insertion remains hampered by challenges such as the intricate interplay of physical effects and insufficient paired training data. In this work, we introduce OmniPaint, unified framework that re-conceptualizes object removal and insertion as interdependent processes rather than isolated tasks. Leveraging pre-trained diffusion prior along with progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object insertion while faithfully preserving scene geometry and intrinsic properties. Furthermore, our novel CFD metric offers robust, reference-free evaluation of context consistency and object hallucination, establishing new benchmark for high-fidelity image editing. Project page: 1. Introduction Object-oriented image editing has evolved from simple pixel-level adjustments to complex scene manipulation tasks, including object removal [7, 43, 50, 58] and insertion [5, 38, 47]. Classic approaches for object removal/insertion in images have followed two distinct technical routes without intersection, such as object harmonization [2, 46] and image completion [22, 28]. Recent advances in large diffusion-based generative models [19, 28] have broadened the horizons of object-oriented editing, enabling not only high-fidelity inpainting of masked regions [6, 37, 58] but also creative synthesis of new objects seamlessly integrated into existing images [5, 33, 34, 38]. These models further allow manual manipulation of object attributes and appearances through text prompts or reference images, demonstrating unique industrial value for vi1 sual content modification and creation. Despite the transformative potential of diffusion-based models, their application to general object editing presents unique challenges. The first challenge lies in the path dependency on large-scale paired real-world datasets [43, 44] or synthetic datasets [14, 20, 21]. For specific tasks like object insertion, achieving correct geometric alignment and realistic integration requires not only high-quality synthesis but also deep understanding of complex physical effects like shadows, reflections, and occlusions. Insufficient paired training samples may lead to models lacking identity consistency or failing to integrate objects with realistic physical effects [43]. The second challenge involves ensuring reliable object removal that not only eliminates unwanted foreground elements but also maintains background continuity and prevents unintended introduction of artifacts or hallucinated objects [7] - particularly problematic given the lack of robust evaluation metrics for flagging ghost elements generated by large models random hallucinations. These limitations have led to separate modeling of object removal and insertion, whether text-driven [10, 38, 50, 54] or mask-guided [7, 33, 34, 43, 58]. However, deploying large generative models simultaneously across different editing subtasks (e.g., removal and insertion) that currently employ completely different technical implementations risks potential conflicts and increased costs. To address these challenges, we propose OmniPaint - framework that reconceptualizes object removal and insertion as interdependent tasks rather than isolated subtasks. Leveraging pre-trained diffusion priors (employing FLUX [19] in this work), we optimize LoRA [12] parameters through collected small-scale real-world paired samples while enabling easy task switching via learnable text embeddings. For realistic object removal, our model achieves semantic elimination of masked foreground elements while removing their physical effects. For object insertion, we go beyond simple blending to achieve harmonious synthesis respecting scene geometry and reference identity through our proposed CycleFlow mechanism. By incorporating well-trained removal parameters into insertion training, we enable the utilization of large-scale unpaired samples, significantly reducing dependence on massive real paired datasets. key innovation is our Context-Aware Feature Derivation (CFD) score, specialized no-reference metric for object removal. As illustrated in Fig. 2, it evaluates object hallucinations and context coherence, setting new standard for realistic object-oriented editing. Experiments demonstrate OmniPaints significant improvements in both interdependent editing tasks: the model better handles complex physical effects like shadows and reflections during removal while achieving seamless background reconstruction, and generates more natural geometric alignment and Figure 2. Visualization of CFD metric assessment for object removal. The segmentation results are obtained using SAM [17] with refinement, with purple masks for background, orange masks for segments fully within the original mask, and unmasked for those extending beyond the original mask. Note that the orange masked regions correspond to hallucinated objects. higher ReMOVE [4] score is better, while lower CFD score is preferable. In these cases, ReMOVE scores are too similar to indicate removal success, while CFD score offers clearer distinction. illumination consistency during insertion. Ablation studies reveal that omitting CycleFlow prevents full utilization of unpaired data, leading to deficiencies in identity consistency and physical effect generation. In summary, We propose diffusion-based solution for object removal/insertion with physical and geometric consistency in physical effects including shadows and reflections. We introduce progressive training pipeline, where the proposed CycleFlow technique enables unpaired posttraining, minimizing reliance on paired data. We further develop novel no-reference metric called CFD for object removal quality through hallucination detection and context coherence assessment. 2. Related Works 2.1. Image Inpainting The task of image inpainting, specifically filling in missing pixels within masked regions, has been extensively studied in the literature. End-to-end learning approaches [22, 35, 52], which aim for pixel-wise fidelity, produce blurry or repetitive patterns when dealing with large masks or complex backgrounds. Methods leveraging pre-trained generative models, such as GANs [51, 53, 56] and diffusion models [1, 41], as priors to generate realistic content for missing regions. Recently, based on text-to-image models [28] have enabled controllable inpainting [37, 43, 58], allowing guided synthesis within masked regions. Key Differences from Conventional Inpainting. Our approach departs from traditional inpainting paradigms in two 2 Figure 3. Illustration of the proposed CFD metric for evaluating object removal quality. Left: We apply SAM to segment the inpainted image into object masks and classify them into nested (ΩMn ) and overlapping (ΩMo ) masks. Middle: The context coherence term measures the feature deviation between the inpainted region (ΩM) and its surrounding background (ΩBM) in the DINOv2 feature space. Right: The hallucination penalty is computed by comparing deep features of detected nested objects (ΩMn ) with their adjacent overlapping masks (ΩMo ) to assess whether unwanted object-like structures have emerged. fundamental ways: Standard inpainting reconstructs masked images to match the original, while we explicitly model object removal and insertion as distinct yet interdependent processes. Traditional inpainting fills hole, while our approach may adjust surrounding content for seamless integration. 2.2. Realistic Object Removal Realistic object removal aims to eliminate foreground semantics while ensuring seamless background blending and preventing object hallucination. Existing methods fall into two categories: text-driven and mask-guided. Text-driven approaches [10, 50, 54] specify objects for removal via instructions but are constrained by text embedding performance [25, 48], particularly in handling multiple objects and attribute understanding. Mask-guided methods [6, 7, 43, 58] provide more precise control. Recent advances, such as MagicEraser [20], generate removal data by shifting objects within an image, while SmartEraser [14] synthesizes million-sample dataset using alpha blending. RORem [21] leverages synthetic datasets like MULAN [40] for training, though synthetic data limit models ability to replicate realistic object effects. We introduce the CFD score, reference-free metric designed exclusively for object removal, to evaluate object hallucination and context coherence. This enables more effective assessment of object removal techniques. 2.3. Generative Object Insertion Object insertion aims to seamlessly integrate new objects into existing scenes. Early methods focused on harmonization and blending [2, 39, 46] but struggled with complex physical effects like shadows and reflections. Recent approaches leverage real-world datasets [43], synthetic blender-generated data [42], or test-time tuning [6] to improve objectbackground interactions, yet they remain limited in geometric alignment modeling. Diffusion models offer promising alternative, often using object-specific embeddings from CLIP [33, 47] or DINOv2 [5, 34] to preserve identity, attributes, and texture. Unlike these existing works, our approach builds on FLUX without additional feature extractors. Concurrent methods [32, 44] tackle generative object insertion along the direction of text-driven subject generation. ObjectMate [44] constructs millions of paired samples covering multiple reference subjects. In contrast, we focus on single-subject image-driven insertion, ensuring subject alignment and effect integration using only 3K real-world paired samples as training data, followed by CycleFlow unpaired post-training. This approach significantly alleviates the requirement for large paired datasets. 3. Preliminaries Flow Matching (FM) [23] is generative modeling framework that learns velocity field ut(zt) to map source distribution p0 to target distribution p1 through timedependent flow. The goal of FM is to train neural network θ to make prediction uθ (zt) to approximate the velocity field ut(zt). This is achieved by minimizing the Flow Matching Loss, defined as: LFM(θ) = Et,ztpt (cid:2)uθ (zt) ut(zt)2(cid:3) , (1) where zt pt and U[0, 1]. Directly optimizing LFM(θ) is intractable due to the complexity of estimating the ground truth velocity field ut(zt) for arbitrary pt. To simplify optimization, the Conditional Flow Matching (CFM) framework introduces conditional distributions pt1(zz1) = (cid:0)ztz1, (1 t)2I(cid:1), which focuses on paths conditioned on target samples z1 = Z1. The velocity field under this conditional setting is analytically given by: ut(zz1) = z1 1 . (2) 3 The conditional probability path pt1(zz1) follows linear interpolation: zt = tz1 + (1 t)z zt = Zt pt1(z1) (3) where z0 = Z0 p0 and z1 = Z1 p1. Using this formulation, the Conditional Flow Matching Loss is defined as: LCFM(θ) = Et,z0p0,z1p1 (zt) ut(ztz1)2(cid:3) . (4) This loss avoids the need to estimate ut(zt) directly by (cid:2)uθ leveraging the known form of ut(zz1). 4. Methodology We frame image inpainting as dual-path, object-oriented process that consists of two key directions: object removal and object insertion. Given an image RHW 3 and binary mask {0, 1}HW denoting the edited region (where Mij = 1 indicates masked pixels), our model operates on the masked input = (1 M) to facilitate targeted modifications. The object removal pathway suppresses semantic traces within M, ensuring smooth boundary transitions while preventing unintended artifacts or hallucinations. Meanwhile, the object insertion pathway integrates new object RH 3 (H < H, < ), maintaining global coherence and context-aware realism. 4.1. The OmniPaint Framework OmniPaint builds upon FLUX-1.dev [19], diffusion-based architecture featuring Multi-Modal Diffusion Transformer (MM-DiT) [8] backbone. While preserving FLUXs strong text-to-image priors, we introduce the image-conditioning mechanisms used in [36] tailored for object-aware editing. Masked Image Conditioning. The model refines Gaussian noise z0 = Z0 p0 towards z1, using the masked image as denoising guide for object removal and insertion. We leverage the existing FLUX networks, including its VAE encoder and 2 2 patchify layer, to map into shared feature space, yielding the conditioned token sequence zX . Reference Object Conditioning. For object insertion, the model conditions on both the masked image and reference object image O. To preserve object identity while minimizing background interference, we preprocess with Carvekit [31] for background removal before resizing it to match Xs spatial dimensions. The reference object undergoes the same latent encoding and patchification as the masked image, producing corresponding latent sequence zO . The final condition token is obtained by concatenating both sequences along the token dimension: zc = [zX ; zO ]. Prompt-Free Adaptive Control. Given the highly imageconditioned nature of our task, textual prompts may introduce ambiguity. To mitigate this, we adopt promptfree adaptive control mechanism, replacing text embeddings with learnable task-specific parameters. Specifically, 4 we introduce two trainable vectors: τremoval, τinsertion (0, I), (5) initialized from the embedding of an empty string and optimized separately for each task. Inference switches between removal and insertion via embedding selection. To facilitate computational efficiency, we freeze the FLUX backbone and perform Parameter-Efficient FineTuning (PEFT), optimizing two LoRA [12] parameter sets, θ and ϕ, for object removal and insertion, respectively. 4.2. Data Collection and Mask Augmentation We collect dataset of 3,300 real-world paired samples captured across diverse indoor and outdoor environments, encompassing various physical effects such as shadows, specular reflections, optical distortions, and occlusions (see Appendix for examples). Each triplet I, Iremoved, is meticulously annotated to ensure high quality. To enhance model robustness against diverse mask variations, we apply distinct augmentation strategies for object removal and insertion. For removal, we introduce segmentation noise via morphological transformations, randomly applying dilation or erosion with configurable parameters. Imprecise masks are simulated by perturbing boundaries and adding or removing geometric shapes (e.g., circles, rectangles). Augmented examples and the effectiveness analysis are provided in the Appendix. For object insertion, since explicit object detection is not required, we simplify mask augmentation by expanding segmentation masks to their bounding boxes or convex hulls, ensuring adaptability to various reference object formats. Reference object image augmentation follows prior work [34]. 4.3. Training Pipeline Fortunately, object In our experiments, we observe that the current training data are insufficient to maintain reference identity for object insertion, as in Fig. 7(b) and Table in the Appendix. Bootstrapping paired data via trained models, akin to ObjectDrop [43], is straightforward solution but requires reliable filtering mechanism, which remains an open challenge. insertion and object removal are mathematically complementary inverse problems (i.e., each can be viewed as inverting the other). Inspired by cycleconsistency approaches [45, 57], we propose utilizing unpaired data rather than relying on paired augmentations. In particular, we utilize large-scale object segmentation datasets, which lack explicit removal pairs, to enhance object insertion. This section presents our three-phase training pipeline: (1) inpainting pretext training, (2) paired warmup, and (3) CycleFlow unpaired post-training. 4.3.1. Inpainting Pretext Training To endow our model with basic inpainting abilities, we first fine-tune it on pretext inpainting task, initializing θ and ϕ for later stages. Using mask generator [35], we apply random masks to LAION dataset [30] and train the model to reconstruct missing regions by minimizing CFM loss, Lpretext(θ, ϕzt, zX Et,z0,z1 ) = (cid:104) uθ,ϕ (zt, zX ) ut(ztz1)2(cid:105) (6) , where z1 = Z1 p1(I), which enforces the model to complete the masked region so that the entire image approximates I. We show in the appendix that pretext training benefits object editing performance. 4.3.2. Paired Warmup Next, we leverage our 3,000 paired samples for real-world object insertion and removal training. In the Paired Warmup stage, θ and ϕ are trained separately, enabling effect-aware object removal (e.g., removing reflections and shadows) and insertion with effect integration. For insertion, z1 is drawn from Z1 p1(I), where means images retaining the foreground object. We optimize the following objective by modifying Equation 4: Lwarmup(θzt, zc, τ ) = Et,z0,z (cid:2)uθ (zt, zc, τ ) ut(ztz1)2(cid:3) , (7) ; zO where zc = [zX ] represents the conditioning token sequence, concatenating masked image and object identity features, and τ denotes the corresponding task-specific embedding. For removal, z1 is sampled from Z1 p1(Iremoved), where Iremoved means images with the foreground object physically removed. Given conditioning on zX , the optimization objective becomes: Lwarmup(ϕzt, zX , τ ) = (cid:104) uϕ Et,z0,z1 (zt, zX , τ ) ut(ztz1)2(cid:105) (8) . In practice, we assume linear interpolation path for computational efficiency [24], setting ut(ztz1) = (z1z0) in both objectives. This warmup stage enhances object removal, effectively handling reflections and shadows  (Fig.6)  . However, with only 3,000 paired samples, it struggles to maintain reference identity in object insertion (Fig.7(b)). 4.3.3. CycleFlow Unpaired Post-Training To enhance training for object insertion, we leverage large-scale object segmentation datasets, including COCOStuff [3] and HQSeg [16], as unpaired data sources. These datasets provide foreground object masks, enabling us to easily construct the models conditioning inputs and O. We continue tuning θ using the same objective as in Equation 7 on this larger dataset, improving identity preservation, as shown in Fig. 7(b). The case where γ = 0 corresponds to training solely with Equation 7. However, these Figure 4. Illustration of CycleFlow. The mapping removes the object, predicting an estimated target 1, while reinserts the object, generating estimated target z1. Cycle consistency is enforced by ensuring reconstructs the original latent z1 from the effect removal output. Dashed arrows indicate the cycle loss supervision. segmentation datasets lack annotations for object effects, such as shadows and reflections, meaning that the masked image input still retains these effects. This suppresses the models ability to synthesize realistic object effects, making insertions appear more like copy-paste operations of the reference object, as observed in the γ = 0 case of Fig. 7(b). To overcome this limitation, we use our well-trained removal parameters ϕ, which even at NFE = 1 remove object effects (See Fig. 7(a)). Leveraging ϕ as preprocessing step enables insertion training on latents with effects removed. Thus, we introduce the CycleFlow mechanism, comprising two mappings: (removal direction) and (insertion direction). These mappings predict the velocity field at zt, estimating their target samples z1 = Z1, Z1 p1: :z :z1 1 zt uϕ uθ (zt, zX (z , τremoval) t, t, zc, τinsertion) t, (9) (10) where 1 and z1 denote the estimated target samples for removal and insertion, respectively. Here, we also rely on the ut(ztz1) = (z1 z0) linear interpolation setting [24]. As illustrated in Fig. 4, we design Remove-Insert cycle, ensuring that reinserting removed object approximately restores its original latent representation. z1 zt (zt) G(z t) z1. (11) To enforce this cycle consistency, we define Cycle Loss: Lcycle(θ) = Et, zt (cid:104)(cid:13) (cid:13) Gθ (cid:0)F (zt)(cid:1) z1 2(cid:105) , (cid:13) (cid:13) (12) where denotes gradient truncation operator, treating its output as constant during backprop to fix parameters ϕ. During CycleFlow post-training, we optimize an overall loss: Lwarmup(θzt, zc, τinsertion) + γLcycle on unpaired training data, where γ controls the strength of cycle consistency (analyzed in Sec. 5.5). 5 Empirically, this work focuses solely on CycleFlow for object insertion, as warmup-alone suffices for removal. 4.4. Context-Aware Feature Deviation (CFD) Score We introduce the Context-Aware Feature Deviation (CFD) score to quantitatively assess object removal performance. As illustrated in Fig. 3, CFD comprises two components: hallucination penalty that detects and penalizes unwanted object-like structures emerging in the removed region, and context coherence term that evaluates how well the inpainted region blends with the surrounding background. Hallucination Penalty. Given an object mask M, let ΩM = {(i, j) Mij = 1} denote the pixels of removed region. Define = bbox(M) as its bounding box. After removal, we aim to identify whether the synthesized content introduces spurious object-like structures. We apply the off-the-shelf SAM-ViT-H [17] model to k=1. Focusing on segment the image into masks {Mk}K masks near M, we categorize them as: Nested masks, Mn = {Mn ΩMn contained within the removed region. Overlapping masks, Mo = {Mo ΩMo ΩM = ΩM}, partially overlapping ΩM but extend- , ΩMo ing beyond. naive hallucination penalty would simply count nested masks, but some may arise from segmentation noise. Instead, we leverage deep feature similarity to assess whether mask plausibly integrates into its context. To refine segmentation, we merge overlapping masks adjacent to any Mn Mn: ΩM}, entirely Mpaired = (cid:110) (Mn , Mi) Mi = (cid:91) (cid:111) , Mo adj(Mo ,Mn ) (13) Mo denotes overlapping mask, and ) = 1 if the masks share boundary pixel where Mo adj(Mo or their one-pixel dilation overlaps. , Mn The hallucination penalty is then defined as: dhallucination = (cid:88) (cid:16) ωi 1f (ΩMn )f (ΩMi (cid:17) ) , (Mn ,Mi)Mpaired (cid:80) (14) ΩMn where ωi = Mn ΩMn weights the contribution of each nested mask. Feature embeddings (Ω) are extracted from the pre-trained vision model DINOv2 [26]. Context Coherence. Even when dhallucination = 0 (i.e., no nested objects are detected), the inpainted content may still not align with the surrounding background. To quantify this structural consistency, we compute feature deviation: dcontext = 1 (ΩM)f (ΩBM), (15) where denotes the bounding box excluding the masked region. 6 Figure 5. Qualitative comparison on object insertion. Given masked images and reference object images (top row), we compare results from AnyDoor [5], IMPRINT [34], and OmniPaint. Final CFD Metric. The final CFD score is computed as: CFD = dcontext + dhallucination. (16) lower CFD signifies better removal qualityminimal hallucination and seamless contextual blending. 5. Experiments 5.1. CFD Analysis We perform qualitative analyses to determine whether our CFD score effectively captures both contextual coherence and hallucination artifacts, thereby offering more reliable evaluation of object removal quality compared to existing metrics such as ReMOVE [4]. As illustrated in Fig. 2, FLUX-Inpainting [37] generates conspicuous hallucinationsphantom objects like ships, human figures, or floating canistersyet still attains high ReMOVE scores. In contrast, CFD effectively penalizes these hallucinations by using SAM to segment the inpainted region and by examining feature-level discrepancies within nested and overlapping masks. Similarly, while LaMa [35] interpolates background textures in the masked area, its limited generative prior often leads to ghostly artifacts due to insufficient object effect detection. Conversely, our OmniPaint demonstrates superior removal fidelity by completely eliminating Figure 6. Qualitative comparison of object removal in challenging scenarios. Top: Simultaneous removal of objects and glass reflections. Middle: Shadow-free removal under real-world lighting. Bottom: Occlusion-robust inpainting, reconstructing background objects without distortion. The compared methods include FreeCompose [6], PowerPaint [58], CLIPAway [7], and FLUX-Inpainting [37]. Table 1. Quantitative results on our 300-sample removal test set. Table 2. Quantitative results on the 1000-sample RORD test set. Method FID CMMD CFD ReMOVE PSNR SSIM LPIPS Method FID CMMD CFD ReMOVE PSNR SSIM LPIPS LaMa MAT SD-Inpainting FLUX-Inpainting CLIPAway PowerPaint FreeCompose OmniPaint (Ours) 105.10 147.37 153.13 132.60 115.72 103.61 88.77 51. 0.3729 0.6646 0.3997 0.3257 0.2919 0.2182 0.1790 0.0473 0.3531 0.5104 0.4874 0.4609 0.5242 0.4031 0.3743 0.2619 0.7311 0.6162 0.6234 0.6765 0.7396 0.8013 0.8654 0.8610 20.8632 18.2229 18.8760 20.8560 19.5259 19.4559 21.2729 23.0797 0.8278 0.7845 0.6932 0.8002 0.7085 0.7102 0.7320 0.8135 0.1353 0.1900 0.1830 0.1451 0.1641 0.1428 0.1182 0. LaMa MAT SD-Inpainting FLUX-Inpainting CLIPAway PowerPaint FreeCompose OmniPaint (Ours) 49.20 86.33 75.31 62.24 49.07 42.65 46.37 19.17 0.4897 0.8689 0.4733 0.3805 0.4569 0.4599 0.5125 0.2239 0.4660 0.7723 0.6648 0.6077 0.5442 0.4128 0.5215 0.3682 0.8321 0.7070 0.8227 0.8461 0.8696 0.8933 0.9008 0.9053 19.2941 20.3080 19.8308 21.9159 20.3077 20.1832 20.5678 23. 0.5571 0.7815 0.6233 0.7769 0.6055 0.6066 0.6152 0.7867 0.1075 0.1429 0.1235 0.0975 0.1132 0.0968 0.1090 0.0424 target objects without introducing unwanted artifacts, as reflected by its significantly lower CFD scores. By concurrently quantifying both the emergence of unwanted objects and contextual alignment, CFD aligns closely with human visual perception. These findings substantiate CFD as robust evaluation metric that helps ensure that object removal not only achieves seamless blending but also minimizes erroneous content hallucination. 5.2. Experimental Settings For removal, we compare against end-to-end inpainting models MAT [22] and LaMa [35], the diffusion-based SDInpaint [28], and FLUX-Inpainting [37] to ensure fair backbone comparison. Additionally, we include recent open-source object removal methods CLIPAway [7], PowerPaint [58], and FreeCompose [6]. Experiments are conducted on two benchmarks: test set of 300 real-world object removal cases we captured, resized to 5122 for testing, and the RORD [29] dataset with 1,000 paired samples at their original 540 960 resolution, both providing ground truth from physically removed objects. We report PSNR, SSIM, perceptual similarity metrics (FID [11], CMMD [13], LPIPS [55]), and object removal-specific metrics, including ReMOVE [4] and our CFD score. For object insertion, we compare against Paint-byExample (PbE) [47], ObjectStitch [33], FreeCompose [6], AnyDoor [5], and IMPRINT [34]. Since ObjectStitch and IMPRINT do not have public implementations, we obtain official code, checkpoints, and test sets from the authors. Our insertion benchmark consists of 565 samples with 5122 resolutions, combining the IMPRINT test set with realworld cases we captured. Each sample includes background image, reference object image, and binary mask. Reference images are preprocessed using CarveKit [31] for background removal. To evaluate identity consistency, we measure feature similarity between the inserted object and its reference counterpart using CUTE [18], CLIP-I [27], DINOv2 [26], and DreamSim [9], with the latter being more aligned with human perception. Beyond local identity preservation, we assess overall image quality using nonreference metrics: MUSIQ [15] and MANIQA [49]. For fairness, we apply the same image-mask pairs across all baselines and use official implementations with their default hyperparameters, such as inference step counts. For OmniPaint, we employ the Euler Discrete Scheduler [8] during inference and set the number of inference steps to 28 for primary quantitative and qualitative experiments. Additional implementation details are provided in the Appendix. 5.3. Evaluation of Object Removal Performance We evaluate OmniPaint on realistic object removal, comparing against inpainting and object removal methods. As shown in Table 1 and Table 2, OmniPaint consistently out7 Table 3. Quantitative comparison of object insertion methods. Object Identity Preservation Overall Image Quality CLIP-I DINOv2 CUTE DreamSim MUSIQ MANIQA PbE ObjectStitch FreeCompose Anydoor IMPRINT OmniPaint 84.1265 86.4506 88.1679 89.2610 90.6258 92.2693 50.0008 59.6560 76.0085 76.9560 76.8940 84.3738 65.1053 74.0478 82.8641 85.2566 86.1511 90.2936 0.3806 0.3245 0.2134 0.2208 0.1854 0.1557 70.26 68.87 66.67 69.28 68.72 70.59 0.5088 0.4755 0.4775 0.4593 0.4711 0. performs prior approaches across all datasets, achieving the lowest FID [11], CMMD [13], LPIPS [55], and CFD while maintaining high PSNR, SSIM, and ReMOVE [4] scores. These results highlight its ability to remove objects while preserving structural and perceptual fidelity, effectively suppressing object hallucination. Fig. 6 provides visual comparison in challenging realworld cases. In the first row, OmniPaint successfully removes both objects and their glass reflections, failure by all baselines. The second row highlights OmniPaints ability to eliminate shadows under natural lighting, where other methods leave residual artifacts. The third row demonstrates robust inpainting in occlusion scenarios, ensuring seamless background reconstruction without distortion. By effectively handling reflections, shadows, and occlusions, OmniPaint surpasses prior methods in generating coherent and realistic object removal results. 5.4. Evaluation of Object Insertion Performance We evaluate OmniPaint on object insertion, comparing it with advanced methods. As shown in Table 3, OmniPaint achieves the highest scores across all object identity preservation metrics, including CLIP-I [27], DINOv2 [26], CUTE [18], and DreamSim [9], demonstrating superior alignment with the reference object. Additionally, it outperforms all baselines in overall image quality, as measured by MUSIQ [15] and MANIQA [49], indicating better perceptual realism and seamless integration. Fig. 5 presents visual comparisons. Given masked input and reference object, OmniPaint generates inserted objects with more accurate shape, texture, and lighting consistency. In contrast, other methods struggle with identity distortion, incorrect shading, or noticeable blending artifacts. Notably, OmniPaint preserves fine details while ensuring the inserted object naturally aligns with scene geometry and illumination. By maintaining high-fidelity identity preservation and improving perceptual quality, OmniPaint sets new standard for realistic object insertion. 5.5. Hyperparameter Analysis Cycle Loss Weight. We analyze the impact of the cycle loss weight γ on object insertion by comparing results across different values in Fig. 7(b). Lower γ values (e.g., γ = 0) result in weak physical effect synthesis, as the unpaired training data (COCO-Stuff [3] and HQSeg [16]) lack object effects segmentation such as shadows and reflections. Figure 7. Impact of inference steps and cycle loss weights. (a) Removal (top) and insertion (bottom) results across different neural function evaluations (NFE). (b) Insertion results with varying cycle loss weights γ, with OmniPaint defaulting to γ = 1.5. This limits the models ability to learn effect generation, as insertion training relies on input images that already contain these effects. Increasing γ enhances effect synthesis. At γ = 1.5, OmniPaint achieves the optimal balance, effectively learning from unpaired data while preserving realistic effect synthesis. However, further increasing γ to 3.0 overrelaxes effect generation, leading to unnatural artifacts like exaggerated shadows. Neural Function Evaluation. We analyze the impact of neural function evaluations (NFE) on object removal and insertion, as illustrated in Fig. 7(a). Lower NFE values, such as 1 or 4, lead to noticeable blurring, especially within masked regions. Interestingly, for removal tasks, even NFE=1 effectively eliminates the object and its associated effects. At NFE=18, objects are removed cleanly without residual artifacts, while inserted objects exhibit high fidelity with realistic shading and reflections. Further increasing NFE to 28 yields only marginal gains, indicating diminishing returns. Nonetheless, we set NFE=28 as the default to ensure optimal visual quality. 6. Conclusion We present OmniPaint for object-oriented image editing that reconceptualizes object removal and insertion as interdependent tasks. By leveraging pre-trained diffusion prior and progressive training pipeline comprising initial paired sample optimization and subsequent large-scale unpaired refinement via CycleFlow, OmniPaint achieves precise foreground elimination and seamless object integration while preserving scene geometry and other intrinsic properties. Extensive experiments demonstrate that OmniPaint effectively suppresses object hallucination and mitigates artifacts, with the novel CFD metric providing robust, reference-free assessment of contextual consistency."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. TOG, 2023. 2 [2] Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and Compositional gan: Learning imageTrevor Darrell. conditional binary composition. IJCV, 2020. 1, 3 [3] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. In CVPR, Coco-stuff: Thing and stuff classes in context. 2018. 5, [4] Aditya Chandrasekar, Goirik Chakrabarty, Jai Bardhan, Remove: In CVPR WorkRamya Hebbalaguppe, and Prathosh AP. reference-free metric for object erasure. shops, 2024. 2, 6, 7, 8 [5] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, 2024. 1, 3, 6, 7 [6] Zhekai Chen, Wen Wang, Zhen Yang, Zeqing Yuan, Hao Chen, and Chunhua Shen. Freecompose: Generic zero-shot image composition with diffusion prior. In ECCV, 2024. 1, 3, 7 [7] Yigit Ekin, Ahmet Burak Yildirim, Erdem Eren Caglar, Aykut Erdem, Erkut Erdem, and Aysegul Dundar. CLIPAway: Harmonizing focused embeddings for removing objects via diffusion models. In NeurIPS, 2024. 1, 2, 3, 7 [8] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 4, 7 [9] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In NeurIPS, 2023. 7, [10] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. Guiding instruction-based image editing via multimodal large language models. In ICLR, 2024. 2, 3 [11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 7, 8 [12] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. In Lora: Low-rank adaptation of large language models. ICLR, 2022. 2, 4 [13] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking FID: towards better evaluation metric for image generation. In CVPR, 2024. 7, 8 [14] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using masked-region guidance. arXiv preprint arXiv:2501.08279, 2025. 2, 3 [15] Junjie Ke, Qifei Wang, Yilin Wang, Peyman Milanfar, and Feng Yang. MUSIQ: multi-scale image quality transformer. In ICCV, 2021. 7, 8 [16] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality. In NeurIPS, 2023. 5, 8 [17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. 2, 6 [18] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Dan Yamins, and Jiajun Wu. Are these the same apple? comparing images based on object intrinsics. In NeurIPS, 2023. 7, 8 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 1, 2, [20] Fan Li, Zixiao Zhang, Yi Huang, Jianzhuang Liu, Renjing Pei, Bin Shao, and Songcen Xu. Magiceraser: Erasing any objects via semantics-aware control. In ECCV, 2024. 2, 3 [21] Ruibin Li, Tao Yang, Song Guo, and Lei Zhang. Rorem: Training robust object remover with human-in-the-loop. arXiv e-prints, pages arXiv2501, 2025. 2, 3 [22] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. Mat: Mask-aware transformer for large hole image inpainting. In CVPR, 2022. 1, 2, 7 [23] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 3 [24] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. [25] Pablo Marcos-Manchon, Roberto Alcover-Couso, Juan C. SanMiguel, and Jose M. Martınez. Open-vocabulary attention maps with token optimization for semantic segmentation in diffusion models. In CVPR, 2024. 3 [26] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. TMLR, 2024. 6, 7, 8 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. 7, 8 [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 1, 2, 7 [29] Min-Cheol Sagong, Yoon-Jae Yeo, Seung-Won Jung, and Sung-Jea Ko. RORD: real-world object removal dataset. In BMVC, 2022. 7 [30] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo 9 Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 5 [31] Nikita Selin. Carvekit: Automated high-quality background removal framework. https://github.com/ OPHoperHPO/image-background-remove-tool, 2023. 4, 7 [32] Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. CoRR, abs/2411.15466, 2024. 3 [33] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, and Daniel G. Aliaga. Objectstitch: Object compositing with diffusion model. In CVPR, 2023. 1, 2, 3, [34] Yizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian L. Price, Jianming Zhang, Soo Ye Kim, He Zhang, Wei Xiong, IMPRINT: generative object comand Daniel G. Aliaga. positing by learning identity-preserving representation. In CVPR, 2024. 1, 2, 3, 4, 6, 7 [35] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. 2, 5, 6, 7 [36] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and uniarXiv preprint versal control for diffusion transformer. arXiv:2411.15098, 2024. 4 [37] Alimama Creative Team. Flux-controlnet-inpainting. https : / / github . com / alimama - creative / FLUX-Controlnet-Inpainting, 2024. 1, 2, 6, 7 [38] Yoad Tewel, Rinon Gal, Dvir Samuel, Yuval Atzmon, Lior Wolf, and Gal Chechik. Add-it: Training-free object insertion in images with pretrained diffusion models. CoRR, abs/2411.07232, 2024. 1, 2 [39] Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan Sunkavalli, Xin Lu, and Ming-Hsuan Yang. Deep image harmonization. In CVPR, 2017. 3 [40] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In CVPR, pages 2241322422, 2024. [41] Hao Wang, Yongsheng Yu, Tiejian Luo, Heng Fan, and Libo Zhang. Magic: Multi-modality guided image completion. In ICLR, 2024. 2 [42] Tianyu Wang, Jianming Zhang, Haitian Zheng, Zhihong Ding, Scott Cohen, Zhe L. Lin, Wei Xiong, Chi-Wing Fu, Luis Figueroa, and Soo Ye Kim. Metashadow: Objectcentered shadow detection, removal, and synthesis. CoRR, abs/2412.02635, 2024. 3 [43] Daniel Winter, Matan Cohen, Shlomi Fruchter, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectdrop: Bootstrapping counterfactuals for photorealistic object removal and insertion. In ECCV, 2024. 1, 2, 3, 4 10 [44] Daniel Winter, Asaf Shul, Matan Cohen, Dana Berman, Yael Pritch, Alex Rav-Acha, and Yedid Hoshen. Objectmate: recurrence prior for object insertion and subject-driven generation. CoRR, abs/2412.08645, 2024. 2, 3 [45] Chen Henry Wu and Fernando De la Torre. latent space of stochastic diffusion models for zero-shot image editing and guidance. In ICCV, 2023. [46] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. gp-gan: Towards realistic high-resolution image blending. In ACM MM, 2019. 1, 3 [47] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In CVPR, 2023. 1, 3, 7 [48] Danni Yang, Ruohan Dong, Jiayi Ji, Yiwei Ma, Haowei Wang, Xiaoshuai Sun, and Rongrong Ji. Exploring phraselevel grounding with text-to-image diffusion model. In ECCV, 2024. 3 [49] Sidi Yang, Tianhe Wu, Shuwei Shi, Shanshan Lao, Yuan Gong, Mingdeng Cao, Jiahao Wang, and Yujiu Yang. MANIQA: multi-dimension attention network for noIn CVPR Workshops, reference image quality assessment. 2022. 7, 8 [50] Ahmet Burak Yildirim, Vedat Baday, Erkut Erdem, Aykut Erdem, and Aysegul Dundar. Instructing to remove objects with diffusion models. arXiv preprint arXiv:2304.03246, 2023. 1, 2, 3 Inst-inpaint: [51] Ahmet Burak Yildirim, Hamza Pehlivan, Bahri Batuhan Bilecen, and Aysegul Dundar. Diverse inpainting and editing with gan inversion. In ICCV, 2023. 2 [52] Yongsheng Yu, Dawei Du, Libo Zhang, and Tiejian Luo. Unbiased multi-modality guidance for image inpainting. In ECCV, 2022. 2 [53] Yongsheng Yu, Libo Zhang, Heng Fan, and Tiejian Luo. High-fidelity image inpainting with gan inversion. In ECCV, 2022. 2 [54] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. In NeurIPS, 2024. 2, 3 [55] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 7, 8 [56] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Eli Shechtman, Connelly Barnes, Jianming Zhang, Ning Xu, Sohrab Amirghodsi, and Jiebo Luo. Image inpainting with In cascaded modulation gan and object-aware training. ECCV, 2022. [57] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017. 4 [58] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In ECCV, 2024. 1, 2, 3,"
        }
    ],
    "affiliations": [
        "Adobe Research",
        "University of Rochester"
    ]
}