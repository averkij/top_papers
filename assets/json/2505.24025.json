{
    "paper_title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
    "authors": [
        "Chenbin Pan",
        "Wenbin He",
        "Zhengzhong Tu",
        "Liu Ren"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose \\textbf{DINO-R1}, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces \\textbf{Group Relative Query Optimization (GRQO)}, a novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train a series of DINO-R1 family models that integrate a visual prompt encoder and a visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 5 2 0 4 2 . 5 0 5 2 : r DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models Chenbin Pan1,2 Wenbin He1,2 Zhengzhong Tu3 Liu Ren1,2 1Bosch Research North America 2Bosch Center for Artificial Intelligence (BCAI) 3Texas A&M University {chenbin.pan@us.bosch.com} https://Christinepan881.github.io/DINO-R"
        },
        {
            "title": "Abstract",
            "content": "The recent explosive interest in the reasoning capabilities of large language models, such as DeepSeek-R1, has demonstrated remarkable success through reinforcement learning-based fine-tuning frameworks, exemplified by methods like Group Relative Policy Optimization (GRPO). However, such reasoning abilities remain underexplored and notably absent in vision foundation models, including representation models like the DINO series. In this work, we propose DINO-R1, the first such attempt to incentivize visual in-context reasoning capabilities of vision foundation models using reinforcement learning. Specifically, DINO-R1 introduces Group Relative Query Optimization (GRQO), novel reinforcement-style training strategy explicitly designed for query-based representation models, which computes query-level rewards based on group-normalized alignment quality. We also apply KL-regularization to stabilize the objectness distribution to reduce the training instability. This joint optimization enables dense and expressive supervision across queries while mitigating overfitting and distributional drift. Building upon Grounding-DINO, we train series of DINO-R1 family models that integrate visual prompt encoder and visual-guided query selection mechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that DINO-R1 significantly outperforms supervised fine-tuning baselines, achieving strong generalization in both open-vocabulary and closed-set visual prompting scenarios."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large reasoning models (LRMs) [110], exemplified by the impressive performance of DeepSeek-R1 [8, 1], have demonstrated remarkable capabilities across complex reasoning tasks like math reasoning and coding. This breakthrough is largely driven by innovative reinforcement (RL) learning strategies such as Group Relative Policy Optimization (GRPO) [1]. By iteratively generating synthetic data and optimizing reasoning models through verifiable rewards, DeepSeek-R1 has attained superior reasoning abilities competitive with state-of-the-art proprietary models, such as OpenAI o1, significantly reshaping the language modeling landscape. Despite these impressive advancements, however, the equivalent advancement of reasoning remains notably limited in vision foundation models (VFMs) [1117]. Current VFMs largely rely on supervised training paradigms focused on predefined visual categories [1820] or self-supervised objectives [21 23]. These conventional supervised methods inherently lack robust reasoning mechanisms, limiting their ability to effectively generalize to novel, ambiguous, or high-variance scenarios frequently encountered in practical, real-world applications. Preprint. Under review. One emerging and increasingly important scenario in VFMs is visual prompting [24, 16, 25, 15, 26], new paradigm wherein users specify detection targets using visual exemplars. This approach offers substantial practical utility in broad applications such as auto-labeling, industrial inspection, and robotic manipulation. [2736] However, training visual-prompting models poses new challenges despite their practical relevance due to the high diversity and intra-class variation among visual exemplars. Compared to their language-prompted counterparts [14, 37, 38, 26, 39], training recipes for visual prompting detectors remain largely underdeveloped. We have observed that training with supervised fine-tuning (SFT) alone typically struggles under these conditions, exhibiting unstable convergence, limited generalization to out-of-domain data, as well as poor alignment of query predictions with visual prompts (Sec. 4). These findings suggest that vanilla SFT is insufficient for effectively training visual prompting detectors, motivating us to explore fundamentally new training strategies capable of effectively reasoning over diverse visual inputs for robust generalization. Inspired by the recent breakthroughs in RL-based training frameworks for LRMs [69, 1, 40 42, 2, 10, 35], which efficaciously exploit large-scale noisy training data, we aim to similarly unlock the potential of reasoning capabilities within pure vision models, e.g., VFMs. Yet, naïve application of language-based RL methods such as GRPO to vision presents non-trivial challenges. For one thing, GRPO assumes the model behaves as probabilistic generator, explicitly sampling diverse outputs from learned distributions per input. In contrast, vision models typically produce deterministic structured predictions, making it nontrivial to optimize over sampled output space. For another, GRPOs KL-regularization that stabilizes training via constraining token-level output distributions in language models can not be easily translated to structured visual predictions due to fundamental differences in language and vision formulation. To this end, we present novel vision-centric RL learning method called group related query optimization (GRQO) designed to incentivize reasoning capabilities in VFMs, notably DINO families. Specifically, GRQO introduces query-level relative reward module that evaluates each querys quality within its group and computes normalized reward based on its advantage over the group average. By encouraging every query to exceed the dynamic group baseline, this mechanism provides denser and more informative training signals  (Fig. 1)  in contrast to the traditional one-to-one matching scheme. Additionally, we propose KL-divergence regularization on the object probability distribution at the query selection stage to help mitigate model drift and catastrophic forgetting during training. To support this new training paradigm, we implemented diverse visual prompting by introducing visual prompt encoder and visual-guided query selection mechanism, resulting in text-free variant we refer to as the VIS-G-DINO baseline. We then train this model using our proposed GRQO framework, which has delivered set of vision LRMs, here referred to as DINO-R1 for the final artifact. Our main contributions are summarized as follows: We propose Group Relative Query Optimization (GRQO), the first reinforcement-style training paradigm designed to address the high variance of visual prompts in open-set object detection. We introduce query-level relative reward module and KL-divergence regularization strategy to improve training stability, query quality, and generalization under visual prompting. We develop VIS-G-DINO, an RL training framework for visual prompt-based VFMs (e.g., DINO), and define DINO-R1 as the resulting detector trained with GRQO. We conduct extensive experiments on COCO, LVIS, and ODinW, where DINO-R1 consistently outperforms supervised fine-tuning baselines and demonstrates strong generalization in both open-vocabulary and closed-set visual prompting scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Vision Foundation Models and DETR. Vision foundation models (VFMs) [43, 16, 15, 44, 14, 37, 45, 46, 11, 12, 47, 24, 25, 48, 49] have enabled significant progress across wide spectrum of visual tasks by learning general-purpose image representations from large-scale datasets. Among them, DETR [50] and its derivatives [51, 52, 13, 16, 14, 53, 15, 37, 38] formulate object detection as set prediction problem using transformer-based architectures, offering open-set capabilities and strong performance on dense prediction tasks. Grounding DINO [14, 38], in particular, extends DETR to open-vocabulary detection by incorporating vision-language alignment, enabling phrase-grounded object detection via language prompts. 2 Figure 1: SFT vs. GRQO. SFT leads to limited and homogeneous supervision signals, while GRQO produces richer and more diverse learning signals, encouraging queries to be more expressive. Open-Vocabulary and Prompt-Based Detection. Open-vocabulary detection [54](OVD) aims to recognize categories beyond the supervised training set by leveraging external knowledge sources such as pretrained text encoders or image-text pairs. Most existing OVD methods [55, 39, 53, 54, 56, 14] focus on language prompts to bridge the category gap, leaving the space of visual promptingusing visual exemplars instead of textlargely underexplored. Recent works [26, 16, 24, 25, 15] have explored using reference images or bounding boxes to ground object-level semantics. However, these methods often rely on inference-time conditioning and lack robust training paradigm for learning from high-variance visual prompts. Reinforcement Learning in Foundation Models. Reinforcement learning [41, 40, 42, 1, 6] has played central role in fine-tuning large language models (LLMs)[2, 3, 57, 46, 8] via methods such as Reinforcement Learning from Human Feedback (RLHF) [41] and Group Relative Policy Optimization (GRPO)[1, 8]. These approaches enable models to align better with diverse, weaklysupervised, or ambiguous objectives. However, the application of reinforcement-style training to vision foundation models, especially for dense prediction tasks like object detection, remains underexplored. Our work bridges this gap by adapting GRPO principles to query-level learning in transformer-based object detectors through our proposed Group Relative Query Optimization."
        },
        {
            "title": "3 Methodology",
            "content": "While language-prompted object detection has received growing attention in the vision-language community, the training strategy for visual prompt-based detection remains underexplored. To address the challenge of high-variance visual prompts and unlock the potential of prompt-guided detector, in this work, we introduce novel training paradigm for the visual prompting object detection, Group Relative Query Optimization (GRQO). Built on top of the Grounding-DINO (G-DINO) [14] framework (3.1), our approach integrates visual prompt encoder (3.2) and the GRQO mechanism (3.3) to enhance query learning and improve detection robustness. 3.1 Preliminary Given an image-text pair (Image, ext), G-DINO extracts multi-scale visual features RNI using an image backbone Bimg (e.g., Swin Transformer), and textual features RNtxtC using text backbone Btxt(e.g., BERT). These features are passed through cross-modal feature enhancer FI,t to obtain refined features and via combination of deformable self-attention (for images), vanilla self-attention (for text), and bidirectional cross-attention for fusion. To leverage textual 3 Figure 2: Overview of the proposed Group Relative Query Optimization (GRQO) framework. The query reward module enriches supervision by assigning group-relative rewards to enhance query learning. In parallel, KL-regularization constrains the trust region of query updates, enabling the detector to progressively absorb diverse visual prompts while preserving previously acquired detection behaviors. guidance during detection, G-DINO employs language-guided query selection mechanism, where Nq image locations that are most relevant to the text prompt are selected based on cross-modal similarity, and further serving as the positional part for the decoder queries. set of learnable queries are conducted to attend to prompts and objects through multi-modal decoder, which consists of self-attention, image cross-attention, text cross-attention, and feedforward module. The final class prediction is made through contrastive similarity between queries and refined prompt features. Following DETR-style supervision, the model is trained using focal loss for classification and combination of L1 and GIoU loss for bounding box regression. The overall loss is: LG-DINO = Lfocal + Ll1 + LGIoU. (1) 3.2 VIS-G-DINO We extend Grounding DINO to support visual prompting and refer to the resulting model as VIS-GDINO. Different from G-DINO conditioning on free-form text, VIS-G-DINO conditions detection on visual promptsuser-specified bounding boxes over reference imagesenabling open-set detection without language description. The reference image can be the same as the target image or come from different context. Visual prompt encoding. We design visual prompt encoder Evis that transforms each input bounding box on reference image into localized visual feature. Each box is first embedded using sine-cosine positional encoding and projected to match the transformer input space. These embeddings, along with learnable visual query, are used to attend to multi-scale image features via deformable cross-attention. self-attention and feedforward layer further refine these into compact visual prompt embeddings that capture region-level semantics. The process can be expressed as: Qpos Evis Evis = Linear(P(b1, ..., bN)) : RN 4C RN C, = MSDeformAttn(QEvis, Qpos Evis , b, ˆI), = FFN(SelfAttn(Q Evis )). (2) (3) (4) Semantic Alignment and Prompt Sampling. To reinforce semantic consistency, we apply regionlevel contrastive learning between visual prompts and their corresponding text embeddings. This 4 anchors the visual prompts in the same semantic space as the pretrained language model. During training, we apply random sampling over visual prompts to improve generalization. Specifically, prompts are randomly sampled per class in batch to form the final visual instructions v. We find = 1 yields the best trade-off between diversity and stability. Image-prompt fusion and query selection. Following G-DINOs architecture, we fuse image features and visual prompts via multimodal feature enhancer FI,v to obtain refined image features and visual prompt features v. To guide detection process, we introduce visual-guided query selection mechanism. Given refined image tokens and visual prompt features v, we compute an image-prompt similarity matrix via dot product. For each image token, we take the maximum similarity across prompt axis as its objectness score, representing the likelihood that prompted object exists at that location. We select the top-Nq image tokens with the highest objectness scores as positional embeddings for the decoder queries. The top-Nq indices selection can be expressed as: = TopNq (Max(1)(I The corresponding regions serve as coarse proposals, while the content embeddings of the queries remain learnable. The remaining stages mirror the G-DINO pipeline."
        },
        {
            "title": "Idxv\nNq",
            "content": "(5) )). Overall training objective. The VIS-G-DINO model is optimized using composite objective: LVIS-G-DINO = Lcontra + Lfocal + LL1 + LGIoU, where Lcontra promotes semantic alignment, and the remaining losses follow standard detection objectives for classification and localization. (6) 3.3 Group Relative Query Optimization Visual prompting detection demands that object queries align with highly diverse visual exemplars that share the same semantics. This setting introduces greater intra-class variance than language prompts, requiring the model to both memorize diverse appearances and generalize across unseen variations. Inspired by the generalization ability of GRPO [1] in LLMs community, we propose Group Relative Query Optimization (GRQO, Fig.2)a novel training paradigm that enhances query quality and learning stability through group-based reward modeling and distributional regularization. Query-level relative reward. In DETR-style architectures, queries interact via selfand crossattention across layers and serve as the main carriers of detection capacity. However, the standard one-to-one bipartite matching provides sparse supervision, updating only small subset of queries and leaving others under-optimized  (Fig.1)  . To address this, we introduce query-level reward mechanism that densifies supervision across all queries. Instead of relying solely on bipartite matching to backpropagation gradients to limited subset of queries, we compute an auxiliary reward signal for each query based on its alignment quality with ground-truth instances. Specifically, for , Qbox each decoder query prediction Qpred }, we compute pair-wise matching cost to ground-truths within the same image. The matching cost is weighted sum of classification and localization terms: = {Qcls Ci,j = λf ocalCf ocal(qpredi , gj) + λl1Cl1(qpredi , gj) + λGIoU CGIoU (qpredi , gj). (7) The minimum total cost among GT instances is selected as the metrics for evaluating the query quality. The reward ri for query is then defined as the inverse of this minimal cost: γi = min Ci,j, ri = γi, (8) and ground-truth gj. lower cost where Ci,j denotes the matching cost between query qpredi implies better alignment, and thus higher reward. To make the learning signal more robust and harness the group dynamic, we normalize the reward across all queries within the same sample to compute relative advantage: ˆAi = ri µr σr (9) where µr and σr are the mean and standard deviation of the rewards within the group. This groupnormalized advantage provides stable, comparative gradients, encouraging all queries to improve relative to the dynamic group baseline. 5 Figure 3: Qualitative comparison of visual prompting detection between SFT and GRQO. SFT results exhibit both false positives (row 2,3,4) and missed detections (row 1), reflecting limited query expressiveness and weak alignment with visual prompts. In contrast, GRQO produces more accurate and complete detections, better aligning with the prompted semantics. These results highlight GRQOs ability to enhance query reasoning and robustness under high-variance visual inputs. KL-divergence regularization. To further stabilize training under high-variance visual prompts and prevent distributional drift, we introduce KL-divergence-based regularization term on the objectness probability distribution. In our setting, the objectness distribution captures the models confidence over image tokens being relevant to the prompted object. Due to the diverse appearance and structure of visual prompts, these objectness predictions can fluctuate across iterations, leading to training instability. To mitigate this, we regularize the objectness probability distribution Oθ of the current model with respect to reference model distribution Oref . Given the selected top Nq token indices, the two distribution is generated as Eq.: Oθ = Max(1)(I ], Oref = Max(1)(I (10) ], ref )[Idxv Nq )[Idxv Nq ref where Then, the KL-divergence is computed as: ref and ref denote the refined image features and prompt features from reference model. DKL[Oθ Oref ] = Oref (qiI, v) Oθ(qiI, v) log Oref (qiI, v) Oθ(qiI, v) 1, (11) where qi denotes the i-th query token, represents the query objectness distribution conditioned on target image features and sampled visual prompts v. This regularization encourages the current model to remain close to the reference distribution, which is frozen copy of an earlier training state. By anchoring the learning dynamics to stable prior, KL-regularization helps the model retain generalizable knowledge while progressively absorbing the diversity of visual prompts. Overall training objective. Our proposed GRQO loss introduces group-relative reinforcement signals and regularization to improve query quality and training stability. Specifically, the GRQO loss is defined as: LGRQO = 1 Nq Nq (cid:88) (α ˆAi β DKL[Oθ Oref ]), i=1 (12) where α and β are scalar weights that balance the reward signal and regularization strength. GRQO incentivizes both query-level learning and stable objectness modeling. To complement this group-level supervision, we include standard per-query detection losses. In addition, we retain the region-level contrastive loss Lcontra to align visual prompts with corresponding semantic concepts. The final training objective of DINO-R1 is guided by composite objective: LDINO-R1 = LGRQO + Lcontra + Lfocal + LL1 + LGIoU. (13) 6 Table 1: Object detection results under Zero-Shot and Fine-Tuning settings across multiple datasets. Fine-Tune COCO AP LVIS-minival AP APr APc APf ODinW13 ODinW35 COCO AP Zero-Shot Dataset APavg APavg Model Epoch GDINO-T [14] MM-GDINO-T [38] MM-GDINO-L [38] O365v2,O,G O365,G,C O365,G SFT VIS-GDINO-T VIS-GDINO-B VIS-GDINO-L GRQO DINO-R1-T DINO-R1-B DINO-R1-L O365 O365 O365 O365 O365 O365 Text Prompting 48.4 50.4 53. 28.8 35.7 - 18.8 28.1 - 24.2 30.2 - 34.7 42.0 - Visual Prompting 19.9 24.8 26. 24.0 26.1 28.1 15.3 17.1 18.4 17.4 19.0 20.1 11.8 12.5 15.9 14.1 15.9 17.8 15.6 16.4 17. 17.9 19.4 19.8 17.1 18.9 20.3 20.8 21.2 23.4 30 30 42 6 6 6 1+5 1+5 1+ 51.4 45.3 75.3 4.6 11.2 15.3 5.0 14.7 24.1 22.7 20.2 35.2 2.6 6.7 8.2 3.0 7.2 12. 58.1 58.2 - 32.5 38.1 39.2 37.2/39.6 41.3/42.4 43.5/44.1 This multi-component loss encourages DINO-R1 to benefit from both group-level optimization signals and instance-level supervision, enabling robust and generalizable visual prompting detection."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment Setup Baseline and Base Model. We compare GRQO with standard supervised fine-tuning (SFT). All experiments are conducted using the MM-Grounding-DINO [38] implementation, which we adapt to support visual prompting. We use visual exemplars (images with user defined bounding boxes) as input prompts to guide the detection in the target image. Datasets and Implementation Details. We evaluate our approach under two settings: ❶ Zero-Shot. (Out-domain Evaluation). We conduct open-vocabulary detection where we train on Objects365 (O365) [20] and test on COCO [19], LVIS-minival [58], ODinW13 [45], and ODinW35 [45]. We train the model for 6 epochs as SFT baselines. For GRQO, we first train 1 epoch with SFT to obtain the reference model, then apply GRQO for 5 additional epochs. ❷ Fine-Tune. (In-domain Evaluation). We fine-tune on the COCO training split for 12 epochs and evaluate on the COCO validation set. For GRQO, we apply both pretrained weights of SFT and GRQO for evaluation. 4.2 Main Results The visual prompting object detection results are summarized in Table.1. Out-of-domain detection on COCO and LVIS. We evaluate models trained on Objects365 under zero-shot transfer settings. As shown in Table.1, DINO-R1 consistently achieves better generalization to COCO and LVIS datasets. On COCO, DINO-R1-T improves mAP by +4.1 (19.9 24.0) compared to SFT. On the more challenging LVIS dataset, which contains long-tailed categories, DINO-R1-B improves over SFT by +3.4 (12.5 15.9) on rare category, demonstrating its stronger generalization to diverse and rare categories. This validates the effectiveness of GRQOs group-wise learning and regularization in handling open-vocabulary visual conditions. Fig. 4(a) presents the training dynamics, where DINO-R1 exhibits more stable training compared to SFT. Out-of-domain detection in the wild. We further evaluate our method on ODinW, which includes various real-world domains. On the 13-dataset and 35-dataset ODinW subsets, DINO-R1-L outperforms SFT by +8.8 and +4.4 mAP, respectively. These consistent gains across varied domains reflect not only improved generalization, but also enhanced visual reasoning capability. By optimizing queries with group-relative rewards and stable objectness supervision, DINO-R1 learns to better align high-level semantics across disparate scenes and object stylesan essential property for visual in-context reasoning in open-world scenarios. We provide qualitative comparison in Fig.3. 7 Table 2: Ablation of the query-level reward and KL-regularization modules. Both individually improve performance over SFT, while combining them yields the highest gains in both settings on COCO. Table 3: Design analysis of the query reward. Ablation of reward formulation using different combinations of focal, L1, and IoU-based costs. Group-relative rewards consistently outperform absolute variants, and layer-wise reward propagation () further enhances performance. Reward COCO Method COCO Zero-Shot AP Fine-Tune AP SFT only reward only KL-Div GRQO 19.9 22.8 21.0 24.0 32.5 36.1 34.2 37. Method Focal BboxL1 IoU Zero-Shot AP Fine-Tune AP SFT GRQO (relative) GRQO (relative) GRQO (relative) GRQO (relative) GRQO (absolute) GRQO (relative) - - - 19.9 21.3 22.7 21.8 23.5 20.1 24.0 32.5 34.1 33.6 34.0 36.8 31.4 37.2 In-domain detection on COCO. GRQO also provides consistent gains under the closed-set detection setting on COCO across multiple training strategies. When fine-tuning the SFT-pretrained model with GRQO, DINO-R1-L achieves 43.5 mAP, outperforming continued SFT training (39.2 mAP) by +4.3. Notably, using the GRQO-pretrained model as the starting point leads to even larger improvements, with DINO-R1 surpassing the SFT baseline by +4.9 mAP. These results demonstrate that GRQO not only generalizes better but also improves training efficiency and effectiveness within the same domain. 4.3 Ablation Study Effectiveness of each component. To evaluate the contributions of the two key components in GRQOquery-level relative reward and KL-divergence regularizationwe conduct controlled ablations by enabling each module independently. Table 2 shows that both components individually improve performance over the SFT baseline for both out-domain and in-domain detection. Specifically, the reward module yields 2.9 and 3.6 mAP gains, while the KL-regularization contributes 1.1 and 1.7 mAP improvements. When both components are applied together, the full GRQO framework further outperforms the SFT baseline by 4.1 and 4.7, respectively. These results confirm that both modules are beneficial, and their combination further enhances the models capability to generalize under visual prompting settings. Query reward design. We ablate the design choices in the reward function used to optimize query quality. Since our goal is accurate detection guided by visual prompts, we consider both classification and localization cues for reward formulation. We test the classification reward (reverse focal cost), the localization reward (the reverse L1 and GIoU). Additionally, we compare the use of absolute reward values versus group-relative reward values. Table 4: Impact of loss weights in GRQO. We vary the scaling of the querylevel reward and KL-regularization losses. The best performance is achieved with reward weight of 10e3 and KL weight of 0.04, highlighting the importance of balancing learning signal strength and regularization. Loss Weight COCO Reward KL-Div As shown in Table.3, using all three components together with group-relative reward achieves the best performance of 23.5 and 36.8 mAP. Notably, the relative reward outperforms absolute reward by margin of 3.4 and 5.4 mAP, highlighting that group-wise normalization improves reward stability and allows the model to focus on inter-query discriminability rather than absolute query quality, which is often sensitive to instance-level noise. Furthermore, we examine layer-wise reward strategy, where intermediate decoder layers are also supervised by the reward function. As shown in the last row of Table.3, this design further boosts performance by 0.5 and 0.4 mAP, suggesting that earlier query refinement stages also benefit from reinforcement-style optimization. 0.4 0.04 0.04 0.04 0.04 0. 1.0 1.0 10.0 10e3 10e4 10e3 33.4 35.2 35.1 37.2 36.8 35.3 20.2 21.6 22.4 24.0 23.1 21.5 Fine-Tune AP Zero-Shot AP Effect of Loss Scaling. We investigate the sensitivity of GRQO to the scaling of its two key loss components: the query reward term and the KL-divergence regularization. Specifically, we vary the weight of the reward loss across 1.0, 10.0, 10e2, 10e3, 10e4, and the KL-regularization across 0.4, 8 0.04, 0.004. As shown in Table 4, the best performance is achieved when the reward weight is set to 10e3 and the KL weight to 0.04. This indicates that moderately strong reward signals encourage more effective query discrimination, while excessively large weights lead to inferior optimization. Similarly, the KL-regularization coefficient of 0.04 strikes good balance between stability and generalization, helping the model resist distributional drift during training on diverse visual prompts. Number of prompts. The diversity and quantity of visual prompts play crucial role in training robust visual-prompting detectors. We ablate the number of randomly sampled prompts per class during training, and further evaluate models with varying numbers of prompts at inference. As shown in Fig.4(b)(c) and Table 5, training with only one random prompt per class significantly outperforms settings with more prompts. We hypothesize that this is due to the increased diversity and higher variance in the sampled prompt pool, allowing the model to generalize over wider range of visual appearances. By seeing more varied exemplars across training iterations, the model learns broader and more adaptable visual concept space. Conversely, during inference, the performance improves as the number of prompts per class increases, suggesting that ensemble-style prompting helps reinforce object identity and reduce ambiguity in open-set scenarios. Figure 4: (a) Training curves comparing SFT and GRQO. GRQO consistently achieves more stable training with higher final performance. (b)(c) Effect of the number of prompts per class during training/inference. # 1 8 16 32 COCO Zero-Shot Inference Train 24.0 11.2 17.6 16.4 22.0 15.2 21.4 15.1 24.0 14.9 Table 5: Effect of the number of prompts. We vary the number of sampled prompts per class during training and inference."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce DINO-R1, novel training paradigm that advances the reasoning capabilities of vision foundation models under the visual prompting setting. Built on Grounding DINO, DINO-R1 rethinks how object queries are trained by shifting from sparse, instance-level supervision to dense, groupaware optimization. At the heart of our method is Group Relative Query Optimization (GRQO), which evaluates and refines queries through relative rewards within query groupsmirroring the collaborative inference process inherent in transformer-based detectors. To further stabilize training and prevent forgetting, we propose KL-divergence regularization on the objectness distribution, anchoring the model around stable representations while progressively learning from diverse prompts. Together, these components offer principled and scalable approach for training detectors that generalize robustly across domains, exhibit stronger reasoning over visual prompts, and remain stable under the high variance characteristic of open-world conditions. Our extensive experiments on COCO, LVIS, and ODinW validate the effectiveness of DINO-R1, showing consistent improvements over supervised fine-tuning across both zero-shot and fine-tuned evaluations. We believe DINO-R1 opens promising direction for reinforcement-inspired training in dense visual tasks and provides foundation for future research in visual in-context learning, multi-modal alignment, and prompt-driven visual reasoning."
        },
        {
            "title": "6 Future Work & Limitations",
            "content": "Our work primarily focuses on the optimization strategy rather than architectural enhancements. The visual prompt encoder used in DINO-R1 adopts relatively simple design to isolate and highlight the contributions of our GRQO framework. We believe there is substantial room to explore more expressive and structured visual prompt encoding methods. In future work, we plan to integrate advanced visual prompt architectures, extend DINO-R1 to more challenging and diverse datasets, and explore its application in other open-world settings such as referring expression comprehension, retrieval-augmented detection, and multi-shot visual reasoning. We see DINO-R1 as foundational step toward scalable, prompt-driven visual understandingand aim to build on this foundation by further closing the gap between model flexibility and reasoning robustness."
        },
        {
            "title": "References",
            "content": "[1] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [2] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [3] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [4] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [5] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [6] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [7] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, et al. Deepseek-vl: towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [9] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [12] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [13] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. [14] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [15] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. [16] Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Jianwei Yang, Chunyuan Li, et al. Visual in-context prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1286112871, 2024. [17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 10 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [20] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. [21] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [22] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. [23] Souradip Chakraborty, Aritra Roy Gosthipaty, and Sayak Paul. G-simclr: Self-supervised contrastive learning with guided projection via pseudo labelling. In 2020 international conference on data mining workshops (ICDMW), pages 912916. IEEE, 2020. [24] Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, and Lei Zhang. T-rex: Counting by visual prompting. arXiv preprint arXiv:2311.13596, 2023. [25] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic object detection via text-visual prompt synergy. In European Conference on Computer Vision, pages 3857. Springer, 2024. [26] Qibo Chen, Weizhong Jin, Jianyue Ge, Mengdi Liu, Yuchao Yan, Jian Jiang, Li Yu, Xuanjiang Guo, Shuchang Li, and Jianzhong Chen. Cp-detr: Concept prompt guide detr toward stronger universal object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 21412149, 2025. [27] Hsi-Ai Tsao, Lei Hsiung, Pin-Yu Chen, Sijia Liu, and Tsung-Yi Ho. Autovp: An automated visual prompting framework and benchmark. arXiv preprint arXiv:2310.08381, 2023. [28] Fangchen Liu, Kuan Fang, Pieter Abbeel, and Sergey Levine. Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. In First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024, 2024. [29] Yuang Ai, Huaibo Huang, Xiaoqiang Zhou, Jiexiang Wang, and Ran He. Multimodal prompt perceiver: Empower adaptiveness generalizability and fidelity for all-in-one image restoration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2543225444, 2024. [30] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, et al. Pivot: Iterative visual prompting elicits actionable knowledge for vlms. arXiv preprint arXiv:2402.07872, 2024. [31] Gunwoo Yong, Kahyun Jeon, Daeyoung Gil, and Ghang Lee. Prompt engineering for zero-shot and few-shot defect detection and classification using visual-language pretrained model. Computer-Aided Civil and Infrastructure Engineering, 38(11):15361554, 2023. [32] Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, and Tianyi Zhang. Promptcharm: Text-to-image generation through multi-modal prompting and refinement. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pages 121, 2024. [33] jinheng zhou, Wu Liu, Guang Yang, He Zhao, and feiniu yuan. Prompting industrial anomaly segment with large vision-language models. In Proceedings of the 6th ACM International Conference on Multimedia in Asia, pages 11, 2024. [34] Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan Rossi, Ruiyi Zhang, et al. Visual prompting in multimodal large language models: survey. arXiv preprint arXiv:2409.15310, 2024. [35] Aochuan Chen, Yuguang Yao, Pin-Yu Chen, Yihua Zhang, and Sijia Liu. Understanding and improving visual prompting: label-mapping perspective. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1913319143, 2023. 11 [36] Yang Chen, Yingwei Pan, Haibo Yang, Ting Yao, and Tao Mei. Vp3d: Unleashing 2d visual prompt for text-to-3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48964905, 2024. [37] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. [38] Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang. arXiv preprint An open and comprehensive pipeline for unified object grounding and detection. arXiv:2401.02361, 2024. [39] Yu Du, Fangyun Wei, Zihe Zhang, Miaojing Shi, Yue Gao, and Guoqi Li. Learning to prompt for openvocabulary object detection with vision-language model. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1408414093, 2022. [40] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [41] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [42] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [44] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021. [45] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan In Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. [46] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. [47] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. [48] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [49] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Realtime open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [50] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [51] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1361913627, 2022. [52] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022. 12 [53] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional matching. In European conference on computer vision, pages 106122. Springer, 2022. [54] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1439314402, 2021. [55] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021. [56] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Openvocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 29552966, 2023. [57] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. [58] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019."
        }
    ],
    "affiliations": [
        "Bosch Center for Artificial Intelligence (BCAI)",
        "Bosch Research North America",
        "Texas A&M University"
    ]
}