{
    "paper_title": "Emergence of Linear Truth Encodings in Language Models",
    "authors": [
        "Shauli Ravfogel",
        "Gilad Yehudai",
        "Tal Linzen",
        "Joan Bruna",
        "Alberto Bietti"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce a transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. We study one simple setting in which truth encoding can emerge: a data distribution where factual statements co-occur with other factual statements (and vice-versa), encouraging the model to learn this distinction in order to lower the LM loss on future tokens. We corroborate this pattern with experiments in pretrained language models. Finally, in the toy setting we observe a two-phase learning dynamic: networks first memorize individual factual associations in a few steps, then -- over a longer horizon -- learn to linearly separate true from false, which in turn lowers language-modeling loss. Together, these results provide both a mechanistic demonstration and an empirical motivation for how and why linear truth representations can emerge in language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 4 0 8 5 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Emergence of Linear Truth Encodings\nin Language Models",
            "content": "Shauli Ravfogel1 Gilad Yehudai1 Tal Linzen1 Joan Bruna1 Alberto Bietti2 1New York University 2Flatiron Institute"
        },
        {
            "title": "Abstract",
            "content": "Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. We study one simple setting in which truth encoding can emerge: data distribution where factual statements co-occur with other factual statements (and vice-versa), encouraging the model to learn this distinction in order to lower the LM loss on future tokens. We corroborate this pattern with experiments in pretrained language models. Finally, in the toy setting we observe two-phase learning dynamic: networks first memorize individual factual associations in few steps, thenover longer horizonlearn to linearly separate true from false, which in turn lowers language-modeling loss. Together, these results provide both mechanistic demonstration and an empirical motivation for how and why linear truth representations can emerge in language models."
        },
        {
            "title": "Introduction",
            "content": "Recent observations suggest that large language models (LMs) often encode low-rank linear subspace that distinguishes true from false statements across wide range of domains [Azaria and Mitchell, 2023, Burns et al., 2022, Li et al., 2024b, Marks and Tegmark, 2024, Bürger et al., 2025, Orgad et al., 2025]. Specifically, in many layers of the residual stream representation in transformerbased LMs, linear separation emerges between representations corresponding to true versus false assertions. Moreover, this separation generalizes across domains: there exists single separating subspace such that statements like 2 + 2 = 4 (true) and The capital city of France is Rome (false) fall on opposite sides of the same separating plane. These findings have sparked interest among practitioners, because they may aid in mitigating hallucinations [Li et al., 2024b, Orgad et al., 2025]. We investigate the emergence of unified truth subspacea low-dimensional linear manifold that cleanly separates true from false statements. Prior work shows (i) that truth-encoding directions generalize remarkably well across diverse tasks and prompts, and (ii) that causal interventions along those directions can steer LMs toward factual or counter-factual completions [e.g. Meng et al., 2022]. Yet we still lack satisfying answer to two fundamental questions: why do such subspaces arise during training, and how are they actually computed at inference time? We address both questions in single theoretical and empirical framework. For the how, we build on the growing understanding of keyvalue associative memories in transformers. Geva et al. [2021] showed that the first linear layer produces key matchese.g. aligning the prefix The capital city of France is with an internal querywhile the second linear layer retrieves the associated value, such as the hidden representation of Paris. Subsequent studies refined the mathematical description of this mechanism and demonstrated its causal role in factual recall and reasoning [Geva et al., 2022b, Bietti et al., 2023, Cabannes et al., 2024b, Nichani et al., 2025]. We hypothesize that linear truth code takes advantage of the memorized factual associations: it emerges as result of the model contrasting 39th Conference on Neural Information Processing Systems (NeurIPS 2025). the internal prediction it built with the observed attribute. This results in different pattern when the two match or mismatch, and is translated into linearly separable signal. For the why, we propose the Truth Co-occurrence Hypothesis (TCH): in naturally occurring text, true statements are statistically more likely to co-occur with other true statements, and falsehoods with other falsehoods. This assumption is closely related to recent persona explanations of factual inconsistency in LMs [Li et al., 2023a, Joshi et al., 2024]: the claim that LMs learn to model certain personas in the data distribution, some truthful and some not. TCH offers very simple way to quantify the persona hypothesis and provably characterize its influence. Under the TCH, inferring latent truth variable is loss-reducing: if the model recognizes that Its well known that the moon landing was hoax is false, it can raise the probability of continuation such as and that the Earth is flat, which is likewise false. We test the truth-co-occurrence hypothesis (TCH) in the minimal transformer, with single selfattention layer, one head, and normalization layer. Training examples are four-token sequences x with subjects x, (The capital city of France; Churchills nationality) and attributes y, (Paris; British); with probability ρ, the attributes y, are both the correct attribute; with probability 1 ρ, they are replaced with random one. Under our simplified generative story, truth is identified with the attribute that is frequent in the training data for particular subject. When we train transformer LMs on such dataset, we find that after the keyvalue lookup circuit forms, gradient descent pushes hidden states toward linear separator that clusters true vs. false contexts, and the model uses it modify its confidence when predicting the attribute. Training shows two phases: rapid keyvalue acquisition followed by slower emergence of linear encoding. Although our toy model is far simpler than natural training data (see Section 6), it predicts the observed sensitivity to false context (Section 5.3), where false prefixes bias later predictions (supporting TCH), and reproduces the way normalization layers regulates confidence [Stolfo et al., 2024]. Taken together, we show that linear truth encoding can arise without any built-in semantics."
        },
        {
            "title": "2 Related work",
            "content": "A growing body of work shows that pretrained LMs linearly encode simple notion of truthconsistency with the majority of examples in the training datain both hidden states and individual MLP/attention outputs [Azaria and Mitchell, 2023, Burns et al., 2022, Li et al., 2024b, Bürger et al., 2025]. This feature is generally robust for frequent atomic facts, though its subspace can shift in the presence of negation [Marks and Tegmark, 2024] and may by biased to dataset-specific features [Orgad et al., 2025]. The encoded truth dimension is behaviorally relevant: intervening on it nudges the model toward truthful completions [Li et al., 2024b] although the models predictions sometimes do not agree with the latent encoding [Liu et al., 2023]. Yet the mechanism behind this encoding remains unclear. Extending the persona hypothesis of Li et al. [2023a], Joshi et al. [2024], Ghandeharioun et al. [2024] link truthful behavior to lexical personasfor instance, the formal, encyclopedic style typical of Wikipedia versus the more casual tone common in social-media post. We show that, given sufficient training, LMs also acquire lexicon-independent abstract truth dimension that emerges more slowly. The line of work on truth encoding is closely related to findings suggesting that models encode different aspects related to their knowledge and confidence. It was shown that it is possible to decode latent knowledge from the model Gekhman et al. [2025], and that measures of uncertainty can be decoded from hidden states [Slobodkin et al., 2023, Farquhar et al., 2024, Ferrando et al., 2025]. Our work is related to, but distinct form, works on mechanistic understanding of hallucinations [Yu et al., 2024]; while both rely on the associative memory used by the model [Geva et al., 2021, 2022a,b, Bietti et al., 2023, Cabannes et al., 2024a], we focus on the emergence of separation between true and false assertions, and come up with toy model that allows us to analyze its properties."
        },
        {
            "title": "3 The Truth Co-occurrence Hypothesis",
            "content": "We previously described the TCH, the assertion that false statements tend to co-occur. To quantify that, we use the MAVEN-FACT corpus [Li et al., 2024a], where annotators assign FactBankstyle factuality label to every event mention inside news article. After discarding all but certain judgments, each mention is labeled CERTAIN-TRUE or CERTAIN-FALSE and grouped by the document 2 in which it appears.1 We find the following: (i) the overall certain-false rate is = 0.0209; (ii) the chance that two event mentions from the same article are both certain-false is 0.0009, exceeding the independence baseline p2 = 0.00044 by factor of 2; and (iii) the clustering ratio Varobs(ˆpi)/ Varbinom = 1.23 shows 23 % extra article-to-article heterogeneity. χ2 test of independence confirms the association (χ2 = 4.17 103, 9 1049). This shows that false assertions are not sprinkled at random but tend to cluster on the same article. For language model, tracking latent truth bit is therefore loss-reducing: once page provides evidence that one statement is refuted, the conditional probability that subsequent claim is also refuted increases. This motivates the design of simple data-generating process that instantiates the hypothesis and tests whether it gives rise to truth encoding. 3.1 Data Generating Process Natural text confounds truth with stylistic cues, topic priors, and corpus frequency [Orgad et al., 2025]. Therefore, Consequently, if we probe LMs on raw text, we risk discovering features that merely track these proxies. To uncover minimal conditions that force an LM to represent truth, we build toy world in which: 1. Every subject pair has exactly one canonical attribute (ground truth). 2. small, controllable fraction of examples are corrupted by uniform noise (the attribute is replaced with another attribute). 3. importantly, the truthfulness of neighboring sequences correlates; this models the tendency of speakers to consistently be less or more truthful [Joshi et al., 2024]. Despite its simplicity, this environment reproduces the linear-separability we see in large-scale LMs (5). Data format. Each training example is sequence x with subjects x, and attributes y, A. For every there exists unique ground-truth attribute g(x) memorized by the data generator. Examples are corrupted as follows: Sample Bernoulli(ρ) once per example, such that TRUE If =1, set yi = g(x), FALSE If =0, draw each y, independently and uniformly from A. = g(x). Truth as latent variable. Because predicting the second attribute token is easier when is known, an LM can lower its language-model loss by internally inferring early in the sequence and propagating that bit forward. Without inferring , the conditional distribution over the second attribute given the prefix (x, y, x) is: Pr(cid:0)y = g(x) (cid:12) (cid:12) x, y, x(cid:1) = ρ + Pr(cid:0)y = = g(x) (cid:12) (cid:12) x, y, x(cid:1) = 1 ρ . 1 ρ , Assume the LM can memorize and (optionally) infer perfectly. Let LT be its per-token crossentropy for predicting when it does not access , and let LT be the loss when it embeds internally. Then, in the limit, LT LT = H2(ρ), the binary entropy of ρ. Hence representing single bit yields maximal benefit at ρ = 0.5, where H2 is largest (see section for complete derivation)."
        },
        {
            "title": "4 Analysis on a Toy Model",
            "content": "In this section, we study the emergence of truth directions in simplified one-layer setup with orthogonal embeddings. Empirically, we find that this minimal setup already captures the mechanism of truth direction, and leverages layer-norm to adjust confidence for the second attribute depending on truthfulness of the first one. Our empirical and theoretical analysis shows that this happens in phases, and that layer-norm is crucial to provide the relevant structure in the gradients. Furthermore, 1Data-handling details are deferred to App. A. 3 Figure 1: Visualization of the value matrix for the one-layer model at different training steps. We see that the ex ug(x) block is learned first, along with the pt block. Later the ex ex and ey uy blocks, and finally the ey eg1(y) block. such truth direction can already emerge when there are only true sequences. In appendix section D.1, we discuss how these results may be extended to non-orthogonal and to learned embeddings. Setup. Consider the following one-hot token embedding, positional embedding, and unembedding vectors in Rd with embedding dimension = 4N + 3, where [2N ] is an input or output token (input tokens are in [N ] while outputs are in [N + 1, 2N ]), and [3] position: [ez]i = 1{i = z} (1) [pt]i = 1{i = 2N + t} (2) [uz]i = 1{i = 2N + 3 + z}. (3) We consider one-layer transformer with uniform causal attention, and basic layer-norm operation. Concretely, for an input sequence z1:3 = (x, y, x) and position [3], define: (cid:32) FW (z1:t)t = ezt + pt + (cid:33) (ezs + ps) , 1 (cid:88) s=1 (4) 1:2N = [0; I2N ] R2N is projection on the unemwhere denotes the value matrix, = bedding dimensions, and N(v) = v/v is layer-norm operation. The predicted probabilities are then given by ˆp(zt+1 = z1:t) = Sβ(F (z1:t)), where Sβ denotes the softmax operation with inverse temperature β. Our experiments use β = d, due to the use of RMS norm in layer-norm over embeddings of dimension d. We assume here that x, Unif([N ]) i.i.d., and conditioned on these as well as on truth random variable Ber(ρ), we have = g(x) and = g(x) when = 1, and y, Unif([N + 1, 2N ]) otherwise. Denoting z1:4 = (x, y, x, y), the population loss then takes the form L(W ) = 3 (cid:88) t=1 Li(W ) = 3 (cid:88) t=1 Ez1:t+1 (cid:2) log Sβ(FW (z1:t))zt+1 (cid:3) . (5) Probing the mechanism and its emergence. Figure 1 shows visualization of the value matrix in our toy model, at different steps of training, with = 20, ρ = 0.8 and batch size 16. We see that clear block-structure emerges in the matrix , with different blocks arising in different phases. Some blocks show negative identity structure, while others show permutation structure according to the knowledge mapping g. Positional embeddings show more uniform patterns across unembeddings, with different signs depending on whether the next token is an input or label. In Figure 2, we show the representations at the token for examples of true and false sequences, before and after layernorm, as well as the probabilities obtained after projecting to the unembedding space and applying softmax. In the false sequence (bottom plot), we notice large spikes in the input embedding dimensions (1-20) at positions = 5 and g1(y) = 16. These do not exist in true sequences, since they cancel out. We see similar behavior on unembedding dimensions (65-84) at smaller scales. The cancellation leads to smaller norm on true sequences, which causes an amplification of the logits, and finally spiked distribution on true sequences, versus flatter one on false sequences, though we still some lower confidence spikes on g(x) and g(x) (note the y-axis scale difference). 4 Figure 2: Visualization of representations on true (top) and false (bottom) sequences. The plots show representations before (left) and after (center) layer-norm, as well as predicted probabilities (right). Structure of the value matrix . We now study construction that resembles the one observed empirically in Figure 1. Later we will provide theoretical justification for this structure and its emergence in phases by analyzing training dynamics. The leftmost column of the matrix maps ex to its corresponding label ug(x), while also subtracting ex itself: ex = α1ex + β1ug(x), (6) with α1, β1 > 0. The second column has the following symmetric behavior: ey = α2eg1(y) β2uy. Finally, the third column maps the different positional embeddings to mixtures of uniform distributions over the inputs or labels: (7) p1 = γ1( (cid:88) uy (cid:88) ux) (cid:88) p2 = γ2( (cid:88) uy ux) p3 = γ3( (cid:88) uy (cid:88) ux). (8) (9) (10) In the statements above, we assume all the coefficients α1/2, β1/2, γ1/2/3 to be positive. Linear separation and sharpening mechanism. One important consequence of the structure above is that any token that attends to both and (this could be either or x) has the following quantity in its residual stream: ζ(x, y) := (ex + ey) = α1ex + α2eg1(y) + β1ug(x) β2uy. (11) We then have ζ(x, g(x))2 = ζ(x, y)2 2α1α2 2β1β2, for = g(x). Since α1, α2, β1, β2 > 0, the norm of ζ on true sequence is always smaller than on false sequence, leading to useful feature for detecting truth (see illustration in fig. 16). Combined with the layernorm operation, this provides mechanism for sharpening the prediction of towards g(x) when the model detects true sentence, by adjusting the temperature in the softmax via inverse norm scaling. Theorem 1 (Sharpening of predictions). Suppose we have solution that satisfies Eqs. (6)-(10). Denote by := 2 + γ2(2N 2)+2α2 For any x, and = g(x), we have: 1+β2 1 9 (x, g(x), x)g(x) max k=g(x) (x, g(x), x)k (x, y, x)g(x) max k=g(x) (x, y, x)k = 0 β1 max(0, β1 β2) 3(cid:112)c + (β1 β2 + γ)2 + (β1 + γ)2 5 The proof is in section E.2. This shows that the structure of along with layer-norm provide simple mechanism to make the model more confident about its knowledge when the context is truthful. For false sequences, the zero gap comes from the fact that logits for g(x) and g(x) are tied, as we show empirically in Figure 2. This aligns with previous interpretability work on confidence neurons [Stolfo et al., 2024]. Beyond improving prediction performance, we now show that this model provides linear encoding of truth in the representations after layer-norm. Theorem 2 (Linear truth direction). Suppose we train the model in (4) as explained above, and reach solution for that satisfies Eqs. (6)-(10). Then, we have the following: 1. If the model in (4) does not contain N, then its output on the token does not admit linear separator for true and false samples. 2. If the model in (4) contains and 2α1α2 + 2β1β2 = 0, then its output on the token admits linear separation for true and false samples. Moreover, if γ1 = γ2, α1 = α2, β1 = β2 1 then the margin is at least δ = 1 2 (cid:19) . 1 (cid:18) 2 1+α2+β2 The proof is in section E.3. Theoretical analysis of training dynamics. We now study how such structure in emerges from training dynamics in simplified setting. Theorem 3 (Sequential gradient learning; informal). In simplified model with no positional embeddings, taking two gradient steps on L1 followed by one on L3, all with step-size Θ(N ), leads to the desired structure for as in Eqs. (6)-(7), up to negligible entry-wise O(1/N ) terms. See formal statement and proof in section E.1. This result shows that gradient dynamics in our model can quickly lead to the block structure observed in Figure 1, despite the non-convexity induced by normalization. In fact, the analysis reveals that the layer-norm operation is crucial here to obtain many of the desired blocks other than the ex ug(x). Interestingly, our theory shows that this structure arises even when ρ = 1, and empirically we found that both sharpening and linear separation indeed happen in this setting, demonstrating an emergent out-of-distribution generalization to false sequences. We note, however, that this may not happen in more expressive model: we empirically found that if we also train the key-query matrix with ρ = 1, the model quickly learns to focus its attention to the current token, which makes information from the context inaccessible from the residual stream. While this may improve predictions of g(x) on true sequences by removing noise in the residual stream coming from (x, y), this also results in failure to handle false sequences."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Synthetic Setting Setup. We train transformer-based LMs on the synthetic dataset described above. The model contains self-attention layers with single attention head, followed by layer normalization, with no feedforward network. See Section for more details. In this setting, in contrast to the toy model described above, we train all parameters, including the dense embeddings and the attention module.2 Each training example is concatenation of subject (x), an attribute (y), an additional, uniformly-sampled subject (x), and an additional attribute (y). The attributes y, are either sampled uniformly or taken to be the correct attributes g(x), g(x), according to the true probability ρ. In line with the Truth Co-occurrence Hypothesis, we aim to measure whether in this training setting the model is able to recover the latent truthfulness of the first sequence (verifying whether = g(x)) and use it to decrease LM loss on the second attribute y. We experiment with true-attributes rates ρ, and with {1, 2, 3} layers, and assume perfect correlation between the truthfulness of the first and second attributes (that is, = g(X) if, and only if, = g(x)). Along training, we fit logistic-regression classifiers on all hidden states to predict whether or not the sequence is false (a binary classification problem). We fit individual classifiers both the first attribute position (y), as well as on the second subject position (x), from which the 2We release the code in https://github.com/shauli-ravfogel/truth-encoding-neurips. 6 second attribute is predicted. While in training the LM we use varying true-attribute rate ρ, the linear classifiers are always trained and evaluated on balanced set, containing 50% true sequences. We report mean results over 5 runs with different random seeds. Unless specified otherwise, we present here results for = 1 and ρ = 0.99, = = 512 and dmodel = 256; results for other settings are deferred to Section D. (a) Truth classification results, second subject (b) [g(x)] on false sequences, for which = g(x)) Figure 3: Truth linear classification results alongside probability assigned by the LM to the true attribute on false sequences. 5.2 Results Two-phase dynamics. In Figure 3a we show the linear truthfulness classification AUC as function of training steps, on the second subject. for 1layer model with true-attribute probability ρ = 0.99. Additionally, we plot the probability the LM assigned to the correct attribute on false sequences (P (y = g(x) = g(x)); Figure 5b). When this probability is minimized, the model improves its loss on false sequences. In line with the toy model, we detect distinct phases in training. 1. Memorization. As can be seen in Figure 5b, memorization happens rapidlywithin the first 1000 batchesas the model converges to probability of around 1 to g(x) on both true and false examples. Indeed, the model predicts the correct attributes on over 99% of the true sequences. 2. Truth encoding. The model does learn to linearly encode the truth latent variable. This encoding emerges abruptly, after around 7,500 batches, during which the model saw around 1 million examples, relatively long after the model achieves perfect memorization. The model learns to decrease the probability it assigns to the correct attribute on the second attribute position (g(x) = y) roughly at the same time linear classification emerges. Truth circuit. We aim to understand how the linear truth subspace is being computed. While it has been empirically shown that LM linearly encode many human-interpretable concepts [Bolukbasi et al., 2016, Vargas and Cotterell, 2020, Ravfogel et al., 2022], it is not well-understood why linear representations emerge in hidden layers [Park et al., 2024, Jiang et al., 2024]. The toy model we propose allows us to empirically study the origins of the linear signal, and the way it is being used to decrease LM loss on the second attribute. The truth encoding appears in 1-layer model (classification accuracy in the input embeddings layer is at majority level). As can be seen in the first layer attention pattern in (Figure 4b), this attention head calculates an approximate mean of the embeddings of and y, after application of the V, self attention matrices, in line with the uniform attention assumed in the toy model. One key difference is that here, we learn the input embeddings. Interestingly, inspecting the PCA of the input subject and attribute tokens (Figure 4c) reveals that approximately, ex = eg(x) on the first principal component. This explains why both the true and false representations tend to cluster around the origin. Following the attention averaging, we apply RMSNorm. We find that linear classification emerges only after normalization; classification accuracy is at majority level before it. Indeed, PCA plot (Figure 4) shows that, as predicted by the toy model, the TRUE class is centered around the origin, with larger 7 (a) PCA of layer-1 representations on the token (b) Attention pattern (1-layer model) (c) PCA of input embeddings versus Figure 4: LM representations over false sequences. variance for the TRUE class than the FALSE class. Normalization induces linear separability, that is also evident in the first 2 PCA components. Additional settings. So far we have analyzed single-layer transformereither with one-hot embeddings, or with trainable dense embeddings and ρ = 0.99. Results for other configurations appear in Section D; here we outline the main trends. The patterns in Figures 3a and 5b persist across layer counts l, noise levels ρ, and corpus sizes S, A. Higher ρ delays (but does not prevent) the onset of linear separability, which still emerges at ρ = 0.999 (Figure 7a at the appendix); only the degenerate case ρ = 1.0 shows no emergence, contrary to the toy model. We discover similar structures to Figure 1 also when training with frozen dense embeddings and when learning the KV matrices instead of using fixed attention. preliminary analysis of this setting is provided in section D.1 and section E.1.1, and we leave more complete understanding for future work. With additional layers the model sometimes encodes truth in the first attribute y, then copies it to before predicting y; in other runs it reverts to the single-layer strategy where attends directly to and y. This influences whether we see linear encoding on both and x, or on alone (Figure 7b in the appendix). 5.3 Testing the TCH in Real LM The theory we specified relies on set of assumptions and architecture that do not exist in pretrained transformers (those have, for instance, MLP layers in addition to the attention layer; have multiple attention heads; and are trained primarily on natural language distributions). Below, we (i) train regular transformer models on natural language data that instantiates the truth co-occurrence hypothesis; (ii) assess to what extent aspects of the mechanism we propose exist in pretrained LLMs. 5.3.1 Instantiating the TCH in Natural Language In section 5.1, we created synthetic dataset that respects the TCH and showed that training an attention-only transformer on this data results in linear truth encoding. Here, we aim to assess whether the same thing happens when training real transformers on natural language data. Setup. We evaluate on the CounterFact dataset [Meng et al., 2022], collection of simple factual assertions spanning relations such as SPEAKSLANGUAGE and BORNIN. We select the 25 most frequent relations and, for each positive instance (x, r, a), construct negative by replacing the attribute with different attribute from the same relation. To instantiate the TCH, we form paired examples by concatenating two randomly sampled instances that share the same truth label (both true or both false). We then train small transformer with RMS normalization, 2 attention heads and single MLP module per layer, hidden size = 256, and depth {2, 5, 9} on this corpus. We use ρ = 0.99. We train on data from single relation at time, and report mean and standard deviations over 5 random relations.3 3We leave the question of generalization between relations to future work. (a) Truth classification results (b) [correct attribute] on false sequences, for which the observed attribute is not the correct one. Figure 5: Truth linear classification results alongside probability assigned by the LM to the true attribute on false sequences. Results. Across all seeds and architectural choices, the training dynamics mirror those on synthetic data: rapid memorization, followed by the emergence of linear encoding, and an increase in entropy on false sequences. In fig. 5, we show results for single relation (WORKSIN; averaged over five random seeds). By the end of training, the final hidden layer is nearly perfectly separable by the truth label, and on false sequences the probability assigned to the memorized (true) attribute declines. Notably, the 1-layer model exhibits epoch-wise double descent: classification accuracy rises early, dips, and then rises again. Across the five seeds, relations, and model sizes, memorization proceeds at roughly the same rate; the main variance lies in how quickly the probability declines on false sequences. 5.3.2 The TCH in Pretrained LLMs (a) The mechanism proposed above assumes very specific data generating process, and simplified transformer model. As such, it is not likely that the same mechanism applies to real LMs; we see the toy model as proof of concept, and aim to study more complicated models in future work. Yet, in this section, we compare the predictions following from our hypothesis with pretrained LMs in these aspects: (1) the sensitivity of the models predictions to preceding false sentences, in line with the truth co-occurrence hypothesis; (2) the behavioral relevance of the linear truth encoding in situation where sentence follows misleading false sentences. We experiment with LLAMA3-8B model [Grattafiori et al., 2024] and the CounterFact dataset (SPEAKSLANGUAGE relation). We let the model predict the first token of the last word of sentence, when it is (i) preceded by false sentences; or (ii) preceded by true sentences. In line of the hypothesis, we expect to see decrease in the probability of the correct answer. Models predictions are sensitive to preceding false sentences. The results, over 128 ntuples, are presented in Figure 6a (light bars) and are in line with our hypothesis; for instance, in the two 9 leftmost box plots, we see that preceding the sentence with two false sentences (FF) yields higher negative likelihood (smaller probability) to the correct attribute compared with when preceding it with one true sentence (TT). The difference in negative log likelihood is 1.52, corresponding to 4.55 decrease in the probability of the correct attribute. Intervention in the truth subspace. LLAMA3-8B encodes truthfulness linearly: linear classifier reaches over 95% accuracy on all middle and last layers in separating true instances from the dataset from counterfactual ones. Our theory predicts that, in the presence of misleading context, the direction that distinguishes true from false vectors actively pulls the model away from the correct answer. To test that, we intervene in the truth subspace. Following previous work on linear steering [Li et al., 2023b, Singh et al., 2024], we calculate the mean vector of the TRUE and FALSE classes in the representation space, µT and µF , and add steering vector α(µT µF ) to all representations in the same layer with the goal of increasing the probability of the correct attribute. We choose layer = 11 based on preliminary experiments that showed that classification peaks at that layer, and α = 3.0. The results, presented in Figure 6a (darker bars), show that the models tend to increase the probability of the correct attribute post-intervention, even in the presence of false context. Emergence along training See section E.4 for preliminary analysis of the emergence of truth encoding along training."
        },
        {
            "title": "6 Discussion and Limitations",
            "content": "Although our analysis was grounded in deliberately minimalist transformer, it discovers twophase dynamicrapid keyvalue memorization followed by the slower emergence of linear truth encoding. The key prerequisite appears to be the presence of (i) an associativememory circuit able to retrieve subjectattribute pairs and (ii) correlation among the truth values of adjacent clauses. While we replicate the core phenomena we witness in large LMs, we emphasize that this is one, and probably not unique, mechanism that can induce truth encoding. core advantage of the minimalist model is that it does not assume any lexical cues that help the model discern the truth latent variable. In that sense, this is more challenging setting than the previously studied one [Joshi et al., 2024], where it is assumed that true and false assertions are associated with different lexical distributions. Several core differences exist between our simplified generative story and real-world setting. Our synthetic corpus contains only one latent relation. natural extension is to sample tuples from set of heterogeneous relationsBORNIN, CAPITALOF, CURRENCYOF, . . . while maintaining correlation in the latent truth bit. Doing so forces the model to contextualize its memory: the same subject embedding must participate in multiple keyvalue slots distinguished by the relation. Real corpora have logical and semantic dependencies that go far beyond pairwise subjectattribute pairs: transitivity (A is in B is in A is in C), mutual exclusivity (isAlive vs. IsDead), and type constraints (capitalOf only applies to geopolitical entities). These constraints also greatly limit the range of plausible counterfactual variants we may see in the training data; while we assume uniform corruption for simplicity, in practice false variants of factual claims come from unique conditional distribution."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced small transformer and synthetic data-generation process that jointly suffice to yield robust linear truth subspace. Our analytical and empirical results demonstrate two-phase training dynamic: memorization followed by truth-code emergence. Unlike prior persona-based accounts, our theory does not rely on surface correlations between individual tokens and truthfulness, and points out to possible mechanism behind the emergence of linear of the truth signal as latent variable inferred by the model."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported in part through the NYU IT High Performance Computing resources, services, and staff expertise, and by the National Science Foundation (NSF) under Grant No. IIS2239862 to TL. We thank Yanai Elazar for his valuable comments on previous version of this paper."
        },
        {
            "title": "References",
            "content": "Amos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 967976, 2023. Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. Advances in Neural Information Processing Systems, 2023. Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, and Adam Kalai. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29, 2016. Lennart Bürger, Fred Hamprecht, and Boaz Nadler. Truth is universal: Robust detection of lies in llms. Advances in Neural Information Processing Systems, 37:138393138431, 2025. Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. arXiv preprint arXiv:2212.03827, 2022. Vivien Cabannes, Elvis Dohmatob, and Alberto Bietti. Scaling laws for associative memories. In International Conference on Learning Representations, 2024a. Vivien Cabannes, Berfin Simsek, and Alberto Bietti. Learning associative memories with gradient descent. In Proceedings of the 41st International Conference on Machine Learning, pages 5114 5134, 2024b. Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers in embedding space. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1612416170, 2023. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. Javier Ferrando, Oscar Balcells Obeso, Senthooran Rajamanoharan, and Neel Nanda. Do know this entity? knowledge awareness and hallucinations in language models. In The Thirteenth International Conference on Learning Representations, 2025. Zorik Gekhman, Eyal Ben David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, and Roi Reichart. Inside-out: Hidden factual knowledge in llms, 2025. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.446. URL https://aclanthology.org/2021.emnlp-main.446/. Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. Lm-debugger: An interactive tool for inspection and intervention in transformer-based In Proceedings of the 2022 Conference on Empirical Methods in Natural language models. Language Processing: System Demonstrations, pages 1221, 2022a. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2022b. Asma Ghandeharioun, Ann Yuan, Marius Guerard, Emily Reif, Michael A. Lepori, and Lucas Dixon. Whos asking? user personas and the mechanics of latent misalignment. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 11 Yibo Jiang, Goutham Rajendran, Pradeep Kumar Ravikumar, Bryon Aragam, and Victor Veitch. On the origins of linear representations in large language models. In International Conference on Machine Learning, pages 2187921911. PMLR, 2024. Nitish Joshi, Javier Rando, Abulhair Saparov, Najoung Kim, and He He. Personas as way to model truthfulness in language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 63466359, 2024. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, ICLR (Poster), 2015. Belinda Li, Alex Tamkin, Noah Goodman, and Jacob Andreas. Eliciting human preferences with language models. arXiv preprint arXiv:2310.11589, 2023a. Chunyang Li, Hao Peng, Xiaozhi Wang, Yunjia Qi, Lei Hou, Bin Xu, and Juanzi Li. MAVEN-FACT: large-scale event factuality detection dataset. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1114011158, Miami, Florida, USA, November 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.651. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from language model. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 4145141530. Curran Associates, Inc., 2023b. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. Advances in Neural Information Processing Systems, 36, 2024b. Kevin Liu, Stephen Casper, Dylan Hadfield-Menell, and Jacob Andreas. Cognitive dissonance: Why do language model outputs disagree with internal representations of truthfulness? In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. In First Conference on Language Modeling, 2024. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in Neural Information Processing Systems, 35:1735917372, 2022. Eshaan Nichani, Jason Lee, and Alberto Bietti. Understanding factual recall in transformers via associative memories. In International Conference on Learning Representations, 2025. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. LLMs know more than they show: On the intrinsic representation of LLM hallucinations. In The Thirteenth International Conference on Learning Representations, 2025. Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. In International Conference on Machine Learning, pages 3964339666. PMLR, 2024. Shauli Ravfogel, Michael Twiton, Yoav Goldberg, and Ryan Cotterell. Linear adversarial concept erasure. In International Conference on Machine Learning, pages 1840018421. PMLR, 2022. Shashwat Singh, Shauli Ravfogel, Jonathan Herzig, Roee Aharoni, Ryan Cotterell, and Ponnurangam Kumaraguru. Representation surgery: Theory and practice of affine steering. In ICML, 2024. Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido Dagan, and Shauli Ravfogel. The curious case of hallucinatory (un) answerability: Finding truths in the hidden states of over-confident large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 36073625, 2023. Alessandro Stolfo, Ben Wu, Wes Gurnee, Yonatan Belinkov, Xingyi Song, Mrinmaya Sachan, and Neel Nanda. Confidence regulation neurons in language models. In Advances in Neural Information Processing Systems, 2024. 12 Francisco Vargas and Ryan Cotterell. Exploring the linear subspace hypothesis in gender bias mitigation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 29022913, 2020. Lei Yu, Meng Cao, Jackie CK Cheung, and Yue Dong. Mechanistic understanding and mitigation of language model non-factual hallucinations. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 79437956, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.466."
        },
        {
            "title": "Appendix",
            "content": "A Detailed MAVEN-FACT Analysis Data extraction. We use the train split of MAVEN-FACT v1.0 (73,939 eventmentions drawn from 2 913 news articles).4 Each mention carries FactBank-style factuality code (CT++, CT+, CT-, CT, PS , PR , CF , U, NA, . . . ). We retain only certain judgments: certain-true = {CT++, CT+}, certain-false = {CT- -, CT-}. All other codes are discarded, leaving = 71, 274 labelled mentions. Grouping key. Mentions are grouped by their originating article ID (doc_id), giving = 2, 913 documents with at least two certain mentions (ni > 1). Let Zij {0, 1} indicate whether mention in document is certain-false. Statistics reported in the main text. Corpus certain-false rate. = 1 (cid:80) i,j Zij = 0.0209. Pairwise certain-false probability. Pr(Zj = Zk = 1 same doc) = 0.00090, where fi = (cid:80) Zij. (cid:80) (cid:80) (cid:1) (cid:0)fi 2 (cid:1) = (cid:0)ni 2 Independence baseline. p2 = 0.00044. (cid:80) Clustering ratio. Varobs(ˆpi) Varbinom = 1 (cid:80) i(ˆpi p)2 p(1 p)/ni 1 = 1.23 , with ˆpi = fi/ni. χ2 test. The 2 contingency table of {fi, ni fi} yields χ2 = 4 174 (p 91049). These figures show that certain-false events, though rare (2.1%), occur about twice as often as chance would predict when two events come from the same article, and the distribution of false rates across articles is 23 % more heterogeneous than binomial model would permitconfirming the co-occurrence signal predicted by TCH. The MAVEN-ED dataset is released with CC BY-SA 4.0 license. The MAVEN-ARG and MAVENERE are published with GPLv3 license."
        },
        {
            "title": "B Entropy incentive",
            "content": "Setup. We consider sequences (x, y, x, y) with subjects x, and attributes y, A. Let : be the ground-truth attribute map. latent bit Bernoulli(ρ) governs whether attributes are truthful (T =1) or random (T =0): = 1 : = g(x), = g(x) (deterministic); We study the optimal next-token loss for predicting given the prefix (x, y, x) under two cases: (i) the model does not access ; (ii) the model does access (and can memorize g). = 0 : y, i.i.d. Unif(A). A. Predictive distribution of without access to By the law of total probability over and the generator above, Pr(cid:0)y = g(x) (cid:12) (cid:12) x, y, x(cid:1) = Pr(T =1 x, y, x) 1 + Pr(T =0 x, y, x) 1 . (12) When we say the model is ignorant of , we mean it does not exploit any posterior signal about from the prefix; thus we use the prior Pr(T =1 x, y, x) = ρ and Pr(T =0 x, y, x) = 1 ρ. Hence Pr(cid:0)y = g(x) (cid:12) (cid:12) x, y, x(cid:1) = ρ + 1 ρ . For any specific wrong {g(x)}, Pr(cid:0)y = (cid:12) (cid:12) x, y, x(cid:1) = ρ 0 + (1 ρ) 1 = 1 ρ . 4Available at https://github.com/THU-KEG/MAVEN-FACT. (13) (14) B. Optimal per-token cross-entropy without Let α := ρ + 1ρ conditional in (13)(14), so its per-token cross-entropy equals the entropy of that distribution: . The optimal model (that does not access ) matches the true and β := 1ρ (cid:16) LT = (cid:17) α, β, . . . , β (cid:124) (cid:123)(cid:122) (cid:125) A1 times = α log α (A 1) β log β. (15) C. Optimal per-token cross-entropy with access to If the model does access (and has memorized g): = 1 : loss 0 (since = g(x) deterministically); Averaging over gives = 0 : loss log (uniform on A). LT = (1 ρ) log A. (16) D. The gap and its limit as Subtracting (16) from (15): := LT LT = α log α (A 1)β log β (1 ρ) log A. (17) Using β = 1ρ , (A 1)β log β = (1 ρ) (cid:16) 1 1 (cid:17) log(1 ρ) + (1 ρ) (cid:16) 1 1 (cid:17) log A. Plugging into (17) and simplifying, = α log α (1 ρ) (cid:16) (cid:17) 1 1 log(1 ρ) 1 ρ log A. (18) Since α = ρ + 1ρ ρ and the last term is O( log A ), we obtain the limit ρ log ρ (1 ρ) log(1 ρ) H2(ρ) (up to the log base). (19)"
        },
        {
            "title": "C Experimental Setup",
            "content": "Model. We experiment with an attention-only transformer with single attention head with post-attention LN: 0 = + // E, RV (token + positional embeddings) Q(i) = (i1)W (i) A(i) = softmax(cid:0) Q(i)K(i) , (i) A(i) = A(i)W (i) , (i) = (i1)W (i) (cid:1)V (i) Rdd , (i) = (i1)W (i) (i) = N(cid:0)X (i1) + A(i)(cid:1), = 1, . . . , Z = (l)WO + bO, WO RdV , bO RV ˆY = softmax(Z) (20) (21) // attention mix A(i) Rd (22) // single-head attention output (23) // residual + normalization (24) (25) (26) Experiments with one-hot models (section 4). The theoretical analysis is driven by experiments on models equipped with frozen, one-hot embeddings and uniform attention, the latter obtained by setting the attention-key matrix to the zero matrix. Under these conditions the columns of the attention valueoutput product KV map directly to individual vocabulary items, exposing clear block structure in the matrix  (fig. 1)  . As detailed in the main text, the vocabulary is organized so that indices 120 encode input subject embeddings, 2140 input attribute embeddings, 4144 positional embeddings, 4564 output subject embeddings, and 6584 output attribute embeddings. 15 Methodology: interpreting one-hot embeddings. Figure 2 contrasts two sequencesa correct one (top row) and an incorrect one (bottom row)by showing the final-layer activations before projecting to the logit space. The one-hot embeddings make the activation patterns in that layer interpretable. We display the activations for the raw representations (left), after layer normalization (middle), and after applying the unembedding matrix and the softmax transformation (right). Observe the differing y-axis scales: normalization substantially magnifies the component corresponding to the correct answer in the true sequence, while the effect is far less pronounced for the false sequence. The model that produced fig. 1 was trained with SGD, learning rate 1.0 and batch size 16. The output matrix was fixed to identity, and only the value matrix was learned, from zero initialization. Experiments with fully-trained models (section 5): In section 5, we train all components, including the input embeddings and the attention matrix. The model is trained for 50,000 batches of size 128 and is optimized with the Adam optimizer [Kingma and Ba, 2015] with learning weight of 1e-4 and weight decay of 1e-5. We do not include biases in the attention modules, and use RMSNorm as layer normalization. We run all experiments on 4 NVIDIA GeForce GTX 1080 GPUs. Training single model lasts up to half an hour."
        },
        {
            "title": "D Additional Experiments",
            "content": "In the main text we concentrated on single-layer model (l = 1) with true-attribute probability of ρ = 0.99. Here we extend the analysis to additional settings. Our primary focus was the linear separability at the second-subject token, x, where the model predicts the second attribute. This is the only position where the truth signal is behaviorally relevant. Nevertheless, the theory also predicts linear truth encoding at the first-attribute token y, owing to the fixed attention pattern. When the attention KV matrix is learned, however, this need not occurthe model can rely exclusively on the attention paid to and leave uninformative. The same theory further implies that linear truth direction should eventually emerge for any true-sentence rate ρ, even though the gradient magnitude (and therefore the speed of emergence) does depend on ρ. Varying the true sentence rate, ρ. In fig. 7b we vary ρ across five random seeds and measure linear separability at both token positions. As predicted, when the attention pattern is learned, separability is much stronger at the second subject than at the first attribute. The time to emergence grows as ρ increases, yet linear encoding still appears even at the extreme setting of ρ = 0.999. Developing theory that precisely predicts this ρ-dependent timing is left to future work. (a) (b) Figure 7: Dependency of linear separability on ρ. Dependency on dmodel and S. In fig. 8 we plot the linear separability at the final checkpoint, for different hidden sizes and number of facts to memorize (ρ = 0.99, = 1 are fixed). With the exception of dmodel = 32, the separability persists over the second subject for different combinations of these parameters. Additional layers. As we discuss in the main-text (section 5), in model with single self-attention layer, it is the second attribute (x) token that attends to both and y. With more layers, there are additional strategies. For instance, may attend to both and itself in the first layer, in the same way attends to both and in the theoretical 1-layer model; then, in the next layer, attends to y, 16 (a) (b) Figure 8: Dependency of linear separability on dmodel and S. Figure 9: attention patterns of 3-layer model. (a) (b) Figure 10: Linear separability across layers for 3-layer model; linear separability on the token is created after copying the signal from the token in the second layer. 17 copies the signal and create linear separation that persists the last layer. This is the mechanism that emerges in 4/5 random initializations of 3-layer model, and is clearly manifested in the attention patterns  (fig. 9)  and in the linear classification accuracy across layers  (fig. 10)  . D.1 Bridging the gap between the fully-trainable model and the toy model. Our theoretical analysis (section E) is motivated by the structured patterns that emerge in the attention kernelthe OV matrixwhen it is visualized  (fig. 1)  . To test whether comparable mechanism appears when we employ dense embeddings and allow the KV matrices to train freely (thus removing the enforced uniform attention over x, y), we train model with large hidden dimension but only small set of facts to memorize (S = 32 and dmodel = 512). We freeze the randomly-initialized dense embeddings and train all other parameters. The limited number of subjects makes the memorization patterns easier to inspect, while the high dimensionality approximates the regime of mutually orthogonal embeddings required by the theory. (a) EV OE with frozen dense embeddings (b) EV OE with trainable dense embeddings Figure 11: Visualization of the attention matrix with dense embeddings. Because the model now uses dense embeddingsso individual coordinates no longer correspond directly to vocabulary itemswe do not expect an obvious block structure in the raw OV matrix. Instead, following Dar et al. [2023], we visualize EV OE, where concatenates the input and output embedding matrices. This operation computes the pairwise similarities between embeddings as induced by the transformation. Concretely, (EV OE)ij = V, O, Ej measures how strongly the value vector elicited by symbol aligns with the output direction that scores symbol j, so every cell again describes relation between concrete symbols, exactly what the raw OV matrix showed when the embeddings were one-hot. The resulting heat-map (fig. 11a) exhibits strikingly similar pattern to that observed with frozen one-hot embeddings and fixed attention pattern, suggesting that the dense model converges to similar underlying mechanism. In contrast, when we do train the embeddings, the pattern partially disappears, as parts of the memorization can occur in the embeddings themselves (fig. 11b). In general, there is much more variability between runs and hyperparameters when training the embeddings, where some hyperparameter choices do not show pattern that is highly similar to the idealized one. With full set of = dmodel = 512 tokens, the global pattern is hard to spot at first glance. If we instead sub-sample 28 tokens, retain only their partners g(x), and then sort the rows/columns, the latent memorization re-emerges: the lower-left block collapses into clear diagonal (the previously random pattern in the leftmost lower block in fig. 11a is transformed into diagonal due to the sorting). This diagonal appears whether the embeddings are frozen or trainable (see figs. 12a and 12b). 18 (a) EV OE with frozen dense embeddings (subsampled and sorted) (b) EV OE with trainable dense embeddings (subsampled and sorted) Figure 12: Visualization of the attention matrix with dense embeddings. Figure 13: Visualization of learned embeddings and value matrix for model as in Section 4 with learned embeddings, initialized to one-hot. One possible circuit with learned embeddings. We now present one possible circuit that we found when initializing with the one-hot embeddings, in simplified architecture with uniform attention as in Section 4. We still denote ex, ey, ux, uy the one-hot embeddings as in Section 4, which only refer to the initialization in this setting with learned embeddings. After training, we may visualize the learned embeddings and interpret them as linear combinations of the initial one-hot embeddings, as shown in Figure 13. Denoting ex, ey, ux, uy the embeddings after training, the circuit we found 19 looks as follows: ex = ex eg(x) ey = ey eg1(y) (cid:88) (cid:88) ux = ux uy uy = uy + eg1(y) = (cid:88) (ug(x) ex)e (cid:88) (ey + uy)e . The approximation ex = ex eg(x), for instance, follows from the two large positive and negative spikes in the left part of fig. 13, for indices 1 and 25/36. Similar to our analysis of Section 4, we compute the quantity (ex + ey), which appears in the residual stream for both token and token x: (ex + ey) = ug(x) ex + eg(x) + ug(x) ey uy uy + eg1(y) We observe that this vanishes when = g(x), suggesting that similar mechanism as in the fixed embeddings case studied in Section 4 is at play, where layer-norm can lead to sharper predictions for true sequences, as well as provide truth direction. 20 Figure 14: Structure of the value matrix when training without positional embeddings. Figure 15: Structure of the value matrix when training with euclidean normalization."
        },
        {
            "title": "E Theoretical analysis",
            "content": "This section contains theoretical analysis and proofs for the results in Section 4. E.1 Training dynamics We now provide some theoretical insights on the training dynamics in the simple one-layer model of Section 4. We further simplify the model here by removing positional embeddings. Figure 14 shows that the model still learns the relevant blocks even without positional embeddings, though some of the uniform distributions on unembeddings are now absorbed in other blocks. The lemma below highlights the structure of the gradient for softmax classification model consisting of linear model followed by layer-norm operation. Lemma 1. Consider the model FW (x) = N(ax + bx) R2N , with N(v) = v/v, and the following cross-entropy population loss on some distribution over (x, y): L(W ) = Ex,y[ log S(FW (x))y], (27) where is the label and the softmax operation. The gradient with respect to is then given by: (cid:20) S(U N(vx))k 1{y = k} vx P(vx/vx)ukb L(W ) = Ex,y 2N (cid:88) (28) (cid:21) , k=1 21 with vx = ax + bx and where Pθ = θθ is the projection onto the tangent space at θ Sd."
        },
        {
            "title": "Let us decompose the population loss as",
            "content": "L(W ) = L1(W ) + L2(W ) + L3(W ), (29) where Lt(W ) is the next-token prediction loss for predicting zt+1 from z1:t, with z1:4 = (x, y, x, y). We show the following result. Theorem 3. Consider the following algorithm, with step-size η = N/ρ, and initialization W0 = 0: 1. Set W1 = W0 ηL1(W0) 2. Set W2 = W1 ηL1(W1) 3. Set W3 = W2 ηL3(W2) Then, we have W3 = (cid:88) x=1 (cid:0)β1ug(x) α1ex (cid:1) + (cid:88) (α2eg1(y) β2uy)e + O(1/N ), (30) where α1, α2, β1, β2 > 0 can be found in the proof, and O(1/N ) is matrix where all entries are O(1/N ). Comment: Euclidean norm vs. RMS Norm. The updates in this section are derived under Euclidean layer-norm (v) = v/v2, so the normalized scores entering the softmax are attenuated (compared to our experiments which use inverse temperature β = )) by factor of . Consequently, in the early regime with fixed unembedding and Θ(N ) competing order 1/ classes, the correct-token probability σx,g(x) is O(1/N ). In our implementation we use RMSNorm, NRMS(v) = , which is equivalent to keeping Euclidean LN but applying softmax with inverse temperature β = d. Since = Θ(N ) (here = 4N + 3), this multiplies every Euclidean ) become Θ(1). As score difference by result, in the same early regime the correct-token probability becomes Θ(1). Empirically, we see similar structures emerge in the matrix during early training when using the euclidean norm instead of the RMS norm  (fig. 15)  . ), so relative advantages that were O(1/ = Θ( = Θ( v2 Proof. Let us decompose each loss into contributions from true and false sequences, which follows from the fact that the data distribution is mixture of the two: Li(W ) = ρLT (W ) + (1 ρ)LF (W ). In the first step, we take gradient step only on the loss L1 for the prediction of the second Step 1. token at the first token x, starting from initialization W0 = 0. Recall that this model takes the form (x) = N(ex + ex), so that in the notation of Lemma 1 we have ax = bx = vx = ex and Pvx/vxuk = uk. Note that since the logits are all zero, we have S(0)k = 1 We begin with the gradient on true sequences. Multiplying eq. (28) by η and setting = g(x) gives 2N . ηLT 1 (W0) = ηEx[ug(x)e ] η 2N (cid:88) k=1 S(0)kukEx[e ] = = η η (cid:88) x=1 (cid:88) x= ug(x)e η 2N 2 2N (cid:88) (cid:88) z= x=1 uze ug(x)e + O(η/N 2). 22 On false sequences, we have ηLF 1 (W0) = ηEx[ 2N (cid:88) S(0)kuke ] + ηEx,y[uye ] k= (cid:88) 2N (cid:88) = η 2 y=N +1 x=1 = O(η/N 2), uye η 2N 2 2N (cid:88) (cid:88) z= x=1 uze using the fact that and are independent. With η = N/ρ, we obtain W1 = W0 ηL1(W0) = (cid:88) x= ug(x)e + O(1/N ). Step 2. The second step is taken at = W1 = (cid:80)N + R, with = O(1/N ). Thus, we have vx = ex + W1ex = ex + ug(x) + εx, with εx = O(1/N ) since ex is one-hot, which implies εx2 = O(1/ ). We also denote σx,k := S(U N(vx))k, which satisfies σx,k = O(1/N ) for all and k, since exp(u N(vx)) = Θ(1) for all and k. On true sequences, we have: x=1 ug(x)e η (cid:88) x=1 1 vx2 (cid:18) (cid:19) vxv vx2 2 ug(x)e η (cid:88) 2N (cid:88) x=1 k=1 σx,k vx2 (cid:18) (cid:19) vxv vx2 2 uke . ηLT 1 (W1) = Note that we have (cid:113) vx = 2 + (ex + ug(x))εx + εx2 2 = (cid:112)2 + O(1/N ) = 2 + O(1/N ). by Taylor expansion. Then, (cid:18) (cid:19) uk = vx,k := 1 vx vxv vx2 2 1 2 + O( 1 1 2 where O(1/N ) is vector with ℓ norm O(1/N ) and δk,g(x) = 1{k = g(x)} denotes the Kronecker delta. Plugging back into the gradient above, we obtain (ex + ug(x)) + O(1/N ), δk,g(x) + O( 1 ) 2 + O( 1 ) 2 ) δk,g(x) 2 uk + uk + vx = ηLT 1 (W1) = η (cid:88) (cid:18) x=1 ug(x) 1 2 (ex + ug(x)) + (cid:18) 1 (cid:19)(cid:19) η 2 (cid:88) 2N (cid:88) x=1 k=1 σx,kvx,ke = η 2N (cid:88) (ug(x) ex)e + O(η/N 2), x=1 which follows by noticing that (cid:80)2N k=1 σx,kvx,k = O(1/N ). For false sequences, we have ηLF 1 (W1) = ηEx,y[vx,ye ] η (cid:88) 2N (cid:88) x=1 k=1 σx,kvx,ke = η 2 (cid:88) 2N (cid:88) vx,ye y=N +1 x=1 = O(η/N 2). η N (cid:88) 2N (cid:88) x=1 k=1 σx,kvx,ke This again follows by noticing that 1 2N (cid:88) y=N +1 vx,y = O(1/N ) and 2N (cid:88) k=1 vx,k = O(1/N ). 23 With η = N/ρ, this yields W2 = W1 ηL1(W1) = (cid:88) t=1 (cid:0)αug(t) et (cid:1) + O(1/N ), with α = 1 + 1 2 . Step 3. The third step takes one gradient step on the loss L3 at the third token, i.e., predicting from (x, y, x). The model now takes the form (x, y, x) = N(ex + 1 3 (ex + ey + ex)), with uniform attention on the first three tokens. We get the gradient of the loss on from (28), giving5 vx,y,x = ex + 1 3 W2(ex + ey + ex) = 1 3 2 3 α 3 ex + ex ug(x) + ug(x) + εx,y,x =: vx,x + εx,y,x, α 3 with εx,y,x = O(1/N ), since it is the sum of 3 columns of the O(1/N ) term in W2. As in the second step, we have εx,y,x2 x,xεx,y,x = O(1/N ), so that by Taylor expansion we have vx,y,x = vx,x + O(1/N ) = 1 5 + 2α2 + O(1/N ) for = and vx,y,x = 3 vx,x + O(1/N ) = 1 1 + 2α2 + O(1/N ) for = x. Note that we once again have σx,y,x,k := 3 S(U N(vx,y,x))k = O(1/N ) due to the normalization. On true sequences, we have (cid:32) 2 = O(1/N ) and (cid:33) (cid:34) (cid:35) ηLT 3 (W2) = ηEx,x η 2N (cid:88) k=1 1 3vx,y,x (cid:34) Ex,x vx,y,xv x,y,x vx,y,x2 (cid:32) σx,g(x),x,k 3vx,y,x vx,y,xv x,y,x vx,y,x (cid:33) (cid:35) uk(ex + eg(x) + ex) . ug(x)(ex + eg(x) + ex) Let us first show that the error terms εx,y,x lead to negligible contributions to the gradient. Note that we have (vx,x + O( 1 ))(v vx,x2 + O( 1 x,x + O( 1 ) )) (cid:33) uk vx,y,x,k := 1 3vx,y,x (cid:32) (cid:33) uk vx,y,xv x,y,x vx,y,x2 (cid:32) = 1 3(vx,x + O( 1 (cid:18) )) vx,xvx,x vx,x (cid:19) = 1 3vx,x With εx,y,x,k = O(1/N ), where we used 1 uuT + O(1/N ) for ϵ = O(1/N ). Then, taking expectations with respect to independent x, x, it is easy to check that uk + εx,y,x,k, a+ϵ = Ex,x[εx,g(x),x,g(x)(ex + eg(x) + ex)] = O(1/N 2) Ex,x[σx,g(x),x,kεx,g(x),x,k(ex + eg(x) + ex)] = O(1/N 3). The gradient update can then be rewritten as + O(ϵ) for > 0, and (u + ϵ)(uT + ϵ) = ηLT 3 (W2) = ηEx,x (cid:34) (cid:32) 1 3vx,x (cid:34) σx,g(x),x,k 3vx,x vx,xv x,x vx,x2 (cid:32) (cid:33) (cid:35) ug(x)(ex + eg(x) + ex) vx,xv x,x vx,x2 (cid:33) (cid:35) uk(ex + eg(x) + ex) 2N (cid:88) η Ex,x k=1 + O(η/N 2) (31) (32) (33) 5We use the fact W2et = αug(t) et + O(1/N ) for and O(1/N ) otherwise."
        },
        {
            "title": "We now check that",
            "content": "the second term is also of order O(η/N 2). (cid:18) (cid:19) Indeed, the prois sparse with rows of bounded ℓ1 norm, and we jector matrix have (cid:80) 1 3vx,x vx,x x,x vx,x 2 σx,g(x),x,kuk = O(1/N ), so that 2N (cid:88) k= σx,g(x),x,k 3vx,x (cid:32) (cid:33) vx,xv x,x vx,x2 uk =: ζx,x = O(1/N ), for all x, x. We then have (cid:34) Ex,x 2N (cid:88) k=1 σx,g(x),x,k 3vx,x (cid:32) (cid:33) vx,xv x,x vx,x2 = Ex,x[ζx,x(ex + eg(x) + ex)] uk(ex + eg(x) + ex) (cid:35) ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) x=1 ζx,xe +"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) x=1 ζx,xe g(x) +"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) x=1 ζx,xe = O(1/N 2). For the first term (31), we have (cid:34) ηEx,x 1 3vx,x (cid:20) (cid:32) vx,xv x,x vx,x2 (cid:21) = ηEx,x 1 3vx,x (cid:33) (cid:35) ug(x)(ex + eg(x) + ex) ug(x)e + O(η/N 2) ηEx,x (cid:20) α(1 + δg(x),g(x)) 9vx,x3 vx,x(ex + eg(x) + ex) (cid:21) ug(x)e ηEx,x (cid:2)γx,xvx,x(ex + eg(x) + ex)(cid:3) + O(η/N 2), = ηβ1 (cid:88) x=1 with β1 = Ex[ 1 3vx,1 ] and γx,x = α(1 + δg(x),g(x)) 9vx,x3 . In () we used (i) that Ex,x[ of and x; and (ii) the fact that uT orthogonality. 1 3vx,x ug(x)(ex + eg(x))] = O(1/N 2) thanks to the independence 3 (1 + δg(x),g(x)) by definition of vx,x and thanks to g(x)vx,x = α We have ηEx,x (cid:2)γx,xvx,x(ex + eg(x) + ex)(cid:3) = ηEx[Ex[γx,xvx,xx](ex + eg(x))] ηEx[Ex[γx,xvx,xx]e x] = ηβ2 (cid:88) (ex αug(x))(ex + eg(x)) x=1 ηβ2 (cid:88) (2ex + αug(x))e + O(η/N 2) x= = ηβ2 (cid:88) x=1 exe + ηβ2 2N (cid:88) (eg1(y) αuy)e + O(η/N 2), y=N +1 with β2 = 1 3 Ex[γ1,x] = 1 3 Ex[γx,1] = 1 3N γ1,1 + 1 3N γ1,2. In () we condition on and decompose vx,x = A(x) + B(x), where A(x) is independent of x; then linearity gives Ex[γx,xvx,x x] = A(x) Ex[γx,x] + Ex[γx,xB(x)]. By permutation symmetry of labels, Ex[γx,x] = Ez[γ1,z] = 3β2, while the B(x) part averages over uniformly random index and is O(1/N ); the same holds with and swapped for the second bracket. Substituting these two conditionals into the split expectation, replacing outer expectations by uniform 25 sums, and renaming the dummy index yields the displayed () line. In () we perform change of variables = g(x). We have thus shown ηLT 3 (W2) = η (cid:88) (β1ug(x) β2ex)e + x=1 ηβ2 2N (cid:88) (eg1(y) αuy)e + O(η/N 2). (34) y=N + For false sequences, it can be checked that ηLF N/ρ yields 3 (W2) = O(η/N 2). Thus, taking step-size η = W3 = W2 ηL3(W2) = (α + β1) (cid:88) x= ug(x)e (1 + β2) (cid:88) x=1 exe + β 2N (cid:88) (eg1 (y) αuy)e + O(1/N ). y=N +1 E.1.1 Learning (positional) attention. We now turn to learning the keyquery matrix under positional attention, assuming that the value matrix has already been learned with the structure described above. Specifically, we show that the gradient of the keyquery matrix on true sequences drives positional attention to focus on the token, effectively causing the model to ignore the initial (x, y) pair. This observation may account for the absence of emergence at ρ = 1, although it does not explain why emergence still occurs when attention is trainable and ρ < 1. For this part, assume simple architecture of the following form for the prediction of the fourth token logits given the first three tokens: FWKQ (z1:3) = 3 (cid:88) t=1 WKQp3) exp(p t=1 exp(p WKQp3) (cid:80)3 WV ezt, where z1:3 = (ex, ey, ex), and p1:3 are the positional embeddings defined in (2). We assume WV fixed to the the structure in (6)-(7), with β1 = β2 =: β for simplicity. We consider the following population loss for WKQ on the last token for true sequences: L(WKQ) = Ex,x[ℓ(g(x), FWKQ (x, g(x), x))]. Then, the negative gradient direction at WKQ = 0 is given by L(WKQ) = 1 3 3 (cid:88) 2N (cid:88) t= k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))u WV ezt(pt p1:3)p 3 ], (35) where we denote z1:3 = (x, g(x), x) and p1:3 = 1 3 (p1 + p2 + p3), and ˆp are probability predictions at WKQ = 0, which we assume satisfy the following, given the assumed structure on WV , for some small ϵ:6 ˆp(kx, x) = (1 ϵ)/2, 1 ϵ, O(1/N ), if {g(x), g(x)} and = x, if = g(x) and = x, o/w. Let us now write the update in (35) as L(WKQ) = 1 3 3 (cid:88) t=1 wt(pt p1:3)p 3 , 6This requires that we the early phase is run for long enough so that β is large enough, say O(log ). and study the values of the wt. For = 1, we have w1 = 2N (cid:88) k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))u WV ex] 2N (cid:88) k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))1{g(x) = k}] 2N (cid:88) Ex[1{g(x) = k}]Ex[1{g(x) = k}] βEx,x[ˆp(g(x)x, x)] = β = β k=N +1 β β For = 2, we have = (1 ϵ) β(N 2 ) 2 1 ϵ 2 β 1 ϵ 2 + O(β/N ). w2 = 2N (cid:88) k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))u WV eg(x)] = β 2N (cid:88) k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))1{x = k}] = 0 + βEx,x[ˆp(xx, x)] = O(β/N ). For = 3, we have w3 = 2N (cid:88) k=1 Ex,x[(1{g(x) = k} ˆp(kx, x))u WV ex] = β 2N (cid:88) k= Ex,x[(1{g(x) = k} ˆp(kx, x))1{g(x) = k}] = β Ex,x[ˆp(g(x)x, x)] β β 1 ϵ 2 β Thus, when β, we have w3 w1, w3 w2 β/2 + O(β/N ). Taking 1 KQ = ηL, the gap in attention logits between = 3 and = 1, 2 is of order ηβ/6, so that for η large enough, the attention mostly focuses on the third token x. E.2 Proof of Theorem 1 Suppose we are given (x, y, x), where we assume for simplicity that = and g(x) = y. Denote by fW (z1:t) the output of the model in (4) before applying the LN and the unembedding layer. Then, we have that: fW (x, y, x) = ex + p3 + 1 3 γ (cid:32) (cid:88) uy (cid:88) (cid:33) ux + + 1 3 (cid:0)α1ex + β1ug(x) + α2eg1(y) β2uy α1ex + β1ug(x) (cid:1) (36) Denote by c1 := 2 + γ2(2N 2)+2α2 = g(x) we have that: 9 1+β2 and c2 := 2 + γ2(2N 3)+2α2 1+β2 1 9 . for true sample where fW (x, g(x), x)2 = + (β1 β2 + γ)2 + (β1 + γ)2 . 27 Hence, after applying the LN and unembedding layer we have that: (FW (x, g(x), x))g(x) = β1 + γ 3(cid:112)c1 + (β1 β2 + γ)2 + (β1 + γ)2 max y=g(x) (FW (x, g(x), x))y = γ + max(0, β1 β2) 3(cid:112)c1 + (β1 β2 + γ)2 + (β1 + γ)2 For false sample where = g(x) we have that: fW (x, g(x), x)2 = c2 + 2(β1 + γ)2 + (β2 + γ)2 . Hence, after applying the LN and unembedding layer we have that: (FW (x, y, x))g(x) = β1 + γ 3(cid:112)c2 + 2(β1 + γ)2 + (β2 + γ) max y=g(x) (FW (x, y, x))y = β1 + γ 3(cid:112)c2 + 2(β1 + γ)2 + (β2 + γ)2 . Plugging in these terms finishes the proof. E.3 Proof of Theorem Figure 16: Illustration for LN-induced linear separability. Proof. We first describe the output of the model in (4) before applying LN. Denote by vT , vF R4N +3 these outputs for true and false samples respectively. Recall that true sample (x, y) is when = g(x) and false otherwise. Then, we have that: vT = ey + p2 + vF = ey + p2 + 1 2 1 (cid:32) (α2 α1)ex + (β1 β2)uy + (γ1 γ2) (cid:32) (cid:88) (cid:32) (cid:33)(cid:33) uy (cid:88) ux α1ex + α2eg1(y) + β1ug(x) β2uy + (γ1 γ2) (cid:32) (cid:88) uy (cid:88) ux x (37) (cid:33)(cid:33) (38) We will first show that without adding the samples above cannot be separated for general and y. Assume otherwise, that there exists linear separator = and bias term such that w, vT 0 and w, vF < 0 for every true or false sample w1 w2 w3 w4 w5 with w1, . . . , w4 RN , w5 28 respectively. We slightly abuse notation and write w1, ex as multiplying w2 by ey, w3 by ux, w4 by uy and w5 by pt. (cid:19) (cid:28)(cid:18) w1 03N + (cid:29) , ex , and similarly when := (cid:42) 1 (γ1 γ2) (cid:32) (cid:88) uy (cid:88) (cid:33) (cid:43) ux , w3 + w4 + w5, p2 the terms in the inner products that are independent of the sample. Then, using the linear separator on these four samples we have: (α2 α1) exi, w1 + eyiw2 + (β1 β2) uyi , w4 + (cid:11) + (cid:11) + (cid:10)eyj w2 (α2 α1) (cid:10)exj , w1 (cid:10)exj , w1 (cid:11) β2 uyi, w4 + α2 exi, w1 α1 (cid:10)exj , w1 (cid:10)uyj , w4 (cid:11) + . α2 Adding up (41) and (42) we have that: (cid:11) + (β1 β2) (cid:10)uyj , w4 (cid:10)uyj , w4 (cid:11) + β1 uyi, w4 β2 (cid:11) α1 exi, w1 + (cid:10)eyj , w2 (cid:11) + eyi, w2 + β1 2b 2c (α2 α1) (cid:10)exj , (cid:11) + (β1 β2) (cid:10)uyj , w4 (cid:11) + (cid:10)eyj w2 + (α2 α1) exi, w1 + eyiw2 + (β1 β2) uyi, w4 , (cid:11) + (39) (40) (41) (42) (43) (44) which is contradiction to (39) and (40). This means that there is no linear separator, regardless of the values of the parameters, which proves the first item. Assume there is layer normalization after the prediction as in (4). This means that the output of the model is . Consider the linear predictor = p2, and bias term that will be determined later. Then, the output of the linear predictor is exactly w, = 1 . We will now calculate the norm of both true and false samples. For true sample (x, g(x)) we have that: vT 2 = 2 + (α2 α1)2 + (γ1 γ2)2 (2N 1) + (γ1 γ2 + β1 β2)2 . (45) For negative sample (x, y) with g(x) = we have: vF 2 = 2 + α2 1 + α2 2 + (γ1 γ2)2 (2N 2) + (γ1 γ2 + β1)2 + (γ1 γ2 β2)2 . (46) There exists linear separator as long as non-zero, this is equivalent to vT 2 = vF 2. By the above calculation, we have that: vT = 0. Since the vectors vT and vF are both vF 1 1 vF 2 vT 2 = α2 1 + α2 = 2α1α2 + 2β1β2 . 2 (α1 α2)2 (γ1 γ2)2 + (γ1 γ2 + β1)2 + (γ1 γ2 β2)2 (γ1 γ2 + β1 β2)2 This shows that if 2α1α2 + 2β1β2 = 0 then we have linear separation between true and false samples. Further assuming that α1 = α2, β1 = β2, γ1 = γ2 we have that vT 2 = 2 and vF 2 = 2 + 2α2 + 2β2. To find the optimal margin for this predictor we pick: = 1 2 (cid:18) vT 1 vF (cid:19) = 1 2 (cid:32) 1 1 (cid:112)1 + α2 + β2 (cid:33) . We will now prove that there is linear separation after predicting the token. Using the output of the model as in (4) we get: vT = + vF = + 1 3 1 3 (cid:0)(α2 α1)ex + (β1 β2)uy α1ex + β1ug(x) (cid:1) (cid:0)α1ex + α2ug1(y) + β1ug(x) β2uy + α1ex + β1ug(x) (cid:1) , (47) (48) 29 where = ex + p3 + ˆγ 3 (cid:16)(cid:80) uy (cid:80) ux (cid:17) . We can now calculate: vT 2 = 2 + 1 9 (cid:0)(α2 α1)2 + (β1 β2 + γ)2 + α2 1 + (β1 + γ)2 + (2N 2)γ2(cid:1) vF 2 = 2 + 1 (cid:0)2α2 1 + α2 2 + 2(β1 + γ)2 + (γ β2)2 + (2N 3)γ2(cid:1) . (49) (50) We now have that: vF 2 vT 2 = = 1 9 2 9 (cid:0)α2 1 + α2 2 + (β1 + γ)2 + (γ β2)2 (α2 α1)2 (β1 β2 + γ)2 γ2(cid:1) (α1α2 + β1β2) . By similar argument to the previous case, if α1α2 + β1β2 = 0 then there is linear separation between true and false samples. Further assuming that α1 = α2, β1 = β2 and γ = 0, to find the optimal margin for the predictor we pick: (cid:19) α2 + β2 = 1 2 (cid:18) 1 vT 1 vF = (cid:113) 4 + 8 9 (α2 + β2) + 1 27 (α2 + β2)2 . 30 E.4 Evaluating checkpoints of real LM To test whether the two-phase dynamics also appear in large model trained on open-web data, we analyzed the Pythia-6.9B training checkpoints released by EleutherAI. Using the COUNTERFACT dataset we construct each input by concatenating = 4 factual statements whose preceding context is either entirely true or entirely false, mirroring our previous setup. For every checkpoint we measure three signals on the final statement: Memorization: percentage of cases where greedy decoding succeeds in completing the correct token. Uncertainty: entropy of the models full-vocabulary distribution for predicting the last token; we record the difference between true and false context. Linear separability: accuracy of linear probe trained to classify the truth value of the surrounding context. Table 1: Pythia-6.9B metrics across training steps. step memorization 0.000 0.000 0.006 0.242 0.435 0.547 0.655 0.727 0.772 0.822 0.835 0.849 0.842 0.858 0.875 0.001 0.006 0.005 0.219 0.217 0.286 0.355 0.329 0.421 0.419 0.479 0.485 0.536 0.565 0.518 0 512 1000 3000 5000 10000 20000 40000 60000 80000 100000 110000 120000 130000 143000 probe AUC 0.383 0.435 0.467 0.587 0.648 0.667 0.754 0.759 0.802 0.799 0.818 0.835 0.835 0.783 0. Notes. denotes the entropy gap between matched prompt pairs presented with false versus true context. The memorization rate is the share of instances in which the models output distribution places the correct continuation token at the top-1 position. Probe AUC is the ROC-AUC of linear classifier trained to predict the surrounding contexts truth value from model activations. Findings. Early training ( 1k steps). 0; the model memorizes indiscriminately. Mid training (3k80k). Memorization jumps, then plateaus, while and probe accuracy climb steadily. Late training ( 80k). Entropy separation continues to widen even after memorization saturates, mirroring Phase 2 but over longer horizon. Overall, this echoes the two-phase pattern observed in simpler experiments: an initial jump in memorization followed by slower, steadier increase in entropy separation. Differences remain: the second phase is more gradual, and classification and entropy increase even before memorization stabilizes. We hypothesize this stems from continual exposure to new facts during training, unlike our idealized setup where all facts are seen in single gradient step. The modest terminal memorization and classification scores are consistent with reports that the Pythia series is under-trained relative to its capacity. Finally, in Pythia-6.9B we do not find evidence that layer normalization itself induces linear separability; rather, linearly decodable truth signal emerges gradually with depth across many layers. Our aim was to advance one plausible mechanism for the phenomenon observed in pretrained LMs, not to claim uniqueness. Given the models deeper architecturewith numerous layers and MLP blocksand the richness of natural-language data, additional or distinct mechanisms are likely at play. systematic study of these mechanisms is an important direction for future work."
        }
    ],
    "affiliations": [
        "Flatiron Institute",
        "New York University"
    ]
}