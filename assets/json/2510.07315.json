{
    "paper_title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
    "authors": [
        "Ming Zhong",
        "Xiang Zhou",
        "Ting-Yun Chang",
        "Qingze Wang",
        "Nan Xu",
        "Xiance Si",
        "Dan Garrette",
        "Shyam Upadhyay",
        "Jeremiah Liu",
        "Jiawei Han",
        "Benoit Schillings",
        "Jiao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models' code instruction following capabilities with measurable signals, we present VeriCode, a taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, a testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, a composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing a concrete path for benchmarking and developing models that better align with user preferences in coding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 5 1 3 7 0 . 0 1 5 2 : r Vibe Checker: Aligning Code Evaluation with Human Preference Ming Zhong, Xiang Zhou, Ting-Yun Chang, Qingze Wang, Nan Xu, Xiance Si, Dan Garrette, Shyam Upadhyay, Jeremiah Liu, Jiawei Han, Benoit Schillings and Jiao Sun 2025-10-08 Large Language Models (LLMs) have catalyzed vibe coding, where users leverage LLMs to generate and iteratively refine code through natural language interactions until it passes their vibe check. Vibe check is tied to real-world human preference and goes beyond functionality: the solution should feel right, read cleanly, preserve intent, and remain correct. However, current code evaluation remains anchored to pass@k and captures only functional correctness, overlooking the non-functional instructions that users routinely apply. In this paper, we hypothesize that instruction following is the missing piece underlying vibe check that represents human preference in coding besides functional correctness. To quantify models code instruction following capabilities with measurable signals, we present VeriCode, taxonomy of 30 verifiable code instructions together with corresponding deterministic verifiers. We use the taxonomy to augment established evaluation suites, resulting in Vibe Checker, testbed to assess both code instruction following and functional correctness. Upon evaluating 31 leading LLMs, we show that even the strongest models struggle to comply with multiple instructions and exhibit clear functional regression. Most importantly, composite score of functional correctness and instruction following correlates the best with human preference, with the latter emerging as the primary differentiator on real-world programming tasks. Our work identifies core factors of the vibe check, providing concrete path for benchmarking and developing models that better align with user preferences in coding. 1. Introduction Large Language Models (LLMs) have reshaped how humans write code, fostering workflow termed vibe coding (Karpathy, 2025; Willison, 2025). In this paradigm, AIs role shifts from one-shot code completion tool for developers to an interactive collaborator for broader audience, including users with limited coding experience. Through multi-turn natural language interactions, users can create and refine solutions from scratch, requiring the model to maintain context, adapt to evolving requirements, and iteratively improve the code until it meets their needs (Ross et al., 2023; Yang et al., 2023). The users final accept/reject choice serves as real-time evaluation: what we call the vibe check, subjective preference typically based on whether the solution feels right, reads cleanly, avoids obvious issues or anti-patterns, and preserves intent and correct functionality. This collaborative workflow, popularized by tools such as Copilot1 and Cursor2, is rapidly becoming standard practice in modern software development (Peng et al., 2023; Stack Overflow, 2025). Despite the shift toward vibe coding, existing code evaluation remains anchored to functional correctness, typically measured as pass@k (Austin et al., 2021; Chen et al., 2021; Jimenez et al., 2024). These metrics indicate whether code passes unit tests but abstract away non-functional expectations that users apply when selecting response, including adherence to project conventions, documentation clarity, minimal and targeted edits, and preservation of prior intent across interactions. This disconnection is evident in platforms such as Copilot Arena (Chi et al., 2025), large-scale vibe-checking scenario where human programmers choose preferred candidate snippets. Strikingly, 1https://github.com/features/copilot 2https://cursor.com Corresponding author(s): mingz5@illinois.edu and jiaosun@google.com Ming Zhong and Jiawei Han (UIUC); Ting-Yun Chang (USC). Work done while at Google DeepMind. 2025 Google DeepMind. All rights reserved Vibe Checker: Aligning Code Evaluation with Human Preference Figure 1 Vibe check goes beyond functionality, requiring code to satisfy non-functional instructions such as coding style and logic patterns, which are also key factors of human preference. rankings of code LLMs from Copilot Arena exhibit weak or negative correlations with functional scores on popular benchmarks. Moreover, pass@k remains dominant verifiable reward signal in RLVR training (Da et al., 2025; DeepSeek-AI, 2025), steering optimization toward an incomplete notion of code quality. Consequently, models can achieve high leaderboard scores yet fail the vibe check in practice, producing code that is technically correct but misaligned with user preferences. To bridge this gap, we hypothesize that the non-functional signals emerging from interactions are an important, yet under-measured, component of the vibe check. We first introduce VeriCode, taxonomy of verifiable code instructions designed to capture what users routinely screen for during code selection. Grounded in hundreds of rules from industrial linters and style guides, we perform manual curation and automated filtering to distill core set of 30 instructions across five categories. Each instruction is paired with verifier implemented using standard linters and abstract syntax tree analysis. These verifiers yield binary pass or fail score, enabling reliable automatic evaluation while also providing verifiable and scalable reward source for model training. Building on VeriCode, we augment established benchmarks, BigCodeBench (Zhuo et al., 2025) and LiveCodeBench (Jain et al., 2025), with these verifiable instructions to better simulate real-world interactions. We refer to the augmented variants as BigVibeBench and LiveVibeBench. For each user query, an LLM-driven selector chooses relevant and non-conflicting subset of instructions from our taxonomy to add as explicit constraints. Functional unit tests together with our instruction verifiers constitute unified testbed, Vibe Checker, which measures both functional correctness and instruction following (IF). Using this testbed, we evaluate 31 LLMs from 10 model families in two realistic settings: single-turn generation, in which the model must satisfy all constraints in one pass, and multi-turn editing, in which constraints are introduced sequentially while preserving prior intent. This setup allows us to study both dimensions across interaction contexts. Our analysis on Vibe Checker testbed yields several key insights into the code evaluation: Non-functional instructions cause notable functional regression. Although the added instructions do not target functionality, pass@1 decreases across all models. Under five instructions, average pass@1 drops by 5.85% and 6.61% on the two augmented benchmarks (Section 4.2). Following multiple instructions remains challenging for LLMs. Even the best performing model reaches only 46.75% and 40.95% success rate under five instructions on BigVibeBench and LiveVibeBench (Section 4.3). Models also exhibit position bias for instruction following, with mid-position instructions followed less reliably than those at the beginning or end (Section 4.4). Single-turn vs. multi-turn interactions alter LLM behavior. Under the same tasks, single-turn generation better preserves functionality but follows fewer instructions, whereas multi-turn 2 Vibe Checker: Aligning Code Evaluation with Human Preference editing achieves higher IF at the cost of more functional regressions (Sections 4.2 and 4.3). Human preference reflects mixture of functional correctness and instruction following. On the coding subset of LMArena (Chiang et al., 2024), composite of functional correctness and our IF score correlates better with model ratings than either metric alone, with IF emerging as the key differentiator among advanced models on the real-world programming tasks (Section 4.5). In summary, this work establishes IF as an essential, yet overlooked, component of code evaluation. Our VeriCode taxonomy and Vibe Checker testbed offer concrete path to benchmark and develop models against more human-aligned notion of code quality beyond functionality. 2. VeriCode: Taxonomy of Verifiable Code Instructions To quantify the IF capability, we first construct VeriCode, taxonomy of verifiable code instructions. This section presents its design principles, construction process, and resulting structure. 2.1. Design Principles We design VeriCode around four core principles to ensure it is rigorous, relevant, and useful: Verifiability. Each instruction is paired with an automated, deterministic verifier that returns binary pass/fail signal, enabling objective and scalable evaluation. Practice Grounding. Instructions reflect common developer expectations and conventions, drawing on widely used standards rather than synthetic or adversarial constraints. Comprehensive Coverage. The set spans key non-functional aspects, including coding style, logic patterns, documentation, error handling, and API or library constraints. Difficulty. Instructions are curated to be meaningfully challenging and diagnostic, ensuring that recent advanced LLMs exhibit imperfect adherence. 2.2. Taxonomy Construction Process We carefully curate VeriCode in three stages: sourcing candidate pool, performing multi-stage filtering, and finalizing the set with expert review and verifier implementation. Candidate Pool Sourcing. We source our initial candidate pool from Ruff, an industry-standard Python linter that aggregates more than 800 rules drawn from popular tools3. This provides high-coverage inventory of practices that users routinely follow and check. Static linting, however, inspects only code and cannot evaluate instructions that target the entire response (e.g., append JSON explanation after the code block). To close this gap, we add set of instructions focusing on documentation outside the code blocks, extending coverage beyond what static analysis can capture. Scope and Relevance Filtering. The initial pool is first filtered for scope and relevance. We apply top-down consolidation to address rule overlap, prioritizing broader instructions over their more specific subsets. This stage ensures that each instruction is broadly applicable across common coding tasks and not confined to niche scenarios. Difficulty Filtering. We then screen the remaining candidates for difficulty. Using Gemini 2.5 Flash (Gemini Team, 2025) on challenging test set, BigCodeBench-Hard (Zhuo et al., 2025), we measure instruction following rate alongside functional correctness at pass@1. Any instruction with success rate above 90% and no degradation in pass@1 is removed. Borderline cases are flagged for manual review. This step focuses on non-trivial constraints that challenge advanced LLMs. 3https://docs.astral.sh/ruff/rules 3 Vibe Checker: Aligning Code Evaluation with Human Preference"
        },
        {
            "title": "Parameter",
            "content": "Coding Style & Conventions Logic & Code Patterns Write code ensuring all lines are no longer than {line_length} characters. E501 Rule line_length (int) Recommended: 79 (classic), 88 (modern) Ensure each function has at most {max_branches} branches. PLR0912 Rule max_branches (int) Recommended: 24 Documentation & Commenting Document your code using the {convention} docstring format."
        },
        {
            "title": "D Rule",
            "content": "convention (str) Supported: Google, NumPy, PEP 257 Error Handling & Management Library & API Constraints Replace all aliases with the canonical OSError exception. Replace all os, os.path, glob, and open with their pathlib equivalents. UP024 Rule"
        },
        {
            "title": "None",
            "content": "Table 1 Refined examples from VeriCode taxonomy. Each instruction maps to verifiable linter rule and includes tunable parameters where applicable. Full versions are provided in Appendix B.2. Review and Verifier Implementation. The final instruction set is manually reviewed by domain experts on the author team with coding-research experience to ensure clarity and real-world relevance. For verification, we prioritize linter-backed checks when available and implement deterministic tests using Abstract Syntax Tree (AST) analysis and regular expressions when no direct rule exists. All verifiers share common interface: testing function that returns binary pass or fail, enabling scalable evaluation and reproducibility. 2.3. Resulting VeriCode Taxonomy The multi-stage construction process yields our final verifiable taxonomy VeriCode4. Taxonomy Structure. The final set contains 30 instructions organized into five categories: Coding Style & Conventions (9), Logic & Code Patterns (9), Documentation & Commenting (6), Error Handling & Exception Management (4), and Library & API Constraints (2). The taxonomy is organized hierarchically: the root represents the overall concept of verifiable code instructions, the five categories form the top-level nodes, and the 30 individual instructions are the leaf nodes. Our current instantiation focuses on Python, the dominant language in code evaluation, but the framework is language-agnostic and can be applied to other languages using standard linters. Instruction Schema. Each instruction specifies five necessary elements: 1) category, 2) description, 3) distinct prompts for both single-turn generation and multi-turn editing, 4) configurable parameters with recommended or supported values, and 5) the verification code that returns binary score. full version of the instructions is available in Appendix B.2. key feature of our taxonomy is its extensibility, which is achieved through the Parameters field. As illustrated in Table 1, parameters such as line_length, max_branches, or documentation conventions allow single instruction to generate multiple variants with different difficulty levels. This flexibility enables our set of 30 core instructions to be programmatically expanded into hundreds of distinct and checkable constraints, providing scalable framework for future research. 4We will publicly release the taxonomy together with the corresponding verifiers to support community use. 4 Vibe Checker: Aligning Code Evaluation with Human Preference 3. Vibe Checker: New Testbed for Code Evaluation Building on proposed VeriCode, we introduce Vibe Checker testbed that extends standard code benchmarks with explicit, verifiable instructions. It evaluates models under both singleand multi-turn protocols, measuring functional correctness as well as instruction following capabilities. 3.1. Benchmark Augmentation We ground our evaluation in established benchmarks, which allows us to leverage their unit tests to consistently measure functional correctness and situate our analysis within widely used evaluation suites. Concretely, we construct two augmented variants: BigVibeBench, adapted from BigCodeBench to cover real-world programming tasks. LiveVibeBench, adapted from LiveCodeBench to cover algorithmic/contest problems. This combination ensures that our evaluation covers diverse range of coding challenges. Our augmentation process involves the following stages: Instruction Selection. For each user query, we randomly permute the full set of 30 taxonomy instructions to form an ordered list. An LLM-based selector then scans this permuted list once, deciding whether to keep or discard each instruction based on two criteria: 1) Relevance: the instruction must pertain to the query and plausibly influence the implementation, and 2) Non-conflict: the instruction must not contradict any instruction already selected earlier in the pass. The accepted instructions, in this permuted order, constitute the constraint set used to evaluate all models. Parameter Selection and Validation. Once the instructions are selected, we prompt an LLM to assign specific parameter values to each one. To guide this generation, the prompt includes the supported keys, types, ranges, and recommended values in our taxonomy, as well as the context of the user query, aiming for parameters that are both achievable and challenging. Finally, the generated parameters undergo rule-based validation step: any parameter keys not explicitly defined for that instruction are removed, and any invalid values are reverted to predefined defaults. Both Gemini 2.5 Pro (Gemini Team, 2025) and Claude 4 Opus (Anthropic, 2025) are tested as selectors in our augmentation pipeline, yielding similar instruction-category distributions. The final benchmark is augmented by Claude 4 Opus, chosen for its lower invalid-parameter rate (0.96% vs. 2.47% for Gemini 2.5 Pro). The resulting distributions show that instructions for Coding Logic, Coding Style, and Documentation are most prevalent, with Coding Logic being particularly frequent in the algorithm-focused LiveCodeBench (see Figure 12 in the Appendix for full breakdown). 3.2. Evaluation Protocol Our evaluation protocol, illustrated in Figure 2, mirrors real-world usage by providing singleand multi-turn interactive settings with two evaluation metrics. Interactive Settings. We use two settings that differ in how instructions are presented: Single-Turn Generation presents all selected instructions after the original query within one prompt. The model returns single implementation. Multi-Turn Editing first elicits an initial implementation in response to the original query, then reveals the selected instructions one at time. At each round, the model sees the full interaction history and updates the solution. The code from the last round is used for evaluation. Evaluation Metrics. For both settings, we evaluate the code on two axes: 5 Vibe Checker: Aligning Code Evaluation with Human Preference Figure 2 Our evaluation protocol simulates two real-world interaction patterns: single-turn generation, where all instructions are given upfront in one prompt, and multi-turn editing, where instructions are introduced sequentially to refine solution. Both are measured for functionality and IF. Functionality: We measure functional correctness with unit tests and report functional regression FRð‘˜ from adding ð‘˜ instructions. Let ð‘†ð‘˜ denote the functional score (typically pass@1) after injecting ð‘˜ instructions, with ð‘†0 the score on the original prompt. The rate is calculated as: FRð‘˜ = ð‘†0 ð‘†ð‘˜ ð‘†0 . Instruction Following: We report IF at two granularities. For task with ð‘˜ instructions, let ð¼ ð‘— {0, 1} indicate whether instruction ð‘— passes its verifier. The instruction-level score averages per-instruction passes, and the task-level score requires all passes: IFinstruction = 1 ð‘˜ ð‘˜ ð‘—=1 ð¼ ð‘—, IFtask = 1 ð‘˜ ð‘—=1 ð¼ ð‘— = ð‘˜ . Here, task refers to benchmark problem together with its selected instruction set. 4. Experiments Based on Vibe Checker, this section investigates the trade-off between functionality and instruction following, analyzes LLM behaviors, and ultimately correlates our metrics with user preference. 4.1. Experimental Setup Models. To ensure comprehensive analysis, we select cohort of 31 powerful LLMs spanning 10 distinct model families, including Gemini (Gemini Team, 2025), Claude (Anthropic, 2024, 2025), OpenAI (OpenAI, 2024, 2025), DeepSeek (DeepSeek-AI, 2024, 2025), Qwen (Hui et al., 2024; Qwen Team, 2025), Grok (xAI, 2025a,b), Gemma (Gemma Team, 2025), Mistral (Mistral AI, 2025), MiniMax (MiniMax, 2025), and Kimi (Kimi Team, 2025). 6 Vibe Checker: Aligning Code Evaluation with Human Preference Model Single-Turn Generation Multi-Turn Editing Base 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst BigVibeBench: Real-World Programming Tasks Gemini 2.5 Pro Gemini 2.5 Flash Claude 4 Opus Claude 4 Sonnet GPT 5 o4 mini Kimi K2 50.35 47.37 51. 51.84 46.49 52.28 47.19 0.34 0. -0.86 -0.17 0.56 4.02 -1.12 2. 1.12 -2.23 -0.52 5.66 9.39 -0. 0.87 2.60 -4.31 0.33 2.26 5. -0.93 -0.36 1.31 -1.72 0.50 3. 7.38 0.17 LiveVibeBench: Algorithmic Programming Contest Problems Gemini 2.5 Pro Gemini 2.5 Flash Claude 4 Opus Claude 4 Sonnet GPT 5 o4 mini Kimi K2 85.31 74. 68.72 66.35 71.47 80.95 63.58 -0. 3.56 4.55 4.57 1.72 5.74 8. 3.45 5.34 8.56 5.00 2.13 2. 8.01 8.41 3.71 3.32 9.02 15.48 9.02 16. 2.45 5.60 8.13 6.99 7.16 11.37 15. 1.39 2.41 -2.08 0.50 1.89 9. 2.03 2.45 6.74 8.96 9.00 6.76 12. 16.36 1.75 0.93 0.51 0.85 1. 2.18 2.23 0.67 0.12 2.07 0. 2.25 3.63 2.64 2.44 1.12 1. 2.03 2.82 4.71 4.09 1.34 1. 1.38 0.86 4.24 8.91 5.63 4. 1.48 2.06 3.55 4.35 7.04 2. 1.01 1.65 1.51 1.15 5.57 10.19 9. 4.89 2.98 3.25 4.05 5.27 7. 4.45 1.89 3.44 2.34 1.72 7.43 11. 12.49 5.04 3.72 3.78 5.40 5. 8.05 6.12 2.23 3.69 2.34 2. 9.02 15.92 12.79 Table 2 Top-performing models still suffer from functional regression when non-functional instructions are added. Base is pass@1 on the original query. All other columns report the regression rate (%) relative to Base. ð‘˜ Inst is the number of added instructions. Light red marks > 5% regression and deep red denotes > 10%. Full results for all 31 LLMs are listed in the Appendix D.2. Benchmarks. We construct BigVibeBench and LiveVibeBench by augmenting the full sets of BigCodeBench (1,140 instances) and LiveCodeBench v1v6 (1,055 problems, May 2023 to May 2025). Each instance across both benchmarks is augmented with 5 instructions from VeriCode taxonomy, resulting in total of over 10K instruction-level evaluations. Implementation Details. All models are queried via the Vertex AI5 and OpenRouter6 APIs. During benchmark augmentation, we use deterministic temperature of 0.0. During evaluation, we follow the defaults of the underlying benchmarks: 0.0 for BigVibeBench and 0.2 for LiveVibeBench. We enable thinking mode on all models that support it. For Claude models with thinking mode enabled, the API requires temperature 1.0, so we set it accordingly; all other models use the benchmark defaults. The context length is set to each models supported maximum, capped at 32,768 tokens. 4.2. Results for Functionality Adding non-functional instructions leads to functional regression. Table 2 reports regression rates on BigVibeBench for real-world programming and LiveVibeBench for algorithmic problems. Handling multiple non-functional instructions is routine in practice, yet it still causes notable functional loss even for state-of-the-art models. On BigVibeBench, under multi-turn editing with five instructions, every model shows regression above 5% except Gemini 2.5 Flash and Claude 4 Opus. The effect is amplified on LiveVibeBench: regressions above 5% occur frequently for all models except Gemini 2.5 Pro, with the impact particularly pronounced for o4 mini and Kimi K2, which exceed 10% in more than half of the test configurations. Single-turn generation better preserves functionality than multi-turn editing. As illustrated 5https://cloud.google.com/vertex-ai/docs/reference/rest 6https://openrouter.ai 7 Vibe Checker: Aligning Code Evaluation with Human Preference (a) Functional Regression Rate (b) Task-Level Instruction Following Figure 3 Trends averaged over all evaluated models. As the number of instructions increases, functional regression grows steadily, while the task-level IF score drops markedly. Single-turn generation better preserves functionality, whereas multi-turn editing achieves higher instruction following. Model Single-Turn Generation Multi-Turn Editing 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst BigVibeBench: Real-World Programming Tasks Gemini 2.5 Pro Gemini 2.5 Flash Claude 4 Opus Claude 4 Sonnet GPT 5 o4 mini Kimi K2 82. 81.67 88.77 84.91 82.89 84.82 85. 60.70 61.05 76.32 67.19 67.63 70. 68.86 48.16 43.68 64.21 52.28 54. 57.11 53.68 37.46 30.53 52.98 42. 42.98 47.98 41.23 LiveVibeBench: Algorithmic Programming Contest Problems Gemini 2.5 Pro Gemini 2.5 Flash Claude 4 Opus Claude 4 Sonnet GPT 5 o4 mini Kimi K2 75. 66.54 78.86 75.73 82.18 73.18 62. 56.78 45.97 57.91 56.40 68.53 53. 41.61 45.50 32.89 47.96 44.17 55. 43.22 27.77 37.63 23.03 38.96 35.36 47.01 33.36 19. 30.70 25.70 46.75 35.26 34.39 41.32 30. 29.57 17.06 35.17 28.53 40.95 27.20 11.94 84. 78.68 87.02 86.40 84.91 88.51 89. 78.96 72.80 85.59 84.45 85.59 81. 76.97 68.33 56.75 73.16 72.54 72. 74.74 77.11 61.61 51.09 72.89 73. 74.50 66.64 57.35 55.61 40.96 61. 61.23 62.98 61.23 66.40 51.18 34. 61.71 62.37 66.64 54.60 44.17 44.21 29. 51.32 51.05 55.26 50.09 53.95 41.04 25. 52.04 52.70 57.35 42.84 35.73 33.68 21. 42.11 42.89 48.51 41.84 44.04 32.80 17. 43.70 44.64 50.14 32.61 27.87 Table 3 Following multiple instructions remains challenging for top-performing models. We report the task-level IF scores on both benchmarks. Light red marks IF score < 50 and deep red indicates IF < 30. Full results for all 31 LLMs are provided in the Appendix D.3. in Figure 3a, regression increases monotonically with the number of instructions. On BigVibeBench, average regression for single-turn climbs from 2.48% with one instruction to 5.76% with five, while multi-turn rises from 3.18% to 9.31% over the same range. On LiveVibeBench, the gap is smaller: with two instructions, the two interaction modes are comparable, but as constraints increase, the single-turn setting gradually opens clearer lead. Overall, single-turn generation more reliably preserves functionality, and its advantage grows with the number of instructions. 4.3. Results for Instruction Following Task-level success collapses under multiple instructions. Table 3 presents the task-level IF score, where success requires satisfying all constraints simultaneously. The performance decay is rapid: with three or more instructions, most advanced models fall below 50 across both benchmarks. The decline 8 Vibe Checker: Aligning Code Evaluation with Human Preference is sharper on LiveVibeBench, where 5 of the 7 leading models do not reach 30 in the single-turn setting. Such steep drop is not entirely unexpected, as even the best models remain below 90 on single instruction. With each added instruction, the probability of satisfying all constraints decreases multiplicatively, yielding an exponential decay in task-level success. Such performance degradation indicates that IF remains challenge for state-of-the-art models and should be prioritized in both evaluation and training to meet the demands of real-world, multi-instruction scenarios. Multi-turn editing is more effective for following instructions. In contrast to the functionality results, multi-turn editing consistently outperforms single-turn generation in instruction following, as shown in Figure 3b. On BigVibeBench, the multi-turn setting maintains 3% to 4.5% advantage in the task-level IF score. This gap widens on LiveVibeBench, where the advantage reaches around 8%. Given that the tasks are identical across settings, the consistent gap plausibly reflects the difference between the interactive patterns: single-turn must integrate all constraints in one pass and tends to prioritize preserving overall correctness, whereas the iterative nature of multi-turn supports targeted revisions that better satisfy newly introduced instructions. 4.4. Instruction Position Analysis Models exhibit position bias in instruction following. We define instruction position as the index of each constraint: for single-turn generation, the number in the list appended to the base prompt; for multi-turn editing, the round in which the constraint is introduced, starting at 1. On BigVibeBench, Figure 4 shows clear U-shape, the classic lost-in-the-middle pattern typically reported for long-context generation (Liu et al., 2024), despite our prompts being only few hundred tokens long. Furthermore, single-turn generation shows primacy bias, performing best on the first instruction, while multi-turn editing displays clear recency bias, peaking on the final position. While the distinct U-shape does not generalize to the algorithmic tasks in LiveVibeBench, the underlying positional preferences remain consistent: single-turn generation favors the first instruction, while multi-turn editing consistently performs best on the last. Figure 4 Average instruction-level IF trends by instruction position. 4.5. Correlating with Human Preference (a) Real-World Programming (b) Algorithmic Programming Figure 5 Human preference aligns best with mix of IF and functionality. We correlate LMArena coding Elo with composite score ð›¼ IF + (1 ð›¼) Func, where ð›¼ [0, 1] is the weight on IF (x-axis). The peak correlation (starred) for both benchmarks is achieved with mixture of the two metrics. Having established metrics for both functionality and instruction following, we now investigate 9 Vibe Checker: Aligning Code Evaluation with Human Preference how these signals relate to overall human preference in coding tasks. To explore this, we use LMArena (Chiang et al., 2024), currently the largest source of human preference data for LLMs. Its coding subset alone contains over 800K human votes, aggregated into Elo ratings for each model7. We take the latest default Elo ratings from this subset (see Appendix Table 4) and compute correlations against two metrics derived from Vibe Checker: Func, defined as pass@1 on the original problems, and IF, taken from the single-turn setting under one instruction. We then evaluate composite score ð›¼ IF + (1 ð›¼) Func with ð›¼ [0, 1], and report correlations. Human preference correlates best with mixture of instruction following and functionality. Across both benchmarks, the peak correlation occurs at intermediate ð›¼ (starred in Figure 5), indicating that neither IF nor Func alone explains preference as well as their combination. Concretely, on BigVibeBench, the optimum for Pearson correlation places 40% weight on IF (ð›¼ = 0.4), while for Spearman correlation, the weight on IF rises to 70% (ð›¼ = 0.7). The optimal blend for LiveVibeBench is remarkably similar. In all cases, the mixture outperforms either isolated metric by clear margin. Additional correlation types and results with LMArena style control (Li et al., 2024) disabled are reported in Appendix E.2, with conclusions remaining consistent. Which single factor users value depends on the coding scenario. While mix is always best, the importance of each metric considered alone differs by the type of programming task. For the real-world programming tasks in BigVibeBench, instruction following plays more critical role. On the Spearman correlation, pure IF (ð›¼ = 1) correlates over 0.1 points higher with human preference than pure Func (ð›¼ = 0). For algorithmic programming tasks in LiveVibeBench, the opposite is true: pure Func holds clear advantage over pure IF. This suggests that for practical, day-to-day coding, users place high value on models ability to adhere to non-functional instructions, whereas, in competitive programming scenarios, functional correctness is the paramount factor. Overall implication. Our results provide evidence that instruction following is critical, undermeasured component of human preference in coding tasks. Beyond functional correctness, adherence to non-functional constraints offers strong signal for distinguishing real-world utility. Consequently, integrating instruction following alongside functionality in both evaluation and training provides practical path toward models that align more closely with real-world user preferences. 5. Related Work Instruction Following. Research in general instruction following focuses on stress-testing models with synthetic constraints (e.g., forced word repetition) and evaluates with either deterministic checkers (Pyatkin et al., 2025; Wang et al., 2025; Zhou et al., 2023) or LLM-as-a-judge (Jiang et al., 2024; Qin et al., 2024). prevailing trend leverages large-scale, verifiable instructions to boost capabilities via post-training, such as SFT and RL (Pyatkin et al., 2025; Wang et al., 2025). In contrast, instructions in the coding domain are tied to practical software development, concerning aspects such as logic patterns, coding style, and library usage. Prior work is sparse, and existing benchmarks for such code instructions lack verifiability. They typically compare to ground truth with Diff BLEU (Singhal et al., 2024) or use LLM and human judgment (Yan et al., 2025), which is unreliable and hard to scale. To bridge this gap, our work introduces taxonomy of verifiable code instructions, each paired with binary verifier, enabling scalable evaluation and training. Code Evaluation. Functional correctness dominates code evaluation: the generated code is run against unit tests, from snippet-level functions (Austin et al., 2021; Chen et al., 2021; Du et al., 7https://lmarena.ai/leaderboard/text/coding 10 Vibe Checker: Aligning Code Evaluation with Human Preference 2023; Hendrycks et al., 2021; Jain et al., 2025; Lai et al., 2023; Liu et al., 2023; Zheng et al., 2025; Zhuo et al., 2025) to repository-level tasks (Chowdhury et al., 2024; Jimenez et al., 2024; MÃ¼ndler et al., 2024; Yang et al., 2025; Zan et al., 2025; Zhang et al., 2025; Zhao et al., 2025). Research on non-functional requirements is relatively small branch of research, covering aspects like adherence to task-oriented instructions (Yan et al., 2025), runtime efficiency, maintainability, and security (Singhal et al., 2024). We move beyond evaluating these aspects in isolation. On top of Vibe Checker testbed, we systematically analyze the trade-off between functional correctness and instruction following, and provide evidence that human preference reflects composite of both dimensions. This work aims to align the code evaluation with the real-world user preferences. 6. Conclusion In this paper, we challenged the prevailing focus on functional correctness in code evaluation. We study the vibe check as subjective judgment tied to real-world human preference and approximate it with measurable signals. We present VeriCode, verifiable taxonomy of non-functional code instructions, and Vibe Checker, testbed that augments established evaluation suites. Across 31 leading LLMs, composite of functional correctness and instruction following predicts human preference substantially better than either metric alone. Our work calls for moving beyond pass@k and for optimizing both functional and non-functional qualities in future research for coding."
        },
        {
            "title": "References",
            "content": "Anthropic. Introducing claude 3.5, 2024. URL https://www.anthropic.com/news/ claude-3-5-sonnet. Anthropic. Introducing claude 4, 2025. URL https://www.anthropic.com/news/claude-4. J. Austin, A. Odena, M. I. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. J. Cai, M. Terry, Q. V. Le, and C. Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732. M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray, R. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P. Mishkin, B. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser, M. Bavarian, C. Winter, P. Tillet, F. P. Such, D. Cummings, M. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss, A. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji, S. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam, V. Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage, M. Murati, K. Mayer, P. Welinder, B. McGrew, D. Amodei, S. McCandlish, I. Sutskever, and W. Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. W. Chi, V. Chen, A. N. Angelopoulos, W.-L. Chiang, A. Mittal, N. Jain, T. Zhang, I. Stoica, C. Donahue, and A. Talwalkar. Copilot arena: platform for code LLM evaluation in the wild. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum? id=9bYOqwtAud. W. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, B. Zhu, H. Zhang, M. I. Jordan, J. E. Gonzalez, and I. Stoica. Chatbot arena: An open platform for evaluating llms by human preference. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=3MW8GKNyzI. 11 Vibe Checker: Aligning Code Evaluation with Human Preference N. Chowdhury, J. Aung, C. J. Shern, O. Jaffe, D. Sherburn, G. Starace, E. Mays, R. Dias, M. Aljubeh, M. Glaese, C. E. Jimenez, J. Yang, L. Ho, T. Patwardhan, K. Liu, and A. Madry. Introducing SWE-bench verified. OpenAI Blog, 2024. URL https://openai.com/index/ introducing-swe-bench-verified/. J. Da, C. Wang, X. Deng, Y. Ma, N. Barhate, and S. Hendryx. Agent-rlvr: Training software engineering agents via guidance and environment rewards. CoRR, abs/2506.11425, 2025. doi: 10.48550/ ARXIV.2506.11425. URL https://doi.org/10.48550/arXiv.2506.11425. DeepSeek-AI. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. doi: 10.48550/ARXIV. 2412.19437. URL https://doi.org/10.48550/arXiv.2412.19437. DeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10. 48550/arXiv.2501.12948. X. Du, M. Liu, K. Wang, H. Wang, J. Liu, Y. Chen, J. Feng, C. Sha, X. Peng, and Y. Lou. Classeval: manually-crafted benchmark for evaluating llms on class-level code generation. CoRR, abs/2308.01861, 2023. doi: 10.48550/ARXIV.2308.01861. URL https://doi.org/10.48550/ arXiv.2308.01861. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025. doi: 10.48550/ARXIV. 2507.06261. URL https://doi.org/10.48550/arXiv.2507.06261. Gemma Team. Gemma 3 technical report. Mar. 2025. doi: 10.48550/arXiv.2503.19786. URL https://arxiv.org/abs/2503.19786. First posted March 25, 2025. D. Hendrycks, S. Basart, S. Kadavath, M. Mazeika, A. Arora, E. Guo, C. Burns, S. Puranik, H. He, D. Song, and J. Steinhardt. Measuring coding challenge competence with APPS. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. URL https://datasets-benchmarks-proceedings.neurips.cc/paper/ 2021/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract-round2.html. B. Hui, J. Yang, Z. Cui, J. Yang, D. Liu, L. Zhang, T. Liu, J. Zhang, B. Yu, K. Dang, A. Yang, R. Men, F. Huang, X. Ren, X. Ren, J. Zhou, and J. Lin. Qwen2.5-coder technical report. CoRR, abs/2409.12186, 2024. doi: 10.48550/ARXIV.2409.12186. URL https://doi.org/10.48550/ arXiv.2409.12186. N. Jain, K. Han, A. Gu, W. Li, F. Yan, T. Zhang, S. Wang, A. Solar-Lezama, K. Sen, and I. Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum?id=chfJJYC3iL. Y. Jiang, Y. Wang, X. Zeng, W. Zhong, L. Li, F. Mi, L. Shang, X. Jiang, Q. Liu, and W. Wang. Followbench: multi-level fine-grained constraints following benchmark for large language models. In L. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 46674688. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. ACL-LONG.257. URL https://doi.org/10.18653/v1/2024.acl-long.257. 12 Vibe Checker: Aligning Code Evaluation with Human Preference C. E. Jimenez, J. Yang, A. Wettig, S. Yao, K. Pei, O. Press, and K. R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. A. Karpathy. Vibe coding wikipedia, 2025. URL https://en.wikipedia.org/wiki/Vibe_ coding. Kimi Team. Kimi k2: Open agentic intelligence. July 2025. doi: 10.48550/arXiv.2507.20534. URL https://arxiv.org/abs/2507.20534. Y. Lai, C. Li, Y. Wang, T. Zhang, R. Zhong, L. Zettlemoyer, W. Yih, D. Fried, S. I. Wang, and T. Yu. DS1000: natural and reliable benchmark for data science code generation. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1831918345. PMLR, 2023. URL https://proceedings.mlr. press/v202/lai23b.html. T. Li, A. Angelopoulos, and W.-L. Chiang. Does style matter? disentangling style and substance in chatbot arena, August 2024. URL https://blog.lmarena.ai/blog/2024/style-control/. J. Liu, C. S. Xia, Y. Wang, and L. Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/ 2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html. N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157173, 2024. doi: 10.1162/tacl_a_00638. URL https://aclanthology.org/ 2024.tacl-1.9/. MiniMax. Minimax-m1: Scaling test-time compute efficiently with lightning attention. June 2025. doi: 10.48550/arXiv.2506.13585. URL https://arxiv.org/abs/2506.13585. Mistral AI. Mistral medium 3, 2025. URL https://mistral.ai/news/mistral-medium-3. N. MÃ¼ndler, M. N. MÃ¼ller, J. He, and M. T. Vechev. Swt-bench: Testing and validating real-world bug-fixes with code agents. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/ hash/94f093b41fc2666376fb1f667fe282f3-Abstract-Conference.html. OpenAI. Gpt-4o system card. CoRR, abs/2410.21276, 2024. doi: 10.48550/ARXIV.2410.21276. URL https://doi.org/10.48550/arXiv.2410.21276. OpenAI. Openai o3 and o4-mini system card. Technical report, OpenAI, Apr. 2025. URL https: //openai.com/index/o3-o4-mini-system-card/. S. Peng, E. Kalliamvakou, P. Cihon, and M. Demirer. The impact of AI on developer productivity: Evidence from github copilot. CoRR, abs/2302.06590, 2023. doi: 10.48550/ARXIV.2302.06590. URL https://doi.org/10.48550/arXiv.2302.06590. 13 Vibe Checker: Aligning Code Evaluation with Human Preference V. Pyatkin, S. Malik, V. Graf, H. Ivison, S. Huang, P. Dasigi, N. Lambert, and H. Hajishirzi. Generalizing verifiable instruction following. CoRR, abs/2507.02833, 2025. doi: 10.48550/ARXIV.2507.02833. URL https://doi.org/10.48550/arXiv.2507.02833. Y. Qin, K. Song, Y. Hu, W. Yao, S. Cho, X. Wang, X. Wu, F. Liu, P. Liu, and D. Yu. Infobench: Evaluating instruction following ability in large language models. In L. Ku, A. Martins, and V. Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1302513048. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.772. URL https://doi.org/10.18653/v1/ 2024.findings-acl.772. Qwen Team. Qwen3 technical report. CoRR, abs/2505.09388, 2025. doi: 10.48550/ARXIV.2505. 09388. URL https://doi.org/10.48550/arXiv.2505.09388. S. I. Ross, F. Martinez, S. Houde, M. J. Muller, and J. D. Weisz. The programmers assistant: Conversational interaction with large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces, IUI 2023, Sydney, NSW, Australia, March 27-31, 2023, pages 491514. ACM, 2023. doi: 10.1145/3581641.3584037. URL https://doi.org/10.1145/3581641.3584037. M. Singhal, T. Aggarwal, A. Awasthi, N. Natarajan, and A. Kanade. Nofuneval: Funny how code lms falter on requirements beyond functional correctness. CoRR, abs/2401.15963, 2024. doi: 10.48550/ARXIV.2401.15963. URL https://doi.org/10.48550/arXiv.2401.15963. Stack Overflow. Ai 2025 stack overflow developer survey, 2025. URL https://survey. stackoverflow.co/2025/ai. Survey report. Z. Wang, J. Jiang, H. Zhou, W. Zheng, X. Zhang, C. Bansal, and H. Yao. Verifiable format control for large language model generations. In L. Chiruzzo, A. Ritter, and L. Wang, editors, Findings of the Association for Computational Linguistics: NAACL 2025, Albuquerque, New Mexico, USA, April 29 - May 4, 2025, pages 34993513. Association for Computational Linguistics, 2025. doi: 10.18653/V1/ 2025.FINDINGS-NAACL.194. URL https://doi.org/10.18653/v1/2025.findings-naacl. 194. S. Willison. Not all ai-assisted programming is vibe coding (but vibe coding rocks), Mar. 2025. URL https://simonwillison.net/2025/Mar/19/vibe-coding/. Blog post, Simon Willisons Weblog. xAI. Grok 3 beta the age of reasoning agents, 2025a. URL https://x.ai/news/grok-3. xAI. Grok 4 model card. 2025b. URL https://data.x.ai/2025-08-20-grok-4-model-card. pdf. K. Yan, H. Guo, X. Shi, S. Cao, D. Di, and Z. Li. CodeIF: Benchmarking the instruction-following capabilities of large language models for code generation. In G. Rehm and Y. Li, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 6: Industry Track), pages 12721286, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-288-6. doi: 10.18653/v1/2025.acl-industry.89. URL https://aclanthology. org/2025.acl-industry.89/. J. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive coding with execution feedback. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference 14 Vibe Checker: Aligning Code Evaluation with Human Preference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 4b175d846fb008d540d233c188379ff9-Abstract-Datasets_and_Benchmarks.html. J. Yang, C. E. Jimenez, A. L. Zhang, K. Lieret, J. Yang, X. Wu, O. Press, N. Muennighoff, G. Synnaeve, K. R. Narasimhan, D. Yang, S. Wang, and O. Press. Swe-bench multimodal: Do AI systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview. net/forum?id=riTiq3i21b. D. Zan, Z. Huang, W. Liu, H. Chen, L. Zhang, S. Xin, L. Chen, Q. Liu, X. Zhong, A. Li, S. Liu, Y. Xiao, L. Chen, Y. Zhang, J. Su, T. Liu, R. Long, K. Shen, and L. Xiang. Multi-swe-bench: multilingual benchmark for issue resolving. CoRR, abs/2504.02605, 2025. doi: 10.48550/ARXIV.2504.02605. URL https://doi.org/10.48550/arXiv.2504.02605. L. Zhang, S. He, C. Zhang, Y. Kang, B. Li, C. Xie, J. Wang, M. Wang, Y. Huang, S. Fu, E. Nallipogu, Q. Lin, Y. Dang, S. Rajmohan, and Y. Zhang. Swe-bench goes live! CoRR, abs/2505.23419, 2025. doi: 10.48550/ARXIV.2505.23419. URL https://doi.org/10.48550/arXiv.2505.23419. W. Zhao, N. Jiang, C. Lee, J. T. Chiu, C. Cardie, M. GallÃ©, and A. M. Rush. Commit0: Library generation from scratch. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=MMwaQEVsAg. Z. Zheng, Z. Cheng, Z. Shen, S. Zhou, K. Liu, H. He, D. Li, S. Wei, H. Hao, J. Yao, P. Sheng, Z. Wang, W. Chai, A. Korolova, P. Henderson, S. Arora, P. Viswanath, J. Shang, and S. Xie. Livecodebench pro: How do olympiad medalists judge llms in competitive programming? CoRR, abs/2506.11928, 2025. doi: 10.48550/ARXIV.2506.11928. URL https://doi.org/10.48550/arXiv.2506.11928. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023. doi: 10.48550/ARXIV.2311. 07911. URL https://doi.org/10.48550/arXiv.2311.07911. T. Y. Zhuo, M. C. Vu, J. Chim, H. Hu, W. Yu, R. Widyasari, I. N. B. Yusuf, H. Zhan, J. He, I. Paul, S. Brunner, C. Gong, J. Hoang, A. R. Zebaze, X. Hong, W. Li, J. Kaddour, M. Xu, Z. Zhang, P. Yadav, and et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=YrycTjllL0. Vibe Checker: Aligning Code Evaluation with Human Preference"
        },
        {
            "title": "B VeriCode Taxonomy",
            "content": "17 17 B.1 Verification Code with Ruff . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Case Studies from VeriCode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Vibe Checker Testbed",
            "content": "23 C.1 Instruction Category Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.2 Evaluation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "D Experiments",
            "content": "26 D.1 Details of Evaluated Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 D.2 Detailed Results for Functionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 D.3 Detailed Results for Instruction Following . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "E Analysis",
            "content": "33 E.1 Instruction Position Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 E.2 Correlation Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 16 Vibe Checker: Aligning Code Evaluation with Human Preference A. LLM Usage Statement In the preparation of this manuscript, we use LLMs (e.g., Gemini) only to assist with language polishing. Its function is strictly limited to improving grammar, correcting spelling, and optimizing phrasing for clarity and readability. The LLMs do not contribute to any substantive part of the research, such as ideation, literature review, data analysis, or the generation of core arguments. All technical content, claims, and conclusions come from the authors. The authors review and approve the final text and take full responsibility for its accuracy and integrity. LLMs are not authors or contributors. B. VeriCode Taxonomy B.1. Verification Code with Ruff Given that 27 of the 30 verifiers in our VeriCode taxonomy are implemented via Python linter Ruff, we present the helper function in Figure 6. Figure 6 Implementation of the core helper function used to run Ruff checks within VeriCode. 17 Vibe Checker: Aligning Code Evaluation with Human Preference B.2. Case Studies from VeriCode The full version of 5 instructions in Table 1 are presented in Figures 7, 8, 9, 10, and 11. Figure 7 Full version of style_3 instruction from VeriCode taxonomy. Vibe Checker: Aligning Code Evaluation with Human Preference Figure 8 Full version of logic_3 instruction from VeriCode taxonomy. 19 Vibe Checker: Aligning Code Evaluation with Human Preference Figure 9 Full version of doc_3 instruction from VeriCode taxonomy. Vibe Checker: Aligning Code Evaluation with Human Preference Figure 10 Full version of error_3 instruction from VeriCode taxonomy. 21 Vibe Checker: Aligning Code Evaluation with Human Preference Figure 11 Full version of library_1 instruction from VeriCode taxonomy. Vibe Checker: Aligning Code Evaluation with Human Preference C. Vibe Checker Testbed C.1. Instruction Category Distributions Figure 12 illustrates the complete distribution of instruction categories selected for both augmented benchmarks. As shown, the three most frequent categories are Coding Logic, Coding Style, and Documentation. The distributions also reflect the distinct focus of each benchmark: the algorithm-oriented LiveVibeBench features higher proportion of Coding Logic instructions (42.3% vs. 35.9%), while the real-world-task-focused BigVibeBench includes more instructions related to Error Management and Library Constraint instructions (6.3% vs. 0.9% and 2.2% vs. 0.1% respectively). Figure 12 Percentage distribution of instruction categories on both augmented benchmarks. C.2. Evaluation Prompts For BigVibeBench and LiveVibeBench, the system instruction and the evaluation prompts are shown in Figures 13 and 14. As we adopt BigCodeBenchs original instruct_prompt, we do not provide any additional evaluation prompt on this benchmark. 23 Vibe Checker: Aligning Code Evaluation with Human Preference Figure 13 System prompt used for BigVibeBench. LiveVibeBench keeps the same wording with one minor change: complete, runnable code complete Python functions, since algorithmic contest tasks often require only functions rather than full programs. 24 Vibe Checker: Aligning Code Evaluation with Human Preference Figure 14 Evaluation prompts used in LiveVibeBench for the two task types. 25 Vibe Checker: Aligning Code Evaluation with Human Preference D. Experiments D.1. Details of Evaluated Models For completeness and reproducibility, we list the comprehensive details of the 31 LLMs evaluated in our study, including their specific LMArena designations and the Elo ratings (Sep. 18, 2025) used for our human preference correlation analysis in Table 4. Notably, on the LiveVibeBench benchmark, models demonstrate significantly higher rate of failure to generate complete responses. These failures are attributed to either OpenRouter provider errors or exceeding the 32,768-token limit. In our experiments, each task is attempted up to three times, and persistent failure is recorded as an error. To ensure the reliability of our results, we exclude models with an error rate exceeding 10%. Consequently, the LiveVibeBench analysis is conducted on the remaining 24 LLMs, with full results presented in Tables 6, 9, 10, and 12. Model LMArena Name Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash Lite Claude 4 Opus Claude 4 Sonnet Claude 3.7 Sonnet Claude 3.5 Sonnet Claude 3.5 Haiku Claude 3 Haiku DeepSeek R1 0528 DeepSeek V3 GPT 5 o4 mini o3 mini high GPT 4.1 GPT 4.1 mini GPT 4o GPT 4o mini Grok 4 Grok 3 mini beta Qwen 3 235B A22B Qwen 3 32B Qwen 3 30B A3B Qwen 2.5 72B Instruct Qwen 2.5 Coder Gemma 3 27B Gemma 3 12B gemini-2.5-pro gemini-2.5-flash gemini-2.0-flash-lite-preview-02-05 claude-opus-4-20250514-thinking-16k claude-sonnet-4-20250514-thinking-32k claude-3-7-sonnet-20250219-thinking-32k Claude 3.5 Sonnet (10/22) claude-3-5-haiku-20241022 claude-3-haikudeepseek-r1-0528 deepseek-v3-0324 gpt-5-high o4-mini-2025-04-16 o3-mini-high gpt-4.1-2025-04-14 gpt-4.1-mini-2025-04-14 GPT-4o (08/06) GPT-4o-mini (07/18) grok-4-0709 grok-3-mini-beta qwen3-235b-a22b qwen3-32b qwen3-30b-a3b qwen2.5-72b-instruct qwen2.5-coder-32b-instruct gemma-3-27b-it gemma-3-12b-it Mistral Medium mistral-medium-2505 MiniMax M1 Kimi K2 minimax-m1 kimi-k2-0711-preview Elo Rating w/o SC 1468 w/ SC 1470 1422 1336 1430 1407 1353 1337 1285 1436 1389 1440 1380 1379 1399 1371 1289 1297 1431 1375 1392 1375 1346 1298 1274 1348 1309 1368 1391 1419 1352 1481 1460 1430 1418 1370 1287 1458 1431 1467 1428 1421 1447 1423 1352 1440 1384 1423 1407 1378 1346 1325 1370 1332 1421 1409 Table 4 Details of the 31 LLMs evaluated in our experiments. For each model, we list its name as reported in this paper, its LMArena designation, and the Elo ratings used to analyze correlations with human preference. These ratings are from the September 18, 2025 leaderboard, presented under two conditions: with Style Control (w/ SC) and without (w/o SC). 26 Vibe Checker: Aligning Code Evaluation with Human Preference D.2. Detailed Results for Functionality We present the detailed results for functionality on both benchmarks in Tables 5 and 6. Models Gemini 2.5 Pro Gemini 2.5 Flash Gemini 2.0 Flash Gemini 2.0 Flash Lite Claude 4 Opus Claude 4 Sonnet Claude 3.7 Sonnet Claude 3.5 Sonnet Claude 3.5 Haiku Claude 3 Haiku DeepSeek R1 0528 DeepSeek V3 GPT 5 o4 mini o3 mini high GPT 4.1 GPT 4.1 mini GPT 4o GPT 4o mini Grok 4 Grok 3 mini beta Qwen 3 235B A22B Qwen 3 32B Qwen 3 30B A3B Qwen 2.5 Coder Gemma 3 27B Gemma 3 12B Mistral Medium 3 MiniMax M1 Kimi Single-Turn Generation Multi-Turn Editing Base 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 50.35 47.37 48.42 46.93 51. 51.84 51.32 48.42 46.58 38.07 49. 50.18 46.49 52.28 49.91 47.54 49. 49.82 46.05 53.07 48.77 48.86 47. 46.40 49.39 45.70 40.00 45.44 48. 0.34 0.74 2.54 5.05 -0.86 -0. 1.54 5.08 5.28 0.24 1.24 1. 0.56 4.02 4.57 -1.85 0.55 1. 5.91 0.17 2.52 1.25 0.36 2. 6.53 5.87 6.72 5.27 5.22 4. 2.60 1.12 0.35 7.29 0.87 2. 1.63 7.10 -0.36 1.31 3.61 6. 1.39 2.41 4.89 8.42 -2.23 -4. -1.72 -2.08 -0.52 1.03 5.62 4. 1.16 0.18 0.88 5.66 9.39 10. -0.19 1.61 2.99 5.52 1.15 4. 1.99 2.58 3.58 8.52 3.20 7. 3.73 8.30 4.68 0.33 1.71 5. 6.98 7.38 -1.24 2.99 2.26 5. 9.84 1.28 2.69 4.40 7.23 1. 7.91 1.80 4.60 3.41 0.50 2. 5.43 7.34 7.14 1.61 4.90 3. 7.38 0.50 2.92 8.16 9.42 7. 3.03 2.99 1.89 9.56 14.93 13. 4.80 4.49 3.87 6.28 3.64 8. 3.42 5.14 5.09 6.63 5.38 3. 7.99 1.00 9.35 3.05 6.99 7. 10.88 11.08 12.05 6.22 6.91 7. 9.07 3.41 11.56 11.91 9.58 9. 9.46 5.22 0.17 8.05 7.90 10. 3.59 2.03 1.75 0.93 2.89 2. 0.51 0.85 1.03 5.08 6.98 6. 1.61 5.08 1.70 2.18 2.62 2. 4.30 2.45 7.80 1.32 2.15 1. 2.94 1.87 8.90 5.87 3.63 5. 5.79 0.35 2.23 2.44 1.12 5. 4.67 1.02 2.03 1.38 6.69 9. 4.01 1.48 6.53 5.24 2.06 3. 3.08 8.69 4.89 2.98 8.32 8. 3.25 4.05 4.25 5.04 3.72 9. 8.78 3.78 5.40 5.30 10.86 12. 15.44 17.13 21.28 10.82 13.61 17. 17.97 1.08 7.87 2.82 4.71 5. 5.53 6.44 4.58 6.47 2.15 4. 3.95 4.41 4.91 3.92 9.27 4. 7.04 7.19 7.19 6.99 6.50 9. 1.98 5.76 5.94 6.99 5.86 3. 4.27 11.90 16.26 5.27 7.04 9. 7.36 8.24 7.03 5.46 8.05 10. 7.93 8.77 7.91 11.62 11.62 3. 7.73 8.27 9.03 7.56 4.47 9. 8.80 10.69 7.93 10.88 12.26 14. 16.02 8.89 5.36 6.58 6.18 1. 4.09 12.43 12.98 12.98 6.72 9. 8.69 3.78 2.78 8.64 11.71 12. 15.35 9.07 5.40 4.45 9.86 6. 6.12 47.19 -1.12 -0.19 -0.93 Qwen 2.5 72B Instruct 44.39 Table 5 Results for functionality on BigVibeBench. Base is the pass@1 score for the original query. All other cells report the functional regression rate (%) relative to the base. Lower is better, and negative values indicate improvement. Here, ð‘˜ Inst denotes the number of added instructions. 27 Vibe Checker: Aligning Code Evaluation with Human Preference Models Single-Turn Generation Multi-Turn Editing Base 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst Gemini 2.5 Pro 85.31 -0.11 Gemini 2.5 Flash Gemini 2.0 Flash 74. 41.33 Gemini 2.0 Flash Lite 34.12 Claude 4 Opus Claude 4 Sonnet 68. 66.35 3.56 0.92 1.11 4.55 4. Claude 3.7 Sonnet 61.80 -0.31 Claude 3.5 Sonnet Claude 3.5 Haiku 45. 37.63 1.67 1.51 3.45 5.34 1. 2.45 8.01 0.70 2.45 5.60 1. 2.45 6.74 3.44 -7.50 -8.06 -10. -6.95 8.56 5.00 2.30 2.49 5. 8.41 3.71 3.37 2.09 9.06 8. 6.99 1.68 5.22 7.81 8.96 9. 4.90 6.48 0.67 0.12 0.00 2. 2.07 0.42 -0.47 1.34 1.14 1. 5.88 1.01 1.65 3.00 1.89 3. 2.76 2.23 3.69 4.36 8.95 10. 13.18 1.38 1.51 2.34 2.34 0. 0.45 1.15 0.92 1.72 1.23 2. 1.99 2.09 5.22 7.09 8.06 11. 11.08 6.54 14.86 17.88 19.90 23. Claude 3 Haiku 22.09 11.18 19.74 21.05 25. 30.06 6.02 8.60 12.04 13.31 16. DeepSeek V3 0324 57.25 1.15 GPT 5 o4 mini GPT 4. GPT 4.1 mini GPT 4o GPT 4o mini 4.30 2.13 9. 71.47 80.95 1.72 5.74 53.08 -2. -1.60 3.53 0.23 7.88 0.23 58. 42.75 22.27 6.29 3.32 9.02 1. 8.85 4.00 7.11 7.16 6.95 6. 1.48 6.95 7.62 13.57 17.55 2. 4.24 5.57 7.43 9.02 11.37 12. 3.63 8.91 10.19 11.71 15.92 3. 1.07 10.79 1.44 1.54 1.78 2. 5.18 4.99 5.10 9.34 6.25 7. 8.65 6.78 8.21 8.42 9.29 8. 9.75 8.94 10.60 12.75 -11.50 -11. -18.77 -15.36 -12.80 12.93 11.93 0. 3.30 5.46 6.03 7.90 Grok 3 mini beta 65. 2.58 Qwen 3 30B A3B 72.42 Qwen 2.5 72B Instruct 39.05 Gemma 3 27B Gemma 3 12B 35.92 29.29 0.26 0.97 3. 2.90 3.15 0.66 1.95 3.67 6. 1.05 4.84 5.01 -4.85 -1.60 -2. Mistral Medium 3 40.66 3.03 7.45 6.98 3. 1.19 8.02 9.49 1.30 4.89 0. 3.87 1.96 6.79 1.84 9.22 3. 4.20 8.96 10.68 1.03 6.32 10. 8.16 12.67 4.54 10.99 15.23 14. 18.44 -0.25 2.78 2.78 4.65 5. Kimi K2 63.58 8.92 15.48 16.07 15. 16.36 2.64 5.63 9.50 12.49 12. Table 6 Results for functionality on LiveVibeBench. Base is the pass@1 score for the original query. All other cells report the functional regression rate (%) relative to the base. Lower is better, and negative values indicate improvement. Here, ð‘˜ Inst denotes the number of added instructions. 28 2.15 9.97 2.67 0. 5.33 5.01 Vibe Checker: Aligning Code Evaluation with Human Preference D.3. Detailed Results for Instruction Following The detailed results for instruction-level and task-level IF scores on both benchmarks are provided in Tables 7, 8, 9, and 10. Models Single-Turn Generation Multi-Turn Editing 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst Gemini 2.5 Pro 82.19 78.03 79.18 78.82 79. 84.56 82.54 81.73 81.54 80.47 Gemini 2.5 Flash 81.67 77.81 77.34 75.35 75.91 78. 75.35 74.62 73.57 73.25 Gemini 2.0 Flash 73. 72.76 72.95 72.35 72.04 78.86 75. 74.65 73.95 73.30 Gemini 2.0 Flash Lite Claude 4 Opus 70. 88.77 69.96 87.46 69.30 86.05 68. 85.55 68.89 85.60 74.39 71.01 70. 69.39 68.82 87.02 85.75 85.18 84. 84.30 Claude 4 Sonnet 84.91 82.54 81.37 81. 81.37 86.40 85.39 84.85 84.12 83. Claude 3.7 Sonnet 80.26 76.27 75.47 74.25 74. 81.58 79.82 78.83 78.62 78.18 Claude 3.5 Sonnet 80.61 77.54 76.02 75.18 74.70 84. 80.53 79.39 78.49 77.49 Claude 3.5 Haiku 64. 64.74 63.71 63.07 63.82 79.91 73. 71.26 68.20 65.60 Claude 3 Haiku 67.89 65. 64.88 63.73 64.53 76.32 72.63 72. 70.96 70.56 DeepSeek R1 0528 74.04 69.78 69. 69.28 67.71 77.02 74.12 72.37 71. 71.05 DeepSeek V3 0324 67.89 63.77 64.04 63. 65.09 73.95 70.61 70.41 69.23 GPT o4 mini 82.89 82.28 81.96 81.64 81. 84.91 85.18 85.94 85.83 84.82 84. 83.25 83.68 84.25 88.51 86.10 84. 83.60 83.28 67.72 86.39 o3 mini high 80. 75.79 73.63 72.79 71.68 82.46 80. 79.71 78.38 78.16 GPT 4.1 81.40 78. 78.60 77.28 77.75 82.63 80.31 79. 78.36 77.79 GPT 4.1 mini 78.16 75.57 75. 74.19 73.42 79.21 76.71 75.88 74. 73.09 GPT 4o 77.46 74.87 74.56 73. 73.44 85.09 82.37 80.79 79.45 78. GPT 4o mini 76.40 73.82 73.13 73.33 73. 78.16 76.40 Grok 4 87.11 85.61 84. 84.39 84.81 88.51 87.19 75.18 86. 74.10 86.29 73.60 85.37 Grok 3 mini beta 82. 80.04 78.25 77.81 77.46 79.21 77. 76.49 75.37 75.05 Qwen 3 235B A22B 83.95 81. 80.38 79.30 78.63 85.09 81.89 80. 80.13 78.89 Qwen 3 32B 76.75 72.81 71. 70.92 71.35 82.02 80.53 78.92 77. 76.33 Qwen 3 30B A3B 73.42 71.67 70.79 69. 69.81 79.91 78.51 78.07 77.17 76. Qwen 2.5 72B Instruct 73.68 72.32 71.78 70.48 70. 79.47 75.57 75.32 73.88 72.75 Qwen 2.5 Coder 71.40 67.11 67.57 65.42 65.77 73. 71.71 71.26 69.76 68.86 Gemma 3 27B Gemma 3 12B 68.42 67.06 65.50 64.56 65.02 73. 69.65 69.30 68.14 66.72 65.96 66. 65.44 65.26 65.00 67.54 67.76 67. 66.05 64.95 Mistral Medium 3 73.60 72.11 71. 70.79 70.54 76.05 75.44 74.06 73. 71.65 MiniMax M1 74.12 70.75 71.70 71. 70.89 Kimi K2 85.00 83.46 81.46 80. 79.14 77.63 89.12 74.30 87.46 74. 73.60 72.95 86.70 85.09 84.19 Table 7 Instruction-level IF scores on BigVibeBench. Higher is better. 29 Vibe Checker: Aligning Code Evaluation with Human Preference Models Single-Turn Generation Multi-Turn Editing 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst Gemini 2.5 Pro 82.19 60. 48.16 37.46 30.70 84.56 68.33 55. 44.21 33.68 Gemini 2.5 Flash 81.67 61.05 43. 30.53 25.70 78.68 56.75 40.96 29. 21.75 Gemini 2.0 Flash 73.42 53.77 39.47 26. 18.16 78.86 59.39 44.56 32.46 22. Gemini 2.0 Flash Lite Claude 4 Opus 70.44 88.77 48.60 76. 32.63 64.21 22.02 52.98 15.26 46. 74.39 50.61 35.18 24.12 15.35 87. 73.16 61.05 51.32 42.11 Claude 4 Sonnet 84. 67.19 52.28 42.98 35.26 86.40 72. 61.23 51.05 42.89 Claude 3.7 Sonnet 80.26 56. 39.91 27.46 22.28 81.58 63.51 48. 38.16 29.39 Claude 3.5 Sonnet 80.61 59.74 42. 32.37 24.47 84.21 66.40 52.54 42. 32.28 Claude 3.5 Haiku 64.56 42.46 26.14 15. 10.09 79.91 57.63 42.72 30.00 19. Claude 3 Haiku 67.89 41.84 26.05 16.93 11. 76.32 53.60 37.89 26.49 18.77 DeepSeek R1 74.04 49.21 33.42 25.00 17.63 77. 55.18 38.16 26.67 18.51 DeepSeek V3 0324 67. 39.21 24.74 15.00 10.88 73.95 52. 37.19 24.65 GPT 5 o4 mini 82.89 67. 54.04 42.98 34.39 84.91 72.37 62. 55.26 84.82 70.79 57.11 47.98 41. 88.51 74.74 61.23 50.09 41.84 14. 48.51 o3 mini high 80.70 60.61 45.88 36. 28.25 82.46 66.32 53.16 42.11 34. GPT 4.1 81.40 59.91 47.81 35.44 28. 82.63 65.26 50.88 39.82 31.58 GPT 4.1 mini 78.16 56.23 41.49 30.26 21.75 79. 59.39 44.74 33.68 25.53 GPT 4o 77. 55.00 39.56 27.63 20.79 85.09 68. 52.72 40.88 30.70 GPT 4o mini 76.40 53. 38.68 29.30 21.84 78.16 59.74 44. Grok 4 87.11 73.42 60.18 51.84 43. 88.51 76.40 66.05 32.54 55.96 23. 47.19 Grok 3 mini beta 82.81 64.21 48.86 36. 28.42 79.21 61.40 46.93 34.91 25. Qwen 3 235B A22B 83.95 66.75 52.28 42.28 31. 85.09 67.63 51.84 41.32 32.28 Qwen 3 32B 76.75 53.86 36.49 26.58 20.70 82. 65.79 51.49 39.82 30.70 Qwen 3 30B A3B 73. 52.46 36.23 25.79 19.56 79.91 62. 48.16 37.46 29.56 Qwen 2.5 72B Instruct 73.68 53. 37.37 24.56 16.84 79.47 60.53 45. 33.25 24.21 Qwen 2.5 Coder 71.40 44.82 30. 20.09 12.81 73.33 52.46 36.93 24. 15.88 Gemma 3 27B Gemma 3 12B 68.42 44.56 27. 16.93 10.96 73.60 48.42 33.33 21. 14.12 65.96 44.39 27.98 18.42 11. 67.54 46.75 31.58 20.09 12.81 Mistral Medium 73.60 51.93 36.32 25.09 16.05 76. 58.33 41.58 28.60 19.30 MiniMax M1 74. 51.23 37.98 28.07 20.35 Kimi K2 85. 68.86 53.68 41.23 30.18 77.63 89. 57.19 77.11 42.11 66.40 31.75 22. 53.95 44.04 Table 8 Task-level IF scores on BigVibeBench. Higher scores are better. 30 Vibe Checker: Aligning Code Evaluation with Human Preference Models Single-Turn Generation Multi-Turn Editing 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst Gemini 2.5 Pro 75.83 74.60 76.21 77.37 76. 78.96 77.87 78.99 78.89 78.98 Gemini 2.5 Flash 66.54 67.11 67.84 68.01 68.13 72. 71.42 69.64 68.93 67.89 Gemini 2.0 Flash 61. 61.37 61.42 62.01 62.48 74.41 71. 70.36 69.45 69.52 Gemini 2.0 Flash Lite 62.94 63. 65.28 65.40 65.69 67.30 64.36 63. 61.37 61.19 Claude 4 Opus 78.86 76.02 77. 77.65 78.75 85.59 85.36 85.21 84. 84.11 Claude 4 Sonnet 75.73 74.69 75.29 74. 75.20 84.45 85.40 85.24 84.50 83. Claude 3.7 Sonnet 72.42 68.53 68.18 68.06 68. 79.53 78.48 77.91 77.18 76.76 Claude 3.5 Sonnet 70.52 68.25 68.56 68.15 67.28 80. 76.40 75.23 74.29 73.12 Claude 3.5 Haiku 63. 60.19 61.45 61.80 62.77 78.67 75. 72.51 70.52 68.99 Claude 3 Haiku 61.61 59. 60.98 60.97 60.45 72.80 71.28 69. 67.39 67.45 DeepSeek V3 0324 52.80 54.74 55. 55.81 55.79 70.05 66.49 65.72 64. 62.71 GPT 5 o4 mini GPT 4.1 82.18 82. 81.86 82.09 82.82 85.59 86.30 87. 85.85 85.76 73.18 72.27 73.33 73. 73.82 81.52 80.47 79.53 76.99 75. 68.63 65.12 66.76 66.30 66.67 74. 72.89 71.97 71.75 70.81 GPT 4.1 mini 67. 66.40 68.63 67.89 67.41 71.75 69. 69.23 68.53 67.72 GPT 4o 60.85 60. 61.48 61.75 61.93 76.40 73.13 71. 70.31 69.93 GPT 4o mini 65.88 65.40 65. 65.64 65.63 73.93 71.85 70.36 68. 67.22 Grok 3 mini beta 70.05 70.05 69.61 69. 68.99 78.67 75.45 73.46 72.23 71. Qwen 3 30B A3B 67.77 62.89 63.76 64.10 63. 73.27 71.04 70.74 68.98 68.91 Qwen 2.5 72B Instruct 64.83 63.65 65.97 64.88 66.14 74. 69.43 69.38 67.70 67.51 Gemma 3 27B 61. 62.09 62.53 63.51 63.56 66.92 64. 64.01 63.44 63.41 Gemma 3 12B 61.33 62. 62.46 63.25 62.29 66.92 63.65 63. 61.73 60.76 Mistral Medium 3 62.37 61.28 61. 62.44 62.45 69.67 64.31 63.95 63. 63.37 Kimi K2 62.75 63.65 64.80 64. 64.76 76.97 74.64 74.31 72.94 73. Table 9 Instruction-level IF scores on LiveVibeBench. Higher is better. 31 Vibe Checker: Aligning Code Evaluation with Human Preference Models Single-Turn Generation Multi-Turn Editing 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst 1 Inst 2 Inst 3 Inst 4 Inst 5 Inst Gemini 2.5 Pro 75. 56.78 45.50 37.63 29.57 78.96 61. 51.18 41.04 32.80 Gemini 2.5 Flash 66.54 45. 32.89 23.03 17.06 72.80 51.09 34. 25.31 17.82 Gemini 2.0 Flash 61.71 37.25 22. 13.46 8.44 74.41 51.28 36.02 24. 17.73 Gemini 2.0 Flash Lite 62.94 41.33 27.68 18. 12.89 67.30 42.75 27.11 17.35 10. Claude 4 Opus 78.86 57.91 47.96 38.96 35. 85.59 72.89 61.71 52.04 43.70 Claude 4 Sonnet 75.73 56.40 44.17 35.36 28.53 84. 73.46 62.37 52.70 44.64 Claude 3.7 Sonnet 72. 47.01 31.85 23.51 18.96 79.53 62. 48.53 38.58 30.33 Claude 3.5 Sonnet 70.52 47. 31.94 22.37 14.88 80.57 60.28 44. 35.73 27.20 Claude 3.5 Haiku 63.22 35.92 22. 16.40 11.66 78.67 58.58 41.23 30. 22.46 Claude 3 Haiku 61.61 36.68 23.13 15. DeepSeek V3 0324 52.80 29.76 19.24 11.28 9. 7.77 72.80 52.32 36.59 26.82 18. 70.05 45.31 31.56 20.38 11.37 GPT o4 mini GPT 4.1 82.18 68.53 55.17 47. 40.95 85.59 74.50 66.64 57.35 50. 73.18 53.93 43.22 33.36 27.20 81. 66.64 54.60 42.84 32.61 68.63 42. 29.48 20.57 13.65 74.12 54.31 41. 32.70 24.08 GPT 4.1 mini 67.20 43.60 30. 21.99 14.88 71.75 49.67 34.60 26. 18.20 GPT 4o 60.85 36.40 22.75 14. 9.95 76.40 53.93 37.91 28.44 20. GPT 4o mini 65.88 42.65 28.25 20.19 14. 73.93 52.32 36.21 24.93 17.06 Grok 3 mini beta 70.05 51.09 38.10 28.15 20.66 78. 58.39 43.89 32.99 24.64 Qwen 3 30B A3B 67. 42.09 29.67 22.65 14.50 73.27 52. 39.72 29.10 21.90 Qwen 2.5 72B Instruct 64.83 40. 28.63 18.86 15.92 74.50 50.24 37. 26.73 19.24 Gemma 3 27B 61.99 37.73 24. 16.30 11.00 66.92 41.71 27.30 17. 12.32 Gemma 3 12B 61.33 38.20 24.17 16. Mistral Medium 3 62.37 37.25 23.13 15.83 9. 9.86 66.92 41.71 26.54 16.40 10. 69.67 42.94 27.96 18.48 11.66 Kimi 62.75 41.61 27.77 19.05 11.94 76. 57.35 44.17 35.73 27.87 Table 10 Task-level IF scores on LiveVibeBench. Higher is better. Vibe Checker: Aligning Code Evaluation with Human Preference E. Analysis E.1. Instruction Position Analysis As listed in Tables 11 and 12, we also provide detailed results for per-position instruction-level IF scores on both benchmarks. Models Single-Turn Generation Multi-Turn Editing Pos 1 Pos 2 Pos 3 Pos 4 Pos Pos 1 Pos 2 Pos 3 Pos 4 Pos 5 Gemini 2.5 Pro 81.40 79.91 78.86 78.60 78.60 79. 78.16 79.56 81.58 83.16 Gemini 2.5 Flash 79. 74.04 75.44 75.00 75.26 72.19 69. 72.81 73.25 78.25 Gemini 2.0 Flash 74.56 71. 72.02 70.96 71.40 72.81 71.75 74. 73.77 73.68 Gemini 2.0 Flash Lite Claude 4 Opus 72.11 87. 68.77 85.18 67.89 85.18 67.89 83. 67.81 86.23 68.16 67.81 69.12 68. 70.79 84.56 82.81 84.82 83.60 85. Claude 4 Sonnet 83.77 79.65 80.26 81.67 81. 84.21 80.18 83.95 84.39 87.19 Claude 3.7 Sonnet 77.81 73.16 72.19 73.07 75.09 77. 76.75 76.93 78.60 80.88 Claude 3.5 Sonnet 77. 72.81 72.72 75.18 75.35 76.40 75. 75.09 78.33 82.11 Claude 3.5 Haiku 65.00 62. 61.49 62.89 67.28 60.70 63.68 64. 65.79 73.16 Claude 3 Haiku 67.89 62.72 62. 63.86 65.88 72.11 66.40 69.47 68. 75.88 DeepSeek R1 0528 69.30 66.58 66.58 67. 67.98 68.86 68.33 70.53 71.32 76. DeepSeek V3 0324 67.63 62.81 64.47 64.74 65. 82.72 81.40 81.05 81.58 82.11 GPT o4 mini 61.67 86.05 63.95 86.05 68. 87.02 69.12 85.96 75.61 86.84 88. 85.53 82.81 83.07 83.77 86.05 81. 80.44 82.11 84.04 o3 mini high 73.25 72. 69.04 71.84 72.11 77.46 74.65 77. 78.95 82.54 GPT 4.1 78.95 76.75 76. 78.33 78.33 78.68 75.88 77.63 77. 79.56 GPT 4.1 mini 75.00 72.63 72.63 71. 75.35 75.26 70.26 71.84 72.02 76. GPT 4o 78.77 72.72 71.40 71.75 72. 78.42 75.79 77.19 78.42 81.93 GPT 4o mini 76.14 71.32 71.84 72.81 74.47 71. 71.84 72.89 73.51 78.16 Grok 4 86. 83.77 84.39 83.86 85.70 84.91 84. 85.70 84.74 86.93 Grok 3 mini beta 79.56 76. 76.40 75.44 79.47 73.07 72.89 72. 75.26 81.14 Qwen 3 235B A22B 82.54 78.16 77. 77.28 77.89 77.98 76.84 77.54 80. 81.49 Qwen 3 32B 73.68 70.61 71.75 69. 70.88 76.49 73.42 76.23 77.02 78. Qwen 3 30B A3B 71.67 67.81 68.60 68.95 72. 77.19 74.39 77.02 75.53 78.60 Qwen 2.5 72B Instruct 73.07 71.05 69.47 68.33 69.74 73. 70.88 72.89 72.72 74.21 Qwen 2.5 Coder 68. 64.74 64.74 64.39 66.93 67.98 65. 69.12 69.56 71.67 Gemma 3 27B Gemma 3 12B 68. 64.30 63.86 64.74 64.12 65.96 65. 67.63 66.75 67.89 67.98 66.14 64. 63.60 62.98 65.00 62.98 65.35 63. 67.63 Mistral Medium 3 74.65 69.82 68.42 70. 69.04 70.88 68.86 69.74 71.75 77. MiniMax M1 72.37 70.70 70.09 70.00 71. 70.61 70.61 72.89 73.86 76.75 Kimi 83.42 79.12 77.46 78.25 77.46 84. 83.86 82.19 83.51 86.93 Table 11 Instruction-position analysis on BigVibeBench. We report instruction-level IF scores under the setting of five instructions, comparing positions 15 in each setting. In Single-Turn Generation, position ð‘– denotes the ð‘–-th item in the numbered instruction list given to the model. In Multi-Turn Editing, position ð‘– indicates the ð‘–-th instruction introduced as separate turn. Vibe Checker: Aligning Code Evaluation with Human Preference Models Single-Turn Generation Multi-Turn Editing Pos 1 Pos Pos 3 Pos 4 Pos 5 Pos 1 Pos 2 Pos Pos 4 Pos 5 Gemini 2.5 Pro 76.78 75.83 77. 76.97 77.25 76.49 76.59 78.77 80. 82.46 Gemini 2.5 Flash 70.52 67.39 69.57 67. 65.59 66.16 64.93 67.39 67.87 73. Gemini 2.0 Flash 63.41 60.95 63.79 61.33 62. 69.10 67.87 68.53 69.76 72.32 Gemini 2.0 Flash Lite 67.39 65.21 65.97 63.60 66.26 60. 60.09 61.42 60.19 63.89 Claude 4 Opus 79. 77.91 78.29 77.06 80.57 84.83 84. 83.41 82.75 84.93 Claude 4 Sonnet 75.64 75. 75.26 74.03 75.64 81.90 85.31 84. 82.75 85.21 Claude 3.7 Sonnet 70.81 65.97 69. 67.01 68.72 76.30 76.40 76.02 75. 79.34 Claude 3.5 Sonnet 67.87 67.01 67.77 65. 67.96 72.32 71.94 71.94 72.51 76. Claude 3.5 Haiku 63.03 62.46 61.52 62.65 64. 67.58 66.26 68.06 68.44 74.60 Claude 3 Haiku 62.65 57.82 60.66 60.38 60.76 68. 66.35 68.25 64.74 69.29 DeepSeek V3 0324 55. 56.11 56.11 55.55 55.55 58.29 59. 62.75 64.74 68.44 GPT 5 o4 mini GPT 4. 83.51 82.37 83.70 81.33 83.22 86. 86.92 86.07 84.55 85.02 75.17 74. 74.41 71.56 73.65 73.46 74.31 76. 75.55 79.62 67.20 65.97 68.44 64. 67.20 71.28 69.76 72.42 69.29 71. GPT 4.1 mini 68.82 67.49 69.95 63.79 67. 67.30 66.54 69.38 65.97 69.38 GPT 4o 62.84 61.61 63.32 60.28 61.61 68. 68.15 71.09 68.06 73.93 GPT 4o mini 67. 64.83 66.26 64.45 65.59 67.30 65. 67.49 64.83 70.52 Grok 3 mini beta 69.10 67. 69.48 68.44 70.52 67.11 69.86 69. 72.23 76.49 Qwen 3 30B A3B 62.94 63.03 64. 62.65 61.99 66.73 68.06 69.86 69. 70.71 Qwen 2.5 72B Instruct 66.54 64.08 68.25 65. 65.97 66.35 65.88 68.82 67.20 69. Gemma 3 27B 65.97 62.56 64.17 63.41 61. 62.27 61.80 62.65 63.13 67.20 Gemma 3 12B 64.36 62.37 63.13 60.76 60.85 60. 58.67 60.57 58.96 64.83 Mistral Medium 3 63. 61.23 63.41 61.71 62.56 60.38 58. 63.79 64.83 69.19 Kimi K2 66.64 64. 65.97 62.94 64.08 72.61 72.51 74. 73.27 75.17 Table 12 Instruction-position analysis on LiveVibeBench. We report instruction-level IF scores under the setting of five instructions, comparing positions 15 in each setting. In Single-Turn Generation, position ð‘– denotes the ð‘–-th item in the numbered instruction list given to the model. In Multi-Turn Editing, position ð‘– indicates the ð‘–-th instruction introduced as separate turn. 34 Vibe Checker: Aligning Code Evaluation with Human Preference E.2. Correlation Analysis (a) BigVibeBench w/ Style Control (b) LiveVibeBench w/ Style Control (c) BigVibeBench w/o Style Control (d) LiveVibeBench w/o Style Control Figure 15 Human preference aligns best with mix of IF and functionality. We correlate LMArena coding Elo with composite score ð›¼ IF + (1 ð›¼) Func, where ð›¼ [0, 1] is the weight on IF (x-axis). We also vary the correlation type and toggle the style-control function on/off. Nevertheless, the peak correlation (starred) consistently occurs at mixture of the two metrics across all settings."
        }
    ],
    "affiliations": [
        "Google",
        "Google DeepMind",
        "University of Illinois Urbana-Champaign (UIUC)",
        "University of Southern California (USC)"
    ]
}