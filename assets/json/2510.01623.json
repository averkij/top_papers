{
    "paper_title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
    "authors": [
        "Angen Ye",
        "Zeyu Zhang",
        "Boyuan Wang",
        "Xiaofeng Wang",
        "Dapeng Zhang",
        "Zheng Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1."
        },
        {
            "title": "Start",
            "content": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models Angen Ye12 Zeyu Zhang1 Boyuan Wang12 Xiaofeng Wang13 Dapeng Zhang2 Zheng Zhu1 1GigaAI 2CASIA 3Tsinghua University Equal contribution. Corresponding author: zhengzhu@ieee.org. 5 2 0 O 2 ] . [ 1 3 2 6 1 0 . 0 1 5 2 : r Abstract Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and crossscene generalization with broad impact on embodied AI. However, current VLA models often lack explicit stepby-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-ofdomain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and realworld performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github. com/GigaAI-research/VLA-R1. Website: https:// gigaai-research.github.io/VLA-R1. formatting, I. INTRODUCTION VisionLanguageAction (VLA) models unify perception, language, and action. They first learn openvocabulary semantics and cross-modal alignment from internet-scale imagetext pretraining. These semantics are then grounded into the action space through multitask manipulation data. This enables analogical transfer to unseen objects and compositional generalization to novel commands. Compared with modular pipelines [6], [41] or state-driven policies [9], VLAs show stronger cross-task and cross-scene generalization [35], [17], [18], [26], [16], [37]. Representative works include VoxPoser [14] for zero-shot trajectory planning, and ManipLVM-R1 [35] and RoboBrain [17] for integrating affordance perception and pose estimation. Meanwhile, Reinforcement Learning from Verifiable Rewards (RLVR) enhances reasoning and generalization in visionlanguage models. Vision-R1 [15] matches larger Fig. 1. VLA-R1: pipeline from instruction to execution, with benchmark comparisons against baselines. models through cold-start data and progressive training; LMM-R1 [30] employs two-stage regimen from textual reasoning to multimodal tasks; and VLM-R1 [33] applies R1-style reinforcement to visual grounding, boosting open-vocabulary detection. However, existing VLA models present two significant challenges. First, they often lack step-by-step reasoning: models tend to emit final actions directly without explicit inference over affordance constraints, geometric relations, or container selection. This limitation leads to instruction-disambiguation failures under color similarity, duplicate instances, or multiple candidate receptacles. Second, post-training rarely provides systematic reinforcement of reasoning. Current method relies on supervised fine-tuning (SFT) with little reward optimization targeted at reasoning quality and execution efficacy. Even when Reinforcement Learning (RL) is used, reward design is typically single-objective and struggles to jointly optimize region alignment and trajectory consistency, degrading performance on out-ofdistribution data and in the real world. To address these challenges, we propose VLA-R1, post-training-enhanced VLA model capable of stepby-step reasoning. Unlike prior approaches, VLA-R1 simultaneously emphasizes data-level Chain-of-Thought (CoT) supervision and optimization-level reward alignment, bridging the gap between reasoning and execution. This enables the model to not only provide answers but also explain them, making it robust to challenges like color similarity, repeated instances, and multiple receptacle choices during reasoning. To further enhance the models reasoning capabilities, we introduce an RLVR-based post-training strategy at the optimization layer. Specifically, we employ Group Relative Policy Optimization (GRPO) [32] with three verifiable rewards: an affordance reward based on Generalized Intersection over Union (GIoU) [31] to provide informative gradients for non-overlapping predicted and ground truth affordance regions, speeding up learning; distance-based reward using the improved Fr√©chet distance to ensure reasonable trajectory curvature and segment length; and an output-format reward to enforce well-formed reasoning and action specifications. These optimizations enable VLA-R1 to generate accurate affordance regions and well-formed execution trajectories, enhancing decision-making. Moreover, many existing datasets, although large in scale, fail to fully support complex reasoning tasks due to the lack of detailed explanations and reasoning processes in their annotations. To address this, we develop the VLA-CoT data engine, which generates the highquality VLA-CoT-13K dataset, making reasoning steps explicit. The engine aligns CoT with affordance and trajectory annotations, encouraging the model to learn task-consistent reasoning and enabling it to acquire basic reasoning capabilities during the SFT phase. Finally, we conduct comprehensive evaluations of VLA-R1 across in-domain, out-of-domain, simulation, and real-robot settings. Empirically, VLA-R1 achieves an IoU of 36.51 on the in-domain affordance benchmark, 17.78% improvement over the baseline; on the in-domain trajectory benchmark it attains an Average TABLE COMPARISON OF DATASETS ON AFFORDANCE, TRAJECTORY, REASONING, SCENES, AND ROBOTS. INDICATES THE DATASET INCLUDES THAT ANNOTATION; INDICATES IT DOES NOT. #SCENES COUNTS DISTINCT ENVIRONMENTS. 24+ FOR VLA-IT MEANS AT LEAST 24 KNOWN ENVIRONMENTS (FROM BRIDGEDATA V2) WITH ADDITIONAL RT-1 SITES NOT CONSOLIDATED. IF FOLLOWING THE OFFICIAL UMD RELEASE, SET #SCENES TO 3. Dataset #Aff #Traj #Reasoning #Scenes #Robot UMD VAIT VLA-IT ShareRobot VLA-CoT-13K 4 24+ 102 102 13 2 12 12 distance of 91.74 (lower is better), reducing the baseline by 17.25%. It also delivers state-of-the-art (SOTA) performance in the out-of-domain setting. On physical hardware, VLA-R1 reaches 62.5% success for affordance perception and 75% for trajectory execution. These results demonstrate the methods effectiveness under controlled conditions and its robustness and practicality across domains and real-world scenarios."
        },
        {
            "title": "Contributions in our paper can be summarized in the",
            "content": "following three folds: that We propose VLA-R1, VLA foundation model that VLA foundation model introduces an RLVR optimization scheme with carefully designed rewards (region alignment, trajectory consistency, and output formatting), augmented by GRPO, to systematically strengthen reasoning and execution robustness while reducing reliance on manual annotation. We introduce the VLA-CoT data engine, which produces high-quality VLA-CoT-13K aligned with affordance and trajectory labels and incorporates verifiable rewards, explicitly remedying the lack of step-wise reasoning in existing VLA models. We comprehensively evaluate VLA-R1 on indomain and out-of-domain datasets, in simulation, and on real-robot platforms, empirically verifying its effectiveness and cross-domain generalization. II. RELATED WORK A. VLA Models Early manipulation research often relied on statebased reinforcement learning [8], [1], but these methods inputs. More struggled with high-dimensional visual recently, vision-centric approaches have become dominant, harnessing the reasoning capabilities of large language models (LLMs) to improve generalization [5], [20], [39], [14] uses vision- [24]. VoxPoser [36], language models to generate 3D value maps, enabling zero-shot trajectory planning. RoboFlamingo [21] finetunes on manipulation datasets to perform languageconditioned tasks, while ManipLLM [20] incorporates chain-of-thought reasoning to integrate object understanding, affordance perception, and pose prediction into an interpretable framework. Building on this line, OpenVLA [18] and RoboMamba [24] leverage finegrained CoT data and supervised fine-tuning for further performance gains [34]. Other works, such as Embodied-Reasoner [42], Cosmos-Reason1 [2], and RoboBrain [17], focus on long-horizon reasoning, interpretability, and logical consistency in manipulation tasks. Despite progress, most approaches still depend on large-scale annotated datasets. In contrast, ManipLVMR1 [35] reduces reliance on supervision by combining small amounts of labeled data with RLVR-based selfimprovement, yielding robust generalization under limited supervision. B. RLHF for VLMs Large vision-language models (LVLMs) have demonstrated remarkable reasoning capabilities across diverse visual tasks [23], [22], [19], [3], [38], [11], [13], [40], [25]. Recently, reinforcement learning with verifiable rewards (RLVR) has emerged as promising way to enhance their reasoning abilities [12], [7]. For example, Vision-R1 [15] leverages cold-start math dataset and Progressive Thinking Suppression Training to achieve results comparable to much larger models without relying on human annotations. LMM-R1 [30] adopts two-stage framework, first refining reasoning on textual data and then extending to multimodal and agent-based reasoning tasks. Similarly, VLM-R1 [33] applies an R1style reinforcement learning approach to visual grounding, improving open-vocabulary detection and generalization. While these works highlight RLVRs potential, their scope remains limited to non-embodied domains. To bridge this gap, ManipLVM-R1 [35], adapts RLVR to robotic manipulation, enhancing both reasoning and action execution. LVLMs have demonstrated remarkable reasoning capabilities across diverse visual tasks [23], [22], [19], [3], [38]. Recently, RLVR has emerged as promising way to enhance their reasoning abilities. For example, Vision-R1 [15] leverages cold-start math dataset and Progressive Thinking Suppression Training to achieve results comparable to much larger models without relying on human annotations. LMM-R1 [30] adopts two-stage framework, first refining reasoning on textual data and then extending to multimodal and agent-based reasoning tasks. Similarly, VLM-R1 [33] applies an R1-style reinforcement learning approach to visual grounding, improving open-vocabulary detection CoT Data Engine. After ingesting multimodal data, the Fig. 2. system parses tasks based on type (e.g., affordance or trajectory), performs scene understanding and localization, validates feasibility, and generates structured CoT traces for training. and generalization. While these works highlight RLVRs potential, their scope remains limited to non-embodied domains. To bridge this gap, ManipLVM-R1 [35], adapts RLVR to robotic manipulation, enhancing both reasoning and action execution. III. METHOD A. Overview The overall architecture of VLA-R1 is shown in Fig. 3. Given an input image and natural language instruction, VLA-R1 encodes multimodal information through vision-language backbone and then produces low-level control signals via an action decoder. Specifically, the vision branch processes raw images through visual encoder that projects features into shared embedding space. In parallel, the language branch tokenizes and embeds the task instruction. Both modalities are fused in the multimodal decoder, which jointly reasons over visual cues, textual context, and temporal history to generate structured output consisting of reasoning segment and an action prediction. The reasoning trace makes intermediate steps explicit, while the action output is represented in discrete token space. Finally, the action de-tokenizer maps the predicted tokens into continuous 7D robot actions (x, Œ∏, and Grip), which can be directly executed on the robot arm. This design allows VLA-R1 to bridge high-level task descriptions with grounded low-level control, while maintaining interpretability through explicit reasoning traces. B. Data Synthesis To further strengthen the reasoning ability of our model, we construct CoT dataset using Qwen2.5-VL72B. As shown in Table and Figure 2, we employ the model to automatically generate intermediate reasoning steps for both affordance and trajectory tasks. In total, 13K CoT annotations are produced, which serve as high-quality supervision to bridge perception and Fig. 3. Overall architecture of VLA-R1. Training has two stages: Stage 1 uses SFT with CoT supervision to learn reasoning over images and instructions; Stage 2 refines reasoning and actions via RL with verifiable rewards (GRPO). During inference, control stack converts outputs into joint-level robot commands. action. These CoT data not only enrich the semantic interpretability of the training corpus but also provide explicit step-by-step guidance, enabling the model to learn task-consistent reasoning patterns. attach an action decoder that we implement to map hidden states to control outputs for downstream tasks. This initialization provides strong balance of accuracy and efficiency for long temporal contexts. C. Supervised Fine-Tuning D. Reinforcement Learning We perform supervised fine tuning on our synthetic high quality VLA-CoT-13K dataset, which presents step by step think chains paired with grounded visual evidence and action targets. Compared with naive question and answer instruction tuning, chain of thought provides intermediate supervision signals that encourage explicit decomposition, stronger visual grounding, and stable credit assignment across time. This produces policies that reason before acting, which improves sample efficiency and prepares the model for subsequent post training under verifiable rewards. In practice we supervise both the structured <think> segment and the final <output> or action segment, which regularizes reasoning style, reduces hallucination, and yields more reliable action decoding under long horizon inputs. We initialize our foundation model with Qwen2.5VL-3B [4]. The vision pathway is redesigned Vision Transformer with window attention and 2D RoPE that supports native input resolution and dynamic frame rate sampling for videos. Visual tokens are softly compressed by an MLP merger before being fed into the language decoder. The text side adopts the Qwen2.5 tokenizer with large vocabulary and the standard Qwen2.5 decoder stack. On top of the multimodal decoder we After SFT, we further optimize VLA-R1 through RL, as shown in Fig. 3. We adopt the GRPO algorithm, recently proposed by DeepSeek [10], [32] as scalable variant of RLHF. We extend this approach to multimodal action reasoning, allowing the model to benefit from structured verifiable rewards while maintaining training stability. For input q, GRPO samples {o1, . . . , on} from œÄold, scores each with reward function to get rg. Normalize via intra-group mean and std œÉr: ÀÜAg = (rg r)/œÉr. For process supervision, step-wise rewards are normalized similarly, with token-wise advantages accumulated and shared across outputs. For the k-th token of the g-th output, the new/old policy probability ratio is: rg,k(Œ∏) = œÄŒ∏(og,k q, og,<k) œÄold(og,k q, og,<k) . (1) GRPOs objective: LGRPO(Œ∏) = (cid:88) g= 1 og og (cid:88) (cid:104) (cid:16) min k= rg,k(Œ∏) ÀÜAg,k, clip(cid:0)rg,k(Œ∏), 1 Œµ, 1 + Œµ(cid:1) ÀÜAg,k (cid:0)œÄŒ∏ œÄref Œ≤ DKL (cid:1)(cid:105) . (cid:17) (2) Fig. 4. Case Analysis: The figure illustrates VLA-R1s reasoning process and outcomes for both affordance and trajectory tasks. VLA-R1 parses the action requirements, infers relevant objects and spatial relations, and outputs the corresponding bounding boxes or waypoint sequences. The affordance form and trajectory form are fixed prompt templates that instruct the model to produce outputs in specified format. where clip() bounds the ratio to [1 Œµ, 1 + Œµ], and the last term is KL penalty to avoid excessive policy drift. Fr√©chet Trajectory Reward. The primary reward measures alignment using Angle-Length Augmented Fr√©chet distance (ALAF). Unlike pointwise Euclidean losses, ALAF respects the temporal ordering of the curves and augments it with local geometry. We represent each trajectory as sequence of triples = {pi, ti, ‚Ñìi}, where pi is the 2D waypoint (normalized image coordinates), ti is the unit motion direction at pi (forward/backward difference at endpoints and normalized blend of adjacent segment directions for interior vertices), and ‚Ñìi is the local segment length (distance to the next waypoint; for the last vertex, to the previous one). ALAF combines the positional discrete Fr√©chet term with an angle penalty between unit tangents and scale penalty based on the log ratio of neighboring segment lengths, weighted by ŒªŒ∏ and Œªr; see Eq. (3). DALHF( ÀÜT , ) = min Œ¶ max (i,j)Œ¶ (cid:104) + ŒªŒ∏ arccos (cid:124) (cid:16) ÀÜtit ÀÜti (cid:123)(cid:122) angle (cid:17) (cid:125) 2 (cid:125) ÀÜpi (cid:123)(cid:122) (cid:124) position (cid:12) )(cid:12) (cid:12) log(ÀÜ‚Ñìi/‚Ñì (cid:12) (cid:123)(cid:122) (cid:125) (cid:124) length ratio +Œªr (3) (cid:105) , where Œ¶ denotes all order-preserving couplings between the sequences. ÀÜT = {ÀÜpi, ÀÜti, ÀÜ‚Ñìi} denotes the groundtruth trajectory. = {p , }denotes the predicted trajectory. The trajectory reward is defined as , ‚Ñì Rtraj = 1 DALAF( ÀÜT , ). (4) Here, DALAF denotes the ALAF distance normalized to [0, 1]; larger Rtraj indicates better alignment. GIoU Affordance Reward. For spatial grounding, we introduce GIoU [31] reward between predicted and ground-truth bounding boxes. While IoU only considers the overlapping region, GIoU additionally accounts for Fig. 5. Visualization of evaluation in real-world scenarios. the smallest enclosing box, penalizing misaligned predictions even when boxes do not overlap. This improves spatial robustness, especially in cluttered environments where partial overlaps are common: RGIoU = GIoU(ÀÜb, b). (5) responses that Format Reward. Finally, we enforce structural correward. The model must rectness with format output follow the required structure (<think>...</think> reasoning segment followed by <output>...</output> action segment). The format reward is binary: Rformat = (cid:40) 1 if the output adheres to format, otherwise. (6) This encourages interpretable reasoning traces and prevents degenerate outputs during post-training. IV. EXPERIMENT To rigorously evaluate the effectiveness and generalization capacity of the proposed approach, we conduct experiments across 4 settings: in-domain datasets, outof-domain datasets, simulation environments, and realrobot platforms. We compare with strong baselines and ablate each component to show its impact. A. Dataset and Metrics 1) In domain datasets: All baseline models and our proposed VLA-R1 are trained on the ShareRobot dataset[17], large-scale corpus purpose-built to advance affordance perception and trajectory prediction. ShareRobot is curated from 23 selected datasets within Open X-Embodiment[29], spanning 12 robotic embodiments, 102 manipulation scenarios, and hundreds of primitive actions; it undergoes multiple rounds of human auditing to ensure high image resolution, successful task execution, accurate annotations, and complete, clean trajectory traces. The corpus comprises over one million planning questionanswer pairs, 6,522 images with affordance annotations, and 6,870 images with trajectory annotations. In our experiments, we restrict training to the affordanceand trajectory-annotated image subsets and, on this basis, synthesize CoT rationales for model training. 2) Out of domain datasets: To assess generalization, we conduct out-of-domain (OOD) evaluations. For affordance perception, we adopt subset of the UMD Part Affordance dataset[27] as the OOD benchmark. UMD spans 105 tools commonly encountered in gardening, kitchen, and workshop contexts. We construct our OOD test set by randomly sampling 1,200 examples from four affordance categoriesgrasp, cut, pound, and scoop. For trajectory prediction, we evaluate on VAIT, the validation split of LLARVAs pretraining corpus[28]. As VAIT originates from the highly diverse Open X-Embodiment collection, we select 500 samples and manually rectify trajectories exhibiting excessive deviation to ensure fair and meaningful evaluation. 3) Metric Setting: For affordance perception, we adopt Intersection over Union (IoU) as the principal metric. IoU quantifies spatial localization fidelity by measuring the overlap between predicted and groundtruth regions; higher values indicate more accurate detection and alignment. For trajectory prediction, we evaluate the concordance between predicted and groundtruth trajectories. Following prior work[17], [35], trajectory is represented as an ordered set of 2D waypoints normalized to the interval [0, 1000). Similarity is assessed using three complementary metrics: Discrete Fr√©chet Distance (DFD), capturing global shape and TABLE II SIMULATION EVALUATION ON DIFFERENT ROBOT PLATFORMS. Model Task Piper UR5 avg VLA-R1 NORA affordance trajectory affordance trajectory 60% 80% 50% 10% 50% 55% 60% 70% 30% 40% 5% 0% R1-3B achieves the best results across all metrics: IoU = 36.51, DFD = 106.2, HD = 97.9, and RMSE = 71.12. Relative to the strong baseline ManipLVM-R1, IoU improves by 17.78%, and the overall trajectory error is reduced by 17.25%, attesting to the effectiveness of our training paradigm.From the OOD results, despite substantial distribution shifts, VLA-R1-3B remains superior on trajectory prediction: IoU increases to 33.96, while DFD, HD, and RMSE decrease to 114.3, 98.43, and 68.97, respectivelysurpassing the strongest baseline, ManipLVM-R1-3B, and demonstrating strong cross-domain generalization and robustness. For the two types of tasks, the analysis process of VLA-R1 can be seen in Fig 4. C. Experiment on Simulation Implementation Details. To assess performance under controlled yet stochastic conditions, we conducted additional experiments in simulated tabletop environment. Using the RoboTwin simulator, we instantiated single randomized tabletop clutter generator that dynamically varies object categories, colors, poses/positions, and the table color throughout each trial. To evaluate the crossrobot generality of VLA-R1, we tested two robotic embodimentsPiper and UR5. Each embodiment was evaluated over ten independent trials with randomized initialization. Experiment Results. Because our training data are drawn entirely from real-world settings, the simulated environment exhibits greater variability; nevertheless, as shown in Table II anf Fig. 6, VLA-R1 attains strong performance on both tasks. For affordance perception, VLA-R1 achieves 6/10 successes on Piper and 5/10 on UR5 (average SR = 55%). For trajectory execution, performance improves to 8/10 on Piper and 6/10 on UR5 (average SR = 70%), indicating that once reliable grasp is established, the trajectory policy remains highly stable in simulation. By contrast, the NORA baseline performs notably worse under the same conditions: on the affordance task, SR drops to 50% (Piper) and 30% (UR5); on the trajectory task, it records 1/10 on Piper and 0/10 on UR5. Overall, these results confirm that VLA-R1 preserves robust cross-embodiment stability and superior generalization, even under heightened environmental variation. Fig. 6. Visualization of simulation. temporal alignment; Hausdorff Distance (HD), measuring the maximum pointwise deviation; and Root Mean Square Error (RMSE), quantifying the average pointwise error. Together, these metrics furnish comprehensive assessment across global shape, worst-case discrepancy, and average error, characterizing both the accuracy and consistency of trajectory prediction. In both real-world and simulated evaluations, we report Success Rate (SR) as the task-level metric, defined as the ratio of successful executions to total trials. Success is determined as follows: Affordance tasks: trial is deemed successful if an object is present in the scene, the predicted bounding box correctly localizes the target object, and the system successfully grasps it; if no object is present, the model should emit no bounding box, which is likewise counted as success. Trajectory tasks: trial if the executed trajectory terminates within the designated goal location (or region) and the target object is reliably transported to that endpoint. is deemed successful B. Experiment on Benchmark Implementation Details. To ensure fair comparison, we curate contemporary suite of baselines. Specifically, we evaluate Gemma-3-12B-it, Gemma-3-27B-it, Phi-4multimodal-Instruct, and the Qwen2.5-VL-3B-Instruct, Qwen2.5-VL-32B-Instruct. All open-source models are assessed under few-shot prompting to furnish minimal perception prior. To validate the effectiveness of our training framework, we further include supervised fine-tuning baselinesInternVL2-2B , LLaVA-1.6-7B, RoboBrain-7B , and NORA-3Bas well as an RL posttrained model, ManipLVM-R1-3B. Experiment Results. As shown in the table III, opensource multimodal instruction-following models perform poorly on the in-domain dataset: despite large parameter counts, IoU remains below 10, and trajectory errors (DFD, HD, RMSE) are uniformly high. This indicates that generic models alone are inadequate for the precision demands of embodied tasks. Supervised finetuning (SFT) yields clear gainse.g., RoboBrain-7B and NORA-3B attain higher IoU and lower trajectory errors than open-source baselinesyet their IoU typically remains in the 525 range. By contrast, VLATABLE III IN-DOMAIN AND OUT-OF-DOMAIN PERFORMANCE COMPARISON."
        },
        {
            "title": "Method",
            "content": "In-Domain"
        },
        {
            "title": "Out of Domain",
            "content": "IoU DFD HD RMSE Avg IoU DFD HD RMSE Avg Open-source Models Phi-4-multimodal-Instruct Gemma-3-12b-it Gemma-3-27b-it Qwen2.5-VL-3B-Instruct Qwen2.5-VL-32B-Instruct Supervised Fine-Tuning LLaVA-1.6-7B InternVL2-2B RoboBrain-7B NORA-3B 0.58 1.18 1.32 6.15 7.40 3.98 6.74 11.79 23.48 243.92 206.72 257.42 208.02 125.54 184.40 250.20 156.10 139.65 224.73 190.64 230.29 179.12 113.00 178.00 239.34 136.52 126. 189.27 154.96 184.47 144.14 85.05 133.28 194.74 106.71 92.97 228.21 184.10 224.09 175.37 107.86 165.23 228.09 133.11 120.45 2.17 4.65 8.20 23.96 25.14 5.90 15.25 22.00 22. 240.18 204.94 232.86 211.80 182.73 170.88 165.98 220.94 154.81 235.44 209.88 268.03 205.00 176.51 167.10 167.84 214.14 129.84 202.69 175.42 209.25 140.49 133.17 160.79 145.64 173.02 95. 226.10 193.75 250.67 250.67 164.14 166.25 157.50 202.70 126.77 Supervised Fine-Tuning + Reinforcement learning ManipLVM-R1-3B VLA-R1-3B 134.18 106.20 31.00 36.51 111.14 97. 87.28 71.12 110.87 91.74 28.00 33.96 146.82 114.30 140.52 98.43 108.64 68. 131.99 93.90 D. Experiment on the Real World Implementation Details. To comprehensively assess real-world performance, we design four canonical scenarios on tabletop platform. We instantiate: S1: Bowl picking, containing bowls of multiple colors placed in diverse locations; the model must grasp the userspecified color and, for trajectory tasks, place it precisely into designated frame/basket of given color. S2: Fruit picking, featuring repeated instances of the same fruit; the model must disambiguate and grasp the specified item and, for trajectory tasks, place it into the basket or onto the plate indicated by the instruction. S3: Kitchen scenario, comprising an open microwave, plates, and food props, where the model must contend with visual occlusion from the door and the spatial constraints of the cavity. S4: Mixed scenario, in which bowls, produce, baskets, and plates co-occur, requiring grasp-and-place under multi-category, multi-attribute distractors. Each scenario is evaluated over ten independent trials; we randomize initial object placements and poses and shuffle scenario order to mitigate potential ordering effects. Experiment Results. As shown in Table IV and Figure 5, VLA-R1 achieves an average Success Rate (SR) of 62.5% across the four scenarios for affordance perception. By contrast, trajectory prediction attains higher SR of 75%. The NORA-3B baseline records 35% on affordance perception and 47.5% on trajectory prediction. We observe that distractors such as color similarity and positional variation materially affect the models decisions, constituting the primary sources of error. Nevertheless, even under heavy clutter, VLA-R1s preTABLE IV REAL-WORLD EXPERIMENTS Model Task S1 S2 S3 S4 avg VLA-R affordance trajectory 80% 60% 70% 60% 62.5% 75% 60% 80% 80% 80% NORA affordance trajectory 40% 30% 30% 40% 35% 40% 50% 30% 70% 47.5% dictions typically concentrate near the target rather than diverging arbitrarily, indicating degree of tolerance and self-correction in perception and trajectory generation; when the target cannot be fully locked, the model still preserves reasonable local spatial consistency. Overall, these results validate the methods practical viability in real settings and its ability to maintain stability under attribute similarity and visual clutter. E. Ablation Study To rigorously assess the impact of Chain-of-Thought (CoT) reasoning and Reinforcement Learning (RL) on performance, we conduct an ablation study with three configurations: (1) without CoT and RL (w/o CoT & RL); (2) CoT only; and (3) CoT+RL. All models are trained under identical hyperparameters to ensure fair comparison. From the table V, using CoT alonerelative to the configuration without CoT or RLraises IoU from 23.74 to 28.37 and reduces the average distance metric from 128.38 to 124.6. The improvement is more pronounced for IoU, indicating that CoT confers degree of attribute disambiguation and thus benefits affordancethe model centric tasks. When combined with RL, TABLE ABLATION STUDY ON THE EFFECT OF COT REASONING AND RL. HIGHER IOU IS BETTER, LOWER DFD/HD/RMSE/AVG ARE BETTER. Method IoU DFD HD RMSE Avg w/o CoT & RL CoT only CoT + RL 23.74 28.37 36. 149.38 145.51 106.20 135.72 131.26 97.90 100.04 97.03 71.12 128.38 124.60 91.74 achieves substantial gains across all metrics, underscoring the complementarity of CoT and RLVR in trajectory prediction: CoT provides structured task decomposition and reasoning, while RLVR leverages reward signals to refine execution policies, producing significant end-toend performance improvements. V. LIMITATION AND FUTURE WORK While VLA-R1 demonstrates strong performance across benchmarks, simulation, and real-robot settings, key limitation is that it has not yet been developed or validated on other types of robotic platforms such as bimanual robot arms and quadruped robot dogs. Extending VLA-R1 to these embodiments represents an important direction for future work, enabling broader applicability and testing its generalization in more diverse real-world scenarios. VI. CONCLUSION that In this work, we introduced VLA-R1, reasoninginteenhanced VisionLanguageAction model grates chain-of-thought supervision with reinforcement learning from verifiable rewards. By designing the VLACoT-13K dataset and incorporating an RLVR-based post-training strategy, VLA-R1 explicitly strengthens both step-by-step reasoning and execution robustness. Comprehensive experiments across in-domain, out-ofdomain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves state-of-the-art performance and superior generalization. We believe this work provides promising step toward bridging the gap between reasoning quality and action execution in embodied AI."
        },
        {
            "title": "REFERENCES",
            "content": "[1] M. Andrychowicz, B. Baker, M. Chociej, et al., Learning dexterous in-hand manipulation, International Journal of Robotics Research, vol. 39, no. 1, pp. 320, 2020. [2] A. Azzolini, H. Brandon, P. Chattopadhyay, et al., Cosmosreason1: From physical common sense to embodied reasoning, arXiv preprint arXiv:2503.15558, 2025. [3] J. Bai, S. Bai, Y. Chu, et al., Qwen technical report, arXiv preprint arXiv:2309.16609, 2023. [4] S. Bai, K. Chen, X. Liu, J. Wang, et al., Qwen2. 5-vl technical report, arXiv preprint arXiv:2502.13923, 2025. [5] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, et al., Rt2: Vision-language-action models transfer web knowledge to robotic control, arXiv preprint arXiv:2307.15818, 2023. [6] C. R. Garrett, R. Chitnis, R. Holladay, B. Kim, T. Silver, L. P. Kaelbling, and T. Lozano-P√©rez, Integrated task and motion planning, Annual review of control, robotics, and autonomous systems, vol. 4, no. 1, pp. 265293, 2021. [7] J. Ge, T. Cheng, B. Wu, Z. Zhang, S. Huang, J. Bishop, G. Shepherd, M. Fang, L. Chen, and Y. Zhao, Vasevqa: Multimodal agent and benchmark for ancient greek pottery, arXiv preprint arXiv:2509.17191, 2025. [8] Y. Geng, B. An, H. Geng, et al., Rlafford: End-to-end affordance learning for robotic manipulation, in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2023, pp. 58805886. [9] S. Gu, E. Holly, T. Lillicrap, and S. Levine, Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates, in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 33893396. [10] D. Guo, D. Yang, H. Zhang, et al., Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, arXiv preprint arXiv:2501.12948, 2025. [11] T. Huang, Z. Zhang, et al., 3d coca: Contrastive learners are 3d captioners, arXiv preprint arXiv:2504.09518, 2025. [12] T. Huang, Z. Zhang, and H. Tang, 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding, arXiv preprint arXiv:2507.23478, 2025. [13] T. Huang, Z. Zhang, R. Zhang, and Y. Zhao, Dc-scene: Datacentric learning for 3d scene understanding, arXiv preprint arXiv:2505.15232, 2025. [14] W. Huang, C. Wang, R. Zhang, Y. Li, et al., Voxposer: Composable 3d value maps for robotic manipulation with language models, in Proceedings of the Conference on Robot Learning (CoRL), ser. Proceedings of Machine Learning Research, vol. 229, 2023, pp. 540562. [15] W. Huang, B. Jia, Z. Zhai, S. Cao, et al., Vision-r1: Incentivizing reasoning capability in multimodal large language models, arXiv preprint arXiv:2503.06749, 2025. [16] P. Intelligence, K. Black, N. Brown, J. Darpinian, K. Dhabalia, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, et al., œÄ0. 5: vision-language-action model with open-world generalization, 2025, arXiv preprint arXiv:2504.16054, 2025. [17] Y. Ji, H. Tan, J. Shi, X. Hao, et al., Robobrain: unified brain model for robotic manipulation from abstract to concrete, arXiv preprint arXiv:2502.21257, 2025. [18] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al., Openvla: An open-source vision-language-action model, arXiv preprint arXiv:2406.09246, 2024. [19] B. Li, Y. Zhang, D. Guo, et al., Llava-onevision: Easy visual task transfer, arXiv preprint arXiv:2408.03326, 2024. [20] X. Li, M. Zhang, Y. Geng, et al., Manipllm: Embodied mullarge language model for object-centric robotic matimodal nipulation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 18 06118 070. [21] X. Li, M. Liu, H. Zhang, Yu, et al., Vision-language founimitators, arXiv preprint dation models as effective robot arXiv:2311.01378, 2023. [22] H. Liu, C. Li, Y. Li, and Y. J. Lee, Improved baselines with visual instruction tuning, in Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition (CVPR), 2024, pp. 26 296 26 306. [23] H. Liu, C. Li, Q. Wu, and Y. J. Lee, Visual instruction tuning, in Advances in Neural Information Processing Systems (NeurIPS), vol. 36, 2024. [24] J. Liu, M. Liu, Z. Wang, Lee, et al., Robomamba: Multimodal state space model for efficient robot reasoning and manipulation, arXiv preprint arXiv:2406.04339, 2024. [25] Q. Liu, T. Huang, Z. Zhang, and H. Tang, Nav-r1: Reasoning and navigation in embodied scenes, arXiv preprint arXiv:2509.10884, 2025. [26] S. Liu, L. Wu, B. Li, et al., Rdt-1b: diffusion foundation model for bimanual manipulation, arXiv preprint arXiv:2410.07864, 2024. [27] A. Myers, C. L. Teo, C. Ferm√ºller, and Y. Aloimonos, Affordance detection of tool parts from geometric features, in 2015 IEEE international conference on robotics and automation (ICRA). IEEE, 2015, pp. 13741381. [28] D. Niu, Y. Sharma, G. Biamby, J. Quenum, et al., Llarva: Vision-action instruction tuning enhances robot learning, arXiv preprint arXiv:2406.11815, 2024. [29] A. ONeill, A. Rehman, A. Maddukuri, et al., Open XEmbodiment: Robotic learning datasets and RT-X models, in 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2024, pp. 68926903. [30] Y. Peng, G. Zhang, M. Zhang, et al., Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rulebased rl, arXiv preprint arXiv:2503.07536, 2025. [31] H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese, Generalized intersection over union: metric and loss for bounding box regression, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 658666. [32] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al., Deepseekmath: Pushing the limits of mathematical reasoning in open language models, arXiv preprint arXiv:2402.03300, 2024. [33] H. Shen, P. Liu, J. Li, et al., Vlm-r1: stable and generalizable r1-style large vision-language model, arXiv preprint arXiv:2504.07615, 2025. [34] Z. Song, G. Ouyang, M. Fang, et al., Hazards in daily life? enabling robots to proactively detect and resolve anomalies, arXiv preprint arXiv:2411.00781, 2024. [35] Z. Song, G. Ouyang, M. Li, et al., Maniplvm-r1: Reinforcement learning for reasoning in embodied manipulation with large vision-language models, arXiv preprint arXiv:2505.16517, 2025. [36] W. Wan, H. Geng, Liu, et al., Unidexgrasp++: Improving dexterous grasping policy learning via geometry-aware curriculum and iterative generalist-specialist learning, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023, pp. 38913902. [37] B. Wang, X. Meng, X. Wang, Z. Zhu, A. Ye, Y. Wang, Z. Yang, C. Ni, G. Huang, and X. Wang, Embodiedreamer: Advancing real2sim2real transfer for policy training via embodied world modeling, arXiv preprint arXiv:2507.05198, 2025. [38] P. Wang, S. Bai, S. Tan, et al., Qwen2-vl: Enhancing visionlanguage models perception of the world at any resolution, arXiv preprint arXiv:2409.12191, 2024. [39] Q. Wang, H. Zhang, C. Deng, Y. You, et al., Sparsedff: Sparseview feature distillation for one-shot dexterous manipulation, arXiv preprint arXiv:2310.16838, 2023. [40] Z. Wu, Y. Wang, Y. Wen, Z. Zhang, B. Wu, and H. Tang, Stereoadapter: Adapting stereo depth estimation to underwater scenes, arXiv preprint arXiv:2509.16415, 2025. [41] A. Ye, Y. Song, J. Su, and D. Zhang, Non-invasive spatial registration using customized dental bracket and improved genetic algorithms, in 2024 IEEE 20th International Conference on Automation Science and Engineering (CASE). IEEE, 2024, pp. 32503255. [42] W. Zhang, M. Wang, G. Liu, et al., Embodied-reasoner: Synergizing visual search, reasoning, and action for embodied interactive tasks, arXiv preprint arXiv:2503.21696, 2025."
        }
    ],
    "affiliations": [
        "CASIA",
        "GigaAI",
        "Tsinghua University"
    ]
}