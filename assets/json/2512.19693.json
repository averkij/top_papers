{
    "paper_title": "The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding",
    "authors": [
        "Weichen Fan",
        "Haiwen Diao",
        "Quan Wang",
        "Dahua Lin",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder's feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 3 9 6 9 1 . 2 1 5 2 : r The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding Weichen Fan1,2 Haiwen Diao1 Quan Wang2 Dahua Lin2 Ziwei Liu1,(cid:66) 1S-Lab, Nanyang Technological University 2SenseTime Research weichen002@e.ntu.edu.sg, haiwen.diao@ntu.edu.sg, {wangquan,dhlin}@sensetime.com, ziwei.liu@ntu.edu.sg Github: https://github.com/WeichenFan/UAE. Figure 1. The Prism Hypothesis. Our conceptual prism decomposes various natural inputs into spectral components along frequency. Low frequency bands capture global semantics and abstract meaning, while high frequency bands encode local detail and fine visual texture. This motivates our Unified Autoencoding (UAE), which harmonizes semantic and pixel representations within single latent space."
        },
        {
            "title": "Abstract",
            "content": "fectively unifies semantic abstraction and pixel-level fidelity into single latent space with state-of-the-art performance. Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers highly inspiring and rarely explored correspondence between an encoders feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as projection of the natural world onto shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MSCOCO benchmarks validate that our UAE ef1. Introduction Trained on massive corpora, recent foundation models have profoundly reshaped perception and generation systems, generalizing well across diverse downstream tasks [3, 6, 26, 29]. Yet, early advances in perception and generation evolve along largely separate trajectories. Their objectives are typically distributed across distinct network structures, e.g., employing pretrained semantic encoders [3, 26, 29], to capture high-level meaning, or pixel encoders [20, 31] to compress fine-grained visual detail. While each module excels within its own domain, this fragmentation compels subsequent unification efforts [12, 28, 39] to depend simultaneously on semantic and pixel encoders, forcing networks to reconcile fundamentally heterogeneous representations. The sharp mismatch lowers training efficiency and induces representational conflicts, with these incompatible features often interfering rather than complementing one another. 1 This fragmentation uncovers deep-seated tension between abstraction and fidelity, which is driving force in shaping subsequent foundation models. To alleviate it, recent studies [34, 49, 51] attempt to transfer semantic encoders into visual generation domains alongside strong pixel decoders. This strategy substantially accelerates convergence and improves semantic correspondence, yet remains limited in recovering fine-grained visual details. In parallel, another research direction seeks to endow pixel encoders with semantic awareness through text supervision [44, 45, 47], semantic encoder distillation [4, 13], and hierarchical feature integration [44]. While these efforts move toward unifying understanding and generation representations in single module, they often achieve coexistence through trade-offs rather than genuine integration. At the core of this development lies fundamental question: How is information about the world represented such that multimodal inputs share common semantic meaning while preserving their native granularity of detail? Impressively, we empirically observe that pre-trained semantic features, whether derived from text or vision, tend to reside at the coarse end of the decoupled feature spectrum, primarily capturing low-frequency structures such as categories, attributes, and relations. In contrast, pre-trained pixel-level features extend toward the finer end of the spectrum, representing higher-frequency components that convey intricate appearance and geometric detail. Strikingly, these complementary representations can be harmoniously integrated within unified encoder, extremely aligning well with the spectral arrangement and fostering progressive synergy from semantic perception to detailed reconstruction. We posit this as the Prism Hypothesis in Figure 1: the real-world inputs project onto continuous shared feature spectrum, and what we perceive as different modalities are distinct slices of this underlying continuum. From this perspective, we introduce Unified Autoencoding (UAE), tokenizer that learns shared latent space and harmonizes semantic structure and pixel-level detail. Specifically, UAE features an innovative frequency-band modulator that factorizes real-world content into fundamental semantic band and residual pixel bands with controllable fine granularity. This design is not only grounded in comprehensive empirical evidence but also validated across diverse reconstruction and perception tasks. With its compact yet semantically expressive representations, UAE outperforms concurrent RAE [51], SVG [34], and UniFlow [49] across rFID, PSNR, gFID and Accuracy metrics on ImageNet, demonstrating that its learned latent space is both semantically representative and pixel-faithful. Our work makes the following contributions. 1. We introduce the Prism Hypothesis, unified spectral perspective connecting natural inputs via shared fundamental band and distinct modality-specific bands, supported by extensive empirical observations. 2. We present Unified Autoencoding, simple yet effective tokenizer that combines frequency band modulator and seamlessly aligns with existing diffusion transformers. 3. Our UAE demonstrates optimal reconstruction quality on ImageNet and MS-COCO together with diffusionfriendly latents, and extensive ablations prove the effectiveness of the proposed frequency-band factorization. 2. Related Work Unified Tokenizers and Unified Representations. Unifying the representations between pixel and semantic embeddings has become central objective for existing foundation models. Joint embedding approaches align images and text in shared representation and enable strong zeroshot transfer [16, 29], and have been extended to many modalities, e.g., audio, depth, thermal, and inertial signals [9]. In parallel, modality-agnostic backbones aim to build unified architecture that can process diverse input modalities and generate task-specific outputs through learned queries or shared token representation [1, 15, 24, 25, 42]. On the tokenizer side, discrete codebook methods have demonstrated that the design and granularity of visual tokens play crucial role in determining how effectively single sequence model can adapt to vision tasks [7, 48]. Recent work goes further and seeks tokenizers that support both understanding and generation at the same time. OmniTokenizer [41] learns joint image video tokenizer with spatial-temporal decoupled transformer and reports strong reconstruction and synthesis across both domains. Very recent studies deepen this trend. UniFlow [49] proposes unified pixel flow tokenizer that adapts pretrained encoder through layer-wise self-distillation and employs lightweight patch-wise flow decoder, explicitly targeting the long-standing tension between semantic abstraction and pixel-faithful reconstruction. Two concurrent works remove the traditional variational bottleneck entirely and build unified representation latents for diffusion transformers. Diffusion Transformers with Representation Autoencoders (RAE) [51] replace the usual reconstruction-only encoder with pretrained representation encoder, e.g., DINO or SigLIP, and trained decoder, arguing that semantically rich latents accelerate convergence and improve generative fidelity. SVG [34] trains diffusion model on DINO features with small residual branch for details, reporting faster training, few-step sampling, and improved quality. Our UAE aligns with this shift. It serves as unified tokenizer that decouples continuous latent features explicitly corresponding to the underlying spectral structure by factorizing real-world contents into low-frequency base and residual high-frequency bands. It anchors semantics in the core representation while relegating fine-grained details to residuals for progressive reconstruction. 2 Frequency and Multi-Resolution Modeling. Classical image synthesis adopted pyramids and wavelets to separate structure by scale, enabling coarse-to-fine generation and targeted refinement of detail. typical example is the Laplacian pyramid of adversarial networks [5], which trains generator per level and synthesizes images by successively adding higher-frequency residuals. Subsequent analyses of neural networks from spectral perspective showed that standard architectures prioritize low frequencies and learn higher frequencies later, phenomenon known as spectral bias [30]. Two lines of work respond to this bias. The first uses input or architecture design to improve access to high frequencies, e.g., Fourier feature mappings and periodic activations that help multi-layer perceptrons represent fine detail [35, 38]. The second introduces frequency-aware objectives and signal processing choices, such as focal frequency loss to emphasize hard frequencies and alias-free synthesis to avoid spurious high-frequency artifacts [17, 18]. Modern generative models retain this multi-resolution view. Cascaded diffusion trains models at increasing resolutions, allowing each stage to learn the appropriate frequency band and its own error distribution [11]. Variants construct explicit feature pyramids or hierarchical patch schedules, enabling efficient diffusion on large images and video while preserving high-frequency detail [8, 36]. Recent latent diffusion designs introduce cross-magnification spaces or zoomable pyramids that share information across scales and enable large-image reconstruction without retraining [46]. In autoregressive models, VAR [40] casts generation as predicting the next scale or resolution, showing strong ImageNet results and clean scaling trends. This explicit progression from global layout to fine detail emerges as viable alternative to diffusion. Building on this idea, Next Visual Granularity (NVG) generation [43] produces sequences at fixed resolution but with progressively finer token granularity, surpassing prior VAR baselines while maintaining structured control over detail. Similarly, NFIG [14] performs discrete next-frequency prediction, demonstrating strong generative performance on the ImageNet benchmark. Our approach aligns with these trends but focuses on unified representation rather than the generators training schedule. 3. Methodology 3.1. Preliminary Findings The Prism Hypothesis. Natural inputs are regarded as projections of common real-world signal onto shared frequency spectrum. Semantic encoders emphasize compact low-band that carries categories, attributes, and relations, while pixel encoders observe the same fundamental base together with higher bands that encode edges, textures, and fine appearance. The hypothesis indicates that cross-modal alignment depends primarily on the shared low band. 3 Figure 2. Frequency energy distribution. Normalized energy e(k) across frequency bands for diverse tokenizers. DINOv2 and CLIP focus on low-frequency (semantic) content, while SD-VAE retains more high-frequency energy, capturing finer details. Formalization. Here, and denote two-dimensional discrete Fourier transform and its inverse. For an image [0, 1]3HW and smooth radial mask MLP that passes frequencies within normalized radius ρ (0, 1], ρ = (cid:0)MLP LP ρ = (cid:0)MHP HP ρ F(I)(cid:1), ρ F(I)(cid:1), (2) (1) ρ ρ ρ ρ , and HP is complementary to MLP where MHP ρ , and both masks use cosine transitions to limit ringing artifacts. All filtering is performed in linear space prior to any modelspecific normalization. With frozen vision-language encoder E, we compute cosine similarities between text embeddings and image embeddings from I, LP . If semantic alignment is carried by the shared base, then the retrieval score satisfies RLP(ρ) is nondecreasing in ρ, RHP(ρ) is nonincreasing in ρ, and RHP(ρ) approaches chance once the shared base is removed. Empirical Verification. Here, we validate the Prism Hypothesis via two complementary analyses: frequency energy decomposition and retrieval robustness under spectral filtering. Exp1: In Figure 2, we first measure the normalized frequency energy distribution e(k) across encoders by averaging the magnitude spectrum of each latent feature channel. This metric quantifies how much representational energy is allocated to each frequency band, revealing that semantic encoders such as DINOv2 and CLIP concentrate most of their energy in the low-frequency region (k=0), whereas pixel-oriented models like SD-VAE retain stronger midand high-frequency components that capture fine details. Exp2: To assess how these spectral tendencies affect semantic alignment, we conduct controlled retrieval experiment in Figure 4, using textimage retrieval recall (R@5) as the evaluation metric. Images are progressively filtered with radial low-pass or high-pass masks of Figure 3. Overall architecture of our proposed Unified Autoencoding (UAE). The input image is separately encoded by both pretrained Semantic Encoder (e.g., DINOv2) and the trainable Unified Encoder. The unified encoder is initialized from the semantic encoder and optimized under two complementary objectives: semantic-wise loss that aligns low-frequency components decomposed from the semantic encoders representations, and pixel-wise reconstruction loss that enforces visual fidelity via the Pixel Decoder by adaptively dilating the high-frequency components. The decoder employs spectral transform blocks to refine residual-frequency content and produce the reconstructed image. This joint optimization harmonizes semantic structure and pixel detail within single latent space. increasing cutoff (fraction of Nyquist), while text features remain unchanged. R@5 remains stable under low-pass filtering until the lowest bands are removed, but drops sharply under high-pass filtering, approaching random chance. Together, these results confirm that cross-modal alignment and semantic consistency primarily reside in the shared lowfrequency base of the latent spectrum, while higher bands encode modality-specific, fine-grained visual detail. 3.2. Unified AutoEncoder Using DINOv2 as an example, we initialize our UAE from the pretrained model and remove the register tokens, leaving channel patch tokens. Input images are center-cropped and resized to the encoder input size, i.e., 224. The resulting tokens are then reshaped into latent grid RBCHW for subsequent frequency-domain processing. 3.3. Residual Split Flow Given latent grid RBCHW , we decompose into frequency bands zf RBKCHW using an FFT Band Projector followed by an Iterative Split procedure. Inspired by the coupling mechanism in flow-based models such as Glow [19], the decomposition is performed in the frequency domain via FFT-based projection, allowing each band to capture distinct spectral component while maintaining invertibility and spatial consistency. FFT Band Projector. To isolate frequency components, we first apply two-dimensional discrete Fourier transform to the latent grid, obtaining complex-valued spectral coefficients. Each frequency band is defined by smooth radial mask Mk, which partitions the Fourier magnitude into concentric rings. These masks overlap slightly to ensure Figure 4. Retrieval results via frequency filtering. TextImage retrieval (R@5) remains stable under low-pass filtering but degrades sharply under high-pass filtering, confirming that semantic alignment primarily resides in low-frequency components. continuity across spectral boundaries and to prevent ringing artifacts when reconstructed. For each band k, they are transformed back to the spatial domain via an inverse FFT, yielding spatially coherent band-limited feature. (0) = z, Iterative Split. With the latent representation zf and residual of frequency r(0) = z, we iteratively decompose the original feature into frequency bands as: r(k)(cid:17) where the residual term is defined as: (k) = Pk zf (cid:16) , r(k+1) = r(k) Pk (cid:16) r(k)(cid:17) . (3) (4) Here, Pk denotes the FFTBandProejector corresponding to the k-th radial mask Mk. At the first step (k=0), the low4 frequency base b(0) is directly extracted without residual subtraction. For subsequent steps (k>0), the per-band la- (k) is progressively calculated by removing each extent zf tracted band. After iterations, the decomposition yields the full set {zf k=0 as multi-band latent representations. This residual decomposition encourages spectral disentanglement in one latent space. Low-frequency bands encode global semantics and smooth structures, while higherfrequency bands capture localized edges and fine details. (k)}K1 3.4. Frequency Band Modulator After the latent representation is decomposed into multiple frequency bands, the bands are processed by the frequency modulation module, mapped back to the spatial domain, and then passed through the ViT decoder to reconstruct the RGB image. During training, the multi-band latent is first processed by noise injection module that perturbs only the high-frequency bands to improve robustness. The resulting latent is then passed through spectral transform module, which maps it back to the spatial domain before being decoded by the ViT-based pixel decoder. Noise Injection. To further boost the capacity of the decoder, we perform noise corruption during the training. Specifically, we build binary condition {0, 1}BK indicating which bands are kept from corruption. For each band tensor b(k) we form noise injection through: (cid:101)b(k) = m:,k b(k) + m:,k (cid:0)0, σ2I(cid:1) , (5) where σ is drawn from [0, 1]. (cid:16) Spectral Transform. As shown in Figure 3, we concatenate processed bands along channels, obtain = RBKCHW , and pass Concatch through two layer convolutional block with SiLU to predict residual RBCHW . Note that the final fused latent can be formulated as follows: {(cid:98)b(k)}K1 k= (cid:17) = + K1 (cid:88) k=0 (cid:98)b(k) RBCHW . (6) Notably, this tensor maintains the same shape as the encoder output, regardless of how many bands are retained, and is the only tensor consumed by the decoder. 3.5. Semantic-wise Loss To retain the semantic priors of the pretrained teacher while expanding frequency coverage, we apply semantic-wise loss between the unified encoder and the frozen semantic encoder on the lowest Kbase frequency bands only. Let fs and fu denote the features from the pretrained-semantic encoders and trainable-unified encoders, respectively. After 5 }K1 frequency decomposition, we obtain band-wise representations {f k=0 . The semantic alignment is enforced only for the first Kbase (low-frequency) bands that encode global structure and object semantics: k=0 and {f }K1 Lsem = (cid:80)Kbase1 k=0 2 Kbase . (7) This restricted alignment ensures that UAE inherits the semantic layout and category-level organization from the teacher in the low-frequency domain, while leaving the remaining high-frequency bands unconstrained for learning modality-specific, pixel-level detail. In practice, this selective supervision stabilizes training and encourages the unified encoder to harmonize semantic and pixel representations without collapsing into purely pixel features. Noted that in our experiments, we set Kbase = 1 by default. 4. Experiments 4.1. Implementation Details Dataset. Following the standard settings, we train our UAE on ImageNet-1K train set and evaluate it on ImageNet-1K val set and MS-COCO 2017 dataset for fair comparisons. Training Settings. We train UAE with DINOv2-B and DINOv2-L respectively in 256x256 resolution. The training processes are decomposed into three stages. In stage 1, the semantic encoder is frozen, where the decoder is trained by reconstruction loss. In stage 2, we unfreeze the encoder and finetune the full model with semantic-wise loss and reconstruction loss. In stage 3, we finetune the full model end-to-end with noise injection and GAN loss, where the GAN loss setting follows the representative RAE [51]. 4.2. Visual Reconstruction Quantitative Evaluation. In Table 1, We quantify reconstruction quality at 256 256 on ImageNet-1K and MSCOCO 2017. The compared baselines include widely-used generative tokenizers and variational decoders. For fair comparison, we report the evaluation results for our UAE and the corresponding baseline that follows the same configurations of DINOv2-base. Here, we report PSNR and SSIM for fidelity, and rFID for perceptual quality. Notably, our UAE delivers state-of-the-art reconstruction quality among unified tokenizers. On ImageNet-1K, UAE improves over the RAE baseline from 18.05 to 29.65 in PSNR and from 0.50 to 0.88 in SSIM, while reducing FID from 2.04 to 0.19. On MS-COCO, UAE raises PSNR from 18.37 to 29.23 and SSIM from 0.49 to 0.89, with FID decreasing from 2.56 to 0.18. These notable gains validate that moving masking out of the decoder and factorizing latents into low-frequency base and residual bands preserve fine detail while maintaining semantic structure. Table 1. Reconstruction quality on ImageNet-1K and MS-COCO 2017 (256x256). Note that our proposed UAE achieves state-of-theart reconstruction among unified tokenizers and remains competitive with strong autoencoders such as Flux-VAE and SD3-VAE. Under identical DINOv2 encoders, UAE significantly outperforms the RAE baseline in both PSNR and SSIM while reducing rFID by over 90%. When scaled to DINOv2-L, UAE attains the best overall perceptual quality (rFID=0.16) and fidelity (PSNR=33.08, SSIM=0.94), demonstrating the effectiveness of frequency-aware factorization in preserving both semantic and fine-grained visual detail. Method Type Ratio ImageNet-1K MS-COCO 2017 SD-VAE [32] SD-VAE-EMA [32] SD3-VAE [23] Flux-VAE [21] Continuous Continuous Continuous Continuous Discrete TokenFlow [28] Discrete DualViTok [12] Continuous EMU2 [37] Continuous UniLIP [39] Continuous RAE (DINOv2-B) UAE (DINOv2-B) Continuous UniFlow (DINOv2-L) Continuous UAE (DINOv2-L) Continuous 8 8 8 8 16 16 14 32 14 14 14 14 PSNR SSIM rFID PSNR SSIM rFID 25.68 24.99 29.58 31.07 21.41 22.53 13.49 22.99 18.05 29.65 32.32 33.08 0.72 0.71 0.86 0. 0.69 0.74 0.42 0.75 0.5 0.88 0.91 0.94 0.75 0.63 0.21 0.17 1.37 1.37 3.27 0.79 2.04 0.19 0.17 0.16 25.43 24.76 29.50 30.99 - - - - 18.37 29.23 32.29 32.84 0.73 0.72 0.87 0. - - - - 0.49 0.89 0.90 0.94 0.76 0.57 0.19 0.19 - - - - 2.56 0.18 0.18 0.17 Figure 5. Qualitative comparison of reconstruction fidelity across autoencoding paradigms. We visualize reconstructed samples from representative methods, including SD-VAE [33], RAE [51], and our proposed UAE. Each row corresponds to reconstructions from fixed source set spanning text, human, object, and artistic domains. UAE produces the most consistent and semantically faithful reconstructions, preserving both high-frequency details (e.g., texture and edge sharpness) and global structure (e.g., layout and color harmony), while reducing the blurring and semantic drift observed in SD-VAE and RAE. (The detail comparisons are denoted in the yellow boxes.) Beyond unified tokenizers, UAE is competitive with the best generative-only tokenizers. On ImageNet-1K, UAE attains an FID of 0.19, matching SD three VAE and approaching Flux VAE, while maintaining high PSNR and SSIM. 6 Table 2. Class-conditional generation performance on ImageNet (256x256). We compare our proposed UAE with recent diffusion and autoregressive models using standard metrics. Note that the UAE performs generation in causal manner, progressing from lowto high-frequency bands in the latent space. Table 3. Linear probing on ImageNet-1K. UAE achieves 83.0% top-1 accuracy with ViT-B backbone, matching RAE and surpassing larger ViT-L models such as MAE, MAGE, and UniFlow. This shows that UAEs unified latent space preserves strong semantic discriminability with compact model size."
        },
        {
            "title": "Methods",
            "content": "gFID IS Prec Rec"
        },
        {
            "title": "Size",
            "content": "ACC DiT-XL/2 [27] VAR [40] UniFlow [49] RAE [51]"
        },
        {
            "title": "UAE",
            "content": "2.27 1.73 2.45 1.51 1.68 271.3 350.2 228.0 242.9 301.6 0.83 0.82 - 0.79 0. 0.57 0.6 - 0.63 0.61 similar pattern holds on MS-COCO, where UAE equals or surpasses the strongest baselines in perceptual quality. Qualitative Evaluation. Figure 5 compares source images with the resulting reconstructions from SD-VAE, RAE, and UAE. Note that our UAE can well preserve straight edges, fine textures, and small text, e.g., street signs and printed documents. The improvement is consistent across natural photos and illustrations, as shown by the montage on page one of the uploaded figure. The detailed comparisons are shown in the yellow boxes in the images. 4.3. Generative Modeling To further assess the effectiveness of the proposed UAE, we conduct class-conditional image generation experiments on ImageNet-1K at resolution of 256 256. We conduct all generative experiments in the multi-band latent space. All experimental settings follow those used in RAE [51] to ensure fair comparison. As shown in Table 2, our UAE attains gFID of 1.68 and an IS of 301.6, achieving performance on par with existing state-of-the-art generative models. This suggests that the unified frequencybased representation enables the generative model to progressively capture both global structure and fine-grained details in coherent manner, keeping highly generative while preserving strong semantic quality. Overall, the UAE latent space, constructed through explicit lowand high-frequency decomposition, provides an effective and diffusion-friendly foundation for large-scale visual generation. 4.4. Semantic Understanding To evaluate the learned semantic discriminability, we perform linear probing on ImageNet-1K following the standard protocol in [49]. All encoder weights are frozen, and single linear classifier is trained on top of the global embeddings for 30 epochs using SGD with cosine learning rate schedule and base rate of 0.1. Consistent augmentations are applied across methods for fair comparison. As shown in Table 3, our UAE achieves top-1 accuracy of 83.0% based on ViT-B backbone, outperforming previVFMTok [50] ViT-L ViT-L BEiT [2] ViT-L MAE [10] MAGE [22] ViT-L UniFlow [49] ViT-L ViT-B RAE [51]"
        },
        {
            "title": "UAE",
            "content": "ViT-B 69.4 73.5 75.8 78.9 82.6 83.0 83.0 t-SNE visualization of semantic embeddings. We Figure 6. compare the feature distributions from the DINOv2 encoder (left) and the band-0 (low-frequency) component of UAE (right). The two plots exhibit similar global structures and class separability, indicating that UAE effectively preserves the semantic organization of the original encoder while introducing unified latent space that remains compatible with frequency-based factorization. ous self-supervised and tokenizer-based approaches. Compared to VFMTok (69.4%) and BEiT (73.5%), our UAE exhibits substantial improvement, demonstrating that the unified latent space preserves stronger semantic alignment. Even against large-scale models such as MAGE (78.9%) and UniFlow (82.6%), UAE maintains competitive classification accuracy despite its smaller backbone. To further visualize this property, Figure 6 shows t-SNE comparisons between the DINOv2 encoder and the band-0 (low-frequency) component of UAE. The two plots share highly similar global structures and class separability, confirming that UAEs low-frequency representations retain the original semantic organization of DINOv2, while enabling unified latent space compatible with frequency-based factorization. This qualitative visualization supports our design goal of preserving global semantics while enriching fine-grained visual detail in higher-frequency bands. 7 Table 4. Configurations of UAE modules. Progressively integrating various strategies improves both fidelity and perceptual realism. The BandProjector enhances structure recovery, tuning pretrained encoders largely improves pixel-level fidelity, and noise injection stabilizes training for the best reconstruction quality. Table 6. Semantic comparisons in DINOv2. Comparison of classification accuracy when probing different representations derived from DINOv2. Band0 denotes using only the lowest-frequency band for linear probing, whereas Concat refers to probing with the concatenation of all frequency-band representations. Methods PSNR SSIM rFID Methods DINOv2 Band0 Concat Baseline + BandProjector + Tuning Encoder + Noise Injection 15.27 22.13 29.02 29.65 0.45 0.71 0.88 0. 22.98 15.22 0.21 0.19 Table 5. Configurations different band splits. UAE maintains consistent and stable reconstruction and semantic performance across different numbers of frequency bands, demonstrating the robustness of the proposed frequency factorization design. Bands PSNR SSIM rFID ACC 10 8 6 4 29.65 29.37 29.41 29.52 29.55 0.88 0.88 0.88 0.88 0.88 0.19 0.19 0.19 0.19 0.19 83.0 83.0 83.0 83.0 83.0 5. Ablation Study 5.1. The Effectiveness of the UAE Modules In Table 4, we evaluate the influence of each core configuration in our proposed UAE on ImageNet-1K. Starting from baseline that directly reconstructs images from the frozen DINOv2-B encoder, we progressively add the BandProjector, Encoder Tuning, and Noise Injection strategies. Note that these components introduce solid and progressive improvements for visual fidelity. The BandProjector enhances structural recovery by decomposing latent signals across frequency bands, increasing PSNR from 15.27 to 22.13 and SSIM from 0.45 to 0.71. Tuning the encoder further aligns the latent representation with the reconstruction objective, raising PSNR to 27.02 and reducing rFID from 15.22 to 7.81. Finally, the noise injection strategy yields large gain, achieving PSNR 29.65, SSIM 0.88, and rFID 0.19, indicating impressive perceptual fidelity. Together, these results confirm that frequency decomposition, semantic alignment, and controlled noise regularization jointly enable UAE to harmonize semantic structure and fine visual detail in one unified representation space. 5.2. The Configurations of Band Splits We further examine how the number of frequency bands affects reconstruction fidelity and semantic consistency. In Table 5, varying the number of bands from 2 to 10 produces nearly identical performance across all metrics: PSNR stays Accuracy(%) 83.0 83.3 83.0 around 29, SSIM remains at 0.88, rFID is consistently 0.19, and linear probing accuracy (ACC) stays fixed at 83.0%. These results demonstrate that UAEs frequency factorization is highly robust to band granularity. Its decomposition reliably preserves both low-frequency and highfrequency information even when the spectrum is split coarsely. The minimal variation further indicates that most of the frequency energy is captured by the base and the first few residual bands, confirming that the unified representation does not depend on fine-grained spectral partitioning. In practice, using fewer bands slightly reduces computation without harming reconstruction quality or downstream performance. With more multi-frequency latents, our UAE also naturally supports iterative image generation. 5.3. The Semantic Comparisons in DINOv2 As shown in Table 6, using only the lowest-frequency component (Band0) achieves slightly higher accuracy (83.3%) than both the original DINOv2 features and the concatenated multi-band representation (83.0%). This indicates that the low-frequency band effectively retains the global semantic structure of DINOv2, which dominates discriminative performance in classification. 6. Conclusion We propose the Prism Hypothesis, which views diverse natural inputs as projections of shared spectrum composed of compact low-frequency semantic component and residual higher-frequency detail. Preliminary experiments about frequency-band distributions of various visual and textual encoders strongly enlighten this connection between feature spectra and representational function. Hence, we launch Unified AutoEncoding (UAE), which harmonizes semantic and pixel information within single latent space via hierarchical frequency-band modulator. Extensive experiments confirm that it delivers greater generative capability over existing unified tokenizers, e.g., RAE, SVG, and UniFlow, meanwhile preserving reconstruction quality on par with top-tier models like Flux-VAE. We position the UAE as promising and practical route towards unified tokenizers for understanding and generation. ."
        },
        {
            "title": "References",
            "content": "[1] Alexei Baevski, Wei Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. data two vec general framework for self supervised learning in speech vision and language. In International Conference on Machine Learning, 2022. 2 [2] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT pre-training of image transformers. In International Conference on Learning Representations, 2022. 7 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self supervised vision transformers. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 1 [4] Jun Chen, Chenchen Zhu, Guocheng Qian, Bernard Ghanem, and Zhicheng Yan. Exploring open-vocabulary semantic segmentation from clip vision encoder distillation only. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1540815418, 2023. 2 [5] Emily Denton, Soumith Chintala, and Rob Fergus. Deep generative image models using laplacian pyramid of adversarial networks. In Advances in Neural Information Processing Systems, 2015. 3 [6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth sixteen by sixteen words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 1 [7] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high resolution image synthesis. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. [8] Wei-Chiu Fan, Ting-Yi Lin, Zhiyu Zeng, Yi-Chang Shih, Min Sun, and Alexander Schwing. Frido feature pyramid In Proceeddiffusion for complex scene image synthesis. ings of the AAAI Conference on Artificial Intelligence, 2023. 3 [9] Rohit Girdhar, Alaaeldin El Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind one embedding space to bind them all. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [10] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 7 [11] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):133, 2022. 3 [12] Runhui Huang, Chunwei Wang, Junwei Yang, Guansong Lu, Yunlong Yuan, Jianhua Han, Lu Hou, Wei Zhang, Lanqing Hong, Hengshuang Zhao, et al. Illume+: Illuminating unified mllm with dual visual tokenization and diffusion refinement. arXiv preprint arXiv:2504.01934, 2025. 1, 6 [13] Wei Huang, Zhiliang Peng, Li Dong, Furu Wei, Jianbin Jiao, and Qixiang Ye. Generic-to-specific distillation of masked autoencoders. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1051210522, 2023. [14] Zhihao Huang, Xi Qiu, Yukuo Ma, Yifu Zhou, Junjie Chen, Hongyuan Zhang, Chi Zhang, and Xuelong Li. Nfig: Multiscale autoregressive image generation via frequency ordering. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. 3 [15] Andrew Jaegle, Sebastian Borgeaud, Jean Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver io general architecture for structured inputs and outputs. In Advances in Neural Information Processing Systems, 2021. 2 [16] Chao Jia, Yinfei Yang, Ye Xia, Yi Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision language representaIn International tion learning with noisy text supervision. Conference on Machine Learning, 2021. 2 [17] Lei Jiang, Bo Dai, Wayne Wu, Chen Change Loy, and Ziwei Liu. Focal frequency loss for image reconstruction and synthesis. In Proceedings of the IEEE International Conference on Computer Vision, 2021. 3 [18] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Advances in Neural Information Processing Systems, 2021. 3 [19] Durk Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. Advances in neural information processing systems, 31, 2018. [20] Diederik Kingma and Max Welling. Auto encoding variational bayes. In Proceedings of the International Conference on Learning Representations, 2014. 1 [21] Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 6 [22] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21422152, 2023. 7 [23] Joshua Lopez. Stable diffusion 3: Research paper. https: //stability.ai/news/stable-diffusion-3research-paper, 2025. Stability AI. 6 [24] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified io unified model for vision language and multi modal tasks. arXiv preprint arXiv:2206.08916, 2022. 2 [25] Jiasen Lu, Christopher Clark, Sangho Lee, et al. Unified io two scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2 [26] Maxime Oquab, Timothee Darcet, Tarik Moutakanni, Mahmoud Assran, Karel Lenc, Daniel Haziza, Pierre Zhang, Nicolas Ballas, Laurent Mazare, Gregory Rogez, Ishan Misra, Piotr Bojanowski, and Armand Joulin. Dinov2 learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 1 [27] William Peebles and Saining Xie. Scalable diffusion modIn Proceedings of the International els with transformers. Conference on Machine Learning, 2023. 7 [28] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25452555, 2025. 1, 6 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 1, [30] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In Proceedings of the International Conference on Machine Learning, 2019. 3 [31] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high fidelity images with vq vae 2. In Advances in Neural Information Processing Systems, 2019. 1 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 6 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High resolution image synIn Proceedings of the thesis with latent diffusion models. IEEE Conference on Computer Vision and Pattern Recognition, 2022. 6 [34] Minglei Shi, Haolin Wang, Wenzhao Zheng, Ziyang Yuan, Xiaoshi Wu, Xintao Wang, Pengfei Wan, Jie Zhou, and Jiwen Lu. Latent diffusion model without variational autoencoder. arXiv preprint arXiv:2510.15301, 2025. 2 [35] Vincent Sitzmann, Julien N. P. Martel, Alexander W. Bergman, David B. Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In Advances in Neural Information Processing Systems, 2020. [36] Ivan Skorokhodov, Mohamed Elhoseiny, et al. Hierarchical patch diffusion models for high resolution video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 3 [37] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. 2023. 6 [38] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. In Advances in Neural Information Processing Systems, 2020. 3 [39] Hao Tang, Chenwei Xie, Xiaoyi Bao, Tingyu Weng, Pandeng Li, Yun Zheng, and Liwei Wang. Unilip: Adapting clip for unified multimodal understanding, generation and editing. arXiv preprint arXiv:2507.23278, 2025. 1, 6 [40] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling scalable image generation via next scale prediction. In Advances in Neural Information Processing Systems, 2024. 3, [41] Junke Wang, Yi Jiang, Zehuan Yuan, Binyue Peng, Zuxuan Wu, and Yu Gang Jiang. Omnitokenizer joint image video tokenizer for visual generation. In Advances in Neural Information Processing Systems, 2024. 2 [42] Wenhui Wang, Hangbo Bao, Li Dong, et al. Image as foreign language beit pretraining for vision and vision language tasks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2 [43] Yikai Wang, Zhouxia Wang, Zhonghua Wu, Qingyi Tao, Kang Liao, and Chen Change Loy. Next visual granularity generation. arXiv preprint arXiv:2508.12811, 2025. 3 [44] Ji-Jia Wu, Andy Chia-Hao Chang, Chieh-Yu Chuang, YuLun Liu, Min-Hung Chen, Hou-Ning Hu, and Yen-Yu Lin. Image-text co-decomposition for text-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 51235133, 2024. 2 [45] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: Semantic segmentation emerges from text supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1813418144, 2022. 2 [46] Sriram Yellapragada et al. Zoomldm latent diffusion model for multi scale image generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2025. 3 [47] Muyang Yi, Quan Cui, Hao Wu, Cheng Yang, Osamu Yoshie, and Hongtao Lu. simple framework for textIn Proceedings of the supervised semantic segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1290412914, 2023. [48] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2024. 2 [49] Zhengrong Yue, Haiyu Zhang, Xiangyu Zeng, Boyu Chen, Chenting Wang, Shaobin Zhuang, Lu Dong, Kunpeng Du, Yi Wang, Limin Wang, and Yali Wang. Uniflow unified pixel flow tokenizer for visual understanding and generation. arXiv preprint arXiv:2510.10575, 2025. 2, 7 10 [50] Anlin Zheng, Xin Wen, Xuanyang Zhang, Chuofan Ma, Tiancai Wang, Gang Yu, Xiangyu Zhang, and Xiaojuan Qi. Vision foundation models as effective visual tokenizarXiv preprint ers for autoregressive image generation. arXiv:2507.08441, 2025. 7 [51] Boyang Zheng, Nanye Ma, Shengbang Tong, and Saining Xie. Diffusion transformers with representation autoencoders. arXiv preprint arXiv:2510.11690, 2025. 2, 5, 6,"
        }
    ],
    "affiliations": [
        "S-Lab, Nanyang Technological University",
        "SenseTime Research"
    ]
}