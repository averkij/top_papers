{
    "paper_title": "Mitigating Object Hallucinations via Sentence-Level Early Intervention",
    "authors": [
        "Shangpin Peng",
        "Senqiao Yang",
        "Li Jiang",
        "Zhuotao Tian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify a critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose **SENTINEL** (**S**entence-level **E**arly i**N**tervention **T**hrough **IN**-domain pr**E**ference **L**earning), a framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using a context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90\\% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at https://github.com/pspdada/SENTINEL."
        },
        {
            "title": "Start",
            "content": "Mitigating Object Hallucinations via Sentence-Level Early Intervention Shangpin Peng1 Senqiao Yang2 Li Jiang3 Zhuotao Tian1(cid:66) 1Harbin Institute of Technology, Shenzhen 2The Chinese University of Hong Kong 3The Chinese University of Hong Kong, Shenzhen 5 2 0 2 6 1 ] . [ 1 5 5 4 2 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal large language models (MLLMs) have revolutionized cross-modal understanding but continue to struggle with hallucinations - fabricated content contradicting visual inputs. Existing hallucination mitigation methods either incur prohibitive computational costs or introduce distribution mismatches between training data and model outputs. We identify critical insight: hallucinations predominantly emerge at the early stages of text generation and propagate through subsequent outputs. To address this, we propose SENTINEL (Sentence-level Early iNtervention Through IN-domain prEference Learning), framework that eliminates dependency on human annotations. Specifically, we first bootstrap high-quality in-domain preference pairs by iteratively sampling model outputs, validating object existence through cross-checking with two open-vocabulary detectors, and classifying sentences into hallucinated/non-hallucinated categories. Subsequently, we use context-coherent positive samples and hallucinated negative samples to build context-aware preference data iteratively. Finally, we train models using context-aware preference loss (C-DPO) that emphasizes discriminative learning at the sentence level where hallucinations initially manifest. Experimental results show that SENTINEL can reduce hallucinations by over 90% compared to the original model and outperforms the previous state-of-the-art method on both hallucination benchmarks and general capabilities benchmarks, demonstrating its superiority and generalization ability. The models, datasets, and code are available at https://github.com/pspdada/SENTINEL. 1. Introduction Recent advancements in multimodal large language models (MLLMs) have demonstrated significant progress in aligning visual and textual representations through crossmodal feature integration, marking pivotal step toward * Equal contribution. (cid:66) Corresponding author (tianzhuotao@hit.edu.cn). (a) Ultra-large proprietary model/human annotator-dependent methods (b) Response rewriting method (c) SENTINEL (Ours) Figure 1. Comparative analysis of data construction strategies for hallucination mitigation in MLLMs. Our proposed approach demonstrates superior efficiency and effectiveness in generating high-quality, domain-specific preference learning datasets, offering robust solution for reducing hallucination in MLLMs. the development of general-purpose AI systems [2, 9, 33 35, 44, 65, 87]. However, critical challenge persists in multimodal settings: the phenomenon of hallucinations [4, 36, 53], wherein models generate factually inconsistent or fabricated information that deviates from the image content provided by users. This issue not only degrades user trust and experience but also poses substantial risks in real-world applications of MLLMs, thereby impeding the realization of trustworthy general AI systems [5, 18, 68]. To address this challenge, recent work has explored enhanced decoding strategies [8, 19, 27] as means to mitigate hallucinations. While these approaches show promise, they often introduce trade-offs, including increased computational overhead during inference, higher latency, and re1 liance on specific dependencies, which may limit their scalability and practicality in resource-constrained scenarios. On the other hand, preference alignment methods [31, 51, 60] avoid additional inference costs but face other challenges. As shown in Fig. 1a, many of them rely on large proprietary models (e.g., GPT [1]) [21, 69, 75, 80, 82, 86] or human annotators [13, 76], incurring high costs. Additionally, Fig. 1b highlights that output rewriting [69, 82, 86] can create distributional discrepancies, while Lai et al. [25] and our experiments in Tab. 2 show that out-of-domain training data harms generalization. Therefore, the high costs, and the distribution disparities inherent in the curated training data may compromise hallucination mitigation efforts. Key observations. To address hallucination with greater efficacy and efficiency, we investigate the dynamics of hallucination within the models output. Our analysis reveals that hallucination intensity escalates with the length of generated text, while mitigating hallucinations at specific sentences significantly reduces their prevalence in subsequent outputs, as detailed in Figs. 2a and 2b. These findings suggest that early interventiontargeting hallucinations at their initial occurrenceis crucial to preventing their propagation in later generations. This raises key question: How can we effectively implement an early intervention strategy to address hallucinations of MLLMs as they arise? In this work, we propose SENTINEL Our solution. (Sentence-level Early iNtervention Through IN-domain prEference Learning), which provides early intervention for the initial occurrence of hallucinations during generation. Unlike existing methods, SENTINEL operates without relying on external large language models for rewriting, ensuring that the learning targets remain strictly within the domain of the models original outputs. This approach preserves the models intrinsic distribution and expression patterns while effectively curbing hallucination propagation. Specifically, SENTINEL first employs an in-domain candidate bootstrapping strategy, which performs multiple sampling rounds on the current model, extracts objects from the outputs, and applies consistency cross-checking to classify objects as hallucinated, uncertain, or factual. This is followed by context-aware preference data generation process, which constructs preference pairs using non-hallucinated positive samples and hallucinated negative ones, enhanced by iterative contextual bootstrapping. Finally, context-aware preference learning is performed using the modified context-aware DPO loss, maximizing the likelihood of generating context-coherent positive samples while minimizing hallucinated negative ones. By focusing on captions where hallucinations first emerge, SENTINEL effectively halts their propagation in subsequent outputs. Experimental results across various benchmarks demonstrate that SENTINEL effectively mitigates object hallucination while preserving the generalization capabilities of MLLMs. Specifically, on Object Halbench [55] and AMBER [63], hallucinations are reduced by about 92% and 65%, respectively, with consistent improvements on HallusionBench [12]. Furthermore, SENTINEL preserves its performance on VQAv2 [10] and TextVQA [59], and achieving decent gains on both ScienceQA [41] and MM-Vet [78]. To summarize, our contributions are as follows: We demonstrate that early intervention at the first occurrence of hallucination is crucial for preventing its propagation in subsequent model outputs of MLLMs. We propose SENTINEL, which effectively and efficiently mitigates hallucinations without requiring extensive external resources or manual effort. The model-agnostic SENTINEL achieves state-of-the-art performance on hallucination benchmarks without compromising MLLMs general capabilities. 2. Background and Motivation In this section, we briefly introduce the foundational concepts and methods relevant to this study in Sec. 2.1, establishing the necessary background. Following this, in Sec. 2.2, we outline our key insights and elucidate the motivations behind our proposed designs. 2.1. Related Work and Preliminaries Object Hallucination (OH) in Multimodal Large Language Models (MLLMs) is characterized by the generation of text that is semantically coherent yet inconsistent with the visual content of the provided image [4, 53]. To mitigate this issue, recent advancements have focused on innovative decoding strategies, which aim to reduce the prevalence of OH by refining the generation process of MLLMs [6, 8, 19, 27]. Concurrently, preference learning has emerged as an alternative approach for addressing OH, leveraging its capacity to align MLLMs with human expectations for truthfulness and traceability [13, 28, 32]. Notably, the Proximal Policy Optimization algorithm (PPO) [57] enhances model reliability by training an auxiliary reward model to assess response quality and then guide the model in optimizing its outputs based on the reward signals. Moreover, Direct Preference Optimization (DPO) [51] has emerged as simpler alternative, learning directly from pre-collected feedback without requiring reward model. The DPO loss is: (cid:34) LDPO(θ) = E(x,yw,yl)D (cid:16) β log log σ πθ(ywx) πref(ywx) (cid:35) (cid:17) πθ(ylx) πref(ylx) (1) , β log where =[v, q]. Here, represents the preference dataset for learning, σ denotes the sigmoid function, πθ indicates the policy ing tasks. Specifically, as illustrated in Fig. 2b, eliminating hallucinated objects in the second sentencecompared to vanilla greedy decodingsignificantly reduces the likelihood of hallucinated objects in subsequent sentences while increasing the probability of factual objects present in the image. Similar results are observed when addressing hallucinations in the third sentence, as shown in Appendix A.2. These findings underscore the necessity of early intervention to mitigate hallucinations effectively. To enable early intervention, an open-vocabulary object detector [7, 37] could be employed during inference to verify the presence of the objects generated by the model within the image. While this method effectively reduces hallucinations without sacrificing caption diversity, as demonstrated in Appendix A.2, it is time-consuming; despite the object detector being efficient, the models sampling process incurs significant computational overhead. Consequently, we opt for preference learning strategy during model training, which mitigates hallucinations without compromising the original inference efficiency. 3. Method 3.1. Overview Existing preference learning methods may use an external model to rewrite sentences or rely on model-generated responses as training data. However, these methods may introduce discrepancies in distribution and expression patterns between the training data and the models original output. Hence, we propose SENTINEL, which performs sentence-level early intervention to mitigate object hallucinations through preference learning with in-domain data, without manual effort or dependence on extensive LLMs. As shown in Fig. 3, the proposed SENTINEL method takes six essential steps. Specifically, Sec. 3.2 presents the process of generating the in-domain candidates containing the factual and hallucinated objects. Subsequently, Sec. 3.3 introduces the construction of preference data pairs derived from these in-domain candidates. These two steps can be integrated into the In-domain Preference Data Construction phase (shown in Algorithm 1). Finally, in Sec. 3.4 we elaborate on how SENTINEL leverages the curated preference data to achieve preference learning. 3.2. In-domain Candidate Bootstrapping To construct positive and negative preference data pairs without relying on external models for rewriting, we perform multiple sampling rounds on the current model and extract objects from the outputs. We then apply consistency cross-checking method to classify the models output objects into three categories: hallucinated, uncertain, and factual, which are used to construct preference data in subsequent steps. This process is termed In-domain Candidate (a) (b) Figure 2. Object position distribution in MLLM hallucination analysis. (a) illustrates the progressive deterioration of hallucination effects in Multimodal Large Language Models (MLLMs) with increasing description length in the image captioning task, while (b) demonstrates the effectiveness of early-stage intervention in mitigating the propagation of hallucination. model under training, πref represents the unchanged reference model, yw stands for the positive sample, and yl represents the negative sample, both based on the input x, which includes the image and prompt q. The hyperparameter β governs the separation between the policy model and the reference model. Many recent methods [16, 69, 77, 82, 86] leverage DPO to mitigate hallucinations by curating preference data to guide the models. More related works of this study are discussed in Appendix F. 2.2. Motivation This section outlines the motivations behind this work. The implementation details of related experiments are provided in Appendix A. Hallucination grows with text length. To better understand the causes of Object Hallucination (OH), we analyze the distributions of hallucinated and factual objects in image captions generated by MLLMs. Specifically, as shown in Fig. 2a, where the horizontal axis represents the normalized position of an object in the caption (as percentage), while the vertical axis denotes the normalized frequency (probability density), the blue curve represents hallucinated objects, and the orange curve corresponds to objects present in the image. The comparison reveals that as caption length increases, the model becomes more prone to hallucinations, with fewer factual objects described and more hallucinated ones introduced. This trend is further corroborated by sentence-level analysis in Fig. 2b. These findings lead us to hypothesize that intervening at the initial occurrence of hallucination could be critical in reducing its recurrence in subsequent model outputs. Early intervention mitigates hallucinations. To evaluate the effectiveness of early intervention in curbing hallucination propagation, we analyze the impact of addressing hallucinations at the sentence level in image caption3 Figure 3. The overview of SENTINEL. The proposed SENTINEL takes six essential steps: (1) Generate multiple in-domain responses conditioned on the input image, prompt, and context c. (2) Identify and extract all mentioned objects from each generated sentence. (3) Utilizing two object detectors to validate the existence of extracted objects through cross-referencing. (4) Categorize generated sentences into hallucinated and non-hallucinated groups based on detection results. (5) Extend the generation context with verified non-hallucinated sentences to guide subsequent outputs. (6) Fine-tune the model using the context-aware DPO (C-DPO) loss with the in-domain, styleconsistent, and context-varying preference data. Booststrapping, as illustrated in Fig. 3 (1)-(3). In-domain candidate sampling. In our approach, we use sampling-based decoding to obtain candidate samples. This ensures that the positive (yw) and negative (yl) samples are drawn from the same distribution as the current model, preserving consistency in textual styles and linguistic structures. The generation halts upon sentence completion (e.g., detection of period), at which point sentences are automatically segmented for subsequent discrimination. Object extraction. After generating candidate sentences, we extract the mentioned objects from the text for hallucination detection. To achieve this, we utilize the SceneGraphParser [30] model to transform the textual descriptions into series of triplet-based scene graphs. By parsing these scene graphs, we identify specific noun entities from the subjects and objects, which are subsequently used as candidate objects for existence verification. Object presence validation. Following object extraction, we apply cross-checking to validate the presence of candidate objects in the image. Specifically, we utilize two openvocabulary object detectors, GroundingDINO [37] and Yolo World [7], for cross-validation. This approach demonstrates superior performance compared to using single detector, as shown in Fig. 8 of the ablation study. The cross-checking results are categorized into three types: (1) hallucinated (both models confirm absence), (2) factual (both models confirm presence), and (3) uncertain (conflicting results). Sentences containing hallucinated objects are tagged as hallucinated, whereas those only containing factual objects are tagged as non-hallucinated, forming positive-negative sample pairs for preference learning. To ensure data quality and minimize detector bias, we ignore uncertain objects. Algorithm 1 In-domain Preference Data Construction Input: Image v, prompt q, context (initially empty) Output: Training samples (v, q, c, y+ w, yl) Sample in-domain candidates si using v, q, and for each sample si do 1: while Model does not generate </s> do 2: 3: 4: 5: 6: 7: 8: 9: Select y+ Select yl as hallucinated sample Construct preference samples (v, q, c, y+ Append non-hallucinated sample y+ Extract entities from the sample Validate the presence of entities using object detectors as context-coherent non-hallucinated sample to the context w, yl) 3.3. Context-aware Preference Data Generation labeled as hallucinated or nonWith sentences hallucinated from Sec. 3.2, this section introduces contextaware preference data generation. As illustrated in Fig. 3 4 Figure 4. Categories of in-domain candidates. The in-domain candidates fall into three types. Employing non-hallucinated, context-coherent descriptions (y+ w) as positive samples, paired with hallucinated descriptions (yl), enhances the models generalization performance and robustness. (4)-(5), this process extracts contextually relevant data, ensuring the training data better represents the models output distribution. The specifics are elaborated below. Preference data construction. The preference data is typically composed of the image, the corresponding prompt, the positive sample, the negative sample, and the context (i.e., all generated sentences excluding the current one). In the construction of sample pairs, positive samples yw are selected from the non-hallucinated sentences, while negative samples yl are derived from the hallucinated sentences. Subsequently, we partition the positive samples yw into (1) the context-coherent positive sample two categories: y+ w, wherein some of the described objects are explicitly referenced in the context, and (2) the context-agnostic positive sample w, where none of the objects are mentioned in the context. In essence, the objects described in y+ exhibit strong correlation with the context, while those in display weaker or negligible correlation. Illustrative examples are provided in Figs. 3 and 4. We observe that the context-coherent sample y+ can effectively mitigate hallucinations without compromising the models generalization capabilities, and incorporating as the positive samples results in performance reduction, as shown in Tab. 3. This observation underscores the importance of contextual signals in guiding the models generation process. Specifically, the richer contextual information in y+ samples appears to enhance the models ability to preserve contextual coherence and prioritize salient content, resulting in performance improvements [15]. Iterative Contextual Bootstrapping (ICB). The proposed SENTINEL framework is designed to enable early intervention for mitigating hallucinations in generative models. Given the context c, which represents the hallucination-free content preceding the current output, the model is trained to distinguish between non-hallucinated positive sample y+ and hallucinated negative sample yl. To enhance robustFigure 5. Visualization of the Iterative Contextual Bootstrapping (ICB) framework. Given an input image and corresponding question, this pipeline iteratively generates diverse contextual samples, enabling robust hallucination mitigation across varying contexts and significantly improving model generalization. ness across diverse contexts, we introduce the Iterative Contextual Bootstrapping (ICB) strategy, as depicted in Fig. 5. Specifically, given the query q, the input image v, and the current context ci, we generate multiple candidate outputs by sampling repeatedly from the MLLM. These candidates are then processed through structured pipeline consisting of (2) object extraction, (3) object presence validation, and (4) preference data construction, as illustrated in Fig. 3. This pipeline is designed to identify nonhallucinated positive sample y+ and hallucinated negative sample yl. By aggregating v, q, ci, y+ and yl, we construct preference data pair (v, q, ci, y+ w, yl), which is subsequently appended to the dataset for preference learning. Furthermore, to bootstrap the preference data with different hallucination-free contexts, we construct ci+1 = ci+y+ for the next iteration by appending the positive sample y+ to the current context ci. The updated context ci+1 is then processed through the same procedure as described above to generate new preference data pair. This iterative approach ensures that the preference data is enriched with progressively more complex and varied contexts, enabling the model to generalize its hallucination mitigation capabilities across different scenarios. The effectiveness of this pipeline is validated and discussed in Appendix B.2. 3.4. Context-aware Preference Learning The preference data generated through the processes outlined in Sec. 3.2 and Sec. 3.3 can be formally represented as (x, c, y+ w, yl), where is the input, including the image and the prompt q, denotes the context, y+ is the context5 Model Method Object HalBench [55] Hallucination benchmarks AMBER [63] HallusionBench [12] VQAv2 [10] TextVQA [59] ScienceQA [41] MM-Vet [78] General benchmarks Resp. Ment. CHAIR Hal. Cog. Question Acc. Acc. Acc. Image Acc. Overall baseline VCD [27] OPERA [19] DoLa [8] EFUF [70] HA-DPO [82] POVID [86] CLIP-DPO [45] RLAIF-V [77] TPO [16] Ours baseline VCD [27] vanilla-DPO [69] HSA-DPO [69] Ours 52.7 51.3 45.3 44.0 39.3 37.0 33.4 - 7.8 5.6 4.3 46.0 43.7 6.7 5.3 3.3 28.0 25.9 22.9 25.1 22.6 20.9 16.6 - 4.2 3.2 2. 23.0 21.6 3.6 3.2 1.9 8.4 9.1 6.5 6.2 5.8 6.7 5.3 3.7 2.8 3.6 2.9 6.9 7.8 2.8 2.1 2.7 35.5 39.8 28.5 27.7 28.2 30.9 28.7 16.6 15.7 20.5 14.6 31.9 36.2 15.5 13.4 11.7 4.0 4.2 3.1 2.9 3.1 3.3 3.0 1.3 0.9 1.6 1. 3.3 3.7 1.6 1.2 0.9 LLaVA-v1.5-7B LLaVA-v1.5-13B 46.86 - - - 47.03 47.74 46.59 - 35.43 40.12 47.56 46.43 - 46.41 46.14 46.77 78.5 77.0 78.2 76.3 78.1 77.6 77.2 - 75.2 75.9 78. 80.0 78.5 79.2 78.3 79.9 58.2 56.1 58.2 56.6 57.2 56.7 56.6 56.4 55.1 55.3 58.2 61.2 59.5 60.4 60.0 61.0 66.8 68.7 68.2 67.5 66.4 69.7 68.8 67.6 68.2 67.1 69.2 71.6 72.0 71.8 71.3 72.8 31.0 29.8 30.3 30.8 31.2 30.6 31.8 - 29.9 25.7 32. 36.0 33.7 35.0 33.7 36.2 Table 1. Comparison of hallucination mitigation methods in MLLMs: effectiveness and general capabilities. This evaluation highlights the best and second-best results in bold and underlined, respectively. All comparisons are performed under identical model size constraints. Resp. and Ment. denote response-level and mention-level hallucination rates, while Hal. and Cog. represent the Hallucination Score and Cognitive Score, respectively. More evaluation details are provided in Appendix D. coherent positive sample, and yl is the negative sample. The learning objective is to guide the model, conditioned on the input and the context c, to maximize the likelihood of generating the contextually coherent positive sample y+ while minimizing the likelihood of producing the negative sample yl. To achieve this, we adapt the Direct Preference Optimization (DPO) loss by incorporating the context as part of the input. We term this modified loss as contextaware DPO (C-DPO), which is formulated as follows: LC-DPO(θ) = (x,y+ w,yl)D (cid:34) log σ (cid:16) β log πθ(y+ πref(y+ wx) wx) (cid:35) , (cid:17) πθ(ylx) πref(ylx) β log where =[x, c] = [v, q, c]. (2) In C-DPO, the context is excluded from the loss computation, and gradients are only derived from the discrimination between y+ and yl. This design ensures that the model focuses on learning the contextual coherence of the positive sample without being directly influenced by the context during gradient updates. Further discussions and comparisons between the proposed C-DPO and the standard DPO are provided in Appendix C.3. 4. Experiments In this section, we conduct comprehensive experiments to evaluate the effectiveness of our SENTINEL in reducing hallucinations while improving the general abilities of the model. We first introduce the experimental setup in Sec. 4.1, then present the main results in Sec. 4.2, and finally conduct ablation studies in Sec. 4.3 to analyze our methods effectiveness. More results are in Appendices and D. 6 4.1. Experimental Setup Training. To ensure fair comparison, we follow the settings of prior works [16, 19, 26, 27, 34, 45, 56, 66, 69, 69, 70, 77, 79, 82, 86], using LLaVA-v1.5 as the reference model across all experiments. For data collection, we prompt the model with detailed image descriptions [76] to generate training data, with images sourced from the Visual Genome dataset [23]. Model training is conducted using C-DPO (Eq. (2)) in combination with LoRA [17], and optimized with AdamW [40]. The 7B and 13B models are trained for one epoch on 8.6K and 7.0K samples, respectively, with learning rates of 2 107 and 3 107. Additional training details are provided in Appendix C. Evaluation benchmarks. We evaluate the hallucination extent and general capabilities of our SENTINEL method across multiple benchmarks. For hallucination evaluation, we use widely adopted benchmarks, including Object HalBench [55], AMBER [63], and HallusionBench [12]. To assess general capabilities, we employ VQAv2 [10], TextVQA [59], ScienceQA [41], and MM-Vet [78]. Further details of these benchmarks are provided in Appendix D.1. Baselines. To show the effectiveness of our method, we compare SENTINEL with several state-of-the-art (SOTA) Specifically, VCD [27], OPERA [19], and methods. DoLa [8] focus on enhanced decoding strategies, while HADPO [82], POVID [86], CLIP-DPO [45], RLAIF-V [77], and TPO [16] leverage preference training. Additionally, Vanilla DPO applies the original DPO objective Eq. (1) using training data from HSA-DPO, while EFUF [70] is an unlearning-based approach. Details are in Appendix D.2. 4.2. Main Results Comparison with recent SOTAs. As shown in Tab. 1, we compare our method with baseline methods across several Figure 6. Impact on different hallucination types. Comparison between multiple methods shows that our method reduces hallucination in all six hallucination types. benchmarks. The results demonstrate that SENTINEL significantly reduces the models hallucination rate. Specifically, for the 7B model, our method achieves 4.3 responselevel (Resp.) and 2.6 mention-level (Ment.) hallucination rate. Compared to the previous SOTA method, TPO [16], which achieves 5.6 response-level and 3.2 mention-level hallucination rate, our proposed SENTINEL surpasses it by further reducing hallucinations by total of 24% on Object Halbench. Furthermore, even on the 13B model, compared to the baseline, which achieves 6.9 CHAIR, 31.9 Hallucination score (Hal), and 4.0 Cognitive score (Cog), our proposed SENTINEL significantly improves performance, achieving 2.7 CHAIR, 11.7 Hal score, and 0.9 Cog score, respectively. These results demonstrate that our method is also effective on larger models. Comprehensive hallucination mitigation. To further evaluate the effect of our method on various hallucination types, we conducted experiments on the discriminative part of the AMBER [63] benchmark and report the F1 scores for each hallucination type. As shown in Fig. 6, LLaVA-v1.5 with SENTINEL outperforms the baseline across all six hallucination types, demonstrating the effectiveness of our method in mitigating various hallucination issues. Notably, for the Existence hallucination type, our method improves the 7B model by 6.3 and the 13B model by 7.6 compared to the baseline. Detailed results are provided in Appendix D.4. Improved general capabilities. As shown in Tab. 1, SENTINEL enhances the general capabilities of the model on multiple benchmarks. Specifically, SENTINEL demonstrates stable performance on VQAv2 and TextVQA, whereas previous methods designed for hallucination mitigation suffer from significant performance degradation. Moreover, on ScienceQA and MM-Vet, our method not only surpasses previous hallucination mitigation methods but also outperforms the vanilla LLaVA-v1.5. These results show that SENTINEL not only significantly mitigates halFigure 7. Qualitative results of SENTINEL. Our method can effectively eliminate hallucinations in MLLMs while enhancing the models general capabilities. Method Object HalBench AMBER MM-Vet Resp. Ment. Acc F1 Overall LLaVA-v1.5-7B Ours (8.6K (y+ Ours (8.6K Rewrited (y+ w, yl)) w, yl)) 52.7 4.3 4.80.5 27.9 2.6 2.90.3 74.1 79. 71.5 76.1 75.01.1 78.01.3 31.31.3 31.1 32.6 Table 2. Effects of rewritten samples. Rewriting the preference training samples (y+ w, yl) results in performance reduction. lucinations but also improves general capability. Qualitative results. To further demonstrate the effectiveness of our method, we conduct case studies. As shown in Fig. 7, the baseline model misinterprets the image conIn contrast, our tent, leading to an incorrect conclusion. model effectively understands image content and provides more detailed and precise description. This example highlights how our approach effectively reduces hallucinations while simultaneously enhancing the models overall capability. We conduct more case studies in Appendix G. 4.3. Ablation Studies In this section, we conduct series of ablation experiments to further analyze the effectiveness of SENTINEL. More discussions can be found in Appendix D.5. Effectiveness of data style consistency. To analyze the effect of preference data style, we train the model using rewritten data for comparison. Specifically, we instructed GPT-4 [1] to rewrite (y+ w, yl) while ensuring coherence with the context c. As shown in Tab. 2, the rewriting results show performance degradation in reducing hallucinations and general ability. This highlights the advantage of our approach in preserving data style consistency. Furthermore, we conduct detailed analysis in Appendix D.5, which shows that models trained on in-domain data converge to lower preference optimization loss and achieve better differentiation between positive and negative samples, whereas training with rewritten data provides less improvements. Effectiveness of cross-checking. To validate the effectiveness of cross-checking for object presence, we conduct experiments using only the Grounding DINO or YOLO World for detection. In this setting, if the model determines that an 7 Method Data Scale Object HalBench TextVQA ScienceQA MM-Vet Resp. Ment. Acc I-Acc Overall LLaVA-v1.5-7B y+ 100% 50% + y+ 100% - 8.6K 50% 10.0K1.4K 4.80.5 14.0K5.4K 4.60.3 52.7 4. 27.9 2.6 2.90.3 3.00.4 58.2 58.2 58.10.1 58.10.1 66.8 69.2 69.00.2 68.70.5 31.1 32.6 32.00.6 31.61.0 Table 3. Comparison between context-coherent samples y+ and context-agnostic samples . This table reveals that incorporating context-coherent samples y+ yields better performance. Method Object HalBench[55] AMBER[63] Resp. Ment. CHAIR Hal Cog LLaVA-v1.5-7B Non-hallucinated context Natural context Hallucinated context 52.7 4.3 8.6 14.3 27.9 2.6 4.7 7.1 8.4 2.9 3.3 3.9 35.5 14.6 15.6 18.6 4.0 1.2 1.5 1.8 Table 4. Comparison between different new context formation strategies during the iterative contextual bootstrapping pipeline. Appending non-hallucinated sample y+ to the existing context ci yields superior performance compared to incorporating hallucinated samples yl or greedy decoding contexts, highlighting the effectiveness of our proposed approach. object is absent, it is directly classified as hallucinated. As shown in Fig. 8, leveraging two object detectors for crossvalidation significantly outperforms using single model, effectively reducing the hallucination rate. Effect of different yw types on model performance. As shown in Tab. 3, we conduct detailed study on the impact of different types and proportions of the positive data yw on model performance. The results show that y+ samples, which contain richer contextual information, enhance the models generalization ability while achieving similar hallucination reduction with less data. Effect of non-hallucinated sentences as context c. To analyze the impact of using non-hallucinated sentences as context c, we evaluate three different settings for generating new context: selecting hallucinated sentence, selecting non-hallucinated sentence, or directly using model-generated sentence from greedy decoding. As shown in Tab. 4, using non-hallucinated sentence as context improves the models ability to distinguish hallucinations and significantly reduces their occurrence in the output. This further demonstrates that intervening at the first instance of hallucination is critical for minimizing its recurrence. Effect of data scale. To analyze the impact of the training data scale on our method, we train the model using different dataset sizes (1k/2k/4k/6k/8k) and evaluate its performance on Object Halbench. As shown in Fig. 8, our method further mitigates model hallucinations as data scale up. This demonstrates the potential and scalability of SENTINEL. Furthermore, since our method does not rely on ultra-large proprietary models or human annotators for dataset construction, it can efficiently collect more training data. Integrating with existing preference learning methods. Figure 8. Impact of training data quantity on hallucination rate in Object Halbench [55]. The results show that SENTINEL demonstrates better efficiency, effectiveness, and scalability, while effectively reducing hallucination rates across varying data scales. Method LLaVA-v1.5-7B HA-DPO [82] HA-DPO + Ours (6K) Object HalBench [55] AMBER [63] HallusionBench [12] TextVQA [59] MM-Vet [78] Resp. Ment. Acc F1 Question Acc 28.0 52.7 37.029.8% 20.925.4% 74.22.7 78.03.9 4.678.0% 76.62.4 84.26.2 8.0 78.4% 71.5 74.1 46.86 47.740.88 48.720.98 Acc 58.2 56.71.5 57.10.4 Overall 31.0 30.60.4 33.52.9 Table 5. Effectiveness of combining the proposed SENTINEL with HA-DPO. Only subset of our training data is needed to reduce hallucinations while enhancing generalization effectively. To further demonstrate SENTINELs generalization, we explore integrating with previous hallucination mitigation approaches. As shown in Tab. 5, incorporating subset of our data into the GPT-generated dataset collected by HADPO [82] effectively mitigates hallucinations while significantly enhancing the models generalization. This highlights SENTINELs complementarity with other preference learning methods and its potential for broader applicability. 5. Concluding Remarks Summary. In this work, we address the critical challenge of hallucinations in multimodal large language models (MLLMs). While prior methods have shown promise, they often introduce significant computational overhead, rely on costly resources, or create distributional discrepancies. To tackle these issues, we propose SENTINEL, framework that intervenes early at the onset of hallucinations by leveraging in-domain preference learning. SENTINEL employs an in-domain candidate bootstrapping strategy, context-aware preference data generation, and context-aware DPO (C-DPO) loss to effectively curb the propagation of hallucinations while preserving the models intrinsic distribution. Experimental results across multiple benchmarks demonstrate the superiority of SENTINEL, establishing it as scalable, efficient, and model-agnostic solution for enhancing the reliability of MLLMs. Limitation. Currently, as SENTINEL lacks the capability to incorporate spatiotemporal information, it might not be able to effectively address the hallucination issues which require long-term reasoning in video MLLMs. This limitation highlights the need for further research in this area. Acknowledgments. This work is supported by the Shenzhen Science and Technology Innovation Program (JCYJ20240813105901003, KJZD20240903102901003), Guangdong Basic and Applied Basic Research Foundation (2025A1515011546), and National Key R&D Program of China (2024YFE0215300)."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 7, 6, 8, 9 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization. Text Reading, and Beyond, 2023. 1, 9 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 8, 9 [4] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. 1, 2, 9 [5] Long Chen, Oleg Sinavski, Jan Hunermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund, and Jamie Shotton. Driving with llms: Fusing object-level vecIn 2024 tor modality for explainable autonomous driving. IEEE International Conference on Robotics and Automation (ICRA), 2024. 1 [6] Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, and Jiawei Zhou. Halc: Object hallucination reduction via adaptive focal-contrast decoding. arXiv preprint arXiv:2403.00425, 2024. 2, [7] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, XingYolo-world: Real-time gang Wang, and Ying Shan. In Proceedings of the open-vocabulary object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 3, 4 [8] Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. Dola: Decoding by contrasting layers improves factuality in large language models. arXiv preprint arXiv:2309.03883, 2023. 1, 2, 6, 7, 9 [9] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 1, 9 [10] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answerIn Proceedings of the IEEE conference on computer ing. vision and pattern recognition, 2017. 2, 6, 5, 9 [11] Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, and Kai Chen. Mask-dpo: Generalizable fine-grained factuality alignment of llms. arXiv preprint arXiv:2503.02846, 2025. [12] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 6, 8, 5, 9 [13] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 2, 7 [14] Yudong Han, Liqiang Nie, Jianhua Yin, Jianlong Wu, and Yan Yan. Visual perturbation-aware collaborative learning for overcoming the language prior problem. arXiv preprint arXiv:2207.11850, 2022. 9 [15] Zongbo Han, Zechen Bai, Haiyang Mei, Qianli Xu, Changqing Zhang, and Mike Zheng Shou. Skipn: simple method to reduce hallucination in large vision-language models. arXiv preprint arXiv:2402.01345, 2024. 5 [16] Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, and Lu Sheng. topic-level self-correctional approach to mitigate hallucinations in mllms. arXiv preprint arXiv:2411.17265, 2024. 3, 6, 7 [17] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 2022. [18] Mingzhe Hu, Shaoyan Pan, Yuheng Li, and Xiaofeng Yang. Advancing medical imaging with language models: journey from n-grams to chatgpt. arXiv preprint arXiv:2304.04920, 2023. 1 [19] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai Yu. Opera: Alleviating hallucination in multimodal large language models via over-trust penalty and In Proceedings of the IEEE/CVF retrospection-allocation. Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 6, 7, 9 [20] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 7, 8 [21] Liqiang Jing and Xinya Du. Fgaif: Aligning large visionarXiv language models with fine-grained ai feedback. preprint arXiv:2404.05046, 2024. 2, 7 [22] Liqiang Jing, Ruosen Li, Yunmo Chen, and Xinya Du. Faithscore: Fine-grained evaluations of hallucinations in large vision-language models. arXiv preprint arXiv:2311.01477, 2023. [23] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 2017. 6, 3 9 [24] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 9 [25] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-dpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. 2 [26] Seongyun Lee, Sue Hyun Park, Yongrae Jo, and Minjoon Seo. Volcano: mitigating multimodal hallucination through self-feedback guided revision. arXiv preprint arXiv:2311.07362, 2023. 6 [27] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 1, 2, 6, 7, [28] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models. arXiv preprint arXiv:2312.10665, 2023. 2 [29] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv:2403.18814, 2023. 9 [30] Zhuang Li, Yuyang Chai, Terry Yue Zhuo, Lizhen Qu, Gholamreza Haffari, Fei Li, Donghong Ji, and Quan Hung Tran. Factual: benchmark for faithful and consistent textual scene graph parsing. arXiv preprint arXiv:2305.17497, 2023. 4, 1, 2 [31] Sheng-Chieh Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Scott Yih, and Xilun Chen. Flame: Factualityaware alignment for large language models. Advances in Neural Information Processing Systems, 2024. 2 [32] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating hallucination in large arXiv multi-modal models via robust instruction tuning. preprint arXiv:2306.14565, 2023. 2 [33] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 2023. 1, 9 [34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 6, 1, [35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llavanext: Improved reasoning, ocr, and world knowledge, 2024. 1, 8, 9 [36] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei Peng. survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253, 2024. 1, 9 [37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, 2024. 3, 4, 1 [38] Yijun Liu, Jiequan Cui, Zhuotao Tian, Senqiao Yang, Qingdong He, Xiaoling Wang, and Jingyong Su. TypicalnessarXiv preprint aware learning for failure detection. arXiv:2411.01981, 2024. 9 [39] Edward Loper and Steven Bird. Nltk: The natural language toolkit. arXiv preprint cs/0205028, 2002. [40] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6, 3 [41] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 2022. 2, 6, 5, 9 [42] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 2024. 9 [43] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, XianSheng Hua, and Ji-Rong Wen. Counterfactual vqa: causeeffect look at language bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021. 9 [44] OpenAI. GPT-4V(ision) system card, 2023. 1, 6, 9 [45] Yassine Ouali, Adrian Bulat, Brais Martinez, and Georgios Tzimiropoulos. Clip-dpo: Vision-language models as source of preference for fixing hallucinations in lvlms. In European Conference on Computer Vision, 2024. 6 [46] Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. 9 [47] Shangpin Peng, Weinong Wang, Zhuotao Tian, Senqiao Yang, Xing Wu, Haotian Xu, Chengquan Zhang, Takashi Isobe, Baotian Hu, and Min Zhang. Omni-dpo: dualperspective paradigm for dynamic preference learning of llms. arXiv preprint arXiv:2506.10054, 2025. 9 [48] Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, and Jiaya Jia. Does your vision-language model get arXiv preprint lost in the long video sampling dilemma? arXiv:2503.12496, 2025. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, 2021. 3 [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 9 [51] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 2023. 2, 4, 9 10 [52] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training In SC20: International Confertrillion parameter models. ence for High Performance Computing, Networking, Storage and Analysis, 2020. 3 [53] Vipula Rawte, Amit Sheth, and Amitava Das. survey of hallucination in large foundation models. arXiv preprint arXiv:2309.05922, 2023. 1, 2, [54] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. 1 [55] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. arXiv preprint arXiv:1809.02156, 2018. 2, 6, 8, 4, 5, 7, 9 [56] Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad Beirami, Sercan Arık, and Tomas Pfister. Data-augmented phrase-level alignment for mitigating object hallucination. arXiv preprint arXiv:2405.18654, 2024. 6, 7 [57] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 2 [58] Tong Shao, Zhuotao Tian, Hang Zhao, and Jingyong Su. Explore the potential of clip for training-free open vocabulary In European Conference on Comsemantic segmentation. puter Vision, pages 139156. Springer, 2024. 9 [59] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019. 2, 6, 8, 4, 5, [60] Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher Manning, and Chelsea Finn. Fine-tuning language models In The Twelfth International Conference on for factuality. Learning Representations, 2023. 2 [61] Zhuotao Tian, Michelle Shu, Pengyuan Lyu, Ruiyu Li, Chao Zhou, Xiaoyong Shen, and Jiaya Jia. Learning shape-aware In Proceedings of embedding for scene text detection. the IEEE/CVF conference on computer vision and pattern recognition, pages 42344243, 2019. 9 [62] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 2 [63] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, et al. Amber: An llm-free multi-dimensional bencharXiv preprint mark for mllms hallucination evaluation. arXiv:2311.07397, 2023. 2, 6, 7, 8, 5 [64] Junjie Wang, Bin Chen, Yulin Li, Bin Kang, Yichi Chen, and Zhuotao Tian. Declip: Decoupled learning for openIn Proceedings of the Comvocabulary dense perception. puter Vision and Pattern Recognition Conference, pages 1482414834, 2025. 9 [65] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 9 [66] Kai Wu, Boyuan Jiang, Zhengkai Jiang, Qingdong He, Donghao Luo, Shengzhi Wang, Qingwen Liu, and Chengjie Wang. Noiseboost: Alleviating hallucination with noise perturbation for multimodal large language models. arXiv preprint arXiv:2405.20081, 2024. 6 [67] Yike Wu, Yu Zhao, Shiwan Zhao, Ying Zhang, Xiaojie Yuan, Guoqing Zhao, and Ning Jiang. Overcoming language priors in visual question answering via distinguishing superficially similar instances. In Proceedings of the 29th International Conference on Computational Linguistics, 2022. 9 [68] Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan. Embodied task planning with large language models. arXiv preprint arXiv:2307.01848, 2023. 1 [69] Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He, Haoyuan Li, Zhelun Yu, Fangxun Shu, Hao Jiang, and Linchao Zhu. Detecting and mitigating hallucination in large vision language models via fine-grained ai feedback. arXiv preprint arXiv:2404.14233, 2024. 2, 3, 6, 7, 9 [70] Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai. Efuf: Efficient fine-grained unlearning framework for mitigating hallucinations in multimodal large language models. arXiv preprint arXiv:2402.09801, 2024. 6, 7, [71] Senqiao Yang, Jiaming Liu, Ray Zhang, Mingjie Pan, Zoey Guo, Xiaoqi Li, Zehui Chen, Peng Gao, Yandong Guo, and Shanghang Zhang. Lidar-llm: Exploring the potential of large language models for 3d lidar understanding. arXiv preprint arXiv:2312.14074, 2023. 9 [72] Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, and Jiaya Jia. An improved baseline for reasoning segmentation with large language model. arXiv preprint arXiv:2312.17240, 2023. [73] Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. arXiv preprint arXiv:2412.04467, 2024. 9 [74] Senqiao Yang, Zhuotao Tian, Li Jiang, and Jiaya Jia. Unified In Proceedlanguage-driven zero-shot domain adaptation. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2340723415, 2024. 9 [75] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models. Science China Information Sciences, 2024. 2, 1, 7 [76] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 6, 7 [77] Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source 11 ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. 3, 6, 5, 7, 9 [78] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 2, 6, 8, 4, 9 [79] Zihao Yue, Liang Zhang, and Qin Jin. Less is more: Mitigating multimodal hallucination from an eos decision perspective. arXiv preprint arXiv:2402.14545, 2024. 6, 1 [80] Mengxi Zhang, Wenhao Wu, Yu Lu, Yuxin Song, Kang Rong, Huanjin Yao, Jianbo Zhao, Fanglong Liu, Haocheng Feng, Jingdong Wang, et al. Automated multi-level preference for mllms. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 7 [81] Tiancheng Zhao, Peng Liu, and Kyusong Lee. Omdet: Large-scale vision-language multi-dataset pre-training with multimodal detection network. IET Computer Vision, 2024. [82] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023. 2, 3, 6, 8, 1, 4, 7, 9 [83] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language modIn Proceedings of the 62nd Annual Meeting of the els. Association for Computational Linguistics (Volume 3: System Demonstrations). Association for Computational Linguistics, 2024. 8 [84] Zhisheng Zhong, Chengyao Wang, Yuqi Liu, Senqiao Yang, Longxiang Tang, Yuechen Zhang, Jingyao Li, Tianyuan Qu, Yanwei Li, Yukang Chen, et al. Lyra: An efficient and speech-centric framework for omni-cognition. arXiv preprint arXiv:2412.09501, 2024. 9 [85] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. arXiv preprint arXiv:2310.00754, 2023. 9 [86] Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao. Aligning modalities in vision large lanarXiv preprint guage models via preference fine-tuning. arXiv:2402.11411, 2024. 2, 3, 6, 7 [87] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 1, 12 Mitigating Object Hallucinations via Sentence-Level Early Intervention"
        },
        {
            "title": "Overview",
            "content": "This material provides supplementary details to the main paper, including the following sections: (A) Motivation Details (A.1) Object Position Distribution (A.2) Decode Based Early Intervention (B) Method Details (B.1) Object Extraction (B.2) Iterative Contextual Booststrapping (B.3) Selection of Object Detector (B.4) Treatment of Uncertain Objects (C) Training Details (C.1) Training Dataset (C.2) Training Setup (C.3) Training Objective (D) Evaluation Details (D.1) Evaluation Benchmarks (D.2) Evaluation Counterparts (D.3) Evaluation Settings (D.4) Evaluation Results (D.5) Details of Ablation Study (E) SENTINEL with Other Baselines (F) Related Work (G) Additional Case Studies A. Motivation Details In this section, we deepen the discussion supporting the key observations from the main paper. A.1. Object Position Distribution Following the approach of Caption Hallucination Assessment with Image Relevance [54], we select 300 images from the COCO2014 dataset and use the provided captions and segmentation annotations as references to determine whether the objects described by the model exist in the images. As shown in the main paper Fig. 2, as the model generates longer outputs, the number of real objects described decreases while hallucinated objects increase, indicating that hallucinations of the model become more severe with output length. Notably, towards the end of the generation (around the last 10% tokens), both the number of hallucinated and real objects decrease. This is because, towards the end of the image description, the model tends to conclude with abstract summaries about the atmosphere or emotions rather than providing concrete object descriptions. 1 Figure 9. Effect of intermediate hallucination mitigation on subsequent generations. Showing the effectiveness of early-stage intervention in mitigating the propagation of hallucinations. Model Method Object HalBench AMBER Resp. Ment. CHAIR Hal Cog LLaVA-v1.5-7B [34] baseline Woodpecker [75] VCD [27] OPERA [19] EOS [79] HA-DPO [82] Decode based early intervention 52.7 39.6 52.7 40.0 40.0 37. 33.5 27.9 26.4 27.3 21.9 22.2 20.9 17.6 8.4 - 9.1 6.5 6.4 6.7 5.5 35.5 - 39.8 28.5 27.4 30. 26.8 4.0 - 4.2 3.1 2.6 3.3 2.6 Table 6. Effectiveness of decode based early intervention. A.2. Decode Based Early Intervention As preliminary investigation, we explore training-free approach to mitigating object hallucinations in MLLMs. In essence, our method dynamically verifies each generated sentence against the image content and filters out any hallucinated ones before proceeding. Specifically, for the image captioning task, we sample multiple candidate sentences (n = 5) from the models output, stopping generation at the first period. These candidate sentences are then parsed using SceneGraphParser [30] to extract mentioned objects. We subsequently employ an open-vocabulary object detector, Grounding DINO [37], to verify the existence of these objects in the image. sentence without hallucinated objects is selected as the current generated sentence, and then continues generating the subsequent content. This approach effectively prevents the further propagation of hallucinations. As shown in the main paper Fig. 2b, even when applied at just single sentence, eliminating hallucinations as early as the second sentence significantly reduces the likelihood of generating hallucinated objects in subsequent outputs. similar effect is observed when intervention occurs only at the third sentence, as shown in Fig. 9. Next, we extract entities from these triplets. We apply the following rules: If the predicate belongs to {is, are}, it represents an attribute relationship. In this case, we consider only the subject as potential entity. Otherwise, both the subject and object are considered potential entities. To refine the entity extraction process, we leverage the SpaCy natural language processing library to analyze the part of speech (POS) of the extracted candidates and filter out words that are neither nouns nor proper nouns. Furthermore, we utilize NLTKs WordNet Lemmatizer [39] in conjunction with lexicographic filtering mechanism to exclude non-entity nouns. Specifically, we examine the lexicographer category of each word, and if it falls within the following non-concrete categories, it is removed: noun.feeling, noun.state, noun.time, noun.cognition, noun.communication, noun.relation, noun.location. noun.act, noun.attribute, noun.shape, noun.quantity, noun.event, Our method effectively extracts entities without the need for large auxiliary models such as GPT-4 [1] or LLaMA-270B [62]. Instead, it relies solely on lightweight NLP tools and libraries, ensuring both high extraction accuracy and maintaining an open-vocabulary nature. B.2. Iterative Contextual Booststrapping To ensure robustness across different contexts, we introduce the iterative contextual bootstrapping (ICB) strategy, as shown in the main paper Fig. 5. By leveraging contextually bootstrapped data, early intervention can be seamlessly integrated into diverse contexts, effectively mitigating hallucinations and enhancing robustness. To further investigate the impact of iterative contextual bootstrapping (ICB), we conduct an ablation study where we exclude ICB and instead sample non-hallucinated description y+ only at the first occurrence of hallucination, using it as the positive sample during constructing pairs, while the original hallucinated description serves as the negative sample yl. We then train the model using the same method and dataset size mentioned in the main paper. The results, as presented in Tab. 7, demonstrate that our approach, when incorporating ICB, exhibits greater robustness and effectively reduces hallucinations across different scenarios. Figure 10. Time cost analysis of decode-based methods. Decode-based early intervention increases inference time, primarily due to the additional generation steps required by MLLM sampling, whereas the object detector remains highly efficient. When this early intervention strategy is applied throughthe entire caption generation process, as shown out in Tab. 6, it effectively mitigates object hallucinations when evaluated on the Object Halbench [55] benchmark. However, as illustrated in Fig. 10, it increases inference time, primarily due to the additional sampling time of the MLLM, while the object detector remains highly efficient. These findings highlight the detectors role as both an effective and computationally efficient component, reinforcing its potential for constructing high-quality training data for hallucination mitigation. B. Method Details In this section, we detail our methods for extracting concrete objects from models outputs in Appendix B.1, and propose iterative contextual bootstrapping (ICB) to enhance robustness in Appendix B.2. In Appendix B.3, we discuss the selection of the object detector. Finally, in Appendix B.4, we describe how we handle uncertain objects. Our approach reduces hallucinations efficiently without relying on large auxiliary models. B.1. Object Extraction In this section, we detail our approach to extracting the mentioned objects from the models output automatically and efficiently. Our objective is to obtain identifiable and concrete entity descriptions, following structured pipeline. First, we employ SceneGraphParser [30] to convert the input descriptions into series of triplets representing relationships within the scene. Specifically, each triplet is treated as (subject, predicate, object) tuple. For example: little black cat sits on chair next to table. B.3. Selection of Object Detector is parsed into the following structured triplets: (cat, is, little) (chair, next to, table) (cat, is, black) (cat, sit on, chair) Detectors are more cost-effective for providing training guidance for MLLMs than human annotators. SENTINEL is not constrained to particular detectors; any model with open-world recognition ability can be employed. As shown 2 Method Object HalBench AMBER MM-Vet Resp. Ment. CHAIR Hal Cog Overall 8.4 2.9 3.10.2 52.7 4.3 5.31.0 27.9 2.6 3.20.6 LLaVA-v1.5-7B 31.1 32.6 Ours w/ ICB Ours w/o ICB 31.80.8 Table 7. Effect of Iterative Contextual Booststrapping. Iterative Contextual Bootstrapping (ICB) enables early intervention to be seamlessly integrated into various contexts, effectively mitigating hallucinations and ensuring robustness across different scenarios. 35.5 14.6 14.90.3 1.40.2 4.0 1. Method Object HalBench Resp. Ment. LLaVA-v1.5-7B OmDet [81] Grounding DINO [37] YOLO World [7] Grounding DINO [37] + YOLO World [7] 52.7 19.3 14.3 12.3 6.6 28.0 9.9 7.7 6.9 3. Table 8. Results with different detectors. We observe that detector OmDet [81] often produces false positives, identifying objects that do not exist in the images, which may lead to less reliable results. Generally, detectors with more human-like real-world perception abilities yield better performance. Method Object HalBench MM-Vet Resp. Ment. Overall LLaVA-v1.5-7B Ignore uncertain Uncertain as factual Uncertain as hallucinated 52.7 4.3 10.3 8. 28.0 2.6 6.9 5.0 31.0 32.6 31.8 32.0 Table 9. Treatments of uncertain objects. Ignoring uncertain objects can improve the quality of training data, thereby enhancing final model performance. in Tab. 8, more effective detectors lead to superior performance, and the cross-validation technique effectively mitigates the phenomenon of false positives. B.4. Treatment of Uncertain Objects As mentioned in the main paper, we ignore uncertain objects to maintain data quality and reduce detector bias. We also conduct ablation studies that treat uncertain objects alternately as factual or hallucinated. Tab. 9 shows that ignoring uncertain objects yields better results. We hypothesize that it is because 1) uncertainfactual may bring hallucinations to the context during iterative contextual bootstrapping (ICB), contradicting the early intervention strategy based on the hallucination-free contexts. 2) uncertainhallucinated may introduce noisy and ambiguous negative samples for preference learning. Model Setting LLM Vision encoder Projector Learning rate Batch size per GPU Trainable parameters LoRA rank LoRA alpha α LoRA beta β Projector lr Learning rate scheduler Optimizer Model max lenght Weight decay Epochs Global batch size Memory optimization LLaVA-v1.5-7B LLaVA-v1.5-13B Vicuna-v1.5-7B Vicuna-v1.5-13B CLIP ViT-L336px/14[49] mlp2x gelu 2e-6 16 3e-6 8 LoRA trains only LLMs linear layers. 128 256 0.1 0 Cosine AdamW [40] 2048 0. 1 64 ZeRO stage 2 [52] Table 10. Training hyperparameters used in our experiments. C.1. Training Dataset Visual Genome. Visual Genome (VG) [23] is publicly available large-scale vision-language dataset that provides dense annotations for about 108K images, with each image containing an average of 21 objects, 18 attributes, and 18 object relationships. In addition to object annotations, VG includes 1.7 million visual question-answering pairs in multi-choice format, covering six question types: What, Where, When, Who, Why, and How. Compared to traditional VQA datasets, VG offers more balanced distribution of question types while also serving as one of the most comprehensive resources for bridging visual concepts with language. In our study, VG images are utilized for constructing the training dataset. Training Data. We use approximately 4K images from VG for training dataset construction, selected based on their appropriate information density and appropriate level of object diversity. Notably, we do not utilize any labels or ground-truth annotations from VG or other datasets when constructing the preference dataset. Instead, our approach automatically and efficiently generates highly discriminative preference training data in cost-effective manner. C.2. Training Setup We strictly follow the official setup provided by LLaVA to ensure reproducibility. The details of the training hyperparameters used in our training are presented in Tab. 10. C. Training Details C.3. Training Objective In this section, we provide detailed overview of the preference training process. The dataset used for training is described in Appendix C.1, the training setup is outlined in Appendix C.2, and the training objective is analyzed in detail in Appendix C.3. As shown in the main paper Eq. (2), we employ the contextaware DPO (C-DPO) objective to train the model to differentiate between hallucinated and non-hallucinated content at the first occurrence of hallucination, aiming to mitigate its propagation. In this section, we provide detailed anal3 ysis of (1) the rationale for excluding context from the loss computation, (2) the key differences between our proposed C-DPO and the standard DPO, and (3) comparison between our training objective and Mask-DPO [11]. Why mask context in loss calculation? We implemented pseudocode for calculation based on the context-aware DPO (C-DPO) formula. As shown in Algorithm 2, to compute the C-DPO loss, we need to evaluate the log probabilities (logps) of the output tokens given an input. If we do not mask out the context during loss computation, the context remains identical in both positive and negative samples. Since the context and its preceding tokens are the same, for the policy model, the logps of the context tokens will be the same across both forward passes. This adds an identical term to both policy chosen logps and policy rejected logps, which cancels out in the policy logratios computation at line 7, leaving the loss unaffected. From gradient perspective, since is derived from the same model parameters θ based on identical preceding tokens in both forward passes, its gradient remains the same due to the autoregressive nature of the model. As result, this gradient term cancels out as well and does not affect model training. Therefore, to reduce unnecessary computation and mitigate potential numerical errors, we exclude the context from the loss calculation in C-DPO. Algorithm 2 Pseudocode for C-DPO Training Input: Training sample (v, q, c, y+ Output: C-DPO loss w, yl) w, yl)) torch.Tensor: # policy model forward pass policy chosen logps = model.dpo forward((v, q, c, y+ w)) policy rejected logps = model.dpo forward((v, q, c, yl)) policy logratios = policy chosen logps - policy rejected logps import torch import torch.nn.functional as 1: 2: 3: 4: def get cdpo loss(self, (v, q, c, y+ 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: # reference model forward pass with torch.no grad(): # compute C-DPO loss logits = policy logratios - ref logratios loss = -F.logsigmoid(dpo beta * logits) return loss.mean() ref chosen logps = ref model.dpo forward((v, q, c, y+ w)) ref rejected logps = ref model.dpo forward((v, q, c, yl)) ref logratios = ref chosen logps - ref rejected logps model.dpo forward() returns the sum of the log probabilities of all tokens that have not been masked out. Comparison with Standard DPO. To validate the effectiveness of our proposed context-aware DPO (C-DPO), we conducted an additional experiment using standard DPO for training. In this setup, no context was included, and both yw and yl are complete image captions based on the given image and prompt q. yw consisted of sentences with minimal hallucinations (using non-hallucinated context and ensuring the current sentence itself is hallucination4 free until the end of generating), while yl contained sentences with maximal hallucinations (using hallucinated context and ensuring the current sentence itself contained hallucinations until the end of generating). Both methods were trained on the same scale of data (8.6K samples). As shown in Tab. 11, our proposed context-aware DPO (C-DPO) more effectively guides the model in distinguishing hallucinated content from non-hallucinated content, leading to improved hallucination suppression while maintaining generalization capabilities. To further analyze the underlying reasons, we track the training dynamics of both objectives, including policy model log probabilities (logps) and loss. As illustrated in Fig. 11, the standard DPO exhibits greater logps variations between yw and yl during training due to the substantial differences between sentence pairs. Prior studies by Rafailov et al. [51] and Zhao et al. [82] suggest that such variability can dominate gradient updates, potentially compromising training stability. This instability may hinder the models ability to capture long-range dependencies, leading to slower convergence and more gradual reduction in training loss. Method Object HalBench[55] TextVQA[59] MM-Vet[78] Resp. Ment. Acc Overall 27.9 2.6 5.52.9 52.7 4.3 10.15.8 LLaVA-v1.5-7B C-DPO Eq. (2) (8.6K data) Standard DPO Eq. (1) (8.6K data) Table 11. Effectiveness of C-DPO. Compared to standard DPO, C-DPO enables the model to better learn to distinguish between correct and incorrect responses at the onset of hallucination, effectively mitigating hallucinations from the outset. 31.0 32.6 31.70.9 58.2 58.2 58.10.1 Differences between our objective and DPO with Mask. Gu et al. [11] propose preference learning approach that selectively retains factually correct sentences from preferred samples while avoiding penalties on factual content within inferior samples, thereby mitigating ambiguity issues inherent in preference learning. While this method effectively prioritizes high-quality samples, it primarily relies on masking certain parts of the training data without fully considering their potential impact on model learning. As demonstrated in the main paper Tab. 4, within our workflow, the choice of context cwhich is masked from loss calculation during trainingplays crucial role in shaping the final training outcomes. This highlights the importance of carefully considering these factors to ensure that our approach effectively guides the model toward learning accurate and reliable knowledge. D. Evaluation Details In this section, we provide detailed information about the evaluation process. The evaluation benchmarks we used are described in Appendix D.1, where we showcase the strong performance of our method. In Appendix D.2, we outline where obj represents the set of objects mentioned in the models response, and Aobj denotes the set of objects that actually exist in the image. Hal score: Measures the proportion of responses containing hallucinations. response is considered hallucinatory if CHAIR(R) = 0. It is computed as: Hal(R) = (cid:40) 1 if CHAIR(R) = 0, otherwise. (4) Cog score: This metric assesses the alignment between model-generated hallucinations and human cognitive tendencies. It measures the probability of the model generating objects from predefined set of hallucinatory target objects Hobj, calculated as: Cog(R) = len(R obj Hobj) obj) len(R . (5) In the discriminative component of AMBER, hallucination severity is evaluated based on six factors: object existence, attributes, relationships, state, number, and actions. We report the F1 score to assess the models performance across these aspects. HallusionBench. HallusionBench [12] is benchmark designed to assess multimodal large language models (MLLMs) in image-context reasoning, specifically focusing on hallucination and illusion phenomena. By incorporating carefully curated set of challenging reasoning tasks, HallusionBench enables systematic evaluation of both language-based hallucinations and visiondriven illusions. To quantify model performance, we report the overall accuracy across all questions, covering both straightforward and complex cases. VQAv2. VQAv2 [10] is widely used general visual question answering benchmark that enhances dataset balance by collecting complementary images for each question. TextVQA. TextVQA [59] is benchmark designed for text-rich visual question answering, requiring models to not only recognize textual content within images but also reason about the extracted information. This task evaluates models ability to accurately identify text characters while effectively handling the inherent noise present in OCR-generated outputs. ScienceQA. ScienceQA [41] is multiple-choice benchmark designed to evaluate zero-shot generalization in scientific question answering. It features multimodal questions covering diverse range of science topics, with annotated answers supported by corresponding lectures and explanations. These annotations provide general external knowledge and specific reasoning for deriving the correct answer. In our study, we conduct experiments on the image subset of ScienceQA to assess model performance in (a) Logps comparison (b) Loss comparison Figure 11. Comparison between C-DPO and standard DPO during model training. The proposed C-DPO promotes more stable gradient updates, enhancing training stability. the counterparts used for comparison. In Appendix D.3, we present the detailed evaluation setup. In Appendix D.4, we provide detailed results from some of the experiments. Additionally, in Appendix D.5, we present specific details of the ablation studies. D.1. Evaluation Benchmarks We provide detailed description of the evaluation benchmarks used in our study. Object HalBench. Object HalBench [55] is widely used benchmark for assessing common object hallucinations in detailed image descriptions. Following [77], we incorporate eight diverse prompts to enhance evaluation stability. We report two key metrics: the response-level hallucination rate (Resp.), which measures the proportion of responses containing hallucinations, and the mentionlevel hallucination rate (Ment.), which quantifies the percentage of hallucinated object mentions. AMBER. AMBER [63] is widely used metric for hallucination evaluation, assessing the frequency of hallucinatory objects in model-generated responses. In the generative component of AMBER, hallucination is quantified using the following three metrics: CHAIR score: CHAIR(R) = 1 len(R obj Aobj) obj) len(R . (3) 5 multimodal scientific reasoning. consistency. MM-Vet. MM-Vet [78] is comprehensive benchmark designed to assess models ability to engage in visual conversations across diverse tasks. It evaluates response correctness and helpfulness through GPT-4 [1] scoring. The dataset includes wide range of image types, such as real-world scenes, artworks, statistical graphs, and memes, paired with open-ended questions that require multimodal reasoning. MM-Vet focuses on six core evaluation capabilities: recognition, knowledge, optical character recognition (OCR), spatial awareness, language generation, and math. D.2. Evaluation Counterparts We compare our SENTINEL approach with various methods designed to mitigate hallucinations in MLLMs, all of which are trained on or applied to LLaVA-v1.5 [34] to ensure fairness. VCD. VCD [27] is training-free method designed to mitigate hallucinations in vision-language models by enhancing their focus on image content. It achieves this by contrasting output distributions derived from both original and distorted visual inputs. This contrastive approach helps the model better align its responses with actual image content rather than relying on spurious correlations. The computational cost of single inference step using VCD is approximately twice that of standard greedy decoding. OPERA. OPERA [19] addresses hallucination in multimodal language models through two strategies: OverTrust Penalty and Retrospection-Allocation. The OverTrust Penalty reduces overconfidence by adjusting model logits during beam search, while RetrospectionAllocation revisits previously generated tokens to correct potential errors, improving response accuracy. DoLa. DoLa [8] enhances factual accuracy by leveraging contrastive decoding across different model layers. This approach effectively reduces the generation of incorrect facts and consistently improves truthfulness in model responses. EFUF. EFUF [70] mitigates hallucinations without requiring paired data by employing gradient ascent and three specialized loss functions. It applies gradient descent when encountering real objects and gradient ascent when detecting hallucinated ones, effectively refining the models output. HA-DPO. HA-DPO [82] formulates hallucination mitigation as preference selection task, training the model to prefer non-hallucinated responses when given two outputs for the same image. To ensure training stability, it incorporates causal language modeling objective into the DPO loss. Additionally, both positive and negative samples are rewritten in GPT-4s style to maintain stylistic POVID. POVID [86] highlights the role of inferior responses in training and enhances them by modifying images and introducing extra hallucinations via GPT4V [44]. The approach then fine-tunes LLaVA-1.5-7B using set of 17K preference data. RLAIF-V. RLAIF-V [77] employs Feedback From Peer strategy, where the overall response score is derived by aggregating scores from decomposed sub-responses, reducing reliance on costly, ultra-large proprietary models like GPT4. The model is trained using an iterative alignment approach, conducting DPO training over four iterations, with each iteration consisting of four epochs. TPO. TPO [16] is self-correction approach that enables the model to mitigate its own hallucinations at the topic level. Using deconfounded strategy, it replaces each topic in the response with either the best or worst alternatives generated by the model. This process creates more contrasting pairwise preference feedback, improving the quality of feedback without requiring human intervention or proprietary models. HSA-DPO. HSA-DPO [69] first trains hallucination detection model using datasets constructed by GPT-4V [1]. This model is then leveraged in detect-then-rewrite pipeline to generate 6K preference data for training. Finally, MLLMs are aligned using the proposed hallucination severity-aware DPO method. D.3. Evaluation settings Our overall evaluation setup strictly follows the guidelines provided by LLaVA-v1.5 [34], with certain hyperparameter settings detailed in Tab. 12. D.4. Evaluation Results Detailed results of MM-Vet. We present the detailed results of the MM-Vet [78] benchmark in Tab. 13. The results indicate that, compared to existing methods, our approach achieves the most significant improvement on the 7B model, with an increase of 1.6 points. Notably, for the 13B model, while other methods exhibit varying degrees of performance degradation, our method continues to yield improvements. This demonstrates the effectiveness of our approach in enhancing both the correctness and helpfulness of model responses. Detailed results of AMBER. We present the detailed results of the discriminative part of the AMBER [78] benchmark in Tab. 14. The results show that some of the existing methods may experience decline in performance across certain specific hallucination categories. In contrast, our approach demonstrates improvements in every specific hallucination category, regardless of whether the 7B or 13B model is used. Notably, for the Existence hallucination type, our method improves the 7B model by 6.3 and the 6 Method Parameters Amplification Factor α Adaptive Plausibility Threshold Diffusion Noise Step Value 1.0 0.1 Repetition Penalty θ Adaptive Plausibility Threshold β Pre-mature Layers 1.2 0.1 [0, 2 , 32] VCD [27] DoLa [8] OPERA [19] Self-attention Weights Scale Factor θ Attending Retrospection Threshold Beam Size Penalty Weights 50 15 3 1 Table 12. Evaluation hyperparameters of decode-based methods. Method Rec OCR Know Gen Spat Math Overall LLaVA-v1.5-7B [34] VCD [27] OPERA [19] DoLa [8] EFUF [70] HA-DPO [82] POVID [86] RLAIF-V [77] TPO [16] Ours Ours + HA-DPO [82] 35.9 34.5 34.9 36.1 36.5 35.5 - 34.4 31.8 37.7 38.4 23.3 21.9 21.6 21.3 21.4 22.1 - 23.4 15.4 23.1 25.0 17.1 18.3 18.7 19.4 17.1 18.3 - 18.7 16.7 22.7 21.2 22.0 20.6 21.1 20.9 19.5 21.9 - 23.7 19.6 25.6 23.7 25.9 24.8 25.7 26.9 27.9 26.3 - 27.7 22.1 26.8 29.3 11.5 3.8 7.7 7.7 7.7 7.7 - 7.3 7.7 7.7 7. 31.00.2 29.81.2 30.30.7 30.80.2 31.20.2 30.60.4 31.80.8 29.91.1 25.75.3 32.61.6 33.52.5 LLaVA-v1.5-13B [34] 36.0 VCD [27] 33.72.3 vanilla-DPO [69] 35.01.0 HSA-DPO [69] 33.72.3 36.20.2 Ours Table 13. Full evaluation results of MM-Vet benchmark. indicates that the results are from [86]. 39.7 38.7 38.4 35.9 38.9 28.8 24.4 29.7 28.4 30.2 23.2 22.4 17.9 16.4 22.6 34.5 30.1 35.6 34.5 32. 24.2 26.4 21.0 18.9 23.1 11.5 7.7 11.5 15.0 15.0 13B model by 7.6 compared to the baseline. Method Existence Attribute State Number Action Relation Overall LLaVA-v1.5-7B VCD [27] DoLa [8] EFUF [70] HA-DPO [82] Ours 82.4 81.11.3 87.6 85.3 88.2 88.76.3 57.7 61.8 62.1 69.9 67.52.4 72.8 64.0 65.6 67.5 61.22.8 55.52.2 65.14.8 66.1 67.63.6 61.33.6 74.84.9 56.51.2 78. 81.1 80.90.2 82.4 80.40.7 82.3 82.11.0 67.7 66.71.0 56.311.4 67.40.3 68.7 70.62.9 74.1 73.90.2 77.8 75.0 78.0 79.35.2 78.5 78.5 86.17.6 LLaVA-v1.5-13B 73.1 VCD [27] 73.8 78.75.6 Ours Table 14. Full evaluation results of AMBERs descriminative part. We report F1 scores for each category and the overall score. 74.2 70.0 71.7 73.60.6 72.62.6 66.60.6 81.67. 44.9 45.6 51.56.6 82.2 81.60.6 82.60.4 66.0 69.0 D.5. Details of Ablation Study In this section, we provide more specific details of the ablation studies to validate the effectiveness of our method. Effect of style consistency. Many preference training methods adopt rewriting techniques to construct nonhallucinated training samples [16, 69, 82]. To validate the negative impact of rewritten training data on the models generalization performance, we follow the approach of HADPO [82] and design prompts to instruct GPT to rewrite the preference training samples. Specifically, we prompt GPT4[1] to rewrite yw and yl in different style while ensuring coherence with the given context. The prompt template is provided in Tab. 18, and the results are presented in the main paper Tab. 2. To evaluate how our in-domain training data affects the models linguistic qualities, we adopt the approach from [56] and use GPT-4o-mini [20] as judge. Responses are rated on scale of 0 to 10 across four aspects: grammatical correctness, fluency, detailedness, and choice of words. We assess the models performance on 300 image description tasks from Object HalBench [55]. The evaluation prompt is shown in Tab. 17. As demonstrated in Tab. 15, our training not only preserves the models linguistic capabilities but also improves the detailedness of the descriptions. Method Grammatical Correctness Fluency Detailedness Choice Of Words LLaVA-v1.5-7B SENTINEL 9.92 9.970. 9.28 9.530.25 8.21 8.320.11 8.94 8.970.03 9.95 9.980.03 LLaVA-v1.5-13B SENTINEL Table 15. Language quality evaluation results. Our in-domain training data preserves the models language quality in image detail description tasks while improving the level of detailedness. 9.44 9.600. 8.95 8.980.03 8.29 8.400.11 To further investigate the impact of rewritten data on training, we analyze the log probabilities (logps) and loss trends of the policy model when trained with in-domain data versus rewritten data, as shown in Fig. 12. Our observations indicate that the rewritten data, due to its deviation from the models original output style and linguistic domain, significantly lowers the logps of both positive and negative samples. Additionally, the rewriting process obscures the fundamental distinction between positive and negative samples (i.e., whether hallucinations are present), thereby weakening the models ability to distinguish between them and diminishing the effectiveness of the training signal. As result, models trained on in-domain data converge to lower loss and achieve superior differentiation between positive and negative samples, whereas training with rewritten data fails to provide comparable improvements. Effect of data scaling up. Since our proposed SENTINEL does not rely on ultra-large proprietary models [21, 69, 75, 80, 82, 86] or human annotators [13, 76] for preference learning dataset construction, it can efficiently collect more training data. As shown in Tab. 16, although RLHF-V [76] leverages high-quality human-annotated training data to achieve lower hallucination rate with fewer training samples, their high cost limits the scalability of the training data. Our method enables cost-effective scaling up, leading to improved model performance. Method Data Scale Object HalBench[55] AMBER[63] Resp. Ment. Acc F1 LLaVA-v1.5-7B RLHF-V [76] SENTINAL SENTINAL - 1.4K 2.0K 8.6K 52.7 12.2 39.0 4.3 28.0 7. 20.0 2.6 71.5 72.6 72.2 76.1 74.1 75.0 74.9 79.3 Table 16. Impact of training data quantity. The results show that SENTINEL demonstrates better efficiency and scalability. Following is detailed image description. Your task is to assess the response on the following criteria: 1. Grammatical Correctness: Analyze the response for grammar, punctuation, and syntax accuracy. 2. Fluency: Evaluate whether the response flows smoothly, reads naturally, and maintains coherence throughout. 3. Detailedness: Check if the response provides sufficient and relevant detail to address the topic comprehensively, without redundancy or unnecessary information. 4. Choice of Words: Assess if the words used are appropriate, varied, and effectively convey the intended message. Rate each criterion on scale from 0 to 10, where 0 indicates poor quality and 10 signifies an excellent response. Here is the image description to evaluate: (a) Training log probabilities (logps) comparison {description} Your response should be in this format: Grammatical Correctness: SCORE Fluency: SCORE Detailedness: SCORE Choice of Words: SCORE Table 17. Prompts for linguistic quality evaluation. Responses are rated on scale of 0 to 10 across four aspects: grammatical correctness, fluency, detailedness, and choice of words. Rewrite training data Please help me rewrite the given sentences in style different from the original. You will be provided with three parts: context refers to the previously generated sentences, and option one and option two represent two choices for the sentence that follows the context. Your goal is to make the new versions from the original while preserving all details and information. Avoid adding any new information or changing the original meaning. Please rewrite the two options that differ in tone, structure, word choice, and phrasing compared to the original, while ensuring coherence and natural flow with the given context. The format of your output should be: Option one: ... Option two: ... The sentences are: Context: {context} Option one: {y win} Option two: {y lose} Table 18. Prompts for rewriting. We prompt GPT-4 [1] to rewrite y+ and yl in style different from the original while ensuring coherence with the given context to show the effect of rewriting on the models generalization performance. indicate that our SENTINEL approach consistently reduces hallucinations across range of model families and sizes, while preserving or even enhancing overall performance, thereby demonstrating its robustness and effectiveness. During thus experiments, to generate training data for each target model, we simply replace the sampling model within the SENTINEL framework with the corresponding model, thereby demonstrating SENTINELs modelagnostic design. For training, we employ the widely used LLaMA-Factory [83] framework to ensure fairness and reproducibility. Evaluation follows the same protocol described above1. All training data, configuration details, and associated resources will be released publicly. 1For efficiency, in this set of experiments we use the GPT-4o [20] model for HallusionBench [12] evaluation, which makes these results not directly comparable to those reported for the benchmark in the main paper. (b) Training loss comparison Figure 12. Impact of rewriting on the training process. Training with rewritten data fails to achieve the same level of convergence, resulting in higher final loss and weaker differentiation between positive and negative samples, demonstrates the necessity of in domain training data. Complement with existing preference learning methods. HA-DPO [82] employs GPT-4 [1]-based rewriting approach to modify both positive and negative samples in the preference training data, ensuring stylistic consistency between them. However, this rewriting process introduces stylistic discrepancies between the training data and the target models original outputs, potentially affecting its generalization ability. To assess the effectiveness of in-domain preference learning data, we augment the HA-DPO [82] training dataset (approximately 4.4K samples) with subset of our constructed dataset (6K samples from the full 8.6K) and train LLaVA-v1.5-7B under the same training settings as HA-DPO. As shown in the main paper Tab. 5, integrating even partial set of our training data significantly reduces hallucinations while enhancing the models overall performance. These results further demonstrate that our sentencelevel preference training approach is complementary to existing sample-level preference learning methods. E. SENTINEL with Other Baselines In this section, we explore the effectiveness of our SENTINEL approach when applied to other baselines, specifically LLaVA-v1.6 [35], Qwen2-VL [3] and Qwen2.5VL [3]. The results are presented in Tab. 19. The findings 8 Model Hallucination benchmarks General benchmarks Object HalBench [55] HallusionBench [12] VQAv2 [10] TextVQA [59] ScienceQA [41] MM-Vet [78] Resp. Ment. Question Acc. Acc. Acc. Image Acc. Overall LLaVA-v1.6-vicuna-7B 15.35.0 10.13.4 36.7337. 81.581.5 59.459.4 74.374.2 40.945.4 LLaVA-v1.6-vicuna-13B 13.74. 7.72.6 41.1041.36 82.282.2 63.663.5 77.778.0 47.848. Qwen2-VL-2B-Instruct 15.32.3 8.61.7 41.2842.16 81.581.5 78.378. 76.977.4 49.449.8 Qwen2-VL-7B-Instruct 14.34.8 8.54.0 51.5553. 83.783.8 82.282.2 85.786.9 62.762.8 Qwen2.5-VL-7B-Instruct 15.04. 9.22.8 52.0052.08 84.084.0 77.777.7 88.688.5 72.072. Table 19. Comparison of hallucination mitigation methods with other baseline models: effectiveness and general capabilities (baselineSENTINEL). This evaluation highlights the best and second-best results in bold and underlined, respectively. All comparisons are performed under identical model size constraints. Resp. and Ment. denote response-level and mention-level hallucination rates, while Hal. and Cog. represent the Hallucination Score and Cognitive Score, respectively. F. Related Work Multimodal large language models. In recent years, vision-language models (VLMs) have made remarkable progress [38, 50, 58, 61, 64, 74]. With the advancement of large language models (LLMs), multimodal large language models (MLLMs) have achieved impressive alignment between visual and textual representations through crossmodal feature integration, marking crucial milestone toward truly general-purpose AI systems [2, 3, 9, 24, 29, 33 35, 44, 48, 65, 7173, 84, 87]. However, mitigating hallucination and building reliable models for real-world scenarios remain significant challenges. Object Hallucination. Object Hallucination (OH) refers to the phenomenon where MLLMs generate text that is semantically coherent but misaligned with the given image [4, 36, 53]. Prior studies suggest that this issue may arise during generation due to an over-reliance on linguistic priors and insufficient attention to visual features [14, 43, 67]. Furthermore, research indicates that hallucination tends to intensify over time [22, 85]. Mitigate OH with improved decoding strategies. Several approaches have explored enhanced decoding strategies to mitigate object hallucination. VCD [27] enhances the models focus on image content during generation by applying contrastive decoding between the original image and noise-corrupted version. DoLa [8] improves factual accuracy by leveraging contrastive decoding across layers to better surface factual knowledge and reduce incorrect outputs. OPERA [19] introduces Over-Trust Penalty and Retrospection-Allocation to address hallucination in multimodal language models. HALC [6] reduces object hallucination through an adaptive focal-contrast decoding approach, incorporating dynamic auto-focal grounding mechanism for real-time token correction and refined beam search strategy to effectively suppress hallucinations while maintaining text quality. Mitigate OH by preference learning. Preference learning 9 is powerful paradigm for aligning large language models with human judgments and values. Recently, Direct Preference Optimization (DPO) [51] and its variations [42, 46, 47] have made preference learning more accessible and easier to integrate. Another line of research on mitigating OH employs preference learning to tackle object hallucination by reformulating it as preference optimization problem. These approaches construct high-quality, stylistically consistent positive-negative sample pairs to enhance model training. Rewriting is an effective method for obtaining non-hallucinated training data. HA-DPO [82] utilizes GPT [1] to directly detect and rewrite the models original output, ensuring that both positive and negative samples undergo rewriting. HSA-DPO [69] distills smaller hallucination detection model from the proprietary model GPT and applies it to detect hallucinations and refine responses through rewriting. In contrast, RLAIF [77] does not employ rewriting; instead, it constructs datasets using the Feedback from Peer approach, leveraging open-source models outputs as feedback. This method directly utilizes the models full outputs as both positive and negative samples. Eliminating OH via unlearning. EFUF [70] employs an unlearning-based approach to mitigate OH in MLLMs. Specifically, it applies gradient ascent when hallucinated objects are detected during the models generation, effectively suppressing hallucinations in targeted and efficient manner. This method enables precise removal of hallucinated content during captioning without compromising the models overall performance. G. Additional Case Studies In this section, we provide more case studies to further illustrate the effectiveness of our proposed SENTINEL in mitigating hallucinations and enhancing its generalization capabilities. To ensure fair comparison, results for both LLaVA-v1.5 and SENTINEL are obtained using greedy decoding, while GPT-4V refers to gpt-4-vision-preview [44]. Figure 13. Comparing general image description results between SENTINAL and its base model LLaVA-v1.5-7B. Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing more detailed description. Figure 14. Comparing detailed image description results between SENTINAL and its base model LLaVA-v1.5-7B. Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing more detailed description. 11 Figure 15. Comparing visual question answering results between SENTINAL and its base model LLaVA-v1.5-7B. Our method effectively mitigates hallucinations while enhancing general performance of the base model, leading to more accurate and detailed answers. 12 Figure 16. Comparing general image descriptions between SENTINAL and its base model LLaVA-v1.5-13B. Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing more detailed description. Figure 17. Comparing detailed image descriptions between SENTINAL and its base model LLaVA-v1.5-13B. Our method effectively mitigates hallucinations while enhancing the general performance of the base model, providing more detailed description. 14 Figure 18. Comparing visual question answering between SENTINAL and its base model LLaVA-v1.5-13B. Our method effectively mitigates hallucinations while enhancing the general performance of the base model, leading to more accurate answers."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology, Shenzhen",
        "The Chinese University of Hong Kong",
        "The Chinese University of Hong Kong, Shenzhen"
    ]
}