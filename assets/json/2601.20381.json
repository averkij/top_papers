{
    "paper_title": "STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation",
    "authors": [
        "Alexandre Chapin",
        "Emmanuel Dellandréa",
        "Liming Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 2 ] . [ 1 1 8 3 0 2 . 1 0 6 2 : r STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation Alexandre Chapin1[0009000037799387], Emmanuel Dellandrea1[000000017346228X], and Liming Chen1[0000000236549498] Ecole Centrale de Lyon {first-name.surname}@ec-lyon.fr Abstract. Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and controllability in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), lightweight object-centric adaptation module that augments frozen visual foundation models with small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs multi-phase training strategy: object-centric slots are first stabilized through visualsemantic pretraining using language embeddings, then jointly adapted with downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control. Keywords: Object-centric representation Robotics Semantic aware."
        },
        {
            "title": "Introduction",
            "content": "Robotic manipulation requires perceptual representations that capture fine-grained spatial structure while remaining interpretable and controllable by high-level task specifications. Recent visual foundation models such as DINOv2 [26] provide powerful dense feature representations that generalize across domains, but they do not expose explicit object-level structure. As result, downstream policies must implicitly learn to attend to relevant entities, often leading to brittle behavior and poor generalization under visual variations [3,6]. Slot-based object-centric representation learning offers an appealing alternative. By decomposing scenes into discrete latent \"slots\", each representing an individual object or part, such models promote modular reasoning, interpretable structure and compositional generalization. Frameworks such as SlotAttention [22] and its successors [9,31,33] have shown that meaningful object 2 A. Chapin et al. representations can emerge without supervision when proper inductive biases are applied. However, these models remains unguided, the slot formation process is purely visual, without semantic control. As result, the learned slots may not correspond to task-relevant entities [6], limiting their usefulness in action-oriented contexts such as robotics. Recent efforts have attempted to integrate semantics into slot-based learning. Shatter & Gather [17] achieved strong alignment between slots and text post hoc through contrastive learning. CTRL-O [8] successfully demonstrated language controllability, but its training relies on several large scale visual language models (Llama2Vec [21] and CLIP [30]). While effective for object retrieval and imagegeneration conditioning, these approaches have yet to be adapted for robotic control scenarios, where aligning visual perception with instruction semantics is essential for manipulation. In this work, we introduce STORM, lightweight slot-based task-aware module that operates on frozen DINOv2 features. Our key contributions are twofold: Multi-phase adaptation of visual foundation model: We introduce two-stage learning strategy for adapting frozen visual foundation model features (DINOv2) into task-relevant, object-centric representations. The strategy first stabilizes slot formation through visual-semantic pretraining, and then aligns the same object-centric layers with downstream manipulation objectives via joint training with control policy. Lightweight semantic-aware object-centric representation: We propose STORM, compact, stable slot-based module consisting of small number of object-centric layers placed on top of frozen DINOv2 features. Despite its minimal architectural overhead, STORM produces semantically grounded, interpretable object representations that are directly consumable by control policies. Overall, our results demonstrate that multi-phase strategy is an efficient ingredient for effectively transforming powerful but generic foundation model features into task-aware, object-centric representations suitable for robotic manipulation."
        },
        {
            "title": "2.1 Pre-trained visual foundation models",
            "content": "Recent advances in visual representation learning have been driven by largescale, self-supervised methods trained on internet-scale, unlabeled data. Many of these methods adopt Vision Transformer (ViT) architectures [10] and training objectives such as masked image modeling [13], contrastive learning [4,5,7], or hybrids that combine both paradigms [26,39]. The outcome of this line of work are highly robust pre-trained vision models (PVMs), for example DINOv2 [26], that produce rich spatially-structured features well suited for downstream perception and reasoning. At the same time, multimodal training on paired imagetext corpora has produced aligned visionlanguage models such as CLIP [30], enabling open-vocabulary and semantic supervision for visual systems. STORM 3 Most robotics pipelines today rely on such pre-trained encoders for sampleefficient policy learning [28,29,32]. However, directly using dense, high-dimensional PVM features for control can be computationally expensive and may include task-irrelevant information. In contrast, STORM produces compact, discrete slot-based latents that are both task-aware and immediately usable by downstream policies. This structured, low-dimensional output trades spatial resolution for efficiency and robustness, yielding higher throughput and reduced sensitivity to visual noise in many control settings."
        },
        {
            "title": "2.2 Object-centric representation learning",
            "content": "Object-Centric Representation Learning (OCRL) formalizes the idea of decomposing scenes into set of object-like latent slots, which promotes modularity and compositionality in perception. Early methods such as Slot Attention [22] demonstrated strong results on synthetic data but struggled to scale to realworld imagery. To bridge this gap, recent work has combined stronger visual backbones, for example, replacing pixel-level reconstruction targets with features from DINO [31], and has explored more expressive decoders based on Transformers or generative diffusion models [35,33,34,15,37]. parallel strand of research seeks to ground slots semantically by leveraging external visionlanguage models. Approaches that use CLIP supervision or contrastive grouping encourage slots to align with human-interpretable concepts [12,17,38]. Nevertheless, slot assignments often remain underconstrained: slots can be noisy, split or merged unpredictably, and may not consistently correspond to entities that are relevant for downstream tasks. Methods like CTRLO [8] attempt to impose language-conditioned control over slot semantics, but doing so has typically required composing multiple large models (e.g., LLM2Vec [1], CLIP), which increases system complexity and computation. Importantly, most language-guided OCRL work to date focuses on static objectives (retrieval, segmentation, or generation). There has been limited exploration of whether text-grounded slots can serve as reliable inputs for dynamic, closed-loop control where temporal reasoning and action-affordance estimation are critical. STORM explicitly targets this gap by learning text-guided, objectcentric slots and evaluating their effectiveness on robotic manipulation tasks, demonstrating that semantically aligned slots can support not only recognition but also action prediction under dynamic constraints."
        },
        {
            "title": "2.3 Object-centric representations for robotics",
            "content": "Object-centric representations have recently gained significant attention in robotics as means to improve generalization, compositionality, and data efficiency in manipulation policies [2,6,36,40]. growing body of work leverages pre-trained segmentation and mask-based models to obtain object-level abstractions from raw visual inputs. In particular, the Segment Anything Model (SAM) [18] and its variants have been widely adopted to extract object masks that serve as intermediate representations for perception, planning, and control [28,32,40]. These 4 A. Chapin et al. masks have been used to build object-centric state representations, to initialize object proposals for manipulation, and to enable open-vocabulary perception in conjunction with language supervision. While mask-based approaches offer strong spatial localization and benefit from powerful pre-training, they typically rely on heuristic selection, post-processing, or external prompting to identify task-relevant objects. As result, the resulting object sets can be brittle in cluttered scenes, sensitive to occlusions, or misaligned with the objects that are most relevant for control. Moreover, segmentation masks alone provide limited semantic abstraction and often require additional processing to be integrated into downstream policies, increasing system complexity. In contrast, slot-based object-centric models learn compact, continuous representations that can encode object identity, attributes, and relations in form that is directly consumable by control policies. However, when deployed in robotic settings, the lack of explicit control over slot formation can lead to failures under visual distraction or distributional shift [6]. This highlights fundamental trade-off between bottom-up mask generation and learned object abstractions. Our approach explicitly condition slot formation on task instructions and introduce multi-phase strategy that first stabilizes semantic object discovery and then aligns object-centric representations with downstream control objectives."
        },
        {
            "title": "3 Method",
            "content": "Our method, STORM  (Fig. 1)  , extends prior object-centric learning approaches [8] with several novel design choices tailored to robotic control. The following sections detail these innovations."
        },
        {
            "title": "3.1 Multi-phase learning for object-centric adaptation",
            "content": "Although frozen foundation model features provide strong visual representations, directly training object-centric layers and control policy end-to-end often results in unstable or degenerate slot assignments, as shown in Section 4.2. To address this, we adopt two-stage strategy that progressively introduces task supervision. Stage 1: VisualSemantic Slot Pretraining (Section 3.2): In the first stage, the object-centric module is trained independently of the control policy. Given frozen DINOv2 [26] feature maps, STORM produces fixed number of slots, each corresponding to candidate object. We supervise these slots using visualsemantic objective that aligns slot embeddings with CLIP text [30] embeddings corresponding to object-level descriptions. An additional regularization loss encourage distinct and spatially localized slot assignments. Importantly, no policy gradients are used at this stage. Stage 2: Joint SlotPolicy Training (Section 3.3): In the second stage, the pretrained object-centric module is trained alongside downstream policy using imitation learning. The slot representations produced by the objectcentric layers serve as the visual input to the policy. Crucially, the visual STORM 5 Fig. 1. Overview of STORM. STORM follows two-stage training to produce taskaware object-centric representations for robotic control. (Step 1) Semantic learning: Frozen DINOv2 features are aggregated by Slot-Attention module conditioned on noun embeddings extracted from text prompts using frozen CLIP-text encoder. Reconstruction and contrastive losses encourage stable, semantically grounded slot formation. (Step 2) Dynamic task alignment: The pretrained object-centric module extracts slots from camera observations, which are combined with task embeddings, robot proprioception, and learnable [ACT] token and processed by Transformer decoder policy. An GMM action head predicts the next action from the [ACT] token. 6 A. Chapin et al. module and the policy are optimized independently: gradients from the policy loss are not backpropagated into the visual backbone, and gradient detachment is applied at the visual feature level. This training scheme allows the object-centric layers to preserve the semantic structure acquired during Stage 1 while adapting to task-relevant visual statistics, without destabilizing slot formation through policy-driven gradients. The resulting representations remain semantically grounded while becoming better aligned with the demands of manipulation. This training strategy decouples object discovery from control, reducing optimization interference and leading to more robust object-centric representations. In Section 4, we empirically show that removing this multi-phase strategy significantly degrades performance, even when using identical architectures and training budgets."
        },
        {
            "title": "3.2 Visual-Semantic learning",
            "content": "The first training phase focuses on properly decomposing an input image given set of textual prompts. The framework is composed of two main parts: an object-centric decomposition and language conditioning branch as shown in the top part of Fig. 1. Object-centric decomposition The object-centric decomposition part follows existing works on object-centric models [31]. An input image is encoded by frozen DINOv2 [26] backbone to obtain patch features = {f1, .., fN } = B(I). set of slots = {s1, ..., sK} is randomly initialized and cross-attend with Slot-Attention [22]. Slot-Attention is modified cross-attention with normalization over queries (slots) instead of keys (features). Finally, each slot are given to MLP decoder in order to reconstruct the feature maps of DINOv2: Lrecons = ˆF (1) with ˆF = D(S), being the MLP decoder used to reconstruct the patch features from the slots. Language conditioning and semantic learning In order to control the generation of slots, we follow prior work [8] by initializing part of the slots with semantic knowledge from visual-language model. The original work uses LLM2Vec [1] to semantically initialize some slots and CLIP-text [30] to force contrastive alignment during training. To reduce the memory and compute requirements for our robotic use case, we discard LLM2Vec and only use CLIPtext as input and training signal, going from 7b parameters model to around 300M. In order to align slots with textual semantics, each conditioned slot uses its corresponding mask mk to mask-pool image patches FCLIP, from dense CLIP visual encoder [12]. We then apply contrastive loss between the mask-pooled visual features masked = mk FCLIP and the CLIP text embeddings zemb: STORM 7 Lsem = (cid:88) i=1 log exp (cid:0)zemb t=1 exp (cid:0)zemb masked /τ (cid:1) masked i (cid:80)T /τ (cid:1) , (2) where denotes the number of textual prompts. This loss encourages semantically initialized slots to bind to relevant spatial regions, but may suffer from slot collapse, where all slots attend uniformly or single slot dominates. To mitigate this effect, we introduce an entropy-based slot usage penalty. Let mk RBKN denote the soft assignment masks, where is the batch size, the number of slots, and the number of patches. Slot collapse is quantified by aggregating slot usage (using masks weights), normalizing across slots, and computing the entropy: Sb,k = (cid:88) n=1 mb,k,n, Pb,k = Sb,k j=1 Sb,j + ϵ (cid:80)K , Hb = 1 log K (cid:88) k=1 Pb,k log(Pb,k + ϵ), Lpen = 1 (3)"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) b=1 Hb. Minimizing Lpen encourages balanced usage of all slots and discourages degenerate solutions in which attention collapses onto few slots. Overall loss The model is then pre-trained by combining all the losses together: LOverall = Lrecons + Lsem + Lpen (4)"
        },
        {
            "title": "3.3 Joint Slot–Policy Training",
            "content": "In the second phase, we study whether semantically grounded object-centric representations improve robustness in robotic manipulation. To this end, we integrate pretrained object-centric perception module into an imitation learning pipeline, yielding our final STORM setup. Given dataset of expert demonstrations = τ1, . . . , τn with trajectories τi = [(o0, a0), . . . , (oT , aT )], the policy π predicts actions from observations consisting of visual inputs, task instructions, and proprioceptive states. As shown in the bottom of Fig. 1, camera observations are processed by the object-centric module to produce fixed set of slot representations, which serve as visual tokens for the policy. Spatial features extracted from each slots mask are concatenated to the corresponding slot embedding. Task instructions are encoded using frozen CLIP text encoder [30] and provided as global task 8 A. Chapin et al. embedding, while nouns parsed from the instruction condition the object-centric module to emphasize task-relevant entities. Proprioceptive inputs are projected as additional tokens. All tokens are processed by Transformer decoder together with learnable [ACT] token. The transformer processes history of 4 frames and predicts sequence of 10 future actions. The action head, modeled as Gaussian Mixture Model (GMM), maps the [ACT] token to the next relative joint command. The object-centric module and policy are trained jointly. To preserve semantic structure learned in Stage 1, gradients from the policy loss do not propagate into the visual part, and feature-level gradient detachment is applied. The object-centric component is trained with reduced learning rate and augmented with SlotSlot contrastive loss [24] and single Transformer layer to encourage temporal consistency. The policy is trained using standard imitation learning objective."
        },
        {
            "title": "4 Experiments",
            "content": "We first validate our visual semantic learning on classical computer vision scenarios and compare it with existing object-centric models on object-discovery and decomposition scenarios. Then, we evaluate our task-aware multi-phase learning scheme on top of an existing VFM (DINOv2) with simple Transformer decoder policy to learn robotic manipulation tasks. Our evaluations are performed in two different simulation benchmarks on in-domain data but also on generalization to new distractors. We finally ablate the components of our framework to understand what constitutes the success of STORM."
        },
        {
            "title": "4.1 Object decomposition and grounding",
            "content": "Following prior work on object-centric representation, we first assess how effectively our visual-semantic model can discover in images. Pre-training setup. Our visual-semantic model described in Section 3.2 is pretrained for 300k steps on the VG-COCO dataset [8] using the AdamW optimizer [23]. We use learning rate of 4 104 with cosine decay schedule and 10,000 warmup steps, batch size of 64 on single V100 GPU. We set the number of slots to 7 with slot dimension of 256. We use the DINOv2-B/14 version. Metrics. Object discovery performance is commonly evaluated using the Foreground Adjusted Rand Index (FG-ARI) [14], which quantifies segmentation accuracy and object-consistency, and Mean Best Overlap (mBO) [27], which measures the pixel-wise overlap between predicted and ground-truth object masks. Object discovery. We benchmark our visual-semantic model on objectcentric discovery tasks using PASCAL VOC 2012 [11] and COCO [19] datasets, both of which contain complex, multi-object scenes. These datasets are standard in the literature, allowing direct comparison with previous object-centric STORM 9 methods. The goal of this evaluation is to asses how well our model separates distinct entities in an image. We follow established evaluation protocols and report results on the validation sets of both datasets. We compare STORM against several state-of-the-art baselines: DINOSAUR [31], SPOT [16], Stable-LSD [15], Slot-Diffusion [37], and our closest baseline CTRL-O which is weakly-supervised but uses stronger semantic model as input (e.g. LLM2Vec) [8] For all baselines, we report the results as provided in their respective papers  (Table 1)  . STORM surpasses all unsupervised models in terms of FG-ARI and performs competitively with CTRL-O on COCO, outperforming it by 0.6 point on the mBOi metric. While our model slightly lags in mBO compared to some methods, it clearly exceeds them in segmentation quality (FG-ARI). Note that STORM uses the original MLP decoder from DINOSAUR, which is known to produce less precise masks than transformer or diffusion decoders. Integrating such decoders could further enhance mask sharpness and overall segmentation quality. Table 1. Object discovery performance. Metrics include mBOi (mean BestOverlap, instance-level), mBOc (class-level), and FG-ARI (Foreground Adjusted Rand Index) on Pascal VOC and COCO. Bold is the best result, underline is the second best. Unsupervised (U), Weakly-Supervised (WS) Sup. Model VOC COCO mBOi mBOc FG-ARI mBOi mBOc FG-ARI DINOSAUR [31] SPOT [16] Stable-LSD [15] Slot-Diffusion [37] CTRL-O [8] STORM (Ours) 39.5 48.3 50.4 40.6 40.9 55.6 55.3 46.5 WS 24.6 19.9 17.8 45.8 27.7 35.0 30.4 31.0 27.2 27.8 30.9 44.7 35.0 36.6 40.3 37.0 35.0 37.2 47.5 44."
        },
        {
            "title": "4.2 Robotic manipulation",
            "content": "In our robotic manipulation experiments, we fine-tune the pre-trained objectcentric layers in parallel with the policy but always keep frozen the backbone (DINOv2). The policy architecture is shared across all visual models and follows the structure detailed in Section 3.3. Joint-training is performed for 150k steps directly on the example trajectories. The visual loss is identical to that used during pre-training, but with reduced learning rate of 1e5 all other hyperparameters for the visual module remain unchanged. The policy is optimized using an MSE loss between the normalized predicted actions and the ground-truth actions. 10 A. Chapin et al. Manipulation and generalization evaluations Table 2 reports the overall manipulation performance on MetaWorld [25] and LIBERO [20], evaluated both in-distribution (ID) and under new visual distractors (ND). On MetaWorld, we observe clear hierarchy between visual representations. The frozen DINOv2 baseline performs adequately in ID settings (73.8%) but suffers severe degradation under visual shifts, dropping 34.2 points to 39.6% in the ND setting. Fine-tuning the backbone (DINOv2 ft.) slightly improves robustness (+4.2 points in ND) but at the cost of ID performance, suggesting trade-off between specialization and generalization. In contrast, STORM achieves the best performance across both settings. It not only maintains strong ID accuracy (74.8%) but also substantially boosts robustness to unseen distractors, improving the ND success rate by 12.7 points over the frozen baseline. On LIBERO, the performance gap becomes even more pronounced. While the frozen DINOv2 baseline maintains respectable success rate of 78.9% (ID) and 70.3% (ND), fine-tuning the backbone (DINOv2 ft.) does not yield the same robustness gains observed in MetaWorld, actually resulting in slight performance drops in both settings. This suggests that standard fine-tuning may struggle with the higher visual complexity and task diversity present in the LIBERO suite. STORM, however, outperforms the best baseline by 10.7 points in ID settings and 19.3 points under new distractors. Notably, STORM exhibits almost no performance decay when transitioning from ID to ND environments on LIBERO (89.6% vs. 89.3%), whereas the baselines show more noticeable gap. This suggests that the object-centric inductive biases in STORM allow the policy to remain focused on task-relevant features, effectively ignoring visual perturbations that typically degrade standard global representations. Table 2. Overall evaluations Success rate (%) on MetaWorld and LIBERO benchmarks. We report performance both In-Distribution (ID) and under New Distractor (ND) conditions. Bold is the best result, underline is the second best. Relative performance to DINOv2 frozen is shown in (green) and (red). Visual model DINOv2 DINOv2 (ft.) STORM (Ours) MetaWorld LIBERO ID 73.8 71.8 (-2.0) 74.8 (+1.0) ND 39.6 43.8 (+4.2) 52.3 (+12.7) ID 78.9 77.5 (-1.4) 89.6 (+10.7) ND 70.3 65.3 (-5.0) 89.3 (+19.0) Ablation studies We conduct ablations to evaluate the effects of STORMs key components. Specifically, we investigate: (1) the impact of the multi-phase learning, (2) the influence of different masks representations. We perform all of our ablative studies on subset of 10 tasks from the MetaWorld [25] benchmark. We evaluate each model on in-distribution data and out-of-distribution with the STORM Fig. 2. Robotics environments visualization. Examples of the simulated environments used in our experiments: MetaWorld (top row) and LIBERO (bottom row). introduction of different visual distractor objects. Results detailed in Table 3 expose that policy relying solely on frozen DINOv2 visual backbone already achieves strong in-distribution performance on MetaWorld. However, its generalization substantially degrades under out-of-distribution (OD) perturbations, highlighting the limitations of purely dense, non-object-centric representations when confronted with novel distractors. Multi-phase learning Naïvely introducing object-centric slots trained from scratch on the downstream manipulation data leads to significant performance drop in both ID and OD settings. This degradation indicates that jointly learning object decomposition and policy control from limited task-specific demonstrations is unstable and detrimental. As illustrated in the top-row of Figure 3, the resulting slot masks are noisy, poorly localized, and fail to consistently bind to task-relevant entities, which in turn negatively impacts policy learning. Pre-training the slot-based object-centric layers and then keeping them frozen during the policy training partially alleviates this issue, yielding comparable ID performance and notable improvement in OD generalization. Nevertheless, the best results are achieved with our proposed multi-phase training strategy (STORM), where object-centric representations are first learned independently and then carefully adapted during policy training. This decoupling stabilizes optimization and enables the slots to maintain coherent object bindings while adapting to task-specific cues, resulting in consistent improvements in both ID and OD performance. Mask representation: In addition to learning object-centric slots, we provide the policy with explicit spatial information derived from the slot masks. Mask representations encode complementary geometric cues (objects location, extent, and shape) that are not fully captured by appearance features alone. We evaluate three different mask encodings: center, which uses the masks center of mass; bbox, which encodes the bounding box coordinates; and mask, which directly embeds the binary mask using shallow CNN. A. Chapin et al. Table 3. Effect of object-centric layers and multi-phase learning. Success Rate (SR) (%) on MetaWorld. ID corresponds to the standard benchmark, while OD corresponds to out-of-distribution evaluations. Relative performance to DINOv2 is shown in (green) and (red). Model Pre-train DINOv2 [26] DINOv2 (ft.) DINOv2 + Slots scratch DINOv2 + Slots DINOv2 + Slots tuning (STORM) MetaWorld (SR %) ID 73.8 71.8 (-2.0) 69.0 (-4.8) 72.3 (-1.5) 74.8 (+1.0) 52.3 (+12.7) OD 39.6 43.8 (+4.2) 32.5 (-7.1) 48.4 (+8.8) Fig. 3. Comparison of slot masks. Visualization of slot masks obtained from training from scratch (top row) versus our two-step training with adaptation (bottom row). Our setup provides much sharper masks with focus on task-specif objects: the robot arm, the gripper, the drawers handle and finally the drawer body; while the training from scratch generates noisy masks with no proper focus. Table 4 shows that incorporating mask information is crucial for strong performance. Removing mask cues leads to an important drop in success rates, confirming that spatial grounding is essential for manipulation tasks. Among the evaluated representations, the simple center encoding performs best on MetaWorld, outperforming more expressive alternatives, and is comparable to mask on MetaWorld-Gen. This suggests that compact and stable representation of object position is sufficient, and even preferable, for policy learning. While directly encoding the full mask also yields strong results, it introduces additional complexity that does not consistently translate into higher performance. In contrast, bounding box representations appear too coarse and sensitive to noise, offering limited benefit over not using mask information at all."
        },
        {
            "title": "5 Conclusion",
            "content": "We presented STORM, slot-based, task-aware object-centric representation learning framework that integrates weak language supervision with multi-phase training strategy for robotic manipulation. By guiding slot formation using semantic cues from task descriptions and subsequently refining these representaTable 4. Effect of mask representation. Success Rate (SR) (%) on MetaWorld and Metaworld-Gen. Bold is the best result, underline is the second best. STORM 13 Mask repre. MetaWorld (SR %) Metaworld-Gen (SR %) center bbox mask 64.4 74.8 68.8 69.4 45.5 52.3 46.2 52.8 tions through joint policy optimization, STORM achieves improved stability and performance across range of simulated manipulation benchmarks. Despite these gains, our approach has several limitations. Evaluations are conducted in simulation, and it remains an open question how well the learned object-centric representations transfer to real-world robotic settings. Second, STORM relies on fixed number of slots and simple noun extraction from task descriptions, which may be insufficient for tasks involving complex relational language or large number of objects. Future work will explore extending STORM to real-robot platforms, incorporating more expressive language grounding mechanisms, and dynamically adapting the number and structure of slots. We also aim to investigate alternative forms of semantic supervision and more principled criteria for transitioning between training phases. Acknowledgments ."
        },
        {
            "title": "References",
            "content": "1. BehnamGhader, P., Adlakha, V., Mosbach, M., Bahdanau, D., Chapados, N., Reddy, S.: Llm2vec: Large language models are secretly powerful text encoders (2024), https://arxiv.org/abs/2404.05961 2. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P., Fu, C., Arenas, M.G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu, J., Ichter, B., Irpan, A., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, L., Lee, T.W.E., Levine, S., Lu, Y., Michalewski, H., Mordatch, I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M., Salazar, G., Sanketi, P., Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H., Vanhoucke, V., Vuong, Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., Zitkovich, B.: Rt-2: Visionlanguage-action models transfer web knowledge to robotic control (2023), https: //arxiv.org/abs/2307.15818 3. Burns, K., Witzel, Z., Hamid, J.I., Yu, T., Finn, C., Hausman, K.: What makes pretrained visual representations successful for robust manipulation? (2023), https: //arxiv.org/abs/2312.12444 14 A. Chapin et al. 4. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., Joulin, A.: Unsupervised learning of visual features by contrasting cluster assignments (2021), https:// arxiv.org/abs/2006. 5. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers (2021), https:// arxiv.org/abs/2104.14294 6. Chapin, A., Machado, B., Dellandrea, E., Chen, L.: Object-centric representations improve policy generalization in robot manipulation (2025), https://arxiv.org/ abs/2505.11563 7. Chen, X., Xie, S., He, K.: An empirical study of training self-supervised vision transformers (2021), https://arxiv.org/abs/2104.02057 8. Didolkar, A., Zadaianchuk, A., Awal, R., Seitzer, M., Gavves, E., Agrawal, A.: Ctrl-o: Language-controllable object-centric visual representation learning (2025), https://arxiv.org/abs/2503.21747 9. Didolkar, A., Zadaianchuk, A., Goyal, A., Mozer, M., Bengio, Y., Martius, G., Seitzer, M.: Zero-shot object-centric representation learning. arXiv:2408.09162 (2024), https://arxiv.org/abs/2408. 10. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition at scale (2021), https://arxiv.org/abs/2010.11929 11. Everingham, M., Gool, L.V., Williams, C.K.I., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. International Journal of Computer Vision 88(2), 303338 (2010). https://doi.org/10.1007/s11263-009-0275-4 12. Fan, K., Bai, Z., Xiao, T., Zietlow, D., Horn, M., Zhao, Z., Simon-Gabriel, C.J., Shou, M.Z., Locatello, F., Schiele, B., Brox, T., Zhang, Z., Fu, Y., He, T.: Unsupervised open-vocabulary object localization in videos (2024), https: //arxiv.org/abs/2309.09858 13. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are scalable vision learners (2021), https://arxiv.org/abs/2111.06377 14. Hubert, L., Arabie, P.: Comparing partitions. Journal of Classification 2(1), 193 218 (1985). https://doi.org/10.1007/BF01908075 15. Jiang, J., Deng, F., Singh, G., Ahn, S.: Object-centric slot diffusion (2023), https: //arxiv.org/abs/2303.10834 16. Kakogeorgiou, I., Gidaris, S., Karantzalos, K., Komodakis, N.: Spot: Self-training with patch-order permutation for object-centric learning with autoregressive transformers (2024), https://arxiv.org/abs/2312.00648 17. Kim, D., Kim, N., Lan, C., Kwak, S.: Shatter and gather: Learning referring image segmentation with text supervision (2023), https://arxiv.org/abs/2308.15512 18. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything (2023), https://arxiv.org/abs/2304.02643 19. Lin, T.Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C.L., Dollár, P.: Microsoft coco: Common objects in context (2015), https://arxiv.org/abs/1405. 20. Liu, B., Zhu, Y., Gao, C., Feng, Y., Liu, Q., Zhu, Y., Stone, P.: Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310 (2023) 21. Liu, Z., Li, C., Xiao, S., Shao, Y., Lian, D.: Llama2vec: Unsupervised adaptation of large language models for dense retrieval (2025), https://arxiv.org/abs/2312. 15503 STORM 15 22. Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., Kipf, T.: Object-centric learning with slot attention (2020), https://arxiv.org/abs/2006.15055 23. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization (2019), https: //arxiv.org/abs/1711.05101 24. Manasyan, A., Seitzer, M., Radovic, F., Martius, G., Zadaianchuk, A.: Temporally consistent object-centric learning by contrasting slots (2025), https://arxiv.org/ abs/2412.14295 25. McLean, R., Chatzaroulas, E., McCutcheon, L., Röder, F., Yu, T., He, Z., Zentner, K.R., Julian, R., Terry, J.K., Woungang, I., Farsad, N., Castro, P.S.: Meta-world+: An improved, standardized, rl benchmark (2025), https://arxiv.org/abs/2505. 11289 26. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V., Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., Assran, M., Ballas, N., Galuba, W., Howes, R., Huang, P.Y., Li, S.W., Misra, I., Rabbat, M., Sharma, V., Synnaeve, G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: Dinov2: Learning robust visual features without supervision (2024), https://arxiv.org/ abs/2304.07193 27. Pont-Tuset, J., Arbelaez, P., T.Barron, J., Marques, F., Malik, J.: Multiscale combinatorial grouping for image segmentation and object proposal generation. IEEE Transactions on Pattern Analysis and Machine Intelligence 39(1), 128140 (Jan 2017). https://doi.org/10.1109/tpami.2016.2537320, http://dx.doi.org/10. 1109/TPAMI.2016.2537320 28. Qian, J., Li, Y., Bucher, B., Jayaraman, D.: Task-oriented hierarchical object decomposition for visuomotor control (2024), https://arxiv.org/abs/2411.01284 29. Qian, J., Panagopoulos, A., Jayaraman, D.: Recasting generic pretrained vision transformers as object-centric scene encoders for manipulation policies (2024), https://arxiv.org/abs/2405. 30. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., Sutskever, I.: Learning transferable visual models from natural language supervision (2021), https://arxiv.org/abs/ 2103.00020 31. Seitzer, M., Horn, M., Zadaianchuk, A., Zietlow, D., Xiao, T., Simon-Gabriel, C.J., He, T., Zhang, Z., Schölkopf, B., Brox, T., Locatello, F.: Bridging the gap to realworld object-centric learning (2023), https://arxiv.org/abs/2209.14860 32. Shi, J., Qian, J., Ma, Y.J., Jayaraman, D.: Composing pre-trained object-centric representations for robotics from \"what\" and \"where\" foundation models (2024), https://arxiv.org/abs/2404.13474 33. Singh, G., Deng, F., Ahn, S.: Illiterate dall-e learns to compose (2022), https: //arxiv.org/abs/2110.11405 34. Singh, G., Wu, Y.F., Ahn, S.: Simple unsupervised object-centric learning for complex and naturalistic videos (2022), https://arxiv.org/abs/2205.14065 35. Singh, K., Schaub-Meyer, S., Roth, S.: Glass: Guided latent slot diffusion for objectcentric learning (2025), https://arxiv.org/abs/2407.17929 36. Wen, X., Zhao, B., Chen, Y., Pang, J., Qi, X.: data-centric revisit of pre-trained vision models for robot learning (2025), https://arxiv.org/abs/2503.06960 37. Wu, Z., Hu, J., Lu, W., Gilitschenski, I., Garg, A.: Slotdiffusion: Object-centric generative modeling with diffusion models (2023), https://arxiv.org/abs/2305. 11281 16 A. Chapin et al. 38. Xu, J., Mello, S.D., Liu, S., Byeon, W., Breuel, T., Kautz, J., Wang, X.: Groupvit: Semantic segmentation emerges from text supervision (2022), https://arxiv.org/ abs/2202.11094 39. Zhou, J., Wei, C., Wang, H., Shen, W., Xie, C., Yuille, A., Kong, T.: ibot: Image bert pre-training with online tokenizer (2022), https://arxiv.org/abs/2111. 07832 40. Zhu, Y., Jiang, Z., Stone, P., Zhu, Y.: Learning generalizable manipulation policies with object-centric 3d representations (2023), https://arxiv.org/abs/2310."
        }
    ],
    "affiliations": [
        "Ecole Centrale de Lyon"
    ]
}