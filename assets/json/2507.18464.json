{
    "paper_title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts",
    "authors": [
        "Miguel Aspis",
        "Sebastián A. Cajas Ordónez",
        "Andrés L. Suárez-Cetrulo",
        "Ricardo Simón Carbajo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarse-grained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through a novel co-training framework. DriftMoE features a compact neural router that is co-trained alongside a pool of incremental Hoeffding tree experts. The key innovation lies in a symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using a multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with a clear training signal while accelerating expert specialization. We evaluate DriftMoE's performance across nine state-of-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-of-the-art stream learning adaptive ensembles, offering a principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . s [ 1 4 6 4 8 1 . 7 0 5 2 : r DriftMoE: Mixture of Experts Approach to Handle Concept Drifts Miguel Aspis*1[000900057560513X], Sebastián A. Cajas Ordoñez*1[0000000305796178] ((cid:0)), Andrés L. Suárez-Cetrulo1[0000000152665053], and Ricardo Simón Carbajo1[0000000221212841] Irelands National Centre for Artificial Intelligence (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9 Irelands Centre for Applied AI (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9, Ireland {miguel.aspis1, sebastian.cajasordonez, andres.suarez-cetrulo, ricardo.simoncarbajo}@ucd.ie Abstract. Learning from non-stationary data streams subject to concept drift requires models that can adapt on-the-fly while remaining resource-efficient. Existing adaptive ensemble methods often rely on coarsegrained adaptation mechanisms or simple voting schemes that fail to optimally leverage specialized knowledge. This paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture that addresses these limitations through novel co-training framework. DriftMoE features compact neural router that is co-trained alongside pool of incremental Hoeffding tree experts. The key innovation lies in symbiotic learning loop that enables expert specialization: the router selects the most suitable expert for prediction, the relevant experts update incrementally with the true label, and the router refines its parameters using multi-hot correctness mask that reinforces every accurate expert. This feedback loop provides the router with clear training signal while accelerating expert specialization. We evaluate DriftMoEs performance across nine stateof-the-art data stream learning benchmarks spanning abrupt, gradual, and real-world drifts testing two distinct configurations: one where experts specialize on data regimes (multi-class variant), and another where they focus on single-class specialization (task-based variant). Our results demonstrate that DriftMoE achieves competitive results with state-ofthe-art stream learning adaptive ensembles, offering principled and efficient approach to concept drift adaptation. All code, data pipelines, and reproducibility scripts are available in our public GitHub repository: https://github.com/miguel-ceadar/drift-moe. Keywords: online incremental learning concept drift data streams mixture of experts. * The first two authors have equally contributed to the development of this work. 2 1 Aspis, Cajas Ordoñez, et al."
        },
        {
            "title": "Introduction",
            "content": "Real-world and production data architectures are characterized by the proliferation of high-velocity data streams, generated continuously by an ever-expanding network of sources, including IoT sensors, financial tickers, social media feeds, and network monitoring systems [12]. Unlike static datasets, these streams are inherently non-stationary; their underlying data-generating process can and often does change over time. This phenomenon, known as concept drift [25], invalidates the core assumption of classical machine learning that training and test data are drawn from the same distribution. Consequently, predictive models trained on historical stream data can suffer model decay when the concept they have learned shifts. This challenge is stressed in applications spanning the edge-to-cloud continuum [13], where resource-constrained edge devices must perform real-time inference and adapt to local environmental changes without constant retraining from central cloud, making on-the-fly adaptation critical requirement [7]. Learning in dynamic, streaming environments requires departure from traditional batch-learning paradigms. The field of online incremental learning addresses this by processing data one instance at time, continuously updating the model as new information arrives. However, this introduces challenge known as the stability-plasticity dilemma [20]. model must be plastic enough to quickly adapt to new, evolving concepts but also stable enough to retain previously acquired knowledge that may still be relevant, preventing catastrophic forgetting. Achieving this balance is the central goal of online incremental learning algorithms designed for concept-drifting data streams. To address this challenge, predominant and successful strategy for tackling concept drift is the use of adaptive online ensembles [14, 6]. Drawing inspiration from batch ensemble methods, these approaches maintain collection of diverse base learners, typically incremental decision trees like the Hoeffding Tree [10]. Their strength lies in their modularity and inherent ability to adapt. As surveyed in [24], these ensembles typically employ one of two strategies: active or passive. Active methods explicitly use change detection algorithm (e.g., ADWIN [2]) to signal drift, triggering mechanism to adapt the ensemble, such as resetting the worst-performing learner. Passive methods, on the other hand, adapt continuously through mechanisms like dynamic weighting or sliding window over the data. Models like Adaptive Random Forest (ARF) [14] and Leveraging Bagging [3] have become state-of-the-art benchmarks by demonstrating robust performance across wide variety of drift scenarios. Despite their success, current adaptive ensemble approaches face several limitations [24, 12]. Active methods rely heavily on drift detection algorithms, which can suffer from false positives or delayed detection, leading to suboptimal adaptation timing [2]. When drift is detected, the typical response, such as resetting underperforming learners, is often coarse-grained and reactive, with performance decay until the change is detected [12]. Passive methods typically use simple mechanisms that may not optimally leverage the specialized knowledge of individual learners. Furthermore, most existing approaches lack principled mechanism for experts to develop specialized knowledge for different data regimes or DriftMoE: Mixture of Experts Approach to Handle Concept Drifts concepts, limiting their ability to handle complex, multi-faceted drift scenarios efficiently. To address these limitations, we propose DriftMoE, novel architecture that reframes the adaptation mechanism through the lens of Mixture of Experts (MoE). Instead of relying on explicit drift detectors or simple voting schemes, DriftMoE features lightweight neural network router that is co-trained alongside pool of incremental decision tree experts. The router dynamically assigns incoming data instances to the most suitable experts, while both components adapt continuously through symbiotic online training loop. This integrated approach offers more nuanced method for managing expert contributions and specialized knowledge in non-stationary environments. The principal novelty of our approach lies in its symbiotic, fully online training loop between the router and the experts. We explore two distinct configurations for expert specialization: i) multi-class approach where experts learn different data regimes, and ii) task-based approach where each expert specializes in single class. For each incoming data instance, the router dynamically assigns the instance to the most suitable expert. Once the true label is revealed, the relevant experts (Top-K in the multi-class variant, all in the task variant) update themselves incrementally. The router then refines its own parameters in small online batches using novel training signal: multi-hot correctness mask that positively reinforces every expert that predicted the instance correctly, creating cooperative feedback mechanism. As experts specialize and become more accurate on specific data regimes or tasks, they provide clearer training signal to the router. In turn, as it gets smarter, the router improves at channeling the right data to the right expert, accelerating specialization and improving the overall predictive performance of the model. This tight integration contrasts with traditional ensembles, where adaptation is often more coarse and reactive process. This paper is organized as follows. Section 2 reviews related work on adaptive ensembles and MoE architectures. Section 3 details our approach and online training procedure. Section 4 presents the experimental setup and results. Section 5 discusses key findings, and Section 6 concludes with future directions."
        },
        {
            "title": "2 Related Work",
            "content": "Our research lies at the intersection of two well-established fields: online incremental learning on data streams and MoE architectures. We first review the dominant ensemble-based approaches for handling concept drift and then discuss the MoE paradigm, highlighting the novelty of applying it in fully online, streaming context. 2.1 Adaptive Ensembles for Concept Drift The challenge of learning in non-stationary environments, where the underlying data distribution changes over time, is central focus of data stream learning 4 Aspis, Cajas Ordoñez, et al. research. As reviewed in [24], one of the prevailing strategies for handling concept drift is through the use of online learner ensembles. These methods leverage the diversity and modularity of multiple base learners, typically incremental models like the Hoeffding Tree [10], to create system that is robust to shifts and drifts in the underlying data. The core objective of these ensembles is to resolve the stability-plasticity dilemma [20]: the model must be plastic enough to adapt to new concepts but stable enough to retain knowledge from past, still-relevant concepts. Existing online ensemble methods can be broadly categorized into two groups: Active methods: These approaches incorporate an explicit drift detection mechanism to signal change in the data distribution. Upon detection, corrective action is triggered. state-of-the-art algorithm in this matter is the Adaptive Random Forest (ARF) [14], which keeps each of its base learners with drift detector, such as the ADaptive WINdowing (ADWIN) algorithm [2]. When detector signals \"warning\", it begins training new \"background\" tree. If the detector confirms \"drift,\" the original tree is replaced by its background learner, allowing the ensemble to adapt quickly to abrupt drifts that impact its base learners. While highly effective, this approach can be computationally demanding, often requiring large number of trees to maintain performance and diversity. Passive methods: In contrast, passive methods adapt to drifts continuously without an explicit detection component. These algorithms often rely on dynamic weighting or instance-based adjustments. For example, OzaBag [21] uses online bagging with drift detector to reset the worst-performing learner when change is detected. Other methods, like Online Accuracy Updated Ensemble [6], update learner weights based on their recent performance on blocks of data. Boostingbased approaches [8] modify instance weights to focus on misclassified examples. Methods like Leveraging Bagging [3] focus on increasing diversity by using higher Poisson sampling rate, which has been shown to improve resilience to drift. While these ensemble methods represent the state of the art, their resource consumption can be significant drawback, particularly in resource-constrained environments like IoT gateways or edge devices [7]. Our work proposes an alternative to these large-scale ensembles by drawing inspiration from different architectural paradigm. 2.2 Mixture of Experts in Online Learning The Mixture of Experts (MoE) architecture, first introduced by Jacobs et al. [17], is model based on the divide-and-conquer principle [19]. It consists of two key components: set of \"expert\" networks and \"gating network\" (or router). The gating network learns to partition the input space, dynamically routing each input instance to the expert best suited to handle it. The final prediction is typically weighted sum of the outputs from all experts, with weights determined by the gating network. The primary advantage of MoE is conditional computation; by activating only sparse subset of experts for any DriftMoE: Mixture of Experts Approach to Handle Concept Drifts 5 given input, the models capacity can be vastly increased without proportional rise in computational cost. Traditional MoEs: Traditionally, MoEs have been applied in batch learning settings to model complex functions or multi-modal data. More recently, they have seen resurgence in large-scale deep learning use cases [26], [11], [9]. These approaches typically focus on scaling model capacity for static datasets, where the primary challenge is handling complex, high-dimensional data rather than temporal distribution shifts. Their application to online or continual learning is less common but holds significant promise. In continual learning, MoEs have been explored as way to mitigate catastrophic forgetting by allocating new tasks to new experts, thereby protecting previously acquired knowledge [18]. MoEs in Continual/Streaming Learning: MoE architectures have shown promise in continual learning by assigning new tasks to separate experts, helping prevent forgetting [18]. However, these methods often assume task boundaries are known, which contrasts with streaming settings where concept drift is gradual and undefined. While recent work, such as Yang et al. [27], has applied MoEs to handle drift in specific domains, most approaches still rely on explicit drift detection or are domain-specific, limiting their adaptability to general streaming scenarios. DriftMoE advances this direction by introducing general-purpose, online co-training framework. Instead of static drift detectors or majority voting, continuously trained neural router dynamically assigns data to incremental experts, enabling proactive, efficient adaptation in non-stationary environments."
        },
        {
            "title": "3 DriftMoE: Architecture and Training Procedure",
            "content": "3.1 Model Architecture The DriftMoE model implements two-component mixture-of-experts (MoE) architecture [22], composed of set of streaming experts and lightweight neuralnetwork router. Let = {Ei}K denote the expert ensemble and Rθ the router i=1 parameterized by θ. The router produces expert weights while each expert maintains its own incremental concept model. DriftMoE has two different variants, MoE-Data and MoE-Task, which differ only in the experts used, MoE-Data uses multi-class data experts updated in Top-K fashion and MoE-Task uses one-vs-rest binary experts updated every step. Both variants are trained online in hybrid regime: experts update instance-byinstance, while the router updates in mini-batches. Experts. Each expert Ei is instantiated as Hoeffding tree [10], producing incremental updates on each observed instance. Two variants of expert configuration are supported: 1. Data-mode, Multiclass Experts: set of Hoeffding trees, each issuing C-way classification over the label set. 6 Aspis, Cajas Ordoñez, et al. 2. Task-mode experts: One Hoeffding tree per class (K = C), each formulated as binary classifier for its associated class. Given an input xt, expert Ei outputs pi(xt) = (cid:2)p(1) , . . . , p(C) (cid:3) , p(c) = (y = xt, Ei). In the task-mode variant, each expert reports scalar pi(xt) = (y = xt, Ei). Router. The router Rθ is three-layer multilayer perceptron (MLP) that maps xt Rd to expert logits ot = Rθ(xt) RK. softmax normalization yields gating weights wt,i = exp(ot,i) j=1 exp(ot,j) (cid:80)K , wt = softmax(ot). At inference time, the router may either (i) mix expert outputs via wt, or (ii) select the top-k experts by largest wt,i. 3.2 Joint Online Training Expert Updates. Multiclass mode (MoE-Data): For each incoming pair (xt, yt), identify the top-k experts under wt, and update each selected Ei using its Hoeffding tree rule: Ei.train(xt, yt). Task mode (MoE-Task): Define binary label yt,i = 1[yt = i] for each expert. Then update every Ei on (xt, yt,i). Router Updates. 1. Correctness Mask. For each expert Ei, let ˆyt,i = arg max p(c) (xt), mt,i = (cid:40) 1, ˆyt,i = yt, 0, otherwise. If (cid:80) mt,i = 0, set mt,yt = 1 to guarantee at least one positive target. 2. Mini-Batch Optimization. Collect logits {on}B and masks {mn}B n=1 n= into batch of size B. Define the binary cross-entropy loss LBCE = 1 (cid:88) (cid:88) n= i=1 mn,i log σ(on,i) + (1 mn,i) log(cid:0)1 σ(on,i)(cid:1)(cid:105) (cid:104) , where σ() denotes the sigmoid function applied to the router logits. Update θ via Adam with learning rate η: θ θ η θLBCE. DriftMoE: Mixture of Experts Approach to Handle Concept Drifts"
        },
        {
            "title": "3.3 Evaluation Metrics",
            "content": "At each time step t, the system prediction ˆyt is obtained from the single expert Ei with the highest router weight: = arg max wt,i, ˆyt = arg max p(c) (xt). Performance is assessed in streaming (prequential) evaluation loop by comparing ˆyt with the true label yt. We report accuracy, kappa and kappa temporal metrics. These metrics reflect the expert selected as the most reliable by the router in each instance."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup We used six synthetic concepts, drift streams, and three real-world datasets that are popular in relevant data stream learning research [14, 2, 3] offering mix of abrupt and gradual drifts to showcase our models performance in different scenarios LED streams [5] comprise 24 binary attributes of which 7 are informative and 17 are irrelevant. Drift is introduced at three points, at 250k instances, 500k instances, and 750k instances, respectively, drifting 3, 5, and 7 features. We use two variants: one with abrupt changes, where the transition width between regimes is 50 instances, and one with gradual changes, whose transition width is 50,000 instances. SEA concepts are generated via the SEA generator [23], which produces data streams with three continuous attributes, each ranging between 0 and 10. Only two determine class membership by following the following procedure: instances are uniformly sampled in the plane created by these and classified by comparing their sum against block-specific threshold. controlled 10 percent label noise may be injected, and class balance is enforced. We simulate three drifts between different thresholds to create an abrupt and gradual version. RBF streams use the radial basis function generator, which places centroids at random positions, each associated with standard deviation, weight, and class label. Instances are created by sampling centroid (proportional to its weight) and offsetting it by Gaussian perturbation. We used 50 centroids, and to model incremental drift, all centroids continuously move: in RBFm (moderate drift), the speed is set to 0.0001, while in RBFf (fast drift) it is set to 0.001. Airlines show records of flight departure delays, and the objective is to predict whether flight will be delayed or not, given seven mixed-type attributes and naturally occurring, season-dependent drift. The Electricity dataset from the Australian New South Wales Electricity Market offers 5-minute updates to wholesale price snapshots described by 8 attributes; market dynamics such as supply and demand introduce drift, and the goal is to predict whether the price will go up or down. 8 Aspis, Cajas Ordoñez, et al. CoverType comprises remote-sensing measurements with 54 attributes and 7 forest classes, collected over the years and therefore affected by long-term distributional shifts. All synthetic streams are produced with CapyMOA 1.3 [15], an extensible and efficient Python library that allows an integrated interface with external frameworks such as MOA [4] and Pytorch, and were generated by executing the exact command lines reported by Adaptive Random Forest (ARF) [14]; this yields identical drift positions and allows for direct comparison between performance metrics. Table 1: Datasets. Category Stream Instances Features Classes Synthetic LED (Abrupt) 1,000,000 LED (Gradual) 1,000,000 SEA (Abrupt) 1,000,000 SEA (Gradual) 1,000,000 RBFm 1,000,000 RBFf 1,000,000 539,383 45,312 581,012 Airlines Electricty Cover Type Real 24 24 3 3 10 10 7 8 10 10 2 2 5 5 2 2 7 These experiments have been run on server with 32-core AMD Ryzen Threadripper PRO 5975WX, 256 GB of RAM, and 2 NVIDIA GeForce RTX 4090 GPUs. Each individual expert in our MoE framework is implemented as Hoeffding Tree with fixed grace period of 50, meaning each leaf must observe 50 instances before evaluating potential splits. We use the variation with Naive Bayes at each leaf. We do not impose an explicit parameter to limit the number of splits; instead, split decisions are governed purely by the Hoeffding bound with our chosen confidence and grace period settings. Preliminary sweeps on the LED stream (Figure 3) reveal broad accuracy plateau for 12 20 experts and 3 5 for the multi-class mode. To keep runtime low and avoid per-dataset tuning we fix = 12 experts and = 3 for all datasets. This choice retains at least 97 % of the peak accuracy while reducing compute (by comparison with the 20-expert model) by roughly 30 %. All experts are initialised as identical, empty trees. We employ the interleaved test-then-train procedure [10]. And each stream is processed ten times with independent pseudo-random seeds; results are reported as mean standard deviation. The same protocol is applied to both expert configurations: multi-class experts and task-specific (one-versus-rest) experts to guarantee fair and consistent comparison across modes. DriftMoE: Mixture of Experts Approach to Handle Concept Drifts 9 We adopt the single-model prequential protocol because it mirrors real-time deployment: the learner must predict each instance before observing its label and adapt continuously through drift, yielding conservative, deployment-oriented performance estimate. Beyond prequential accuracy, we measure Kappa-M, which corrects accuracy for chance agreement under class imbalance and drift [1], and Kappa-Temporal, which discounts autocorrelation effects [28] We compare our method against five widely-used adaptive stream ensembles, re-using the hyperparameters presented in their original studies, and they are assessed using the same prequential protocol as both DriftMoE variants. Adaptive Random Forest (ARF) [14]: 100 Hoeffding-Tree base learners, each equipped with an ADWIN warning & drift detector and background tree; majority vote with tree-level weighting. OzaBag [21]: online bagging of 10 Hoeffding Trees using Poisson(1) resampling; when ADWIN signals drift, the worst tree is reset. OzaBoost [21] online boosting of 10 Hoeffding Trees using Poisson-weighted instance re-sampling. Online Smooth Boosting (SmoothBoost) [8]: stage-wise boosting of 30 Hoeffding Trees with smooth instance weighting that caps the influence of any single example and restarts learners when drift is detected. Leveraging Bagging (LevBag) [3]: bagging with amplified diversity via Poisson(λ = 6) re-sampling and output perturbation; the ARF benchmarks use an ensemble of 15 trees Streaming Random Patches (SRP) [16] keeps an ensemble of incremental Hoeffding Trees, each grown on Poisson-resampled stream and fixed, globally random subset of features. Every tree monitors its error with ADWIN; when drift is signalled the worst performer is discarded and restarted with fresh feature patch. These heavyweight baselines give the forthcoming results extra context: our router-based MoE matches or surpasses them while using far fewer trees. 4.2 Experimental Results Table 2 reports average accuracy over ten independent runs. MoE-Task ranks third on RBFm and fourth on RBFf , confirming its aptitude for rapid highfrequency drift, whereas MoE-Data performs best on AIRL and remains consistenyl competitive on all LED and SEA variants. MoE-Data finishes last only on the on COVT yet still secures podium position on AIRL, LED and SEA streams, illustrating the best overall stability trade-off. In contrast, MoE-Task collapses on class-imbalanced streams such as ELEC and COVT, suggesting over-specialisation of its one-vs-rest experts. Figure 1 displays these accuracy trends. During experiment runs we observed that the router reacts to concept shifts with surprisingly little latency. Figure 2 illustrates this for the LEDg stream: 10 Aspis, Cajas Ordoñez, et al. after each scheduled drift, accuracy for the MoE-Data rebounds at essentially the same recovery speed achieved by the much larger ADWIN-equipped ensembles (ARF and SRP), suggesting that the MoE design can match state-of-the-art drift-reaction speed while relying on an order-of-magnitude fewer base learners. Fig. 1: Prequential accuracy (%) of baseline learners and DriftMoE variants across the nine benchmark datasets Table 2: Accuracybaselines and DriftMoE variations Dataset ARF LevBag OzaBag OzaBoost SmoothBoost SRP MoEData MoETask Airlines CovType Electricity 90.08 0.04 89.16 0.07 82.63 0.09 86.30 0.40 87.51 0.00 LEDa LEDg RBFf RBFm SEAa SEAg 64.51 0.02 63.50 0.03 65.08 0.02 63.45 0.06 65.74 0.00 68.55 0.04 70.33 0.18 60.92 0.01 94.78 0.02 93.31 0.02 84.41 0.09 90.19 0.23 86.95 0.00 95.27 0.01 81.28 0.75 58.28 0.31 83.76 0.45 68.73 0.85 73.96 0.08 73.82 0.08 69.22 0.14 69.35 0.34 72.88 0.15 73.98 0.08 73.77 0.18 71.11 0.54 73.04 0.07 73.15 0.08 69.21 0.11 69.45 0.19 72.44 0.06 73.18 0.07 73.11 0.11 70.82 0.38 61.90 0.20 75.45 0.11 79.89 0.48 88.65 0.07 89.09 0.05 88.04 0.09 88.74 0.05 87.76 0.04 86.37 1.30 84.47 1.63 66.46 1.89 67.95 2.18 74.84 1.48 92.04 1.39 90.99 1.37 82.73 1.13 84.29 1.72 88.31 1.24 89.68 0.04 88.17 0.28 86.70 0.06 87.81 0.11 88.95 0.07 89.33 0.07 88.77 0.08 86.69 0.08 87.86 0.09 88.79 0.04 82.66 1.53 90.55 1.23 87.39 0.32 86.98 0.64 89.64 0.12 Kappa  (Table 3)  and Kappa temporal  (Table 4)  scores broadly reflect the accuracy findings. Across all streams the kappa metrics mirror the complementary strengths of the two variants while exposing their shared weakness on class-imbalanced data. noteworthy detail is that, on all of the LED and SEA variants, MoE-Data exhibits the joint smallest gap between accuracy and Kappa M, implying that the multi-hot router is indeed capable of steering experts towards informative regions of the feature space. Finally, the findings can be summarised in two points: (1) joint router-expert training captures both sharp drift and regime shifts, and (2) both MoE variants remain sensitive to pronounced class imbalance, pointing the need for costsensitive losses or adaptive sampling in the future. DriftMoE: Mixture of Experts Approach to Handle Concept Drifts 11 Table 3: Kappa Mbaselines and DriftMoE variations Dataset ARF LevBag OzaBag OzaBoost SmoothBoost SRP MoEData MoETask 76.63 0.09 74.46 0.17 59.07 0.22 67.71 0.94 70.57 0. airl covt elec led_a led_g rbf_f 69.99 3.38 64.18 6.10 20.76 12.78 20.83 8.22 43.34 5.66 rbf_m 81.43 3.75 79.72 3.34 55.80 8.32 65.25 4.47 72.42 4.34 sea_a 74.17 0.09 70.35 0.67 66.68 0.20 69.49 0.30 72.32 0.20 sea_g 73.26 0.16 71.86 0.19 66.65 0.20 69.59 0.22 71.89 0.12 20.33 0.05 18.05 0.06 21.60 0.04 17.95 0.14 23.09 0.00 29.40 0.10 33.39 0.42 12.27 0.02 89.82 0.04 86.94 0.05 69.58 0.17 80.86 0.44 74.54 0.00 90.78 0.03 63.46 1.47 18.58 0.60 61.74 1.05 26.33 2.00 71.02 0.09 70.86 0.09 65.76 0.16 65.89 0.38 69.83 0.17 71.04 0.09 70.81 0.20 67.84 0.60 70.00 0.08 70.13 0.09 65.73 0.12 66.01 0.22 69.33 0.06 70.16 0.08 70.08 0.13 67.53 0.42 23.51 0.40 50.71 0.22 59.64 0.96 77.22 0.14 72.79 0.12 70.18 0.22 71.90 0.13 69.47 0.10 61.30 7.90 78.07 4.27 68.41 0.77 67.37 1.60 75.59 0.28 Table 4: Kappa Mbaselines and DriftMoE variations Dataset ARF LevBag OzaBag OzaBoost SmoothBoost SRP MoEData MoETask 16.75 0.04 12.88 0.15 32.37 0.27 26.09 0.50 -18.43 0.63 airl covt elec 71.06 0.08 70.91 0.09 led_a 70.04 0.08 70.17 0.09 led_g rbf_f 72.29 2.43 68.23 3.72 rbf_m 83.64 2.81 81.68 2.76 sea_a 78.32 0.08 75.12 0.54 sea_g 77.58 0.14 76.42 0.14 15.40 0.06 12.98 0.06 6.84 0.02 -5.62 0.41 -35.44 0.48 -215.57 1.74 -98.56 4.58 -164.15 0.00 4.31 0.30 -279.11 15.28 -744.69 6.24 -10.70 3.04 -113.17 5.80 67.88 0.60 70.85 0.19 67.56 0.42 70.11 0.13 50.95 0.22 23.87 0.40 77.33 0.13 59.83 0.95 74.92 0.19 77.12 0.10 74.33 0.08 76.38 0.11 29.37 0.81 14.85 0.00 69.87 0.16 71.08 0.09 69.38 0.06 70.20 0.08 64.64 4.01 48.88 3.29 80.60 2.66 75.97 2.71 73.46 0.63 76.76 0.17 72.62 1.34 76.46 0. 6.58 2.73 65.80 0.16 65.94 0.37 65.79 0.12 66.05 0.21 30.54 4.33 32.83 2.12 63.39 3.66 68.23 3.53 72.06 0.15 74.39 0.30 72.03 0.16 74.49 0.18 25.03 0.10 29.26 0.44 18.34 0.00 Fig. 2: Accuracy over time plot for LEDg dataset"
        },
        {
            "title": "5 Discussion",
            "content": "Across all three metrics, accuracy, Kappam, and Kappat DriftMoE is competitive with state-of-the-art ensembles while using far fewer trees. It tops the Airlines stream and stays within 2 pp of the leader on both RBF streams, streams characterised by high-frequency drift. MoETask shows the highest reactivity, excelling 12 Aspis, Cajas Ordoñez, et al. in volatile conditions, while MoEData consistently performs near the top and emerges as the most robust across datasets. However, both variants show limitations under class imbalanced settings such as ELEC and COVT. These findings underscore three key takeaways: (1) adaptive synergy between router and experts enables efficient adaptation in dynamic environments, (2) sensitivity to imbalance remains challenge for current router training strategies, and (3) variant selection matters where MoEData offers better stability and MoETask suits fast drift balanced scenarios. In summary, DriftMoE can match or outperform established ensembles under concept drift with far fewer resources, making it promising alternative for scalable online learning, though future work should improve its handling of imbalance. To our knowledge, this work represents one of the first implementations of Mixture of Experts applied directly data streams and online learning environments. We believe this is promising direction with many unexplored design choices. key area for improvement lies in enhancing the quality of the experts, particularly under more challenging and nonstationary data. We believe future work can benefit from more principled regime detection and dynamic expert allocation strategies, potentially improving accuracy and robustness across tasks. Exploring uncertainty-based routing, or drift-aware expert adaptation could further strengthen DriftMoEs applicability across domains. Fig. 3: Grid search on the LED stream showing prequential accuracy as function of the number of experts and the gating parameter Top-K. The plateau at (N =12, Top-K=3) motivates our fixed configuration used in 4. DriftMoE: Mixture of Experts Approach to Handle Concept Drifts"
        },
        {
            "title": "6 Conclusion",
            "content": "This work introduced DriftMoE, fully streaming Mixture of Experts framework designed to handle concept drift through online co-training of neural router and pool of incremental experts. We evaluated two variants: multi-class and task-specific, and showed that DriftMoE achieves competitive or superior performance to established ensemble baselines across synthetic and real-world streams, particularly in highly dynamic settings. The results demonstrate that DriftMoE maintains high adaptability with fewer learners, offering more efficient alternative to large ensemble models like ARF. However, performance on class-imbalanced datasets highlights the need for better router calibration and expert assignment under skewed distributions. This opens several avenues for future work, including improving expert quality under harsh drift, enhancing the clustering of data regimes, and exploring more adaptive routing strategies. Overall, DriftMoE represents flexible and efficient framework for streaming learning under nonstationarity, with strong potential for further improvement and extension. Acknowledgments. This work was partially supported by the project Towards functional continuum operating system (ICOS) funded by the European Commission under Project code/Grant Number 101070177 through the HORIZON EU program. This work was partially supported by the project Open CloudEdgeIoT Platform Uptake in Large Scale Cross-Domain Pilots (O-CEI) funded by the European Commission under Project code/Grant Number 101189589 through the HORIZON EU program. Ricardo Simón Carbajo is the Director of Innovation and Development in Irelands Centre for AI (CeADAR) and has been partially supported by the project Trustworthy Efficient AI for Cloud-Edge Computing (MANOLO) funded by the European Commission under Project code/Grant Number 101135782 through the HORIZON EU program. Disclosure of Interests. The authors have no competing interests to declare that are relevant to the content of this article."
        },
        {
            "title": "Bibliography",
            "content": "[1] Bifet, A., de Francisci Morales, G., Read, J., Holmes, G., Pfahringer, B.: Efficient online evaluation of big data stream classifiers. In: Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. pp. 5968 (2015) [2] Bifet, A., Gavalda, R.: Learning from time-changing data with adaptive windowing. In: Proceedings of the 2007 SIAM international conference on data mining. vol. 7, pp. 443448. SIAM (2007) [3] Bifet, A., Holmes, G., Pfahringer, B.: Leveraging bagging for evolving data streams. In: Joint European conference on machine learning and knowledge discovery in databases. pp. 135150. Springer (2010) [4] Bifet, A., Holmes, G., Pfahringer, B., Kranen, P., Kremer, H., Jansen, T., Seidl, T.: MOA: Massive Online Analysis, framework for stream classification and clustering. Journal of Machine Learning Research - Proceedings Track 11, 4450 (2010) [5] Breiman, L., Friedman, Classification And https://doi.org/10.1201/9781315139470 Regression J.H., Olshen, Trees. R.A., Stone, Routledge C.J.: (1984). [6] Brzezinski, D., Stefanowski, J.: Combining block-based and online methods in learning ensembles from concept drifting data streams. Information Sciences 265, 5067 (May 2014). https://doi.org/10.1016/j.ins.2013.12.011 [7] Cajas, S.A., Samanta, J., Suárez-Cetrulo, A.L., Simón Carbajo, R.: Adaptive machine learning for resource-constrained environments. In: Piangerelli, M., Prenkaj, B., Rotalinti, Y., Joshi, A., Stilo, G. (eds.) Discovering Drift Phenomena in Evolving Landscapes. pp. 319. Springer Nature Switzerland, Cham (2025). https://doi.org/10.1007/978-3-031-82346-6_1 [8] Chen, S.T., Lin, H.T., Lu, C.J.: An online boosting algorithm with theoretical justifications (2012). https://doi.org/10.48550/ARXIV.1206.6422 [9] Dai, D., Deng, C., Zhao, C., Xu, R.X., Gao, H., Chen, D., Li, J., Zeng, W., Yu, X., Wu, Y., Xie, Z., Li, Y.K., Huang, P., Luo, F., Ruan, C., Sui, Z., Liang, W.: Deepseekmoe: Towards ultimate language modspecialization in mixture-of-experts els (2024). https://doi.org/10.48550/ARXIV.2401.06066, https://arxiv. org/abs/2401.06066 expert [10] Domingos, P., Hulten, G.: Mining high-speed data streams. In: Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 7180 (2000) [11] Fedus, W., Zoph, B., Shazeer, N.: Switch transformers: Scalsparhttps: ing to trillion parameter models with simple and efficient sity //arxiv.org/abs/2101.03961 https://doi.org/10.48550/ARXIV.2101.03961, (2021). DriftMoE: Mixture of Experts Approach to Handle Concept Drifts 15 [12] Gama, J., Žliobaite, I., Bifet, A., Pechenizkiy, M., Bouchachia, A.: survey on concept drift adaptation. ACM computing surveys (CSUR) 46(4), 137 (2014) [13] Garcia, J., Masip-Bruin, X., Giannopoulos, A., Trakadas, P., Cajas, S.A., Samanta, J., Suárez-Cetrulo, A.L., Simón Carbajo, R., Michalke, M., Jukan, A., Jaworski, A., Kotliński, M., Giammatteo, G., DAndria, F.: Icos: An intelligent metaos for the continuum. In: Proceedings of the 2nd International Workshop on MetaOS for the Cloud-Edge-IoT Continuum. p. 5359. MECC 25, Association for Computing Machinery, New York, NY, USA (2025) [14] Gomes, H.M., Bifet, A., Read, J., Barddal, J.P., Enembreck, F., Pfharinger, B., Holmes, G., Abdessalem, T.: Adaptive random forests for evolving data stream classification. Machine Learning pp. 127 (6 2017) [15] Gomes, H.M., Lee, A., Gunasekara, N., Sun, Y., Cassales, G.W., Liu, J., Heyden, M., Cerqueira, V., Bahri, M., Koh, Y.S., Pfahringer, B., Bifet, A.: CapyMOA: Efficient machine learning for data streams in Python (2025). https://doi.org/10.48550/ARXIV.2502.07432 [16] Gomes, H.M., Read, J., Bifet, A.: Streaming random patches for evolving data stream classification. In: 2019 IEEE International Conference on Data Mining (ICDM). pp. 240249. IEEE (2019) [17] Jacobs, R.A., Jordan, M.I., Nowlan, S.J., Hinton, G.E.: Adaptive mixtures of local experts. Neural computation 3(1), 7987 (1991) [18] Li, H., Lin, S., Duan, L., Liang, Y., Shroff, N.B.: Theory on mixture-ofexperts in continual learning. arXiv preprint arXiv:2406.16437 (2024) [19] Liu, B., Ding, L., Shen, L., Peng, K., Cao, Y., Cheng, D., Tao, D.: Diversifying the mixture-of-experts representation for language models with orthogonal optimizer. arXiv preprint arXiv:2310.09762 (2023) [20] Mermillod, M., Bugaiska, A., Bonin, P.: The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects. Frontiers in psychology 4, 504 (2013) [21] Oza, N.C., Russell, S.J.: Online bagging and boosting. In: International Workshop on Artificial Intelligence and Statistics. pp. 229236. PMLR (2001) [22] Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., Dean, J.: Outrageously large neural networks: The sparsely-gated mixtureof-experts layer. arXiv preprint arXiv:1701.06538 (2017) [23] Street, W.N., Kim, Y.: streaming ensemble algorithm (SEA) for largescale classification. In: Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 377382. ACM (2001) [24] Suárez-Cetrulo, A.L., Quintana, D., Cervantes, A.: survey on machine learning for recurring concept drifting data streams. Expert Systems with Applications 213, 118934 (2023) [25] Tsymbal, A.: The Problem of Concept Drift: Definitions and Related Work. Technical Report: TCD-CS-2004-15, Department of Computer Science Trinity College, Dublin (2004) 16 Aspis, Cajas Ordoñez, et al. [26] Xue, F., Zheng, Z., Fu, Y., Ni, J., Zheng, Z., Zhou, W., You, Y.: Openmoe: An early effort on open mixture-of-experts language models. arXiv preprint arXiv:2402.01739 (2024) [27] Yang, G., Mou, W., Xia, T., Fan, L.: MEDA: MoE-based Concept Drift Adaptation for In-vehicle Network Intrusion Detection. In: 2024 IEEE 24th International Conference on Communication Technology (ICCT). pp. 634 638 (2024). https://doi.org/10.1109/ICCT62411.2024.10946581 [28] Žliobaite, I., Bifet, A., Read, J., Pfahringer, B., Holmes, G.: Evaluation methods and decision theory for classification of streaming data with temporal dependence. Machine Learning 98(3), 455482 (Apr 2014). https://doi.org/10.1007/s10994-014-5441-"
        }
    ],
    "affiliations": [
        "Irelands Centre for Applied AI (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9, Ireland",
        "Irelands National Centre for Artificial Intelligence (CeADAR) University College Dublin, Belfield, Dublin, D04 V2N9"
    ]
}