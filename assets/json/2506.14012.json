{
    "paper_title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
    "authors": [
        "Amr Mohamed",
        "Yang Zhang",
        "Michalis Vazirgiannis",
        "Guokan Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code-switching (CSW) is the act of alternating between two or more languages within a single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As a result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents a systematic evaluation of LLM comprehension under code-switching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English text$\\unicode{x2013}$even under linguistic constraints$\\unicode{x2013}$embedding English into other languages often improves comprehension. Though prompting yields mixed results, fine-tuning offers a more stable path to degradation mitigation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 2 1 0 4 1 . 6 0 5 2 : r Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text Amr Mohamed1, Yang Zhang2, Michalis Vazirgiannis1,2, Guokan Shang1 1MBZUAI, 2Ecole Polytechnique Correspondence: {amr.mohamed, guokan.shang}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "Code-switching (CSW) is the act of alternating between two or more languages within single discourse. This phenomenon is widespread in multilingual communities, and increasingly prevalent in online content, where users naturally mix languages in everyday communication. As result, Large Language Models (LLMs), now central to content processing and generation, are frequently exposed to code-switched inputs. Given their widespread use, it is crucial to understand how LLMs process and reason about such mixed-language text. This paper presents systematic evaluation of LLM comprehension under codeswitching by generating CSW variants of established reasoning and comprehension benchmarks. While degradation is evident when foreign tokens disrupt English texteven under linguistic constraintsembedding English into other languages often improves comprehension. Though prompting yields mixed results, finetuning offers more stable path to degradation mitigation."
        },
        {
            "title": "Introduction",
            "content": "Code-switching (CSW)the act of alternating between two or more languages within single discourse (Das et al., 2023; Zhang et al., 2023; Ochieng et al., 2024)is common phenomenon in multilingual communities (Bullock and Toribio, 2009; Parekh et al., 2020; Dogruöz et al., 2021), and increasingly prevalent in online content (Kodali et al., 2024), where users naturally mix languages in everyday informal communications. Large Language Models (LLMs) have demonstrated remarkable capabilities across wide range of natural language processing tasks (Zhao et al., 2023). As they are increasingly used to process and generate content, the widespread availability of code-switched inputs makes it crucial to understand how LLMs reason about such mixed-language data, 1 and whether their multilingual fluency reflects genuine understanding or superficial pattern matching (Zhang et al., 2023). To systematically assess LLMs handling of such data, we turn to insights from linguistic theories that define the structural constraints governing natural CSW. Linguistic theories have long studied the structure of CSW text, proposing formal constraints on permissible switch points, such as the Equivalence Constraint Theory (ECT), which posits that switches occur at positions where the surface structures of both languages are grammatically compatible (Poplack, 1978), and the Matrix Language Frame model (MLF), which distinguishes between Matrix Language (ML) that provides the grammatical frame of the clause and an Embedded Language (EL) that contributes inserted content without disrupting this structure (Myers-Scotton, 1993). These frameworks aim to identify the grammatical boundaries and syntactic compatibility that make CSW possible and natural. While such theories offer testable hypotheses for analyzing CSW, current efforts in synthetic CSW generation often prioritize producing fluent mixed-language text over probing whether LLMs genuinely internalize and apply these structural constraints in their reasoning (Pratapa et al., 2018; Potter and Yuan, 2024; Kuwanto et al., 2024; Heredia et al., 2025). Despite the availability of well-established linguistic theories, existing evaluation benchmarks fall short of leveraging these insights to assess deeper comprehension in code-switched contexts. Current benchmarks for evaluating the CSW capabilities of language models primarily focus on surface-level tasks (Khanuja et al., 2020; Aguilar et al., 2020; Patwa et al., 2020). However, they largely overlook the challenge of evaluating deeper reasoning and semantic understanding in mixedlanguage settings (Yadav et al., 2024; Gupta et al., 2024; Ng and Chan, 2024), leaving critical gap in assessing the true extent of LLMs code-switched Figure 1: An example illustrating the noun-token CSW methodology from Experiment 1. The figure demonstrates how different embedded languages (Arabic, French, German, Chinese) for the noun beauty in an English matrix sentence can lead to varied model outputs. comprehension abilities. To address these gaps, we introduce systematic evaluation framework that leverages constrained, multi-step LLM pipeline to generate linguistically grounded code-switched variants of established benchmarks in reading comprehension, multi-domain knowledge, and natural language inference. Code and data are publicly available1. Our experiments reveal that code-switching has nuanced impact on LLM comprehension, influenced by the languages involved and the switching style, as illustrated by the example in Figure 1. In particular: Embedding non-English tokens into an English matrix language consistently degrades performance, even when the switches follow linguistic constraints, suggesting structural vulnerability that cannot be explained solely by token-level unfamiliarity. Embedding English tokens into non-English matrix languages often improves comprehension, especially for models with limited proficiency in the matrix language, indicating facilitative role for English in such contexts. While strategic prompting can help some models, it negatively affects others, highlighting inconsistency in controllability; by contrast, fine-tuning on code-switched data leads to more stable, albeit partial, performance recovery. Our work advances the ongoing debate over how LLMs process the mixed-language content that now permeates social media, messaging apps, and other corners of the web. We show that models falter when non-English tokens disrupt an English 1https://github.com/amr-mohamedd/Lost-in-the-Mix.git sentence, yet paradoxically grow more confident when English words are embedded in other languages. This asymmetric behavior reveals structural imbalance and raises broader concerns about linguistic equity as LLM-generated text is recycled, re-posted, and ultimately re-learned by future models."
        },
        {
            "title": "2 Related Work",
            "content": "Code-Switching in Language Models. Early multilingual encoder-based models (e.g., mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020)), while effective on monolingual tasks, consistently faltered on code-switched inputs (Winata et al., 2021a). This gap spurred specialized methods for mixed-language text, including new architectures and training regimes (Winata et al., 2019; Liu et al., 2020; Winata et al., 2021b). Although existing benchmarks (Khanuja et al., 2020) supported these efforts, research predominantly focused on encoder-centric models (Winata et al., 2019; Tan and Joty, 2021; Zhu et al., 2023). Consequently, decoder-only architectures, now central to state-ofthe-art NLP, have received markedly less scrutiny regarding CSW. While some studies probed adversarial code-mixing in autoregressive models (Das et al., 2022), meaningful evaluation of such models requires access to high-quality, linguistically coherent code-switched text. This has motivated growing interest in controlled CSW text generation. Code-Switched Text Generation. Synthetic code-switched text generation plays critical role in data augmentation and diversification for multilingual language models (Pratapa et al., 2018; Zhang et al., 2023). Methods range from linguis2 tically motivated approachessuch as the Equivalence Constraint Theory (ECT) (Poplack, 1978) and Matrix Language Frame (MLF) model (MyersScotton, 1993)to heuristic token-level substitutions (Myslín, 2014; and, 2018; Chan et al., 2024). Recent work often relies on word-level aligners to guide borrowing from embedded-language texts while preserving grammatical structure (Kuwanto et al., 2024). Although these techniques aim for token-level accuracy, they overlook the growing capacity of LLMs to perform context-aware, linguistically grounded substitutions. Leveraging this potential, recent studies have explored LLM-based generation using linguistic constraints (Kuwanto et al., 2024), fine-tuning on CSW data (Heredia et al., 2025), or zero-shot prompting (Potter and Yuan, 2024). Still, challenges remain in controlling switch placement, scaling across language pairs, and conducting robust evaluation. Our work addresses these challenges by leveraging modern LLMs to generate linguistically grounded codeswitched text, grounded in established theoretical constraints, to support more rigorous evaluation of model comprehension in mixed-language contexts. Evaluation of LLM CSW Capabilities. LLM CSW evaluation has largely focused on surfacelevel tasks through benchmarks like GLUECoS (Khanuja et al., 2020), LINCE (Aguilar et al., 2020), and SemEval (Patwa et al., 2020) (e.g., language ID, sentiment, PoS tagging), thus neglecting deeper semantic or reasoning capabilities. Although more recent studies assess CSW sentiment classification (Winata et al., 2021a), and question answering (Huzaifah et al., 2024), they are limited in scope, emphasizing task-specific metrics over broader comprehension. In contrast, our approach introduces linguistically grounded CSW variants of established comprehension and reasoning tasks, enabling more rigorous assessment of LLMs capacity to reason over mixed-language input beyond surface-level performance."
        },
        {
            "title": "3.1 Notations",
            "content": "B = {Bp}P p=1 be set of standard benchmarks. Let = {lj}L j=1 be set of languages from which the matrix and embedded languages are selected for codeswitched benchmarks generation. Let = {mk}K k=1 be set of LLMs. To evaluate the performance of an LLM mk on code-switched text comprehension, we generate code-switched version of benchmark Bp using single matrix language lmatrix and set of embedded languages Lembedded, where Lembedded lmatrix and Lembedded 1, which we denote by BlmatrixLembedded . 3.2 CSW Methods To investigate how different CSW strategies affect LLM comprehension, we generate inputs using two distinct approaches: linguistically grounded nountoken method (Poplack, 1988; Muysken, 2000; Moyer, 2002; Chan et al., 2024) and heuristic ratio-token method (Chan et al., 2024). In the noun-token method, we replace nouns in the matrix language text with their aligned counterparts from parallel sentence in the embedded language. Substitutions are only applied when they preserve grammatical well-formedness according to the Equivalence Constraint Theory and the Matrix Language Frame model, which mandates that the matrix language maintains control over the clauses morpho-syntactic structure. In contrast, the ratio-token method replaces ratio of tokens at random, regardless of linguistic structure. This comparison allows us to isolate the role of syntactic and grammatical constraints in LLM comprehension of code-switched text."
        },
        {
            "title": "Approaches",
            "content": "Given parallel corpus, we create code-switched sentences by swapping embeddedlanguage words into matrixlanguage sentences. To this end, we evaluated two distinct methods for code-switched text generation: an alignment-based method and an LLM-centric method. Alignment-based method. We first align the matrixand embedded-language sentences with the AWESOME aligner (Dou and Neubig, 2021) enhanced by LaBSE embeddings (Feng et al., 2022). Two variants guide how words are substituted. In the noun-token variant, we use Stanza POS tagger (Qi et al., 2020) to locate matrix-language nouns and replace each with its aligned counterpart from the embedded-language sentence, prompting 3 Claude 3.5 Sonnet (hereafter Claude) to perform the replacements, ensuring that the switch respects the Equivalence Constraint Theory and the Matrix Language Frame model. In the ratio-token variant, 20% of aligned tokens are chosen at random and replaced, intentionally relaxing all linguistic constraints to match the setup of Chan et al. (2024). LLM-centric method. Inspired by recent work showing that large language models can fluidly generate code-switched text (Potter and Yuan, 2024), we let Claude perform two-step procedure. First, Claude rewrites the matrix-language sentence while inserting masked placeholders at candidate switch pointsnouns for the noun-token variant and randomly selected tokens for the ratiotoken variant. Second, in subsequent and independent step, Claude fills each placeholder with context-appropriate word taken from the embedded-language sentence, yielding the final code-switched output."
        },
        {
            "title": "3.4 Code-Switching Approach Evaluation",
            "content": "For each embedded language, we assembled 300sample test-set, and generated code-switched variants using both approaches from Section 3.3. GPT4o then conducted blind, pairwise comparisons under the LLM-as-a-Judge framework (Zheng et al., 2023), evaluating fluency, depth of mixing, grammatical validity at switch points, and overall coherence. In every case, GPT-4o preferred the two-step LLM-Centric approach, demonstrating its superior capacity to produce high-quality, linguistically coherent code-switched text (See Appendix for details on the embedding model, LLM setup, and CSW approach selection and evaluation)."
        },
        {
            "title": "3.5 Evaluation Metrics",
            "content": "We evaluate models using three key metrics to capture baseline performance and the effects of codeswitching: accuracy, weighted average accuracy, and accuracy delta. Accuracy. For model mk and benchmark B, whether monolingual test Bp or its code-switched variant BlmatrixLembedded , we define accuracy as: Acc(mk, B) = (cid:88) 1 i=1 1(Correct(mk, instancei)), (1) 4 where denotes the number of samples in benchmark B, instancei is its i-th example, and 1() is the indicator function. Weighted Average Accuracy. To report an aggregate performance measure for model mk across multiple benchmarks B, we compute the weighted average accuracy as: Accweighted(mk, lmatrix, Lembedded) = (cid:80) BpB Bp Acc(mk, BlmatrixLembedded BpB Bp (cid:80) ) , (2) Accuracy Delta (Acc). We quantify the codeswitching impact by computing the accuracy delta, i.e., the difference between models score on the code-switched benchmark and its score on the original monolingual benchmark, as: Acc(mk, BlmatrixLembedded Acc(mk, BlmatrixLembedded ) = ) Acc(mk, Bp). (3) Positive Acc indicates an improvement under code-switching, negative values drop."
        },
        {
            "title": "4 Experimental Setting",
            "content": "Languages selection We consider set of languages = {English, Arabic, German, French, Chinese} We hypothesize that this set creates varying degrees of semantic, lexical, and syntactic similarities between the matrix language and the embedded languages set, which may differentially affect the degradation caused by CSW, akin to effects observed in machine translation (Guerin et al., 2024; Mohamed et al., 2025). Models selection We evaluated LLaMA 3.2 Instruct (3B) and LLaMA 3.1 Instruct (8B, 70B) (Grattafiori et al., 2024), Qwen 2.5 Instruct (3B, 7B, 72B) (Yang et al., 2025), Mistral 7B Instruct (v0.3) (Albert et al., 2023), and ALLaM 7B (Bari et al., 2024), encompassing wide range of scales and pretraining curricula. Allam currently represents the state-of-the-art in Arabic LLMs, while Qwen and Mistral excel in Chinese and French, respectively, even as they maintain strong multilingual capabilities. The Llama family delivers consistently robust multilingual performance, enabling us to isolate the effects of architecture and model scale on CSW resilience. Benchmarks selection We assess LLM comprehension on three established tasks: Belebele (Bandarkar et al., 2023) for passage-level reading comprehension (with both passages and questions code-switched), MMLU2 (Hendrycks et al., 2020) for broad-domain multiple-choice reasoning (codeswitching applied to questions), and XNLI (Conneau et al., 2018) natural language inference (both premise and hypothesis code-switched). To ensure consistent, scalable evaluation across models, we used and adapted EleutherAIs Language Model Evaluation Harness (Gao et al., 2024) for our codeswitched variants."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experiment 1: Linguistically motivated CSW Setup We use English as the matrix language lmatrix, and perform CSW on the benchmarks with each language in lmatrix as the embedded language separately, using the noun-token CSW method, and compare the performance of the codeswitched benchmarks with the original English benchmarks. Hypothesis 1 (H1) We hypothesize that LLM performance on code-switched benchmarks degrades in proportion to the linguistic distance between the matrix and embedded languages. Results Table 1 and Figure 2 show consistent drops in LLM performance on noun-token codeswitched benchmarks compared to their English versions. The extent of degradation varied by embedded language and model. For example, LLaMA-70Bs weighted average accuracy declined from 0.70 (English) to 0.66 on ENAR/ENDE ( 0.04) and 0.67 on ENZH ( 0.03). Mistral-7B showed minimal loss on ENFR ( 0.01), and ALLaM-7B retained relatively strong performance on ENAR ( 0.06). Qwen models exhibited consistent degradation across languages (e.g., Qwen-7B: 0.03 to 0.06), with larger models achieving better absolute scores but similar relative drops. These trends held across all three tasks, underscoring both the general difficulty of CSW and the role of languagespecific model strengths. 2https://huggingface.co/datasets/openai/MMMLU Model Llama 3B Qwen 3B Allam 7B Mistral 7B Qwen 7B Llama 8B Llama 70B Qwen 72B ENAR ENDE ENFR ENZH EN 0.54 0.56 0.58 0.57 0.61 0.59 0.70 0.69 0.47 0.52 0.53 0.52 0.56 0.52 0.67 0.65 0.47 0.49 0.55 0.47 0.52 0.48 0.66 0.65 0.50 0.51 0.53 0.51 0.57 0.51 0.67 0.65 0.47 0.50 0.52 0.52 0.55 0.51 0.66 0. Table 1: Weighted average accuracy of selected LLMs on noun-token code-switched benchmarks (EN AR, ENDE, ENFR, ENZH) compared to the monolingual English baseline. Cell colors indicate relative performance from highest (green) to lowest (red). The highest scores are indicated in bold. 5.2 Experiment 2: Non-linguistically motivated CSW Setup In this experiment, we retain the experimental framework of Experiment 1, replacing the linguistically motivated noun-token CSW method with the ratio-token method. Hypothesis 2 (H2) We hypothesize that nonlinguistically motivated CSW leads to sharper performance degradation in LLMs than that observed on linguistically motivated CSW, as such input is less likely to align with patterns encountered during pre-training. Results Results are show in Table 2. All models exhibited decline in weighted average accuracy, consistent with the patterns observed in Experiment 1. The extent of degradation varied with model size and language pairing. Smaller models experienced the most pronounced drops; for example, Llama 3B decreased from 0.54 (EN) to 0.43 on ENDE ( = 0.11) and to 0.47 on ENAR ( = 0.07). In contrast, Llama 70B showed minimal degradation, with weighted average accuracy decreasing from 0.70 to 0.68 across all embedded languages ( 0.02). Language-specific resilience was also observed. Allam 7B and Mistral 7B relatively strong performance on ENAR on ENFR, respectively. Qwen 7B exhibited consistent, moderate degradation, decreasing from 0.61 to range of 0.530.57 depending on the embedded language ( = 0.08 to 0.04)."
        },
        {
            "title": "6 Ablations",
            "content": "Building on Section 5, which found comparable degradation from noun-token and ratio-token CSW, we proceed with ablation studies using exclusively the noun-token method. 5 Figure 2: Comparison of LLM accuracy on monolingual English versions of Belebele, MMLU, and XNLI benchmarks (baseline) versus their noun-token code-switched counterparts. English serves as the matrix language, with Arabic (ENAR), French (ENFR), German (ENDE), and Chinese (ENZH) as embedded languages. Model Llama 3B Qwen 3B Allam 7B Mistral 7B Qwen 7B Llama 8B Llama 70B Qwen 72B ENAR ENDE ENFR ENZH EN 0.54 0.56 0.58 0.57 0.61 0.59 0.70 0.69 0.46 0.52 0.53 0.53 0.56 0.53 0.68 0. 0.43 0.51 0.51 0.52 0.55 0.52 0.67 0.66 0.47 0.50 0.56 0.49 0.53 0.50 0.68 0.66 0.51 0.51 0.54 0.52 0.57 0.54 0.68 0.66 Table 2: Weighted average accuracy of selected LLMs on ratio-token code-switched benchmarks (EN AR, ENDE, ENFR, ENZH) compared to the monolingual English baseline. Cell colors indicate relative performance from highest (green) to lowest (red). The highest scores are indicated in bold."
        },
        {
            "title": "6.1 English as an embedded language",
            "content": "To assess whether embedding English improves comprehension in other matrix languages, we reversed the language roles from the main experiments, using each language in lmatrix as the matrix language, and English as the sole embedded language. We generated code-switched versions (Blmatrix{English} ) of the Belebele, MMLU, and XNLI benchmarks. By comparing model performance on these variants against their original monolingual counterparts, we aimed to assess any comprehension enhancement attributable to the embedded English words. Results are presented in Table 3. Embedding English into lower-resource matrix languages often improved model performance or, at minimum, avoided large degradations. Gains were especially prominent when models lacked proficiency in the matrix language. For instance, Mistral 7Bs weighted average accuracy in Arabic rose from 0.35 to 0.48 ( = +0.13), while its score in Chinese increased by +0.07 points. In contrast, when models already demonstrated strong matrix lan6 Model AREN DEEN FREN ZHEN Orig CSW Orig CSW Orig CSW Orig CSW Llama 3B 0.37 0.45 0.35 0.38 0.43 0.45 0.42 0.47 Qwen 3B 0.40 0.48 0.49 0.52 0.50 0.53 0.48 0.48 Allam 7B 0.51 0.52 0.39 0.43 0.49 0.52 0.44 0.51 Mistral 7B 0.35 0.48 0.50 0.54 0.52 0.55 0.46 0.53 Qwen 7B 0.47 0.52 0.51 0.53 0.56 0.57 0.56 0.55 Llama 8B 0.38 0.44 0.50 0.50 0.50 0.52 0.49 0.53 Llama 70B 0.61 0.66 0.67 0.67 0.68 0.68 0.64 0.66 Qwen 72B 0.63 0.66 0.68 0.68 0.68 0.68 0.66 0.66 Table 3: Weighted average accuracy of LLMs on monolingual (Orig) versus English-embedded code-switched (CSW) benchmarks across Arabic, German, French, and Chinese, rounded to two decimals. Bold indicates the higher score in each Orig/CSW pair. Italic indicates instances where performance did not change between the original and code-switched versions. guage proficiency, improvements were minimal or absent. Allam 7B (Arabic) and Mistral 7B (French) saw gains of only +0.01 and +0.03, respectively. High-performing models such as Llama 70B and Qwen 72B showed no change in several settings. Only one case showed minor drop: Qwen 7B on Chinese ( 0.01). This suggests that embedded English may introduce interference when matrix language representations are already strong."
        },
        {
            "title": "6.2 When Code-Switching Goes Extreme",
            "content": "To assess performance under more complex multilingual mixing, an \"extreme\" CSW experiment was conducted on the MMLU benchmark. English served as the matrix language, with nouns code-switched using three distinct embedded languages sets: Setting 1 featured non-Latin script pair (Lembedded = {Arabic,Chinese}), Setting 2 used Latin script pair (Lembedded = {French,German}), and Model Llama 3B Qwen 3B Allam 7B Mistral 7B Qwen 7B Llama 8B Llama 70B Qwen 72B Setting 1 0.48 0.54 0.56 0.53 0.58 0.49 0.72 0.74 Setting 2 0.46 0.55 0.54 0.56 0.60 0.51 0.70 0.74 Setting 3 0.47 0.53 0.54 0.55 0.59 0.49 0.70 0.73 EN 0.55 0.59 0.58 0.59 0.65 0.60 0.77 0. Model Llama 3B Qwen 3B Allam 7B Mistral 7B Qwen 7B Llama 8B Llama 70B Qwen 72B ENAR ENDE ENFR ENZH EN 0.54 0.56 0.58 0.57 0.61 0.59 0.70 0.69 0.31 0.51 0.56 0.46 0.54 0.41 0.53 0.70 0.32 0.54 0.54 0.50 0.58 0.48 0.64 0.71 0.34 0.53 0.53 0.50 0.56 0.47 0.53 0.71 0.32 0.53 0.53 0.50 0.59 0.47 0.50 0. Table 4: MMLU accuracy for extreme CSW with English as the matrix language and the embedded languages being Arabic and Chinese (Setting 1), French and German (Setting 2), and Arabic, Chinese, French, and German (Setting 3), alongside the monolingual English baseline. The highest scores are indicated in bold. Impact of an instructional prompt on Table 5: LLM weighted average accuracy for noun-token codeswitched benchmarks. English serves as the matrix language, with results shown for various embedded languages. The highest scores are indicated in bold 3 all four combined Setting languages (Lembedded = {Arabic,Chinese,French,German}). For generating the code-switched text across these settings, Claude was, additionally, prompted to borrow words evenly from the specified embedded languages for each instance. Table 4 demonstrates that all models experience decline in MMLU accuracy under extreme code-switching relative to the monolingual English baseline. For example, Llama 70Bs score decreases from 0.77 to between 0.70 and 0.72, and Qwen 72Bs from 0.77 to 0.730.74. Analyzing language-script effects by comparing the non-Latin mix (Setting 1) against the Latin mix (Setting 2) reveals no uniform penalty for non-Latin scripts. Allam 7B achieves higher accuracy with the non-Latin pair (0.56 vs. 0.54), whereas Mistral 7B performs better with the Latin pair (0.56 vs. 0.53). Moreover, extending the embedded set to all four languages (Setting 3) does not invariably yield the lowest scores, while Llama 70B (0.70) and Qwen 72B (0.73) record their minima in Setting 3, other models exhibit accuracies intermediate between those in Settings 1 and 2."
        },
        {
            "title": "7 Mitigation strategies",
            "content": "To mitigate the performance declines induced by CSW, we investigate two strategies: promptbased approach, which prepends explicit instructions to code-switched inputs, and model-based approach, which fine-tunes LLMs on synthetic CSW data."
        },
        {
            "title": "7.1 Prompt-based Mitigation",
            "content": "Each noun-token code-switched benchmark instance was prepended with an explicit instruction indicating that the input involves English mixed 7 with an embedded language. Further details on the prompts used per benchmark are provided in Appendix C. The results of the prompt-based mitigation approach, presented in Table 5, show considerable variation across models when compared to unprompted noun-token CSW  (Table 1)  . For some models, most notably the Qwen family, the addition of an explicit instruction led to consistent performance gains. Qwen 72B improved across all language pairs, most remarkably surpassing its monolingual English weighted average accuracy (ENZH: 0.72 vs. EN: 0.69). Similarly, Qwen 7B also benefited, with ENZH improving from 0.57 to 0.59 ( = +0.02). Allam 7B exhibited minor improvements as well, such as ENAR increasing from 0.55 to 0.56 ( = +0.01). Conversely, for other models, particularly the Llama family and Mistral 7B, the prompt-based strategy was frequently detrimental. Llama 8B saw weighted average accuracy declines across all embedded languages (e.g., ENFR dropped from 0.52 to 0.48, = 0.04). More substantial drops were observed for Llama 70B, especially on ENAR and ENZH, where performance fell by 13 and 17 points respectively. Llama 3B and Mistral 7B similarly exhibited declines (e.g., Llama 3B ENAR:a = 0.16)."
        },
        {
            "title": "7.2 Model-based Mitigation",
            "content": "Directly fine-tuning LLMs on code-switched text presents another avenue for mitigation. For this, Llama 8B was selected, primarily due to its limited responsiveness to prompting within its size category. parallel corpus of TED Talk transcripts (Qi et al., 2018) spanning English, Arabic, Chinese, French, and German was utilized. The instruction-tuning dataset was constructed by first information processing under these conditions. Our findings reveal several nuanced insights. LLM comprehension of English as matrix language is significantly disrupted by the introduction of elements from other languages. Our experiments consistently show that inserting tokens from other languagesArabic, Chinese, French, or Germaninto English text leads to drop in LLM comprehension. This drop does not appear to stem solely from unfamiliarity with CSW, as similar performance declines were observed when randomly inserting foreign tokens (as in the ratiotoken method from Experiment 2). Instead, these findings point to more fundamental difficulty: LLMs struggle to process disrupted monolingual structures and integrate mixed linguistic signals effectively. Embedding English tokens into other languages often improves LLM comprehension of the original text. LLMs frequently exhibited improved comprehension on non-English texts when English tokens were embedded, surpassing their baseline performance on the original monolingual versions of the same benchmarks. Code-switching complexity does not linearly correlate with performance degradation. In our \"extreme\" CSW experiments, increasing the number of embedded languages or mixing script types did not consistently lead to greater declines in model performance compared to simpler two-language settings. These findings suggest that degradation is not direct function of multilingual complexity, but rather emerges from nuanced interaction between specific language combinations and model-specific linguistic representations. While prompting helps some models mitigate degradation, fine-tuning offers more reliable solution. We evaluated two strategies for mitigating the effects of code-switching: prompt-based and model-based. Explicitly prepending instructions about upcoming code-switched input  (Table 5)  proved effective for some architecturesmost notably the Qwen family. However, this strategy was less effective, or even detrimental, for others like Llama and Mistral, likely due to interference with their internal processing. For models that did not benefit from prompting, such as Llama 8B, we explored direct instruction fine-tuning on code-switched data. This approach led to more consistent improvement. As shown in Figure 3, Llama 8B, which suffered performance drops under Figure 3: Comparison of Llama 8B and its instructiontuned variant (CSW-Llama 8B) on monolingual English benchmarks (Belebele, MMLU, and XNLI) versus their noun-token code-switched counterparts. English serves as the matrix language, with Arabic, French, German, and Chinese, as embedded languages. selecting samples from the parallel corpus where the English sentence length was greater than 70 words. This filtering yielded approximately 3,650 pairs per language combination. Noun-token CSW, with English as matrix language, was then applied to these, resulting in an instruction-tuning dataset of approximately 14,600 training samples. The instruction required the model to generate the code-switched text from the original English and embedded-language sentences, using five distinct prompt templates to ensure instructions diversity (further details in Appendix D). The impact of this instruction fine-tuning is illustrated in Figure 3. The baseline Llama 8B model achieved an English-only weighted average accuracy of 0.59 on the combined benchmarks. Introducing noun-token CSW without fine-tuning resulted in weighted average accuracy reduction of up to 0.11 points, depending on the embedded language. After fine-tuning on the code-switched corpus (yielding CSW-Llama 8B), partial recovery of performance was observed. The most significant improvement was for the ENAR setting, where the weighted average accuracy increased by +0.04 points over the baseline. The smallest gain was for ENFR, with an increase of +0.03 points."
        },
        {
            "title": "8 Discussion and Conclusion",
            "content": "As LLMs increasingly process multilingual and mixed-language inputs, understanding their comprehension limits is paramount. This study systematically evaluated LLM performance on codeswitched text, yielding multifaceted insights into 8 prompting, partially recovered its accuracy after instruction tuningdemonstrating that fine-tuning is more promising path for improving LLM robustness to code-switching. These findings underscore that while LLMs exhibit impressive multilingual capabilities, CSW introduces specific comprehension challenges distinct from monolingual processing. The asymmetric impact of English as matrix versus embedded language highlights areas requiring further research. While mitigation is possible, the modelspecific nature of these solutions points towards the need for more adaptive approaches to ensure reliable LLM performance in real-world multilingual environments."
        },
        {
            "title": "Limitations",
            "content": "While our study adopts controlled evaluation setup for both linguistically and non-linguistically motivated code-switching, the noun-token approach we employ reflects one of the fundamental forms of linguistically grounded, naturalistic switching. However, more complex forms of codeswitching may induce more severe performance degradation. Future work should investigate how higher-complexity switching patterns affect LLMs understanding. Additionally, in our non-linguistically motivated ratio-token experiments, the substitution rate was fixed at 20%. Exploring how variation in this ratio affects model behavior could yield more nuanced understanding of the impact of non-linguistically grounded switching on LLM comprehension."
        },
        {
            "title": "References",
            "content": "Gustavo Aguilar, Sudipta Kar, and Thamar Solorio. 2020. LinCE: centralized benchmark for linguistic code-switching evaluation. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 18031813, Marseille, France. European Language Resources Association. Jiang Albert, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, and Devendra Singh Chaplot. 2023. Mistral 7b. arXiv. Li Nguyen and. 2018. Borrowing or code-switching? traces of community norms in vietnamese-english speech. Australian Journal of Linguistics, 38(4):443 466. Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. The belebele benchmark: parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv:2308.16884. Saiful Bari, Yazeed Alnumay, Norah Alzahrani, Nouf Alotaibi, Hisham Alyahya, Sultan AlRashed, Faisal Mirza, Shaykhah Alsubaie, Hassan Alahmed, Ghadah Alabduljabbar, et al. 2024. Allam: Large language models for arabic and english. arXiv preprint arXiv:2407.15390. Barbara E. Bullock and Almeida Jacqueline Toribio. 2009. The Cambridge Handbook of Linguistic Codeswitching. Cambridge Handbooks in Language and Linguistics. Cambridge University Press. Kelvin Wey Han Chan, Christopher Bryant, Li Nguyen, Andrew Caines, and Zheng Yuan. 2024. Grammatical error correction for code-switched sentences by learners of English. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 79267938, Torino, Italia. ELRA and ICCL. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 24752485, Brussels, Belgium. Association for Computational Linguistics. Richeek Das, Sahasra Ranjan, Shreya Pathak, and Preethi Jyothi. 2023. Improving pretraining techniques for code-switched NLP. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11761191, Toronto, Canada. Association for Computational Linguistics. Sourya Dipta Das, Ayan Basak, Soumil Mandal, and Dipankar Das. 2022. Advcodemix: Adversarial attack on code-mixed data. In Proceedings of the 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD), pages 125129. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 9 41714186, Minneapolis, Minnesota. Association for Computational Linguistics. A. Seza Dogruöz, Sunayana Sitaram, Barbara E. Bullock, and Almeida Jacqueline Toribio. 2021. survey of code-switching: Linguistic and social perspectives for language technologies. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 16541666, Online. Association for Computational Linguistics. Zi-Yi Dou and Graham Neubig. 2021. Word alignment by fine-tuning embeddings on parallel corpora. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 21122128, Online. Association for Computational Linguistics. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic BERT sentence embedding. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 878891, Dublin, Ireland. Association for Computational Linguistics. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. The language model evaluation harness. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Nicolas Guerin, Shane Steinert-Threlkeld, and Emmanuel Chemla. 2024. The impact of syntactic and semantic proximity on machine translation with backtranslation. arXiv preprint arXiv:2403.18031. Ayushman Gupta, Akhil Bhogal, and Kripabandhu Ghosh. 2024. Code-mixer ya nahi: Novel approaches to measuring multilingual llms code-mixing capabilities. arXiv preprint arXiv:2410.11079. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Maite Heredia, Gorka Labaka, Jeremy Barnes, and Aitor Soroa. 2025. Conditioning llms to generate codeswitched text: methodology grounded in naturally occurring data. arXiv preprint arXiv:2502.12924. Muhammad Huzaifah, Weihua Zheng, Nattapol Chanpaisit, and Kui Wu. 2024. Evaluating code-switching In Protranslation with large language models. ceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 6381 6394, Torino, Italia. ELRA and ICCL. Pranjal Khanuja et al. 2020. Improving code-switched nlp using data augmentation. In Proceedings of ACL 2020, pages 18601871. Prashant Kodali, Anmol Goel, Likhith Asapu, Vamshi Krishna Bonagiri, Anirudh Govil, Monojit Choudhury, Manish Shrivastava, and Ponnurangam Kumaraguru. 2024. From human judgements to predictive models: Unravelling acceptability in codemixed sentences. arXiv preprint arXiv:2405.05572. Garry Kuwanto, Chaitanya Agarwal, Genta Indra Winata, and Derry Tanti Wijaya. 2024. Linguistics theory meets llm: Code-switched text generation via equivalence constrained large language models. arXiv preprint arXiv:2410.22660. Zihan Liu, Genta Indra Winata, Zhaojiang Lin, Peng Xu, and Pascale Fung. 2020. Attention-informed mixedlanguage training for zero-shot cross-lingual taskoriented dialogue systems. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):8433 8440. Amr Mohamed, Mingmeng Geng, Michalis Vazirgiannis, and Guokan Shang. 2025. Llm as broken telephone: Iterative generation distorts information. arXiv preprint arXiv:2502.20258. Melissa G. Moyer. 2002. Pieter muysken, bilingual speech: typology of code-mixing. cambridge: Cambridge university press, 2000. pp. xvi, 306. hb 59.95. Language in Society, 31(4):621624. P. Muysken. 2000. Bilingual Speech: Typology of Code-Mixing. Cambridge University Press. R. Myers-Scotton. 1993. Social Motivations for CodeSwitching: Evidence from Africa. Oxford University Press. Mark Myslín. 2014. Codeswitching and predictability of meaning in discourse. In Codeswitching and predictability of meaning in discourse. Lynnette Hui Xian Ng and Luo Qi Chan. 2024. What talking you?: Translating code-mixed messaging texts to english. arXiv preprint arXiv:2411.05253. Millicent Ochieng, Varun Gumma, Sunayana Sitaram, Jindong Wang, Vishrav Chaudhary, Keshet Ronen, Kalika Bali, and Jacki ONeill. 2024. Beyond metrics: evaluating llms effectiveness in culturally nuanced, low-resource real-world scenarios. arXiv preprint arXiv:2406.00343. Tanmay Parekh, Emily Ahn, Yulia Tsvetkov, and Alan Black. 2020. Understanding linguistic accommodation in code-switched human-machine dialogues. In Proceedings of the 24th Conference on 10 Genta Indra Winata, Andrea Madotto, Zhaojiang Lin, Rosanne Liu, Jason Yosinski, and Pascale Fung. 2021b. Language models are few-shot multilingual In Proceedings of the 1st Workshop on learners. Multilingual Representation Learning, pages 115, Punta Cana, Dominican Republic. Association for Computational Linguistics. Genta Indra Winata, Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2019. Code-switched language models using neural based synthetic data from parallel sentences. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 271280, Hong Kong, China. Association for Computational Linguistics. Anjali Yadav, Tanya Garg, Matej Klemen, Matej Ulcar, Basant Agarwal, and Marko Robnik Sikonja. 2024. Code-mixed sentiment and hate-speech prediction. arXiv preprint arXiv:2405.12929. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Ruochen Zhang, Samuel Cahyawijaya, Jan Christian Blaise Cruz, Genta Winata, and Alham Fikri Aji. 2023. Multilingual large language models are not (yet) code-switchers. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1256712582, Singapore. Association for Computational Linguistics. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2). Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Zhihong Zhu, Xuxin Cheng, Zhiqi Huang, Dongsheng Chen, and Yuexian Zou. 2023. Enhancing codeswitching for cross-lingual SLU: unified view of semantic and grammatical coherence. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78497856, Singapore. Association for Computational Linguistics. Computational Natural Language Learning, pages 565577, Online. Association for Computational Linguistics. Parth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj Pandey, Srinivas PYKL, Björn Gambäck, Tanmoy Chakraborty, Thamar Solorio, and Amitava Das. 2020. SemEval-2020 task 9: Overview of sentiment analysis of code-mixed tweets. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 774790, Barcelona (online). International Committee for Computational Linguistics. Shana Poplack. 1988. 8. Contrasting patterns of codeswitching in two communities, pages 215244. De Gruyter Mouton, Berlin, New York. Susan Poplack. 1978. Sometimes ill start sentence in spanish termino en español: Toward typology of code-switching. Linguistics, 16(7-8):581618. Tom Potter and Zheng Yuan. 2024. LLM-based codeswitched text generation for grammatical error correction. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1695716965, Miami, Florida, USA. Association for Computational Linguistics. Adithya Pratapa, Gayatri Bhat, Monojit Choudhury, Sunayana Sitaram, Sandipan Dandapat, and Kalika Bali. 2018. Language modeling for code-mixing: The role of linguistic theory based synthetic data. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15431553, Melbourne, Australia. Association for Computational Linguistics. Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: python natural language processing toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 101108, Online. Association for Computational Linguistics. Ye Qi, Devendra Sachan, Matthieu Felix, Sarguna Padmanabhan, and Graham Neubig. 2018. When and why are pre-trained word embeddings useful for neural machine translation? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 529535, New Orleans, Louisiana. Association for Computational Linguistics. Samson Tan and Shafiq Joty. 2021. Code-mixing on sesame street: Dawn of the adversarial polyglots. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 35963616, Online. Association for Computational Linguistics. Genta Winata et al. 2021a. Multilingual pretrained models are effective for code-switching nlp. In Proceedings of EMNLP 2021, pages 23452356."
        },
        {
            "title": "A Additional Details",
            "content": "All experiments were conducted using NVIDIA A100 (40GB VRAM) and A10 (24GB VRAM) GPU clusters. The compute allocation totaled 22 GPU-days, comprising 8 GPU-days on 8A100 nodes and 14 GPU-days on 4A10 nodes. Code-Switched Text Generation Approaches and Component Selection This section details our selection process for model components used in generating code-switched (CSW) text, as introduced in Section 3. Our objective was to identify the most effective LLM and alignment backbone for producing fluent, grammatically valid CSW outputs suitable for benchmark construction. B.1 LLM Selection for Generation We compared Claude 3.5 Sonnet and GPT-4o as generation modules for both the Alignment-Based and LLM-Centric pipelines. For each matrixembedded language pair (ENAR, ZH, FR, DE), we sampled 100 samples from the Belebele, MMLU, and XNLI benchmarks. Both models generated noun-token CSW sentences under linguistically grounded prompting that adhered to the Equivalence Constraint Theory (ECT) and Matrix Language Frame (MLF) model. Bilingual annotators conducted pairwise preference evaluations of the outputs, focusing on single criterion: which code-switched sentence sounded more natural to them. Claude was consistently favored, with preference rates ranging from 52% to 62% across languages, as shown in Table 6. Accordingly, Claude was selected as the generation model for all subsequent CSW construction. Embedded Language Claude (%) GPT-4o (%)"
        },
        {
            "title": "Arabic\nChinese\nFrench\nGerman",
            "content": "55 57 52 62 45 43 48 38 Table 6: Human preferences for CSW text generated by Claude vs. GPT-4o (100 examples per language pair). B.2 Embedding Backbone Selection To identify the best embedding model for alignment in the Alignment-Based Pipeline, we evaluated AWESOME with mBERT (AWESOMEs default embedding model) and LaBSE. For each language pair, 300 noun-token CSW sentences were generated using alignments from each configuration, with substitution handled by Claude. Using GPT-4o as an LLM-based judge, we found that LaBSE-based alignments consistently yielded more natural and fluent code-switched outputs than those derived from mBERT, with clear preferences observed for Arabic (89.0%), Chinese (91.3%), and French (74.7%). For German, the preference was more modest (55.3%), though still in favor of LaBSE. GPT-4o was selected as the evaluator due to its strong multilingual capabilities and demonstrated aptitude in CSW understanding across typologically diverse languages. Importantly, using GPT-4o rather than Claude to evaluate outputs avoids the potential biases introduced by self-evaluation, such as output familiarity or training data memorization, thus providing more neutral and reliable assessment of generation quality. Results presented in Table 7, informed our decision to adopt LaBSE as the default embedding backbone for alignment in all subsequent experiments. 12 Embedded Language LaBSE (%) mBERT (%) Arabic Chinese French German 89.0 91.3 74.7 55.3 11.0 8.7 25.3 44.7 Table 7: GPT-4o preference rates for CSW text generated using LaBSE vs. mBERT alignments. Percentages reflect outcome ratios from 300 evaluation instances per language. B.3 LLM-Centric Method Prompts The LLM-centric pipeline (Section 3.3) uses two-step prompting strategy: 1. Placeholder identification mark every switchable noun in the English sentence with placeholder mask (#######). 2. Placeholder filling substitute each sentinel with the aligned word(s) from the parallel targetlanguage sentence, yielding the final code-switched version. You are an expert linguist and code - switching analyst . Based on the Equivalence Constraint Theory and the Matrix Language Frame model , identify nouns in the input English sentence that would serve as appropriate code - switching points . - Input variable : text (a single English sentence ) - Task : Find every noun ( as free content morpheme ) that can be switched under the theories above . - Transformation : Replace each identified noun in the sentence with \"#######\". - Output : Return only the transformed sentence with nouns replaced by \"#######\". - The substituted words blend seamlessly into the text , following natural bilingual speech patterns . - Adjust the target language words as needed (e.g., inflection , gender , number ) so that the text remains syntactically correct . - Ensure that nouns in common expressions are not code - switched . - Don 't return any summary or introduction , just the processed text [ English text ] { text } Figure 4: Step 1 Placeholder identification prompt (noun-token variant). You will be given pair of parallel texts in English and { target_language }. Your goal is to produce code - switched version of the English text by replacing each of the hashtag - sequences (#######) in the English text with their { target_language } counterparts from the { target_language } text , ensuring that : - The substituted words blend seamlessly into the text , following natural bilingual speech patterns . - The text should be grounded with the principles of the Equivalence Constraint Theory and the Matrix Language Frame model . - Adjust the target language words as needed (e.g., inflection , gender , number ) so that the text remains syntactically correct . - The original meaning and flow of the text are maintained . - All the hashtag - sequences (#######) have to be replaced with text from the { target_language } text . - Use only the words from the { target_language } text . - Return only the code - switched text , without any additions or explanations . [ English text with placeholders ] { placeholder_text } [{ target_language } text ] { target_text } [ Code - switched English and { target_language }] Figure 5: Step 2 Placeholder filling prompt (noun-token variant). You will be given an English sentence with placeholders (#######) and its parallel sentence in { target_language }. Replace each placeholder with the corresponding segment from the { target_language } text , ensuring : - The inserted text matches the target - language phrasing ( inflections , gender , number ). - The final sentence reads naturally as mixed English and { target_language }. - Preserve the original sentence order . Return only the filled sentence , no extra comments . [ English with placeholders ] { placeholder_text } [{ target_language } parallel text ] { target_text } [ Mixed code - switched result ] Figure 6: Prompt used in the ratio-token variant (random placeholder insertion). B.4 Final Generation Approach Selection We compared the Alignment-Based Pipeline and the LLM-Centric Method for generating noun-token CSW text across 100 samples per language and benchmark. Results are presented in Table 8. Pairwise evaluation via GPT-4o favored the LLM-Centric approach in all settings, with the strongest preferences for Chinese (66%) and French (63.8%). Based on these results, we adopt the LLM-Centric Method for all noun-token CSW benchmark construction, while retaining the Alignment-Based Pipeline for tasks requiring explicit control over substitution rates (e.g., ratio-token generation). Embedded Language LLM-Centric (%) Alignment-Based (%)"
        },
        {
            "title": "Arabic\nChinese\nFrench\nGerman",
            "content": "56.1 66.0 63.8 53.4 43.9 34.0 36.2 46.6 Table 8: GPT-4o preferences between generation methods for noun-token CSW outputs. You have two code - switched sentences , and , each blending English ( matrix language ) with { second_language }. Follow these steps and then choose the better sentence (A or B): 1. Assess Fluency : check which sentence flows most naturally , like plausible bilingual speech . 2. Assess Depth of Mixing : check which sentence meaningfully integrates both languages rather than inserting isolated tokens . 3. Assess Switch Grammar : check which sentence has grammatically valid switch points under Equivalence Constraint Theory . 4. Assess Consistency : check which sentence uses English as its grammatical frame and embeds { second_language } elements appropriately under the Matrix Language Frame model . 5. Assess Overall Coherence : check which sentence remains clear and plausible as whole despite the language mixing . After evaluating all five criteria , return or with no further explanation . Sentences : A: { sentence_one } B: { sentence_two } Output : Figure 7: The prompt given to Claude 3.5 Sonnet for choosing the best summary between the baseline and LLMgenerated summaries. 14 Instructional Prompt for Prompt-Based Mitigation Belebele Prompt You are an expert in understanding code - switched text . You will be given passage and question in code - switched English and Arabic . You have to understand them and respond to the given question with best answer : , , , or D. Figure 8: Instructional prompt prepended for Belebele multiple-choice QA tasks. MMLU Prompt You are an expert in understanding code - switched text . You will be given question in code - switched English and Arabic . You have to understand it and respond to the given question with best answer : , , , or D. Figure 9: Instructional prompt prepended for MMLU multiple-choice QA tasks."
        },
        {
            "title": "XNLI Prompt",
            "content": "You are an expert in understanding code - switched text . You will be given two code - switched passages that correspond to premise and hypothesis in code - switched English and Arabic text . You have to understand them and respond with the best answer : 0, 1, or 2. Figure 10: Instructional prompt prepended for XNLI natural language inference tasks. Instruction Tuning for Model-Based Mitigation We fine-tuned LLaMA-3.1-8B-Instruct to improve its comprehension of code-switched text using targeted instruction-tuning dataset. Full-model training was conducted over single epoch using learning rate of 2 106 with linear decay and 5% warmup. Training leveraged mixed-precision BF16 and dynamic sequence packing within 4096-token window, and batch-size of four. D.1 Dataset Preparation The training data was derived from parallel TED Talk translations (Qi et al., 2018), selecting English sentences longer than 70 words and their Arabic, Chinese, French, and German equivalents. Each English sentence was converted into four code-switched variants using the LLM-Centric Method (Appendix B.4). The final dataset included over 14,000 examples, shuffled and formatted as instructionresponse pairs. D.2 Prompt Templates for Instruction Tuning To prevent overfitting to fixed phrasing, each training instance was paired with randomly selected prompt from pool of five semantically equivalent instruction templates. These templates varied in their surface structure but uniformly instructed the model to blend the matrix English sentence with embedded nouns from the translation. Figures 1115 illustrate the five styles used. Take this English sentence and infuse it with < LANGUAGE > code - switching : English : \" < ENGLISH_SENTENCE >\" < LANGUAGE >: \" < TRANSLATION_SENTENCE >\" Figure 11: Infusion-style template. 15 Convert the following English line into code - switched mix with < LANGUAGE >: English : \" < ENGLISH_SENTENCE >\" < LANGUAGE >: \" < TRANSLATION_SENTENCE >\" Figure 12: Conversion-style template. Blend English and < LANGUAGE > in the sentence below : English text : \" < ENGLISH_SENTENCE >\" < LANGUAGE > equivalent : \" < TRANSLATION_SENTENCE >\" Figure 13: Blending-style template. Generate code - switched rendition by swapping in < LANGUAGE >: English original : \" < ENGLISH_SENTENCE >\" < LANGUAGE > snippet : \" < TRANSLATION_SENTENCE >\" Figure 14: Rendition-style template. Switch parts of this English sentence into < LANGUAGE >: English : \" < ENGLISH_SENTENCE >\" < LANGUAGE >: \" < TRANSLATION_SENTENCE >\" Figure 15: Switching-style template."
        }
    ],
    "affiliations": [
        "Ecole Polytechnique",
        "MBZUAI"
    ]
}