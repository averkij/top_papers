{
    "paper_title": "MiMo-Embodied: X-Embodied Foundation Model Technical Report",
    "authors": [
        "Xiaoshuai Hao",
        "Lei Zhou",
        "Zhijian Huang",
        "Zhiwen Hou",
        "Yingbo Tang",
        "Lingfeng Zhang",
        "Guang Li",
        "Zheng Lu",
        "Shuhuai Ren",
        "Xianhui Meng",
        "Yuchen Zhang",
        "Jing Wu",
        "Jinghui Lu",
        "Chenxu Dang",
        "Jiayi Guan",
        "Jianhua Wu",
        "Zhiyi Hou",
        "Hanbing Li",
        "Shumeng Xia",
        "Mingliang Zhou",
        "Yinan Zheng",
        "Zihao Yue",
        "Shuhao Gu",
        "Hao Tian",
        "Yuannan Shen",
        "Jianwei Cui",
        "Wen Zhang",
        "Shaoqing Xu",
        "Bing Wang",
        "Haiyang Sun",
        "Zeyu Zhu",
        "Yuncheng Jiang",
        "Zibin Guo",
        "Chuhong Gong",
        "Chaofan Zhang",
        "Wenbo Ding",
        "Kun Ma",
        "Guang Chen",
        "Rui Cai",
        "Diyun Xiang",
        "Heng Qu",
        "Fuli Luo",
        "Hangjun Ye",
        "Long Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied."
        },
        {
            "title": "Start",
            "content": "MiMo-Embodied: X-Embodied Foundation Model Technical Report"
        },
        {
            "title": "Xiaomi Embodied Intelligence Team",
            "content": "See Contributions and Acknowledgments section for full author list."
        },
        {
            "title": "Abstract",
            "content": "We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/ MiMo-Embodied. 5 2 0 2 0 2 ] . [ 1 8 1 5 6 1 . 1 1 5 2 : r Figure 1 Performance Comparison in Autonomous Driving and Embodied AI Benchmarks. MiMo-Embodied achieves state-of-the-art performance on both benchmarks, surpassing previous open-source, closed-source, and specialized VLMs, highlighting its superior capabilities in various autonomous driving and embodied AI tasks."
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Contributions and Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "A Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1 General Visual Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Embodied Visualization Examples A.2.1 Spatial Understanding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.2 Affordance Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2.3 Planning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.1 Scene perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.2 Prediction ability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3.3 Planning Task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Autonomous Driving Visualization Examples 3 4 4 4 5 5 5 6 6 7 7 8 8 9 10 10 10 11 11 12 12 12 14 16 16 20 22 23 28 29 29 30 30 38 48 52 52 60 63"
        },
        {
            "title": "Introduction",
            "content": "Vision-Language Models (VLMs) [5, 8, 20, 25, 50, 55] demonstrate significant promise in advancing multimodal perception, understanding, and reasoning capabilities. Recently, some specialized embodied VLMs have emerged in domains such as autonomous driving and embodied AI, highlighting their interactions with the physical world [21, 27, 35, 49, 50]. Specifically, specialized VLMs for embodied AI, such as RoboBrain [27, 49] and VeBrain [35], emphasize individual capabilities like task planning and spatial understanding, delivering critical information for robots during physical interactions. Similarly, specialized VLMs in autonomous driving, such as RoboTron-Drive [24] and DriveLMM-o1 [26], focus on enhancing specific aspects of driving performance, including environmental perception, status prediction, and driving planning, providing necessary information support for autonomous driving systems. While these specialized embodied VLMs have made significant progress, they are constrained by their narrowly defined application scenarios. The focus of embodied AI on indoor tasks and autonomous driving on outdoor roads creates significant domain gap, which hinders the cross-domain generalization of capabilities. We summarize the challenges faced by specialized embodied VLMs as follows: (1) Lack of Unified Embodied VLMs. Existing VLMs focus on single domain and lack unified VLM that bridges autonomous driving and embodied AI. This fragmentation hinders the generalization of spatial understanding and reasoning across diverse indoor and outdoor scenarios, ultimately limiting the models capability to interact effectively with the physical world in dynamic environments. (2) Absence of Comprehensive Cross-Embodiment Capability Evaluation. Existing VLMs only assess partial capabilities in either autonomous driving or embodied AI, lacking comprehensive cross-embodiment capability evaluation to assess the overall performance of specialized embodied VLMs. To address these challenges, we present MiMo-Embodied, unified VLM that merges the tasks of autonomous driving and embodied AI into single model. To our knowledge, MiMo-Embodied is the first open-source VLM to integrate these critical areas, significantly enhancing understanding and reasoning in dynamic physical environments. To evaluate MiMo-Embodied, we conduct series of comprehensive cross-embodiment capability benchmarks to assess the overall performance of specialized embodied VLMs. As shown in Figure 2, our MiMoEmbodied model focuses on key capabilities in both autonomous driving and embodied AI. For autonomous driving, we evaluate three core capabilities: (1) Environmental Perception: understanding traffic scenes through semantic and geometric analysis to scene comprehension, region-level interpretation and potential hazards detection; (2) Status Prediction: forecasting the behaviors of agents and multi-agent interactions to facilitate proactive decision-making; and (3) Driving Planning: generating safe driving maneuvers with explainable justifications based on traffic logic, ensuring compliance with road rules while optimizing safety and efficiency. For embodied AI, we evaluate three core capabilities: (1) Affordance Prediction: inferring actionable interaction possibilities from visual scenes to enable effective interactions; (2) Task Planning: translating abstract instructions into executable action sequences for accomplishing specific goals; and (3) Spatial Understanding: reasoning about spatial relationships, including directions, distances, and layouts, to facilitate navigation and interaction within the environment. Finally, MiMo-Embodied not only advances the state of specialized embodied VLMs but also sets new standard for integrating diverse competencies, paving the way for more intelligent and adaptable systems in complex real-world applications. To equip MiMo-Embodied with the capability to handle both embodied AI and autonomous driving tasks, we constructed diverse dataset of spanning general visual-language understanding, embodied tasks, and autonomous driving scenarios. This dataset provides the multimodal foundation for MiMo-Embodieds core capabilities in perception, prediction, and planning for both autonomous driving and embodied AI. Based on the constructed dataset, we develop progressive four-stage training strategy, which is critical to achieving state-of-the-art performance. The training stages comprises: Stage 1: General and Embodied Knowledge Learning establish core affordance understanding, high-level task planning, and spatial reasoning abilities based on general visual knowledge. Stage 2: Autonomous Driving Knowledge Learning integrates cross-domain understanding through mixed supervision from both autonomous driving and embodied data. Stage 3: Chain-of-Thought Fine-tuning enhances complex reasoning by incorporating generated rationales. Stage 4: Reinforcement Learning Fine-tuning further refines task-specific performance via GRPO [19] optimization. Extensive evaluations demonstrate MiMo-Embodieds strength on 17 benchmarks in embodied AI, and leading performance across 12 benchmarks in autonomous driving, which outperforms current open-source and closed-source general VLMs, as well as specialized models. 3 Figure 2 Overview of MiMo-Embodied Capabilities. MiMo-Embodied supports both Autonomous Driving and Embodied AI tasks, featuring 12 benchmarks in Autonomous Driving that cover Environmental Perception, Status Prediction and Driving Planning, along with 17 benchmarks in Embodied AI tasks focusing on Affordance Prediction, Task Planning, and Spatial Understanding. This report offers comprehensive overview of the development of MiMo-Embodied, including its model architecture, pretraining dataset, training strategy, benchmark evaluation results, and application cases. The remaining sections of this report are organized as follows: Section 2 details the model architecture, establishing the foundation for our unified framework. Section 3 outlines the data curation and construction process across three categories: general multimodal understanding, embodied AI, and autonomous driving. Section 4 elaborates on the four-stage training strategy, which is crucial to achieving our state-of-the-art performance. Section 5 presents comprehensive quantitative and qualitative results, validating the superior performance of our model across various domains."
        },
        {
            "title": "2.1 Core Components of MiMo-Embodied",
            "content": "The MiMo-Embodied architecture consists of three main components: (1) Vision Transformer (ViT) for encoding visual inputs; (2) projector that transforms these visual encodings into latent space aligned with the Large Language Model (LLM); and (3) the LLM, which is responsible for text understanding and reasoning. To effectively process high-resolution inputs, we adopt the vision encoder from MiMo-VL [59]. Similarly, the LLM backbone and the projector are initialized using their respective pre-trained weights from MiMo-VL [59], inheriting its established vision-language alignment and powerful reasoning capabilities. The projector is implemented using multi-layer perceptron (MLP), which maps visual tokens to the LLMs input space. The overall architecture of MiMo-Embodied is shown in Figure 3."
        },
        {
            "title": "2.2 Visual Input Processing",
            "content": "During the visual input processing stage, the ViT is essential for encoding various types of visual inputs, including single images, multiple images, and videos. This component employs self-attention mechanism to extract significant features from the input data, allowing the model to discern complex patterns and relationships. The visual tokens generated through this process are formatted for seamless integration with the subsequent stages of the architecture. This encoding is vital for accurately representing the visual context, facilitating its interpretation by the LLM and enabling richer interactions and more coherent reasoning. 4 Figure 3 Model architecture of MiMo-Embodied. The MiMo-Embodied model architecture is designed for embodied AI and autonomous driving tasks, effectively processing single images, multiple images, and videos, and consists of three main components: (1) Vision Transformer for encoding visual inputs; (2) projector that maps visual encodings to latent space aligned with LLM; and (3) the LLM itself for textual understanding and reasoning."
        },
        {
            "title": "2.3 Latent Space Projection and Output Generation",
            "content": "After encoding the visual input, the projector maps these visual tokens into latent space compatible with the LLM. The projection stage utilizes MLP to transform the high-dimensional visual representation, making it compatible with the LLM while preserving its essential characteristics. Once in the latent space, the LLM initiates thinking process, interpreting the projected data and generating coherent and contextually relevant responses. By seamlessly integrating the visual and textual domains, MiMo-Embodied enhances the potential for diverse multimodal reasoning tasks and applications."
        },
        {
            "title": "3 Training Dataset",
            "content": "In this section, we outline the composition and curation of the multimodal training dataset utilized for developing MiMo-Embodied, specifically targeting foundational capabilities for embodied AI and autonomous driving. As shown in Figure 4, our training data encompasses general dataset, along with specialized datasets for embodied AI and autonomous driving scenarios. Each subset is meticulously sourced and processed to ensure diversity, scalability, and alignment with the demands of real-world applications."
        },
        {
            "title": "3.1 General Dataset",
            "content": "To build versatile and powerful foundation for both general and specialized multimodal tasks, we leverage the training corpus of MiMo-VL [59]. This dataset integrates diverse data types including high-resolution images, videos, long text, long documents, and synthetic reasoning data, ensuring broad coverage of perceptual, reasoning, and interactive capabilities. These data can be categorized into the following categories: (1) Visual Grounding data cultivates fine-grained object-level localization and cross-modal alignment across diverse scenes, enabling precise region-reference understanding and object attribute recognition. (2) Document and Chart Comprehension resources develop structural understanding of complex textual layouts, tables, diagrams, and information graphics, facilitating robust OCR capability and logical information extraction from formatted content. (3) Video Understanding materials enable temporal reasoning through dense event 5 Figure 4 Overview of the Training Data used by MiMo-Embodied. Our model comprises three core components of training datasets: the General Dataset establishes foundational capabilities, the Embodied AI Dataset enhances capabilities in affordance, planning, and spatial perception, and the Autonomous Driving Dataset focuses on improving capabilities in perception, prediction, and planning for autonomous driving. captioning and dynamic scene analysis, supporting both short-term action recognition and long-range temporal dependency modeling. (4) Multimodal Reasoning components build complex logical inference capabilities spanning mathematical, scientific, and symbolic domains, incorporating both perceptual reasoning and longchain logical deduction tasks. This strategically balanced composition ensures synergistic development of both general vision-language understanding and specialized task proficiency, providing solid foundation for diverse real-world multimodal scenarios."
        },
        {
            "title": "3.2 Embodied AI Dataset",
            "content": "The embodied AI dataset are categorized into three parts based on the target capabilities as follows: affordance prediction, high-level task planning, and spatial understanding."
        },
        {
            "title": "3.2.1 Affordance Prediction",
            "content": "To establish comprehensive understanding of affordance, we collect data from the following sources: PixMoPoints [13], RoboAfford [48], and RoboRefIt [34]. PixMo-Points PixMo-Points [13] provides large-scale collection of diverse web images specifically designed to enhance the models ability to perform fine-grained localization through natural language queries. It supports two core capabilities: (1) Object Counting by Pointing, where the model enumerates instances by sequentially pointing to each occurrence, and (2) Visual Explanation via Pointing, where the model grounds its answers by referring to relevant regions in the image. By training on PixMo-Points, the model learns to associate linguistic expressions with precise spatial locations, improving both referential clarity and interactive capabilities. RoboAfford RoboAfford [48] is unified affordance-centric dataset systematically structured to address both object-level and scene-level interaction reasoning. It covers the following five task categories: (1) Visual Captioning: generating contextual descriptions of scenes and objects; (2) Visual Grounding: establishing correspondences between language descriptions and object locations using bounding boxes; (3) Pointing: identifying objects based on category and attribute descriptions; (4) Object Affordance Grounding and 6 Pointing: localizing functional parts of objects such as handles or graspable areas using both bounding boxes and points; and (5) Spatial Affordance Localization: identifying suitable placement regions in free space or on supporting surfaces to facilitate object manipulation. This multi-task setup enables the model to reason about not only what actions an object affords, but also where those actions can be executed. RoboRefIt Beyond object pointing and functional grounding, referential understanding in spatially complex settings is essential for embodied interaction. To this end, we incorporate RoboRefIt [34], dataset featuring referring expressions collected from 187 cluttered indoor scenes. RoboRefIt emphasizes challenging visual grounding scenarios with multiple instances of the same object category, requiring the model to resolve referential ambiguity through attributes and spatial relations. Its annotations include both 2D bounding boxes and segmentation masks, supporting tasks such as referring expression comprehension (REC) and segmentation (RES). By integrating RoboRefIt, the model strengthens its ability to interpret language directives in context-rich, multi-object environments."
        },
        {
            "title": "3.2.2 High-level Task Planning",
            "content": "To advance the capabilities of multimodal models in embodied reasoning and long-horizon task planning, we construct comprehensive task planning dataset by processing and integrating three key sources: CosmosReason1 [3], EgoPlan-IT [10], and RoboVQA [45]. Cosmos-Reason1 The Cosmos-Reason1 [3] component forms the foundation of planning dataset, providing large-scale collection focused on grounding language in physical reality. We specifically leverage its data derived from BridgeData V2 [52], RoboVQA [45], AgiBot [6], and HoloAssist [57], which together cover wide spectrum of embodiments, including robot arms, humanoid robots, and human activities. The data is structured into two complementary annotation types: (1) Understanding Annotations: These provide dense, structured captions for video clips, detailing objects, their attributes, states, and the actions being performed. This serves as the foundational visual context. (2) Reasoning Annotations: This subset includes complex questions accompanied by long-chain reasoning traces generated by DeepSeek-R1 [19], which require multi-step physical and embodied reasoning beyond simple caption understanding. We filter out short videos and use longer videos for training. This dataset equips our model with robust understanding of intuitive physics and cross-embodiment reasoning, enabling the model to perform task completion verification, action affordance judgment, and next-action prediction for variety of agents. EgoPlan-IT To equip the model with human-like planning from first-person perspective, we incorporate high-quality subset of EgoPlan-IT [10]. This dataset is built upon large-scale egocentric video datasets (Epic-Kitchens [11], Ego4D [18]), mirroring how humans perceive and interact with the world. Each sample is structured as multiple-choice question that encapsulates real-world planning problem. The input includes current observation image with task goal described in open-form language, and video clip showing the historical progress of the task. The model is required to predict the correct next action plan from set of candidate options, which include the ground-truth action and several carefully selected negative answers derived from the same task goal. RoboVQA To enhance long-horizon reasoning, we integrate data from RoboVQA [45] including ten types of QA pairs that are automatically generated from long-horizon activity sequences. This component is vital for training models on broad range of visual question-answering tasks that are fundamental to robotics."
        },
        {
            "title": "3.2.3 Spatial Understanding",
            "content": "To equip the model with spatial reasoning capabilities, we integrated comprehensive collection of datasets specifically focused on 3D visual grounding, spatial reasoning, and embodied scene understanding. These datasets provide rich supervision for understanding object relationships in 3D space, interpreting scene layouts, and responding to complex spatial queries, thereby forming foundational pillar for tasks requiring spatial awareness. The data are collected from the following datasets: SQA3D and Self-curated Data We utilize SQA3D [36] and curated 3D dataset for 3D Question Answering (3D-QA), task requiring holistic 3D scene understanding from textual questions. To advance this paradigm, we introduce 3D bounding box localization. Our data combines questions with calibrated images and 7 camera parameters, supporting diverse challenges from object attributes to inter-object spatial reasoning. Furthermore, we generate large-scale 3D grounding data from existing datasets, where each sample includes an RGB image with spatial query, requiring the model to regress 3D boxes in camera coordinate system. This diverse collection, spanning various scenes and object scales, provides crucial supervision for monocular 3D understanding and domain generalization. VLM-3R We utilize spatial reasoning and navigation QA pairs from the VLM-3R [16] dataset, which extends the VSI-Bench [61] framework with large-scale automated generation of spatio-temporal questionanswer pairs. The data encompass seven core spatial reasoning tasks and embodied route planning, each sample combining egocentric video inputs with questions about 3D object relations and navigational actions. RefSpatial To enhance spatial reasoning capabilities for robotic referring tasks, we select samples from RefSpatial [69] dataset. This collection focuses on spatial understanding tasks including object localization, spatial relation comprehension, and free-space identification for placement. The dataset features multi-modal RGB-D inputs paired with textual instructions requiring precise 2D coordinate predictions. Each sample contains multi-turn conversations that progressively address spatial constraints, supporting both single-step perception and multi-step reasoning. The data format integrates visual grounding with structured responses, enabling the model to learn fine-grained spatial relationships essential for real-world robotic interaction. EmbSpatial To enhance spatial reasoning capabilities in embodied scenarios, we employ data from EmbSpatialSFT [15], which comprise spatial relationship identification and object localization tasks. These samples enables our model to develop robust spatial understanding from an egocentric perspective, which is essential for embodied task execution."
        },
        {
            "title": "3.3 Autonomous Driving Dataset",
            "content": "The autonomous driving dataset is categorized into three parts based on the core functional modules of the system as follows: environmental perception, status prediction, and driving planning."
        },
        {
            "title": "3.3.1 Environmental Perception",
            "content": "Environmental perception serves as the fundamental cornerstone for autonomous driving systems to comprehend surrounding environments, laying the groundwork for subsequent processes. To fulfill this objective, the relevant data not only encompass both panoramic overviews and local specifics but also demonstrate multi-level capabilities, including information comprehension and target localization. General Scene Understanding As core component, its primary function is to holistically describe the overall context of the driving scenario and identify key environmental and traffic elements within it. This task aims to build macro-level environmental cognition, distinguishing scenario types and identifying large-scale traffic components. Specifically, CODA-LM [9] includes general perception subtask that focuses on understanding key road entities in driving scenarios, covering the appearance and location of these entities, as well as the reasons why they influence the driving behavior of the ego vehicle. scenery dataset proposed in prior research [38] incorporates annotations across roughly 15 different categories. These categories cover driver actions, justifications, attention, observations of vehicles, observations of pedestrians, road elements, and environmental details. Building on nuScenes, DriveLM [46] generates identify questions related to objects and traffic elements. Similarly, related research [53] generates QA pairs that cover comprehensive range of perception and understanding tasks for driving scenarios, encompassing core dimensions to fully address the understanding of scenario descriptions and key attention elements: it includes scene description, identifying close objects, identifying threatening traffic elements, object countings, recognition of object color and relative position, and OCR-type tasks. nuScenes-QA [41] generates scene graphs using existing 3D detection annotations and manually designs question templates to support the generation of general perception data. MME-RealWorld [68] emphasizes key tasks including object identification, object counting, and driver attention understanding, while IDKB [33] specializes in encoding knowledge related to traffic laws and regulations, providing regulatory foundation for interpreting scenario elements and their implications for driving behavior. MAPLM [7] focuses on road-centric analysis tasks, covering road scene understanding, point 8 cloud quality analysis, road intersection recognition, lane counting, and road and lane description. Collectively, these datasets contribute to multi-faceted understanding of driving scenarios. Regional Object Understanding As object-level perception task, its core objective is to enable fine-grained QA interactions for specific target objects within driving scenarios, focusing on detailed recognition, attribute analysis, and behavioral impact interpretation of localized objects. This involves not only detailed recognition of these objects but also in-depth attribute analysis and interpretation of their behavioral impacts. Specifically, CODA-LM [9] is designed to understand corner-case objects in scenarios where specific bounding boxes are provided. Beyond simply describing the objects within these predefined bounding boxes, it delves into elaborating on the specific mechanisms through which these often rare or high-risk objects influence the driving behavior of the ego vehicle. DriveLM [46] concentrates on two key aspects of localized traffic elements: it supports both traffic elements visual description, which captures static attributes such as shape, color, and texture, and traffic elements motion State, which analyzes dynamic characteristics like speed, direction, and acceleration. DriveAction [22] enriches regional object understanding with 14 vision and language tasks. These tasks cover wide range of aspects, including road structure details, traffic sign attributes, and both dynamic and static traffic elements. MME-RealWorld [68] focus on critical dynamic and attribute tasks. These include pedestrians and vehicles motion attribute identification and traffic signal attribute identification. nuScenes-QA [41]enhances object-centric QA by supporting three core tasks: object recognition, which confirms object categories; object status, which assesses states such as stopped; and objects comparison, for example, judging which vehicle is closer to the ego car. Complementing these, IDKB [33] provides knowledge support specific to the autonomous driving domain by encoding detailed information about road signs, traffic signals, and lane markings. This knowledge base serves as foundation for interpreting the semantic meaning of regional objects and how these objects regulate driving decisions. Collectively, these datasets form comprehensive framework for Regional Object Understanding, enabling precise, targeted analysis of localized objects in driving scenarios that also takes context into account. Regional Object Localization object-level positioning task, its core objective is to achieve accurate target detection for critical objects that influence driving decisions in driving scenarios, rather than detecting all objects without distinction. It focuses on locking the spatial positions of critical objects and outputting positioning information applicable to subsequent decision-making, thereby providing fundamental support for key links of autonomous driving systems. This task not only requires accurately identifying the categories of critical objects from complex driving environment but also demands clarifying their specific positions. Specifically, related work [37] is representative dataset in this task field. Its core function is to identify critical objects in driving scenarios and output corresponding 2D coordinates. Unlike conventional datasets that seek comprehensiveness in object detection, it focuses on objects that directly impact driving decisions, such as sudden obstacles, important traffic signs, and pedestrians crossing the road."
        },
        {
            "title": "3.3.2 Status Prediction",
            "content": "Intent Prediction core task in autonomous driving perception and planning, its primary goal is to forecast the future driving behaviors of surrounding traffic elements in dynamic driving scenarios. By analyzing historical motion data, current states, and contextual information of traffic elements, this task enables autonomous driving systems to proactively anticipate potential action trendssuch as whether preceding vehicle will turn, pedestrian will cross the road, or an adjacent vehicle will change lanesthereby laying critical foundation for safe path planning, collision avoidance, and adaptive speed adjustment. This task not only requires capturing short-term motion patterns of traffic elements but also demands integrating environmental context to improve the accuracy and robustness of behavior predictions, effectively reducing the uncertainty of autonomous driving systems in complex interaction scenarios. Specifically, DriveLM [46] serves as dataset for advancing intent prediction, with dual focus on two critical aspects of traffic element behavior analysis. First, it supports traffic elements motion prediction, which models the future motion of traffic elements based on multi-frame sequential data. Second, it emphasizes object interaction between traffic elements, capturing the interdependencies between different traffic participants. Complementing DriveLM, MME-Realworld [68] focuses on enhancing the practicality of intent prediction in real-world driving environments, with dedicated emphasis on traffic elements intention prediction. These datasets form complementary framework for intent prediction in autonomous driving."
        },
        {
            "title": "3.3.3 Driving Planning",
            "content": "Action Decision The tasks primary goal is to predict the ego vehicles meta actions in dynamic driving scenarios. Meta actions refer to high-level decision-making directions that guide the ego vehicles subsequent operations. To determine the most appropriate high-level action strategy, this task requires the autonomous driving system to analyze information. For this task, two key requirements must be met: the predicted meta actions need to align with real-world traffic rules, and multiple objectives such as safety priority and driving efficiency must be balanced. Specifically, DriveLM [46] focus on two critical dimensions of ego vehicle meta action prediction. First, it supports the classification of safe or unsafe action by labeling whether the ego vehicles potential meta actions conform to safety standards. Second, it emphasizes the definition of goal action, which associates meta actions with specific driving goals. MME-Realworld [68] focuses on enhancing the adaptability of action decision in real-world scenarios, with dedicated emphasis on ego intention prediction. This dataset is built on large number of real driving cases that cover urban, suburban and highway scenarios. In addition, IDKB [33] provides critical knowledge support for action decision by integrating information on vehicle control and driving techniques. These datasets collectively form complementary framework for action decision in autonomous driving, covering safety validation, goal alignment, real-scenario adaptation, and control knowledge integration. Driving Reasoning Different from action decision, driving reasoning is to predict the ego vehicles driving decisions while explicitly outputting the corresponding reasoning process. This task aims to build interpretable decision-making cognition, not only determining the optimal driving actions for the ego vehicle but also clarifying the logical basis behind decisions such as how environmental factors, traffic rules, and safety priorities jointly guide the generation of specific decisionsthereby enhancing the transparency and trustworthiness of autonomous driving systems. Specifically, multiple datasets focus on the integration of decision suggestion and reasoning explanation for Driving Reasoning. CODA-LM [9] supports the task of formulating driving advice, which requires models to first accurately perceive both the general context and regional details of the current driving environment, then provide the ego vehicle with optimal driving suggestions while implicitly embedding the reasoning logic in the advice. Nuinstruct [14] complements this by focusing on high-level decision-making and safety reasoning. novel dataset [38] is built from recorded driving sessions featuring notable events that trigger changes in the cars behaviorincluding decelerations, accelerations, lane changes, narrow gap passes, and turns. Each event is paired with two key pieces of information: descriptions of the cars current action and the corresponding justification for that action. Related work [31] extends this to video-based scenarios: given driving video clip, it requires models to output the ego vehicles specific action and the corresponding reasoning. DriveLM [46] provides comprehensive support for driving reasoning through its planning and reasoning task. IDKB [33] strengthens the reasoning foundation by specializing in encoding knowledge related to driver responsibility and defensive driving. Collectively, these datasets form multi-faceted framework for driving reasoning in autonomous driving, and this framework covers decision-advice reasoning, event-justification matching, and knowledge-guided logical analysis."
        },
        {
            "title": "4 Training Strategy",
            "content": "MiMo-Embodied develops comprehensive multimodal capabilities through progressive four-stage training strategy. Building on MiMo-VL1 [59], we systematically introduce specialized supervision in embodied AI and autonomous driving, culminating in advanced reasoning capabilities through chain-of-thought fine-tuning and reinforcement learning. This strategy facilitates the model to build upon previously acquired capabilities, enabling robust performance across embodied interaction and autonomous driving domains. The training configuration for each stage is detailed in Table 1."
        },
        {
            "title": "4.1 Stage 1: Embodied AI Supervised Fine-tuning",
            "content": "The initial stage establishes core vision-language understanding and embodied reasoning capabilities by integrating general data with specialized embodied AI datasets. We fine-tune our model on the data sourced from MiMo-VLs training corpus [59] and our curated embodied datasets covering affordance prediction, 1In this work, MiMo-VL refers to the 7B-SFT-2508 checkpoint, which serves as the base model for MiMo-Embodied. The original MiMo-VL technical report reports results for the earlier 7B-SFT-2505 checkpoint under the same name."
        },
        {
            "title": "Trainable Components",
            "content": "Stage 1 General Data + Embodied AI 512 2106 AdamW 0.05 Cosine 32768 All Stage 2 Stage 3 Stage Previous + Autonomous Driving 512 2 106 AdamW 0.05 Cosine 32768 All Previous + CoT Data 512 2 106 AdamW 0.05 Cosine 32768 All"
        },
        {
            "title": "RL Data",
            "content": "32 1 106 AdamW 0.0 Cosine 32768 All Table 1 Detailed configuration for each training stage of MiMo-Embodied. high-level task planning, and spatial understanding. This combination enables the model to develop robust spatial reasoning while maintaining broad visual recognition capabilities. The training emphasizes multi-scale understanding from fine-grained object part localization to scene-level spatial relationships. Through this stage, the model acquires essential capabilities for interpreting instructions in physically grounded contexts and reasoning about object interactions in real-world environments."
        },
        {
            "title": "4.2 Stage 2: Autonomous Driving Supervised Fine-tuning",
            "content": "Building upon the embodied AI foundation established in Stage 1, the second stage specializes the model for autonomous driving through intensive driving-specific samples. This phase focuses on developing critical capabilities for understanding dynamic environments, including multi-view spatial reasoning, temporal consistency across video sequences, and complex traffic scenario analysis. The training incorporates diverse driving conditions for environment comprehension, status prediction, and driving planning. Special attention is given to safety-critical perception tasks, such as identifying hazardous objects, predicting the intentions of traffic participants, and understanding complex road geometries. The model learns to process multi-camera inputs simultaneously, maintain object permanence across frames, and reason about the kinematic relationships among vehicles, pedestrians, and infrastructure elements. This specialized training enables robust performance in autonomous driving scenarios, where precise scene understanding and spatial-temporal reasoning are essential for decision-making."
        },
        {
            "title": "4.3 Stage 3: Chain-of-Thought Reasoning Supervised Fine-tuning",
            "content": "The third stage enhances the models reasoning transparency and logical coherence through chain-of-thought (CoT) methodology with generated reasoning samples. We sample subset from our training data for CoT data generation by employing structured approach that breaks down multifaceted problems into sequential reasoning steps. Each training sample includes explicit reasoning chains that demonstrate how to analyze situational context, generate candidate solutions, evaluate alternatives, and justify final decisions. For embodied domains, this includes reasoning about object affordances and spatial constraints; for autonomous driving, it encompasses risk assessment, trajectory evaluation, and behavior justification. This stage significantly improves the models ability to handle multi-step problems that require integrating perceptual information with domain knowledge and logical inference, producing interpretable reasoning processes that enhance trustworthiness in safety-critical applications."
        },
        {
            "title": "4.4 Stage 4: Reinforcement Learning Fine-Tuning",
            "content": "In the final stage, we employ reinforcement learning (RL) fine-tuning to further enhance the models precision and reliability. Building upon the visual perception and reasoning capabilities established in previous stages, this phase optimizes both correctness and response quality on the carefully curated samples spanning diverse multimodal tasks. We implement the Group Relative Policy Optimization (GRPO) [19] algorithm, which samples multiple responses for each query and computes advantages through group-wise normalization. The training specifically addresses corner cases and failure modes identified in previous stages, with particular focus on enhancing performance in visual reasoning, spatial grounding, and complex instruction following. To enable the multi-task mixed training, we design different reward signals for tasks with deterministic solutions. For multi-choice visual reasoning, the reward is evaluated by exact answer matching; For spatial grounding and pointing, it is assessed via IoU between predicted and ground-truth boxes or point in mask. The format compliance is verified through strict template checks. Through this targeted, rule-driven optimization, the model systematically learns to produce more precise and reliable outputs."
        },
        {
            "title": "5 Evaluation",
            "content": "In this section, we evaluate the performance of MiMo-Embodied through both quantitative and qualitative analyses. The quantitative comparisons involve objective assessments against diverse range of established academic and industry benchmarks for embodied AI and autonomous driving, enabling direct empirical comparisons with leading models. Complementarily, the qualitative evaluation showcases MiMo-Embodieds practical efficacy in real-world tasks, highlighting its deployment in complex robotic and autonomous driving scenarios and providing tangible evidence of its ability to translate learned capabilities into effective performance. Next, we will explore the experimental results and analyses in detail."
        },
        {
            "title": "5.1.1 Embodied AI Benchmarks",
            "content": "To evaluate the embodied capabilities of MiMo-Embodied, we conduct comprehensive evaluation across three core domains: affordance prediction, task planning, and spatial understanding. The results in Table 2 and Table 3 show that MiMo-Embodied delivers competitive results, showing particular strength in affordance prediction and spatial understanding compared to both general-purpose multimodal models and specialized embodied models. Affordance Prediction Capability We evaluate the models ability to infer actionable interaction possibilities from visual scenes using five specialized benchmarks, each targeting distinct aspect of affordance understanding. Roborefit [34] assesses object reference capabilities in robotic manipulation scenarios, specifically testing the identification of target with multiple visually similar objects that can only be distinguished through relational reasoning. Where2Place [62] evaluates the models ability to localize semantically appropriate and physically feasible free space for object placement based on the spatial relationships described in instructions. The pointing subset of VABench [63] (VABench-Point) evaluates models precision in grounding natural language commands to specific coordinate locations for robotic manipulation, requiring predicted points to fall within human-annotated regions of target objects or free space. Part-Afford [39] examines fine-grained object part-level affordance recognition across diverse object categories. RoboAfford-Eval [48] provides comprehensive benchmark covering both object and spatial affordance types in robotic manipulation contexts. As shown in Table 2, MiMo-Embodied achieves SOTA performance across all affordance prediction benchmarks. Specially, our model outperforms other embodied VLMs by large margin on the VABench-Point [63], PartAfford [39], and RoboAfford-Eval [48] benchmarks, demonstrating strong capabilities in fine-grained affordance reasoning. The results highlight MiMo-Embodieds effectiveness in interpreting natural language commands for physical interactions, establishing solid foundation for real-world embodied manipulation tasks. Task Planning Capability Task planning capability reflects models proficiency in translating abstract instructions into executable action sequences, requiring long-horizon reasoning, causal inference, and compositional logic. We evaluate this capacity through three specialized benchmarks: EgoPlan2 [42], RoboVQA [45], and 12 Model Info Affordance Planning Names Params RoboRefIt Where2Place VABench-Point Part-Afford RoboAfford-Eval EgoPlan2 RoboVQA Cosmos MiMo-VL [59] Qwen2.5-VL [5] InternVL3.5 [55] GPT-4o [25] Claude-Sonnet-4 [2] Gemini2.5-Pro [17] Qwen-VL-Max [4] Cosmos-Reason1 [3] VeBrain [35] Magma [60] RoboBrain-1.0 [27] RoboBrain-2.0 [49] MiMo-Embodied (Ours) 7B 7B 8B 7B 8B 8B 7B 7B 7B 68.92* 80.42* 39.38* 14.15* 25.75* 38.44* 70.31* 47.25* 32.15* 4.95 70.40* 82.30 Open-Source Models 29.60* 42.00* 16.49* 35.13* 24.50* 11.93* 15.98* 42.65* 16.92* Closed-Source (Proprietary) Models 20.41 25.63 42.38 18.92* 11.40 12.27 9. 53.04 63.59 63.60 13.67* 15.83* 27.92* 41.50* Embodied Models 5.96* 1.67* 26.67* 46.93 13.25* 10.20* 25.53* 65.35* 5.10* 32.10* 31.20* 65.50 43.88* 16.10 28.72* 20.50 18.40* 23.40* 37.92* 12.62* 2.08* 51.46* 69.81 34.14* 39.67* 42.92 35.27* 57.17* 28.55* 50.91* 53.70 48.24* 53.30 46.53* 48.64 66.36* 61.80 41.79 41.26 42.85 44.68* 34. 8.51 33.90 54.37* 26.87 27.30 4.09 33.23 43.00 46.32* 33.82* 61.99 56.80 Table 2 Comparison of MiMo-Embodied with other models on affordance and planning benchmarks. We evaluate the model against various open-source, closed-source, and specialized embodied VLMs to show comprehensive performance overview. Results marked with * are obtained using our evaluation framework. The best results among the listed models are bolded and the second-best is underlined. Cosmos-Reason1 [3]. EgoPlan2 [42] assesses long-horizon planning from video, requiring model to generate the correct action based on current observed image to reach goal. RoboVQA [45] specifically tests causal reasoning and future state prediction, where model must answer questions about the outcome of robotic actions, thereby probing its understanding of action consequences. Cosmos-Reason1 [3] presents multifaceted challenge in compositional reasoning and planning, evaluating the ability to generate complex, multi-step action plans for robotic manipulation tasks requiring both logical and physical understanding. The comparison results in Table 2 demonstrates MiMo-Embodieds advanced task planning capabilities. It outperforms other models on RoboVQA[45], showcasing its superior ability in causal inference and understanding goal-oriented outcomes. MiMo-Embodied also achieves highly competitive performance on the long-horizon planning benchmark EgoPlan2 [42]. This proficiency in analyzing multi-step action sequences underscores MiMo-Embodieds effectiveness in long-horizon reasoning, establishing it as capable foundation for autonomous agents operating in complex, dynamic environments. Spatial Understanding Capability Spatial understanding capability measures the models proficiency in parsing spatial relationships, direction, distance, size, count, configuration, and etc, which is essential for navigation, object localization, and scene comprehension. We evaluate this capacity across nine benchmarks organized into three categories: spatial relationship reasoning (EmbSpatial [15], RoboSpatial [47], SAT [44], VSI-Bench [61], CRPE-relation [54]), spatial language grounding (RefSpatial-Bench [69], ERQA [50], the VQA subset of MetaVQA [56]), and comprehensive spatial intelligence (CV-Bench [51]). It can be seen that MiMo-Embodied achieves state-ofWe present the comparison results in Table 3. the-art results on CV-Bench [51] among comprehensive spatial intelligence tasks, and leads performance on RoboSpatial [47], RefSpatial-Bench [69], and the relation subset of CRPE [54] in spatial relationship reasoning and language grounding. Meanwhile, MiMo-Embodied achieves highly competitive performance on EmbSpatial [15], SAT [44], and the VQA subset of MetaVQA [56]. These results across diverse spatial reasoning tasks validate MiMo-Embodieds capacity for embodied reasoning in the physical world, demonstrating robust understanding of spatial relationships, object references, and contextual question answering. 13 Model Info Spatial Names Params CV-Bench ERQA EmbSpatial SAT RoboSpatial RefSpatial CRPE MetaVQA VSI-Bench MiMo-VL [59] Qwen2.5-VL [5] InternVL3.5 [55] GPT-4o [25] Claude-Sonnet-4 [2] Gemini2.5-Pro [17] Qwen-VL-Max [4] Cosmos-Reason1 [3] VeBrain [35] Magma [60] RoboBrain-1.0 [27] RoboBrain-2.0 [49] MiMo-Embodied (Ours) 7B 7B 8B 7B 8B 8B 7B 7B 7B Open-Source Models 81.69* 75.40 81.46* 41.50* 38.80 41.00 71.29* 70.25 70.26* 59.33* 52.00 59.33* 50.05* 49.33 37.77* 29.79* 38.00* 16.80* 72.33* 76.40 75.10 60.89* 58.63* 60.52* Closed-Source (Proprietary) Models 78.63 78. 84.59 79.42* 68.57 79.68 65.88 76.22 85.75 88. 32.48 45.75 51.02 40.91 39.09 37.29 25. 36.52 30.31 46.75 71.92 73.18* 78.74 71.53* 66.67 75. 79.33 56.67* Embodied Models 65.22 70.52 64.59 68.13 76. 76.24 60.67 58.00 71.33 59.33 75.33 78. 44.42 53.72* 59.87 46.92* 38.81 42.48 33.71 51.53 54. 61.76 8.78 20.78* 38.16 5.44 0.30 4. 9.92 32.50 48.00 76.60 73.20* 72.17* 68.68* 69.42* 68.65* 71.58* 77.15 62.80 61.70* 69.47* 61.52* 65.81* 55.38* 61.11* 67.33 42.10* 35.90 56. 43.60 47.02 47.81 41.80* 25.64 26.30 12. 31.12 36.10 48.49 Table 3 Comparison of MiMo-Embodied with other models on spatial benchmarks. We evaluate the model against various open-source, closed-source, and specialized embodied VLMs to show comprehensive performance overview. Results marked with * are obtained using our evaluation framework. The best results among the listed models are bolded and the second-best is underlined."
        },
        {
            "title": "5.1.2 Autonomous Driving Benchmarks",
            "content": "To comprehensively assess the autonomous driving capabilities of MiMo-Embodied, we conduct systematic evaluation across three critical dimensions: Perception Capability, Prediction Capability, and Planning Capability. Specifically, we evaluate the models performance on 12 benchmarks across 4 data types in terms of its ability to understand complex traffic scenes, predict the behaviors of dynamic road agents, and generate safe and efficient driving suggestions. As shown in Table 4 and Table 5, MiMo-Embodied delivers exceptional performance, outperforming general large multimodal models and specialized autonomous driving models. Perception Capability Perception capability aims to evaluate the models ability to understand its surrounding environment and serves as the foundation for downstream planning tasks. This capability requires the model to accurately extract both semantic and geometric information from sensor inputs, thereby constructing comprehensive and structured representation of the driving scene. Different application scenarios impose varying requirements on the granularity of perception: panoramic perception emphasizes holistic scene understanding, including integrated analysis of foreground traffic participants, such as vehicles, pedestrians, and cyclists (e.g., language-guided scene understanding as evaluated in LingoQA [38]), as well as high-level modeling of road topology, lane layouts, and traffic semantics (e.g., map-level understanding assessed by MAPLM [7]). In contrast, local perception focuses on fine-grained identification and reasoning about specific regions or objects, such as detailed analysis of rare or high-risk scenarios in CODA-LM [9], accurate grounding and recognition of objects referred to by natural language descriptions in MME-RealWorld [68], and precise coordinate-level localization of critical objects within complex dynamic scenes as required by DRAMA [37]. This multi-granular perception capability, spanning from scene comprehension to object-level reasoning, forms the foundation for robust understanding in real-world autonomous driving. The complete suite of perception benchmarks evaluated includes CODA-LM [9], DriveAction [22], DRAMA [37], MME-RealWorld [68], IDKB [33], DriveLM [46], MAPLM [7], nuScenes-QA [41], LingoQA [38], and OmniDrive [53]. As shown in Table 4 and Table 5, MiMo-Embodied achieves strong performance across all perception 14 Model Info CODA-LM Drama MME-RealWorld IDKB OmniDrive NuInstruct Names Params PER. & PLA. PER. PER. & PRE. & PLA. PER. & PLA. PER. & PLA. PLA. MiMo-VL [59] InternVL3.5 [55] InternVL3.5 [55] Qwen2.5-VL [5] Qwen2.5-VL [5] GPT-4o [25] Gemini2.5-Pro [17] Qwen-VL-Max [4] Specialist Model DriveLMM-o1 [26] RoboTron-Drive [24] MiMo-Embodied (Ours) 7B 8B 38B 7B 72B 8B 8B 7B Open-Source Models 31.02 32.61 28.14 35.75 35.80 51. 0.08 0.00 54.32 0.00 54.05 49. 54.95 58.60 50.78 Closed-Source (Proprietary) Models 34.18 53. 37.72 0.00 0.24 0.16 58.00 67.00 61. Autonomous Driving Models 45.46 51.53 58.10* 68.40 0.00 0. 58.55 76.14 49.59 41.30 60. 18.90 19.66 18.96 13.44 18.41 20.65 23. 16.82 11.38 8.32 43.42 3. 12.81 16.43 10.07 7.32 19.22 10. 10.57 40.97 9.77 48.76* 45.21 0.68 0.00 0. 0.43 5.67 7.08 53.20 0.03 35. 2.88 83.00* 83.58 Table 4 Comparison of MiMo-Embodied with other models on four single-view image benchmarks and two multi-view video benchmarks in autonomous driving. We evaluate the model against various open-source, closed-source, and specialized autonomous driving VLMs to show comprehensive performance overview. Results marked with * are obtained using our evaluation framework. Specialist models correspond to the performance of different models [14, 37, 53, 66]. The best results among the listed models are bolded and the second-best is underlined. Here PER. denotes perception, PRE. denotes prediction, PLA. denotes planning. benchmarks, demonstrating state-of-the-art results in panoramic semantic understanding tasks while also exhibiting exceptional robustness in challenging local perception scenarios. These results provide compelling evidence that MiMo-Embodied possesses multi-level, high-fidelity environmental perception capabilities, enabling it to effectively adapt to diverse real-world perception demands across varying granularities. Prediction Capability Prediction capability assesses the models capacity to reason about the future evolution of dynamic driving scenes and is fundamental to enabling safe, efficient, and proactive autonomous driving. It requires the model to go beyond momentary perception by integrating current and historical observations to accurately infer the latent intentions. This future-oriented understanding directly informs critical downstream functions such as trajectory planning, collision avoidance, and interactive driving behaviors. The prediction task encompasses multiple facets. At the individual level, it involves forecasting behavioral intent. Examples include whether vehicle will change lanes or pedestrian intends to cross, and this is typically evaluated in benchmarks such as MME-RealWorld [68]. At the interaction level, it requires modeling dynamic relationships among agents, including behaviors like yielding, cooperative maneuvers, and implicit social conventions in multi-agent scenarios. These interaction-centric capabilities are explicitly assessed in tasks like DriveLM [46] under the framework of object interaction between traffic elements. Experimental results show that the model achieves strong performance on both the single image benchmark MME-RealWorld and the multi-view images benchmark DriveLM, accurately capturing individual behavioral intentions and effectively modeling complex interactions among multiple agents. Planning Capability Planning capability evaluates the models ability to synthesize perception and prediction outputs into safe, coherent, and context-aware driving actions. It directly determine how an autonomous system 15 Model Info DriveLM MAPLM nuScenes-QA LingoQA BDD-X DriveAction Names Params PER. & PRE. & PLA. PER. PER. PER. & PLA. PLA. PER. & PLA. MiMo-VL [59] InternVL3.5 [55] InternVL3.5 [55] Qwen2.5-VL [5] Qwen2.5-VL [5] GPT-4o [25] Gemini2.5-Pro [17] Qwen-VL-Max [4] Specialist Model DriveLMM-o1 [26] RoboTron-Drive [24] MiMo-Embodied (Ours) 7B 8B 38B 7B 72B 8B 8B 7B Open-Source Models 29.76 29.74 24.57 25.39 29. 30.95 14.24 23.55 24.76 45.94 33.94 17. 3.32 25.78 26.23 Closed-Source (Proprietary) Models 41.21 39.92 26. 26.64 26.12 24.83 34.26 16.12 6. Autonomous Driving Models 57.00 37.05 61.30* 57.85 71.76 25.51 74.34* 74. 53.40 16.76 21.15 56.71 54.80 46. 57.60 55.60 62.20 56.00 64.10 58.80 60. 47.80 69.20* 69.90 5.66 6.97 7.41 11.45 9. 12.38 4.80 7.53 48.61 3.43 12. 52.18 78.89 78.10 76.00 73.40 74.50 72. 73.53 72.60 45.89 58.87 80. Table 5 Comparison of MiMo-Embodied with other models on three multi-view image benchmarks and three single-view video benchmarks in autonomous driving. We evaluate the model against various open-source, closed-source, and specialized autonomous driving VLMs to show comprehensive performance overview. Results marked with * are obtained using our evaluation framework. Specialist models correspond to the performance of different models [7, 29, 38, 41, 65]. The best results among the listed models are bolded and the second-best is underlined. Here PER. denotes perception, PRE. denotes prediction, PLA. denotes planning. navigates complex, real-world traffic scenarios. Effective planning requires not only selecting appropriate low-level control commands including acceleration, braking, or turning but also reasoning about high-level driving strategies in alignment with traffic rules, social conventions, and dynamic scene context. The first action decision focuses on generating precise, goal-directed driving maneuvers in reaction to the evolving traffic environment. Benchmarks such as DriveLM [46], MME-RealWorld [68] and IDKB [33] evaluate this aspect by assessing the models ability to select appropriate driving actions in response to the current scene state. The second capability is driving reasoning, which provides explicit, interpretable justifications for those actions by grounding decisions in scene semantics and driving logic. This is assessed in benchmarks including LingoQA [38], CODA-LM [9], OmniDrive [53], NuInstruct [14] and BDD-X [31], which require the model not only to choose the correct action but also to explain why it is appropriate, for instance, yielding to an oncoming vehicle that is expected to proceed first, or stopping due to red traffic light ahead. The empirical results, summarized in Table 4 and Table 5, clearly demonstrate that MiMo-Embodied achieves outstanding performance across all planning-oriented benchmarks. This consistent superiority underscores the models strong capacity to not only generate accurate, context-appropriate driving decisions but also produce coherent and interpretable reasoning that aligns with real-world traffic logic and driving norms."
        },
        {
            "title": "5.2.1 MiMo-Embodied for Embodied Navigation & Manipulation",
            "content": "To validate MiMo-Embodieds practical utility in complex interactive settings, we evaluate its performance on two fundamental downstream applications: embodied navigation and manipulation. The evaluation assesses the models capacity to translate high-level instructions into precise spatial goals. For navigation, this involves predicting keypoints on map in global and egocentric views; for manipulation, it entails estimating 16 Figure 5 Results of deploying MiMo-Embodied to downstream embodied navigation tasks. The target positions are indicated by cyan points. functionally-grounded interaction points for object affordance. Our analysis first examines MiMo-Embodieds capacity for planning and reasoning from long-horizon instructions to generate precise target positions for execution, and then compares these predictions against baseline models to highlight MiMo-Embodieds superior performance in spatial reasoning and affordance understanding. Embodied Navigation We evaluate MiMo-Embodieds capabilities in downstream embodied navigation tasks using the long-horizon navigation task proposed in avA3 [67]. Given high-level human instruction (e.g., want to sleep or Water the plants in the study room), the model first needs to infer the spatial region to be explored and the target object to be located. Then, the model marks points on top-down map, indicating the area the robot needs to navigate to. Subsequently, it identifies and marks the target object in first-person perspective image, guiding the robot to successfully navigate to the target. We designed four distinct scenarios to evaluate our MiMo-Embodieds spatial reasoning and object localization capabilities. As shown in Figure 5, the visualization results demonstrate that MiMo-Embodied performs well in four household navigation tasks: locating the bed in the bedroom, finding the vacuum cleaner in the dining room, identifying plants in the study room, and locating the toilet in the bathroom. Each task requires the model to decompose the high-level instruction into spatial navigation (region identification on the map) and object localization (precise localization from an egocentric perspective), thus showcasing the models spatial understanding capabilities. To intuitively demonstrates the performance, we present comparative visualization of aforementioned navigation tasks across four multimodal models: GPT-4o [25], Qwen2.5-VL [5], RoboBrain-2.0 [49], and our proposed MiMo-Embodied. The visual comparisons in Figure 6 highlight MiMo-Embodieds enhanced 17 Figure 6 Visualization of different models for target object localization in embodied navigation tasks. The target positions are indicated by cyan points. object localization abilities and consistent performance under diverse household scenes. In single object localization tasks, both GPT-4o [25] and RoboBrain2.0 [49] tend to produce multiple scattered or clustered points that may deviates from the object center, indicating limitations in spatial precision. Our MiMoEmbodied model consistently achieves precise center localization, placing points directly at the core of target objects with remarkable accuracy. For plants localization, where the instruction implicitly requires recognizing multiple instances, MiMo-Embodied demonstrates the ability to comprehend the plural nature of the query, successfully identifying and accurately positioning both plant pots in the scene. In contrast, other models show some deficiencies in handling such compositional reasoning, either missing one instance or providing ambiguous localizations. These results highlight MiMo-Embodieds superior understanding of spatial relationships and linguistic nuances, particularly in complex scenarios requiring both precise object localization and comprehension of implicit semantic cues. The demonstrated capabilities provide solid foundation for downstream embodied navigation tasks where accurate target identification and spatial reasoning are crucial for successful task completion. Embodied Manipulation We evaluate MiMo-Embodieds manipulation capability through series of hierarchical pick-and-place tasks involving planning, affordance prediction, and spatial reasoning, as shown in Figure 7. The model is required to interpret high-level instructions and decompose them into sequence of precise 18 Figure 7 Results of deploying MiMo-Embodied to downstream embodied manipulation tasks. The target positions are indicated by cyan points. actions, including spatial reasoning for target localization and object affordance understanding for successful grasping and placement. The first task corresponds to the first two rows in Figure 7 assess the models ability in affordance prediction, such as distinguishing the lid of the pot and the handle of pink spoon for grasping and spatial locations for placement. The second task demonstrates the models advanced counting and spatial reasoning. Given the instruction Grasp the third orange from the left in the first row, place it between the oranges in the bottom row, the model must identify the correct orange using ordinal counting and then determine the target placement by interpreting the spatial relationship between. Another task involves placing the left bread in the specified plate, requiring estimation of height to identify the correct placement location given multiple plates. These results collectively highlight MiMo-Embodieds ability to integrate object affordance with sophisticated spatial and relational reasoning for embodied manipulation. We visualize the affordance predictions of GPT-4o [25], Qwen2.5-VL [5], RoboBrain-2.0 [49], and our proposed MiMo-Embodied in Figure 8. The evaluation focuses on three functionally-grounded tasks: identifying the graspable handle of pink spoon, locating an intermediate placement position between the oranges in the bottom row, and selecting the leftmost bread for pickup. These tasks assess the models capacity to interpret affordances and translate linguistic cues into spatially-grounded interaction points. The results reveal substantial differences in functional reasoning accuracy: both GPT-4o [25] and RoboBrain-2.0 [49] frequently generate points misaligned with functionally relevant regions, particularly struggling to identify graspable components and relational positions for object arrangement. While Qwen2.5-VL [5] demonstrates outstanding performance in object affordance, it shows limitations in handling spatial affordance in the second task. Notably, MiMo-Embodied consistently identifies functionally appropriate regions across all tasks. These findings underscore MiMo-Embodieds strong affordance and spatial reasoning capabilities, establishing reliable foundation for embodied manipulation tasks. 19 Figure 8 Visualization of different models for affordance prediction in embodied manipulation tasks. The target positions are indicated by cyan points. Model InternVL3 [70] MiMo-Embodied (Ours) ReCogDrive-Large-IL [32] MiMo-Embodied-IL (Ours) ReCogDrive-Large-RL [32] MiMo-Embodied+RL (Ours) Params Input Token NC DAC TTC Conf EP PDMS 8B 7B 8B 7B 8B 7B C 4096 796 2304 2304 796 97.4 97.9 98.1 98. 97.9 98.3 93.7 94.3 94.5 94. 97.3 98.1 93.2 93.8 94.2 94. 94.9 95.5 100 100 100 100 100 81.2 81.7 80.9 82. 86.9 86.3 86.0 86.5 86.5 87. 90.4 91.0 Table 6 We evaluate MiMo-Embodied on the NAVSIM [12] planning dataset and present comparison with the InternVL3 [70]. IL (Imitation Learning) denotes the setting where the VLM forwarding results are utilized as conditions for diffusion-based regression. RL (Reinforcement Learning) indicates the GRPO is further applied on top of IL stage. The best results are highlighted in bold."
        },
        {
            "title": "5.2.2 MiMo-Embodied for Autonomous Driving",
            "content": "To validate the efficacy of our training strategy, we evaluate our model on trajectory planning, core capability for autonomous driving systems. Our assessment is designed to be both fair and comprehensive, utilizing dual-pronged approach: we first establish performance on the challenging public NAVSIM benchmark [12] for standardized comparison, and then test the models capabilities on large-scale, proprietary dataset rich with diverse, real-world driving scenarios. Trajectory Planning on Public Benchmark NAVSIM [12] is public autonomous driving planning dataset widely used to evaluate the performance of planners. Given one single front-view image, the ego vehicles current state, and navigation command (e.g., go straight, turn left or turn right), the model is required to analyze the environment, perform reasoning, and finally generate 4-second future trajectory. To assess the trajectory planning capability of MiMo-Embodied, we provide quantitative comparisons in Table 6, where MiMo-Embodied is evaluated on the NAVSIM benchmark [12] against InternVL3-8B [70] and its derivative ReCogDrive [32] planner of comparable scale. To ensure fair comparison with counterparts, we adopt the denoising policy from ReCogDrive [32] during the imitation IL stage, where noisy trajectories are 20 Figure 9 Qualitative results of trajectory planning by MiMo-Embodied on the NAVSIM benchmark. alternately fused with VLM hidden states, ego states, and historical trajectories. During the RL stage, we employ DiffGRPO [32] to further enhance the models performance. Here, NC, DAC, TTC, Conf, EP denote No At-Fault Collision, Drivable Area Compliance, Time-to-Collision, Comfort and Ego Process, respectively. The PDMS (Predictive Driver Model Score) metric is the weighted combination of above sub-scores, with detailed computation procedures available in [12]. All models take single front-view image as input and are trained for the same number of epochs. MiMo-Embodied consistently outperforms the competing models. Notably, unlike InternVL3 [70] that discretizes single-frame image into patches, MiMo-Embodied employs 3D convolutions to significantly reduce the number of LLM tokens while preserving detailed spatial context. Furthermore, we showcase set of representative driving scenarios. As illustrated in Figure 9, the qualitative results demonstrate that MiMo-Embodied can handle diverse autonomous driving situations and accomplish challenging tasks, including intersection turning, U-turning on curved roads, car-following and lane-change overtaking. In each case, the model is expected to perceive the road context, integrate the ego status and navigation intent, and produce coherent decision. Trajectory Planning on Proprietary Data To validate the efficacy of our training strategy in practical autonomous driving contexts, we evaluate the models trajectory planning performance on proprietary, large-scale dataset. This dataset, gathered from diverse geographic locations under wide spectrum of traffic conditions, features comprehensive set of driving scenarios. Beyond standard maneuvers such as straight-line driving and turns, it is enriched with challenging, safety-critical situations that involving detouring around vulnerable road users (VRUs), overtaking or bypassing slow and parked vehicles, and other complex interactions. The demonstration trajectories are sourced from expert human drivers, selected for their exemplary safety records and ability to perform smooth, comfortable maneuvers. These drivers exhibit strict adherence to traffic regulations and proficient defensive driving skills, ensuring the dataset provides high-quality, near-optimal driving exemplars for imitation learning. The entire evaluation set is meticulously annotated with precise trajectories and scenario labels. 21 Figure 10 Relative performance improvement of MiMo-Embodied 7B over baseline method Qwen2.5-VL 7B [5] of trajectory planning on the proprietary dataset. Improvement is quantified as the percentage reduction in error metric, with higher bars indicating greater error reduction and better performance. In our experimental setup, we predict ego vehicles 3-second trajectory based on five consecutive front-view camera frames sampled at 2 Hz, with predictions represented in the ego-vehicles coordinate. To ensure fair and rigorous comparison, our proposed MiMo-Embodied and the Qwen2.5-VL [5] baseline are evaluated under identical settings, including the input representation, full fine-tuning training strategy and the optimization procedure. We assess performance in an open-loop evaluation protocol, using the L2 distance [23] over the 3-second prediction horizon as the primary metric. As illustrated in Figure 10, which depicts the relative performance over the baseline, MiMo-Embodied consistently outperforms the baseline across all evaluated categories. Notably, the performance gains are most pronounced in complex, interactive maneuvers such as turns, nudges around obstacles, and lane changes. These scenarios are inherently challenging as they require anticipating the behavior of other road users and precise vehicle control. This substantial improvement, particularly in high-complexity scenarios, provides strong evidence that our embodied training paradigm equips the model with superior capacity for reasoning in complex driving situations. It also translates to better generalization across unseen road conditions with universal driving logic. Consequently, the model generates trajectories that are more accurate and more aligned with expert human driving behavior."
        },
        {
            "title": "5.3 Ablation Study",
            "content": "To verify the effectiveness of our proposed multi-stage training strategy, we conducted systematic ablation experiments, with the results presented in Table 7. The experimental results show that the model trained using only embodied data (MiMo-VL w/ Embodied) achieves strong performance in both domains, whereas the model trained solely on autonomous driving data (MiMo-VL w/ AD) excels in its own domain but suffers from significant performance drop on embodied tasks. This indicates that simple single-task training is insufficient for cross-domain generalization. Under the experimental condition of ensuring approximately the same number of loss tokens, directly training embodied and autonomous driving tasks together (MiMoVL w/ Embodied+AD) achieves improvements on embodied tasks, but the performance of autonomous driving declines slightly. In contrast, our proposed MiMo-Embodied employs multi-stage training strategy, effectively mitigating cross-domain task interference through progressive course learning. It achieves an 22 Model Embodied AD Multi-Stage Affordance Spatial Plan Embodied Avg. Autonomous Driving MiMo-VL (Baseline) MiMo-VL w/ Embodied MiMo-VL w/ AD MiMo-VL w/ Embodied+AD MiMo-Embodied (Ours) (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) 38.7 58.9 26. 59.6 65.6 55.3 61.0 56.3 62. 66.0 46.2 51.0 47.0 53.8 55. 46.76 56.9 43.2 58.4 62.4 32. 57.6 57.5 55.2 63.3 Table 7 Ablation study of different model configurations. The (cid:33)indicates the feature is enabled, while the (cid:37)indicates it is disabled. Multi-stage denotes our proposed multi-stage training strategy. The best results are highlighted in bold. average performance of 62.4% on embodied tasks (4% improvement compared to Embodied+AD) and best performance of 63.3% on autonomous driving tasks (8.1% improvement compared to Embodied+AD). This fully demonstrates that our multi-stage training strategy can achieve synergistic improvement of embodied intelligence and autonomous driving capabilities without sacrificing the performance of single task, providing an effective training paradigm for building unified embodied foundation model."
        },
        {
            "title": "6 Conclusion and Next Steps",
            "content": "This report introduces MiMo-Embodied, pioneering cross-embodied vision-language model that achieves state-of-the-art performance in both autonomous driving and embodied AI tasks. As the first open-source VLM integrating these two critical domains, MiMo-Embodied significantly enhances understanding and reasoning in dynamic physical environments. Extensive evaluations across 29 benchmarks show that MiMo-Embodied achieves superior performance in both embodied and autonomous driving tasks, significantly outperforming existing open-source and closed-source general VLMs, as well as specialized VLMs for single domain. The report provides comprehensive overview of the design, data construction, training methodologies, and practical applications of MiMo-Embodied, aiming to inspire future research in the field. Building on the capabilities of our MiMo-Embodied model, we will explore Embodied AI Vision-Language Action (VLA) Models to enhance interaction in complex environments, enabling more intuitive task execution through natural language understanding. Additionally, we aim to develop Autonomous Driving VLA Models that allow these systems to interpret real-time traffic conditions and respond effectively to dynamic driving scenarios. Furthermore, we will investigate the integration of multi-modal learning, such as utilizing 3D point clouds, to improve long-term planning and decision-making capabilities, as well as advance human-robot interaction. These areas are crucial for enhancing overall performance and driving innovation in both embodied AI and autonomous driving technologies."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. Claude 3.7 sonnet and claude code. 2025. [2] Anthropic. Claude sonnet 4. 2025. [3] Alisson Azzolini, Junjie Bai, Hannah Brandon, Jiaxin Cao, Prithvijit Chattopadhyay, Huayu Chen, Jinju Chu, Yin Cui, Jenna Diamond, Yifan Ding, et al. Cosmos-reason1: From physical common sense to embodied reasoning. arXiv preprint arXiv:2503.15558, 2025. [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Qingwen Bu, Jisong Cai, Li Chen, Xiuqi Cui, Yan Ding, Siyuan Feng, Shenyuan Gao, Xindong He, Xuan Hu, Xu Huang, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems. arXiv preprint arXiv:2503.06669, 2025. [7] Xu Cao, Tong Zhou, Yunsheng Ma, Wenqian Ye, Can Cui, Kun Tang, Zhipeng Cao, Kaizhao Liang, Ziran Wang, James Rehg, et al. Maplm: real-world large-scale vision-language benchmark for map and traffic scene understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2181921830, 2024. [8] Jiahong Chen, Jing Wang, Long Chen, Chuwei Cai, and Jinghui Lu. Nanovla: Routing decoupled vision-language understanding for nano-sized generalist robotic policies. arXiv preprint arXiv:2510.25122, 2025. [9] Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, et al. Automated evaluation of large vision-language models on self-driving corner cases. In IEEE/CVF Winter Conference on Applications of Computer Vision, pages 78177826, 2025. [10] Yi Chen, Yuying Ge, Yixiao Ge, Mingyu Ding, Bohao Li, Rui Wang, Ruifeng Xu, Ying Shan, and Xihui Liu. Egoplan-bench: Benchmarking multimodal large language models for human-level planning. arXiv preprint arXiv:2312.06722, 2023. [11] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In Proceedings of the European conference on computer vision, pages 720736, 2018. [12] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, et al. Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking. Advances in Neural Information Processing Systems, 37:2870628719, 2024. [13] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for stateof-the-art vision-language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 91104, 2025. [14] Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. Holistic autonomous driving understanding by birds-eye-view injected multi-modal large models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1366813677, 2024. [15] Mengfei Du, Binhao Wu, Zejun Li, Xuanjing Huang, and Zhongyu Wei. Embspatial-bench: Benchmarking spatial understanding for embodied tasks with large vision-language models. arXiv preprint arXiv:2406.05756, 2024. [16] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. [17] Google. Gemini 2.5 pro preview: even better coding performance. https://developers.googleblog.com/en/ gemini-2-5-pro-io-improved-coding-performance/, 2025. Accessed: 2025-05-06. [18] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. 24 In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. [19] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [20] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [21] Xiaoshuai Hao, Yingbo Tang, Lingfeng Zhang, Yanbiao Ma, Yunfeng Diao, Ziyu Jia, Wenbo Ding, Hangjun Ye, and Long Chen. Roboafford++: generative ai-enhanced dataset for multimodal affordance learning in robotic manipulation and navigation. arXiv preprint arXiv:2511.12436, 2025. [22] Yuhan Hao, Zhengning Li, Lei Sun, Weilong Wang, Naixin Yi, Sheng Song, Caihong Qin, Mofan Zhou, Yifei Zhan, and Xianpeng Lang. Driveaction: benchmark for exploring human-like driving decisions in vla models. arXiv preprint arXiv:2506.05667, 2025. [23] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In European Conference on Computer Vision, pages 533549, 2022. [24] Zhijian Huang, Chengjian Feng, Feng Yan, Baihui Xiao, Zequn Jie, Yujie Zhong, Xiaodan Liang, and Lin Ma. Robotron-drive: All-in-one large multimodal model for autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 80118021, 2025. [25] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [26] Ayesha Ishaq, Jean Lahoud, Ketan More, Omkar Thawakar, Ritesh Thawkar, Dinura Dissanayake, Noor Ahsan, Yuhao Li, Fahad Shahbaz Khan, Hisham Cholakkal, et al. Drivelmm-o1: step-by-step reasoning dataset and large multimodal model for driving scenario understanding. arXiv preprint arXiv:2503.10621, 2025. [27] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 17241734, 2025. [28] Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. [29] Bu Jin and Haotian Liu. Adapt: Action-aware driving caption transformer. In CAAI International Conference on Artificial Intelligence, pages 473477, 2023. [30] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In European conference on computer vision, pages 235251, 2016. [31] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving vehicles. In Proceedings of the European conference on computer vision, pages 563578, 2018. [32] Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, et al. Recogdrive: reinforced cognitive framework for end-to-end autonomous driving. arXiv preprint arXiv:2506.08052, 2025. [33] Yuhang Lu, Yichen Yao, Jiadong Tu, Jiangnan Shao, Yuexin Ma, and Xinge Zhu. Can lvlms obtain drivers license? benchmark towards reliable agi for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 58385846, 2025. [34] Yuhao Lu, Yixuan Fan, Beixing Deng, Fangfu Liu, Yali Li, and Shengjin Wang. Vl-grasp: 6-dof interactive grasp policy for language-oriented objects in cluttered indoor scenes. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 976983, 2023. [35] Gen Luo, Ganlin Yang, Ziyang Gong, Guanzhou Chen, Haonan Duan, Erfei Cui, Ronglei Tong, Zhi Hou, Tianyi Zhang, Zhe Chen, et al. Visual embodied brain: Let multimodal large language models see, think, and control in spaces. arXiv preprint arXiv:2506.00123, 2025. 25 [36] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. arXiv preprint arXiv:2210.07474, 2022. [37] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. Drama: Joint risk localization and captioning in driving. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 10431052, 2023. [38] Ana-Maria Marcu, Long Chen, Jan Hnermann, Alice Karnsund, Benoit Hanotte, Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton, et al. Lingoqa: Visual question answering for autonomous driving. In European Conference on Computer Vision, pages 252269, 2024. [39] Austin Myers, Ching Teo, Cornelia Fermller, and Yiannis Aloimonos. Affordance detection of tool parts from geometric features. In IEEE International Conference on Robotics and Automation, pages 13741381, 2015. [40] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180, 2023. [41] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. Nuscenes-qa: multi-modal visual question answering benchmark for autonomous driving scenario. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 45424550, 2024. [42] Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, and Xihui Liu. Egoplan-bench2: benchmark for multimodal large language model planning in real-world scenarios. arXiv preprint arXiv:2412.04447, 2024. [43] Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, and Anh Totti Nguyen. Vision language models are blind: Failing to translate detailed visual features into words. arXiv preprint arXiv:2407.06581, 2024. [44] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [45] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil Joshi, et al. Robovqa: Multimodal long-horizon reasoning for robotics. In IEEE International Conference on Robotics and Automation, pages 645652, 2024. [46] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Jens Beiwenger, Ping Luo, Andreas Geiger, and Hongyang Li. Drivelm: Driving with graph visual question answering. In European conference on computer vision, pages 256274, 2024. [47] Chan Hee Song, Valts Blukis, Jonathan Tremblay, Stephen Tyree, Yu Su, and Stan Birchfield. Robospatial: Teaching spatial understanding to 2d and 3d vision-language models for robotics. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1576815780, 2025. [48] Yingbo Tang, Lingfeng Zhang, Shuyi Zhang, Yinuo Zhao, and Xiaoshuai Hao. Roboafford: dataset and benchmark for enhancing object and spatial affordance learning in robot manipulation. In Proceedings of ACM International Conference on Multimedia, pages 1270612713, 2025. [49] BAAI RoboBrain Team, Mingyu Cao, Huajie Tan, Yuheng Ji, Xiansheng Chen, Minglan Lin, Zhiyu Li, Zhou Cao, Pengwei Wang, Enshen Zhou, et al. Robobrain 2.0 technical report. arXiv preprint arXiv:2507.02029, 2025. [50] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini robotics: Bringing ai into the physical world. arXiv preprint arXiv:2503.20020, 2025. [51] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Adithya Jairam Vedagiri IYER, Sai Charitha Akula, Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [52] Homer Rich Walke, Kevin Black, Tony Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, et al. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning, pages 17231736, 2023. [53] Shihao Wang, Zhiding Yu, Xiaohui Jiang, Shiyi Lan, Min Shi, Nadine Chang, Jan Kautz, Ying Li, and Jose Alvarez. Omnidrive: holistic vision-language dataset for autonomous driving with counterfactual reasoning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2244222452, 2025. 26 [54] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, et al. The all-seeing project v2: Towards general relation comprehension of the open world. In European Conference on Computer Vision, pages 471490, 2024. [55] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [56] Weizhen Wang, Chenda Duan, Zhenghao Peng, Yuxin Liu, and Bolei Zhou. Embodied scene understanding for vision language models via metavqa. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2245322464, 2025. [57] Xin Wang, Taein Kwon, Mahdi Rad, Bowen Pan, Ishani Chakraborty, Sean Andrist, Dan Bohus, Ashley Feniello, Bugra Tekin, Felipe Vieira Frujeri, et al. Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2027020281, 2023. [58] Penghao Wu and Saining Xie. V*: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. [59] LLM-Core-Team Xiaomi. Mimo-vl technical report, 2025. URL https://arxiv.org/abs/2506.03569. [60] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal ai agents. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1420314214, 2025. [61] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [62] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. [63] Yifu Yuan, Haiqin Cui, Yibin Chen, Zibin Dong, Fei Ni, Longxin Kou, Jinyi Liu, Pengyi Li, Yan Zheng, and Jianye Hao. From seeing to doing: Bridging reasoning and decision for robotic manipulation. arXiv preprint arXiv:2505.08548, 2025. [64] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 1513415186, 2025. [65] Shuang Zeng, Xinyuan Chang, Mengwei Xie, Xinran Liu, Yifan Bai, Zheng Pan, Mu Xu, and Xing Wei. Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. arXiv preprint arXiv:2505.17685, 2025. [66] Enming Zhang, Xingyuan Dai, Min Huang, Yisheng Lv, and Qinghai Miao. Minidrive: More efficient vision-language models with multi-level 2d features as text tokens for autonomous driving. arXiv preprint arXiv:2409.07267, 2024. [67] Lingfeng Zhang, Xiaoshuai Hao, Yingbo Tang, Haoxiang Fu, Xinyu Zheng, Pengwei Wang, Zhongyuan Wang, Wenbo Ding, and Shanghang Zhang. nava3: Understanding any instruction, navigating anywhere, finding anything. arXiv preprint arXiv:2508.04598, 2025. [68] Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024. [69] Enshen Zhou, Jingkun An, Cheng Chi, Yi Han, Shanyu Rong, Chi Zhang, Pengwei Wang, Zhongyuan Wang, Tiejun Huang, Lu Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025. [70] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "Contributors",
            "content": "Xiaoshuai Hao Lei Zhou Zhijian Huang Zhiwen Hou Yingbo Tang Lingfeng Zhang Guang Li Zheng Lu Shuhuai Ren Fuli Luo Hangjun Ye Long Chen Xianhui Meng Yuchen Zhang Jing Wu Jinghui Lu Chenxu Dang Jiayi Guan Jianhua Wu Zhiyi Hou Hanbing Li Shumeng Xia Mingliang Zhou Yinan Zheng Zihao Yue Shuhao Gu Hao Tian Yuannan Shen Jianwei Cui Wen Zhang Shaoqing Xu Bing Wang Haiyang Sun Zeyu Zhu Yuncheng Jiang Zibin Guo Chuhong Gong Chaofan Zhang Wenbo Ding Kun Ma Guang Chen Rui Cai Diyun Xiang Heng Qu"
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to sincerely thank for the tremendous support from the broader team, including those not listed above: Naiyan Wang, Yongkang Li, Chitian Sun, Lin Liu, Feiyang Jia, Jie Wang, Haochen Tian, Yihang Qiu, Junli Wang, Yinfeng Gao. Project Leader"
        },
        {
            "title": "A Appendix",
            "content": "A.1 General Visual Understanding To verify that MiMo-Embodieds specialized training does not come at the cost of general proficiency, we evaluate it across range of general visual understanding benchmarks including MMMU-Pro (Standard and Vision) [64], Mantis [28], AI2D [30], V* [58], VLMs are Blind [43], PixmoCount [13], and CountBench [40]. The evaluation metrics for all benchmarks are based on accuracy. We compare MiMo-Embodied with its base model, MiMo-VL [59], as well as top-tier proprietary and open-source models including Qwen2.5-VL [5], InternVL3 [70], GPT-4o [25], and Claude 3.7 Sonnet [1]). Model Info General Visual Understanding Names Params MMMUProstandard MMMUProvision Mantis AI2D V* VLMs are Blind PixmoCount CountBench MiMo-VL [59] Qwen2.5-VL [5] InternVL3 [70] GPT-4o [25] Claude 3.7 Sonnet [1] MiMo-Embodied (Ours) Open-Source Models 42.37 34.70 45.60 35. 29.40 37.80 81.57 74.70 72.80 81.83 83. 85.20 81.68 73.80 72.80 Closed-Source (Proprietary) Models 42. 56.50 52.08 36.10 45.80 45.84 75. 75.10 82.60 81.40 81.10 84.20 73. 82.72 74.91 37.40 36.80 49.80 72. 72.32 7B 7B 8B 7B 73.35 60.70 62.00 54.40 53. 77.50 85.34 74.10 80.00 85.70 90. 87.37 Table 8 Comparison of MiMo-Embodied with other models on general visual understanding benchmarks. Results for Qwen2.5-VL [5], InternVL3 [70], GPT-4o [25], and Claude 3.7 Sonnet [1] are sourced from the MiMo-VL [59] technical report for consistent comparison. The best results among the listed models are bolded and the second-best is underlined. The comprehensive results in Table 8 reveal that after multi-stage embodied and autonomous driving finetuning, MiMo-Embodieds general capabilities remain intact. MiMo-Embodied preserves the proficiency of its base model MiMo-VL [59] while achieving notable improvements across some benchmarks. On complex reasoning tasks, MiMo-Embodied achieves 9.71 and 9.88 points absolute gain on MMMU-Prostandard and MMMU-Provision benchmarks [64], respectively. For visual question answering, MiMo-Embodied delivers competitive results with 81.10% on Mantis [28] and 84.20% on AI2D [30]. It also achieves the best performance on V* [58] benchmark. On general counting benchmarks, MiMo-Embodied excels MiMo-VL [59] with an improvement of 4.15 points on PixmoCount [13] and 2.03 points on CountBench [40]. The improvements on general visual understanding benchmarks indicate that the specialized training for MiMo-Embodied in embodied AI and autonomous driving yields benefits not only for target domains but also for fundamental visual comprehension. For instance, the gains in complex reasoning and counting likely stem from enhanced fine-grained object recognition and structural understanding, while spatial benchmarks benefit from improved dynamic relationship modeling and 3D spatial reasoning. This confirms that MiMo-Embodieds specialized training not only preserves but amplifies its general vision-language proficiency. 29 A.2 Embodied Visualization Examples A.2.1 Spatial Understanding Figure 11 Embodied spatial understanding example 1. 30 Figure 12 Embodied spatial understanding example 2. 31 Figure 13 Embodied spatial understanding example 3. 32 Figure 14 Embodied spatial understanding example 4. 33 Figure 15 Embodied spatial understanding example 5. 34 Figure 16 Embodied spatial understanding example 6. Figure 17 Embodied spatial understanding example 7. 36 Figure 18 Embodied spatial understanding example 8. 37 A.2.2 Affordance Prediction Figure 19 Embodied affordance prediction example 1. 38 Figure 20 Embodied affordance prediction example 2. 39 Figure 21 Embodied affordance prediction example 3. 40 Figure 22 Embodied affordance prediction example 4. 41 Figure 23 Embodied affordance prediction example 5. 42 Figure 24 Embodied affordance prediction example 6. 43 Figure 25 Embodied affordance prediction example 7. 44 Figure 26 Embodied affordance prediction example 8. 45 Figure 27 Embodied affordance prediction example 9. 46 Figure 28 Embodied affordance prediction example 10. 47 A.2.3 Planning Task Figure 29 Embodied planning task example 1. 48 Figure 30 Embodied planning task example 2. 49 Figure 31 Embodied planning task example 3. 50 Figure 32 Embodied planning task example 4. A.3 Autonomous Driving Visualization Examples A.3.1 Scene perception Figure 33 Autonomous driving scene perception example 1. 52 Figure 34 Autonomous driving scene perception example 2. 53 Figure 35 Autonomous driving scene perception example 3. 54 Figure 36 Autonomous driving scene perception example 4. 55 Figure 37 Autonomous driving scene perception example 5. 56 Figure 38 Autonomous driving scene perception example 6. 57 Figure 39 Autonomous driving scene perception example 7. 58 Figure 40 Autonomous driving scene perception example 8. A.3.2 Prediction ability Figure 41 Autonomous driving prediction ability example 1. 60 Figure 42 Autonomous driving prediction ability example 2. 61 Figure 43 Autonomous driving prediction ability example 3. A.3.3 Planning Task Figure 44 Autonomous driving planning task example 1. 63 Figure 45 Autonomous driving planning task example 2. 64 Figure 46 Autonomous driving planning task example 3. 65 Figure 47 Autonomous driving planning task example 4. Figure 48 Autonomous driving planning task example 5. 67 Figure 49 Autonomous driving planning task example 6."
        }
    ],
    "affiliations": [
        "Xiaomi"
    ]
}