{
    "paper_title": "MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization",
    "authors": [
        "Bhavya Sukhija",
        "Stelian Coros",
        "Andreas Krause",
        "Pieter Abbeel",
        "Carmelo Sferrazza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks."
        },
        {
            "title": "Start",
            "content": "Preprint, under review MAXINFORL: BOOSTING EXPLORATION IN REINFORCEMENT LEARNING THROUGH INFORMATION GAIN MAXIMIZATION ,1, Stelian Coros1 Andreas Krause1, Pieter Abbeel2, Carmelo Sferrazza2 Bhavya Sukhija ETH Zurich 1, UC Berkeley2 sukhijab, scoros, krausea { pabbeel, csferrazza { @ethz.ch } @berkeley.edu } 4 2 0 2 6 1 ] . [ 1 8 9 0 2 1 . 2 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce framework, MAXINFORL, for balancing intrinsic and extrinsic exploration. MAXINFORL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks. (a) Normalized of average MAXINFORL across several deep RL benchmarks on state-based tasks. performance (b) Normalized of average MAXINFODRQV2 on the humanoid visual control tasks (stand, walk, and run). performance Figure 1: We summarize the normalized performance of different variants of MAXINFORL; MAXINFOSAC for state-based control and MAXINFODRQV2 for visual control (cf., Section 4 for more details). We report the mean performance across five seeds with one standard error."
        },
        {
            "title": "INTRODUCTION",
            "content": "Reinforcement learning (RL) has found numerous applications in sequential decision-making problems, from games (Silver et al., 2017), robotics (Hwangbo et al., 2019; Brohan et al., 2023), to fine-tuning of large language models (Ouyang et al., 2022). However, most widely applied RL algorithms such as PPO (Schulman et al., 2017) are inherently sample-inefficient, requiring hundreds of hours of environment interactions for learning. Off-policy methods like SAC (Haarnoja et al., 2018), REDQ (Chen et al., 2021), and DroQ (Hiraoka et al., 2022) offer more sample and compute efficient alternative and have demonstrated success in real-world learning (Smith et al., Open-source implementations: https://sukhijab.github.io/projects/maxinforl/ 1 Preprint, under review 2022). Despite this, they often require dense reward signals and suffer in the presence of sparse rewards or local optima. This is primarily due to their use of naive exploration schemes such as ϵ-greedy or Boltzmann exploration and effectively take random sequences of actions for exploration. These strategies are known to be suboptimal even for basic tasks (Cesa-Bianchi et al., 2017; Burda et al., 2018; Sukhija et al., 2024b), yet remain popular due to their simplicity and scalability. Several works (Burda et al., 2018; Pathak et al., 2017; 2019; Sekar et al., 2020; Sukhija et al., 2024b) use intrinsic reward signals, e.g., curiosity or information gain, to improve the exploration of RL agents. Moreover, information gain (Lindley, 1956) is also widely applied in Bayesian experiment design (Chaloner & Verdinelli, 1995) and is the basis of many active learning methods (Krause et al., 2008; Balcan et al., 2010; Hanneke et al., 2014; Hubotter et al., 2024). In RL, exploration via maximizing information gain offers strong theoretical guarantees (Mania et al., 2020; Sukhija et al., 2024b) and achieves state-of-the-art empirical performance (Sekar et al., 2020; Mendonca et al., 2021). However, significant gap persists in both the theoretical and practical understanding of how to effectively balance intrinsic exploration objectives with naive extrinsic exploration algorithms. The goal of this work is to bridge this gap. To this end, we revisit the traditional, widely-used Boltzmann exploration, and enhance it by incorporating exploration bonuses derived from intrinsic rewards, like information gain. Our approach is grounded in both theoretical insights and practical motivations, and we empirically validate it across several deep RL benchmarks. The key contributions of this work are summarized as follows: Contributions 1. We propose MAXINFORL, novel class of off-policy model-free algorithms for continuous state-action spaces that augments existing RL methods with directed exploration. In essence, MAXINFORL builds on standard Boltzmann exploration and guides it via an intrinsic reward. We propose practical auto-tuning procedure that largely simplifies trading off extrinsic and intrinsic objectives. This yields algorithms that explore by visiting trajectories that achieve the maximum information gain about the underlying MDP, while efficiently solving the task. As result, MAXINFORL retains the simplicity of traditional RL methods while adding directed exploration through intrinsic rewards. Additionally, we show how the same idea can be combined with other naive exploration techniques, such as ϵgreedy. 2. In the simplified setting of stochastic multi-armed bandits in continuous spaces, we show that MAXINFORL has sublinear regret. In addition, we show that MAXINFORL benefits from all theoretical properties of contraction and convergence that hold for max-entropy RL algorithms such as SAC (Haarnoja et al., 2018). 3. In our experiments, we use an ensemble of dynamics models to estimate information gain and combine MAXINFORL with SAC (Haarnoja et al., 2018), REDQ (Chen et al., 2021), DrQ (Yarats et al., 2021), and DrQv2 (Yarats et al., 2022). We evaluate it on standard deep RL benchmarks for state and visual control tasks and show that MAXINFORL performs the best across all tasks and baselines, obtaining the highest performance also in challenging exploration problems (see., Fig. 1 for the average performance of MAXINFORL across several environments)."
        },
        {
            "title": "2 BACKGROUND",
            "content": "A core challenge in RL is deciding whether the agent should leverage its current knowledge to maximize rewards or try new actions in pursuit of better solutions. Striking this balance between explorationexploitation is critical. Here, we first introduce the problem setting, then we discuss two of the most commonly used exploration strategies in RL: ϵ-greedy and Boltzmann exploration. , , p, γ, r, ρ), where the state and action spaces are continuous, i.e., 2.1 PROBLEM SETTING We study an infinite-horizon Markov decision process (MDP, Puterman, 2014), defined by the tuple Rda , and the ( ) represents the probability density of the next state unknown transition kernel : st+1 and action at . At each step in the environment, the agent observes the state st, samples an action at from the policy π : st), [0, 2 rmax, 1 2 rmax]. The agents goal is to learn policy π and receives reward : [ that maximizes the γ discounted reward w.r.t. the initial state distribution s0 given the current state st S A ), at AS π(a Rds , [0, ρ. π = arg max J(π) = arg max π Π π Π 2 Es0,a0,... (cid:34) t=0 (cid:88) γtrt . (cid:35) (1) Preprint, under review In the following, we provide the definitions of the state-action value function Qπ and the value function π: Qπ(st, at) = Est+1,at+1 π,... (cid:34) (cid:88)l= γlrt+l (cid:35) , π(st) = Eat π,st+1,at+1 π,... (cid:34) (cid:88)l=0 γlrt+l . (cid:35) ϵGREEDY AND EXPLORATION"
        },
        {
            "title": "2.2\nThe ϵ-greedy strategy (Kearns & Singh, 2002; Mnih, 2013; Van Hasselt et al., 2016) is widely ap-\nplied in RL to balance exploration and exploitation, where the RL agent follows this simple decision\nrule below to select actions",
            "content": "at = (cid:40) Unif( arg max ) Q(st, a) A with probability ϵt else, (2) Here is the estimate of the optimal state-action value function. Therefore, at each step t, ) is sampled, else the greedy action at = Unif( with probability ϵt, random action at maxa Q(st, a) is picked. Lillicrap (2015); Fujimoto et al. (2018) extend this strategy to continuous state-action spaces, where deterministic policy πθ is learned to maximize the value function and combined with random Gaussian noise for exploration. 2.3 BOLTZMANN EXPLORATION Boltzmann exploration is the basis of many RL algorithms (Sutton, 2018; Szepesvari, 2022). The policy distribution π for Boltzmann is represented through s) π(a exp α 1Qπ(s, a) , (3) 1(s) in the definition for simplicity. As α where α is the temperature parameter that regulates exploration and Qπ is the soft-Q function. We (cid:0) 0, the policy greedily neglect the normalization term maximizes Qπ(s, a), i.e. it exploits, and as α , effectively performing uniform exploration. Intuitively, Boltzmann exploration can be interpreted as smoother alternative to ϵgreedy, with α serving similar role to ϵ in controlling the degree of exploration. Cesa-Bianchi et al. (2017) show that the standard Boltzmann exploration is suboptimal even in the simplest settings. They highlight that key shortcoming of Boltzmann exploration is that it does not reason about the uncertainty of its estimates. the policy adds equal mass to all actions in (cid:1) Overall, both ϵgreedy and Boltzmann exploration strategies are undirected. They fail to account for the agents lack of knowledge and do not encourage riskor knowledge-seeking behavior. The agent explores by sampling random action sequences, which leads to suboptimal performance, particularly in challenging exploration tasks with continuous state-action spaces. INTRINSIC EXPLORATION WITH INFORMATION GAIN 2.4 Intrinsic rewards or motivation are used to direct agents toward underexplored regions of the MDP. Hence they enable RL agents to acquire information in more principled manner as opposed to the aforementioned naive exploration methods. Effectively, the agent explores by selecting policies that maximize the γ-discounted intrinsic rewards. common choice for the intrinsic reward is the information gain (Cover & Thomas, 2006; Sekar et al., 2020; Mendonca et al., 2021; Sukhija et al., 2024b). Accordingly, for the remainder of the paper, we center our derivations around using information gain as the intrinsic reward. However, our approach is flexible and can also be combined with other intrinsic exploration objectives, such as RND (Burda et al. (2018), see Appendix D). We study non-linear dynamical system of the form st+1 = (st, at) + wt. (4) Here st+1 = [st+1, rt] represents the next state and reward, represents the unknown dynamics and reward function of the MDP and wt is the process noise, which we assume to be zero-mean i.i.d., σ2-Gaussian. Note this is very common representation of nonlinear systems with continuous state-action spaces (Khalil, 2015) and the basis of many RL algorithms (Pathak et al., 2019; Kakade et al., 2020; Curi et al., 2020; Mania et al., 2020; Wagenmaker et al., 2023; Sukhija et al., 2024a). Furthermore, it models all essential and unknown components of the underlying MDP; the transition kernel and the reward function. 3 Preprint, under review (si, ai, si) Approximating information gain Given dataset of transitions i=0, e.g., } replay buffer, we learn Bayesian model of the unknown function , to obtain posterior distribution p(f Dn) for . This distribution can be Gaussian, e.g., Gaussian process models (Rasmussen & Williams, 2005) or represented through Bayesian neural networks like probabilistic ensembles (Lakshminarayanan et al., 2017). As opposed to the typical model-based RL setting, similar to Burda et al. (2018); Pathak et al. (2017; 2019), our learned model is only used to determine the intrinsic reward. The information gain I(s; Dn), reflects the uncertainty about the unknown s, a, dynamics from observing the transition (s, a, s). Moreover, let σ(s, ds+1 denote the model epistemic uncertainty or disagreement of . Sukhija et al. (2024b, Lemma 1.) show that Dn) = [σj(s, a)]j Dn = { I(s; s, a, Dn) = H(s s, a, Dn) H(s s, a, , Dn) ds+1 j=1 (cid:88) σ2 log 1 + (cid:32) 1,j(st, at) σ2 (5) (cid:33) Iu(s,a) where denotes the (differential) entropy (Cover & Thomas, 2006) and in Eq. (5) the equality holds when p(f Dn) is Gaussian. Note that while the above is an upper bound, Sukhija et al. (2024b) motivate this choice from theoretical perspective proving convergence of the active learning algorithm for the model-based setting. In this work, similar to Pathak et al. (2019); Sekar et al. (2020); Mendonca et al. (2021); Sukhija et al. (2024b), we use the upper bound of the information gain for our practical algorithm. The upper bound has natural interpretation, since by picking actions at that maximize it, we effectively visit areas where we have high uncertainty about the unknown function , therefore performing exploration in both state and action space. Empirically, this approach has shown to perform well, e.g., Sekar et al. (2020); Mendonca et al. (2021). (cid:123)(cid:122) (cid:124) (cid:125) Data dependence of intrinsic rewards Information gain and other intrinsic rewards depend on the data Dn, making them inherently nonstationary and non-Markovian. Intuitively, underexplored areas of the MDP become less informative once visited (c.f., Prajapat et al. (2024) for more details). However, in RL, intrinsic rewards are often treated similarly to extrinsic rewards, simplification that works very well in practice (Burda et al., 2018; Sekar et al., 2020). We take similar approach s, a) from hereon for simplicity. in this paper and omit the dependence of on Dn and use I(s;"
        },
        {
            "title": "3 MAXINFORL",
            "content": "In this section, we present our method for combining intrinsic exploration with classical exploration strategies. While MAXINFORL builds directly on Boltzmann exploration, we begin by illustrating its key ideas in the context of an ϵgreedy strategy, due to its mathematical simplicity and natural distinction between exploration and exploitation steps. The insights gained from this serve as motivation for developing our main method: MAXINFORL with Boltzmann exploration algorithms, which we evaluate in Section 4. 3.1 MODIFYING ϵGREEDY FOR DIRECTED EXPLORATION We modify the ϵgreedy strategy from Section 2.2 and learn two critics, Qextrinsic and Qintrinsic, where Qextrinsic is the state-action value function of the extrinsic reward and Qintrinsic the critic of an intrinsic reward function rintrinsic, for instance, the information gain (see Eq. (5)). Unlike traditional ϵgreedy exploration, we leverage intrinsic rewards to guide exploration more effectively by selecting actions that maximize Qintrinsic, leading to more informed exploration rather than random sampling. At each step t, we pick greedy action that maximizes Qextrinsic with probability 1 ϵt, while for exploration, the action that maximizes the intrinsic critic is selected, i.e., at = maxa Qintrinsic(st, a). at = arg max arg max Qintrinsic(st, a) with probability ϵt Qextrinsic(st, a) else, (6) We call the resulting exploration strategy ϵMAXINFORL. This approach is motivated by the insight that in continuous spaces, intrinsic rewards cover the state-action spaces much more efficiently than undirected random exploration, making them more effective for exploration in general (Aubret et al., 2023; Sekar et al., 2020; Sukhija et al., 2024b). In Appendix A, to give theoretical intuition of our approach, we study ϵMAXINFORL in the simplified setting of multi-armed bandit (MAB). We show that as more episodes are played, it gets closer to the optimal solution, i.e. has sublinear-regret. 4 Preprint, under review The key takeaway from ϵMAXINFORL is that instead of exploring with actions that maximize entropy in the action spaces, e.g., uniform sampling, we select policies that also yield high information about the MDP during learning. In the following, we leverage this idea and modify the target distribution of Boltzmann exploration to incorporate intrinsic exploration bonuses. Moreover, ϵMAXINFORL has two practical drawbacks; (i) it requires training two actor-critics and (ii) practically, the probability ϵt is specified by the problem designer. We address both these limitations in the section below and present our main method."
        },
        {
            "title": "3.2 MAXINFORL WITH BOLTZMANN EXPLORATION\nIn Section 3.1, we modify ϵ–greedy to sample actions with high intrinsic rewards during exploration\ninstead of randomly picking actions. Motivated from the same principle, we augment the distribution\nof Boltzmann exploration with the intrinsic reward I(˜s′; f ∗",
            "content": "s, a) to get the following s) π(a exp α 1Qπ(s, a) + I(s; s, a) . (7) (cid:0) The resulting distribution encourages exploration w.r.t. information gain, with α playing similar role to ϵ in Eq. (6). Therefore, Eq. (7) can be viewed as soft formulation of Eq. (6). Effectively, instead of randomly sampling actions, for large values of the temperature, we pick actions that yield high information while maintaining the exploitative behavior for smaller temperatures. This distribution is closely related to the epistemic risk-seeking exponential utility function from Klearning (ODonoghue, 2021) and probabilistic inference in RL (Tarbouriech et al., 2024). As we show in the following, this choice of parameterization results in very intuitive objective for the policy. Given the previous policy πold and Qπold , akin to Haarnoja et al. (2018), we select the next policy πnew through the following optimization (cid:1) πnew = arg min DKL s) 1(s) exp Qπold (s, π( (cid:32) Ea π( s) π Π = arg max π Π (cid:13) (cid:13) (cid:13) Qπold (cid:13) (cid:13) Qπold (cid:104) 1 α (cid:18) α log(π(a ) + I(s; s, a) (cid:19)(cid:33) (s, a) s)) + αI(s; s, a) (cid:105) Ea Π s) π( (s, a) = arg max π here in the last line we used that Ea s) + s, a)] = H(a s)[ H(w). Hence, the policy πnew trades off maximizing H(s s, a, ) = H(s, s) the value function with the entropy of the states, rewards, and actions. This trade-off is regulated through the temperature parameter α. We provide different perspective to Eq. (8) from the lens of control as inference (Levine, 2018; Hafner et al., 2020) in Appendix C. s)) + I(s; log(π(a + αH(s, s), H(s a, s) (8) π( (cid:104) (cid:105) Separating exploration bonuses MAXINFORL has two exploration bonuses; (i) the policy entropy, and (ii) the information gain (Eq. (5)). The two terms are generally of different magnitude and tuning the temperature for the policy entropy is fairly well-studied in RL (Haarnoja et al., 2018). To this end, we modify Eq. (8) and introduce two individual temperature parameters α1 and α2 to separate the bonuses. Furthermore, since information gain does not have closed-form solution in general, akin to prior work (Sekar et al., 2020; Sukhija et al., 2024b), we use its upper bound Iu(s, a) (Eq. (5)) instead. πold (π πnew( s) = Ea s) = arg max π( s) (cid:104) πold Qπold (π s) π Π (s, a) α1 log(π(a s)) + α2Iu(s, a) (cid:105) (9) For α1, we can either use deterministic policy with α1 = 0 like Lillicrap (2015) or auto-tune α1 as suggested by Haarnoja et al. (2018). Notably, for α2 = 0 we get the standard max entropy RL methods (Haarnoja et al., 2018). Therefore, by introducing two separate temperatures, we can treat information gain as another exploration bonus in addition to the policy entropy and combine it with any RL algorithm. Auto-tuning the temperature for the information gain bonus Haarnoja et al. (2018) formulate the problem of soft-Q learning as constrained optimization. π [Qπ(s, a)] s.t., H(a s) := arg max Ea π( s) Π π := arg max π Π min 0 α1 Ea π Qπ(s, a) (cid:2) 5 α1(log(π(a s)) + H) . (cid:3) Preprint, under review The entropy coefficient is then auto-tuned by solving this optimization problem gradually via stochastic gradient descent (SGD). In similar spirit, we propose the following constraints to autotune the temperatures for the entropy and the information gain Π π := arg max π π( s) := arg max Ea π [Qπ(s, a)] s.t., H(a s) H, Ea π [Iu(s, a)] Iu(s) (10) Π Qπ(s, a) min α1,α2 Haarnoja et al. (2018) use simple heuristic = ) for the target entropy. However, we cannot specify general desired information gain since this depends on the learned Bayesian model p(f ). This makes choosing Iu task-dependent. For our experiments, we maintain target policy π, updated similarly to target critic in off-policy RL, and define Iu as s)) + H) + α2(Iu(s, a) α1(log(π(a Iu(s)) dim( (cid:2) (cid:3) Iu(s) := ds+1 j=1 (cid:88) Ea π( s) log 1 + σ 2σ2 1,j(s, a) (11) (cid:2) (cid:0) (cid:1)(cid:3) Intuitively, the constraint enforces that the current policy π explores w.r.t. the information gain, at least as much as the target policy π. In principle, any other constraint can be used to optimize for α2. We consider our constraint since (i) it is easy to evaluate, (ii) it can be combined with other intrinsic rewards1, and (iii) it is modular, i.e., it can be added to any RL algorithm. Moreover, as MAXINFORL can be combined with any base off-policy RL algorithm such as SAC (Haarnoja et al., 2018) or DDPG (Lillicrap, 2015), it benefits from the simplicity and scalability of these methods. In addition, it introduces the information gain as directed exploration bonus and automatically tunes its temperature similar to the policy entropy in Haarnoja et al. (2018). Therefore, it benefits from both the strengths of the naive extrinsic exploration methods and the directedness of intrinsic exploration. We demonstrate this in our experiments, where we combine MAXINFORL with SAC (Haarnoja et al., 2018), REDQ (Chen et al., 2021), DrQ (Yarats et al., 2021), and DrQv2 (Yarats et al., 2022). Convergence of MAXINFORL In the following, we study our modified Boltzmann exploration strategy and show that as in Haarnoja et al. (2018), the update rules for function and the policy Π. We make very general assumption that the entropy of the converge to an optimal policy π policy and the model epistemic uncertainty are all bounded for all (s, a) . The proof of the theorem and the related lemmas are provided in Appendix B. We define the Bellman operator π πQ(s, a) = r(s, a) + γEs s,a[V π(s)], where is the soft-value function. π(s) = Ea π( s)[Q(s, a) α1 log(π(a s)) + α2Iu(s, a)] (12) (13) Π, Theorem 3.1 (MAXINFORL soft learning). Assume that the reward, the entropy for all π and the model epistemic uncertainty σn are all bounded for all . The repeated application of soft policy evaluation (Eq. (12)) and soft policy update (Eq. (9)) to any π (s, a) for all π Theorem 3.1 shows that our reformulated expression for Boltzmann exploration exhibits the same convergence properties from Haarnoja et al. (2018). . Π such that Qπ(s, a) Π converges to π Π, (s, a) 0, (s, a) Qπ"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate MAXINFORL with Boltzmann exploration from Section 3.2 across several deep RL benchmarks (Brockman, 2016; Tassa et al., 2018; Sferrazza et al., 2024) on state-based and visual control tasks. In all our experiments, we report the mean performance with standard error evaluated over five seeds. For the state-based tasks we combine MAXINFORL with SAC (Haarnoja et al., 2018) and for the visual control tasks with DrQ (Yarats et al., 2021) and DrQv2 (Yarats et al., In the following, we refer to these algorithms as MAXINFOSAC, MAXINFODRQ, and 2022). MAXINFODRQV2, respectively. To further demonstrate the generality of MAXINFORL, in Appendix D, we provide additional experiments, where we combine MAXINFORL with REDQ (Chen et al., 2021), OAC (Ciosek et al., 2019), DrM (Xu et al., 2024), use RND (Burda et al., 2018) as 1I could represent different intrinsic reward function, e.g., RND Burda et al. (2018). 6 Preprint, under review an intrinsic reward instead of the information gain, and also evaluate the ϵMAXINFORL from Section 3.1 with curiosity (Pathak et al., 2017) and disagreement (Pathak et al., 2019; Sekar et al., 2020; Mendonca et al., 2021) as intrinsic rewards. Baselines: For the state-based tasks, in addition to SAC, we consider four baselines, all of which use SAC as the base RL algorithm: 1. DISAGREEMENT: Employs an explore then exploit strategy, where it maximizes only the intrinsic reward for the first 25% of environment interaction and then switches to the exploitation phase where the extrinsic reward is maximized. We use disagreement in the forward dynamics model for the intrinsic reward. 2. CURIOSITY: The same as DISAGREEMENT but with curiosity as the intrinsic reward. 3. SACINTRINSIC: Based on Burda et al. (2018), where normalized intrinsic reward is added to the extrinsic reward. 4. SACEIPO: Uses weighted sum of intrinsic and extrinsic rewards, where the weight is tuned with the extrinsic optimality constraint from Chen et al. (2022). For the visual control tasks, we use DrQ and DrQv2 as our baselines. More details on our baselines and experiment details are provided in Appendix E. Does MAXINFOSAC achieve better performance on state-based control problems? In Figure 2 we compare MAXINFOSAC with the baselines across several tasks of varying dimensionality2 from the DeepMind control (DMC, Tassa et al., 2018) and OpenAI gym benchmark suite (Brockman, 2016). Across all tasks we observe that MAXINFOSAC consistently performs the best. While the other baselines perform on par with MAXINFOSAC on some tasks, they fail to solve others. On the contrary, MAXINFOSAC consistently achieves the highest performance in all environments (cf., Fig. 1). To further demonstrate the scalability of MAXINFOSAC, we evaluate it on practical robotics benchmark, namely HumanoidBench (Sferrazza et al., 2024) that features simulated Unitree H1 robot on variety of tasks. We use the no-hand version of the benchmark, and evaluate our algorithm on the stand, walk, and run tasks. We compare MAXINFOSAC to SAC in Fig. 3 and again observe that MAXINFOSAC achieves overall higher performance, except for minor convergence delay on the stand task, which is trivial exploration problem concerning pure stabilization. Figure 2: Learning curves of all methods on several environments from the OpenAI gym and DMC suite benchmarks. Does MAXINFOSAC solve hard exploration problems? Naive exploration techniques often struggle with challeging exploration tasks (Burda et al., 2018; Curi et al., 2020; Sukhija et al., 2024b). To test MAXINFOSAC in this context, similar to Curi et al. (2020), we modify the reward in Pendulum, CartPole, and Walker by adding an action cost, raction(a) = 2, where controls the penalty for large actions. Curi et al. (2020) empirically show that even for small values, naive exploration methods fail, converging to the sub-optimal solution of applying no actions. 2including the humanoid from DMC: ds = 67, da = 21 7 Preprint, under review Figure 3: Performance of MAXINFOSAC and SAC on the HumanoidBench benchmark. In Figure 5 we compare MAXINFOSAC with the baselines. We observe that SAC struggles with action costs, especially in CartPole and Walker. Both SACEIPO and SACINTRINSIC underperform, likely due to poor handling of extrinsic and intrinsic rewards. Specifically, SACEIPO quickly reduces its intrinsic reward weight to zero (cf., Fig. 8 in Appendix D), making it overly greedy. Disagreement and curiosity-based methods perform better since we manually tune their number of intrinsic exploration interactions. However, MAXINFOSAC achieves the best performance by automatically balancing intrinsic and extrinsic rewards. For MAXINFOSAC and SAC, we also depict the phase plot from the exploration on the pendulum environment in Figure 4. MAXINFOSAC covers the state space much faster than SAC, effectively solving the Pendulum swing-up task (Target = (0, 0)) within 10K environment steps. Figure 4: Phase plots during learning of MAXINFORL and SAC on the Pendulum environment. MAXINFOSAC covers the state space much faster and effectively solves the swing-up task within 10K environment interactions. Figure 5: Learning curves for state-based tasks for different values of the action cost parameter K. 8 Preprint, under review Does MAXINFORL scale to visual control tasks? In this section, we combine MAXINFORL with Figure 6: Learning curves from visual control tasks of the DMC suite. DrQ and evaluate it on visual control problems from the DeepMind control suite (DMC, Tassa et al., 2018). DrQ is visual control algorithm based on the max entropy framework from SAC, therefore, it can easily be combined with MAXINFORL. We call the resulting algorithm MAXINFODRQ. We compare MAXINFODRQ with DrQ and DrQv2 in Figure 6. From the figure, we conclude that MAXINFODRQ consistently reaches higher rewards and better sample efficiency than the baselines across all tasks. This illustrates the scalability and generality of MAXINFORL. Solving challenging visual control tasks with MAXINFORL For challenging visual control problems, in particular, the humanoid tasks from DMC, Yarats et al. (2022) propose DrQv2, modified version of DrQ, which uses nstep returns and DDPG with noise scheduling instead of SAC for the base algorithm. They claim that switching to DDPG with noise scheduling is particularly useful for solving the humanoid tasks. To this end, we combine MAXINFORL with DrQv2 (MAXINFODRQV2) and evaluate it on the stand, walk, and run humanoid tasks from DMC. This demonstrates the flexibility of MAXINFORL, as it can seamlessly be combined with DrQv2, cf., Appendix for more details and Appendix for the performance of MAXINFODRQV2 on other DMC tasks. We compare MAXINFODRQV2 with DrQv2 in Figure 7. MAXINFODRQV2 results in substantial gains in performance and sample efficiency compared to DrQv2. To our knowledge, those shown in Figure 7 are the highest returns reached in these challenging visual control tasks by modelfree RL algorithms in the literature. This further illustrates the advantage of directed exploration via information gain/intrinsic rewards. Figure 7: Learning curves from the visual control humanoid tasks of the DMC suite."
        },
        {
            "title": "5 RELATED WORKS",
            "content": "Naive Exploration Naive exploration approaches such as ϵgreedy or Boltzmann are widely applied in RL due to their simplicity (Mnih, 2013; Schulman et al., 2017; Lillicrap, 2015; Haarnoja et al., 2018; Hafner et al., 2023). In particular, the maximum entropy framework (Ziebart et al., 2008) is the basis of many sample-efficient model-free deep RL algorithms (Haarnoja et al., 2018; Chen et al., 2021; Hiraoka et al., 2022; Yarats et al., 2021). However, these methods often perform suboptimally, especially in challenging exploration problems such as those with sparse rewards or local optima (cf., Section 4). Effectively, the agent explores the underlying MDP by taking random sequences of actions. In continuous spaces, this makes sufficiently covering the state and action space exceptionally challenging. Moreover, even in the simplest setting of MAB in continuous 9 Preprint, under review spaces, the most common and theoretically sound exploration strategies are Thompson sampling (TS), upper-confidence bound (UCB) (Srinivas et al., 2012; Chowdhury & Gopalan, 2017) and information-directed sampling (Russo & Van Roy, 2018; Kirschner, 2021). There are several RL algorithms based on these strategies (Brafman & Tennenholtz, 2002; Jaksch et al., 2010; Ouyang et al., 2017; Nikolov et al., 2019; Ciosek et al., 2019; Kakade et al., 2020; Curi et al., 2020; Sukhija et al., 2024a). Similarly, there are methods from Bayesian RL (Osband et al., 2018; Fellows et al., 2021; Buening et al., 2023) that perform principled exploration. However, the naive exploration approaches remain ubiquitous in deep RL due to their simplicity. Instead, intrinsic rewards are often used to facilitate more directed exploration. However, how to combine extrinsic and intrinsic exploration is much less understood both theoretically and practically. Intrinsic exploration Several works use intrinsic rewards as surrogate objective to facilitate exploration in challenging tasks (cf., Aubret et al., 2023, for comprehensive survey). Common choices of intrinsic rewards are model prediction error or Curiosity (Schmidhuber, 1991; Pathak et al., 2017; Burda et al., 2018), novelty of transitions/state-visitation counts (Stadie et al., 2015; Bellemare et al., 2016), diversity of skills/goals (Eysenbach et al., 2018; Sharma et al., 2019; Nair et al., 2018; Pong et al., 2019), empowerment (Klyubin et al., 2005; Salge et al., 2014), stateentropy (Mutti et al., 2021; Seo et al., 2021; Kim et al., 2024), and information gain over the forward dynamics (Sekar et al., 2020; Mendonca et al., 2021; Sukhija et al., 2024b). In this work, we focus on the information gain since it is the basis of many theoretically sound and empirically strong active learning methods (Krause et al., 2008; Balcan et al., 2010; Hanneke et al., 2014; Mania et al., 2020; Sekar et al., 2020; Sukhija et al., 2024b; Hubotter et al., 2024). Furthermore, we also motivate the choice of information gain/model epistemic uncertainty theoretically (cf., Theorem A.1 and Appendix C). Nonetheless, MAXINFORL can also be used with other intrinsic exploration objectives. In this work, we study how to combine the intrinsic rewards with the extrinsic ones for exploration in RL. Moreover, approaches such as Pathak et al. (2017; 2019); Sukhija et al. (2024b); Sekar et al. (2020) execute an explore then exploit strategy, where initially the extrinsic reward is ignored and policy is trained to maximize the intrinsic objective. After this initial exploration phase, the agent is trained to maximize the extrinsic reward. This strategy is common in active learning methods and does not result in sublinear cumulative regret even for the simple MAB setting. This is because the agent executes finite number of exploration steps instead of continuously trading off exploration and exploitation. To this end, Burda et al. (2018); Chen et al. (2022) combine the extrinsic and intrinsic exploration rewards by taking weighted sum rtot = λrextrinsic + rintrinsic. While Burda et al. (2018) use fixed weight λ, Chen et al. (2022) propose the extrinsic optimality constraint to auto-tune the intrinsic reward weight. The extrinsic optimality constraint enforces that the policy maximizing the sum of intrinsic and extrinsic rewards performs at least as well as the policy which only maximizes the extrinsic ones. Keeping fixed weight λ across all tasks and environment steps might be suboptimal and the constraint from Chen et al. (2022) may quickly lead to the trivial greedy solution 1 = 0 (cf., Fig. 8 in Appendix D). We compare MAXINFORL with these two methods, as well as λ with explore then exploit strategies. We show that MAXINFORL outperforms them across several deep RL benchmarks (cf., Section 4)."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we introduced MAXINFORL, class of model-free off-policy RL algorithms that train an agent to trade-off reward maximization with an exploration objective that targets high entropy in the state, action, and reward spaces. The proposed approach is theoretically sound, simple to implement, and can be combined with most common RL algorithms. MAXINFORL consistently outperforms its baselines in multitude of benchmarks and achieves state-of-the-art performance on challenging visual control tasks. limitation of MAXINFORL is that it requires training an ensemble of forward dynamics models to compute the information gain, which increases computation overhead (c.f. Appendix Table 1). In addition, while effective, the constraint in Equation (10) also requires maintaining target policy. The generality of the MAXINFORL exploration bonus and the trade-off we propose with reward maximization are not limited to the off-policy model-free setting, which we focus on here due to their sample and computational efficiency. Future work will investigate its applicability to other classes of RL algorithms, such as model-based RL, where the forward dynamics model is generally part of the training framework. In fact, in this work we only use the forward dynamics model for the intrinsic rewards. Another interesting direction is to include samples from the learned model in the policy training, which may yield additional gains in sample efficiency. Lastly, extending our theoretical guarantees from the bandit settings to the MDP case is also an interesting direction for future work. 10 Preprint, under review ACKNOWLEDGMENTS The authors would like to thank Kevin Zakka for the helpful discussions. This project was supported in part by the ONR Science of Autonomy Program N000142212121, the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, the Microsoft Swiss Joint Research Center, the SNSF Postdoc Mobility Fellowship 211086, and an ONR DURIP grant. REFERENCES Rika Antonova, Akshara Rai, and Christopher G. Atkeson. Deep kernels for optimizing locomotion controllers. In CORL, pp. 4756, 2017. Arthur Aubret, Laetitia Matignon, and Salima Hassas. An information-theoretic perspective on intrinsic motivation in reinforcement learning: survey. Entropy, 2023. Maria-Florina Balcan, Steve Hanneke, and Jennifer Wortman Vaughan. The true sample complexity of active learning. Machine learning, 80:111139, 2010. Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. NeurIPS, 2016. Felix Berkenkamp, Andreas Krause, and Angela P. Schoellig. Bayesian optimization with safety constraints: safe and automatic parameter tuning in robotics. Machine Learning, 2021. Ronen Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. Journal of Machine Learning Research, 2002. Brockman. Openai gym. arXiv preprint arXiv:1606.01540, 2016. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2023. Thomas Kleine Buening, Christos Dimitrakakis, Hannes Eriksson, Divya Grover, and Emilio Jorge. Minimax-bayes reinforcement learning. In AISTATS, 2023. Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018. Roberto Calandra, Andre Seyfarth, Jan Peters, and Marc Deisenroth. Bayesian optimization for learning gaits under uncertainty. Annals of Mathematics and Artificial Intelligence, 2016. Nicol`o Cesa-Bianchi, Claudio Gentile, Gabor Lugosi, and Gergely Neu. Boltzmann exploration done right. Advances in neural information processing systems, 30, 2017. Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: review. Statistical science, 1995. Eric Chen, Zhang-Wei Hong, Joni Pajarinen, and Pulkit Agrawal. Redeeming intrinsic rewards via constrained optimization. NeurIPS, 2022. Xinyue Chen, Che Wang, Zijian Zhou, and Keith Ross. Randomized ensembled double q-learning: Learning fast without model. ICLR, 2021. Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In ICML, 2017. Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic actor critic. NeurIPS, 32, 2019. Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing). Wiley-Interscience, 2006. 11 Preprint, under review Sebastian Curi, Felix Berkenkamp, and Andreas Krause. Efficient model-based reinforcement learning through optimistic policy search and planning. NeurIPS, 33:1415614170, 2020. Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without reward function. arXiv preprint arXiv:1802.06070, 2018. Mattie Fellows, Kristian Hartikainen, and Shimon Whiteson. Bayesian bellman operators. NeurIPS, 2021. Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In ICML, 2018. Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018. Danijar Hafner, Pedro Ortega, Jimmy Ba, Thomas Parr, Karl Friston, and Nicolas Heess. Action and perception as divergence minimization. arXiv preprint arXiv:2009.01791, 2020. Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. arXiv preprint arXiv:2301.04104, 2023. Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends in Machine Learning, 2014. Takuya Hiraoka, Takahisa Imagawa, Taisei Hashimoto, Takashi Onishi, and Yoshimasa Tsuruoka. Dropout q-functions for doubly efficient reinforcement learning. ICLR, 2022. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. Science Robotics, 2019. Jonas Hubotter, Bhavya Sukhija, Lenart Treven, Yarden As, and Andreas Krause. Transductive active learning: Theory and applications, 2024. Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 2010. Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun. Information theoretic regret bounds for online nonlinear control. NeurIPS, 33:1531215325, 2020. Parnian Kassraie, Nicolas Emmenegger, Andreas Krause, and Aldo Pacchiano. Anytime model selection in linear bandits. Advances in Neural Information Processing Systems, 36, 2024. Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. Machine learning, 2002. Hassan Khalil. Nonlinear control, volume 406. Pearson New York, 2015. Dongyoung Kim, Jinwoo Shin, Pieter Abbeel, and Younggyo Seo. Accelerating reinforcement learning with value-conditional state entropy exploration. Advances in Neural Information Processing Systems, 36, 2024. Johannes Kirschner. Information-Directed Sampling-Frequentist Analysis and Applications. PhD thesis, ETH Zurich, 2021. Alexander Klyubin, Daniel Polani, and Chrystopher Nehaniv. Empowerment: universal agent-centric measure of control. In IEEE congress on evolutionary computation, 2005. Ilya Kostrikov. JAXRL: Implementations of Reinforcement Learning algorithms in JAX, 10 2021. URL https://github.com/ikostrikov/jaxrl. Andreas Krause, Ajit Singh, and Carlos Guestrin. Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies. Journal of Machine Learning Research, 9(2), 2008. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. NeurIPS, 30, 2017. 12 Preprint, under review Tor Lattimore and Csaba Szepesvari. Bandit algorithms. Cambridge University Press, 2020. Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018. TP Lillicrap."
        },
        {
            "title": "Continuous control with deep reinforcement",
            "content": "learning. arXiv preprint arXiv:1509.02971, 2015. D. V. Lindley. On Measure of the Information Provided by an Experiment. The Annals of Mathematical Statistics, pp. 986 1005, 1956. Horia Mania, Michael Jordan, and Benjamin Recht. Active learning for nonlinear system identification with guarantees. arXiv preprint arXiv:2006.10277, 2020. Alonso Marco, Philipp Hennig, Jeannette Bohg, Stefan Schaal, and Sebastian Trimpe. Automatic LQR tuning based on Gaussian process global optimization. In ICRA, pp. 270277, 2016. Russell Mendonca, Oleh Rybkin, Kostas Daniilidis, Danijar Hafner, and Deepak Pathak. Discovering and achieving goals via world models. NeurIPS, 2021. Volodymyr Mnih. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. Mirco Mutti, Lorenzo Pratissoli, and Marcello Restelli. Task-agnostic exploration via policy gradient of non-parametric state entropy estimate. In AAAI, 2021. Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. NeurIPS, 2018. Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Informationdirected exploration for deep reinforcement learning. ICLR, 2019. Brendan ODonoghue. Variational bayesian reinforcement learning with regret bounds. NeurIPs, 2021. Ian Osband, John Aslanides, and Albin Cassirer. Randomized prior functions for deep reinforcement learning. NeurIPS, 2018. Ian Osband, Zheng Wen, Seyed Mohammad Asghari, Vikranth Dwaracherla, Morteza Ibrahimi, Xiuyuan Lu, and Benjamin Van Roy. Epistemic neural networks. Advances in Neural Information Processing Systems, 36:27952823, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. NeurIPS, 2022. Yi Ouyang, Mukul Gagrani, Ashutosh Nayyar, and Rahul Jain. Learning unknown markov decision processes: thompson sampling approach. NeurIPS, 30, 2017. Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International conference on machine learning, pp. 27782787. PMLR, 2017. Deepak Pathak, Dhiraj Gandhi, and Abhinav Gupta. Self-supervised exploration via disagreement. In ICML, pp. 50625071. PMLR, 2019. Vitchyr H. Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skewfit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019. Manish Prajapat, Mojmir Mutny, Melanie Zeilinger, and Andreas Krause. Submodular reinforcement learning. In ICLR, 2024. Martin Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons, 2014. Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann. Stable-baselines3: Reliable reinforcement learning implementations. Journal of Machine Learning Research, 2021. 13 Preprint, under review Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005. Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed sampling. Operations Research, 2018. Christoph Salge, Cornelius Glackin, and Daniel Polani. Empowermentan introduction. Guided Self-Organization: Inception, 2014. Jurgen Schmidhuber. possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pp. 222227, 1991. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. Planning to explore via self-supervised world models. In International conference on machine learning, 2020. Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient exploration. In ICML, 2021. Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, and Pieter Abbeel. HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation. In RSS, 2024. Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-aware unsupervised discovery of skills. arXiv preprint arXiv:1907.01657, 2019. David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 2017. Laura Smith, Ilya Kostrikov, and Sergey Levine. walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning. arXiv preprint arXiv:2208.07860, 2022. Niranjan Srinivas, Andreas Krause, Sham M. Kakade, and Matthias W. Seeger. Informationtheoretic regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on Information Theory, 2012. Bradly Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015. Bhavya Sukhija, Matteo Turchetta, David Lindner, Andreas Krause, Sebastian Trimpe, and Dominik Baumann. Gosafeopt: Scalable safe exploration for global optimization of dynamical systems. Artificial Intelligence, 2023. Bhavya Sukhija, Lenart Treven, Florian Dorfler, Stelian Coros, and Andreas Krause. Neorl: Efficient exploration for nonepisodic rl. arXiv preprint arXiv:2406.01175, 2024a. Bhavya Sukhija, Lenart Treven, Cansu Sancaktar, Sebastian Blaes, Stelian Coros, and Andreas Krause. Optimistic active exploration of dynamical systems. NeurIPS, 2024b. Richard Sutton. Reinforcement learning: An introduction. Bradford Book, 2018. Csaba Szepesvari. Algorithms for reinforcement learning. Springer nature, 2022. Jean Tarbouriech, Tor Lattimore, and Brendan ODonoghue. Probabilistic inference in reinforcement learning done right. NeurIPs, 2024. Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint arXiv:1801.00690, 2018. Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds in gaussian process bandits. In AISTATS, 2021. 14 Preprint, under review Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In AAAI, 2016. Andrew Wagenmaker, Guanya Shi, and Kevin Jamieson. Optimal exploration for model-based rl in nonlinear systems. arXiv preprint arXiv:2306.09210, 2023. Guowei Xu, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, Xiaoyu Liu, Jiaxin Yuan, Pu Hua, et al. Drm: Mastering visual reinforcement learning through dormant ratio minimization. ICLR, 2024. Denis Yarats, Ilya Kostrikov, and Rob Fergus. Image augmentation is all you need: Regularizing deep reinforcement learning from pixels. In ICLR, 2021. Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. Mastering visual continuous control: Improved data-augmented reinforcement learning. ICLR, 2022. Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In AAAI, 2008. 15 Preprint, under review ANALYZING ϵGREEDY FOR MULTI-ARMED BANDITS In this section, we enunciate theorem that shows that our modified ϵgreedy approach from Section 3.1 has sublinear regret in the simplified setting of multi-arm bandits. 1 A.1 SUBLINEAR REGRET FOR ϵMAXINFORL This explorationexploitation trade-off is also fundamental in multi-armed bandits (MAB) (Lattimore & Szepesvari, 2020), where the task is to optimize an unknown objective function Rd being compact set over which we seek the optimal 2 Jmax, 1 : Θ [ Θ and obtain noisy arm θ = arg maxθ measurement yt of the function value, that is, yt = J(θt) + wt. Our goal is to maximize the sum t=1 J(θt) over learning iterations/episodes, thus to perform essentially as well as of rewards θ (as quickly as possible). For example, θ can be parameters of the policy distribution πθ and our objective maximizing the discounted rewards as defined in Eq. (1), i.e., maxθ Θ J(πθ). This formulation is the basis of several sample-efficient RL algorithms (Calandra et al., 2016; Marco et al., 2016; Antonova et al., 2017; Berkenkamp et al., 2021; Sukhija et al., 2023). In each round t, we choose point θt 2 Jmax], with Θ Θ J(θ). (cid:80) t=1 J(θ) the cumulative regret defined as The natural performance metric in this context J(θt). desirable asymptotic property of any learning algorithm is RT = that it is no-regret: limT RT /T = 0. In the following theorem, we show that under standard continuity assumptions on J, i.e., we can model it through Gaussian process regression (Rasmussen R+, our exploration strategy from Eq. (7) & Williams, 2005) with given kernel k( , with the model epistemic uncertainty as the intrinsic objective has sublinear regret. ) : Θ (cid:80) Θ is Theorem A.1. Assume that the objective function lies in the RKHS kernel k(θ, θ), (1/4, 1/2), δ α (0, 1] and consider the following ϵgreedy exploration strategy B, and that the noise wt is zero-mean σ-sub Gaussian. Let ϵt = t2α Hk corresponding to the 1 for θt = arg max θ Θ arg max Θ θ σt(θ) with probabilityϵt µt(θ) else, where µt is our mean estimate of and σt the epistemic uncertainty (c.f., (Rasmussen & Williams, 2005) for the exact formula). Then we have with probability at least 1 RT where ΓT (k) is the maximum information gain (Srinivas et al., 2012). The maximum information gain ΓT (k) measures the complexity of learning the function w.r.t. the number of data points and depends on the choice of the kernel k, for common kernels such as the RBF and linear kernel, it grows polylogarithmically with (Srinivas et al., 2012; Vakili et al., 2021), resulting in sublinear regret bound. Intuitively, by sampling informative actions sufficiently often, we will improve our estimate of and thus gradually suffer less regret. JmaxT 2α + 2ΓT (k)T 1 (cid:0) (cid:1) δ α , A.2 PROOF OF THEOREM A.1 From hereon, let Zt ϵ-greedy GP bandit from Theorem A.1 is defined as: Bernoulli(ϵt) be Bernoulli random variable. The acquisition function for θt = arg max θ Θ arg max Θ θ σt(θ) if Zt = µt(θ) else Let Qt = all and s=1 Zs (cid:80) ϵs. In the following, we analyze the sequence Q1:t. Note that Qt ϵt] + E[Qt Q1:t Q1:t 1] E[Qt 1 1] = E[Zt = Qt 1. (14) [0, t] for Therefore, Q1:t is martingale sequence, or equivalently, sequence. Zs { ϵs}s 1:t is martingale difference In the following, we use the time-uniform Azuma Hoeffding inequality from Kassraie et al. (2024) to give bound on s=1 Zs. (cid:80) 16 Preprint, under review Lemma A.2. Let ϵ1 = 1, then we have with probability at least δ, for all 1 max Zs (cid:40) 5 2 ϵs t(log log(et) + log(2δ)), (cid:41) s=1 (cid:88) Proof. Note that Zt Azzuma-Hoeffding inequality (Kassraie et al., 2024, Lemma 26.) to get s=1 (cid:88) Qt ϵt 1 Qt (cid:112) = [0, 1] for all t. Therefore, we can use the time-uniform Pr : (cid:32) s=1 (cid:88) ϵs Zs 5 2 t(log log(et) + log(2δ)) δ (cid:33) Therefore, we have with probability at least 1 Zs ϵs s=1 (cid:88) s=1 (cid:88) t(log log(et) + log(2δ)) for all 1. Furthermore, since ϵ1 = 1, we have Z1 = 1. Therefore, max Zs ϵs (cid:40) s=1 (cid:88) s=1 (cid:88) t(log log(et) + log(2δ)), 1 (cid:112) (cid:41) (cid:112) δ 5 2 (cid:112) 5 2 s=1 Zs. In the next lemma, we derive bound on Et. (1/4, 1/2), then for any δ δ. Let Et denote the number of exploration steps after iterations of the algorithm, Et = Lemma A.3. Let ϵt = t2α (cid:80) 2t2α for all that Et Proof. Note that ϵt is monotonously decreasing with t. Therefore, 1 with α t0 with probability at least 1dx x2α s+1 (0, 1] exists > 0, t0 > 0 such that is, ϵs and hence s=1 (cid:88) ϵs 1 (cid:90) x2α 1dx = (cid:82) t2α 2α Therefore, we get ϵs s=1 (cid:88) Therefore, 5 2 t2α 2α 5 2 (cid:18) (cid:112) t(log log(et) + log(2δ)) + 1 2α (cid:19) t(log log(et) + log(2δ)), 1 (cid:112) t(log log(et) + log(2δ)) + (cid:41) 1 2α , 1 (cid:19) (cid:27) t(log log(et) + log(2δ)) (cid:112) Et = Zs s=1 (cid:88) max max (cid:40) (cid:26) 4 , for any δ 5 2 ϵs s=1 (cid:88) t2α 2α 5 2 5 2 (cid:18) (cid:112) (0, 1] Note that since α > 1 t(log log(et) + log(2δ)) + 1 2α o(t2α). t2α Moreover max 2α (0, 1] exists > 0, t0 > 0 such that Et (cid:112) t(log log(et) + log(2δ)) + 1 2α 2t2α for all (cid:112) (cid:110) (cid:16) 5 17 (cid:17) , 1 t0 with probability at least 1 Θ(t2α), therefore for any δ δ. (cid:111) Preprint, under review Next, we use the results from Chowdhury & Gopalan (2017) on the well-calibration of w.r.t. our mean and epistemic uncertainty estimates. Lemma A.4 (Theorem 2, Chowdhury & Gopalan (2017)). Let the assumptions from Theorem A.1 hold. Then, with probability at least 1 δ, the following holds for all θ Θ and 1: 1(θ) µt J(θ) βt 1σt 1(θ), 1 + 1 + ln(1/δ)), where Γt 1 is the maximum information gain (Srinivas with βt 1 = +σ et al., 2012) after 2(Γt 1 rounds. (cid:112) Proof of Theorem A.1. The ϵgreedy algorithm alternates between uncertainty sampling and maximizing the mean. In the worst case, it may suffer maximal regret Jmax during the uncertainty sampling stage. Furthermore, assume at time t, we execute the exploitation stage, then we have J(θ) J(θt) = J(θ) J(θ) βt(σt(θ) + σt(θt)) µt(θ) + µt(θ) µt(θ) + µt(θt) J(θt) J(θt) (µt(θt) µt(θ)) (Lemma A.4) Therefore, we can break down the cumulative regret RT into these two stages. RT = EZ1:T ,w1:T (cid:34) t=1 (cid:88) J(θ) J(θt) (cid:35) [EZ1:t1,w1:t1 Ewt,Zt[J(θ) J(θt)]] t=1 (cid:88) t=1 (cid:88) = = EZ1:t1,w1:t1[Ewt[J(θ) J(θt) Zt = 0](1 ϵt) + Ewt[J(θ) J(θt) Zt = 1]ϵt Jmax ϵt + βtEZ1:t1,w1:t1[σt(θ) + σt(θt)] t=1 (cid:88) t=1 (cid:88) Assume Et is given, then we have for all max θ Θ σ2 (θ)Et = max Θ θ σ2 (θ) Zs s=1 (cid:88) σ2 (θ) s=1 (cid:88) s=1 (cid:88) Zs max θ Θ Zsσ (θs) σ2 (θs) Therefore, σ2 probability at least 1 (θ) for all θ CΓt Et δ for all s=1 (cid:88) CΓt 4 , 1 . Since, Et 2 Θ, there exists constant such that t0, θ 2t2α for α (cid:0) 1 , we have with (cid:1) Going back to the regret, we get Rt Jmax σt(θ) Γt tα T ϵt + 2βtK t=1 (cid:88) t=1 (cid:88) JmaxT 2α + 2KβT Γt tα ΓT 1 α (cid:112) (cid:17) (cid:16) 18 Preprint, under review Here we used the fact that, α 1 4 , 2 , therefore, (cid:1) 2KβT ΓT (cid:0) Γt tα t=1 (cid:88) 2βtK t=1 (cid:88) 1 tα Θ(2KβT ΓT 1 α). (cid:112) (cid:112) Similarly since ϵt = t2α t=1 ϵt = Θ(T 2α) The regret is sublinear for ubiquitous choices of kernels such as the RBF. Indicating, consistency of the ϵ-greedy exploration strategy with the model-epistemic uncertainty as the exploration signal. 1, we have (cid:80) PROOF OF THEOREM 3.1 Lemma B.1. Consider the Bellman operator and define Qk+1 = sequence Qk will converge to the soft-Q function of π as πQk. Furthermore, let the assumptions from Theorem 3.1 hold. Then, the π from Eq. (12), and mapping Q0 : T . Proof. Consider the following augmented reward rπ(s, a) = r(s, a) + Es Ea where is the next state and reward given (s, a). Then from Eq. (12), we have π( s)[ α1 log(π(a s)) + α2I(s; s,a (cid:2) s, a)] , (cid:3) πQ(s, a) = rπ(s, a) + γEs,a s,a[Q(s, a)]. Furthermore, rπ is bounded since is bounded and the policy entropy and the information gain are bounded for all π Π by assumption. Therefore, we can apply standard convergence results on policy evaluation to show convergence (Sutton, 2018). Lemma B.1 shows that the updates satisfy the contraction property and thus converge to the soft-Q function of π. Next, we show that the policy update from Eq. (8) leads to monotonous improvement of the policy. Lemma B.2. Let the assumptions from Theorem 3.1 hold. Consider any πold denote the solution to Eq. (8). Then we have for all (s, a) that Qπold (s, a) Π, and let πnew (s, a). Qπnew Proof. Consider any πnew( Ea Ea = πold Qπold (s, a) s) (cid:104) s) Qπold (cid:104) πold( (s) (s, a) s) + α2I(s; α1 log πnew(a α1 log πold(a s) + α2I(s; s, a) (cid:105) s, a) (cid:105) (15) Next consider any pair (s, a) Qπold (s, a) = r(s, a) + γEs r(s, a) + γEs ... s,a[V πold s,a[Ea (s)] πnew( s)[Qπold (s, a) α1 log πnew(a s) + α2I(s; s, a)]] Qπnew (s, a) where we have repeatedly expanded Qπold the bound in Eq. (15). on the RHS by applying the soft Bellman equation and Finally, combining Lemma B.1 and Lemma B.2, we can prove Theorem 3.1. Proof of Theorem 3.1. Note that since the reward, information gain, and entropy are bounded, Qπ(s, a) is also bounded for all π . Furthermore, in Lemma B.2 we show that the policy is monotonously improving. Therefore, the sequence of policy updates converges to polJ π Π such that π icy π Π. Applying the same argument as Haarnoja et al. (2018, Theorem 1), we get Qπ (π) for all π (s, a) Qπ(s, a) for all π Π, (s, a) Π, (s, a) (π) . 19 Preprint, under review MAXINFORL FROM THE PERSPECTIVE OF KL-MINIMIZATION We take inspiration from Hafner et al. (2020) and study the Boltzmann exploration formulation from Section 3.2 from the action perception and divergence minimization (APD) perspective. We denote with τ = 0 the trajectory in state, reward, and action space. The goal of the RL agent is to maximize the cumulative reward w.r.t. the true system . Therefore, common and natural choice for target/desired distribution for the trajectory τ is (Levine, 2018; Hafner et al., 2020) st, at, rt}t { p(τ ) = 0 (cid:89)t p(st+1, rt st, at, ) exp 1 α . rt 0 (cid:88)t However, is unknown in our case and we only have an estimate of the underlying distribution p(f ) from which it is sampled. Given state-pair (st, at), we get the following distribution for (st+1, rt). p(st+1, rt st, at) = (cid:90) p(st+1, rt st, at, )dp(f ) = Ef [p(st+1, rt st, at, )] . For given policy π, we can write the distribution of its trajectory via τ π as ˆpπ(τ ) = p(st+1, rt st, at)π(at st) 0 (cid:89)t = p(s1) Ef [p(st+1, rt st, at, )] π(at st) . 0 (cid:89)t (16) (17) natural objective for deciding which policy to pick is to select π such that its resulting trajectory τ π is close (in-distribution) to τ . That is, π = arg min π DKL(ˆpπ(τ ) p(τ )). (18) In the following, we break down the term from Eq. (18) and show that it results in the MAXINFORL objective. DKL(ˆpπ(τ ) p(τ )) = Eτ ˆpπ(τ ) p(τ ) ˆpπ(τ ) log (cid:20) (cid:18) (cid:19)(cid:21) = Eτ ˆpπ(τ ) 0 (cid:88)t log(p(st+1, rt 1 α st, at, )) + rt st, at, )]) + log(π(at 0 (cid:88)t st)) (1) log (Ef [p(st+1, rt log(π(at st)) 1 α rt + Eτ ˆpπ(τ ) 0 (cid:88)t = Eτ ˆpπ(τ ) 0 (cid:88)t + Eτ ˆpπ(τ ) 0 (cid:88)t log (Ef [p(st+1, rt st, at, )]) log(p(st+1 st, at, )) (2) 20 Preprint, under review The term (1) is commonly minimized in max entropy RL methods such as Haarnoja et al. (2018). The term (2) has very natural interpretation, which we discuss below Eτ ˆpπ(τ ) 0 (cid:88)t log (Ef [p(st+1, rt st, at, )]) log(p(st+1, rt st, at, )) = Eτ = Eτ = Eτ = Eτ ˆpπ(τ ) 0 (cid:88)t ˆpπ(τ ) 0 (cid:88)t ˆpπ(τ ) 0 (cid:88)t ˆpπ(τ ) 0 (cid:88)t log (p(st+1, rt st, at)) log(p(st+1, rt st, at, )) log (p(st+1, rt st, at)) log(p(wt)) H(wt) H(st+1, rt st, at). H(st+1, rt st, at, ) H(st+1, rt st, at). Here in the last step, we applied the equality that the entropy of st+1, rt given the previous state, input, and dynamics is the same as the entropy of wt. In summary, we get Eτ ˆpπ(τ ) 0 (cid:88)t log (Ef [p(st+1, rt st, at, )]) log(p(st+1, rt st, at, )) = Eτ ˆpπ(τ ) 0 (cid:88)t I(st+1, rt; st, at) . Therefore Eq. (18) can be rewritten as π = arg max π Eτ ˆpπ(τ ) 0 (cid:88)t rt α log(π(ut zt)) + αI(st+1, rt; st, at) . Intuitively, to bring our estimated distribution of the policy ˆpπ(τ ) closer to the desired distribution p(τ ), we need to pick policies that maximize the underlying rewards while also gathering more information about the unknown elements of the MDP; the reward and dynamics model. While the former objective drives us in areas of high rewards under our estimated model, the latter objective ensures that our model gets more accurate over time."
        },
        {
            "title": "D ADDITIONAL EXPERIMENTS",
            "content": "We present additional experiments and ablations in this section. Covergence of intrinsic reward coefficient λ for MAXINFOSAC: In Fig. 8 we report the tuned intrinsic reward coefficient λ for SACEIPO and MAXINFOSAC (α2 for MAXINFOSAC). As shown in Fig. 5 in Section 4, SACEIPO obtains suboptimal performance compared to MAXINFOSAC on the Pendulum, CartPole, and Walker tasks. We ascribe this to its tendency to underexplore, i.e. 0 much faster and converges to the local optima. Figure 8 validates our intuition since the λ intrinsic reward coefficient quickly decays to zero for SACEIPO whereas for MAXINFOSAC it does not. MAXINFORL with REDQ: In Fig. 9 we combine MAXINFORL with REDQ. In all our tasks, REDQ does not obtain high rewards, we believe this is due to the high update to data ratio, which leads to the function converging to sub-optimal solution. However, when combined with MAXINFORL, the resulting algorithm obtains considerably better performance and sampleefficiency. 21 Preprint, under review Figure 8: Evolution of the intrinsic reward coefficient λ of SACEIPO and MAXINFOSAC over environment interaction. Figure 9: We combine MAXINFORL with REDQ (MAXINFOREDQ) and report the learning curves of SAC, REDQ, MAXINFOSAC, and MAXINFOREDQ. MAXINFORL with DrQv2 on additional DMC tasks: In Fig. 10 we compare MAXINFODRQV2 with DrQv2 and MAXINFODRQ. For MAXINFODRQV2, we use fixed noise variance with σ = 0.2 and σ = 0.0. We observe that on all tasks, MAXINFODRQV2 performs better than DrQv2, even when no noise is added for exploration. This empirically demonstrates that already by just maximizing the information gain, we get good exploration. Figure 10: Learning curves of MAXINFODRQV2 with different noise levels σ pared to DrQv2 and MAXINFODRQ. 0.0, 0.2 } { comIn Fig. 11, we evaluate the exploration behavior of MAXINFODRQV2 without action noise on the DMC Humanoid Walk task. The figure shows that even without noise, MAXINFODRQV2 outperforms DrQv2, while the noise-free DrQv2 struggles to achieve high rewards. MAXINFOSAC with different intrinsic reward: We evaluate MAXINFOSAC with RND as intrinsic reward. Moreover, we initialize random NN model and train another model to predict the output of the random model as proposed in Burda et al. (2018). We train an ensemble of NNs to learn the random model and use their disagreement as the intrinsic reward3. Effectively, we replace the information gain term in Eq. (9) with the intrinsic reward derived from RND. In Fig. 12 we report the learning curves. From the figure, we conclude that MAXINFORL also works with RND as the exploration bonus. Moreover, while the standard MAXINFOSAC performs better on these tasks, MAXINFOSAC with RND performs significantly better than SAC. This illustrates the generality of our approach. 3Burda et al. (2018) use curiosity as the intrinsic reward but we observed that disagreement performed much better and robustly in our experiments. 22 Preprint, under review Figure 11: MAXINFODRQV2 evaluated on the humanoid walk task with no action noise. Figure 12: Learning curves of MAXINFOSAC with RND as the intrinsic reward, instead of the information gain, compared to SAC and standard MAXINFOSAC. Experiments with ϵMAXINFORL We also evaluate our ϵgreedy version from Section 3.1, ϵ MAXINFORL, empirically. For the intrinsic reward, we use the disagreement (Pathak et al., 2019) in the ensemble models of and as the base algorithm, we use SAC. In Fig. 13 we compare it to MAXINFOSAC and SAC and report the learning curves. We also evaluate ϵMAXINFORL with different action costs, as in Section 4, in Fig. 14. For ϵ we specify linear decay schedule (see Appendix E). In the figures, we observe that ϵMAXINFORL performs better than SAC and often on par to MAXINFOSAC. However, ϵMAXINFORL requires training two different actorcritic networks and manually specifying the schedule for ϵ, as opposed to MAXINFOSAC which automatically trades off the extrinsic and intrinsic rewards. Experiments with ϵMAXINFORL and Curiosity In Fig. 15 we compare ϵMAXINFORL with disagreement and curiosity as intrinsic rewards. We observe that overall both perform better than SAC, with disagreement obtaining slightly better performance than curiosity. Experiments with ϵMAXINFORL with switching frequency of one between policies. In all the ϵMAXINFORL experiments above, we switched between the extrinsic and intrinsic policies every 32 steps. Note that our proposed algorithm from Section 3.1 is still the same and we simply use different schedule for the ϵ parameter. This is because we believe that instead of switching after every time step, following the extrinsic or intrinsic policies for several steps helps in exploration since we can collect longer trajectories of explorative data. In Fig. 16 we test this hypothesis on the pendulum environment, where we observe that indeed the switching frequency of 32 steps performs better. Preprint, under review Figure 13: Learning curves of ϵMAXINFORL with disagreement as the intrinsic reward, SAC and MAXINFOSAC. Figure 14: Learning curves of ϵMAXINFORL with disagreement as the intrinsic reward, SAC and MAXINFOSAC for varying levels of action costs K. Experiments with OAC (Ciosek et al., 2019). In Fig. 17, we compare OAC (Ciosek et al., 2019) an optimism based actor-critic algorithm with MAXINFOSAC. Furthermore, we also test MAXINFORL version of OAC, called MAXINFOOAC. As depicted in the figure, we observe that while OAC performs better than SAC, the MAXINFORL perform the best. Experiments with DrM (Xu et al., 2024). In Fig. 18, we evaluate DrM (Xu et al., 2024) another exploration algorithm for visual control tasks. DrM uses dormant ratio-guided exploration to solve challenging visual control problems. In our experiments, we evaluate the version of DrM without the expectile regression and use the same exploration schedule proposed in the paper. We combine DrM with MAXINFORL and report the performance on the humanoid and dog tasks from DMC."
        },
        {
            "title": "E EXPERIMENT DETAILS",
            "content": "In this section, we present the experiment details of MAXINFORL and our baselines. MAXINFOSAC We train an ensemble of deterministic neural networks with mean squared error to predict the next state4 and rewards. We evaluate the disagreement with σn(s, a) = Var( are the parameters of each ensemble member. We { } quantify the information gain using the upper bound in Equation (5) and for the aleatoric uncertainty, fϕi (s, a) ϕi}i { ), where 1,...,P 1,...,P }i { { } 4Specifically we predict st+1 st. 24 Preprint, under review Figure 15: Learning curves of ϵMAXINFORL with disagreement and curiosity as the intrinsic reward compared with SAC. Figure 16: Learning curves of ϵMAXINFORL with disagreement. We compare version of ϵ MAXINFORL which switches between extrinsic and intrinsic policy every step with one which switches every 32 steps. 3. In principle, we could learn the aleatoric uncertainty, however, we found that we use σ = 10 this approach already works robustly. Lastly, as suggested by Burda et al. (2018) we normalize the intrinsic reward/information gain term when training the critic. limitation of our work is that it requires training an ensemble of forward dynamics models to quantify the epistemic uncertainty. This is highlighted in Table 1 where we compare the training time required to train SAC with MAXINFOSAC. We report the numbers for our Stable-Baselines 3 (SB3) (Raffin et al., 2021) (torch-based) implementation and JaxRL (Kostrikov, 2021) (Jax-based) implementation. Table 1: Computation cost comparison for MAXINFOSAC on NVIDIA GeForce RTX 2080 Ti Algorithm SAC (SB3) MAXINFOSAC (SB3) SAC (JaxRL) MAXINFOSAC (JaxRL) Training time for 100k environment interactions 16.96 min +/- 1.64317 min 39 min +/- 1 min 5.6 min +/- 0.2min 7.3 min +/- 0.75 25 Preprint, under review Figure 17: Learning curves of OAC compared with MAXINFOSAC and MAXINFORL version of OAC (MAXINFOOAC). Figure 18: Learning curves of DrM compared with our version of DrM (MAXINFODRM). As the number suggests MAXINFOSAC requires more time to train due to the additional computational overhead from learning the dynamics model. However, our JaxRL-based implementation already bridges the computational gap and is much faster. To further reduce the computational cost, we can consider cheaper methods for uncertainty quantification such as Osband et al. (2023). Explore then exploit We train two different actor-critics, one for the extrinsic and one for the intrinsic reward. For the first 25% of the episodes, we only use the intrinsic actor for data collection. For the remaining 75%, the extrinsic actor is used. As the base actor-critic algorithm we use SAC. Both the intrinsic and extrinsic actor-critics share the same data buffer. Therefore, the data collected during the exploration phase is also used to train the policy for the exploitation phase. For the intrinsic reward, we use the disagreement calculated as specified above for MAXINFOSAC. We also eval26 Preprint, under review uate this approach with curiosity as the intrinsic reward. Here we train neural network model to predict the next state and reward and use the mean squared error in the prediction as the intrinsic reward. SACINTRINSIC Here we follow the same procedure as proposed in Burda et al. (2018). We train two critics, one for the extrinsic reward and another one for the intrinsic rewards/exploration bonuses. We use the (normalized) information gain and policy entropy as the exploration bonuses. As suggested in Burda et al. (2018) the non-episodic returns are used in training the intrinsic critics. The policy is trained to maximize Es is the data buffer. extrinsic(s, a) + Qπold intrinsic(s, a)]], where s)[Qπold [Ea D π( SACEIPO We follow the same procedure as in SACINTRINSIC in training the critics. For the policy, we maximize weighted sum of the extrinsic and intrinsic critics, i.e., the policy π is trained to maximize Es intrinsic(s, a)]]. We determine λ using the extrinsic optimality constraint from Chen et al. (2021). For the policy π, the extrinsic optimality constraint is given by extrinsic(s, a) + Qπold s)[λQπold [Ea π( Es [Ea s)[Qπ π( extrinsic(s, a)]] max π Π Effectively, we encourage the policy π that maximizes the intrinsic and extrinsic rewards performs at least as well as the policy which only maximizes the extrinsic ones. To evaluate this constraint, we train another actor and critic that greedily maximizes the extrinsic reward only, i.e., we use the Π Es data buffer to learn the solution πE for maxπ extrinsic(s, a)]] = 0 s)[Qπ π( π( [Ea Es s)[Qπ [Ea Ea s)[Qπ π( L(λ) = λ extrinsic(s, a)]] extrinsic(s, a)]] (cid:17) extrinsic(s, a)]] > Intuitively, Es [Ea extrinsic(s, a)]], we increase λ, effectively focusing more on the extrinsic reward. We update λ with gradient descent on L(λ), akin to the temperature parameters in MAXINFOSAC. if our constraint is not satisfied, s)[Qπ s)[Qπ i.e., Es [Ea [Es D D ( ( π π π( (cid:16) Es [Es extrinsic(s, a)]]. s)[Qπ [Ea MAXINFODRQ We add our information gain on top of DrQ using the same architecture and image augmentations from Yarats et al. (2021). The policy and critic updates are identical to MAXINFOSAC. However, since we deal with images, we train an ensemble of deterministic neural networks with mean squared error to predict the next state st+1, rewards rt, and compressed image embedding et which we obtain from the encoder for the observation ot. Effectively, we compress the output of the encoder into de dimensional embedding using max pooling and use that as label for our ensemble training. Crucially, we want the ensemble to learn and explore all unknown components of the MDP, the dynamics, rewards and observation model. For the state s, we use the learned representation from the target critic. MAXINFODRQV2 We use the same procedure for training the ensemble model as MAXINFODRQ. For MAXINFODRQV2, we separate the intrinsic and extrinsic reward critic. This is primarily because DrQv2 uses nstep returns for training the extrinsic critic. By separating the two critics, we can train the extrinsic critic with the nstep returns and the intrinsic with the standard 1step ones. This makes implementing the algorithm easier. MAXINFOSAC[RND] In Fig. 12 we evaluate MAXINFOSAC with RND as intrinsic reward. We initialize target neural network to predict an output embedding given (s, a). We train an ensemble of neural networks to learn the target network and use the disagreement as the intrinsic reward. The remaining training procedure is identical to MAXINFOSAC. ϵMAXINFORL We also evaluate the ϵMAXINFORL algorithm from Equation (6). We train two actor-critics similar to the explore and then exploit baselines. However, instead of exploring for fixed amount of iterations initially, we specify probability ϵt to alternate between the exploitation and exploration phases. Furthermore, changing the between policies after every time-step can lead to noisy trajectories. Therefore, we choose frequency = 32 for the switches. That is, we change the sampling strategy (explore or exploit) after minimum of steps. E.1 ALGORITHMS We provide more details about MAXINFOSAC, MAXINFODRQ, and MAXINFODRQV2. All three are off-policy actor-critic algorithms and learn two critics Qθ1 and Qθ2 to avoid overestimation bias. From here on, we denote with ψ the parameters of the policy and θ1,2, ψ the parameters of the target critics and policy. 27 Preprint, under review Losses for MAXINFOSAC and MAXINFODRQ We sample trajectory τ from following for the critic and policy losses. and use the Critic loss: JQ(θ) = 1 2 Eτ [(Qθ(s, a) y)2] (s, a) = + γ min 1,2 { Qθk } s) + α2Iu(s, a). α1 log πψ( (19) where Policy loss: πψ( s). Jπ(ψ) = Es [Ea πψ [α1 log πψ( s) α2Iu(s, a) min 1,2 { } Qθk (s, a)]]. (20) Entropy coefficient loss; J(α1) = Es [ α1(log πψ( s) + H)]. (21) Critic loss for MAXINFODRQV2 For MAXINFODRQV2 we train separate critics for the intrinsic and extrinsic rewards. This allows us to use nstep returns for the extrinsic rewards while keeping simple implementation, using 1step returns, for the intrinsic ones. Effectively, for computational efficiency, we adapt the critic network to output two heads, one for the extrinsic and one for the intrinsic reward. Eτ 1 2 [(Qextrinsic θ (s, a) yextrinsic)2 + (Qintrinsic θ (s, a) yintrinsic)2] Critic loss: JQ(θ) = yextrinsic = γlrl + γn min 1,2 Qextrinsic θk (sn, an) (cid:88)l= { yintrinsic = Iu(s, a) + γ min 1,2 } Qintrinsic θk where an Policy loss: sn) and πψ( { πψ( } s). (s, a). (22) Jπ(ψ) = Es Ea (cid:20) πψ min 1,2 { } (cid:20) (cid:18) Qextrinsic θk Loss for the information gain coefficient (s, a) + α2 min 1,2 { } Qintrinsic θk (s, a) . (23) (cid:19)(cid:21)(cid:21) J(α2) = α2 Es [Ea πψ [Iu(s, a)] Ea πψ [Iu(s, a)]] . ϕi}i { 1,...P } { (cid:1) Loss of ensemble model for MAXINFOSAC. Let Φ = (cid:0) fϕi(s, a) 2 (cid:35) J(Φ) = Eτ (cid:34) i=1 (cid:88) s), . = (s (cid:2) (cid:3) Loss of ensemble model for MAXINFODRQ and MAXINFODRQV2. J(Φ) = Eτ (cid:34) i=1 (cid:88) s, r, = fϕi(s, a) 2 (cid:35) . (24) (25) (26) (27) (28) (cid:3) In Algorithm 1 we present the main structure of the algorithms. We omit algorithm-specific details of DrQ, DrQv2 such as noise scheduling and image augmentation in Algorithm 1 and refer the reader to the respective papers for those details (Yarats et al., 2021; 2022). (cid:2) E.2 HYPERPARAMETERS 28 Preprint, under review Algorithm 1 Algorithm structure for MAXINFOSAC, MAXINFODRQ, MAXINFODRQV2 Init: θ1, θ2, ψ θ1, θ2 θ1 for iterations = 1, . . . , do θ2, ψ = ψ for each environment step do at πψ( st) p(st+1, rt st+1, rt st, at) st, at, st+1, at} { end for for each gradient step do Initialize target Sample action Observe state and reward Update critics with stochastic gradient descent (SGD) on JQ(θ1) + JQ(θ2) Update critics with SGD on Jπ(ψ) Update temperatures with SGD on J(α1), J(α2)5 Update ensemble with SGD on J(Φ). Update θ1,2, ψ Policy update target updates end for end for Table 2: Hyperparameters for results in Section 4. Environment Learning rate Policy/Critic Architecture Model architcture Polyak Coefficient action repeat feature and embedding dim DMC and Gym Humanoid Bench DMC 3 4 10 10 3 5 State Based Tasks (256, 256) Visual Control tasks 0.005 0.005 1 for Gym 2 for DMC - - 10 4 for MAXINFODRQ, else 10 10 5, 8 4 for the model and α2 5 for the model and α2 10 4 (256, 256) (256, 256) 0.005 for MAXINFODRQ, else 10 0.01 4 2 Feature: 50 Embedding: 32 Feature: 100 Embedding: 32 Humanoid and Dog tasks (DrM) 8 10 (1024, 1024) (1024, 1024)"
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "UC Berkeley"
    ]
}