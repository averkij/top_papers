{
    "paper_title": "Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models",
    "authors": [
        "Yang Sui",
        "Yu-Neng Chuang",
        "Guanchu Wang",
        "Jiamu Zhang",
        "Tianyi Zhang",
        "Jiayi Yuan",
        "Hongyi Liu",
        "Andrew Wen",
        "Shaochen",
        "Zhong",
        "Hanjie Chen",
        "Xia Hu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the \"overthinking phenomenon\". In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 1 4 6 1 . 3 0 5 2 : r Stop Overthinking: Survey on Efficient Reasoning for Large Language Models Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen (Henry) Zhong, Hanjie Chen, Xia Hu Department of Computer Science Rice University yang.sui@rice.edu, xia.hu@rice.edu Project Website: https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as OpenAI o1 and DeepSeek-R1, have further improved performance in System-2 reasoning domains like mathematics and programming by harnessing supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the Chainof-Thought (CoT) reasoning. However, while longer CoT reasoning sequences improve performance, they also introduce significant computational overhead due to verbose and redundant outputs, known as the overthinking phenomenon. Efficient Reasoning, which seeks to optimize reasoning length while preserving reasoning capabilities, offers practical benefits such as reduced computational costs and improved responsiveness for real-world applications. Despite its potential, efficient reasoning remains in the early stages of research. In this paper, we provide the first structured survey to systematically investigate and explore the current progress toward achieving efficient reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we categorize existing works into several key directions: (1) model-based efficient reasoning, which considers optimizing full-length reasoning models into more concise reasoning models or directly training efficient reasoning models; (2) reasoning output-based efficient reasoning, which aims to dynamically reduce reasoning steps and length during inference; (3) input prompts-based efficient reasoning, which seeks to enhance reasoning efficiency based on input prompt properties such as difficulty or length control. Additionally, we introduce the use of efficient data for training reasoning models, explore the reasoning capabilities of small language models, and discuss evaluation methods and benchmarking. We maintain public repository to continuously track and update the latest research in this promising area."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have emerged as exceptionally powerful AI tools, demonstrating advanced capabilities in natural language understanding and complex reasoning. Recently, the emergence of reasoning-focused LLMs, also referred to as Large Reasoning Models (LRMs) [91], such as OpenAI o1 [61] and DeepSeek-R1 [31], has significantly enhanced their performance in System-2 reasoning domains [8, 44], including mathematics [16, 35] and programming [7, 17]. Evolving from foundational pre-training models (e.g., LLaMA [30, 80], Qwen [95]) with next-token prediction training [23], these models leverage Chain-of-Thought (CoT) [86] prompting to generate Last Update: March, 2025. Figure 1: The pipeline of developing efficient reasoning for LLMs. reasoning model can be trained on the base model using SFT, RL, or combination of both. While reasoning models demonstrate strong reasoning capabilities, they often suffer from the overthinking phenomenon, generating unnecessarily lengthy reasoning steps. To improve efficiency, various methods can be applied to reduce redundant steps while maintaining accuracy, or to fine-tune non-reasoning models to incorporate efficient reasoning capabilities. This approach enables the model to answer questions with concise and effective reasoning steps. In this paper, we explore the latest progress in efficient reasoning for LLMs, aiming to provide insights that can guide future research and the development of reasoning-driven applications across various domains. explicit, step-by-step reasoning sequences before arriving at final answer, considerably improving their effectiveness in reasoning-intensive tasks. Reasoning abilities in LLMs are typically developed through supervised fine-tuning (SFT) and reinforcement learning (RL), which promote iterative and systematic problem-solving abilities. Specifically, the OpenAI o1 [61] training pipeline potentially1 incorporates SFT and RL with Monte Carlo Tree Search (MCTS) [71] and processed reward model (PRM) [47]. DeepSeek-R1 is initially fine-tuned using SFT with Long CoT reasoning data generated by RL-trained DeepSeek-R1-Zero, and then further refined through RL with rule-based reward function. However, while long CoT reasoning significantly enhances reasoning capabilities and accuracy, the introduction of the CoT-like mechanisms (e.g., Self-consistency [84], Tree-of-Thought [96], Incentivizing RL [31]) also results in lengthy output responses, leading to substantial computational overhead and thinking time. For instance, overthinking problem happens when asking what is the answer of 2 plus 3? [10] to OpenAI-o1, DeepSeek-R1, and QwQ-32B-Preview. The reasoning sequences from such models can sometimes span thousands of tokens, many of which are redundant and do not substantially contribute to deriving the correct answer. Such verbosity directly raises inference costs and latency, restricting the practical use of reasoning models in computation-sensitive real-world applications, including real-time autonomous driving systems, interactive assistants, robotics control, and online search engines. Efficient reasoning, particularly the reduction of reasoning length, offers significant benefits, such as cutting costs and enhancing reasoning capabilities in real-world deployments. Recently, numerous studies [32, 33, 54, 56, 98] have attempted to develop more concise reasoning paths, making efficient reasoning prominent and rapidly evolving area of research. In this paper, we provide the first structured survey to systematically explore the current progress of efficient reasoning for LLMs. As shown in Figure 2, we categorize existing works into below key directions: (1) model-based efficient reasoning, which involves optimizing full-length reasoning models into more concise reasoning models or directly fine-tuning models to achieve efficient reasoning; (2) reasoning output-based efficient reasoning, which dynamically reduces reasoning steps and output length during inference; (3) input prompts-based efficient reasoning, which improves reasoning efficiency by leveraging prompt properties, such as prompt-guided length or prompt difficulty. Orthogonal to the definition of model compression techniques in LLMs, such as quantization [27, 48] or kv cache compression [52,103], which focus on compressing the model size and enabling lightweight inference, efficient reasoning in LLMs emphasizes Smart and Concise Reasoning by optimizing the 1Based on public speculation since GPT-o1 has not released detailed training methods. 2 Figure 2: Overview of efficient reasoning methods, which can be summarized as model-oriented (Left: I, II) and reasoning output-oriented (Middle: III, IV), and input prompts-oriented (Right: V, VI) methods. Specifically, (I) Reinforcement Learning with Length Reward Design (Section 3.1); (II) Supervised Fine-Tuning with Variable-Length CoT Data (Section 3.2); (III) Compressing Reasoning Steps into Fewer Latent Representation (Section 4.1); (IV) Dynamic Reasoning Paradigm during Inference (Section 4.2); (V) Prompt-guided Efficient Reasoning (Section 5.1); (VI) Routing Prompts to Optimize Reasoning Efficiency (Section 5.2); reasoning length and reducing thinking steps. Overall, we summarize the efficient reasoning methods into below categories: RL with Length Reward Design (Section 3.1); SFT with Variable-Length CoT Data (Section 3.2); Compressing Reasoning Steps into Fewer Latent Representation (Section 4.1); Dynamic Reasoning Paradigm during Inference (Section 4.2); Prompt-guided Efficient Reasoning (Section 5.1); Routing Prompts to Optimize Reasoning Efficiency (Section 5.2); Additionally, in this paper, we explore the other interesting topics, including Training Reasoning Models with Efficient Data (Section 6.1); Reasoning Ability of Small Language Models and Model Compression (Section 6.2); Evaluation and Benchmark of Efficient Reasoning Models (Section 7); We will continuously update our public repository with the latest research on efficient reasoning."
        },
        {
            "title": "2 Background: Long CoT Reasoning Models and Overthinking Phenomenon",
            "content": "2.1 Chain-of-Thought (CoT) CoT [86] is one key approach by which reasoning capabilities have been purposely introduced in LLMs. In this setting, models are often prompted to output reasoning thinking chain before arriving at final answer. Techniques proposed in this realm have been shown to improve overall accuracy-like performance [86] since via improving the quality of the generation context, more consistent final result is likely to be achieved. Some notable CoT variants include Self-consistency CoT [84], which replaces the greedy-like thinking chain generation by sampling more diverse reasoning paths and then selects the most consistent answer by marginalizing and aggregating such paths. Similarly, Tree-of-Thought prompting [96] proposes problem-solving by structuring thought processes into tree with backtracking allowed, which greatly improves the efficiency of parallelizable subtasks. Graph-of-Thoughts [6] prompting further extends this concept by structuring thoughts as graph and permits looping to refine an individual thought. There exist many further variants of CoT, but generally, they are done within the scope of prompting the model in different ways to enable different behaviors and (optionally) 3 Model-based Efficient Reasoning Reasoning Output-based Efficient Reasoning o a Input Prompts-based Efficient Reasoning Efficient Data and Models RL Optimization via Length Reward SFT with Variable-Length CoT Latent Representation Compression Dynamic Reasoning Paradigm Prompt-Guided Efficient Reasoning Routing by Question Attributes Less Training Data Pruning & Quantization & Distillation e.g. Kimi k1.5 [78]; O1-Pruner [54]; L1 [1]; Training [3]; Demystifying [98]; DAST [69]; MRT [64]; Stepwise [21]; e.g. Distilling 2-1 [99]; C3oT [37]; TokenSkip [89]; CoT-Valve [56]; SelfTraining [59]; Learn to Skip [50]; Token-Budget [32] e.g. Coconut [33]; CODI [70]; CCoT [12]; Heima [68]; Token Assorted [74]; Loop [66]; SoftCoT [93] e.g. Speculative Rejection [76]; Sampling-Efficient TTS [85]; DPTS [24]; Certaindex [28]; Fast MCTS [41]; ST-BoN [85]; More is Less [88]; RSD [45]; LightThinker [101]; INFTYTHINK [94]; SCoT [90]; RASC [83]; Adaptive Reasoning [100]; AdaptiveStep [51]; e.g. Token-Budget [32]; Chain of Draft [92]; Token Complexity [40]; Concise Chain-of-Thought (CCoT) [65] e.g. Claude 3.7 Sonnet [2]; SoT [4]; e.g. LIMO [97]; s1 [58]; S2R [55]; Light-R1 [87]; e.g. Struggle [43]; Strong Verifiers [73]; TinyR1-32B-Preview [77]; Mixed Distillation [13]; Counterfactual Distillation [26]; Feedback-Driven Distillation [105]; SKIntern [46]; Adaptive Thinking [9]; PRR [104]; Benchmark & Insights Evaluation & Benchmarks e.g. 1B vs. 405B [49]; Sys2Bench [62]; Danger [19]; Figure 3: Taxonomy of existing literature on efficient reasoning for LLMs. implementing controller-like component to manage the progression of thought generation and usage. 2.2 The Mechanism behind Large Reasoning Models Multi-step reasoning capability refers to the ability of LLMs to actively generate reasoning process before committing final answer. Reasoning steps can significantly improve LLM inference, especially in logic-intensive tasks such as math and programming. On broader scale, reasoning-capable model LLMs are typically favored by human users than their non-reasoning-capable counterparts, as showcased in the Chatbot Arena LLM Leader board2. Recent reasoning models, such as DeepSeek-R1 [31] and OpenAI o1 [54], are known (or speculated) to internally learn reasoning behaviors without relying on explicit and computationally intensive test-time argumentations. These models can produce detailed CoT reasoning by iteratively generating intermediate steps and progressively refining solutions through sequentially decoding until reaching final answer. This approach is fundamentally different from the prompting-based strategy widely adopted in CoT variants, as the reasoning capability is internalized within those reasoning-capable models via training. OpenAI o1 model, as speculated, leverages tree-based approach such as MCTS [18, 38] with Process Reward Model to be trained to systematically explore various reasoning paths and determine 2A community-driven evaluation for best LLM and AI chatbots https://lmarena.ai/?leaderboard Figure 4: An example of the overthinking phenomenon: when the reasoning model is asked Which is larger, 0.9 or 0.11?, it takes an unnecessarily long time (i.e., 19 seconds for QwQ-32B [79] and 42 seconds for DeepSeek-R1 [31]) to deliver its final answer. The example was tested in March 2025. the optimal solutions through guided simulations3. Models like DeepSeek-R1 are specifically trained to enable its reasoning capability. Through combination of supervised fine-tuning and reinforcement learning recipes with special focus on rule-based rewards upon math and coding tasks, DeepSeek-R1 and the like learn to generate the reasoning process within predefined format before providing the final answer. 2.3 Overthinking Problem in Long CoT Reasoning Models Overthinking phenomenon [10,78] in long CoT reasoning refers to the phenomenon in which LLMs generate overly detailed or unnecessarily elaborate reasoning steps, thereby impairing their problemsolving efficiency. Specifically, some modern reasoning LLMs especially some reasoning-capable offerings at smaller parameter scales tend to produce verbose reasoning or redundant intermediate steps, making them incapable of providing an answer before the user-defined token budget runs out or worse: providing an incorrect answer because the verbose reasoning steps introduce errors or diminishes the logical clarity. Figure 4 showcases the example of overthinking. Although overthinking CoT reasoning produces the correct answer within its initial steps, its generated reasoning steps often include multiple redundant intermediate steps, resulting in unnecessary complexity and reduced clarity. Given the extreme resource cost associated with the decoding phase of LLMs (e.g., OpenAI o1 costs $60 per 1M of generated tokens), this is highly undesirable behavior; and it goes without saying that it would be even worse if an extended reasoning generation leads to wrong answer. In contrast, efficient thinking with fewer steps can gain the correct answers, which highlights the possibility of reducing the extra budget of overthinking tokens. Note that this challenge of efficient reasoning is considered significant because the pretraining recipes of reasoning-capable models often explicitly encourage models to generate extended reasoning steps to pursue the correct answer. For instance, when DeepSeek-R1-Zero is trained longer, its response length and benchmark performance both grow [31]; where observing such two trends are often considered the proxy of successful reasoning-enabling training recipe. Thus, wanting to achieve reasoning efficiency at inference is, by design, working against certain pre-training goals of the model and thereby requires non-trivial considerations. This work aims to summarize the different schools of thought, as well as their exemplary methods, in achieving the challenging but rewarding goal of having an efficient yet capable reasoning model. 3We emphasize that there is no official confirmation regarding the training details and mechanism of OpenAI o1, so we are merely presenting one popular speculation here. Sources like and discuss this topic in-depth and are recommended to interested readers 5 Table 1: Comparison of different length reward-based RL methods. L() denotes the way of calculating the prediction length. rc Reward (correct/wrong) for L() = Lmax(). re is the exceed length penalty. yGT represents the ground truth answer of input data x. πref is the policy of reference model. 0 denotes reward (correct/wrong) for L()=0. rc L/rw 0/rw Method RL Length Constraint Reward O1-Pruner [54] PPO KIMI [78] PPO ExD (cid:104) Eπθ ,πref (cid:104) L(yref) L(ypred) (cid:105) (cid:105) (cid:40) λ min(0, λ) if correct, λ = 0.5 L(ypred)Lmin LmaxLmin if wrong. Data GSM8K GaoKao MATH-500 Model Marco-o1-7B QwQ-32B-Preview L1 [1] PPO (cid:40) xnew = CONCAT (x, Think for tokens.), r(y, yGT , L(yGT )) = I(ypred = yGT ) α L(yGT ) L(ypred) Demystifying [98] PPO 0 + 0.5 (rc rc rc 0 + 0.5 (rw re, rc rw 0)(1 + cos( πL(ypred) ), Lmax 0 )(1 + cos( πL(ypred) Lmax ), if correct, if wrong if L(ypred) = Lmax, DAST [69] SimPO Trained with constructed length preference data Training [3] PG ExD [1{ypred = yGT}(1 αf (L(ypred)))] AMC GPQA LAST MMLU MATH-500 AIME-2024 Olympiad-Bench MATH-500 AIME-2024 TheoremQA MMLU-Pro-1k MATH-500 AIME-2024 GSM8K MATH-500 AIME-2024 DeepSeek-R1-Distill-Qwen-1.5B LLaMA-3.1-8B Qwen2.5-7B-Math DeepSeek-R1-Distill-Qwen-8B DeepSeek-R1-Distill-Qwen-32B DeepSeek-R1-Distill-Qwen-1.5B DeepSeek-R1-Distill-Qwen-7B Table 2: Comparison of different policy optimization methods in CoT length controls. ˆRt represents the reward model. πref is the policy of reference model. γ is target reward margin term for SimPO. The yw is for winning responses and yl is for losing responses. Method Optimization Objective Policy Gradient (PG) PPO [67] SimPO [57] (cid:104) θ log πθ(ytxt) ˆRt (cid:105) Eπθ (cid:16) (cid:104) log σ β log πθ (yw x) πref(yw x) β log πθ (ylx) πref(ylx) (cid:17)(cid:105) (cid:104) log σ (cid:16) β yw log πθ(yw x) β yl log πθ(yl x) γ (cid:17)(cid:105)"
        },
        {
            "title": "3 Model-based Efficient Reasoning",
            "content": "From the model perspective, these works focus on fine-tuning LLMs to improve their intrinsic ability to reason concisely and efficiently. 3.1 RL with Length Reward Design Most reasoning models are trained using RL-based methods (e.g., DeepSeek-R1 [31], DeepSeekR1-Zero [31], OpenAI o1 [61], QwQ-32B-Preview [79]) which focus on the accuracy reward and format rewards [31]. To enhance reasoning-length efficiency, some studies propose integrating length reward into the RL framework, which effectively shortens the reasoning process (as shown in Table 5). In principle, the length reward assigns higher scores to short, correct answers while penalizing lengthy or incorrect ones, thereby optimizing the length of the reasoning path. The key question is: How to formulate the length reward in RL? Existing works leverage traditional RL optimization techniques combined with explicit length-based reward to control the length of CoT reasoning. Some detailed length rewards are shown in Table 1. The work [3] proposes utilizing length-based rewards conditioned on correctness, where shorter correct answers receive higher rewards. They then apply traditional policy gradient methods guided by this reward scheme to encourage LLMs to produce concise reasoning steps. Expanding from the policy gradient, the following discussed work is primarily built upon proximal policy optimization (PPO) [67] with CoT length penalty. Demystifying [98] presents empirical findings from RL Figure 5: Illustration of the method for RL fine-tuning with length reward designs. In principle, the length reward assigns higher rewards to short, correct answers and penalizes lengthy or wrong answers to achieve efficient reasoning LLMs. experiments examining how reasoning capability is influenced by length. They demonstrate that RL does not consistently or reliably increase the length and complexity of CoT reasoning, emphasizing the necessity of controlling CoT length growth to ensure stable performance. To mitigate these issues, they proposed Cosine Reward based on Dirichlet function of concise reward formula [53] and the proposed exceed length penalty scores. Due to the performance impact of CoT length, Kimi k1.5 [78] incorporates length penalty in the policy optimization for long-CoT activations and model merging. Besides optimizing with length penalty reward, L1 [1] modify the training data with the designated length constraint instruction (i.e., Think for tokens) before launching the policy optimization with pre-trained reasoning LLMs. O1-Pruner [54] introduces the Length-Harmonizing Reward, combined with PPO-style loss, to optimize reasoning LLMs by effectively shortening the CoT length. Specifically, the Length-Harmonizing Reward is computed based on the ratio of CoT lengths between the reference model output and the predicted results. Additionally, this reward incorporates accuracy-based constraints comparing predictions to the reference model outputs, ensuring that shortening the reasoning process does not degrade task performance. Without relying on reference model, DAST [69] employs SimPO [57] to fine-tune reasoning LLMs using constructed length-preference dataset. This dataset is generated based on self-defined token-length budget measurement Lbudget, defined as linear combination of the average token length of correct responses and the maximum allowed generation length. These RL-based methods enable the mitigation of overthinking in reasoning-capable LLMs, where overthinking refers to unnecessarily extended reasoning processes, leading to longer inference times and exceeding computational budgets. By achieving nearly lossless alignment with the original reasoning capabilities of LLMs, these budget-efficient RL strategies democratize the deployment of reasoning LLMs in resource-constrained scenarios. 3.2 SFT with Variable-Length CoT Data Fine-tuning LLMs with variable-length CoT data is an effective way to improve the efficiency of reasoning. As shown in Figure 6, this series of works typically involves: (1) Constructing variablelength CoT reasoning datasets via various methods, and (2) Applying SFT with collected data on reasoning models to enable LLMs to learn compact reasoning chains that encapsulate effective knowledge. Note that this method is not limited to RL-trained reasoning models; it can also directly enhance reasoning models by injecting efficient reasoning capabilities, similar to those used in distilled reasoning models.(e.g., DeepSeek-R1-Distill-Qwen [31]). The key question is: How to collect variable-length CoT reasoning data, especially for short CoT data? Figure 6: Illustration of methods for utilizing SFT with variable-length CoT reasoning datasets. 7 Table 3: Comparison of various approaches that utilize SFT with variable-length CoT reasoning datasets. Method Source Data Reasoning Pruning SFT LLMs Self-Training [59] TokenSkip [89] C3oT [37] Distilling2-1 [99] Token-Budget [32] CoT-Value [56] GSM8K MATH GSM8K MATH GSM8K MathQA ECQA StrategyQA OASST2 GSM8K GSM8K-Z MathBench GSM8K PRM800k Sampling reasoning then select the shortest one Skip tokens according to semantic importance GPT-4 as compressor to make concise reasoning Standard Standard Llama-3.2-{1B,3B} Llama-3.1-8B LLaMA-3.1-8B-Instruct Qwen2.5-Instruct Standard Llama-2-chat-{7B,13B} Removing reasoning Standard Llama-2-70B-chat Persuing an optimal token budget for LLMs to complete the reasoning Standard Llama-3.1-8B-Instruct Merging parameters of non-reasoning and long reasoning LLMs Progressive QwQ-32B-Preview DeepSeek-R1-Distill-Llama-8B LLaMA-3.1-8B LLaMA-3.2-1B Qwen32B-Instruct LearnSkip [50] Analog of Algebra Multi-digit Addition Directional Reasoning Stage 1: Manually skipping Stage 2: Prompting LLMs for shorter reasoning Standard & Progressive Llama-2-7B Phi-3-mini (3.8B) 3.2.1 Constructing Variable-Length CoT Reasoning Datasets Variable-length CoT reasoning datasets refer to datasets of long/short reasoning steps that could guide LLMs to achieve correct answers. Existing works typically gather long CoT data by prompting pre-trained reasoning models with questions. Based on the long CoT data, the key challenge is: How to collect short CoT data? Overall, variable-length CoT reasoning datasets can be created via either post-reasoning or during-reasoning. We list some detailed approaches in Table 3. Post-reasoning CoT Compression. This approach collects short CoT data by reducing redundant reasoning steps after full-length reasoning, either by heuristic criterion or LLMs, as proposed in [99], [37], and [89]. Specifically, [99] uses reasoning-capable LLMs to generate the reasoning and answers. After generating full-length CoT data, they discard the reasoning process, only using the questions and answers to distill system-1 LLMs. Another work C3oT improves the reasoning efficiency by compressing the reasoning process [37]. The long CoT reasoning steps were generated by explicitly prompting LLMs. Then, it employs GPT-4 as compressor to reduce the length of the reasoning process while ensuring the compressed reasoning retains all key information and removes redundant words. In addition, TokenSkip reduce the reasoning steps driven by interpretation [89]. It estimates the semantic importance of each reasoning part to the final answer and reduces the reasoning tokens. The important parts preserve the key reasoning steps that could improve the accuracy of the final answer. The advantage of post-reasoning CoT compression is that it can achieve higher reduction rate of the reasoning steps, which advances more efficient reasoning. Obtaining Compressed CoT Data during Reasoning. This approach collects short CoT data by prompting LLMs to generate short reasoning steps during inference and reasoning, as proposed in [50], [59], [32], and [56]. Specifically, [50] proposes human-like step-skipping method for generating shorter reasoning steps. In the first stage, based on the original training datasets, they manually create solutions by skipping steps, either guided by human expertise or by randomly merging or removing steps. Further, these concise data are labeled with prompts such as Solve it in steps.. After SFT, the model is able to generate shorter reasoning paths. In the second stage, they prompt this model to solve problems by intrinsically skipping or compressing steps during reasoning. The generated concise reasoning steps with questions and answers are collected as datasets, which are then used in SFT to make LLMs solve problems with fewer steps. Moreover, Token-Budget [32] has an important insight: an optimal token budget helps LLMs actively follow the token constraint to complete the reasoning process. Motivated by this insight, it proposes binary search-based method to achieve the optimal token budgets, and follow these budgets to generate short reasoning steps. In 8 addition, [59] proposes sampling-based method to improve reasoning efficiency. Specifically, it examines the distribution of reasoning lengths and finds that shorter solutions appear more frequently than the typical reasoning length. Driven by this finding, it proposes Best-of-N (BoN) Sampling at test time, which generates paths of reasoning and selects the shortest one. These short reasoning paths are collected as the dataset. Finally, CoT-Value [56] controls the reasoning length by mix-up the parameters of long reasoning and non-reasoning LLMs for generating variable-length reasoning steps. The advantage of CoT compression during reasoning is that the naturally generated reasoning steps align with the intrinsic knowledge of LLMs, which advances more effective learning of LLMs. 3.2.2 Fine-Tuning Approaches After collecting variable-length CoT data, existing works fine-tune LLMs to achieve efficient reasoning in several ways, which include standard fine-tuning (e.g., parameter-efficient fine-tuning such as LoRA [36] or full fine-tuning) and progressive fine-tuning. Standard Fine-tuning. Most of the work adopts standard methods to fine-tune LLMs [32, 37, 50, 59, 89, 99]. Specifically, these approaches adopt LoRA [36] or full fine-tuning [37] to minimize the perplexity loss function or DPO loss function [32] on the reasoning-efficient datasets. The LoRA enables LLMs to adapt to short reasoning steps with less than 1% of the parameters tuned. In addition, [50] observed the growing reasoning efficiency can generalize to out-of-domains beyond the collected datasets. Progressive Fine-tuning. Progressive fine-tuning aims to smoothly reduce the reasoning steps during fine-tuning [50, 56]. One way is to progressively reduce the reasoning steps of data during finetuning LLMs, as employed in [50]. Another effective way is to progressively adjust the generation of reasoning steps, as proposed by CoT-Value [56]. Specifically, it first learns LoRA adaptor θN and θL, where LLMs with θN have no reasoning steps, and that with θL have long reasoning. Then, it mix-up θN and θlong by αθN + (1 α)θL to generate dataset reasoning with variable length. Here 0 < α < 1 controls the parameter to shift from θN to θL, controlling the reasoning length generated by LLMs. Finally, it fine-tunes LLMs on the generated data while progressively reducing α from 1 to 0. In this way, it progressively improves the reasoning efficiency during fine-tuning."
        },
        {
            "title": "4 Reasoning Output-based Efficient Reasoning",
            "content": "From the perspective of reasoning steps in the output, these works focus on modifying the output paradigm to enhance the ability of LLMs to reason concisely and efficiently. 4.1 Compressing Reasoning Steps into Fewer Latent Representation Although standard CoT methods improve LLM performance by explicitly writing reasoning steps, recent work [22] has shown that simply adding intermediate thinking tokens, or even meaningless filler (e.g., ......) [63], can also increase performance. [29] scales up deeper reasoning through recurrent expansions in the hidden space rather than verbose text. These findings highlight that the benefit often lies in more hidden computation rather than purely textual decompositions. Building on the insight that latent reasoning can allow LLMs to reason more efficiently and flexibly, with fewer (or no) explicit textual intermediate steps, several new methods focus on compressing or replacing explicit CoT with more compact latent representations. The key question is: How to compress reasoning steps into latent space? In general, these methods can be categorized into two types: training LLMs to inference using latent representations or using an auxiliary model. visualized comparison of some of these approaches is presented in Figure 7. Training LLMs to Leverage Latent Representations. Among the first explorations, Coconut (Chain of Continuous Thought) [33] treats the final-layer hidden states of an LLM as continuous thought to replace traditional discrete tokens. It then reuses these hidden states as the next input 9 Figure 7: Comparison of methods of compressing reasoning steps into fewer latent representations. embeddings. Trained step by step, Coconut gradually adds these latent CoT tokens. The results suggest that compressing tokens into latent representations improves both accuracy and efficiency by reducing the number of intermediate thinking tokens. CODI [70] leverages different training process compared to Coconut, which learns the continuous latent CoT via self-distillation. In CODI, the model serves both teacher and student, jointly learning explicit and implicit CoT while aligning hidden activations on the token, generating the final answer. This self-distillation process enables LLMs to perform reasoning internally without generating explicit CoT tokens. Similarly, CCOT [12] condenses long CoT reasoning into short contentful and continuous contemplation tokens. First, it precomputes the full CoT for query and selects the most important hidden states as gold standard for compression. The CCOT module (a LoRA) is trained to predict these key tokens. Then, the DECODE module (another LoRA) is trained on the query plus compressed tokens. During inference, CCOT generates compressed tokens, which DECODE uses to produce concise reasoning steps. Another type of work, summarization-based dynamic reasoning, as mentioned in Section 4.2 explores compressing and summarizing reasoning steps in discrete space during inference, which is similar to the introduction of contemplation token. Another work, Heima [68], inspired by Coconut [33], brings latent reasoning into Multimodal Large Langue Models (MLLMs). Instead of always using full, lengthy reasoning explanations, Heima replaces each stage of detailed reasoning with single thinking token. With this change, the training data is updated. Instead of long textual explanations, each reasoning stage is just one of these thinking tokens. Then, they continue fine-tuning the model to achieve efficient reasoning. Token Assorted [74] adopts hybrid approach. During training, part of the CoT is replaced by discrete latent tokens learned via VQ-VAE [82], and then the LLM is trained with partial and high-level abstract of the reasoning steps. The authors show that mixing text tokens with latent tokens can facilitate training and inference by representing some reasoning steps in compact latent form. Other than explicitly compressing the discrete tokens into latent space, [66] demonstrates that looping k-layer transformer times can emulate the performance of kL-layer model. This looping mechanism effectively increases the depth of the model depth without adding parameters, enabling iterative reasoning processes within the latent space. The study reveals that looped models implicitly generate latent thoughts, allowing them to simulate multiple steps of CoT reasoning through successive loops. Training Auxiliary Modules while Keeping LLMs Frozen. While most methods for continuousspace reasoning fine-tune the pre-trained LLM, SoftCoT [93] keeps the underlying LLM frozen. lightweight auxiliary model generates instance-specific soft thought tokens projected into the embedding space of the frozen LLM. Experiments show that SoftCoT consistently boosts performance, demonstrating the viability of augmenting LLMs with external latent reasoning tokens. These methods hint at broader move toward latent reasoning, where critical thinking occurs in compressed, non-textual forms. Such approaches can unlock improved speed, adaptive inference, parallel backtracking, and new ways to interpret or partially reveal the model reasoning. As LLMs grow larger and tasks become more complex, balancing thorough reasoning with computational efficiency is greatly beneficial from these flexible and compact latent CoT paradigms. 10 4.2 Dynamic Reasoning Paradigm during Inference Existing works focus on modifying the reasoning paradigm for more efficient inference. The key during inference is choosing the proper criterion to guide the reasoning strategy. Current training-free approaches explore dynamic reasoning using various criteria, such as reward-guided, confidencebased, and consistency-based selective reasoning. Additionally, summarization-based dynamic reasoning method intrinsically integrates the output summarization paradigm of LLMs during training. The key question is: Which criterion to guide the inference? What is the appropriate efficient inference paradigm? Table 4: Comparison of different methods of dynamic reasoning paradigm of test time compute during inference. Category Method Trainingfree? Reward-guided Efficient Reasoning Speculative Rejection [76] Yes Reward-Guided Speculative Decoding (RSD) [45] Yes Confidence/ Certainty-based Adaptive Reasoning Dynamic Tree Search [24] Parallel Yes Dynasor (Certaindex-based Scheduling) [28] Yes FastMCTS [41] Yes Length-filtered Vote [88] Yes Baseline and Its Drawbacks Method Description Best-of-N (BoN) Decoding: underutilizes GPU memory and computational resources during the early stages, leading to lower final rewards. Starts BoN with large initial batch size and rejects unpromising sequences periodically, efficiently achieving higher rewards. Speculative Decoding: strictly enforces unbiasedness, discarding useful intermediate outputs and leading to computational inefficiency. speculative decoding method that leverages reward model (PRM) to selectively accept high-quality outputs from lightweight draft model, reducing computation while preserving accuracy. Tree-of-Thoughts: difficult to parallelize due to frequent switching of reasoning focus, and inefficient because of redundant exploration of suboptimal solutions Dynamically parallelizes node expansion through adaptive batching and implements search-and-transition mechanism (including Early Stop and Deep Seek) to prune unpromising paths early. Serving systems with uniform resource allocation: allocate compute uniformly, leading to inefficient resource usage and unmet latency targets Dynamically allocates compute for reasoning queries based on Certaindex, statistical measure of reasoning progress, to maximize accuracy within resource constraints. Rejection Sampling: inefficient, discards intermediate steps, and fails to balance problem difficulty An MCTS-inspired sampling strategy that efficiently generates high-quality multi-step reasoning data, providing steplevel evaluation signals and balanced sampling across problem difficulties. Majority Voting: ignores reasoning quality, includes suboptimal CoT lengths, and suffers from noisy predictions voting strategy that selects answers with the optimal CoT length by filtering out excessively short or long reasoning paths. Consistencybased Selective Reasoning Self-Truncation Best-of-N BoN) [85] (STYes Best-of-N Sampling: fully generates all samples and relies on costly reward models Estimates the most promising sample early via internal embedding consistency, truncating inferior samples prematurely. Summarizationbased Dynamic Reasoning LightThinker [101] No InftyThink [94] No Chain-of-Thought (CoT): high memory and computational overhead due to the generation of an excessive number of tokens Trains LLMs to learn when and how to compress intermediate thoughts, condensing long reasoning chains into gist tokens, and uses sparse-patterned attention mask during inference to enhance computational efficiency. Monolithic Reasoning: reasoning output is verbose, and can quickly exceed the context window limit of the LLM, resulting in poor performance An iterative reasoning paradigm that interleaves reasoning steps with intermediate summarization, enabling unbounded reasoning depth without architectural modifications. 4.2.1 Dynamic Reasoning via Explicit Criteria Train-time scaling with RL [31] can significantly enhance the reasoning ability of LLMs. However, it requires substantial computational resources to scale up the model training, making it prohibitively expensive [31]. As an alternative, researchers have explored test-time reasoning, also known as 11 test-time scaling [72]. Instead of relying on training to learn CoT reasoning steps, test-time scaling leverages various inference strategies that allow models to think longer and broader on complex problems. This approach consistently improves performance on challenging math and code problems that require reasoning by increasing the computational resources allocated during inference [5, 72]. Test-time scaling utilizes various inference strategies to generate longer and higher-quality CoT responses. There are several ways to scale up the inference. (1) Best-of-N sampling [76, 85] involves generating multiple responses for given prompt, expanding the search space to identify better solutions. After generation, the best response is selected using either majority voting, where the most frequently occurring response is chosen; or by reward model, which evaluates response quality based on pre-defined criteria. This method has been shown to significantly enhance the reasoning capabilities of LLMs [5]. (2) Beam-based searching [5, 24, 28], which differs from Best-of-N by structuring generation into multiple steps. Instead of generating an entire response in one pass, beam search selects the most promising intermediate outputs with process reward model [81] at each step, while discarding less the optimal ones. This enables more fine-grained optimization of both response generation and evaluation. (3) Monte Carlo Tree Search (MCTS) [41], where multiple solution paths are explored in parallel. MCTS generates partial responses along different branches of solution tree, evaluates them, and back-propagates reward values to earlier nodes. The model then selects the branch with the highest cumulative reward, ensuring more refined selection process compared to traditional beam search. Although test-time scaling can significantly reduce train-time scaling up overhead [5], the large number of generated responses still makes inference computationally expensive. To address this, recent works have been exploring methods to optimize test-time scaling. Reward-guided Efficient Reasoning. Speculative Rejection [76] is an efficient inference-time reasoning algorithm that optimizes Best-of-N (BoN) decoding by dynamically reducing computational overhead (as shown in Figure 8, left). It generates multiple responses until memory limits are nearly reached, then discards low-quality outputs based on evaluation by reward model. This adaptive filtering substantially reduces inference costs compared to vanilla BoN. On the other hand, RewardGuided Speculative Decoding (RSD) [45] enhances the efficiency of speculative decoding specifically for multi-step reasoning tasks. Unlike traditional speculative decoding methods, which strictly require exact token matching between the draft model and target model, RSD leverages Process Reward Model (PRM) to dynamically evaluate intermediate outputs from the smaller, more efficient draft model. Outputs with high reward scores are directly accepted, while those with lower scores are further refined by larger, more capable target model. Confidence/Certainty-based Adaptive Reasoning. Dynamic Parallel Tree Search (DPTS) [24] optimizes tree-based reasoning in LLMs by addressing two main inefficiencies by introducing: (1) Parallelism Streamline optimizes memory and compute by storing only incremental KV cache updates and dynamically adjusting the number of extended nodes based on available GPU memory, (2) Search and Transition Mechanism balances exploration and exploitation using confidence-based criteria. Overall, during inference, the system cuts off uncertain paths to save time. FastMCTS [41] is another confidence-based method that aims to optimize multi-step reasoning data synthesis. Traditional rejection sampling generates multiple candidate responses independently, selecting only the correct ones, but it is often inefficient and struggles with imbalanced sampling. Inspired by MCTS, FastMCTS prioritizes high-confidence traces for deep reasoning. Additionally, it adjusts tree expansion based on problem complexity, improving both efficiency and reasoning diversity. Another line of research leverages certainty or uncertainty measures to guide adaptive reasoning. Certaindex [28], certainty metric, quantifies the confidence of LLMs throughout reasoning using semantic entropy, reward model scores, or combination of both. higher certaindex indicates that further reasoning steps are unlikely to change the final answer, allowing early termination to free resources for more challenging queries. Dynasor, an inference system built on this principle, optimizes compute scheduling by dynamically tracking reasoning progress instead of allocating resources uniformly. Length-filtered Vote [88] is another work that leverages uncertainty to improve CoT reasoning. The study finds that longer reasoning chains do not always improve accuracy; instead, performance initially improves but eventually declines due to error accumulation. The authors provide mathematical analysis proving the existence of an optimal CoT length, determined by model capability and task difficulty. To exploit this, they propose Length-filtered Voting, length-aware majority voting method that groups answers by CoT length and selects the most reliable group based on prediction uncertainty. 12 Figure 8: Examples of efficient Best-of-N sampling methods. (Left) Speculative Rejection [76] uses reward model to estimate partial generation quality. It then early stops the sampled sequence with lower scores. (Right) ST-BoN [85] evaluates the latent embedding of the early generation. The latent embedding of each thinking path will be used to calculate pairwise consistency between other tokens. The sequence with the highest consistency is more likely to arrive at the correct answer. Consistency-based Selective Reasoning. Self-Truncation Best-of-N (ST-BoN) [85] enhances BoN sampling efficiency by introducing early termination (as shown in Figure 8, right), similar to Speculative Rejection [76]. However, unlike Speculative Rejection using reward models, STBoN leverages consistency as the metric to measure the importance. Specifically, it leverages the consistency of latent embeddings to evaluate response quality. The core insight is that the closer sample is to others, the more likely its path will lead to the correct answer. Then, ST-BoN selects the most consistent Chain-of-Embedding (CoE) to others and regards it as the optimal sample. 4.2.2 Summarization-based Dynamic Reasoning Some existing methods choose to optimize reasoning efficiency by training LLMs to summarize intermediate thinking steps. LightThinker [101] proposes to train LLMs to learn when and how to compress intermediate reasoning steps. Instead of storing long thought chains, LightThinker compresses verbose reasoning into compact gist tokens to reduce memory and computational costs. Implementing this summarization paradigm requires sparse-patterned attention mask, ensuring the model focuses only on essential compressed representations. InftyThink [94] introduces an iterative reasoning method that enables essentially infinite reasoning chains while maintaining strong accuracy without surpassing the context window limit. It achieves this by iteratively generating thought, summarizing it, and discarding previous thoughts and summaries, retaining only the most recent summary. Additionally, InftyThink provides technique for converting existing reasoning datasets into an iterative format for training models under this paradigm. 5 Input Prompts-based Efficient Reasoning From the perspective of input prompts and questions, these works focus on enforcing length constraints or routing LLMs based on the characteristics of input prompts to enable concise and efficient reasoning. 5.1 Prompt-guided Efficient Reasoning Prompt-guided efficient reasoning explicitly instructs LLMs to generate fewer reasoning steps, can be straightforward and highly effective method for improving the efficiency of reasoning models. As shown in Table 5, different methods propose different prompts to ensure concise reasoning outputs from the model. The key question is: Which prompts can accurately control the reasoning length of LLMs? Enforcing Concise Reasoning via Varying Prompts. Token-Budget [32] proposes setting token budget in prompts to reduce unnecessary reasoning tokens. To optimize efficiency while preserving accuracy, [32] introduced TALE-EP, training-free, zero-shot method for budget estimation. TALE13 Table 5: summary of prompts used with reasoning models to generate concise reasoning outputs. For further details, refer to Section 5.1. Method Prompt TALE-EP [32] Budget Estimation: (...) Task: Analyze the given question and estimate the minimum number of tokens required to generate complete and accurate response. Please give the response by strictly following this format: [[budget]], for example, Budget: [[12]]. Token-budget-aware CoT: Please answer the above question. Lets think step by step and use less than <Token-Budget> tokens. CoD [92] Think step by step, but only keep minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after separator ####. CCoT [65] Be concise. Token Complexity [40] BulletPoints (...) only use bullet points. OnlyNumbers (...) only use numbers or equations. NoSpaces (...) do not use any spaces or line breaks. NoProperGrammar (...) do not use proper grammar. AbbreviateWords (...) abbreviate words as much as possible. WordLimit(k) (...) use at most words. {1, . . . , 100} CharLimit(k) (...) use at most letters. {1, . . . , 500} TokenLimit(k) (...) use at most tokens. {1, . . . , 500} StepLimit(k) (...) use at most steps. {1, . . . , 5} ChineseCoT (...) Respond in Chinese ChineseCoT(k) (...) Use at most Chinese characters. {1, . . . , 500} EP first estimates reasonable token budget by prompting the LLM itself. It then incorporates this estimate into prompt that specifies the token constraint, guiding the LLM to generate more token-efficient yet accurate response. CoD [92] observes that LLMs often generate excessively verbose reasoning steps, whereas humans typically record only the most essential insights. To enhance reasoning efficiency, they propose Chain-of-Draft prompting. Similar to CoT prompting, CoD encourages step-by-step reasoning but introduces policies to limit verbosity. For instance, their prompt instructs: Think step by step, but only keep minimum draft for each thinking step, with at most five words. They find that this approach preserves the necessary intermediate steps while maintaining accuracy, significantly reducing token usage. [40] systematically studies the relationship between reasoning length and model accuracy across various prompts with explicit compression instructions (e.g., use 10 words or less). Their analysis reveals universal trade-off between reasoning length and accuracy, showing that different prompt-based compression strategies align on the same accuracy-compression curve. They hypothesize that each task has an intrinsic token complexity, the minimum number of tokens required for successful problem-solving. By computing information-theoretic limits on the accuracy-compression trade-off, they found that existing prompt-based compression methods fall far short of these limits, indicating significant room for improvement. [65] introduced Concise Chain-of-Thought (CCoT) prompting, technique that prompts LLMs to perform step-by-step reasoning while explicitly instructing them to be concise. Fine-tuning after Prompting. As noted in Section 3, some approaches collect short CoT data using prompt-based methods, then apply SFT to develop an efficient reasoning model [32]. Beyond performing direct prompt-based reasoning, these fine-tuned models often deliver more promising performance when tackling complex reasoning challenges. 5.2 Prompts Attribute-Driven Reasoning Routing User-provided prompts can range from easy to difficult tasks. Routing strategies for efficient reasoning dynamically determine how language models handle queries based on their complexity and uncertainty. Ideally, reasoning models can automatically assign simpler queries to faster but less 14 reasoning-capable LLMs, while directing more complicated queries to slower but stronger reasoning LLMs. The key question is: What criterion should be used to determine the attributes (e.g., difficulty) of prompts? Unknown Criteria. Anthropic releases Claude 3.7 Sonnet [2], notable for being the first hybrid reasoning model. Claude 3.7 Sonnet was developed through RL, enabling it to allocate more time to complex reasoning tasks that require deeper analysis, ultimately producing better results. The model offers two response modes: quick answers or step-by-step thinking. Users can leverage API to manage the amount of time the model spends thinking. Although the specifics of the routing criterion remain unclear, Claude 3.7 Sonnet represents the first hybrid reasoning model, setting foundation for subsequent routing-based large reasoning models. Training Classifier. RouteLLM [60] trains query router to dispatch incoming queries to suitable LLMs based on complexity. The authors utilize substantial amount of preference data collected from Chatbot Arena as training data, enabling effective routing decisions for question-answering and reasoning tasks. Consequently, simpler queries are directed to low-latency LLMs, while complex queries are assigned to higher-latency, more powerful LLMs, significantly accelerating overall reasoning efficiency. Sketch-of-Thought (SoT) [4] leverages routing and prompting to minimize token usage during reasoning. lightweight DistilBERT-based router dynamically selects the most suitable paradigm based on the characteristics of the questions. Inspired by cognitive science, SoT employs three distinct paradigms: Conceptual Chaining, which connects ideas with minimal verbalization; Chunked Symbolism, which structures mathematical reasoning into concise symbolic representations; and Expert Lexicons, which adopts domain-specific shorthand used by experts. Uncertainty. Besides relying on additional routers, Self-Ref [15] enables LLMs to autonomously decide when to route by extracting intrinsic uncertainty scores as self-routing indicators. Specifically, they fine-tune uncertainty-specialized tokens within the LLMs to align uncertainty predictions with prediction correctness in both question-answering and reasoning tasks. This ensures that only uncertain or incorrect outputs trigger routing to more capable LLMs, which decreases the latency of LLM inference. Confident or Seek Stronger [14] aims to provide calibrated data for predicting and initializing routing strategies in both LLM question-answering and reasoning tasks without requiring access to user queries. This approach enables more efficient and reliable decision-making in determining whether an LLM should confidently generate an answer or escort the query to stronger model, ultimately improving reasoning efficiency from query-level perspective in online LLM service scenarios."
        },
        {
            "title": "6 Reasoning Abilities via Efficient Training Data and Model Compression",
            "content": "6.1 Training Reasoning Models with Less Data Improving the efficiency of reasoning models requires optimizing not just the model architecture but also the data used for training. Recent work has shown that carefully selecting, structuring, and leveraging training data can significantly reduce data requirements while maintaining or even improving reasoning performance. Although all approaches focus on efficient data selection, they vary in defining and utilizing efficiency. The key question is: How to construct less but high-quality training data? Minimal but High-Impact Data Selection. LIMO [97] challenges the conventional belief that complex reasoning tasks require extensive training data. They introduce LIMO, framework that elicits sophisticated reasoning abilities using minimal but precisely curated examples. By choosing high-quality questions based on Level of difficulty, Generality, and Knowledge Diversity and highquality solutions based on Optimal Structural Organization, Effective Cognitive Scaffolding, and 15 Rigorous Verification, with only 817 carefully selected training samples, LIMO can outperform previous models that utilized over 100,000 examples. s1 [58] focuses on enhancing reasoning performance by controlling test-time computational resources. They curate compact dataset based on Quality, Difficulty and Diversity, s1K, comprising 1,000 high-quality questions paired with reasoning traces. Through supervised fine-tuning on this dataset and implementing budget forcing, which regulates the reasoning duration during inference, s1-32B exceeds OpenAI o1-preview on MATH and AIME24, demonstrating that strategic test time scaling can effectively enhance reasoning capabilities without extensive training data. Self-Verification as Data-Efficient Training Signal. S2R [55] infuse LLMs with self-verification and self-correction abilities through RL. Initially, models are fine-tuned on curated dataset to establish these capabilities. Subsequently, RL both at the outcome level and the process level is employed to enhance these skills further. With only 3,100 initialization samples, their fine-tuned models consistently improve the performance on reasoning tasks among all base models. S2R fine-tuned Qwen2.5-Math-7B can outperform models trained on comparable amounts of long CoT distilled data on the MATH500 and GSM8K. 6.2 Reasoning Capabilities of Small Language Models via Distillation and Model Compression LLMs have demonstrated remarkable reasoning capabilities across various complex tasks, benefiting from their extensive training on diverse datasets. However, their substantial computational and memory demands pose challenges for deployment in resource-constrained environments, such as edge devices, mobile applications, and real-time systems. In scenarios where efficiency, cost, or latency is primary concern, Small Language Models (SLMs) offer viable alternative. The ability of SLMs to retain strong reasoning capabilities while operating under strict resource constraints is crucial for expanding the accessibility and practicality of AI-powered reasoning systems. To achieve this, two main categories of approach are explored: Distillation and Model Compression. The key question is: How do small language models perform on reasoning tasks? What impact does model compression (e.g., quantization) have on their reasoning abilities? Distillation. Distillation is crucial technique for transferring the reasoning capabilities of LLMs to SLMs while maintaining efficiency. However, [43] finds phenomenon named Small Model Learnability Gap, which highlights the challenges of distilling complex reasoning processes from large model to small model, showing that SLMs struggle to emulate the reasoning depth of their larger counterparts. To address this, various approaches have been proposed. Both [43] and [13] explored mixed distillation, with [43] blending long and short CoT reasoning examples, while [13] combined CoT and PoT (Program of Thought) to improve the effectiveness of knowledge distillation from LLMs to SLMs on specific tasks. In comparison, [26] introduced counterfactual distillation, augmenting the training set by masking causal features in the original question, prompting the LLM to complete the masked text, and generating multi-view CoT (positive and negative views) of each data for enhancing the effectiveness of knowledge distillation. In addition, [105] developed feedback-driven distillation technique that iteratively refines distillation datasets. They first prompt an LLM to generate an initial distillation dataset, then expand it by creating diverse and complex questions from existing ones, and finally, this enriched dataset is used to fine-tune SLMs. Another strategy, proposed by [104], incorporates probing and retrieval mechanisms into the distillation pipeline. It trains two complementary distilled SLMs, probing model and reasoning model, where the probing model retrieves relevant knowledge, which the reasoning model then uses to construct step-by-step rationale for the answer. [9] introduced adaptive thinking during distillation, allowing the models to dynamically adjust reasoning strategies based on the complexity of the task. Furthermore, [46] proposed SKIntern, framework that internalizes symbolic knowledge into SLM to improve CoT reasoning quality and efficiency, while [102] introduces SCORE, pipeline that generates self-correction data from SLMs and fine-tunes the model to function as self-correcting reasoner. These diverse distillation techniques demonstrate that efficiently transferring reasoning capabilities from LLMs to SLMs requires not only reducing the model size but also carefully and strategically structuring the knowledge transfer process to preserve logical depth and generalization. 16 Pruning and Quantization. Beyond directly distilling knowledge from LLMs to SLMs, an alternative approach involves compressing an LLM into an SLM using techniques such as quantization and pruning. [73] conducted comprehensive study analyzing the impact of various model compression techniques on reasoning ability. Their findings reveal that quantization, which reduces model precision to lower-bit representations, preserves reasoning performance remarkably well, allowing SLMs to maintain logical coherence and problem-solving capabilities while significantly reducing memory and computational costs. In contrast, pruning, which removes specific weights or neurons in the model based on their importance, leads to severe degradation in reasoning quality, disrupting the models ability to follow multi-step logical processes. This suggests that compression-based approaches are more effective than training SLMs from scratch, as they allow models to retain reasoning structures inherited from LLMs. However, critical challenge remains: SLMs often struggle with the instruction following, indicating that compression alone is insufficient. Additional fine-tuning or adaptation methods may be required to align compressed models with user intent and ensure they can effectively interpret and execute complex reasoning tasks."
        },
        {
            "title": "7 Evaluation and Benchmark",
            "content": "Recent research has introduced innovative benchmarks and evaluation frameworks to systematically assess the reasoning capabilities of LLMs. As LLMs continue to advance in their ability to perform complex reasoning tasks, the need for rigorous, standardized evaluation metrics and frameworks has become increasingly important. Sys2Bench. [62] develops Sys2Bench, which is comprehensive suite designed to evaluate LLMs across five reasoning categories, including arithmetic, logical, commonsense, algorithmic, and planning tasks. This benchmark comprises eleven diverse datasets, covering various reasoning tasks. It includes GSM8K and AQuA for arithmetic problems, StrategyQA and HotPotQA for commonsense reasoning, ProntoQA for logical reasoning, Game of 24 and Bin Packing for algorithmic tasks, and BlocksWorld, Rubiks Cube, TripPlan, and Calendar Plan for planning tasks. The study revealed that scaling inference-time computation alone has limitations, as no single technique consistently excels across all reasoning tasks, and this emphasizes the need for diverse approaches to enhance LLM reasoning capabilities. Evaluating Overthinking. [19] introduces framework to systematically analyze the \"overthinking\" in LLMs, where models favor extended internal reasoning over necessary environmental interactions. By examining 4,018 trajectories in agentic tasks, the study identified patterns such as Analysis Paralysis, Rogue Actions, and Premature Disengagement. [19] also proposed novel overthinking score and showed strong correlation between higher scores and decreased task performance. Mitigation strategies such as selecting solutions with lower overthinking scores can improve performance by 30% and at the same time reduce computational overhead by 43%. Compute-Optimal Test-Time Scaling (TTS). [49] investigates the impact of TTS strategies on LLM performance, focusing on how policy models, process reward models, and problem difficulty influence TTS effectiveness. Their findings indicate that compute-optimal TTS strategies are highly dependent on these factors. The paper finds that, with appropriate TTS strategies, smaller models (e.g., 1B parameter LLM) are able to outperform significantly larger models (e.g., 405B parameter LLM) on complex reasoning tasks like MATH-500, and this underscores the importance of tailored TTS approaches in evaluating and enhancing LLM reasoning."
        },
        {
            "title": "8 Applications and Discussion",
            "content": "8.1 Applications Autonomic Driving. Efficient reasoning LLMs are able to greatly improve autonomic driving [20] by helping them understand large amounts of sensor data in human-like way. They make the cars better at making decisions, so the vehicles can plan for difficult driving situations and react quickly when unexpected events occur. By combining information from cameras, LiDAR, radar, and other 17 sensors, these models help cars drive more safely, choose better routes, and assess risks as they happen. Moreover, because they can explain why they make certain decisions, both passengers and regulators feel more confident in the technology, and the cars can interact more smoothly with smart road systems. Embodied AI. Efficient reasoning LLMs make embodied AI [25] much smarter by helping robots and smart devices understand and react to the world around them. These models process lots of data from cameras, sensors, and other inputs in way that resembles human thinking. This deep understanding means that robot can quickly decide the best way to move, handle unexpected changes, and interact safely with people. For example, in busy factory or home setting, robot using these models can navigate obstacles, adjust to new situations, and even explain its actions in simple terms. Altogether, efficient reasoning LLMs boost the reliability, safety, and usefulness of embodied AI systems in daily environments. Healthcare. Efficient reasoning LLMs would improve healthcare [34] by helping doctors and researchers work with large amounts of medical data more easily. They can quickly analyze patient records, test results, and medical research to spot important trends and patterns that might be hard to see otherwise. This support can lead to faster and more accurate diagnoses, better treatment recommendations, and fewer mistakes. In addition, these models can break down complex medical information into plain language, making it easier for both medical professionals and patients to understand. Generally, efficient reasoning LLMs make healthcare processes smoother and more reliable, leading to better care and outcomes for patients. 8.2 Discussion Improving Reasoning Ability. From another perspective on efficiency, improving reasoning performance is an important topic [11, 75]. To prioritize promising avenues by discarding ineffective strategies early, Meta-Reasoner [75] leverages contextual multi-armed bandits for evaluating reasoning progress and selecting the optimal strategy. In each round, the LLM produces new reasoning step, and the meta-reasoner evaluates its output and generates progress report, the meta-reasoner uses contextual multi-arm bandit to choose the best guidance strategy for the reasoning step. ITT [11] treats each transformer layer as step in an internal thinking process. By dynamically allocating extra processing to difficult tokens through adaptive routing, ITT enables smaller language models to achieve performance comparable to larger models while using fewer training resources. Safety of Efficient Reasoning. Safety and efficiency in LLMs often pull in opposite directions, as optimizing one always leads to the performance degradation of the other. When enhancing safety, such as filtering harmful content, mitigating adversarial attacks, and enabling self-correction, the reasoning model typically requires additional computational resources and longer reasoning sequences, leading to increased inference costs and slower response times. Conversely, prioritizing efficiency by minimizing token usage and computational overhead may reduce the reasoning ability to self-reflect, verify its outputs, or defend against adversarial manipulations. This trade-off reflects the well-known principle that there is no free lunch, making it crucial to strike careful balance between safety and efficiency. [39] investigates the robustness of safety checks in large CoT reasoning models, revealing severe security flaws in commercial systems. They introduce the malicious-educator benchmark and demonstrate that with their hijacking Chain-of-Thought (H-CoT) attack, models can drastically reduce their refusal rates, leading to the generation of harmful content. [42] investigate the safety of long reasoning models. It is observed that while longer outputs enable self-correction and enhance safety, some attack strategies exploit extended generations. They propose dynamic output length control via an RL-based method to maintain both reasoning quality and security. Balancing safety and efficiency in long reasoning models remains challenging yet crucial area of investigation. RL vs. SFT, which is better? When comparing RL (Section 3.1) and SFT (Section 3.2) for creating efficient reasoning language models, the answer is unclear as each method has its own strengths. RL allows model to learn by trial and error, rewarding it for satisfactory decisions, which can assist it find creative ways to solve problems in new situations. However, this approach can sometimes be unpredictable and require lot of training. On the other hand, SFT teaches the model using carefully chosen efficient CoT examples constructed by either humans or models, leading to more consistent behavior and easier control. Yet, SFT might struggle when faced with challenges that are not covered in its training data. In practice, combining both methods might be promising 18 direction and potentially works best because it harnesses the creativity of RL and the reliability of SFT, resulting in model that is both adaptable and stable."
        },
        {
            "title": "9 Conclusion",
            "content": "This paper provides the first structured survey of efficient reasoning in LLMs, categorizing existing approaches into three areas: model-based, reasoning output-based, and input prompts-based methods. Additionally, it discusses efficient data utilization, reasoning capabilities of smaller models, evaluation techniques, and benchmarking, accompanied by continuously updated public repository to support future research. Crucially, efficient reasoning approaches offer significant practical benefits across various domains: reducing computational costs in healthcare diagnostics, enhancing real-time decision-making and safety in autonomous driving, boosting the reliability and usefulness of embodied AI systems, and enabling quicker, more profitable responses in financial algorithmic trading and risk assessment. These advancements highlight the broad economic and societal value of efficient reasoning in LLMs."
        },
        {
            "title": "References",
            "content": "[1] Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. 4, 6, 7 [2] Anthropic. Claude 3.7 sonnet, 2023. Accessed: March 10, 2025. 4, 15 [3] Daman Arora and Andrea Zanette. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463, 2025. 4, 6 [4] Simon Aytes, Jinheon Baek, and Sung Ju Hwang. Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching. arXiv preprint arXiv:2503.05179, 2025. 4, [5] Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute with open models. 12 [6] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1768217690, 2024. 3 [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 1 [8] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. 1 [9] Xiaoshu Chen, Sihang Zhou, Ke Liang, and Xinwang Liu. Distilling reasoning ability from large language models with adaptive thinking. arXiv preprint arXiv:2404.09170, 2024. 4, [10] Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. 2, 5 [11] Yilong Chen, Junyuan Shang, Zhenyu Zhang, Yanxi Xie, Jiawei Sheng, Tingwen Liu, Shuohuan Wang, Yu Sun, Hua Wu, and Haifeng Wang. Inner thinking transformer: Leveraging dynamic depth scaling to foster adaptive internal thinking. arXiv preprint arXiv:2502.13842, 2025. 18 [12] Jeffrey Cheng and Benjamin Van Durme. Compressed chain of thought: Efficient reasoning through dense representations. arXiv preprint arXiv:2412.13171, 2024. 4, 10 [13] Li Chenglin, Qianglong Chen, Liangyue Li, Caiyu Wang, Feng Tao, Yicheng Li, Zulong Chen, and Yin Zhang. Mixed distillation helps smaller language models reason better. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 16731690, 2024. 4, 16 [14] Yu-Neng Chuang, Leisheng Yu, Guanchu Wang, Lizhe Zhang, Zirui Liu, Xuanting Cai, Yang Sui, Vladimir Braverman, and Xia Hu. Confident or seek stronger: Exploring uncertainty-based on-device llm routing from benchmarking to generalization. arXiv preprint arXiv:2502.04428, 2025. 15 [15] Yu-Neng Chuang, Helen Zhou, Prathusha Kameswara Sarma, Parikshit Gopalan, John Boccio, Sara Bolouki, and Xia Hu. Learning to route with confidence tokens. arXiv preprint arXiv:2410.13284, 2024. 15 [16] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 1 [17] Codeforces. Codeforces - competitive programming platform, 2025. Accessed: 2025-03-18. 1 [18] Rémi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International conference on computers and games, pages 7283. Springer, 2006. 4 [19] Alejandro Cuadron, Dacheng Li, Wenjie Ma, Xingyao Wang, Yichuan Wang, Siyuan Zhuang, Shu Liu, Luis Gaspar Schroeder, Tian Xia, Huanzhi Mao, Nicholas Thumiger, Aditya Desai, Ion Stoica, Ana Klimovic, Graham Neubig, and Joseph E. Gonzalez. The danger of overthinking: Examining the reasoning-action dilemma in agentic tasks, 2025. 4, [20] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, et al. survey on multimodal large language models for autonomous driving. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 958979, 2024. 17 [21] Yingqian Cui, Pengfei He, Jingying Zeng, Hui Liu, Xianfeng Tang, Zhenwei Dai, Yan Han, Chen Luo, Jing Huang, Zhen Li, et al. Stepwise perplexity-guided refinement for efficient chain-of-thought reasoning in large language models. arXiv preprint arXiv:2502.13260, 2025. 4 [22] Yuntian Deng, Yejin Choi, and Stuart Shieber. From explicit cot to implicit cot: Learning to internalize cot step by step. arXiv preprint arXiv:2405.14838, 2024. 9 [23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 41714186, 2019. 1 [24] Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, et al. Dynamic parallel tree search for efficient llm reasoning. arXiv preprint arXiv:2502.16235, 2025. 4, 11, 12 [25] Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu, and Cheston Tan. survey of embodied ai: From simulators to research tasks. IEEE Transactions on Emerging Topics in Computational Intelligence, 6(2):230244, 2022. [26] Tao Feng, Yicheng Li, Li Chenglin, Hao Chen, Fei Yu, and Yin Zhang. Teaching small language models reasoning through counterfactual distillation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 58315842, 2024. 4, 16 [27] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. In The Eleventh International Conference on Learning Representations. OpenReview, 2023. 2 [28] Yichao Fu, Junda Chen, Siqi Zhu, Zheyu Fu, Zhongdongming Dai, Aurick Qiao, and Hao Zhang. Efficiently serving llm reasoning programs with certaindex. arXiv preprint arXiv:2412.20993, 2024. 4, 11, 12 [29] Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: recurrent depth approach. arXiv preprint arXiv:2502.05171, 2025. 9 [30] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 1 [31] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, 2, 4, 5, 6, 7, 11 [32] Tingxu Han, Chunrong Fang, Shiyu Zhao, Shiqing Ma, Zhenyu Chen, and Zhenting Wang. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. 2, 4, 8, 9, 13, 14 [33] Shibo Hao, Sainbayar Sukhbaatar, DiJia Su, Xian Li, Zhiting Hu, Jason Weston, and Yuandong Tian. Training large language models to reason in continuous latent space. arXiv preprint arXiv:2412.06769, 2024. 2, 4, 9, 10 [34] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694, 2023. 18 [35] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021. 1 [36] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 9 [37] Yu Kang, Xianghui Sun, Liangyu Chen, and Wei Zou. C3ot: Generating shorter chain-ofthought without compromising effectiveness. arXiv preprint arXiv:2412.11664, 2024. 4, 8, [38] Levente Kocsis and Csaba Szepesvári. Bandit based monte-carlo planning. In European conference on machine learning, pages 282293. Springer, 2006. 4 [39] Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, DaCheng Juan, Hai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893, 2025. 18 [40] Ayeong Lee, Ethan Che, and Tianyi Peng. How well do llms compress their own chain-ofthought? token complexity approach. arXiv preprint arXiv:2503.01141, 2025. 4, 14 [41] Peiji Li, Kai Lv, Yunfan Shao, Yichuan Ma, Linyang Li, Xiaoqing Zheng, Xipeng Qiu, and Qipeng Guo. Fastmcts: simple sampling strategy for data synthesis. arXiv preprint arXiv:2502.11476, 2025. 4, 11, 12 [42] Xuying Li, Zhuo Li, Yuji Kosuga, and Victor Bian. Output length effect on deepseek-r1s safety in forced thinking. arXiv preprint arXiv:2503.01923, 2025. [43] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. 4, 16 [44] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. 1 [45] Baohao Liao, Yuhui Xu, Hanze Dong, Junnan Li, Christof Monz, Silvio Savarese, Doyen Sahoo, and Caiming Xiong. Reward-guided speculative decoding for efficient llm reasoning. arXiv preprint arXiv:2501.19324, 2025. 4, 11, 12 [46] Huanxuan Liao, Shizhu He, Yupu Hao, Xiang Li, Yuanzhe Zhang, Jun Zhao, and Kang Liu. Skintern: Internalizing symbolic knowledge for distilling better cot capabilities into small language models. In Proceedings of the 31st International Conference on Computational Linguistics, pages 32033221, 2025. 4, 16 [47] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. 2 [48] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. 2 [49] Runze Liu, Junqi Gao, Jian Zhao, Kaiyan Zhang, Xiu Li, Biqing Qi, Wanli Ouyang, and Bowen Zhou. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling, 2025. 4, 17 [50] Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Can language models learn to skip steps? arXiv preprint arXiv:2411.01855, 2024. 4, 8, 9 [51] Yuliang Liu, Junjie Lu, Zhaoling Chen, Chaofeng Qu, Jason Klein Liu, Chonghan Liu, Zefan Cai, Yunhui Xia, Li Zhao, Jiang Bian, et al. Adaptivestep: Automatically dividing reasoning step through model confidence. arXiv preprint arXiv:2502.13943, 2025. 4 [52] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. 2 [53] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [54] Haotian Luo, Li Shen, Haiying He, Yibo Wang, Shiwei Liu, Wei Li, Naiqiang Tan, Xiaochun Cao, and Dacheng Tao. O1-pruner: Length-harmonizing fine-tuning for o1-like reasoning pruning. arXiv preprint arXiv:2501.12570, 2025. 2, 4, 6, 7 [55] Ruotian Ma, Peisong Wang, Cheng Liu, Xingyan Liu, Jiaqi Chen, Bang Zhang, Xin Zhou, Nan Du, and Jia Li. S2r: Teaching llms to self-verify and self-correct via reinforcement learning, 2025. 4, 16 [56] Xinyin Ma, Guangnian Wan, Runpeng Yu, Gongfan Fang, and Xinchao Wang. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. 2, 4, 8, 9 [57] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198 124235, 2024. 6, 7 [58] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. 4, 16 [59] Tergel Munkhbat, Namgyu Ho, Seohyun Kim, Yongjin Yang, Yujin Kim, and Se-Young Yun. Self-training elicits concise reasoning in large language models. arXiv preprint arXiv:2502.20122, 2025. 4, 8, [60] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data. arXiv preprint arXiv:2406.18665, 2024. 15 [61] OpenAI. Learning to reason with llms. urlhttps://openai.com/index/learning-to-reason-with-llms/. Accessed: 15 March 2025. 1, 2, 6 [62] Shubham Parashar, Blake Olson, Sambhav Khurana, Eric Li, Hongyi Ling, James Caverlee, and Shuiwang Ji. Inference-time computations for llm reasoning and planning: benchmark and insights, 2025. 4, 17 [63] Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. 9 [64] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. [65] Matthew Renze and Erhan Guven. The benefits of concise chain of thought on problemsolving in large language models. In 2024 2nd International Conference on Foundation and Large Language Models (FLLM), pages 476483. IEEE, 2024. 4, 14 [66] Nikunj Saunshi, Nishanth Dikkala, Zhiyuan Li, Sanjiv Kumar, and Sashank Reddi. Reasoning with latent thoughts: On the power of looped transformers. arXiv preprint arXiv:2502.17416, 2025. 4, 10 [67] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 6 22 [68] Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Efficient reasoning with hidden thinking. arXiv preprint arXiv:2501.19201, 2025. 4, 10 [69] Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, and Shiguo Lian. Dast: Difficulty-adaptive slow-thinking for large reasoning models. arXiv preprint arXiv:2503.04472, 2025. 4, 6, 7 [70] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chain-of-thought into continuous space via self-distillation. arXiv preprint arXiv:2502.21074, 2025. 4, 10 [71] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. nature, 550(7676):354359, 2017. 2 [72] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. 12 [73] Gaurav Srivastava, Shuxiang Cao, and Xuan Wang. Towards reasoning ability of small language models. arXiv preprint arXiv:2502.11569, 2025. 4, 17 [74] DiJia Su, Hanlin Zhu, Yingchen Xu, Jiantao Jiao, Yuandong Tian, and Qinqing Zheng. Token assorted: Mixing latent and text tokens for improved language model reasoning. arXiv preprint arXiv:2502.03275, 2025. 4, 10 [75] Yuan Sui, Yufei He, Tri Cao, Simeng Han, and Bryan Hooi. Meta-reasoner: Dynamic guidance for optimized inference-time reasoning in large language models. arXiv preprint arXiv:2502.19918, 2025. 18 [76] Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. Fast best-of-n decoding via speculative rejection. arXiv preprint arXiv:2410.20290, 2024. 4, 11, 12, 13 [77] Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu Zhu, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, et al. Tinyr1-32b-preview: Boosting accuracy with branch-merge distillation. arXiv preprint arXiv:2503.04872, 2025. 4 [78] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. 4, 5, 6, [79] Qwen Team. Qwq-32b-preview. urlhttps://qwenlm.github.io/blog/qwq-32b-preview/. Accessed: 15 March 2025. 5, 6 [80] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [81] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. 12 [82] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [83] Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. arXiv preprint arXiv:2408.17017, 2024. 4 [84] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. 2, 3 [85] Yiming Wang, Pei Zhang, Siyuan Huang, Baosong Yang, Zhuosheng Zhang, Fei Huang, and Rui Wang. Sampling-efficient test-time scaling: Self-estimating the best-of-n sampling in early decoding. arXiv preprint arXiv:2503.01422, 2025. 4, 11, 12, 13 [86] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 1, 3 23 [87] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025. 4 [88] Yuyang Wu, Yifei Wang, Tianqi Du, Stefanie Jegelka, and Yisen Wang. When more is less: Understanding chain-of-thought length in llms. arXiv preprint arXiv:2502.07266, 2025. 4, 11, [89] Heming Xia, Yongqi Li, Chak Tou Leong, Wenjie Wang, and Wenjie Li. Tokenskip: Controllable chain-of-thought compression in llms. arXiv preprint arXiv:2502.12067, 2025. 4, 8, 9 [90] Kun Xiang, Zhili Liu, Zihao Jiang, Yunshuang Nie, Kaixin Cai, Yiyang Yin, Runhui Huang, Haoxiang Fan, Hanhui Li, Weiran Huang, et al. Can atomic step decomposition enhance the self-structured reasoning of multimodal large models? arXiv preprint arXiv:2503.06252, 2025. 4 [91] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686, 2025. 1 [92] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. 4, 14 [93] Yige Xu, Xu Guo, Zhiwei Zeng, and Chunyan Miao. Softcot: Soft chain-of-thought for efficient reasoning with llms. arXiv preprint arXiv:2502.12134, 2025. 4, 10 [94] Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, and Yueting Zhuang. Inftythink: Breaking the length limits of long-context reasoning in large language models. arXiv preprint arXiv:2503.06692, 2025. 4, 11, 13 [95] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 1 [96] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. 2, 3 [97] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning, 2025. 4, [98] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. 2, 4, 6 [99] Ping Yu, Jing Xu, Jason Weston, and Ilia Kulikov. Distilling system 2 into system 1. arXiv preprint arXiv:2407.06023, 2024. 4, 8, 9 [100] Zishun Yu, Tengyu Xu, Di Jin, Karthik Abinav Sankararaman, Yun He, Wenxuan Zhou, Zhouhao Zeng, Eryk Helenowski, Chen Zhu, Sinong Wang, et al. Think smarter not harder: Adaptive reasoning with inference aware optimization. arXiv preprint arXiv:2501.17974, 2025. 4 [101] Jintian Zhang, Yuqi Zhu, Mengshu Sun, Yujie Luo, Shuofei Qiao, Lun Du, Da Zheng, Huajun Chen, and Ningyu Zhang. Lightthinker: Thinking step-by-step compression. arXiv preprint arXiv:2502.15589, 2025. 4, 11, [102] Yunxiang Zhang, Muhammad Khalifa, Lajanugen Logeswaran, Jaekyeom Kim, Moontae Lee, Honglak Lee, and Lu Wang. Small language models need strong verifiers to self-correct reasoning. arXiv preprint arXiv:2404.17140, 2024. 16 [103] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. 2 [104] Yichun Zhao, Shuheng Zhou, and Huijia Zhu. Probe then retrieve and reason: Distilling In Proceedings of the probing and reasoning capabilities into smaller language models. 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1302613032, 2024. 4, 16 24 [105] Xunyu Zhu, Jian Li, Can Ma, and Weiping Wang. capabilities of small language models via feedback-driven distillation. arXiv:2411.14698, 2024. 4,"
        },
        {
            "title": "Improving mathematical reasoning\narXiv preprint",
            "content": ""
        }
    ],
    "affiliations": [
        "Department of Computer Science, Rice University"
    ]
}