{
    "paper_title": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement",
    "authors": [
        "Runhui Huang",
        "Chunwei Wang",
        "Junwei Yang",
        "Guansong Lu",
        "Yunlong Yuan",
        "Jianhua Han",
        "Lu Hou",
        "Wei Zhang",
        "Lanqing Hong",
        "Hengshuang Zhao",
        "Hang Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ILLUME+ that leverages dual visual tokenization and a diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities in a unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization, due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. To mitigate this, LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces a unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling a coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ a diffusion model as the image detokenizer for enhanced generation quality and efficient super-resolution. ILLUME+ follows a continuous-input, discrete-output scheme within the unified MLLM and adopts a progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding, generation, and editing benchmarks. With its strong performance, ILLUME+ provides a scalable and versatile foundation for future multimodal applications. Project Page: https://illume-unified-mllm.github.io/."
        },
        {
            "title": "Start",
            "content": "ILLUME+: Illuminating Unified MLLM with Dual Visual Tokenization and Diffusion Refinement Runhui Huang2 Chunwei Wang1 Junwei Yang1 Guansong Lu1 Yunlong Yuan1 Jianhua Han1 Lu Hou1 Wei Zhang1 Lanqing Hong1 Hengshuang Zhao2 Hang Xu1 1Huawei Noahs Ark Lab, 2The University of Hong Kong 5 2 0 2 2 ] . [ 1 4 3 9 1 0 . 4 0 5 2 : r Figure 1: ILLUME+ can understand and generate images at any resolution. Compared to our previous work, ILLUME [63], it demonstrates improved texture preservation in image editing tasks. Equal contribution, Corresponding author, Project leader. Preprint."
        },
        {
            "title": "Abstract",
            "content": "We present ILLUME+, an enhanced version of the previous ILLUME model, which leverages dual visual tokenization and diffusion decoder to improve both deep semantic understanding and high-fidelity image generation. Existing unified models have struggled to simultaneously handle the three fundamental capabilities expected of unified model: understanding, generation, and editing. Models like Chameleon and EMU3 utilize VQGAN for image discretization. Due to the lack of deep semantic interaction, they lag behind specialist models like LLaVA in visual understanding tasks. LaViT and ILLUME employ semantic encoders for tokenization, but they struggle with image editing due to poor texture preservation. Meanwhile, Janus series decouples the input and output image representation, limiting their abilities to seamlessly handle interleaved image-text understanding and generation. In contrast, ILLUME+ introduces unified dual visual tokenizer, DualViTok, which preserves both fine-grained textures and text-aligned semantics while enabling coarse-to-fine image representation strategy for multimodal understanding and generation. Additionally, we employ diffusion model as the image detokenizer to enhance generation quality and enable efficient super-resolution. ILLUME+ follows continuous-input, discrete-output scheme within the unified Multimodal Large Language Model (MLLM) and adopts progressive training procedure that supports dynamic resolution across the vision tokenizer, MLLM, and diffusion decoder. This design allows for flexible and efficient context-aware image editing and generation across diverse tasks. ILLUME+ (3B) exhibits competitive performance against existing unified MLLMs and specialized models across multimodal understanding generation, and editing benchmarks. Its support for flexible high-resolution images enhances visual understanding tasks and enables detailed image synthesis up to 10241024 resolution. With its strong performance, ILLUME+ provides scalable foundation for future multimodal model applications. Project Page: https://illume-unified-mllm.github.io/. Code and models will be publicly available soon."
        },
        {
            "title": "Introduction",
            "content": "What cannot create, do not understand. Richard Feynman Recent advancements in Large Language Models (LLMs) have significantly enhanced their capability to handle multimodal tasks, particularly by integrating visual inputs into language models. Efforts such as the LLaVA series and the QwenVL series [38, 37, 1, 64] have demonstrated remarkable visual comprehension performance. Meanwhile, the development of text-to-image generation models, such as diffusion-based approaches [53, 50, 51, 54] and more recent autoregressive approaches [52, 16, 74], has made substantial strides in generating high-fidelity images. These developments have driven the push towards creating unified Multimodal Large Language Models (MLLMs) that seamlessly integrate both visual understanding and generation capabilities. unified model holds promise not only for advancing task coordination and generalization but also for contributing to the exploration of artificial general intelligence (AGI). By merging understanding and generation capabilities within single framework, unified models can genuinely grasp the deep relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. To build such unified models in an autoregressive framework, existing approaches have explored several distinct paradigms. As illustrated in Fig. 2 (a), the earliest models, e.g., Chameleon [59] and Emu3 [65], utilized VQGAN [18] to discretize images, enabling shared vocabulary for text and vision within an autoregressive (AR) framework. However, even with large-scale training, they lag behind models like LLaVA [38] in visual understanding tasks. To address this issue, works like LaViT [28] and ILLUME [63] (Fig. 2 (b)) learn the semantic codebook from the pretrained text-aligned semantic encoders [78, 84], and employ diffusion models to reconstruct images based on semantic tokens. This improves image-text alignment in MLLM pretraining and achieves strong performance in both understanding and generation tasks. However, the lack of texture preservation in vision tokenizers limits their capability in fine-grained image editing. To mitigate this, another 2 Figure 2: Characteristics comparison among existing unified models. Existing methods explore distinct paradigms to balance visual understanding, generation, and editing capabilities. Early approaches using VQGAN discretization struggle in understanding and context-aware generation tasks due to limited semantic alignment. Later frameworks incorporate semantic encoders, achieving better alignment but compromising texture preservation essential for fine-grained editing. ILLUME+ deep-integrates image understanding, generation, and editing into single, unified architecture, enabling more intelligent and flexible interactions and task execution. line of work (Fig. 2 (c)) integrates pre-aligned vision encoders with image-text contrastive learning during VQGAN training. Due to the usage of RQVAE to balance the pixel reconstruction and imagetext alignment, it requires separate head for image generation, increasing design complexity and potentially posing bottleneck when scaling up the models. Finally, methods in Fig. 2 (d) decouple image understanding and generation tasks by employing the semantic encoder and VQGAN tokenizer independently. While effective, this structure fails to support interleaved image-text tasks and multiturn dialogues, as it necessitates manual specification of whether task involves understanding or generation, contradicting the flexibility expected of unified foundation model. By reviewing the current research, we suggest that strong unified foundation model should demonstrate three core competencies: visual understanding (accurate image interpretation), generation (high-quality image synthesis), and editing (instruction-following modification while maintain consistency of other parts). These are fundamental capabilities that guarantee the model has the potential to benefit from scaling in both model capacity and task diversity. Then it raises question: How can we construct such unified foundation model? We summarize the following key design principles: Pre-text-align vision encoder. Previous studies [45, 62] have shown that the pre-text-aligned vision encoder, i.e., CLIP-like models, significantly benefit to visual understanding capability. Additionally, in contrast to VQGAN-based models (Fig. 2 (a)) that purely supervised by image reconstruction, these semantic encoders facilitate image-text alignment as demonstrated in ILLUME [63]. Thus, incorporating pre-text-aligned semantic encoder is essential for unified foundation model. Image texture preservation. The quality of image reconstruction of vision tokenizers is essential for handling editing tasks, which determines the up-bound of unified models to maintain consistency of unchanged regions in images. Therefore, not only semantic information but also texture preservation is required as crucial consideration in vision tokenizer design choices. No information loss for image input. While the vision tokenizer is the key to enabling unified autoregressive image-text generation, it inevitably introduces information loss during the quantization process. To this end, using continuous features before the quantizer of vision tokenizer as visual input for LLM serves as more suitable choice to guarantee fine-grained multimodal understanding capability. 3 Unified image input and output representation. The decoupled mechanism in Janus series [67, 11], i.e., semantic representation for visual input while pixel representation for visual output, inevitably hinders the models ability to accurately interpret and further modify its own visual outputs in multiround steps. Hence, unified representation for visual input and output is necessary to further support image-text interleaved generation, multi-turn dialogues and chain-of-thought reasoning. Unified text and image head. In single autoregressive framework, unified output head for both image and text is preferable, as it not only simplifies infrastructure design but also enhances crossmodal interactions. In contrast, the requirement of separate image head in Fig. 2 (c) introduces challenges in modality switching during generation. For example, as we need special line separator tokens interleaved with visual tokens to represent an image in different resolutions, how can the model seamlessly transition between text and image heads during inference? To avoid such complexity, unified head offers more effective and elegant solution. Building on the above analysis, we introduce ILLUME+, an enhanced version of the ILLUME model that encompasses all the key designs mentioned above but also exhibits all the preferable abilities, listed in Fig. 2s Table). ILLUME+ supports flexible any-resolution visual input and output and excels in multimodal understanding, generation, and editing tasks, as demonstrated in Fig. 1. Its key features are outlined below: Dual vision tokenizer for semantic and texture preservation. We introduce the DualViTok, dual-branch vision tokenizer designed to capture both deep semantics and fine-grained textures. The semantic branch utilizes pre-trained text-aligned vision encoder for semantic feature extraction, supervised by feature reconstruction loss. In parallel, the pixel branch integrates quantized features from both the semantic encoder and CNN-based pixel encoder to enhance pixel-level reconstruction. To improve robustness against incorrect token predictions in autoregressive generation, we introduce noise injection during training by randomly perturbing visual tokens. Despite its simplicity, DualViTok is specifically designed for unified models, ensuring both semantic and texture preservation while maintaining robust token decoding. Unified MLLM with unified coarse-to-fine image representation. Unlike the Janus series [67, 11], which decouples visual input and output representations, we adopt coarse-to-fine strategy, first generating semantic tokens followed by pixel tokens. This sequential arrangement enables LLMs to utilize unified LM head with simple vocabulary expansion while leveraging semantic visual tokens as bridge to enhance alignment between text and visual textures. Additionally, to prevent information loss at the input stage, we employ continuous-input, discrete-output scheme following ILLUME [63], using pre-quantized continuous features as inputs while generating discrete tokens for image synthesis. Diffusion decoder for enhanced generation quality and efficient super-resolution. We incorporate diffusion model as an optional choice for image generation, offering two key benefits: (i) Higher generation quality. Diffusion models refine details and reduce artifacts, surpassing direct token decoding from vision tokenizer in both fidelity and robustness. (ii) Efficient super-resolution. They upscale images during decoding, mitigating the token explosion issue in autoregressive highresolution generation. Progressive training procedure for flexible resolution visual input and output. We employ progressive training procedure for all the above three modules, gradually increasing resolution from fixed low to flexible high, to ensure training stability and final performance. Additionally, during MLLM training, we incrementally increase tasks diversity and complexity, with carefully designed data distribution for each stage. Based on these design choices, ILLUME+ with only 3B LLM excels among existing unified MLLMs and exhibits competitive performance against specialized models across multimodal understanding, generation, and editing benchmarks. Its support for high-resolution images enhances both visual understanding and detailed image synthesis, surpassing existing unified models in document-oriented tasks and enabling generation up to 10241024 resolution. Additionally, ILLUME+ improves texture preservation over ILLUME in image editing as illustrated in Fig. 1. ILLUME+s robust performance across these diverse benchmarks highlights its effectiveness in unifying understanding, generation, and editing, providing scalable solution for future multimodal applications."
        },
        {
            "title": "2 Related Work",
            "content": "MLLM for image understanding. Recent advancements in Large Language Models (LLMs) have led to the development of Multimodal Large Language Models (MLLMs) for image understanding. Early models like LLaVA [39] and MiniGPT-4 [82] used vision adapters to align visual features with LLMs, showing strong performance in visual perception tasks. Later models such as QwenVL series [1, 64], and InternVL series [13, 12] improved upon this with higher-quality datasets and better training strategies, but still focus primarily on visual understanding. However, despite the strong understanding capabilities of these models, they primarily focus on visual perception and comprehension. This highlights the need for more comprehensive solutions that unify both understanding and generation, allowing models to learn deeper relationships between visual and textual information, enabling more intelligent and flexible interactions and task execution in complex real-world scenarios. Image generation model. Generative adversarial networks (GANs [23]) is the pioneering method for image generation in the era of deep learning. However, it suffers from the problem of unstable training process and mode collapse. In recent years, diffusion-based methods [26, 53, 50, 54, 2] have shown excellent image generation capabilities. These models learn to predict Gaussian noise in forward diffusion process, and then generate high-quality images through an inverse denoising process. Among them, the latent diffusion model [53, 51, 17] addresses computational challenges by creating images from low-dimensional latent representations. Another line of research turns to explore the image generation in the autoregressive model [52, 16, 74, 61], which converts images into discrete tokens using VQGAN-like vision tokenizers [18, 30] and generates images by predicting the tokens autoregressively. In this paper, we unify the image generation and image editing into one MLLM with image understanding tasks under the autoregressive manner, and further adopt diffusion model to further improve the reconstruct quality from the predicted tokens. Unified multimodal understanding and generation. Early efforts to unify visual understanding and generation using LLMs include models like Emu [58] and X-VILA [73], which adopt unified autoregressive approaches to predict multimodal elements. However, the non-unified optimization of different modalities limits feature integration, and additional components like diffusion decoders reduce efficiency. Models such as LWM [40], Chameleon [59], and VILA-U [68] use VQ tokenizers to convert images into vision tokens, enabling unified training framework for text and image generation. Despite these advancements, challenges remain in integrating understanding and generation. Janus series [67, 11] decouples visual encoding for understanding and generation, which may suffer from misaligned representations due to separate branches for understanding and generation. These limitations highlight the need for better solutions that allowing flexible and efficient context-aware image understanding and generation across various tasks."
        },
        {
            "title": "3 Method",
            "content": "Figure 3 provides an overview of our proposed framework, ILLUME+, which comprises dual vision tokenizer, MLLM, and diffusion decoder. Our architectures core design principle is the unified dual visual tokenization mechanism that captures both deep semantic information and fine-grained texture details, ensuring comprehensive image representation for visual understanding, generation, and editing tasks. The following section elaborates on the architectural details, training procedures, and data composition."
        },
        {
            "title": "3.1 Dual Vision Tokenizer",
            "content": "Although vision tokenizers have been studied for years, their design for unified models that simultaneously support understanding, generation, and editing remains an open challenge. To mitigate this, we propose the Dual Vision Tokenizer (DualViTok) tailored for unified models with semantic and texture preservation and robust decoding. Semantic and texture information preservation. As shown in the Fig. 3 (a), DualViTok incorporates dual-branch design to learn both deep semantics and fine-grained textures. The semantic branch leverages the pre-trained text-aligned vision encoder, QwenViT [64], to extract high-level semantic features, which are then quantized into discrete tokens and reconstructed using lightweight decoder. The semantic reconstruction is optimized with cosine similarity loss between the reconstructed 5 Figure 3: Architecture of ILLUME+. (a) The dual vision tokenizer preserves both semantic and texture information. (b) The diffusion refiner decodes discrete tokens into high-quality images. (c) The unified MLLM enables deep semantic interactions and context-aware image generation. (d) We introduce an unambiguous image representation of discrete tokens in chain-of-thought pattern (semantic tokens first, followed by pixel tokens), resulting in improved generation performance. features and semantic features from the vision encoder. Meanwhile, the pixel branch follows MoVQGAN-based architecture [81]. After quantization, the pixel and semantic tokens are concatenated along the channel dimension for image decoding. Following standard VQGAN procedures [18], the pixel branch is trained with L1 loss, perceptual loss, and GAN loss. More specifically, for the semantic branch, we adopt 28 downsampling rate commonly used in state-of-the-art understanding models [32, 64] and employ 16 rate for the pixel branch to preserve fine-grained textures. To further minimize the information loss induced by tokenization, we incorporate space-to-channel and channel-to-space transformations inspired by DC-AC [7] in the downsampling and upsampling process. Additionally, to ensure reconstruction fidelity, we employ larger codebook sizes compared with previous works [18], 32,768 for the semantic branch and 98,304 for the pixel branch, with SimVQ [83] as quantization method to maintain high codebook utilization rate. Please refer to ablation studies in Sec. 4 for more discussions about detailed design choices of our vision tokenizer. Robust decoding for incorrect token predictions. LLMs may occasionally predict incorrect visual tokens, and the tokenizer decoder should be resilient to such errors to minimize artifacts in the generated images. To improve robustness, we introduce noise injection during tokenizer training: each sample has an α=10% probability of being perturbed, with β=10% of its tokens randomly replaced, helping the decoder better handle erroneous token predictions."
        },
        {
            "title": "3.2 Unified Multimodal Large Language Model",
            "content": "As shown in Fig. 3 (c), ILLUME+ inherits the architecture of existing VLMs [37, 38] by extending LLMs with an additional vision vocabulary to generate discrete vision tokens, employing continuousinput, discrete-output scheme for image processing. To avoid information loss caused by tokenizer quantization, we employ both the semantic encoder and pixel encoder within dual vision tokenizers to extract features from input images, which are then aligned with the LLMs input space via two separate vision adaptors. For visual generation, images are converted into discrete tokens, enabling unified modeling of visual and text tokens within the LLM. By using the same next-token prediction loss, both modalities are seamlessly integrated into shared prediction head. During inference, we apply the classifier-free guidance (CFG) following [70, 63] for text-to-image generation and image editing tasks, where the unconditioned setting serves as masked text descriptions and editing instructions, respectively. Coarse-to-fine unified image representation. As illustrated in Fig. 3 (d), we adopt unified representation for images with coarse-to-fine sequence arrangement, i.e., first generating semantic tokens followed by pixel tokens. Since semantic representations align more naturally with text, generating semantic tokens first allows the model to determine content before refining details based on semantic information, thus enhancing alignment between text and visual textures. Specifically, 6 Figure 4: Illustration of our progressive training pipeline. We first pre-train the dual-tokenizer system by reconstruction of the semantic and pixel information. We then fine-tune the diffusion model as high-quality image decoder. The MLLM training consists of three main stages that gradually increase task resolution and complexity. we use <start-of-image/semantic/pixel> and <end-of-image/semantic/pixel> markers to indicate the boundaries of the entire image, semantic representation, and texture representation, respectively. Additionally, <end-of-line> tokens are inserted at the end of each row to distinguish different resolutions, while the height and width indicators at the sequences start provide explicit resolution information during image generation."
        },
        {
            "title": "3.3 Diffusion Decoder",
            "content": "We introduce an additional diffusion model to decode image from predicted discrete tokens for enhanced generation quality and efficient super-resolution. Specifically, based on the SDXL model [51], our diffusion decoder replaces text encoders with zero embeddings in the cross-attention layers. Semantic and pixel tokens from dual vision tokenizer are mapped to feature representations using the learned codebooks and then injected into the UNet model by concatenation with the noisy image latent. Note that the model performs super-resolution, doubling the image size (i.e., 256256 to 512512), to mitigate the token explosion issue in autoregressive high resolution generation. During training, similar to our dual vision tokenizer, random perturbations are applied to 50% of the samples, replacing 10% of the tokens to enhance robustness against noisy input tokens. To prioritize semantic tokens, we use smaller masking probability (10%) for semantic features and larger (50%) for texture."
        },
        {
            "title": "3.4 Training Procedure and Data Composition",
            "content": "Figure 4 illustrates the whole training procedure of our proposed ILLUME+, where we adopt progressive training paradigm designed to support flexible resolutions and leverage the capabilities of each component. An overview of data distribution at each stage is illustrated in Fig. 5. Dual vision tokenizer training. As illustrated in Fig. 4, all components of our proposed DualViTok except for the pretrained semantic encoder are trainable. To ensure stable training across arbitrary resolutions, we progressively scale the input resolution: starting from fixed 256256, then 512512, and finally allowing flexible resolutions up to 512512. For training efficiency, we employ bucketresolution strategy, batching samples with similar resolutions. The training corpus consists of 63M samples of various types with the data composition shown in Fig. 5. Diffusion decoder training. We incorporate an additional diffusion model to decode image for enhanced generation quality and efficient super-resolution. In this stage, the two encoders and codebooks from DualViTok are frozen, while the pixel decoder is replaced by diffusion-based decoder to reconstruct and and upscales images by factor of 2. To support flexible resolutions, we predefine 11 commonly used aspect ratios: {1:1, 3:4, 4:3, 2:3, 3:2, 1:2, 2:1, 1:3, 3:1, 1:4, 4:1}. Each image is matched to the closest predefined aspect ratio and cropped accordingly. Note that images that cropped more than 20% of the original content is removed during training to preserve image integrity. For high-resolution support and efficient training, we adopt two-stage process: the first stage handles images with total pixel count near 5122, while the second stage scales up to approximately 10242. The training data consists of 10M-image subset from our tokenizer dataset. MLLM training. Following ILLUME, the training procedure consists of three stages as below, where we progressively unfreeze more parameters and increase the complexity and variety of tasks. 7 Stage Dataset Tokenizer & Diffusion COYO [4], EMOVA, in-house aesthetics data MLLM Pre-training MLLM SFT COYO, Wukong [24], EMOVA-Pretrain [9], LLAVA-SFT [39], in-house aesthetics data, UltraEdit [80], SEED-Edit [21], AnyEdit [75] Magpie [71], OpenOrca [35], SCP-116K [44], OpenHermes [60], OPC-SFT-Stage1 [27] EMOVA-SFT, Pixmo [15], M4-Instruct [37] COYO, in-house aesthetics data, OmniEdit [66], AnyEdit [75], UltraEdit [80], Instruct-Pix2Pix [3], Magpie [71] Figure 5: Summary of the data mixture in each stage. Our training data gradually covers wide range of tasks and various image resoluton. Settings DualViTok Diffusion Decoder Stage 1 Stage 2-1 Stage 2SFT MLLM Learning Rate 1e-4 2e-5 Batch Size Training Steps Image Res. Mode Image Main/Max Res. 256 270k/50k/78k Fix/Fix/Anyres 256/512/512 128 265k Multi-ratio 512/1024 Vis. Adapter 1e-3 Vis. Embed. & Head 2e-4 1024 5k Fix 256 Vis. Adapter 5e-5 LLM 5e-5 1024 98k Fix 256 512 40k Fix Vis. Adapter 2e-5 LLM 2e-5, ViT 2e-6 256 40k Anyres 1024 Table 1: Training hyperpparameters of experiments. Stage 1: Visual Embedding Initialization. The primary goal of this stage is to initialize good visual representation. We optimize vision-related components, namely the adapter, the vision embedding, and the LM head of the vision part, on image reconstruction and image captioning tasks. In this stage, we fix the image resolution as 256 256. Stage 2: Unified Image-Text Alignment. This stage focuses on image-text alignment to learn on multimodal data. We unfreeze the LLM and vision adaptor, with training data covering variety of tasks, including text data, image caption data for both natural images and documents, text-to-image generation, and image editing data. This process contains two sub-stages: the first sub-stage fixed image resolution as 256 256 while the second one uses fixed image resolution as 512 512. Stage 3: Supervised Fine-tuning. After pretraining, we train the whole model with task-specific data to handle various multimodal understanding, generation, and editing tasks. In this stage, we leverage flexible resolution training. Specifically, for understanding tasks, it processes images with their naive resolutions, while for image generation and editing tasks, each input image is matched to predefined aspect ratio and resize-and-crop accordingly with total pixel count near 5122. Note that benefit to the diffusion decoder, our final model enables generation up to 1024 1024 resolution with multiple aspect ratios."
        },
        {
            "title": "4 Experiments",
            "content": "4."
        },
        {
            "title": "Implementation Details",
            "content": "In our experiments, we use Qwen2.5 [72] as the base LLM. In our DualViTok, the semantic encoder uses pretrained QwenVIT [64]. The semantic decoder consists of 4 attention blocks with 2D-RoPE. The pixel encoder and decoder follow MoVQGAN-based architecture [81] with the basic channel of 128 and 384, respectively. We use codebook size of 32,768 for the semantic branch and 98,304 for the pixel branch. Codebook dimensions are 32 for both semantic and pixel codebooks. We apply AdamW optimizer [43] without weight decay and constant learning rate for DualViTok, diffusion decoder and MLLM. The specific training hyperparameters of three parts are summarized in Table 1. The training process of DualViTok and the diffusion decoder took around 3+3 days on cluster of 256 Ascend NPUs. Then 3B MLLM model took about 13 days to finish the 3-stage training. 8 Method LLM. POPE MMBench SEED MME-P MM-Vet MMMU AI2D VQA-text ChartQA DocVQA InfoVQA OCRBench General Doc InstructBLIP [14] Vicuna-7B Qwen-VL-Chat [1] Qwen-7B LLaVA-1.5 [37] Vicuna-7B ShareGPT4V [10] Vicuna-7B LLaVA-NeXT [38] Vicuna-7B Emu3-Chat [65] 8B from scratch - - 85.9 - 86.5 85.2 6.8B from scratch 7B from scratch LLaMA-2-7B Phi-1.5B 87.7 Unified-IO 2 [46] - Chameleon [59] 75.2 LWM [40] 73.8 Show-o [70] 83.9 VILA-U (256) [68] LLaMA-2-7B 85.8 VILA-U (384) [68] LLaMA-2-7B Janus [67] DeepSeek-LLM-1.3B 87.0 Janus-Pro-1B [67] DeepSeek-LLM-1.3B 86.2 87.4 Janus-Pro-7B [67] DeepSeek-LLM-7B ILLUME ILLUME+ Vicuna-7B Qwen-2.5-3B 88.5 87.6 Understanding Only 53.4 58.2 58.6 69.7 64.7 68.2 - 1487.5 1510.7 1567.4 1519 - 26.2 - 31.1 37.6 43.9 37.2 30.6 35.9 35.4 37.2 35.1 31.6 33.8 45.9 54.8 58 66.6 70.0 Unify Understanding and Generation 61.8 - - - 56.3 59 63.7 68.3 72.1 72.9 73. - - - 948.4 1336.2 1401.8 1338.0 1444.0 1567.1 1445.3 1414.0 - 8.3 9.6 - 27.7 33.5 34.3 39.8 50.0 37.0 40.3 - 22.4 - 25.1 - - 30.5 36.3 41.0 38.2 44. - - - - - - - - - 71.4 74.2 36.0 60.6 64.3 68.8 67.4 58.5 - - - - - - 69.4 75.5 79.2 75.1 80.8 50.1 61.5 58.2 60.4 64.9 64. - - 18.8 - 48.3 60.8 - - - 72.1 69.9 12.5 66.3 18.2 21.3 54.8 68.6 13.9 62.6 28.1 - 74.4 76.3 - - 25.8 - 37.1 43.8 - - - - - - - - - - - - - - - - - - - - - - - - - - - 276 488 318 371 532 687 - - - - - - - - - 66.7 69.9 76.0 80. 45.5 44.1 669 672 Table 2: Quantitative results on visual understanding benchmarks. Our performance is close to and even outperforms both understanding only and unified models. The performance with top-1 and top-2 value are denoted in bold and underline respectively. Method Params. Type MJHQ30k GenAI-bench GenEval FID Basic Advanced Overall Single Obj Two Obj. Counting Colors Position Color Attri. Generation Only SDv1.5 [53] PixArt-α [8] SDXL [51] Emu3-Gen [65] 0.9B 0.6B 2.6B 8B Autoregressive Diffusion Diffusion Diffusion - 6.14 9.55 - - - 0.83 - - - 0.63 - 0.43 0.48 0.55 0. 0.97 0.98 0.98 0.98 Unify Understanding and Generation Chameleon [59] LWM [40] Show-o [70] VILA-U(256) [68] VILA-U(384) [68] Janus [67] Janus-Pro-1B [11] Janus-Pro-7B [11] ILLUME [63] ILLUME+ 7B Autoregressive 7B Autoregressive 1.5B Autoregressive 7B Autoregressive 7B Autoregressive 1.3B Autoregressive 1.3B Autoregressive 7B Autoregressive 7B Autoregressive 3B Autoregressive - 17.77 15.18 12.81 7.69 10.1 - - 7.76 6.00 - 0.63 0.70 0.76 0.73 0.75 0.72 - 0.53 0.60 0.64 0.61 0.60 0. 0.39 0.47 0.53 0.61 0.73 0.80 0.61 0.72 - 0.93 0.95 0.97 0.98 0.99 0.99 0.99 0.38 0.5 0.74 0.71 - 0.41 0.52 0.68 0.82 0. 0.86 0.88 0.35 0.44 0.39 0.34 - 0.46 0.49 0.3 0.51 0.59 0.45 0.62 0.76 0.8 0.85 0.81 - 0.79 0.82 0.84 0.89 0. 0.71 0.84 0.04 0.08 0.15 0.17 - 0.09 0.11 0.46 0.65 0.79 0.39 0.42 0.06 0.07 0.23 0.21 - 0.15 0.28 0.42 0.56 0. 0.28 0.53 Table 3: Quantitative results on text-to-image generation benchmarks. ILLUME+ achieves comparable results with specialist models and unified MLLMs. The performance with top-1 and top-2 value are denoted in bold and underline respectively."
        },
        {
            "title": "4.2 Compare to State-of-the-Art",
            "content": "Multimodal understanding. To evaluate the multimodal understanding capabilities, we conduct evaluation on two types of widely-used benchmarks: (1) General, including POPE [34], MMBench [41], SEED [31], MME-P [20], MM-Vet [76], MMMU [77] and AI2D [29]; (2) Document-oriented, including VQA-text [56], ChartQA [47], DocVQA [49], InfoVQA [48] and OCRBench [42]. As shown in our results, despite using only 3B model, ILLUME+ achieves competitive performance compared to state-of-the-art unified models, including Janus-Pro-7B and ILLUME-7B, on general benchmarks. Notably, our 3B model demonstrates exceptional performance on document-related tasks, challenge for most existing unified models. This highlights the effectiveness of our dual-encoder design in preserving strong understanding capabilities within unified model. More visualizations on understanding tasks are illustrated in Fig. 6. Multimodal image generation. To evaluate the multimodal visual generation capability, we use the MJHQ-30K [33], GenAI-bench [36] and GenEval [22] benchmarks in Table 3. For MJHQ-30K, we adopt the Fréchet Inception Distance (FID [25]) metric on 30K generated images compared to 9 Method Res. ratio # Scales Dim Size rFID PSNR SSIM 256 VQGAN [18] MaskGIT [6] 256 LLamaGEN [57] 256 VILA-U [68] VILA-U [68] 16 16 16 256 14.2 384 14.2 DualViTok DualViTok DualViTok 256 384 512 16 16 1 1 1 4 16 2 2 2 256 256 32 256 256 32 32 32 16384 1024 16384 16384 16384 131072 131072 4.99 2.28 2.19 1.80 1.25 1.37 0.69 0.45 20.00 20.79 - - 22.53 23.62 24.83 0.629 0.675 - - 0.741 0.769 0. Method Type Tasks Emu Edit DINO CLIP-I CLIP-T CLIP-DIR InstructPix2Pix [3] Diffusion Diffusion MagicBrush [79] Diffusion OmniGen [69] Diffusion Emu Edit [55] Edit only Edit only Edit only Edit only 0.762 0.776 0.804 0.819 PUMA [19] ILLUME ILLUME+ AR AR AR Edit only 0.785 Und, Gen, Edit 0.791 Und, Gen, Edit 0.826 0.834 0.838 0.836 0. 0.846 0.879 0.872 0.219 0.222 0.233 0.231 0.270 0.260 0.275 0.078 0.09 - 0.109 - - 0.101 Table 4: Comparisons with other visual tokenizers. The evaluations are on ImageNet 50k validation set under different image resolution. Table 5: Quantitative results on image editing benchmarks. The performance with top-1 and top-2 value are denoted in bold and underline. Continuous Semantic Pixel Image Reconstruction Image Gen. Encoder Encoder rFID PSNR SSIM gFID Image Understanding POPE MMB SEED MME-P MM-Vet MMMU VQA-text 5.48 2.08 1.83 1.83 15.69 21.86 21.68 21.68 0.487 0.720 0.714 0. - 28.24 26.70 26.70 81.6 66.0 82.1 85.3 46.9 30.7 50.4 70.9 54.1 41.4 56.0 66.6 1171.9 820.7 1203.7 1491.6 15.1 13.9 17.7 34. 38.0 36.1 39.7 42.4 42.0 40.6 43.3 56.2 Input Table 6: Ablation study of visual tokenizer on image reconstruction and image generation. The rFID, PSNR and SSIM are evaluated on ImageNet 50k validation set. The gFID is evaluated on MJHQ30k. The setting of the main experiments are marked in gray. 30K high-quality images, measuring the generation quality and diversity. GenAI-bench [36] and GenEval [22] are challenging text-to-image generation benchmarks designed to reflect the consistency between text descriptions and generated images. We compare ILLUME+ with previous state-ofthe-art multimodal generation-only and unified models. With our dual vision tokenizer, ILLUME achieves 7.15 FID score on the MJHQ30K benchmark, surpassing unified models such as VILA-U and ILLUME. Notably, replacing token decoding with diffusion decoder further improves the FID score to 6.00, achieving state-of-the-art performance across both generation-only and unified models. This highlights the superior generation quality and diversity enabled by our diffusion-based approach. Additionally, ILLUME+ achieves competitive results on the GenAI-bench and GenEval benchmarks and attains the highest accuracy (0.72) in advanced categories on GenAI-bench, demonstrating its ability to understand and generate images from complex text descriptions. Figure 7 shows more results of ILLUME+ on generating flexible resolution images. Multimodal image editing. To assess the multimodal image editing capability of our method, we evaluate it on the Emu Edit [55] benchmark and report the CLIP-I, CLIP-T, CLIP-DIR and DINO [5] scores. The CLIP-I and DINO scores measure the models ability to preserve elements from the source image, while the CLIP-T and CLIP-DIR score measures the consistency between the output image and the target caption. As illustrated in Table 5, our model demonstrates strong performance in image editing tasks, surpassing specialized models, particularly in the CLIP-T metric. This indicates that the unified models superior understanding enhances its ability to interpret editing instructions, resulting in more precise modifications. Furthermore, our dual-codebook design, which accounts for texture information, improves consistency with the original image as shown in Fig. 8. Image reconstruction of vision tokenizer. Table 7 compares various state-of-the-art visual tokenizers on the ImageNet 50k validation set across different image resolutions using the rFID, PSNR, and SSIM. At resolution of 256 256, our DualViTok achieves state-of-the-art performance, exhibiting the best performance among the compared methods. Notably, DualViTok demonstrates the capability to handle multiple resolutions within single model. For instance, when comparing performance at higher resolution of 384x384, DualViTok significantly outperforms VILA-U [68] at the same resolution, with substantial improvement of 0.56 in rFID. This highlights DualViTok advantage in achieving superior reconstruction quality across varying input sizes with single, versatile model, showcasing its efficiency and flexibility compared to fixed-resolution approaches like the specific VILA-U [68] instance presented for 384x384."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "In our ablation studies, we train our DualViTok on the ImageNet training set for 20 epochs at an image resolution of 256 256. Unless specified otherwise, both the semantic codebook and the 10 Figure 6: More visualizations on understanding tasks. pixel codebook have vocabulary size of 32768, respectively. For image generation, we train the MLLM with DualViTok on dataset of 10M high-fidelity images. For image understanding, we train the MLLM on LLaVAs pretraining and SFT datasets. The performance of image reconstruction is assessed on ImageNet val (50k), and the gFID is assessed on MJHQ30k. Dual tokenization vs. single tokenization. We compare our DualViTok with the single tokenization method, i.e., semantic autoencoder and pixel autoencoder. Specifically, the semantic autoencoder is trained on both the image and semantic reconstruction task. The pixel autoencoder is only trained Figure 7: More visualizations on generation tasks. on image reconstruction. As shown in Table 6, our dual tokenization, DualViTok, which fuses the semantic and pixel branches, outperforms single tokenization methods in image reconstruction. Moreover, combining the semantic and pixel codebooks via the coarse-to-fine generation enhances the generation performance, i.e., 1.54 rFID improvement. According to the image understanding abilities, the semantic encoder outperforms the pixel encoder, and our DualViTok boosts performance across all benchmarks. 12 Figure 8: More visualizations on editing tasks. Encoder Decoder Up/down Codebook Channel Channel Block Size Noise rFID PSNR SSIM 256 128 128 128 128 128 256 256 384 384 384 384 Origin Origin Origin 32k/32k 1.83 32k/32k 1.67 32k/32k 1.54 DC Block 32k/32k 1.41 1.44 DC Block 32k/98k DC Block 32k/98k Random 1.33 None None None None None 21.68 21.95 21.98 21.76 22.29 22.21 0.714 0.721 0.723 0.716 0.736 0.731 Table 7: Ablation study of DualViToks components. The first value of the Codebook Size is semantic codebook size and the second is the pixel codebook size. The setting of the main experiments are marked in gray. Quantizer Codebook Dim rFID PSNR SSIM Codebook Utilization VQ SimVQ SimVQ SimVQ 32 32 16 8 2.24 1.83 1.82 1.84 20.81 21.68 21.89 21.95 0.666 0.714 0.721 0.715 6.59% 100% 100% 100% Table 8: Ablation study of visual tokenizer about quantization types and codebook dimensions. The setting of the main experiments are marked in gray. Noise Type α β rFID PSNR SSIM - - - Random 10 100 50 Random 10 10 Random 10 Random 50 10 Random 100 10 10 100 50 10 10 10 50 10 100 10 Zero Zero Zero Zero Zero 1.83 1.76 1.87 1.81 1.90 2.06 1.88 1.73 1.90 1.75 1.79 21.68 21.85 21.60 21.98 21.47 21.65 21.57 21.97 21.97 21.88 21.75 0.714 0.722 0.713 0.722 0.710 0.707 0.717 0.725 0.722 0.722 0.716 Table 9: Ablation of noise on visual tokenizer. The noise has α% probability to perturbe the current sample, with β% of its tokens randomly replaced. The setting of the main experiments are marked in gray. Continuous input vs. discrete input. The last two rows of Table 6 compare the performance differences between continuous visual input and discrete visual input. Obviously, continuous input leads to better performance on all benchmarks, proving the importance of continuous input in achieving superior image understanding ability. Ablation of the components of DualViTok. Table 7 demonstrates the components to improve our DualViToks baseline from 1.83 to 1.33 on rFID. First, we find the smaller encoder and larger decoder can improve the reconstruction performance from 1.83 to 1.54. Then the application of the DC block [7] brings 0.13 improvements. Scaling the pixel codebook size from 32k to 98k can further improve the performance on PSNR and SSIM while the import of random noise in the pixel codebook can improve the rFID to 1.33. 13 Ablation of the random noise in DualViTok. Table 9 presents an ablation study on the impact of random noise and zero noise of the visual tokenizer as well as the effects of the α and β as described in Sec. 3.1. The results indicate that both random and zero noise can achieve better reconstruction performance compared to the baseline model. However, because random noise more accurately reflects the erroneous tokens predicted by LLMs it was chosen for the main experiments. Effect of the quantization method. We compare vanilla VQ [18] with SimVQ [83] on Table 8. Results show that SimVQ can achieves better reconstruction performance on ImageNet and maintains high codebook utilization rate. Besides, we compare different codebook dimension and find similar performance between dimension of 32, 16 and 8."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present ILLUME+, an enhanced version of the ILLUME model, which advances the integration of visual understanding, generation, and editing in unified multimodal large language model. ILLUME+ proposes the dual visual tokenizor, DualViTok, to preserve semantic and texture in images and utilizes diffusion decoder to enhance image generation and achieve super-resolution. By leveraging unified coarse-to-fine image representation and progressive training procedure for dynamic visual resolution, ILLUME+ with only 3B parameters enables to process flexible resolution visual inputs and outputs, and demonstrates good performance across various benchmarks in multimodal understanding, generation, and editing tasks. There are several promising directions for future work, including scaling to larger model sizes (7B+) for enhanced task generalization and developing more advanced image-text interleaved pretraining techniques. Further improvements can be made by constructing more complex multimodal datasets and exploring post-training strategies for unified models. These advancements will help unlock the full potential of unified multimodal models in real-world applications, supporting more sophisticated tasks and enabling broader generalization across domains."
        },
        {
            "title": "References",
            "content": "[1] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. [2] J. Betker, G. Goh, L. Jing, T. Brooks, J. Wang, L. Li, L. Ouyang, J. Zhuang, J. Lee, Y. Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [3] T. Brooks, A. Holynski, and A. A. Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392 18402, 2023. [4] M. Byeon, B. Park, H. Kim, S. Lee, W. Baek, and S. Kim. Coyo-700m: Image-text pair dataset. https: //github.com/kakaobrain/coyo-dataset, 2022. [5] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. [6] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11315 11325, 2022. [7] J. Chen, H. Cai, J. Chen, E. Xie, S. Yang, H. Tang, M. Li, Y. Lu, and S. Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [8] J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [9] K. Chen, Y. Gou, R. Huang, Z. Liu, D. Tan, J. Xu, C. Wang, Y. Zhu, Y. Zeng, K. Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. [10] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [11] X. Chen, Z. Wu, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, and C. Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [12] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. [13] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2418524198, 2024. [14] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. [15] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [16] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin, X. Zou, Z. Shao, H. Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in neural information processing systems, 34:1982219835, 2021. [17] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, D. Podell, T. Dockhorn, Z. English, K. Lacey, A. Goodwin, Y. Marek, and R. Rombach. Scaling rectified flow transformers for high-resolution image synthesis, 2024. [18] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [19] R. Fang, C. Duan, K. Wang, H. Li, H. Tian, X. Zeng, R. Zhao, J. Dai, H. Li, and X. Liu. Puma: Empowering unified mllm with multi-granular visual generation. arXiv preprint arXiv:2410.13861, 2024. [20] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, Y. Wu, and R. Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. [21] Y. Ge, S. Zhao, J. Zhu, Y. Ge, K. Yi, L. Song, C. Li, X. Ding, and Y. Shan. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396, 2024. [22] D. Ghosh, H. Hajishirzi, and L. Schmidt. Geneval: An object-focused framework for evaluating text-toimage alignment. Advances in Neural Information Processing Systems, 36, 2024. [23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. [24] J. Gu, X. Meng, G. Lu, L. Hou, N. Minzhe, X. Liang, L. Yao, R. Huang, W. Zhang, X. Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [26] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [27] S. Huang, T. Cheng, J. K. Liu, J. Hao, L. Song, Y. Xu, J. Yang, J. H. Liu, C. Zhang, L. Chai, R. Yuan, Z. Zhang, J. Fu, Q. Liu, G. Zhang, Z. Wang, Y. Qi, Y. Xu, and W. Chu. Opencoder: The open cookbook for top-tier code large language models. 2024. [28] Y. Jin, K. Xu, K. Xu, L. Chen, C. Liao, J. Tan, Y. Mu, et al. Unified language-vision pretraining in llm with dynamic discrete visual tokenization. In International Conference on Learning Representations, 2024. [29] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. [30] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [31] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [32] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, Y. Li, Z. Liu, and C. Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [33] D. Li, A. Kamko, E. Akhgari, A. Sabet, L. Xu, and S. Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv preprint arXiv:2402.17245, 2024. [34] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. [35] W. Lian, B. Goodson, E. Pentland, A. Cook, C. Vong, and \"Teknium\". Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://huggingface.co/datasets/Open-Orca/ OpenOrca, 2023. [36] Z. Lin, D. Pathak, B. Li, J. Li, X. Xia, G. Neubig, P. Zhang, and D. Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2025. [37] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [38] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. [39] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [40] H. Liu, W. Yan, M. Zaharia, and P. Abbeel. World model on million-length video and language with ringattention. arXiv preprint arXiv:2402.08268, 2024. [41] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pages 216233. Springer, 2025. [42] Y. Liu, Z. Li, B. Yang, C. Li, X. Yin, C.-l. Liu, L. Jin, and X. Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [43] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations. [44] D. Lu, X. Tan, R. Xu, T. Yao, C. Qu, W. Chu, Y. Xu, and Y. Qi. Scp-116k: high-quality problem-solution dataset and generalized pipeline for automated extraction in the higher education science domain. arXiv preprint arXiv:2501.15587, 2025. 16 [45] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, et al. Deepseek-vl: Towards real-world vision-language understanding. arXiv preprint arXiv:2403.05525, 2024. [46] J. Lu, C. Clark, S. Lee, Z. Zhang, S. Khosla, R. Marten, D. Hoiem, and A. Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2643926455, 2024. [47] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [48] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. [49] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. [50] W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [51] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [52] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [53] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [54] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [55] S. Sheynin, A. Polyak, U. Singer, Y. Kirstain, A. Zohar, O. Ashual, D. Parikh, and Y. Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88718879, 2024. [56] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [57] P. Sun, Y. Jiang, S. Chen, S. Zhang, B. Peng, P. Luo, and Z. Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [58] Q. Sun, Q. Yu, Y. Cui, F. Zhang, X. Zhang, Y. Wang, H. Gao, J. Liu, T. Huang, and X. Wang. Generative pretraining in multimodality. arXiv preprint arXiv:2307.05222, 2023. [59] C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [60] Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. [61] K. Tian, Y. Jiang, Z. Yuan, B. Peng, and L. Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37:8483984865, 2024. [62] P. Tong, E. Brown, P. Wu, S. Woo, A. J. V. IYER, S. C. Akula, S. Yang, J. Yang, M. Middepogu, Z. Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. [63] C. Wang, G. Lu, J. Yang, R. Huang, J. Han, L. Hou, W. Zhang, and H. Xu. Illume: Illuminating your llms to see, draw, and self-enhance. arXiv preprint arXiv:2412.06673, 2024. [64] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [65] X. Wang, X. Zhang, Z. Luo, Q. Sun, Y. Cui, J. Wang, F. Zhang, Y. Wang, Z. Li, Q. Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [66] C. Wei, Z. Xiong, W. Ren, X. Du, G. Zhang, and W. Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. [67] C. Wu, X. Chen, Z. Wu, Y. Ma, X. Liu, Z. Pan, W. Liu, Z. Xie, X. Yu, C. Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding and generation. arXiv preprint arXiv:2410.13848, 2024. [68] Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [69] S. Xiao, Y. Wang, J. Zhou, H. Yuan, X. Xing, R. Yan, S. Wang, T. Huang, and Z. Liu. Omnigen: Unified image generation. arXiv preprint arXiv:2409.11340, 2024. [70] J. Xie, W. Mao, Z. Bai, D. J. Zhang, W. Wang, K. Q. Lin, Y. Gu, Z. Chen, Z. Yang, and M. Z. Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [71] Z. Xu, F. Jiang, L. Niu, Y. Deng, R. Poovendran, Y. Choi, and B. Y. Lin. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. arXiv preprint arXiv:2406.08464, 2024. [72] A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu, M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Wan, Y. Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [73] H. Ye, D.-A. Huang, Y. Lu, Z. Yu, W. Ping, A. Tao, J. Kautz, S. Han, D. Xu, P. Molchanov, et al. X-vila: Cross-modality alignment for large language model. arXiv preprint arXiv:2405.19335, 2024. [74] J. Yu, Y. Xu, J. Y. Koh, T. Luong, G. Baid, Z. Wang, V. Vasudevan, A. Ku, Y. Yang, B. K. Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [75] Q. Yu, W. Chow, Z. Yue, K. Pan, Y. Wu, X. Wan, J. Li, S. Tang, H. Zhang, and Y. Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [76] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [77] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [78] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [79] K. Zhang, L. Mo, W. Chen, H. Sun, and Y. Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. [80] H. Zhao, X. S. Ma, L. Chen, S. Si, R. Wu, K. An, P. Yu, M. Zhang, Q. Li, and B. Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. [81] C. Zheng, T.-L. Vuong, J. Cai, and D. Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems, 35:2341223425, 2022. [82] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [83] Y. Zhu, B. Li, Y. Xin, and L. Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024. [84] Y. Zhu, Y. Zhou, C. Wang, Y. Cao, J. Han, L. Hou, and H. Xu. Unit: Unifying image and text recognition in one vision encoder. arXiv preprint arXiv:2409.04095, 2024."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "The University of Hong Kong"
    ]
}