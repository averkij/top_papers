{
    "paper_title": "Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding",
    "authors": [
        "Jiaqi Tang",
        "Jianmin Chen",
        "Wei Wei",
        "Xiaogang Xu",
        "Runtao Liu",
        "Xiangyu Wu",
        "Qipeng Xie",
        "Jiafei Wu",
        "Lei Zhang",
        "Qifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, a novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce a specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-the-art robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 3 5 7 1 . 2 1 5 2 : r Robust-R1: Degradation-Aware Reasoning for Robust Visual Understanding Jiaqi Tang1*, Jianmin Chen2*, Wei Wei2, Xiaogang Xu3, Runtao Liu1, Xiangyu Wu4, Qipeng Xie1, Jiafei Wu5, Lei Zhang2, Qifeng Chen1 1Hong Kong University of Science and Technology 2Northwestern Polytechnical University 3Chinese University of Hong Kong 4Nanjing University of Science and Technology 5University of Hong Kong cqf@ust.hk, weiweinwpu@nwpu.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models struggle to maintain reliable performance under extreme real-world visual degradations, which impede their practical robustness. Existing robust MLLMs predominantly rely on implicit training/adaptation that focuses solely on visual encoder generalization, suffering from limited interpretability and isolated optimization. To overcome these limitations, we propose Robust-R1, novel framework that explicitly models visual degradations through structured reasoning chains. Our approach integrates: (i) supervised fine-tuning for degradation-aware reasoning foundations, (ii) reward-driven alignment for accurately perceiving degradation parameters, and (iii) dynamic reasoning depth scaling adapted to degradation intensity. To facilitate this approach, we introduce specialized 11K dataset featuring realistic degradations synthesized across four critical real-world visual processing stages, each annotated with structured chains connecting degradation parameters, perceptual influence, pristine semantic reasoning chain, and conclusion. Comprehensive evaluations demonstrate state-of-theart robustness: Robust-R1 outperforms all general and robust baselines on the real-world degradation benchmark R-Bench, while maintaining superior anti-degradation performance under multi-intensity adversarial degradations on MMMB, MMStar, and RealWorldQA. Figure 1: Comparison with other existing robustness enhancement approaches. (A) is based on implicit training/adaptation, which only considers the visual encoder feature alignment. (B) is ours, and we explicitly integrate the degradation-aware reasoning chain into MLLM. Code github.com/jqtangust/Robust-R1 Data huggingface.co/datasets/Jiaqi-hkust/Robust-R1 Model huggingface.co/Jiaqi-hkust/Robust-R1 Space huggingface.co/spaces/Jiaqi-hkust/Robust-R1 Introduction 1 Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in visual understanding tasks (Liu et al. 2024; Tang et al. 2024a, 2025; Lu *These authors contributed equally. Corresponding Author: Qifeng Chen; Co-corresponding Author: Wei Wei. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. et al. 2024a). However, their performance degrades significantly under real-world visual degradations (e.g., noise, blur, occlusion) (Malik et al. 2025; Schlarmann et al. 2024; Tang et al. 2023, 2024b), compromising reliability in practical applications. Therefore, enhancing robustness against such degradations remains critical challenge for deploying MLLMs in uncontrolled environments (Long et al. 2025). Existing approaches primarily rely on implicit training/adaptation strategies to integrate robustness, such as adversarial training (Wang et al. 2024b), robust visionlanguage alignment (Hossain and Imteaj 2024; Schlarmann et al. 2024; Yuan et al. 2024), or large-scale adversarial pretraining (Malik et al. 2025). These methods focus on fortifying visual encoders against distortions through data-centric optimization. While effective, they suffer from two fundamental limitations (as indicated in Figure 1-A): (i) Limited Interpretability: They lack explicit mechanisms to diagnose degradation impacts on original semantic information. (ii) Isolated Optimization: They neglect the degradationpropagation relation between the visual encoder and large language model. To overcome these limitations, we propose Robust-R1, novel framework that explicitly models visual degradations through structured reasoning. Unlike implicit paradigms, Robust-R1 firstly perceives degradation parameters (type and intensity), then analyzes their semantic impact on visual content, and finally reconstructs distortionfree interpretations to derive robust results. This explicit approach significantly enhances robustness while providing interpretable reasoning traces (as shown in Figure 1-B). Our implementation comprises three core stages: First, we perform Supervised Fine-Tuning (SFT) to equip pretrained MLLMs with foundational degradation-aware reasoning abilities. Second, we design reward function that aligns model outputs with accurate degradation parameters. Finally, we introduce complementary reward function to dynamically scale the reasoning chain length according to degradation severity, ensuring optimal efficiency. To support this approach, we construct an 11K dataset from A-OKVQA (Schwenk et al. 2022), comprising 10K training and 1K validation samples. For each sample, we synthesize realistic degradations by simulating four key stages: acquisition transmission environment postprocessing with random intensities. We then generate structured reasoning chains that link: (i) degradation parameters (Dd), (ii) their influence (d), (iii) the pristine semantic reasoning chain (TX), and (iv) the final conclusion (Yd). The complexity of these reasoning chains is dynamically scaled with the degradation intensity to balance robustness with computational efficiency. Comprehensive evaluations demonstrate Robust-R1s superior robustness. On the real-world degradation benchmark R-Bench (Li et al. 2024), Robust-R1 achieves stateof-the-art (SOTA) performance across all degradation intensities (low, medium, and high), outperforming existing general MLLMs and robust MLLMs. Furthermore, when subjected to adversarial degradation on general visual understanding benchmarks (MMMB (Sun et al. 2025), MMStar (Chen et al. 2024a), and RealWorldQA (xAI 2024)), Robust-R1 maintains significantly robust performance. It exhibits markedly smaller performance drop compared to all baselines under multi-level degradation intensities (25%, 50%, and 100%). Our contributions are summarized as: We propose Robust-R1, novel approach that explicitly mitigates visual degradations in MLLMs through structured reasoning chains, providing interpretable degradation diagnostics alongside enhanced robustness. We construct dataset of 11K samples featuring realistic degradations synthesized across four critical stages, each annotated with structured reasoning chains for degradation-aware reasoning. Robust-R1 achieves SOTA performance on the realworld robust visual understanding benchmark (R-Bench) robustness under adverand demonstrates superior sarial degradation on established general benchmarks (MMMB, MMStar, RealWorldQA), significantly outperforming existing general and robust MLLM baselines."
        },
        {
            "title": "In contrast",
            "content": "to these implicit adaptation paradigms, Robust-R1 introduces novel degradation-aware reasoning mechanism that explicitly enhances interpretability while improving robustness. Multimodal Reasoning Multimodal reasoning empowers MLLMs to solve complex tasks by integrating perception, contextual understanding, and logical inference (Wei et al. 2022). Prior work has made considerable progress in domains such as mathematical visual reasoning, where models are required to interpret and reason over problems involving both symbolic notations and visual elements (Wang et al. 2024a; Lu et al. 2024b). Subsequent research has expanded into broader visual reasoning scenarios, exemplified by frameworks like Visual CoT (Shao et al. 2024a) and V* (Wu and Xie 2024), which focus on parsing scene elements and their relational structure. Robust-R1 builds upon and extends this line of work by harnessing the MLLMs intrinsic reasoning capacity, pioneering its application to explicitly reason about and overcome visual distortions, thereby establishing new paradigm for robust multimodal understanding."
        },
        {
            "title": "3 Methodology\nProblem Definition Multimodal Large Language Models\n(MLLMs) frequently exhibit performance degradation when\nprocessing visually corrupted inputs in real-world scenar-\nios (Xu et al. 2025b,a), which undermines their interpreta-\ntion accuracy. This challenge can be represented as Eq. (1),",
            "content": "Yd = MMLLM(Xd P), (1) where Xd is the degraded visual input, derived as Xd = D(X), with as the original input and D() representing the degradation function. denotes the text prompt. MMLLM() denotes the original MLLM framework. Yd is the generated output under current conditions. indicates the multimodal Figure 2: Overview of Robust-R1. (A) Supervised Fine-Tuning (SFT): we train the model using reasoning data to equip it with basic degradation-aware reasoning capability; (B) Reinforcement Learning (RL): we propose two reward functions to (i) align precise degradation-aware space while (ii) adaptively scaling to suitable reasoning lengths based on degradation intensity. combination operator. To tackle this issue, we aim to develop robust MLLM framework that satisfies: M(Robust) MLLM (Xd P) approx MMLLM(X P), (2) where M(Robust) proximation operator proximating the output under pristine visual conditions. MLLM () denotes our enhanced model, and the apapprox signifies the objective of apOverview of Degradation-Aware Reasoning To address the above problem, Robust-R1 incorporates an explicit degradation-aware reasoning process that perceives degradation parameters (type and intensity), analyzes their impact on visual content, and reconstructs high-fidelity interpretations. This process is formulated as: {Mp (cid:0)Dd, Xd MMLLM M(Robust) MLLM (Xd P) (cid:0)TX Dd, d, Xd, P(cid:1) (cid:0)Yd (TX, Dd, d) Xd P(cid:1)}, (cid:1) Mr"
        },
        {
            "title": "3.1 Acquiring Basic Reasoning Ability\nTokenization of Reasoning Chain To enable structured\ndegradation-aware reasoning, we formalize the reasoning\nchain using special tokens (enclosed in“<” and “>”) that\nsegment distinct reasoning phases:",
            "content": "<TYPE>Dd<TYPE END>, <INFLUENCE>d<INFLUENCE END>, <REASONING>TX<REASONING END>, <CONCLUSION>Yd<CONCLUSION END>, (4) <ANSWER>Y(answer) <ANSWER END> (Optional), where Y(answer) denotes the task-specific answer output during benchmark evaluation. This tokenization enforces sequential reasoning flow to maintain structured output. (3) Supervised Fine-Tuning (SFT) We optimize model parameters θ through next-token prediction (as shown in Figure 2-A) on the structured reasoning chain: where Mp() is degradation parameters perception process, to perceive Dd = {τ (i) }I i=1 (types τd and intensities sd) and their impact d; Mr() reconstructs the pristine semantic representation TX of original X; and original MMLLM() can generate the robust output Yd conditioned on degradation-aware reasoning chain. , s(i) Workflow Firstly, to integrate degradation-aware reasoning capabilities, We first fine-tune the pretrained visionlanguage model to establish foundational degradation-aware reasoning capabilities (Section 3.1). Subsequently, We employ reinforcement learning with dedicated reward function to align the models perception with accurate degradation parameters (Dd) (Section 3.2). Finally, we dynamically adjust the reasoning chain length based on degradation intensity to optimize the trade-off between robustness and efficiency (Section 3.3). LSFT = E(Xd,P,Y)PT (cid:88) n=1 log Pθ (wn w<n, Xd, P) , (5) where = (wn, . . . , wN ) {Dd, TX Yd} represents the output reasoning chain. denotes the sequence length, Pθ is the models conditional probability distribution, PT denotes the distribution of training data. This optimization enables the model to acquire foundational degradation-aware reasoning ability by sequentially generating the structured reasoning chain."
        },
        {
            "title": "3.2 Aligning Accurate Degradation Parameters\nAlthough SFT equips\nthe MLLM with foundational\ndegradation-aware reasoning ability, it still lacks an accu-\nrate perception of degradation parameters (types and inten-\nsities). As quantitatively demonstrated in Figure 6-A (w/o",
            "content": "where len(YGT) is the optimal length from ground truth. This reward equals 1 when lengths match exactly len(Y) = len(YGT), and decreases linearly with relative length discrepancy. Reinforcement Learning (RL) We integrate these two rewards into unified optimization framework: R(Y, YGT) = rdeg(Y, YGT) + rlen(Y, YGT), where R() represents the comprehensive reward function. This composite reward drives Group Relative Preference Optimization (GRPO) (Shao et al. 2024b), and for each input pair Xd P, we sample candidate responses {Y(g)}G g=1. The group-relative advantage is computed as: (9) R(g) µR σR where R(g) = R(Y(g), YGT), with: ˆA(g) = , (10) µR ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) g=1 R(g), σR = (cid:118) (cid:117) (cid:117) (cid:116)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) g=1 (R(g) µR)2, (11) Through GRPO optimization (Shao et al. 2024b), we maximize the expected composite reward: θ = arg max θ E(Xd,P)PT [R (Y, YGT)] . (12) This optimization strategy achieves dual objectives: (1) accurate alignment with degradation parameters through rdeg, and (2) suitable allocation of computational efficiency through rlen. The combined approach ensures robust visual understanding while maintaining efficiency across diverse real-world degradation scenarios."
        },
        {
            "title": "4 Data Construction\nExisting visual understanding datasets (e.g., LLaVA (Liu\net al. 2024), R-Bench (Li et al. 2024), A-OKVQA (Schwenk\net al. 2022), Conceptual Captions (Sharma et al. 2018)) lack\nexplicit annotations for degradation parameters (Dd), their\nimpacts (∆d), and pristine semantic reasoning chains (TX).\nThis gap hinders training degradation-aware MLLMs. To\nbridge this gap, we construct a specialized dataset featur-\ning synthetically generated degradations and structured rea-\nsoning annotations. Our dataset is built upon a subset of A-\nOKVQA (Schwenk et al. 2022), comprising 10K samples\nfor training and 1K for validation.",
            "content": "Our whole automated annotation pipeline, illustrated in Figure 4. The procedure consists of the following five steps: Step (1): Synthesizing Real-World Degradations We construct comprehensive degradation model D() that simulates degradations introduced across four real-world image processing stages: 1. Acquisition (Lens Blur, Lens Flare, Motion Blur, Dirty Lens, Saturation), 2. Transmission (Compression, Block Change, Shifting, Scan Lines), 3. Environment (Darkness, Atmospheric Turbulence, Noise, Color Diffusion), and 4. Postprocessing (Sharpness Change, Graffiti, Watermark Damage). For each pristine image X, we generate degraded version by: (cid:16) Xd = X ; {τ (i) , s(i) }I i=1 (cid:17) , (13) Figure 3: Correlation between degradation intensity and reasoning chain length on Seed-1.5-VL (Guo et al. 2025). Higher degradation intensities require longer chains to maintain accuracy, even multi-step reasoning. Dd), lacking precise alignment exhibits significant deviation from practical degradation parameters, leading to limited degradation perception ability. Reward for Accurate Degradation Parameters To achieve high-fidelity alignment, we design reward function that directly operates in the degradation parameter space (as shown in Figure 2-B (left)). The reward function rdeg(Y, YGT) explicitly evaluates degradation parameter deviation: rdeg(Y, YGT) = (cid:88) (cid:88) i=1 j=1 δ(τ (i) = τ (j) GT ) (cid:16) 1 (cid:12) s(j) (cid:12)s(i) (cid:12) GT (cid:12) (cid:17) (cid:12) (cid:12) δ(τ (i) = τ (j) GT ), (6) where δ() denotes the Kronecker delta function (web 2025). This formulation specifically: (1) penalizes type mismatches with 1 reward; (2) rewards type matches proportionally to intensity accuracy (1 s); and (3) aggregates rewards across all instances (i = 1, . . . , and = 1, . . . , J)."
        },
        {
            "title": "3.3 Scaling to Suitable Reasoning Length\nAlthough we achieve accurate Dd alignment, longer rea-\nsoning chains may introduce computational redundancy. As\nidentified in (Sui et al. 2025), such “overthinking” reduces\ninference efficiency without improving output quality.",
            "content": "Observation Through empirical analysis in Figure 3, we observe strong correlation between degradation intensity and required reasoning length, as: (cid:34) (cid:35) len(Y) (cid:88) s(i) , (7) i=1 where len(Y) denotes the length of the generated reasoning chain. Higher degradation levels necessitate longer reasoning chains, while simpler degradations only require shorter responses. Reward for Suitable Reasoning Length To optimize computational efficiency while maintaining robustness, we introduce length-modulation reward (Figure 2-B (right)): rlen(Y, YGT) = 1 len(Y) len(YGT) len(YGT) , (8) Figure 4: Data generation pipeline. The original images undergo various real-world processing stages, where multiple degradations are randomly added to obtain degraded images and their corresponding degradation <TYPE>s. Based on these and the original question-answering pairs (QAs), the pipeline progressively generates <INFLUENCE>, <REASONING>, and <CONCLUSION>. Finally, the reasoning chain is scaling according to different intensities to achieve optimal efficiency. where the degradation function D() is parameterized by randomly sampled types τ (i) U[0, 1]. Step (2): Generating Degradation Influence We employ GPT-4o (Hurst et al. 2024) with fixed prompt template ΨINFLUENCE to produce textual description of the degradations semantic impact: and intensities s(i) = GGPT-4o(X, Xd, Dd, YGT ; ΨINFLUENCE). This narrative establishes causal link between the visual degradation and its effect on content interpretation, providing the necessary supervision for training the perception module Mp(). (14) Step (3): Generating Pristine Semantic Reasoning Using distinct prompt template ΨREASONING, we instruct GPT4o to infer the original semantic reasoning chain TX by compensating for the degradation influence: TX = GGPT-4o(Xd, Dd, d, YGT ; ΨREASONING), (15) This step recovers the underlying reasoning process as if performed on the undistorted image, which is crucial for training the reconstruction module Mr(). Step (4): Generating Reasoning Conclusion The final reasoning conclusion Yd is generated by conditioning on the pristine semantic reasoning and the ground-truth answer, using prompt template ΨCONCLUSION: Yd = GGPT-4o(TX, YGT ; ΨCONCLUSION). Step (5): Scaling Reasoning Chain Length To enable adaptive computational allocation, we dynamically adjust the length of the complete reasoning chain based on the total degradation intensity: (cid:32) (16) (cid:33) ˆC = GGPT-4o ; ΨLen( s(i) ) , (17) (cid:88) i=1 where ˆC denotes the scaled reasoning chain, and ΨLen() is set of intensity-calibrated prompt templates. This procedure ensures reasoning efficiency and is instrumental for optimizing the length reward rlen. Quality and Robustness The resulting dataset, structured according to the reasoning process defined in Eq. (3), supports both the SFT and the subsequent GRPO optimization of our robust model M(Robust) MLLM (). Besides, the inverse relation between image quality and degradation intensity validates that the distribution of corruptions in our dataset mirrors real-world conditions. The lexical diversity of the reasoning corpus, demonstrates its inherent capacity to model complex logical relationships. This establishes foundation for achieving robust performance. More details in the supplementary material."
        },
        {
            "title": "5 Experiments\nTraining Configuration Our model\nis built upon\nQwen2.5-VL-3B (Bai et al. 2025), which employs a re-\ndesigned Vision Transformer (ViT) as its vision encoder.\nWe adopt a dual-stage optimization strategy:",
            "content": "Supervised Fine-Tuning (SFT): 25% training data used to establish basic instruction-following ability. Reinforcement Learning (RL): 75% data for align accurate degradation parameters and suitable chain length. Notably, we freeze both the vision encoder and visual projection layers while performing full-parameter fine-tuning on the language model. This design preserves visual feature stability while empowering the MLLM to develop robust degradation-aware reasoning mechanisms. Baselines We compare against two categories SOTA baselines: (i) General MLLMs, including Qwen2.5-VL-3B (Bai"
        },
        {
            "title": "Robust\nMLLM",
            "content": "Qwen2.5-VL-3B (Bai et al. 2025) Gemma3-4B (Team et al. 2025) InternVL-4B (Chen et al. 2024b) TeCoA (Wang et al. 2024b) Robust CLIP (Schlarmann et al. 2024) Robust LLaVA (Malik et al. 2025)"
        },
        {
            "title": "MCQ",
            "content": "low mid high low 0.6411 0.5823 0.6235 0.4647 0.4705 0. 0.6176 0.6529 0.6022 0.5776 0.6024 0.4223 0.4658 0.2608 0.6087 0.6391 0.5732 0.5060 0.5914 0.4024 0.4024 0. 0.5610 0.6097 0.4872 0.4865 0.4982 0.4687 0.4503 0.2607 0.4804 0."
        },
        {
            "title": "VQA",
            "content": "mid 0.4854 0.4630 0.4539 0.3994 0.4339 0.2212 0.4836 0.4909 high low 0.4904 0.4419 0.5108 0.4461 0.4743 0.2443 0.5012 0.4980 0.3778 0.4048 0.3667 0.2111 0.2290 0.0068 0.4080 0."
        },
        {
            "title": "CAP",
            "content": "mid 0.3704 0.3746 0.3041 0.2195 0.2219 0.0065 0.3858 0.3781 high 0.3330 0.3480 0. 0.1937 0.1983 0.0067 0.3518 0."
        },
        {
            "title": "Overall",
            "content": "0.4845 0.4649 0.4706 0.3586 0.3718 0.1830 0.4886 0.5017 Table 1: Quantitative performance on R-Bench (Li et al. 2024) on MCQ/VQA/CAP tasks with three degradation strength levels (from low to high). The best/second best results are shown in Red/Blue respectively. Category Method General MLLM Qwen2.5-VL-3B (Bai et al. 2025) Gemma3-4B (Team et al. 2025) InternVL-4B (Chen et al. 2024b) Robust MLLM TeCoA (Wang et al. 2024b) Robust CLIP (Schlarmann et al. 2024) Ours SFT SFT and RL MMMB (Sun et al. 2025) MMStar (Chen et al. 2024a) RealWorldQA (xAI 2024) clean 80.60 71.01 77.97 57.17 58.83 80.85 81. Intensity 25% 50% 100% 79.19 70.30 77.47 65.71 58.28 79.45 79.49 78.68 70.20 76.66 56.11 57. 78.68 79.04 74.50 69.14 74.59 51.76 53.33 74.94 75.35 clean 54.73 43.93 51. 30.46 33.00 55.20 56.86 Intensity 25% 50% 100% 52.90 43.20 50.26 30.60 32.26 53.00 54. 51.86 42.60 49.60 30.73 31.80 51.86 53.60 48.66 41.33 46.93 28.06 29.46 49.53 49. clean 65.22 55.42 57.38 40.00 43.26 68.23 67.71 Intensity 25% 50% 100% 64.96 54.77 58. 39.73 42.48 67.58 66.40 63.39 53.72 57.64 39.47 42.61 67.32 67.05 60.65 52.81 54. 38.69 41.43 63.92 63.26 Table 2: Quantitative performance for anti-degradation on three visual understanding benchmarks (MMMB (Sun et al. 2025), MMStar (Chen et al. 2024a), and RealWorldQA (xAI 2024)) with three degradation intensity levels (from 25% to 100%). The best/second best results are showed in Red/Blue respectively. et al. 2025), Gemma3-4B (Team et al. 2025), and InternVL4B (Chen et al. 2024b); (ii) Robust MLLMs, comprising TeCoA (Wang et al. 2024b), Robust CLIP (Schlarmann et al. 2024), and Robust LLaVA (Malik et al. 2025). Benchmarks We conduct rigorous evaluation across two dimensions: (i) Real-World Robustness: Directly assessing robust visual understanding ability on R-Bench (Li et al. 2024); (ii) Adversarial Robustness: Evaluation under synthetic degradation attacks by applying multi-type, multi-level real-world degradations to visual content in MMMB (Sun et al. 2025), MMStar (Chen et al. 2024a), and RealWorldQA (xAI 2024). This dual-strategy comprehensively measures both intrinsic degradation comprehension and performance preservation under visual corruption."
        },
        {
            "title": "5.1 Performance on R-Bench",
            "content": "R-Bench (Li et al. 2024) is benchmark designed to directly evaluate image understanding capabilities under realworld degradation conditions. It incorporates three distinct task types (Multiple Choice Questions (MCQ), Visual Question Answering (VQA), and Image Captioning (CAP)) with three degradation intensity levels (low, mid, and high) to systematically assess the robustness of visual comprehension. As shown in Table 1, Robust-R1 (Ours) demonstrates significant improvements in image understanding capabilities following both Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning optimization (SFT and RL). Experimental results indicate that our model surpasses Figure 5: Qualitative evaluation for anti-degradation. Ours (SFT and RL) can provide robust and efficient result."
        },
        {
            "title": "MCQ",
            "content": "low mid high low"
        },
        {
            "title": "VQA",
            "content": "mid high low"
        },
        {
            "title": "CAP",
            "content": "mid"
        },
        {
            "title": "Overall",
            "content": "high Qwen2.5-VL-3B (Bai et al. 2025) 0.6411 0.6022 0.5732 0. 0.4854 0.4904 0.3778 0.3704 0.3330 0. Ours (w/o Reasoning) Ours (w/o rdeg) Ours (w/o rlen) Ours 0.6588 0.6647 0.6647 0.5901 0.6398 0.6354 0.4756 0.5505 0.5975 0.4905 0.4912 0.4904 0.4900 0.4894 0. 0.4862 0.5056 0.4877 0.2901 0.3684 0.3656 0.2673 0.3578 0.3678 0.2758 0.3248 0.3189 0.4471 0.4880 0.4907 0. 0.6391 0.6097 0.4914 0.4909 0.4980 0. 0.3781 0.3484 0.5017 Table 3: Ablation study on R-Bench (Li et al. 2024) on MCQ/VQA/CAP tasks with three degradation strength levels (from low to high). The best/second best results are showed in Red/Blue respectively. existing general and robust MLLMs baselines in overall performance on this benchmark."
        },
        {
            "title": "5.2 Anti-Degradation Performance\nTo rigorously evaluate our model’s robustness against\nimage degradation, we conduct comprehensive experi-\nments on three established visual understanding benchmarks\n(MMMB (Sun et al. 2025), MMStar (Chen et al. 2024a), and\nRealWorldQA (xAI 2024)). We introduce random degrada-\ntions at varying intensity levels (25%, 50%, and 100%) to\nthe original images, creating challenging test conditions that\nassess the model’s anti-degradation capability.",
            "content": "Quantitative Results As demonstrated in Table 2, our model achieves SOTA performance across all degradation levels compared to existing baselines. This evidence confirms our models exceptional robustness to diverse image degradations under adversarial conditions. Qualitative Result Figure 5 presents qualitative comparisons of our outputs. Compared to the original baseline, Robust-R1 significantly reduces hallucinations and errors in visual understanding through reasoning. Furthermore, after preference optimization, Robust-R1 achieves optimal balance between inference efficiency and accurate degradation parameters perception."
        },
        {
            "title": "5.3 Ablation Study\nReasoning vs. Adaptation To validate the effectiveness\nof explicit reasoning versus implicit adaptation, we con-\nduct an ablation study by removing degradation reasoning\nchains from our training data, relying solely on fine-tuning\nfor adaptation (Table 3, w/o Reasoning). The experimental\nresults reveal two critical findings: (i) Adaptation provides\nonly marginal performance gains in specific intensity ranges\ncompared to the base model, and fails catastrophically in\nhigh-intensity degradation scenarios; (ii) Explicit reasoning\ndemonstrates significantly improved robustness over both\nthe adaptation-only model and the original baseline. These\nresults conclusively demonstrate that explicit reasoning ca-\npability is essential for robust visual understanding, enabling\nsystematic analysis and compensation for visual degrada-\ntions rather than mere adaptation.",
            "content": "Effectiveness of rdeg To validate the critical role of the degradation reward rdeg, we conduct an ablation study comparing model performance with and without this component. Figure 6: Statistics analysis for (A) rdeg and (B) rlen. As shown in Table 3, incorporating rdeg substantially improves visual understanding performance on R-Bench compared to the ablated variant. This improvement stems from rdegs ability to enhance precise alignment with degradation parameters. Furthermore, statistical analysis on our out-ofdomain testset (Section 4) in Figure 6-A reveals that rdeg significantly reduces two key error types: (i) degradationtype misclassification and (ii) degradation-intensity estimation bias. These results demonstrate that rdeg increases model precision in identifying degradation parameters, directly contributing to superior robustness. Efficiency of rlen To evaluate the effectiveness of the length-modulation reward rlen, we conduct an ablation study by removing this component. As shown in Figure 6-B, incorporating rlen reduces the average reasoning chain length while maintaining performance, demonstrating its ability to improve computational efficiency. Notably, the model adaptively adjusts reasoning depth based on degradation intensity: longer chains are allocated for severe degradation, while simpler cases require less inference. This taskadaptive allocation not only optimizes resource usage but also enhances overall performance, as evidenced by the quantitative improvements in Table 3 (w/o rlen)."
        },
        {
            "title": "6 Conclusion\nWe propose Robust-R1, a novel paradigm that incorpo-\nrates explicit degradation reasoning chains to enhance multi-\nmodal understanding robustness. We believe this work opens\nnew avenues for building more robust, interpretable, and ef-\nficient multimodal systems capable of operating reliably in\nvisually challenging environments.",
            "content": "Acknowledgments The work described in this paper was supported by grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project Reference Number: AoE/E-601/24-N). Besides, this work was supported in part by the National Natural Science Foundation of China (No. 62472359, 62372379), in part by the Xians Key Industrial Chain Core Technology Breakthrough Project: AI Core Technology Breakthrough under Grand 24ZDCYJSGG0003. Also, this work was supported by the Key Project of the National Natural Science Foundation of China (No. 62536007), the Zhejiang Province Science Foundation (No. LD24F020002) and the Zhejiang Provinces 2025 Leading Goose + Science and Technology Plan (No. 2025C02034)."
        },
        {
            "title": "References",
            "content": "https://en.wikipedia.org/wiki/ 2025. Kronecker delta. Kronecker delta. Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; Zhong, H.; Zhu, Y.; Yang, M.; Li, Z.; Wan, J.; Wang, P.; Ding, W.; Fu, Z.; Xu, Y.; Ye, J.; Zhang, X.; Xie, T.; Cheng, Z.; Zhang, H.; Yang, Z.; Xu, H.; and Lin, J. 2025. Qwen2.5-VL Technical Report. arXiv. Chen, L.; Li, J.; Dong, X.; Zhang, P.; Zang, Y.; Chen, Z.; Duan, H.; Wang, J.; Qiao, Y.; Lin, D.; et al. 2024a. Are We on the Right Way for Evaluating Large Vision-Language Models? In NeurIPS. Chen, Z.; Wu, J.; Wang, W.; Su, W.; Chen, G.; Xing, S.; Zhong, M.; Zhang, Q.; Zhu, X.; Lu, L.; et al. 2024b. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In CVPR. Fu, B.; Wei, W.; Tang, J.; Nie, J.; Ye, Y.; Xu, X.; Chen, Y.-C.; and Zhang, L. 2025. Co-Painter: Fine-Grained Controllable Image Stylization via Implicit Decoupling and Adaptive Injection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 1683016839. Guo, D.; Wu, F.; Zhu, F.; Leng, F.; Shi, G.; Chen, H.; Fan, H.; Wang, J.; Jiang, J.; Wang, J.; et al. 2025. Seed1. 5-vl technical report. arXiv. Sim-clip: UnHossain, M. Z.; and Imteaj, A. 2024. supervised siamese adversarial fine-tuning for robust and semantically-rich vision-language models. arXiv. Hurst, A.; Lerer, A.; Goucher, A. P.; Perelman, A.; Ramesh, A.; Clark, A.; Ostrow, A.; Welihinda, A.; Hayes, A.; Radford, A.; et al. 2024. Gpt-4o system card. arXiv. Li, C.; Zhang, J.; Zhang, Z.; Wu, H.; Tian, Y.; Sun, W.; Lu, G.; Liu, X.; Min, X.; Lin, W.; and Zhai, G. 2024. R-Bench: Are your Large Multimodal Model Robust to Real-world Corruptions? IEEE JSTSP. Liu, H.; Li, C.; Li, Y.; and Lee, Y. J. 2024. Improved Baselines with Visual Instruction Tuning. In CVPR. Long, J.; Xu, Z.; Jiang, T.; Yao, W.; Jia, S.; Ma, C.; and Chen, X. 2025. Robust SAM: On the Adversarial Robustness of Vision Foundation Models. In AAAI. Lu, H.; Niu, X.; Wang, J.; Wang, Y.; Hu, Q.; Tang, J.; Zhang, Y.; Yuan, K.; Huang, B.; Yu, Z.; et al. 2024a. Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing. In CVPR. Lu, P.; Bansal, H.; Xia, T.; Liu, J.; Li, C.; Hajishirzi, H.; Cheng, H.; Chang, K.-W.; Galley, M.; and Gao, J. 2024b. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR. Ma, K.; Tang, J.; Guo, B.; Dang, F.; Liu, S.; Zhu, Z.; Wu, L.; Fang, C.; Chen, Y.-C.; Yu, Z.; et al. 2025. SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity. In Proceedings of the Computer Vision and Pattern Recognition Conference, 3051430523. Malik, H. S.; Shamshad, F.; Naseer, M.; Nandakumar, K.; Khan, F.; and Khan, S. 2025. Robust-llava: On the effectiveness of large-scale robust image encoders for multi-modal large language models. In ICCVW. Schlarmann, C.; Singh, N. D.; Croce, F.; and Hein, M. 2024. Robust clip: Unsupervised adversarial fine-tuning of vision In embeddings for robust large vision-language models. ICML. Schwenk, D.; Khandelwal, A.; Clark, C.; Marino, K.; and Mottaghi, R. 2022. A-OKVQA: Benchmark for Visual Question Answering using World Knowledge. In ECCV. Shao, H.; Qian, S.; Xiao, H.; Song, G.; Zong, Z.; Wang, L.; Liu, Y.; and Li, H. 2024a. Visual cot: Advancing multimodal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. NeurIPS. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024b. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv. Sharma, P.; Ding, N.; Goodman, S.; and Soricut, R. 2018. Conceptual Captions: Cleaned, Hypernymed, Image Alttext Dataset For Automatic Image Captioning. In ACL. Sui, Y.; Chuang, Y.-N.; Wang, G.; Zhang, J.; Zhang, T.; Yuan, J.; Liu, H.; Wen, A.; Zhong, S.; Chen, H.; and Hu, X. 2025. Stop Overthinking: Survey on Efficient Reasoning for Large Language Models. TMLR. Sun, H.-L.; Zhou, D.-W.; Li, Y.; Lu, S.; Yi, C.; Chen, Q.-G.; Xu, Z.; Luo, W.; Zhang, K.; Zhan, D.-C.; and Ye, H.-J. 2025. Parrot: Multilingual Visual Instruction Tuning. arxiv. Tang, J.; Lu, H.; Wu, R.; Xu, X.; Ma, K.; Fang, C.; Guo, B.; Lu, J.; Chen, Q.; and Chen, Y.-C. 2024a. HAWK: Learning to Understand Open-World Video Anomalies. In NeurIPS. Tang, J.; Wu, R.; Xu, X.; Hu, S.; and Chen, Y.-C. 2024b. Learning to Remove Wrinkled Transparent Film with Polarized Prior. In CVPR. Tang, J.; Xia, Y.; Wu, Y.-F.; Hu, Y.; Chen, Y.; Chen, Q.-G.; Xu, X.; Wu, X.; Lu, H.; Ma, Y.; Lu, S.; and Chen, Q. 2025. LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization. arxiv. Tang, J.; Xu, X.; Hu, S.; and Chen, Y.-C. 2023. High Dynamic Range Image Reconstruction via Deep Explicit Polynomial Curve Estimation. In ECAI. Team, G.; Kamath, A.; Ferret, J.; Pathak, S.; Vieillard, N.; Merhej, R.; Perrin, S.; Matejovicova, T.; Rame, A.; Rivi`ere, M.; et al. 2025. Gemma 3 technical report. arXiv. Wang, K.; Pan, J.; Shi, W.; Lu, Z.; Ren, H.; Zhou, A.; Zhan, M.; and Li, H. 2024a. Measuring multimodal mathematical reasoning with math-vision dataset. NeurIPS. Wang, S.; Zhang, J.; Yuan, Z.; and Shan, S. 2024b. Pretrained model guided fine-tuning for zero-shot adversarial robustness. In CVPR. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi, E.; Le, Q. V.; Zhou, D.; et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. NeurIPS. Wu, P.; and Xie, S. 2024. V?: Guided visual search as core mechanism in multimodal llms. In CVPR. xAI. 2024. Grok-1.5 Vision Preview. Xu, X.; Wu, J.; Yan, Q.; Cui, J.; Hong, R.; and Yu, B. 2025a. Learnable Feature Patches and Vectors for Boosting Lowlight Image Enhancement without External Knowledge. In CVPR. Xu, X.; Zhou, K.; Hu, T.; Wu, J.; Wang, R.; Peng, H.; and Yu, B. 2025b. Low-Light Video Enhancement via SpatialTemporal Consistent Decomposition. In IJCAI. Yuan, F.; Qin, C.; Xu, X.; and Li, P. 2024. Helpd: Mitigating hallucination of lvlms by hierarchical feedback learning with vision-enhanced penalty decoding. In EMNLP."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "Hong Kong University of Science and Technology",
        "Nanjing University of Science and Technology",
        "Northwestern Polytechnical University",
        "University of Hong Kong"
    ]
}