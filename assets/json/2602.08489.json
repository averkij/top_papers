{
    "paper_title": "Beyond Correctness: Learning Robust Reasoning via Transfer",
    "authors": [
        "Hyunseok Lee",
        "Soheil Abbasloo",
        "Jihoon Tack",
        "Jinwoo Shin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final answer correctness leaves a critical gap: it does not ensure the robustness of the reasoning process itself. We adopt a simple philosophical view, robust reasoning should remain useful beyond the mind that produced it, and treat reasoning as a form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether a partial reasoning prefix from one model can guide a separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH500, RLTR achieves a +3.6%p gain in Maj@64 compared to RLVR and matches RLVR's average accuracy with roughly 2.5x fewer training steps, providing both more reliable reasoning and significantly more sample efficient."
        },
        {
            "title": "Start",
            "content": "Beyond Correctness: Learning Robust Reasoning via Transfer Hyunseok Lee 1 Soheil Abbasloo 2 Jihoon Tack 2 Jinwoo Shin 1 6 2 0 2 9 ] . [ 1 9 8 4 8 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently strengthened LLM reasoning, but its focus on final-answer correctness leaves critical gap: it does not ensure the robustness of the reasoning process itself. We adopt simple philosophical viewrobust reasoning should remain useful beyond the mind that produced itand treat reasoning as form of meaning transfer that must survive truncation, reinterpretation, and continuation. Building on this principle, we introduce Reinforcement Learning with Transferable Reward (RLTR), which operationalizes robustness via transfer reward that tests whether partial reasoning prefix from one model can guide separate model to the correct answer. This encourages LLMs to produce reasoning that is stable, interpretable, and genuinely generalizable. Our approach improves sampling consistency while improving final-answer accuracy, and it reaches comparable performance in substantially fewer training steps. For example, on MATH-500, RLTR achieves +3.6%p gain in Maj@64 compared to RLVR and matches RLVRs average accuracy with roughly 2.5 fewer training steps, providing both more reliable reasoning and significantly more sample-efficient. 1. Introduction Large language models (LLMs) combined with reinforcement learning (RL) have recently demonstrated remarkable performance across various domains, such as solving complex mathematical problems (Shao et al., 2024), serving as coding assistants (Le et al., 2022), and contributing to scientific discoveries (Romera-Paredes et al., 2024). Compared with simple supervised fine-tuning, RL enables LLMs to learn more generalizable and robust behaviors (Ouyang et al., 2022). However, the success of RL in LLMs is ofWork done during an internship at Microsoft Research Asia 1KAIST 2Microsoft Research. Correspondence to: Soheil Abbasloo <soheil.abbasloo@microsoft.com>. Preprint. ten bottlenecked by its heavy reliance on reward models. While reward models reduce the need for costly human annotations, they introduce critical challenges such as reward hacking (Skalse et al., 2022) and increased training complexity (Casper et al., 2023). To tackle this issue, reinforcement learning with verifiable reward (RLVR) has recently emerged (Shao et al., 2024; Guo et al., 2025). RLVR replaces learned reward models with rule-based verification of the correctness of final answers, which significantly simplifies the training pipeline and alleviates reward hacking and the need for extensive human annotations. As result, RLVR achieves remarkable success on complex reasoning tasks, especially in competitive math domains. Yet despite these gains, RLVR optimizes reasoning solely through final-answer correctness, leaving critical dimension of reasoning quality underexplored: the robustness of the reasoning process itself. In real-world and practical settings, models that consistently produce correct answers across different sampled solutions, decoding variations, or reasoning paths are far more valuable than models that succeed only sporadically. However, RLVRs objective evaluates only the generators final output, providing no incentive for intermediate reasoning to be robust, reusable, or stable under such perturbations. Prior work has observed that the model loses consistency and diversity as more samples are generated, suggesting that final-outcome correctness alone may be an incomplete signal for promoting reliable reasoning behavior (Yue et al., 2025). To move beyond outcome-only evaluation, we draw inspiration from human learning: strong reasoning is not merely correctit is also explainable in way that others can reliably follow. This motivates our central concept, reasoning transferability, defined as the extent to which reasoning prefix produced by one model can be completed by another model to reach verifiably correct answer. We operationalize this idea by truncating generators reasoning trace and asking separate receiver model to continue it; transfer succeeds when the combined reasoning yields correct final answer. Building on this principle, we propose Reinforcement Learning with Transferable Reward (RLTR), new reinforcement learning framework that explicitly optimizes cross-model reasoning transferability. The core mechanism is transfer reward that evaluates generators partial reasoning via receiver-model continuation, integrated into Beyond Correctness: Learning Robust Reasoning via Transfer RLVR through transfer roll-out procedure and weighted combination of answer, transfer, and format rewards. To our knowledge, RLTR is the first RLVR framework that directly incentivizes LLMs to produce reasoning traces that are transferable across models. An overview of RLTR is shown in Figure 1. We demonstrated the effectiveness of RLTR through extensive evaluations on multiple reasoning datasets covering wide range of difficulties across mathematical and scientific domains. Overall, RLTR consistently improves Majority Voting at (Maj@K), proxy for multi-sample consistency, while also improving average accuracy. For instance, on AMC23 (MAA, 2023), RLTR improves Maj@64 from 61.7 to 67.5 compared to RLVR. Across training, RLTR achieves the same average accuracy level with about 2.5 fewer training steps than RLVR on MATH-500, indicating faster learning and improved sample efficiency. Finally, we show transferability correlates with multi-sample consistency by tracking transferability and Maj@K throughout training. We highlight the main contributions of the paper below: We introduce RLTR, simple extension of RLVR that adds transfer reward to optimize reasoning transferability across models. We demonstrate that RLTR increases consistency, as shown by higher majority voting accuracy, which reflects more robust reasoning compared to RLVR. Our training framework improves training sample efficiency e.g., RLTR matches RLVRs performance with 2.5 fewer training steps and improved average accuracy on MATH-500. 2. Related Work Reinforcement Learning in LLM. Reinforcement learning (RL) has become cornerstone for aligning LLMs with human intent, primarily through Reinforcement Learning from Human Feedback (RLHF; Ouyang et al., 2022). Standard approaches typically employ Proximal Policy Optimization (PPO; Schulman et al., 2017) to optimize policy against learned reward model (RM) that mimics human preferences. However, this dependence on neural reward models introduces significant challenges. As the policy optimizer exploits the reward model, it often leads to reward hacking, where the model generates high-reward but low-quality or nonsensical outputs due to the imperfections of the proxy reward (Skalse et al., 2022; Gao et al., 2023; Saito et al., 2023). Furthermore, training robust reward model requires extensive human annotation and often creates bottleneck in scaling reasoning capabilities. Reinforcement Learning with Verifiable Reward. To mitigate the limitations of neural reward models, recent research has shifted towards RL with Verifiable Reward (RLVR), 2 where the reward is derived from ground-truth correctness (e.g., mathematical answers or code execution) rather than learned proxy. Pioneering works such as DeepSeek-Math (Shao et al., 2024) and DeepSeek-R1 (Guo et al., 2025) introduced Group Relative Policy Optimization (GRPO) to stabilize RL training without critic model, with verifiable reward that can induce emergent reasoning behaviors, significantly boosting performance on complex benchmarks. The following works were mainly focusing on stabilize the learning algorithm and enhance the learning objectives (Yu et al., 2025; Liu et al., 2025). While these methods effectively address reward hacking by relying on objective outcomes, they primarily evaluate the result rather than the process. Unlike prior approaches that rely solely on self-consistency or final-answer correctness, RLTR introduces novel reward signal based on reasoning transferability. Process Reward Models. Process Reward Models (PRMs) offer an alternative by providing dense step-level supervision using learned verifiers trained on large annotated reasoning traces. Process supervision has been shown to reduce reasoning errors compared to outcome-only training (Uesato et al., 2022). Verifier-based step scoring has been explored for GSM8K (Cobbe et al., 2021), and large-scale datasets (e.g., PRM800K) enable powerful verifier models that outperform outcome-only training on MATH (Lightman et al., 2023). Recent variants include Q-value PRMs (Li & Li, 2024), reasoning-driven PRMs (She et al., 2025), and generative verifier models for long chain-of-thought (Khalifa et al., 2025). However, PRMs require high-capacity learned step-level verifiers, substantial step-level annotations, and reintroduce proxy reward mismatch. Unlike PRMs, RLTR operates without any additional annotation requirements beyond those of the answer-verification dataset, and does not require training any reward models for intermediate reasoning supervision. 3. RLTR: Reinforcement Learning with"
        },
        {
            "title": "Transferable Reward",
            "content": "In this section, we introduce Reinforcement Learning with Transferable Reward (RLTR), reinforcement learning framework that encourages robust and reusable reasoning traces by augmenting RLVR with transfer reward. Building on RLVR-style final answer supervision, RLTR introduces transfer reward that evaluates whether another model (namely, the receiver model) can recover the correct answer from truncated reasoning prefix, and maximizes this signal as an auxiliary reward objective. We first formalize the learning framework, RLVR, in Section 3.1 and at the top of Figure 1. We then present RLTR, the core training algorithm that integrates the transfer reward into the RL objective (Section 3.2 and bottom in Figure 1). Beyond Correctness: Learning Robust Reasoning via Transfer Figure 1. Overview of RLTR: RLTR augments standard RLVR with transfer reward. Top: trainable generator model produces full completion, whose final-answer correctness yields on answer reward for policy optimization. Bottom: We then truncate the generated reasoning to form prefix and feed it to frozen receiver model to produce continued completions whose final-answer correctness defines transfer reward that measures the transferability of partial reasoning across models. The answer reward and transfer reward are combined into unified reward signal used to update the generator policy. 3.1. Preliminary: RLVR 3.2. Reinforcement Learning with Transferable Reward RLVR optimizes the model to produce correct final answer under fixed output format. Given an input x, the generator model Mgen samples full response (including reasoning and the final answer), i.e., ygen Mgen(x). The reward is primarily defined by an answer reward that checks whether the generators final answer matches the ground truth, i.e., Rans(ygen) = 1[answer(ygen) = ygt]. In addition, format reward Rfmt(ygen) is used to encourage well-formed outputs (e.g., valid reasoning structure and answer formatting), and to penalize invalid generations."
        },
        {
            "title": "The overall reward is defined as",
            "content": "RRLVR(ygen) = Rans(ygen) + Rfmt(ygen). (1) For given dataset := {(x, y)}, the generator is optimized via reinforcement learning with the objective as: E(x,ygt)D (cid:2)EygenMgen(x) [RRLVR(ygen)](cid:3) . (2) max Mgen In this paper, we mainly optimize the RL policy with group relative policy optimization (GRPO; Shao et al., 2024; Guo et al., 2025), which is known to be simple yet strong when use with RLVR. Overall, conventional RLVR optimizes the correctness of the generators own final output. However, it does not explicitly account for step-level noise and brittleness in the reasoning trajectory, nor does it directly encourage reusable or interpretable reasoning that can be reliably continued by other models or decoding processes. To address this limitation, we propose RLTR, training method that augments RLVR with transfer reward to encourage reasoning transferable, whether partial reasoning that can be reliably reused by another model to reach the correct answer. Intuitively, generator has higher transferability when its truncated reasoning prefixes contain stable and informative intermediate structure, enabling receiver to reliably complete the solution, rather than relying on the generators own continuation. Transfer Reward. We define transfer reward which identify the reasoning transferability. Concretely, given an input x, the generator model Mgen produces full reasoning sequence ygen Mgen(x). Then, we truncate the prefix of reasoning sequence and passed to receiver model Mrcv for continuation, yrcv Mrcv(x, ygen[: ℓ]), where truncation ratio τ (0, 1] and truncation length ℓ = τ ygen. We define transfer reward as: Rtrans(ygen, Mrcv) = 1[answer(yrcv) = ygt] (3) which indicates whether the receiver can reach the correct answer from the truncated reasoning. In practice, we compute Rtrans using single continuation, and sample τ uniformly between 0.3 and 0.9 to ensure the model learns reasoning pattern robust arbitrary interruptions, rather than overfitting to fixed truncation boundaries."
        },
        {
            "title": "The overall reward is defined as",
            "content": "RRLTR(ygen, Mrcv) := RRLVR(ygen) + Rtrans(ygen, Mrcv) (4) 3 Beyond Correctness: Learning Robust Reasoning via Transfer Training Objective. The generator is optimized via reinforcement learning with the objective E(x,ygt)D (cid:2)EygenMgen(x) [RRLTR(ygen, Mrcv)](cid:3) . max Mgen (5) 4. Experiments We provide an empirical evaluation of Transferability and RLTR by investigating the following questions: For training RLTR, we adopt reward weight = 0.1 and = 1.0 from Eq. 1 and Eq. 4. Additional implementation details are provided in Appendix A.3. Baseline. We compare RLTR the base model (the initial checkpoint before RL) and RLVR, which applies conventional reinforcement learning with verifiable reward by using the same generator initialization, training data, and decoding/evaluation protocol with RLTR. We mostly followed the baseline training setup from Zeng et al. (2025). RQ1: Can RLTR improve accuracy and consistency? 4.1. Performance of RLTR (Table 1, Table 7, and Table 5) RQ2: How does RLTR affect training dynamics and efficiency? (Figure 2 and Table 4) RQ3: Is transferability correlated with consistency of reasoning process? (Figure 2) RQ4: How does the proposed components contribute to performance. (Table 2 and Table 3) Evaluation setup. We mainly report majority voting at (Maj@K) and average accuracy (Acc.) as samplingbased metric. Maj@K selects the final answer by taking the most frequent answer among the samples, and counts it as correct if this majority answer matches the ground truth. Specifically, Maj@K captures reasoning consistency and robustness by evaluating whether the correct answer dominates across stochastic generations. Average accuracy is expectation of the single-samples accuracy, which acquired by simply averaging the accuracy of sampled solutions. Additionally, in analysis we measure transferability as the average accuracy under cross-model reasoning transfer continuation. For each model and benchmark, we repeat sampling-based evaluation three times with different random seeds and report the average. We evaluate our method on MATH-500 (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC23 (MAA, 2023), and AIME2024 (MAA, 2024), which are widely used mathematical reasoning benchmarks, and additionally include the challenging scientific reasoning benchmark GPQA (Rein et al., 2024). During the evaluation, we generate with consistent sampling hyperparameter, temperature = 1.0. We report the specified evaluation setup details in Appendix A.4. Training setup. For the main experiment, we trained RLTR on Qwen2.5-7B-Instruct (Yang et al., 2025) checkpoint, which is provided in HuggingFace. Unless stated otherwise, we use Qwen2.5-3B-Instruct (Yang et al., 2025) as the receiver model Mrcv. For analysis, we additionally consider alternative receivers (e.g., Qwen2.5-1.5B-Instruct (Yang et al., 2025) and Llama3.2-3B-Instruct (Grattafiori et al., 2024)). We perform RL training on 3K subset of the MATH (Hendrycks et al., 2021) training set, restricted to problems with difficulty 3 following Zeng et al. (2025). We present the main result by comparing the performance on math problem-solving. Here, we mainly evaluate RLTR against RLVR and the base model, reporting average accuracy (Acc.) and Majority at (Maj@K) to assess performance and consistency. The main results were evaluated in (i) two moderate-difficulty tasks: MATH-500 and GSM8K and (ii) two challenging tasks: AIME2024 and AMC23. RLTR performance gain on moderate-difficulty tasks. We first evaluate RLTR on two moderate-difficulty tasks that one is in-distribution benchmark (i.e., MATH500 (Hendrycks et al., 2021)) and the other is out-ofdistribution benchmark (i.e., GSM8K (Cobbe et al., 2021)). As shown in Table 1a, overall, RLTR consistently outperforms RLVR across all benchmarks and metrics (e.g., at average accuracy and Maj@K). It is worth noting that while RLVR achieves strong single-sample average accuracy performance (71.076.2 on MATH-500), its multi-sample consistency become less pronounced than those of the base model in Maj@K at larger K, which is consistent with findings in prior work (Yue et al., 2025). For instance, RLVR underperforms the base model at higher K: Maj@16 decreases from 81.2 (base) to 80.2 (RLVR). This suggests that RLVR can produce reasoning trajectories that are brittle. As more samples are drawn, variations in the reasoning paths leads to conflicting outcomes rather than converging toward consistent correct answer, which in turn manifests as reduced robustness under majority-vote evaluation. In contrast, RLTR yields substantially better high-K multi-sample aggregation behavior, mitigating RLVRs degradation in Maj@K at large and consistently outperforming both RLVR and the base model. For example, on MATH-500, RLTR improves Maj@16 from 81.2 to 83.8 and Maj@64 from 82.6 to 84.2, and also improves average accuracy from 71.0 to 77.0. The stronger gains in Maj@K suggesting that the added transfer reward provides direct learning signal for robust intermediate reasoning rather than only final-answer correctness. We observe the same, and even clearer, trend on the outof-distribution math benchmark, GSM8K. As shown in Table 1a, while RLVR slightly improves single-sample average accuracy, its majority-voting performance degrades at larger 4 Beyond Correctness: Learning Robust Reasoning via Transfer Table 1. Performance of RLTR. Average accuracy (%) and Maj@K (%) of RLTR (Ours) compared with baselines, including the base model and RLVR-trained model. We evaluate on two medium-difficulty math reasoning benchmarks, MATH-500 (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), as well as two more challenging benchmarks, AIME2024 (MAA, 2024) and AMC23 (MAA, 2023). We report average accuracy (Acc.) over 64 samples and majority@K (Maj@K), where Maj@K selects the final answer by majority voting among samples. Bold values denote the best performance within each group. RLTR consistently outperforms other baselines. Notably, the performance gains of RLTR are substantially larger on the harder and more challenging benchmarks. (a) Moderate-difficulty tasks MATH-500 (In-distribution) GSM8K (Out-of-distribution) Metrics Acc. Maj@4 Maj@16 Maj@64 Acc. Maj@4 Maj@16 Maj@64 Base model RLVR RLTR (Ours) 71.0 76.2 77. 78.0 78.2 79.0 81.2 80.2 83.8 82.6 82.2 84.2 89.1 89.4 92.0 91.5 90.9 93.4 92.7 92.5 94. 93.3 92.9 94.2 (b) Hard and challenging tasks AIME2024 (Out-of-distribution) AMC23 (Out-of-distribution) Metrics Acc. Maj@4 Maj@16 Maj@64 Acc. Maj@4 Maj@16 Maj@ Base model RLVR RLTR (Ours) 9.8 11.6 14.8 10.0 14.4 18.9 13.3 17.8 20.0 16.7 18.9 21.1 46.2 52.8 53. 51.7 55.8 59.2 59.2 62.5 66.7 60.8 61.7 67.5 compared to the base model. In contrast, RLTR improves both accuracy and consistency across all metrics (e.g., Acc.: 89.192.0), and consistently outperforms RLVR for all K. This consistent improvements on an out-of-distribution benchmark suggests that the transfer reward provides general process-level supervision signal, improving robustness beyond the training distribution rather than overfitting to training dataset specific patterns. RLTR performance gain on harder tasks. We further evaluate RLTR on two challenging math reasoning benchmarks that are out-of-distribution: AIME2024 (MAA, 2024) and AMC23 (MAA, 2023). AIME2024 and AMC23 are competition-level math benchmarks that are substantially harder than MATH-500 (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021). As shown in Tables 1b, RLTR consistently outperforms both the base model and RLVR across sampling-based metrics, indicating that the gains are not limited to the moderate-difficulty math domain benchmarks. Notably, the improvements are more pronounced on the harder math benchmarks. On AIME2024, RLTR boosts average accuracy from 9.8 to 14.8 and improves Maj@64 from 16.7 to 21.1, whereas RLVR provides little to low gain at high (half of gain at Maj@64). similar trend holds on AMC23, while RLVR yields modest gains, RLTR achieves larger improvement in majority voting, suggesting more reliable aggregation under challenging settings. Overall, these results suggest that optimizing the transfer reward encourages more robust and reusable reasoning trajectories that generalize better under distribution shift, with particularly strong benefits on harder reasoning tasks. We believe that this effect is amplified on competition-level problems, where correctness depends more critically on maintaining coherent reasoning process. In this settings, the transfer-based signal provides useful process-level supervision beyond final-answer correctness, leading to large gains on harder reasoning tasks. 4.2. Training Dynamics of RLTR To better understand the effect of the proposed transfer reward, we analyze the training dynamics of RLTR using the following three metrics: (i) average accuracy, (ii) Maj@64 accuracy, and (iii) reasoning transferability. Here, we evaluate intermediate checkpoints on MATH-500 and compare RLTR with RLVR. RLTR improves sample efficiency. As shown in Figures 2a, RLTR improves substantially faster than RLVR in average accuracy, and also attains higher final accuracy, consistent with the overall results in Table 1a. For example, RLTR require about 2.5 fewer training steps to reach comparable accuracy with RLVR. This result suggest that the transfer reward provides richer process-level signal that incentivizes robust reasoning trajectories, helping the model converge faster than RLVR. Moreover, as shown in Figure 2b, the Maj@64 curve also follows this improvement, indicating that the gains are not driven by sporadic correct samples but by progressively more reproducible and consistent reasoning mode. We discussed additional analysis about compute-performance trade-off in Section 4.4 and Table 4. Relationship between transferability and Maj@K. We next study how reasoning transferability evolves over training by truncating generators reasoning trace into prefix and continually generating separate, frozen receiver model to complete it. Specifically, we retain the first fraction of tokens with fixed truncation ratio = 0.7, and measure 5 Beyond Correctness: Learning Robust Reasoning via Transfer (a) Average accuracy (b) Maj@64 accuracy (c) Transferability Figure 2. Training dynamics and transferability. (a) Average accuracy: Our method RLTR, significantly reduce the training steps to achieve comparable performance with RLVR-trained models. Notably, RLTR matches RLVR with 2.5 fewer training steps. (b) Maj@64: Compared to RLVR, RLTR progressively improves consistency over training, whereas RLVRs performance degrades as training proceeds. (c) Transferability: Transferability follows Maj@64, supporting our view that optimizing the transfer reward through RLTR improves reasoning consistency and robustness. Across all three views, RLTR consistently outperforms RLVR, indicating shift toward reliably correct dominant solution modes and more stable optimization progress under the proposed transfer-based reward. transferability as the receivers verifiable final-answer accuracy under this cross-model continuation protocol. Across training steps, RLTR exhibits consistently stronger transferability growth than RLVR, with the gap generally widening as training progresses. Notably, RLVR shows clear drop in transferability at later training stages even when average accuracy remains stable, and this drop coincides with degradation in Maj@64. This coupling suggests that RLVR can become less consistent over training: it may preserve average correctness while increasingly relying on fragile or idiosyncratic traces that do not transfer well and thus fail to support self-consistency. In contrast, RLTR improves transferability together with Maj@64, implying that the transfer-based reward promotes reasoning traces that are both reusable under cross-model continuation and more reliably reproducible. Overall, these dynamics provide direct evidence that the proposed transfer reward is effectively optimized during training, shaping the generator toward robust, transferable reasoning prefixes that remain useful beyond the original models continuation. Furthermore, additional results for performance in various truncation ratios, including τ {0.3, 0.5, 0.7, 0.9}, are illustrated in Appendix B.2. 4.3. Component ablations In this section, we analyze the contribution of key design choices in RLTR. Specifically, we ablate two factors: (i) the reward composition (answer reward vs. transfer reward) and (ii) the receiver model. Effect of the reward ratio. To study the contribution of the transfer reward relative to the answer reward, we vary the weights (a, t) in Eq. 1 Eq. 4, where scales the final-answer correctness reward and scales the transferability reward. We evaluate the base model, RLVR (a=1.0, t=0.0), and several RLTR variants with Table 2. Effect of reward ratio. Average accuracy (%) and Maj@K(%) on MATH-500 (Hendrycks et al., 2021) for RLTR (Ours) by varying weight of transfer reward. Acc. denotes average accuracy. The bold indicates the best result within the group. MATH-500 1.0 1.0 1.0 0. 0.0 0.1 1.0 1.0 Acc. Maj@4 Maj@16 Maj@64 76.8 75.8 77.0 77.0 78.2 77.4 81.0 79.0 80.2 80.6 83.2 83. 82.2 81.2 82.0 84.2 different answertransfer reward ratios: (a=1.0, t=1.0), (a=0.1, t=1.0), and (a=1.0, t=0.1). As shown in Table 2, increasing the transfer reward weight consistently improves performance in high-K regimes. Models with larger achieve substantially higher Maj@K, while reducing the answer reward weight has comparatively smaller effect. In contrast, when the transfer reward is downweighted (t=0.1), the results closely resemble RLVR and exhibit limited scaling with K. Assigning larger transfer weight yields much stronger improvements: (a=0.1, t=1.0) increases Maj@64 from 82.2 to 84.2. Overall, these results suggest that emphasizing the transfer reward is important for improving reliability and scaling under large-K aggregation, beyond optimizing final-answer correctness alone. This is consistent with our objective: encouraging transferable (i.e., reusable) reasoning prefixes makes independently sampled solutions more likely to converge to the same correct outcome, thereby improving consensus-driven metrics such as Maj@K in high-K regimes. We further discuss deeper analysis of multi-sampling performance in Appendix B.3. Effect of receiver model. The receiver model is used to continue truncated reasoning traces and thereby determines the transfer reward signal; thus, it can substantially 6 Beyond Correctness: Learning Robust Reasoning via Transfer Table 3. Receiver model ablation. Average accuracy (%) and Maj@K(%) on MATH-500 (Hendrycks et al., 2021) for RLTR (Ours) by varying receiver model, including Base model, RLVRtrained model. Acc. denotes average accuracy. The bold indicates the best result within the group. Table 5. RLTR generalize beyond mathematical domain. Accuracy (%) and Maj@K (%) on GPQA (Rein et al., 2024) for the base model and RL baselines. Acc. denotes average accuracy. The bold indicates the best result. MATH-500 Receiver Acc. Maj@4 Maj@16 Maj@64 Llama-3B Qwen-1.5B Qwen-3B 74.0 77.6 77.0 78.4 79.6 79.0 81.2 82.4 83.8 82.0 82.8 84.2 GPQA Acc. Maj@4 Maj@8 Maj@ Base RLVR RLTR (Ours) 32.4 33.0 34.8 35.3 35.2 37.3 35.0 36.8 37.5 35.2 37.0 37.7 Table 4. Computational cost. Comparison between RLVR and RLTR with EFLOPs (i.e., 1018 FLOPs) and Average accuracy (Acc.) on MATH-500 evaluation. MATH-500 Train Steps EFLOPs () Acc. () RLVR RLTR (Ours) RLVR RLTR (Ours) 40 40 100 60 37.10 39.76 92.75 59. 74.5 76.2 76.2 76.8 affect training. To isolate this factor, we vary the receiver while keeping the generator initialization and training data fixed. Specifically, we compare three receivers (i.e., Llama3.2-3B-Instruct (Llama-3B) (Grattafiori et al., 2024), Qwen2.5-1.5B-Instruct (Qwen-1.5B) (Yang et al., 2025), and Qwen2.5-3B-Instruct (Qwen-3B) (Yang et al., 2025))and train RLTR with the same generator and dataset. As shown in Table 3, the receiver choice leads to noticeable differences, especially for majority voting at larger K. For Maj@16 and Maj@64, Qwen-3B yields the best results (83.8 and 84.2), whereas Qwen-1.5B achieves the best Maj@4 (79.6). This pattern suggests that higher-capacity receiver can provide more informative transfer signal that improves high-k consistency under majority voting. Overall, using stronger receiver tends to improve robustness under majority voting. Notably, RLTR still outperforms RLVR even with the smallest receiver (Qwen-1.5B). These results indicate that the benefits of the transfer reward are not dependent on large receiver, although stronger receivers further improve high-K consistency. We further discuss deeper analysis of multi-sampling performance in Appendix B.4. 4.4. Additional Analysis Computational cost analysis. One possible concern of RLTR is the extra computation introduced by the receiver (i.e., an additional model) during training. To this end, we demonstrate that, even with the extra computation, RLTR still (i) achieves the same performance with less computation, and (ii) outperforms RLVR as the computational budget increases. Here, we use FLOPs as the measure of computational cost, with the detailed computation procedure provided in Appendix B.1. As shown in Table 4, RLTR increases the training FLOPs by approximately 7% under the same number of training steps, while achieving substantially better performance. More importantly, we observed that RLVR requires 2.3 higher computational cost to reach the same performance level as RLTR (39.7692.75 EFLOPs), highlighting both the sample efficiency and computational efficiency of RLTR. Lastly, it is worth noting that the accuracy should be regarded as the primary metric: RLTR not only outperforms the best accuracy achieved by RLVR (at 92.75 EFLOPs), but does so with fewer training steps and lower computational cost. Performance on an out-of-domain reasoning task. We further analyze to verify that RLTR also enhances performance in out-of-domain, GPQA (Rein et al., 2024), whereas GPQA is science question-answering benchmark that primarily tests logical and scientific reasoning beyond pure mathematics. As shown in Table 5, RLTR also yields consistent improvements in average accuracy and Maj@K. For example, average accuracy increased from 32.4 to 34.8 and Maj@16 from 35.2 to 37.7. This suggests that the transfer reward is largely domain-agnostic, generalizing beyond mathematics to scientific reasoning. Accuracy under reasoning truncation. We evaluate reasoning robustness by measuring the transfer accuracy (i.e., transferability from the generator to Qwen2.5-3B-Instruct) of traces truncated prefix at various ratios on MATH-500. As shown in Figure 3, RLTR consistently outperforms baselines (e.g., Base model and RLVR) across the entire spectrum τ from 0.1 to 0.9. This demonstrates that RLTR learns reasoning remains solvable under arbitrary interruptions, rather than exploiting specific truncation boundaries. Furthermore, it is worth noting that RLTR maintains larger advantage even at small truncation ratios (e.g., τ [0.1, 0.3]), suggesting that it produces more informative prefixes from the very early stages of reasoning that provide sufficient intermediate structure for the receiver to complete the solution. Results on different LLM. We also demonstrate the effectiveness of RLTR on another LLM family. To this end, we train RLTR from Llama3.1-8B-Instruct, where we use the 7 Beyond Correctness: Learning Robust Reasoning via Transfer Table 6. RLTR retains diversity better in notion of Pass@k. Average accuracy (%) and Pass@K (%) for RLTR (Ours) and other baselines, including Base model, and RLVR trained model. We consider two math reasoning benchmarks, MATH-500 (Hendrycks et al., 2021), and AMC23 (MAA, 2023). Acc. denotes average accuracy. The bold indicates the best result within the group. MATH-500 AMC23 Metrics Acc. Pass@4 Pass@ Pass@64 Acc. Pass@4 Pass@16 Pass@64 Base Model RLVR RLTR (Ours) 71.0 76.2 77. 86.2 85.2 88.2 92.4 88.4 92.2 95.2 92.2 95.0 46.2 52.8 53.5 70.0 72.5 75.0 82.5 77.5 85. 95.0 92.5 95.0 Table 7. RLTR is adaptable on other LLM architectures. Average accuracy (%) and Maj@K(%) on MATH-500 (Hendrycks et al., 2021) for RLTR (Ours) Llama-3.1-8B-Instruct. We compared the RLTR with several baselines including Base model, RLVR trained model. The bold indicates the best result within the group. Method Acc. Maj@4 Maj@16 Maj@64 Base Model RLVR RLTR (Ours) 24.8 50.0 45. 34.4 55.2 52.6 51.2 58.6 59.2 60.8 59.8 63.4 receiver model as Llama3.2-3B-Instruct on the MATH-train dataset. We report the average accuracy and majority voting accuracy (Maj@K) on the MATH-500 dataset. As shown in Table 7, RLTR outperforms the baselines at high-K Maj@K even when trained with different LLM family. For example, RLTR achieves Maj@16 of 59.2 and Maj@64 of 63.4, substantially higher than both the base model and RLVR-trained model. In contrast, RLVR primarily improves single-sample accuracy; however, these gains do not translate into sampling consistency, as performance improvements saturate under high-K majority voting and even underperform the base model at Maj@64. These results are consistent with the main findings in Section 4.1, highlighting the importance of robust reasoning, which is also demonstrated across different LLM families. Pass@K Evaluation. We have discussed the consistency and robustness of the reasoning process in prior sections. Now, we analyze the diversity aspect of the training. We quantify the meaningful diversity of the reasoning using the Pass@K metric. Pass@K measures whether at least one of the sampled solutions contains the correct final answer. As shown in Table 6, RLVR improves single-sample accuracy on MATH-500, but its high-K diversity degrades compared to the Base model (e.g., Pass@64: 95.292.2). In contrast, RLTR preserves strong high-K Pass@K while maintaining competitive single-sample average accuracy (Acc.) on MATH-500, RLTR achieves comparable Pass@K performance, while outperforming other baselines in singlesample average accuracy. similar pattern holds on AMC23, where RLVR again reduces high-K Pass@K, whereas RLTR consistently improves or matches the Base model across all (e.g., Pass@64: 95.0). These results Figure 3. RLTR improves transferability consistently. Transferability (%) comparison among base model, RLVR, and RLTR. The truncation ratio is the fraction of tokens remained from the begging of reasoning trace (higher means longer prefixes continuation). RLTR consistently achieves higher transferability than baselines. suggest that conventional RLVR tends to concentrate probability mass on narrower set of solution patterns, improving average single-sample accuracy but harming diversity, consistent with observations in Yue et al. (2025). By contrast, incorporating transfer-based signals enables RLTR to improve robustness while preserving meaningful diversity in sampled solutions. 5. Conclusion We introduced Reinforcement Learning with Transferable Reward (RLTR), transfer-augmented RLVR framework that optimizes reasoning transferability, the ability of truncated reasoning prefix to be completed by another model to verifiably correct answer. By leveraging transfer reward, RLTR encourages reasoning traces that remain reusable under truncation and cross-model continuation. Across multiple reasoning benchmarks, RLTR improves average accuracy and sampling-based consistency (e.g., majority voting) indicating enhanced robustness of learned reasoning processes. Moreover, RLTR significantly improves sample efficiency which reduce training steps to achieve comparable performance with baseline. We further shows robust generalization across base models and out-of-distribution settings. Overall, our results suggest that optimizing transferability leads to more robust LLM reasoning. Beyond Correctness: Learning Robust Reasoning via Transfer"
        },
        {
            "title": "Impact Statement",
            "content": "This work aims to advance robust LLM reasoning by introducing transferability as training signal. We show that RLTR can improve not only accuracy but also reasoning consistency, while significantly enhancing sample efficiency and enabling faster learning. Today, training and sample efficiency is an important goal to both research and industry, because of the increasing computation cost and concerns about sustainable AI development. By improving the consistency and sample efficiency, our approach has potential to benefit to the communities. Furthermore, in the application side, domains that require consistency and heavily annotated training samples (e.g., Vision Language Action (VLA) model or Computer Use Agent (CUA) ) can easily adopt our method by augmenting the existing RLVR frameworks. Consequently, by enabling reliable reaosning in such complex applications, our method may contribute to the societal/technological innovation."
        },
        {
            "title": "References",
            "content": "Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., et al. Open problems and fundamental limitations of reinforcement learning from human feedback. arXiv preprint arXiv:2307.15217, 2023. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verarXiv preprint ifiers to solve math word problems. arXiv:2110.14168, 2021. URL https://arxiv. org/abs/2110.14168. Gao, L., Schulman, J., and Hilton, J. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp. 1083510866. PMLR, 2023. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Khalifa, M., Agarwal, R., Logeswaran, L., Kim, J., Peng, H., Lee, M., Lee, H., and Wang, L. Process reward models that think. arXiv preprint arXiv:2504.16828, 2025. Le, H., Wang, Y., Gotmare, A. D., Savarese, S., and Hoi, S. C. H. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35: 2131421328, 2022. Li, W. and Li, Y. Process reward model with q-value rankings. arXiv preprint arXiv:2410.11287, 2024. URL https://arxiv.org/abs/2410.11287. ICLR 2025 (conference version). Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and Cobbe, K. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. URL https:// arxiv.org/abs/2305.20050. Liu, Z., Chen, C., Li, W., Qi, P., Pang, T., Du, C., Lee, W. S., and Lin, M. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. MAA. American mathematics competitions - amc 2023. American Mathematics Competitions - AMC 2023., 2023. MAA. American invitational mathematics examination - aime 2024. American Invitational Mathematics Examination - AIME 2024, 2024. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al. Mathematical discoveries from program search with large language models. Nature, 625 (7995):468475, 2024. Saito, K., Wachi, A., Wataoka, K., and Akimoto, Y. Verbosity bias in preference labeling by large language models. arXiv preprint arXiv:2310.10076, 2023. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 9 Beyond Correctness: Learning Robust Reasoning via Transfer reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. She, S., Liu, J., Liu, Y., Chen, J., Huang, X., and R-prm: Reasoning-driven process reHuang, S. In Proceedings of the 2025 Conward modeling. ference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2025. URL https://aclanthology.org/2025. emnlp-main.679/. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 1279 1297, 2025. Skalse, J., Howe, N., Krasheninnikov, D., and Krueger, D. Defining and characterizing reward gaming. Advances in Neural Information Processing Systems, 35:94609471, 2022. Uesato, J., Kushman, N., Kumar, R., Song, H. F., Siegel, N., Wang, L., Creswell, A., Irving, G., and Higgins, I. Solving math word problems with processand outcomebased feedback. arXiv preprint arXiv:2211.14275, 2022. URL https://arxiv.org/abs/2211.14275. calflops: flops and params calcuxiaoju ye. for neural networks in pytorch framelate tool work, 2023. URL https://github.com/MrYxJ/ calculate-flops.pytorch. Yang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Li, C., Liu, D., Huang, F., Wei, H., Lin, H., Yang, J., Tu, J., Zhang, J., Yang, J., Yang, J., Zhou, J., Lin, J., Dang, K., Lu, K., Bao, K., Yang, K., Yu, L., Li, M., Xue, M., Zhang, P., Zhu, Q., Men, R., Lin, R., Li, T., Tang, T., Xia, T., Ren, X., Ren, X., Fan, Y., Su, Y., Zhang, Y., Wan, Y., Liu, Y., Cui, Z., Zhang, Z., and Qiu, Z. Qwen2.5 technical report, 2025. URL https://arxiv.org/ abs/2412.15115. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Yue, Y., Chen, Z., Lu, R., Zhao, A., Wang, Z., Song, S., and Huang, G. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. Zeng, W., Huang, Y., Liu, Q., Liu, W., He, K., Ma, Z., and He, J. Simplerl-zoo: Investigating and taming zero 10 Beyond Correctness: Learning Robust Reasoning via Transfer A. Experimental Details In this section, we describe the experimental details of Section 4, including RLTR and the baselines. A.1. Dataset Details Here, we describe the dataset we used in training and evaluation. GSM8K. The GSM8K dataset (Cobbe et al., 2021) consists of 7,473 train samples and 1,319 test samples of moderatelevel of math problems. This is diverse grade school math word problems. We used test set in our experiments which consists of problem and answer pairs. MATH. The MATH dataset (Hendrycks et al., 2021) consists of 7,500 train samples and 5,000 test samples. This dataset consists of diverse range of difficulty level. We utilized 3K subset from MATH train-set which has high difficulty (=3). Also, for the evaluation we used MATH-500 which is subset of test-set (Lightman et al., 2023). AMC23. The AMC23 dataset (MAA, 2023) consists of 40 competition-level hard problems, provided as linguistic problem and answer pairs. We used this dataset for evaluation in our experiments. AIME2024. The AIME2024 dataset (MAA, 2024) consists of 30 competition-level hard problems, provided as linguistic problem and answer pairs. We used this dataset for evaluation in our experiments. This is the most difficult benchmark among our evaluation sets. GPQA. The GPQA dataset (Rein et al., 2024) consists of 448 multiple-choice question and answer pairs. GPQA is science question-answering benchmark that primarily tests logical and scientific reasoning beyond pure mathematics. We used the GPQA dataset for out-of-distribution evaluation in our experiments. A.2. Model Details Here, we describe the model and their source we used in training and evaluation. We utlized checkpoints provided in huggingface. Qwen2.5-7B-Instruct. (Yang et al., 2025;https://huggingface.co/Qwen/Qwen2.5-7B-Instruct) Qwen2.5-3B-Instruct. (Yang et al., 2025;https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) Qwen2.5-1.5B-Instruct. (Yang et al., 2025; https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) Llama3.1-8B-Instruct.(Grattafiori et al., 2024; https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct) Llama3.2-3B-Instruct. (Grattafiori et al., 2024; https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct) A.3. Training Details Training framework. For the training framework, we utilize VERL (Sheng et al., 2025) for both RLTR and RLVR. VERL (https://github.com/volcengine/verl) Training hyperparameters. We describe our training hyperparameters in Table 8, which we used for training RLTR and RLVR. Compute Resource. For the main development, we mainly use eight H200 141 GB GPUs. A.4. Evaluation Details Evaluation Framework. For the evaluation framework, we utilize Math Verify and SimpleRL (Zeng et al., 2025). SimpleRL. (https://github.com/hkust-nlp/simpleRL-reason) Math Verify. (https://github.com/huggingface/Math-Verify) 11 Beyond Correctness: Learning Robust Reasoning via Transfer Table 8. Hyperparameters for RLTR and RLVR training. ( denotes the hyperparameter only used for RLTR.)"
        },
        {
            "title": "Hyperparmeter",
            "content": "Optimizer Algorithm Training steps Learning rate Batch size Generation per sample kl coef Generator temperature Receiver temperature Max new generation length"
        },
        {
            "title": "Value",
            "content": "Adam GRPO 150 1e-6 1,024 8 0.001 1.0 1.0 8,192 B. Additional Results B.1. Rigorous Analysis of Computation Efficiency of RLTR In this section, we provide analysis of the computational cost of RLTR compared to the standard RLVR baseline. Ultimately, we argue that while RLTR has overhead in per step computational cost, this is outweighted by its training step efficiency, leading to significant advantage in terms of total FLOPs to reach comparable performance. To rigorously assess the training cost, we estimate the Floating Point Operations (FLOPs) per token following standard scaling laws (Shao et al., 2024). Specifically, we followed the xiaoju ye (2023) to calculate the FLOPs precisely. Table 9. Model architectures for Qwen2.5-7B-Instruct (Yang et al., 2025) and Qwen2.5-3B-Instruct (Yang et al., 2025)."
        },
        {
            "title": "Hyperparameter",
            "content": "Qwen2.5-7B-Instruct Qwen2.5-3B-Instruct Parameters Hidden Size (dmodel) Layers (L) Attention Heads Vocab Size (V ) 7.6B 3,584 28 28 152,064 3.1B 2,048 36 16 151,936 Table 10. FLOPs breakdown per training step. RLTR adds receiver rollout phase compared to RLVR."
        },
        {
            "title": "Component",
            "content": "RLVR RLTR (Ours) (PFLOPs/step) (PFLOPs/step) Policy Rollout (Generation) Policy Forward (Logprobs) Reference Forward (KL) Policy Training (Fwd+Bwd) Receiver Rollout (Transfer)"
        },
        {
            "title": "Total FLOPs per Step",
            "content": "141.65 157.17 157.17 471.50 927.48 141.65 157.17 157.17 471.50 66.49 993.96 First, we describe the model architectures of Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct, which were mainly used in our experiments, in Table 9. For these calculations we set the hyperparameters as described in Table 8. Since we adopts GRPO, there are four primary computational phases, policy rollout, policy forward (Logprobs), reference forward (KL), and policy training (forward and backward passes). Specifically, we calculated the FLOPs for forward, backward, and autoregressive generation operations using the formulas provided by xiaoju ye (2023). Also, in the case RLTR, an additional computation needs for the transfer rollout using the smaller receiver model (i.e., Qwen2.5-3B-Instruct). Specifically, as shown in Table 10, the total FLOPs per step is 927.48 PFLOPs for RLVR and 993.96 PFLOPs for RLTR, which results in approximately 7.2% overhead. However, this per-step overhead is negligible compared 12 Beyond Correctness: Learning Robust Reasoning via Transfer (a) Truncation Ratio (τ ): 0.3 (b) Truncation Ratio (τ ): 0.5 (c) Truncation Ratio (τ ): 0.7 (d) Truncation Ratio (τ ): 0. Figure 4. Detailed Analysis of transferability on various truncation ratios τ {0.3, 0.5, 0.7, 0.9}. Table 11. Effect of Reward Ratio. Accuracy (%) for RLTR (Ours) by varying weight of transfer reward, including Base model, Baset-RL trained model. We consider math reasoning benchmarks, MATH-500 (Hendrycks et al., 2021). The bold indicates the best result within the group. 1.0 1.0 1.0 0.1 0.0 0.1 1.0 1. Acc. 76.8 75.8 77.0 77.0 Pass@K 4 85.2 84.4 85.2 88.2 89.8 89.0 89.4 92.2 64 92.2 92.4 92.6 95.0 to the gains in sample efficiency. As demonstrated in Figure 2a of the main text, RLTR converges to the same accuracy level approximately 60% reduced than RLVR. Consequently, the total computational cost to train RLTR is reduced by roughly 57% compared to the baseline, making it highly efficient framework for robust reasoning training. B.2. Additional Analysis for Transferability We extended our analysis of training dynamics of transferability, which is described in Figure 2c. We reported the transferability dynamics and set the truncation ratio τ = 0.7 in the main part. Here, we vary the truncation ratio from τ {0.3, 0.5, 0.7, 0.9} while keeping the same settings. As shown in Figure 4, the transferability gap between RLVR and RLTR is consistently getting larger through all truncation ratios. This suggests that RLTR learns robust reasoning process which is unspecified to certain truncation ratio boundary. B.3. Additional Analysis on Reward Ratio We extended our analysis on the reward ratio model, which is discussed in Table 2. Specifically, we analyze diversity in the notion of Pass@K. As shown in Table 11, assigning larger transfer weight yields much stronger improvements: 13 Beyond Correctness: Learning Robust Reasoning via Transfer Table 12. Receiver Model Ablation. Accuracy (%) for RLTR (Ours) by varying receiver model, including Base model, RLVR trained model. We consider math reasoning benchmark, MATH-500 (Hendrycks et al., 2021). The bold indicates the best result within the group. Receiver Llama-3B Qwen-1.5B Qwen-3B Acc. 74.0 77.6 77.0 Pass@K 4 83.8 87.6 88. 16 91.8 91.0 92.2 64 95.2 95.0 95.0 (a=0.1, t=1.0) increases Pass@64 from 92.2 to 95.0. Overall, larger transfer weight improves diversity, indicating that our transfer reward approach can also help retain diversity degrade problem of RLVR, which was suggested in Yue et al. (2025). B.4. Additional Analysis on Reveiver Model We extended our analysis on the receiver model, which is discussed in Table 3. Specifically, we analyze diversity in the notion of Pass@K. As shown in Table 12, all receivers achieve similar performance at large (Pass@64: 95.2 vs. 95.0 vs. 95.0), indicating comparable diversity in producing at least one correct sample. However, average accuracy varies more across receivers (74.0/77.6/77.0), suggesting that receivers can influence single-sample correctness while preserving diversity. Overall, the diversity captured by Pass@K is largely preserved. C. Prompt Examples In this section, we describe the prompt template used for training and evaluation in our experiments. As shown in Table 13, we instruct the model to reason before generating the final answer. Moreover, as shown in Table 14, for GPQA, which provides four choices per problem, we included these options in the prompt in random order. For training and evaluation, we place these prompt in user turn of the model-specific chat templates. Table 13. The user templates for MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), AMC23 (MAA, 2023), and (MAA, 2024). We used it for training and evaluation. User template for training and evaluation. {Problem} You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The final answer MUST BE put in boxed{}. Table 14. The user template for GPQA (Rein et al., 2024). User template for evaluating the GPQA. {Problem} You FIRST think about the reasoning process as an internal monologue and then provide the final answer. This is multiple-choice question with options A, B, C, D. Your final answer MUST be ONLY the letter of the correct option (A, B, C, or D). Put your final answer in boxed{}, for example: boxed{A} or boxed{B}. Options: A. {Answer1} B. {Answer2} C. {Answer3} D. {Answer4} D. Future Work Transfer as verifier. An interesting next step is to leverage reasoning transferability beyond training as test-time scaling mechanism. Since our results suggest that optimizing for transferability yields more consistent and reusable reasoning traces, 14 Beyond Correctness: Learning Robust Reasoning via Transfer natural extension is to incorporate transferability into the verification stage itself. This would enable verification module that explicitly prefers solutions whose partial reasoning is robust under cross-model continuation. E. Generation Examples We compare generation exmples from RLVR-trained model and RLTR-trained model. As shown in Tables 15 and 16, both models solve same problem and the final answer is correct. However, the underlying reasoning differs substantially. The RLVR-trained model contains clear inconsistency in an intermediate reasoning trajectory (highlight in red). In contrast, RLTR produces coherent and logically consistent trajectory. This example supports our hypothesis that optimizing only final-answer correctness in RLVR can fail to penalize brittle reasoning traces, while RLTR successfully incentivize to capture the flawless in the reasoning process Table 15. Generation Example of RLTR-trained model on MATH-500 (Hendrycks et al., 2021). Define Find way to write = (cid:88) k=1 1 and = (cid:88) k=1 1 k3 . (cid:88) (cid:88) j=1 k=1 1 (j + k)3 in terms of and q. To find the value of the double sum (cid:80) j=1 We can rewrite the double sum as follows: (cid:80) k=1 1 (j+k)3 in terms of and q, we start by considering the inner sum. (cid:88) (cid:88) j= k=1 1 (j + k)3 . We can change the order of summation by considering the sum over all possible values of = + k. For fixed n, can range from 1 to 1 and = j. Therefore, the double sum can be rewritten as: (cid:88) n1 (cid:88) n= j=1 1 n3 . The inner sum is simply (n 1) 1 n3 , so we have: (cid:88) n= 1 n3 = (cid:88) n=2 (cid:18) n3 (cid:19) = 1 n3 (cid:88) n=2 (cid:18) 1 n2 (cid:19) . 1 n3 We can split this into two separate sums: (cid:88) n= 1 n2 (cid:88) n=2 1 n3 . We can express these sums in terms of and q. Note that: and (cid:88) n=2 1 n2 = (cid:88) n=1 1 n2 1 12 = 1, (cid:88) n=2 1 n3 = (cid:88) n= 1 n3 1 13 = 1. Therefore, the original sum becomes: ... Thus, the value of the double sum is: . 15 Beyond Correctness: Learning Robust Reasoning via Transfer Table 16. Generation Example of RLVR-trained model on MATH-500 (Hendrycks et al., 2021). Highlighted errorneous part in red. We need to evaluate the double sum (cid:80) j=1 (cid:80) k=1 (j+k)3 in terms of and q, where and 1. **Rewriting the Double Sum:** = (cid:88) k= 1 k2 = (cid:88) k=1 1 k3 . (cid:88) (cid:88) j=1 k=1 1 (j + k)3 2. **Index Transformation:** Change the order of summation by letting = + k. Thus, for fixed n, can range from 1 to 1 (since and are both positive integers and = j) and can range from 1 to 1. The sum becomes the sum over and the corresponding values of and k: (cid:88) (cid:88) j=1 k=1 1 (j + k)3 = (cid:88) n1 (cid:88) n=2 j=1 1 n3 . 3. **Inner Sum Evaluation:** The inner sum over for fixed is: n1 (cid:88) j= 1 n3 = 1 n3 n1 (cid:88) j=1 1 = 1 n3 (n 1). 4. **Simplification:** Substituting the inner sum back into the outer sum, we get: (cid:88) n1 (cid:88) n=2 j=1 1 (j + k)3 = (cid:88) n=2 1 n3 . 5. **Further Simplification of the Sum:** Now, we need to simplify (cid:80) n= n1 n3 : (cid:88) n=2 1 n3 = (cid:88) n= (cid:18) n3 (cid:19) = 1 n3 (cid:88) n=2 1 n2 (cid:88) n=2 1 n3 . 6. **Separating the Series:** We split the series into two parts: And for the second part: (cid:88) n=2 1 n2 = (cid:32) (cid:88) n= (cid:33) 1 n2 1 = 1. (cid:88) n=2 1 n3 = (cid:32) (cid:88) n=1 (cid:33) 1 n3 1 = 1. 7. **Combining the Results:** Substituting these back, we get: (cid:88) n=2 1 n3 = (p 1) (q 1) = q. Therefore, the expression for the sum (cid:80) j=1 (cid:80) k=1 1 (j+k)3 in terms of and is: ."
        }
    ],
    "affiliations": [
        "KAIST",
        "Microsoft Research",
        "Microsoft Research Asia"
    ]
}