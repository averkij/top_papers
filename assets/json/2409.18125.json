{
    "paper_title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
    "authors": [
        "Chenming Zhu",
        "Tai Wang",
        "Wenwei Zhang",
        "Jiangmiao Pang",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce a simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we employ a simple yet effective representation, 3D Patch, which connects 2D CLIP patch features with their corresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish a unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5x faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 5 2 1 8 1 . 9 0 4 2 : r LLaVA-3D: Simple yet Effective Pathway to Empowering LMMs with 3D-awareness Chenming Zhu1,2 Tai Wang2, Wenwei Zhang2 Jiangmiao Pang2 Xihui Liu1, 2Shanghai AI Laboratory 1The University of Hong Kong https://zcmax.github.io/projects/LLaVA-3D corresponding author Figure 1. Overview of LLaVA-3D. Block (a) shows that LLaVA-3D could perform both 2D and 3D vision-language tasks. The left block (b) shows that compared with previous 3D LMMs, our LLaVA-3D achieves state-of-the-art performance across wide range of 3D benchmarks while maintaining comparable performance on various 2D benchmarks compared with LLaVA-1.5. The middle block (c) demonstrates that LLaVA-3D is built on the 2D LMM: LLaVA, and leverages 3D patches to endow it with 3D spatial awareness, enabling it to perform various 3D vision-and-language tasks in the physical world. The right blocks (d) and (e) highlights the significantly faster convergence and inference speeds of LLaVA-3D compared to existing 3D LMMs."
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced their proficiency in 2D visual understanding tasks, enabling them to effectively process and understand images and videos. However, the development of LMMs with 3D-awareness for 3D scene understanding has been hindered by the lack of large-scale 3D vision-language datasets and powerful 3D encoders. In this paper, we introduce simple yet effective framework called LLaVA-3D. Leveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D efficiently adapts LLaVA for 3D scene understanding without compromising 2D understanding capabilities. To achieve this, we employ simple yet effective representation, 3D Patch, which connects 2D CLIP patch features with their corresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs and employing joint 2D and 3D vision-language instruction tuning, we establish unified architecture for both 2D image understanding and 3D scene understanding. Experimental results show that LLaVA-3D converges 3.5 faster than existing 3D LMMs when trained on 3D vision-language datasets. Moreover, LLaVA-3D not only achieves state-of-the-art performance across various 3D tasks but also maintains comparable 2D image understanding and vision-language conversation capabilities with LLaVA. 1. Introduction in Large Multimodal Models Recent advancements (LMMs) [2, 8, 30, 43] have significantly enhanced their ability to understand and reason over visual and language inputs, leading to remarkable performance in 2D visual tasks, such as 2D perception, understanding, and reasoning. Despite their advanced perceptual and reasoning capabilities, LMMs are primarily confined to virtual interactions through images or video, lacking the critical ability to interact with the physical world. To enable their deployment in real-world applications and to facilitate the emergence of new capabilities through physical interactions, it is imperative to equip LMMs with 3D spatial intelligence. key aspect of 3D spatial intelligence is the ability to perceive and understand the 3D world. Similar to how 2D LMMs align 2D visual features with language models using large-scale 2D vision-language datasets, common approach to developing 3D LMMs [12, 19, 20] involves integrating 3D features encoded from point clouds into Large Language Models (LLMs) and training them on 3D point cloud-language datasets. However, in contrast to the abundance of large-scale 2D datasets, 3D datasets remain relatively scarce. Meanwhile, there are no powerful pre-trained 3D point cloud encoders, akin to CLIP ViT [42] in 2D, to serve as foundational model that can provide strong 3D features to LLMs. Since real-world embodied agents typically rely on egocentric, multi-view images as their raw observations, we aim to build 3D foundation model based on such inputs rather than 3D point clouds. There have been attempts [17, 18] to leverage the 2D foundation models, like CLIP, alongside LLMs to advance this goal. These methods resort to 2D object segmentation results [26] to extract and aggregate CLIP features from object-centric image patches, constructing pixel-aligned 3D scene features [23]. However, this multi-view image object segmentation and object-centric feature extraction pipeline is inherently complex and computationally intensive. In contrast, 2D LMMs [2, 8, 30, 37, 38] directly leverage CLIPs image patch features with richer, fine-grained information for effective image understanding and visual reasoning. This naturally leads to the question: Can we directly build 3D LMM upon the strong 2D priors from 2D LMMs, bypassing the obstacles in 3D data scale and 3D encoders? In light of recent progress in 2D LMMs, we propose simple yet effective framework, LLaVA-3D, which extends the well-established 2D LLaVA model to efficiently comprehend the 3D world while preserving its robust 2D multimodal perception and reasoning capabilities. The core innovation in our approach is the introduction of 3D Patch, new 3D representation that bridges 2D features within 3D spatial context. This representation is derived by simply augmenting 2D patch-wise features with 3D positional embeddings, eliminating the need for additional complex processing. After enhancing the 2D visual tokens to 3D patch features, we explore various pooling strategies to compress the 3D patch features across extensive input frames before being fed into LLM. Fine-tuned on the existing 3D vision-language datasets, our model converges rapidly and acquires 3D spatial understanding and grounding capabilities. Furthermore, the unified model architecture allows LLaVA-3D to retain the strong 2D understanding and reasoning abilities of LLaVA through joint instruction-tuning on 2D vision-language datasets. Our experimental results demonstrate that LLaVA-3D achieves state-of-the-art performance on wide range of 3D tasks and benchmarks [3, 6, 13, 3941, 51], including 3D captioning, 3D question answering, and 3D grounding while requiring significantly less training time and fewer epochs than existing 3D LMMs. Additionally, LLaVA-3D achieves comparable capabilities in 2D image understanding, reasoning, and conversation to LLaVA through joint tuning on 2D and 3D vision-language instructions. 2. Related Work 2D LMMs. Building on the success of recent LLMs, numerous studies [2, 8, 30, 35, 37, 38] explored LMMs that can jointly process visual and linguistic information. For example, LLaVA [37, 38] aligned 2D images with language models through an image encoder and projection layer, while BLIP2 [30] employed more sophisticated Q-Former architecture to guide the compression of visual features using textual cues. However, most early 2D LMMs were trained on single-image datasets, limiting their ability to tackle the complexities of multi-image understanding. Recently, there has been increasing interest in expanding LMMs to handle multi-image inputs, addressing the demands of real-world scenarios. For video LMMs [28, 31, 34, 47], multi-image input forms the basis for capturing temporal or action-related dynamics across sequences of video frames. On the other hand, multi-view images of the 3D scene can implicitly reveal 3D spatial relationships and other abstract relations in the environment. Recent works [36, 41] explored whether 2D LMMs [1, 44] can leverage multi-view images to perform spatial understanding. However, these methods primarily relied on implicit learning from the data, without directly modeling the 3D world. In contrast, our LLaVA-3D explicitly models the 3D world from multi-view images, enabling advanced 3D spatial understanding and grounding capabilities. Injecting 3D into LLMs. As 2D LMMs achieved substantial progress in visual perception, similar efforts have been made in the 3D domain. For 3D scene-level understanding, recent works explored ways to integrate 3D inputs such as point clouds [12, 19, 20] or multi-view images [17, 18] into LLMs to enable advanced 3D scene understanding and reasoning. An important distinction among these methods is how they construct the 3D scene representation. LL3DA [12] directly used scene-level 3D point cloud encoder to extract the 3D scene representation. LEO [20] and Chat3D v2 [19] first segmented 3D objects from the scene point cloud using the off-the-shelf 3D instance segmentation model, and then independently extracted 3D object features with objectlevel 3D encoders to represent the 3D scene. On the other hand, starting from multi-view images, 3D-LLM [18] and Scene-LLM [17] resorted to manually crafted 2D object segmentation to extract and aggregate CLIP features from object-centric image patches, constructing pixel-aligned 3D point representation. Unlike these approaches, our LLaVA3D directly builds on the well-trained 2D LMM with multiview images as input. Utilizing the 3D position embeddings, it brings the the 2D patches within 3D spatial context to construct 3D Patches. This 3D representation enables quick adaption of LLaVA for 3D scene understanding while preserving its strong 2D image understanding ability. 3. Method Previous 2D LMMs typically consist of visual encoder to extract 2D image features, which are then aligned with the LLM via the projection layer for joint visual and language reasoning tasks. In this section, we introduce how to bridge the 2D image features within 3D spatial context to construct 3D patches (Sec. 3.1, 3.2), and then demonstrate the 3D-aware pooling strategies to compress the 3D patches (Sec. 3.2) and finally present the 3D-aware position encoding and decoding process (Sec. 3.4), as illustrated in Fig. 2. 3.1. Preliminary We choose LLaVA [38], 2D LMM, to explore building 3D LMM based on it. LLaVA uses the pre-trained CLIP encoder to extract the 2D patch features Xv Rcwh from the input image R3W , and then align the 2D patch features Xv into with LLM space with the projection layer. simple multi-view image adaptation [29] for LLaVA could be extracting multi-view 2D Patch features from multiv RV cwh and sequentially feeding view images them into LLM for multi-image understanding. To empower LLaVA with 3D awareness, we introduce 3D Patch, novel 3D representation that integrates 3D spatial information into 2D patch features. 3.2. 3D Patch Our 3D Patch representations are built upon the 2D patch features extracted from multi-view images with CLIP visual encoder to leverage the strong visual-semantic alignment. To construct the 3D Patches, we inject the 3D position information into the aforementioned 2d patches so that the 3D Patches can explicitly model 3D spatial information while preserving the semantic information from 2D patches. As illustrated in left block of Fig. 2, given the RV cwh, we obtain multi-view 2D patch features their 3D positions RV 3wh in the 3D world, using nearest neighbor depth and known camera intrinsic and extrinsic parameters. The 3D positions are then encoded into 3D position embeddings RV whd through learnable two-layer MLP, which are subsequently added to the 2D patch visual tokens, resulting in the 3D patches 3D RV whd: 3D = + MLP(P ) (1) 3.3. 3D Patch Pooling While the 3D Patches equip 2D patches with 3D spatial awareness, the number of 3D patches scales directly with the number of input images. Capturing full 3D scene often necessitates large set of images, which significantly increases the computational overhead for large language models (LLMs). To address this, we introduce 3D-aware pooling mechanism to reduce the number of 3D patches, as illustrated in the middle block of Fig. 2. In the 2D image or video domain, pooling is commonly applied along the 2D spatial or temporal dimensions to compress the number of tokens and extract essential semantic information. However, for 3D scene understanding, we must pool the 3D patches based on their 3D locations to ensure these features can cover and preserve the entire scenes structure as completely as possible. We explore two parameterfree pooling strategies to achieve this: Voxelization Pooling. Voxelization discretizes the 3D space into volumetric grid. Within each occupied voxel, the 3D patches undergo average pooling, resulting in updated voxel visual tokens. Only the visual tokens from occupied voxels are passed to the LLM, with the number of tokens varying across different 3D scenes. While the number of 3D patches scales with the number of images, the number of voxel patches is only determined by the partition of voxel grids. We can easily balance the number of visual tokens and the preservation of fine-grained scene features by controlling the voxel size. Figure 2. LLaVA-3D Architecture. Based on LLaVA, we directly add the corresponding 3D position embeddings to 2D patch visual tokens of multi-view images to construct the 3D Patches, then the 3D Patches will undergo 3D pooling and be sent into the projection layer of LLaVA to map into the LLM space and align with the LLM using 3D-visual-language data. FPS Pooling. Farthest Point Sampling (FPS) is widely used sampling strategy to select representative subset of points from larger set of points cloud. We apply FPS to sample 3D patches from multi-view images to fixed number of tokens, ensuring that the sampled tokens represent the entire scene structure. While fixing the number of tokens helps the LLM efficiently process visual information, it may also result in loss of scene information. Compared to FPS Pooling, Voxelization Pooling offers equivalent efficiency in visual token compression while preserving more detailed scene information. Furthermore, the explicit construction of voxel grids can better handle dynamic 3D scene updates, whereas FPS Pooling excels at preserving the overall 3D scene structure. We conduct quantitative experiments in Sec. 5.7 to thoroughly assess the effectiveness of these pooling strategies. 3.4. 3D-aware Position Encoding & Decoding In the previous sections, we detailed the construction of the 3D scene representation from multi-view images, establishing the foundation for further interaction with the 3D scene. Building on this, the LLM could process multi-modal inputs such as the 3D scene, language instructions, and 3D coordinate cues to generate outputs such as language responses and 3D bounding boxes, as illustrated in the right block of Fig. 2. In this section, we introduce how the model is equipped to interpret 3D coordinate information from inputs and subsequently output precise 3D bounding boxes when specific location-related task requirements are needed. through two-layer MLP. These 3D coordinate tokens are fed into LLM together with 3D Patch tokens and text tokens, enabling 3D-aware perception and reasoning. Decoding of 3D Bounding Box Output. The integration of the 3D coordinate token empowers the model to process 3D coordinate information from input instructions effectively. However, experiments show that directly outputting 3D bounding boxes is quite challenging for LLM, and empirically, the performance is poor. To overcome this, we leverage an approach similar to previous method [51], introducing specialized location token that guides the grounding module to generate accurate 3D bounding boxes. Specifically, the LLM predicts special location token to represent 3D box prediction when the task necessitates 3D bounding box outputs. We then derive the last layer embedding of this location token and send it into the grounding module. The grounding module utilizes the location token embedding and the 3D scene feature to predict the 3D box coordinates of the target object. This process facilitates the precise generation of 3D bounding box outputs. More details can be found in the appendix. 4. Training To leverage the 2D priors from established 2D LMMs, we train our LLaVA-3D model based on the pre-trained LLaVA1.5. Considering the scarcity of 3D scene-language data, our training procedure comprises two stages, each focusing on different training targets of the model. Encoding of 3D Coordinate Input. In scenarios such as 3D dense object captioning or object-centric question answering, the language instruction contains 3D coordinates. To handle such tasks, we introduce the 3D Coordinate Token to allow the model to integrate the provided coordinates as context into its reasoning processes. Specifically, we obtain the 3D coordinate token by feeding the 3D coordinates Stage 1: 3D Patch Language Alignment. During the first training stage, we use the region-level and scene-level caption data that describe spatial relationships among 3D objects to align the 3D patches with the LLM for enhanced 3D spatial comprehension. At this stage, the input multi-view images used are selected from sequences that correspond Table 1. Quantitative comparison with SOTA models on various 3D QA tasks. stands for CIDEr, B-4 for BLEU-4, for METEOR, for ROUGE, Sim for sentence similarity, and EM@1 for top-1 exact match. Gray indicates evaluation results with refined exact-match protocol. ScanQA (val) SQA3D (test) B-4 EM@1 EM@ Task-specific models Scan2Cap [13] ScanRefer+MCAN [49] ClipBERT [27] ScanQA [3] 3D-VisTA [52] Task-specific fine-tuned 3D LMMs 3D-LLM (FlanT5) [18] LL3DA [37] Chat-3D v2 [19] LEO [20] Scene-LLM [17] Zero-shot 2D LMMs VideoChat2 [32] LLaVA-NeXT-Video [28] GPT-4V Gemini Claude LLaVA-3D - 55.4 - 64.9 69.6 69.4 76.8 87.6 101.4 49.2 46.2 59.6 68.3 57.7 91.7 - 7.9 - 10.1 10.4 12.0 13.5 14.0 13.2 12.0 9.6 9.8 - - - - 11.5 - 13.1 13. 14.5 15.9 - 20.0 16.6 9.5 9.1 13.5 11.3 10.0 - 30.0 - 33.3 35.7 35.7 37.3 - 49.2 40.0 28.2 27.8 33.4 35.4 29.3 - 18.6 - 21.1 22. 41.0 - 43.3 47.2 48.5 20.5 - - 24.5 (47.6) 27.2 54.7 50.0 (52.4) 54.2 19.2 18.7 - - - 37.3 34.2 - - - 14. 20.7 50.1 27.0 (45.0) 55.6 (57.6) requirements. 5. Experiments In this section, we conduct extensive evaluations to examine the capabilities of LLaVA-3D. To begin with, we introduce the implementation details (Sec. 5.1). Then, we compare our models 3D scene understanding (Sec. 5.2, 5.3, 5.4)and 2D image understanding (Sec. 5.5) capability with previous methods. Finally, we conduct thorough analysis to validate the effectiveness of the components and designs of our LLaVA-3D (Sec. 5.6, 5.7). 5.1. Implementation Details LLaVA-3D is built upon the LLaVA-1.5-7B, utilizing their pre-trained weights from the HuggingFace library. For 3D tasks, we add the 3D position embeddings to the 2D patch visual tokens, and utilize the voxelization pooling strategy to reduce 3D patch number before passing the input visual tokens to the projection layer and LLM. The number of views is set to be 20 and voxel size is set to 0.2m. Due to the LLM context length limitation, the maximum number of 3D patch tokens after 3D-aware pooling is set to 3096. For 2D tasks, LLaVA-3D functions the same as LLaVA. All experiments are conducted on 8 80G A100 GPUs. We train our model for 1 epoch with learning rate of 1e-3 and batch size of 32 in stage 1, and fine-tune on the collected LLaVA-3D-Instruct-1M dataset, with learning rate of 2e-5 and batch size of 16 in stage 2. Figure 3. LLaVA-3D-Instruct-1M. The hybrid 2D and 3D Dataset Collection. Left: Distribution of data across categories, with the outer circle representing all categories and the inner circle illustrating data subset distribution. Right: Detailed dataset quantities. to specific regions or entire scenes. We freeze the vision encoder and LLM parameters, and only train the projection layer and 3D position embedding layer, encouraging efficient alignment between 3D patch features and text space. Since 3D patches are derived from CLIP features augmented with 3D positional information, the alignment between 3D Patch and LLM converges rapidly. Stage 2: Task Instruction Tuning. During the instructiontuning stage, LLaVA-3D is optimized to respond to complex 3D V&L tasks and maintain its inherent 2D image reasoning and instruction-following capabilities. To facilitate this capability, we collect the LLaVA-3D-Instruct-1M dataset, hybrid collection of 2D and 3D data specifically tailored for instruction tuning. The overall distribution of the dataset collection is shown in Fig 3, more details about the instructional tuning datasets are listed in the appendix. The 2D Data of LLaVA-3D-Instruct-1M is derived from existing LLaVA1.5 instruction tuning data, ensuring the preservation of 2D image comprehension and vision-language conversation abilities. When tuning with 2D data, we keep the vision encoder frozen and jointly train the projection layer and LLM. The 3D Data of LLaVA-3D-Instruct-1M, on the other hand, comprises data from diverse 3D QA, 3D dense captioning, and 3D grounding tasks. During the 3D data instruction tuning, the 3D position embedding layer will be added to jointly train with the other modules. Additionally, for tasks where the instruction contains 3D coordinate information or requires 3D bounding box outputs, the corresponding encoding and decoding modules will be trained together. During instruction tuning, the 3D data pathway includes the 3D position embeddings and 3D patches, while the 2D data pathway is the original LLaVA. All modules except for the 3D position embeddings to construct 3D patches are shared across 2D and 3D data. This training setup ensures that LLaVA-3D is capable of processing both 2D and 3D visual tokens effectively, and is adaptive to various task formulations and Table 2. Quantitative comparison on MMScan QA benchmark. S.-BERT\", B-1, B-4, R.-L., MET. represents Sentence-BERT\", BLEU-1, BLEU-4, ROUGE-L, METEOR, respectively. Here, we report the top-1 exact match with (the refined exact-match protocol results) for EM@1. Methods Setting Overall 3D-LLM [18] Chat3D-v2 [19] LL3DA [12] LEO [20] LL3DA [12] LEO [20] LLaVA-3D Zero-Shot Fine-tuning Generalist 28.6 27.9 15.8 22.2 38.5 47.8 52.3 Single-target Inter-target ST-attr 37.8 38.1 15.5 28.9 40.4 55.5 61. ST-space OO-attr OO-space OR 15.4 13.5 4.3 15.0 26.4 32.1 44.5 26.3 22.4 25.2 20.4 47.1 45.6 61.2 18.8 18.3 14.7 17.6 46.2 49.5 54.4 13.7 9.3 14.2 18.1 14.7 36.1 28.7 Advanced 20.8 25.4 6.4 16.3 7.1 38.4 43. Traditional Metrics Data-driven Metrics SimCSE S.-BERT B-1. B-4. R.-L MET. 13.4 18.0 5.4 11.0 26.4 32.0 38.7 1.5 3.0 2.1 0.7 8.5 12.5 13.1 17.3 22.9 16.4 17.1 44.3 52.1 55.5 40.4 45.4 40.7 40.4 65.3 71.2 74.6 40.3 46.3 43.6 41.0 67.0 72.2 76. 6.0 7.5 4.4 4.9 14.7 17.7 19.5 EM@1 6.2 (19.6) 10.2 (19.6) 8.3 (19.4) 9.6 (18.7) 30.2 (37.6) 36.6 (44.5) 45.2 (51.4) Table 3. Quantitative comparison with SOTA models on OpenEQA benchmark LLaVA-3D could perform as generalist and achieve the SOTA performance on these benchmarks. Models Frame Accuracy LLaMA2 [45] GPT-4 [1] Claude3 Gemini-Pro [44] GPT-4V [1] GPT-4V [1] Human LLaVA-3D 0 0 20 15 15 50 Full 20 28.3 33.5 36.3 44.9 54.6 55.3 86.8 51. 5.2. Evaluation on 3D Question Answering 3D Question Answering requires model to generate responses to the natural language queries questioning towards 3D scene. In this section, we validate LLaVA3D performance on various 3D question answering benchmarks: ScanQA [3], SQA3D [40], MMScan QA [39], and OpenEQA [41]. Spatial Understanding with ScanQA and SQA3D. ScanQA and SQA3D are both built on ScanNet dataset. The ScanQA dataset consists of 41363 questions about 800 scenes, including 32337 unique questions. Its validation set contains 4675 questions about 71 scenes. SQA3D comprises 20.4k descriptions of 6.8k unique situations collected from 650 ScanNet scenes and 33.4k questions about these situations. Questions in ScanQA require basic recognition and 3D reasoning capabilities, and SQA3D further incorporates the situation understanding and situated reasoning into embodied 3D scene understanding. Following prior works, we adopt BLEU scores, METEOR, ROUHE-L, CIDEr and EM (exact match) as our evaluation metrics for ScanQA and EM for SQA3D respectively. As shown in Tab. 1, current 2D LMMs fail to achieve competitive performance with the latest 3D LMMs trained with 3D awareness, which might be attributed to the lack of explicit 3D representation. Besides, compared with task-specific fine-tuned 3D LMMs that need to be further fine-tuned on the corresponding datasets, our Coordinate Spatial Understanding with MMScan QA. MMScan QA includes 5.2k scans from ScanNet, 3RScan, and Matterport3D, along with 116k training questions and 29k validation questions. These questions span existential inquiries, attribute understanding, and more advanced queries. Unlike ScanQA and SQA3D, some MMScan QA questions require 3D reasoning based on object coordinates, rather than relying solely on text descriptions, demanding the model capable of understanding 3D coordinates information. All data samples are classified into one of the following subcategories: ST-attr, ST-space, OO-attr, OO-space, OR, where ST stands for Single-target, attr for attribute, OO for ObjectObject, and OR for Object Region. Besides, there is minor part of QA samples for advanced understanding and reasoning, such as situated QA related to everyday life. We present the results under GPT-4 evaluation, data-driven metrics, and traditional metrics respectively in Tab. 2. In this benchmark, the well-trained LL3DA and LEO are further fine-tuned on the full 1.2M MMScan QA training dataset. Our LLaVA-3D, trained on LLaVA-3D-Instruct-1M (which includes 440K MMScan QA training samples), achieves significantly better performance on the MMScan QA benchmark compared to the specially fine-tuned LL3DA and LEO. The results highlight the training efficiency of LLaVA-3D and its strong 3D understanding ability to serve as the generalist model. Embodied Question Answering with OpenEQA. OpenEQA is the first open-vocabulary benchmark designed for spatial understanding and embodied reasoning in embodied question answering, specifically in the era of foundation models. It features an automated evaluation protocol powered by LLMs, which shows strong alignment with human judgment. Our evaluations are conducted using the EM-EQA data split of OpenEQA, which includes over 1,600 high-quality, human-generated questions from diverse real-world environments. In EM-EQA, the model is required to generate textual answer to question based on an episode history, which typically consists Table 4. Quantitative Comparisons with SOTA models for 3D Dense Captioning on Scan2Cap. stands for CIDEr, B-4 for BLEU4, for METEOR, for ROUGE, Sim for sentence similarity, and EM@1 for top-1 exact match. The n-gram metrics for Scan2Cap are governed by IoU@0.5. Scan2Cap [13] MORE [24] SpaCap3D [46] D3Net [7] UniT3D [14] 3DJCG [5] 3D-VLP [25] 3D-VisTA [52] Vote2Cap-DETR [11] LL3DA [12] LEO [20] LLaVA-3D Scan2Cap (Val) C@0.5 B-4@0.5 M@0.5 R@0.5 39.08 40.94 44.02 46.07 46.69 49.48 54.94 61.60 61.81 65.19 72.4 79.21 23.32 22.93 25.26 30.29 27.22 31.03 32.31 34.10 34.46 36.79 38.2 41.12 21.97 21.66 22.33 24.35 21.91 24.22 24.83 26.80 26.22 25.97 27.9 30. 44.78 44.42 45.36 51.67 45.98 50.80 51.51 55.00 54.40 55.06 58.1 63.41 Table 5. Quantitative comparison with SOTA models on the MMScan Captioning benchmark. model LL3DA [12] LEO [20] LLaVA-3D Evaluator GPT GPT GPT Type Color 26.3 10.0 29.7 34.9 79.2 39.9 Shape 40.6 63.0 89. Position 38.9 63.7 82.2 Function Design Overall 21.7 42.7 88.0 67.5 75.0 94.1 33.6 51.3 78.8 of RGB images, depth information, camera poses, and intrinsic camera data. The results in Tab. 3 demonstrate that LLaVA-3D surpasses Claude3 and Gemini-Pro, and achieves comparable performance with powerful GPT-4V on this benchmark with significantly fewer model parameters. 5.3. Evaluation on 3D Dense Captioning 3D dense captioning requires the model to localize all the objects in 3D scene and then generate descriptive sentences for each object. To evaluate our model on the 3D dense captioning tasks, we utilize the off-the-shelf segmentation model Mask3D to generate object proposals. Then we further construct the 3D coordinate tokens based on the 3D object center coordinates to guide the model to perform the task. Additionally, we utilize two types of textual instructions that prompt the model to either describe the object or describe and output the bounding box of the object. We report the performance of various methods on two 3D dense captioning benchmarks: Scan2Cap and MMScan Captioning. Scan2Cap. Scan2Cap requires the model to describe the objects appearance and the spatial relations with nearby objects and output the corresponding 3D bounding box. As illustrated in Tab. 4, our method consistently outperforms the existing method on the Scan2Cap benchmark. MMScan Captioning. MMScan Captioning focuses on identifying common aspects of 3D objects such as Object Type, Color, Shape, Position, Function, and Design. The Table 6. Quantitative comparison with SOTA models on various 3D VG tasks. Task-specific models ScanRefer [6] MVT [21] 3DVG-Trans [50] ViL3DRel [10] BUTD-DETR [22] ReGround3D [51] Task-specific fine-tuned 3D LLMs LLM-Grounder [48] 3D-LLM [18] Chat3D-v2 [19] LLaVA-3D ScanRefer Acc@0.25 Acc@0.5 37.3 40.8 45.9 47.9 52.2 53.1 17.1 30.3 35.9 54. 24.3 33.3 34.5 37.7 39.8 41.1 5.3 - 30.4 42.2 benchmark utilizes GPT4 to assess whether these aspects are correct in the object description. We benchmark various methods on the MMScan Captioning benchmark in Tab. 5. The results show that our method surpasses existing approaches across all metrics by substantial margin, especially achieving 49.5% improvement in the Color score and 43.3% improvement in the Design score. This significant performance boost can be attributed to our 2D LMM-based architecture. Uniquely, LLaVA-3D takes multi-view images as inputs, enabling user-friendly feature where users can simply click on selected images to generate both 3D object captions and 3D bounding boxes. We refer to this capability as 2D Clickbased 3D Dense Captioning. (as illustrated in Fig. 4). 5.4. Evaluation on 3D Visual Grounding 3D visual grounding aims to localize the target object in the 3D scene using natural language descriptions. In this section, we initially report the performance on the ScanRefer [6] benchmark. For quantitative comparisons, we include both task-specific approaches and generalist models: the stateof-the-art specialists in 3D VG and generalist models like LLM-Grounder [48], 3D-LLM [18], and Chat3D-v2 [19]. 3D-LLM [18]] uses the location tokens to discretize the 3D scene and predicts the bounding box as the location tokens that were added to the vocabulary. Chat-3D v2 [19] first detects all the objects and then identifies the object that best matches the provided description. The results in Tab. 6 demonstrate that by combining with the grounding module, our LLaVA-3D could achieve even better performance compared with the task-specific specialists model as generalist. Such paradigm allows LLaVA-3D to integrate with various powerful 3D grounding modules and inject the 3D understanding capability and world knowledge into the grounding process, as illustrated in Fig. 7. Figure 4. LLaVA-3D enables the user-friendly interaction with the 3D scene across various 3D understanding and reasoning tasks. It allows the users to just click on the 2D images or the video frame to simply conduct the interactive 3D question answering and 3D dense captioning. Table 7. Quantitative Comparisons with SOTA models on zeroshot 2D benchmarks. Table 9. Effectiveness of 3D Patch Representation. Training using the instruction tuning datasets, the only difference between multi-image LLaVA and LLaVA-3D is the patch type. Method LLM Res. VQAT MMB MME MM-Vet MobileVLM [15] MLLaMA 2.7B 336 224 InstructBLIP [16] 224 InstructBLIP [16] 448 Qwen-VL [4] 448 Qwen-VL-Chat [4] 224 Shikra [9] 336 LLaMA-VID [33] 336 LLaMA-VID [33] 336 LLaVA-1.5 [38] 336 LLaVA-1.5 [38] Vicuna-7B Vicuna-13B Qwen-7B Qwen-7B Vicuna-13B Vicuna-7B Vicuna-13B Vicuna-7B Vicuna-13B 47.5 50.1 50.7 63.8 61.5 58.2 61.3 LLaVA-3D Vicuna-7B 336 57.8 59.6 36.0 38.2 60.6 58.8 65.1 66.6 65.2 69.2 65.0 1289 1213 1488 1521 1542 1511 1531/295 - 26.2 25.6 31.1 36.1 30.9 Table 8. Architecture Comparison on various 3D V&L Benchmark. Connector Q-Former 3D Feature (SAM + CLIP) / PE (SAM + CLIP) / PE Pooling + MLP Pooling + MLP Pooling + MLP Pooling + MLP CLIP / PE CLIP / PE CLIP / PE (a) (b) (c) (d) (e) LLM / LMM ScanQA SQA3D Inference time Vicuna-7B Vicuna-7B Vicuna-7B LLaVA-1.5 PLLaVA 900s 900s 0.2s 0.2s 0.2s 49.3 49.2 51.2 55.6 56.2 21.9 22.1 23.4 27.0 27.9 5.5. Evaluation on 2D benchmarks Since our model is trained on mix of 2D and 3D datasets, we evaluate it across various 2D benchmarks to ensure it retains the 2D image understanding capabilities of the original LLaVA. As demonstrated in Tab. 7, LLaVA-3D achieves performance comparable to that of LLaVA-1.5 across several 2D image understanding benchmarks. This performance underscores the architectural superiority of our model compared to other 3D LMMs. 5.6. Architecture Analysis In this section, we delve deeper into the architectural benefits and efficacy of adapting 2D large multimodal model Method multi-image LLaVA LLaVA-3D Patch Type 2D 3D ScanQA 24.1 27.0 (+2.9) SQA3D 52.3 55.6 (+3.3) MMScan QA 32.7 41.5 (+8.8) Scan2Cap 29.1 79.2 (+50.1) (LMM) to 3D LMM, as opposed to developing 3D LMM solely from LLMs. notable approach in the latter category is 3D-LLM, which leverages foundational 2D visual models such as SAM and CLIP for feature extraction. This method then employs 3D multi-view fusion to generate 3D point features, followed by the use of Q-Former to condense these point features into fixed number of tokens. Architecture Comparison. To ensure the fairness of the experiment as much as possible, starting from LLM, we first ablate different 3D feature encoders, and 3D-language connectors using the same instruction tuning datasets. As shown in Tab. 8, the Q-Former (a) and Pooling + MLP (b) share similar performance on 3D V&L benchmarks. Notably, using CLIP (c) alone instead of SAM + CLIP (b) achieves better performance and significantly reduces 3D feature extraction computation time from 900s to 0.2s. Effectiveness of 3D Patch. Once we dont decorate the 2D patches with corresponding 3D position embedding, LLaVA3D will degenerate into 2D multi-image LMMs. To further ascertain the efficacy of our proposed 3D representation: 3D Patch, we conduct additional experiments across variety of 3D question answering and 3D dense captioning benchmarks. As shown in Tab. 9, integrating 2D patches within 3D context enhances the models 3D spatial understandTable 10. Comparsion on different pooling strategies. Table 11. Comparison on performance on 3D QA tasks under different number of multi-view images. Pooling Stratgey Voxel Size Token Number 0.4 0.3 0.2 - - Voxelization Voxelization Voxelization FPS FPS Dynamic Dynamic Dynamic 576 1024 ScanQA SQA3D 24.1 25.9 27.0 25.7 26.3 53.2 54.8 55.6 54.9 55.2 Number of Views Number of Tokens ScanQA SQA3D 16 20 24 40 9216 11520 13824 26.2 27.0 27.0 26.7 55.1 55.6 55.4 55.2 ing and reasoning capabilities, resulting in 2.9% and 3.3% improvements on the ScanQA and SQA3D benchmarks, respectively. Additionally, 3D patches are crucial for tasks that require explicit 3D world modeling, leading to significant improvements of 8.8% on the MMScan QA benchmark and an impressive 50.1% on the Scan2Cap benchmark. Benefits from pre-trained 2D LMM. Leveraging our foundation in 2D LMMs, our framework benefits significantly from the robust pre-training on extensive image/videotext datasets. In Tab. 8, we explore the advantages of initializing from pre-trained 2D LMM compared to starting directly from an LLM. The experimental results consistently demonstrate that starting with well-tuned 2D LMM substantially could enhance performance in 3D spatial understanding tasks. Besides, we observe that stronger base 2D LMM could lead to better 3D spatial understanding performance. Besides, we find that initializing from pre-trained 2D LMM could achieve 3.5 faster training convergence speed of LLaVA-3D compared with existing 3D LMMs [12, 20]. More details could be found in our appendix. egocentric images of the 3D scene in our experiment. In this section, we explore the effect of different image sampling strategies during the inference stage: 1) Uniform Sampling: To achieve comprehensive coverage of the entire 3D scene, straightforward approach is uniform sampling, which samples images evenly. 2) Text-Guided Sampling: such sampling strategy uses CLIP to select the frames related to the text instruction during inference based on the image-text CLIP similarity score. Our experiments demonstrate that these two sampling strategies share similar performance on ScanQA and SQA3D, so we choose uniform sampling for simplicity. Number of Views. An intuitive assumption is that sampling more views from the 3D scene will preserve more information about the 3D scene. We conduct comparative experiment varying the number of views sampled from 3D scenes. Tab. 11 presents the Exact Match (EM) scores on ScanQA and SQA3D across different settings, revealing that the increase in EM score is marginal as the number of views increases. Additionally, the experimental results indicate that exceeding certain number of views can degrade the models performance. 5.7. More Analysis 6. Conclusion To better understand the impact of different components of our LLaVA-3D, we conduct thorough ablation study on the ScanQA and SQA3D benchmarks. Impact of Pooling Strategy. Given the maximum token length limitation of LLMs, we apply pooling to the 3D patch tokens extracted from multi-view images to reduce the number of tokens. However, this pooling process inevitably leads to some information loss. To understand its impact on performance, we conducted experiments to evaluate the effects of pooling. As shown in Tab. 10, the voxelization pooling strategy outperforms the FPS pooling method on 3D QA benchmarks. Reducing the voxel size in the voxelization pooling or increasing the number of 3D patch tokens obtained through FPS pooling can both enhance the models performance to some extent. Multi-View Images Sampling Strategy. Due to the redundancy of information among multi-view images and considering computational costs, we sample views from the We propose LLaVA-3D, simple yet effective framework built upon the well-established 2D LLaVA model. LLaVA3D extends the LLaVAs capabilities to understand the 3D world by using 3D patches to bridge 2D features within 3D space, enabling spatial understanding while efficiently preserving the original 2D image understanding and reasoning capability. Experimental results show that our method sets new state-of-the-art performance on wide range of 3D tasks and benchmarks. We hope that our model will inspire new ideas for building 3D LMMs, and in the future, we plan to explore the application of LLaVA-3D in more downstream scenarios, such as robot manipulation and navigation."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 6 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35:2371623736, 2022. 2 [3] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19129 19139, 2022. 2, 5, 6 [4] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 8 [5] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong Xu. 3djcg: unified framework for joint dense captioning and visual grounding on 3d point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1646416473, 2022. 7 [6] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. 2, 7 [7] Dave Zhenyu Chen, Qirui Wu, Matthias Nießner, and Angel Chang. 3 net: unified speaker-listener architecture for 3d dense captioning and visual grounding. In European Conference on Computer Vision, pages 487505. Springer, 2022. [8] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 2 [9] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. 8 [10] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. Advances in Neural Information Processing Systems, 35:2052220535, 2022. 7 [11] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2capdetr. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1112411133, 2023. 7 [12] Sijin Chen, Xin Chen, Chi Zhang, Mingsheng Li, Gang Yu, Hao Fei, Hongyuan Zhu, Jiayuan Fan, and Tao Chen. Ll3da: Visual interactive instruction tuning for omni-3d understanding reasoning and planning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2642826438, 2024. 2, 3, 6, 7, 9 [13] Zhenyu Chen, Ali Gholami, Matthias Nießner, and Angel Chang. Scan2cap: Context-aware dense captioning in rgbIn Proceedings of the IEEE/CVF conference on scans. computer vision and pattern recognition, pages 31933203, 2021. 2, 5, [14] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nießner, and Angel Chang. Unit3d: unified transformer for 3d dense captioning and visual grounding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1810918119, 2023. 7 [15] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, et al. Mobilevlm: fast, reproducible and strong vision language assistant for mobile devices. arXiv preprint arXiv:2312.16886, 2023. 8 [16] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning, 2023. 8 [17] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-llm: Extending language model for arXiv preprint 3d visual understanding and reasoning. arXiv:2403.11401, 2024. 2, 3, 5 [18] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. arXiv preprint arXiv:2307.12981, 2023. 2, 3, 5, 6, 7 [19] Haifeng Huang, Zehan Wang, Rongjie Huang, Luping Liu, Xize Cheng, Yang Zhao, Tao Jin, and Zhou Zhao. Chat-3d v2: Bridging 3d scene and large language models with object identifiers. arXiv preprint arXiv:2312.08168, 2023. 2, 3, 5, 6, [20] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. 2, 3, 5, 6, 7, 9 [21] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multiview transformer for 3d visual grounding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1552415533, 2022. 7 [22] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Katerina Fragkiadaki. Bottom up top down detection transformers for language grounding in images and point clouds. In Computer VisionECCV 2022: 17th European Conference, Tel Aviv, Israel, October 2327, 2022, Proceedings, Part XXXVI, pages 417433. Springer, 2022. 7 [23] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint arXiv:2302.07241, 2023. 2 [24] Yang Jiao, Shaoxiang Chen, Zequn Jie, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. More: Multi-order relation mining for dense captioning in 3d scenes. In European Conference on Computer Vision, pages 528545. Springer, 2022. 7 [25] Zhao Jin, Munawar Hayat, Yuwei Yang, Yulan Guo, and Yinjie Lei. Context-aware alignment and mutual masking for 3d-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1098410994, 2023. 7 [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 2 [27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 73317341, 2021. 5 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 2, 5 [29] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 3 [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 2 [31] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 2 [32] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. [33] Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. arXiv preprint arXiv:2311.17043, 2023. 8 [34] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual represenarXiv preprint tation by alignment before projection. arXiv:2311.10122, 2023. 2 [35] Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2668926699, 2024. 2 [36] Benlin Liu, Yuhao Dong, Yiqin Wang, Yongming Rao, Yansong Tang, Wei-Chiu Ma, and Ranjay Krishna. Coarse correspondence elicit 3d spacetime understanding in multimodal language model. arXiv preprint arXiv:2408.00754, 2024. 2 [37] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. 2, 5 [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 3, 8 [39] Ruiyuan Lyu, Tai Wang, Jingli Lin, Shuai Yang, Xiaohan Mao, Yilun Chen, Runsen Xu, Haifeng Huang, Chenming Zhu, Dahua Lin, et al. Mmscan: multi-modal 3d scene dataset with hierarchical grounded language annotations. arXiv preprint arXiv:2406.09401, 2024. 2, 6 [40] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: SitarXiv preprint uated question answering in 3d scenes. arXiv:2210.07474, 2022. [41] Arjun Majumdar, Anurag Ajay, Xiaohan Zhang, Pranav Putta, Sriram Yenamandra, Mikael Henaff, Sneha Silwal, Paul Mcvay, Oleksandr Maksymets, Sergio Arnaud, et al. Openeqa: Embodied question answering in the era of foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1648816498, 2024. 2, 6 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2 [43] Mohit Shridhar, Lucas Manuelli, and Dieter Fox. Perceiveractor: multi-task transformer for robotic manipulation. In Conference on Robot Learning, pages 785799. PMLR, 2023. 2 [44] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 6 [45] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. 6 [46] Heng Wang, Chaoyi Zhang, Jianhui Yu, and Weidong Cai. Spatiality-guided transformer for 3d dense captioning on point clouds. arXiv preprint arXiv:2204.10688, 2022. 7 [47] Lin Xu, Yilin Zhao, Daquan Zhou, Zhijie Lin, See Kiong Ng, and Jiashi Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [48] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David Fouhey, and Joyce Chai. Llmgrounder: Open-vocabulary 3d visual grounding with large language model as an agent. arXiv preprint arXiv:2309.12311, 2023. 7 [49] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 62816290, 2019. 5 [50] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvgtransformer: Relation modeling for visual grounding on point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 29282937, 2021. 7 [51] Chenming Zhu, Tai Wang, Wenwei Zhang, Kai Chen, and Xihui Liu. Empowering 3d visual grounding with reasoning capabilities. arXiv preprint arXiv:2407.01525, 2024. 2, 4, 7 [52] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. arXiv preprint arXiv:2308.04352, 2023. 5, 7 Figure 5. LLaVA-3D could perform 2D Click-based 3D dense captioning, generating the corresponding object caption and 3D bounding box. Figure 6. LLaVA-3D could perform 2D Click-based 3D question answering, now users could click on the 2D images and ask the question. Figure 7. LLaVA-3D exhibits powerful 3D visual grounding capability, enabling accurate 3D bounding boxes output."
        }
    ],
    "affiliations": [
        "Shanghai AI Laboratory",
        "The University of Hong Kong"
    ]
}