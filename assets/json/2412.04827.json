{
    "paper_title": "PanoDreamer: 3D Panorama Synthesis from a Single Image",
    "authors": [
        "Avinash Paliwal",
        "Xilong Zhou",
        "Andrii Tsarov",
        "Nima Khademi Kalantari"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we present PanoDreamer, a novel method for producing a coherent 360$^\\circ$ 3D scene from a single input image. Unlike existing methods that generate the scene sequentially, we frame the problem as single-image panorama and depth estimation. Once the coherent panoramic image and its corresponding depth are obtained, the scene can be reconstructed by inpainting the small occluded regions and projecting them into 3D space. Our key contribution is formulating single-image panorama and depth estimation as two optimization tasks and introducing alternating minimization strategies to effectively solve their objectives. We demonstrate that our approach outperforms existing techniques in single-image 360$^\\circ$ scene reconstruction in terms of consistency and overall quality."
        },
        {
            "title": "Start",
            "content": "PanoDreamer: 3D Panorama Synthesis from Single Image Avinash Paliwal1 Xilong Zhou1,3 Nima Khademi Kalantari1 Andrii Tsarov2 1Texas A&M University 2Leia Inc. 3 Max Planck Institute for Informatics 4 2 0 2 6 ] . [ 1 7 2 8 4 0 . 2 1 4 2 : r Figure 1. We introduce novel method for 360 3D scene synthesis from single image. Our approach generates panorama and its corresponding depth in coherent manner, addressing limitations in existing state-of-the-art methods such as LucidDreamer [3] and WonderJourney [36]. These methods sequentially add details by following generation trajectory, often resulting in visible seams when looping back to the input image. In contrast, our approach ensures consistency throughout the entire 360 scene, as shown. The yellow bars show the regions corresponding to the input in each result."
        },
        {
            "title": "Abstract",
            "content": "In this paper, we present PanoDreamer, novel method for producing coherent 360 3D scene from single input image. Unlike existing methods that generate the scene sequentially, we frame the problem as single-image panorama and depth estimation. Once the coherent panoramic image and its corresponding depth are obtained, the scene can be reconstructed by inpainting the small occluded regions and projecting them into 3D space. Our key contribution is formulating single-image panorama and depth estimation as two optimization tasks and introducing alternating minimization strategies to effectively solve their objectives. We demonstrate that our approach outperforms existing techniques in single-image 360 scene reconstruction in terms of consistency and overall quality1. 1. Introduction Generating immersive and realistic 3D scenes from single input image has emerged as one of the important topics in computer vision/graphics, driven by its broad applications including virtual/augmented reality (VR/AR) and gaming. While early algorithms [10, 12, 15, 17, 22, 24, 25, 42] have achieved high-quality results, they are generally limited to 1people.engr.tamu.edu/nimak/Papers/PanoDreamer synthesizing novel views with only minor deviation from the input camera position. Consequently, these techniques cannot reconstruct full 360 scene, which is the primary goal of our work. With the introduction of diffusion models, the more recent approaches have focused on utilizing these powerful models for 3D scene reconstruction. Specifically, several methods [16, 40, 41] propose various ways to generate 3D scenes from an input text prompts. These methods first generate entire panorama from text prompt using pretrained text-to-panorama diffusion models (DMs) and then lift it to 3D. Unfortunately, these approaches are fully generative and do not have mechanism for reconstructing 3D scene which is also consistent with single input image. Several methods [3, 8, 18, 23, 36, 37] specifically address the problem of 3D scene reconstruction from single image. Starting from the input image, these methods typically project it into 3D space, render it from novel view, and then inpaint the missing regions using diffusion model. They repeat this process for series of cameras along specific path to reconstruct the complete 3D scene. However, major limitation of these approaches is that, due to the progressive nature of the scene building, they often fail to synthesize coherent 360 scenes, i.e., the start and end of the 360 scenes are contextually different. In this work, we propose novel framework, coined PanoDreamer, for generating coherent 360 3D scene from 1 single input image. Departing from the existing methods, which generate the 3D scene one image at time, we start by producing single coherent 360 panorama from the input image using standard pre-trained inpainting diffusion models. Inspired by MultiDiffusion [1], we formulate the problem as an optimization with two loss terms and propose an alternating minimization strategy to optimize the objective, resulting in coherent and seamless panoramic image. The next stage of our approach involves estimating the depth of the panoramic image to project pixels into 3D space and reconstruct the 3D scene. While powerful monocular depth estimation methods [31] exist, these techniques are typically optimized for specific resolutions and struggle to handle large panoramic images effectively. To address this problem, we formulate panoramic depth reconstruction as an optimization task, aiming to simultaneously produce coherent panoramic depth map and parametric function that aligns the range of monocular depth to the target depth. We propose an alternating minimization approach to efficiently solve this objective, resulting in coherent and seamless panoramic depth map. Given the panoramic image and depth, we directly apply the approach of Shih et al. [22] to construct layered depth image (LDI) and inpaint the missing regions in each layer. Next, we build 3D Gaussian splatting (3DGS) representation[11] by initializing set of Gaussians through the projection of LDI pixels into 3D space. We then optimize the 3DGS representation to sharpen details and obtain the final scene. We demonstrate that PanoDreamer can reconstruct consistent 360 3D scenes from single input images that outperform existing methods. In summary, our work makes the following contributions: We propose novel framework for synthesizing coherent 3D panoramic scene from single image. We formulate the problem of single-image panorama generation using an inpainting diffusion model as an optimization task and solve it using an alternating minimization strategy. We frame the task of obtaining panoramic depth from existing monocular depth estimation methods as an optimization problem and propose an alternating minimization method to solve it. 2. Related Work 2.1. Panorama Generation Diffusion models (DMs) have shown promising results across various generative tasks. In particular, several approaches [1, 4, 6, 13, 14, 26, 33, 38] have proposed leveraging pretrained DMs to synthesize panoramic images. For example, DiffCollage [38] reconstructs complex factor graphs and aggregates intermediate output from DMs defined by nodes to generate panorama. PanoGen [14] utilizes latent diffusion models combined with recursive outpainting to create indoor panoramic images. MultiDiffusion [1] frames the problem of panoramic image generation from pretrained DMs as an optimization process to produce globally consistent images. SynDiffusion [13] builds on this idea and incorporates the LPIPS score [39] between neighboring denoised images into the optimization process. StitchDiffusion [26] further proposes averaging the overlapping denoising predictions and fine-tuning low-rank adaptation (LoRA) module [9]. To improve the efficiency of the generation process, SpotDiffusion [6] shifts non-overlapping denoising windows over time to synthesize coherent panorama in an efficient manner. All of these methods generate panoramas purely from text prompt and cannot incorporate an input image into the generation process. In contrast to these approaches, PanoDiffusion [30] is designed to generate panoramas from masked input image. Similarly, MVDiffusion [4] can produce panorama from single image by stitching multiple images using pixel-wise correspondences and attention modules. However, both of these approaches require training and struggle to generalize to diverse scenarios. 2.2. View synthesis from single input image Numerous methods have been proposed to synthesize scenes from single input image. One category of these methods [10, 12, 17, 22] addresses this problem in modular manner, decomposing the synthesis process into several independent components. For example, Shih et al.[22] estimate layered depth image (LDI) representation to reconstruct novel views. Niklaus et al.[17] use the estimated depth to map the input image to point cloud and train network to fill in the missing areas. The second group of methods [15, 24, 25, 42] synthesizes scenes from single input image in an end-to-end manner. Among these approaches, Zhou et al.[42] propose synthesizing scenes by first estimating optical flow and then warping the input image to novel views. Srinivasan et al.[24] use two sequential convolutional neural networks to estimate disparity and refine the warped images. Several approaches propose synthesizing intermediate scene representations to achieve view synthesis. For example, SynSin [28] estimates point cloud of scene, and several methods [15, 25] synthesize light fields using the estimated Multiple Plane Image (MPI) representation. PixelNeRF [34] trains NeRF prior and can synthesize NeRF from single input image without performing test-time optimization. Additionally, several approaches [2, 19] focus on improving the view-dependent effects for single-view view synthesis. However, all of these methods are designed only for view synthesis within narrow viewing angle or 2 restricted camera movement and cannot be generalized well to the entire 360 scene. 2.3. 3D Scene Generation Reconstructing an entire 3D scene is challenging problem, as it requires maintaining both content and depth consistency across wide range of camera trajectories. Many approaches have been proposed to achieve 3D scene generation, typically leveraging pretrained, powerful 2D diffusion priors, such as latent diffusion models (LDMs), to synthesize 3D scenes by optimizing different 3D representations, such as NeRF and 3DGS. These approaches can be categorized into two groups based on the input condition. The first group of methods [3, 5, 18, 23, 3537] generates 3D scenes from text or images in progressive manner. Starting from single image, either provided by the user or generated from text prompt, these methods typically perform progressive inpainting, monocular depth estimation, and 3D optimization for novel views in the 3D scene. These approaches differ in their 3D representation, image inpainting, and depth refinement strategies. However, since the 3D scene is generated through progressive inpainting of single inputs, these methods struggle to preserve coherency, making it difficult to synthesize consistent 360 scenes. The second group of methods [16, 40, 41] generates 360 3D scenes in two-step process. They synthesize coherent panoramas by leveraging pretrained text-to-panorama DMs, which are then lifted to 3D using different inpainting and depth estimation strategies. Although these approaches are capable of generating consistent 3D scenes from single inputs, they are text-conditioned only and do not have any mechanism to reconstruct scene consistent with single input image. In comparison, our method, PanoDreamer, not only generates coherent 3D scenes but also allows users to condition the generation on any single input image. 3. Preliminaries In this section, we describe MultiDiffusion [1], an approach that leverages pre-trained diffusion model, without any fine-tuning, to produce results in various image or condition spaces. For example, this technique can generate outputs at resolutions different from the base models native resolution (e.g., panoramas) or synthesize images using region-based text prompts. Here, we focus our discussion on the former example, as it is most relevant to our approach. MultiDiffusion uses pre-trained diffusion model, Φ, which operates on images of size as the base model. Starting with an image IT initialized with Gaussian noise and conditioned on text prompt p, the base model iteratively denoises IT , producing sequence of intermediate images IT 1, , I1 and ultimately generating clean image I0 as follows: It1 = Φ(Itp). (1) The goal of MultiDiffusion is to leverage this base model to generate an image at larger resolution . The MultiDiffusion process, similarly, begins with noisy high-resolution image, JT , and produces clean image J0 through sequence of gradually denoised images JT 1, , J0. At each time step, given the optimal highresolution image , the key idea is to ensure that the next step, Jt1, aligns closely with the output of the base diffusion model. Specifically, the desired Jt1 ensures that Fi(Jt1) Φ(Fi(J )p), where Fi is an operator that maps the high-resolution image space to the base models space (via cropping, in this case). Enforcing this similarity in the L2 sense, we arrive at the following objective: t1 = arg min (cid:88) i=1 Wi [Fi(J) Φ(Fi(J )p)]2 , (2) where Wi is weight map (Wi = 1 in this case) and denotes the element-wise product. Since this objective is quadratic, the solution can be easily obtained in closed form as follows: t1 = (cid:88) i=1 (Wi) 1 i=1 1 (Wi) (cid:80)n 1 (Φ(Fi(J )p)), (3) where 1 is the inverse of the cropping operator, which places the content into appropriate location in the highresolution image. Starting from the noisy high-resolution image = JT , MultiDiffusion uses this process to obtain the optimal intermediate high-resolution images , resulting in the final clean image 0 . 4. Algorithm Given single input image I, our goal is to reconstruct coherent 360 scene using 3D Gaussian representation [11]. Unlike existing methods that produce the 3D scenes through progressive projection and inpainting, we begin by generating coherent 360 panorama from the input image (Sec. 4.1). We then estimate coherent and consistent depth from the generated panorama (Sec. 4.2). Finally, we inpaint the occluded regions using layered depth image (LDI) inpainting and use the inpainted layers to reconstruct 3DGS representation (Sec. 4.3). 4.1. Single-Image Panorama Generation We begin by discussing the problem of generating larger image from single input image, then explain the specific details for panorama generation. Given an input image placed on larger canvas of size , our goal is 3 Figure 2. We compare the results of our MultiConDiffusion process against MultiDiffusion and progressive inpainting. The green bar shows the location of the input image. to fill in missing areas in using an inpainting diffusion model Φ, which operates on fixed lower-resolution images of size . In addition to text prompt p, this model takes mask denoting the missing regions and masked image (1 ) as inputs. It progressively denoises Gaussian noise image IT to obtain clean image I0 containing the hallucinated details, with each step following It1 = Φ(Itp, M, (1 ) X). straightforward approach is to use this model to gradually outpaint the highresolution image, starting from the regions covered by the input. However, this approach often results in noticeable contextual inconsistencies and seams, as shown in Fig. 2. Inspired by MultiDiffusion, we address this issue by formulating the problem as an optimization task. Similar to MultiDiffusion, we aim to optimize sequence of iteratively denoised high-resolution images JT 1, . . . , J0, beginning with the noisy image JT . To construct our objective, we first discuss our two desired properties. First, given the optimal denoised high-resolution image at the current time , the image at the next time Jt1 should closely approximate the output of the base inpainting model, i.e., Fi(Jt1) Φ(Fi(J )p, Mij, Mij Fi(L)). This property is analogous to that in MultiDiffusion, but with the distinction that here, the similarity holds not only for random crops defined by Fi but also for random inpainting masks within each crop, Mij. The first property alone is insufficient for producing high-quality results, as shown in Fig. 2 (MultiDiffusion). Unlike MultiDiffusion, our approach conditions the process on masked image, Mij Fi(L). Starting with fixed initial (missing content everywhere except in the input), the inpainting model must hallucinate entire patches when Fi does not overlap with the input region, leading to disconnection from the original input. To address this, we treat as parameter to be optimized during our process. To Figure 3. We show the results of MultiConDiffusion during different iterations of the optimization. set up the objective, we consider what would be ideal for in this context. If the clean high-resolution image J0 were available, the best possible condition for guiding the diffusion process to reach J0 would be to set to J0. Armed by this observation, we establish our second property: ensuring that J0. By formalizing these two properties in the L2 sense, we arrive at the following objective: J0 JT 1, = arg min J0JT 1,L where (cid:34) 1 (cid:88) L(Jt1J ) + J02 (cid:35) t=T (4) (cid:88) L(JJ ) = (cid:88) Mij [Fi(Jt1) Φ(Fi(J )Cij)] 2, j=1 i= and Cij = {p, Mij, Mij Fi(L)}. (5) (6) Here, the first term ensures that the cropped image at the next time step, Fi(Jt1), is similar to the output of the base inpainting diffusion model in the masked regions Mij. The second term simply forces the condition image to be close to the clean high-resolution image J0. Note that our highresolution diffusion process, called MultiConDiffusion, is also conditional. The output of this process, JT 1, . . . , J0, depends on the condition image L. As such, is the optimal solution at time given the current condition image L, and it is different from the final optimal solution, Jt, which is obtained given the optimal condition image L. Simultaneously solving for all the images in this objective is difficult task. Therefore, we propose an alternating minimization strategy that solves for JT 1, . . . , J0 and in the following two stages: Stage 1: Here, we fix and minimize Eq.4 by finding the optimal JT 1, . . . , J0. Since JT 1, . . . , J1 do not influence the second term (different steps are assumed to be independent), we can simply use Eq.3 to obtain their solution in closed form. The main difference here is that the base diffusion model is replaced with the inpainting model, and Wi = Mij. On the other hand, since J0 appears in both terms and they are both quadratic with respect to it, the final solution is weighted combination of the solution to the first term (Eq. 3) and the second term (J 0 = L). In practice, however, we found that plausible results can still be obtained even when ignoring the second term. Stage 2: During this stage, we fix JT 1, . . . , J0 and find the optimal that minimizes Eq.4. influences both the first term, as the diffusion model is conditioned on it (see Eq. 6), and appears in the second term. The optimal solution to the first term is = L. The idea is that if was used to produce the current JT 1, . . . , J0, it is likely the best option for reproducing the same results. On the other hand, since the second term is quadratic, the solution is simply = J0. Although obtaining the solution to each term is straightforward, computing the optimal solution considering both terms is difficult. However, assuming that and J0 are close to each otheri.e., MultiConDiffusion does not diverge significantly from the condition image in one passit is reasonable to assume that = J0 is close to the optimal solution for both terms. We perform the optimization by first initializing JT with Gaussian noise and by progressively inpainting the image. We then alternate between stages 1 and 2 iteratively until convergence. At the end of this process, we can use either J0 or as the final result. Fig. 2 compares MultiConDiffusion with MultiDiffusion and progressive inpainting, while Fig. 3 shows the results of our approach across different iterations of the optimization process. 4.1.1. Panorama Generation Details We slightly modify the MultiConDiffusion process to adapt it for generating panoramas from single image. Our goal is to produce cylindrical panorama, so in this case, MultiConDiffusion operates in the cylindrical domain, and the sequence JT , . . . , J0 is defined within this domain. Since the base diffusion model operates on perspective images, Fi performs both cropping and projection from the cylindrical to the perspective domain. Similarly, 1 projects the pixels from the perspective image back to the cylindrical image, placing them in the appropriate locations. Figure 4. We compare the result of our method, PanoDepthFusion, against applying Depth Anything V2 (DA V2) [32] on the full image. The results obtained by DA V2 lacks details and is geometrically inconsistent. Our approach, on the other hand, produces highly detailed and consistent depth maps. Figure 5. On the top left, we show the result of averaging the patch depth estimates. As seen, since the depth maps are relative, the depth from different patches are not consistent, producing results with clear edges. Since we initialize Gθi with identity line, patchwise average results are indeed our initial depth estimate during the optimization of Eq. 7. We also show our results after one, two, and four iterations of optimization. After only four iterations, the seems disappear. We experimented with bilinear interpolation during the projection; however, interpolation smoothed out the noise, which negatively affected the performance of the diffusion model. Therefore, we instead use nearest neighbor interpolation for both Fi and 1 . Additionally, we use an FOV of 45 for the perspective camera and carefully set the resolution of the cylindrical image to ensure rough one-to-one mapping between the pixels of the cylindrical and perspective images. This process allows us to produce contextually coherent and seamless 360cylindrical panorama, which we use to reconstruct the 3D scene. In our experiments, we apply 20 iterations of MultiConDiffusion (Stage 1 + Stage 2) to obtain the final cylindrical panorama. 4.2. Panorama Depth Estimation Given the panoramic image J0, our goal is to estimate its depth D. In recent years, several powerful monocular depth estimation methods [31, 32] have been introduced. These approaches can estimate highly detailed relative depth but typically perform best at specific image size. Beyond this optimal resolution, they often produce results that lack detail and geometric consistency. Consequently, applying 5 these methods directly to panorama depth estimation leads to poor results, as shown in Fig. 4. We address this problem by obtaining through combination of estimated depth maps on patches using an existing technique, Ψ, i.e., Ψ(Fi( J0)). However, naıvely combining the patches (e.g., through averaging) leads to unsatisfactory results (see Fig. 5), as the patch depth estimates are relative and can be inconsistent. To overcome this, we pose the problem of obtaining panoramic depth from patch depth estimates as an optimization task. Our key insight is that the panoramic depth DJ should be close to the estimated depth after it has been globally aligned through parametric function. This can be formally written as: D, θ = arg min D,θ (cid:88) i=1 Fi(D) Gθi(Fi( J0))2, (7) where Gθi is the parametric function, and θ = {θ1, . . . , θn} represents the set of parameters for different patches. In our implementation, we use piecewise linear function, where each parameter consists of series of scale and shift values. Solving for both and θ simultaneously is challenging. Therefore, we propose performing this optimization through alternating minimization, consisting of two stages. In the first stage, we fix θ and find the optimal D. Since the objective is quadratic, the solution can be obtained in closed form, similar to Eq. 3. The only difference is that Φ, the diffusion model, is replaced with Ψ, and Wi = 1. In the second stage, we fix and find the optimal θ. This is least-squares regression problem, which can be solved using standard packages. Starting with all Gθi as the identity line (i.e., linear function with slope of 1), we alternate between the first and second stages iteratively until convergence (four iterations in our implementation). Once converged, we obtain coherent and consistent panoramic depth, as shown in Figs. 4 and 5. 4.3. Inpainting and Optimization The estimated depth can now be used to project the cylindrical image into 3D space. However, when the scene is viewed from any position other than the panoramas center of projection, occluded regions become visible. To address this, we utilize the layered depth image (LDI) inpainting approach by Shih et al. [22], which performs effective depth-aware texture inpainting while also providing the corresponding depth. We use four-layer LDI representation (foreground, background, and two intermediate layers) based on agglomerative clustering by disparity. We then use these cylindrical layered images and depth maps to initialize set of 3D Gaussians. Specifically, we assign Gaussian to each pixel of the image at each layer and project them into 3D according to the corresponding Figure 6. We compare the panoramas generated by MultiConDiffusion with those from other methods. Other approaches often result in sharp discontinuities and contextual inconsistencies. For instance, in the top example, the MultiDiffusion result shows mismatch between the generated sky and the input sky. depth. The color of each Gaussian is initialized based on the pixel color (without using spherical harmonics); we initialize the rotation matrix with identity, assign the scale following Paliwal et al. [20], and set the opacity to 0.5. During this process, we keep track of which Gaussians correspond to which layer, as this information is required for optimization. Next, we perform 3DGS optimization for 1000 iterations. To do this, we set up 240 evenly rotation cameras 6 Table 1. Numerical comparison of MultiConDiffusion against other panorama generation approaches. Q-IQA denotes CLIP-IQA+ quality, Q-Align denotes Q-Align quality, A-Align denotes Q-Align aesthetic score, A-Clip denotes CLIP aesthetic score, C-Clip denotes CLIP consistency, and C-Style denotes style consistency. Method Progressive MultiDiffusion [1] SyncDiffusion [13] MultiConDiffusion (ours) Q-IQA Q-Align A-Align A-Clip C-Clip C-Style (103) 0.520 0.523 0.535 0.530 4.164 4.39 4.290 4. 3.314 3.516 3.429 3.696 5.779 5.953 5.893 5.992 0.896 0.900 0.898 0.902 0.881 1.23 0.798 0.793 Table 2. Evaluation on rendered 3D scene. The evaluation metrics of 3D scene are the same as panorama evaluation. Method LucidDreamer WonderJourney [36] PanoDreamer (ours) Q-IQA Q-Align A-Align A-Clip C-Clip C-Style 0.495 0.504 0.443 2.911 3.506 3.305 2.705 2.834 2.772 5.253 5.368 5.673 0.848 0.820 0. 0.058 0.058 0.025 Given this optimized 3DGS representation, we can synthesize novel views of the scene and produce coherent and seamless results. 5. Results In this section, we compare our approach against state-ofthe-art panorama generation and 3D scene generation methods, both visually and numerically. We demonstrate that our method generates more coherent panoramas and 3D scenes than other approaches. For evaluation, we compile test set of 28 real and synthetic scenes sourced from LucidDreamer [3] and WonderJourney [36]. For numerical evaluation, we employ the following three metrics: (1) Quality metrics. We assess the quality of generated panorama and 3D scenes using CLIP-IQA+ [27] and Q-Align [29] score. CLIP-IQA+ and Q-Align are built upon Contrastive Language-Image Pre-training (CLIP) [21] and large multi-modality models (LMMs) for image quality assessment, respectively. (2) Consistency metrics. To assess the consistency of the generated panoramas and 3D scene, we use CLIP and style metrics, where we compute the CLIP embedding similarity and style loss [7] between random pairs of non-overlapping window images in the panorama and 3D scene. (3) Aesthetic score. Similar to WonderWorld[35], we also evaluate the aesthetic score using CLIP aesthetic score and Q-Align aesthetic score. 5.1. Panorama Reconstruction Comparisons In Fig. 6, we show visual results of MultiConDiffusion, and compare our method against SyncDiffusion [22], MultiDiffusion [1] and vanilla progressive inpainting using DMs. Note that, since these images are not as wide as cylindrical panoramas, we perform the optimization for 15 iterations instead of 20. As seen, progressive inpainting generates panoramic images with noticeable seams and strong Figure 7. We show the results of our approach on the same input image across multiple runs. As shown, our approach produces diverse yet consistent results. from the center of projection and project the layered images and depth maps to these cameras. During the optimization, we randomly sample one of these cameras and optimize the Gaussians according to their corresponding layer. Additionally, we composite all the four layers and use the composited image as reference to optimize all the Gaussians. We use the original 3DGS reconstruction loss along with an L2 loss between the rendered and layered depth maps. In addition, to be able to produce consistent results from novel views, we use the depth-based novel view loss, proposed by Zhu et al. [43]. 7 Figure 8. We compare renderings of PanoDreamer with LucidDreamer [3] and WonderJourney [36]. The input image and camera are at the leftmost column. For each methods, we render 3D scene from two novel views (orange and green cameras). As is shown in the figure, LucidDreamer and WonderJourney results are inconsistent, suffering severe seam-artifacts for novel views. In comparison, PanoDreamer is capable of generating coherent renderings from novel views. For more visual results and video comparison, please refer to our supplementary materials. inconsistency. The discontinuity improves in MultiDiffusion and SyncDiffusion, but they still struggle to preserve good contextual consistency across the entire panoramas. In contrast, MultiConDiffusion can generate coherent and seamless panoramas that are significantly better than other approaches. In addition, given the same input image, MultiConDiffusion can synthesize diverse and coherent results across multiple run, as is shown in Fig. 7. Table 1 show the numerical comparison of MultiConDiffusion with other methods. We evaluate on 28 scenes provided by WonderJourney [36] and LucidDreamer [3]. Surprisingly, although our system does not incorporate image quality or aesthetics constraints, MultiConDiffusion can synthesis panorama with better quality and aesthetic score than other approaches. More importantly, panoramas generated by MultiConDiffusion show better consistency, aligning with our observation in Fig. 6 and demonstrating the effectiveness of our approach. 5.2. 3D Scene Reconstruction Comparisons Fig. 8 shows the visual comparison of PanoDreamer against LucidDreamer [3] and WonderJourney [36]. We set up their methods on the official code released by the authors, and use exactly the same training cameras as ours for fair comparison. As is shown in the figure, neither LucidDreamer nor WonderJourney is capable of synthesizing coherent 360 3D scene from single input image. Their results suffer strong seam artifacts when the camera moves back to the beginning location. In contrast, PanoDreamer can synthesize consistent and seamless scene at any novel views. Numerical results are shown in Table 2. The image quality and aesthetics of PanoDreamer results is slightly worse than WonderJourney and LucidDreamer, yet our consistency is significantly better. We argue that the rendering quality is more influenced by the consistency of the entire scene than the quality of each individual frame. For example, in rendering video with 360 rotated camera, results of PanoDreamer appears to be coherent through the entire 360 scene without noticeable seam or content inconsistency, which is significantly better than other approaches. Please refer to our supplementary materials for video comparison and more visual results. 6. Conclusion In conclusion, we have presented novel method for generating 360 3D scenes from single input image. Our approach first generates panoramic image along with its corresponding depth map. After inpainting occluded regions, these images are used to optimize 3DGS representation from which novel views can be rendered. To create coherent and globally consistent panorama, we frame the task as an optimization problem with two terms, solving it effectively through an alternating minimization strategy. Additionally, we pose the problem of estimating panorama depth using an existing monocular depth estimation method as an optimization and address it with alternating minimization. Extensive experiments show that our approach outperforms state-of-the-art methods in both panorama generation and reconstructed 3D scenes."
        },
        {
            "title": "Acknowledgements",
            "content": "The project was funded by Leia Inc. (contract #415290). Portions of this research were conducted with the advanced computing resources provided by Texas A&M High Performance Research Computing."
        },
        {
            "title": "References",
            "content": "[1] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. 2023. 2, 3, 7 [2] Juan Luis Gonzalez Bello and Munchurl Kim. Novel view synthesis with view-dependent effects from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1041310423, 2024. 2 [3] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free genarXiv preprint eration of 3d gaussian splatting scenes. arXiv:2311.13384, 2023. 1, 3, 7, 8 [4] Zijun Deng, Xiangteng He, Yuxin Peng, Xiongwei Zhu, and Lele Cheng. Mv-diffusion: Motion-aware video diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pages 72557263, 2023. 2 [5] Paul Engstler, Andrea Vedaldi, Iro Laina, and Christian Rupprecht. Invisible stitch: Generating smooth 3d scenes with depth inpainting. arXiv preprint arXiv:2404.19758, 2024. 3 [6] Stanislav Frolov, Brian Moser, and Andreas Dengel. Spotdiffusion: fast approach for seamless panorama generation over time. arXiv preprint arXiv:2407.15507, 2024. 2 [7] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. [8] Lukas Hollein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d In Proceedings of meshes from 2d text-to-image models. the IEEE/CVF International Conference on Computer Vision, pages 79097920, 2023. 1 [9] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2 [10] Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin, Dominik Kaeser, William Freeman, David Salesin, Brian Curless, et al. Slide: Single image 3d photography with soft layering and depth-aware inpainting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12518 12527, 2021. 1, 2 [11] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2, 3 [12] Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge, Yangming Chong, Josh Patterson, JanMichael Frahm, Shu Wu, Matthew Yu, et al. One shot 3d photography. ACM Transactions on Graphics (TOG), 39(4): 761, 2020. 1, 2 [13] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. Advances in Neural Information Processing Systems, 36:5064850660, 2023. 2, [14] Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. Advances in Neural Information Processing Systems, 36:2187821894, 2023. 2 [15] Qinbo Li and Nima Khademi Kalantari. Synthesizing light field from single image with variable mpi and two network fusion. ACM Trans. Graph., 39(6):2291, 2020. 1, 2 [16] Wenrui Li, Yapeng Mi, Fucheng Cai, Zhe Yang, Wangmeng Zuo, Xingtao Wang, and Xiaopeng Fan. Scenedreamer360: Text-driven 3d-consistent scene generation with panoramic gaussian splatting. arXiv preprint arXiv:2408.13711, 2024. 1, 3 [17] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from single image. ACM Transactions on Graphics (ToG), 38(6):115, 2019. 1, 2 [18] Hao Ouyang, Kathryn Heal, Stephen Lombardi, and Tiancheng Sun. Text2immersion: Generative immersive scene with 3d gaussians. arXiv preprint arXiv:2312.09242, 2023. 1, 3 [19] Avinash Paliwal, Brandon Nguyen, Andrii Tsarov, and Nima Khademi Kalantari. Reshader: View-dependent highlights for single image view-synthesis. ACM Transactions on Graphics (TOG), 42(6):19, 2023. [20] Avinash Paliwal, Wei Ye, Jinhui Xiong, Dmytro Kotovenko, Rakesh Ranjan, Vikas Chandra, and Nima Khademi Kalantari. Coherentgs: Sparse novel view synthesis with coherent 3d gaussians. In European Conference on Computer Vision, pages 1937. Springer, 2025. 6 [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 7 [22] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin Huang. 3d photography using context-aware layered depth In Proceedings of the IEEE/CVF Conference inpainting. on Computer Vision and Pattern Recognition, pages 8028 8038, 2020. 1, 2, 6, 7 [23] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199, 2024. 1, 3 [24] Pratul Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi, and Ren Ng. Learning to synthesize 4d rgbd light field from single image. In Proceedings of the IEEE International Conference on Computer Vision, pages 2243 2251, 2017. 1, 2 [25] Richard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 551560, 2020. 1, [26] Hai Wang, Xiaoyu Xiang, Yuchen Fan, and Jing-Hao Xue. Customizing 360-degree panoramas through text-to-image In Proceedings of the IEEE/CVF Windiffusion models. ter Conference on Applications of Computer Vision, pages 49334943, 2024. 2 9 IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2 [40] Haiyang Zhou, Xinhua Cheng, Wangbo Yu, Yonghong Tian, and Li Yuan. Holodreamer: Holistic 3d panoramic arXiv preprint world generation from text descriptions. arXiv:2407.15187, 2024. 1, 3 [41] Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari, Tejas Bharadwaj, Suya You, Zhangyang Wang, and Achuta Kadambi. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, pages 324342. Springer, 2025. 1, 3 [42] Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei Efros. View synthesis by appearance flow. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 286301. Springer, 2016. 1, [43] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian In European Conference on Computer Vision, splatting. pages 145163. Springer, 2024. 7 [27] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 25552563, 2023. 7 [28] Olivia Wiles, Georgia Gkioxari, Richard Szeliski, and Justin Johnson. Synsin: End-to-end view synthesis from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74677477, 2020. 2 [29] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, et al. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. arXiv preprint arXiv:2312.17090, 2023. 7 [30] Tianhao Wu, Chuanxia Zheng, and Tat-Jen Cham. Panodiffusion: 360-degree panorama outpainting via diffusion. In The Twelfth International Conference on Learning Representations, 2023. 2 [31] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. 2, [32] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv preprint arXiv:2406.09414, 2024. 5 [33] Weicai Ye, Chenhao Ji, Zheng Chen, Junyao Gao, Xiaoshui Huang, Song-Hai Zhang, Wanli Ouyang, Tong He, Cairong Zhao, and Guofeng Zhang. Diffpano: Scalable and consistent text to panorama generation with spherical epipolaraware diffusion. arXiv preprint arXiv:2410.24203, 2024. 2 [34] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. 2 [35] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Interactive 3d arXiv preprint Freeman, and Jiajun Wu. Wonderworld: scene generation from single image. arXiv:2406.09394, 2024. 3, 7 [36] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: In Proceedings of Going from anywhere to everywhere. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. 1, 7, 8 [37] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 2024. 1, 3 [38] Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, and Ming-Yu Liu. Diffcollage: Parallel generation of large content with diffusion models. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1018810198. IEEE, 2023. [39] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the"
        }
    ],
    "affiliations": [
        "Leia Inc.",
        "Max Planck Institute for Informatics",
        "Texas A&M University"
    ]
}