{
    "paper_title": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion",
    "authors": [
        "Yi Zhou",
        "Xuechao Zou",
        "Shun Zhang",
        "Kai Li",
        "Shiying Wang",
        "Jingming Chen",
        "Congyan Lang",
        "Tengfei Cao",
        "Pin Tao",
        "Yuanchun Shi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/."
        },
        {
            "title": "Start",
            "content": "Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion Yi Zhou, Xuechao Zou, Shun Zhang, Kai Li, Student Member, IEEE, Shiying Wang, Jingming Chen, Congyan Lang, Tengfei Cao, Pin Tao, and Yuanchun Shi, Fellow, IEEE 1 5 2 0 2 8 2 ] . [ 1 5 3 0 3 2 . 2 1 5 2 : r AbstractSemi-supervised remote sensing (RS) image semantic segmentation offers promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, stable semi-supervised RS segmentation framework that synergistically fuses priors from visionlanguage models and self-supervised models. Specifically, we construct heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/. Index TermsSemi-supervised learning, remote sensing images, semantic segmentation, and vision foundation models. I. INTRODUCTION"
        },
        {
            "title": "R EMOTE sensing imagery has become indispensable for a",
            "content": "wide range of Earth observation applications, including land-cover mapping, urban planning, ecological monitoring, and disaster assessment [13]. With the rapid advancement and widespread deployment of high-resolution satellite and aerial sensors, remote sensing data now exhibit large spatial This work was partly supported by the Kunlun Talent and High-end Innovative and Entrepreneurial Talent program in Qinghai Province. Yi Zhou and Xuechao Zou have contributed equally to this work. Corresponding author: Shiying Wang. Yi Zhou, Shiying Wang, Jingming Chen, and Tengfei Cao are with the School of Computer Technology and Application, Qinghai University, Xining, China. Shiying Wang and Tengfei Cao are also with the Intelligent Computing and Application Laboratory of Qinghai Province, Qinghai University, Xining, China (e-mail: yizhou.cs@foxmail.com; wangshiying.qhu@foxmail.com; chenjingming.qhu@foxmail.com; caotf@qhu.edu.cn). Xuechao Zou, Shun Zhang, and Congyan Lang are with the Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China (e-mail: xuechaozou@foxmail.com; xiaoshun3238@gmail.com; cylang@bjtu.edu.cn). Kai Li, Pin Tao, and Yuanchun Shi are with the Department of Computer Science and Technology, Tsinghua University, Beijing, China. Pin Tao and Yuanchun Shi are also with the Key Laboratory of Pervasive Computing, Ministry of Education (e-mail: tsinghua.kaili@gmail.com; taopin@tsinghua.edu.cn; shiyc@tsinghua.edu.cn). Fig. 1: The radar chart compares the mIoU(%) of different semi-supervised semantic segmentation methods across six remote sensing datasets under the 1/8 labeled ratio. Co2S consistently maintains leading performance across all benchmarks. coverage [4], complex scene layouts [5], and fine-grained category structures [6]. Consequently, pixel-wise semantic segmentation has emerged as core task, requiring models to be simultaneously sensitive to high-level semantics (e.g., category identity and contextual relationships) and low-level details (e.g., object boundaries and small structures [7]). Nevertheless, the fully supervised deep models that currently dominate this field heavily depend on exhaustive pixel-level annotations, which are notoriously expensive and time-consuming to obtain due to the need for expert interpretation [8]. In practice, only small fraction of available images can be labeled, while the vast majority remain unlabeled [9]. Under such conditions, there is pressing need for semi-supervised segmentation methods that can effectively exploit abundant unlabeled data, cope with label scarcity, and maintain stable optimization despite the presence of noisy pseudo-labels [10, 11]. To mitigate the reliance on massive annotations, recent research has made substantial strides in semi-supervised learning, evolving through three primary paradigms in the remote sensing domain: generative adversarial networks (GANs) [12 14], consistency-based methods [1519], and pseudo-labeling methods [2023]. Early GAN-based approaches, exemplified by Hung et al. [12] and Souly et al. [13], align feature distributions via adversarial learning but often suffer from training instability and convergence difficulties. Subsequently, consistency-based methods like FixMatch [18] and UniMatch [19] became mainstream by enforcing prediction invariance under perturbations; however, they remain susceptible to confirmation bias, where incorrect predictions are reinforced over time. Similarly, pseudo-labeling strategies like U2PL [21] and DWL [24] expand the training set with high-confidence predictions, yet they struggle with noise accumulation when the initial predictions are unreliable. Crucially, since these paradigms primarily rely on self-generated supervisory signals to guide training, they remain vulnerable to pseudo-label drift, particularly in complex remote sensing scenes. Without strong external guidance to rectify errors, these methods frequently encounter difficulties in distinguishing semantically similar categories or delineating precise object boundaries under severe label scarcity. To tackle these challenges, we propose Co2S, stable semisupervised remote sensing segmentation framework. We construct heterogeneous dual-student architecture that synergizes complementary priors derived from vision-language models and self-supervised models to mitigate pseudo-label drift. The framework is underpinned by an explicit-implicit semantic collaborative guidance mechanism and global-local feature collaborative fusion strategy. Collectively, these components establish stable mutual learning process, ensuring superior segmentation performance even under severe label scarcity. The contributions of this work are summarized as follows: 1) We propose stable dual-student learning framework for semi-supervised remote sensing segmentation, instantiated with two heterogeneous ViT-based vision foundation models. By leveraging distinct priors from pretrained CLIP and DINOv3, it enables reliable mutual learning and significantly alleviates pseudo-label drift. 2) We introduce an explicit-implicit semantic co-guidance mechanism that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, thereby jointly enhancing semantic consistency. 3) We develop global-local feature collaborative fusion strategy, effectively fusing the global contextual information captured by CLIP with the local details produced by DINOv3. This complementary fusion enables the model to generate highly precise segmentation results. Extensive experiments on six diverse remote sensing benchmarks, including WHDLD [25], LoveDA [26], Potsdam [27], GID-15 [28], MER, and MSL [29], demonstrate the effectiveness of Co2S. The results show that our framework consistently delivers leading performance across various partition protocols, outperforming existing semi-supervised methods, particularly in regimes with extremely scarce annotations. 2 II. RELATED WORK A. Remote Sensing Semi-Supervised Semantic Segmentation In the early exploration phase, generative adversarial networks were incorporated into the Semi-Supervised segmentation framework to serve as an auxiliary supervision branch, facilitating model optimization through the adversarial discrimination of prediction distributions. Souly et al. [13] leveraged generator network to produce fake images using class-level information as extra training examples, forcing real samples to cluster in the feature space to improve pixel classification. Focusing on the output space, Hung et al. [12] designed discriminator in fully convolutional manner to differentiate predicted probability maps from the ground truth, enabling the discovery of trustworthy regions in unlabeled data to facilitate effective semi-supervised learning. Subsequently, consistency-based methods gained prominence by enforcing prediction invariance under perturbations. Meanwhile, FixMatch [18] utilizes high-confidence predictions from weakly augmented images to supervise strongly augmented ones, providing simplified yet effective means to leverage unlabeled data. Building on this, UniMatch [19] introduces dual-stream input and feature perturbations to guide two strong views with common weak view, successfully exploring broader perturbation space for superior segmentation accuracy. Tailored for remote sensing, WSCL [30] employs strong data augmentation and adaptive reweighting to decouple self-biased predictions and alleviate overfitting. Similarly, MUCA [31] integrates multiscale uncertainty consistency and cross-teacher-student attention to construct discriminative feature representations, improving the distinction of highly similar objects. Parallel to consistency strategies, pseudo-labeling methods represent another active research direction, focusing on generating high-quality labels for self-training. U2PL [21] enhances data utilization by identifying unreliable pixels based on entropy and using them as negative samples. Addressing the specific challenges of noise and class imbalance in remote sensing, DWL [24] employs decoupled learning module alongside ranking-based weighting module to adaptively filter and balance pseudo-labels. Different from these methods that rely on homogeneous co-training architectures or standard teacher-student paradigms, Co2S introduces heterogeneous dual-student paradigm. By exploiting the complementary nature of distinct vision foundation models, our framework in conventional cobreaks the confirmation bias inherent training, offering an effective solution for pseudo-label drift. B. Semantic Segmentation with Vision Foundation Models Vision foundation models have marked paradigm shift in semantic segmentation, branching into vision-language models (VLMs) and self-supervised models (SSMs). VLMs [3236] leverage text-image alignment for transferable visual understanding. CLIP [32] utilizes large-scale contrastive pre-training to distill generic visual concepts, while methods like DenseCLIP [33] and MaskCLIP [34] adapt it for dense prediction tasks. In remote sensing, RemoteCLIP [35] aligns visual features of remote sensing images with textual descriptions to learn robust visual features with rich semantics. Parallelly, SSMs [3744] focus on extracting intrinsic spatial features, capturing hierarchical visual patterns without relying on manual annotations or textual supervision. The DINO family [37 39] employs self-distillation to learn high-quality dense features effectively, whereas RingMo [40] leverages generative self-supervised learning to specifically target dense and small objects in remote sensing scenes, Cloud-Adapter [45] proposes parameter-efficient tuning strategy that adapts DINOv2 [38] to the cloud segmentation without retraining the backbone. Building on these advancements, recent studies explore architectures incorporating both VLM and SSM components [4648]. Barsellotti et al. [48] proposed Talk2DINO, which aligns CLIPs textual embeddings with DINOs visual features via learned mapping, resulting in less noisy segmentations with improved foreground distinction. For remote sensing, Ye et al. [49] developed GSNet, employing query-guided feature fusion to integrate representations from generalist and specialist encoders to achieve more accurate mask predictions. Similarly, RSKT-Seg [46] aggregates multi-directional cost maps to capture rotation-invariant cues, thereby enhancing both inference efficiency and segmentation accuracy. Alternatively, shifting the focus to utilizing frozen representations, Zhang et al. [47] leveraged the static backbones of CLIP and DINO to generate high-quality pseudo-labels, enabling effective weakly supervised segmentation with reduced training costs. In similar vein, CLIP-DINOiser [50] applies DINOs spatial affinities to refine the dense MaskCLIP features, producing smoother segmentation outputs without the need for pixel-level annotations. However, these approaches primarily target open-vocabulary supervised or weakly supervised scenarios. The potential of synergizing CLIP and DINO within learning framework, specifically semi-supervised mutual to mitigate pseudo-label drift under label scarcity, remains unexplored. Our work bridges this gap by integrating their distinct priors to stabilize training when pixel-level annotations are severely limited. III. METHOD A. Stable Dual-Student Semi-supervised Learning Framework Co2S constructs dual-student architecture comprising two heterogeneous ViT-based vision foundation models initialized from CLIP [32] and DINOv3 [39], respectively. As illustrated in Fig. 2, the training set consists of labeled subset Dl = {(xl, y)} and an unlabeled subset Du = {xu}, where images xl, xu R3HW and ground truth labels R1HW . For labeled sample (xl, y), we generate weakly augl R3HW , which is fed into both stumented view xw dents. The CLIP-based student produces the global prediction Mglobal pred R1HW , and the DINOv3-based student produces pred R1HW . The supervised the local prediction Mlocal training objective is applied independently to each student: sup = CE(Mglobal Lglobal pred , y), Llocal sup = CE(Mlocal pred , y). (1) where CE(, ) represents the cross-entropy calculation, Lglobal and Llocal denote the supervised losses for the CLIP and sup DINOv3 students, respectively. sup 3 , xs1 For an unlabeled sample xu, we strictly adhere to the dual-stream perturbation paradigm of UniMatch [19]. We generate weakly augmented view xw , two strongly augu , and one feature-perturbed view xf , xs2 mented views xs1 , R3HW . Let pw RKHW where xw , xf , xs2 denote the probability map from the weak view. For each pixel at location (i, j), we derive the pseudo-label ˆyu,i,j = arg maxk(pw i,j), where ˆyu,i,j {1, . . . , K} and ci,j [0, 1]. To formulate the consistency regularization, we first define generic masked cross-entropy function between target pseudo-label map ˆy R1HW and prediction map RKHW : i,j) and the confidence score ci,j = maxk(pw H(p, ˆy, c) = (cid:88) i,j 1(ci,j τ ) CE(pi,j, ˆyi,j), (2) where 1() is the indicator function and τ is the confidence threshold. The final consistency loss Lct is calculated as weighted sum of the losses from the feature-perturbed view and the two strongly augmented views: 1 4 1 2 Lct = H(pf p, ˆyu, c) + (cid:0)H(ps1, ˆyu, c) + H(ps2, ˆyu, c)(cid:1), (3) where pf p, ps1, ps2 RKHW represent the predictions resulting from the feature perturbation and the two strong augmentation views, respectively."
        },
        {
            "title": "The total training objective is formulated as the weighted",
            "content": "sum of the supervised, consistency, and stability loss: = λsupLsup + λctLct + λstaLsta, (4) where we set λsup = λct = 0.5 in default. To prevent noisy interaction during the early training phase, λsta follows cosine ramp-up schedule. The detailed definition of stability loss Lsta is provided in Sec. III-C. B. Explicit-Implicit Semantic Collaborative Guidance To fully exploit heterogeneous pretraining, Co2S injects complementary semantic priors into the two students. The CLIPbased student receives explicit class-level guidance from frozen text encoder, while the DINOv3-based student is driven by implicit class-level guidance via learnable queries. For the CLIP-based student, we employ concept-based prompt ensembling strategy to improve semantic precision. For each semantic category k, we curate set of fine-grained concept descriptions Ck as defined in the dataset configurations. Each concept Ck is formatted using prompt template function (c) and processed by the frozen CLIP text encoder Etxt. We then aggregate these embeddings via averaging to obtain representative class prototype tk Rd1 as follows: tk = 1 Ck (cid:88) cCk (cid:0)T (c)(cid:1)."
        },
        {
            "title": "Etxt",
            "content": "(5) Subsequently, lightweight projection layer ϕ() maps these averaged embeddings into the decoder space to generate the final explicit query matrix Ft clip RKd1: Ft clip = ϕ([t1, . . . , tK]), (6) 4 Fig. 2: Overview of the proposed Co2S framework. It integrates CLIP-based student (top) using text embeddings for explicit semantic guidance and DINOv3-based student (bottom) using learnable queries for implicit guidance. For unlabeled data, the global-local collaborative fusion strategy enforces training stability by arbitrating supervision based on pixel-wise confidence. which provides explicit class-level guidance for all categories using language-derived semantics. In contrast, for the DINOv3-based student, we utilize learnable queries RKd2 to provide implicit classlevel guidance. These queries are implemented as trainable embedding layer and are optimized alongside the DINOv3 backbone and the segmentation decoder: = [q1, . . . , qK]. (7) clip"
        },
        {
            "title": "Let Fglobal",
            "content": "Unlike the fixed text embeddings Ft clip, these learnable queries flexibly adapt to the underlying visual distribution of the imagery to encode implicit class-level embeddings. Rd1hw and Flocal dino Rd2hw denote the visual features from the CLIP-based and DINOv3-based students, respectively. Both students employ homogeneous query-based decoder. The CLIP student produces the global prediction Mglobal pred and the DINOv3 student produces the local prediction Mlocal pred . The segmentation logits are computed as:"
        },
        {
            "title": "Mglobal",
            "content": "pred = Sim(Ft clip, Fglobal clip ), Mlocal pred = Sim(Q, Flocal dino ), (8) where Sim(, ) denotes dot-product similarity implemented with an einsum operator. This formulation maintains unified architectural structure while allowing the two students to leverage distinct semantic sources, namely explicit text embeddings versus implicit learnable queries. Although the CLIP text embeddings Ft clip and the DINOv3 in the forward pass, learnable queries are independent Fig. 3: Visualization of attention maps from different heads of the CLIP image encoder (a-c) and DINOv3 backbone (d-f). they become coupled during optimization via the stability loss Lsta. On mutually confident pixels, Lsta enforces consistency between the two predictions. Crucially, this mechanism enables the explicit class-level guidance from CLIP to rectify potential semantic category confusion in the DINOv3 student. Simultaneously, the implicit class-level guidance from DINOv3 reciprocally refines the coarse spatial localization of the CLIP student. Through this explicit and implicit semantic collaboration, Co2S effectively mitigates pseudo-label drift and stabilizes the dual-student learning process. C. Global-Local Feature Collaborative Fusion Conventional co-training frameworks [16, 17, 51] typically rely on identical network architectures with different random initializations. Despite the divergent starting points, these homogeneous models often converge to similar error patterns due to confirmation bias. To mitigate this error accumulation, Co2S introduces global-local feature collaborative fusion strategy. As visually evidenced in Fig. 3, the attention maps reveal distinct representational characteristics where CLIP produces diffuse activations rich in global contextual information while DINOv3 generates sharp responses highlighting local details. Building on this complementarity, our mechanism effectively fuses the global contextual information captured by CLIP [32] with the local details produced by DINOv3 [39]. This complementary fusion enables the model to generate segmentation maps with more accurate semantics and sharper, well-defined boundaries. Given an unlabeled image xu, its weakly augmented view is fed into the two students to obtain the global logits Mglobal pred and the local logits Mlocal pred . First, we compute the confidence maps Cglobal pred R1HW for the entire image: pred R1HW and Clocal k"
        },
        {
            "title": "Cglobal",
            "content": "pred )(cid:3) (cid:2)σ(Mlocal pred = max (cid:2)σ(Mglobal k, Clocal pred = max pred )(cid:3) k, (9) where σ() denotes the softmax operator over the class dimension. To perform pixel-wise optimization, let cglobal pred [0, 1] denote the scalar confidence values at specific spatial location from Cglobal pred respectively. The joint maxipred mum confidence for this pixel is defined as: pred , clocal and Clocal = max (cid:0)cglobal pred , clocal pred (cid:1). (10) We then build interaction masks based on three confidence scenarios. When both students are confident with cglobal pred τ and clocal pred τ , the lower-confidence student learns from the higher-confidence one. If their confidence is equal, they engage in mutual learning. When only one student is confident with τ and the other is below τ , the low-confidence student is supervised by the high-confidence peer. When both confidences are low with cglobal pred < τ , the pixel is ignored and does not contribute to stabilization. Formally, the stability loss is defined as: pred < τ and clocal Lsta(t) = λ(t) (cid:40)MSE(cid:0)pℓ, ph (cid:1), τ, 0, < τ, (11) where MSE(, ) represents the mean squared error calculation, pℓ, ph RK denote the probability vectors from the lower and higher-confidence students, τ is the confidence threshold, and λ(t) is ramp-up weight depending on the training step t. In the special case cglobal pred τ , symmetric mutual learning mechanism is applied. pred = clocal This pixel-wise fusion strategy bridges global contextual clues with local texture details based on pixel-wise confidence. By synergizing these distinct priors at the optimization level, the proposed strategy effectively compensates for limited supervision, yielding predictions that are both semantically stable and spatially precise even under scarce labels. TABLE I: Segmentation results (mIoU %) on the WHDLD dataset under different labeled ratios. The best and second-best results are highlighted in bold and underlined, respectively. 5 Method OnlySup FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) 1/ 1/16 53.6 56.0 57.1 56.8 57.4 56.5 56.5 61.1 55.2 57.6 58.3 58.6 58.8 57.0 58.2 61.5 1/8 58.0 59.8 59.9 59.8 60.4 57.1 60.0 62.2 1/ 60.5 60.8 61.1 61.1 61.5 58.9 60.5 62.6 IV. EXPERIMENTS A. Datasets We evaluate Co2S on six widely used remote sensing segmentation benchmarks, covering diverse range of sensors, spatial resolutions, geographic regions, and scene complexities. 1) WHDLD: WHDLD [25] comprises 4,940 optical images captured by GaoFen-1 and ZiYuan-3 satellites, each having size of 256256 pixels and 2-meter spatial resolution. It provides pixel-wise annotations for six categories: building, road, pavement, vegetation, bare soil, and water. 2) LoveDA: LoveDA [26] provides 5,987 images of size 1,0241,024 pixels with 0.3-meter resolution. Spanning urban and rural scenes, it annotates seven categories: background, building, road, water, barren, forest, and agriculture. 3) Potsdam: Potsdam [27] consists of 38 aerial orthophotos with 5-cm resolution and 6,0006,000 dimensions. It covers six classes: impervious surface, building, low vegetation, tree, car, and clutter. These images are cropped into 5,472 nonoverlapping 512512 patches for training and evaluation. 4) GID-15: GID-15 [28] originates from ten GaoFen-2 satellite images with approximate dimensions of 7,2006,800 pixels. It provides pixel-level annotations for 15 fine-grained categories, spanning industrial, residential, agricultural, and water regions. These large images are tiled into 3,420 nonoverlapping 512512 patches to facilitate large-scale landcover classification. 5) MER and MSL: Derived from HiRISE imagery for Mars terrain analysis [29], MER contains 1,023 grayscale images sized 1,0241,024, while MSL comprises 4,155 RGB images measuring 560500. They share nine distinct planetary surface categories: martian soil, sands, gravel, bedrock, rocks, tracks, shadows, unknown, and background. B. Implementation Details The proposed Co2S framework is optimized using the AdamW optimizer [52] with weight decay of 0.01 and polynomial learning rate decay schedule (power = 0.9). Structurally, both students utilize ViT-B/16 backbone [53] and employ decoders with identical architectures but independent parameters. Regarding data augmentation, we strictly follow the protocols established in UniMatch [19]. Specifically, labeled TABLE II: Per-class IoUs(%) and mIoU(%) results on the LoveDA dataset under different labeled ratios. The best and secondbest results are highlighted in bold and underlined, respectively. 6 Method OnlySup FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) Method"
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours)"
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours)"
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) 1/40 Labeled Images Background Building Road Water Barren Forest Agriculture mIoU 48.0 50.3 47.4 52.9 52.8 50.4 49.2 51.6 37.5 52.1 47.5 52.1 55.9 57.8 50.8 59.7 36.1 53.3 50.9 62.8 54.6 57.7 51.0 58.3 55.3 69.9 69.8 68.1 70.1 75.7 73.5 76. 27.3 28.7 30.0 31.2 30.2 36.4 33.9 35.9 53.2 56.4 56.5 57.7 59.3 58.5 58.9 61.5 57.1 62.3 63.8 63.8 65.5 65.1 64.7 64.2 45.9 53.3 52.3 54.1 55.5 57.4 54.6 58.2 1/16 Labeled Images Background Building Road Water Barren Forest Agriculture mIoU 50.8 55.5 53.5 53.8 55.7 52.7 52.6 54.0 51.0 58.4 58.3 55.8 59.2 60.6 54.1 62.2 49.5 55.4 56.6 55.2 57.0 59.1 54.5 59.0 65.9 72.2 72.6 71.6 72.4 73.7 75.0 76.3 33.5 37.4 38.0 37.5 37.7 36.0 34.1 40. 57.1 59.3 59.9 61.3 60.9 62.9 61.2 63.6 63.6 66.9 67.3 66.6 67.6 68.2 65.0 67.6 53.1 57.9 58.0 57.4 58.6 59.0 56.7 60.4 1/8 Labeled Images"
        },
        {
            "title": "Agriculture",
            "content": "mIoU 52.8 55.9 54.8 56.6 56.8 56.2 55.5 57.2 56.4 60.9 60.9 60.0 61.1 61.5 57.4 63.8 53.4 58.8 59.4 58.5 59.6 60.0 58.2 60.4 69.8 73.1 72.0 72.9 73.3 78.9 77.3 79.3 38.0 40.6 40.3 38.6 39.4 47.1 38.3 42. 59.7 62.3 61.6 61.1 61.7 66.6 65.4 66.7 66.2 69.2 68.4 68.0 68.3 69.4 69.2 69.2 56.6 60.1 59.6 59.4 60.0 62.8 60.2 62.7 1/4 Labeled Images"
        },
        {
            "title": "Agriculture",
            "content": "mIoU 55.7 56.9 57.4 57.6 58.0 56.5 55.9 57.0 58.8 61.9 61.5 61.3 61.1 61.8 61.8 63.4 57.9 60.6 60.6 59.3 60.8 60.6 61.1 61.0 72.8 73.8 74.6 74.3 74.2 79.3 78.0 79.1 42.3 43.3 42.9 43.4 44.6 44.5 44.8 48. 53.2 64.2 64.3 63.2 63.5 68.8 68.3 67.8 68.5 69.8 69.2 68.6 69.8 72.4 71.6 71.2 58.9 61.5 61.5 61.1 61.7 63.4 63.1 64.0 images and weak unlabeled views undergo basic geometric transformations, including random resizing (scale range 0.52.0) and horizontal flipping. strong unlabeled views are generated via an aggressive perturbation chain comprising color jittering, random grayscale conversion, Gaussian blur, and CutMix [54]. Furthermore, feature-space perturbation TABLE III: Segmentation results (mIoU %) on the Potsdam dataset under different labeled ratios. The best and second-best results are highlighted in bold and underlined, respectively. Method OnlySup FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) 1/32 1/16 61.5 69.4 67.2 69.9 70.7 74.2 66.9 74.3 64.9 71.5 71.3 72.3 72.6 78.3 71.5 76.6 1/ 70.2 73.5 73.8 73.9 74.8 79.8 76.0 79.8 1/4 73.8 75.7 75.9 75.4 76.4 80.3 79.2 80.2 is applied via dropout [55] (rate = 0.5) within the dualstudent forward mechanism. The unsupervised loss is guided by confidence-thresholded pseudo-labels (threshold = 0.95). Regarding the training schedule, each experiment runs for 80 epochs, with the exception of the MER dataset [29]. Due to its limited sample size, MER requires prolonged schedule of 150 epochs to mitigate noisy updates and ensure stable convergence. All experiments were conducted on workstation equipped with NVIDIA RTX 3090 GPUs. We utilize the mean intersection-over-union (mIoU) as the evaluation metric. C. Comparison with State-of-the-Art Methods We report comparison results across six widely used remotesensing benchmarks to evaluate the performance of the proposed Co2S comprehensively. We conduct extensive comparisons against the supervised baseline (OnlySup), which is trained exclusively on the labeled data, as well as several general semi-supervised baselines, including FixMatch [18], UniMatch [19], U2PL [21], and remote-sensing-specific approaches like WSCL [30], DWL [24], and MUCA [31]. 1) Quantitative Comparison and Analysis: The comprehensive comparisons across six diverse benchmarks (Tables I-VI) demonstrate that Co2S consistently achieves leading performance, particularly in regimes with extremely scarce annotations. On the WHDLD [25] dataset, Co2S outperforms the strong competitor UniMatch by 3.7% under the extremely sparse 1/24 split, demonstrating its capability to parse diverse land-cover features accurately. Similarly, on the LoveDA [26] dataset, the method yields remarkable 12.3% improvement over the supervised baseline at the 1/40 ratio, showing superior performance against significant urban-rural distribution shifts. For the ultra-high-resolution Potsdam [27] dataset, Co2S achieves the best performance at the 1/32 split with 74.3% mIoU, surpassing remote-sensing-specific designs like DWL. Extending to the large-scale GID-15 [28] dataset, our method outperforms UniMatch by 1.5% at the 1/8 split, indicating proficiency in handling diverse land-cover characteristics across vast spatial areas. Furthermore, in the extraterrestrial environments of MER and MSL [29], Co2S delivers the highest accuracy on these benchmarks characterized by extreme class imbalance. Notably, on MSL, it boosts the mIoU by 2.3% over the runner-up DWL at the 1/4 ratio, showing significant 7 gains in identifying challenging targets such as tracks where it exceeds the second-best UniMatch by 5.2%. Crucially, consistent trend observed across all benchmarks is that the performance advantage of Co2S over baselines amplifies as the labeled ratio decreases. This phenomenon validates the stability of our Co2S, confirming that the synergistic foundation model priors effectively prevent model collapse and sustain high accuracy even in the most extreme low-data annotation regimes. 2) Qualitative Visualization Analysis: To further substantiate these quantitative findings, we provide visual comparison of segmentation results across all six datasets in Fig. 4. As observed, baseline methods frequently struggle with semantic confusion in complex scenes. For instance, in the Potsdam dataset (Row f), UniMatch [19] and DWL [24] incorrectly classify large areas of impervious surface (white) as clutter (red). Similarly, in WHDLD (Row a), the vegetation (green) in the top-left corner is misidentified as water (blue) by most competitors. Moreover, our method demonstrates superior capability in handling fine-grained objects and texture details. In the MER dataset (Row c), baselines suffer from severe oversegmentation, generating excessive false positives for scattered small rocks (purple), whereas Co2S precisely suppresses this noise to match the Ground Truth. Likewise, in MSL (Row d), other methods tend to misclassify large areas of bedrock (blue) as gravel (olive green), while Co2S accurately preserves the correct material distribution. These visualizations confirm that our framework effectively harmonizes global semantic consistency with local structural precision across diverse and challenging remote sensing environments, successfully rectifying category confusion while preserving fine-grained details. 3) Pseudo-label Quality and Stability: To empirically verify our claim that Co2S effectively mitigates pseudo-label drift, we monitor the quality of pseudo-labels during the critical early training phase. The pseudo-label accuracy is calculated as the proportion of correctly classified pixels among those that pass the confidence threshold τ : Accuracy = (cid:80) 1(ˆyu = ygt) 1(c τ ) (cid:80) 1(c τ ) , (12) where ˆyu is the pseudo-label, ygt is the ground truth, and is the confidence score. The evolution curves for the first 10 epochs on the WHDLD dataset (1/24 split) are plotted in Fig. 5. The results reveal distinct training dynamics: 1) The supervised baseline (OnlySup) exhibits extreme volatility, oscillating drastically between 50% and 90%. This highlights the instability of learning from scratch with scarce annotations. 2) Standard methods like FixMatch [18] and UniMatch [18] show signs of early saturation. They quickly plateau at suboptimal accuracy level (80%-88%), suggesting that they are trapped by confirmation bias early in the training process and fail to correct initial errors. 3) In contrast, Co2S (red curve) demonstrates immediate semantic alignment. It surges to over 95% accuracy within the very first epoch and maintains this superior level with minimal variance. This rapid convergence and sustained stability (10% gap over baselines) indicate that Co2S successfully establishes high-quality supervision signal TABLE IV: Per-class IoUs(%) and mIoU(%) results on the MER dataset under different labeled ratios. The best and secondbest results are highlighted in bold and underlined, respectively. 8 Method OnlySup FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) Method"
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) 1/8 Labeled Images Martian Soil Sands Gravel Bedrock Rocks Tracks Shadows Unknown Background mIoU 4.7 2.0 5.2 3.5 5.5 5.9 5.3 3.0 55.1 53.6 51.8 54.8 54.4 51.0 56.0 56.3 71.4 73.5 73.2 69.9 72.7 74.9 72.6 73. 61.7 65.8 70.5 62.8 68.5 67.2 60.5 67.9 38.0 40.0 45.7 36.0 44.7 41.4 36.1 37.0 49.5 54.8 54.6 56.9 55.2 61.9 52.9 66.3 34.2 31.5 26.2 26.1 38.0 28.3 34.5 36.5 73.0 72.4 73.8 76.0 73.8 69.7 67.7 79.7 87.0 90.4 93.2 80.9 91.1 90.4 87.8 91. 52.8 53.8 54.9 51.9 56.0 54.5 52.6 56.8 1/4 Labeled Images Martian Soil Sands Gravel Bedrock Rocks Tracks Shadows Unknown Background mIoU 21.6 19.4 7.1 8.2 19.7 13.8 1.0 13. 56.3 55.4 56.4 52.8 54.6 55.9 55.1 63.3 74.4 74.0 73.7 71.1 74.3 74.8 73.8 76.1 66.3 57.1 69.7 65.5 69.3 70.3 65.6 69.4 38.0 43.6 41.9 35.7 44.5 36.2 38.4 37.1 52.3 57.3 60.8 61.0 54.9 57.0 56.3 63.1 22.1 39.8 23.3 31.4 37.4 28.8 28.6 41. 74.5 75.7 81.4 79.0 77.4 66.0 73.7 77.1 92.2 90.9 90.3 89.0 90.5 89.7 84.9 91.4 55.3 58.1 56.1 54.9 58.1 54.7 53.0 59.1 TABLE V: Per-class IoUs(%) and mIoU(%) results on the MSL dataset under different labeled ratios. The best and second-best results are highlighted in bold and underlined, respectively."
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours)"
        },
        {
            "title": "OnlySup",
            "content": "FixMatch [18] U2PL [21] WSCL [30] UniMatch [19] DWL [24] MUCA [31] [NeurIPS20] [CVPR22] [TGRS23] [CVPR23] [ISPRS24] [TGRS25] Co2S (Ours) 1/8 Labeled Images"
        },
        {
            "title": "Shadows Unknown Background mIoU",
            "content": "26.8 31.0 26.7 25.8 29.3 33.5 15.4 36.7 61.6 65.4 64.7 65.0 66.7 65.9 63.9 68.4 66.8 69.6 70.3 68.3 69.8 71.2 67.0 71.7 72.2 75.0 73.3 73.9 75.4 74.7 71.5 76.0 37.9 40.4 39.8 38.8 39.2 44.7 40.8 42.6 45.5 49.6 43.1 69.6 50.8 48.8 33.4 58. 37.0 39.8 40.3 39.6 43.0 44.8 41.0 44.1 49.1 54.2 53.1 57.0 57.9 56.3 55.9 60.9 85.1 86.5 87.7 86.0 89.2 90.9 82.9 89.3 53.6 56.8 55.5 58.2 57.9 59.0 52.4 60.9 1/4 Labeled Images"
        },
        {
            "title": "Shadows Unknown Background mIoU",
            "content": "32.2 29.7 36.1 30.9 32.2 33.8 33.7 36.0 65.0 65.5 62.7 64.7 67.7 68.2 69.3 67.7 69.0 69.9 72.7 70.4 72.6 72.3 72.9 71.8 75.1 77.1 76.3 75.5 77.0 76.1 76.9 77.8 43.6 42.5 43.3 42.0 42.5 43.2 46.2 45.5 57.6 63.8 63.7 58.3 71.8 57.9 40.4 77. 44.0 48.1 44.2 42.8 49.1 48.2 49.9 49.2 57.9 60.7 65.6 65.0 61.9 70.7 55.6 76.7 86.6 87.5 90.1 89.7 90.8 91.9 89.2 91.7 59.0 60.5 61.6 59.9 62.8 63.6 59.3 65.9 TABLE VI: Segmentation results (mIoU %) on the GID-15 dataset under different labeled ratios. The best and second-best results are highlighted in bold and underlined, respectively. Method OnlySup FixMatch [18] [NeurIPS20] U2PL [21] WSCL [30] UniMatch [19] [CVPR22] [CVPR23] [TGRS23] DWL [24] MUCA [31] [TGRS25] [ISPRS24] Co2S (Ours) 1/8 1/4 67.5 74.1 70.0 74.8 67.2 75.3 72.8 75. 73.9 75.9 72.6 76.9 67.4 72.6 75.4 77.7 Fig. 4: Visual comparison of semantic segmentation results on the six datasets under the 1/8 labeled ratio. from the onset, thereby effectively stifling pseudo-label drift at its source and ensuring reliable optimization. TABLE VII: Ablation study on the explicit and implicit semantic guidance mechanisms (ESG: Explicit Semantic Guidance; ISG: Implicit Semantic Guidance) on the WHDLD dataset. D. Ablation Studies and Analysis 1) Impact of Explicit and Implicit Semantic Guidance: To validate the efficacy of the proposed explicit-implicit semantic co-guidance mechanism, we analyze the individual contributions of text embeddings and learnable queries which provide explicit and implicit class-level guidance respectively. As detailed in Table VII, the baseline model without guidance achieves 58.97 mIoU. Implementing implicit class-level guidance via learnable queries alone results in slight performance drop to 58.86. This decline is attributed to the"
        },
        {
            "title": "ESG",
            "content": ""
        },
        {
            "title": "ISG",
            "content": "mIoU 58.97 58.86 60.77 61.09 fact that the learnable queries are randomly initialized parameters lacking pre-trained priors. Optimizing these queries from scratch typically requires substantial annotated data, whereas TABLE IX: Ablation study on the contribution of different optimization objectives on the WHDLD dataset (1/24)."
        },
        {
            "title": "Lsup",
            "content": ""
        },
        {
            "title": "Lct",
            "content": ""
        },
        {
            "title": "Lsta",
            "content": "mIoU 59.98 60.73 61.09 TABLE X: Ablation study of synergizing CLIP with different SSM priors on the WHDLD dataset (1/24). SSM MAE [41] BEiTv2 [42] iBOT [43] SimMIM [44] DINOv3 [39] mIoU 60.84 60.98 60.96 60.99 61.09 correcting each others mistakes, rendering them prone to coupled error accumulation and confirmation bias. In contrast, our proposed heterogeneous architecture combining CLIP and DINOv3 achieves the highest performance of 61.09 mIoU. This superiority confirms that the framework successfully synergizes the global semantic representations of CLIP with the local structural details of DINOv3. By arbitrating between these complementary feature streams, Co2S ensures both semantic correctness and boundary precision, thereby stabilizing the semi-supervised training under scarce data conditions. 3) Effectiveness of Optimization Objectives: To evaluate the individual contribution of each term in our overall training objective, we conduct an ablation study as listed in Table IX. The supervised baseline trained solely with supervised loss Lsup achieves 59.98 mIoU. Incorporating the consistency loss Lct boosts the performance to 60.73, the weak-to-strong constraints effectively mine useful supervisory signals from unlabeled data to augment model training. Furthermore, adding the proposed stability loss Lsta yields the best performance of 61.09 mIoU. This further improvement confirms that Lsta is essential for mitigating pseudo-label drift by actively arbitrating between the two students based on confidence, thereby ensuring more reliable mutual learning. indicating that 4) Synergizing CLIP with Different SSM Priors: To assess the advantage of heterogeneous model collaboration, we evaluate CLIP [32] paired with range of self-supervised models as well as with itself as homogeneous baseline. The baseline attains an mIoU of 60.78, representing the performance ceiling achievable by relying solely on vision-language priors. As reported in Table X, combining CLIP with any of the examined self-supervised models [39, 4144] yields performance gain over this baseline. The smallest improvement is 0.06 with MAE [41], while the largest improvement is 0.31 with DINOv3 [39]. This consistent uplift across all pairings confirms that complementary visual priors from diverse self-supervised paradigms substantially enhance mutually semantic interactive learning beyond what CLIP alone can deliver. Fig. 5: Evolution of pseudo-label accuracy during the early training phase (first 10 epochs) on the WHDLD dataset (1/24). TABLE VIII: Ablation study on the heterogeneity of the dualstudent architecture on the WHDLD dataset (1/24). Dual-student Architecture DINOv3 + DINOv3 (Local + Local) CLIP + CLIP (Global + Global ) CLIP + DINOv3 (Global + Local) mIoU 45.20 60.78 61. the scarce labeled samples in our semi-supervised setting fail to provide sufficient supervision for their effective convergence. In contrast, integrating explicit class-level guidance via text embeddings Ft clip alone yields substantial gain of 1.8 and reaches 60.77 mIoU. This improvement stems from the inherent text-image alignment capability of CLIP acquired during pre-training, which enables the text embeddings to serve as powerful and reliable semantic anchors immediately. Notably, the optimal performance of 61.09 is attained only when both mechanisms are engaged. This result demonstrates critical synergy where text embeddings ensure semantic correctness while learnable queries capture data-specific visual features, thereby jointly enhancing semantic consistency. 2) Benefit from Global and Local Feature Fusion: To verify the effectiveness of the proposed global-local feature collaborative fusion strategy, we investigate the necessity of the heterogeneous dual-student design by comparing it against homogeneous initialization strategies, as detailed in Table VIII. the homogeneous configuration where both students First, utilize DINOv3 performs poorly, yielding an mIoU of only 45.20. This indicates that without strong global semantic priors, students initialized solely with local-structural features struggle to maintain category consistency, leading to severe semantic drift. Second, the homogeneous setting where both students employ CLIP attains competitive mIoU of 60.78 due to rich global semantic representations. However, identical initialization results in highly correlated predictions. This lack of complementary viewpoints prevents the students from V. CONCLUSION We propose Co2S, stable semi-supervised framework for remote sensing segmentation that leverages complementary priors from two heterogeneous vision foundation models: vision-language model (e.g., CLIP) and self-supervised model (e.g., DINOv3). By co-guiding and co-fusing their inherently distinct representations, Co2S establishes driftresistant mutual learning process that simultaneously ensures semantic consistency and boundary accuracy. Experiments on six benchmarks demonstrate state-of-the-art performance, with especially pronounced gains under extreme label scarcity."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Li, Y. Cai, Q. Li, M. Kou, and T. Zhang, review of remote sensing image segmentation by deep learning methods, International Journal of Digital Earth, vol. 17, no. 1, p. 2328827, 2024. [2] L. Huang, B. Jiang, S. Lv, Y. Liu, and Y. Fu, Deeplearning-based semantic segmentation of remote sensing images: survey, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 17, pp. 83708396, 2023. [3] A. A. Aleissaee, A. Kumar, R. M. Anwer, S. Khan, H. Cholakkal, G.-S. Xia, and F. S. Khan, Transformers in remote sensing: survey, Remote Sensing, vol. 15, no. 7, p. 1860, 2023. [4] J. Xia, N. Yokoya, B. Adriano, and C. Broni-Bediako, Openearthmap: benchmark dataset for global highresolution land cover mapping, in WACV, 2023, pp. 62546264. [5] L. Wang, R. Li, C. Zhang, S. Fang, C. Duan, X. Meng, and P. M. Atkinson, Unetformer: unet-like transformer for efficient semantic segmentation of remote sensing urban scene imagery, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 190, pp. 196214, 2022. [6] L. Gao, H. Liu, M. Yang, L. Chen, Y. Wan, Z. Xiao, and Y. Qian, Stransfuse: Fusing swin transformer and convolutional neural network for remote sensing image semantic segmentation, IEEE journal of selected topics in applied earth observations and remote sensing, vol. 14, pp. 10 99011 003, 2021. [7] A. Ma, J. Wang, Y. Zhong, and Z. Zheng, Factseg: Foreground activation-driven small object semantic segmentation in large-scale remote sensing imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 116, 2021. [8] Y. Hua, D. Marcos, L. Mou, X. X. Zhu, and D. Tuia, Semantic segmentation of remote sensing images with sparse annotations, IEEE Geoscience and Remote Sensing Letters, vol. 19, pp. 15, 2021. [9] A. Yu, Y. Quan, R. Yu, W. Guo, X. Wang, D. Hong, H. Zhang, J. Chen, Q. Hu, and P. He, Deep learning methods for semantic segmentation in remote sensing with small data: survey, Remote sensing, vol. 15, no. 20, p. 4987, 2023. 11 [10] L. Ran, Y. Li, G. Liang, and Y. Zhang, Semi-supervised semantic segmentation based on pseudo-labels: survey, IEEE Transactions on Circuits and Systems for Video Technology, pp. 19, 2024. [11] X. Zhu, X. Zhang, T. Zhang, C. Fang, X. Tang, and L. Jiao, Regionmatch: Pixel-region collaboration for semi-supervised semantic segmentation in remote sensing images, in IJCAI, 2025, pp. 25302538. [12] W.-C. Hung, Y.-H. Tsai, Y.-T. Liou, Y.-Y. Lin, and M.-H. Yang, Adversarial learning for semi-supervised semantic segmentation, BMVC, pp. 117, 2018. [13] N. Souly, C. Spampinato, and M. Shah, Semi supervised semantic segmentation using generative adversarial network, in ICCV, 2017, pp. 56885696. [14] S. Mittal, M. Tatarchenko, and T. Brox, Semi-supervised semantic segmentation with high-and low-level consistency, IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 4, pp. 13691379, 2019. [15] A. Tarvainen and H. Valpola, Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results, in NeurIPS, vol. 30, 2017, pp. 110. [16] Z. Ke, D. Wang, Q. Yan, J. Ren, and R. W. Lau, Dual student: Breaking the limits of the teacher in semisupervised learning, in ICCV, 2019, pp. 67286736. [17] X. Chen, Y. Yuan, G. Zeng, and J. Wang, Semisupervised semantic segmentation with cross pseudo supervision, in ICCV, 2021, pp. 26132622. [18] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li, Fixmatch: Simplifying semi-supervised learning with consistency and confidence, in NeurIPS, vol. 33, 2020, pp. 596608. [19] L. Yang, L. Qi, L. Feng, W. Zhang, and Y. Shi, Revisiting weak-to-strong consistency in semi-supervised semantic segmentation, in CVPR, 2023, pp. 72367246. [20] L. Yang, W. Zhuo, L. Qi, Y. Shi, and Y. Gao, St++: Make self-training work better for semi-supervised semantic segmentation, in ICCV, 2022, pp. 42684277. [21] Y. Wang, H. Wang, Y. Shen, J. Fei, W. Li, G. Jin, L. Wu, R. Zhao, and X. Le, Semi-supervised semantic segmentation using unreliable pseudo-labels, in CVPR, 2022, pp. 42484257. [22] J. Jin, W. Lu, H. Yu, X. Rong, X. Sun, and Y. Wu, Dynamic and adaptive self-training for semi-supervised remote sensing image semantic segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 114, 2024. [23] Z. Feng, Q. Zhou, Q. Gu, X. Tan, G. Cheng, X. Lu, J. Shi, and L. Ma, Dmt: Dynamic mutual training for semi-supervised learning, Pattern Recognition, vol. 130, p. 108777, 2022. [24] W. Huang, Y. Shi, Z. Xiong, and X. X. Zhu, Decouple and weight semi-supervised semantic segmentation of remote sensing images, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 212, pp. 1326, 2024. [25] Z. Shao, W. Zhou, X. Deng, M. Zhang, and Q. Cheng, Multilabel remote sensing image retrieval based on fully convolutional network, IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 13, pp. 318328, 2020. F. Massa, A. El-Nouby et al., Dinov2: Learning robust visual features without supervision, Transactions on Machine Learning Research, pp. 132, 2024. [26] J. Wang, Z. Zheng, A. Ma, X. Lu, and Y. Zhong, Loveda: remote sensing land-cover dataset for domain adaptive semantic segmentation, NeurIPS, pp. 116, 2021. [27] F. Rottensteiner, G. Sohn, J. Jung, M. Gerke, C. Baillard, S. Benitez, and U. Breitkopf, The isprs benchmark on urban object classification and 3d building reconstructhe Photogrammetry, Remote tion, ISPRS Annals of Sensing and Spatial Information Sciences, vol. I-3, pp. 293298, 2012. [28] X.-Y. Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang, Land-cover classification with high-resolution remote sensing images using transferable deep models, Remote Sensing of Environment, vol. 237, p. 111322, 2020. [29] J. Li, S. Zi, R. Song, Y. Li, Y. Hu, and Q. Du, stepwise domain adaptive segmentation network with covariate shift alleviation for remote sensing imagery, IEEE Transactions on Geoscience and Remote Sensing, vol. 60, pp. 115, 2022. [30] X. Lu, L. Jiao, L. Li, F. Liu, X. Liu, S. Yang, Z. Feng, and P. Chen, Weak-to-strong consistency learning for semisupervised image segmentation, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 115, 2023. [31] S. Wang, X. Sun, C. Chen, D. Hong, and J. Han, Semisupervised semantic segmentation for remote sensing images via multi-scale uncertainty consistency and crossteacher-student attention, IEEE Transactions on Geoscience and Remote Sensing, pp. 115, 2025. [32] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., Learning transferable visual models from natural language supervision, in ICML, 2021, pp. 87488763. [33] Y. Rao, W. Zhao, G. Chen, Y. Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu, Denseclip: Language-guided dense prediction with context-aware prompting, in CVPR, 2022, pp. 18 08218 091. [34] C. Zhou, C. C. Loy, and B. Dai, Extract free dense labels from clip, in ECCV, 2022, pp. 696712. [35] F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, Q. Ye, L. Fu, and J. Zhou, Remoteclip: vision language foundation model for remote sensing, IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp. 116, 2024. [36] X. Guo, J. Lao, B. Dang, Y. Zhang, L. Yu, L. Ru, L. Zhong, Z. Huang, K. Wu, D. Hu et al., Skysense: multi-modal remote sensing foundation model towards universal interpretation for earth observation imagery, in CVPR, 2024, pp. 27 67227 683. [37] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski, and A. Joulin, Emerging properties in self-supervised vision transformers, in ICCV, 2021, pp. 96509660. [39] O. Simeoni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa et al., Dinov3, arXiv preprint arXiv:2508.10104, 2025. [40] X. Sun, P. Wang, W. Lu, Z. Zhu, X. Lu, Q. He, J. Li, X. Rong, Z. Yang, H. Chang et al., Ringmo: remote sensing foundation model with masked image modeling, IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp. 122, 2022. [41] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, Masked autoencoders are scalable vision learners, in CVPR, 2022, pp. 16 00016 009. [42] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, Beit v2: Masked image modeling with vector-quantized visual tokenizers, CVPR, pp. 115, 2022. [43] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong, Ibot: Image bert pre-training with online tokenizer, ICLR, pp. 129, 2022. [44] Z. Xie, Z. Zhang, Y. Cao, Y. Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, Simmim: simple framework for masked image modeling, in CVPR, 2022, pp. 96539663. [45] X. Zou, S. Zhang, K. Li, S. Wang, J. Xing, L. Jin, C. Lang, and P. Tao, Adapting vision foundation models for robust cloud segmentation in remote sensing images, IEEE Transactions on Geoscience and Remote Sensing, vol. 63, pp. 114, 2025. [46] B. Li, H. Dong, D. Zhang, Z. Zhao, J. Gao, and X. Li, Exploring efficient open-vocabulary segmentation in the remote sensing, arXiv preprint arXiv:2509.12040, 2025. [47] B. Zhang, S. Yu, J. Xiao, Y. Wei, and Y. Zhao, Frozen clip-dino: strong backbone for weakly supervised semantic segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 117, 2025. [48] L. Barsellotti, L. Bianchi, N. Messina, F. Carrara, M. Cornia, L. Baraldi, F. Falchi, and R. Cucchiara, Talking to dino: Bridging self-supervised vision backbones with language for open-vocabulary segmentation, in ICCV, 2025, pp. 22 02522 035. [49] C. Ye, Y. Zhuge, and P. Zhang, Towards openvocabulary remote sensing image semantic segmentation, in AAAI, vol. 39, no. 9, 2025, pp. 94369444. [50] M. Wysoczanska, O. Simeoni, M. Ramamonjisoa, A. Bursuc, T. Trzcinski, and P. Perez, Clip-dinoiser: Teaching clip few dino tricks for open-vocabulary semantic segmentation, in ECCV, 2024, pp. 320337. [51] S. Qiao, W. Shen, Z. Zhang, B. Wang, and A. Yuille, Deep co-training for semi-supervised image recognition, in ECCV, 2018, pp. 135152. [52] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, ICLR, pp. 119, 2017. [53] A. Dosovitskiy, An image is worth 16x16 words: Transformers for image recognition at scale, ICLR, pp. 121, 2021. [38] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, [54] S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, Cutmix: Regularization strategy to train strong classifiers with localizable features, in ICCV, 2019, pp. 60236032. [55] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, Dropout: simple way to prevent neural networks from overfitting, The journal of machine learning research, vol. 15, no. 1, pp. 19291958, 2014. 13 Jingming Chen is pursuing the B.E. degree with the School of Computer Technology and Application at Qinghai University, Xining, China. His research interests primarily focus on computer vision, particularly in the field of remote sensing image processing. Congyan Lang received the Ph.D. degree from the Beijing Key Laboratory of Traffic Data Analysis and Mining, School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China, in 2006. She was Visiting Professor with the Department of Electrical and Computer Engineering, National University of Singapore, Singapore, from 2010 to 2011. From 2014 to 2015, she was visiting professor at the Department of Computer Science, University of Rochester, Rochester, NY, USA. She is Professor at the School of Computer and Information Technology, Beijing Jiaotong University. She has published over 80 research articles in various journals and refereed conferences. Her research areas include computer vision and machine learning. Tengfei Cao received the Ph.D. degree from Beijing University of Posts and Telecommunications (BUPT), Beijing, China, in July 2020. He is currently an Associate Professor with the Department of Computer Technology and Applications, Qinghai University, Xining, China. He has published several high-quality research papers, such as ACM Transactions on Multimedia Computing, Communications, and Applications, IEEE TRANSACTIONS ON NETWORK AND SERVICE MANAGEMENT, and CHINA COMMUNICATIONS. His research interests include multimedia communications and privacy security. Pin Tao received his B.S. degree and Ph.D. in computer science and technology from Tsinghua University, Beijing, China, in 1997 and 2002. He is an Associate Professor at the Computer Science and Technology Department, Tsinghua University, Beijing, China. He is also the vice director of the Key Laboratory of Pervasive Computing, Ministry of Education. Dr. Tao has published more than 80 papers and over 10 patents. His current research interests mainly focus on human-AI hybrid intelligence and multimedia-embedded processing. Yuanchun Shi received the PhD, MS and BS degrees in computer science from Tsinghua University. She is Changjiang distinguished professor at the Department of Computer Science, Tsinghua University. She was senior visiting scholar at the MIT AI Lab during 2001-2002. She had chaired several conferences, including ACM Ubicomp2011. She serves as the area editor of pervasive and mobile computing (PMC, Elsevier), editor of interacting with computer (IwC, Oxford University Press), and vice editor-in-chief of Communications of China Computer Federation (CCF). She has published more than one hundred papers in IJHCS, IEEE TPDS, TKDE, ACM CHI, MM, UIST, etc. Her research interests include human-computer interaction, pervasive computing, and multimedia communication. She is Fellow of the IEEE. Yi Zhou is pursuing the B.E. degree with the School of Computer Technology and Application at Qinghai University, Xining, China. His research interests primarily focus on computer vision, particularly in the field of remote sensing image processing. Xuechao Zou received the B.E. degree in 2021 and the M.S. degree in 2024 from the School of Computer Technology and Application at Qinghai University, Xining, China. He is currently pursuing Ph.D. degree with the School of Computer Science and Technology at Beijing Jiaotong University. His research interests focus on computer vision, particularly remote sensing image processing. Shun Zhang is pursuing the B.E. degree with the School of Computer Technology and Application at Qinghai University, Xining, China. He is currently pursuing the M.S. degree with the School of Computer Science and Technology at Beijing Jiaotong University. His research interests primarily focus on computer vision, particularly in the field of remote sensing image processing. Kai Li received his B.E. degree from the School of Computer Technology and Application at Qinghai University, Xining, China, in 2020, and his M.S. degree from the Department of Computer Science and Technology at Tsinghua University, Beijing, China, in 2024. He is pursuing Ph.D. in the Department of Computer Science and Technology at Tsinghua University. His research interests focus primarily on speech separation and audio-visual learning. Shiying Wang received the B.E. degree from Jilin Agricultural University, Changchun, China, in 2015, the M.S. degree and Ph.D. degree from the School of Computer Technology and Application, Qinghai University, Xining, China, in 2018 and 2025. Her research interests mainly include remote sensing image processing, particularly in grassland informatics."
        }
    ],
    "affiliations": [
        "Department of Computer Science and Technology, Tsinghua University, Beijing, China",
        "Intelligent Computing and Application Laboratory of Qinghai Province, Qinghai University, Xining, China",
        "Key Lab of Big Data & Artificial Intelligence in Transportation (Ministry of Education), School of Computer Science & Technology, Beijing Jiaotong University, Beijing, China",
        "Key Laboratory of Pervasive Computing, Ministry of Education",
        "School of Computer Technology and Application, Qinghai University, Xining, China"
    ]
}