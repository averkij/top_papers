{
    "paper_title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
    "authors": [
        "Bingda Tang",
        "Boyang Zheng",
        "Xichen Pan",
        "Sayak Paul",
        "Saining Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper does not describe a new method; instead, it provides a thorough exploration of an important yet understudied design space related to recent advances in text-to-image synthesis -- specifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing a clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation."
        },
        {
            "title": "Start",
            "content": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis Bingda Tang 1 Boyang Zheng 1 Xichen Pan 1 Sayak Paul 2 Saining Xie 1 1 New York University 2 Hugging Face 5 2 0 2 5 1 ] . [ 1 6 4 0 0 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "This paper does not describe new method; instead, it provides thorough exploration of an important yet understudied design space related to recent advances in text-toimage synthesisspecifically, the deep fusion of large language models (LLMs) and diffusion transformers (DiTs) for multi-modal generation. Previous studies mainly focused on overall system performance rather than detailed comparisons with alternative methods, and key design details and training recipes were often left undisclosed. These gaps create uncertainty about the real potential of this approach. To fill these gaps, we conduct an empirical study on text-to-image generation, performing controlled comparisons with established baselines, analyzing important design choices, and providing clear, reproducible recipe for training at scale. We hope this work offers meaningful data points and practical guidelines for future research in multi-modal generation. Code is available at this repository: https://github.com/tang-bd/fuse-dit. 1. Introduction Text-to-image diffusion models have made remarkable progress in generating high-quality images from descriptive texts. Current state-of-the-art systems [2, 5, 10, 17, 30] typically derive text representations from specialized encoders, such as CLIP [31] and T5 [32]. With the rise of decoderonly large language models (LLMs), there has been growing amount of interest in their potential as replacements for these traditional text encoders [7, 15, 21, 22, 25, 28, 48, 51]. However, simply substituting LLMs has not yielded expected performance gains unless coupled with sophisticated architectural adaptations [15, 25, 47]. Prior work [25] attributes this to the misalignment between the next token prediction training objective of LLMs and the need for discriminative text representations in diffusion models. Recent advancements [20, 26, 46, 50] have successfully unified auto-regressive decoding and denoising difFigure 1. Illustration of the deep fusion approach and baselines. We conduct controlled comparisons with baseline methods that incorporate text representations from single text encoder layer into each DiT layer using late fusion within the attention mechanism, strategy we term as the shallow fusion approach. fusion within single transformer [43], enabling seamless multi-modal generation. This unified approach supports wide range of tasks, including instructed image-to-text, text-to-image synthesis, and interleaved image-text generation. While earlier methods often relied on large-scale pretraining of the entire model, latest research [21, 38] flexibly leverages the computationally intensive pre-training of LLMs by deeply fusing them with diffusion transformers (DiTs) [29] through layer-wise shared self-attention. This design facilitates rich cross-modal interactions while maintaining modality-specific computation by using distinct sets of weights. When optimized for text-to-image generation, it claims state-of-the-art performance [21]. Deep fusion presents compelling alternative to existing architectures for text-to-image synthesis, which typically conditions directly on representations from single text encoder layer. By aligning diffusion models with the auto-regressive decoding nature of LLMs, deep fusion enables more natural and tight-knit use of these models. However, despite the existing positive signals, its true potential remains uncertain. Current research [21, 38] prioritizes system-level benchmarks over controlled comparisons with established baselines, obscuring its position within the broader research landscape. More critically, the design space remains severely underexplored, and essential implementation details, such as training recipes, are often undisclosed. These limitations impede reproducibility and hinder broader adoption within the research community. In this paper, we bridge these gaps through an empirical study on the deep fusion of frozen LLM and trainable DiT for text-to-image synthesis. We conduct controlled comparisons between the deep fusion approach and baselines, examine key design choices, and introduce scalable, reproducible training recipe that delivers competitive performance on the established benchmarks for text-to-image generation. We believe that the evidence and unresolved questions highlighted in this study are of significant importance. We hope this work serves as valuable resource, offering meaningful data points and practical guidelines to drive future advancements in multi-modal generation. 2. Related Work Conditioning mechanisms in diffusion models. Numerous studies have examined effective methods for integrating linguistic conditional information into diffusion models to facilitate text-to-image synthesis. Latent diffusion models (LDM) [35] pioneered the use of cross-attention between image and text features, technique that has since become standard in U-Net [36] based architectures [18, 30]. With the rise of DiTs [29], vision transformer [8] has emerged as the dominant architecture for diffusion models, prompting reassessment of conditioning mechanisms. The vanilla DiT employs adaLN-Zero modulation to inject conditional information. However, this approach is limited to using pooled text representations, which capture only coarse-grained information. Subsequent architectures have explored cross-attention [5, 6, 47] and selfattention [10, 11, 17, 51] for text conditioning, which typically extract representations from single text encoder layer, usually the last or penultimate one. While this strategy aligns well with CLIP [31] and T5 [32] text encoders, it is inherently mismatched with the next-token prediction training objective of LLMs, where the last layer focuses on next-token prediction instead of learning discriminative representation. In contrast, the deep fusion approach shows promise in harnessing the internal information flow of LLMs, aligning with their in-context self-attention mechanism for processing information. Taming LLMs for diffusion models. The prevalence of decoder-only LLMs has driven extensive efforts to tame them for text-to-image diffusion models. The most effective and widely adopted practice involves leveraging LLMs to enrich input prompts [2], as well as employing multimodal LLMs (MLLMs) to generate synthetic captions for image data [2, 5, 10, 21, 47]. Another popular direction integrates (M)LLMs as system components, such as planners and discriminators [19, 44, 45, 49]. While these approaches have demonstrated effectiveness, they do not tap into the architecture of diffusion models. Figure 2. Illustration of the attention mask. Each dotted square indicates whether the row can attend to the column. To integrate LLMs into diffusion models, previous works attempt to replace text encoders with LLMs, either by training from scratch [25, 47, 51] or by aligning feature spaces [7, 15, 22, 28]. However, this substitution alone has not yielded the expected performance gains unless paired with sophisticated architectural adaptations [15, 25, 47]. Recent research [20, 26, 46, 50] has proven that autoregressive decoding and denoising diffusion can be effectively unified within single transformer for multi-modal generation. While earlier methods relied on large-scale pretraining of the entire model, latest research [21, 38] flexibly leverages the computationally intensive pretraining of LLMs by deeply fusing them with DiTs. From text-toimage perspective, this architecture introduces novel way to tame LLMs for text-to-image diffusion models, with the potential for achieving state-of-the-art performance [21]. 3. Deep Fusion of LLMs and DiTs 3.1. Model Architecture In the deep fusion approach, we integrate frozen decoderonly LLM with trainable DiT using layer-wise shared self-attention  (Fig. 1)  . The DiT mirrors the LLMs transformer architecture, differing only in its input/output layers and timestep conditioning modules. This implements two-stream transformer architecture that facilitates rich cross-modal interactions while maintaining modalityspecific computation by utilizing distinct weight sets for processing tokens from different modalities. The fused model processes text embeddings through the LLM stream and noisy image latents through the DiT stream. At each layers self-attention operation, we concatenate token sequences from both streams, enabling the DiT to extract conditional information from the linguistic context. To preserve the pretrained LLMs functionality, we apply causal attention mask to the text sequence and bidirectional mask to the image sequence, permitting the image tokens to attend to text tokens but not vice versa  (Fig. 2)  . After the final layer, we discard text tokens and use only image tokens to predict velocity, as typically done when training rectified flow models [23]. Notably, only the key and value states of the text hidden states are needed for the image tokens. These remain constant throughout the diffusion process, allowing them to be efficiently cached and reused during inference. 3.2. Training Objective We adopt the rectified flow formulation [23] to learn transport maps between the standard Gaussian noise distribution π0 and the data distribution π1, by connecting straight paths xt = tx1 + (1 t)x0 between samples x0 π0, x1 π1 and learning an ODE model dzt = vθ(zt, t)dt on time [0, 1] which converts z0 from π0 to z1 following π1. We fit the velocity with x1 x0 under prescribed time distribution πt by solving the following regression problem: min Etπt,x0π0,x1π1[v(xt, t) (x1 x0)2]dt (1) We parameterize with network θ and solve Eq. (1) by stochastic optimization with empirical draws. 4. Experiment Setup To ensure fully open and reproducible comparison between the deep and shallow fusion approaches, we provide comprehensive details on the experimental setup, including the model, dataset, training, inference, and evaluation. For the same purpose, we exclusively use open-source pre-trained LLMs and publicly available datasets. Model. We employ frozen Gemma 2B [27] as the base LLM for all experiments (excluding Sec. 7). We pair it with randomly initialized 2.5B-parameter DiT. The transformer configurations of the DiT strictly follows the base LLM, including the hidden size, number of layers, number of attention heads, FFN design, and other architectural details, ensuring both models have an identical 2B-parameter backbone. Following the vanilla DiT setup [29], we use 2D frequency absolute positional encoding, adaLN-Zero timestepconditioning, ViT [8]-style weight initialization, and patch size of 2. To further stabilize training, we apply QK normalization to all layers. For all experiments we adopt the same 16-channel VAE from Stable Diffusion 3 (SD 3) [10]. Dataset. We use the CC12M [4] dataset with communitysourced synthetic captions [9] as our training set for all experiments excluding Sec. 7. Our downloaded version of the dataset includes 10.9M image-caption pairs. The images are resized and center-cropped to 512 512 and the texts are padded or truncated to 256 tokens. Training. We train all models with batch size of 512 using AdamW [24] optimizer (β1 = 0.9, β2 = 0.999) in BF16 mixed precision. We use constant learning rate of 1 104, weight decay of 1 104, and gradient clipping with threshold of 1.0. Exponential moving average of the weights are gathered by decay factor of 0.99 every 100 steps. We employ the same logit-normal distribution as used in SD 3 for timestep sampling. During training, 10% of the texts are randomly dropped to learn unconditional generation. Training is carried out using Google TPU v4-256 pods and FSDP implemented by PyTorch / XLA SPMD. Inference. We conduct inference using Euler discretization with 25 sampling steps, and classifier-free guidance scale of 6 which we find to be near optimal for text-image alignment. We employ identical sampling steps and guidance scale across all experiments. Evaluation. We evaluate image-text alignment using GenEval [12] and DPG-Bench [15] metrics, prioritizing GenEval for its robustness. While both benchmarks provide valuable insights, DPG-Bench exhibits certain limitations, such as rapid performance saturation and potential measurement errors [21]. To ensure comprehensive evaluation, we also provide visual quality measurements using FID [14] on MJHQ-30K [18]. Notably, image-text alignment does not always correlate positively with visual quality, often presenting trade-offs. Our sampling and evaluation are primarily carried out using NVIDIA L40S GPUs. 5. Comparing Deep and Shallow Fusion The deep fusion approach fuses an LLM and DiT through layer-wise shared self-attention, creating interconnections throughout the network. However, established architectures typically condition on representations from single text encoder layer. To investigate the true potential of deep fusion, we conduct controlled comparisons with baseline methods. For fair and meaningful comparison, we examine common architectural paradigm which we refer to as shallow fusion. In this approach, representations from single text encoder layer are integrated into each DiT layer through late fusion within the attention operation. Unlike deep fusion, which involves multi-layered interactions, shallow fusion maintains fixed connection between each DiT layer and prescribed text encoder layer  (Fig. 1)  . 5.1. Shallow Fusion Baselines We consider two shallow fusion architectures that condition on last-layer hidden states of LLMs as our baselines. As illustrated in Fig. 3, the two architecture differ in how they aggregate information from the condition. Figure 3. Illustration of cross-modal attention in the shallow fusion baselines. The key and query states of the condition are directly projected from text representations. Figure 4. Illustration of timestep conditioning strategies. Removing timesetp conditioning leads to the fewest parameters and the best overall performance. Self-attention DiT. In this design, text representations are projected to key and value states and then concatenated with those of image hidden states in self-attention, which can also be decoupled by running self-attention and crossattention in parallel and merging their outputs. This approach resembles architectures proposed by [11, 51]. Cross-attention DiT. This design also projects text representations to key and values states. However, unlike the previous approach, they are used for additional crossattention with image hidden states, applied after the selfattention in each layer. This architecture follows the methodology employed in [5, 6, 47]. Compared to the deep fusion approach, both baseline models include RMS normalization and linear layer for text representations before passing them through additional key and value projection layers. Additionally, the crossattention DiT model uses extra query projections in its cross-attention mechanism for image hidden states. Other model configurations follows the DiT design in Sec. 4. Notably, the deep fusion approach can be reinterpreted as variant of the self-attention DiT architecture, as they both aggregate conditional information through in-context selfattention. The key difference lies in how the key and value states of the condition are derived: self-attention DiT employs trainable projection to generate these states from single text encoder layer, whereas deep fusion extracts them from corresponding LLM layers. Despite their similarities, the two approaches have fundamentally different conceptual implications: deep fusion treats the LLM and DiT as equal components of unified model. Apart from the aforementioned designs, SD3 [10] introduced an alternative fusion strategy that uses two-stream transformer (MM-DiT) to jointly processes noised image latents and linguistic representations. While this is another popular approach worthy of investigation, an applesto-apples fair comparison between the deep fusion approach and MM-DiT is not feasible, as both streams in MM-DiT are trainable. Consequently, our analysis focuses solely on the shallow fusion baselines detailed in this section. tectures, and train them for 300K steps following the setup detailed in Sec. 4. As shown in Tab. 1, the deep fusion model achieve significantly better performance in image-text alignment than the self-attention DiT model and also surpass the crossattention DiT model, while shallow fusion models demonstrate better visual quality. In terms of inference efficiency, deep fusion also demonstrates competitive performance, as shown in Tab. 2. This positive evidence underscores the compelling positioning of the deep fusion approach within the current landscape. Method Self-Attention Cross-Attention Deep Fusion Params. 2.47B 2.62B 2.45B GenEval 0.42 0.49 0.51 DPG 73.9 76.3 76.6 FID 26.16 24.00 27.33 Table 1. Comparison of performance between deep and shallow fusion models. Deep fusion beats shallow fusion in textimage alignment while underperforms in visual quality. Method Self-Attention Cross-Attention Deep Fusion Params. 2.47B 2.62B 2.45B Inference Latency (s) 1.75 1.86 1.66 Table 2. Comparison of inference latency between deep and shallow fusion models. The numbers are measured with batch size of 1 in automatic mixed precision on an NVIDIA A100 GPU. 6. Examining Key Design Choices In this section, we examine key design choices of the deep fusion approach through text-to-image-centric lens. We begin by assessing the necessity and potential redundancy of parameters for timestep conditioning, determining whether to optimize their use or eliminate them altogether (Sec. 6.1). Next, we compare various positional encoding strategies (Sec. 6.2). Additionally, we investigate how the choice of base LLM and the use of instruction prompts impacts text-to-image performance (Sec. 6.3). For all experiments, we use the default design as the baseline and train the models for 300K steps, following the setup detailed in Sec. 4. 5.2. Controlled Comparison 6.1. Timestep Conditioning For fair comparison, we design self-attention DiT, crossattention DiT, and the deep fusion model with similar archiDiT [29] has introduced AdaLN-Zero as the standard mechanism for injecting timestep and class label information. The AdaLN modules typically accounts for large proportion of the model parameters, 0.5B out of total 2.5B in our case. However, since our text-to-image model does not use class labels, AdaLN serves only for timestep conditioning. This substantial parameter allocation raises critical question about its necessity and potential redundancy: Could these parameters be utilized more effectively, or are they redundant altogether? Question 1.1. Can adaLN parameters be utilized more effectively by integrating additional text conditioning? Prior research [10, 11, 17] has integrated text information into AdaLN-Zero by augmenting timestep embeddings with pooled text representations through summation. To optimize parameter efficiency, we follow this approach by leveraging embeddings1 from the CLIP L/14 text encoder [31], which provides high-quality linguistic representations trained on large-scale multi-modal data. Method adaLN-Zero + CLIP L/14 GenEval 0.51 0.50 DPG 76.6 76.2 FID 27.33 24.00 The results indicate that integrating text modulation results in slight improvements in FID but weakens image-text alignment. Additionally, it further increases compute. Question 1.2. Can we shrink the parameters for timestep conditioning? We explore the possibility of eliminating timestep conditioning parameters to develop more streamlined architecture akin to that of the LLM. Specifically, we compare four timestep conditioning strategies from previous work, each with progressively reduced number of parameters, as illustrated in Fig. 4. adaLN-Zero. Following the vanilla DiT [29], we regress zero-initialized modulation parameters for each layer from the timestep embedding. adaLN-Single. Following PixArt-α [6], we compute global set of modulation parameters and refine them per layer by adding learnable embeddings. Addition. Following Transfusion [50], we directly add the timestep embedding to all image tokens. w/o Timestep. Inspired by recent study [42], we completely remove the timestep conditioning from the model. Method adaLN-Zero adaLN-Single addition w/o timestep FID 27.33 27.09 26.40 21.27 Table 3. Evaluation of timestep conditioning strategies. Removing timestep conditioning yields surprisingly strong results. GenEval 0.51 0.47 0.47 0.49 Params. 2.47B 2.01B 1.99B 1.98B DPG 76.6 75.2 75.6 76.7 Surprisingly, the results in Tab. 3 indicate that reducing the number of parameters in timestep conditioning consis1We apply RMS-normalization and an MLP to the text embeddings before adding them to the timestep embeddings. Figure 5. Illustration of RoPE. The indices denote position IDs. tently enhances visual quality, whereas the performance in image-text alignment exhibits fluctuations. Notably, the strategy that completely removes timestep conditioning not only achieves significantly better FID but also maintains comparable GenEval and DPG-Bench performance. This finding is consistent with [42], where timestep conditioning removal improved FID in rectified flow models trained on smaller datasets. Furthermore, the complete removal of timestep conditioning eliminates the need for associated parameters, resulting in 20% reduction in the total number of model parameters. Although this approach slightly lags behind AdaLN-Zero in terms of textimage alignment metrics, we prefer it due to its parameter efficiency and architectural simplicity. 6.2. Positional Encoding Absolute positional encoding (APE) is widely used in textto-image diffusion models, whereas in the context of LLMs, rotary positional embedding (RoPE) [40] is the predominant choice. As deep fusion models are inherently multimodal and differ from traditional text-to-image diffusion models, it is unclear which positional encoding (or their combinations) is best suited for mixed-modal sequences. Question 2.1. Is RoPE more advantageous than APE for enhancing the performance of deep fusion models? 1D RoPE + APE: We apply 1D RoPE to the text sequence and APE to the image sequence respectively. 1D RoPE: We extend 1D RoPE to encompass both text and image sequences. 1D + 2D RoPE: We apply 1D RoPE to the text sequence and 2D RoPE to the image sequence respectively. Method 1D-RoPE + APE 1D-RoPE 1D + 2D-RoPE GenEval 0.51 0.46 0.51 Table 4. Comparing different positional encoding strategies. 1D + 2D-RoPE achieves the best overall performance. DPG 76.6 77.0 76.4 FID 27.33 27.94 25.42 As shown in Tab. 4, the 1D + 2D-RoPE configuration achieves the best overall performance, with only marginal decrease in DPGBench compared to the 1D-RoPE + APE variant. The superiority of 2D-RoPE over APE suggests RoPE is more effective for modeling image sequences in deep fusion models. Using only 1D-RoPE slightly reduces performance, indicating that while deep fusion models treat text and image sequences as unified input, their distinct positional characteristics are best modeled separately. Question 2.2. Do deep fusion models benefit from RoPE specifically designed for mixed-modal sequences? Previous work on MLLMs has explored RoPE strategies for handling mixed-modal sequences. Naturally, we are curious whether deep fusion models can benefit from these positional encodings. Follow Qwen2-VL, we implement M-RoPE, variant of RoPE that applies 2D positional IDs to chunked 1D RoPE frequencies, allowing it to function as 1D RoPE for text sequences while approximating 2D RoPE for image sequences. Method 1D + 2D-RoPE M-RoPE GenEval 0.51 0.49 DPG 76.4 74.9 FID 25.42 27.60 Although M-RoPE elegantly unifies 1D and 2D-RoPEs, it still falls short compared to their direct combination. This underscores the challenge of designing position encodings for mixed-modal sequences. 6.3. Base LLM LLMs trained with different paradigms and data demonstrate diverse capabilities and behaviors. In this section, we examine how the choice of base LLM impacts the performance of deep fusion models. Question 3.1 Can instruction tuning, combined with instruction prompts, improve text-to-image performance? Instruction tuning enables LLMs to effectively follow complex instructions. We explore whether this process can also enhance their internal information flow, leading to more contextualized and discriminative representations for text-to-image synthesis. We compare Gemma 2B with Gemma 2B IT, its instruction-tuned variant. We also experiment with using it with simple instruction prompt, Imagine: , which we find sufficient for guiding the LLM to generate detailed and relevant expansions of the input. Method Gemma 2B + instruction tuning + instruction prompt GenEval 0.51 0.49 0.50 DPG 76.6 75.4 75. FID 27.33 27.04 25.28 compare Gemma 2B with the base LLM of PaliGemma 3B PT [3], multi-modal extension of Gemma 2B that undergoes additional pretraining on image-text data. Method Gemma 2B + multi-modal tuning GenEval 0.51 0.52 DPG 76.6 76.2 FID 27.33 26. which yields small improvements in performance. This observation indicates that multi-modal finetuning provides some benefit to the deep-fusion model. Question 3.3 Do improved LLM capabilities translate to stronger text-to-image performance? Finally, as the base LLM become more proficient in understanding and generating text, they could potentially foster synergistic improvements in DiT performance. We compare Gemma 2B with Gemma 2 2B [34], the next generation of Gemma 2B which demonstrates 6% absolute performance improvement (0.44 0.50) on an average of 8 language-only benchmarks. Notably, Gemma 2 2B features different transformer architecture than Gemma 2B, prompting us to adjust the DiT architecture accordingly. This modification increases the number of adaLN parameters by 0.3B. However, as demonstrated in Sec. 6.1, these parameters do not contribute to model performance. Model Gemma 2B Gemma 2 2B GenEval 0.51 0.54 DPG 76.6 79.1 FID 27.33 23. Upgrading from Gemma 2B to Gemma 2 2B yields drastic performance boost. This finding suggests that the DiTs performance in deep fusion models is strongly dependent on the capabilities of the underlying base LLM. 7. Training at Scale In this section, we present final recipe for the deep fusion model, building on the original framework while incorporating key insights from previous exploration. We conduct large-scale training to benchmark our model against established systems, showcasing its scalability and competitive performance on the leaderboard. Table 5. Evaluating the effect of instruction tuning. Using instruction-tuned LLMs does not improve performance. 7.1. Final Recipe As shown in Tab. 5, instruction tuning appears to have slightly negative impact on performance. While the use of an instruction prompt mitigates this effect to some extent, consistent with findings in [25, 47], it still falls short of the baseline. This result highlights challenge in effectively leveraging the instruction-following capabilities of LLMs. Question 3.2 Can multi-modal tuning improve text-toimage performance? Additionally, we are interested in the effect of multimodal tuning. While multi-modal tuning differs significantly from our setup, its shift in data distribution may still potentially enhance adaptability to multi-modal tasks. We Building on the insights from Sec. 6, we introduce the following design modifications to our model: Remove AdaLN-Zero modules. Replace 1D-RoPE + APE with 1D + 2D-RoPE. Replace Gemma 2B with Gemma 2 2B, adjusting the DiT configurations accordingly. We train our model, named FuseDiT, for 800K steps on mixed dataset comprising CC12M [4], SA-1B [16], and the training subset of JourneyDB [41], amounting to approximately 26M image-caption pairs. Notably, state-of-the-art text-to-image models typically rely on high-quality datasets of much larger scale to achieve superior performance. For approach, strictly enforcing identical transformer configurations for both models. However, this rigid constraint limits the flexibility of deep fusion. In practice, we need the ability to scale the LLM and DiT independently, as different modalities follow distinct scaling laws and network design principles, and training and deployment scenarios vary. To address this, we explore modifying the DiT models hidden size and number of transformer layers, as illustrated in Fig. 6. The adapted model is fused into the middle layers of the LLM, which contain richer semantic information [39]. Additionally, in self-attention, hidden states are still projected to query, key, and value states that match the LLMs dimensionality, ensuring compatibility. Since the most successful DiTs are generally much smaller than stateof-the-art LLMs, we focus on shrinking the size of our DiT. Hidden size 2048 1792 1536 Params. 2.5B 2.1B 1.8B 1.4B GenEval 0.51 0.50 0.49 0.48 DPG 76.6 77.1 76.2 74.8 FID 27.33 24.27 25.46 24.64 Table 7. Evaluating models of different hidden sizes. The default hidden size is 2048. Layers 18 14 Params. 2.5B 1.9B 1.4B GenEval 0.51 0.47 0.33 DPG 76.6 74.6 68.0 FID 27.33 23.46 28.34 Table 8. Evaluating different numbers of layers. The default number of layer is 18. As shown in Table Tab. 7, the models performance degrades gracefully as we reduce the hidden size, with visual quality actually improving in some cases. While decreasing the number of transformer layers (Tab. 8) also yields acceptable results, performance deteriorates more quickly. We hypothesize this occurs because Gemma 2B already employs fewer layers than typical model architectures of the same size. These findings suggest that LLM and DiT model designs can be effectively decoupled, enabling the application of separate scaling laws and design principles. 8.2. Attention Mechanism In Sec. 3, we built on prior work by defining the deep fusion architecture with shared self-attention to bridge the LLM and DiT. Inspired by extensive research on cross-attention in MLLMs [1, 13] and our findings in Sec. 5, which highlight the superior performance of cross-attention DiT over self-attention DiT, we explore an alternative deep fusion variant. This new approach replaces shared self-attention with cross-attention mechanisms, similar to cross-attention DiT but with key distinction: we substitute the projected linguistic key and value states in traditional cross-attention DiT with corresponding states from LLM layers. Figure 6. Illustration of architecture alignment. Dashed boxes indicate parameters that have been reduced by decreasing either the hidden size or the number of layers. CC12M and SA-1B, we utilize synthetic captions [6, 9]. Other experimental setup follows Sec. 4. 7.2. Performance Comparison"
        },
        {
            "title": "Model",
            "content": "Params Data Gen. DPG SD 1.5 [35] DALL-E 2 [33] SDXL [30] PG 2.5 [18] SD 3 [10] DALL-E 3 [2] FLUX.1 [dev] [17] PG 3 [21] MicroDiT [37] PixArt-α [6] Lumina Next [11] PixArt-Σ [5] Transfusion [50] Sana 1.0 1.6B [47] FuseDiT (Ours) 0.9B 4.2B 2.6B 2.6B 2B 12B 24B 1.2B 0.6B 2B 0.6B 7.3B 1.6B 2B 4.8B 2.6B 1.6B 1B 0.43 0.52 0.55 0.56 0.62 0.67 0.67 0.76 37M 0.46 25M 0.48 0.46 46M 0.54 0.63 3.5B 0.66 26M 0.60 63.2 74.7 75.5 84.1 83.5 84.0 87. 71.1 74.6 80.5 84.8 81.6 FID 6.63 6.09 11.92 10.15 7.58 6.15 5.76 7. Table 6. Comparison of performance with state-of-the-art systems. Table adapted from [48, 50]. Industrial baselines are presented at the top, while academic baselines and our model are listed at the bottom. We compare our model with the most advanced text-toimage diffusion models in Tab. 6. Despite being trained with limited compute and data in simplified setting, our model surpasses many industry-standard systems and delivers competitive results. We present qualitative examples from our model in Figure 7. Our model demonstrates the ability to generate highquality images with superior prompt alignment. 8. Further Exploration In this section, we present preliminary studies exploring more aggressive modifications to the deep fusion approach. For all experiments, we train the models for 300K steps, following the setup detailed in Sec. 4. 8.1. Architecture Alignment Up to this point, our approach has followed prior work by aligning the LLM and DiT backbones in layer-by-layer Figure 7. Samples generated by FuseDiT. Method self-attention cross-attention GenEval 0.51 0.52 DPG 76.6 76.5 FID 27.33 26. This modification yields minor gains, though at cost to the LLM-DiT parity. Additionally, we find that although cross-attention introduces negligible increase in FLOPs and parameter count, it leads to approximately 12% increase in latency2 (1.66s vs. 1.86s). Therefore, we retained the self-attention design in our final configuration. 2Latency is measured with batch size of 1 under automatic mixed precision on an NVIDIA A100 GPU 9. Conclusion We have studied the recently popular deep fusion of frozen LLM with trainable DiT for text-to-image synthesis. Our findings provide empirical evidence supporting its advantages over baselines. We highlight key design choices, identify unresolved problems, and offer meaningful data points alongside practical guidelines. We hope our empirical work help advance multi-modal generation and bridge the gap between auto-regressive decoding and denoising diffusion."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank the reviewers for their constructive feedback. We also thank Shusheng Yang, Shengbang Tong, Wenhao Chai, Nanye Ma, Sihan Xu, and Chenyu Li for insightful discussions. This work was mainly supported by the Google TPU Research Cloud (TRC) program, the Google Cloud Research Credits program (GCP19980904), Open Path AI Foundation, and Lambda Labs. SX also acknowledges support from Intel AI SRS, IITP grant funded by the Korean Government (MSIT) (RS-2024-00457882, National AI Research Lab Project), Amazon Research Award, and NSF Award IIS-2443404."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: In NeurIPS, visual language model for few-shot learning. 2022. [2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. https://cdn. openai. com/papers/dall-e-3. pdf, 2023. [3] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv:2407.07726, 2024. [4] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021. [5] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In ECCV, 2024. [6] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In ICLR, 2024. [7] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. In ICLR, 2024. [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. Emporium. conceptual-captions-cc12mllavanext. https://huggingface.co/datasets/ CaptionEmporium / conceptual - captions - cc12m-llavanext, 2024. [9] Caption [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [11] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv:2405.05945, 2024. [12] Dhruba Ghosh, Hanna Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating textto-image alignment. arXiv:2310.11513, 2023. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv:2407.21783, 2024. [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. [15] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv:2403.05135, 2024. [16] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, 2023. [17] Black Forest Labs. Announcing black forest labs. https: / / blackforestlabs . ai / announcing - black - forest-labs/, 2024. [18] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground v2. 5: Three insights towards enhancing aesthetic quality in text-to-image generation. arXiv:2402.17245, 2024. [19] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llmgrounded diffusion: Enhancing prompt understanding of text-to-image diffusion models with large language models. arXiv:2305.13655, 2023. [20] Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis, Wen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv:2411.04996, 2024. [21] Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving text-toimage alignment with deep-fusion large language models. arXiv:2409.10695, 2024. [22] Mushui Liu, Yuhang Ma, Xinfeng Zhang, Yang Zhen, Zeng Zhao, Zhipeng Hu, Bai Liu, and Changjie Fan. Llm4gen: Leveraging semantic representation of llms for text-to-image generation. arXiv:2407.00737, 2024. [23] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv:2209.03003, 2022. [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. layer: Uncovering hidden representations in language models. arXiv:2502.02013, 2025. [40] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 2024. [41] Keqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin, Yi Wang, et al. Journeydb: benchmark for generative image understanding. In NeurIPS, 2024. [42] Qiao Sun, Zhicheng Jiang, Hanhong Zhao, and Kaiming Is noise conditioning necessary for denoising generaHe. tive models? arXiv:2502.13129, 2025. [43] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. [44] Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, and Zhenguo Li. Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation. arXiv:2401.15688, 2024. [45] Tsung-Han Wu, Long Lian, Joseph Gonzalez, Boyi Li, and Trevor Darrell. Self-correcting llm-controlled diffusion models. In CVPR, 2024. [46] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. arXiv:2409.11340, 2024. [47] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Yujun Lin, Zhekai Zhang, Muyang Li, Yao Lu, and Song Han. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv:2410.10629, 2024. [48] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, Han Cai, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. arXiv:2501.18427, 2025. [49] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In ICML, 2024. [50] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv:2408.11039, 2024. [51] Le Zhuo, Ruoyi Du, Han Xiao, Yangguang Li, Dongyang Liu, Rongjie Huang, Wenze Liu, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, et al. Lumina-next: Making lumina-t2x stronger and faster with next-dit. arXiv:2406.18583, 2024. [25] Bingqi Ma, Zhuofan Zong, Guanglu Song, Hongsheng Li, and Yu Liu. Exploring the role of large language models in prompt encoding for diffusion models. arXiv:2406.11831, 2024. [26] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv:2411.07975, 2024. [27] Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Gemma: Open models based on gemini research and technology. arXiv:2403.08295, 2024. Juliette Love, et al. [28] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. In ICLR, 2024. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [30] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv:2307.01952, 2023. [31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. [32] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. In JMLR, 2020. [33] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 2022. [34] Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Leonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Rame, et al. Gemma 2: Improving open language models at practical size. arXiv:2408.00118, 2024. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [37] Vikash Sehwag, Xianghao Kong, Jingtao Li, Michael Spranger, and Lingjuan Lyu. Stretching each dollar: Diffusion training from scratch on micro-budget. arXiv:2407.15811, 2024. [38] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Lmfusion: Adapting pretrained language models for multimodal generation. arXiv:2412.15188, 2024. [39] Oscar Skean, Md Rifat Arefin, Dan Zhao, Niket Patel, Jalal Naghiyev, Yann LeCun, and Ravid Shwartz-Ziv. Layer by"
        }
    ],
    "affiliations": [
        "Hugging Face",
        "New York University"
    ]
}