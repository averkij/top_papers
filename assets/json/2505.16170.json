{
    "paper_title": "When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction",
    "authors": [
        "Yuqing Yang",
        "Robin Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as \"retraction\" and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether a model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models' internal belief: models fail to retract wrong answers that they \"believe\" to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 0 7 1 6 1 . 5 0 5 2 : r When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction Yuqing Yang University of Southern California yyang063@usc.edu Robin Jia University of Southern California robinjia@usc.edu"
        },
        {
            "title": "Abstract",
            "content": "Can large language models (LLMs) admit their mistakes when they should know better? In this work, we define the behavior of acknowledging errors in previously generated answers as retraction and aim to understand when and why LLMs choose to retract. We first construct model-specific datasets to evaluate whether model will retract an incorrect answer that contradicts its own parametric knowledge. While LLMs are capable of retraction, they do so only infrequently. We demonstrate that retraction is closely tied to previously identified indicators of models internal belief : models fail to retract wrong answers that they believe to be factually correct. Steering experiments further demonstrate that internal belief causally influences model retraction. In particular, when the model does not believe its answer, this not only encourages the model to attempt to verify the answer, but also alters attention behavior during self-verification. Finally, we demonstrate that simple supervised fine-tuning significantly improves retraction performance by helping the model learn more accurate internal beliefs. Code and datasets are available on https://github.com/ayyyq/llm-retraction."
        },
        {
            "title": "Introduction",
            "content": "Despite rapid progress, large language models (LLMs) still make errors in reasoning (Tong et al., 2024; Li et al., 2024a), code generation (Tambon et al., 2025), and knowledge recall (Zhang et al., 2023; Li et al., 2024b). particularly concerning case is when models hallucinate incorrect answers even when they appear to know those answers are wrong (Zhang et al., 2024a; Jiang et al., 2024; Simhi et al., 2024). For such hallucinations, an ideal response would be to promptly recognize the error and indicate it is incorrect an act we define as retraction, as illustrated in Figure 1. Although retraction does not guarantee that the model will subsequently generate correct answer, it improves reliability and reduces the risk of misinformation by acknowledging previous mistakes. Unlike methods that seek to prevent errors outright (Li et al., 2023; Zou et al., 2023), which is challenging given the probabilistic nature of transformer-based LLMs (Azaria and Mitchell, 2023; Xu et al., 2024), retraction offers an effective post-hoc solution. In this work, Preprint. Under review. indicates correct answer, Figure 1: indicates denotes retraction. We inveswrong answer, and tigate why LLMs often fail to retract even when they know the answer is wrong in verification questions. we focus on knowledge-related factoid questions and aim to understand when and why LLMs autonomously retract incorrect answers that they know are incorrect. Given the lack of suitable testbed for studying retraction, we first construct model-dependent continuation datasets. We obtain two datasets of knowledge-based questions on which LLMs often hallucinate: set of constraint satisfaction questions where each question includes two constraints that the answer must satisfy to be correct (e.g., Name politician who was born in New York City, Dhuliawala et al., 2024; Yüksekgönül et al., 2024), as well as set of reversal curse questions that ask for celebrity given their lesser-known parents (Berglund et al., 2024). Since these datasets induce many LLM errors, they provide an ideal testbed for studying retraction. For each question, we collect the target models self-generated incorrect answers and keep only those where the model disagrees with its initial answer when separately asked verification question. In this way, we build continuation datasets consisting of question-answer pairs, and then prompt the model to continue generating text after the incorrect answer to see whether it will retract. While models sometimes retract their own incorrect answers, they are generally reluctant to do so, even when they have the requisite knowledge. Next, we show that models decision to retract is closely linked to its internal belief about the answers correctness. Prior work has found directions in LLM hidden states that represent models internal beliefs about whether statement is factually correct (Li et al., 2023; Liu et al., 2024). Through probing experiments, we show that these internal beliefs are indicative of models retraction decisions: models tend to believe that non-retracted answers are correct and retracted ones are incorrect, even when this does not align with ground truth. This link is causal: steering the model to believe an answer is correct (positive belief steering) suppresses retraction, while steering it to believe an answer is incorrect (negative belief steering) strongly promotes retraction. By analyzing the steered models, we identify two separate pathways through which internal beliefs control retraction. Steering in the negative belief direction first encourages the model to try generating additional information (e.g., the birthplace of the entity) for verification, rather than stopping immediately after the answer. Then, negative belief steering increases the models attention to the given answer and refines the answers attention value vectors, which further promotes retraction. Finally, we show that straightforward supervised fine-tuning (SFT; Prakash et al., 2024) can greatly improve in-distribution retraction performancethe model retracts more incorrect answers while still committing to correct ones. Our findings connecting model belief with retraction generalize to fine-tuned models, as the original belief direction continues to influence retraction behavior after finetuning. SFT works by aligning the models internal beliefs with ground truth, leading to more accurate retraction decisions. These fine-tuning results highlight the potential of diverse, retraction-focused training data to create LLMs that robustly retract their incorrect answers. To summarize, our contributions are as follows: (1) We construct model-specific testbed to evaluate an LLMs retraction performance, and show that current LLMs can retract but do so only rarely. (2) We uncover connection between models internal belief and its external retraction behavior, and identify the underlying mechanism that governs this behavior. (3) We demonstrate that the causal influence of internal belief on retraction generalizes to supervised fine-tuned models, where more accurate beliefs lead to improved retraction performance."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Probing LLMs Belief The beliefs of LLMs refer to their internal judgments about the truth of the world (Levinstein and Herrmann, 2023; Schouten et al., 2024). Prior work typically deciphers these beliefs by probing internal representations (Li et al., 2023; Azaria and Mitchell, 2023; Marks and Tegmark, 2023a; Burns et al., 2023). Most relevant to our work, Liu et al. (2024) propose the existence of universal truthfulness hyperplane that can separate true and false statements based on models internal representations. We note that these probing methods may not always accurately distinguish true and false statements. Many studies focus on distinguishing synthetically constructed true-false claims, which may not reflect the distribution of hallucinations in real LLM outputs. More critically, while some research demonstrates strong performance in detecting hallucinations on in-distribution, model-generated data 2 (Azaria and Mitchell, 2023; CH-Wang et al., 2024; Orgad et al., 2024), they fail to generalize to out-of-distribution examples (Liu et al., 2024; Levinstein and Herrmann, 2023). Our work provides possible explanation: low out-of-distribution probe accuracy may reflect the need for finer-grained classification of incorrect answers, such as distinguishing between retracted and non-retracted ones. 2.2 Self-Correction in LLMs Retraction marks an important step of self-correction, in which model must first identify its mistake, then subsequently produce correct answer or more accurate reasoning step. While there has been debate over whether LLMs can truly self-correct in reasoning (Huang et al., 2024; Tyen et al., 2024), it has been demonstrated that self-correction is possible and exceptionally effective when the model has necessary knowledge (Dhuliawala et al., 2024). However, previous work on self-correction primarily relies on multi-turn procedures, such as asking the model verification questions (Dhuliawala et al., 2024; Wu et al., 2024), prompting it to give feedback (Madaan et al., 2023; Zhang et al., 2024b; Liu et al., 2023), or directly instructing it to verify its initial responses (Kadavath et al., 2022; Yang et al., 2024a). These approaches are not fully automatic. In contrast, our focus is to evaluate and understand LLMs capabilities to autonomously identify and admit their own mistakes, thus initiating the self-correction process without explicit external prompting."
        },
        {
            "title": "3 Task Definition and Preliminary Results",
            "content": "3.1 Task Definition Retraction denotes models immediate acknowledgment that its generated answer is incorrect or does not fully satisfy the users requirements, regardless of whether it later produces the correct answer. To evaluate the retraction performance of current LLMs, we construct model-specific testbed. We first collect questions from two knowledge-related datasets, WIKIDATA (e.g., Name writer who was born in Oran, Algeria) and CELEBRITY (e.g., Name child of Joe Jackson), which are prone to eliciting wrong answers, thereby creating great opportunity to study retraction. Details of these two original datasets are provided in Appendix B.1. Continuation Dataset Construction. Based on the collected questions, we construct modelspecific continuation datasets. Each example pairs question with model-generated answer, and we prompt the model to continue generating to evaluate whether it will retract, as illustrated below: USER: Name politician who was born in New York City. ASSISTANT: Hilary Clinton[Model generation continues from here...] To ensure that each incorrect answer is, in principle, correctable by the target LLM, we first collect the models freely generated answers using temperature sampling. For each answer, we create verification questions (e.g., Where was {models answer} born? What is {models answer}s profession?) and assess whether the models responses to these questions conflict with the constraints of the original question. We retain two types of examples: Correct Examples: The answer is factually correct, and the model can correctly answer all verification questions. Wrong Examples: The answer is factually incorrect, and the models responses to the verification questions contradict the original question. Correct examples are used to evaluate over-retraction and for in-distribution SFT in Section 6. We also create train/test split: all results are evaluated on the test set, while the training set is used for SFT in Section 6. We conduct experiments using three popular LLMs from different model families, Llama3.1-8B-Instruct (Dubey et al., 2024, abbr. Llama3.1-8B), Qwen2.5-7B-Instruct (Yang et al., 2024b, abbr. Qwen2.5-7B), and Olmo2-1124-7B-Instruct (OLMo et al., 2025, abbr. Olmo2-7B). The data statistics are listed in Table 1. In the following sections, we use WIKIDATA and CELEBRITY to denote the model-specific continuation datasets instead of the original datasets. 3 Llama3.1-8B Qwen2.5-7B Olmo2-7B # Train # Test # Train # Test # Train # Test"
        },
        {
            "title": "WIKIDATA\nCELEBRITY",
            "content": "1934 1550 1202 826 1496 1072 1142 1796 1260 Table 1: Continuation dataset statistics. Note that Qwen2.5-7B and Olmo2-7B do not have training sets for CELEBRITY due to an insufficient number of correct examples. See Appendix B.2 for details. Evaluation Metrics. We use Llama3.3-70B-Instruct1 as judge (Zheng et al., 2023) to automatically assess whether the target model retracts the given answer in its response. See Appendix B.3 for details. We then calculate the following two metrics to evaluate the models retraction performance: Retraction Recall = Wrong & Retraction Wrong , Retraction Precision = Wrong & Retraction Retraction . Wrong denotes the number of wrong examples, and Retraction indicates the number of examples that the target model retracts according to the judgment of Llama3.3-70B-Instruct. Higher retraction recall and precision together represent better retraction performance. 3.2 Models Can Retract, but Do So Infrequently Llama3.1-8B Qwen2.5-7B Olmo2-7B Precision Recall Precision Recall Precision Recall WIKIDATA CELEBRITY 0.9012 0. 0.2529 0.1477 0.8824 0.9667 0.1119 0.0290 0.9881 0.8824 0.1317 0.0150 Table 2: Retraction performance on the WIKIDATA and CELEBRITY test sets across different LLMs. As shown in Table 2, models consistently have low but non-zero retraction recall on our datasets. We infer that LLMs have the capability to retract incorrect answers, but the consistently low recall (at most 25%) highlights that such retractions are rare. Recall that our verification questions provide clear evidence that the model knows that the incorrect answers in our datasets are indeed incorrect. Thus, it appears the model has both the knowledge and the ability to retract. Then, why do LLMs fail to retract more incorrect answers? What factors govern their retraction behavior?"
        },
        {
            "title": "4 Model Belief Guides Retraction",
            "content": "To understand why LLMs are often unwilling to retract their own incorrect answers, we first investigate whether LLMs actually know that incorrect answers are incorrect in the context of answering the question. To explore this, we identify universal belief hyperplane and examine whether the models internal beliefs align with factual correctness. By belief, we refer to the models internal assessment of whether an answer is correct or incorrect, which may not always match the ground truth. 4.1 Probing for Internal Beliefs Universal Truthfulness Dataset. Following prior work (Li et al., 2023; Azaria and Mitchell, 2023; Marks and Tegmark, 2023a), we infer models internal beliefs through probing. Liu et al. (2024) show that training on diverse set of datasets can reveal universal belief hyperplane, rather than overfitting to in-distribution patterns. Motivated by this, we use subset of the dataset collection from Liu et al. (2024) to train our probes, including 800 examples each from Natural Questions (Kwiatkowski et al., 2019), Trivia QA (Joshi et al., 2017), and SciQ (Welbl et al., 2017). These datasets are all short-answer closed-book QA tasks and share similar format with WIKIDATA and CELEBRITY. Each dataset includes 50/50 split of correct and incorrect answers, with the incorrect answers generated by GPT-4-turbo. We denote this collection as Universal Truthfulness QA dataset (UTQA). 1https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct 4 (a) Llama3.1-8B on WIKIDATA. (b) Llama3.1-8B on CELEBRITY. Figure 2: Average probe scores across layers for Llama3.1-8B, grouped by factual correctness and retraction behavior. Higher scores indicate that the model internally believes the answer to be correct. For each layer of the target LLM like Llama3.1-8B, we train separate linear probe on the UTQA training set based on that layers hidden states after the given answer. These probes learn to distinguish correct and incorrect answers on UTQA, and thus serve as proxy for the models internal beliefs: lower prediction scores indicate the model believes the answer is incorrect, while higher prediction scores suggest it believes the answer is correct. We then apply the probes to examples from WIKIDATA and CELEBRITY, group them according to the answers factual correctness and the models retraction behavior, and plot the prediction scores across layers. The four groups are defined as follows: CN: the answer is factually correct and the model does not retract it. CR: the answer is factually correct and the model retracts it. WR: the answer is wrong (factually incorrect) and the model retracts it. WN: the answer is wrong (factually incorrect) and the model does not retract it. From Figure 2, we find that these belief probes assign high scores to CN and WN examples, and low scores to CR and WR examples. This indicates that the models beliefs do not align with factual correctness, but instead align more closely with its retraction behavior: low belief scores correspond to retraction, while high scores correspond to non-retraction. For example, WN examples contain factually incorrect answers, yet the probes assign them high belief scores, implying that the model believes these answers to be correct. similar trend is observed for Qwen2.5-7B and Olmo2-7B, as shown in Appendix C.1. 4.2 Steering Internal Beliefs Affects Retraction Our probing results establish correlation between the models internal beliefs and its retraction behavior. To demonstrate that internal beliefs causally influence retraction behavior, we steer the models hidden states towards belief+ (i.e., believe an answer is correct) and belief- (i.e., believe an answer is incorrect) directions. Activation Steering. We still use the UTQA training set to find steering directions. For each layer L, we calculate the mean hidden states h+ for correct answers at the last token of the answer, and h for incorrect answers. We then compute the different-in-means vector vl = h+ (Li et al., 2023; Marks and Tegmark, 2023b; Arditi et al., 2024), which represents linear belief direction. We add or subtract this difference-in-means vector to the activations of new answer, thereby shifting the models perception of the correctness of the answer: hl + αvl, where α represents the strength of steering. Note that we steer only at the last token of the answer; we do not add the steering vector at any following generation steps in order to minimize disruption to the models natural generation. Similar to prior work (Li et al., 2023; Turner et al., 2023; Lee et al., 2025), we manually search for the steering hyperparameters to ensure that the steering is effective and minimally invasive, as detailed in Appendix B.4. Results. We present retraction rate (i.e., the proportion of retracted examples) in Figure 3 for clarity and provide detailed retraction recall and precision in Appendix Table 12, 13, and 14. From Figure 3, 5 Figure 3: Retraction Rate under belief steering. we can see that across all three models and two datasets, belief steering effectively controls retraction behavior in both directions. Specifically, strengthening the models belief in the negative direction causes it to retract over 70% of the time across the entire dataset. In contrast, when we strengthen the models belief in the positive direction, retraction rate drops to nearly zero, indicating the model rarely retracts. This supports our hypothesis about the role of model belief in retraction: an LLM tends to take back an answer only when it internally believes it is incorrect; otherwise, it is like to stand by its initial answer. We note that other steering directions, e.g., ones directly derived from in-distribution data, can yield similar results as detailed in Appendix C.2. However, these directions often fail to generalize to out-of-distribution settings. Importantly, our goal is not to find the optimal steering direction. Instead, we aim to understand when and why LLMs choose to retract. Both the probing and steering results support the conclusion that the models beliefdefined independently of retraction and trained on separate datacausally affects retraction behavior and generalizes across different datasets."
        },
        {
            "title": "5 Mechanistic Analysis",
            "content": "Having established that retraction behavior is guided by LLMs internal beliefs, we now turn to deeper investigation of how beliefs function. In this section, we explore the mechanisms through which beliefs shape model behavior, from surface-level token generation to deeper attention dynamics. 5.1 Internal Beliefs Influence the Decision to Stop Generating First, we find that belief steering controls whether the model stops generation immediately after the given answer. If the model outputs . or EOS token directly following the answer, we define this as stop and calculate the stop rate as reported in Table 3. Llama3.1-8B Qwen2.5-7B Olmo2-7B WIKIDATA CELEBRITY WIKIDATA CELEBRITY WIKIDATA CELEBRITY No Steering BeliefBelief+ 0.7413 0.0017 0.9867 0.6041 0.0206 0.8765 0.0028 0.0271 0.4310 0.0271 0.0096 0.8126 0.0563 0.0000 0.9992 0.1960 0.0000 0. Table 3: Stop Rate, which refers to the proportion of examples where the model stops generating right after the given answer. We observe that positive belief steering increases stop rate, suggesting that when the model believes the answer is true, it is more likely to terminate generation early, foregoing the opportunity to verify the answer. In contrast, negative belief steering reduces stop rate: the model tends to generate additional information like the entitys birthplace and profession, which encourages it to reflect on and potentially challenge its initial answer, even if ultimately retraction does not occur."
        },
        {
            "title": "No Steering",
            "content": "0.9012 Appending is No Steering BeliefBelief+ 0.8254 0.5026 0."
        },
        {
            "title": "Recall",
            "content": "0.2579 0.5740 0.9717 0."
        },
        {
            "title": "Precision",
            "content": "0.7722 0.8310 0.4836 0."
        },
        {
            "title": "Recall",
            "content": "0.1477 0.1429 0.8232 0.0726 Table 4: Retraction performance for Llama3.1-8B under the is-appended setting. At the same time, belief steering does more than just changing immediate next token, as evidenced by the stop rate of Qwen2.5-7B and Olmo2-7B. To further demonstrate this, we append is after the given answer to prevent early stopping, e.g., Hilary Clinton is[Model generation continues from here...]. As shown in Table 4, simply appending continuation token can, in some cases, increase retraction recall for Llama3.1-8B, leading to improved retraction performance. Belief steering under this is-appended setting still further increases retraction recall, indicating that its influence extends beyond influencing the immediate next token. 5.2 Beliefs Influence Retraction at Later Tokens Primarily via Attention Value Vectors So far, we have shown that belief steering influences retraction behavior after the token following the answer. Since we only applying steering at the last token of the answer, this effect must involve the models attention mechanism. Here, we investigate how belief steering alters attention outputs at later timesteps to influence retraction. Belief steering changes attention weights. We start by measuring how belief steering changes attention weights. One hypothesis is that models fail to retract when they do not sufficiently attend to the given answer. To see if belief steering influences retraction by modulating attention to the given answer, we calculate the attention weights from the last token of the answer to the answer span. Table 5 presents the average change in attention weights under different belief steering directions. Consistent with our hypothesis, negative belief steering increases the models attention to the entity name when generating the next token, while positive belief steering decreases it. Llama3.1-8B Qwen2.5-7B Olmo2-7B WIKI. CELEB. WIKI. CELEB. WIKI. CELEB. No SteeringBeliefNo SteeringBelief+ 0.0329 -0.0056 0.0369 -0.0110 0.0413 -0. 0.0307 -0.0093 0.0360 -0.0019 0.0350 -0.0051 Table 5: Change in attention weights to the answer span. Attention values have stronger causal influence on retraction than attention weights. Is this change in attention weights the primary way that beliefs influence retraction? We conduct patching experiments (Meng et al., 2022; Geva et al., 2023) to answer this question. Instead of directly adding steering vectors to the hidden states of each layer, we selectively retain specific components, such as attention weights or attention value vectors, from the steered model, and patch them into an unsteered model. In this setup, the model itself is not steered; rather, the decisive influence comes from the patched module, allowing us to pinpoint which components are responsible for the observed behavioral changes. We experiment with patching attention weights from salient heads (i.e., heads whose attention to the answer changes significantly after steering), as well as attention value vectors at all layers, for the last token of the answer (Refer to Appendix B.5 for implementation details). We present patching results for Llama3.1-8B in Table 6. First, we find that although steering indeed changes attention weights (c.f., Table 5), patching attention weights alone has relatively minor impact on retraction recall, especially under negative steering. The relatively stronger effect in the positive direction might be because the model can then simply copy attributes from the question. In contrast, negative steering may have limited or no effect if the answer representations lack negation-related cues or factually correct attributes. This motivates us to patch the attention value vectors, as belief steering may not only shift the models attention focus but also alter the attended representations. 7 WIKIDATA CELEBRITY WIKIDATA CELEBRITY Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. No Steer 0.9012 0.2579 0.7722 0. No Steer 0.8254 0.5740 0.8310 0.1429 beliefPatch 0.8325 0.5249 Patch 0.5157 Full Steer belief+ Patch 0.8984 0.9700 Patch 1.0000 Full Steer 0.2729 0.5441 0.9268 0.7113 0.6351 0.4803 0.1671 0.3245 0.7676 0.1913 0.1614 0.0067 0.7333 0.6552 0. 0.1065 0.0920 0.0291 beliefPatch 0.7694 0.5069 Patch 0.5026 Full Steer belief+ Patch 0.8261 0.9851 Patch 0.9847 Full Steer 0.5940 0.9784 0.9717 0.8228 0.5055 0.4836 0.1574 0.5569 0. 0.5691 0.3311 0.3211 0.8261 0.7955 0.8108 0.1380 0.0847 0.0726 Table 6: Patching results for Llama3.1-8B on continuation test sets. Table 7: Patching results for Llama3.1-8B under the is-appended setting. Patching attention value vectors restores more the retraction behavior observed with full steering in both directions. This implies that belief steering primarily acts by modifying the internal representation of the answer, in addition to affecting next token prediction. In Table 7, we also present patching results for Llama3.1-8B under the is-appended setting, to mitigate the effect of next-token prediction. When this influence is reduced, attention value vectors play more prominent role. This is also verified by experiments on Qwen2.5-7B and Olmo2-7B as shown in Appendix C.3."
        },
        {
            "title": "6 Supervised Fine-Tuning Can Learn Better Internal Belief",
            "content": "Given that supervised fine-tuning can enhance existing capabilities of LLMs (Prakash et al., 2024; Yang et al., 2024c), do our findings on the role of model beliefs in retraction generalize to finetuning models with enhanced retraction performance? In this section, we show that straightforward supervised fine-tuning can help LLMs develop better internal beliefs about the factual correctness of generated answers and thus improve retraction performance. WIKIDATA CELEBRITY Precision 0.9012 0.7815 0.5013 0. Recall 0.2529 0.8453 1.0000 0.2845 Precision 0.7722 0.8988 0.5092 0. Recall 0.1477 0.9031 1.0000 0.5763 Baseline SFT Belieffor SFT Belief+ for SFT Table 8: In-distribution supervised fine-tuning results and follow-up steering performance for LLaMA3.1-8B. Steering directions from the original model are reused on the fine-tuned model. For each of our datasets, we synthetically construct an in-distribution supervised fine-tuning training set (e.g., using the WIKIDATA training set). Specifically, we append is the correct answer. to correct examples that contain factually correct answers in the training dataset, and is not the correct answer. to wrong examples, and use LoRA (Hu et al., 2022) to fine-tune LLMs. The results of Llama3.1-8B are shown in Table 8. We can see that supervised fine-tuning effectively teaches the model appropriate retraction behavior. The model learns to distinguish between factually correct and incorrect answers and respond accordingly, i.e., saying is the correct answer to correct answers and saying is not the correct answer to incorrect ones. We note that out-of-distribution performance is worse than in-distribution performance as detailed in Appendix C.4.1; we view our findings as proof-of-concept and hypothesize that larger and more diverse training dataset could yield robust retraction capabilities. Then we apply the same belief steering vectors from the original model and the same hyperparameters2 to steer the fine-tuned model. As shown in Table 8, the steering vectors can be generalized to the 2Note that these may not be the optimal hyperparameters. In fact, extending steering from layers 6-14 to 6-20 reduces retraction recall on Belief+ CELEBRITY set from 0.5763 to 0.2300 with no change in response format. 8 fine-tuned model and change its retraction behavior in both directions, without altering its response format, i.e., is (not) the correct answer. This suggests that, even though fine-tuning greatly alters the models retraction behavior, the underlying mechanisms remain the same, and even the same subspace from the original model can be used to steer the fine-tuned model. Similar results for Qwen2.5-7B and Olmo2-7B, presented in Appendix C.4.2, further confirm this observation. (a) Llama3.1-8B on WIKIDATA. (b) Llama3.1-8B on CELEBRITY. Figure 4: Average probe scores across layers for Llama3.1-8B (Base) and its fine-tuned variant (SFT). denotes correct examples, and denotes wrong examples. Finally, we probe the models internal beliefs after supervised fine-tuning. As shown in Figure 4, factually incorrect answers previously receive high probe prediction scores, indicating that the model tends to treat them as correct. But after supervised fine-tuning, these scores decrease, suggesting that the model has learned to recognize factually incorrect answers and respond appropriately. The performance in the later layers on CELEBRITY shows some deviation, possibly because top-layer representations are more focused on surface-level decoding. Since we reuse the probes from the original model without re-training, there might also be some distribution shift. Nevertheless, the larger gap between probe scores for correct and wrong examples indicates that supervised fine-tuning enables LLMs to form more accurate internal beliefs."
        },
        {
            "title": "7 Conclusions and Limitations",
            "content": "In this paper, we evaluate and analyze the underlying mechanisms behind retraction in LLMs. Using our model-specific continuation datasets, we find that while LLMs are capable of retracting their own incorrect answers, they do so infrequently. Through probing and steering experiments, we demonstrate that retraction is causally influenced by the models internal belief: model fails to retract an incorrect answer because it genuinely believes it is correct. We further show that beliefs guide retraction by affecting both the surface-level token predictions and deeper attention dynamics. More encouragingly, these mechanisms generalize to supervised fine-tuned models. Our work contributes to the development of more transparent and reliable LLMs. There are several limitations for future research. First, although different LLMs share the same overall retraction mechanismbeing causally influenced by the models internal beliefthe specific layers where this influence is most pronounced vary across models. As shown in Appendix B.4, steering at early to mid layers is effective for Llama3.1-8B and Qwen2.5-7B, whereas Olmo2-7B requires intervention at higher layers to elicit stronger retraction. These differences likely stem from variations in training recipes, including data and optimization strategies used. Second, our analysis focuses on short-answer knowledge-related question answering tasks. One natural extension is to long-form generation, such as Name 15 politicians who were born in New York City. This introduces new challenges, including how to accurately locate each generated answer and how to isolate the influence of earlier outputs on later ones. Moreover, as many self-correction studies target reasoning tasks, it would be valuable to examine whether our findings generalize to that domain. However, caution is needed to disentangle limitations in retraction from other capabilities required for reasoning tasks, such as arithmetic computation and problem understanding."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Sincere thanks to Tingyun Chang and everyone in Allegro Lab. This work was supported in part by the National Science Foundation under Grant No. IIS-2403436. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
        },
        {
            "title": "References",
            "content": "Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, and Jingbo Shang. Can llms learn from previous mistakes? investigating llms errors to boost for reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 30653080. Association for Computational Linguistics, 2024. doi: 10.18653/ V1/2024.ACL-LONG.169. URL https://doi.org/10.18653/v1/2024.acl-long.169. Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, and Fuli Feng. Evaluating mathematical reasoning of large language models: focus on error identification and correction. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1131611360. Association for Computational Linguistics, 2024a. doi: 10.18653/V1/2024. FINDINGS-ACL.673. URL https://doi.org/10.18653/v1/2024.findings-acl.673. Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh, Michel C. Desmarais, and Giuliano Antoniol. Bugs in large language models generated code: an empirical study. Empir. Softw. Eng., 30(3):65, 2025. doi: 10.1007/S10664-025-10614-4. URL https://doi.org/10. 1007/s10664-025-10614-4. Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi. Sirens song in the AI ocean: survey on hallucination in large language models. CoRR, abs/2309.01219, 2023. doi: 10.48550/ARXIV.2309.01219. URL https://doi.org/10.48550/ arXiv.2309.01219. Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. The dawn after the dark: An empirical study on factuality hallucination in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1087910899. Association for Computational Linguistics, 2024b. doi: 10.18653/V1/2024.ACL-LONG.586. URL https://doi.org/10. 18653/v1/2024.acl-long.586. Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model hallucinations can snowball. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024a. URL https://openreview. net/forum?id=FPlaQyAGHu. Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong Meng, Mo Yu, Bowen Zhou, and Jie Zhou. On large language models hallucination with regard to known facts. In Kevin Duh, Helena Gómez-Adorno, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pages 10411053. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. NAACL-LONG.60. URL https://doi.org/10.18653/v1/2024.naacl-long.60. Adi Simhi, Jonathan Herzig, Idan Szpektor, and Yonatan Belinkov. Distinguishing ignorance from error in LLM hallucinations. CoRR, abs/2410.22071, 2024. doi: 10.48550/ARXIV.2410.22071. URL https://doi.org/10.48550/arXiv.2410.22071. Kenneth Li, Oam Patel, Fernanda B. Viégas, Hanspeter Pfister, and Martin Wattenberg. Inferencetime intervention: Eliciting truthful answers from language model. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, 10 Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 81b8390039b7302c909cb769f8b6cd93-Abstract-Conference.html. Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks. Representation engineering: top-down approach to AI transparency. CoRR, abs/2310.01405, 2023. doi: 10.48550/ARXIV.2310.01405. URL https: //doi.org/10.48550/arXiv.2310.01405. Amos Azaria and Tom M. Mitchell. The internal state of an LLM knows when its lying. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 967976. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.68. URL https: //doi.org/10.18653/v1/2023.findings-emnlp.68. Ziwei Xu, Sanjay Jain, and Mohan S. Kankanhalli. Hallucination is inevitable: An innate limitation of large language models. CoRR, abs/2401.11817, 2024. doi: 10.48550/ARXIV.2401.11817. URL https://doi.org/10.48550/arXiv.2401.11817. Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces hallucination in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 3563 3578. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL. 212. URL https://doi.org/10.18653/v1/2024.findings-acl.212. Mert Yüksekgönül, Varun Chandrasekaran, Erik Jones, Suriya Gunasekar, Ranjita Naik, Hamid Palangi, Ece Kamar, and Besmira Nushi. Attention satisfies: constraint-satisfaction lens In The Twelfth International Conference on Learning on factual errors of language models. Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=gfFVATffPd. Lukas Berglund, Meg Tong, Maximilian Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. The reversal curse: Llms trained on \"a is b\" fail to learn \"b is a\". In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=GPKTIktA0k. Junteng Liu, Shiqi Chen, Yu Cheng, and Junxian He. On the universal truthfulness hyperplane inside llms. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 1819918224. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.emnlp-main.1012. Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning enhances existing mechanisms: case study on entity tracking. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=8sKcAWOf2D. Benjamin A. Levinstein and Daniel A. Herrmann. Still no lie detector for language models: Probing empirical and conceptual roadblocks. CoRR, abs/2307.00175, 2023. doi: 10.48550/ARXIV.2307. 00175. URL https://doi.org/10.48550/arXiv.2307.00175. Stefan F. Schouten, Peter Bloem, Ilia Markov, and Piek Vossen. Truth-value judgment in language models: belief directions are context sensitive. CoRR, abs/2404.18865, 2024. doi: 10.48550/ ARXIV.2404.18865. URL https://doi.org/10.48550/arXiv.2404.18865. Samuel Marks and Max Tegmark. The geometry of truth: Emergent linear structure in large language model representations of true/false datasets. CoRR, abs/2310.06824, 2023a. doi: 10.48550/ARXIV. 2310.06824. URL https://doi.org/10.48550/arXiv.2310.06824. 11 Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. URL https://openreview.net/forum?id=ETKGuby0hcs. Sky CH-Wang, Benjamin Van Durme, Jason Eisner, and Chris Kedzie. Do androids know theyre only dreaming of electric sheep? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 44014420. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.260. URL https://doi.org/10.18653/v1/ 2024.findings-acl.260. Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, and Yonatan Belinkov. Llms know more than they show: On the intrinsic representation of LLM hallucinations. CoRR, abs/2410.02707, 2024. doi: 10.48550/ARXIV.2410.02707. URL https: //doi.org/10.48550/arXiv.2410.02707. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, In The Twelfth and Denny Zhou. Large language models cannot self-correct reasoning yet. International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum?id=IkmD3fKBPQ. Gladys Tyen, Hassan Mansoor, Victor Carbune, Peter Chen, and Tony Mak. Llms cannot find reasoning errors, but can correct them given the error location. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1389413908. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.FINDINGS-ACL.826. URL https: //doi.org/10.18653/v1/2024.findings-acl.826. Zhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan Tan, Chao Shen, and Meng Jiang. Large language models can self-correct with minimal effort. CoRR, abs/2405.14092, 2024. doi: 10. 48550/ARXIV.2405.14092. URL https://doi.org/10.48550/arXiv.2405.14092. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/ paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html. Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, and Minlie Huang. Understanding the dark side of llms intrinsic self-correction. CoRR, abs/2412.14959, 2024b. doi: 10.48550/ARXIV.2412.14959. URL https://doi.org/10.48550/arXiv.2412.14959. Tengxiao Liu, Qipeng Guo, Yuqing Yang, Xiangkun Hu, Yue Zhang, Xipeng Qiu, and Zheng Zhang. Plan, verify and switch: Integrated reasoning with diverse x-of-thoughts. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 28072822. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.169. URL https://doi.org/10.18653/v1/2023.emnlp-main.169. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. CoRR, abs/2207.05221, 2022. doi: 10.48550/ARXIV.2207.05221. URL https://doi.org/10.48550/arXiv.2207.05221. 12 Zhe Yang, Yichang Zhang, Yudong Wang, Ziyao Xu, Junyang Lin, and Zhifang Sui. Confidence v.s. critique: decomposition of self-correction capability for llms. CoRR, abs/2412.19513, 2024a. doi: 10.48550/ARXIV.2412.19513. URL https://doi.org/10.48550/arXiv.2412.19513. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.48550/arXiv.2407.21783. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024b. doi: 10.48550/ARXIV.2412.15115. URL https://doi.org/10.48550/arXiv.2412.15115. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi. 2 olmo 2 furious. CoRR, abs/2501.00656, 2025. doi: 10.48550/ARXIV.2501.00656. URL https: //doi.org/10.48550/arXiv.2501.00656. Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/hash/ 91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur P. Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: benchmark for question answering research. Trans. Assoc. Comput. Linguistics, 7:452466, 2019. doi: 10.1162/TACL_A_00276. URL https://doi.org/10. 1162/tacl_a_00276. Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. In Regina Barzilay and Min-Yen Kan, editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers, pages 16011611. 13 Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL https: //doi.org/10.18653/v1/P17-1147. Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice science questions. In Leon Derczynski, Wei Xu, Alan Ritter, and Tim Baldwin, editors, Proceedings of the 3rd Workshop on Noisy User-generated Text, NUT@EMNLP 2017, Copenhagen, Denmark, September 7, 2017, pages 94106. Association for Computational Linguistics, 2017. doi: 10.18653/V1/ W17-4413. URL https://doi.org/10.18653/v1/w17-4413. Sam Marks and Max Tegmark. Diff-in-means concept editing is worst-case optimal, 2023b. URL https://blog.eleuther.ai/diff-in-means/. Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by single direction. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers.nips.cc/paper_files/paper/2024/hash/ f545448535dfde4f9786555403ab7c49-Abstract-Conference.html. Alexander Matt Turner, Lisa Thiergart, David Udell, Gavin Leech, Ulisse Mini, and Monte MacDiarmid. Activation addition: Steering language models without optimization. CoRR, abs/2308.10248, 2023. doi: 10.48550/ARXIV.2308.10248. URL https://doi.org/10.48550/ arXiv.2308.10248. Bruce W. Lee, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Erik Miehling, Pierre L. Dognin, Manish Nagireddy, and Amit Dhurandhar. Programming refusal with conditional activation steering. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=Oi47wc10sm. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/ hash/6f1d43d5a82a37e89b0665b33bf3a182-Abstract-Conference.html. Mor Geva, Jasmijn Bastings, Katja Filippova, and Amir Globerson. Dissecting recall of factual associations in auto-regressive language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1221612235. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.751. URL https: //doi.org/10.18653/v1/2023.emnlp-main.751. Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. Alignment for honesty. In Amir Globersons, Lester Mackey, Danielle Belgrave, Angela Fan, Ulrich Paquet, Jakub M. Tomczak, and Cheng Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024c. URL http://papers.nips.cc/paper_files/ paper/2024/hash/7428e6db752171d6b832c53b2ed297ab-Abstract-Conference.html. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=nZeVKeeFYf9. OpenAI. Openai o1 system card, 2024. URL https://openai.com/index/ openai-o1-system-card/. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, 14 Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. doi: 10.48550/ARXIV.2501.12948. URL https://doi.org/10.48550/arXiv.2501.12948. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, 2025a. URL https: //qwenlm.github.io/blog/qwq-32b/. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. Do NOT think that much for 2+3=? on the overthinking of o1-like llms. CoRR, abs/2412.21187, 2024. doi: 10.48550/ARXIV.2412.21187. URL https://doi.org/10.48550/arXiv.2412.21187. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, and Xia Ben Hu. Stop overthinking: survey on efficient reasoning for large language models. CoRR, abs/2503.16419, 2025. doi: 10.48550/ARXIV.2503.16419. URL https://doi.org/10.48550/arXiv.2503.16419. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. Reasoning models know when theyre right: Probing hidden states for self-verification, 2025. URL https: //arxiv.org/abs/2504.05419. Qwen Team. Qwen3: Think deeper, act faster, 2025b. URL https://qwenlm.github.io/blog/ qwen3/. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. CoRR, abs/2403.13372, 2024. doi: 10.48550/ARXIV.2403.13372. URL https://doi.org/10.48550/arXiv.2403.13372."
        },
        {
            "title": "A Preliminary Analysis of Large Reasoning Models",
            "content": "Recent large reasoning models (LRMs) such as OpenAI o1 (OpenAI, 2024), DeepSeek-R1 (DeepSeekAI et al., 2025), and QwQ (Qwen Team, 2025a) are known for automatic self-reflection, where the model reflects on its own responses without any external hints or instructions in their thinking mode. However, beyond necessary self-correction triggered by incorrect answers or flawed reasoning, these models have also been reported to exhibit overthinking or redundant self-reflection (Chen et al., 2024; Sui et al., 2025). That is to say, they habitually double-check or find an alternative answer even if they know the previous answer is correct (Zhang et al., 2025), placing them at the opposite extreme from the models we study like Llama3.1-8B-Instruct. Nonetheless, there appear to be connections between LLMs and LRMs. According to our experiments in Section 5.1, when Llama3.1-8B is forced to continue generating after the answer, its retraction performance improves. This parallels the typical behavior of LRMs, which tend to produce follow-up verification content by default, as illustrated in Figure 5. Section 6 also offers insights into the potential gap between LLMs and LRMs, and how post-training may help bridge it. Lastly, we observe that an LRM in non-thinking mode tends to behave like non-thinking model (See Figure 6). This leaves open the question of how self-correction mechanisms differ between the two modes within the same model. Figure 5: Qwen3-32B (Qwen Team, 2025b) in thinking mode. Figure 6: Qwen3-32B in non-thinking mode."
        },
        {
            "title": "B Experimental Details",
            "content": "B.1 Details of Original Datasets 16 We focus on knowledge-related question answer tasks, where it is transparent whether an LLM has the necessary knowledge to identify its mistakes. To facilitate the study of retraction, we collect questions from two datasets, WIKIDATA and CELEBRITY, which are easy to induce hallucinations. The number of questions in each split of the datasets is reported in Table 9 # Train # Test"
        },
        {
            "title": "WIKIDATA\nCELEBRITY",
            "content": "2000 1584 1160 800 Table 9: Number of questions. WIKIDATA. WIKIDATA was originally proposed by Dhuliawala et al. (2024), and is characterized by each question containing two constraintsprofession and birthplaceboth of which must be satisfied for the answer to be correct. This makes the task challenging for LLMs, resulting in relatively low accuracy. However, the original dataset was not publicly released. To reconstruct it, we collect set of popular professions and cities, and generate new questions by paring them. We retain only those combinations for which correct exists. For accuracy evaluation, we query the Wikidata API3. An example question is: Name writer who was born in Oran, Algeria. CELEBRITY. CELEBITY was originally introduce by Berglund et al. (2024). In their work, they highlighted the reversal curse: LLMs can more easily answer questions about celebritys parent (e.g., Who is Tom Cruises mother?) than the reverse (e.g., Who is Mary Lee Pfeiffers son?, where the correct answer is Tom Cruise). We focus on the reverse questions. However, in their evaluation, model was prompted 10 times per question and considered correct if it produced the target answer (i.e., the celebrity child) at least once. This evaluation cannot determine whether an answer is correct. To address this, we reconstruct the dataset by collecting list of celebrities, their parents, and all children of those parents. This allows us to directly compare the models answer with the ground truth set of valid answers. An example question is: Name child of Joe Jackson. B.2 Details of Continuation Datasets In addition to constructing wrong examples, we also include correct examples with factually correct answers, for both evaluation and training purposes. To avoid introducing bias during supervised finetuning, we aim to maintain balanced number of correct and wrong examples. Since model-generated answers are often incorrect on these datasets, we supplement the correct examples by selecting gold answers for which the model answers the corresponding verification questions correctly. We create the training and test sets using questions from the train and test splits of the original datasets, respectively. However, Qwen2.5-7B and Olmo2-7B know little about the correct answers on CELEBRITY. As result, in Table 1, only their test sets for CELEBRITY are imbalanced, containing 1000 incorrect examples and smaller number of correct ones. Additionally, these two models do not have training set for CELEBRITY, which only impacts the in-distribution supervised fine-tuning experiments discussed in Section 6. Importantly, our findings are consistently supported across the other four settings: Llama3.1-8B + WIKIDATA, Llama3.1-8B + CELEBRITY, Qwen2.5-7B + WIKIDATA, and Olmo2-7B + WIKIDATA. B.3 Retraction Detection Using LLM-as-a-Judge The prompt for retraction detection using Llama3.3-70B-Instruct is shown below. Note that we use four different demonstrations for the WIKIDATA and CELEBRITY datasets in order to improve judgment accuracy. We manually evaluated 40 randomly selected examples for each judge prompt. Llama3.3-70B-Instruct showed high agreement with human judgment, achieving 39/40 on WIKIDATA and 38/40 on CELEBRITY. 3https://query.Wikidata.org/"
        },
        {
            "title": "Retraction Detection Prompt",
            "content": "You will be given users question, an assistants response, and the initial answer provided in the assistants response. Your task is to determine whether the assistant explicitly stated, after giving the initial answer, that the answer does not fully satisfy the users question. Guidelines: 1. Step-by-step analysis: Carefully examine the assistants response for explicit statements indicating that the initial answer does not meet one or more requirements of the users question. Explicit statements may include words like but or however, or they may mention information that highlights conflict or discrepancy with the users question. 2. Focus only on the assistants statements. Do not use your own knowledge to judge whether the initial answer is correct or not. Output Format: 1. Analysis: Provide clear explanation of which part(s) of the assistants response show that the initial answer does not fully satisfy the users question, or explain why there is no such indication. 2. Output: Output True if the assistant indicated that the initial answer does not fully satisfy the users question. Otherwise, output False. Here are few examples for reference: Example 1: User Question: Name an actor who was born in Sheffield, United Kingdom. Assistant Response: Michael Palin. Initial Answer: Michael Palin Analysis: The assistant provided the answer Michael Palin but did not include any explicit statement indicating that the answer does not fully satisfy the users question. There is no mention of conflict or discrepancy with the questions requirements. Output: False Example 2: User Question: Name an association football manager who was born in Sydney, Australia. Assistant Response: Guus Hiddink, born in Melbourne, Australia (near Sydney, Australia). Initial Answer: Guus Hiddink Analysis: The assistant explicitly mentioned that Guus Hiddink was born in Melbourne, Australia, which is near Sydney, but not in Sydney as specified in the question. This indicates discrepancy between the answer and the users question. Output: True [Omit the other two examples here...] Now, process the following: User Question: {question} Assistant Response: {response} Initial Answer: {models answer} B.4 Hyperparameters for Steering The choice of steering layers and strength is critical to clearly demonstrate the effect of steering without compromising the models original capabilities. Similar to other works in activation steering, we manually search for appropriate steering hyperparameters. Specially, we randomly construct 10 additional wrong WIKIDATA examples as validation set and select hyperparameters based on the following criteria: using minimal set of layers and the smallest effective strength that still preserves natural generation. Table 10 compares our selected configuration and oversteered settings. Although hyperparameters are chosen using only wrong WIKIDATA examples for negative belief steering, 18 Question Properly Steered Response Oversteered Response Name poet who was born in Panama City, Panama. Giannina Braschi the answer, however, Giannina Braschi was born in San Juan, Puerto Rico. is not nor Omar Giannina Braschi Cabezas are not the answer am looking for. Name television actor who was born in Johannesburg, South Africa. Sterling K. Brown isnt from Johannesburg, South Africa. The actor born there is Sharlto Copley. Sterling K. Brown Nope, thats incorrect. Let me try again. Jonny Lee Miller was born in Johannesburg, South Africa. Table 10: Comparison between properly steered and oversteered responses. (1) When steering Llama3.1-8B from layers 6-14 to layers 0-30, the model consistently generates nor following the given answer. Although it can be regarded as retraction, the phrasing is unnatural. (2) When increasing the steering strength α from 1.5 to 3.0 for Olmo2-7B, the model frequently generates Nope or notwithstanding right after the given answer, which is also not natural."
        },
        {
            "title": "Layers",
            "content": "Strength α Llama3.1-8B Qwen2.5-7B Olmo2-7B 6-14 10-18 8-30 1.2 2.5 1.5 Table 11: Steering hyperparameters. they generalize well to positive belief steering, positive examples, the CELEBRITY dataset, and the is-appended setting, demonstrating the generalizability of the belief steering. The final choices are listed in Table 11. B.5 Implementation Details for Patching Patching Attention Weights. First, we identify the top-K (K = 48) salient heads at the last token position of the answerspecifically, those whose attention weights to the answer change most significantly between negative and positive belief steering. Then we patch the model by replacing the attention weights of these heads with the steered values, without directly applying full steering to the model. Patching Attention Value Vectors. We patch the attention value vectors at all layers for the last token of the answer. Note that since steering may not start from the first layer, the value vectors in the earlier layers remain unchanged in practice. B.6 Hyperparameters for Supervised Fine-Tuning We fine-tune models using LoRA for 2 epochs with learning rate of 1e4 and batch size of 8, implemented via LLaMA-Factory (Zheng et al., 2024). During training, loss is computed on the assistants response, excluding the prompt. All experiments including probing, steering, and supervised fine-tuning, are conducted on single A6000 GPU."
        },
        {
            "title": "C Additional Results",
            "content": "C.1 Probing Plots Since the retraction recall of Qwen2.5-7B and Olmo2-7B on CELEBRITY is below 3%, the number of WR examples is too small to be statistically meaningful. Therefore, we report probe scores for these two models only on WIKIDATA, as shown in Figure 7. Both models consistently separate WN and WR examples into distinct categories. 19 (a) Qwen2.5-7B on WIKIDATA. (b) Olmo2-7B on WIKIDATA. Figure 7: Average probe scores across layers for Qwen2.5-7B and Olmo2-7B on the WIKIDATA test set, grouped by factual correctness and retraction behavior. C.2 Other Steering Directions Except for the belief direction, we also try another two directions that are likely to affect retraction behavior. (1) WIKIDATA retraction direction: The positive examples are those that the model actually retracts from the WIKIDATA training set, and negative examples are those that the model does not retract. (2) WIKIDATA correctness direction: The positive examples contain factually correct answers from the WIKIDATA training set, and negative examples contain factually incorrect answers. We search for the best hyperparameters as described in Appendix B.4, and find that those used in belief steering yield the best retraction performance among the hyperparameters we explored for Llama3.1-8B. We show the results in Table 12. No Steering BeliefWIKIDATA Retraction+ WIKIDATA CorrectnessBelief+ WIKIDATA RetractionWIKIDATA Correctness+ WIKIDATA CELEBRITY Precision 0.9012 0.5157 0.5029 0.5075 1.0000 0 0.5000 Recall 0.2579 0.9268 0.7321 0.7903 0.0067 0 0.0083 Precision 0.7722 0.4803 0.5638 0. 0.5217 0.6667 0.6667 Recall 0.1477 0.7676 0.6634 0.5569 0.0291 0.0048 0.0097 Table 12: Retraction Performance for Llama3.1-8B on continuation test sets. WIKIDATA CELEBRITY WIKIDATA CELEBRITY Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. No Steer BeliefBelief+ 0.8824 0.5051 1.0000 0.1119 0.8358 0.0131 0.9667 0.8547 1.0000 0.0290 0.7000 0.0090 No Steer BeliefBelief+ 0.9881 0.5206 1.0000 0.1317 0.7619 0.0016 0.8824 0.8217 0 0.0150 0.7420 0 Table 13: Retraction Performance for Qwen2.57B on continuation test sets. Table 14: Retraction Performance for Olmo2-7B on continuation test sets. It can be observed that both in-distribution steering directions suffer from poor generalization to out-of-distribution data, as evidenced by their unsatisfactory performance on the CELEBRITY dataset. Additionally, for the WIKIDATA retraction direction, the mean hidden state representations may be unrepresentative due to (1) limited number of retracted examples serving as positive examples, 20 and (2) the use of in-distribution data. As result, the derived linear direction leads to unnatural generation. Notably, around 57% of retracted examples on the WIKIDATA test set, produced via positive WIKIDATA retraction steering, take form of {models answer}s [friend/teammate/son/etc.]. This may be influenced by the training datawhere 18% retracted examples follow this pattern, compared to only 1% of non-retracted examples. While this can technically be considered retraction (and is judged as such by Llama3.3-70B-Instruct), the phrasing is awkward. This pattern persists across different steering hyperparameter settings. C.3 Patching Results Patching results under is-appended setting for Qwen2.5-7B and Olmo2-7B are shown in Table 15 and 16. As we can see, patching attention weights is useless for both models, while patching the steered models attention value vectors significantly regulates retraction. Note that for Olmo2-7B, we increase the original α from 1.5 to 5.0 to make belief steering effective under is-appended setting. This implies that, at α = 1.5, belief steering in Olmo2-7B primarily takes effect through next token prediction. Nevertheless, larger α values still modify the attention value vectors in manner consistent with our overall conclusions. This discrepancy likely arises from differences in the training recipes across LLMs. WIKIDATA CELEBRITY WIKIDATA CELEBRITY Prec. Rec. Prec. Rec. Prec. Rec. Prec. Rec. No Steer 0.8500 0.0951 1.0000 0.0320 No Steer 1.0000 0.0730 1.0000 0.0130 beliefPatch 0.8846 0.5209 Patch 0.5079 Full Steer belief+ Patch 0.8814 0.9375 Patch 0.9444 Full Steer 0.0877 0.9049 0.8955 1.0000 0.8371 0.8601 0.0340 0.3700 0.7560 0.0970 0.0280 0.0317 1.0000 1.0000 1.0000 0.0310 0.0270 0. beliefPatch 0.9767 0.5012 Patch 0.5140 Full Steer belief+ Patch 1.0000 0.9200 Patch 1.0000 Full Steer 0.0667 0.9762 0.5810 1.0000 0.8230 0.6980 0.0140 0.9580 0.1410 0.0619 0.0365 0. 1.0000 0.9545 1.0000 0.0170 0.0210 0.0150 Table 15: Patching results for Qwen2.5-7B under the is-appended setting. Table 16: Patching results for Olmo2-7B under the is-appended setting with α = 5.0. C.4 Supervised Fine-tuning Results C.4.1 Out-of-distribution Results Table 17 shows the out-of-distribution supervised fine-tuning results for Llama3.1-8B. As expected, performance in the out-of-distribution setting is lower than in the in-distribution case. To support broader applicability, developing diverse, large-scale, retraction-focused supervised fine-tuning dataset holds great promise. WIKIDATA CELEBRITY Precision Recall Precision Recall Baseline 0.9012 0.2529 0.7722 0.1477 In-distribution SFT Out-of-distribution SFT UTQA SFT 0.7815 0.5180 0. 0.8453 0.6705 0.5225 0.8988 0.6635 0.6244 0.9031 0.3390 0.2978 Table 17: Out-of-distribution supervised fine-tuning results for Llama3.1-8B. The first row indicates the evaluation dataset. For Out-of-distribution SFT, the model is trained on the training set of the other dataset. UTQA SFT denotes fine-tuning on the UTQA training set. 21 C.4.2 SFT Results for Qwen and Olmo Building on Llama3.1-8B, we demonstrate that our findings on the causal relationship between model belief and retraction generalize to supervised fine-tuned models. This is further supported by results from Qwen2.5-7B and Olmo2-7B. As shown in Table 18, the same belief steering directions remain effective after fine-tuning. Additionally, Figure 8 indicates that supervised fine-tuning leads to more accurate internal beliefs. Qwen2.5-7B Olmo2-7B"
        },
        {
            "title": "Precision",
            "content": "0.8824 0.8350 0.5023 0."
        },
        {
            "title": "Recall",
            "content": "0.1119 0.7929 1.0000 0."
        },
        {
            "title": "Precision",
            "content": "0.9881 0.8869 0.5179 0."
        },
        {
            "title": "Recall",
            "content": "0.1317 0.8460 0.9873 0."
        },
        {
            "title": "Baseline",
            "content": "SFT Belieffor SFT Belief+ for SFT Table 18: In-distribution supervised fine-tuning results for Qwen2.5-7B and Olmo2-7B on WIKIDATA. (a) Qwen2.5-7B on WIKIDATA. (b) Olmo2-7B on WIKIDATA. Figure 8: Average probe scores across layers for Qwen2.5-7B, Olmo2-7B (Base), and their fine-tuned variants (SFT). C.4.3 Practical Application The continuation setting is synthetic setup designed to facilitate controlled study. Here, we consider more realistic scenario: given question from WIKIDATA or CELEBRITY, what does supervised fine-tuning achieve? The results are shown in Table 19. While supervised fine-tuning does not improve accuracy, it substantially enhances retraction performance, thereby making the model more reliable. WIKIDATA CELEBRITY Precision Recall Accuracy Precision Recall Accuracy Llama3.1-8B Llama3.1-8B SFT 0.9928 0.9481 0.2715 0.7079 0.0841 0.0840 0.8125 0.9774 0.0884 0.8162 0.3163 0. Table 19: Supervised fine-tuning results for Llama3.1-8B in realistic setting. Note that we still use in-distribution evaluation: the fine-tuned model tested on WIKIDATA questions is trained on the corresponding WIKIDATA training set."
        }
    ],
    "affiliations": [
        "University of Southern California"
    ]
}