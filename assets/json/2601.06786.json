{
    "paper_title": "EpiCaR: Knowing What You Don't Know Matters for Better Reasoning in LLMs",
    "authors": [
        "Jewon Yeom",
        "Jaewon Sok",
        "Seonghyeon Park",
        "Jeongjae Park",
        "Taesup Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring a substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as a form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemically-calibrated reasoning (EpiCaR) as a training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Pareto-superiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables a 3X reduction in inference compute, matching the K=30 performance of STaR with only K=10 samples in capable models."
        },
        {
            "title": "Start",
            "content": "EPICAR: Knowing What You Dont Know Matters for Better Reasoning in LLMs Jewon Yeom1 Jaewon Sok2 Seonghyeon Park3 Jeongjae Park1 Taesup Kim1,* 1Graduate School of Data Science, Seoul National University 2Department of Rural Systems Engineering, Seoul National University 3Department of Aerospace Engineering, Seoul National University 6 2 0 2 1 1 ] . [ 1 6 8 7 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Improving the reasoning abilities of large language models (LLMs) has largely relied on iterative self-training with model-generated data. While effective at boosting accuracy, existing approaches primarily reinforce successful reasoning paths, incurring substantial calibration cost: models become overconfident and lose the ability to represent uncertainty. This failure has been characterized as form of model collapse in alignment, where predictive distributions degenerate toward low-variance point estimates. We address this issue by reframing reasoning training as an epistemic learning problem, in which models must learn not only how to reason, but also when their reasoning should be trusted. We propose epistemicallycalibrated reasoning (EPICAR) as training objective that jointly optimizes reasoning performance and calibration, and instantiate it within an iterative supervised fine-tuning framework using explicit self-evaluation signals. Experiments on Llama-3 and Qwen-3 families demonstrate that our approach achieves Paretosuperiority over standard baselines in both accuracy and calibration, particularly in models with sufficient reasoning capacity (e.g., 3B+). This framework generalizes effectively to OOD mathematical reasoning (GSM8K) and code generation (MBPP). Ultimately, our approach enables 3 reduction in inference compute, matching the = 30 performance of STaR with only = 10 samples in capable models."
        },
        {
            "title": "Introduction",
            "content": "The advent of Large Language Models (LLMs) has revolutionized complex reasoning tasks such as mathematics and logic. Techniques like Chain-ofThought (CoT) prompting (Wei et al., 2022) have significantly boosted performance by decomposing problems into intermediate steps. However, critical challenge remains: the discrepancy between * Corresponding author. Figure 1: Pareto-Superior Improvement in Reasoning and Reliability. We visualize the relative improvement in reasoning accuracy ( Accuracy, %) and reliability ( ECE reduction) compared to the base model (at the origin). Solid and dashed arrows represent the trajectories of our proposed EPICAR and the STaR baseline, respectively. While standard iterative SFT often incurs trade-off (Calibration Cost Zone), our method consistently drives diverse model families (Llama-3 and Qwen-3) into the Pareto-Superior Zone, achieving simultaneous gains in both task performance and uncertainty calibration. models accuracy and its self-knowledge, or calibration (Kadavath et al., 2022; Guo et al., 2017). Current models frequently exhibit confident misalignment, hallucinating plausible-sounding but incorrect answers with high certainty (OpenAI, 2023; Lin et al., 2022). For example, model may fail at final arithmetic step while assigning near-certainty to its erroneous conclusion. Such behavior highlights systematic failure to represent uncertainty about reasoning outcomes, raising critical concerns for high-stakes applications where knowing when model should not be trusted is as important as producing correct answer. In the pursuit of higher reasoning accuracy, iterative self-training methods such as STaR (Zelikman et al., 2022) and ReST (Gulcehre et al., 2023) have become dominant paradigm, reinforcing successful reasoning paths through ground-truth verifier. While effective for boosting raw accuracy, we argue that this positive-only feedback loop imposes significant calibration cost (Hu et al., 2025) that is often overlooked. The outcome of exclusively reinforcing correct paths is manifestation of Model Collapse (Shumailov et al., 2023), where the models predictive behavior converges toward low-variance point estimates, reinforcing its own biased beliefs while discarding the distributional tails necessary for reliable uncertainty estimation. Recent advancements in reasoning models like DeepSeek-R1 (DeepSeek-AI et al., 2025) have addressed reasoning performance through reinforcement learning (RL) frameworks like GRPO. Evidence suggests that such models can express their confidence more accurately by engaging in Slow Thinking behaviors such as self-verification and backtracking within an extended CoT (Yoon et al., 2025). Furthermore, Zhang et al. (2025) found that models encode correctness signals within their hidden states, yet they often suffer from overthinking and fail to exploit this information during standard generation. While effective, the reliability of these models is strictly tied to the computational overhead of increased inference-time compute. While inference-time scaling offers partial solution, it does not fundamentally resolve the underlying miscalibration of the base policy. In this work, we address this issue by reframing reasoning training as an epistemic learning problem, where models must learn to reason while simultaneously discerning the reliability of their own outputs. We propose epistemically-calibrated reasoning (EPICAR), training objective that jointly optimizes reasoning performance and calibration. By instantiating EPICAR within an iterative SFT framework using explicit self-evaluation signals, we enable models to navigate the trade-off between accuracy and overconfidence. As shown in Figure 1, our framework drives models into the Pareto-Superior Zone, matching high-sample-count performance with significantly lower inference overhead."
        },
        {
            "title": "2 Related Work",
            "content": "Iterative Reasoning & Self-Improvement Selftraining techniques like STaR (Zelikman et al., 2022) and ReST (Gulcehre et al., 2023) bootstrap reasoning capabilities by fine-tuning on selfgenerated correct paths. To address the datainefficiency of discarding incorrect attempts, VSTaR (Hosseini et al., 2024) utilizes both correct and incorrect solutions to train verifier that judges the correctness of generated solutions. Recent approaches explicitly target the models metacognition; for instance, MASA (Kim et al., 2025) aligns the models predictions of solution attributes (e.g., difficulty, length) with actual rollout statistics to enhance training efficiency. While V-STaR requires separate verifier and MASA focuses on metadata for efficient gating, our method addresses the underlying calibration cost (Hu et al., 2025) and Model Collapse (Shumailov et al., 2023) by internalizing the evaluation process directly into the generators objective. Calibration Cost & Alignment Tax Conventional studies on the alignment tax have almost exclusively focused on the degradation of general task performance and accuracy during the alignment process (Lu et al., 2024; Lin et al., 2024; Fu et al., 2024). However, Hu et al. (2025) identify more pervasive side effect: the calibration cost. They argue that while the alignment tax on capability often yields mixed or inconsistent results across different benchmarks, the cost to calibration is universal, manifesting as significant rise in overconfidence that undermines models reliability. Unlike the alignment tax, which concerns what model can do, the calibration cost represents fundamental loss in knowing what it knows. While Hu et al. (2025) propose post-hoc Model Merging to navigate this trade-off, our approach, EPICAR, mitigates this cost intrinsically during the learning process. By employing dual-task objective that balances reasoning performance with reliability, we enable models to achieve Pareto-superiority without the need for extrinsic post-hoc interventions. Calibration & Uncertainty Estimation Calibration approaches have evolved from logit scaling such as Temperature Scaling (Guo et al., 2017) to LLM-specific strategies like Verbalized Confidence (Kadavath et al., 2022; Lin et al., 2022), though verbalized outputs remains susceptible to prompt variations (Xia et al., 2025) and struggle with long-form reasoning (Yang et al., 2025b). To enhance reliability, inference-time ensembles like Self-Consistency (Wang et al., 2024) utilize response consistency but suffer from prohibitive computational costs. Recent literature, notably Yoon et al. (2025), identifies that reasoning models better express their confidence by leveraging Slow Thinking behaviors such as self-verification within an extended CoT. Furthermore, while models encode correctness in hidden states (Zhang et al., 2025), and auxiliary interventions like LitCab (Liu et al., 2024) or separate calibration models (Shen et al., 2024) have been proposed for efficiency, our approach internalizes calibration directly into the generators core objective. By balancing reasoning and self-evaluation within unified SFT task, we achieve robust uncertainty estimation without the need for the extended inference compute."
        },
        {
            "title": "3 Preliminaries",
            "content": "We formally define the problem of calibrated reasoning, the mechanism for verbalized confidence estimation, and the theoretical limitations of iterative frameworks."
        },
        {
            "title": "3.1 Problem Formulation and Verbalized",
            "content": "Confidence Let = {(xi, yi)}N i=1 be dataset of reasoning problems xi and ground truth answers yi. An LLM Pθ generates reasoning path and predicted answer ˆy. Our objective is to maximize Pθ(yx, r) while ensuring reliable confidence score [0, 1]. Following Kapoor et al. (2024), we adopt verbalized confidence estimation. Instead of raw token probabilities, we prompt the model to assess its own correctness via binary query (e.g., Is the answer correct? yes/no). The confidence is the normalized probability of the affirmative token: = Pθ(yes x, r, ˆy) Pθ(yes x, r, ˆy) + Pθ(no x, r, ˆy) (1) This semantic approach captures high-level certainty more effectively than sequence perplexity for reasoning tasks (Kuhn et al., 2023)."
        },
        {
            "title": "Discrimination",
            "content": "To quantify calibration, we employ Expected Calibration Error (ECE) (Guo et al., 2017), which measures the weighted average discrepancy between accuracy and confidence. Let be the total number of evaluation samples. We partition the predictions into equally-spaced intervals (bins) based on their predicted confidence scores, where Bm denotes the set of indices of samples whose confidence falls into the m-th interval. ECE is defined as: ECE = (cid:88) m=1 Bm acc(Bm) conf(Bm) (2) where acc(Bm) and conf(Bm) represent the average accuracy and the average confidence within bin Bm, respectively. perfectly calibrated model achieves an ECE = 0. Furthermore, we report the Brier Score (Brier, 1950) as comprehensive measure of the quality of predicted probabilities. For set of binary outcomes (correct or incorrect), the Brier Score is defined as the mean squared error between the predicted confidence fi and the actual outcome oi {0, 1}: BS ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (fi oi)2 (3) The Brier Score is proper scoring rule that can be decomposed into calibration, resolution, and uncertainty, providing holistic view of the models predictive performance. lower Brier Score indicates better-calibrated and more accurate model. In addition to these metrics, we report the Area Under the Receiver Operating Characteristic curve (AUROC) to evaluate the models discriminative ability. While ECE measures absolute alignment, AUROC assesses the models capacity to assign higher confidence scores to correct answers than to incorrect ones, providing measure of reliability that is invariant to logit scaling or temperature shifts."
        },
        {
            "title": "3.3 The Epistemic Cost of Iterative Training",
            "content": "Standard iterative methods like STaR (Zelikman et al., 2022) utilize positive-only feedback loop, fine-tuning exclusively on successful reasoning paths. We characterize the failure of this approach through the lens of the epistemic learning problem. While aleatoric uncertainty stems from inherent stochasticity, epistemic uncertainty reflects lack of knowledge regarding specific reasoning patterns (Hüllermeier and Waegeman, 2021). By training only on correct samples, the model suffers from epistemic signal truncation: it learns the distribution (rx, = 1) but never encounters the decision boundary between correct and incorrect paths. This leads to form of Model Collapse (Shumailov Algorithm 1 Epistemically-Calibrated Reasoning Require: Base Model M0, Dataset D, Sampling size 1: for iteration = 1 . . . do Dreason , Deval 2: Step 1: Generation Phase 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: for do ˆy1, . . . , ˆyK Sample(Mt1, x) for = 1 . . . do if IsCorrect(ˆyk) then Dreason.add(x, ˆyk) Deval.add((x, ˆyk), yes) else Deval.add((x, ˆyk), no) end if end for end for Step 2: Mixing Phase Dtotal Shuffle(Dreason Deval) Step 3: Dual-Objective Training 15: Mt SFT(Mt1, Dtotal) 16: end for et al., 2023), where the model converges toward low-variance point estimates and discards the distributional tails necessary for representing uncertainty. Consequently, the model incurs severe calibration cost (Hu et al., 2025), manifesting as pathologically high confidence in logically flawed generations."
        },
        {
            "title": "4 Methodology",
            "content": "We propose EPICAR, framework that internalizes epistemic calibration into the iterative selftraining loop without the need for auxiliary models or inference-time compute scaling. 4.1 Iterative Dual-Loop Framework Our framework alternates between generating reasoning traces and optimizing unified objective that balances problem-solving and self-evaluation. The detailed procedure is outlined in Algorithm 1, and training objective is presented in Section I. Internalizing Evaluation Task Unlike STaR, which discards incorrect generations, we utilize them as negative signals for the self-evaluation task. For every generated path (x, ˆy), if the generation is correct, the instance is added to the reasoning task for accuracy reinforcement and labeled as yes for the self-evaluation task. Conversely, if the generation is incorrect, the instance is excluded from reasoning reinforcement but labeled as no for the self-evaluation task. This dual-task structure is motivated by recent findings from Zhang et al. (2025), which observed that while reasoning models encode correctness in their hidden states, standard non-reasoning models exhibit significantly degraded signals. They hypothesize that this latent capacity is acquired through exposure to reasoning patterns containing both correct and incorrect steps. By explicitly training on no labels for erroneous paths, our framework provides the necessary exposure to elicit and strengthen these internal signals in standard LLMs. This forces the model to exploit latent features of logical failure, effectively mitigating the alignmentinduced Model Collapse and the resulting calibration cost described by Shumailov et al. (2023), and Hu et al. (2025)."
        },
        {
            "title": "4.2 Adaptive Injection Decoding (AID)",
            "content": "To eliminate noise caused by formatting failures, we adapt the method from Jin et al. (2025). When generating reasoning paths, we enforce format compliance (e.g., boxed{}) by injecting rigid completion strings during decoding. This ensures that valid reasoning paths are not mislabeled as no due to parsing errors, which would otherwise provide confusing training signal for the selfevaluation task. We provide stateful implementation of AID to handle edge cases such as premature termination and unclosed formatting tags; see Appendix for the detailed algorithmic logic, and Appendix for the ablation study."
        },
        {
            "title": "5 Experimental Setup",
            "content": "Datasets We evaluate our framework using the MATH1 dataset (Hendrycks et al., 2021) for the main iterative training loop (T = 3). We test outof-distribution (OOD) generalization on GSM8K2 (Cobbe et al., 2021) and cross-domain robustness on MBPP3 (Austin et al., 2021). For calibration tuning, we use 500-instance split-validation for MATH, and use the official validation set for MBPP. Specifically, for the scaling analysis in Section 6.4, we evaluate on the MATH-5004 subset to ensure 1https://huggingface.co/datasets/EleutherAI/ hendrycks_math 2https://huggingface.co/datasets/openai/gsm8k 3https://huggingface.co/datasets/ google-research-datasets/mbpp 4https://huggingface.co/datasets/ HuggingFaceH4/MATH-"
        },
        {
            "title": "Method",
            "content": "Perf. Reliability & Calibration Acc () AUROC () ECE () ECE (+TS) () Brier () Llama-3-1B Llama-3-3B Llama-3-8B"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL)"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL)"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL) 3.30% 3.00% 3.46% 3.54% 3.12% 3.30% 3.53% 3.14% 7.56% 5.84% 7.38% 7.60% 6.24% 8.58% 7.86% 6.46% 13.30% 12.22% 13.46% 13.72% 13.06% 14.42% 15.02% 14.38% 0.525 0.507 0.491 0.518 0.469 0.573 0.555 0.576 0.555 0.529 0.562 0.562 0.548 0.568 0.593 0.571 0.544 0.387 0.570 0.555 0.415 0.595 0.571 0.435 0.841 0.827 0.838 0.838 0.826 0.871 0.848 0.885 0.376 0.605 0.382 0.377 0.609 0.108 0.167 0.440 0.496 0.448 0.494 0.492 0.436 0.415 0.443 0. 0.516 0.517 0.515 0.515 0.516 0.532 0.514 0.534 0.315 0.460 0.344 0.318 0.456 0.053 0.050 0.436 0.384 0.384 0.381 0.381 0.375 0.362 0.361 0.361 0.740 0.716 0.737 0.737 0.716 0.800 0.754 0.817 0.216 0.424 0.219 0.217 0.432 0.097 0.106 0.257 0.368 0.316 0.365 0.365 0.312 0.298 0.328 0. Table 1: Llama-3 Family Results. EPICAR generally outperforms STaR (Zelikman et al., 2022) in both accuracy and calibration, with significant gains observed in the 3B and 8B variants. While the 1B model improves discriminative power (AUROC) over the baseline, it exhibits trade-offs in reasoning accuracy due to limited capacity. We compare our results with weight-space interventions (Hu et al., 2025) and inference-time scaling (Yoon et al., 2025). comparability with prior literature. For MBPP, we adopt 3-shot prompting setup and evaluate functional correctness via sandboxed execution environment; see Appendix for details on code extraction and the verification pipeline. Models and Baselines We employ the Llama3 (Dubey et al., 2024) and Qwen-3 (Yang et al., 2025a) families, focusing on 8B variants for MBPP and ablation studies. We compare our method (1) the Base against three primary baselines: Model, (2) STaR (Zelikman et al., 2022) for iterative self-improvement, and (3) Slow Thinking (ICL) (Yoon et al., 2025) for reliability. Additionally, we evaluate post-hoc Model Merging (Hu et al., 2025) by sweeping λ {0.0, 0.2, . . . , 1.0} to navigate the alignment-calibration frontier. Detailed algorithmic backgrounds and implementation for these baselines are provided in Appendix E. Inference-time Scaling Protocol To evaluate how internalized calibration scales with inferencetime compute, we sample {1, 10, 30} reasoning paths. We compare two aggregation strategies: (1) Self-Consistency (SC) (Wang et al., 2023), which selects the plurality answer via simple majority voting, and (2) Confidence-Informed SelfConsistency (CISC) (Taubenfeld et al., 2025). Unlike SC, CISC performs weighted majority vote by utilizing the models self-assessed confidence V(ri) as fidelity signal. This allows the model to surpass the accuracy of high-sample-count SC with significantly fewer samples (e.g., = 10 matching = 30), thereby reducing inference compute. The formal definition of the CISC aggregation and normalization process is provided in Appendix F."
        },
        {
            "title": "Method",
            "content": "Perf. Reliability & Calibration Acc () AUROC () ECE () ECE (+TS) () Brier () Qwen-3-1.7B Qwen-3-4B Qwen-3-8B"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL)"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL)"
        },
        {
            "title": "Base Model",
            "content": "+ Slow Thinking (ICL)"
        },
        {
            "title": "STaR",
            "content": "+ Model Merging + Slow Thinking (ICL) Ours (EPICAR) + Model Merging + Slow Thinking (ICL) 41.44% 41.34% 38.16% 42.02% 42.98% 42.34% 43.08% 32.06% 40.66% 27.98% 43.20% 42.58% 55.78% 43.50% 43.54% 41.06% 45.86% 55.16% 49.52% 48.08% 54.52% 49.76% 49.72% 55.56% 0.408 0.601 0.430 0.413 0.617 0.637 0.543 0.631 0.676 0.755 0.765 0.784 0.796 0.835 0.826 0.820 0.727 0.673 0.710 0.712 0.659 0.797 0.769 0.780 0.101 0.091 0.124 0.111 0.042 0.297 0.161 0.204 0.093 0.275 0.273 0.240 0.150 0.137 0.176 0.126 0.196 0.134 0.179 0.190 0.190 0.131 0.123 0. 0.074 0.101 0.102 0.072 0.049 0.079 0.018 0.054 0.093 0.269 0.154 0.168 0.131 0.139 0.164 0.080 0.121 0.065 0.117 0.122 0.032 0.088 0.103 0.148 0.255 0.245 0.257 0.257 0.239 0.323 0.270 0.253 0.232 0.240 0.283 0.263 0.232 0.193 0.211 0.186 0.259 0.251 0.258 0.261 0.271 0.206 0.217 0. Table 2: Qwen-3 Family Results. EPICAR demonstrates superior discriminative reliability (AUROC) across all scales and outperforms STaR (Zelikman et al., 2022), especially at 3B and 8B scale. Smaller models exhibit mixed results in calibration error and scaling efficiency compared to the baseline. We compare our results with weight-space interventions (Hu et al., 2025) and inference-time scaling (Yoon et al., 2025). Implementation Performance is measured via Accuracy, AUROC, ECE, and Brier Score. To assess the models intrinsic uncertainty beyond logitlevel shifts, we also report ECE after applying Temperature Scaling (TS) (Guo et al., 2017). Training uses LoRA (r = 16, α = 32) on 4x NVIDIA H100 GPUs. Following Kapoor et al. (2024), we update hidden features to support accurate verbalized confidence. Detailed hyperparameters, prompting templates, and the optimization procedure for TS are provided in Appendix B, and Appendix G."
        },
        {
            "title": "6 Results and Analysis",
            "content": "In this section, we evaluate our methods performance across different model families and sizes. We report Accuracy for reasoning capability, and AUROC, ECE, and Brier Score for calibration quality. To ensure rigorous comparison, we contrast our EPICAR against the standard STaR baseline, as well as state-of-the-art inference-time scaling (Slow Thinking (Yoon et al., 2025)) and weight-space intervention (Model Merging (Hu et al., 2025)) strategies. For Model Merging, we report the results obtained using the optimal λ that yields the best result with respect to accuracy and calibration, following Hu et al. (2025). See Appendix for full results for all λ."
        },
        {
            "title": "Synergy",
            "content": "Our results across both model families (Tables 1 and 2) demonstrate that internalizing the selfevaluation objective allows LLMs to navigate the reasoning-reliability frontier more effectively, particularly as model scale increases. Mitigating Calibration Cost of Iterative SFT Standard iterative SFT (STaR) consistently incurs calibration cost, improving accuracy while often degrading discriminative reliability. For instance, in Llama-3-1B, while STaR marginally boosts accuracy, it drops AUROC to 0.491. In contrast, our approach recovers this discriminative power (AUROC 0.573), albeit with slight trade-off in reasoning accuracy at this scale. However, for larger models like Llama-3-3B and 8B, our method achieves strict Pareto-dominance, effectively preventing the overconfident model collapse typical of positive-only feedback loops (e.g., reducing ECE from 0.376 to 0.108 in Llama-3-3B). Notably, EPICAR achieves the lowest Brier Score in most cases (e.g., 0.097 for Llama-3-3B), indicating superior overall predictive quality. Foundation for Inference-time Scaling pivotal finding is the synergy between our internalized calibration and Slow Thinking (Yoon et al., 2025) in capable models. While Slow Thinking generally boosts performance, it can be unstable on uncalibrated models. Our calibrated framework stabilizes this behavior in the 8B variants, achieving performance of 55.56% on Qwen-3-8B. We note, however, that this synergy is scale-dependent; in intermediate sizes (e.g., Qwen-3-4B), the added inference complexity does not yield performance gains over STaR, suggesting that critical mass of reasoning capability is required to effectively leverage the self-evaluation signal. Weight-space Interpolation & Merging We observe significant synergy with Model Merging (Hu et al., 2025), indicating that our reliability signals are complementary to weight-space interventions. While STaR yields marginal gains from merging, applying it to our checkpoints unlocks substantial headroom: the Ours + Merging variant achieves the highest Llama-3-8B accuracy of 15.02%. Precision calibration is also enhanced, with Qwen-31.7B reaching near-perfect ECE (+TS) of 0.018, demonstrating the flexibility of our method for posthoc optimization."
        },
        {
            "title": "6.2 Out-of-Distribution Generalization",
            "content": "To evaluate the generalization performance of our internalized calibration objective, we conduct zeroshot evaluations on GSM8K (Cobbe et al., 2021). This serves as critical out-of-distribution (OOD) benchmark, as the models were trained exclusively on the MATH dataset. Reliability of Verbalized Confidence Rankings As shown in Table 3, our method consistently enFamily Method Acc. () AUROC () ECE () Brier () Llama-3-1B Llama-3-3B Llama-3-8B Qwen-3-1.7B Qwen-3-4B Qwen-3-8B Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) 3.49% 3.49% 3.64% 14.94% 17.13% 21.46% 31.39% 32.15% 37.45% 77.33% 78.54% 79.30% 85.06% 86.05% 88.25% 85.90% 87.95% 89.46% 0.545 0.478 0. 0.527 0.497 0.606 0.565 0.561 0.565 0.490 0.493 0.660 0.611 0.693 0.756 0.654 0.678 0.722 0.841 0.840 0. 0.303 0.284 0.020 0.329 0.338 0.223 0.445 0.541 0.671 0.364 0.099 0.132 0.215 0.215 0.364 0.741 0.740 0. 0.223 0.228 0.166 0.324 0.332 0.284 0.376 0.463 0.610 0.255 0.122 0.106 0.162 0.147 0.216 Table 3: Zero-shot GSM8K Results. EPICAR consistently improves reasoning accuracy while maintaining or enhancing AUROC. hances the models discriminative reliability across both families. key finding is that while standard iterative self-training (STaR) often leads to degradation in AUROC (e.g., Llama-3-1B falling to 0.478), our approach significantly improves it, reaching 0.645 and 0.606 for Llama-3-1B and 3B, respectively. This demonstrates that the verbalized confidence scores produced by our framework are better separated, assigning substantially higher probabilities of correctness to true reasoning paths than to erroneous ones, even in unseen domains. Discriminative Power vs. Absolute Calibration Despite high AUROC, some variants exhibit elevated Raw ECE. We hypothesize that while our dual-objective training effectively teaches the model to distinguish between correct and incorrect paths, it does not explicitly supervise the absolute alignment of verbalized probability logits with empirical accuracy. Consequently, the model maintains strong sense of relative certainty, but the absolute values remain somewhat detached from the true probability space. We identify the explicit optimization of these verbalized probability logits as crucial direction for future work to bridge the gap between discriminative power and absolute calibration."
        },
        {
            "title": "6.3 Generalization to Other Tasks",
            "content": "Finally, we examine whether our approach generalizes beyond mathematics to the domain of code generation. We apply our framework to the MBPP benchmark (Austin et al., 2021), focusing on the 8B model variants. Model Method Acc. () AUROC () ECE (+TS) () Brier () Llama-3-8B Qwen-3-8B Base Model STaR Ours (EPICAR) Base Model STaR Ours (EPICAR) 37.35% 37.74% 39.30% 45.14% 45.14% 45.91% 0.551 0.523 0.538 0.622 0.577 0. 0.398 0.390 0.113 0.066 0.066 0.059 0.391 0.387 0.246 0.286 0.285 0.285 Table 4: Results on MBPP coding task. Our method (EPICAR) consistently improves Pass Rate (Accuracy) and demonstrates competitive calibration performance across different model families. Table 4 summarizes the performance across three stages: Base Model, STaR, and Ours. While our method does not always achieve the absolute highest AUROC, it consistently outperforms the STaR baseline in discriminative reliability. Specifically, in Llama-3-8B, STaR incurs significant calibration cost, dropping AUROC from 0.551 to 0.523, whereas our method mitigates this degradation, recovering it to 0.538. Crucially, EPICAR achieves significantly lower Brier Score (0.246 vs. 0.387 in Llama-38B) and the lowest ECE (+TS) across both families, reaching 0.113 for Llama-3-8B and 0.059 for Qwen-3-8B. This improvement in proper scoring rule (Brier Score) confirms that internalizing self-evaluation signals effectively prevents the overconfident model collapse typical of positive-only self-training, even in cross-domain reasoning tasks like programming. 6.4 Inference-time Scaling Performance We further investigate how internalized calibration impacts the inference-time scaling laws of reasoning models. By ensembling sampled reasoning paths, we analyze the synergy between internalized confidence and test-time compute. Figure 2 illustrates the results on the MATH-500 benchmark. Our framework demonstrates superior scaling efficiency compared to the STaR baseline. Notably, when paired with CISC (Taubenfeld et al., 2025), our 8B model achieves 3 reduction in inference computematching or exceeding the = 30 performance of STaR with only = 10 samples. This suggests that EPICAR provides more robust foundation for compute scaling, as internalized reliability signals effectively suppress erroneous consensus paths that often cause performance saturation in uncalibrated models. comprehensive analysis of all scales and reliability trajectories is provided in Appendix A. Figure 2: Inference-time Scaling on MATH-500. Visual representation of ensemble accuracy across sample sizes K. Our framework paired with CISC achieves superior scaling efficiency, outperforming STaR and establishing new frontier for compute-optimal reasoning."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we addressed the calibration cost inherent in iterative self-training, where models gain reasoning accuracy at the expense of reliability and uncertainty representation. We introduced EPICAR, framework that reframes reasoning training as an epistemic learning task by internalizing dual-objective of accuracy reinforcement and explicit self-evaluation. Our experiments across Llama-3 and Qwen-3 families demonstrate that EPICAR consistently achieves Pareto-superiority, simultaneously enhancing both performance and calibration while generalizing robustly to OOD math (GSM8K) and code generation (MBPP). Furthermore, we established that internalized calibration serves as critical driver for inference-time scaling efficiency; by leveraging self-assessed confidence through weighted ensembling (CISC), our models match high-samplecount performance with significantly fewer reasoning paths, yielding an effective 3 reduction in inference compute. These findings suggest that teaching models to know what they dont know is not merely post-hoc safety constraint but fundamental prerequisite for building compute-optimal reasoning systems. Ultimately, this work advocates for paradigm shift in model alignment, where uncertainty calibration is treated as an integral training objective for trustworthy reasoning agents."
        },
        {
            "title": "Limitations",
            "content": "Despite the performance gains and improved calibration, our work has several limitations. Domain Scope and Verification. While we demonstrated that EPICAR generalizes across mathematics (MATH, GSM8K) and code generation (MBPP), our evaluation remains centered on domains with objective, automated ground-truth verification. It remains to be seen whether EPICAR can be effectively applied to more subjective or ambiguous domains, such as legal reasoning or creative writing, where defining clear signal for knowing what it does not know is inherently more challenging. Model Scale and Capacity Constraints. Our experiments were conducted on models up to 8B parameters. As identified in Appendix A, our method is sensitive to the baseline reasoning capacity. In extremely low-accuracy regimes, such as Llama-31B, the scarcity of successful reasoning paths during the iterative loop can prevent the model from learning the nuanced discriminative features necessary for accurate self-evaluation. This suggests critical mass threshold for effective internalized calibration, and the dynamics may further differ in ultra-large scale models or specialized frontier models. Generalization Gap in Absolute Calibration. We observe notable discrepancy between discriminative power and absolute numerical calibration in out-of-distribution (OOD) scenarios. While EPICAR consistently maintains superior AUROC across benchmarks like GSM8Kindicating that it generalizes the relative ranking of reasoning paths effectivelythe absolute probabilities it verbalizes (measured by Raw ECE) exhibit higher volatility compared to in-distribution results on MATH. This suggests that while the model successfully semanticizes its internal certainty to distinguish correctness, the precise mapping from these semantic tokens to empirical probabilities is sensitive to domain shifts. This highlights fundamental gap between learning to rank outputs and achieving absolute logit alignment in unseen domains. Comparison with RL and Verifier-based Methods. While recent RL-based approaches like RLCR (Damani et al., 2025) achieve calibration through specialized reward structures, they often suffer from training instability and high hyperparameter sensitivity. In contrast, EPICAR operates within stable iterative SFT framework. Furthermore, unlike V-STaR (Hosseini et al., 2024), which requires separate verifier model to filter reasoning paths, our approach unifies generation and selfevaluation into single model. While this integration is the key driver behind our reported 3 reduction in inference compute by eliminating the overhead of multiple model passes, it may be bounded by the internal capacity of the single model compared to systems with dedicated external verification components. Potential for Reinforcement Learning Integration. promising direction for future research is the integration of our internalized calibration objective into reinforcement learning (RL) frameworks. The self-evaluation signals generated in our dual-loop framework could serve as an intrinsic reward mechanism or dense feedback signal for RL algorithms such as PPO (Schulman et al., 2017), DPO (Rafailov et al., 2023), or GRPO (DeepSeekAI et al., 2025). By using the models own epistemic certainty to weight or mask rewards during exploration, it may be possible to further stabilize the learning of complex reasoning trajectories and mitigate the overoptimization issues common in standard RLHF pipelines. the Verbalization Paradigm. Sensitivity of the verbalized confidence EPICAR adopts paradigm, following evidence that LLMs can semanticize internal certainty more effectively than logit-based indicators (Kadavath et al., 2022; Lin et al., 2022). While this approach captures high-level semantic certainty, it remains susceptible to the well-documented phenomenon of prompt sensitivity (Xia et al., 2025). Although our results demonstrate that EPICAR significantly improves the calibration of these verbalized signals, further research is required to ensure their absolute robustness across wider range of elicitation templates and linguistic contexts to prevent self-evaluation from being biased by phrasing."
        },
        {
            "title": "Ethical Considerations",
            "content": "Our research aims to improve LLM reliability, critical step toward safe AI deployment. However, we acknowledge several ethical implications. Misuse and Over-reliance: By enhancing models ability to express confidence, there is risk that users may over-rely on outputs when the model reports high certainty. While EPICAR significantly reduces overconfidence, no technique is infallible, and \"confident hallucinations\" may still occur, potentially leading to errors in high-stakes decision-making. Bias Amplification: As our framework utilizes iterative self-training on model-generated data, there is potential to amplify biases present in the pretrained state or the training distribution. We recommend applying our method alongside rigorous bias-detection and mitigation protocols. Environmental Impact: The iterative nature of our framework involves multiple rounds of generation and fine-tuning, incurring higher computational costs during training compared to single-pass SFT. However, we mitigate this impact by demonstrating that our models achieve superior reliability with significantly reduced inference-time compute via efficient scaling. Use of AI Assistants. In accordance with the ACL Policy on AI Assistance, we acknowledge the use of Gemini 3 Pro5 to assist with code debugging and writing polishing. All experimental designs, data analyses, and scientific claims presented in this work were verified by the authors."
        },
        {
            "title": "References",
            "content": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and 1 others. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732. Glenn Brier. 1950. Verification of forecasts expressed in terms of probability. Monthly Weather Review, 78(1):13. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Mehul Damani, Isha Puri, Stewart Slocum, Idan Shenfeld, Leshem Choshen, Yoon Kim, and Jacob Andreas. 2025. Beyond binary rewards: Training lms to reason about their uncertainty. arXiv preprint arXiv:2507.16806. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao 5https://deepmind.google/technologies/gemini/ Zhu, Shirong Ma, Peiyi Wang, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Abhimanyu Dubey, Akhil Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Tingchen Fu, Deng Cai, Lemao Liu, Shuming Shi, and Rui Yan. 2024. Disperse-then-merge: Pushing the limits of instruction tuning via alignment tax reduction. In Findings of the Association for Computational Linguistics: ACL 2024, pages 29672985. Association for Computational Linguistics. Caglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, and 1 others. 2023. Reinforced self-training (rest) for language modeling. arXiv preprint arXiv:2308.08998. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. 2017. On calibration of modern neural netIn International Conference on Machine works. Learning, pages 13211330. PMLR. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. NeurIPS. Arian Hosseini, Xingdi Yuan, Nikolay Malkin, Aaron Courville, Alessandro Sordoni, and Rishabh Agarwal. 2024. V-STaR: Training verifiers for self-taught reasoners. In First Conference on Language Modeling (COLM). Tiancheng Hu, Benjamin Minixhofer, and Nigel Collier. 2025. Navigating the alignment-calibration trade-off: pareto-superior frontier via model merging. arXiv preprint arXiv:2510.17426. Eyke Hüllermeier and Willem Waegeman. 2021. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. Machine Learning, 110:457506. Hyunbin Jin, Je Won Yeom, Seunghyun Bae, and Taesup Kim. 2025. \"well, keep thinking\": Enhancing llm reasoning with adaptive injection decoding. arXiv preprint arXiv:2503.10167. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zachary Dodds, Nova DasSarma, Eli TranJohnson, and 1 others. 2022. Language models arXiv preprint (mostly) know what they know. arXiv:2207.05221. Sanyam Kapoor, Manley Roberts, Katherine Collins, Arka Pal, Nate Gruver, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. 2024. Large language models must be taught to know what they dont know. In Advances in Neural Information Processing Systems, volume 36. Yoonjeon Kim, Doohyuk Jang, and Eunho Yang. 2025. Meta-awareness enhances reasoning models: Selfalignment reinforcement learning. arXiv preprint arXiv:2510.03259. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. In The Eleventh International Conference on Learning Representations (ICLR). Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles (SOSP), pages 611626. ACM. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research. Yong Lin, Hangyu Lin, Wei Xiong, Shizhe Diao, Jianmeng Liu, Jipeng Zhang, Rui Pan, Haoxiang Wang, Wenbin Hu, Hanning Zhang, Hanze Dong, Renjie Pi, Han Zhao, Nan Jiang, Heng Ji, Yuan Yao, and Tong Zhang. 2024. Mitigating the alignment tax of rlhf. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 580606. Association for Computational Linguistics. Xin Liu, Muhammad Khalifa, and Lu Wang. 2024. LitCab: Lightweight language model calibration over shortand long-form responses. In The Twelfth International Conference on Learning Representations. Keming Lu, Bowen Yu, Fei Huang, Yang Fan, Runji Lin, and Chang Zhou. 2024. Online merging optimizers for boosting rewards and mitigating tax in alignment. arXiv preprint arXiv:2405.17931. OpenAI. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher Manning, and Chelsea Finn. 2023. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems (NeurIPS). John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Maohao Shen, Subhro Das, Kristjan Greenewald, Prasanna Sattigeri, Gregory Wornell, and Soumya Ghosh. 2024. Thermometer: Towards universal calibration for large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 4460744631. PMLR. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson. 2023. The curse of recursion: Training on generated data makes models forget. arXiv preprint arXiv:2305.17493. Amir Taubenfeld, Tom Sheffer, Eran Ofek, Amir Feder, Ariel Goldstein, Zorik Gekhman, and Gal Yona. 2025. Confidence improves self-consistency in LLMs. In Findings of the Association for Computational Linguistics: ACL 2025, pages 2009020111, Vienna, Austria. Association for Computational Linguistics. Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi, Jinsong Su, and Dong Yu. 2024. Selfconsistency boosts calibration for math reasoning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 60236029, Miami, Florida, USA. Association for Computational Linguistics. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. In International Conference on Learning Representations. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 2482424837. Yuxi Xia, Pedro Henrique Luz De Araujo, Klim Zaporojets, and Benjamin Roth. 2025. Influences on LLM calibration: study of response agreement, loss functions, and prompt styles. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 37403761. Association for Computational Linguistics. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, and Deqing Yang. 2025b. LoGU: Long-form generation with uncertainty expressions. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1894718968. Association for Computational Linguistics. Dongkeun Yoon, Seungone Kim, Sohee Yang, Sunkyoung Kim, Soyeon Kim, Yongil Kim, Eunbi Choi, Yireun Kim, and Minjoon Seo. 2025. Reasoning models better express their confidence. In The 39th Conference on Neural Information Processing Systems (NeurIPS). Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. Star: Bootstrapping reasoning with reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 1547615488. Anqi Zhang, Yulin Chen, Jane Pan, Chen Zhao, Aurojit Panda, Jinyang Li, and He He. 2025. Reasoning models know when theyre right: Probing hidden states for self-verification. In The 2nd Conference on Language Modeling (COLM). Full Results and Analysis of Inference-time Ensembling A.1 Comprehensive Ensemble Accuracy of"
        },
        {
            "title": "All Scales",
            "content": "We perform an extensive evaluation of the scaling properties of EpiCaR across all model variants using the MATH-500 test set. By varying the number of sampled reasoning paths {1, 10, 30}, we map the compute-performance frontier as summarized in Table 5. Internalized Calibration as Scaling Foundation. Our experimental results consistently demonstrate that our method (Ours) establishes superior and more stable baseline for inferencetime scaling compared to both the pre-trained Base Model and the STaR baseline. At the 8B parameter scale, the synergy between internalized calibration and weighted aggregation becomes most apparent. Pairing our model with CISC (Taubenfeld et al., 2025) achieves state-of-the-art results: 25.40% for Llama-3 and 59.80% for Qwen-3 at = 30. This trajectory suggests that when model is trained to generate its own reliability signals, the performance gains from additional inference compute do not just accumulatethey accelerate, as the models selfevaluation serves as high-fidelity filter for the sampling process. Synergy, Error Suppression, and Saturation. The synergy between EpiCaR and the CISC aggregation protocol is pivotal finding. While standard Self-Consistency (SC) (Wang et al., 2023) relies on frequentist plurality vote, it is vulnerable to \"common errors\" where incorrect reasoning paths form deceptive majority. As observed in the Qwen-3 8B results, the STaR baseline exhibits significant performance saturation, reaching plateau at 57.80% for = 30. In contrast, our calibrated model continues to scale effectively, effectively breaking the saturation ceiling. This confirms that model that \"knows what it knows\" can utilize verbalized confidence to suppress high-frequency erroneous paths. By assigning low weights to incorrect but common reasoning traces, EpiCaR ensures that the ensemble consensus is driven by logical fidelity rather than mere repetition. The Critical Mass Threshold in Reasoning capacity. nuanced and critical finding is that the advantages of internalized calibration are not uniform across all model scales, identifying what we term \"Critical Mass Threshold.\" In the extremely low-accuracy regime, such as with Llama-3 1B (baseline accuracy 1.80%), the gap between our method and the STaR baseline diminishes. We hypothesize that when the models initial reasoning capability is too sparse, the training process lacks sufficient density of correct reasoning paths to learn the subtle discriminative features required for accurate self-evaluation. This suggests that while EpiCaR is powerful scaling tool, its benefits are most salient once model reaches fundamental level of reasoning competence. Future work should investigate whether specialized data augmentation can lower this threshold for smaller model architectures. A.2 Full Reliability Analysis of Scaling: SC vs."
        },
        {
            "title": "CISC",
            "content": "To assess the qualitative aspects of uncertainty estimation within an ensemble, we evaluate reliability metrics across {5, 10, 30}. For the CISC variants, we derive scalar ensemble confidence score CCISC for each unique answer a: CCISC(a) = (cid:80) i:ans(ri)=a V(ri) (cid:80)K j=1 V(rj) (4) where V(ri) [0, 1] represents the models internalized verbalized confidence for path ri. The comprehensive results for AUROC and ECE are detailed in Table 6. Reliability Collapse in STaR Baseline. striking phenomenon observed in our scaling analysis is the Reliability Collapse of the STaR baseline as increases. For instance, in Llama-3 8B, STaRs AUROC degrades significantly from 0.7895 (K = 5) to 0.7387 (K = 30). This degradation suggests fundamental flaw in positiveonly iterative training: by exclusively reinforcing successful paths, the model homogenizes its error patterns, leading to incorrect reasoning paths that form deceptively high-confidence consensus. As more samples are added, these \"confident hallucinations\" dominate the ensemble, undermining its discriminative power. In sharp contrast, our model (Ours) sustains or even improves its AUROC as compute scales. By internalizing logical failure modes during training, EpiCaR maintains clear margin between correct and incorrect reasoning paths, which is essential for the trustworthy deployment of LLMs in high-stakes reasoning tasks. Metric Divergence: The Interplay of ECE and AUROC. We observe characteristic divergence in reliability trends as scales: while ECE generally improves, AUROC tends to decay. We hypothesize that this is driven by two counteracting statistical forces: Statistical Smoothing: ECE benefits from larger sample sizes as the averaged ensemble confidence naturally converges toward the true population accuracy, reducing absolute calibration error. Consensus-Driven Noise: AUROC suffers as the probability of incorrect paths forming high-confidence plurality increases with K, narrowing the discriminative gap between true and false positives. Our framework effectively mitigates the decay rate of AUROC compared to all baselines. By ensuring that the models confidence is tied to the intrinsic logic of the path rather than the extrinsic frequency of the answer, EpiCaR achieves more robust reliability-compute trade-off."
        },
        {
            "title": "Size Method",
            "content": "Ensemble = 1 = 10 = 30 Llama-3 1B 3B 8B"
        },
        {
            "title": "Base Model",
            "content": "1.7B"
        },
        {
            "title": "Ours",
            "content": "Qwen-"
        },
        {
            "title": "Base Model",
            "content": "4B 8B"
        },
        {
            "title": "SC\nCISC",
            "content": "1.80% 3.20% 1.60% 5.40% 4.80% 7.40% 9.80% 10.40% 11.60% 29.60% 37.20% 40.40% 36.60% 41.80% 44.40% 40.80% 47.80% 46.00% 2.00% 2.40% 3.20% 3.00% 2.80% 3.20% 8.20% 8.40% 8.40% 9.40% 3.80% 4.40% 4.40% 3.80% 3.40% 3.40% 10.60% 10.20% 15.00% 14.60% 13.60% 13.40% 13.60% 13.20% 19.60% 20.20% 18.00% 17.80% 23.60% 23.40% 21.60% 21.80% 24.80% 21.00% 21.80% 25.40% 33.40% 35.80% 46.60% 46.80% 39.60% 39.80% 48.80% 47.40% 51.40% 53.00% 52.40% 51.20% 51.40% 51.60% 53.00% 53.60% 54.60% 55.40% 54.60% 55.00% 54.20% 53.80% 55.00% 57.20% 55.80% 55.80% 56.60% 56.80% 58.00% 58.40% 57.80% 58.00% 57.40% 59.20% 58.00% 59.80% Table 5: Comprehensive Ensemble Accuracy on MATH-500 (%). Comparison of unweighted Self-Consistency (SC (Wang et al., 2023)) and Confidence-Informed Self-Consistency (CISC (Taubenfeld et al., 2025)) across multiple model families and sizes. = 1 values are merged as the methods are identical without multiple samples. Bolding indicates the highest accuracy within each size group for specific K."
        },
        {
            "title": "Size Method Ensemble",
            "content": "K = 5 = 10 = 30 AUROC ECE AUROC ECE AUROC ECE Llama-3 Qwen-3 1B 3B 8B 1.7B 4B 8B"
        },
        {
            "title": "SC\nCISC",
            "content": "0.4784 0.4824 0.6870 0.6848 0.5636 0.5564 0.7002 0.7758 0.7413 0.6967 0.8170 0. 0.7470 0.7499 0.7895 0.7892 0.7824 0.8511 0.7315 0.7414 0.7638 0.7771 0.7853 0. 0.7869 0.7834 0.8170 0.8266 0.7737 0.7764 0.7626 0.7791 0.7972 0.8128 0.7767 0. 0.2380 0.2416 0.2172 0.2247 0.2516 0.2551 0.2252 0.2488 0.2356 0.2540 0.2080 0. 0.1952 0.2108 0.1812 0.2057 0.1628 0.2175 0.2156 0.2303 0.1960 0.2177 0.1672 0. 0.1304 0.1360 0.2028 0.2063 0.1776 0.1915 0.0884 0.1205 0.1632 0.1598 0.1640 0. 0.4760 0.5442 0.7390 0.7540 0.6398 0.7078 0.7225 0.7390 0.6972 0.6942 0.7556 0. 0.7283 0.7034 0.7741 0.7810 0.7988 0.7795 0.7182 0.7208 0.7657 0.7572 0.7756 0. 0.7830 0.7954 0.7864 0.7916 0.7544 0.7776 0.7213 0.7359 0.7735 0.7919 0.7926 0. 0.1644 0.1633 0.1490 0.1539 0.1742 0.1731 0.1450 0.1544 0.1470 0.1458 0.1212 0. 0.0978 0.1083 0.1058 0.1164 0.0684 0.0782 0.1508 0.1624 0.1344 0.1307 0.0986 0. 0.0968 0.1015 0.1390 0.1431 0.1242 0.1503 0.0786 0.0945 0.1260 0.1296 0.1098 0. 0.6490 0.6213 0.6643 0.7308 0.6478 0.6588 0.6760 0.6843 0.6169 0.6343 0.7561 0. 0.7184 0.7059 0.7387 0.7300 0.7722 0.7636 0.7065 0.6967 0.7521 0.7688 0.7750 0. 0.7667 0.7721 0.7713 0.7762 0.7859 0.7781 0.7457 0.7516 0.7854 0.7909 0.7914 0. 0.0995 0.0941 0.0915 0.0981 0.1273 0.1284 0.0843 0.0875 0.0460 0.0558 0.0951 0. 0.0472 0.0613 0.0529 0.0452 0.0397 0.0383 0.1181 0.1057 0.1174 0.1157 0.0900 0. 0.0573 0.0478 0.1229 0.1295 0.1108 0.1139 0.0767 0.0707 0.1009 0.1018 0.0959 0. Table 6: Comprehensive Reliability Scaling across All Models (AUROC and ECE). Comparison of frequencybased (SC) and weighted (CISC) confidence estimates across training stages. Bold entries indicate the best performance (highest AUROC or lowest ECE) within each size group for given K."
        },
        {
            "title": "B Full Experimental Configurations",
            "content": "B."
        },
        {
            "title": "Iterative Training Loop Details",
            "content": "For both the STaR baseline and EpiCaR, we conduct three complete iterations (T = 3) of the selfimprovement loop. In each iteration, the model generates candidate reasoning paths for each problem in the MATH training set. For STaR, only paths reaching the ground-truth answer are retained for fine-tuning. For our method, we perform the mixing strategy (Section 4.1) using both correct and incorrect attempts to refine the self-evaluation objective. We find that performance and calibration metrics typically stabilize after the third iteration, after which further training often leads to marginal returns. B.2 Data Processing and Validation Protocol For the iterative SFT loop, we utilize the training split of the MATH dataset (12,500 problems). To evaluate the robustness of internalized calibration in out-of-distribution (OOD) scenarios, we test on GSM8K without further fine-tuning. critical component of our evaluation is the split-validation protocol. Calibration metrics can be overly optimistic if the scaling parameters are optimized on the same set used for reporting. Thus, we randomly sample = 500 instances from the test set as \"calibration-validation\" set. We find the optimal temperature that minimizes ECE on this set and apply it to the remaining test instances (Ntest500) for final reporting. B.3 Data Licensing and Usage We utilize standard public benchmarks consistent with their intended research purposes. The MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) datasets are released under the MIT License, while the MBPP (Austin et al., 2021) dataset is distributed under the CC-BY 4.0 License. B.4 Baseline Implementation Details STaR: We implement the standard self-taught reasoner by filtering self-generated reasoning paths that reach the ground-truth answer. The model is fine-tuned for 3 iterations, with the checkpoint from Ti serving as the generator for Ti+1. Slow Thinking (ICL): We adopt the few-shot prompts from Yoon et al. (2025), which encourage models to \"think slow\" by verbalizing internal verification steps. This baseline represents the upper bound of inference-time calibration without training. Model Merging: We use weight-space interpolation between the base model (θbase) and SFT checkpoints (θSF ). The coefficient λ represents the weight of the SFT model. B.5 Training and Hyperparameters All models were fine-tuned using the Hugging Face TRL library. We employ Parameter-Efficient FineTuning (PEFT) via LoRA to ensure scalability. Hyperparameter Value Optimizer Learning Rate LR Scheduler Batch Size Max Sequence Length LoRA Rank (r) LoRA Alpha (α) LoRA Target Modules All linear layers Precision AdamW 1 105 Cosine 32 2048 16 32 bfloat16 Table 7: Detailed Hyperparameters for Dual-Objective Iterative SFT. B."
        },
        {
            "title": "Inference Implementation",
            "content": "To evaluate the effectiveness of our proposed method in terms of both reasoning accuracy and confidence calibration, we implemented highthroughput inference pipeline using the vLLM library (Kwon et al., 2023). All models were loaded in bfloat16 precision with eager execution mode enabled to ensure stability. B.7 Hyperparameters and Decoding"
        },
        {
            "title": "Strategies",
            "content": "We utilized distinct decoding strategies tailored to the nature of each benchmark: Greedy Decoding for Standard Benchmarks (MATH, GSM8K): For the primary evaluation of reasoning accuracy and calibration, we employed greedy decoding (temperature=0.0). The generation max length was set to 1024 tokens to allow sufficient reasoning steps (CoT). To accurately extract answers and measure verbalized confidence, we utilized prefix injection technique (e.g., So, the answer is boxed{) with forced stop token at the closing brace. This injection step acts as constraint that improves the format compliance and accuracy of the final answer. Sampling for Robustness (MATH-500): To assess the models performance under selfconsistency (Wang et al., 2023), we employed sampling with temperature=0.7 and generated = 30 candidate paths per problem. The repetition_penalty was dynamically adjusted to maintain diversity without degradation. Code Generation (MBPP): For the code generation tasks, we adopted few-shot prompting strategy consistent with prior work (Austin et al., 2021). The inference was performed with temperature=0.0 for pass@1 evaluation, employing specific stop sequences (e.g., Problem:, Tests:) to prevent hallucination beyond the solution function."
        },
        {
            "title": "C Implementation Details of Adaptive",
            "content": "Injection Decoding (AID) Our Adaptive Injection Processor is implemented as stateful LogitsProcessor that monitors the decoding process at each step. Unlike static constraints, it dynamically transitions through states based on the models output. The operational logic is categorized into three primary mechanisms: 1. Triggering and Injection State The processor maintains an is_injecting flag and an injection_step counter. The injection is triggered under two conditions: (1) when the model attempts to emit termination token (EOS) before boxed{} tag is generated, or (2) when the sequence reaches soft length limit (e.g., Lmax 150 tokens). During the injection state, the models logits are overridden to force the generation of the transition phrase: \"nSo, the answer is boxed{\". 2. Stateful Format Enforcement To ensure the injected format is correctly finalized, the processor tracks the sequence for the existence of the boxed token and its corresponding closing brace (}). Premature EOS Prevention: If the model attempts to terminate while box is open (has_injected is True but no closing brace is found), the processor overrides the EOS token with forced closing brace (}). Content Length Control: To prevent infinite or malformed generation within the answer box, the processor enforces maximum content length (Cmax = 40). If exceeded, it mandates closing brace and subsequent termination. 3. Zombie and Sampling Defense The processor utilizes finished_mask to prevent \"zombie\" generations where model might continue after logical EOS. Additionally, in cases where no box is detected, the processor suppresses all stop tokens until the injection logic is triggered, ensuring that every sample produced for the training pipeline is parsable and evaluatable. AID as De-noising Filter. Our ablation study (Section J) shows performance collapse without Adaptive Injection Decoding (AID). We clarify that AID does not introduce reasoning bias; rather, it acts as sanitization process to remove label noise. Without AID, mathematically sound derivation might be labeled as no due to minor formatting errors (e.g., missing closing boxed{}). Such mislabeling provides catastrophic signal, teaching the model that valid logic is incorrect. AID ensures that the no label strictly reflects logical failures, thereby protecting the integrity of the selfevaluation supervision."
        },
        {
            "title": "Execution",
            "content": "For the code generation task, we implement robust evaluation pipeline to ensure that the models reasoning capabilities are not underestimated due to minor formatting inconsistencies. 1. Robust Code Extraction Language models often produce conversational fillers or incomplete code snippets. As shown in our implementation, we employ multi-stage extraction heuristic: Block Extraction: We first attempt to parse the output for Markdown-style Python blocks (```python ... ```). Keyword-based Recovery: If no block is found, the processor searches for functional keywords such as import, def, or return to identify the start of the logic. Signature Injection: common failure mode in LLMs is omitting the function header. To mitigate this, our pipeline checks if the canonical function signature (e.g., def remove_Occ(...)) is present; if missing, it automatically prepends the signature to the models output to ensure the script is functionally complete and ready for execution. 2. Sandboxed Execution and Verification Correctness is verified by executing the extracted code against set of hidden unit tests provided by the MBPP dataset. Environment: Execution is performed in separate subprocess using Pythons multiprocessing to prevent side effects and handle potential infinite loops via 3-second timeout. Dependency Injection: To support diverse the environment is precoding problems, loaded with essential libraries, including math, heapq, collections, and itertools. Strict Pass Criterion: task is labeled as Correct (y = 1) if and only if the code executes without errors and passes every assert statement in the test list. 3. Prompting Strategy We use 3-shot prompt consisting of diverse Python tasks (e.g., sum of squares, average of cubes) to steer the model toward consistent \"Reasoning-then-Solution\" format. This mirrors the iterative self-training objective used in our math reasoning tasks, allowing for direct comparison of calibration performance across different domains."
        },
        {
            "title": "Related Concepts",
            "content": "In this section, we provide detailed technical backgrounds for the primary baselines and concepts utilized in our study: STaR, Slow Thinking (ICL), and Model Merging. E.1 Self-Taught Reasoner (STaR) STaR (Zelikman et al., 2022) is an iterative bootstrapping framework designed to improve models reasoning capabilities using only large dataset of questions and answers without intermediate rationales. Its core loop consists of: 1. Rationale Generation: is prompted (via few-shot) to generate Chainof-Thought (CoT) and final answer."
        },
        {
            "title": "The model",
            "content": "2. Filtering: Only rationales that lead to the correct ground-truth answer are retained. 3. Rationalization: For questions where the model fails, it is provided with the correct answer as \"hint\" to generate backwardreasoned rationale. 4. Fine-tuning: The model is fine-tuned on the combined set of successful and rationalized paths. In our experiments, STaR serves as the primary iterative baseline, representing the \"accuracy-first\" reinforcement approach that often incurs high calibration cost. E.2 Slow Thinking via In-Context Learning (ICL) Based on the findings of Yoon et al. (2025), Slow Thinking refers to the cognitive behaviors exhibited by advanced reasoning models (e.g., DeepSeek-R1, o1), such as exploring alternative approaches, self-verification, and backtracking within an extended CoT. Mechanism: Unlike standard CoT, slow thinking is non-linear. The model dynamically adjusts its internal confidence as it rejects erroneous paths or confirms logical steps. ICL Implementation: For non-reasoning models (baselines), we elicit these behaviors via In-Context Learning (ICL) by providing few-shot exemplars that demonstrate these \"slow\" cognitive patterns. This serves as strong baseline for reliability without parameter updates. Prompt Templates. For the Slow Thinking (ICL) baseline, we adopt the prompting strategy from Yoon et al. (2025)6. This template encourages the model to engage in meta-cognitive reasoning within <think> tags before providing final answer. The full template used for our experiments is provided in Listing 1. First , generate your thought process aloud in first - person in between the < think > tags . Then , outside of the < think > tag , generate your final answer . You must stop thinking outside of the < think > tag , and commit to providing an answer in the format of ** Answer **: $ANSWER . Here are some examples . [ Example 1] Question : Who was the next British Prime Minister after Arthur Balfour ? Output : < think > 1 2 4 5 6 6https://github.com/MattYoon/ reasoning-models-confidence 8 9 10 11 12 14 15 16 17 18 20 21 22 23 24 26 27 28 29 30 Okay , so need to figure out who was the next British Prime Minister after Arthur Balfour ... [ Detailed reasoning process ] ... So , the sequence is Balfour , then Campbell - Bannerman , then Asquith . </ think > ** Answer **: Henry Campbell - Bannerman [ Example 1 End ] [ Example 2] Question : Men Against the Sea and Pitcairn 's Island were two sequels to what famous novel ? Output : < think > ... [ Detailed reasoning process ] ... So the famous novel they ' re sequels to is \" Mutiny on the Bounty .\" </ think > ** Answer **: Mutiny on the Bounty . [ Example 2 End ] [ Example 3] Question : River Phoenix died during the making of which movie ? Output : < think > ... [ Detailed reasoning process ] ... So , to sum up my thoughts : Paramount was originally called the \" Hartford Film Company .\" </ think > ** Answer **: Hartford Film Company . [ Example 3 End ] This is the end of the examples . The following is the question you need to answer following the style of the examples . Listing 1: Full Few-Shot Prompt Template for Slow Thinking (ICL) E.3 Alignment-Calibration Trade-off and"
        },
        {
            "title": "Model Merging",
            "content": "Hu et al. (2025) formalize the Alignment Tax not just as loss of task performance, but as Calibration Costwhere models become universally overconfident after instruction tuning. Model Merging: To mitigate this, they propose interpolating the weights of PreTrained (PT) base model (θP ) and its Instruction-Tuned (IT) counterpart (θIT ): θmerged = (1 λ)θP + λθIT (5) recovering the calibration lost during alignment. We use this post-hoc method to benchmark the limits of weight-space optimization against our intrinsic training-time approach, EpiCaR."
        },
        {
            "title": "F Formal Definition of",
            "content": "Confidence-Informed Self-Consistency (CISC) To maximize the utility of our models internalized calibration during inference, we adopt ConfidenceInformed Self-Consistency (CISC) (Taubenfeld et al., 2025) as our primary scaling strategy. Given question and set of sampled reasoning paths and answers {(r1, a1), . . . , (rm, am)}, the CISC aggregation process consists of three stages: 1. Confidence Extraction. For each reasoning path ri, we extract the verbalized confidence score ci = V(ri). In our framework, these scores are generated via the verbalized confidence described in Section 3.1. 2. Softmax Normalization. To balance the relative importance of answer frequency (majority) and individual path reliability, the raw confidence scores are normalized using temperature-scaled softmax: ci = exp(ci/T ) j=1 exp(cj/T ) (cid:80)m (6) where is tunable hyper-parameter. As , CISC collapses to vanilla SC (uniform weights). As 0, the mechanism prioritizes the single path with the highest confidence, effectively becoming \"greedy\" confidence selection. 3. Weighted Aggregation. The final answer ˆaCISC is determined by summing the normalized confidence weights for each unique answer candidate: ˆaCISC = arg max m (cid:88) i=1 [ai = a] ci (7)"
        },
        {
            "title": "G Calibration and Temperature Scaling",
            "content": "where λ is the merging coefficient."
        },
        {
            "title": "Details",
            "content": "Pareto-Superior Frontier: This simple linear interpolation often reveals \"sweet spot\" where the merged model achieves higher accuracy than both parents while substantially In addition to standard calibration metrics, we employ Temperature Scaling (TS) (Guo et al., 2017) to evaluate the quality of the models confidence scores independently of its overconfidence bias. Temperature Scaling Procedure. We optimize single scalar parameter > 0 for each model family and dataset. The temperature is determined by minimizing the Negative Log-Likelihood (NLL) on held-out validation set (Nval = 500): min Nval(cid:88) i=1 log (σ(zi/T )) (8) where zi represents the models output logits for the correct answer. We denote the ECE calculated using these calibrated probabilities as ECE-TS. This allows us to distinguish between models that are inherently uncalibrated and those that simply require linear adjustment to their confidence variance."
        },
        {
            "title": "H Extended Related Work",
            "content": "Comparison with RL and Verifier-based Methods. While recent RL-based approaches like RLCR (Damani et al., 2025) achieve calibration, they often suffer from training instability and high hyperparameter sensitivity. In contrast, EPICAR operates within stable iterative SFT framework. Furthermore, unlike V-STaR (Hosseini et al., 2024), which requires separate verifier model, our approach unifies generation and verification into single model. This integration is the key driver behind our reported 3 reduction in inference compute, as it eliminates the overhead of managing multiple model forward passes. Prompting vs. Fine-tuning for Calibration. Recent studies have debated whether LLMs can express uncertainty through zero-shot prompting alone. While some suggest that scale improves selfevaluation (Kadavath et al., 2022), Kapoor et al. (2024) argue that prompting on its own is insufficient for reliable calibration due to high sensitivity to linguistic variances. They demonstrate that finetuning on even small graded dataset significantly outperforms black-box prompting by leveraging the models internal features. EPICAR builds on this insight by integrating this calibration-tuning into an iterative reasoning loop, ensuring that the models confidence is grounded in its evolving reasoning capacity rather than static prompt templates."
        },
        {
            "title": "I Detailed Training Objective",
            "content": "We prioritize minimalist design to ensure robustness. Unlike multi-task frameworks that require tuned auxiliary coefficients, EPICAR utilizes the standard Causal Language Modeling (CLM) loss: LEpiCaR = LCLM = (cid:88) log Pθ(wt w<t) (9) The model treats reasoning tokens and the selfevaluation token (e.g., yes/no) as single continuous sequence. Intrinsic Curriculum via Natural Mixing. Rather than using fixed mixing weight, we adopt natural mixing strategy. By pooling all reasoning and self-evaluation samples as described in Algorithm 1, the training distribution dynamically evolves. In early iterations where model accuracy is low, the dataset is naturally dominated by negative self-evaluation samples (no). This forces the model to prioritize self-objectivity and uncertainty representation (epistemic grounding). As reasoning capability improves, the proportion of positive reasoning samples naturally increases. This serves as an intrinsic curriculum that stabilizes the learning trajectory without manual intervention. Ablation Study: Impact of Adaptive"
        },
        {
            "title": "Injection Decoding",
            "content": "To validate the structural importance of our proposed Adaptive Injection Decoding (AID), we perform an ablation analysis using Llama-3-8B and Qwen-3-8B on the MATH dataset. We compare our full method (Ours) against variant trained without injection decoding (w/o AID), where formatting errors are treated as standard incorrect samples. Model Variant Acc. () AUROC () ECE () Llama-3-8B Qwen-3-8B Ours (Full) w/o AID Ours (Full) w/o AID 14.47% 2.56% 49.62% 47.83% 0.594 0.507 0.800 0. 0.415 0.580 0.133 0.152 Table 8: Ablation on Adaptive Injection Decoding (AID). Removing AID leads to significant performance degradation, highlighting its role in preventing formatting noise from corrupting the evaluation signal. As shown in Table 8, the removal of AID results in significant degradation in both reasoning accuracy and calibration metrics. For Llama-3-8B, the accuracy drops catastrophically to 2.56%. This collapse occurs because, without AID, valid reasoning paths that suffer from minor formatting issues (e.g., missing box delimiters) are mislabeled as \"incorrect\" (no). This introduces severe label noise, weight-space intervention. While smaller models like Llama-3.2-1B show mixed results in calibration error, they still exhibit improved AUROC when merged with our epistemically-calibrated weights. This confirms that EPICAR provides more robust initialization for model merging compared to standard self-training objectives, effectively preserving the base models general knowledge while injecting calibrated reasoning capabilities."
        },
        {
            "title": "L Extended Reliability Analysis",
            "content": "In this section, we provide the full set of reliability diagrams across all evaluated benchmarks: MATH (Hendrycks et al., 2021), GSM8K (Cobbe et al., 2021), and MBPP (Austin et al., 2021). These diagrams visualize the relationship between the models verbalized confidence and its empirical accuracy, providing qualitative evidence for the calibration improvements discussed in the main text. We partition the models predictions into = 10 equally-spaced bins based on verbalized confidence. perfectly calibrated model would align with the dashed diagonal line (y = x). As shown in Figures 3 through 6, our method (EPICAR) consistently demonstrates better alignment and higher discriminative power (AUROC) compared to the STaR baseline, which frequently exhibits overconfident clusters in the lower accuracy binsa clear symptom of the calibration cost of standard iterative SFT. confusing the models self-evaluation mechanism and destabilizing the reinforcement loop. AID effectively filters this noise, ensuring that the negative signal purely reflects logical failures rather than syntactic ones."
        },
        {
            "title": "K Full Results for Model Merging",
            "content": "In this section, we provide the comprehensive results of our model merging experiments across the Llama-3 and Qwen-3 model families. Table 9 and Table 10 details the performance and reliability metrics for varying interpolation coefficients λ, ranging from the pure base model (λ = 0.0) to the fully fine-tuned models (λ = 1.0). of"
        },
        {
            "title": "Interpolation Coefficient",
            "content": "(λ) Effect The results demonstrate that model merginginterpolating weights between the base model and the fine-tuned modelserves as an effective regularizer. For most model scales, we observe that an intermediate value of λ (e.g., 0.6 λ 0.8) often provides superior balance between task-specific accuracy and probabilistic calibration. Interestingly, in larger models such as Qwen-3-8B and Llama-3.1-8B, our method (EPICAR) maintains high discriminative performance (AUROC) even at λ = 1.0, whereas STaR often exhibits signs of overconfidence or degraded calibration as the fine-tuning progresses. Comparison: STaR vs. EPICAR Across all tested architectures, EPICAR consistently outperforms the STaR baseline in both discriminative reliability (AUROC) and calibration error (ECE and Brier Score). Notably: Discriminative Power: EPICAR achieves significantly higher AUROC scores, particularly at the 4B and 8B scales. This indicates that the epistemic uncertainty captured during our training process allows the model to better distinguish between correct and incorrect reasoning paths. Calibration Stability: While STaR often shows fluctuating ECE values as λ increases, EPICAR tends to show more stable or improving trend in Brier scores. This suggests that the calibration objective in EPICAR successfully aligns the models confidence with its actual predictive correctness. Scaling Trends We observe clear scaling trend where larger models (8B) benefit more from the Model Method Base Model STaR + Merging Llama-3-1B Ours (EPICAR) + Merging Base Model STaR + Merging Llama-3-3B Ours (EPICAR) + Merging Base Model STaR + Merging Llama-3-8B Ours (EPICAR) + Merging λ 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0. 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 Acc. () AUROC () ECE () Brier () 3.30% 3.34% 3.18% 3.54% 3.50% 3.46% 3.20% 3.53% 3.62% 3.44% 3.30% 7.56% 7.60% 7.46% 7.44% 7.38% 7.38% 7.70% 7.82% 7.80% 7.86% 8.58% 13.30% 13.26% 13.54% 13.74% 13.72% 13.46% 13.90% 14.12% 15.02% 14.04% 14.42% 0.525 0.530 0.529 0.518 0.506 0.491 0.531 0.555 0.551 0.595 0.573 0.555 0.562 0.564 0.562 0.542 0.562 0.568 0.559 0.552 0.593 0. 0.544 0.531 0.546 0.556 0.555 0.570 0.552 0.553 0.571 0.592 0.595 0.841 0.841 0.842 0.838 0.838 0.838 0.845 0.848 0.862 0.880 0. 0.376 0.377 0.378 0.378 0.380 0.382 0.353 0.301 0.237 0.167 0.108 0.496 0.496 0.495 0.494 0.492 0.494 0.486 0.470 0.443 0.434 0. 0.740 0.740 0.741 0.737 0.737 0.737 0.745 0.754 0.779 0.809 0.800 0.216 0.217 0.217 0.217 0.219 0.219 0.200 0.168 0.134 0.106 0. 0.368 0.368 0.367 0.367 0.365 0.365 0.361 0.348 0.328 0.312 0.298 Table 9: Full Results for Llama-3 Family with Model Merging. Accuracy and calibration metrics across all λ values for Llama-3 (1B, 3B, and 8B). Model Method Base Model STaR + Merging Qwen-3-1.7B Ours (EPICAR) + Merging Base Model STaR + Merging Qwen-3-4B Ours (EPICAR) + Merging Base Model STaR + Merging Qwen-3-8B Ours (EPICAR) + Merging λ 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1. 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.2 0.4 0.6 0.8 1.0 Acc. () AUROC () ECE () Brier () 41.44% 41.26% 42.02% 41.64% 41.24% 38.16% 42.46% 42.42% 43.08% 42.72% 42.34% 40.66% 41.78% 41.60% 42.32% 42.58% 43.20% 42.20% 42.72% 43.40% 43.54% 43.50% 45.86% 45.84% 47.14% 47.44% 48.08% 49.52% 46.48% 47.10% 48.54% 49.72% 49.76% 0. 0.407 0.413 0.409 0.421 0.430 0.438 0.497 0.543 0.600 0.637 0.676 0.697 0.746 0.760 0.784 0.765 0.704 0.782 0.804 0.826 0.835 0. 0.714 0.721 0.727 0.712 0.710 0.719 0.729 0.746 0.769 0.797 0.101 0.120 0.111 0.121 0.123 0.124 0.101 0.099 0.161 0.214 0.297 0. 0.102 0.143 0.191 0.240 0.273 0.097 0.138 0.170 0.176 0.137 0.196 0.200 0.193 0.197 0.190 0.179 0.175 0.145 0.112 0.123 0.131 0. 0.256 0.257 0.258 0.261 0.257 0.257 0.257 0.270 0.284 0.323 0.232 0.233 0.235 0.248 0.263 0.283 0.230 0.220 0.222 0.211 0.193 0. 0.264 0.261 0.262 0.261 0.258 0.254 0.243 0.229 0.217 0.206 Table 10: Full Results for Qwen-3 Family with Model Merging. Accuracy and calibration metrics across all λ values for Qwen-3 (1.7B, 4B, and 8B). Figure 3: Reliability Diagram: MATH (Standard). Comparison of calibration performance between the base model, STaR, and EPICAR on the MATH dataset. Figure 4: Reliability Diagram: MATH (Slow Thinking). Visualizing how internalized calibration interacts with inference-time \"slow thinking\" behaviors. Figure 5: Reliability Diagram: GSM8K (Zero-Shot). Evaluation of epistemic uncertainty calibration in an out-of-distribution mathematical reasoning context. Figure 6: Reliability Diagram: MBPP (Code Generation). Cross-domain robustness of EPICAR in programming tasks."
        }
    ],
    "affiliations": [
        "Department of Aerospace Engineering, Seoul National University",
        "Department of Rural Systems Engineering, Seoul National University",
        "Graduate School of Data Science, Seoul National University"
    ]
}