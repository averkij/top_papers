{
    "paper_title": "o1-Coder: an o1 Replication for Coding",
    "authors": [
        "Yuxiang Zhang",
        "Shangxi Wu",
        "Yuqi Yang",
        "Jiangming Shu",
        "Jinlin Xiao",
        "Chao Kong",
        "Jitao Sang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The technical report introduces O1-CODER, an attempt to replicate OpenAI's o1 model with a focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the model's System-2 thinking capabilities. The framework includes training a Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaM-BJTU/O1-CODER ."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 9 2 ] . [ 1 4 5 1 0 0 . 2 1 4 2 : r O1-CODER: AN O1 REPLICATION FOR CODING Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong & Jitao Sang School of Computer Science and Technology Beijing Jiaotong University Beijing, China {yuxiangzhang, wushangxi, yqyang, jiangmingshu, jinlinx, 23120361, jtsang}@bjtu.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "The technical report introduces O1-CODER, an attempt to replicate OpenAIs o1 model with focus on coding tasks. It integrates reinforcement learning (RL) and Monte Carlo Tree Search (MCTS) to enhance the models System-2 thinking capabilities. The framework includes training Test Case Generator (TCG) for standardized code testing, using MCTS to generate code data with reasoning processes, and iteratively fine-tuning the policy model to initially produce pseudocode, followed by the generation of the full code. The report also addresses the opportunities and challenges in deploying o1-like models in real-world applications, suggesting transitioning to the System-2 paradigm and highlighting the imperative for environment state updates. Updated model progress and experimental results will be reported in subsequent versions. All source code, curated datasets, as well as the derived models will be disclosed at https://github.com/ADaMBJTU/O1-CODER ."
        },
        {
            "title": "INTRODUCTION",
            "content": "OpenAI recently introduced the o1 model (OpenAI, 2024), which has demonstrated impressive system-2 thinking capabilities. This model represents significant advancement in AIs ability to perform complex reasoning tasks that require higher-order cognitive functions. Following its release, numerous analysis and replication efforts have emerged, highlighting the growing interest and potential of o1-like models. Notable works include g1 (Benjamin Klieger, 2024), OpenO1 (ope, 2024), O1-Journey (GAIR-NLP, 2024), OpenR (Team, 2024), LLaMA-O1 (SimpleBerry, 2024), LLaMABerry (Zhang et al., 2024), Steiner (Ji, 2024), Thinking Claude (Richards Tu, 2024), LLaVA-o1 (Xu et al., 2024), and several industrial releases such as k0-math, DeepSeek-R1-Lite, Macro-o1 (Zhao et al., 2024), Skywork o1, QwQ (Qwen Team, 2024), and InternThinker (Shanghai AI Lab, 2024) (illustrated in Fig. 1). Prior to the o1 model, large language models (LLMs) primarily exhibited System-1 capabilities, characterized by fast, intuitive responses. These models were trained on datasets consisting mainly of question-answer (Q, A) pairs, lacking the intermediate reasoning steps that involve deliberate and analytical processing. This stems from the fact that humans rarely record their thought processes on the internet or elsewhere. Traditionally, techniques such as Chain-of-Thought (CoT) prompting were used to guide models in generating step-by-step reasoning before arriving at an answer. However, more direct and effective way is to create datasets including the reasoning sequences, e.g., (Q, ..., Si, ..., A), where Si represents an individual reasoning step leading to the final answer. It is widely believed that o1 addresses the lack of reasoning data by combining reinforcement learning with pretraining. Reinforcement learning (RL) is well known for its ability to explore and discover new strategies rather than relying on predefined data. Looking back at key developments in machine learning, we can see that deep learning and large-scale pretraining have driven transformations in model architecture and the requirements for labeled data, respectively. In contrast, reinforcement learning addresses different aspect of transformation on the objective function. In Corresponding author. 1 Figure 1: o1 replication efforts: upper part from academic institutions and open-source communities, and lower part from the industry. situations where explicit guidance or clear goals are absent, RL exploits exploration to search for new knowledge and solutions. Combining pretraining with RL creates powerful synergy of learning and search, where pretraining compresses existing human knowledge, and RL enables the model to explore new possibilities. We chose coding tasks to explore how to employ RL to generate and refine reasoning data. Coding is typical task that requires System-2 thinking, involving careful, logical, and step-by-step problem-solving. Moreover, coding can serve as foundational skill for solving many other complex problems. This technical report presents our attempt to replicate o1 with specific focus on coding tasks. The approach integrates RL and Monte Carlo Tree Search (MCTS) to enable self-play, allowing the model to continually generate reasoning data and enhance its System-2 capabilities."
        },
        {
            "title": "2 FRAMEWORK OVERVIEW",
            "content": "There are two main challenges to address for self-play RL applied to code generation. The first challenge is result evaluation, i.e., assessing the quality of the generated code. Unlike tasks such as Go or mathematics, where results can be directly evaluated based on game rules or correct answers, evaluating code requires running the generated code within testing environment and verifying it against test cases. We cannot assume that code datasets will always provide sufficient test cases. The second challenge involves defining the thinking and search behaviors, i.e., determining the object and granularity of process rewards. For code generation, the key question is how to design the reasoning process and the space of policies to guide the models behavior effectively. To address the first challenge, we propose training Test Case Generator (TCG), which automatically generates test cases based on the question and the ground-truth code 1. This approach will help build standardized code testing environment, providing result rewards for reinforcement learning. For the second challenge, two possible approaches can be considered. One is think before acting, where the model first forms complete chain of thought and then generates the final answer all at once. The other approach, think while acting (Zelikman et al., 2024), involves generating parts of the answer while simultaneously reasoning through the task. We chose the former approach. For code generation, this means first thinking through and writing out detailed pseudocode, which is then used to generate the final executable code. The advantages are twofold: adaptability, as the same pseudocode can lead to different concrete code implementations; and controllable granularity, as adjusting the level of detail in the pseudocode can be adjusted to control the granularity of the reasoning/search behavior. The complete framework pseudocode is provided in Algorithm 1, which consists of six steps. (1) The first step is training the test case generator (TCG) γTCG, which is responsible for automatically generating test cases based on the question. (2) In the second step, we run MCTS on the original code dataset to generate code data with reasoning processes Dprocess, including validity indicator to 1We also propose an alternative approach where test cases are generated based solely on the question. In addition to utilizing code datasets only provide questions, it can also be applied during the inference phase, enabling online reasoning without the need for predefined ground-truth code. distinguish between correct and incorrect reasoning steps. (3) Once we have data that includes the reasoning process, the third step is to fine-tune the policy model πθ, training it to behave in think before acting manner. (4) The reasoning process data can also be used to initialize the process reward model (PRM) ρPRM, which evaluates the quality of reasoning steps. (5) The fifth step is the most crucial: with PRM ρPRM providing process rewards and TCG γTCG provides result rewards, the policy model πθ is updated with reinforcement learning and MCTS. (6) In the 6th step, based on the updated policy model, new reasoning data can be generated. This new data can then be used to fine-tune the PRM again (4th step). Therefore, steps 4, 5, and 6 form an iterative cycle, where self-play continues to drive model improvements. The flow between the six steps is illustrated in Fig. 2. The following section will introduce each step in detail. Algorithm 1 Self-Play+RL-based Coder Training Framework Require: Dcode: dataset containing problems Qi and solution code Ci. πθ: Initial policy model γTCG: Test Case Generator(TCG) to create problem-oriented test samples ρPRM: Process Reward Model(PRM) to evaluate the quality of intermediate reasoning steps ϕ: Aggregation function combining result-based and process-based rewards Ensure: Optimized policy model π θ 1: Train γTCG on Dcode to maximize diversity and correctness of generated test cases {(Ii, Oi)}. ① Train the Test Case Generator (TCG) 2: Based on Dcode = {Qi, Ci}, use MCTS to generate Dprocess = {(Qi, , Sj 1, , m}, where Sj vm = 1 when the generated code pass the test cases. represents reasoning step and vj ② Synthesize Reasoning-enhanced Code Dataset i)j = {0, 1} is validity indicator with , , , vj 3: Finetune πθ with SFT on valid steps D+ process = {(Qi, Sj ③ Finetune the Policy Model (Qi, Sj , i) i) , vj , Dprocess, I(C i) = 1}. 4: while not converged do ④ Initialize/Finetune the Process Reward Model (PRM) Train/Finetune PRM using SFT on Dprocess with point-wise loss, or using DPO with pairwise loss. ⑤ Improve the Policy Model with Reinforcement Learning Initialize ri = 0. for = 1, 2, . . . , do Generate reasoning step Sj Use PRM to compute process-based reward rj πθ(Sj Qi, S1:j1 ). = ρPRM(Qi, S1:j ). end for Based on Qi and the complete reasoning sequence S1:m Use TCG to generate test cases (Ii, Oi) for each problem Qi with the ground-truth code Ci. Execute generated code Compute result-based reward: on inputs Ii to produce outputs i. , generate the final code i. Ri = (cid:26)τpass, τfail, if = Oi, otherwise. Update πθ using reinforcement learning method guided by the aggregated reward 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: ⑥ Generate New Reasoning Data ϕ(Ri, r1:m ). Generate new reasoning data Update dataset: Dprocess Dprocess 16: 17: 18: 19: end while 20: return Optimized policy model π θ process. process using the updated πθ. 3 Figure 2: Self-Play+RL training framework."
        },
        {
            "title": "3 METHOD AND INTERMEDIATE RESULTS",
            "content": "3.1 TEST CASE GENERATOR TRAINING 3.1.1 OBJECTIVE Test Case Generator is tool designed to automate the creation of input-output test cases, which plays critical role in supporting program verification in code generation tasks. During the training phase, the correctness of the generated code is typically assessed with standard input-output test cases. The pass rate of these test cases serves as key metric for evaluating the quality of the generated code and acts as an outcome reward signal to guide the training of the policy model. This reward signal helps the model refine its generation strategy, thereby enhancing its capability to produce accurate and functional code. In the inference phase, when the trained model is tasked with code generation, standard test cases are often not available to verify the correctness of the generated code. The test case generator mitigates this limitation by providing self-validation mechanism for the policy model, which allows the policy model to evaluate before final generation. As result, the policy model is able to select the optimal output path based on the validation results. 3.1.2 TRAINING The training process is divided into two distinct phases: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al., 2024). We denote the generator which is not finetuned as γTCGbase . The primary objective of the SFT phase is to ensure that the generators output adheres to predefined format, enabling the accurate parsing and extraction of the generated test cases. The training data for this phase is derived from the TACO dataset (Li et al., 2023), which follows the format {question, solution, test case}. To standardize the models input and output, we developed template format, as detailed below:"
        },
        {
            "title": "Template format for TCG SFT",
            "content": "### Instruction Please complete the task in the code part and generate some test case in the test part that can be used to test the quality of the generated code. ### Problem {question} ### Code Part {randomly select one solution from the provided solutions} ### Test Part [Generate 3 test cases here to validate the code] {sample 3 test_cases with each formatted as input and output} Figure 3: Template format for TCG SFT The generator is denoted as γTCGsf after SFT. The goal of the DPO phase is to guide the model in generating test cases that align with specific preferences, thereby enhancing both the performance and reliability of the test case generator. In this study, we employ the DPO method with artificially constructed sample pairs to improve the models ability to align with desired preferences by constructing preference dataset. Our DPO fine-tuning relies on pre-constructed preference dataset Dpref = {x, yw, yl}, where is prompt that includes instruction, question, and code; yw is positive example, i.e., test cases that align with the preference; and yl is negative example, i.e., test cases that do not align with the preference. We adopt the following rules to construct preference data: for yw, we directly use the three sampled test cases that are completely matched as positive examples; for yl, we shuffle the outputs of the three sampled test cases and then concatenate the original inputs so that the input-output pairs of the three test cases do not completely match, and use the three incompletely matched test cases as negative examples. The training objective aims to optimize γTCGθ based on initial SFT model γTCGsf , while incorporating implicit reward modeling with the reference model γTCGref , which represents the initial SFT model γTCGsf t. The objective function is as follows: LDPO(γTCGθ ; γTCGref ) = E(x,yw,yl)Dpref (cid:104) log σ (cid:16) β log γTCGθ (ywx) γTCGref (ywx) β log γTCGθ (ylx) γTCGref (ylx) (cid:17)(cid:105) , (1) where σ(x) is the sigmoid function and β represents scaling factor used to adjust the contrast strength between the positive and negative examples during training. The generator is denoted as γTCGdpo after DPO, which represents the final generator γTCG. 3.1.3 EXPERIMENTS We utilize DeepSeek-1.3B-Instruct (Guo et al., 2024) as the base model for the test case generator, followed by SFT and DPO. The fine-tuning phase employs QLoRA technology (Dettmers et al., 2023) with rank parameter = 1 to adapt the following modules: proj, proj, proj, proj, gate proj, up proj, down proj. The learning rate is set to 5 104 to balance training stability and convergence speed. The training data is derived from subset of the TACO train dataset, which adheres to the ACM competition format and contains approximately 10,000 samples. Similarly, the test data is obtained from subset of the TACO test dataset, also conforming to the ICPC competition format, and consists of 314 samples. We tested the quality of the generated test cases at different stages of the TACO test. After the SFT phase, the pass rate of test cases generated by γTCGsf on the standard code was 80.8%, demonstrating the generators ability to efficiently produce test cases following preliminary fine-tuning. Furthermore, γTCGdpo achieved performance of 89.2%, reflecting an notable improvement compared to γTCGsf . This indicates that preference optimization, by refining the models decision-making process, significantly enhanced the generators ability to produce more reliable test cases. 5 In practical scenarios, the generators performance has generally met the requirements for assessing code correctness. Looking ahead, we plan to explore the DPO method, where the model autonomously generates data during inference, potentially optimizing the generators performance further. Additionally, we are considering the incorporation of self-play in the TCGs training. In this setup, the policy model would generate code intended to pass the test cases produced by the TCG, while the TCG would aim to generate progressively more challenging test cases. This adversarial interaction could foster mutual improvements in both the policy model and the test case generator."
        },
        {
            "title": "Pseudocode Prompt",
            "content": "### Instruction Please refer to the given task description and provide thought process in the form of step-by-step pseudocode refinement. curious user has approached you with programming question. You should give step-by-step solutions to the users questions. For each step you can choose one of the following three actions: <Action 1> Defining algorithm Structures Using pseudocode Description: Outline the core functions and overall structure of the solution without getting into implementation details. Define inputs, outputs, and the main tasks each function will perform. <Action 2> Refine part of the pseudocode Description: Add more details to the pseudocode, specifying the exact steps, logic, and operations each function will carry out. This prepares the pseudocode for actual coding. <Action 3> Generate python code from the pseudocode Description: Translate the refined pseudocode into executable Python code, making sure to handle inputs, outputs, and ensure correctness in the implementation. Note: - You can choose one of the three actions for each step. - Provide detailed explanation of the reasoning behind each step. - Try to refer to the reference code as much as possible, but you can also modify it if needed (e.g. change variable names, add some comments, etc.). ### Examples {examples} ### Question {question} Figure 4: Pseudocode Prompt for Step-by-Step Refinement 3.2 REASONING-ENHANCED CODE DATA SYNTHESIS 3.2.1 PSEUDOCODE-BASED REASONING PROCESS The definition of the reasoning process is crucial. As mentioned in the Introduction, we explore pseudocode-based prompting approach designed to guide large language models in deep reasoning for complex code tasks. Pseudocode, serving as an intermediate representation between natural language descriptions and actual code, offers more abstract and concise way to express the logical flow of algorithms or programs. To integrate pseudocode reasoning into step-level Chain-of-Thought 6 Model Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-Coder-7B Vanilla Pseudocode Vanilla Pseudocode Vanilla Pseudocode Vanilla Pseudocode Pass@1(%) ASPR(%) 55.8 49.9 46.7(-9.1) 54.5(+4.6) 56.3 52.0 51.3(-5.0) 70.6(+18.6) 59.8 66.4 50.1(-9.7) 78.1(+11.7) 57.7 49.3 58.2(+0.5) 74.9(+25.6) Table 1: Pseudocode-based code generation results on the MBPP Benchmark. Pass@1 indicates the overall pass rate. ASPR (Average Sampling Pass Rate) indicates the average success rate of reaching the correct reasoning path on the last step. (CoT), as illustrated in Fig. 4, we define three key behavioral actions infused with pseudocode reasoning: Action 1: Defining Algorithm Structures using Pseudocode: In this action, the model outlines the structure and interface of the main functions, without delving into implementation details. The aim is to enable the model to grasp the overall task structure, including the inputs, outputs, and core functionalities of each primary function. Action 2: Refining the Pseudocode: In this action, the model iteratively refines the pseudocode defined in Action 1, progressively clarifying the steps, logic, and operations of each function in preparation for the final code implementation. Action 3: Generating Code from the Pseudocode: The goal of this action is to accurately translate the structure and logic of the pseudocode into executable code, ensuring that the generated code meets the task requirements. These actions ensure that the model employs pseudocode as cognitive tool during the reasoning process, enhancing its reasoning capability for complex code generation tasks. It is important to note that these three actions do not imply that the reasoning chain is limited to only these steps. As demonstrated in Fig. 5, the model may need to repeatedly invoke Action 2 throughout the reasoning process to iteratively refine the pseudocode until it is sufficiently developed for the final code generation. To evaluate the effectiveness of the step-level CoT with pseudocode reasoning, we conducted experiments using the Qwen series of open-source models (Yang et al., 2024) and the Mostly Basic Python Problems (MBPP) dataset (Austin et al., 2021) as the benchmark. In the experiment, we employed sampling strategy based on Monte Carlo Tree Search (MCTS) and compared Pass@1 for regular CoT and CoT with pseudocode reasoning, as well as the Average Sampling Pass Rate (ASPR) of the last step on the correct reasoning path. Our results indicate that incorporating pseudocode significantly improves the quality of the generated code when the reasoning is correct. Table 1 presents the results. While the Pass@1 metric generally decreases with pseudocode-based reasoning, we observed significant increase in ASPR, indicating that pseudocode enhances the overall reasoning process, particularly in refining the path toward the correct final output. This suggests that accurate pseudocode highly contributes to the final correct code. However, vanilla LLMs still face challenges in generating effective pseudocode, which is precisely the goal of the subsequent SFT initialization and Self-Play+RL enhancement. 3.2.2 REASONING PROCESS DATA SYNTHESIS , vj , , i)}, where vj We use Monte Carlo Tree Search (MCTS) (Kocsis & Szepesvari, 2006; Feng et al., 2023; Qi et al., 2024) to construct step-level process reward data in the form of Dprocess = {(Qi, , Sj represents the evaluation of the reasoning path up to step Sj , and . In this process, we employ the standard MCTS rollout strategy for path exploration. For each problem Qi, we apply the pseudocode prompt strategy defined earlier to guide the reasoning process. When terminal node Sm is reached, complete pseudocode reasoning path (Qi, S1 ) is formed. The reward value vm for the terminal node Sm , . . . , Sm is computed based on two key metrics: is the executable code derived from the final step Sm Figure 5: Generated example code with pseudocode CoT 8 Compilation success rate (compile): This metric determines whether the generated code can successfully compile. The value compile is binary, with compile = 1 indicating success and compile = 0 indicating failure. Test case pass rate (pass): Given successful compilation, we further evaluate whether the generated code passes the test cases. The pass rate is calculated as pass = Numpassed , Numtest case where Numpassed is the number of passed test cases and Numtest case is the total number of test cases used for validation."
        },
        {
            "title": "The reward value for the terminal node Sm\ni",
            "content": "is calculated as weighted sum of these two metrics: vm = α compile + (1 α) pass, where α is hyperparameter controlling the relative importance of compilation success and test pass rate. Once the reward value vm is computed for the terminal node, we backpropagate this value to all preceding nodes along the path, assigning reward value vj ). Due to the multiple rollouts in the MCTS process, the cumulative reward for node vj during backpropagation may exceed 1. Therefore, we normalize the reward values for each node along the path using the following formula to obtain the final step validity value. to each step (Sj , vj When constructing the reasoning process dataset, for each problem Qi, if correct answer is found through the search, we are guaranteed to obtain at least one terminal node (Sm = 1. After completing the search, we select the full reasoning path from the correct terminal node (Qi, S1 = 1 to form the initialization dataset for the policy model. This dataset is denoted as: ) with vm , . . . , Sm ), vm , vm , vm D+ process = {(Qi, Sj , i) (Qi, Sj , vj , i) Dprocess, I(C i) = 1}, where I() is an indicator function that returns 1 if the generated code passes all the test cases. 3.3 POLICY MODEL INITIALIZATION After completing the reasoning data synthesis tasks described in Section 3.2, we use each complete reasoning solution in the dataset to initialize the policy model πθ. This step aims to help πθ better understand the task requirements and follow the expected action behavior, providing an optimal starting point for subsequent iterative training. Qi, S1:j Given the question Qi, the specific reasoning step content generated by the policy model πθ at step can be expressed as πθ(Sj represents the content of reasoning step, delimited by specific separators, with denoting the tokens generated by πθ at each decoding step. S1:j1 represents the context formed by the outputs of the previous reasoning steps. The policy model πθ is then initialized using the set of verified, correct reasoning solutions D+ This initialization is performed by optimizing the following training objective: = (w1, w2, . . . , wk). Here, Sj ), where Sj process. LSFT = (cid:88) (Qi,Sj ,C i)D+ process log πθ(S1:m Qi), (2) where denotes the concatenation of the reasoning steps S1:m policy model πSFT and the final code θ will then serve as the foundation for subsequent training stages. i. The initialized 3.4 PRM TRAINING Given problem Qi and solution prefix corresponding to the current state, the Process Reward Model (PRM), denoted as R+, assigns reward value to the current step Sj to estimate its contribution to the final answer. Based on the tree search approach used during data synthesis in Section 3.2, two formats of data organization can be used for training the process reward model, referred to as point-wise and pair-wise, are described in detail below. , vj Point-wise In this format, data collected from the search tree are organized as = , Sj {(Qi, S1:j1 represents the value label assigned to step Sj during the tree search process. Depending on the processing method, this label can be used to derive either hard or soft estimates. Following the approach in (Wang et al., 2024), the PRM is trained using the objective: ) = 1, 2, . . . , }, where is the number of samples, and vj Lpoint-wise PRM = (Qi,S1:j1 ,Sj ,vj )D (cid:104) log r(Qi, S1:j vj ) + (1 vj ) log (cid:16) 1 r(Qi, S1:j (cid:17)(cid:105) ) , (3) ) is the normalized prediction score assigned by the PRM. where r(Qi, S1:j Pair-wise In the pair-wise format, for node nd at depth of the search tree, with its child nodes represented as (cid:80) ) = 1, 2, . . . , }. Here, Sjwin represents the reasoning step that achieved higher value estimate during the tree search compared to Sjlose , preference pair data are organized as Dpair = {(Qi, S1:j1 nd+1 , Sjlose , Sjwin . i Following the Bradley-Terry model (Bradley & Terry, 1952), the PRM is trained using the following objective: Lpair-wise RM = (Qi,S1:j1 ,Sjwin jlose ,S )Dpair (cid:16) (cid:104) log σ(cid:0)r(Qi, S1:j1 , Sjwin ) r(Qi, S1:j1 , Sjlose )(cid:1)(cid:17)(cid:105) , (4) where σ(x) denotes the sigmoid function. Unlike the point-wise setting, the scores here are not normalized. This enables the model to focus on learning relative preferences between actions rather than absolute value predictions. 3.5 RL-BASED POLICY MODEL IMPROVEMENT We model the code generation task as language-augmented Markov Decision Process (MDP), formally represented as = (V, S, A, , R, ϕ) (Team, 2024; Carta et al., 2023). In this framework, denotes the vocabulary, and represents an individual token generated by the model. The action space and the state space are sets of token sequences, meaning that both actions and states are sequences of tokens. In this framework, s0 represents the question, and the action ai is considered reasoning step (referring to the Si in algorithm 1), which consists of both the type of action and its corresponding chain of thought. The state transition function : defines how the current state st changes when an action at is taken. Specifically, the action at appends tokens to the current state, forming new state st+1 = (st, at). This process continues until the model generates the final solution. The reward function : R+ evaluates the quality of intermediate steps, such as the reasoning process or generated code fragments. The function ϕ combines process-based and outcome-based rewards to produce final reward signal. At each step, the model selects an action at A, which transitions the system to new state st+1 = (st, at). After executing the action, the model receives process reward rt = ρPRM(st1, at) from PRM. This process repeats until the model either generates the final code or reaches the predefined maximum depth. Once the model generates the final code or completes the search process, the outcome reward Ri is evaluated by testing the generated code against series of test cases. We propose reward aggregation function that incorporates both time-dependent weights and discount factor: ϕ(Ri, r1:m ) = α(t) Ri + (1 α(t)) 1 (cid:88) j=1 γjrj , where α(t) is time-varying factor that adjusts the balance between the final reward Ri and the cumulative intermediate rewards r1:m over time. For instance, α(t) may decrease over time, gradually placing more weight on the intermediate rewards as the model refines its solution, while reducing the emphasis on the final reward as the model approaches the optimal policy. r1:m , with α(t) typically following schedules such as linear or logarithmic decay. The parameter γ [0, 1] is the discount factor, which determines the importance of future rewards relative to immediate rewards. The aggregated reward signal is employed to refine the models policy, typically through the implementation of reinforcement learning algorithms such as PPO (Ziegler et al., 2019) and iterative DPO(Rafailov et al., 2024). With this setup, we define reinforcement learning environment tailored for the code generation task. The models actions are driven by both process-based rewards, which encourage intermediate reasoning steps, and outcome-based rewards, which reflect the correctness of the final code. This dual reward structure helps the model improve its code generation ability over time."
        },
        {
            "title": "3.6 NEW REASONING DATA GENERATION AND SELF-PLAY",
            "content": "In step 6, the updated policy model πθ is used to generate new reasoning data, denoted as process. This data is created by reasoning through new problem instances Qi, generating step-by-step reasoning paths {S1 , . . . , Sm i. The reasoning steps are generated iteratively, where each step Sj }, with each path culminating in final code output is conditioned on the previous steps. , S2 Once the new reasoning data is generated, it is added to the existing dataset Dprocess to form an updated dataset Dprocess Dprocess process. This update increases the diversity and quality of the reasoning examples, providing more comprehensive training material for subsequent steps. This new data generation process enables the iterative self-play training loop. After adding the new reasoning data, the model undergoes further fine-tuning, starting with updating PRM as described in the 4th step. The PRM, in turn, adjusts the policy model with RL described in the 5th step. This iterative cycle of data generation, reward model updating, and policy improvement ensures sustained improvement in the systems reasoning ability."
        },
        {
            "title": "4 DISCUSSIONS",
            "content": "4.1 BITTER LESSON: DATA IS ALL YOU NEED Over the last decade, the AI field has been developing along central line towards maximizing computation-intelligence conversion efficiency, which is to efficiently convert the ever-increasing computing power into higher intelligence levels. Along this line, as illustrated at the top of Fig. 6, early advancements prioritized improvements on the model side: from SVM to DNN and then to Transformer, scalable model architectures were designed to fully leverage computational power. In recent years, the focus has shifted towards the data side. Techniques such as Semi-Supervised Learning (SSL) in pre-training and Reinforcement Learning (RL) in post-training have aimed to harness data more effectively. The o1 model continues this line. It moves from SFT, which leverages high-quality supervised data, to RLHF, which utilizes environmental feedback to access theoretically unlimited data, and finally to o1s innovative approach of supervising the generation process through reward signals derived from the generated reasoning process itself. This progression suggests that, with Transformer architectures now capable of scaling to handle vast amounts of data and training models of sufficient size, the only remaining challenge converges to acquiring adequate data. One approach is to seek data wherever it is lacking, such as reasoning data for system-2 abilities or physical world trajectories for embodied intelligence. Another approach is to explore data types that do not yet exist in the human world, which requires further exploration of techniques like RL and Self-Play. 4.2 SWEET LESSON: BEYOND HUMAN DATA common criticism of LLM is its reliance on existing human-recorded data, which inherently limits their potential. As Wittgenstein stated, The limits of my language mean the limits of my world. The finite scope and depth of human language records constrain the cognitive capabilities of LLMs. However, the success of o1 demonstrates that we can now explore the underlying thought processes behind these recorded data through RL. This advancement signifies pivotal shift in AI development, moving from mere imitation of human language to the autonomous generation of novel cognitive processes. 11 Figure 6: The trend towards maximizing computation-intelligence conversion efficiency. More interestingly, these thought process data do not necessarily be confined to natural language. As highlighted in recent Nature paper (Fedorenko et al., 2024), language serves primarily as tool for communication rather than the essence of thought. In our observations, some of the thought chains generated by o1 contain nonsensical text, suggesting that the thinking tokens may not correspond to discrete natural language words. If the model has developed itself more efficient form of internal representation for thinking, this will significantly elevate the efficiency of thought processes and problem-solving mechanisms, not only transcending the limitations imposed by human language data but also further unlocking the potential of model capabilities. 4.3 OPPORTUNITIES The self-play+RL framework provides viable solution for exploring underlying data, which opens up the possibility of exploring System-2 solutions for many tasks that were previously reliant on System 1 capabilities. By integrating more thoughtful, step-by-step processes into task execution, we believe that this approach can yield positive results across wide range of domains (Kant et al., 2024; Ganapini et al., 2021; Valmeekam et al., 2024; Lowe, 2024). Tasks traditionally solved using System 1 capabilities, such as reward modeling (Mahan et al., 2024), machine translation (Zhao et al., 2024), retrieval-augmented generation (RAG) (Li et al., 2024), and multimodal QA (Islam et al., 2024), have already benefited from the deeper reasoning capabilities enabled by System-2 thinking. The o1 models system card demonstrates notable improvements in model safety. Inspired by this, we have recently explored the concept of System-2 Alignment, which involves guiding models to thoroughly evaluate inputs, consider potential risks, and correct biases in their reasoning (Wang & Sang, 2024). We introduced three methods to realize System-2 alignment: prompt engineering, supervised fine-tuning, and reinforcement learning with process supervision. We will apply the SelfPlay+RL framework presented in this report to System-2 alignment, aiming to further enhance the models ability to think deliberately and reduce vulnerabilities in complex scenarios. 4.4 CHALLENGES The released o1-preview and o1-mini currently lack multimodal capabilities and functional call features, which are claimed by OpenAI to be included in its complete version. Beyond multimodal and functional call, another critical feature for improvement in o1-like inference models is the optimization of inference time. This includes enhancing inference efficiencyachieving higher performance per unit of timeand enabling adaptive inference time adjustments. Specifically, this involves dynamically adjusting the System 2 reasoning process based on task complexity and achieving more human-like ability to seamlessly switch between System 1 and System 2 reasoning modes. 12 For o1-like inference models to be deployed across broader real-world applications, two major challenges need to be addressed, both involving with the RL environments. The first challenge concerns reward function generalization. This has been already discussed in the community. For example, leveraging the enhanced ability of inference models to understand high-level natural instructions, approaches like Constitutional AI (Bai et al., 2022) might directly define reward functions in natural language. Another strategy focuses on improving coding capability and transforming the other tasks into coding problems for resolution. Another less mentioned challenge concerns environment state update. Unlike classic model-free RL methods, such as Q-learning, where state transitions are not explicitly modeled, o1-like models rely on behavior simulation and forward search, requiring knowledge of the updated state following an action. This shifts the paradigm towards model-based RL. In well-defined domains such as programming, mathematics, and Go, the environment often has deterministic rules. For example, programming uses compiler-defined language specifications, mathematics adheres to axiomatic logic, and Go operates under fixed game rules. These deterministic frameworks allow precise computation of state transition probabilities p(statei+1 statei, actioni) following specific actions. However, in many real-world applications, such as Retrieval-Augmented Generation (RAG), device usage (), and embodied agents, obtaining state updates requires interaction with external environments or simulators. This introduces significant computational and time costs. For example, in device use, behaviors like clicking, inputting, or scrolling must be simulated in way that involves page rendering, state updates, and sometimes complex backend interactions like network requests. Moreover, o1-like models face the limitation of not being able to perform online behavior simulation during inference, which prevents the model from validating or correcting its actions by returning to previous state. This leads to inability to backtrack and refine decisions. Therefore, one of the key directions is to attempt explicit modeling of the environment by developing world model for state transition prediction. The world model takes as input the current and past states and actions, and produces the next state as output. This allows the model to interact with its internal world model, rather than directly with the real environment or simulator. We recognize that one of the ongoing challenges in RL when building such world models is ensuring their accuracy. As result, world models have typically been applied to environments where the dynamics are relatively simple and well-understood. The good news is, recent rapid advancements in generative games (Sang, 2024) offer promising progress that could facilitate more accurate and practical environment modeling for inference models in real-world applications. Prospects. The o1 model is clearly influenced by AlphaGo: AlphaGo utilized imitation learning to initialize the policy network, reinforcement learning to fine-tune the policy and learn the value network, and MCTS as an online search strategy, which parallels LLMs pre-training, post-training, and inference. AlphaGoZero took more advanced approach by not relying on historical data, which exactly mirrors current trends in LLM development increasingly emphasizing the post-training stage. If we follow the evolution of the Alpha series, we can anticipate similar developments in o1-like inference models. Initially, the Alpha series developed towards generalization: AlphaZero was applied to Go, Chess, and Shogi, while MuZero achieved human-level performance across 57 Atari games. The other goal besides generalization, however, is to apply these models to more complex, real-world tasks. This progression is evident in AlphaFolds leap to AlphaCode and AlphaGeometry, as well as the extension of AI to physical environments, such as the 3D virtual agents in SIMA or the embodied intelligence in RT-X."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Yuhang Wang and Jing Zhang for their fruitful discussions and participation."
        },
        {
            "title": "REFERENCES",
            "content": "Open o1: model matching proprietary power with open-source innovation. https://github. com/Open-Source-O1/Open-O1/, 2024. 13 Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Benjamin Klieger. g1: Using Llama-3.1 70b on Groq to create o1-like reasoning chains. https: //github.com/bklieger-groq/g1, 2024. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Thomas Carta, Clement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning. In International Conference on Machine Learning, pp. 36763713. PMLR, 2023. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. ficient finetuning of quantized llms. K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural cessing Systems, volume 36, pp. 1008810115. Curran Associates, Inc., 2023. https://proceedings.neurips.cc/paper_files/paper/2023/file/ 1feb87871436031bdc0f2beaa62a049b-Paper-Conference.pdf. EfIn A. Oh, T. Naumann, A. Globerson, Information ProURL Qlora: Evelina Fedorenko, Steven T. Piantadosi, and Edward A. Gibson. Language is primarily tool for communication rather than thought. Nature, 615:7582, 2024. Xidong Feng, Ziyu Wan, Muning Wen, Stephen Marcus McAleer, Ying Wen, Weinan Zhang, and Jun Wang. Alphazero-like tree-search can guide large language model decoding and training. arXiv preprint arXiv:2309.17179, 2023. GAIR-NLP. O1 replication journey: strategic progress report, 2024. Marianna Bergamaschi Ganapini, Murray Campbell, Francesco Fabiano, Lior Horesh, Jon Lenchner, Andrea Loreggia, Nicholas Mattei, Francesca Rossi, Biplav Srivastava, and Kristen Brent Venable. Thinking fast and slow in ai: the role of metacognition, 2021. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming the rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. Mohammed Saidul Islam, Raian Rahman, Ahmed Masry, Md Tahmid Rahman Laskar, Mir Tafseer Nayeem, , and Enamul Hoque. Are large vision language models up to the challenge of chart comprehension and reasoning? Findings of the Association for Computational Linguistics: EMNLP 2024, 2024(November):33343368, 2024. Yichao Ji. small step towards reproducing openai o1: Progress report on the steiner open source models, October 2024. URL https://medium.com/@peakji/b9a756a00855. Manuj Kant, Marzieh Nabi, Manav Kant, Preston Carlson, and Megan Ma. Equitable access to justice: Logical llms show promise. arXiv preprint arXiv:2410.09904, 2024. Levente Kocsis and Csaba Szepesvari. Bandit based monte-carlo planning. In European conference on machine learning, pp. 282293. Springer, 2006. Huayang Li, Pat Verga, Priyanka Sen, Bowen Yang, Vijay Viswanathan, Patrick Lewis, Taro Watanabe, and Yixuan Su. Alr2: retrieve-then-reason framework for long-context question answering. arXiv preprint arXiv:2410.03227, 2024. Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. Scott C. Lowe. System 2 reasoning capabilities are nigh, 2024. Dakota Mahan, Duy Van Phung, Rafael Rafailov, Chase Blagden, Nathan Lile, Louis Castricato, Jan-Philipp Franken, Chelsea Finn, and Alon Albalak. Generative reward models, 2024. OpenAI. Learning to reason with large language models. https://openai.com/index/ learning-to-reason-with-llms/, 2024. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers. arXiv preprint arXiv:2408.06195, 2024. Qwen Team. QwQ-32b-preview. qwq-32b-preview/, 2024. https://qwenlm.github.io/zh/blog/ Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Richards Tu. Thinking Claude. Thinking-Claude/tree/main, 2024. https://github.com/richards199999/ Jitao Sang. note on generative games: Positioning, progress and prospects. arXiv, 2024. Shanghai AI Lab. InternThinker. https://internlm-chat.intern-ai.org.cn, 2024. SimpleBerry. Llama-o1: Open large reasoning model frameworks for training, inference and evaluation with pytorch and huggingface. https://github.com/SimpleBerry/LLaMA-O1, 2024. Accessed: 2024-11-25. OpenR Team. Openr: An open source framework for advanced reasoning with large language models. https://github.com/openreasoner/openr, 2024. Karthik Valmeekam, Kaya Stechly, Atharva Gundawar, and Subbarao Kambhampati. Planning in strawberry fields: Evaluating and improving the planning and scheduling capabilities of lrm o1, 2024. Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce LLMs step-by-step without human annotations. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.acl-long.510. URL https://aclanthology.org/2024.acl-long.510. Yuhang Wang and Jitao Sang. Dont command, cultivate: An exploratory study of system-2 alignment, 2024. Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step, 2024. URL https://arxiv.org/abs/2411.10440. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024. Eric Zelikman et al. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. URL https://arxiv.org/abs/2403.09629. Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. arXiv preprint arXiv:2410.02884, 2024. URL https://arxiv.org/abs/2410.02884. Accessed: 2024-11-25. Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions, 2024. 15 Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593, 2019."
        }
    ],
    "affiliations": [
        "School of Computer Science and Technology, Beijing Jiaotong University, Beijing, China"
    ]
}