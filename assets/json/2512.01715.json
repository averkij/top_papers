{
    "paper_title": "DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models",
    "authors": [
        "Wanpeng Zhang",
        "Ye Wang",
        "Hao Luo",
        "Haoqi Yuan",
        "Yicheng Feng",
        "Sipeng Zheng",
        "Qin Jin",
        "Zongqing Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, a principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides a meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes a discrepancy measure between empirical distributions of observation and action embeddings, maps it to a modulation weight via a monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 5 1 7 1 0 . 2 1 5 2 : r DiG-Flow: Discrepancy-Guided Flow Matching for Robust VLA Models Wanpeng Zhang1,2 Ye Wang2,3 Hao Luo1,2 Haoqi Yuan1,2 Yicheng Feng1,2 Sipeng Zheng2 Qin Jin3 Zongqing Lu1,2, 1Peking University 2BeingBeyond 3Renmin University of China https://beingbeyond.github.io/DiG-Flow"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models trained with flow matching have demonstrated impressive capabilities on robotic manipulation tasks. However, their performance often degrades under distribution shift and on complex multi-step tasks, suggesting that the learned representations may not robustly capture task-relevant semantics. We introduce DiG-Flow, principled framework that enhances VLA robustness through geometric regularization. Our key insight is that the distributional discrepancy between observation and action embeddings provides meaningful geometric signal: lower transport cost indicates compatible representations, while higher cost suggests potential misalignment. DiG-Flow computes discrepancy measure between empirical distributions of observation and action embeddings, maps it to modulation weight via monotone function, and applies residual updates to the observation embeddings before flow matching. Crucially, this intervention operates at the representation level without modifying the flow matching path or target vector field. We provide theoretical guarantees showing that discrepancy-guided training provably decreases the training objective, and that guided inference refinement converges with contraction. Empirically, DiG-Flow integrates into existing VLA architectures with negligible overhead and consistently improves performance, with particularly pronounced gains on complex multi-step tasks and under limited training data. Date: Dec 1,"
        },
        {
            "title": "Introduction",
            "content": "Vision-Language-Action models represent paradigm shift in robotic learning by leveraging pretrained vision-language representations to enable flexible, instruction-following manipulation policies [13]. By combining powerful vision-language backbones with flow matching or diffusion-based action generation, these models have achieved impressive success across diverse tasks. However, recent studies reveal concerning fragility: performance degrades substantially under modest distribution shifts such as lighting changes, texture variations, or camera angle perturbations [2, 4]. This problem is particularly pronounced on complex tasks where sequential decisions compound, and errors in early steps cascade into failure. The fundamental question is whether the learned representations robustly capture task-relevant semantics Correspondence to Zongqing Lu <lu@beingbeyond.com>. 1 or inadvertently encode spurious patterns. Flow matching provides an elegant framework for learning action distributions by regressing neural vector fields [3, 5], yet the regression objective alone may not sufficiently incentivize semantically grounded representations. We propose complementary perspective: the geometric relationship between observation features and action embeddings reveals representational quality. When observation features from the vision-language backbone and action embeddings from the policy head exhibit low distributional discrepancy, their representations are geometrically compatible, suggesting semantic coherence. Conversely, high discrepancy signals potential misalignment that may indicate spurious patterns or out-of-distribution observations. To solve this problem, we introduce the DiG-Flow framework with 3 main components: discrepancy function that quantifies distributional distance, monotone weight mapping that transforms discrepancy into modulation factor, and lightweight residual operator that adjusts observation features. The framework defaults to Wasserstein distance for its geometric interpretability but accommodates alternative discrepancies. Importantly, DiG-Flow intervenes at the representation level before flow matching, leaving the probability path and target vector field unchanged. This design enables seamless integration into existing architectures while providing theoretical guarantees on training dynamics and inference convergence. Our contributions are threefold. First, we establish discrepancy-guided framework for robust VLA learning. Second, we discuss theoretical motivation for the general problem and provide theoretical guarantee for our method design. Third, we validate that DiG-Flow consistently improves performance across both simulation and real-world benchmarks, with pronounced gains on complex multi-step tasks and in data-limited scenarios, while maintaining negligible computational overhead."
        },
        {
            "title": "2 Related Work",
            "content": "Vision-Language-Action Models. Vision Language Models (VLMs) [6, 7] typically combine LLM reasoning [810] with modal-specialized encoders [1114] for unified multimodal understanding [15, 16]. Pioneering works [1720] demonstrated strong few-shot instruction-following capability. With these visual backbones, VLA models extend them to generate robot actions from visual observations and language instructions, representing paradigm shift in robot learning [13, 21, 22]. Early work like RT-1 [1] demonstrated transformer-based architectures could scale to diverse tasks, while RT-2 [21] showed web-scale vision-language pretraining improves robotic control. Recent advances follow two primary approaches: (1) Direct action prediction: OpenVLA [2] scales to 7B parameters with open-source datasets, Pi0 [3] and its enhanced version Pi0.5 [4] use flow matching for continuous control, while models like Octo [23] provide generalist policies across embodiments; (2) Visual prediction: PaLM-E [22] embeds sensors into language models, GR-3 [24, 25] uses video pretraining for action generation. Specialized variants address specific challenges: SpatialVLA [26] incorporates 3D spatial representations, Otter builds fine-grained visual feature extraction [27], Pi0-Fast [28] improves inference efficiency through action tokenization, gr00t-N1 [29] targets humanoid control, and CoT-VLA [30] adds visual chain-of-thought reasoning. Being-H0 [31] incorporates physical instruction tuning for better motion understanding. OneTwoVLA proposes unified architecture for better adaptive reasoning [32]. Despite these advances, long-horizon performance and robustness to distributional drift remain open challenges, which we address through geometric regularization. Flow Matching and Optimal Transport. Flow Matching [5] trains neural ODEs without simulation, directly learning vector fields that transform distributions. This paradigm improves on Continuous Normalizing Flows [3335] which require expensive ODE solving. Recent developments include Conditional Flow Matching [36] for simplified training with conditional distributions, Rectified Flow [37] for straighter transport paths, and Stochastic Interpolants [38] unifying flows and diffusions. OT-CFM [39, 40] uses optimal transport for trajectory optimization, finding better paths between distributions at the level of flow matching dynamics. Optimal transport theory [41, 42] provides principled tools for comparing distributions geometrically. Computational advances have made OT practical for machine learning: Sinkhorn distances [43] add entropic regularization for efficient approximation, while Sliced Wasserstein [44, 45] projects to 1D for linear complexity in the number of samples. Our key innovation differs from prior OT applications in VLA: rather than using optimal transport to modify the flow matching trajectory (as in OT-CFM), we use Wasserstein distance as an 2 Figure 1: Overview of DiG-Flow and integration of the DiG-Block. (a) We attach the DiG-Block in the pretrained VLM backbone. Observations are projected into shared discrepancy space (obs proj.), while ground-truth (for training) or predicted actions (for inference) are mapped into the same space (act proj.). From these two streams, the block estimates transport-aware discrepancy and converts it into gate = ϕ(D), which softly modulates the backbone features before they are passed to the flow-matching action head. The flow-matching head produces action trajectories from noise as in standard VLA models, and the DiG-Refine module can apply small number of refinement steps entirely within the action head to further polish the predicted actions. (b) Given input embeddings, the backbone first applies standard attention. The post-attention features are then normalized and fed to DiG-Block, which uses the act/obs projections to compute D(µH , µZ) and gate g, and performs gated residual update. This design allows DiG-Flow to modulate the high-level representation using discrepancy signals from actions, while keeping the pretrained backbone architecture and attention blocks intact. auxiliary signal to modulate representation learning. This representation-side intervention complements the flow matching objective and enables robustness to distributional drift without altering the fundamental action generation dynamics. Robustness in Robot Learning. Robustness to distribution shift and spurious correlations has been studied across robot learning contexts [46]. Domain randomization techniques [47] attempt to improve generalization by training under diverse conditions, while domain adaptation methods [48] explicitly align distributions between source and target environments. Recent work has explored ways to identify and mitigate reliance on spurious features through causal reasoning [49] or by learning invariant representations [50]. However, these approaches typically operate during training data collection [51] or through architectural constraints, rather than providing adaptive mechanisms that respond to distributional drift during deployment. Our geometric modulation approach offers complementary perspective: by dynamically measuring and responding to observation-action alignment through Wasserstein distance, we enable the model to adjust its behavior as it encounters distributional shift in long-horizon tasks."
        },
        {
            "title": "3 Preliminaries",
            "content": "We establish notation and background in this section. Flow Matching. Flow matching learns time-dependent vector field vθ(x, t) for [0, 1] that transports base distribution p0 to data distribution p1 via the ODE: dxt = vθ(xt, t), x0 p0. Training minimizes the dt squared regression loss against target vector field v: L(θ) = Et,x1,x0 where xt interpolates between x0 p0 and x1 p1 along chosen probability path. (cid:2)vθ(xt, t) v(xt, t)2(cid:3), (1) VLA Architecture and Action Chunks. VLA model processes multimodal observations = (ovis, olang, oprop) comprising visual images, language instructions, and proprioceptive states. vision-language backbone produces observation features: = (h1, . . . , hT ) RT d, where is the context length and is the feature dimension. Current VLA implementations typically adopt an action chunk formulation: at decision time t, the policy predicts sequence of future actions: at:t+K1 = (at, at+1, . . . , at+K1) RKda , where is the action horizon and da is the raw action dimension. After executing this chunk, the model re-observes the environment at time t+K and generates new chunk, making the policy effectively memoryless between chunks. Flow matching generates these action chunks by conditioning the vector field on observation features: vθ(x, tH). (2) Action Embeddings. To enable geometric comparison between observations and actions, we map raw actions to latent space via an encoder : Rda Rd. For raw action at, we obtain an action embedding: zt = (at) Rd. During training, ground-truth actions from the dataset are encoded to form ground-truth action embeddings zgt ). During inference, predicted actions are encoded as ˆzt = (ˆat). Collecting embeddings over the chunk horizon yields: = (z1, . . . , zK) RKd. The encoder is implemented as lightweight linear projection and trained jointly with the policy. This design allows observations and action embeddings to reside in common feature space where geometric comparisons are meaningful. = (agt Given feature sequences RT and RKd, we form empirical distributions: µH = 1 i=1 δhi, µZ = 1 j=1 δzj , (3) where δx denotes Dirac mass at x. These distributions enable us to quantify the geometric relationship between observations and actions via distributional discrepancy measures."
        },
        {
            "title": "4 Method",
            "content": "We present DiG-Flow, framework for enhancing VLA robustness through geometric regularization of representations. Figure 1(a) shows the overall structure: pretrained vision-language backbone produces multimodal observation tokens, lightweight DiG-Block sits at the interface between the backbone and the flow-matching head, and an optional DiG-Refine module operates purely inside the action expert head."
        },
        {
            "title": "4.1 Discrepancy-guided architecture\nDiscrepancy Function. We define a discrepancy function D : P(Rd) × P(Rd) → R+ that quantifies the\ngeometric distance between probability distributions. Our default choice is the squared 2-Wasserstein distance:",
            "content": "D(µH , µZ) = inf γΓ(µH ,µZ ) y2 dγ(x, y), (4) where Γ(µH , µZ) denotes the set of couplings between µH and µZ. The Wasserstein distance provides geometrically meaningful measure of distributional alignment: lower transport cost indicates that observation and action representations are compatible, while higher cost suggests potential misalignment. For computational efficiency, we approximate 2 2 unit directions ω1, . . . , ωM Uniform(Sd1) and compute: using the sliced Wasserstein distance. We sample random D(µH , µZ) 1 X m=1 2 2 (cid:0)(ω )#µH , (ω )#µZ (cid:1), (5) where (ω computed via the quantile formula: )#µ denotes the pushforward of µ under projection onto ωm. Each 1D Wasserstein distance is 2 2 (cid:0)(ω )#µH , (ω )#µZ (cid:1) = 1 1 (u) 1 (u)2du, (6) Figure 2: Discrepancy-guided gating at training and inference time. Left (training). Observation embeddings hobs and ground-truth action embeddings zgt define empirical distributions (µH , µZ). The discrepancy D(µH , µZ) is mapped to gate = ϕ(D), which modulates residual update on hobs and reweights the flow-matching loss via the gated objective J(θ) = E[g ℓ(θ)]. Right (inference). The same mechanism is applied using encoded model predictions z1, . . . , ˆz instead of ground-truth actions. The gate is computed from the discrepancy between observation embeddings and predicted action embeddings and is then used either once or multiple steps inside the flow-matching head. which reduces to sorting projected features and computing mean squared differences. Alternative discrepancies such as Sinkhorn divergence, maximum mean discrepancy, or cosine distance can be substituted within the same framework. Weight Mapping. We map the discrepancy to modulation weight via monotone decreasing function ϕ : R+ [gmin, 1]. Concretely, we use an exponentially decaying map with lower clip: = ϕ(D) = max{gmin, exp(τ D)}, (7) where τ > 0 is temperature parameter and gmin (0, 1) prevents the gate from vanishing even for large discrepancies. Low-discrepancy (well-aligned) pairs thus receive weights close to 1, while high-discrepancy pairs are down-weighted but still retain non-zero contribution. The map ϕ is monotone decreasing and Lipschitz continuous, matching Assumption 2. Residual Operator. We define lightweight residual operator : RT RT that transforms observation features. In our implementation, is single linear layer with spectral norm regularization: R(H) = WRH + bR, (8) where WR Rdd and bR Rd are learned parameters, and WR2 BR is controlled via spectral normalization. The enhanced observation features are computed via residual update modulated by the gate: = + λ R(H), (9) where λ > 0 is the residual strength parameter. This design ensures that when is large (low discrepancy), the residual adjustment is fully applied, while when is small (high discrepancy), the adjustment is suppressed. The enhanced features are then fed to the flow matching action head in place of the original features H. We integrate these components as module that can be dropped into existing transformer layers without changing their backbone structure. Figure 1(b) illustrates how the DiG-Block is integrated between multi-head attention and the feedforward network. 5 Algorithm 1 DiG-Flow Training Require: Observation o, ground-truth actions agt = {agt operator R, (sliced projections), τ (temperature), λ (residual strength), gmin (gate floor) 1 , . . . , agt K}, flow-matching network vθ, residual ) for = 1, . . . , , replicate to length to form gt RT 1: Extract observation features: VLM(o) RT (agt 2: Encode ground-truth actions: zgt PK k=1 zgt 3: Mean-pool and broadcast: 1 4: Compute discrepancy via sliced Wasserstein: 5: for = 1 to do 6: Sample unit direction ωm Uniform(Sd1) Project: π(m) Sort: π(m), Accumulate: Dm 1 π(m) gtωm ), PT Hωm, sort(π(m) π(m), sort(π(m) H,i π(m), (π(m), 7: 8: ) i=1 ) Z,i PM m=1 Dm 9: 10: end for 11: 1 12: Compute gate (stop gradient): stop_grad(max{gmin, exp(τ D)}) 13: Compute enhanced features: + λ R(H) 14: Compute flow-matching loss (conditional on H): ℓ Et,xtx0 15: Compute gated objective: ℓ 16: Backpropagate θJ and update parameters [vθ(xt, H) v(xt, t)2]"
        },
        {
            "title": "4.2 Training with Transport-aware Gating",
            "content": "During training, DiG-Flow uses ground-truth action chunks to construct the geometric signal; the flowmatching objective itself remains unchanged. For training sample (o, agt), the vision-language backbone produces observation features = VLM(o) RT d, and the action encoder maps ground-truth actions to embeddings zgt ). We form an action semantic centroid by mean-pooling and broadcasting, = (agt = 1 K k=1 zgt , gt = (z, . . . , z) RT d, (10) which induces empirical measures µH and µZgt . This design implies that we effectively model the target action representation as degenerate empirical distribution concentrated at the semantic centroid z. This serves two purposes: (i) it summarizes the semantic intent of the action chunk, and (ii) it makes the discrepancy computation invariant to the temporal ordering of actions, focusing instead on global compatibility. Crucially, we explicitly compute the discrepancy D(µH , µZgt) using the standard Sliced Wasserstein formulation (Algorithm 1). This approach interprets the observation features as distribution and measures the transport cost required to condense them into the action centroid, implicitly penalizing variance and ensuring geometric alignment. While the sorting of gt becomes an identity operation in this degenerate case, we retain the general sorting step in Algorithm 1 to maintain the standard formulation, allowing extensions to non-degenerate action distributions. The gate is obtained by monotone map = max{gmin, exp(τ D)}, with gradients stopped through g. The observation features are then modulated by lightweight residual operator, = + λ R(H), (11) and is fed into the flow-matching head. The final training objective is J(θ) = E(cid:2) sg(g) ℓ(θ; H, t)(cid:3), where ℓ is the per-sample flow-matching loss and sg() denotes the stop-gradient operator. Intuitively, acts purely as data-dependent confidence weight: it suppresses shortcut-like examples with large discrepancy, while avoiding trivial solutions where the backbone collapses features solely to minimize D. Figure 2 summarizes this mechanism. All parameters of the backbone, action encoder, residual operator, and flow-matching head are optimized jointly by differentiating J(θ). (12)"
        },
        {
            "title": "4.3 Inference-time Refinement",
            "content": "Algorithm 2 DiG-Flow Inference with optional refinement Require: Observation o, number of refinement iterations Nrefine 1: Extract observation features: VLM(o) 2: Generate initial action chunk: a(0) FlowModel(H) 3: for = 1 to Nrefine do 4: 5: Encode previous prediction: (i1) {f (a(i1) Compute discrepancy: D(i1) D(µH , µZ(i1)) Compute gate: g(i1) max{gmin, exp(τ D(i1))} 6: Enhance features: (i1) + λ g(i1) R(H) 7: 8: Generate refined action: a(i) FlowModel( (i1)) 9: end for 10: return a(Nrefine) )}K k=1 , aligned to length At inference time, our default configuration mirrors the training-time modulation but avoids extra iterations. Given an observation o, the model first computes = VLM(o) and predicts an action chunk from the flow-matching head. The predicted actions from the previous decision step are encoded and summarized in the same way as during training, yielding an action embedding sequence and empirical measure µZ. The discrepancy is used to form enhanced features = + λ R(H), from which the next action chunk is generated. This single-pass, training-style enhancement is used for all main results. When additional compute is available, DiG-Flow also supports an optional iterative refinement scheme. Starting from an initial prediction a(0) produced using H, we can repeatedly encode a(i), recompute D(i) and g(i), form (i) = + λ g(i) R(H), and obtain refined action a(i+1) from the flow-matching head. In practice, performance saturates within 23 refinement steps. fixed-gate variant of this refinement admits contraction guarantee (Theorem 3). Algorithm 2 describes inference-time usage. We first run the base policy once to obtain an initial action chunk a(0). Refinement iterations then reuse the same discrepancy function D(µH , µZ(i1) ), but now (i1) is computed from the previous prediction instead of the ground truth. The gate g(i1) controls how much residual correction is applied to before feeding it into the flow model again. We call this procedure DiG-Refine."
        },
        {
            "title": "5.1 Optimization and refinement guarantees",
            "content": "We now establish theoretical properties of DiG-Flow that clarify how discrepancy-guided modulation affects optimization and inference. Our analysis addresses three questions that correspond directly to the design choices in DiG-Flow: (i) does training with discrepancy-guided gates define well-behaved optimization objective? (ii) do the residual feature updates help rather than hurt? (iii) can simple refinement scheme converge? We first state the main results and discuss their implications; formal assumptions and proofs appear in Appendix A. Theorem 1 Gated descent on the weighted objective Consider the gated flow-matching objective J(θ) = E(cid:2)g ℓ(θ; H, t)(cid:3), where ℓ(θ; H, t) is the per-sample flowmatching loss and = ϕ(cid:0)D(µH , µZ)(cid:1) [gmin, 1] is data-dependent but parameter-independent weight (i.e., gradients are stopped through when differentiating with respect to θ). Assume that is LJ -smooth in θ. Then for any step size 0 < α < 2/LJ , the gradient descent update θ+ = θ α θJ(θ) satisfies where c1 = α(cid:0)1 αLJ 2 J(θ+) J(θ) c1 θJ(θ)2, (cid:1) > 0. Moreover, under Assumption 2, for all θ, gmin L(θ) J(θ) L(θ), so minimizing controls the original flow-matching loss up to the fixed factor 1/gmin. (13) (14) 7 Theorem 1 shows that training with discrepancy-guided gates yields standard smooth optimization problem: gradient descent on enjoys the usual descent guarantee (13), and uniformly brackets the original loss via (14). In other words, the gates reweight contributions of different samples while keeping and uniformly equivalent as objective functions. Treating as parameter-independent (via stop-gradient) avoids second-order cross terms and preserves this standard descent behaviour. In practice, the observation features and action embeddings are updated jointly with θ, so the gates evolve over training. Theorem 1 therefore characterizes the dynamics of the pseudo-gradient E[g θℓ] under fixed-gate idealization, which matches the implemented stop-gradient update and is empirically observed to lead to stable and robust training. Theorem 2 Residual update improvement Suppose Assumption 3 holds, and assume there exists α0 > 0 such that the residual operator satisfies the gated descent condition E(cid:2)g ℓ(θ; H, t), R(H)(cid:3) α0. Then there exists threshold λmax > 0 such that for all residual strengths 0 < λ λmax, the gated residual update = + λ R(H) satisfies (15) E(cid:2)ℓ(θ; H, t)(cid:3) E(cid:2)ℓ(θ; H, t)(cid:3) β λ, (16) with β = α0/2 > 0. Theorem 2 addresses the effect of the residual operator on the loss. The gated descent condition (15) states that, on average, the direction R(H) aligns with the negative feature gradient once weighted by the gate g. Under smoothness of ℓ in H, sufficiently small gated residual steps strictly decrease the expected loss: the linear improvement from moving in descent direction dominates the quadratic penalty for small enough λ. In practice, is small linear network trained jointly with the policy and implemented with spectral normalization. Because its parameters are optimized to minimize the gated objective J(θ), the network is encouraged to produce residual directions that correlate with loss reduction: if R(H) were uncorrelated with ℓ, it would contribute noise rather than useful signal and be effectively regularized away. We therefore view Eq. (15) as structural assumption capturing the typical behaviour of jointly trained residual operator, rather than an externally imposed constraint. Theorem 3 Fixed-gate refinement convergence Consider an iterative refinement update in action-embedding space: (k+1) = (k) αE(Z (k); g), (17) where the gate is fixed and E(; g) represents an error-correction field. If E(; g) is LE-Lipschitz and µ- , the update defines contraction strongly monotone (see Appendix A.1), then for step sizes 0 < α < 2µ/L2 mapping with rate ρ (0, 1): (k) ρkZ (0) , (18) where is the unique fixed point. Theorem 3 isolates fixed-gate refinement scheme and shows that it admits contraction guarantee under standard conditions. When the refinement field is Lipschitz and strongly monotone (e.g., the gradient of strongly convex functional), the refinement mapping contracts toward unique fixed point, with the error decaying at rate ρk. Our implementation uses more practical variant that recomputes from the current prediction rather than keeping it fixed. Empirically, as the predicted actions improve, the discrepancy D(µH , µZ(i)) decreases and the sequence of gates {g(i)} quickly stabilizes; the local dynamics then behave like contraction with nearly fixed gate, and performance saturates within 3 refinement steps (Section 6.3.4). We therefore view iterative refinement as an optional, well-behaved post-processing step whose behaviour is well captured by this idealized fixed-g analysis. 8 Figure 3: DiG-Flow detects and suppresses shortcut learning through transport-guided gating. Flow matching transforms noise into actions, and multiple solutions can achieve low training loss. Some of these solutions correspond to semantically meaningful alignments between observations and actions (blue distribution), while others exploit spurious correlations or shortcuts (red distribution). DiG-Flow measures the Wasserstein distance between observation and action features to distinguish these pathways. The gate = ϕ(D) selectively modulates learning: Green path (Dlow): semantically aligned features with low transport distance receive strong gates, reinforcing robust behavior; Yellow path (Dmid): intermediate features receive moderate gating; Red path (Dhigh): shortcut-like patterns with high transport distance are down-weighted, discouraging spurious solutions. This geometric reweighting complements the flow-matching loss and helps steer optimization toward representations that remain stable under distribution shift. In this way, DiG-Flow pushes more intelligence toward the foundation model, making VLAs generate more robust actions for general manipulation."
        },
        {
            "title": "5.2 Theoretical Intuition",
            "content": "The previous subsection establishes that the gated objective J(θ) is smooth, that the residual update yields improvement for λ in nontrivial interval, and that the refinement field defines contraction. We now give more explicit intuition in terms of how the discrepancy-controlled gate with above properties reshapes the loss landscape. Dataset decomposition by discrepancy. Let D(H, Z) denote the discrepancy between observation features and action embeddings for particular sample, and let = ϕ(D) with ϕ monotone decreasing. Consider partitioning the dataset into two regions: = {(H, Z) : D(H, Z) δ} = {(H, Z) : D(H, Z) > δ} (geometrically aligned), (geometrically misaligned / shortcut-like), for some threshold δ > 0. Denote the corresponding probabilities by pA = Pr[(H, Z) A], pS = Pr[(H, Z) S], pA + pS = 1. Using this decomposition, the gated objective can be written as J(θ) = E(cid:2)g ℓ(θ; H, t)(cid:3) = pA E(cid:2)g ℓ(θ; H, t) A(cid:3) + pS E(cid:2)g ℓ(θ; H, t) S(cid:3), and its gradient decomposes as θJ(θ) = pA E(cid:2)g θℓ(θ; H, t) A(cid:3) + pS E(cid:2)g θℓ(θ; H, t) S(cid:3). Because ϕ is decreasing in D, we typically have E[g A] 1, E[g S] 1 9 (19) (20) (21) (22) (23) (24) (25) whenever aligned pairs exhibit small discrepancy and shortcut-like pairs exhibit large discrepancy. Thus, even if the raw gradients E[θℓ A] and E[θℓ S] have comparable magnitudes, the effective contributions to the gated gradient in (24) are reweighted by the expected gates. As optimization proceeds, J(θ) therefore behaves as if it were more strongly influenced by the aligned region and less by the shortcut region S. Connection to the residual update condition. The residual update analysis for Theorem 2 uses the alignment condition E(cid:2)g ℓ(θ; H, t), R(H)(cid:3) α0. Using the same decomposition S, this can be rewritten as pA E(cid:2)g ℓ(θ; H, t), R(H) A(cid:3) + pS E(cid:2)g ℓ(θ; H, t), R(H) S(cid:3) α0. (26) (27) If the residual operator is designed to follow the descent direction of the flow-matching loss on aligned samples (so that E[H ℓ, R(H) A] < 0), then the gate further amplifies these contributions relative to those from S. Intuitively, the residual update trusts the directions suggested by geometrically consistent pairs and downweights those arising from high-discrepancy configurations. This is exactly the intuition formalized in Theorem 2: for small enough λ, the first-order improvement from aligned regions dominates the second-order smoothness penalty. Refinement as gated fixed-point iteration. Finally, the refinement operator (Z) = α E(Z; g) can be viewed as fixed-point iteration on the space of action embeddings. Under strong monotonicity and Lipschitz regularity, Theorem 3 shows that is contraction. In practice, this means that once the gate has identified reliable observationaction configurations, the refinement map can iteratively pull predicted actions toward geometry-consistent fixed point without exploding or oscillating. The contraction rate ρ < 1 quantifies how quickly refinement concentrates probability mass around . Figure 3 summarizes the overall picture: discrepancy-guided gating reshapes the effective gradient contributions of different regions of the data space, the residual update exploits this reweighting to improve features in controlled range of strengths, and the refinement operator provides stable way to further align actions with observation geometry at inference time."
        },
        {
            "title": "5.3 Approximation stability of transport-based discrepancy",
            "content": "Finally, we discuss how the theoretical guarantees behave when the exact 2-Wasserstein distance is replaced by practical approximations such as sliced Wasserstein or Sinkhorn divergence. Sliced Wasserstein concentration. Assume that observation and action embeddings lie in bounded ball of radius in Rd. For each random projection direction ωm Sd1, the corresponding one-dimensional Wasserstein distance between the projected empirical measures is bounded by (2R)2. The sliced Wasserstein estimator averages such terms, so by Hoeffdings inequality, for any ϵ > 0, (cid:16) Pr(cid:2) bD E[ bD] > ϵ (cid:3) 2 exp ϵ2 2 (2R)4 (cid:17) . (28) Thus the estimation error scales as O(M 1/2) with high probability. Since the gate is obtained via Lipschitz weight map = ϕ(D) with constant Lϕ (Assumption 2), we have E[g] = ϕ( bD) ϕ(E[ bD]) Lϕ bD E[ bD], (29) so the gate error inherits the same O(M 1/2) concentration rate. In turn, the descent and contraction inequalities in Theorems 13 remain valid up to multiplicative constants that depend smoothly on Lϕ. 10 Sinkhorn divergence. For the Sinkhorn divergence Sε with entropic regularization ε > 0, standard results on entropic optimal transport show that (cid:12) (cid:12)Sε 2 2 (cid:12) (cid:12) b(ε), b(ε) 0 as ε 0, (30) and empirical estimators of Sε concentrate around their population values at rate O(n1/2) in the number of samples n, with constants depending on ε. Propagating this perturbation through the Lipschitz map ϕ again yields bounded perturbation of the gate: (cid:12) (cid:12)ϕ( bSε) ϕ(W 2 2 )(cid:12) (cid:12) Lϕ (cid:16) b(ε) + C(ε) n1/2(cid:17) , (31) for some constant C(ε). Consequently, using Sinkhorn divergence instead of exact Wasserstein distance only affects the numerical constants in our bounds; the qualitative guarantees (existence of positive descent region in λ, and contraction of the fixed-gate refinement) remain unchanged."
        },
        {
            "title": "6 Experiments",
            "content": "We evaluate DiG-Flow on challenging robotic manipulation benchmarks to assess its ability to improve robustness and generalization. Our experiments are designed to answer three questions: 1. Does discrepancy-guided flow matching improve VLA performance? 2. How does DiG-Flow compare to state-of-the-art VLA baselines? 3. Which design choices of DiG-Flow are most important? To demonstrate that DiG-Flow is backbone-agnostic, we select two of the most widely used flow-matching based VLA models, i.e., π0.5 [4] and GR00T-N1 [29], as backbone architectures to evaluate the effectiveness of DiG-Flow across different model designs. For π0.5, we follow the official default configuration. We then insert DiG-Block before the final transformer layer of the VLM backbone, sharing the same observation features as the original policy head. For GR00T-N1, we use the standard tabletop configuration and insert DiG-Block at the final token representation used by the action head. In both models, DiG-Block operates purely at the representation level and does not modify the underlying flow-matching architecture or training objective. We refer to the two variants as π0.5-DiG and GR00T-N1-DiG, respectively."
        },
        {
            "title": "6.1 Simulation Experiments",
            "content": "6.1.1 Experimental Setup LIBERO Benchmark. LIBERO [52] is standardized benchmark for tabletop robotic manipulation with language-conditioned tasks. It consists of four 10-task suites: LIBERO-Spatial (spatial reasoning and object placement), LIBERO-Object (diverse object properties and appearances), LIBERO-Goal (goal-conditioned instructions), and LIBERO-Long (long-horizon, multi-step tasks). Models are evaluated by success rate over 50 rollouts per task, following the official protocol. RoboCasa Benchmark. RoboCasa [53] is scalable household robotics simulation platform built on NVIDIA Isaac Sim, providing photorealistic kitchen scenes and diverse manipulation tasks such as pick-andplace, opening cabinets, and interacting with appliances. We follow the 24 atomic manipulation tasks used in prior work on RoboCasa tabletop evaluation, which group naturally into three categories: (i) pick-and-place, (ii) opening and closing doors or drawers, and (iii) other manipulation skills such as pressing buttons or sliding objects. Each task comes with 50 human demonstrations and thousands of additional synthetic rollouts; to stress generalization in low-data regime, we train all models using only 50 demonstrations per task (one trajectory per demonstration), leaving the synthetic data unused. 11 Training details. Unless otherwise specified, we use the same optimizer and data pipeline as the corresponding backbone. For π0.5-DiG on LIBERO, we train for 30K steps using AdamW with global batch size of 256 across 8 A100 GPUs, peak learning rate of 5 105, and cosine decay. The flow-matching head follows the standard squared regression loss and training-time enhancement. By default, our DiG-Block modules use 32 sliced projections for the discrepancy, temperature of τ = 1.0, residual strength λ = 0.4, and spectral norm bound of BR = 2.0. For GR00T-N1-DiG, we follow the official GR00T-N1 training and evaluation protocol and similarly enable DiG-Block during fine-tuning."
        },
        {
            "title": "6.1.2 Main Results on LIBERO",
            "content": "Table 1 shows that DiG-Flow improves both backbones and achieves state-of-the-art results on LIBERO. On top of the strong π0.5 baseline, π0.5-DiG increases the average success rate from 96.9% to 98.3%, with the largest gain on LIBERO-Long (92.4% 96.4%, +4.0 points). This suite contains long-horizon, multi-step tasks, so the improvement supports our hypothesis that discrepancy-guided modulation reduces error accumulation by encouraging semantically aligned representations. Table 1: Success rates (%) on LIBERO. We report mean success over 50 evaluation episodes per task. DiG-Flow consistently improves both the π0.5 and GR00T-N1 backbones. The largest gains appear on LIBERO-Long, which contains complex multi-step tasks. Method LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Average Diffusion Policy [54] SpatialVLA [26] CoT-VLA [30] OpenVLA [2] OpenVLA-OFT [55] GR00T-N1 [29] π0 [3] π0-Fast [28] π0.5 [4] GR00T-N1-DiG (Ours) π0.5-DiG (Ours) 78.5 88.2 87.5 84.7 97.6 94.4 98.0 96.4 98. 96.0 99.2 87.5 89.9 91.6 88.4 98.4 97.6 96.8 96.8 98.2 98.4 99.0 73.5 78.6 87.6 79.2 97.9 93.0 94.4 88.6 98.0 94.8 98.6 64.8 55.5 69.0 53.7 94.5 90.6 88.4 60.2 92. 92.1 96.4 76.1 78.1 83.9 76.5 97.1 93.9 94.4 85.5 96.9 95.3 98.3 GR00T-N1-DiG exhibits similar behavior, improving the average success rate from 93.9% to 95.3%. Again, the gain is most pronounced on LIBERO-Long (90.6% 92.1%), suggesting that DiG-Flow consistently helps different architectures handle long-horizon reasoning. Importantly, the relative ordering of backbones is preserved: π0.5-DiG remains stronger than GR00T-N1-DiG, indicating that DiG-Flow acts as plug-and-play robustness layer rather than fundamentally changing the underlying policy capacity. 6.1.3 Main Results on RoboCasa (Few-Shot) RoboCasa poses substantially harder generalization challenge than LIBERO: scenes are more diverse, the observation space is more visually complex, and most tasks involve contact-rich manipulation in cluttered kitchens. Moreover, our training regime is intentionally low-data, using only 50 demonstrations per task, which is roughly the number of human demonstrations provided per atomic task in the official benchmark. As shown in Table 2, DiG-Flow yields large gains in this setting. For π0.5, integrating DiG-Flow improves the average success rate from 41.4% to 52.6% (+11.2 points). Improvements are visible in all three categories, but are especially pronounced for door/drawer manipulation (+15.6 points), where precise contact and long-horizon geometry are critical. GR00T-N1 also benefits: GR00T-N1-DiG improves the average success rate from 36.0% to 43.2% (+7.2 points). The fact that both backbones gain substantially from the same DiG-Flow module, under severely limited data, reinforces our interpretation of discrepancy as useful geometric regularizer that reduces overfitting to spurious correlations. 12 Table 2: RoboCasa 24-task benchmark with 50 demonstrations per task. We report success rates (%) averaged within categories and across all tasks. All methods are trained with only 50 demonstrations per task. DiG-Flow substantially improves performance in this challenging few-shot regime. Method Pick & Place Doors / Drawers Others 24-Task Avg π0.5 π0.5-DiG (Ours) GR00T-N1 GR00T-N1-DiG (Ours) 21.5 27.2 18.6 22.4 57.8 73.4 50.2 60.3 44.9 57. 39.1 47.0 41.4 52.6 36.0 43."
        },
        {
            "title": "6.1.4 Robustness Analysis (Simulation)",
            "content": "To further probe robustness in controlled setting, we adopt non-stationary perturbations following previous standards in non-stationarity studies [49, 56]. These perturbations apply time-varying sinusoidal noise to both visual observations and proprioceptive states: c1 cos(c2t), c3 sin(c4t), with coefficients ci (0.01, 0.5). Such perturbations are designed to disrupt brittle correlations that depend on static visual patterns or stereotyped trajectories, while leaving the underlying task semantics intact. Table 3: Robustness to non-stationary perturbations. Success rates (%) under different time-varying noise patterns that disrupt memorized shortcuts. DiG-Flow shows consistent improvements across perturbation types, with the most significant gains on complex long-horizon tasks. Perturbation Method LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Avg Cosine only Sine only Cosine + sine w/o DiG-Flow w/ DiG-Flow w/o DiG-Flow w/ DiG-Flow w/o DiG-Flow w/ DiG-Flow 87.6 87.4 -0.2% 88.2 90.0 +2.0% 87.8 86.4 -1.6% 89.8 89.8 0% 88.4 90.6 +2.5% 87.2 88.2 +1.1% 86.4 91.6 +6.0% 87.2 91.2 +4.6% 86.0 90.8 +5.6% 70.2 80.0 +14.0% 68.6 80.4 +17.2% 67.8 79.2 +16.8% 83.5 87.2 +4.4% 83.1 88.1 +6.0% 82.2 86.2 +4.0% As shown in Table 3, DiG-Flow consistently improves robustness across all perturbation types, with average gains between roughly 46 points. The improvements are most dramatic on LIBERO-Long, where long-horizon execution is particularly sensitive to small systematic biases. Spatial suites show smaller changes, and in few cases performance is slightly reduced, reflecting that not all perturbations exclusively target shortcuts. Overall, these results support the view that discrepancy-guided gating makes policies less reliant on brittle, non-stationary correlations and more grounded in the geometric relationship between observations and actions."
        },
        {
            "title": "6.2 Real Robot Experiments",
            "content": "6.2.1 Real Robot Setup We build real robot setup similar to LIBERO and RoboCasa to validate our experiments in more challenging real-world physical environment. We use 7-DoF Franka Research 3 (FR3) arm equipped with 6-DoF Inspire dexterous hand. Perception is provided by two Intel RealSense cameras mounted at complementary viewpoints, forming stereo-like arrangement that reduces single-view occlusions. Both cameras stream synchronized RGB frames. The VLM backbone receives fused representation of these views together with proprioceptive inputs (joint positions, velocities, and gripper states). Commands are sent to the low-level controller at 20 Hz. Following the chunked-action protocol, the policy outputs action chunks over short horizon, which are then executed in open-loop without additional model-predictive control or trajectory optimization. The overall hardware layout is illustrated in Figure 4(a). Compared to simple two-finger gripper, the dexterous hand substantially increases the control dimensionality and contact complexity: the policy must not only reach the correct pose, but also coordinate multiple finger joints for stable power and precision grasps, tool operation (e.g., spraying, wiping), and in-hand adjustments. This task setting further exposes policy errors: inaccurate predictions cannot be easily corrected by low-level planners, making robustness and alignment particularly important. 13 Figure 4: (a) Real robot hardware setup (single-arm + dexterous hand. (b) Examples of real robot tasks. The two rows correspond to the two camera views. To assess real-world applicability of DiG-Flow, we design four distinct tasks covering different manipulation challenges. These tasks impose realistic 3D geometry, contact dynamics, and visual complexity, making them ideal for testing the hypothesis that discrepancy-guided modulation improves semantic grounding and robustness. We now detail the four real-robot tasks. These tasks are designed to mirror the difficulty patterns observed in LIBERO and RoboCasa while exposing additional challenges from real-world physics, sensor noise, and human interference. The tasks example settings are shown in Figure 4(b). Stack-Bowls: stack bowls from scattered initial positions into tower; tests precise multi-object grasping and placement, as well as collision avoidance between the hand and previously stacked bowls. Spray-Plant: pick up spray bottle and apply water mist to potted plant along its foliage; tests tool-use behavior, fine motion control, and spatial anticipation to cover the plant surface. Wipe-Whiteboard: use cloth to erase pen marks from whiteboard; tests contact-rich surface interaction, maintaining stable pressure, and long-horizon sweeping motions. Sort-Into-Drawer: pick three varied objects and place them into drawer one by one; tests sequential multi-step reasoning, diverse grasping strategies, and error accumulation over multiple object transfers. For each task we only collect 50 human-teleoperated demonstrations for training, and evaluate for both whole-task success and sub-task success. Stack-Bowls has 5 subtasks, while the other tasks have 3 subtasks each. We report overall success rates and detailed analysis on perturbation and robustness tests. Table 4: Success rates (%) of our models vs baseline on real robot tasks. We show sub-task / whole-task success rates. Task Stack-Bowls Spray-Plant Wipe-Whiteboard Sort-Into-Drawer π0.5 (baseline) π0.5-DiG (Ours) 58 / 40 62 / 48 66 / 45 72 / 52 62 / 38 70 / 48 52 / 33 60 / 41 Table 4 shows that DiG-Flow consistently improves performance across all four tasks. On the long-horizon Sort-Into-Drawer task, whole-task success increases from 33% to 41% (+8 pts), illustrating that DiG-Flow helps mitigate error accumulation in sequential decision chains. The more moderate gains on Spray-Plant and Wipe-Whiteboard reflect shorter horizons but still show benefit, particularly in sub-task success. 6.2.2 Robustness Analysis (Real Robot) We next analyze how DiG-Flow behaves under real-world perturbations that were not in the training demonstrations. The perturbed setups are illustrated in Figure 6, and the corresponding success rates are reported in Table 5. 14 Figure 5: Real robot rollouts on four manipulation tasks. Each row shows successful execution by π0.5-DiG from left to right. (a) Stack-Bowls: the robot sequentially grasps and stacks three bowls into stable tower. (b) Spray-Plant: the robot picks up spray bottle and waters the plant along its foliage. (c) Wipe-Whiteboard: the robot wipes away pen marks on the board using cloth. (d) Sort-Into-Drawer: the robot picks and places three objects into drawer, long-horizon task that compounds errors across steps. These qualitative rollouts illustrate that DiG-Flow enables stable multi-step behaviours across diverse contact-rich settings. Table 5: Success rates (%) under unseen / perturbed conditions. We show sub-task / whole-task success rates. Method Stack-Bowls (BG) Sort-Into-Drawer (BG) Spray-Plant (Human) Wipe-Whiteboard (Human) π0.5 π0.5-DiG (Ours) 42 / 15 65 / 40 48 / 20 60 / 35 / 10 62 / 30 44 / 20 58 / 30 Background Shifts. For Stack-Bowls and Sort-Into-Drawer, we change the table cloth color and pattern, add additional objects near the workspace, and vary global lighting conditions. Despite these shifts, our policy with DiG-Flow maintains high success rates that are close to the unperturbed setting: most failures are due to rare extreme occlusions rather than systematic mistakes. Qualitatively, we observe that the robot continues to focus on the geometry of task-relevant objects (bowls, drawer, target items) even when the background appearance changes substantially, suggesting that the transport-based gating helps deprioritize accidental correlations with background textures. Human Interference. For Spray-Plant and Wipe-Whiteboard, we introduce human hand that moves the plant or writes new strokes on the board while the robot is executing the policy. These interventions create both visual distractions and true dynamical changes (targets moving during execution). We find that the policy often adapts by slightly adjusting its trajectory to follow the plant motion or to cover newly written strokes, and overall success remains high relative to the no-interference baseline. Failures typically arise when the human intervention is adversarially timed (e.g., moving the plant exactly as the robot closes its grasp), which is challenging for any open-loop controller. Table 5 shows that all four perturbed variants are challenging: both policies see noticeable drops compared to the clean setting, but π0.5-DiG remains consistently more robust. On Stack-Bowls (BG) and Spray-Plant (Human), the baseline whole-task success falls to 15% and 10%, while π0.5-DiG still achieves 40% and 30%, respectively. Even on the harder long-horizon Sort-Into-Drawer (BG) scenario, π0.5-DiG improves whole-task success from 20% to 35%. similar advantage appears for Wipe-Whiteboard (Human), where π0.5-DiG reaches 30% vs. 20% for the baseline. Overall, the relative gap between the two models is preserved or enlarged under Figure 6: Unseen and interfered real-world settings. We evaluate robustness under (a) Background Shifts, where table textures, cloths, and distractor objects are changed for Stack-Bowls and Sort-Into-Drawer, and (b) Human Interference, where human hand moves the plant or writes on the whiteboard during wiping. These perturbations introduce visual and dynamic distribution shifts beyond the training data and correspond to the robustness results in Table 5. background and human interference, indicating that discrepancy-guided modulation helps maintain stable observation-action alignment rather than overfitting to specific training scenes. Overall, these results support the view that geometry-aware gating improves robustness in the real world: the policy is less sensitive to superficial background statistics and more focused on the observationaction alignment relevant for the task. 6.2.3 High-DoF Humanoid with Active View Control To further stress-test DiG-Flow on more complex embodiments, we deploy π0.5 and π0.5-DiG on 31-DoF upper-body humanoid platform equipped with dual dexterous hands and an actively controlled head camera. As illustrated in Figure 7, the robot has independently actuated joints for the torso, neck, arms, and multi-finger hands, resulting in substantially higher dimensionality and contact complexity than traditional real-robot experiments. Unlike prior VLA settings that operate from fixed third-person view and control only low-DoF end-effector, our policy must simultaneously coordinate head, body, and both hands, making both state estimation and control more challenging. The head houses pair of RGB cameras that provide an egocentric, movable viewpoint. During teleoperation, the operator naturally moves the head to obtain task-relevant views. The policy is trained to imitate this behavior and thus learns to actively adjust the viewpoint while manipulating objects. Figure 8 shows an example rollout: the robot first orients the head to obtain clear view of the workspace, then uses dexterous hands to pick up objects and place them into box while continuously refining its viewing angle. This active-view setup is particularly sensitive to representation misalignment. We collect 1000 teleoperated demonstrations of general tabletop clean-up tasks, where the robot is instructed to place 1-3 objects on the table into box from diverse initial layouts. Both π0.5 and π0.5-DiG are trained on this dataset with batch size of 256 for 20K training steps, using the same flow-matching configuration as in our other real-robot experiments. We evaluate each model on 20 rollouts per condition, and report success rates in Table 6. The Seen setting uses the same object categories and background as training, while the Unseen setting replaces both objects and background textures to induce distribution shift. 16 Figure 7: High-DoF humanoid setup. Our 31-DoF humanoid platform with dual dexterous hands and an actively controlled head. The head mounts dual RGB cameras, while the torso and arms provide upper-body motion. The task is to clear 1-3 objects from the table into the box. This setting requires the policy to jointly coordinate head, body, and hands, rather than controlling only low-DoF gripper from fixed camera. Figure 8: Egocentric active-view rollouts. Example sequence from the head-mounted camera during clean-up episode. The humanoid first adjusts its viewpoint to obtain clear observation of the workspace, then successively grasps objects with its dexterous hands and places them into the box. DiG-Flow helps maintain robust observationaction alignment under camera motion and self-occlusions. On this challenging real-robot setting, the standard π0.5 baseline achieves only moderate performance, particularly when the scene contains multiple objects or unseen visual conditions. In contrast, π0.5-DiG consistently improves success rates by 5-10 points across seen objects and backgrounds. As for unseen objects and background shift, the gains are even more pronounced. The advantages brought by DiG-Flow are also more obvious in complex multi-object tasks. These results indicate that discrepancy-guided modulation remains effective even when the policy coordinates high-DoF humanoid with active view control, providing both stronger performance and more robust behavior under visual and dynamical perturbations."
        },
        {
            "title": "6.3 Further Discussion on Method Design",
            "content": "In this section, we further explain the method design of DiG-Flow through ablation studies and mechanism analyses on the standard LIBERO benchmark. 6.3.1 Discrepancy and Gating Ablations Our method has two key design choices: the discrepancy D(µH , µZ) that measures observationaction mismatch, and the scalar gate = ϕ(D) that maps this signal back to the backbone. We therefore ablate these two components separately. Table 7 examines how different discrepancy functions affect performance when applied to π0.5-DiG on LIBERO. All variants use the same scalar mapping = exp(τ D) and residual operator; only D(µH , µZ) is changed. 17 Table 6: Success rates (%) on high-DoF humanoid clean-up tasks. Each entry reports the success rate over 20 evaluation rollouts for clearing 1-3 objects into the box. Seen uses the same object categories and background as the demonstrations, while Unseen uses novel objects and background textures. Method 1 obj π0.5 π0.5-DiG 75 75 Seen 2 objs 60 65 3 objs 35 45 Unseen & Perturbed 3 objs 1 obj 2 objs 55 65 30 45 25 Table 7: Effect of discrepancy choice on LIBERO (success %). All rows use the same π0.5 backbone, scalar mapping = exp(τ D), and residual operator; only the discrepancy D(µH , µZ) is varied. Wassersteinbased discrepancies provide the strongest signal, especially on LIBERO-Long. Discrepancy Cosine distance MMD (RBF kernel) Sinkhorn (ε = 0.1) LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long 98.2 -1.0% 98.4 -0.8% 98.8 -0.4% 97.8 -1.2% 98.2 -0.8% 98.6 -0.4% 99.0 97.2 -1.4% 97.6 -1.0% 98.2 -0.4% 98.6 92.6 -3.8% 93.4 -3.0% 95.8 -0.6% Avg 96.5 -1.8% 96.8 -1.5% 97.9 -0.4% 96. 98.3 Sliced Wasserstein (Ours) 99.2 Non-transport discrepancies such as cosine distance and MMD already provide moderate gains over the backbone (average 96.5% and 96.8%), confirming that even coarse measures of observationaction similarity can be beneficial. However, Wasserstein-based discrepancies consistently perform better. Sinkhorn divergence (entropic OT) achieves 97.9% average success, while our default sliced 2-Wasserstein distance attains 98.3% and the best results on LIBERO-Long. This matches our theoretical intuition: Wasserstein distances respect the underlying geometry of the feature space, yielding more faithful measure of distributional misalignment and better-behaved supervision signal for the gate. Table 8: Comparison of gating mechanisms. Success rates (%) for different gating strategies with the discrepancy fixed to sliced 2-Wasserstein. In contrast to fixed or random modulation, our transport gate converts the geometric discrepancy into meaningful confidence signal, yielding the best performance, especially on LIBERO-Long. LIBERO-Spatial LIBERO-Object LIBERO-Goal LIBERO-Long Gating Strategy Fixed Gate (g = 0.5) Random Gate (g U(0, 1)) 96.8 -2.4% 95.2 -4.0% Transport Gate (Ours) 99.2 95.4 -3.6% 93.8 -5.3% 99. 92.2 -6.5% 89.4 -9.3% 98.6 84.6 -12.2% 80.8 -16.2% Avg 92.3 -6.1% 89.8 -8.7% 96.4 98. Table 8 complements this study by fixing the sliced 2-Wasserstein discrepancy and varying only the gating mechanism. Replacing our transport gate with fixed scalar (g = 0.5) reduces performance to 92.3% average success (-6.1%), indicating that uniform feature modulation cannot distinguish between semantically useful and spurious patterns. Random gating performs even worse at 89.8% (-8.7%), showing that arbitrary feature suppression actively harms learning by disrupting both beneficial and harmful correlations. The degradation is particularly severe on LIBERO-Long (up to -16.2%), where long-horizon reasoning amplifies the effect of shortcut solutions. In contrast, the discrepancy-guided transport gate preserves performance on simpler suites and yields the largest gains on LIBERO-Long, consistent with our interpretation that geometric alignment is most critical in sequential tasks. Taken together, Tables 7 and 8 show that both the choice of discrepancy and the shape of the gate ϕ(D) matter for robust optimization."
        },
        {
            "title": "6.3.2 Refinement Ablation",
            "content": "In Section 5.1, we state that performance saturates within 3 refinement steps. Here we provide more detailed analysis of this behavior on the standard LIBERO benchmark. Figure 9: Effect of refinement steps Nrefine on LIBERO. We vary the number of refinement iterations at inference time while keeping the training setup fixed. Even without refinement (Nrefine=0), the discrepancyguided flow model already surpasses the backbone policy. Adding small number of refinement steps further improves performance and quickly saturates around Nrefine=3. Figure 9 reports the average LIBERO success rate when we vary the number of refinement iterations Nrefine at test time. We observe three consistent trends: Performance gains even with Nrefine=0. The curve shows that the Nrefine=0 variant already achieves strong success rate on LIBERO. This indicates that the discrepancy-guided residual update improves the learned policy itself, rather than only acting as test-time heuristic. Few refinement steps bring stable improvements. Increasing Nrefine from 0 to 3 yields consistent performance gains and quickly brings the curve close to saturation. This supports our choice of using Nrefine=3 by default: it captures most of the benefit of refinement while keeping the additional computation per control step modest. Beyond 3 steps, performance saturates with small fluctuations. For Nrefine > 3, the success rate fluctuates slightly around the same plateau level, without systematic degradation. We attribute the small variations to evaluation noise and the fact that once the flow model has already produced near-optimal action chunk, further refinement only provides marginal adjustments. Overall, these results confirm that (i) DiG-Flow is already useful without refinement, and (ii) small number of refinement steps provides good trade-off between robustness/performance and inference cost. 6.3.3 Hyperparameter Analysis Figure 10 summarizes the sensitivity of DiG-Flow to its main hyperparameters. Panel 10(a) varies the number of sliced projections used to approximate the Wasserstein discrepancy. We observe smooth, monotone improvement as increases from 4 to 20, after which performance quickly saturates around 2832. This matches the theoretical trade-off: more projections reduce Monte Carlo variance in the sliced Wasserstein estimate, but beyond moderate budget the marginal benefit becomes negligible relative to the dominant transformer cost. In all reported experiments we therefore fix = 32, which lies in the flat regime and offers good balance between accuracy and overhead. Panel 10(b) studies the joint effect of the residual strength λ (in Eq.(11)) and the temperature τ in the gate = exp(τ D). The heatmap reveals two important properties. First, there is broad plateau around (λ, τ ) (0.4, 1.0), indicating that DiG-Flow is not overly sensitive to small mis-tuning and that our default choice lies in robust operating region rather than at sharp optimum. Second, the high-performance region forms an elongated ridge approximately following λτ const, which is consistent with the interpretation that λ and τ jointly control an effective gating strength λg(τ ): increasing λ can be partially compensated by 19 Figure 10: Hyperparameter sensitivity of DiG-Flow. (a) Success rate as function of the number of sliced projections used for approximating the Wasserstein discrepancy. Performance improves steadily from very small and saturates once 2832, indicating that moderate number of projections is sufficient for stable transport estimate. (b) Joint effect of the residual strength λ (in Eq. 11) and the temperature τ in = max{gmin, exp(τ D)}. We observe broad plateau around (λ, τ ) (0.4, 1.0) and an elongated ridge roughly following λτ 0.4, showing that the two hyperparameters jointly control an effective gating strength while still providing some flexibility: increasing λ can be partially compensated by decreasing τ , and vice versa, but moving too far from the ridge degrades performance. reducing τ (which softens the gate), and vice versa. The slight diagonal elongation reflects the fact that the effective step size in feature space scales like HF λ R(H)F λ max{gmin, exp(τ D)} R(H)F , (32) At the same time, the ridge is not perfectly flat, so λ and τ are not fully redundant. In particular, large λ with aggressive τ can lead to over-amplified residuals that violate the small-step assumption in Theorem 2, while very small λ with extremely soft gates under-utilizes the residual pathway and reduces the advantage over the baseline. The heatmap therefore supports the theoretical picture: there exists nontrivial band of (λ, τ ) values where the transport-guided residual update yields consistent gains, and our defaults (λ = 0.4, τ = 1.0) sit well inside this stable band rather than being knife-edge configuration. 6.3.4 Transport-Cost Dynamics and Gating Behaviour In addition to task-level success rates, we also monitor the behaviour of the transport discrepancy D(µH , µZ) during training. Figure 11 reports the evolution of the average sliced 2-Wasserstein cost between the empirical observation and action embedding distributions over training iterations for DiG-Flow on LIBERO. Three observations are worth highlighting. (i) Training phases. At the beginning of training the transport cost is relatively high, reflecting the fact that observation features and action embeddings live in poorly aligned regions of the feature space. As optimisation proceeds, D(µH , µZ) decreases monotonically and then enters steady-state regime where it fluctuates around stable value. This behaviour is consistent with the interpretation of as measuring semantic compatibility between observation and action representations: the model gradually learns to place both in geometrically coherent region. Importantly, the curve in Figure 11 does not keep decreasing towards (ii) as signal, not loss term. zero. Instead, it converges to medium range where is neither vanishing nor exploding. This is exactly the regime in which the gate = max(cid:8)gmin, exp(cid:0)τ D(µH , µZ)(cid:1)(cid:9) retains discriminative power: samples with relatively low discrepancy still receive stronger gates, while samples with larger discrepancy are down-weighted, but no regime dominates completely. Driving to (near-)zero would make 1 for almost all samples and therefore remove the benefit of discrepancy-guided modulation; conversely, extremely large would force 0 and effectively deactivate the residual path. 20 Figure 11: Dynamics of the transport discrepancy D(µH , µZ) during training. We plot the average sliced 2 between observation and action embeddings over training iterations (shaded region indicates 2 variability across batches). The cost decreases steadily in the early phase and then stabilizes in medium range, instead of collapsing to zero, matching the intended role of as discriminative geometric signal rather than pure minimization target. (iii) Connection to gated residual theory and hyperparameters. The observed range of provides empirical support for the theoretical assumptions used in Section 5. Theorem 2 requires that the gated residual update = + λ R(H) remains in regime where the first-order improvement term dominates the quadratic penalty controlled by LH and BR (Assumption 3). The fact that stabilizes in moderate range implies that the effective step size (Eq. (32)) naturally stays within such small but non-negligible band throughout training, matching the regime in which Theorem 2 guarantees expected loss reduction. This also explains the shape of the hyperparameter landscape in Figure 10. Since D(µH , µZ) concentrates in relatively narrow interval once training has converged, there exists family of (λ, τ ) pairs that produce similar effective magnitudes of λg on typical samples. The broad plateau in Figure 10(b) and the smooth one-dimensional curves over λ and τ are consistent with this picture: DiG-Flow is robust to moderate changes in either parameter, as long as the resulting transport cost keeps the gate in the informative middle range illustrated in Figure 11."
        },
        {
            "title": "7 Conclusions and Limitations",
            "content": "We presented DiG-Flow, discrepancy-guided flow matching framework that regularizes VLA representations through simple geometric signal between observation and action embeddings. By mapping this distributional discrepancy to scalar gate and applying residual updates in feature space, DiG-Flow leaves the flow-matching path and objective intact while shaping the representations used by the action head. Our analysis shows that the gated objective admits standard descent guarantees, that suitably small residual updates provably reduce the loss under an alignment condition, and that fixed-gate refinement scheme forms contraction. While DiG-Flow demonstrates strong performance in both simulation and real-world experiments, several limitations warrant discussion. First, our theoretical analysis assumes that features have bounded norms (e.g., HF R). This is reasonable for normalized representations but may require additional scaling or normalization mechanisms when applied to architectures that do not enforce such bounds. Second, the transport distance computation currently relies on batch-level statistics, which can be influenced by outliers or small batch sizes. In principle, this could make the gate slightly sensitive to the composition of mini-batch. Exploring alternatives such as running statistics, more robust discrepancy measures, or fully instance-wise variants would be an interesting direction for future work. Finally, DiG-Flow uses ground-truth actions during supervised training to compute meaningful observationaction discrepancies. Extending the same principle to self-supervised or reinforcement learning regimes would require alternative alignment signals (e.g., consistency across rollouts, value-based or advantage-based weighting), which we leave for future research."
        },
        {
            "title": "References",
            "content": "[1] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, KuangHuei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale, 2023. [2] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Baljekar, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [3] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. [4] Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. [5] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2023. [6] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. [7] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [8] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [9] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [10] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. [11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [12] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. [13] Yicheng Feng, Yijiang Li, Wanpeng Zhang, Sipeng Zheng, Hao Luo, Zihao Yue, and Zongqing Lu. Videoorion: Tokenizing object dynamics in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2040120412, October 2025. [14] Hao Luo, Zihao Yue, Wanpeng Zhang, Yicheng Feng, Sipeng Zheng, Deheng Ye, and Zongqing Lu. OpenMMEgo: Enhancing egocentric understanding for LMMs with open weights and data. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [15] Wanpeng Zhang, Zilong Xie, Yicheng Feng, Yijiang Li, Xingrun Xing, Sipeng Zheng, and Zongqing Lu. From pixels to tokens: Byte-pair encoding on quantized visual modalities. In The Thirteenth International Conference on Learning Representations, 2025. 22 [16] Wanpeng Zhang, Yicheng Feng, Hao Luo, Yijiang Li, Zihao Yue, Sipeng Zheng, and Zongqing Lu. Unified multimodal understanding via byte-pair visual encoding. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1297612986, October 2025. [17] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. [18] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2629626306, 2024. [19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [20] Yadong Lu, Chunyuan Li, Haotian Liu, Jianwei Yang, Jianfeng Gao, and Yelong Shen. An empirical study of scaling instruct-tuned large multimodal models. arXiv preprint arXiv:2309.09958, 2023. [21] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alexander Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. [22] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. In International Conference on Machine Learning, pages 84698488. PMLR, 2023. [23] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, Jianlan Luo, You Liang Tan, Lawrence Yunliang Chen, Pannag Sanketi, Quan Vuong, Ted Xiao, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy, 2024. [24] Chi-Lam Cheang, Guangzeng Chen, Ya Jing, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Hongtao Wu, Jiafeng Xu, Yichu Yang, Hanbo Zhang, and Minzhao Zhu. Gr-2: generative video-language-action model with web-scale knowledge for robot manipulation. arXiv preprint arXiv:2410.06158, 2024. [25] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. Gr-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [26] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, and Xuelong Li. Spatialvla: Exploring spatial representations for visual-language-action model, 2025. [27] Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, and Pieter Abbeel. Otter: vision-language-action model with text-aware visual feature extraction. arXiv preprint arXiv:2503.03734, 2025. [28] Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models, 2025. [29] Johan Bjorck, Fernando Castañeda, Nikita Cherniaiev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [30] Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Ming-Yu Liu, Donglai Xiang, Gordon Wetzstein, and Tsung-Yi Lin. Cot-vla: Visual chain-of-thought reasoning for vision-language-action models, 2025. [31] Hao Luo, Yicheng Feng, Wanpeng Zhang, Sipeng Zheng, Ye Wang, Haoqi Yuan, Jiazheng Liu, Chaoyi Xu, Qin Jin, and Zongqing Lu. Being-h0: vision-language-action pretraining from large-scale human videos. arXiv preprint arXiv:2507.15597, 2025. 23 [32] Fanqi Lin, Ruiqian Nai, Yingdong Hu, Jiacheng You, Junming Zhao, and Yang Gao. Onetwovla: unified vision-language-action model with adaptive reasoning. arXiv preprint arXiv:2505.11917, 2025. [33] Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems, volume 31, 2018. [34] Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. arXiv preprint arXiv:1810.01367, 2018. [35] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. Journal of Machine Learning Research, 22(57):164, 2021. [36] Alexander Tong, Jessie Huang, Guy Wolf, David Van Dijk, and Smita Krishnaswamy. Conditional flow matching: Simulation-free training of continuous normalizing flows. arXiv preprint arXiv:2302.00482, 2023. [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [38] Michael Albergo, Nicholas Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. [39] Aram-Alexandre Pooladian, Heli Ben-Hamu, Carles Domingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky TQ Chen. Multisample flow matching: Straightening flows with minibatch couplings. arXiv preprint arXiv:2304.14772, 2023. [40] Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2024. [41] Cédric Villani. Optimal transport: old and new, volume 338. Springer, 2009. [42] Gabriel Peyré and Marco Cuturi. Computational optimal transport. Foundations and Trends in Machine Learning, 11(5-6):355607, 2019. [43] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in neural information processing systems, 26, 2013. [44] Nicolas Bonneel, Julien Rabin, Gabriel Peyré, and Hanspeter Pfister. Sliced and radon wasserstein barycenters of measures. In Journal of Mathematical Imaging and Vision, volume 51, pages 2245, 2015. [45] Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized sliced wasserstein distances. Advances in neural information processing systems, 32, 2019. [46] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665673, 2020. [47] Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 2330. IEEE, 2017. [48] Eric Tzeng, Coline Devin, Judy Hoffman, Chelsea Finn, Pieter Abbeel, Sergey Levine, Kate Saenko, and Trevor Darrell. Adapting deep visuomotor representations with weak pairwise constraints. In Algorithmic Foundations of Robotics XII: Proceedings of the Twelfth Workshop on the Algorithmic Foundations of Robotics, pages 688703. Springer, 2020. [49] Wanpeng Zhang, Yilin Li, Boyu Yang, and Zongqing Lu. Tackling non-stationarity in reinforcement learning via causal-origin representation. In Proceedings of the 41st International Conference on Machine Learning, volume 235, pages 5926459288. PMLR, 2024. [50] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International conference on machine learning, pages 58155826. PMLR, 2021. [51] Youguang Xing, Xu Luo, Junlin Xie, Lianli Gao, Hengtao Shen, and Jingkuan Song. Shortcut learning in generalist robot policies: The role of dataset diversity and fragmentation. arXiv preprint arXiv:2508.06426, 2025. 24 [52] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. arXiv preprint arXiv:2306.03310, 2023. [53] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. [54] Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. arXiv preprint arXiv:2303.04137, 2023. [55] Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. [56] Fan Feng, Biwei Huang, Kun Zhang, and Sara Magliacane. Factored adaptation for non-stationary reinforcement learning. Advances in Neural Information Processing Systems, 35:3195731971, 2022."
        },
        {
            "title": "A Full Proofs and Further Discussions",
            "content": "This appendix provides formal assumptions and complete proofs of the theoretical results stated in Section 5. We keep the notation from the main text and make explicit the regularity conditions under which the guarantees hold. A.1 Standing Assumptions Recall that for single training example with observation features and time t, the per-sample flow-matching loss is ℓ(θ; H, t) = (cid:13) (cid:13)vθ(H, t) v(H, t)(cid:13) 2 (cid:13) , and the dataset objective is L(θ) = E(cid:2)ℓ(θ; H, t)(cid:3). The discrepancy-guided objective used by DiG-Flow is J(θ) = E(cid:2)g ℓ(θ; H, t)(cid:3), = ϕ(cid:0)D(µH , µZ)(cid:1), (33) (34) (35) where µH and µZ are the empirical measures defined in the main text, and gradients are stopped through when differentiating with respect to θ. We use for the Euclidean norm on vectors and for the Frobenius norm on matrices or tensor-valued sequences (such as RT or RKd). The inner product between two tensors of the same shape is written as , . We now state the regularity assumptions used in the proofs. Assumption 1 Parameter smoothness in θ There exists Lθ > 0 such that L(θ) is Lθ-smooth in θ, i.e., for all θ, θ, L(θ) L(θ) + (cid:10)θL(θ), θ θ(cid:11) + Lθ 2 θ θ2. Assumption 2 Gate properties The weight mapping ϕ : R+ [gmin, 1] satisfies: Monotone decreasing: if s1 s2 then ϕ(s1) ϕ(s2). Lipschitz: there exists Lϕ such that ϕ(s) ϕ(s) Lϕs for all s, 0. Bounded away from zero: = ϕ(D) [gmin, 1] with gmin > 0. In particular, for all θ we have the bracketing inequality in the main text, gmin L(θ) J(θ) L(θ), which is Eq. (14). (36) (37) Assumption 3 Feature smoothness and residual boundedness For fixed (θ, t), the loss as function of the observation features is LH -smooth in Frobenius norm: for all H, , ℓ(θ; , t) ℓ(θ; H, t) + (cid:10)H ℓ(θ; H, t), H(cid:11) + LH Moreover, the residual operator has bounded operator norm: there exists BR > 0 such that 2 H2 . (38) (39) R(H)F BR HF , H. 26 Assumption 4 Bounded feature second moment There exists constant CH > 0 such that E(cid:2)H2 (cid:3) over the training distribution. Assumption 5 Refinement field regularity For each fixed gate g, the refinement error field E(; g) : RKd RKd (40) (41) used in the optional DiG-Refine procedure is LE-Lipschitz and µ-strongly monotone, i.e. for all Z1, Z2, (cid:13)E(Z1; g) E(Z2; g)(cid:13) (cid:13) (cid:13)F LE Z1 Z2F , (cid:10)Z1 Z2, E(Z1; g) E(Z2; g)(cid:11) µ Z1 Z22 , µ > 0. (42) (43) Assumptions 14 are standard in analyses of smooth neural-network objectives: they hold when vθ is implemented by network with Lipschitz activations and bounded weights, and ℓ is the squared error between vθ and target field. Assumption 2 is satisfied by the clipped exponential mapping ϕ(D) = max{gmin, exp(τ D)} used in our implementation, which is monotone and Lipschitz on R+. Assumption 5 captures the usual regularity of gradient-like refinement fields appearing in fixed-point analyses. In addition to these regularity conditions, the residual-update result in Theorem 2 also uses gated descent condition (Eq. (15)) on the learned residual field; although we state this condition directly in the theorem rather than as separate standing assumption, it can be interpreted as description of the average behaviour of the jointly trained residual operator, as discussed below. Remark (On the gated descent condition). The gated descent condition in Eq. (15), used in Theorem 2, is not intended as an arbitrary extra constraint, but as structural description of the learned residual field. When optimizing composite objective of the form E[g ℓ(θ; H, t)] (as in our implementation), the parameters of receive gradients that encourage them to produce directions aligned with loss reduction. If R(H) were orthogonal to ℓ(θ; H, t) on average, it would not consistently decrease the loss and would be effectively regularized away. Thus, under successful training, it is natural to expect E[g ℓ(θ; H, t), R(H)] < 0, so Eq. (15) can be viewed as codifying the typical behaviour of jointly trained residual operator rather than imposing qualitatively new regularity assumption. A.2 Smoothness of the gated objective We first record simple lemma that links the smoothness of to that of J, clarifying why the gated objective remains well-behaved. Lemma 1 Smoothness of the gated objective Under Assumptions 1 and 2, the gated objective J(θ) = E(cid:2)g ℓ(θ; H, t)(cid:3) is LJ -smooth in θ with LJ Lθ. Proof. For each sample, treating as constant w.r.t. θ (gradients are stopped through g), we have By Lθ-smoothness of ℓ(θ; H, t) in θ, θ (cid:2)g ℓ(θ; H, t)(cid:3) = θℓ(θ; H, t). θℓ(θ; H, t) θℓ(θ; H, t) Lθ θ θ. 27 (44) (45) (46) Therefore (cid:13) (cid:13)θ (cid:2)g ℓ(θ; H, t)(cid:3) θ (cid:2)g ℓ(θ; H, t)(cid:3)(cid:13) (cid:13) = θℓ(θ; H, t) θℓ(θ; H, t) Lθ θ θ, (47) (48) because 1. Thus each term ℓ(θ; H, t) is Lθ-smooth in θ, and J(θ), being their expectation, is LJ -smooth with LJ Lθ. A.3 Proof of Theorem 1 (Gated descent) We now prove the gated-descent guarantee stated in the main text. Proof of Theorem 1. By Lemma 1, is LJ -smooth in θ. For any LJ -smooth function , the standard descent inequality holds: for gradient descent step θ+ = θ αF (θ) with 0 < α < 2/LJ , Applying this to = gives (θ+) (θ) α (cid:16) 1 (cid:17) αLJ 2 (θ)2. J(θ+) J(θ) α (cid:16) 1 (cid:17) αLJ 2 θJ(θ)2, which matches Eq. (13) in the main text with c1 = α (cid:16) 1 (cid:17) αLJ > 0. (49) (50) (51) The bracketing relation gminL(θ) J(θ) L(θ) (Eq. (14)) follows directly from Assumption 2: since [gmin, 1] almost surely, gmin ℓ(θ; H, t) ℓ(θ; H, t) ℓ(θ; H, t), (52) and taking expectations over the data distribution gives gmin L(θ) J(θ) L(θ). This proves Theorem 1. A.4 Proof of Theorem 2 (Residual update improvement) We next analyze the effect of the gated residual update = + λ R(H) on the expected loss, under the alignment condition in Eq. (15) of the main text. (53) (54) Proof of Theorem 2. Fix (θ, t) and consider ℓ(θ; H, t) as function of H. By Assumption 3, for any perturbation we have ℓ(θ; + H, t) ℓ(θ; H, t) + (cid:10)H ℓ(θ; H, t), H(cid:11) + LH 2 H2 . (55) We apply this with the gated residual update = λg R(H), so that = + H. Substituting gives ℓ(θ; H, t) ℓ(θ; H, t) + λg (cid:10)H ℓ(θ; H, t), R(H)(cid:11) + LH ℓ(θ; H, t) + λg (cid:10)H ℓ(θ; H, t), R(H)(cid:11) + LH 2 λg R(H)2 H2 2 λ2g2B2 , where we used R(H)F BRHF . 28 Taking expectations over the training distribution and using the gated descent condition (Eq. (15)), we obtain E(cid:2)g ℓ(θ; H, t), R(H)(cid:3) α0, E(cid:2)ℓ(θ; H, t)(cid:3) E(cid:2)ℓ(θ; H, t)(cid:3) + λ E(cid:2)g ℓ(θ; H, t), R(H)(cid:3) + LH B2 λ2E(cid:2)g2H2 (cid:3) E(cid:2)ℓ(θ; H, t)(cid:3) α0λ + LH B2 2 λ2E(cid:2)g2H2 (cid:3). Using 1 and Assumption 4, we may further bound E(cid:2)g2H2 (cid:3) E(cid:2)H2 (cid:3) 2 , so RC 2 E(cid:2)ℓ(θ; H, t)(cid:3) E(cid:2)ℓ(θ; H, t)(cid:3) α0λ + LH B2 2 We would like the linear improvement α0λ to dominate the quadratic term. sufficient condition is λ2. α0λ RC 2 LH B2 2 λ2 0 < λ λmax := 2α0 LH B2 RC 2 . For any 0 < λ λmax we then have E(cid:2)ℓ(θ; H, t)(cid:3) E(cid:2)ℓ(θ; H, t)(cid:3) α0 2 λ, (56) (57) (58) (59) (60) which is of the form stated in Theorem 2 with β = α0/2. This shows that there exists positive threshold (for example λmax) such that the gated residual update strictly decreases the expected loss for all 0 < λ λmax. Remark (On the constant λmax). The proof above makes explicit one convenient sufficient choice λmax = 2α0 LH RC 2 , (61) which depends on the data-dependent second-moment constant CH . In the main text, Eq. (16) only requires the existence of nontrivial interval (0, λmax] for which the residual update improves the loss; the specific closed-form expression reported there can be viewed as an equivalent parameterization after absorbing bounded data constants (such as 2 ) into α0 and using the fact that [gmin, 1]. Importantly, none of our qualitative conclusions depend on the exact numerical value of λmax: what matters is that there exists strictly positive range of λ where the discrepancy-guided residual update provably reduces the objective. A.5 Proof of Theorem 3 (Fixed-gate refinement convergence) We now prove the contraction result for the fixed-gate refinement scheme. Proof of Theorem 3. Consider the refinement map (Z) = α E(Z; g), (62) where the gate is fixed and E(; g) satisfies Assumption 5. For any Z1, Z2 we have (cid:13)T (Z1) (Z2)(cid:13) (cid:13) 2 (cid:13) = (cid:13) (cid:13)(Z1 Z2) α(cid:0)E(Z1; g) E(Z2; g)(cid:1)(cid:13) 2 (cid:13) = Z1 Z22 Z1 Z22 = (cid:0)1 2αµ + α2L2 2α (cid:10)Z1 Z2, E(Z1; g) E(Z2; g)(cid:11) + α2(cid:13) 2αµ Z1 Z22 (cid:1) Z1 Z22 , Z1 Z22 + α2L2 (cid:13)E(Z1; g) E(Z2; g)(cid:13) 2 (cid:13) 29 where we used strong monotonicity and Lipschitz continuity of E(; g). Define ρ2 := 1 2αµ + α2L2 E. If 0 < α < 2µ/L2 , then ρ2 < 1 and hence ρ (0, 1), and we obtain (cid:13)T (Z1) (Z2)(cid:13) (cid:13) (cid:13)F ρ Z1 Z2F . Thus is contraction mapping with rate ρ. By Banachs fixed-point theorem, admits unique fixed point and, for any initialization (0), (cid:13)T k(Z (0)) (cid:13) (cid:13)Z (0) (cid:13) (cid:13)Z (k) (cid:13) (cid:13) (cid:13)F (cid:13)F ρk (cid:13) = (cid:13) (cid:13)F . (63) (64) (65) This is exactly the convergence guarantee stated in Theorem 3."
        }
    ],
    "affiliations": [
        "BeingBeyond",
        "Peking University",
        "Renmin University of China"
    ]
}