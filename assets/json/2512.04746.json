{
    "paper_title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
    "authors": [
        "Wenhua Cheng",
        "Weiwei Zhang",
        "Heng Guo",
        "Haihao Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round."
        },
        {
            "title": "Start",
            "content": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs Wenhua Cheng* and Weiwei Zhang and Heng Guo and Haihao Shen Intel 5 2 0 2 4 ] . [ 1 6 4 7 4 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layerwise bit allocation, and (2) lightweight pretuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving productiongrade performance with 1% variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github. com/intel/auto-round."
        },
        {
            "title": "Introduction",
            "content": "The advent of Large Language Models (LLMs) such as GPT (Brown et al., 2020), LLaMA (Touvron et al., 2023a, 2024), Qwen (Bai et al., 2023; Yang et al., 2025) and DeepSeek (Liu et al., 2024) has marked major shift in modern artificial intelligence. As parameter counts scale from billions to hundreds of billions, these models have demonstrated unprecedented capabilities in reasoning, coding, multimodal understanding, and autonomous agent behaviors. However, such scaling comes at the cost of dramatically increased memory consumption, bandwidth pressure, and inference latency, posing significant barriers for real-world deployment, especially on resource-constrained environments such as consumer GPUs, CPUs, or edge devices. *Correspondence: wenhua.cheng@intel.com These authors contributed equally Figure 1: Average accuracy of pure 2-bit (W2A16) models on Llama 2/3 70B. See detailed results in Table 1. To alleviate these limitations, post-training quantization (PTQ) has emerged as one of the most practical approaches because it avoids costly retraining and can be applied to wide range of pretrained LLMs (Xiao et al., 2023; Frantar et al., 2022; Lin et al., 2024; Cheng et al., 2024). By compressing weights and activations to low-bit representations, PTQ can yield substantial reductions in memory footprint and hardware cost. In particular, the push toward extreme low-bit quantization, for example, sub-4-bit weight-only quantization and 4-bit activation quantization, has become crucial enabler for democratizing LLM deployment. However, achieving this while preserving accuracy remains highly challenging due to increased quantization error, layer sensitivity imbalance, and the lack of hardware-friendly data types. Recent research has moved beyond conventional W4A16 and W8A8 schemes (where WA denotes weight and activation bit-widths) toward more expressive low-bit data types. For example, MXFP4 (Alliance, 2023) and NVFP4 (NVIDIA, 2024) are newly introduced floating-point variants optimized for modern accelerators, demonstrating that well-designed data types can maintain reasonable accuracy even under aggressive activation quantization. In parallel, weight-only quantization continues to push the limits of bit efficiency. In particular, BitNet (Wang et al., 2025; Wu et al., 2025) explores quantization down to approximately 1.58 bits, showing that extremely low-bit representations can still be learned if specialized training or distillation procedures are used. However, these methods require training from scratch or costly post-training distillation, making them less suitable for scenarios where fast, scalable, and hardwareefficient PTQ is desired. natural direction for overcoming the limitations mentioned above is Adaptive Bit-Width Quantization (ABQ). ABQ mitigates the shortcomings of uniform-bit quantization by allocating higher precision to more sensitive layers while aggressively compressing more robust ones (Zhou et al., 2018; Dong et al., 2019; Wang et al., 2019; You et al., 2024). The central challenge therefore becomes the design of an accurate and efficient layer sensitivity metric to guide bit allocation. Early methods estimate sensitivity using the Hessian-related matrix (Dong et al., 2019; You et al., 2024), or rely on layer-wise/model-wise output distortion measured on small calibration dataset (Hubara et al., 2020). However, these heuristics often exhibit imperfect correlation when applied layer-wise, and modelwise approaches typically require slow calibration, especially for large language models (LLMs). In contrast, learning-based or reinforcement learning based approaches automate precision assignment (Lou et al., 2019; Wang et al., 2019), but at the cost of substantial compute overhead, contradicting the primary objective of efficient PTQ. To address these challenges, we introduce SignRoundV2, an enhanced post-training quantization framework building upon the original SignRound (Cheng et al., 2024). Our method is driven by two key insights. First, an effective sensitivity metric must capture both the local parameter distortion caused by quantization and its global impact on task loss. We design simple yet highly effective metric that integrates gradient information with quantization-induced parameter deviation, yielding far more reliable signal for ABQ than existing proxies. Then given target average bit budget, SignRoundV2 employs dynamic programming to convert layer-wise sensitivity scores into an optimal bit configuration. Second, we observe that the initialization of quantization parameters especially scale is critical for extremely low-bit stability, consistent with findings from recent PTQ literature (Lin et al., 2024; Ma et al., 2024). Accordingly, inspired by the importance matrix in llama.cpp (Gerganov, 2023), we introduce lightweight pre-tuning search that rapidly identifies high-quality initialization values before the main tuning stage, substantially improving recoverable accuracy. Figure 1 compares our method with state-of-the-art baselines, showing that it achieves similar performance at significantly lower cost and avoids the issues associated with Quantization Aware Training (QAT), which are discussed in Section 2. In summary, our contributions are three-fold. We propose simple and efficient sensitivity metric that integrates gradient information with quantization-induced parameter deviations. This metric captures both local parameter distortions and their global impact on task loss, providing reliable guide for adaptive bit-width quantization. We incorporate lightweight pre-tuning search for quantization scales prior to the main tuning stage, enhancing both stability and final model accuracy under extremely low-bit settings. Experimental results demonstrate that this strategy achieves performance on par with recent efficient QAT methods. We develop fast, adaptive mixed-precision quantization method and show that it achieves competitive performance compared to existing QAT approaches, while incurring substantially lower quantization cost."
        },
        {
            "title": "2 Related Work",
            "content": "Quantization-Aware Training (QAT). QAT integrates learnable quantizers directly into the training loop, minimizing the task loss and fine-tuning additional parameters beyond the quantizers introduced by low-precision operations, as described in EfficientQAT (Chen et al., 2025), LLM-QAT (Liu et al., 2023b), DL-QAT (Ke et al., 2024), and BitDistiller (Du et al., 2024). Recent work(Chee et al., 2023; Tseng et al., 2024; Egiazarian et al., 2024) has revisited vector quantization for extremely lowbit settings, but these methods still rely on QAT to achieve high accuracy. Despite their effectiveness, QAT methods suffer from several practical limitations. First, they optimize the task loss or the unquantized parameters of the model, which can lead to catastrophic forgetting (Luo et al., 2025; Kalajdzievski, 2024), particularly in domains not covered by fine-tuning. Second, QAT requires careful hyperparameter tuning, and its actual quantization cost is often much higher than what is reported, typically far exceeding that of post-training quantization. Third, QAT generally requires substantially more data than PTQ to mitigate overfitting, further increasing the overall resource demands. Post-Training Quantization (PTQ). Posttraining quantization (PTQ) avoids additional fine-tuning of non-quantized parameters and task loss, and therefore offers simple and resourceefficient pipeline for compressing LLMs (Nagel et al., 2019; Liu et al., 2021; Frantar and Alistarh, 2022; Yao et al., 2021; Xiao et al., 2023; Wei et al., 2023; Dettmers et al., 2022). Because LLM decoding is typically memory-bound, weight-only PTQ has become the dominant approach (Frantar et al., 2022; Lin et al., 2024; Cheng et al., 2023; Yao et al., 2024; Badri and Shaji, 2023; Kim et al., 2023; Cheng et al., 2024) in LLM inference. However, existing weight-only methods still suffer from significant accuracy drop at extremely low bit-widths (e.g., 2-bit)."
        },
        {
            "title": "Precision",
            "content": "Mixed Quantization. Mixedprecision quantization assigns heterogeneous bit-widths across layers or channels to better align with their sensitivity. Early approaches include Hessian-based methods such as HAWQ (Dong et al., 2019) and BAQ (Zhang et al., 2025), as well as reinforcement-learningbased schemes such as HAQ (Wang et al., 2019). However, these methods are prohibitively expensive for multi-billionparameter LLMs due to second-order matrix computations or extensive policy evaluations. More recent systems, such as MicroMix (Liu et al., 2025), apply FP4/FP6/FP8 at the channel level via MXFP microscaling kernels, but their reliance on specialized hardware limits the portability. In practice, Llama.cpp (Gerganov, 2023) adopts more lightweight strategy that relies on heuristic rules to assign mixed precisions for each model and quantization scheme. Recent works (Duanmu et al., 2025; Tao et al., 2025; Chitty-Venkata et al., 2025) focus on mixed-precision settings for mixture-of-expert (MoE) models, which have become particularly popular for large-scale models. tized weights. AdaRound (Nagel et al., 2020) formulates weight rounding as per-weight binary optimization problem, where second-order Taylor expansion of the task loss approximates quantization-induced perturbations, and layerwise local reconstruction loss is minimized via continuous relaxation. This formulation has inspired subsequent works such as BRECQ (Li et al., 2021), SignRound (Cheng et al., 2024). FlexRound (Lee et al., 2023) increases rounding flexibility through element-wise scaling, albeit at the cost of introducing numerous hyperparameters. OscillationFree training (Liu et al., 2023a) highlights instability arising from learnable rounding parameters. Activation-dependent rounding in AQuant (Li et al., 2022) reduces activation error but is not applicable to weight-only inference. SignRound (Cheng et al., 2024) jointly optimizes rounding and weight clipping using sign-based gradient descent, achieving substantial improvements, particularly at extremely low bit-widths such as 2 bits."
        },
        {
            "title": "3 Methodology",
            "content": "This section presents our methodology. We begin in Section 3.1 by introducing the sensitivity metric, DeltaLoss, which guides the mixed-bit assignment. Section 3.2 then leverages DeltaLoss together with dynamic programming to generate adaptive bit-widths for each layer. In Section 3.3, we describe our pre-tuning search for quantization parameters. Finally, Section 3.4 lists all major hyperparameters and details practical trick for loss calculation. Before introducing our method, we first provide brief overview of quantization and SignRoundV1 (Cheng et al., 2024). The following operation can be used to quantize and dequantize (qdq) the weights W: (cid:17) (cid:109) (cid:16)(cid:106)W , n, qdq(W) = clip , n, N, (1) where the rounding operation is typically performed using the Round-to-Nearest (RTN) method. The scale factor is defined as: = max(W) min(W) 2bit 1 , (2) and for simplicity, we ignore the zero point. Rounding and Optimized Quantizer Search. Rounding critically affects the quality of quanSignRoundV1 (Cheng et al., 2024) introduces three trainable parameters v, α and β, to reduce the quantization error. The parameter enhances the rounding operation In practice, we found that using the absolute value function yields more robust results, so we redefine the metric as (cid:109) (cid:17) (cid:16)(cid:106)W +v qdq(W) = sclip , n, N, (3) while α and β refine the scale and zero-point through , n, s = max(W) α min(W) β 2bit 1 (4)"
        },
        {
            "title": "3.1 DeltaLoss Sensitivity Metric",
            "content": "Prior work (Dong et al., 2019; You et al., 2024) largely ignores first-order gradient information and instead relies on second-order approximations, such as the Hessian or Fisher information matrix, to estimate sensitivity, under the assumption that gradients are typically near zero. However, this assumption breaks down when quantization induces large loss. Moreover, although second-order metrics capture local curvature, they share fundamental limitation with purely gradient-based methods: both fail to reflect the actual impact of quantization on the loss, providing only relative trend rather than reliable measure of true loss. To address this limitation, we employ firstorder Taylor expansion to directly estimate the loss change induced by quantization, using the commonly adopted cross-entropy loss. Specifically, for given layer with weight tensor and activation tensor A, the loss variation caused by quantizing the layer can be approximated using the first-order Taylor expansion: gaq (Af Aq) + gwq (Wf Wq) (5) where: gaq = Aq represents the gradient of the loss with respect to the qdq activation tensor gwq = Wq represents the gradient of the loss with respect to the qdq weight tensor Af and Wf denote the full-precision activation and weight values, respectively Aq and Wq denote the qdq activation and weight values, respectively denotes the Hadamard product (cid:12) (cid:12)(gaq (Af Aq)(cid:12) (cid:12) + (cid:12) (cid:12)gwq (Wf Wq)(cid:12) (cid:12) (6) This formulation effectively combines gradient information and parameter distortion to yield more accurate and holistic sensitivity estimate for each layer. To reduce memory consumption, activation quantization has been identified as the dominant contributor to quantization loss as shown in (Xiao et al., 2023; Wei et al., 2023). Therefore, within the context of joint weightactivation quantization, we primarily focus on activation-induced distortion and omit the weight quantization term in our sensitivity formulation. (cid:12) (cid:12)gaq (Af Aq)(cid:12) (cid:12) (7) Figure 2 visualizes the layer-wise sensitivity measured by DeltaLoss. It is evident that sensitivity varies across different data types, making it prohibitively labor-intensive to rely on heuristic rules to manually set quantization parameters for each scheme, as done in llama.cpp (Gerganov, 2023). Moreover, certain layers within block, such as down_proj, exhibit higher sensitivity, suggesting that traditional block-wise fallback strategies may be suboptimal."
        },
        {
            "title": "3.2 Layer-wise Bit Allocation",
            "content": "We formulate layer-wise bit allocation as discrete optimization problem. Let denote the total number of layers, and let = {b1, . . . , bK} be the set of allowed bit-widths. For each layer {1, . . . , n} and bit-width B, we introduce binary variable Ii,b {0, 1}, where Ii,b = 1 indicates that layer is assigned bits b. For simplicity, we assume identical bit-widths for both weights and activations, although this formulation can be extended to accommodate different bit-widths or data types across layers. Let Li(b) denote the delta loss when quantizing layer to bits. Given target average bitwidth , the optimization problem can be written as: (a) W2A16 layer-wise deltaloss sensitivity (b) MXFP4 layer-wise deltaloss sensitivity Figure 2: Layer-wise DeltaLoss sensitivity of Llama-3.1-8B-Instruct under W2A16 and MXFP4. min Ii,b s.t. (cid:88) (cid:88) Li(b) Ii,b i=1 (cid:88) bB (cid:88) bB Ii,b = 1, = 1, . . . , n, (cid:88) Ii,b Pi (cid:88) Pi, i=1 bB Ii,b {0, 1}, = 1, . . . , n, B. (8) This optimization problem can be solved using dynamic programming (Bellman, 1966) or as an Integer Linear Program (Wolsey, 2020; Bertsimas and Tsitsiklis, 1997), both of which are wellestablished methods. We omit the details here and refer the reader to the existing literature."
        },
        {
            "title": "3.3 Quantization Parameters Initialization",
            "content": "In SignRoundV1 (Cheng et al., 2024), all quantization parameters are initialized with trivial values, i.e., weight clipping is set to 1.0 and the rounding perturbation is 0. better initialization could potentially improve performance, similar to the effect of careful initialization in training neural networks (He et al., 2015; Glorot and Bengio, 2010). Although directly initializing the rounding values is challenging, prior works (Lin et al., 2024; Badri and Shaji, 2023; Gerganov, 2023) have proposed training-free methods to search for weight clipping by aligning layer outputs or optimizing certain objectives. While effective, these approaches require layer-wise forward passes, which can be slow, or rely on floating-point zero points for better accuracy. Inspired primarily by the importance matrix in llama.cpp (Gerganov, 2023), we adopt simple variant to initialize the scale. The search process is formulated as min sS"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:0)(WF Wq) max( A)2(cid:1)2 (9) where denotes predefined set of candidate scales, and max( A) represents the input channelwise maximum absolute values, calibrated from set of samples. For symmetric quantization, which is the default setting used in this work, the set S(s0, ..., sc) is constructed from the following candidates si = max(W) 2bit1 + ϵi . (10) with ϵi sampled from ϵ range(0.9, 0.9, step = 0.01) (11) Following SignRoundV1 (Cheng et al., 2024), the best scale sinit that minimizes Eq. 9 is further refined by learnable parameter α, which we constrain to the range [0.5, 1.5] in our experiments. = sinit α (12)"
        },
        {
            "title": "3.4 Hyperparameters and Tricks",
            "content": "For DeltaLoss, we use only 16 calibration samples with sequence length of 256. For tuning, we follow the setup in SignRoundV1 (Cheng et al., 2024). Each transformer block is optimized for 200 steps using sign gradient descent, with learning rate of 1/steps and batch size of 8. The sequence length is fixed at 2048. To reduce quantization cost, we lower the default number of calibration samples from 512 to 128. We additionally provide higher-cost recipe for improved accuracy, denoted as Ours*, where the number of steps is increased from 200 to 500, the learning rate is set to 2.0/steps and the number of calibration samples is restored to 512. Automatic mixed precision (AMP) is applied throughout to improve tuning efficiency. To enhance tuning stability, we exclude the top-k largest losses in batch when computing the mean squared error between the quantized block output and the full-precision block, where is set to 0.1% of the total number of elements. Let TopK(L, k) denote the set of the largest losses in L. The effective loss used for optimization is defined as follows, where denotes the exclusion operation: Leff = (cid:88) Li. (13) LiLTopK(L,k)"
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "follow the LM-EVAL-HARNESS framework (Gao et al., 2023). The benchmark suite includes ARCChallenge (Clark et al., 2018), ARC-Easy (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), LAMBADA (Paperno et al., 2016), MMLU (Hendrycks et al., 2020), OpenBookQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), TruthfulQA (Lin et al., 2022), and WinoGrande (Sakaguchi et al., 2021). Quantization Configurations. Following SignRoundV1 (Cheng et al., 2024), we use The Pile (Gao et al., 2020) as the calibration dataset and adopt symmetric quantization. All experiments are conducted on NVIDIA A100 80GB GPUs. For simplicity, we omit the bit costs associated with scales and zero points. For MXFP4 and MXFP8, we follow the standard definitions in (Alliance, 2023)."
        },
        {
            "title": "4.2 Comparison of Results at W2A16/W4A16",
            "content": "Table 1 presents the average accuracies across 5 tasks (ARC-Challenge, ARC-Easy, HellaSwag, PIQA and WinoGrande) for W2A16 with and without mixed-precision quantization on various LLaMA models. At pure W2A16 setting, our method consistently outperforms existing PTQ approaches such as GPTQ (Frantar et al., 2022), AWQ (Lin et al., 2024), OmniQuant (Shao et al., 2023), and SignRoundV1 (Cheng et al., 2024) by substantial margin. Compared to high cost methods like AQLM (Egiazarian et al., 2024), QuIP# (Tseng et al., 2024), and EfficientQAT (Chen et al., 2025), our approach achieves comparable performance on large models, while slightly trailing on smaller models. At slightly higher bit widths (e.g., 2.5 bits), with the support of DeltaLoss, our method continues to deliver strong accuracy, demonstrating the effectiveness of adaptive layer-wise quantization. Overall, these results confirm that our approach reliably preserves model quality under extreme low-bit mixed-precision settings. Detailed accuracy results for each task are provided in Appendix A."
        },
        {
            "title": "4.3 Comparison of Results at MXFP4/8",
            "content": "Models and Benchmarks. We evaluate SignRoundV2 on two major LLM families, LLaMA (Touvron et al., 2023b; Grattafiori et al., 2024) and Qwen (Bai et al., 2023; Yang et al., 2025), under both low-bit and mixed-precision quantization settings across standard benchmarks. All evaluations Table 2 reports the average accuracies and recovery rates across 10 tasks (all tasks listed in Section 4.1) for MXFP4/8 mixed-precision quantization. Since few prior works report results for standard MXFP4, we primarily compare against our own baselines. The results show that our method consistently outTable 1: Average accuracies across five tasks (Section 4.2) using W2A16 at 2 bits and mixed W2A16 and W4A16G128 at 2.5 bits. \"G\" denotes the group size, and \"Avg. Bits\" denotes the average per-weight bit-width. Ours* indicates our improved recipe (Section 3.4). - denotes the missing data from the original work. Avg. bits Method Group Size Llama2-7B Llama2-13B Llama2-70B Llama3-8B Llama3-70B 16bits AQLM AQLM QuIP# EfficientQAT EfficientQAT 2 2.5 GPTQ AWQ OmniQ SRV1 Ours Ours* Ours* Ours Ours* Ours Ours* - 2x8 1x16 - 128 64 128 128 128 128 128 128 64 128 128 64 64 64.66 57.61 61.85 60.61 59.50 60. 41.56 34.74 46.98 54.50 57.88 58.67 59.04 59.96 60.44 60.08 60.28 67.44 62.22 64.95 64.44 63.88 64.48 48.29 35.99 53.56 60.72 61.88 62.34 62.81 63.65 63.73 64.09 64. 72.71 69.85 70.84 70.91 68.93 69.48 34.38 35.49 54.87 67.70 68.39 68.82 69.30 70.60 70.04 70.25 70.60 68.64 - 64.10 - 59.37 60. - - 52.66 55.25 59.02 59.97 60.38 62.76 62.60 63.67 63.56 75.28 - 70.10 - 67.57 67.89 - - 60.06 64.76 69.02 70.16 70.93 72.37 72.68 71.97 72. Table 2: The average accuracies and recovery rates (%) across 10 tasks(detailed in Section 4.3) with MXFP4 at 4bits and mixed MXFP4/8 for others. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. The \"I\" in the model name is short for \"Instruct\". Avg. Bits Method Llama3.1-8B-I Llama3.1-70B-I Qwen2.5-7B-I Qwen3-8B Qwen3-32B 16 4.5 5 6 16bits RTN SRV1 Ours DL Ours DL Ours DL Ours 64.16 70.00 65.67 63. 67.00 58.31 (90.88%) 60.72 (94.64%) 61.34 (95.59%) 68.71 (98.16%) 69.01 (98.60%) 69.32 (99.04%) 60.62 (92.32%) 64.06 (97.55%) 63.37 (96.50%) 58.54 (92.57%) 60.25 (95.28%) 61.89 (97.87%) 65.07 (97.12%) 66.92 (99.88%) 66.86 (99.79%) 60.64 (94.50%) 62.33 (97.14%) 69.78 (99.70%) 69.96 (99.95%) 63.13 (96.13%) 64.51 (98.24%) 59.81 (94.58%) 62.28 (98.49%) 66.05 (98.58%) 66.89 (99.83%) 62.57 (97.51%) 63.19 (98.49%) 70.10 (100.15%) 70.31 (100.45%) 64.04 (97.52%) 65.04 (99.03%) 61.45 (97.18%) 62.54 (98.89%) 66.00 (98.51%) 67.17 (100.25%) 63.64 (99.18%) 64.12 (99.93%) 70.56 (100.80%) 70.50 (100.71%) 65.45 (99.67%) 65.30 (99.43%) 62.00 (98.04%) 62.19 (98.34%) 66.61 (99.42%) 67.21 (100.32%) performs RTN and SignRoundV1 (SRV1) at low average bit widths (4-5 bits), achieving higher accuracy and recovery rates across nearly all evaluated models. At 4 bits, our approach improves recovery by 1-3% compared to SRV1. At slightly higher bit widths (5-6 bits), both DeltaLoss-only (DL) and our method approach full-precision accuracy, but our method achieves the highest recovery in most scenarios, reaching near or above 99% across all models. Overall, these results demonstrate that our approach effectively preserves model performance under extreme low-bit mixed-precision quantization while minimizing accuracy loss. Detailed accuracy results for each task are provided in Appendix A."
        },
        {
            "title": "4.4 Comparison of Mixed Bits Recipes",
            "content": "The results in Tables 3 and 4 demonstrate that our method consistently outperforms simple head-layer and tail-layer heuristics across range of precision budgets and model architectures. For MXFP quantization (4.5, 5, and 6 bits), the DL strategy achieves the highest average accuracy for all three models without requiring any tuning, showing clear improvements over allocating 8-bit precision only to the head (near the LM head) or tail layers (near the embedding). At lower precision (average 3 bits using W2G128/W4G128), the performance Table 3: Comparison with head-layer (Head)/ tail-layer (Tail) heuristics at 4.5-bit and 5-bit MXFP with only DL, without tuning. The average accuracies are across 10 tasks (detailed in Section 4.3). The \"I\" in the model name is short for \"Instruct\". Avg. bits Method Llama3.1-8B-I Qwen2.5-7B-I Qwen3-8B 4.5 6 Head 8-bit Tail 8-bit DL Head 8-bit Tail 8-bit DL Head 8-bit Tail 8-bit DL 59.18 58.93 60.64 60.82 60.12 62. 59.88 62.65 63.64 60.63 61.38 63.13 61.75 61.35 64.04 62.22 63.39 65.45 58.92 59.26 59.81 59.43 59.65 61. 60.54 61.53 62.00 Table 4: The average accuracies across 10 tasks(Section 4.3) at an average of 3 bits (W2G128 / W4G128). The \"I\" in the model name is short for \"Instruct\". Avg. bits = 3 Llama3.1-8B-I Qwen2.5-7B-I Qwen3-8B Head 4-bit Tail 4-bit Ours 31.98 60.58 61.48 32.70 37.98 40. 31.96 45.36 48.62 gap becomes even more obvious: both headand tail-focused heuristics degrade substantially, while our method remains robust and delivers the best accuracy across all evaluated models."
        },
        {
            "title": "4.5 Ablation Study of Initialization",
            "content": "We evaluate the impact of our initialization strategy on Qwen3-8B and Llama3.1-8B-Instruct under the W2A16G64 setting on SignRoundV1 with or without initialization. As shown in Table 5, enabling initialization consistently improves accuracy across all five tasks. Overall, the results demonstrate that proper pre-tuning initialization significantly stabilizes optimization and leads to substantial accuracy improvements for both models. such as EfficientQAT (Chen et al., 2025), QuIP# (Tseng et al., 2024), and AQLM (Egiazarian et al., 2024) require tens to hundreds of hours to converge. Besides, we argue that QAT requires careful hyperparameter tuning, and its actual quantization cost is often much higher than what is reported. In contrast, SignRoundV1 completes in only 2.2 hours. Our method maintains similarly low computational cost, requiring 2.5 hours, while the enhanced version (Ours*) takes 6 hours due to additional tuning samples and steps. Overall, our approach achieves competitive accuracy with substantially lower time overhead compared with prior methods. Table 6: Time Cost Comparison for Llama2-70B Method One A100-80GB? GPU hours SignRoundV1 EfficientQAT QuIP# AQLM Ours Ours* 2.2 41 270 336 2.5 6 Table 7 summarizes the VRAM and runtime overhead of computing DeltaLoss for weight-only quantization (e.g., W2A16), with MXFP4 exhibiting comparable cost. The results show that DeltaLoss remains computationally practical even for 70B-scale models. Table 7: VRAM and Time Cost of DeltaLoss For W2A16 Model VRAM Cost (GB) Time Cost (s) Qwen3-8B Qwen3-32B Llama-2-70B 15 30 40 60 len of options 180 len of options 420 len of options Table 5: Ablation study of pre-tuning initialization for Qwen3-8B and Llama3.1-8B-Instruct at W2A16G64."
        },
        {
            "title": "5 Conclusion",
            "content": "Model Method Arc-c hella. lamb. mmlu wino. Qwen3-8B W/O init 34.90 44.87 47.18 54.09 63.85 With init 43.69 45.12 54.61 56.12 66.22 Llama3.1-8B-I W/O init 36.43 46.20 52.57 48.05 65.67 With init 36.43 47.32 60.41 50.11 67."
        },
        {
            "title": "4.6 Quantization Cost",
            "content": "Table 6 compares the end-to-end quantization time across several representative methods. Traditional QAT-based and vector quantization approaches We introduced SignRoundV2, practical framework for extreme low-bit post-training quantization of LLMs. By combining gradient-informed sensitivity metric with lightweight pre-tuning search, SignRoundV2 enables stable and accurate quantization even at aggressive bit widths. Experiments show that it delivers production-grade accuracy at 45 bits and strong performance at 2 bits. Our results demonstrate that effective PTQ can significantly reduce the deployment cost of LLMs while preserving model quality."
        },
        {
            "title": "6 Limitations",
            "content": "Despite its strong performance, SignRoundV2 has several limitations. First, without mixed precision, it still exhibits noticeable gap from the full-precision model at extremely low bit widths, particularly for smaller models. Second, the bit configuration is fixed before tuning, without considering how much quantization error could be mitigated during the tuning process. Third, the method requires gradient computation, which limits its applicability in frameworks such as ONNX that do not support gradient-based optimization."
        },
        {
            "title": "Ethics Statement",
            "content": "Our research aims to advance knowledge in LLM quantization. SignRoundV2 utilizes open-source models and publicly available datasets, and is not tied to particular applications, requiring only minimal fine-tuning steps on the original models. This ensures that the technical details of our method carry no potential ethical implications. We acknowledge the contributions of the creators and maintainers of these resources and provide citations to the original sources."
        },
        {
            "title": "References",
            "content": "Askell, and 1 others. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. 2023. Quip: 2-bit quantization of large language models with guarantees. Advances in Neural Information Processing Systems, 36:4396 4429. Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. 2025. Efficientqat: Efficient quantization-aware training for large language models. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10081 10100. Wenhua Cheng, Yiyang Cai, Kaokao Lv, and Haihao Shen. 2023. Teq: Trainable equivalent transformation for quantization of llms. arXiv preprint arXiv:2310.10944. Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Lv Kaokao, and Yi Liu. 2024. Optimize weight rounding via signed gradient descent for the quantization of llms. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1133211350. Krishna Teja Chitty-Venkata, Jie Ye, and Murali Emani. 2025. Mopeq: Mixture of mixed precision quantized experts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40234032. specification Open Compute Project Foundation MX AlOcp microscaling formats comversion foundation technical specification. liance. 2023. (mx) pute project https://www.opencompute.org/documents/ ocp-microscaling-formats-mx-v1-0-spec-final-pdf. [Accessed 17-11-2025]. open 1.0. Hicham Badri and Appu Shaji. 2023. Half-quadratic quantization of large machine learning models. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, and 1 others. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 29242936. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Richard Bellman. 1966. Dynamic programming. science, 153(3731):3437. Dimitris Bertsimas and John Tsitsiklis. 1997. Introduction to Linear Optimization. Athena Scientific. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:30318 30332. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, and 1 others. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 74327439. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Zhen Dong, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer. 2019. Hawq: Hessian aware quantization of neural networks with mixedprecision. In Proceedings of the IEEE/CVF international conference on computer vision, pages 293 302. Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, and Ningyi Xu. 2024. Bitdistiller: Unleashing the potential of sub-4-bit llms via selfdistillation. arXiv preprint arXiv:2402.10631. Haojie Duanmu, Xiuhong Li, Zhihang Yuan, Size Zheng, Jiangfei Duan, Xingcheng Zhang, and Dahua Lin. 2025. Mxmoe: Mixed-precision quantization for moe with accuracy and performance co-design. arXiv preprint arXiv:2505.05799. Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large language models via additive quantization. arXiv preprint arXiv:2401.06118. Elias Frantar and Dan Alistarh. 2022. Optimal brain compression: framework for accurate post-training quantization and pruning. Advances in Neural Information Processing Systems, 35:44754488. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, and 1 others. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, and 5 others. 2023. framework for few-shot language model evaluation. Georgi Gerganov. 2023. llama.cpp: LLM inference in https://github.com/ggml-org/llama. C/C++. cpp. Xavier Glorot and Yoshua Bengio. 2010. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249256. JMLR Workshop and Conference Proceedings. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, and 1 others. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 10261034. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. In International Conference on Learning Representations. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry. 2020. Improving post training neural quantization: Layer-wise calibration and integer programming. arXiv preprint arXiv:2006.10518. Damjan Kalajdzievski. 2024. Scaling laws for forgetting when fine-tuning large language models. arXiv preprint arXiv:2401.05605. Wenjing Ke, Zhe Li, Dong Li, Lu Tian, and Emad Barsoum. 2024. Dl-qat: Weight-decomposed low-rank quantization-aware training for large language models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 113119. Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael MaSqueezellm: honey, and Kurt Keutzer. 2023. arXiv preprint Dense-and-sparse quantization. arXiv:2306.07629. Jung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. 2023. Flexround: Learnable rounding based on element-wise division for post-training quantization. arXiv preprint arXiv:2306.00317. Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu. 2021. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv preprint arXiv:2102.05426. Zhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jingwen Leng, and Minyi Guo. 2022. Efficient activation quantization via adaptive rounding border for post-training quantization. arXiv preprint arXiv:2208.11945. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2024. Awq: Activationaware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978. Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Truthfulqa: Measuring how models mimic human falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 32143252. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, and 1 others. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Shih-Yang Liu, Zechun Liu, and Kwang-Ting Cheng. 2023a. Oscillation-free quantization for low-bit vision transformers. In International Conference on Machine Learning, pages 2181321824. PMLR. Wenyuan Liu, Haoqian Meng, Yilun Luo, Peng Zhang, and Xindian Ma. 2025. Micromix: Efficient mixed-precision quantization with microscaling formats for large language models. arXiv preprint arXiv:2508.02343. Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023b. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888. Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. 2021. Post-training quantization for vision transformer. Advances in Neural Information Processing Systems, 34:2809228103. Qian Lou, Feng Guo, Lantao Liu, Minje Kim, and Autoq: Automated kernelLei Jiang. 2019. wise neural network quantization. arXiv preprint arXiv:1902.05690. Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2025. An empirical study of catastrophic forgetting in large language models during continual fine-tuning. IEEE Transactions on Audio, Speech and Language Processing. Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Lifeng Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 1(4). Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. 2020. Up or down? adaptive rounding for post-training quantization. In International Conference on Machine Learning, pages 71977206. PMLR. Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. 2019. Data-free quantization through weight equalization and bias correction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13251334. NVIDIA. 2024. Nvidia blackwell architecture techhttps://resources.nvidia.com/ nical brief. en-us-blackwell-architecture. [Accessed 1711-2025]. Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. 2016. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15251534. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2023. Omniquant: Omnidirectionally calibrated quantization for large language models. In The Twelfth International Conference on Learning Representations. Wei Tao, Haocheng Lu, Xiaoyang Qu, Bin Zhang, Kai Lu, Jiguang Wan, and Jianzong Wang. 2025. Moqae: Mixed-precision quantization for long-context llm inference via mixture of quantization-aware experts. arXiv preprint arXiv:2506.07533. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, and 1 others. 2023a. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, and 1 others. 2024. Meta llama 3: The most capable openly available llm to date. Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De Sa. 2024. Quip#: Even better llm quantization with hadamard incoherence and lattice codebooks. arXiv preprint arXiv:2402.04396. Hongyu Wang, Shuming Ma, Lingxiao Ma, Lei Wang, Wenhui Wang, Li Dong, Shaohan Huang, Huaijie Wang, Jilong Xue, Ruiping Wang, and 1 others. 2025. Bitnet: 1-bit pre-training for large language models. Journal of Machine Learning Research, 26(125):1 29. Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. 2019. Haq: Hardware-aware automated quanIn Proceedings of tization with mixed precision. the IEEE/CVF conference on computer vision and pattern recognition, pages 86128620. Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xianglong Liu. 2023. Outlier suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145. Laurence Wolsey. 2020. Integer programming. John Wiley & Sons. Xun Wu, Shaohan Huang, Wenhui Wang, Ting Song, Li Dong, Yan Xia, and Furu Wei. 2025. Bitnet distillation. arXiv preprint arXiv:2510.13998. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pages 3808738099. PMLR. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, and 1 others. arXiv preprint 2025. Qwen3 technical report. arXiv:2505.09388. Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael Mahoney, and 1 others. 2021. Hawq-v3: Dyadic neural network quantization. In International Conference on Machine Learning, pages 1187511886. PMLR. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. 2024. Exploring post-training quantization in llms from comprehensive study to low rank compensation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1937719385. Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, and Yingyan (Celine) Lin. 2024. Shiftaddllm: Accelerating pretrained llms via posttraining multiplication-less reparameterization. In Advances in Neural Information Processing Systems, volume 37, pages 2482224848. Curran Associates, Inc. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. Chao Zhang, Li Wang, Samson Lasaulce, and Merouane Debbah. 2025. Baq: Efficient bit allocation quantization for large language models. arXiv preprint arXiv:2506.05664. Yiren Zhou, Seyed-Mohsen Moosavi-Dezfooli, NgaiMan Cheung, and Pascal Frossard. 2018. Adaptive quantization for deep neural network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32."
        },
        {
            "title": "A Detailed Data",
            "content": "This appendix provides detailed accuracy results for each task. Table 8: The detailed accuracies of 5 tasks for Llama2 models with INT2/4 mixed precision. \"Avg. Bits\" indicates average bit-width. Model Method Avg. Bits Group Size ARC-c ARC-e Hella. Piqa Wino. Avg. Llama-2-7B Llama-2-13B Llama-2-70B 16Bits AQLM AQLM QuIP# EfficientQAT EfficientQAT GPTQ AWQ OmniQ SignRoundV1 Ours Ours Ours* Ours* Ours Ours Ours* Ours* 16Bits AQLM AQLM QuIP# EfficientQAT EfficientQAT GPTQ AWQ OmniQ SignRoundV1 Ours Ours Ours* Ours* Ours Ours Ours* Ours* 16Bits AQLM AQLM QuIP# EfficientQAT EfficientQAT GPTQ AWQ OmniQ SignRoundV1 Ours Ours Ours* Ours* Ours Ours Ours* Ours* 16 2 2 2 2 2 2 2 2 2 2 2 2 2 2.5 2.5 2.5 2.5 16 2 2 2 2 2 2 2 2 2 2 2 2 2 2.5 2.5 2.5 2. 16 2 2 2 2 2 2 2 2 2 2 2 2 2 2.5 2.5 2.5 2.5 - 2x8 1x16 - 128 64 128 128 128 128 128 64 128 64 128 64 128 64 - 2x8 1x16 - 128 64 128 128 128 128 128 64 128 64 128 64 128 - 2x8 1x16 - 128 64 128 128 128 128 128 64 128 64 128 64 128 64 42.83 32.85 39.68 37.88 36.52 36.86 21.25 21.08 23.46 32.25 33.53 32.93 34.64 35.32 35.41 35.92 36.01 36.09 47.18 40.10 43.52 42.92 42.83 41.89 21.93 23.12 30.29 38.57 39.08 40.53 40.44 40.78 41.04 41.13 40.02 42. 54.44 51.45 52.99 52.65 49.23 50.77 22.70 22.35 33.28 46.59 49.06 49.06 60.23 50.60 51.71 50.77 50.64 51.02 75.97 66.92 74.07 71.84 69.78 70.96 40.45 24.62 50.13 65.99 69.32 67.97 69.95 69.99 71.46 71.25 72.01 71.72 78.32 73.06 75.25 75.72 75.04 74.83 55.60 26.22 63.22 71.17 73.78 74.33 74.24 74.58 75.42 75.55 75.08 76. 82.70 79.76 81.36 81.90 80.01 80.13 25.08 25.76 67.21 78.37 79.59 78.91 80.22 79.92 80.56 80.77 80.64 81.31 57.27 77.91 69.30 64.66 49.96 73.07 65.27 57.61 53.42 76.88 65.19 61.85 52.19 75.46 65.67 60.61 50.84 74.16 66.22 59.50 51.58 75.30 65.98 60.14 32.59 58.32 55.17 41.56 25.69 52.34 49.96 34.74 40.28 65.13 55.88 46.98 40.28 72.96 61.01 54.50 48.17 74.43 63.93 57.88 48.54 74.21 65.59 57.85 48.92 74.43 65.43 58.67 49.30 74.16 66.41 59.04 50.62 75.30 67.01 59.96 51.44 75.35 66.46 60.08 51.19 74.92 67.25 60.28 51.85 74.81 67.72 60.44 60.25 79.22 72.22 67.44 54.62 77.09 66.22 62.22 57.62 78.29 70.09 64.95 56.53 77.97 69.06 64.44 55.66 76.99 68.90 63.88 55.27 77.04 68.36 63.48 41.06 67.08 55.80 48.29 25.80 52.99 51.85 35.99 46.23 70.13 57.93 53.56 53.35 76.17 64.33 60.72 52.96 75.68 67.88 61.88 53.87 76.44 68.43 62.72 53.15 75.90 67.96 62.34 53.73 76.77 68.19 62.81 55.39 76.77 69.61 63.65 55.81 77.42 70.56 64.09 55.61 76.82 71.11 63.73 55.97 77.37 70.24 64. 64.77 82.15 77.98 72.41 61.94 80.47 75.61 69.85 62.78 81.07 76.01 70.84 62.86 81.39 75.77 70.91 61.58 80.20 73.64 68.93 61.78 80.14 74.59 69.48 25.04 49.51 49.57 34.38 25.46 52.50 51.38 35.49 35.45 74.10 64.33 54.87 59.65 79.00 74.90 67.70 59.99 78.35 74.98 68.39 60.21 79.00 75.61 68.56 60.23 80.22 76.09 68.82 60.52 79.22 76.24 69.30 61.77 79.82 76.64 70.10 61.91 80.30 77.51 70.25 61.68 79.89 77.35 70.04 62.29 80.58 77.82 70.60 Table 9: The detailed accuracies of 5 tasks for Llama3 models with INT2/4 mixed precision. \"Avg. Bits\" indicates average bit-width. Model Method Avg. Bits Group Size ARC-c ARC-e Hella. Piqa Wino. Avg. Llama-3-8B Llama-3-70B 16Bits AQLM EfficientQAT EfficientQAT SignRoundV1 Ours Ours Ours* Ours* Ours Ours Ours* Ours* 16Bits AQLM EfficientQAT EfficientQAT SignRoundV1 Ours Ours Ours* Ours* Ours Ours Ours* Ours* 16 2 2 2 2 2 2 2 2 2.5 2.5 2.5 2.5 16 2 2 2 2 2 2 2 2 2.5 2.5 2.5 2.5 - 1x16 128 128 128 64 128 64 128 64 128 64 - 1x16 128 64 128 128 64 128 64 128 64 128 64 50.17 41.21 36.01 37.03 34.22 35.67 37.37 38.99 39.42 41.72 43.34 41.38 43.00 60.24 50.34 48.81 49. 47.87 49.91 52.22 52.05 54.95 56.83 55.29 56.48 56.40 80.09 74.24 69.15 71.17 67.09 70.03 70.29 72.18 71.42 74.87 75.42 74.87 74.87 86.83 78.83 79.25 77.40 79.71 80.81 81.73 82.41 82.53 83.67 83.08 83.04 82.95 60.14 55.44 50.74 51. 44.15 47.70 48.69 49.19 50.04 52.81 53.27 53.31 54.13 66.40 63.47 60.75 61.60 43.96 59.75 60.93 60.41 61.05 63.06 62.60 63.57 63.07 79.54 77.80 75.30 76.03 71.65 72.96 74.59 73.94 74.43 75.57 76.06 75.46 76.28 82.21 79.65 79.60 77. 78.62 79.49 80.09 79.38 79.82 80.47 81.23 81.56 80.63 73.24 71.82 65.67 67.72 59.12 63.14 66.14 65.57 66.61 68.82 70.24 67.96 69.53 80.74 78.22 69.46 74.03 73.64 75.14 75.69 76.56 76.32 77.82 77.66 78.77 78.53 68.64 64.10 59.37 60. 55.25 57.90 59.42 59.97 60.38 62.76 63.67 62.60 63.56 75.28 70.10 67.57 67.89 64.76 69.02 70.13 70.16 70.93 72.37 71.97 72.68 72.32 Table 10: The detailed accuracies of 10 tasks for Llama-3.1-8B-Instruct model with MXFP4/8 mixed precision. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. Avg. Bits Method ARC-c. ARC-e Boolq Hella. Lamb. Mmlu Open. Piqa Truth. Wino. Avg. 4 4.5 5 6 16Bits 51. 81.78 84.04 59.13 73.14 67.96 33.00 80.03 37.21 73.80 64.16 RTN DL Ours DL Ours DL Ours DL Ours 44.37 46.25 47.35 47.18 49.06 50.43 49.83 51.45 51.54 75.55 79.00 79.63 77.61 80. 79.17 80.93 81.19 81.40 80.86 55.19 61.96 57.06 31.80 76.28 29.25 70.80 58.31 83.09 55.28 69.63 60.55 31.80 77.75 33.41 70.48 60.72 82.60 56.10 69.53 62.00 33.60 77.69 32.80 72.06 61.34 83.70 57.57 63.67 62.83 33.60 77.64 31.21 71.35 60.64 83.18 57.35 71.41 64.86 33.40 78.40 34.76 70.40 62.33 84.10 58.22 68.62 64.77 34.60 78.62 34.52 72.61 62.57 84.19 57.92 72.19 65.43 35.80 79.22 35.37 71.03 63.19 84.59 58.94 69.73 65.98 36.40 79.33 34.76 74.03 63.64 84.89 58.57 71.96 66.85 35.40 79.38 36.84 74.35 64. Table 11: The detailed accuracies of 10 tasks for Llama-3.1-70B-Instruct model with MXFP4/8 mixed precision. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. Avg. Bits Method ARC-c. ARC-e Boolq Hella. Lamb. Mmlu Open. Piqa Truth. Wino. Avg. 16 4 4.5 6 16Bits 62.46 86.87 87.80 65.15 75.51 82.34 37.20 83.30 40.64 78.69 70.00 RTN DL Ours DL Ours DL Ours DL Ours 59.47 58.28 59.90 59.73 59.64 60.15 61. 61.09 62.12 85.10 85.06 85.82 86.24 86.24 86.57 86.95 86.53 86.15 88.90 63.18 73.53 78.72 36.00 82.32 40.88 79.01 68.71 88.53 63.35 75.24 79.66 37.60 82.32 40.15 79.95 69.01 88.53 63.63 75.45 79.97 37.00 81.88 40.76 80.27 69. 88.69 64.68 75.28 80.70 36.40 82.10 41.62 82.40 69.78 88.69 64.51 76.89 81.20 37.20 82.86 40.39 82.00 69.96 88.84 64.94 75.65 81.08 38.20 82.70 41.13 81.77 70.10 89.17 64.89 77.18 81.23 37.40 83.19 41.00 80.90 70.31 88.93 65.45 76.89 81.99 38.00 83.08 41.13 82.48 70.56 88.72 65.65 76.69 82.00 37.60 83.30 40.88 81.85 70.50 Table 12: The detailed accuracies of 10 tasks for Qwen2.5-7B-Instruct model with MXFP4/8 mixed precision. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. Avg. Bits Method ARC-c. ARC-e Boolq Hella. Lamb. Mmlu Open. Piqa Truth. Wino. Avg. 4 4.5 5 6 16Bits 52. 81.61 86.36 62.01 69.44 71.76 34.60 79.65 47.98 70.72 65.67 RTN DL Ours DL Ours DL Ours DL Ours 48.81 53.24 53.67 50.09 53.50 51.11 54.61 53.50 54.18 75.59 80.89 80.18 77.74 81. 80.26 82.62 82.45 82.24 82.48 56.84 57.68 65.15 34.80 75.79 41.62 67.48 60.62 85.47 57.88 64.41 68.05 35.00 77.48 46.76 71.43 64.06 85.08 57.99 66.27 68.30 33.60 76.66 43.08 68.90 63.37 85.96 58.98 67.86 68.39 34.60 75.84 43.33 68.51 63.13 85.90 59.32 69.05 69.59 33.60 78.62 44.92 68.98 64.51 86.85 59.54 68.27 69.42 35.20 76.77 44.31 68.67 64.04 85.90 59.78 69.67 70.43 33.60 78.07 45.90 69.77 65.04 86.51 60.89 68.93 70.64 36.80 77.91 46.39 70.48 65.45 86.30 60.68 69.73 70.52 34.80 78.73 45.78 70.01 65. Table 13: The detailed accuracies of 10 tasks for Qwen3-8B model with MXFP4/8 mixed precision. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. Avg. Bits Method ARC-c. ARC-e Boolq Hella. Lamb. Mmlu Open. Piqa Truth. Wino. Avg. 16 4 4.5 6 16Bits 55.46 83.38 86.73 57.04 64.00 72.96 31.20 76.61 36.72 68.27 63.24 RTN DL Ours DL Ours DL Ours DL Ours 50.17 51.28 53.84 50.51 53.75 51.11 54. 53.84 52.65 76.14 81.02 80.98 79.38 81.61 80.30 81.82 81.57 81.65 84.98 52.00 58.08 65.90 28.60 73.01 33.66 62.83 58.54 85.20 52.86 58.26 68.51 31.40 75.03 32.44 66.54 60.25 85.84 53.09 61.91 68.88 32.00 75.30 36.35 70.72 61. 86.02 52.96 60.76 68.91 28.60 73.78 33.17 64.01 59.81 86.21 54.26 64.16 70.25 31.40 75.57 37.09 68.51 62.28 86.09 54.23 62.04 70.26 31.40 75.19 35.62 68.27 61.45 86.42 54.62 64.20 71.53 30.80 76.01 37.58 68.27 62.54 86.94 55.36 62.10 71.46 31.20 75.95 35.50 66.06 62.00 86.97 55.57 64.06 71.54 30.20 76.17 35.74 67.32 62.19 Table 14: The detailed accuracies of 10 tasks for Qwen3-32B model with MXFP4/8 mixed precision. \"Avg. Bits\" indicates average per-weight bit-width. \"DL\" means DeltaLoss-only without tuning. Avg. Bits Method ARC-c. ARC-e Boolq Hella. Lamb. Mmlu Open. Piqa Truth. Wino. Avg. 4 4.5 5 6 16Bits 57. 84.47 86.39 63.91 67.13 80.74 36.00 80.96 39.05 73.40 67.00 RTN DL Ours DL Ours DL Ours DL Ours 55.38 57.51 59.47 56.23 57.59 56.83 57.85 57.51 58.19 81.82 84.55 83.16 83.33 84. 83.38 84.47 83.04 83.88 85.54 61.54 65.67 76.39 34.40 78.94 38.80 72.22 65.07 87.13 61.61 68.83 78.32 36.20 79.71 41.37 73.95 66.92 87.19 61.70 68.78 78.39 35.40 79.71 41.74 73.01 66.86 86.88 62.40 67.49 78.93 34.60 79.11 40.64 70.88 66.05 88.23 62.51 69.42 79.51 33.00 79.49 41.00 74.03 66.89 84.50 63.18 66.25 79.58 34.00 79.92 40.39 71.98 66.00 87.92 63.33 67.22 80.10 35.60 80.25 40.51 74.43 67.17 86.12 63.28 67.69 80.02 36.40 80.36 40.15 71.51 66.61 86.85 63.44 68.12 80.40 36.40 80.47 40.88 73.48 67."
        }
    ],
    "affiliations": [
        "Intel"
    ]
}