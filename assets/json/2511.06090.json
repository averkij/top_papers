{
    "paper_title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?",
    "authors": [
        "Jeffrey Jian Ma",
        "Milad Hashemi",
        "Amir Yazdanbakhsh",
        "Kevin Swersky",
        "Ofir Press",
        "Enhui Li",
        "Vijay Janapa Reddi",
        "Parthasarathy Ranganathan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 0 9 0 6 0 . 1 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "SWE-FFICIENCY: CAN LANGUAGE MODELS OPTIMIZE REAL-WORLD REPOSITORIES ON REAL WORKLOADS? Jeffrey J. Ma1 Milad Hashemi2 Amir Yazdanbakhsh2 Kevin Swersky2 Ofir Press4 Enhui Li5 Vijay Janapa Reddi1 Parthasarathy Ranganathan3 1Harvard University 5Xian Jiaotong University 2Google DeepMind 3Google 4Princeton University"
        },
        {
            "title": "ABSTRACT",
            "content": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-FFICIENCY, benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given complete codebase and slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15 the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning."
        },
        {
            "title": "INTRODUCTION",
            "content": "Language models (LMs) are becoming an increasingly substantial part of software engineering, from LM-powered auto-complete to autonomous software-engineering agents that plan, implement, and verify changes in large repositories. Recent agentic systems show that LMs can fix functional bugs and implement small features (Jimenez et al., 2024; Jain et al., 2024b; Yang et al., 2024; Wang et al., 2025). However, most benchmarks for these systems focus on what gets fixed or resolved, not the properties of code implementationsoverlooking runtime performance, memory efficiency, style, and other software-engineering concerns. As we reach the limits of hardware, software optimizations become critical and have tremendous impact: Jain et al. (2024a) show that pure software changes can reduce high-utilization workload throughput by 10% on Googles datacenter compute, saving estimated millions of dollars. Recent benchmarks begin to probe code performance (e.g., KernelBench, Ouyang et al. (2025); PIE, Shypula et al. (2024); EffiBench, Huang et al. (2024)), but they avoid real-world, end-to-end workloads on real repositories. We therefore ask: to what extent can LM agents optimize the runtime of real-world repositories on real-world workloads? Recent work has begun to evaluate whether LMs can improve repo-level software runtime, most notably, GSO (Shetty et al., 2025) and SWE-Perf (Fan et al., 2025): both benchmarks tackle the problem of repo-level code optimization. GSO provides each task with an oracle script verifying functional equivalence and runs hidden performance tests at evaluation. SWE-Perf adapts existing Data, code, and leaderboard at swefficiency.com. Correspondence to jeffreyma@g.harvard.edu."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: SWE-FFICIENCY evaluates the investigative, pass-to-pass workflow of performance engineering: given an existing codebase state and performance workload of interest, agents must edit the codebase to speed up that workload while keeping relevant repo unit tests green. repo unit-tests for both correctness and performance measurement, with task instructions pointing agents to optimize specific functions. However, software repositories commonly separate correctness and performance tests (ISO/IEC, 2011), and immediate correctness oracles are usually unavailable. Thus, these setups still insufficiently assess core part of performance engineering: investigating an unfamiliar repository to recover code semantics and correctness from the codebase alone. Performance engineers characterize workload (which can be slow for any myriad of reasons); localize where to intervene; and, just as importantly, localize testsidentifying and executing existing unit-tests to be confident that an optimization does not introduce new functionality. We design our benchmark to target this challenging and open-ended investigative workflow. To address these gaps, we propose SWE-FFICIENCY (pronounced swee-FISH-uhn-see), new benchmark to evaluate how well LMs can improve the performance of real-world workloads through modifying software repositories (Figure 1). To build SWE-FFICIENCY, we propose novel, systematic data collection pipeline for optimization task instances, which uses attribute filtering, static analysis, code coverage, and execution validation. This anchors the realism of the benchmark and usefulness of the optimization tasksa large and diverse set of 498 tasks across 9 codebases across data science, machine learning, and high performance computing. We score LM systems using speedup ratio (SR), which evaluates how well models match or improve upon expert edits and motivates long-term progress on our benchmark. We also conduct holistic evaluation of LMs on SWE-FFICIENCY to better understand strengths and limitations. We reveal systemic gaps: on average, LMs achieve less than 0.15 expert speedup and often introduce correctness bugs via proposed edits. Models struggle to localize the same expert optimization opportunities and prefer superficial speedups than more principled expert algorithmic rewrites. Thus, while LMs exhibit promise in other SWE tasks, substantial advances in repo-level reasoning, systems optimization, and long-horizon planning are needed to close this expert gap. Our contributions. (1) scalable, oracle-free benchmark of 498 tasks across 9 repos, requiring deep codebase investigation and test localization. (2) systematic pipeline for extracting realistic and reproducible performance engineering tasks from GitHub repos. (3) An evaluation metric, speedup ratio, that measures parity with experts and encourages long term benchmark progress. (4) Empirical and qualitative analysis revealing large gaps between LMs and experts in edit localization and principled optimizations. (5) Open-sourced dataset, benchmark harness, and pipeline to accelerate research on automated performance engineering and long-horizon software reasoning."
        },
        {
            "title": "2 SWE-FFICIENCY OVERVIEW",
            "content": "SWE-FFICIENCY is benchmark containing real performance-optimization GitHub pull requests from popular repositories. The task is to generate pull request that modifies the codebase to make given workload faster while preserving the correctness of existing repo tests."
        },
        {
            "title": "Preprint",
            "content": "Table 1: SWE-FFICIENCY jointly (i) evaluates the runtime of performance workloads, (ii) verifies correctness using repositorys own tests, and (iii) uses separate correctness and performance workloads. For more details on related benchmarks, see Section 5. Benchmark Evaluates Runtime Repo Level SWE-BENCH EFFIBENCH MERCURY PIE KERNELBENCH ALGOTUNE GSO SWE-PERF SWE-FFICIENCY (OURS) Correctness Eval: Using Repos Own Tests Performance Eval: Separate end-to-end system test # of Optimization Tasks 0 1000 1889 978 250 154 102"
        },
        {
            "title": "2.1 DATA COLLECTION PROCEDURE",
            "content": "We scrape nine popular Python GitHub repos, including astropy, dask, matplotlib, numpy, pandas, scikit-learn, scipy, sympy, and xarray. Figure 2 shows how our method extends the scraping recipe from SWE-bench, modifying the attribute and execution filtering (Stages 2 and 5) to select previously-excluded performance edits and introducing test coverage filtering and workload annotation (Stages 3 and 4) to identify reproducible, verifiable optimization tasks. Stage I: Repo selection and instance scraping. We target GitHub pull requests (PRs) from popular data science, machine learning, and high-performance computing repositoriesthese domains are performance-sensitive and contain PRs where authors explicitly optimize runtime. Widely-used libraries surface optimizations that are actually useful in real-world software. Stage II: Performance regression attribute filtering. We prune away PRs that clearly are not performance-related or that introduce new behavior: in contrast, issue-resolution benchmarks like SWE-bench filter for the opposite by choosing tasks that add new tests. We select PRs only when (i) metadata includes performance keywords (i.e. perf, speedup, benchmark); (ii) PRs do not modify tests, to avoid behavior-changing edits misrepresenting as optimizations; and (iii) edits meaningfully modify the files abstract syntax tree (AST), excluding no-op or docs-only diffs. Stage III: Identifying covering correctness tests. To enforce our benchmarks invariant specification (i.e. all relevant tests must continue to pass after an edit), we require that at least one existing unit test exercises the modified code. Per instance, we build Docker image with pinned dependencies, run the repositorys test suite, and use line coverage to confirm the edit is exercised. Stage IV: Annotating performance workloads. Unit tests often do not capture runtime behavior, and software generally separates correctness from performance tests (ISO/IEC, 2011). Using PR descriptions and discussion as context, we manually annotate each taskwriting workload script Figure 2: SWE-FFICIENCY collects tasks through multi-stage scraping pipeline: each stage prunes candidate tasks that introduce new behavior, are unlikely to be performance related, or unsuitable for reproducible benchmarking. This yields set of tasks, each of which have an accompanying expert or gold patch. See Appendix for stage-specific details."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: SWE-FFICIENCY contains diverse distribution over performance workload runtime (left); over gold patch speedup (speedup achieved from expert PR edit); and over types of optimizations made by the expert (right). We use an LM to categorize the gold patch for each instance (for high-level analysis only) and manually verify randomly chosen subset: see Appendix B. that, when run before and after the PRs edit, shows measurable performance improvement. Although PR info often includes ad-hoc demo scripts, these are not reliably auto-extractable; likewise, LM-based workload generation from patches fails to consistently elicit claimed gains (see Sec. 4.2). Stage V: Execution-based filtering. To curate final set of verifiable and consistently reproducible optimization tasks, we run each instances unit tests and annotated workload in controlled environment (containerization, resource pinning) to ensure no interference with speedup measurements. We retain only instances that demonstrate significant speedups (runtime improvement greater than 2 measurement std. dev.) and log test statuses for benchmark correctness checks."
        },
        {
            "title": "2.2 SWE-FFICIENCY DATASET DISTRIBUTION AND UNIQUE BENCHMARK FEATURES",
            "content": "Open-ended but precise evaluation criteria. Providing agents with just an executable code snippet (workload) and codebase makes the task very open-ended: agents can choose any approach, including changes different from the experts gold patch. This mirrors the flexibility of real-world performance engineeringthere rarely is single prescriptive path to faster implementations. Our evaluation is made precise by grounding correctness in set of unit tests and measuring LM speedup against gold patch speedup (i.e. how much speedup the expert achieved). Our benchmark encourages creativity in edit strategies while guaranteeing unambiguous criteria for strong optimizations. Clear distinction between performance and correctness tests. Repositories generally require unit tests to run quickly (unlike more substantial performance workloads), and software standards encourage performance benchmarks to be clearly separated from correctness tests (ISO/IEC, 2011). Thus, defining performance workload using unit-tests or combined correctness-performance oracle is not fully reflective of actual performance engineering. Instead, SWE-FFICIENCY clearly separates performance evaluation workloads from repo correctness tests. Preserving existing correctness during optimization. Unlike SWE-benchs issue-resolution setting (evaluating bug-fixes that flip failing tests to passing), our benchmark targets pass-to-pass optimizationspeeding up already-correct code without introducing new behavior. Specifically, we choose PRs that do not introduce new behavior: edits that introduce new features (and new tests) may have unintended performance effects on other workloads, and confound our specific evaluation of code optimization abilities. Evaluating how agents perform edits in this constrained task provides more confidence they can be deployed in real codebases without disrupting existing behavior."
        },
        {
            "title": "2.3 TASK FORMULATION",
            "content": "Model input. An agent is given complete codebase and performance workload exercising codebase functionality. We task the agent with modifying the codebase so that workload runtime improves while expected repository unit tests still pass. Expert performance engineers only require reported slow workload and codebase to start optimizing: first characterizing the workloads bot-"
        },
        {
            "title": "Preprint",
            "content": "tlenecks, modifying files, verifying speedup against the workload and identifying relevant unit tests to check for no regressions. For examples of performance workloads, see Appendix B.2. Evaluation metrics. Our evaluation metric is speedup ratio (SR), which answers the question: normalized to the expert edit, how well does the LMs generated edit perform?. We apply an LMs submitted patch to the codebase and run repository test files associated with each instance. If the patch applies successfully and all tests pass, we compute the instance speedup ratio as SR = SpeedupLM/Speedupgold where gold speedup is Speedupgold = Tpre/Tpost-gold-patch and LM speedup is SpeedupLM = Tpre/Tpost-LM-patch. For example, if the expert achieves gold speedup of 5 and the LM achieves 1.2 on the same instance, SR = 1.2/5 = 0.24. To aggregate across instances, we take the harmonic mean of each SR: if system submits an empty patch or patch that fails unit tests, the instances speedup ratio is SR = 1/Speedupgold. We use harmonic mean since it is most appropriate for averaging speedup ratios (Smith, 1988; Eeckhout, 2024). Why factor-based evaluation metric (not % solved)? We adopt speedup ratio because it provides long runway for progress and explicitly rewards going beyond human parity. Percentagestyle metrics collapse to two regimesnear 0% today and nearer 100% once tests are routinely passedleaving little room to compare systems once the benchmark begins to saturate. It motivates continued progress: anchoring the scale at expert parity (1) turns super-human performance into first-class goal and keeps the leaderboard competitive after models reach expert performance. This means our benchmark stays meaningful for the community both now (when systems only reach 0.15 of expert performance) and later (when models might consistently score above 1)."
        },
        {
            "title": "3 EVALUATION SETUP",
            "content": "Machine Configuration. We containerize each task environment: instance Docker images are built and uploaded to registry for reproducibility and easy integration with agent harnesses. All evaluations are run on single Google Cloud n2-standard-64 VM (64 vCPUs, 256GB Memory). To parallelize the benchmark for faster evaluation without interference from parallel workers, we pin each worker to an exclusive set of physical CPU cores (4 vCPUs), the CPUs corresponding memory node, and assigning memory limit (16GB) per worker. For more details, see Appendix D. Agent Scaffold. We provide baseline performance on two open-source agent harnesses, OPENHANDS (CodeActAgent-v0.51.1) (Wang et al., 2025) and SWE-AGENT (v1.1.0) (Yang et al., 2024). Both scaffolds provide file-editing tools and bash terminal interface for LM agents to easily edit code and execute commands. We configure agents with 3-hour time limit per task, 30-minute timeout per step, maximum action count of 100, and provide the same number of vCPUs and memory as the evaluation setting. The agent is provided task prompt and repo-specific commands for rebuilding (to support possible C/C++/Cython edits) and executing arbitrary test files. In the SWE-AGENT setting, we configure the underlying LM with $1 token-spend max per task to observe performance under limited inference cost. We provide evaluations on both harnesses specifically for CLAUDE-3.7-SONNET, GPT-5 MINI, and GEMINI 2.5 FLASH models. See Appendix for agent prompts and details. Models. We evaluate several frontier models from OpenAI, Anthropic, Google, Z.ai, Moonshot AI, and DeepSeek: GPT-5 (OpenAI, 2025a), GPT-5 MINI OpenAI (2025b), CLAUDE 4.1 OPUS (Anthropic, 2025a), CLAUDE 4.5 SONNET (Anthropic, 2025c), CLAUDE-3.7-SONNET (Anthropic, 2025b)), GEMINI-2.5 PRO (Google, 2025b), GEMINI 2.5 FLASH (Google, 2025a)), GLM-4.6 (Z.ai, 2025), KIMI K2-9005 (Moonshot AI, 2025)), and DEEPSEEK V3.1 (DeepSeek, 2025)). For each instance, we sample single trajectory and report the aggregated speedup ratio. We focus on pass@1 because it best matches both agent capabilities and realistic human workflows: (i) expert pull requests are effectively pass@1 (infeasible to review multiple PR submissions) and (ii) agentic LMs can still explore multiple edits, execute workloads repeatedly, and iterate over alternatives within single trajectory. This also follows common practice in prior benchmarks, including HumanEval and SWE-bench, where pass@1 is the primary metric (Chen et al., 2021)."
        },
        {
            "title": "Preprint",
            "content": "Table 2: SWE-FFICIENCY results across several frontier models (higher is better; human-expert speedup ratio (SR) is 1.0). SR is pass@1: each system submits single patch per instance to be evaluated. SR is calculated by normalizing the speedup from the LM-generated edit to the speedup from the gold (human-written) patch, aggregated across all tasks via harmonic mean. All experiments below used OPENHANDS: SWE-AGENT achieves similar results, see Appendix F. System Speedup Ratio () GPT-5 CLAUDE 4.1 OPUS QWEN3 CODER PLUS CLAUDE 3.7 SONNET CLAUDE 4.5 SONNET GLM-4.6 GPT-5 MINI KIMI K2-0905 GEMINI 2.5 FLASH DEEPSEEK V3.1 GEMINI 2.5 PRO 0.150 0.098 0.064 0.047 0.041 0.026 0.019 0.008 0.008 0.007 0.007 Table 3: Further breakdown of patch outcomes by system. Pre-edit denotes codebase before any edits. Passes correctness tests\" refers to functional correctness only (not necessarily perf. optimal). All experiments below used OPENHANDS. System Fails Tests () Passes Correctness Tests Slower than Pre-edit () Faster than Pre-edit () Faster than Expert () GPT-5 CLAUDE 4.1 OPUS QWEN3 CODER PLUS CLAUDE 3.7 SONNET CLAUDE 4.5 SONNET GLM-4.6 GPT-5 MINI KIMI K2-0905 GEMINI 2.5 FLASH DEEPSEEK V3.1 GEMINI 2.5 PRO 18% 15% 22% 35% 19% 33% 45% 26% 39% 19% 40% 4% 4% 11% 12% 5% 13% 15% 11% 14% 18% 18% 32% 43% 42% 32% 44% 38% 27% 44% 34% 44% 34% 46% 38% 24% 20% 33% 16% 13% 19% 12% 18% 8%"
        },
        {
            "title": "4 EXPERIMENTS AND RESULTS",
            "content": "On SWE-FFICIENCY, leading LM agents trail experts and often introduce correctness bugs. Our benchmark enables key quantitative and qualitative observations about LM agent behavior, namely how models solve easier cases, falter on harder ones, and exhibit convenience biassmall, inputspecific, harder-to-maintain editsunderscoring the gap to expert-level performance engineering."
        },
        {
            "title": "4.1 OVERALL PERFORMANCE",
            "content": "Leading agents struggle on SWE-FFICIENCY. Across all agents, we observe that LM agents struggle to achieve more than 0.15 of expert level performance. Table 2 summarizes speedup ratio performance of leading software-engineering agents on SWE-FFICIENCY. We see substantial capability transfer gap: GPT-5 MINI (OPENHANDS) achieved 0.019 of expert speedup, while the same system scored 62.6% on SWE-bench Verified. This indicates that current agents, while successful on issue-resolution and bug-fix tasks, currently do not immediately transfer to efficiencyoriented program changes, showing substantial headroom for improvement. Agents often introduce bugs during optimization. LM agents often propose edits that cause repository unit tests to newly fail, invalidating any optimizations made. Table 3 unpacks agent performance across different unit test and performance outcomes: even when patches are functionally"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: LMs achieve strong performance on easier problems but struggle on tasks with longer workload runtime duration and larger baseline expert speedups. We bucket LM submissions by perinstance speedup ratio and compute the geometric mean per-bucket of (i) pre-edit workload runtime, (ii) the gold (expert) patch speedup, and (iii) the number of lines in the gold patch. correct, the majority of edits are still slower than the expert. Strikingly, with the exceptions of GPT5, CLAUDE 4.1 OPUS, and CLAUDE 4.5 SONNET, fewer than quarter of solutions are both correct and outperform expert-level speedups. Strong on easy wins, weak on harder speedups. We identify three measures of task difficulty: (1) pre-edit workload runtime (longer duration workloads likely require more algorithmic insight); (2) gold patch length (harder instances require editing more lines); and (3) the speedup factor that the expert edit achieves (instance is harder if expert speedup is larger). Figure 4 shows breakdown of benchmark performance in relation to these task difficulty measures. Across all three measures of task complexity, LMs are able to match expert performance on lower-complexity tasks. However, LMs struggle to solve tasks with longer duration workloads or larger feasible speedup opportunities. Function-level mislocalization severely limits LM performance. Much of LM underperformance appears to stem from failing to optimize the same functions as the expert. If we view expert (gold) speedup as mass\" distributed over edited files and functions, Figure 6 shows that over 68% of expert gains occur in functions the LM never edits. Although LM and expert modify the same files over 55% of the time, they miss the functions carrying most of the experts speedup. Likewise, Figure 7 visualizes function-level breakdown where the LM makes an attempted optimization in deeper function and fails to match the experts improvement. For more details, see Appendix G."
        },
        {
            "title": "4.2 QUALITATIVE ANALYSIS",
            "content": "LMs make satisficing optimizations, giving up before expert parity. Figure 5 shows that the shortest sequences of agent actions (i.e. file-editing, running scripts) happen when LMs achieve speedup ratios exceeding 1expert-level wins are found early. When LMs underperform experts, median trajectory action counts sit at less than mid length (3050 turns), well below the 100 action Figure 5: LMs find expert-level wins earlier on in action trajectories. When they underperform experts, LMs submit satisficing optimizations rather than trying on for expert parity. Figure 6: LMs leave significant portion of expert-achievable speedup on the table due to wrong file/function selection and localization."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: LMs prefer to edit different functions than the gold patch, missing out on major speedups. For workload flamegraph for task pandas-dev__pandas-52054, CLAUDE 3.7 SONNET (SWE-AGENT) (red) chooses different function (and file) than the expert (gold): it does not achieve the experts overall workload speedup, since the experts speedup is at shallower scope. --- a/pandas/core/arrays/arrow/array.py +++ b/pandas/core/arrays/arrow/array.py @@ -406,8 +406,14 @@ def _cmp_method(self, other, op): - - + + result = result.to_numpy() return BooleanArray._from_sequence(result) if result.null_count > 0: values = pc.fill_null(result, False). to_numpy() + + + + + mask = result.is_null().to_numpy() else: values = result.to_numpy() mask = np.zeros(len(values), dtype=np. bool_) return BooleanArray(values, mask) def _evaluate_op_method(self, other, op, arrow_funcs): pc_func = arrow_funcs[op.__name__] --- a/pandas/core/arrays/arrow/array.py +++ b/pandas/core/arrays/arrow/array.py @@ -406,8 +406,16 @@ class ArrowExtensionArray( OpsMixin, ExtensionArray): - - + + + + + + + result = result.to_numpy() return BooleanArray._from_sequence(result) if result.null_count == 0: result_np = result.to_numpy().astype(bool ) return BooleanArray(result_np, np.zeros( len(result), dtype=bool)) result_np = result.to_numpy().astype(bool) mask = result.is_null().to_numpy() return BooleanArray(result_np, mask) Figure 8: Left: Experts edit (gold patch) on instance pandas-dev__pandas-50524 optimizing workload via avoiding conversion to object dtype (20.5 speedup). Right: CLAUDE 3.7 SONNET (OPENHANDS) instead identifies different fast path optimization when no null elements are present, but only achieves 2.3 speedup (scoring speedup ratio of 0.113). cap. This pattern fits satisficing story: once the model secures measurable speedup, it tends to stop instead of pushing any closer to expert parity. Future agents can employ dont-stop-early\" triggers when code heuristics show larger possible speedups. Shortcut bias and caching as crutch vs. systemic cost reduction. LMs preferentially add localized shortcutsidentity checks, ad-hoc early exits, and memoizationsuch as self-equality fast paths or persistent caches. Experts instead restructure code to reduce per-element cost. Figure 8 shows flamegraph both after an LM versus an expert edit, where the expert optimizes by keeping work in fast Arrow kernels and producing BooleanArray from values/mask pair without materializing slow object-dtypes. Experts also use faster backends (Cython/Pythran/BLAS) to reduce Python overhead or remove Python-level work entirelyvectorizing, moving loops to compiled code, or dispatching to type-aware fast paths. LMs yield strong speedups only when these shortcut conditions hold, whereas systemic reductions are more broadly robust. Workload overfitting and semantic drift. Another LM pattern is to bake benchmark properties into patches, producing impressive but brittle wins. This sometimes crosses into correctness drift e.g., returning the original DataFrame from groupby.apply or monkey-patching np.arange. Experts instead target generalizable structure (i.e. multi-index skipping, per-dimensional slice reuse) while preserving functional behavior. With an preliminary version of our evaluation harness, some agents exploited function stackframe info to detect when code is being run in our evaluation environment: we consequently added robust checks for this in the harness (see Appendix J.1)."
        },
        {
            "title": "Preprint",
            "content": "--- a/pandas/core/series.py +++ b/pandas/core/series.py @@ -1818,7 +1818,7 @@ def to_dict(self, into: type[ dict] = dict) -> dict: else: - + return into_c((k, v) for k, in self. items()) return into_c(self.items()) --- a/pandas/core/series.py +++ b/pandas/core/series.py @@ -1816,9 +1816,18 @@ class Series(base. IndexOpsMixin, NDFrame): # type: ignore[misc] else: return into_c((k, v) for k, in self. items()) values = getattr(self, \"_values\", None) if values is None: return into_c((k, v) for k, in self.items()) try: list_vals = values.tolist() except Exception: # fallback to generic iteration list_vals = [v for in values] return into_c(zip(self.index, list_vals )) - + + + + + + + + + 9: Left: Experts Figure optimizing Series.to_dict by replacing key-value pair generator with items() view, eliminating per-element tuple allocation (1.38 speedup). Right: GPT-5 MINI (OPENHANDS) converts the underlying array to Python list, zipping with the index to reduce Python-level boxing when iterating (1.98 speedup). on pandas-dev__pandas-50089, edit Maintainability of generated edits. LM edits are frequently invasiveglobal monkey-patching, module-level mutable caches, or fast paths tied to dynamic object attributes (an example shown in Fig. 9). Expert patches are localized and composableadding function call with precomputed constants, Cython helper mirroring existing logic, or reusing shallow copies of constructor arguments. Expert edits have lower blast radius of code edits and are more maintainable long term. Manually annotated workloads outperform LM generation. Using our evaluation harness, we also study how well LMs can generate performance workloads. We compare the runtime improvement of each expert patch under two workloads: (i) an LM-generated (GEMINI 2.5 FLASH) workload produced from the gold patch and relevant files, and (ii) SWE-FFICIENCYs manually annotated workload (Stage 4, Fig. 2). Our annotations show stronger performance deltas 76% of the time, with 47% of LM workloads showing no significant speedups. Since performance engineering involves both bottleneck workload identification and code optimization, we show how SWE-FFICIENCY can be further used to probe performance understanding in LMs. For more details, see Appendix I."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Foundational optimization and synthesis. Classic superoptimization approaches examined code-to-code transformations (Massalin, 1987; Bansal & Aiken, 2006; Schkufza et al., 2013; SolarLezama, 2008; Torlak & Bodik, 2014). Profile-guided methods (e.g., Graham et al., 1982; Pettis & Hansen, 1990) and RL-for-performance (e.g., AlphaDev (Mankowitz et al., 2023)) added steerability into code edits. However, these lines of work prioritize transformation quality and search, not the evaluation scaffolding needed for repo-scale, regression-free, workload improvement. Function-level efficiency benchmarks. MERCURY (Du et al., 2024), EFFIBENCH (Huang et al., 2024), and PIE (Shypula et al., 2024) quantify how often model-generated functions are slower than human references and study feedbackand goal-conditioned improvement. ECCO (Waghjale et al., 2024) emphasizes the necessity of correctness-preserving edits. Domain-focused work such as KERNELBENCH (GPU kernels; Ouyang et al. (2025)) and ALGOTUNE (algorithmic redesign; Press et al. (2025)) further stress wall-clock runtime as the metric. Repository-scale SWE benchmarks. Benchmarks like SWE-BENCH (Jimenez et al., 2024; Yang et al., 2025), COMMIT0 (Zhao et al., 2024), and SWT-BENCH (Mündler et al., 2024) established that long-horizon reasoning over code repos is substantially harder than snippet tasks, but they mostly target fixing bugs, developing features and writing tests, rather than performance. Agentic systems (e.g. SWE-agent (Yang et al., 2024); OpenHands, (Wang et al., 2025)) supply the tooling to navigate, edit, run, and profile codebases, improving long-horizon outcomes."
        },
        {
            "title": "Preprint",
            "content": "Repository-level performance datasets. Closer to our setting, GSO (Shetty et al., 2025) and SWE-PERF (Fan et al., 2025) curate tasks from GitHub commits and evaluate repo-level runtime. SWE-PERF reuses repository unit tests for both correctness and performance and instructs agents to optimize specified functions; GSO employs both LM-generated correctness tests and performance workloads, providing correctness oracle but not exposing performance workloads to agents. While suitable for measuring speedups, these designs insufficiently assess the localization skills central to performance engineeringcharacterizing workload, localizing bottlenecks and edits, and localizing tests by discovering in-repo unit testscapabilities our benchmark targets directly."
        },
        {
            "title": "6 DISCUSSION",
            "content": "Limitations. SWE-FFICIENCY is primarily Python/Cython changes across nine widely-used libraries; extending to lower-level stacks (C/C++/Rust) requires generalizing our coverage test selection and adding language specific build awareness. However, prior Python-only benchmarks (e.g. HumanEval, SWE-bench) have lead to accelerated progress in their respective research directions, and we believe that SWE-FFICIENCY can similarly motivate the research community. Finally, while we focus on controlled CPU-only setup, scaling to longer-running workloads and heterogeneous hardware would further evaluate agent planning and measurement. Our curation and measurement methodology, prebuilt containers, performance isolation provide solid foundation to build upon. Conclusion. We present SWE-FFICIENCY, repo-level benchmark of 498 optimization tasks across nine widely used repositories. Each task combines performance workload, an expert patch with significant speedup, and correctness tests covering the expert diff, enabling evaluation of pass-to-pass optimization. Our pipeline rigorously combines regression and AST filters, coverageguided test selection, manual workload annotation, and reproducibility checks. We present our metric, speedup ratio, for expert parity comparison and find that current agents remain well below expert performance. Our task containerization integrates with open agent frameworks, and we expose qualitative gaps with LM agents like mislocalization and shortcut bias. Our benchmark motivates long-term progress towards autonomous performance engineering and agentic-first codebases."
        },
        {
            "title": "7 ACKNOWLEDGMENTS",
            "content": "We extend our gratitude towards David Fleet and Deniz Altınbüken for reviewing the paper and providing insightful feedback. We also thank the extended team at Google DeepMind who enabled and supported this research direction. We gratefully acknowledge support from the Google Cloud Research program, Gemini for Research program, and the Amazon Research Awards program for supporting evaluation runs on this paper. We also thank the Graham Neubig, Xingyao Wang, and the All Hands AI team for providing OpenHands runtime credits that enabled harness evaluations on this paper. Computations for this work were performed in part on the FASRC cluster supported by the FAS Research Computing Cluster at Harvard University. We thank Google, Anthropic, Alibaba Qwen, Moonshot AI, Z.ai, and the Harvard Data Science Initiative organizations for sponsoring credits and supporting model evaluations in our work."
        },
        {
            "title": "8 ETHICS STATEMENT",
            "content": "SWE-FFICIENCY is collected entirely from public repositories with licenses that permit software usage that our contributions are in accordance with. Details of the licenses are included in Table 4. We do not collect information about GitHub pull request authors during data collection or evaluation, and SWE-FFICIENCY does not use GitHub data beyond what is available via public API. Our work did not involve any human subject participation: we did not crowdsource or recruit human task workers for any part of SWE-FFFICIENCY. For instance environment setup and annotation, the authors conducted all manual and semi-manual tasks. For the benchmark release, we plan to open source the SWE-FFICIENCY task instances, task collection and evaluation infrastructure, and the experimental results and model trajectories from the paper. We will also clearly document each component according to best practices and include channels for communication to engage the community to improve SWE-FFICIENCY."
        },
        {
            "title": "9 REPRODUCIBILITY STATEMENT",
            "content": "We provide our codebase and all configuration details for the evaluation environment, including container images, CPU pinning, and memory limits; see Appendix for reproducibility techniques and Appendix for agent prompts and harness specifics. Our dataset construction steps, filters, and thresholds are documented in 2.1 and Appendix C; example workloads are shown in Appendix B.2. All instances are containerized with pinned and pre-installed dependencies; we prebuild images to avoid run-to-run variance. We report exact evaluation settings in Section 3 and Appendix E. The benchmark artifacts (containers, instance manifests, and harness package) are provided in the supplemental materials for review, and we plan to release PyPI package and leaderboard website for easy community usage."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Claude 4.1 Opus, 2025a. URL https://www.anthropic.com/news/ claude-sonnet-4-5. Anthropic. Claude 3.7 Sonnet, 2025b. URL https://www.anthropic.com/news/ claude-3-7-sonnet. Anthropic. Claude 4.5 Sonnet, 2025c. URL https://www.anthropic.com/news/ claude-sonnet-4-5. Sorav Bansal and Alex Aiken. Automatic generation of peephole superoptimizers. In ASPLOS, pp. 394403, 2006. URL https://theory.stanford.edu/sbansal/pubs/ asplos06.pdf. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. DeepSeek. DeepSeek V3.1, 2025. URL https://api-docs.deepseek.com/news/ news250821. Mingzhe Du, Anh Tuan Luu, Bin Ji, Qian Liu, and See-Kiong Ng. Mercury: code efficiency benchmark for code large language models. In NeurIPS Datasets and Benchmarks, 2024. URL https://arxiv.org/abs/2402.07844. Lieven Eeckhout. R.i.p. geomean speedup use equal-work (or equal-time) harmonic mean speedup IEEE Computer Architecture Letters, 23(1):7882, 2024. doi: 10.1109/LCA.2024. instead. 3361925. Zhijie Fan, Yiming Huang, Zejian Yuan, Zejun Ma, Qian Liu, et al. SWE-Perf: Can language arXiv, 2025. URL https: models optimize code performance on real-world repositories? //arxiv.org/abs/2507.12415. Google. Gemini 2.5 Flash, 2025a. URL https://deepmind.google/models/gemini/ flash/. Google. Gemini 2.5 Pro, 2025b. URL https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/. Susan L. Graham, Peter B. Kessler, and Marshall K. McKusick. gprof: call graph execution profiler. In SIGPLAN Symposium on Compiler Construction, pp. 120126, 1982. URL https:// web.eecs.umich.edu/weimerw/2012-4610/reading/graham-gprof.pdf."
        },
        {
            "title": "Preprint",
            "content": "Dong Huang, Yuhao Qing, Weiyi Shang, Heming Cui, and Jie M. Zhang. Effibench: Benchmarking the efficiency of automatically generated code. In NeurIPS Datasets and Benchmarks, 2024. URL https://arxiv.org/abs/2402.02037. ISO/IEC. Systems and software engineering Systems and software quality requirements and evaluation (SQuaRE) System and software quality models. ISO/IEC 25010, International Organization for Standardization, Geneva, Switzerland, 2011. Akanksha Jain, Hannah Lin, Carlos Villavieja, Baris Kasikci, Chris Kennelly, Milad Hashemi, and Parthasarathy Ranganathan. Limoncello: Prefetchers for scale. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, pp. 577590, New York, NY, USA, 2024a. Association for Computing Machinery. ISBN 9798400703867. doi: 10.1145/3620666.3651373. URL https: //doi.org/10.1145/3620666.3651373. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024b. URL https://arxiv.org/abs/ 2403.07974. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=VTF8yNQM66. Daniel J. Mankowitz, A. Michi, A. Zhernov, et al. Faster sorting algorithms discovered using deep reinforcement learning. Nature, 618:257263, 2023. doi: 10.1038/s41586-023-06004-9. URL https://www.nature.com/articles/s41586-023-06004-9.pdf. Henry Massalin. Superoptimizer: look at the smallest program. In Proceedings of the 2nd Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS-II), pp. 122126, 1987. doi: 10.1145/36206.36194. URL https://courses.cs. washington.edu/courses/cse501/15sp/papers/massalin.pdf. Moonshot AI. Kimi K2, 2025. URL https://moonshotai.github.io/Kimi-K2/. Niels Mündler, Mark Niklas Mueller, Jingxuan He, and Martin Vechev. SWT-bench: Testing and In The Thirty-eighth Annual Conference on validating real-world bug-fixes with code agents. Neural Information Processing Systems, 2024. URL https://openreview.net/forum? id=9Y8zUO11EQ. OpenAI. GPT-5, 2025a. URL https://openai.com/gpt-5/. OpenAI. GPT-5 Mini, 2025b. URL https://openai.com/gpt-5/. Anne Ouyang, Simon Guo, Simran Arora, Alex L. Zhang, William Hu, Christopher Ré, and Azalia Mirhoseini. Kernelbench: Can llms write efficient GPU kernels? arXiv, 2025. URL https: //arxiv.org/abs/2502.10517. Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. Training software engineering agents and verifiers with swe-gym, 2025. URL https: //arxiv.org/abs/2412.21139. Karl Pettis and Robert C. Hansen. Profile guided code positioning. In PLDI, pp. 1627, 1990. URL https://dblp.org/rec/conf/pldi/PettisH90. Ori Press, Brandon Amos, Haoyu Zhao, Yikai Wu, Samuel K. Ainsworth, Dominik Krupke, Patrick Kidger, Touqir Sajed, Bartolomeo Stellato, Jisun Park, Nathanael Bosch, Eli Meril, Albert Steppi, Arman Zharmagambetov, Fangzhao Zhang, David Perez-Pineiro, Alberto Mercurio, Ni Zhan, Talor Abramovich, Kilian Lieret, Hanlin Zhang, Shirley Huang, Matthias Bethge, and Ofir Press. Algotune: Can language models speed up general-purpose numerical programs?, 2025. URL https://arxiv.org/abs/2507.15887."
        },
        {
            "title": "Preprint",
            "content": "Eric Schkufza, Rahul Sharma, and Alex Aiken. Stochastic superoptimization. In ASPLOS, pp. 305316, 2013. URL https://theory.stanford.edu/aiken/publications/ papers/asplos13.pdf. Manish Shetty, Naman Jain, Jinjian Liu, Vijay Kethanaboyina, Koushik Sen, and Ion Stoica. Gso: Challenging software optimization tasks for evaluating swe-agents, 2025. URL https: //arxiv.org/abs/2505.23671. Alexander Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob Gardner, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan, Osbert Bastani, and Amir Yazdanbakhsh. Learning performance-improving code edits. In ICLR, 2024. URL https://arxiv.org/pdf/2302. 07867. J. E. Smith. Characterizing computer performance with single number. Commun. ACM, 31(10): 12021206, October 1988. ISSN 0001-0782. doi: 10.1145/63039.63043. URL https://doi. org/10.1145/63039.63043. Armando Solar-Lezama. Program Synthesis by Sketching. PhD thesis, EECS Department, University of California, Berkeley, 2008. URL https://www2.eecs.berkeley.edu/Pubs/ TechRpts/2008/EECS-2008-177.html. Emina Torlak and Rastislav Bodik. lightweight symbolic virtual machine for solver-aided host languages. In PLDI, pp. 530541, 2014. URL https://homes.cs.washington.edu/ bodik/ucb/Files/2014/rosette-pldi2014.pdf. Siddhant Waghjale, Vishruth Veerendranath, Zora Zhiruo Wang, and Daniel Fried. ECCO: Can we improve model-generated code efficiency without sacrificing functional correctness? arXiv, 2024. URL https://arxiv.org/abs/2407.14044. Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. Openhands: An open platform for AI software developers as generalist agents. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. SWE-agent: Agent-computer interfaces enable automated software engineering. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://arxiv.org/abs/2405.15793. John Yang, Carlos Jimenez, Alex Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik Narasimhan, Diyi Yang, Sida Wang, and Ofir Press. SWE-bench multimodal: Do AI systems generalize to visual software domains? In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=riTiq3i21b. Z.ai. GLM-4.6, 2025. URL https://docs.z.ai/guides/llm/glm-4.6. Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias Gallé, and Alexander Rush. Commit0: Library generation from scratch, 2024. URL https://arxiv.org/ abs/2412.01769."
        },
        {
            "title": "A LLM USAGE",
            "content": "Language models were used to polish writing, help with grammatical errors and typos, and to help check with compliance against ICLRs author guide. Beyond the LM usage in our benchmark evaluations and experiments, they were not used in any other part of writing this work."
        },
        {
            "title": "Gold Patch",
            "content": "# Instances # Repos # of Lines Runtime (s) 498 9 25.47 4.47 180 257.09 # Lines edited # Files edited Speedup 49.1 2.2 2445 12 2.64 249k"
        },
        {
            "title": "Tests",
            "content": "# Pass to Pass 54k 222k Figure 10: Repository distribution of task instances in the SWE-FFICIENCY dataset. Figure 11: Additional summary statistics for SWEFFICIENCY dataset. Arithmetic mean is used in all cases, except for speedup (harmonic mean)."
        },
        {
            "title": "B ADDITIONAL DATASET INFORMATION",
            "content": "In this section, we provide more details on the dataset summary and distribution of SWEFFICIENCY. We verify that all repositories used have permissive licenses, allowing for data mining and inclusion into the SWE-FFICIENCY benchmark, as shown in Table 4. Figure 10 and 11 show additional details on the distribution of task instances, repositories, and corresponding information. We observe that compared to SWE-bench, SWE-FFICIENCY gold patches are larger on average and have significantly larger numbers of related tests (as checking for correctness regression is stricter than SWE-benchs pass criteria of issue resolution). We also note that SWE-bench does not contain any code optimization tasks as noted in Table 1. Firstly, as we select for changes that do not introduce test file changes, this disqualifies any of SWEbench instances from passing Stage 2 of Figure 2 (attribute filtering). Furthermore, we also run SWE-bench instances through our performance keyword pipeline and randomly sample 50 of the 581 resulting instances for human review: all of these instances had issue statements or hints text that clearly show the change introduces new behavior (which is tested by the SWE-bench instances test_patch). B.1 REPOSITORY DESCRIPTIONS AND PERMISSIVE LICENSES All SWE-FFICIENCY task instances are scraped from public GitHub repos with permissive licenses via the public GitHub API as stated in our Ethics Statement. Repository specific licenses are shown in Table 4, where links to custom licenses are provided inline. Table 4: SWE-FFICIENCY GitHub repositories, package description, and their permissive licenses. Repository Description astropy/astropy dask/dask matplotlib/matplotlib numpy/numpy pandas-dev/pandas pydata/xarray scikit-learn/scikit-learn scipy/scipy sympy/sympy Astronomy and astrophysics core library Parallel computing library for analytics Plotting and graphics library Core scientific computing library Core data analysis library Multi-dimensional array library Machine learning in Python Package for math, science, and engineering Computer algebra system written in Python License BSD-3-Clause BSD-3-Clause Custom Custom BSD-3-Clause Apache 2.0 BSD-3-Clause BSD-3-Clause Custom B.2 EXAMPLE PERFORMANCE WORKLOADS We provide some examples of the performance workloads associated with each task instance. To recap, each performance workload consists of (i) necessary imports, (ii) an optional setup function,"
        },
        {
            "title": "Preprint",
            "content": "which sets up work that should not be runtime benchmarked, (iii) workload function, which runs some repository functionality and runtime to be measured, and (iv) performance measurement harness code. Each problem runs the workload and setup multiple times to generate distribution of runtimes, from which we compute the mean and standard deviation. During our dataset curation pipeline, we reject task instances and workloads that fail to show statistically significant improvements in the execution validation stage (Appendix C.5). Performance Workload for dask__dask-6293 import timeit import statistics import dask import dask.array as da import numpy as np def setup(): global stacked, sub_arrays sub_arrays = [ da.from_delayed( dask.delayed(np.zeros)((100000,), dtype=\"int64\"), shape=(100000,), dtype=\"int64\", name=idx ) for idx in range(10000) ] stacked = da.stack(sub_arrays) def workload(): global stacked, sub_arrays for in range(len(sub_arrays)): stacked[i] runtimes = timeit.repeat(workload, number=1, repeat=5, setup=setup) # Print runtime mean and std deviation. print(\"Mean:\", statistics.mean(runtimes)) print(\"Std Dev:\", statistics.stdev(runtimes)) Performance Workload for numpy__numpy-11720 import timeit import statistics import numpy as np = np.random.random((5, 2)) = np.random.random((5, 5, 2)) = np.random.random((2, 5)) def workload(): out = np.einsum('ij,ixy,ji->xy', b, t, p) runtimes = timeit.repeat(workload, number=100, repeat=10**4) # Print runtime mean and std deviation. print(\"Mean:\", statistics.mean(runtimes)) print(\"Std Dev:\", statistics.stdev(runtimes))"
        },
        {
            "title": "Preprint",
            "content": "Performance Workload for scipy__scipy-12001 import timeit import statistics import numpy as np from scipy.stats import maxwell def setup(): global data data = maxwell.rvs(loc=2.0, scale=3.0, size=100000, (cid:44) random_state=42) def workload(): global data _ = maxwell.fit(data) runtimes = timeit.repeat(workload, number=1, repeat=200, setup=setup) print(\"Mean:\", statistics.mean(runtimes[-100:])) print(\"Std Dev:\", statistics.stdev(runtimes[-100:])) B.3 LM CLASSIFICATION OF GOLD PATCH EDIT TYPES To examine the diversity of gold (expert) patch optimizations that SWE-fficiency submissions are graded against (rightmost plot in Figure 3), we prompt GEMINI 2.5 FLASH with the following task prompt to extract optimization categories. We then randomly sampled 50 LM classifications for manual review and confirmed the high level categorization and explanation for each. We emphasize that during SWE-FFIENCY evaluation, LM agents are free to make any optimization desired to improve the performance workload, and we use this categorization strictly to show the diversity of our collected dataset."
        },
        {
            "title": "Performance Diff Classification Task Prompt",
            "content": "You are an excellent performance engineer. Given code diff and an (cid:44) affected performance workload that shows speedup as result of this diff, output single high-level performance bucket, the concrete signals from the diff that justify that bucket, mechanism-level explanation of **why the specific code edits** improve performance, and confidence score. Prefer software-side mechanisms; ignore hardware/microarchitecture unless explicitly cited. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) ## Inputs * Performance workload and code diffs. ## Buckets (choose **exactly one** `classification`) 1. **Algorithmic / Data Structure Improvements** -- Better asymptotic complexity or more suitable data structures; removes redundant (cid:44) passes. (cid:44) 2. **Memory Efficiency & Management** -- Fewer allocations/copies; pooling/reuse; layout/locality changes; alignment; `reserve()`; (cid:44) SoA/AoS. (cid:44) 3. **Concurrency & Parallelism** -- Threading/async; work (cid:44) 4. **I/O and Storage Efficiency** -- Fewer syscalls; (cid:44) partitioning; lock scope/structure; atomics; SIMD/vectorization. buffering/batching; async I/O; (de)serialization changes; payload trimming. (cid:44) 5. **Code Simplification / Dead-Code Elimination** -- Early exits; pruning logging/asserts on hot paths; removing unnecessary (cid:44) work/branches. (cid:44)"
        },
        {
            "title": "Preprint",
            "content": "6. **Compiler / Build / Low-level Tuning** -- Flags (LTO/PGO), (cid:44) inlining hints, intrinsics, UB fixes enabling optimization, branch hints. (cid:44) 7. **Configuration / Parameter Tuning** -- (cid:44) Constants/thresholds/buffer sizes, thread-pool/GC settings, feature flags altering performance behavior. (cid:44) 8. **Caching & Reuse** -- Memoization, caches, reuse of precomputed artifacts/results, avoiding repeated expensive calls. (cid:44) 9. **Unknown / Not Enough Information** -- Claimed speedup but (cid:44) mechanism not inferable from available changes. ## Secondary Tags (optional) Efficiency & Management\" and \"Caching & Reuse\" could both apply). * Other relevant buckets or keywords, if any (e.g., \"Memory (cid:44) * These can be more specific, e.g., \"memoization\", \"lock-free\", (cid:44) \"buffered I/O\", but try to use standard terms where possible. ### Disambiguation rules * **Algorithmic vs Caching**: If an algorithm was fundamentally (cid:44) changed and cache was added as helper, choose **Algorithmic**; add `memoization` in `mechanism_signals`. (cid:44) * **Concurrency vs Algorithmic**: If parallelism is added without changing the algorithm, choose **Concurrency & Parallelism**. (cid:44) * **I/O vs Memory**: If copies were removed primarily to cut syscalls (cid:44) or shrink payloads, choose **I/O**; if focused on allocation/locality/pressure, choose **Memory**. (cid:44) * **Compiler/Build**: Choose **Compiler / Build** when source logic (cid:44) * **Benchmark-only**: Choose **Workload/Benchmark-Only** when only (cid:44) is the same but flags/hints/toolchain changed. harness/warmup/affinity/timers changed. ## What to extract as \"signals\" Short, concrete phrases tied to the diff, e.g.: allocation, SoA layout lock-free queue, SIMD intrinsics loop, added binary search, streaming parse * containers/algos: `vector``unordered_map`, removed nested (cid:44) * memory: added `reserve()`, object pool, moved to stack (cid:44) * concurrency: introduced thread pool, reduced lock scope, (cid:44) * I/O: batched writes, buffered reader, protobufflat (cid:44) * simplification: early return before parse, pruned logging on hot (cid:44) * compiler/build: enabled LTO/PGO, added `inline`/`cold`/`likely`, UB fix unblocking vectorization (cid:44) * config: increased read buffer to 1MB, thread pool size = cores * caching: added LRU cache, memoized function result * meta: only bench harness changed serialization, compression level tuned, fewer syscalls path, deleted dead branch ## Output Requirements (STRICT) fences. * Output **only** the JSON object -- no prose, no Markdown, no code (cid:44) * Keep `explanation` <= 6 sentences and tie it to specific (cid:44) * If evidence is weak or ambiguous, use `classification: \"Unknown / (cid:44) Not Enough Information\"` and lower `confidence`. lines/files/patterns from the diff. ### JSON Schema"
        },
        {
            "title": "Preprint",
            "content": "```json { if any>\"], justify the classification>\"], \"classification\": \"<one of the 10 buckets>\", \"secondary_tags\": [\"<optional: other relevant buckets or keywords, (cid:44) \"mechanism_signals\": [\"<short phrases pulled from the diff that (cid:44) \"affected_components\": [\"<files/modules/functions inferred from (cid:44) \"explanation\": \"<mechanism-level rationale grounded in the diff: (cid:44) what changed, how it reduces work/contention/latency/allocs/syscalls, and why that maps to the chosen bucket>\", paths/symbols>\"], (cid:44) (cid:44) \"confidence\": \"<highmediumlow>\" } ``` ## Final sanity check (do this before emitting JSON) performance mechanism? 1. Have picked **exactly one** bucket that best explains the (cid:44) 2. Do my `mechanism_signals` cite concrete code changes from the diff (cid:44) 3. Is the explanation mechanism-centric and grounded in the edits (cid:44) that motivate that bucket? (not benchmarks)? --- ### Mini-examples **A. Algorithmic / Data Structure Improvements** ```json { unordered_set\", \"added reserve() to avoid rehash\"], \"classification\": \"Algorithmic / Data Structure Improvements\", \"secondary_tags\": [\"asymptotic complexity\"], \"mechanism_signals\": [\"removed nested O(n^2) scan\", \"introduced (cid:44) \"affected_components\": [\"src/import/dedupe.cpp\", (cid:44) \"explanation\": \"The patch replaces quadratic duplicate search (cid:44) \"Importer::dedupeRecords\"], with hash-based membership checks and preallocates the table to avoid rehash, removing repeated comparisons across the hot loop.\", (cid:44) (cid:44) \"confidence\": \"high\" } ``` **B. Concurrency & Parallelism** ```json { partitioning\", \"narrowed mutex scope\"], \"classification\": \"Concurrency & Parallelism\", \"secondary_tags\": [\"parallelism\", \"lock contention\"], \"mechanism_signals\": [\"introduced thread pool\", \"tile-based work (cid:44) \"affected_components\": [\"decoder/pipeline.cc\", \"decoder/tiler.cc\"], \"explanation\": \"Work is partitioned by tile across shared pool and critical sections are reduced to short regions, lowering (cid:44) contention and enabling parallel execution of the same algorithm.\", (cid:44) \"confidence\": \"high\" (cid:44) } ```"
        },
        {
            "title": "Preprint",
            "content": "**C. Unknown** ```json { edits\"], \"classification\": \"Unknown / Not Enough Information\", \"secondary_tags\": [], \"mechanism_signals\": [\"broad refactor\", \"no evident hot-path (cid:44) \"affected_components\": [\"loader/*\"], \"explanation\": \"Large refactor touches many files without showing (cid:44) hot-path changes or recognizable performance mechanisms, so the cause of any improvement cannot be determined from the diff alone.\", (cid:44) (cid:44) \"confidence\": \"low\" } ``` B.4 DATASET SCHEMA For clarity, we describe the schema and description of our dataset in Table 5. We upload our dataset to HuggingFace (swefficiency-anon/swefficiency) for easy community download, and refer readers to our code supplementary material to see how the dataset is directly used during evaluation."
        },
        {
            "title": "C ADDITIONAL DETAILS ON DATA COLLECTION PROCEDURE",
            "content": "In this section, we provide more concrete details on the dataset collection procedure explained in Section 2.1 and in Figure 2. To recap, our dataset pipeline is comprised of the following stages (with numbers on the yield after each stage and direct references to appendix subsections): 1. Scrape pull requests from nine widely used open-source Python repositories in data science, machine learning, and high-performance computingchosen for mature test suites and stringent performance requirementsyielding 96457 PRs (Appendix C.1). 2. Filter for candidate performance-regression instances by requiring performance-related keywords, excluding PRs that modify tests (and introduce new behavior), and retain only edits that meaningfully change the abstract syntax tree (AST) criteria that notably targets instances intentionally excluded by SWE-bench due to its test-change filter. After attribute filtering and prior to checking for meaning full changes to the AST, we retain 9257 PRs (-90.4%) at this stage (Appendix C.2). 3. Construct an executable Docker environment per instance with manually curated, versionpinned dependencies (tests are often version-sensitive), run repository unit tests, and use line coverage to keep only instances with at least one guarding\" test whose executed lines intersect the edited code. We retain 1041 PRs (-88.8%) up until this stage (Appendix C.3). 4. Annotate each surviving instance with minimal workload script that reliably exposes the pre/post performance delta (Appendix C.4). 5. Run correctness tests and the performance workload before/after applying the patch, retaining only instances with statistically significant improvement, recording post-patch test outcomes, and ensuring reproducibility via Docker with 4 CPU cores and 16GB RAM. This yields our final 498 tasks across 9 repos (Appendix C.5). C.1 REPO SELECTION AND INSTANCE SCRAPING We provide more details on Stage 1 from Figure 2 in how we selected repositories and scraping raw instance information. In this stage, using March 12th, 2025 as cutoff date, we scrape merged pull requests from nine (9) repositories (astropy, dask, matplotlib, numpy, pandas,"
        },
        {
            "title": "Preprint",
            "content": "Table 5: SWE-FFICIENCY dataset columns and description. Column Name repo instance_id base_commit patch created_at version environment_setup_commit workload test_cmd rebuild_cmd image_name covering_tests single_thread_tests PASS_TO_PASS Description (str) Repository identifier for the task (e.g., owner/repo on GitHub). (str) Unique ID for this dataset instance. (str) Git commit SHA to check out before applying any patches; defines the baseline state under evaluation. (str) Expert git patch with source-code changes which solves the task and shows performance optimization. For evaluation purposes, this should never be provided to the agent. (str) ISO-8601 timestamp indicating when this instance was created. (str) Repository version string for this instance (used for repository environment building). Commit SHA (or ref) that pins environment setup artifacts (e.g., dependency files) for reproducible evaluation. (str) Python workload script used for performance measurement (script/benchmark/entrypoint) See Appendix B.2 for examples.. Shell command prefix used to run the test suite for this repo (e.g., pytest -q). Shell command to (re)build/reinstall the project between runs for agent usage (e.g., pip install -e .) Container image tag/name providing the canonical evaluation environment (OS, toolchain, deps). (list of str) List of tests paths that exercise the changed code regions (e.g., from coverage or curated mappings). For evaluation purposes, this should never be provided to the agent. List of tests that must run serially (to avoid flakiness or resource contention) during evaluation. For evaluation purposes, this should never be provided to the agent. (list of str) List of test identifiers that pass after the expert edit and are expected to pass after an LM generated edit (regression guard). For evaluation purposes, this should not be provided to the agent."
        },
        {
            "title": "Preprint",
            "content": "scikit-learn, scipy, sympy, and xarray). Note that we also relax the SWE-bench requirement that an valid PR require linked GitHub issue as problem statement: from our observation, we see that many performance optimization PRs are opportunistic and do not always have linked issue created ahead of time. We note that some repositories overlap with SWE-bench and SWE-Gym (Pan et al., 2025), such as astropy, matplotlib, xarray, scikit-learn, sympy, dask, and pandas, while others are exclusive to SWE-FFICIENCY like numpy and scipy. We examined all the repositories in SWE-bench and SWE-Gym to start and found that most of the repositories in those datasets that are not included in our benchmark contain very few performance related changes: This is due to some of those repositories focusing on general functionality rather than performance optimization specific changes. For example, packages like flask and django in SWE-bench focus on rapid iteration and high-level design rather than optimizing for high degrees of performance, which reflects in their list of merged pull requests (PRs) having very few occurrences of the word perf. C.2 PERFORMANCE REGRESSION ATTRIBUTE FILTERING As discussed in Section 2.1, we modify the attribute filtering from SWE-bench to prune away instances that are not regression-free performance optimization. Specifically, we keep an instance only if it satisfies the following: 1. Does not contribute test changes: We intentionally drop instances if they add test changes, since this indicates, with high likelihood, that new behavior is introduced by that PR. In contrast, SWE-bench selects for the opposite (intentionally selects for new tests, and masks out those tests for instance evaluation), meaning that our dataset is exactly instance-wise disjoint with both SWE-bench and SWE-gym. 2. Contains performance related keywords or tags: We check if the pull request metadata includes any of the following keywords: performance, speedup, speeds up, speed-up, speed up, faster, memory, optimize, optimization, profiling, accelerate, fast, runtime, efficiency, benchmark, latency, concurrency, concurrent, profiling, CPU usage, memory usage, resource usage, cache, caching, timeit, and asv. We also check if pull request has been tagged with any repo specific performance tags and keep those pull requests as well. multithreading, throughput, parallel, 3. PR contains meaningful changes to AST: We finally check that the PR edit has made meaningful changes to each changed files abstract syntax tree (AST) as parsed by tree-sitter. This helps us ignore no-op changes that are comment or doc-string only and select more specifically for substantial performance related changes, which are almost guaranteed to require modification to code files AST. C."
        },
        {
            "title": "IDENTIFYING COVERING CORRECTNESS TESTS",
            "content": "In this third stage, we retain task instance only if the repository installs with all required dependencies, if the tests execute properly, and if we can identify unit tests that intersect the PR diff via line coverage. Installation is usually the most brittle step: repo may install successfully yet fail at test time due to mismatched or missing dependencies. In practice, this requires manual curationpinning versions and resolving transitive constraintsto map each instance to working environment, beyond the constants provided by SWE-bench and SWE-Gym. Once tests run cleanly, we execute the full test suite with coverage enabled and record, for each test, the lines of each source file that are executed. For every test file, we align this dynamic coverage with the lines, functions, classes, and modules modified in the PR to determine whether the test intersects the change. We keep task instance only if at least one unit test intersects the original PR edit. Recall that SWE-bench selected PRs that introduced tests: since we intentionally select tests without test changes, this coverage step is required for us to identify guarding tests in the code repo. Because our correctness check targets performance edits intended to be semantics-preserving, any check violation must appear on executions that traverse the modified regions. We therefore restrict the necessary correctness tests to those whose coverage intersects the PR diff (aggregated across"
        },
        {
            "title": "Preprint",
            "content": "lines, functions, classes, and modules). This change-focused selection is conservative form of test-impact analysis: tests that never execute the modified code cannot surface regressions, yet they would inflate wall-clock time and noise in benchmark setting. Limiting evaluation to intersecting tests preserves detection power for performance regressions, reduces spurious failures from unrelated tests, and yields stable, low-cost runsmaking the benchmark practical for repeated use and community adoption. C.4 ANNOTATING PERFORMANCE WORKLOADS Given performance-related candidate task instances for which we can easily check that edits maintain correctness of code, we need way of also grading whether edits improve performance. We explored using an LM generated pipeline to generate workloads (see Appendix I), but found that manual annotation based on GitHub issue PR and issue metadata was more effective strategy and yielded more realistic workloads (i.e. the same workloads that PR authors used as baseline to implement their optimization edits). Thus, for each candidate task instance in this stage, we examine its linked GitHub pull request and issue info and generate workload script which shows performance delta: we double check these workloads in the next stage to verify the reproducibility and statistical significance of the performance improvements for benchmark inclusion. Each workload consists of four items: (i) required imports (including timeit and statistics for computing runtime distributions); (ii) an optional setup() function for initializing any parts of the performance workload that should not be measured for runtime; (iii) workload() function, which encapsulates the key functionality of interest to measure; and (iv) timing-specific code to run workloads multiple times to generate consistent runtime distributiosn for evaluation and analysis. See Appendix B.2 for examples of performance workloads and Appendix for detailed workload creation instructions. Notably, we find that automatically extracting workloads that show the performance delta from PR information is difficult. For example, pandas PRs #43274, #49596, #59608 each contain at least one (of many) codeblocks with performance script from the PR author, showing the intended performance delta. However, note that each block uses different format, timing mechanism, and method of executing programs, as shown in Figures 12, 13, 14. Unifying these consistently without hugely reducing dataset yield is non-trivial, as shown in other works like GSO (Shetty et al., 2025). We leave LM-pipeline based approaches to automate this extraction to future work. PR Performance Script Codeblock for pandas-dev__pandasIn [1]: from asv_bench.benchmarks.indexing import InsertColumns In [2]: self = InsertColumns() In [3]: self.setup() In [4]: %timeit self.time_assign_with_setitem() 27.6 ms 68.1 µs per loop (mean std. dev. of 7 runs, 10 loops (cid:44) each) #master In [4]: %timeit self.time_assign_with_setitem() 8.6 ms 6.43 µs per loop (mean std. dev. of 7 runs, 100 loops (cid:44) each) #PR Figure 12: This PR uses existing asv benchmarks in the repository and measures performance improvement with the timeit command line entrypoint. PR Performance Script Codeblock for pandas-dev__pandas-49596 import pandas as pd import pandas._testing as tm"
        },
        {
            "title": "Preprint",
            "content": "vals = pd.Series(tm.rands_array(10, 10**6), dtype=\"string\") df = pd.DataFrame({\"cat\": vals.astype(\"category\")}) %timeit df.groupby(\"cat\").size() <- main 1.21 4.71 ms per loop (mean std. dev. of 7 runs, 1 loop each) (cid:44) 15.1 ms 274 µs per loop (mean std. dev. of 7 runs, 100 loops (cid:44) each) <- PR Figure 13: This PR uses bespoke workload (non asv benchmark and measures specific functionality, again using timeit to measure speedup. PR Performance Script Codeblock for pandas-dev__pandas-59608 import pandas as pd import pyarrow as pa import pyarrow.csv as csv import time NUM_ROWS = 10000000 NUM_COLS = 20 # Example Multi-Index DataFrame df = pd.DataFrame( { } f\"col_{col_idx}\": range(col_idx * NUM_ROWS, (col_idx + 1) * (cid:44) for col_idx in range(NUM_COLS) NUM_ROWS) ) df = df.set_index([\"col_0\", \"col_1\"], drop=False) # Timing Operation start_time = time.time() df.to_csv(\"file_A.csv\", index=False) end_time = time.time() print(f\"Operation time: {end_time - start_time} seconds\") # Timing Operation start_time = time.time() df_reset = df.reset_index(drop=True) df_reset.to_csv(\"file_B.csv\", index=False) end_time = time.time() print(f\"Operation time: {end_time - start_time} seconds\") Figure 14: This PR uses both bespoke workload and non-timeit, Python timing functionality to measure speedup. C.5 EXECUTION-BASED FILTERING We finally verify task instances by (i) executing and collecting their after-gold-edit test statuses and (ii) verifying that performance optimizations are statistically significant. Each annotated workload script contains workload() function and measurement harness to run repeated number of iterations, generating distribution of runtimes with both mean and standard deviation (pre-edit as µpre and σpre and µpost and σpost). We filter away any instances where µpre µpost 2σpost (i.e. runtime speedup is larger than two post-edit runtime standard deviations). In this stage we also run correctness tests ten times to filter out any possible flaky tests, as those would cause our aggregated speedup ratio to be lower than the actual value."
        },
        {
            "title": "D TECHNIQUES FOR IMPROVING PERFORMANCE REPRODUCIBILITY",
            "content": "This section describes two implementation choices we use in our benchmark to reduce incidental variability in measured runtime and throughput: (i) prebuilding Docker containers so that environment resolution and installation never occur on the critical path of an evaluation run, and (ii) CPU pinning that separates container execution from Docker management daemons and assigns containers to non-overlapping groups of logical cores. D.1 PREBUILDING INSTANCE DOCKER IMAGES We containerize each benchmark task and prebuild the corresponding Docker images prior to any timed evaluation (uploading it to public Docker image registry). Thus, evaluation runs start from fully built image; they do not perform package installation, environment resolution, or other setup work that would otherwise consume CPU cycles and introduce run-to-run variance. This design ensures that the CPU resources measured during evaluation are dedicated to the containerized program and harness rather than to container initialization. It also makes parallel execution more stable: because images are prepared ahead of time, concurrent workers do not contend for CPU due to onthe-fly dependency installation or environment setup. We provide these scripts in our code artifact release. D.2 PINNING CONTAINERS AND DOCKER DAEMON TO CPU CORES We implement CPU-affinity policy that (a) assigns containers to disjoint groups of logical cores and (b) reserves separate set of physical CPUs for Dockers background services. The policy proceeds as follows. Grouping logical cores. Since each instance is evaluate on 4 vCPUs and 16GB of RAM, we first identify the logical-core (vCPU) topology and partition the available vCPUs into groups of four (4), with the constraint that no two vCPUs in the same group share physical core. This grouping helps reproducibility because cache lines are generally isolated per physical core (and thus isolated between groups of 4 vCPUs), so execution within one group is more insulated from core-level contention within that group. Isolating Docker management. We pin the Docker daemon and containerd to dedicated set of physical CPUs (and their corresponding logical cores) that is disjoint from (cid:83) Gi. As result, containerand image-management activity (e.g., image downloading and setup) is confined to these reserved CPUs and cannot steal cycles from the cores executing benchmark containers. This separation allows us to parallelize workers across multiple groups Gi without coupling their performance to background Docker activity. Memory limits and NUMA node binding. In addition to CPU affinity, we restrict memory on per-container basis, allowing each container to only consume 16GB and assign each container to use the NUMA (Non-Uniform Memory Access) memory node corresponding to the physical cores of the vCPUs that the container is assigned to. This bounds each workers memory footprint and makes memory allocation more predictable and reduces cross-container interference due to hostlevel memory pressure, complementing the CPU isolation described above."
        },
        {
            "title": "E AGENT HARNESS PROMPTS AND DETAILS",
            "content": "This section describes the prompt and harness specific details used to generate our evaluation results in Section 4. More details can also be found in our attached code artifact. E.1 CODE OPTIMIZATION TASK PROMPTS The prompt provided to OPENHANDS and SWE-AGENT is provided below, asking an LM agent to optimize specific workload given repository, file utilties, and bash execution abilities in containerized environment. Note that the agent is also given the commands for (1) rebuilding/reinstalling the repository and (2) the generic prefix command for running an arbitrary unit-test file."
        },
        {
            "title": "Code Optimization Task Prompt",
            "content": "<uploaded_files> {{working_dir}} </uploaded_files>"
        },
        {
            "title": "Preprint",
            "content": "Ive uploaded python code repository in the directory (cid:44) workspace_dir_name. Consider the following python workload showing specific usage and measured performance of the repository: (cid:44) (cid:44) <performance_workload> {{workload}} </performance_workload> Can you help me implement the necessary changes to the repository so that the runtime of the `workload()` function is faster? Basic (cid:44) guidelines: (cid:44) (cid:44) 1. Your task is to make changes to non-test files in the /workspace directory to improve the performance of the code running in (cid:44) `workload()`. Please do not directly change the implementation of the `workload()` function to optimize things: want you to focus on making the workload AS IS run faster by only editing the repository containing code that the `workload()` function calls. (cid:44) (cid:44) (cid:44) 2. Make changes while ensuring the repository is functionally (cid:44) equivalent to the original: your changes should not introduce new bugs or cause already-passing tests to begin failing after your changes. However, you do not need to worry about tests that already fail without any changes made. For relevant test files you find in the repository, you can run them via the bash command `{{test_cmd}} <test_file>` to check for correctness. Note that running all the tests may take long time, so you need to determine which tests are relevant to your changes. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 3. Make sure the `workload()` function improves in performance after you make changes to the repository. The workload can potentially (cid:44) take some time to run, so please allow it to finish and be generous with setting your timeout parameter: for faster iteration, you should adjust the workload script to use fewer iterations. Before you complete your task, please make sure to check that the **original performance workload** and `workload()` function runs successfully and the performance is improved. (cid:44) (cid:44) (cid:44) (cid:44) (cid:44) 4. You may need to reinstall/rebuild the repo for your changes to take effect before testing if you made non-Python changes. (cid:44) Reinstalling may take long time to run, so please be patient with running it and allow it to complete if possible. You can reinstall the repository by running the bash command `{{rebuild_cmd}}` in the workspace directory. (cid:44) (cid:44) (cid:44) 5. All the dependencies required to run the `workload()` function are (cid:44) already installed in the environment. You should not install or upgrade any dependencies. (cid:44) Follow these steps to improve performance: 1. As first step, explore the repository structure. 2. Create Python script to reproduce the performance workload, (cid:44) execute it with python <workload_file>, and examine the printed output metrics. (cid:44) 3. Edit the source code of the repository to improve performance. Please do not change the contents of the `workload()` function (cid:44) itself, but focus on optimizing the code in the repository that the original `workload()` function uses. (cid:44) (cid:44)"
        },
        {
            "title": "Preprint",
            "content": "4. If non-Python changes were made, rebuild the repo to make sure the (cid:44) changes take effect. 5. Rerun your script to confirm that performance has improved. 6. If necessary, identify any relevant test files in the repository (cid:44) related to your changes and verify that test statuses did not change after your modifications. (cid:44) 7. After each attempted change, please reflect on the changes (cid:44) attempted and the performance impact observed. If the performance did not improve, consider alternative approaches or optimizations. (cid:44) (cid:44) 8. Once you are satisfied, please use the finish command to complete (cid:44) your task. Please remember that you should not change the implementation of the `workload()` function. The performance improvement should solely (cid:44) come from editing the source files in the code repository. (cid:44) E.2 DETAILS ON LANGUAGE MODEL SAMPLING PARAMETERS We elaborate on the evaluation settings discussed in 3. For all models, we perform the recommended greedy sampling within the OpenHands and SWE-agent harnesses. For GPT-5 MINI, we sample at temperature of = 1 (as mandated by the API as of August 2025) and at = 0 for all other models. In the SWE-agent setting, we enforce token spending limit of $1, meaning that, in addition to the 100-turn action limit and time-limits specified, evaluation runs per-instance are stopped when (API/token-spending) cost is exceeded, and their patches as of that last action are immediately submitted (as is common practice with cost-limited SWE-agent runs on SWE-bench). We believe this lower-resource, cost-constrained setting is important from an efficiency standpoint of eventually yielding systems that can solve optimization tasks at reasonable dollar costs. In the OpenHands setting, our results only have the action count and time limits enforced. We also provide links to our forks of those agent harnesses for evaluation, which will also be merged upstream with corresponding harness libraries for community reproducibility."
        },
        {
            "title": "F ADDITIONAL DETAILS ON MAIN EVALUATION RESULTS",
            "content": "F.1 BENCHMARK PERFORMANCE FULL RESULTS We provide full versions of evaluation results below for both OPENHANDS and SWE-AGENT below in Tables 6 and 7. Recall that for both harnesses, we set 3 hour wall-clock time limit and 100 turn interaction limit: in the SWE-AGENT case, we also limit LMs to maximum cost of $1. F.2 HOW DOES LM PERFORMANCE SCALE WITH DATASET DIFFICULTY? In addition to the bucketed trends shown in Figure 4, Figure 15 shows curves demonstrating how models perform on our dataset as we increasingly include tasks across the same three dimensions of (i) pre-edit workload duration, (ii) number of lines modified in the gold (expert) patch, and (iii) speedup factor achieved by the gold patch. We notice that as increased difficulty tasks are included, speedup ratio across the dataset decreases, further indicating that LMs are achieving easier wins on lower difficulty problems but struggling at higher difficulties. F.3 EXAMINING MORE EXPENSIVE REASONING MODELS: COMPARING GEMINI 2.5 PRO VS. FLASH We initially (as of September 19th 2025) could not run full benchmark results on full reasoning models like GPT-5, OPUS 4.1, and GEMINI 2.5 PRO due to budget and runtime limitations: runs would cost significant amount and also take much longer per inference call (even with parallel requests) to reasonably complete in time. Instead, we share results from selected subset of 100 SWE-FFICIENCY problems, designated as SWE-FFICIENCY LITE. For this subset, we sample to be representative with respect to pre-edit workload runtime, gold patch speedup, and number of lines in gold patch from the distributions in"
        },
        {
            "title": "Preprint",
            "content": "Table 6: SWE-FFICIENCY results across several frontier models (higher is better; human-expert speedup ratio (SR) is 1.0). SR is pass@1: each system submits single patch per instance to be evaluated. SR is calculated by computing the speedup from the LM-generated edit, normalized by the speedup from the gold (human-written) patch, and aggregated across all tasks via harmonic mean. System Expert GPT-5 (OPENHANDS) CLAUDE 4.1 OPUS (OPENHANDS) QWEN3 CODER PLUS (OPENHANDS) CLAUDE 3.7 SONNET (OPENHANDS) CLAUDE 4.5 SONNET (OPENHANDS) GLM 4.6 (OPENHANDS) GPT-5 MINI (OPENHANDS) KIMI K2-0905 (OPENHANDS) GEMINI 2.5 FLASH (OPENHANDS) DEEPSEEK V3.1 (OPENHANDS) GEMINI 2.5 PRO (OPENHANDS) CLAUDE 3.7 SONNET (SWE-AGENT) GPT 5 MINI (SWE-AGENT) GEMINI 2.5 FLASH (SWE-AGENT) Speedup Ratio 1.0 0.150 0.098 0.064 0.047 0.041 0.026 0.019 0.008 0.008 0.007 0.007 0.041 0.026 0.006 Table 7: Distribution of patch outcomes by system. Passes correctness tests denotes functional correctness only (and not necessarily performance-optimal). System GPT-5 CLAUDE 4.1 OPUS QWEN3 CODER PLUS (OPENHANDS) CLAUDE 3.7 SONNET (OPENHANDS) CLAUDE 4.5 SONNET (OPENHANDS) GLM 4.6 GPT 5 MINI (OPENHANDS) KIMI K2-0905 (OPENHANDS) GEMINI 2.5 FLASH (OPENHANDS) DEEKSEEK V3.1 (OPENHANDS) GEMINI 2.5 PRO (OPENHANDS) CLAUDE 3.7 SONNET (SWE-AGENT) GPT 5 MINI (SWE-AGENT) GEMINI 2.5 FLASH (SWE-AGENT) Passes Correctness Tests Fails Tests () Slower than Pre-edit () Faster than Pre-edit () Faster than Expert () 18.3% 15.2% 22.5% 34.7% 18.7% 32.7% 45.2% 38.2% 39.2% 19.5% 41.8% 39.6% 25.5% 44.2% 4.4% 4.0% 11.0% 12.7% 4.7% 12.8% 14.7% 6.4% 14.5% 17.9% 17.7% 8.6% 11.5% 12.0% 31.5% 42.8% 42.0% 43.8% 32.1% 38.1% 26.5% 36.6% 34.1% 44.2% 32.9% 27.5% 35.7% 35.7% 45.8% 38% 24.5% 32.9% 20.7% 16.3% 13.9% 19.3% 12.7% 18.5% 7.6% 24.7% 27.7% 8.4% Figure 3. Specifically, we construct small, distribution-matched lite\" split by log-spacing each difficulty metric into bins and assigning instances to bins. We allocate per-metric quota for target size = 100 in proportion to each bins population and sample without replacement from those bins (using fixed random seed). We take the union across metrics to cover diverse regions of each marginal distribution and top up any remaining slots by sampling from the unsampled pool with weights inversely proportional to the density of each instances 3-way bin signature, which promotes rare metric combinations and preservess joint structure. If the union overshoots = 100, we trim instances uniformly at random until we reach the desired amount. In Table 8, we see that GEMINI 2.5 PRO performs similarly to its medium-compute counterpart GEMINI 2.5 FLASH on SWE-FFICIENCY LITE, while being more than 5 as expensive dollarwise and incurring an extra 2.74 total hours of inference latency. This suggests that more-expensive state-of-the-art reasoning models also still heavily struggle on SWE-FFICIENCY and larger, agentic"
        },
        {
            "title": "Preprint",
            "content": "Figure 15: LMs achieve easier wins on lower difficulty problems, but struggle as higher difficulty tasks are included across multiple definitions\" of difficulty. For each difficulty measure and each measure upper bound τ , we restrict to instances with difficulty measure τ and report the resulting aggregate speedup ratio, generating our curves shown. advances are needed to make models that reason more in-depth about repo-level performance and that can iterate in harnesses quickly. Table 8: SWE-FFICIENCY LITE results between GEMINI 2.5 PRO and GEMINI 2.5 FLASH (higher is better; human-expert parity is 1.0). Passes tests\" indicates passing functional correctness tests only. LM cost is total token spend (including prompt-caching). Inference latency is sum of total request latency over all requests. System Speedup Ratio Passes Tests LM Cost Inference Latency (hrs) GEMINI 2.5 PRO (OPENHANDS) GEMINI 2.5 FLASH (OPENHANDS) 0.008 0.007 60% 65% $509.52 $98. 9.03 6.29 PROFILING-BASED ATTRIBUTION AND COVERAGE METRICS In this section, we elaborate on how we computed the profiling and function localization results shared in Section 4 and Figure 6, which show that LMs often miss out on expert-level speedup due to function level mislocalization. What is function-level profiler? function-level profiler instruments program execution to record, for every function invocation, (i) the exclusive or self time spent in the function body (excluding callees), often called tottime, (ii) the inclusive or cumulative time spent in the function and its transitive callees, often called cumtime, (iii) call counts, and (iv) the callercallee relationships that induce directed call graph. In our setting we profile benchmark workload entry point before and after code edit, obtaining two traces per actor (expert vs. LLM). We compute all metrics only on instances that pass correctness checks (e.g., unit or regression tests), so any measured speedup does not come at the expense of functional correctness. G.1 DATA AND NOTATION Let denote the set of functions observed by the profiler. We uniquely identify function by its source file (g), line number ℓ(g), and name n(g). For an actor {Expert, LLM} and profiling phase {pre, post}, let τA,p(g) and TA,p(g) R0 denote the exclusive (tottime) and inclusive (cumtime) runtime attributed to g, respectively. We define per-function improvements (positive means faster) as tot (g) := τA,pre(g) τA,post(g), cum (g) := TA,pre(g) TA,post(g). Let denote the top-level entry point (workload); its end-to-end improvement for actor is δW := TA,pre(W ) TA,post(W )."
        },
        {
            "title": "Preprint",
            "content": "We additionally report whole-trace speedups normalized by pre-edit workload time,"
        },
        {
            "title": "SpeedupW",
            "content": "A := δW TA,pre(W ) ,"
        },
        {
            "title": "Speeduptot",
            "content": "A := (cid:80) τA,pre(g) (cid:80) TA,pre(W ) τA,post(g) . Call graph and depths. We first need to isolate the call graph (and function runtimes) that are strictly attributed to the workload function in each performance workload (and disregard function runtimes from any other source, like the setup function). From the pre-edit profile we build directed call graph = (G, E) whose edges point from caller to callee. We define the workload depth d(g) as the minimum caller distance from any node named workload to in C; nodes not reachable from workload have undefined depth. Depths are used only for selection and the diagnostic depth metric in Appendix G.5. Patch-based file filters. We parse unified diffs to extract modified files for each actor and restrict candidate functions to those files. This ensures we attribute improvements to edited regions and reduces noise from unrelated code. G.2 SELECTING CORE FUNCTION-LEVEL IMPROVEMENTS WITHOUT DOUBLE COUNTING Naively summing cum over functions double-counts speedups because callers inclusive time subsumes callee improvements. We therefore select deepest non-overlapping set of improved functions for each actor via depth-aware greedy procedure. Thresholding and candidates. We set per-instance absolute threshold θ := max(cid:0)θsec, θfrac δW Expert (cid:1), where θsec is an absolute time floor (seconds) and θfrac is fraction of the experts end-to-end improvement (default 0.02). Candidates functions are defined below where is the node corresponding to the workload function entry point (see B.2 for workload example): CA := cum (g) > 0, cum (g) θ, : is reachable from W, andf (g) was edited by . cum (g) denote the total positive mass among candidates. Let S+ := (cid:80) gCA (g)/ max(TA,pre(W ), ε), then (iii) larger cum Greedy deepest-first selection. We sort candidates by (i) larger depth d(g) first, (ii) larger share cum (g) (ties broken arbitrarily), and greedily build set EA CA such that no selected function is an ancestor (caller, transitively) of another selected function in the pre-edit call graph. Intuitively, we select set of functions closest to the scope of the speedup (i.e. the function scope where the speedup has the most significant percentage improvement over that time scope). For example, speedup could occur in function but is called by B, which is then called by A: would show the largest percentage speedup relative to the total amount of time spent in that function, since and have other runtime overhead that was not optimized. We stop when the accumulated mass reaches configurable cap ρ (0, 1]: (cid:88) gEA cum (g) ρ S+ (default ρ = 1). If the procedure would select nothing, we include the single best candidate. This yields our set of functions EExpert and ELLM. G.3 EXPERT-RELATIVE COVERAGE (ERC) AND LOSS DECOMPOSITION We measure how well the LLMs edits localize to the same places of improvement as the expert. Let Φ(S) := { (g) : } map set of functions to its set of files. To define the experts attribution mass we (i) keep only functions above threshold and (ii) restrict to files the expert actually optimized (to guard against spurious activity in unrelated files):"
        },
        {
            "title": "MExpert",
            "content": ":= (cid:8) : cum Expert(g) θ, (g) Φ(EExpert) (cid:9). Define per-function expert mass sexp(g) := cum Sexp := (cid:80) sexp(g). Expert(g) for MExpert and 0 otherwise, and let"
        },
        {
            "title": "Preprint",
            "content": "Coverage at file and function granularity. We compute ERC at two granularities: (cid:80) (cid:80) ERCfile := ERCfunc := (cid:80) Φ(ELLM) gMExpert:f (g)=f sexp(g) Sexp , sexp(g) gELLM Sexp . Intuitively, ERCfile asks did the LLM edit the right files? while ERCfunc asks did it optimize the right functions within those files? Loss decomposition. We decompose the portion of expert mass not captured at the function level into two orthogonal failure modes: WrongFileLoss := 1 ERCfile, InFileLoss := max(cid:8)0, ERCfile ERCfunc (cid:9). This yields tight partition of expert mass: WrongFileLoss + InFileLoss + ERCfunc = 1, where WrongFileLoss captures file-selection mistakes and InFileLoss captures localization mistakes within the right files (e.g., editing non-bottleneck functions). G.4 EDITED-FILE OVERLAP We also report the Jaccard similarity of edited files between actors: (cid:12) (cid:12)Φ(EExpert) Φ(ELLM)(cid:12) (cid:12) (cid:12)Φ(EExpert) Φ(ELLM)(cid:12) (cid:12) (cid:12) Jaccard := . When the union is empty (neither actor meets the selection threshold), the quantity is undefined and we omit it. G.5 DEPTH OF OPTIMIZATION FROM THE WORKLOAD As diagnostic we compute depth-of-optimization statistic with respect to the pre-edit call graph and the workload root. Let [x]+ := max{x, 0} and define weights wA(g) := [tot (g)]+ for EA. The weighted average workload depth and its coverage are dA := (cid:80) gEAreach(W ) wA(g) d(g) (cid:80) gEAreach(W ) wA(g) , ReachShareA := (cid:80) gEAreach(W ) wA(g) (cid:80) gEA wA(g) . dA reflects whether improvements concentrate near the entry point or deep in the call tree; ReachShareA indicates how much of the selected mass is reachable from the workload (should be close to 1 in well-instrumented runs). We use θsec = 0, θfrac = 0.02 (i.e., per-function floor at 2% of the experts end-to-end gain δW Expert to disregard speedups that are due to measurement noise), and cap ρ = 1.0, and we restrict candidate functions to edited files for each actor. The call graph used for depths and ancestry tests is always taken from the pre-edit trace to avoid post-edit structural confounds. Note that we only compute these statistics over instances that have passed functional correctness tests. Why cum for selection and tot for depth weights? We select by cum to capture inclusive speedups (including callee effects) while the depth statistic weights by tot to avoid doublecounting along chain. The deepest-first greedy constraint further prevents attributing the same improvement to both caller and its callee. Interpretation. High ERCfile with low ERCfunc indicates that the model navigated to the right files but failed to touch the expert-optimized functions (within-file localization gap). Low ERCfile indicates file-selection gap. Because Sexp is defined over expert-selected files above threshold, the metrics focus on where expert improvements actually occurred, rather than on unrelated noisy regions."
        },
        {
            "title": "Preprint",
            "content": "System ERCfile ERCfunc WrongFileLoss InFileLoss Jaccard (files) CLAUDE 3.7 SONNET (SWE-AGENT) CLAUDE 3.7 SONNET (OPENHANDS) GPT-5 MINI (OPENHANDS) GEMINI 2.5 FLASH (OPENHANDS) DEEPSEEK V3.1 (OPENHANDS) 0.630 0.611 0.551 0.549 0.519 0.298 0.314 0.278 0.265 0.246 0.370 0.389 0.449 0.451 0. 0.332 0.297 0.274 0.283 0.273 0.636 0.604 0.559 0.556 0.531 Table 9: Expert-Relative Coverage (ERC) and related losses. Means computed over instances that pass correctness and have speedup 1. File-overlap Jaccard is averaged over instances where it is defined. System Number Correct Instances Depthexp Depthllm CLAUDE 3.7 SONNET (SWE-AGENT) CLAUDE 3.7 SONNET (OPENHANDS) GPT-5 MINI (OPENHANDS) GEMINI 2.5 FLASH (OPENHANDS) DEEPSEEK V3.1 (OPENHANDS) 196 252 193 211 246 4.85 4.61 4.59 4.51 4.89 4.20 4.18 4.13 3.83 4.28 Table 10: Dataset size (n) and weighted average optimization depth from the workload entry point in the pre-edit call graph (expert vs. LLM). G.6 FULL SPEEDUP ATTRIBUTION RESULTS We compute results only over LM patches that passed correctness and achieve speedup from the pre-edit runtime (but not necessarily faster than the expert). Across systems, the ERC and loss metrics in Table 9 show consistent pattern: WrongFileLoss is roughly 37%45% across models (mean 41.5%), while InFileLoss is roughly 27%33% (mean 29.7%). Taken together, this implies that models choose the wrong function (either by editing the wrong file or the wrong function within the right file) about X+Y 69%73% of the time (mean 71.2%), consistent with ERCfunc 0.260.31. For context on number of instances analyzed per system and how deep optimizations occur in the call tree, Table 10 reports per system and the weighted average depth from the workload root (experts typically operate at call-stack depth 4.54.9, LLMs at 3.84.2)."
        },
        {
            "title": "H COMPARISON OF LM GENERATED EDITS VERSUS EXPERTS",
            "content": "We provide the raw diffs with in-line comments from Figures 8 and 9 below in Figures 16 and 17. We also include an additional diff for 18 comparison. In the main text, we removed comments and surrounding lines to focus only on the lines changed between expert and LM generated diff."
        },
        {
            "title": "I SYNTHETICALLY GENERATING PERFORMANCE WORKLOADS",
            "content": "We provide more details on our investigation on whether LMs can capably generate performance workloads, as discussed in the end of Section 4.2. Rationale. We generate LLM-based workloads to mirror our human curation process and test key hypothesis: when the gold patch is held fixed, workloads curated by expert annotators (our pipeline) expose larger, statistically reliable performance deltas than workloads produced by an LLM from the same evidence. Inputs per instance. For each SWE-FFICIENCY benchmark instance we use: The unified diff of the expert (gold) patch, The pre-edit source files corresponding to paths touched in the diff, repository and commit identifiers. Prompt construction. We parse the diff headers to identify touched files and fetch their pre-edit contents at the base commit. The model is given: 1. The full patch (pre/post diff), and 2. The concatenated pre-edit files for those paths"
        },
        {
            "title": "Preprint",
            "content": "Instruction parity with human annotators. We prompt the LLM (GEMINI 2.5 FLASH) with similar instructions as we used ourselves for workload annotation: produce self-contained Python workload script with setup() (realistic inputs; seeded randomness) and workload() (representative, non-trivial call path), executed via timeit.repeat(...), and printing exactly two linesmean and standard deviation. This parity isolates the effect of workload design quality, not interface differences. We provide the instruction prompt below."
        },
        {
            "title": "Synthetic Workload Generation Prompt",
            "content": "You are performance testing expert. You will be provided code edit as git diff and the pre-edit source files. You need to (cid:44) generate **self-contained Python performance workload script** that measures perfomance of code paths or APIs changed in the diff. (cid:44) (cid:44) (cid:44) Guidelines for the workload script contents. - Use `setup()` function to prepare any realistic, non-trivial data (cid:44) or environment needed for the test. arrays or easily optimizable patterns). - Data must be representative of real-world usage (avoid trivial (cid:44) - Prefer real datasets or realistic synthetic data with (cid:44) - All expensive or one-time setup (e.g., file download, (cid:44) preprocessing) must be in `setup()`, not in the workload. reproducibility (set random seed). - Use `workload()` function to run the actual operation(s) being (cid:44) timed. - The workload should reflect **representative and challenging real-world use case** of the API or library under test. (cid:44) - Avoid corner cases that could be trivially optimized. - Inputs should be varied enough to prevent caching or (cid:44) constant-folding from affecting results. - Run the benchmark using `timeit.repeat(workload, number=..., (cid:44) repeat=..., setup=setup)`. - `number` should match realistic single-run execution count (do (cid:44) - `repeat` should be high enough to gather stable statistics. not batch multiple runs for cumulative timing). - Print the mean and standard deviation of the last set of runtimes (cid:44) using `statistics.mean()` and `statistics.stdev()`. - Output should be clear and ready for performance comparison. - The output must be **complete Python script** containing only: 1. import statements 2. `setup()` function 3. `workload()` function 4. the `timeit.repeat()` call 5. mean/stddev printing The script should only print two lines at the end: the mean of (cid:44) measured runtimes and the standard deviation of runtimes. Example workload to follow (please strictly follow this format of (cid:44) imports, setup function, workload function, timeit call, and print statements). In particular, make sure the mean and standard deviation print statements are exactly as shown below. (cid:44) (cid:44) ```python import timeit import statistics import numpy as np"
        },
        {
            "title": "Preprint",
            "content": "def setup(): global arr np.random.seed(42) arr = np.random.rand(5000, 5000) def workload(): global arr _ = arr @ arr.T runtimes = timeit.repeat(workload, number=1, repeat=10, setup=setup) print(\"Mean:\", statistics.mean(runtimes)) print(\"Std Dev:\", statistics.stdev(runtimes)) ``` Here's commit and it's information that does some optimization in (cid:44) the {repo_name} repository that might be relevant to writing the test: (cid:44) ## Commit Diff: ``` {commit_diff} ``` ## Pre-edit source files: {pre_edit_code} Evaluation (holding the patch fixed). For each instance i, we evaluate both workloadsLLMgenerated and manually annotatedagainst the same code states: 1. Base: Repository at the pre-gold patch commit. 2. Patched: Repository with the gold patch applied. We then compare the magnitude of improvements (speedups) between the LM-generated (GEMINI 2.5 FLASH) workload and the manual workload for the same instance and compute if values are statistically significant and if the LM generated workload outperforms the annotated one. This ablation directly measures whether an LLM, given the same diff and file context and the same instructions as experts, can generate workloads that surface the patchs performance gains as reliably and strongly as manual curation. We see that, in 76% of cases, our manual annotations show larger performance delta than the LM generated workloads on the expert patch, with 47% of workloads showing non-significant performance delta at all."
        },
        {
            "title": "J PREVENTING MODEL REWARD HACKING",
            "content": "In this section, we outline two techniques implemented in our evaluation harness to prevent LM reward hacking behavior. The first, stack-frame-based reward hacking covers when models try to determine their caller (i.e. whether they are being called in performance runtime environment), while the second covers run-to-run caching, which is when models try to cache computations across evaluation runs to improve performance (but technically cheating, as wed like to evaluate specific workloads under non-cached conditions). J.1 PREVENTING STACK-FRAMEBASED REWARD HACKING Overview. We observed that some LLM-generated patches only speed up workloads when they detect they are being timed (i.e. under timeit or from being called from function with name workload, as is done in our benchmark). The common mechanism is Python stack introspection (e.g., inspect.currentframe(), traceback.extract_stack(), sys._getframe(), or reaching frame objects via f_back/tb_frame). These edits can shortcircuit code paths or memoize based on caller identity, inflating measured speedups without actually improving the underlying algorithm. In general, code changes (and in-particular performance improving edits) should never require caller identity information to improve performance. Other types of changes, such as tuning specif-"
        },
        {
            "title": "Preprint",
            "content": "ically based on input attributes (like size or data-type), are actually quite common in performance optimization PRs: we intentionally permit these changes in SWE-FFICIENCY. We verify that none of the expert PRs and gold patches use stackframe information: the one exception is pandas-dev__pandas-45247, which optimizes find_stack_level in pandas, utility to more readably show exception stackframes (and where the expert patch uses these utilities). Goal. We want to flag newly introduced stack-introspection logic in submitted patch while tolerating any pre-existing usage in the codebase (many mature projects legitimately use inspect/traceback during import/configuration) as well as making sure that expert/gold patches are not falsely flagged. Implementation. Our checker takes unified diff as input and analyzes only the post-image of files touched by that diff. It reports an error only if the diff adds lines that contain stack-introspection primitives. Existing occurrences are ignored by design. 1. Patch-scope extraction. We parse the unified diff to recover, for each touched file, the set of line numbers that are newly added on the + side. This yields map added_lines : path (cid:55) {new line numbers}. We also track which files are brand-new in the patch and the set of all post-image paths seen in +++ headers. 2. Standalone-new filtering. Brand-new files (introduced by the patch) that are not imported by any other file touched in the patch are treated as standalone\" and excluded from the check. This avoids flagging ad hoc scripts (e.g., local reproducer/benchmark drivers) that do not affect the library under test, as often LM systems might produce ad hoc scripts like these for debugging and introspection in scratchpad form (which may use some stackframe inspection). This lets us ignore scratch-pad like files introduced by patches, while still checking newly-created files that are imported and used in the repository. We determine referencedness\" by building lightweight import graph among touched files: (a) For each patch edited file, collect its imported module names via AST (both import and from import x). (b) For each brand-new file, derive candidate module names from its path (e.g., foo/bar/baz.py {baz, bar.baz, foo.bar.baz}). (c) Mark the new file as referenced if any other touched file imports one of its candidates (exact or dotted-suffix match). 3. Post-edit AST scan with alias resolution. For each edited file in added_lines, we read the post-edit source file. We parse each post-edit file source with ast and walk the tree once, collecting findings\" whenever the code contains introspection-like constructs. This scan is robust to renaming via an an AST-based import resolve, described below Imports: record module aliases (e.g., import inspect as ins) and function aliases (e.g., from inspect import currentframe as cf). Direct calls: resolve callee to (module, attribute) pair and match against denylist, which includes the following list: inspect.{currentframe, stack,getouterframes, getinnerframes,trace,getframeinfo,getsource, getsourcefile} traceback.{extract_stack,format_stack,print_stack, walk_stack}; sys.{_getframe,settrace,setprofile} gc.{get_referrers,get_objects} Dynamic imports: Detect dynamic imports such as __import__(inspect) and importlib.import_module(inspect). Frame-object attributes: flag attribute reads commonly used to climb or expose frames (.f_back, .tb_frame, .gi_frame, .cr_frame, .ag_frame), regardless of receiver type (conservative heuristic). 4. Added-line projection. After AST scanning, we project identified usages (and line number occurrences) onto the added-line set for that file and retain only those whose source line number is in added_lines[path]. This makes the check purely diff-relative: modifications that reuse pre-existing introspection do not fail the patch (as those usages are valid)."
        },
        {
            "title": "Preprint",
            "content": "5. Reporting. If any filtered findings remain (after standalone-new filtering and pragma suppression), patch fails the check and the LM generated patch is considered incorrect and fails correctness in our evaluation. We use AST parsing instead of regex/simple grep since simple regexes miss aliased imports and produce many false positives/negatives on strings or comments. Our AST pass is cheap, robust, and semantically aware: it resolves inspect aliases, identifies function calls regardless of whitespace/nesting, and recognizes dynamic imports. We verify that this mechanism (i) fails our previously identified LM generated patches that exploit stackframe info and (ii) passes our gold, expert edits (i.e. does not flag any false positives). J.2 PREVENTING RUN-TO-RUN CACHE REWARD HACKING Overview. Repeated measurements within single Python process allow moduleand processlocal state to leak across runs (e.g. module-level dictionaries, @lru_cache, global arrays, ad hoc memo tables). Such state can make later iterations appear faster without changing the true cost of the underlying algorithm. To make per-iteration results robust, we isolate runs so that no Python-level caches (or mutated globals) can persist between repetitions. Implementation. We transform our annotated workload scripts into an equivalent program that executes each timing repetition in fresh child process. Concretely, we (i) parse the original script with ast, (ii) preserve its logic and output formatting, and (iii) replace the in-process timeit.repeat loop with small harness built on multiprocessing using the spawn start method. The fork policy starts brand-new copied process for every repetition, guaranteeing module namespace and empty caches equivalent to the original parent run. This implementation allows us to provide simple scripts at inference time in problem statements to LM agents, while also being able to use those scripts as inputs to yield memory-isolated runtime scripts. 1. Locate the timing site. to runtimes = timeit.repeat(...) (or an equivalent import form), then extract the workload callable, optional setup callable, and the numeric number/repeat parameters. We also record if the script later slices the results (e.g., runtimes[-10000:]) so that summary statistics are computed over the same view."
        },
        {
            "title": "We walk the AST to find the",
            "content": "assignment 2. Preserve surrounding code. The transformer keeps all top-level declarations and statements except the original timeit.repeat assignment and the immediately following summary prints. Any statements that originally lived between those two points are preserved and either (i) executed after the isolated timing (if they are harmless post-processing) or (ii) moved into guarded finally block if they look like teardown of temporary files/directories (simple heuristic over os/shutil calls). 3. Fork-per-run harness. We synthesize minimal harness: child-side that setup=setup) and calls Timer.timeit(number), constructs function timeit.Timer(workload, top-level picklable target that runs the child once and returns the duration through multiprocessing.Queue. driver _run_isolated(number, repeat, start_method=\"fork\") that loops repeat times: for each iteration it creates new process/context, executes the child, checks the exit code, and appends the reported duration. We deliberately use fork so the child interpreter starts from the same memory state as the parent (with copy-on-write) such that any edits to parent memory objects, such as module level Python caches (including @lru_cache and ad hoc dictionaries) cannot carry over between repetitions. 4. Result and summary fidelity. After the harness returns the list of durations, we reconstruct the original slicing intent (if any) into runtimes_view and compute Mean and Std Dev exactly as in the input script. Any non-teardown statements that originally ran between the timing and the summary are executed afterward to preserve observable side effects. Writing the transformation allows us to guarantee each repetition runs in brand-new interpreter process; thus module-level state, Python memo tables, and global variables cannot influence subsequent repetitions. Import-time effects reoccur per iteration, making first-run vs. warmed-run"
        },
        {
            "title": "Preprint",
            "content": "behavior explicit in the measurement. Random number generators also begin from the childs fresh state unless the user seeds them in setup, in which case seeding is applied identically per run. By enforcing fork-per-run, the rewritten benchmarks are robust to module-level caching and other intra-process artifacts. The transformation preserves user-visible behavior (including result slicing and post-processing) while ensuring that any speedups reflect genuine algorithmic improvements rather than residual state from previous iterations. Original Raw Workload for pandas-dev__pandas-56508 import timeit import statistics import pandas as pd import numpy as np np.random.seed(0) = 100_000 data = np.arange(N) arr = pd.array(np.where(np.random.rand(N) > 0.1, data, np.nan), (cid:44) dtype=\"Int32\") def workload(): arr._hash_pandas_object(encoding='utf-8', (cid:44) hash_key=\"1000000000000000\", categorize=False) runtimes = timeit.repeat(workload, number=5, repeat=1000) # Print runtime mean and std deviation. print(\"Mean:\", statistics.mean(runtimes)) print(\"Std Dev:\", statistics.stdev(runtimes)) Transformed and Memory Isolated Workload for pandas-dev__pandas-56508 import timeit import statistics import pandas as pd import numpy as np np.random.seed(0) = 100000 data = np.arange(N) arr = pd.array(np.where(np.random.rand(N) > 0.1, data, np.nan), (cid:44) def workload(): dtype='Int32') arr._hash_pandas_object(encoding='utf-8', (cid:44) hash_key='1000000000000000', categorize=False) ---- # ---- AUTO-GENERATED ISOLATION HARNESS (timeit-in-child, fork-safe) (cid:44) import statistics as _statistics import multiprocessing as _mp import timeit as _timeit def _child_once(number: int): setup_fn = (lambda: None) = _timeit.Timer(workload, setup=setup_fn) return t.timeit(number) # TOP-LEVEL target: picklable under 'fork' def _child_target(q, number):"
        },
        {
            "title": "Preprint",
            "content": "try: dur = _child_once(number) q.put(dur) except BaseException: import traceback traceback.print_exc() q.put(None) def _run_isolated(number: int, repeat: int, start_method: str = (cid:44) \"fork\"): ctx = _mp.get_context(start_method) results = [] for _ in range(repeat): = ctx.Queue() = ctx.Process(target=_child_target, args=(q, number)) p.start() dur = q.get() p.join() if dur is None or p.exitcode != 0: raise RuntimeError(\"Child run failed--see traceback (cid:44) above.\") results.append(dur) return results if __name__ == \"__main__\": _number = 5 _repeat = 1000 runtimes = _run_isolated(_number, _repeat, start_method=\"fork\") runtimes_view = runtimes print(\"Mean:\", _statistics.mean(runtimes_view)) print(\"Std Dev:\", _statistics.stdev(runtimes_view) if (cid:44) len(runtimes_view) > 1 else 0.0)"
        },
        {
            "title": "K COMPARISON OF LM GENERATED EDITS VERSUS EXPERTS",
            "content": "We provide the raw diffs with in-line comments from Figures 8 and 9 below in Figures 16 and 17. We also include an additional diff for 18 comparison. In the main text, we removed comments and surrounding lines to focus only on the lines changed between expert and LM generated diff."
        },
        {
            "title": "Preprint",
            "content": "--- a/pandas/core/arrays/arrow/array.py +++ b/pandas/core/arrays/arrow/array.py @@ -406,8 +406,14 @@ def _cmp_method(self, other, op): f\"{op.__name__} not implemented for { type(other)}\" ) - - + + + + + + + + result = result.to_numpy() return BooleanArray._from_sequence(result) if result.null_count > 0: # GH50524: avoid conversion to object for better perf values = pc.fill_null(result, False). to_numpy() mask = result.is_null().to_numpy() else: values = result.to_numpy() mask = np.zeros(len(values), dtype=np. bool_) return BooleanArray(values, mask) def _evaluate_op_method(self, other, op, arrow_funcs): pc_func = arrow_funcs[op.__name__] --- a/pandas/core/arrays/arrow/array.py +++ b/pandas/core/arrays/arrow/array.py @@ -406,8 +406,16 @@ class ArrowExtensionArray( OpsMixin, ExtensionArray): f\"{op.__name__} not implemented for { type(other)}\" ) - - + + + + + + + + + + result = result.to_numpy() return BooleanArray._from_sequence(result) # Fast path: if there are no nulls, we can avoid the expensive BooleanArray creation if result.null_count == 0: # Ensure we get boolean numpy array result_np = result.to_numpy().astype(bool ) return BooleanArray(result_np, np.zeros( len(result), dtype=bool)) # Slow path: handle nulls result_np = result.to_numpy().astype(bool) mask = result.is_null().to_numpy() return BooleanArray(result_np, mask) def _evaluate_op_method(self, other, op, arrow_funcs): pc_func = arrow_funcs[op.__name__] Figure 16: Left: Expert patch on instance pandas-dev__pandas-50524 optimizing workload via avoiding conversion to object dtype (20.5 speedup). Right: CLAUDE 3.7 SONNET (OPENHANDS) instead identifies different fast path optimization when no null elements are present, but only achieves 2.3 speedup (scoring speedup ratio of 0.113). --- a/pandas/core/series.py +++ b/pandas/core/series.py @@ -1818,7 +1818,7 @@ def to_dict(self, into: type[ dict] = dict) -> dict: else: # Not an object dtype => all types will be the same so let the default # indexer return native python type return into_c((k, v) for k, in self. items()) return into_c(self.items()) - + def to_frame(self, name: Hashable = lib. no_default) -> DataFrame: \"\"\" --- a/pandas/core/series.py +++ b/pandas/core/series.py @@ -1816,9 +1816,18 @@ class Series(base. - - - + + + + + + + + + + + + IndexOpsMixin, NDFrame): # type: ignore[misc] if is_object_dtype(self): return into_c((k, maybe_box_native(v)) for k, in self.items()) else: # Not an object dtype => all types will be the same so let the default # indexer return native python type return into_c((k, v) for k, in self. items()) # Not an object dtype => use numpy fastpath to produce native python types # by converting the underlying values to python list in and zipping # with the index. This reduces Pythonlevel boxing overhead. values = getattr(self, \"_values\", None) if values is None: return into_c((k, v) for k, in self.items()) try: list_vals = values.tolist() except Exception: # fallback to generic iteration list_vals = [v for in values] return into_c(zip(self.index, list_vals )) def to_frame(self, name: Hashable = lib. no_default) -> DataFrame: \"\"\" 17: Left:"
        },
        {
            "title": "Expert",
            "content": "Figure optimizing Series.to_dict by replacing generator of (k, v) pairs with the view self.items(), eliminating per-element tuple allocation. Right: GPT-5 MINI (OPENHANDS) converts the underlying array to Python list and zips with the index to reduce Python-level boxing during iteration (achieving 1.98 speedup vs. the experts 1.38). on pandas-dev__pandas-50089, edit"
        },
        {
            "title": "Preprint",
            "content": "diff --git a/lib/matplotlib/transforms.py b/lib/ matplotlib/transforms.py index c3d1b7929128..b0456773f6a4 100644 --- a/lib/matplotlib/transforms.py +++ b/lib/matplotlib/transforms.py @@ -2002,9 +2002,16 @@ def rotate(self, theta): \"\"\" = math.cos(theta) = math.sin(theta) rotate_mtx = np.array([[a, -b, 0.0], [b, a, 0.0], [0.0, 0.0, 1.0]], float) self._mtx = np.dot(rotate_mtx, self._mtx) mtx = self._mtx # Operating and assigning one scalar at time is much faster. (xx, xy, x0), (yx, yy, y0), _ = mtx.tolist() # mtx = [[a -b 0], [b 0], [0 0 1]] * mtx mtx[0, 0] = * xx - * yx mtx[0, 1] = * xy - * yy mtx[0, 2] = * x0 - * y0 mtx[1, 0] = * xx + * yx mtx[1, 1] = * xy + * yy mtx[1, 2] = * x0 + * y0 self.invalidate() return self - - - + + + + + + + + + + diff --git a/lib/matplotlib/transforms.py b/lib/ matplotlib/transforms.py index c3d1b79291..f85025efff 100644 --- a/lib/matplotlib/transforms.py +++ b/lib/matplotlib/transforms.py @@ -1992,6 +1992,11 @@ class Affine2D(Affine2DBase): self.invalidate() return self # Cache for rotation matrices _rotation_cache = {} _last_theta = None _last_rotate_mtx = None + + + + + def rotate(self, theta): \"\"\" Add rotation (in radians) to this transform in place. @@ -2000,10 +2005,17 @@ class Affine2D(Affine2DBase): calls to :meth:`rotate`, :meth:`rotate_deg`, :meth:`translate` - - - - + + + + + + + + + + + and :meth:`scale`. \"\"\" = math.cos(theta) = math.sin(theta) rotate_mtx = np.array([[a, -b, 0.0], [b, a, 0.0], [0.0, 0.0, 1.0]], float) # Cache the rotation matrix for repeated calls with the same angle if theta == self._last_theta and self. _last_rotate_mtx is not None: rotate_mtx = self._last_rotate_mtx else: = math.cos(theta) = math.sin(theta) rotate_mtx = np.array([[a, -b, 0.0], [b, a, 0.0], [0.0, 0.0, 1.0]], self._last_theta = theta self._last_rotate_mtx = rotate_mtx float) self._mtx = np.dot(rotate_mtx, self._mtx) self.invalidate() return self Figure 18: Left: Expert patch on instance matplotlib__matplotlib-22108 optimizing rotation transform wrkload, avoiding numpy arithmetic overhead by operating and assigning one scalar at time (1.9 speedup). Right: CLAUDE 3.7 SONNET (OPENHANDS) instead identifies last rotation caching mechanism, and achieves 2.4 speedup (scoring speedup ratio of 1.292)."
        }
    ],
    "affiliations": [
        "Google",
        "Google DeepMind",
        "Harvard University",
        "Princeton University",
        "Xian Jiaotong University"
    ]
}