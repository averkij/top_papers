{
    "paper_title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models",
    "authors": [
        "Maciej Chrabąszcz",
        "Katarzyna Lorenc",
        "Karolina Seweryn"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just a few characters and using a small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting a potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, a low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 5 4 6 7 0 . 6 0 5 2 : r a"
        },
        {
            "title": "EVALUATING LLMS ROBUSTNESS IN LESS RESOURCED\nLANGUAGES WITH PROXY MODELS",
            "content": "Maciej Chrab aszcz NASK - National Research Institute, Warsaw, Poland maciej.chrabaszcz@nask.pl Katarzyna Lorenc NASK - National Research Institute, Warsaw, Poland katarzyna.lorenc@nask.pl Karolina Seweryn NASK - National Research Institute, Warsaw, Poland karolina.seweryn@nask.pl"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have demonstrated impressive capabilities across various natural language processing (NLP) tasks in recent years. However, their susceptibility to jailbreaks and perturbations necessitates additional evaluations. Many LLMs are multilingual, but safety-related training data contains mainly high-resource languages like English. This can leave them vulnerable to perturbations in low-resource languages such as Polish. We show how surprisingly strong attacks can be cheaply created by altering just few characters and using small proxy model for word importance calculation. We find that these character and word-level attacks drastically alter the predictions of different LLMs, suggesting potential vulnerability that can be used to circumvent their internal safety mechanisms. We validate our attack construction methodology on Polish, low-resource language, and find potential vulnerabilities of LLMs in this language. Additionally, we show how it can be extended to other languages. We release the created datasets and code for further research."
        },
        {
            "title": "Introduction",
            "content": "Language models (LMs) [1, 2] excel in natural language understanding (NLU) and natural language generation (NLG) tasks, enabling numerous everyday applications. However, recent studies [3, 4, 5, 6, 7] reveal their susceptibility to attacks that mimic human-like input perturbations. Therefore, it is crucial to test these models robustness to perturbations. LLMs research predominantly targets high-resource languages, e.g., English [2, 8]. However, the emergence of multilingual language models [9, 10, 11] introduces new vulnerabilities, particularly when incorporating lowerresourced languages. These languages pose challenge due to limited data, making it difficult to enhance their multilingual robustness against even simple perturbations during fine-tuning processes, such as supervised fine-tuning (SFT) and model alignment. Therefore, assessing the robustness of multilingual models is crucial, especially for low-resource languages. To address the issue of evaluating multilingual language models robustness, we propose framework for generating perturbed datasets by utilizing proxy models and attribution methods. Those datasets can be used to assess LLMs safety in terms of robustness, allowing developers to examine models robustness to selected perturbations and lowering the risk of misleading their models with simple perturbations after release. Our framework leverages proxy models, which, combined with attribution methods, allow for the cheap identification of the most important words for specific task, enabling the creation of evaluation examples by perturbing only the most important words. We validate our Equal contribution. Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Figure 1: Overview of the proposed framework. We can calculate word importance using trained proxy model on the desired datasets. Next, with word importance, we perturb the most important words with perturbations of interest. Then, we can evaluate the robustness of LLMs to those perturbations. By setting threshold, we can create automatic robustness checks highlighting problems with the model during development. methodology on Polish and find potential vulnerabilities in LLMs in this language. This methodology can be easily adapted to other languages with minimal linguistic effort. By performing targeted perturbations on important words, we can rigorously test and ensure the robustness of various language models to perturbations of interest. Our contributions are as follows: We introduce framework that generates human-understandable perturbed examples to assess the LLMs robustness to perturbations. We curate Polish datasets, train proxy models on them, and conduct perturbations, resulting in the creation of the Polish dataset, which can be used to evaluate LLMs robustness in Polish. We perform an extensive assessment of the robustness of LLMs, utilizing the created dataset. We identify the perturbations that result in the most substantial performance degradation. The insights gained from this analysis can assist model developers in improving the robustness of their models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Safety and Robustness Ensuring the robustness of AI models is crucial for maintaining model safety. Recent studies have demonstrated that LMs can be vulnerable to perturbations, leading to incorrect class predictions [3, 12, 6] or the generation of undesirable text, even when the models are well-aligned [13]. These findings underscore the necessity of evaluating models robustness to perturbations. When generating perturbed example, it is essential to preserve the original meaning of the text, which can be done by adding typos [12], synonyms [3], BERT-based substitutions [4]. Consequently, there has been focus on developing 2 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 1: Examples illustrating different perturbations that successfully changed models predictions."
        },
        {
            "title": "Modification Dataset",
            "content": "Sample (Strikethrough = Original Text, red = Perturbation) Label Prediction"
        },
        {
            "title": "Rel",
            "content": "R-Sw"
        },
        {
            "title": "Key",
            "content": "AR AC AC P-O P-O R-Del R-Sub AC"
        },
        {
            "title": "CBD",
            "content": "R-Ins P-O"
        },
        {
            "title": "Dia",
            "content": "AC AC PL: słaba sła6a jakosc, typowa podróbka, nie polecam EN: Poor quality, typical fake, do not recommend. PL: klient akceptuje niniejszy nastepuj acy regulamin EN: client accepts these terms and conditions. PL: Oswiadczam, ze otrzymałem, zapoznałem zapzonałme sie akceptuje EN: declare that have received, read, and accept. PL: dzieki niemu ni emu jeszcze jestem studentem ; EN: thanks to him, Im still student ;p PL: UNIKAC nie polecam poleFXm . . . brak slow ( tz sa ale post mi zlikwiduja ) EN: AVOID, dont recommend... Im at loss for words (well, have words, but theyll delete my post). PL: ad własciwy włsciy dla siedziby powoda. EN: The court competent for the plaintiff PL: @anonymized_account tego Bierz @anonymized_account razem jestescie jesteswZe mocni EN: @anonymized_account together, you are strong. PL: Krotko : cala grupa zaliczyla , nie oddal nikomu ani jednego sprawka . Oceny marzenie mqarzenJie . Tyle : - ) EN: Shortly: the whole group passed, didnt hand in single assignment to anyone. Dream grades, just dream. Thats it : - ) PL: Za ewentualne zniszczenia odpowiada opiekun opiekón EN: The supervisor is responsible for any potential damage. PL: Po opuszczeniu parkingu firma reklamacji nie uwzglednia uwzglednia. EN: After leaving the parking lot, the company does not accept any complaints. @anonymized_account take 1 0 1 1 0 plus minus minus zero 0 1 0 plus zero 0 1 0 1 adversarial datasets, such as AdvGLUE for English data [14], which has been extended for LLMs in the DecodingTrust framework [15]. However, to the best of our knowledge, there are no comparable datasets available for Polish and many other lower-resourced languages. 2.2 Attribution Methods Attribution methods aim to identify the most influential parts of the input for model predictions. While previous works on attacks [12, 4] have calculated word importance by observing changes in predictions when word is omitted or simple saliency attributions [16, 12], these approaches can be problematic due to those methods being unfaithful to the model. 3 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Alternative attribution methods were developed to offer more robust attribution in black and white-box scenarios. In black-box scenarios, where the models internal mechanisms are inaccessible, methods such as SHAP (Shapley Additive explanations) [17] and LIME (Local Interpretable Model-agnostic Explanations) [18] help determine word importance. In white-box scenarios, where model internals are accessible, gradient-based attribution methods have gained prominence. Notable examples include Grad Input [19], Integrated Gradients [20], and SmoothGrad [21]. These approaches leverage the models gradient information to quantify the contribution of each input feature to the final prediction. The widespread adoption of transformer-based models in NLP has led to the development of attribution methods specifically designed for this architecture. These include Attention Rollout [22] and other attention-based techniques [23, 24]. 2.3 Language Modeling Language modeling is fundamental task in NLP that involves predicting the probability distribution of words or tokens in sequence. These models have evolved significantly, starting from simple statistical approaches like n-grams to the more advanced neural network architectures that dominate the field today. Transformer-based language models like GPT [25] and BERT [1] have achieved state-of-the-art results in various tasks, including text classification and generation. In Polish, several transformer-based models have been developed [26, 27, 28]. Among these, Bielik [29] currently stands as the most prominent Polish-dedicated generative model, though some multilingual models also provide support for the Polish language (LLama3.1 [2], OpenChat [30], CommandR [10]). The widespread adoption of LLMs highlights the critical need to evaluate their robustness, as these models can be attacked to generate harmful or misleading outputs. This can have serious consequences, especially in critical domains like healthcare or finance. Therefore, it is crucial to evaluate these models carefully to ensure their reliability and safety, identifying potential vulnerabilities before they can be exploited in real-world scenarios [31, 14, 32]."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Proxy Model To identify the most important words, we could rank those words manually, but manually selecting the most important words is unfeasible. Thus, we used attribution methods to select the most important words. Most attribution methods need model to calculate importance. Pre-trained LLMs could be used as such model. Unfortunately, these models take lot of VRAM on their own, and calculating gradient-based methods for LLMs can be very time-consuming and, for some methods, even impossible due to VRAM constraints. We train small proxy model to address this issue, which we later use to calculate word importance. If the model performs well on dataset, it can be considered good proxy for calculating word importance. Due to attribution methods highlighting what was important for the models predictions, its crucial to use model with high performance for this step. 3.2 Word Importance To identify the importance of individual words, we first calculate token attributions using gradient-based and perturbationbased methods. For example, Grad Input attribution for an input sequence and output is calculated as follows Grad Input(X, y) = y(Xemb) Xemb, (1) where Xemb Rn,d are word embeddings of input tokens and is the embedding size. However, due to the nature of tokenization, tokens may not always correspond to complete words. To address this issue, we group the tokens that form each word and aggregate their attributions using simple mean, as shown in the following equation of importance (I): I(W ) = 1 S(W ) (cid:88) a(t), tS(W ) (2) where is function that splits word into its subtokens, S(W ) denotes the number of subtokens in the word, and a(t) represents the attribution value of subtoken obtained from the chosen attribution method. This approach allows us to determine the importance of each word independently of the tokenization process, providing more interpretable and coherent measure of word-level importance. 4 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Figure 2: Relation of ASR on proxy models and number of perturbed words for different attribution methods. The dotted lines show ASR when selecting random words for perturbation instead of words based on word importance. We exclude tokens representing punctuation marks to ensure more faithful aggregation of word importance. Without excluding these tokens, we would additionally consider the importance of punctuation when calculating word importance. This could lead to word importance scores that are less faithful to the semantic content of the text. By focusing on the semantic content of the text rather than the syntactic structure, we obtain clearer picture of the importance of individual words for prediction. 3.3 Importance-based Perturbation We performed perturbations that we split into two distinct levels: Character Level and Word Level. Character-level Perturbations At the character level, we explored various techniques designed to introduce typographical errors. The specific perturbations include: Keyboard errors (Key): Simulating common typing mistakes resulting from adjacent key presses. Optical character recognition (OCR) errors: Introducing errors caused by graphical similarities between characters, typically observed in OCR systems. Random character insertion (R-Ins): Inserting random characters within words. Character deletion (R-Del): Removing characters from words. Character substitution (R-Sub): Replacing characters with random ones. Characters swapping (R-Sw): Reordering adjacent characters. Diacritical errors (Dia): Omitting diacritical marks, which can drastically change the meaning of words. For example, altering \"k at\" (angle) to \"kat\" (executioner) or \"jezyk\" (language) to \"jezyk\" (little hedgehog) demonstrates how such errors can significantly affect word interpretation in languages like Polish. Implementing character-level perturbations in other languages necessitates updating the dictionary with diacritical errors common in the language of interest. The other perturbations do not require alteration. 5 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Word-level Perturbations At the word level, we applied several perturbation methods to assess their impact on model performance: Random spacing (Split): Inserting spaces at random positions within words to disrupt their conventional structure. Orthographic errors (Ort): Common in Polish, these errors typically involve the substitution of distinctive characters. For example, replacing \"rz\" with \"z\" or \"h\" with \"ch\". Certain spelling mistakes can lead to significant changes in word meaning, e.g., \"morze\" (sea) and \"moze\" (maybe). Relations based on Słowosiec [33] (Rel): The Polish counterpart to WordNet, which provides not only synonyms but also hypernyms, meronyms, and other lexical relations. This resource also accounts for adjectives across different grammatical genders. For instance, substituting \"sympatyczny\" with \"miły\" (nice to pleasant, masculine) or \"sympatyczna\" with \"miła\" (feminine). To extend word-level perturbations to other languages, one must update the orthographic errors specific to the language of interest. Additionally, performing changes based on word relations necessitates access to word relation network for that language. It is important to note that not all perturbations transfer easily to other languages. We believe many language-specific perturbations will not work for Polish. Therefore, carefully adding and removing perturbations into our framework is crucial for evaluating models accurately with language-specific variations. Table 2: ASR of perturbation methods against proxy models across all datasets aggregated over attribution methods and number of words changed."
        },
        {
            "title": "Key OCR",
            "content": "Ort R-Del R-Ins R-Sub R-Sw"
        },
        {
            "title": "Avg",
            "content": "T l R e R R AC AR"
        },
        {
            "title": "CBD",
            "content": "P-I P-O AC AR"
        },
        {
            "title": "CBD",
            "content": "P-I P-O AC AR"
        },
        {
            "title": "CBD",
            "content": "P-I P-O 0.01 0.03 0.01 0. 0.01 0.01 0.02 - 0.00 0. 0.04 0.03 0.00 0.00 0.01 0. 0.22 0.04 0.04 0.16 0.11 0. - 0.03 0.13 0.21 0.22 0. 0.04 0.12 0.13 0.23 0.04 0. 0.19 0.12 0.14 - 0.03 0. 0.24 0.23 0.02 0.05 0.13 0. 0.06 0.01 0.01 0.02 0.02 0. - 0.00 0.02 0.06 0.05 0. 0.01 0.03 0.11 0.23 0.03 0. 0.15 0.09 0.12 - 0.03 0. 0.20 0.21 0.02 0.04 0.11 0. 0.23 0.03 0.05 0.16 0.11 0. - 0.03 0.13 0.20 0.22 0. 0.04 0.12 0.09 0.24 0.05 0. 0.14 0.05 0.12 - 0.02 0. 0.17 0.21 0.03 0.05 0.13 0. 0.12 0.01 0.02 0.04 0.02 0. - 0.01 0.04 0.05 0.10 0. 0.02 0.04 0.07 0.24 0.05 0. 0.15 0.05 0.12 - 0.02 0. 0.14 0.20 0.03 0.04 0.11 0. 0.18 0.03 0.03 0.12 0.06 0. - 0.02 0.09 0.15 0.17 0. 0.03 0.09 0.08 0.24 0.04 0. 0.14 0.04 0.12 - 0.02 0. 0.16 0.21 0.02 0.04 0.12 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 3: Performance of Proxy Models on test sets of all datasets. Model Data AUROC F1 ACC PolBERT AC AR CBD P-I P-O HerBERT AC AR CBD P-I P-O RoBERTa AC AR CBD P-I P-O 0.922 0.836 0.837 0.969 0.896 0.926 0.887 0.547 0.970 0.910 0.926 0.863 0.879 0.972 0.904 0.828 0.480 0.684 0.816 0. 0.836 0.572 0.464 0.856 0.529 0.832 0.542 0.660 0.860 0.529 0.843 0.580 0.893 0.856 0.678 0.851 0.653 0.866 0.892 0.711 0.848 0.624 0.887 0.886 0."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets In the experiments, we used datasets from the KLEJ benchmark [34]. KLEJ is the Polish equivalent of the English GLUE benchmark for text analysis. AC - Polish Abusive Clauses Dataset [35] is used for detecting abusive clauses in legal agreements to protect consumers from unfair terms. It contains two classes: abusive clause and correct agreement statement. AR - Allegro Reviews [34] consists of sentiment-labeled reviews from the Polish e-commerce platform Allegro. Each opinion is assigned value from 1 to 5, representing the sentiment score. CBD - Cyberbullying Detection [36] dataset contains Twitter messages and is used to predict whether given message contains cyberbullying or harmful content. P-I & P-O - PolEmo2.0 [37] is collection of online reviews from the medicine and hotel domains. The task is to predict the sentiment of review. Two separate test sets allow for in-domain (medicine and hotels) and out-of-domain (products and university) evaluation. These datasets provide diverse range of NLP tasks, including sentiment analysis, abusive content detection, and domain-specific text classification, which are crucial for evaluating the performance of models in various Polish language understanding tasks. Details about the sizes of train and test splits are available in the Appendix A. 4.2 Models In our experiments, we used transformer-based classifiers HerBERT [26], PolBERT [27], and Polish RoBERTa [28] as proxy models. These models were fine-tuned on the training set of each analyzed dataset for the specific task. The training process involved 20 epochs on an A100 (40GB) GPU with early stopping, and the batch size was set to 16 (8 for Allegro reviews), for models performance, see Table 3. After training, these models were additionally evaluated for their robustness against perturbations using the test sets. We observed that the HerBERT model failed to train on the CBD dataset, which led to it predicting the same label for all examples. Thus, we omitted using it for further experiments. For LLMs, we used open-source generative models capable of processing the Polish language, such as Bielik [29], Mistral-7B-Instruct [38], and Llama-3.1-8 [2]. We applied zero-shot learning approach for these models, where the models were queried using both original and modified prompts. As final proxy model for the word importance, we used model based on the polish-roberta-base-v2 classifier, which showed the highest performance for all datasets, and the SHAP method as an attribution method, due to SHAP having the highest success rate of the perturbations for proxy models. Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 4: ASR of perturbation methods against smaller models across all datasets aggregated over the number of words changed. Perturbations were generated using SHAP word importance scores and RoBERTa classifier. Robustness values above the threshold are highlighted in red, with more intense red indicating lower robustness. Values below the threshold are highlighted in green, with more intense green indicating higher robustness. Data Diac Key 0. 0.196 0.452 0.302 0.432 0.076 0. 0.348 0.028 0.075 0.333 0.346 0. 0.051 0.137 1 l B 7 t M 8 3 l AC AR CBD P-I P-O AC AR CBD P-I P-O AC AR CBD P-I P-O 0. 0.082 0.087 0.134 0.149 0.010 0. 0.013 0.003 0.015 0.219 0.075 0. 0.004 0.031 4.3 Metric OCR 0.267 0. 0.488 0.308 0.417 0.087 0.202 0. 0.029 0.073 0.334 0.346 0.232 0. 0.146 Ort R-Del R-Ins R-Sub R-Sw Rel 0.170 0.083 0.156 0.167 0. 0.014 0.048 0.029 0.010 0.024 0. 0.166 0.058 0.005 0.042 0.261 0. 0.318 0.264 0.380 0.041 0.156 0. 0.027 0.074 0.314 0.345 0.102 0. 0.111 0.261 0.170 0.441 0.263 0. 0.068 0.141 0.335 0.032 0.058 0. 0.354 0.159 0.043 0.131 0.288 0. 0.477 0.286 0.414 0.076 0.177 0. 0.032 0.078 0.337 0.329 0.157 0. 0.136 0.283 0.173 0.360 0.265 0. 0.056 0.156 0.213 0.026 0.061 0. 0.296 0.127 0.037 0.118 0.182 0. 0.173 0.242 0.307 0.022 0.103 0. 0.016 0.034 0.239 0.221 0.062 0. 0.070 Split 0.224 0.151 0.383 0. 0.361 0.042 0.146 0.214 0.025 0. 0.302 0.261 0.093 0.018 0.098 To evaluate the effectiveness of perturbations, we utilized the Attack Success Rate (ASR) metric. This metric serves as standard measure for assessing the performance of adversarial attacks by calculating the proportion of cases where the attack achieves its intended goal. higher ASR indicates more effective attack, whereas lower ASR suggests greater resistance of the model to adversarial manipulations. Formally, ASR can be defined as: ASR = (cid:88) (x,y)D 1[f (A(x)) = y] 1[f (x) = y] (x,y)D 1[f (x) = y] (cid:80) , (3) where denotes the dataset of input samples and their corresponding true labels y, (x) represents the models prediction for given input x, A(x) is the adversarially perturbated version of the input generated by the perturbation and 1[] is the indicator."
        },
        {
            "title": "5 Results",
            "content": "5.1 Influence of Perturbation Type Results in Table 2 highlight that our LMs are fooled into changing their prediction when faced with simple character perturbations, such as OCR, R-Del, R-Ins, R-sub, R-Sw, and Split. These models havent seen such errors in training data, potentially making them perform poorly on examples with such perturbations. In Table 4, we can observe that even though LLMs were trained on data from the internet, which should have examples with character-level errors, those models can be easily misguided by such perturbations. This should raise the attention of the developers to fix this issue. These are simple classification tasks, but if models make more errors when faced with such perturbations on those tasks, there is high risk that those types of perturbations could be used to deceive models into generating harmful content. 8 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Figure 3: Relation of ASR on Mistral and the number of perturbed words for different attribution methods. The dotted line indicates the robustness cutoff above which the model is considered non-robust to simple perturbations. 5.2 Influence of Attribution Method Based on the results in Figure 2, we can observe that SHAP gives the highest ASR compared to other methods for all numbers of words changed. Vanilla Gradient and SmoothGrad have the worst performance in terms of ASR and perform very similarly to selecting random words. This hints that it will be beneficial to use SHAP to extract word importance when selecting words that should be perturbed, in our case. 5.3 Robustness of LLMs Figure 3 and Table 4 present an example usage of the proposed framework for the Mistral model. The red line denotes the threshold (0.05) equivalent to the model being fooled in 5% of examples with these perturbations, and we decide that when the model is fooled so often, then it could be the case that it has vulnerability related to such perturbations. The analysis reveals that while the model effectively handles diacritic and orthographic perturbations, it is vulnerable to other modifications, such as inserting additional characters at the word level. This finding is particularly significant from an AI safety perspective, as it suggests that although the model may resist harmful prompts in their standard form, subtle alterations, such as introducing spaces within key terms, could increase the likelihood of generating harmful responses."
        },
        {
            "title": "6 Conclusions",
            "content": "In this work, we introduced framework to assess the robustness of LLMs by utilizing perturbations and word importance calculated with proxy models. We validated this framework in Polish by curating set of perturbed datasets that can be used to assess LLMs robustness in Polish. To evaluate robustness to perturbations, we perform the following steps: 1. Create Proxy Model: Train small model on the target dataset. 9 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models 2. Rank: Calculate and aggregate attribution scores based on the proxy model to create word-importance ranking. 3. Perturb: Perturb the most important words according to the ranking. 4. Evaluate: Evaluate the target LLMs on the original and perturbed datasets to assess their robustness. Compatibility with various perturbation methods and tasks is key feature of this framework, assuming attribution calculation is feasible. Using perturbed datasets prepared using our framework is particularly advantageous in LLMs development. By carefully preparing datasets with specified perturbations, developers can effectively assess and control the robustness of the models they create. Should perturbation lead to an LLMs failure on classification task, it underscores vulnerability that warrants attention, given the potential for such perturbations to negatively influence model performance in safety-critical generative tasks, such as the generation of instructions for prohibited activities. Our investigation utilizing the framework for the Polish language revealed that such perturbations, which can be implemented with minimal resources, are unexpectedly effective in deceiving LLMs. We highlight the perturbations that proved most successful in fooling these models, indicating areas requiring attention."
        },
        {
            "title": "7 Limitations",
            "content": "Our word importance relies on attribution methods, which may not faithfully reflect the models internal reasoning processes. On the other hand, the choice of model used for generating attributions can also influence whether the words selected are really important, whether these reflect genuine semantic importance, or merely artifacts specific to the proxy models training. The analysis relies on heuristic perturbations derived from word importance rankings. This approach does not encompass the full spectrum of potential attacks against LLMs, such as those generated through adversarial optimization techniques. Another consideration is that the process of perturbing important words could make key segments of the text unintelligible, even from human perspective. Such semantic degradation may confound the assessment, making isolating model robustness issues from the effects of corrupted input challenging. Moreover, relying on proxy models for generating word importance scores is an approximation. Importance rankings derived directly from the target LLMs could potentially differ and offer more precise basis for identifying words whose perturbation maximally impacts the LLMs performance."
        },
        {
            "title": "References",
            "content": "[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41714186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [2] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and et al. Abhishek Kadian. The llama 3 herd of models, 2024. [3] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? strong baseline for natural language attack on text classification and entailment. In AAAI Conference on Artificial Intelligence, 2019. [4] Linyang Li, Ruotian Ma, Qipeng Guo, X. Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert using bert. ArXiv, abs/2004.09984, 2020. [5] Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, and Bo Li. T3: Tree-autoencoder constrained adversarial text generation for targeted attack. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 61346150, Online, November 2020. Association for Computational Linguistics. [6] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. Word-level textual adversarial attacking as combinatorial optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 60666080, Online, July 2020. Association for Computational Linguistics. [7] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 8007980110. Curran Associates, Inc., 2023. 10 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models [8] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. [9] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In Annual Meeting of the Association for Computational Linguistics, 2019. [10] CohereForAI. Command +, 2024. Accessed: 2024-08-29. [11] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [12] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. In Proceedings 2019 Network and Distributed System Security Symposium. Internet Society, 2019. [13] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models, 2023. [14] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: multi-task benchmark for robustness evaluation of language models. In Advances in Neural Information Processing Systems, 2021. [15] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: comprehensive assessment of trustworthiness in gpt models. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. [16] Simonyan, Vedaldi, and Zisserman. Deep inside convolutional networks: visualising image classification models and saliency maps. In Proceedings of the International Conference on Learning Representations (ICLR). ICLR, 2014. [17] Scott, Lee Su-In, et al. unified approach to interpreting model predictions. Advances in neural information processing systems, 30:47654774, 2017. [18] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. why should trust you?: Explaining the predictions of any classifier. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2016. [19] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just black box: Learning important features through propagating activation differences. ArXiv, abs/1605.01713, 2016. [20] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In International Conference on Machine Learning, 2017. [21] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viégas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. ArXiv, abs/1706.03825, 2017. [22] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In Annual Meeting of the Association for Computational Linguistics, 2020. [23] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 387396, 2021. [24] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782791, June 2021. [25] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models [26] Robert Mroczkowski, Piotr Rybak, Alina Wróblewska, and Ireneusz Gawlik. HerBERT: Efficiently pretrained transformer-based language model for Polish. In Proceedings of the 8th Workshop on Balto-Slavic Natural Language Processing, pages 110, Kiyv, Ukraine, April 2021. Association for Computational Linguistics. [27] Dariusz Kłeczek. Polbert: Attacking polish nlp tasks with transformers. In Maciej Ogrodniczuk and Łukasz Kobylinski, editors, Proceedings of the PolEval 2020 Workshop. Institute of Computer Science, Polish Academy of Sciences, 2020. [28] Sławomir Dadas and Małgorzata Grebowiec. Assessing generalization capability of text ranking models in polish, 2024. arXiv:2402.14318 [cs.CL]. [29] Krzysztof Ociepa, Łukasz Flis, Krzysztof Wróbel, Adrian Gwozdziej, and Remigiusz Kinas. Bielik 7b v0. 1: polish language modeldevelopment, insights, and evaluation. arXiv preprint arXiv:2410.18565, 2024. [30] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. Openchat: Advancing open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235, 2023. [31] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528, 2023. [32] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution perspective. arXiv preprint arXiv:2302.12095, 2023. [33] Marek M. Maziarz, Maciej Piasecki, and Ewa K. Rudnicka. Słowosiec :polski wordnet : proces tworzenia tezaurusa. Polonica, 34:7998, 2014. [34] Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Ireneusz Gawlik. Klej: Comprehensive benchmark for polish language understanding. arXiv preprint arXiv:2005.00630, 2020. [35] Lukasz Augustyniak, Kamil Tagowski, Albert Sawczyn, Denis Janiak, Roman Bartusiak, Adrian Szymczak, Arkadiusz Janz, Piotr Szymanski, Marcin atroba, Mikoł aj Morzy, Tomasz Kajdanowicz, and Maciej Piasecki. This is the way: designing and compiling lepiszcze, comprehensive nlp benchmark for polish. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2180521818. Curran Associates, Inc., 2022. [36] Michal Ptaszynski, Agata Pieciukiewicz, and Paweł Dybała. Results of the poleval 2019 shared task 6: First dataset and open shared task for automatic cyberbullying detection in polish twitter. Proceedings of the PolEval 2019 Workshop, page 89, 2019. [37] Jan Kocon, Piotr Miłkowski, and Monika Zasko-Zielinska. Multi-level sentiment analysis of PolEmo 2.0: Extended corpus of multi-domain consumer reviews. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 980991, Hong Kong, China, November 2019. Association for Computational Linguistics. [38] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. 12 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models"
        },
        {
            "title": "A Models and Datasets",
            "content": "Information about dataset sizes is available in Table 5 contains LLMs performance on original test sets. Table 5: Sizes of train and tests splits of used datasets."
        },
        {
            "title": "Dataset",
            "content": "Polish Abusive Clauses Allegro Reviews Cyberbullying PolEmo2.0 In PolEmo2.0 Out"
        },
        {
            "title": "Test",
            "content": "4284 9577 10041 5783 5783 3453 1006 1000 722 494 Table 6: Performance of LLMs on test sets."
        },
        {
            "title": "Model",
            "content": "Bielik-7B Mistral-7B"
        },
        {
            "title": "Data",
            "content": "F1 ACC"
        },
        {
            "title": "0.60\nAC\nAR\n0.50\nCBD 0.83\n0.58\nP-I\n0.57\nP-O",
            "content": "0.54 0.50 0.65 0.40 0.33 0.54 0.49 0.82 0.68 0.66 0.45 0.47 0.87 0.69 0."
        },
        {
            "title": "B Perturbations",
            "content": "B.1 Character-level All character-level perturbations randomly select the number of characters to change between 1 and min(len(word) 0.15, 4). It ensures that character-level changes make perturbed words easily understandable to human. B.2 Word-level Word-level perturbations always try to modify the provided word, but some methods can be unsuccessful. B.3 Word importance To calculate SmoothGrad and Integrated Gradients attributions, we used 50 steps. For SHAP attributions, we use default arguments provided by SHAP [17] library. 13 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models"
        },
        {
            "title": "C Perturbation attacks success rate",
            "content": "Table 7: ASR of attribution methods against smaller models across all datasets. Model Data AGR AR IG SH SG Avg AC AR PolBERT CBD P-I P-O AC AR HerBERT CBD P-I P-O AC AR RoBERTa CBD P-I P-O 0.08 0.28 0. 0.07 0.18 0.06 0.11 - 0. 0.12 0.15 0.21 0.02 0.04 0. 0.09 0.16 0.03 0.03 0.12 0. 0.09 - 0.02 0.08 0.15 0. 0.02 0.02 0.07 0.07 0.12 0. 0.02 0.07 0.05 0.08 - 0. 0.06 0.13 0.10 0.01 0.02 0. 0.07 0.20 0.03 0.03 0.11 0. 0.10 - 0.02 0.10 0.16 0. 0.02 0.04 0.11 0.08 0.22 0. 0.05 0.15 0.07 0.14 - 0. 0.11 0.15 0.25 0.03 0.06 0. 0.07 0.12 0.03 0.01 0.07 0. 0.08 - 0.02 0.07 0.13 0. 0.01 0.02 0.07 0.08 0.18 0. 0.04 0.12 0.06 0.10 - 0. 0.09 0.14 0.17 0.02 0.03 0. ASR broken down by attribution methods is available in Table 7. Tables 8,9,10,11,12 and 13 present the ASR broken down by model, augmentation method, and the number of words changed for classifiers. Figure 4 indicates that using attribution methods (targeted attack) significantly improves the effectiveness of the attack on language models compared to selecting random words during the attack. Figures 5,6 show robustness checks applied to Llama nad Bielik. 14 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 8: ASR for different numbers of words changed when using SHAP as attribution method, aggregated over datasets. Model Aug 10 8 9 6 5 2 4 1 0.012 0.139 0.159 0.030 0.135 0.138 0.145 0.145 0.049 0.134 0.007 0.111 0.101 0.015 0.074 0.086 0.108 0.083 0.034 0.071 0.013 0.168 0.182 0.033 0.156 0.155 0.167 0.161 0.056 0.136 0.013 0.152 0.170 0.035 0.146 0.146 0.156 0.154 0.054 0.148 0.008 0.120 0.117 0.016 0.085 0.100 0.121 0.092 0.039 0.074 0.018 0.193 0.201 0.042 0.180 0.175 0.186 0.185 0.067 0. 0.014 0.164 0.189 0.039 0.157 0.160 0.168 0.164 0.062 0.159 0.011 0.134 0.126 0.021 0.090 0.111 0.130 0.100 0.043 0.080 0.022 0.211 0.213 0.049 0.199 0.188 0.203 0.200 0.072 0.174 0.015 0.173 0.195 0.042 0.166 0.169 0.178 0.176 0.065 0.167 0.012 0.145 0.133 0.024 0.103 0.116 0.144 0.109 0.050 0.088 0.026 0.225 0.228 0.057 0.213 0.208 0.220 0.219 0.080 0. 0.019 0.180 0.204 0.047 0.172 0.177 0.181 0.184 0.071 0.175 0.013 0.153 0.141 0.025 0.112 0.124 0.152 0.117 0.055 0.093 0.032 0.238 0.242 0.061 0.229 0.223 0.231 0.239 0.091 0.207 0.022 0.188 0.205 0.051 0.179 0.186 0.187 0.191 0.074 0.179 0.016 0.163 0.149 0.028 0.121 0.128 0.165 0.123 0.054 0.098 0.037 0.249 0.253 0.067 0.241 0.232 0.246 0.245 0.098 0. PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.004 0.057 0.066 0.010 0.052 0.058 0.063 0.056 0.023 0.053 0.003 0.033 0.033 0.006 0.025 0.027 0.034 0.027 0.011 0.023 0.007 0.053 0.058 0.013 0.045 0.043 0.050 0.048 0.021 0. 0.006 0.091 0.101 0.015 0.087 0.091 0.093 0.088 0.033 0.083 0.004 0.056 0.053 0.010 0.042 0.049 0.056 0.045 0.018 0.038 0.008 0.096 0.107 0.017 0.082 0.085 0.094 0.093 0.035 0.081 0.007 0.110 0.125 0.020 0.107 0.115 0.117 0.112 0.041 0.107 0.005 0.078 0.075 0.012 0.055 0.064 0.078 0.061 0.023 0.052 0.010 0.127 0.140 0.023 0.111 0.116 0.122 0.123 0.044 0. 0.010 0.124 0.142 0.027 0.123 0.126 0.132 0.127 0.046 0.123 0.006 0.093 0.090 0.015 0.068 0.075 0.096 0.075 0.028 0.061 0.011 0.149 0.167 0.026 0.132 0.139 0.146 0.144 0.051 0.125 15 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 9: ASR for different numbers of words changed when using Gradient as attribution method, aggregated over datasets. Model Aug 10 3 4 1 5 6 7 8 9 0.010 0.079 0.087 0.020 0.072 0.075 0.078 0.079 0.034 0. 0.007 0.063 0.061 0.013 0.044 0.053 0.057 0.048 0.024 0.041 0.011 0.084 0.095 0.017 0.072 0.073 0.078 0.076 0.023 0.066 0.011 0.083 0.094 0.024 0.080 0.080 0.084 0.085 0.036 0.082 0.008 0.070 0.068 0.013 0.047 0.062 0.067 0.052 0.026 0.044 0.015 0.096 0.109 0.023 0.085 0.091 0.094 0.091 0.031 0.078 0.013 0.089 0.103 0.029 0.084 0.090 0.090 0.090 0.040 0. 0.009 0.071 0.072 0.015 0.048 0.065 0.072 0.053 0.029 0.049 0.020 0.108 0.119 0.029 0.097 0.100 0.104 0.102 0.037 0.086 0.015 0.096 0.112 0.032 0.090 0.092 0.095 0.096 0.043 0.092 0.010 0.076 0.078 0.017 0.054 0.068 0.081 0.054 0.033 0.050 0.026 0.119 0.125 0.034 0.106 0.111 0.116 0.112 0.043 0.096 0.018 0.102 0.119 0.034 0.096 0.100 0.103 0.107 0.048 0. 0.012 0.081 0.084 0.021 0.060 0.072 0.085 0.059 0.038 0.057 0.031 0.125 0.136 0.042 0.113 0.120 0.125 0.123 0.051 0.102 0.021 0.106 0.125 0.038 0.099 0.107 0.107 0.110 0.052 0.100 0.014 0.088 0.087 0.023 0.065 0.077 0.093 0.066 0.040 0.059 0.036 0.135 0.143 0.048 0.124 0.129 0.134 0.132 0.058 0.110 PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.005 0.031 0.035 0.007 0.027 0.033 0.032 0.033 0.014 0.031 0.003 0.027 0.026 0.006 0.018 0.022 0.028 0.020 0.011 0.020 0.005 0.029 0.036 0.006 0.026 0.027 0.030 0.030 0.009 0.025 0.007 0.046 0.054 0.011 0.040 0.046 0.046 0.050 0.022 0. 0.004 0.036 0.039 0.008 0.030 0.035 0.041 0.031 0.015 0.032 0.006 0.044 0.056 0.009 0.038 0.041 0.043 0.041 0.011 0.037 0.008 0.060 0.069 0.013 0.054 0.058 0.060 0.058 0.027 0.057 0.005 0.043 0.047 0.011 0.033 0.040 0.046 0.035 0.017 0.035 0.008 0.062 0.073 0.011 0.050 0.055 0.058 0.055 0.016 0.049 0.009 0.073 0.082 0.016 0.064 0.067 0.071 0.073 0.029 0. 0.007 0.054 0.057 0.012 0.037 0.048 0.053 0.045 0.021 0.038 0.009 0.075 0.085 0.013 0.062 0.065 0.070 0.067 0.020 0.059 16 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 10: ASR for different numbers of words changed when using SmoothGrad as attribution method, aggregated over datasets. Model Aug 10 9 1 4 3 5 7 6 8 0.008 0.078 0.090 0.019 0.074 0.079 0.075 0.074 0.030 0.071 0.006 0.060 0.063 0.013 0.043 0.058 0.065 0.051 0.024 0. 0.011 0.080 0.092 0.018 0.068 0.077 0.076 0.077 0.027 0.071 0.010 0.083 0.096 0.023 0.078 0.088 0.081 0.081 0.035 0.078 0.007 0.062 0.067 0.015 0.050 0.063 0.069 0.053 0.027 0.044 0.015 0.092 0.107 0.025 0.083 0.091 0.088 0.090 0.033 0.080 0.013 0.088 0.105 0.028 0.086 0.094 0.085 0.088 0.040 0.083 0.009 0.067 0.073 0.016 0.053 0.067 0.076 0.057 0.032 0. 0.021 0.100 0.115 0.030 0.090 0.100 0.099 0.103 0.038 0.083 0.015 0.096 0.112 0.030 0.093 0.098 0.093 0.093 0.043 0.091 0.010 0.077 0.078 0.018 0.058 0.070 0.081 0.060 0.035 0.051 0.025 0.112 0.124 0.037 0.100 0.110 0.111 0.109 0.044 0.091 0.019 0.102 0.121 0.033 0.097 0.102 0.100 0.102 0.050 0.096 0.012 0.081 0.086 0.021 0.062 0.075 0.086 0.064 0.039 0. 0.030 0.123 0.135 0.044 0.108 0.120 0.120 0.119 0.051 0.098 0.021 0.111 0.126 0.037 0.101 0.108 0.108 0.107 0.052 0.099 0.014 0.087 0.095 0.026 0.070 0.080 0.095 0.071 0.043 0.060 0.037 0.132 0.142 0.051 0.118 0.128 0.130 0.130 0.059 0.109 PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.004 0.030 0.035 0.008 0.029 0.033 0.033 0.032 0.016 0.029 0.003 0.026 0.027 0.006 0.019 0.022 0.025 0.020 0.010 0.018 0.006 0.031 0.037 0.007 0.027 0.029 0.031 0.030 0.011 0.027 0.006 0.046 0.054 0.010 0.041 0.048 0.048 0.048 0.021 0.045 0.005 0.041 0.038 0.008 0.032 0.037 0.039 0.033 0.016 0. 0.007 0.049 0.058 0.010 0.043 0.047 0.046 0.043 0.017 0.042 0.007 0.060 0.066 0.012 0.052 0.058 0.062 0.054 0.023 0.053 0.005 0.049 0.053 0.010 0.036 0.043 0.048 0.041 0.018 0.033 0.008 0.060 0.073 0.012 0.055 0.058 0.059 0.059 0.021 0.053 0.008 0.073 0.080 0.015 0.066 0.072 0.072 0.067 0.029 0.065 0.006 0.055 0.057 0.011 0.042 0.052 0.057 0.045 0.022 0. 0.010 0.071 0.085 0.014 0.061 0.068 0.068 0.071 0.024 0.063 17 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 11: ASR for different numbers of words changed when using IntegratedGradients as attribution method, aggregated over datasets. Model Aug 5 7 4 3 1 2 9 8 0.010 0.113 0.134 0.023 0.119 0.117 0.117 0.110 0.044 0.120 0.005 0.082 0.078 0.018 0.060 0.073 0.081 0.065 0.034 0.050 0.013 0.143 0.154 0.029 0.131 0.137 0.143 0.138 0.049 0. 0.011 0.126 0.147 0.025 0.130 0.124 0.129 0.124 0.050 0.129 0.007 0.090 0.085 0.019 0.070 0.078 0.091 0.075 0.037 0.055 0.017 0.160 0.171 0.035 0.148 0.153 0.160 0.158 0.059 0.134 0.013 0.132 0.160 0.030 0.134 0.131 0.137 0.138 0.055 0.139 0.009 0.098 0.091 0.022 0.078 0.087 0.100 0.081 0.042 0.062 0.021 0.175 0.188 0.042 0.164 0.170 0.175 0.174 0.066 0. 0.015 0.138 0.165 0.034 0.142 0.145 0.144 0.142 0.057 0.146 0.011 0.109 0.101 0.024 0.082 0.093 0.112 0.083 0.043 0.073 0.025 0.195 0.207 0.048 0.177 0.187 0.194 0.194 0.073 0.165 0.019 0.143 0.170 0.037 0.149 0.150 0.145 0.149 0.064 0.150 0.012 0.118 0.111 0.026 0.088 0.097 0.117 0.089 0.044 0.077 0.031 0.208 0.219 0.055 0.192 0.204 0.209 0.208 0.083 0. 0.021 0.149 0.174 0.041 0.155 0.155 0.148 0.157 0.070 0.153 0.014 0.126 0.115 0.029 0.095 0.102 0.125 0.099 0.048 0.083 0.037 0.219 0.230 0.062 0.209 0.212 0.219 0.218 0.089 0.184 PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.003 0.044 0.045 0.009 0.043 0.045 0.043 0.042 0.015 0.048 0.003 0.029 0.028 0.005 0.023 0.026 0.026 0.024 0.010 0.019 0.003 0.045 0.055 0.009 0.040 0.043 0.047 0.040 0.016 0.037 0.006 0.073 0.080 0.014 0.068 0.073 0.072 0.068 0.027 0.074 0.003 0.046 0.048 0.009 0.036 0.041 0.043 0.040 0.018 0.029 0.006 0.079 0.093 0.016 0.070 0.076 0.083 0.074 0.026 0. 0.006 0.088 0.098 0.016 0.085 0.085 0.086 0.082 0.031 0.087 0.004 0.061 0.058 0.013 0.046 0.054 0.057 0.049 0.026 0.039 0.008 0.101 0.119 0.019 0.094 0.099 0.102 0.094 0.033 0.089 0.008 0.101 0.118 0.019 0.104 0.103 0.105 0.099 0.040 0.104 0.005 0.072 0.067 0.014 0.055 0.065 0.070 0.059 0.032 0.046 0.010 0.124 0.136 0.023 0.115 0.120 0.127 0.116 0.041 0. 18 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 12: ASR for different numbers of words changed when using Attention Rollout as attribution method, aggregated over datasets. Model Aug 10 6 7 9 4 5 1 2 0.009 0.111 0.117 0.018 0.106 0.110 0.109 0.107 0.030 0.106 0.006 0.086 0.094 0.011 0.053 0.075 0.085 0.057 0.024 0.059 0.010 0.090 0.113 0.023 0.081 0.090 0.094 0.078 0.024 0.080 0.011 0.127 0.138 0.022 0.122 0.127 0.127 0.130 0.039 0. 0.006 0.099 0.100 0.013 0.060 0.082 0.099 0.064 0.026 0.064 0.015 0.113 0.125 0.030 0.103 0.113 0.110 0.100 0.033 0.102 0.013 0.143 0.153 0.025 0.143 0.138 0.139 0.144 0.048 0.139 0.008 0.105 0.109 0.015 0.066 0.090 0.102 0.070 0.031 0.072 0.020 0.132 0.142 0.037 0.119 0.129 0.128 0.121 0.042 0.119 0.015 0.156 0.167 0.030 0.151 0.147 0.152 0.150 0.053 0. 0.009 0.115 0.114 0.018 0.072 0.098 0.109 0.075 0.035 0.072 0.025 0.140 0.154 0.044 0.130 0.142 0.142 0.137 0.052 0.134 0.019 0.163 0.181 0.034 0.165 0.159 0.164 0.163 0.058 0.162 0.010 0.123 0.117 0.019 0.076 0.102 0.119 0.081 0.038 0.077 0.030 0.158 0.171 0.052 0.147 0.157 0.157 0.155 0.060 0.150 0.021 0.171 0.190 0.038 0.171 0.167 0.173 0.177 0.062 0. 0.013 0.130 0.120 0.022 0.087 0.107 0.125 0.089 0.045 0.079 0.036 0.174 0.180 0.057 0.158 0.168 0.167 0.170 0.071 0.156 PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.004 0.030 0.032 0.004 0.025 0.033 0.032 0.029 0.004 0. 0.003 0.024 0.026 0.005 0.018 0.024 0.028 0.014 0.008 0.017 0.003 0.027 0.036 0.008 0.022 0.028 0.030 0.021 0.004 0.024 0.005 0.057 0.063 0.008 0.048 0.058 0.055 0.052 0.008 0.051 0.003 0.049 0.051 0.006 0.033 0.044 0.049 0.033 0.011 0.033 0.005 0.048 0.065 0.012 0.036 0.047 0.048 0.039 0.008 0.040 0.006 0.070 0.082 0.012 0.064 0.074 0.071 0.066 0.014 0. 0.004 0.068 0.065 0.009 0.041 0.056 0.066 0.042 0.015 0.041 0.006 0.060 0.082 0.013 0.046 0.059 0.063 0.056 0.013 0.052 0.007 0.090 0.100 0.014 0.084 0.092 0.089 0.091 0.023 0.083 0.005 0.077 0.078 0.009 0.050 0.070 0.078 0.049 0.019 0.048 0.007 0.072 0.099 0.016 0.063 0.075 0.075 0.065 0.015 0.065 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Table 13: ASR for different numbers of words changed when using Attention Gradient Rollout as attribution method, aggregated over datasets. Model Aug 1 3 4 5 6 7 9 10 0.014 0.176 0.188 0.027 0.168 0.165 0.180 0.168 0.065 0.172 0.006 0.096 0.099 0.013 0.076 0.074 0.088 0.071 0.029 0.069 0.014 0.138 0.152 0.026 0.129 0.127 0.139 0.134 0.044 0.116 0.015 0.193 0.201 0.029 0.181 0.179 0.188 0.186 0.071 0. 0.008 0.107 0.110 0.014 0.078 0.083 0.102 0.082 0.033 0.076 0.019 0.156 0.169 0.033 0.154 0.150 0.162 0.154 0.054 0.133 0.018 0.200 0.215 0.033 0.194 0.188 0.206 0.197 0.076 0.196 0.009 0.114 0.115 0.018 0.085 0.091 0.115 0.093 0.038 0.082 0.024 0.169 0.184 0.043 0.172 0.166 0.174 0.171 0.062 0.149 0.019 0.208 0.222 0.036 0.204 0.200 0.216 0.204 0.078 0. 0.011 0.122 0.127 0.020 0.093 0.100 0.124 0.099 0.041 0.086 0.028 0.184 0.198 0.049 0.183 0.178 0.185 0.187 0.072 0.161 0.022 0.220 0.231 0.040 0.215 0.212 0.221 0.216 0.084 0.218 0.014 0.130 0.132 0.022 0.100 0.104 0.130 0.105 0.044 0.091 0.033 0.204 0.218 0.057 0.198 0.194 0.203 0.208 0.082 0.178 0.026 0.230 0.239 0.045 0.224 0.219 0.228 0.226 0.089 0. 0.016 0.139 0.141 0.024 0.104 0.111 0.144 0.115 0.048 0.096 0.039 0.216 0.231 0.063 0.211 0.205 0.215 0.223 0.092 0.190 PolBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split HerBERT Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split RoBERTa Diac Key OCR Ort R-Del R-Ins R-Sub R-Sw Rel Split 0.006 0.070 0.076 0.010 0.062 0.066 0.072 0.061 0.019 0. 0.003 0.033 0.034 0.005 0.026 0.026 0.031 0.027 0.007 0.023 0.005 0.038 0.045 0.008 0.035 0.034 0.037 0.035 0.010 0.032 0.007 0.106 0.122 0.014 0.104 0.106 0.115 0.099 0.039 0.097 0.004 0.056 0.053 0.008 0.041 0.044 0.054 0.042 0.015 0.040 0.008 0.067 0.087 0.012 0.060 0.061 0.070 0.066 0.019 0.059 0.008 0.132 0.145 0.020 0.131 0.129 0.137 0.131 0.049 0. 0.005 0.073 0.075 0.009 0.056 0.059 0.070 0.055 0.020 0.054 0.010 0.098 0.115 0.020 0.090 0.089 0.099 0.096 0.029 0.085 0.012 0.153 0.169 0.024 0.157 0.147 0.158 0.151 0.060 0.155 0.005 0.083 0.085 0.011 0.066 0.067 0.082 0.062 0.024 0.062 0.011 0.119 0.132 0.022 0.112 0.105 0.121 0.119 0.037 0.100 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Figure 4: Relation of ASR on smaller LMs and number of perturbed words for different attribution methods. The dotted lines show ASR when selecting random words for perturbation instead of words based on word importance. Figure 5: Relation of ASR on Llama and the number of perturbed words for different attribution methods. The dotted line indicates the robustness cutoff above which the model is considered non-robust to simple perturbations. 21 Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models Figure 6: Relation of ASR on Bielik and the number of perturbed words for different attribution methods. The dotted line indicates the robustness cutoff above which the model is considered non-robust to simple perturbations."
        }
    ],
    "affiliations": [
        "NASK - National Research Institute, Warsaw, Poland"
    ]
}