{
    "paper_title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion",
    "authors": [
        "Shang Liu",
        "Chenjie Cao",
        "Chaohui Yu",
        "Wen Qian",
        "Jing Wang",
        "Fan Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earth's surface, remains an open challenge. We address this through a dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m x 600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, a tailored framework for large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely large-scale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Our project page is available at https://whiteinblue.github.io/earthcrafter/"
        },
        {
            "title": "Start",
            "content": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent Diffusion Shang Liu1,2*, Chenjie Cao1,2,3*, Chaohui Yu1,2, Wen Qian1,2, Jing Wang1,2, Fan Wang1 1DAMO Academy, Alibaba Group, 2Hupan Lab, 3Fudan University {liushang.ls,caochenjie.ccj,huakun.ych, qianwen.qian, yunfei.wj, fan.w}@alibaba-inc.com Project page (code, model, and data): https://whiteinblue.github.io/earthcrafter/ 5 2 0 2 3 2 ] . [ 2 5 3 5 6 1 . 7 0 5 2 : r Figure 1: EarthCrafter enjoys impressive generations conditioned on various guidance, including (a) 1-view aerial semantic and (b) 1-view RGBD. (c) EarthCrafter is powerful enough to handle unconditional generation, sampling reasonable geographic-scale 3D assets from the prior distribution. (d) EarthCrafter is enabled to produce diverse outcomes."
        },
        {
            "title": "Abstract",
            "content": "Despite the remarkable developments achieved by recent 3D generation works, scaling these methods to geographic extents, such as modeling thousands of square kilometers of Earths surface, remains an open challenge. We address this through dual innovation in data infrastructure and model architecture. First, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date, consisting of 50k curated scenes (each measuring 600m600m) captured across the U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene provides pose-annotated multi-view images, depth maps, normals, semantic segmentation, and camera poses, with explicit quality control to ensure terrain diversity. Building on this foundation, we propose EarthCrafter, tailored framework for * denotes equal contributions, Corresponding author. large-scale 3D Earth generation via sparse-decoupled latent diffusion. Our architecture separates structural and textural generation: 1) Dual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D Gaussian Splats (2DGS) into compact latent spaces, largely alleviating the costly computation suffering from vast geographic scales while preserving critical information. 2) We propose condition-aware flow matching models trained on mixed inputs (semantics, images, or neither) to flexibly model latent geometry and texture features independently. Extensive experiments demonstrate that EarthCrafter performs substantially better in extremely largescale generation. The framework further supports versatile applications, from semantic-guided urban layout generation to unconditional terrain synthesis, while maintaining geographic plausibility through our rich data priors from Aerial-Earth3D. Introduction The field of 3D generation has witnessed remarkable progress in recent years, evolving from object-level (Liu et al. 2023b; Shi et al. 2023a; Liu et al. 2024a; Xiang et al. 2025; Ren et al. 2024a; Zhao et al. 2025) to scene-level (Ren et al. 2024b; Zhou et al. 2024; Gao* et al. 2024; Li et al. 2024; Yang et al. 2024; Zhang et al. 2025) synthesis, yielding impressive photorealistic and structurally coherent outcomes. Moreover, recent works have pushed these capabilities toward urbanscale generation under diverse conditions (Xie et al. 2024, 2025b; Deng et al. 2024; Engstler et al. 2025). These achievements lead to new applications in computer graphics, virtual reality, and high-fidelity geospatial modeling. Despite these achievements, critical gap remains in scaling 3D generation to extensive geographic-scalea domain requiring holistic modeling of both anthropogenic structures and natural terrains. We identify two fundamental limitations in existing approaches: 1) Most urban generation frameworks solely focus on the city generation within constrained semantic scopes (Xie et al. 2025b; Deng et al. 2024; Engstler et al. 2025), neglecting other diverse natural formations (e.g., mountains, lakes, and deserts). This requires comprehensive aerial datasets encompassing multi-terrain formations and well-designed models containing scalable capacity to handle the general Earth generation. 2) Since the large-scale 3D generation is inherently intractable, existing generative methods heavily depend on various conditions, including images, semantics, height fields, captions, or combinations of them (Xie et al. 2024, 2025a; Shang et al. 2024; Yang et al. 2024; Ren et al. 2024b; Xiang et al. 2025). While these conditions improve the results, they constrain generative flexibility. Conversely, unconditional generation at geographic scales often collapses into geometric incoherence or textural ambiguity, failing to produce satisfactory outcomes. To address these challenges, we improve both data curation and model architecture to enhance geographic-scale generation. Formally, we present Aerial-Earth3D, the largest 3D aerial dataset created to date. This dataset comprises 50,028 meticulously curated scenes, each spanning 600m600m, sourced across the mainland U.S. with 45 million multi-view frames captured from Google Earth. To effectively cover valid and diverse regions with limited viewpoints, we carefully design heuristic camera poses based on simulated 3D scenes built upon DEM (Models 2001), OSM (OpenStreetMap 2004), and MS-Building (GlobalMLBF 2022) datasets. Since Google Earth does not provide source meshes, we reconstruct 3D meshes via InstantNGP (Muller et al. 2022), applying several post-processing techniques to extract surface planes, fix normals, and refine mesh connectivity. Then these meshes are voxelized as the ground truth for structural generation. Additionally, we employ AIE-SEG (Xu et al. 2023a) to create semantic maps as mesh attributes, comprising 25 distinct classes. As summarized in Table 1, Aerial-Earth3D stands out as large-scale 3D aerial dataset characterized by its diverse terrains and 3D annotations, significantly advancing both 3D generation and reconstruction efforts. Building upon this robust dataset, we present EarthCrafter, novel framework designed for geographic-scale 3D generation through dual-sparse latent diffusion. FollowDataset Area Images Sites Class Source UrbanScene3D (Lin et al. 2022) CityTopia (Xie et al. 2025a) CityDreamer (Xie et al. 2024) Building3D (Wang Ruisheng 2023) MatrixCity (Li et al. 2023) STPLS3D (Chen et al. 2022) SensatUrban (Hu et al. 2021) 136 36 25 998 28 17 7. 128K 3.75K 24K - 519K 62.6K - 16 11 400 16 2 67 3 1 7 6 1 - 32 13 Synth/Real Synth Real Synth Synth Synth/Real Synth/Real Ours 45M 50K 25 Real Table 1: Comparison of aerial-view 3D scene datasets. ing Trellis (Xiang et al. 2025), EarthCrafter inherits the advantages of disentangled structure and texture generations with flexible conditioning and editing capabilities. However, Trellis focuses on object-level generation rather than the geographic scene, while the latter instance contains 10 times more voxels for the geometric modeling, presenting significant challenges in feature storage efficiency, geometric compression, network design, and input condition alignment. Thus, we propose several key innovations to extend this method to geographic scale. Specifically, EarthCrafter integrates dual-sparse VAEs (Xiang et al. 2025) and Flow Matching (FM) diffusion models (Esser et al. 2024) for structure and texture generations, respectively. During the training of the texture VAE, which directly decodes 2D Gaussian Splatting (2DGS) (Huang et al. 2024a) as textural representation, we find that high-resolution voxel features within low channels (Labs 2024) substantially outperform spatially compressed voxel features with large channels (Oquab et al. 2023) in large-scale 3D generation, while the former enjoys lighter I/O overhead. In contrast to (Xiang et al. 2025), we further spatially compress voxel representations of structured VAE via elaborate sparse network design, which allows us to efficiently represent detailed geographic shapes with 97.1% structural accuracy. Additionally, we improve the model designs for both textual and structural FM models to tame the extremely large-scale generation. These models can be flexibly conditioned on images, semantics, or operate without conditions. Especially, we employ novel coarse-to-fine framework for structural FM, which begins by classifying the full voxel initialization into coarse voxel space, followed by refinement phase that converts to fine voxel space while predicting the related latents. This coarse-to-fine modeling enables more precise structures compared to the one-stage dense modeling. We conduct extensive experiments to verify the effectiveness of the proposed method. The key contributions of this paper can be summarized as follows: Aerial-Earth3D is presented as the largest 3D aerial dataset, comprising images captured from diverse structures and natural terrains with annotated 3D presentations. Dual-sparse VAEs are designed for structural and textural encoding, facilitating efficient I/O, superior appearance, and detailed structures for large-scale generation. Tailored flow matching models are proposed to enhance the modeling of latent spaces, while the coarse-to-fine strategy is incorporated for precise structural generation. Related Work 3D Generative Models. Recent advances in 3D generative models have garnered significant attention. Particularly, the rise of 2D generation models (Rombach et al. 2022; Esser et al. 2024; Labs 2024) has led to an exploration of their potential for 3D-aware generation. One line of research involves fine-tuning 2D diffusion models to allow for poseconditioned Novel View Synthesis (NVS) in objects (Liu et al. 2023b,a; Shi et al. 2023a,b; Wang and Shi 2023) or scenes (Sargent et al. 2024; Hollein et al. 2024; Wu et al. 2024; Gao* et al. 2024; Cao et al. 2024). Since their outcomes are primarily multi-view 2D images, converting them into high-quality 3D representations remains challenge due to inherent view inconsistencies. Pioneering work has also been done in distilling priors from 2D diffusion models while optimizing 3D representations (Mildenhall et al. 2021; Kerbl et al. 2023) via Score Distillation Sampling (SDS) (Poole et al. 2023; Kim et al. 2023; Zhu and Zhuang 2023; Wang et al. 2023). However, the test-time SDS is not efficient enough and often produces inferior 3D assets due to over-saturation and multi-face Janus artifacts. Additionally, some research has achieved the creation of large 3D scenes by iteratively stitching 3D representations within depth warping and inpainting novel views (Fridman et al. 2023; Hollein et al. 2023; Yu et al. 2024b; Liang et al. 2024; Shriram et al. 2024; Yu et al. 2024a), suffering from the prohibitive inference cost of testtime dataset updates. Despite the substantial progress made by these approaches, challenges related to 3D consistency, quality, and efficiency continue to impede their extension to complex geographic-scale 3D generation. Feed-Forward 3D Generation. To overcome undesired error accumulation caused by NVS and costly test-time optimization of SDS and warping-inpainting pipelines, several approaches have emerged that directly predict 3D representations, including NeRF (Chan et al. 2023; Xu et al. 2023b; Hong et al. 2024), 3D Gaussian Splatting (3DGS) (Tang et al. 2024; Zou et al. 2024; Ren et al. 2024b; Chen et al. 2024; Li et al. 2024; Yang et al. 2024). Following them, Trellis (Xiang et al. 2025) enhances the capability by employing decoupled generative pipeline that separately models structured intermediates and various types of final 3D outcomes. But Trellis struggles to generate object-level 3D assets with constrained geometric details for large-scale scenes. Moreover, SCube (Ren et al. 2024b) advances this two-stage approach to encompass scene-level generation. However, feed-forward 3D generation is often data-hungry. SCube only considered open-released autonomous driving datasets, lacking exploration into more challenging geographic scenarios. Additionally, many feed-forward methods (Ren et al. 2024b; Chen et al. 2024) stand between the 3D reconstruction and the 3D generation, heavily depending on input conditions such as sparse-view or single-view images. This dependency significantly limits their flexibility for broader applications, including texture editing. Aerial-Earth3D Dataset The Aerial-Earth3D dataset was developed through rigorous process of data curation. Initially, high-quality scenes Figure 2: The overall data pipeline of Aerial-Earth3D. InstantNGP is utilized to achieve source meshes, which are refined with heuristic strategies. Multi-view Flux-VAE features and semantic maps are aggregated on meshes. Then, these featured meshes are voxelized as inputs to TexVAE. were sampled from the Things to do recommendations in Google Earth (GoogleMap), which yielded approximately 150,745 sites of interest across the continental United States. Then, we integrated the OSM driving roads (OpenStreetMap 2004), DEM terrain (Models 2001), and MS-Building (GlobalMLBF 2022) height data to construct high-precision simulated 3D scene, and utilized the simulated scene to design comprehensive viewpoint planning scheme. Next, we feed those viewpoints to Google Earth Studio (GoogleEarth) to render corresponding scene images. Subsequently, we utilize the InstantNGP (Muller et al. 2022) to reconstruct each scene and use marching cube to export 3D scene meshes, and then those meshes are refined through topological repairs. At last, we draw attributes of color, normal, feature, and semantic to those meshes by score aggregation algorithm. In detail, we use the AIE-SEG (Xu et al. 2023a) model for semantic predictions and utilize the Flux-VAE (Labs 2024) encoder to export features; this process is shown in Figure 2. Ultimately, we successfully build 50,028 high-quality 3D scenes, and details are presented in the supplementary. Method Overview. We show the overall pipeline of EarthCrafter in Figure 3(c), comprising separate structure and texture generations. Given randomly initialized 3D noise grid coordi8 )33, the structural flow matching model nates R( 8 )3cs (StructFM) generates structural latents SLa R( with optional depth or semantic conditions, where cs indicates the channel of SLa. Then, the structural VAE (StructVAE) decoder is utilized to decode SLa to high-resolution voxel coordinates Vc RL33. Subsequently, the textural flow matching model (TexFM) generates textural latent TLa RL3ct based on Vc within optional image and semantic conditions, where ct indicates the channel of TLa. We employ the textural VAE (TexVAE) decoder to recover voxel 2DGS VGS RL316 as the final 3D presentation. Dual-Sparse VAEs StructVAE. Differing from the dense architecture of Trellis (Xiang et al. 2025), which suffers from fixed lowresolution voxel space, we propose StructVAE, leveraging spatially compressed structural latent space within sparse voxel modeling to enhance efficiency. As shown in Figure 4(a), StructVAE utilizes an encoder-decoder framework to compress the full voxel coordinates Vc RL33 into Figure 3: Overview of EarthCrafter. EarthCrafter separately models texture and structure in the latent space compressed by TexVAE and StructVAE as illustrated in (a) and (b), respectively. EarthCrafter also contains textural and structural flow-matching models, i.e., TexFM and StructFM, to model related latent presentations. We show the overall pipeline of EarthCrafter in (c), while dashed boxes denote optional conditions. upsample learning of PSS blocks, while full attention layers show superior capacity to learn low-resolution features with ( 8 )3. StructVAE adheres to the VAE learning objective from XCube (Ren et al. 2024a). The innovative StructVAEs architecture achieves both spatially compressed geometry and 97.1% accuracy of structural reconstruction to save the computation of the following generation. TexVAE. 1) Voxelized Features. Following Trellis (Xiang et al. 2025), we aggregate features from images to inject appearance information into geometric voxels. However, challenges persist in the large-scale learning of TexVAE. While we have implemented spatial compression for StructVAE, applying similar feature compression in TexVAE significantly degrades texture recovery performance, as confirmed by our pilot studies. Additionally, learning TexVAE with high-dimensional voxelized features is I/O intolerable; for instance, utilizing 1024-d features from DINOv2 (Oquab et al. 2023) in Trellis (Xiang et al. 2025) would require approximately 471M storage for each scene. Therefore, we propose to use fine-grained, low-channel features instead of coarse, large-channel features for large-scale texture VAE learning. Formally, we select the VAE features trained for FLUX (Labs 2024) (16-channel) as our feature extractor, significantly reducing the feature dimensionality compared to DINOv2. Although FLUX-VAE is tailored for image generation, it demonstrates impressive reconstruction capabilities for texture recovery in our study. To further improve the presentation, we employ the hierarchical FLUX-VAE features through nearest resizing, denoted as [f0; f1; f2] R163=48, where the features are concatenated at scales of 1/1, 1/2, and 1/4, respectively. Moreover, we incorporate the cross-shaped RGB pixels frgb R53=15 and normal features fn R3 as additional features. The final voxelized feature can be expressed as the concatenation: ff eat = [f0; f1; f2; frgb; fn] R66, (1) which only occupies 31M storage for each scene (6.4% compared to DINOv2). Following VCD-Texture (Liu et al. Figure 4: StructVAE. (a) Overview of encoder-decoder based StructVAE. (b) Pseudo-Sparse to Sparse (PSS) block is used to upsample voxels and then classify them from pseudosparse voxels into sparse outcomes as in (c). 8 )3cs, achieving reduction to 1/256 of the origSLa R( inal size, where cs = 32. We first incorporate positional encoding into Vc. Subsequently, 4 transformer layers and one sparse 3D convolution layer (Williams et al. 2024b) with stride of 2 are applied to conduct the voxel downsampling. We should claim that upsampling sparse voxels presents greater challenges than downsampling. Because it is non-trivial to restore accurate sparse geometry after the naive upsample strategies as shown in Figure 4(c). Thus, we present the novel Pseudo-Sparse to Sparse (PSS) block to enable precise upsampling of sparse geometry via sparse pixel shuffle and voxel classification, as detailed in Figure 4(b). Specifically, the tailored pixel shuffle layer is designed to upsample sparse representations, with upsampled voxels termed as pseudo-sparse voxels. This definition indicates that some upsampled voxels are invalid and should be discarded to maintain accurate geometry with reasonable sparsity. Consequently, we propose to employ classification module to recover valid sparse voxels for each PSS block during the upsampling as shown in Figure 4(c). Moreover, sparse Swin transformer (Xiang et al. 2025) is leveraged to improve the or cross-attention mechanisms to align the condition with voxel features. Based on the coarse grid results ˆGclass {1, 1}, we set the threshold of 0, where values greater than 0 indicate valid voxels, while those less than or equal to 0 are deemed invalid. For the fine-grained stage, our model is built within Swin attention (Liu et al. 2021) based U-Net, which takes the threshold filtered results Gc as the sparse input, and fur8 )3cs , where ther predicts the structural latent SLa R( cs = 32. It is crucial to note that the outcomes from this fine-grained stage can also be used to refine the structural coordinates. We set the features of invalid voxels to zero, retaining only those voxels where more than 50% of their channels exceed the threshold of SLa > 0.3. Additionally, to alleviate the domain gap between two stages, we propose voxel dilation augmentation to strengthen the training of the fine-grained stage. Overall, the coarse-to-fine learning of StructFM substantially enhances structural precision, as confirmed by our experiments. Textural Flow Matching (TexFM). Given voxel coordinates Vc RL33 decoded from StructVAE, TexFM produces textural latent features TLa RL3ct with ct = 8 channels. To overcome the computational challenges associated with large-scale texture generation (up to 0.22 million voxels per scene), we enhance the efficiency of the TexFM model through specialized U-Net architecture. Our approach integrates sparse Swin transformer layers with fullattention layers to effectively focus on local and global feature learning, respectively. We empirically find that Swin attention performs well in capturing high-resolution features at scales of 1/1, 1/2, and 1/4, while full attention provides broader receptive field for low-resolution features at scale of 1/8. Such complementary U-Net architecture achieves good balance between the texture quality and learning efficiency. Moreover, TexFM also adopts flexible conditions, including images and semantic segmentations like StructFM. More details about TexFM are presented in our supplementary. Experiments Implementation Details. We employ the AdamW optimizer with learning rate of 1104, following polynomial decay policy. The models are trained for 200,000 iterations using batch size of 64 across 32 H20 GPUs. Regarding data augmentation, we implement voxel cropping and voxel flipping for 3D sparse voxels, with the corresponding camera pose also being transformed. These two basic augmentations are applied across all model training sessions. For TexVAE, StructVAE, and TexFM, we sample training voxels with maximum count limit of 250k. To improve the hole-filling capability of StructFM, we incorporate condition voxel dropping policy based on the condition voxel normal. During inference, the CFG strength and sampling steps are set to 3 and 25 separately. Data Preparation. We begin the data preparation process with filtered set of 50k featured scene meshes. We first extract central mesh of size 500 500 and perform sliding crop to obtain 9 training meshes with size of Figure 5: Overview of the coarse-to-fine StructFM. (a) Condition branch of StructFM, which receives optional inputs: image, semantic, or empty conditions. (b) The coarse stage is devoted to classifying activated voxels. (c) The fine-grained stage focuses on refining voxel coordinates and predicting structural latents based on the outcome from the coarse stage. 2024b), we utilize the score-aggregation to aggregate voxelized features into Vf eat according to distances between projected pixels and voxels, as well as view scores. 2) Model Designs and Learning Objectives. The network architecture of TexVAE follows Trellis (Xiang et al. 2025), utilizing an encoder-decoder model enhanced with 12 sparse Swin transformer layers for each component. The encoder of TexVAE converts the voxelized features Vf eat RL366 to textural latents TLa RL3ct, ct = 8, while the decoder is utilized to translate latent features to 2DGS presentations. These representations consist of offset oi, scaling si, opacity αi, rotation matrix Ri, and spherical harmonics ci. The loss function of TexVAE comprises L1, LPIPS (Zhang et al. 2018), and SSIM losses, and we combine both VGG and AlexNet LPIPS losses to achieve superior visual quality. Additionally, we empirically find that discarding the encoder of TexVAE hinders feature continuity, leading to inferior 2DGS results. Latent Flow Matching (FM) Diffusion Models Structural Flow Matching (StructFM). To handle both accurate voxel classification and structural latent feature prediction, we introduce coarse-to-fine framework as shown in Figure 5. This framework consists of two stages, each of which serves distinctly different predicting objectives. Specifically, the coarse stage is built with pure transformer blocks, focusing on classifying activated voxels, which pre8 )31. The target for dicts the coarse grid ˆGclass R( this classification can be represented as binomial distribution Gclass {1, 1}. So the flow matching of coarse stage models the distribution as p(GclassG, Ccond), where (0, 1) indicates the randomly initialized noise grid; and Ccond {Cimg, Csem, None}, denoting optional image condition Cimg, semantic segmentation Csem, or no condition-based generation. Note that all conditions should be projected into 3D space. For this projection, monocular depth is estimated from the image condition, while semantic and non-condition inputs are assigned dummy depth, i.e., z=128. The conditional network (CondNet) is built within Swin transformer blocks (Liu et al. 2021), utilizing addition ID TexA TexB TexC TexD TexE TexF FeatType DinoSmall f0 f0 f0, f1, f2 ff eat ff eat 200 200 360 360 360 360 Channel 768 16 16 48 66 Net Tube Tube Tube Tube Tube UNet-2 PSNR 17.76 17.36 18.90 19.20 19.49 16.22 L1 0.069 0.070 0.061 0.058 0.056 0.098 Table 2: Ablation studies of TexVAE data type and network. denotes voxel count in training mesh, Tube represents TexVAE without any downsampling, UNET-2 means that TexVAE has two hierarchy layers. materials. Infinite Scene Generation. Due to the constraint of voxel length = 256, we can generate scene area of 146 m2 in single forward pass. Drawing inspiration from mask-based inpainting techniques, which leverage previously generated results to extend the scale of generation, we develop an approach to generate infinite scenes. This method utilizes large semantic map as condition to facilitate the generation of extensive earth scenes using sliding window manner. Constrained by GPU memory limitations in 2DGS rendering, we generate large scenes encompassing 412 m2 with semantic size of 648 648. The results are shown in Figure 7, and more visuals are displayed in supplementary. Note: we obtain large vertical semantic map from validation patchs source scene mesh, which has overlap with training patches. Ablation Studies TexVAE. We first assess voxel feature preparation policy, The experimental results are summarized in Table 2, from which we can draw the following conclusions: 1) Comparing TexA and TexB, under the same voxel resolution = 200, more voxel feature channels can obtain better results. 2) Comparing TexB with TexC, we can conduct fine voxel resolution can achieve significant improvement; Comparing TexA with TexC, we find fine-grained and low-channel voxel features are much better than features with large-channel features and coarse resolution. 3) Comparing TexD and TexE with TexC, we find that large field features and local low-level features can continuously improve performance. 4) Results between TexE and TexF prove that voxel feature can be compressed in the channel dimension, but cant be compressed in the spatial dimension, which largely degrades performance. Based on the above analysis, we choose mixed features ff eat as the basic voxel feature schema, which takes 31M buffers for each ff eat in average. In addition, because TexVAE cant be compressed in spatial, we disentangle structure and texture generation, apply tube-shaped network for TexVAE to compress in channel dimension, and employ U-Net-shaped network for StructVAE for spatial compression. StructVAE. StruceVAE ablations are listed in Table 3, verifying the effectiveness of proposed modules. First, PixShuffle largely boosts the performance, which proves the importance of unambiguity upsample in sparse structure. Next, applying mixed blocks by inserting one convolution block every four Swin transformer blocks can improve performances, too many C-Blocks cant achieve continuous improvement. To Figure 6: Qualitative Comparison. CityDreamer* means that showing results with similar camera distances compared to EarthCrafter. 200 200 m. These training meshes are then voxelized with voxel count of = 360 to generate voxel features Vfeat, where each voxel represents an area of 0.56 m3, calculated as 200/360. This process results in 450k voxel features, and each voxel feature contains 220K voxels on average, which is 10 times larger than Trellis voxel count. Next, we construct global training and validation dataset through height sampling, yielding 447k training and 3, 068 validation samples. Additionally, an ablation dataset is sampled from the New York region, consisting of 3k training items and 300 validation items."
        },
        {
            "title": "Results of Generation",
            "content": "Qualitative Comparison. In this section, we first compare our generation results to other methods. As we focus on BEV scene generation under various conditions. To our knowledge, no research shares similar settings to ours. For example, SCube uses multiple images to generate an FPV scene, and CityDreamer uses strong semantic 3D geometry conditions, which are lifted from 2D height map and semantic map. So we just provide qualitative comparisons. Figure 6 shows the results, and the results of EarthCrafter are generated under 2D semantic map conditions without the height condition. Through qualitative comparison, we observe that SceneDreamer exhibits notable limitations in generating photo-realistic results, particularly manifesting in significant structural artifacts and geometric distortions in architectural elements. While CityDreamer demonstrates improved geometric fidelity at macro level, closer inspection reveals limitations in diversity in both scene geometry and ground-level object distribution, Additionally, the textures rendered by CityDreamer tend to exhibit somewhat cartoonish quality. In contrast, our proposed EarthCrafter framework demonstrates superior performance across multiple aspects, generating results with enhanced photo-realism and greater scene diversity compared to existing baseline methods. Moreover, we show the qualitative results of EarthCrafter based on various conditions in Figure 1, demonstrating flexible capacities, and more visual results are displayed in supplementary Figure 7: Infinite scene (412m2) generation under large semantic condition map. Method Xcube*"
        },
        {
            "title": "Data\nablation\nablation\nablation\nStructVAE ablation\nablation\nglobal\nglobal",
            "content": "PixShuffle C-BLock FullAttn Time Acc 29.1 94.3 14.8 79.2 15.7 94.3 16.8 95.3 19.5 94.9 79.3 96.6 91.0 97.1 Table 3: Ablation results of StructVAE on ablation and train data. Xcube* is re-implemented as StructVAE with the same blocks. PixShuffle means upsample layer in decoder, C-Block means convolution blocks, FullAttn means full attention transformer on lowest resolution layers. Method DenseSFM ClassSFM LatentSFM ClassSFM LatentSFM S-Num S-Index mIoU 3 mIoU 0 1 2 2 2 2 1 1 2 1 2 82.8 84.7 86.1 83.9 84.3 18.9 - 25.4 - 23. Table 4: Results of StructFlows on train data under image condition. S-Num denotes total stage number, S-Index denotes stage index, mIoU 3 and mIoU 0 denote voxel structure metric at L/8 level and level. capture global information, we utilize full attention to replace Swin attention at the lowest layer. However, it observed decline in ablation dataset, but shows improvement in global training set. We empirically think geometry in small dataset prefers local features, but large dataset prefers global features. In addition, we also implement the XCube method with the same block channels, which applies fully sparse convolution block network, as shown in Table 3. Our transformer based StructVAE achieves superior performance to convolution based XCube, while taking less training time. StructFM. To prove the effectiveness of our two-stage sparse struct-latent generation pipeline in coarse-to-fine manner, we implement one-stage dense struct flow model (DenseSFM) to generate sparse struct-latents from dense noised latent volume, and DenseSFM conducts voxel classification and latent generation in one model and applies the same classification schema as the fine stage in StructFM. Table 4 shows the results of different structure generation methods. The pure classification flow model ClassSFM (the first stage in StructFM) achieves better accuracy than DenseSFM. This means latent generation and voxel classification have conflict in dense manner, which leads to performance degradation. However, if we feed the coarse, sparse voxels from ClassSFM to LatentSFM (the second stage in StructFM), we obtain sustained performance gains in voxel classification. In total, compared to the one-stage dense manner, our two-stage method achieves significantly increased classification accuracy(+3.3). Additionally, for the fine-level performance, our two-stage approach also obtains considerable improvement over the one-stage method. This proves the effectiveness of our two-stage coarse-to-fine method. Conclusion In this work, we introduce significant advancements in geographic-scale 3D generation through the development of Aerial-Earth3D and EarthCrafter. By providing the largest 3D aerial dataset to date, we have established robust foundation for effectively modeling diverse range of terrains and structures. Our dual-sparse VAE framework and innovative flow matching models not only enhance the efficiency of generating detailed textures and structures but also address key challenges associated with large-scale computation and data management. The proposed coarse-to-fine structural flow matching model further ensures accurate structural representation while allowing for flexible conditioning based on various inputs. Additionally, we propose to use lowdimensional features from FLUX-VAE to represent voxel textures, enjoying superior reconstruction. The rigorous experiments validate the effectiveness and superiority of our method compared to existing approaches."
        },
        {
            "title": "Supplementary materials",
            "content": "Diverse Generation In addition to the foundational concepts introduced in the main paper, several advanced applications merit exploration. To investigate the potential for diverse generation under the same conditions, three distinct experiments are outlined as follows: Diverse Overall Generation. We assess the overall capacity for diverse generation while maintaining consistent semantic conditions. The results presented in Figure 8 demonstrate our ability to generate varied geometries and textures under the same semantic framework. Semantic-based Diverse Texture Generation. Utilizing ground truth geometry voxels and paired vertical-view semantic map, we generate more realistic and diverse textures. This approach facilitates the enhancement of 3D OpenStreetMap (OSM) data. As shown in Figure 9, this method yields variety of textures under the same geometry and semantic inputs. Unconditional Diverse Texture Generation. Using the ground truth geometry voxels without any conditions, we generate an even broader range of diverse textures. As shown in Figure 10, this method yields variety of textures. More Generation results. We present additional examples of 3D scenes generated by EarthCrafter. These include more semantic condition results Figure 13 , more RGBD condition results Figure 12, more empty(random) condition results Figure 14 and more infinite scene generation results Figure 11. Details of Model Architectures TexVAE. TexVAE employs tube-shaped encoder-decoder architecture, notably designed without any downsampling or upsampling layers. The encoder transforms the voxelized features Vfeat RL366 into textural latent representations TLa RL3ct, where ct = 8. This transformation is achieved using 12 3D-Sparse-Shift-Window Transformer (SWT-block) blocks (Xiang et al. 2025), each configured with 512 feature channels. Conversely, the decoder translates these latent features into 2DGS representations, sharing an identical architectural design. In the final 2DGS representation layer, each voxel spawns 16 Gaussian primitives. Each Gaussian primitive is characterized by 23 associated parameters: its offset oi R3, scaling si R3, opacity αi R1, rotation quaternion Ri R4, and spherical harmonics coefficients ci R12 (corresponding to spherical harmonic degree of 1)."
        },
        {
            "title": "Params",
            "content": "TexVAE StructVAE L/1, L/2 ,L/4 ,L/8 TexFM L/1, L/2 ,L/4 ,L/8 L/"
        },
        {
            "title": "ClassSFM\nLatentSFM",
            "content": "L/8 L/8, L/16 Tube UNet-4 UNet-4 UNet-2 UNet-2 75.8M 258.5M 1191.7M 657.4M 1092.4M Table 5: Summary of Model configurations. StructVAE. StructVAE utilizes an encoder-decoder framework to compress the dense voxel grid coordinates Vc 8 )3cs, RL33 into structural latent representations SLa R( where cs = 32. The encoder of StructVAE performs three downsampling operations, resulting in four hierarchical feature levels. Level T-Blocks Attention C-Blocks Channels D-Sample Level 0 Level 1 Level 2 Level 3 2 2 3 4 3D Swin 3D Swin 3D Swin Full 1 1 1 1 128 256 512 1024 - Table 6: StructVAE endcoder configuration. T-Block: transformer block; C-Block: convolution residual block; DSample: voxel downsampling. Within each encoder level, the processing sequence involves four transformer blocks (T-blocks), followed by one 3D sparse convolution residual block (C-block). Subsequently, 3D sparse convolution layer with stride of 2 is applied for voxel downsampling, and this layer is implemented using the FVDB library (Williams et al. 2024a). Detailed configurations are listed in Table 6. In the decoder, we introduce the novel Pseudo-Sparse to Sparse (PSS) block, designed to enable precise upsampling of sparse geometry at each decoder level. The PSS block operates by first utilizing sparse pixel shuffle layer to upsample the input sparse voxels, generating an initial set of pseudo-sparse voxels. Subsequently, T-blocks are applied to these pseudo-sparse voxels. classification head, comprising two lightweight T-blocks (each with 64 channels), then predicts the activation status of these pseudo-voxels, prediction used to prune inactive or negative voxels and thereby refine the sparse representation. Finally, T-blocks are applied to extract refined sparse voxel features. Detailed configurations of the StructVAE decoder are listed in Table 7. Level Np Ns Attention C-Blocks Channels U-Sample Level 0 Level 1 Level 2 Level 3 4 2 2 0 2 4 4 3D Swin 3D Swin 3D Swin Full 1 1 1 1 128 256 768 1024 - Table 7: StructVAE decoder configuration. Np denotes the transformer blocks used in pseudo-sparse voxels, while Ns refers to the transformer blocks utilized in sparse voxels. USample: voxel upsampling. StructFM. StructFM consists of three parts: the condition branch is multi-level encoder network, the coarse stage (ClassSFM) is DIT-like network with pure transformer blocks, focusing on classifying activated voxels. The finegrained stage (LatentSFM) is U-Net-like architecture aiming to refine first-stage coarse voxels and generate voxel latent feature, which is shown in Figure 15. 1) Condition Branch We first introduce the types of conditions and the associated condition network. There are three Figure 8: Diverse scene generation under semantic condition. Figure 9: Diverse texture generation under semantic condition. Figure 10: Diverse texture generation without condition. Figure 11: Infinite scene (412m2) generation under semantic condition. Figure 12: Scene generation under RGBD image condition. Figure 13: Scene generation under semantic condition. Figure 14: Scene generation without condition."
        },
        {
            "title": "Level Blocks Attention Channels DownSample",
            "content": "Level T-Blocks Attention C-Blocks Channels Level 0 Level 1 Level 2 Level 3 2 2 3 4 3D Swin 3D Swin 3D Swin Full 128 256 512 768 - Table 8: StructFM condition network configuration Level 0 Level 1"
        },
        {
            "title": "Full\nFull",
            "content": "2 2 512 1280 Table 9: LatentSFM network configuration. T-Block: transformer block; C-Block: convolution residual block. categories of conditions: aligned posed image condition Cimg, unaligned semantic condition Csem, and empty condition Cempty. The aligned posed image condition Cimg can be an RGBD or RGB image; in the case of using an RGB image, the depth must be estimated using depth prediction model, such as VGGT. The unaligned semantic condition Csem L2 is represented as 2D semantic map from vertical view, while the empty condition Cempty is also represented as 2D vertical view semantic map with zero semantics. It is essential to note that all conditions need to be projected into 3D space. For the aligned condition Cimg, we project the depth information into 3D space to generate point cloud, which is then voxelized with voxel length of 0.56 to obtain the condition voxel coordic RL33. These coordinates are subsequently fed nates into condition network to produce the final condition voxel Cs , characterized by sparse voxels, features SC where Cs denotes the number of feature channels, set to 768. For unaligned conditions Csem and Cempty, we expand 2D semantic map z-axis to generate 3D plane semantic condic RL33. In experiments, we set the z-axis tion voxels to half of L, and is set to 256. After that, we feed to the condition network, which produces plane condition voxel features, then repeat the plane voxel feature L/8 times to form condition voxel features SC La with dense voxels. Subsequently, we augment SC La with additional voxel position features, which are computed by passing the position embeddings of the condition voxel coordinates of SC La through linear layer. Finally, the condition voxel features are injected into the ClassSFM and LatentSFM networks through addition or cross-attention mechanism. La R( 8 ) The condition network comprises four hierarchical levels, each integrating transformer blocks. Downsampling between levels is achieved through average pooling with stride of 2. The specific configurations of the condition network are listed in Table 8. 2) ClassSFM ClassSFM receives randomly initialized dense voxel grids (0, 1) (L/8)3 as input. It first implements four transformer blocks with full attention, followed by patchify operation on the dense features with stride of 2. Subsequently, 24 additional transformer blocks with full attention are applied. Finally, an up-patchify operation is performed to recover the original voxel resolution, resulting in the final prediction ˆGclass {1, 1}. Specifically, all transformer blocks are configured with 1024 channels, and after patchifying, cross-attention condition injection is conducted every 8 transformer blocks. 3) LatentSFM LatentSFM takes the predicted coarse voxels ˆGc (L/8)3 as inputs. It first employs C-block and Figure 15: StructFM architectures. (a) The condition branch of StructFM. (b) The coarse voxel classification stage, denoted as ClassSFM in SturctFM. (c) The voxel refinement and latent generation, called LatentSFM in StructFM. T-block with full attention as pre-blocks, followed by the insertion of T-block with cross attention to integrate condition features. Subsequently, two-level hierarchical U-Net with skip connection processes the data further. Each U-Net level starts with C-block, followed by several T-blocks with full attention, and concludes with another C-block. Moreover, at the feature merge layer between the first and second levels of the UNet, second cross-attention T-block is introduced to further integrate condition features. LatentSFM performs two simultaneous tasks: coarse voxel refinement and structural voxel latent generation. More detailed configurations are detailed in Table 9. TexFM. TexFM adopts flexible conditions to produce textural latent features TLa RL3ct with ct = 8 channels. TexFM consists of two parts: the condition network (CondNet) and the texture flow network (TexNet). The condition branch is multi-level encoder network, and TexNet is UNet-shaped network, as shown in Figure 16. 1) TexFM CondNet. Unlike the condition network of StructFM, the TexFM condition utilizes rendering projection to map 2D Flux features Cf into 3D space, where Cf is produced by the Flux VAE encoder. In our experiments, we first convert the 2D semantic map Csem into semantic RGB image Csrgb using semantic ID color mapping dictionary. We then feed either Csrgb or Cimg into CondNet to obtain Cf . Specifically, given the camera pose of Cimg, we utilize pinhole rendering model to render Vc RL3 and establish the projection relationship between the image features Cf and Vc, thereby yielding the initial condition voxel features Tc RL3fc, where fc denotes the number of input condition feature channels. For the Flux features Cf derived from the vertical view Csem, we employ parallel up-to-down rays to build the projection relationship, realized through the voxel rendering function in the FVDB library. In the case of"
        },
        {
            "title": "Level Blocks Attention Channels",
            "content": "Level 0 Level 1 Level 2 Level 3 2 2 3 5 3D Swin 3D Swin 3D Swin Full 192 384 768 1280 Table 10: TexFM condition network configuration Level T-Blocks Attention C-Blocks Channels D-Sample Level 0 Level 1 Level 2 Level 3 4 4 8 12 3D Swin 3D Swin 3D Swin Full 2 2 2 2 192 384 768 1280 - Table 11: TexNet network configuration the empty condition, we randomly select 10, 000 voxels from Vc and assign zero values to their features, resulting in empty condition voxel features Tc. Once the condition voxel features Tc are obtained, we feed them into CondNet to generate condition voxel features at each hierarchical level. Similar to the condition network in StructFM, the CondNet in TexFM also comprises four hierarchical levels, each integrating transformer blocks. The specific configurations of the condition network are detailed in Table 10. 2) TexNet. The network of TexNet is similar to LatentSFM; TexNet first employs C-block and T-block with 3D swin attention as pre-blocks. The main architecture is four-level hierarchical U-Net with skip connections. Each U-Net level starts with C-block, followed by several T-blocks, and concludes with another C-block. As TexNet takes 0.22 million voxels as inputs, which is significantly larger than the input voxel number of LatentSFM. In contrast to LatentSFM, which uses cross-attention to integrate condition features. TexNet applies simple addition to merge conditional features, and we design an efficient voxel coordinates alignment algorithm between part condition voxels and global scene voxels to support efficient condition adding and merging. The configurations of TexNet are detailed in Table 11. 3) TexFM Ablations. To enhance efficiency, we conduct detailed ablation studies on the design of TexFM. The experimental results are summarized in Table 12. First, applying C-Blocks at both ends of each UNet boosts performance. Second, while adding deeper layers marginally improves performance, it also considerably increases training time. Third, enlarging the feature channels of each block leads to substantial improvements within reasonable computational budget. Additionally, we explored condition injection using Figure 16: TexFM architectures. (a) The condition branch of TexFM. (b) The main network of TexFM. C-BLock Layers Channels Cross Train Time (h) SN Rimg IDsem 48 51 63 54 17.2 17.3 17.2 17.5 14.2 19.9 19.3 18.9 18.6 19.7 Table 12: Ablation results of TexFM. C-Block: incorporating sparse convolution blocks at both ends of each U-Net level. Layers indicates more transformer blocks at each level, while Channels denotes the enlargement of block channels. Cross signifies condition integration using cross-attention mechanism. SN Rimg and IDsem are used to evaluate the performance under image-based and semantic conditions, respectively. cross-attention mechanism; however, we found minimal correspondence between the condition image and the generated result. Consequently, we opted to replace the cross-attention approach used in StructFM with an additive condition integration method."
        },
        {
            "title": "Training Details",
            "content": "Training Tools We employ the FVDB (Williams et al. 2024a) and SparseTensor (Contributors 2022) libraries to perform sparse layer operations, and utilize PyTorch3D (Nikhila Ravi et al. 2020) to build the rendering projection relation between 3D sparse voxels and 2D image conditions, while utilizing GSplat (Ye et al. 2025) as our 2DGS (Huang et al. 2024b) rendering tool. Training Loss TexVAE Loss. We employ hybrid loss to train TexVAE, which consists of L1, LPIPS (Zhang et al. 2018), and SSIM losses. We combine both VGG and AlexNet LPIPS losses to achieve superior visual quality, which are defined by the following equation: Ltexvae = λl1 Ll1 + λssim Lssim + λvgg Lvgg + λalex Lalex, where Lvgg denotes LPIPS loss with VGG model, Lalex represent LPIPS loss with AlexNet model. The weights of each loss are set to λl1 = 20, λssim = 2, λvgg = 1.4, λalex = 0.6 separately. To evaluate the quality of texture reconstruction using different LPIPS losses, Figure 17 presents the rendering results of 2DGS at the final 3D representation layer of TexVAE. Our findings indicate that TexVAE trained with the VGG LPIPS loss produces lightly blurred texture within the red bounding box, while the tree texture in the yellow bounding box performs well. In contrast, TexVAE trained with the Alex LPIPS loss yields sharper and clearer textures in the red bounding box, but it contains white dot-shaped noise in the yellow bounding box (which becomes more apparent when zoomed out). By combining these two types of LPIPS loss, we are able to achieve improved texture quality in both the red and yellow areas. StructVAE Loss. The StructVAE architecture initially compresses the full voxel coordinates Vc RL33 into la8 )3cs via three successive tent representation SLa R( Figure 17: Results of texture reconstruction changing caused by mixed LPIPS-VGG and LPIPS-ALex loss. VGG means using pure LPIPS-VGG loss; Alex means using pure LPIPS-Alex loss; VGG+ALex means using mixed LPIPS loss. down-sampling operations. Subsequently, these latent features are up-sampled three times, recovering voxel coordinates Vc RL33 through our proposed Pseudo-Sparse to Sparse (PSS) block. Within each PSS block, voxel classification approach is employed to extract valid sparse voxels from the initially up-sampled pseudo-sparse voxels. At each voxel level {0, 1, 2}, the corresponding down-sampled voxels serve as the ground truth for sparse voxels, while represents the up-sampled pseudo-sparse voxels. During training, pseudo-sparse voxels p are matched against the ground truth voxels is also present in , its corresponding classification label is set to 1, indicating an active state and this pseudo voxel should be kept. Otherwise, the label is set to 0, signifying that vl ce, is used to supervise the voxel classification head at each level. The overall StructVAE loss is then defined as: should be filtered. cross-entropy loss, Ll . If pseudo-voxel vl Lstructvae = λ0 L0 ce + λ1 L1 ce + λ2 L2 ce. In our experiments, the loss weights λl for each level were uniformly set to 5. We observed that voxel levels 2 and 1 converged rapidly during training, while convergence at level 0 was delayed until the higher levels had stabilized. During evaluation, the accuracies at levels 2 and 1 reached near-perfect performance (approaching 100%), and level 0 achieved state-of-the-art accuracy of 97.1%, demonstrating the efficacy of our novel network architecture. Flow Model Loss. We employ DiscreteScheduler noise schedule with scheduler shift 3.0, and apply the same conditional flow matching objective as (Xiang et al. 2025) to train TexFM and StructFM."
        },
        {
            "title": "Data Preparation",
            "content": "Collection and 3D Simulation. To create large-scale aerial dataset with representative scenes, we utilized the rating system of the Google Earth platform for scene selection. By setting rating threshold of greater than 3.5 and employing spatial deduplication within 600m grids, we ultimately identified 150,745 high-quality points of interest across the mainland U.S., as shown in Figure 18. Furthermore, we integrated the OSM driving roads (OpenStreetMap 2004), DEM terrain (Models 2001), and MS-Building (GlobalMLBF 2022) height data to construct high-precision simulated 3D scene for the following camera definition. Figure 18: The illustration of data collection of Google Earth. Blue points denote all 150k scenes originally captured from Google Earth, while red points denote the final 50k training scenes filtered for the EarthCrafter training. We employ DEM terrain (Models 2001), MS-Building heights (GlobalMLBF 2022), and driving road paths from OSM (OpenStreetMap 2004) to build the 3D simulation for defining reasonable camera viewpoints. Camera Viewpoints. To achieve maximum scene coverage with minimal sampled perspectives while avoiding outliers such as occlusions and through-modeling, we have developed comprehensive viewpoint planning scheme based on the simulated 3D scene. Specifically, we planned three complementary camera trajectory patterns centered around the latitude and longitude of each point of interest within this simulated 3D scene. 1) Firstly, dual-layer top-down trajectory is established as Top-Pose at height of 500m above ground. This trajectory consists of larger outer square measuring 600m600m and smaller inner square of 100m100m. Eight viewpoints are evenly assigned along the edges of each square, with cameras in the outer square oriented towards the center of the scene and those in the inner square directed toward the scenes edges, which balances capturing the overall scene layout with local details. 2) Secondly, we implemented multi-layer spiral trajectory called Building-Pose, centered around the tallest building in the scene, using top-down sampling method to ensure holistic coverage of the core structure. 3) Finally, we designed an adaptive trajectory system called AdaLevel-Pose. This system creates progressively smaller squares at increasing heights, beginning at minimum altitude of 75m and extending to height of the tallest building plus 225m, while six surrounding viewpoints are assigned to each square. Additionally, an intelligent obstacle avoidance mechanism based on the driving road paths (OpenStreetMap 2004) is incorporated to ensure the accessibility of sampled viewpoints. We finally achieved 97,165 scenes from 150k with valid viewpoint assignments. view direction. The distance score Dn,i is calculated as:"
        },
        {
            "title": "Achievement of Annotated Mesh",
            "content": "InstantNGP Training. We choose an efficient volume rendering approach based NeRF (Mildenhall et al. 2021), called InstantNGP (Muller et al. 2022), as the annotation way to achieve 3D mesh for each aerial scene. Our process begins by converting the latitude and longitude coordinate system into the widely adopted OpenGL coordinate system. First, the latitude and longitude coordinates are converted to EarthCentered, Earth-Fixed (ECEF) coordinates (Earth-centered 2021). Next, we perform transformation from ECEF to East-North-Up (ENU) coordinates (East-North-Up 2012), ultimately establishing an OpenGL coordinate system with the scene center as the origin. In terms of camera trajectory selection, we prioritize retaining views from the Top-Pose and AdaLevel-Pose while selectively excluding trajectories from the Building-Pose to ensure the quality and geometric consistency for reconstruction. For the InstantNGP training of each scene, we set training iterations to 18,000, which takes approximately 5 minutes on single H20 GPU, achieving good balance between computational efficiency and reconstruction quality. Mesh Generation and Post-Processing. To derive 3D meshes from the outputs of InstantNGP, we employ the Marching Cubes algorithm within 600-meter range at voxel resolution of 5123. Next, series of refinements is incorporated to refine the mesh quality. We began by performing the connectivity analysis with Open3D (Qian-Yi Zhou and Koltun 2018) to eliminate floating outliers. Subsequently, we re-render the coarse mesh under each camera pose at resolution of 360640 using NvDiff (Samuli Laine et al. 2020), retaining only the vertices of the coarse mesh that were sampled during rendering. This step reduces the vertex count significantly, from approximately 600M to around 100M, thereby improving the processing efficiency for the subsequent stages. To further enhance mesh quality, we apply PyMeshFix (PyMeshFix) for topological repairs, filling in small holes. For semantic attribute integration, we select all 16 Top-Pose views and 54 highest AdaLevel-Pose views, resulting in total of 72 observation viewpoints that are input into the AIE-SEG (Xu et al. 2023a) model for semantic predictions. Additionally, we refine the mesh for water regions using the average height of water edge to land. After reconstruction and refinement, we retain 88,256 scenes, ultimately filtering down to 50,028 scenes as the final dataset for AerialEarth3D, excluding those with excessive altitudes and overly smooth terrains. Multi-View Score-Aggregation To fuse the view 2D feature maps fn,i (where hw and , representing either RGB images or flux features) into mesh vertex features Fj (where J, and is the number of mesh vertices), we follow multi-step process: We first compute the pixel view score Sn,i, which is the cosine similarity between the mesh normal and the screen Dn,i = 1 dn,i Zf ar , where dn,i denotes the distance between pixel in view and the mesh surface, and Zf ar represents the upper bound of the entire scene. Subsequently, we derive the rendering vertex ID map Vn,i (where Vn,i J) using the NvDiff rasterization operation. Finally, the vertex features Fj are formulated by combining the view score and distance score as follows: Fj = (cid:80)Cj c=1 fc ψ(Dc, τd) ψ(Sc, τs) η , where Cj indicates the number of features with the same vertex index across all view features fn,i, and η = Cj (cid:88) c=1 ψ(Dc, τd) ψ(Sc, τs). Here, ψ represents power function, and both τs and τd are exponents that control the influence of the direction and distance of view, respectively. Specifically, Zf ar is set to 2.0, corresponding to 600 meters in the real scene, and both τs and τd are set to 3.0, indicating that the view distance and the view direction contribute equally to the aggregation of features. Leveraging the efficient rasterization capabilities of NvDiff and PyTorchs index add() operation, we achieve efficient aggregation of multi-view features. The implementation is detailed in the following code snippet. Data Filter The utilization of diverse aerial data sources, such as official Google imagery and Airbus data within Google Earth Studio, combined with the presence of extreme building heights, has led to the reconstruction of scene meshes via Instant-NGP that exhibit irregular geometries. These irregularities primarily manifest in two issues: firstly, scene meshes reconstructed from high-altitude source aerial images, especially from Airbus, tend to overly smooth surfaces, and secondly, there are irregular geometrical distortions observed at the apexes of buildings. To mitigate these issues, scene height map-based filtering approach was implemented. Specifically, we employed Open3D to render 2D downward-looking height maps utilizing parallel downward-facing camera rays. maximum height threshold th was applied to exclude scene meshes exceeding the specified height criterion. Following this, we employ height gradient analysis to filter out smooth meshes. This involves computing the height gradient map and excluding meshes that have an average gradient below predefined minimum threshold tg. Through this comprehensive filtering methodology, we successfully refined the dataset, resulting in final collection of 50,028 scene meshes for Aerial-Earth3D. Figure 19 illustrates the voxel feature attributes, including color, semantics, and normals. The data preparation pipeline effectively reconstructs aerial scenes across diverse environments, capturing precise and varied 3D semantics as well as consistent normal directions. Furthermore, we believe that this dataset can significantly contribute to various tasks, such as 3D and 2D semantic segmentation, building height estimation, outline polygon prediction, and outdoor scene reconstruction for embedded AI and autonomous driving applications. Semantic Distribution To obtain the semantic attributes of scene meshes, we initially attempted to utilize semantic labels from the OpenStreetMap (OSM) dataset. However, the label tags in OSM lack standardization and contain numerous unlabeled regions, which impedes effective semantic annotation. Consequently, we reviewed publicly available segmentation models and selected Florence2 (Xiao et al. 2023), state-of-the-art prompt-based vision foundation model primarily trained on natural images. As illustrated in the top row of Figure 20, the Florence2 model only segments salient objects and fails to detect many targets in crowded aerial view images, such as buildings and trees. Additionally, Florence2 exhibits inefficiency, as it can predict only one prompt class per forward pass. To address these limitations, we explored specialized models within the aerial domain and ultimately adopted the AIE-SEG aerial segmentation model (Xu et al. 2023a). AIE-SEG is trained on proprietary aerial data and supports 25 land cover classification types. The segmentation results are presented in the second row of Figure 20. Compared to Florence2, AIE-SEG effectively segments both object classes (e.g., buildings, cars) and non-object classes (e.g., roads, trees, ground). Moreover, AIE-SEG is more efficient, capable of segmenting all 25 classes in single forward pass. For semantic integration, we selected total of 72 observation viewpoints, comprising 16 Top-Pose views and 54 highest AdaLevel-Pose views. These viewpoints were input into the AIE-SEG model to generate semantic predictions. Subsequently, we integrated the semantic predictions into the scene meshes using the aggregation method. The entire process takes approximately 2 minutes per scene. Furthermore, we analyzed the semantic distribution of all scene meshes, as shown in Figure 21. The analysis reveals that Woodland, Grassland, Building, Pavement, and Road are the top five classes. Detailed semantic distribution information is provided in Table 13. Train and Validation Dataset Split Given filtered set of 50k scene meshes, we generate 450k voxel features, denoted as Vfeat. To construct nearly uniformly distributed validation dataset, we first analyze the maximum voxel heights of each voxel feature. Subsequently, we uniformly partition the voxel features into 20 subgroups based on their maximum voxel heights. From each subgroup, we randomly sample voxel features at ratio of 1/120 to form the validation set, ensuring that each subgroup contributes at least 8 validation samples. This approach yields 447,000 training samples and 3,068 validation samples, which is utilized to train and evaluate both StructVAE and TexVAE models. For the flow matching training, to further mitigate the influence of smooth meshes, we exclude voxel features derived from Airbus data sources by employing an additional template-matching method. This method utilizes mask template based on the Airbus watermark to exclude meshes recon-"
        },
        {
            "title": "Name\nWoodland\nGrassland\nBuilding\nPavement\nRoad\nExcavated Land\nWater\nAgriculture Field\nVehicle\nSoccer Ball Field\nSwimming Pool\nBare Land\nBaseball Diamond\nShip\nTennis Court\nStorage Tank",
            "content": "ID 2 3 4 9 5 6 8 1 17 21 19 7 12 10 13 11 15 Ground Track Field 14 16 23 20 24 22"
        },
        {
            "title": "Basketball Court\nBridge\nHarbor\nRoundabout\nGreenhouse\nPlane\nSolar Panel\nHelicopter",
            "content": "Percentage (%) 28.8137 27.8886 13.9675 13.5912 8.4591 2.3485 1.6379 1.5903 0.9404 0.2278 0.1175 0.1045 0.0610 0.0470 0.0463 0.0385 0.0375 0.0290 0.0243 0.0141 0.0080 0.0047 0.0013 0.0012 0.0001 Color [219, 152, 52] [113, 204, 46] [182, 89, 155] [94, 73, 52] [15, 196, 241] [34, 126, 230] [160, 76, 231] [60, 76, 231] [141, 140, 127] [206, 143, 187] [89, 140, 163] [156, 188, 26] [185, 128, 41] [133, 160, 22] [96, 174, 39] [43, 57, 192] [18, 156, 243] [173, 68, 142] [0, 84, 211] [124, 175, 77] [182, 159, 97] [46, 58, 176] [43, 147, 240] [226, 173, 93] [137, 122, 108] Table 13: Semantic classes with ID, name, percentage, and color. structed from Airbus aerial images, which produce 407,545 training samples and 2,801 validation samples. We use these data to train and evaluate ClassSFM, LatentSFM, and TexFM."
        },
        {
            "title": "Robust Generation",
            "content": "The overall architecture of EarthCrafter consists of distinct structure and texture generation components that utilize three flow models and two variational autoencoder (VAE) models. During the inference process, we arranged these five models sequentially, such that the predictive output of each model serves as the input for the subsequent model. This sequential configuration, however, faces domain gap challenges, primarily stemming from inconsistencies between the outputs generated by earlier models and the inputs required for later models. For instance, we utilize the ground truth highresolution voxel coordinates Vc RL33 to train TexFM. RL33 may However, the predicted voxel coordinates not align precisely with Vc, leading to degradation in performance. To mitigate this discrepancy, we incorporate range of voxel augmentations during model training, enhancing the models robustness to variations in input data. Figure 19: Voxel data visualizations of Aerial-Earth3D. Method TexVAE TexVAE+ jagged perturbation PSNR L1 LPIPS SSIM 0.301 0.314 0.040 0. 0.280 0.292 22.9 22.5 Figure 20: Segmentation comparison between Florance2 and AIE-SEG. Table 14: TexVAE results on global data with/w.o. jagged perturbation. Method TexFM TexFM+ jagged perturbation SN Rimg IDuncond IDsem 20.6 20.3 21.2 25.3 36.7 40.1 Table 15: Results of TexFM on global dataset. SN Rimg denotes PSNR metric under image condition, IDuncond and IDsem represent FID metric under empty condition and semantic map separately."
        },
        {
            "title": "Voxel Roughening",
            "content": "To address the discrepancy between the ClassSFM (the first stage in StructFM) and the LatentSFM (the second stage in StructFM), we implement voxel roughening augmentation during the training of LatentSFM. Voxel roughening comprises two primary operations: voxel dilation and voxel simplification. 8 )3 8 )3 8 )3 La R( We begin by feeding the voxel coordinates Vc RL33 into the StructVAE Encoder to generate the ground truth structural latents SLa R( cs . The latent voxel coordinates in SLa are represented as VLa R( 3. Then, we apply dilation operation with kernel size of dl to expand the sparse latent voxels VLa. Following the dilation, we utilize the voxel simplification operation, which involves downsampling the dilated voxels and then upsampling them by factor of sl. This process ultimately yields the roughened latent voxel 3. Subsequently, we derive the coordinates cs by setting the roughened structural latents SR latent features of voxels that do not exist in SLa to zero, while preserving the original feature values for voxels present in SLa. The roughened voxel coordinates La are then utilized as the training inputs for LatentSFM. During the inference phase, we employ the voxel roughening operation on the sparse voxel outputs generated by ClassSFM. This process is designed to align these outputs more closely with the training inputs utilized by LatentSFM. As illustrated in Figure 23, voxel roughening induces significant changes. comparative analysis of Figure 23(c) and Figure 23(d) reveals that, in the absence of roughening alignment, the discrepancies between the generated coarse voxels and the target voxels can lead to failure in the generation process. La R( 8 )"
        },
        {
            "title": "Voxel Normal Drop",
            "content": "The limitations of aerial views result in unseen regions within the scene mesh, particularly in crowded building environments. Additionally, the Instant-NGP algorithm may encounter difficulties with semi-transparent surfaces, such as water and glass curtain walls of buildings. These two issues contribute to low ratio of scene meshes that contain holes. While small holes in the scene mesh can be filled using PyMeshFix, larger holes are not effectively addressed by third-party tools. Consequently, these holes lead to training Figure 21: Pie chart of the class distribution of AerialEarth3D."
        },
        {
            "title": "Voxel Jagged Perturbation",
            "content": "To address the discrepancies between StructFM and the texture models, we propose voxel coordinate jagged perturbation augmentation policy specifically for training TexFM and TexVAE. Each voxel coordinate in Vc RL33 is represented as pc = (i, j, k) where i, j, [0, L). The generation RL33 inof jagged perturbation voxel coordinates volves the addition of random coordinate offsets to the source voxel coordinates pc. This is computed using the following formula: j = = + randint(1, 1); = k+randint(1, 1). Subsequently, +randint(1, 1); we apply unique operation to remove any duplicated jagged coordinates, and the resulting clean jagged voxel coordinates are utilized as training inputs. For the jagged perturbation of the textural latent variable TLa RL3ct, we modify only the voxel coordinates, preserving the voxel feature values. = (i, j, k); The results presented in Table 14 and Table 15 indicate that the Voxel Jagged Perturbation may initially degrade performance during single model evaluations. However, in the context of sequential inference, we observe that models trained with jagged perturbation exhibit more photorealistic output, as depicted in Figure 22. Figure 22: Results of appearance changing caused by jagged perturbation. (b) means conditional voxels from the conditional images depth. ; (d) denotes generated appearance by using jagged perturbation. Method ClassSFM LatentSFM ClassSFM LatentSFM S-Num 2 2 2 S-Index 1 2 1 2 NormalCrop mIoU 3 mIoU 0 84.7 86.1 83.9 84.3 - 25.4 - 23.3 Figure 23: Visualization comparison with and without roughening. (c) means without applying roughening, (d) means using voxel roughening. voxels that incorporate voids, resulting in both hole-condition voxels and hole-target voxels. As we support one-view image condition generation, and we produce condition voxels by projecting image depth into 3D space and voxelizing it, as shown in Figure 24(b), which provides only partial information. This limitation increases the likelihood of the generated scene voxels exhibiting holes. To resolve this issue, we propose normal direction-based conditional voxel dropping augmentation. Empirically, we have observed that holes in the scene mesh often appear on the side surfaces of objects, such as building facades and the sides of trees. Hence, we drop conditional voxels based on their normal directions. In detail, we first extract two primary normal directions, Nx and Ny, from the conditional voxel normals NC in the XY-plane. Next, we randomly select one normal direction Ns from Nx and Ny and introduce small amount of random noise to Ns. We then compute the cosine similarity Sn between Ns and NC. Subsequently, we drop voxels where Sn > σs, with σs being the dropping score threshold. Finally, we apply dilation and erosion operations to ensure that the resulting drop regions are smoother and more connected. Figure 25 illustrates the outcomes of the voxel normal drop process, showcasing the removal of voxels directed towards the left. Similar to the effectiveness of Voxel Jagged Perturbation, the results presented in Table 16 indicate that applying normal drop augmentation degrades performance during single-task evaluations. This occurs because the predicted voxel scenes are complete, while the ground truth voxels may contain holes. However, the results illustrated in Figure 24 display the geometric changes after applying the voxel normal drop augmentation. When comparing Figure 24(c) with Figure 24(e) and Figure 24(d) with Figure 24(f), it is evident that even with incomplete and irregular conditional voxels, as seen in Figure 24(b), both structure flow models (ClassSFM and LatentSFM) can successfully generate complete scene voxels. Table 16: Results of StructFlows on train data under image condition. S-Num: total stage number; S-Index: stage index; NormalDrop: condition drop augmentation based on condition normal direction; mIoU 3 and mIoU 0 denote voxel structure metric at L/8 level and level."
        },
        {
            "title": "Unbalance Sampling",
            "content": "As we sample interesting scene sites from the mainland of the United States, we encounter significant class imbalance problem. Table 13 illustrates the percentage of each class, revealing that the classes are dominated by ground classes, such as Woodland, Grassland, pavement, and road, the building class constitutes only 14% of the total dataset. To address this imbalance issue, we implement weighted sampling policy that emphasizes the maximum scene voxel height, denoted as Zi. The rationale behind this approach is that larger Zi indicates scene with higher density of buildings. The sampling weight Wi is calculated using the following equations: Wi = zi (cid:80) zi , zi = ( min(Zi, 200) 10 )α (2) Limitations While EarthCrafter exhibits enhanced and diverse capabilities for generating 3D scenes, However it does have certain limitations. First, the limited number of aerial images available for each site results in suboptimal scene mesh geometry generated by Instant-NGP. This limitation manifests in several ways: 1) noticeable holes and distortions can be observed at the corners of trees and the upper portions of tall buildings, and 2) the geometry of roofs and ground surfaces tends to possess higher fidelity compared to side surfaces, such as building facades. These deficiencies negatively influence the overall generation quality, particularly affecting the representation of high-rise structures. Therefore, an enhancement process for the scene mesh geometry may be warranted. Second, the EarthCrafter pipeline is relatively lengthy, comprising five models: three flow models and two variational autoencoders (VAEs). This complexity may hinder efficiency. Figure 24: Results of geometry changing caused by normal drop augmentation. (b) means conditional voxels from the conditional images depth; (e) and (f) represent generated voxels after applying normal drop. Figure 25: Visualization of condition voxels with voxel normal drop. Additionally, the domain gap between the outputs of the previous stages and the training inputs further degrades overall performance, suggesting that simpler approaches warrant exploration to streamline the pipeline. Thirdly, to align with input conditions, our method requires intricate 3D and 2D mapping operations, which constrain flexibility and applicability in out-of-domain scenarios. There is pressing need to investigate more adaptable methods for condition injection, such as leveraging text and image embeddings that do not necessitate strict 3D alignment. This would enhance the systems versatility for use in wider array of applications. Finally, since we exclusively utilize birds eye view (BEV) aerial images from Google Earth Studio, the quality of scene generation in first-person view (FPV) mode is suboptimal. Integrating FPV data in the same location, such as street view images, could improve rendering quality in FPV mode. 1 # planar_vertices represents the mesh vertices, planar_faces represents the mesh faces 2 # is the number of views 3 4 feature = torch.zeros((planar_vertices.shape[0], 3), dtype=torch.float32).to(device) 5 weights = torch.zeros((planar_vertices.shape[0], 1), dtype=torch.float32).to(device) 6 7 for in range(N): 8 9 10 11 cur_render_rast, cur_depth = NvDiff.render_scene_mesh(cur_c2w, planar_vertices, cur_image = load_from_path_i cur_c2w = load_from_pose_i 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 planar_faces) cur_gb_pos, _ = NvDiff.interpolate_one(planar_vertices, cur_render_rast, planar_faces) cur_pix_to_face = cur_render_rast[..., 3:4].long().squeeze() - 1 cur_valid_render_mask = cur_pix_to_face > -1 cur_valid_mask = torch.logical_and(cur_valid_render_mask, water_mask) cur_valid_latents = cur_image[cur_valid_mask] cur_valid_face_idxs = cur_pix_to_face[cur_valid_mask] cur_valid_vertex_idxs = planar_faces[cur_valid_face_idxs] cur_gb_pos = cur_gb_pos[0, cur_valid_mask] cur_t = torch.from_numpy(cur_c2w[:3, 3]).to(cur_gb_pos.device).reshape((1, 3)).float() cur_gb_viewdirs = F.normalize(cur_t - cur_gb_pos, dim=-1) cur_valid_normals = face_normals[cur_valid_face_idxs] cur_valid_normals = F.normalize(cur_valid_normals, p=2, dim=-1, eps=1e-6) cur_view_score = (cur_gb_viewdirs * cur_valid_normals).sum(dim=1).abs() cur_view_score = torch.pow(cur_view_score, exponent_view) cur_valid_depth = torch.clamp(cur_depth[0, cur_valid_mask], 0, 2.0) cur_depth_score = 1.0 - cur_valid_depth / 2.0 cur_depth_score = torch.clamp(cur_depth_score, 0, 1.0) cur_depth_score = torch.pow(cur_depth_score, exponent_depth) cur_score = (cur_view_score * cur_depth_score).view(-1, 1, 1).repeat(1, 3, 1).view(-1, 1) cur_valid_vertex_idxs = cur_valid_vertex_idxs.flatten() cur_valid_latents = cur_latents.view(-1, latent_channel) cur_valid_latents = cur_valid_latents.unsqueeze(1).repeat(1, 3, 1).view(-1, latent_channel) cur_valid_latents = cur_valid_latents * cur_score 43 44 45 46 47 48 feature = feature / (weights + 1e-6) feature.index_add_(0, cur_valid_vertex_idxs, cur_valid_latents) weights.index_add_(0, cur_valid_vertex_idxs, cur_score) Listing 1: 3D Texture Fusion Process Implementation. References Cao, C.; Yu, C.; Liu, S.; Wang, F.; Xue, X.; and Fu, Y. 2024. MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors Enhanced Diffusion Model. arXiv preprint arXiv:2411.16157. Chan, E. R.; Nagano, K.; Chan, M. A.; Bergman, A. W.; Park, J. J.; Levy, A.; Aittala, M.; De Mello, S.; Karras, T.; and Wetzstein, G. 2023. Generative novel view synthesis with 3d-aware diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 42174229. Chen, M.; Hu, Q.; Yu, Z.; Thomas, H.; Feng, A.; Hou, Y.; McCullough, K.; Ren, F.; and Soibelman, L. 2022. Stpls3d: large-scale synthetic and real aerial photogrammetry 3d point cloud dataset. arXiv preprint arXiv:2203.09065. Chen, Y.; Zheng, C.; Xu, H.; Zhuang, B.; Vedaldi, A.; Cham, T.-J.; and Cai, J. 2024. Mvsplat360: Feed-forward 360 scene synthesis from sparse views. arXiv preprint arXiv:2411.04924. Contributors, S. 2022. Spconv: Spatially Sparse Convolution Library. https://github.com/traveller59/spconv. Deng, J.; Chai, W.; Huang, J.; Zhao, Z.; Huang, Q.; Gao, M.; Guo, J.; Hao, S.; Hu, W.; Hwang, J.-N.; et al. 2024. Citycraft: real crafter for 3d city generation. arXiv preprint arXiv:2406.04983. Earth-centered, E.-f. 2021. Earth-centered, Earth-fixed. East-North-Up. 2012. East-North-Up. Engstler, P.; Shtedritski, A.; Laina, I.; Rupprecht, C.; and Vedaldi, A. 2025. SynCity: Training-Free Generation of 3D Worlds. arXiv preprint arXiv:2503.16420. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Fridman, R.; Abecasis, A.; Kasten, Y.; and Dekel, T. 2023. Scenescape: Text-driven consistent scene generation. Advances in Neural Information Processing Systems, 36. Gao*, R.; Holynski*, A.; Henzler, P.; Brussee, A.; MartinBrualla, R.; Srinivasan, P. P.; Barron, J. T.; and Poole*, B. 2024. CAT3D: Create Anything in 3D with Multi-View Diffusion Models. Advances in Neural Information Processing Systems. GlobalMLBF. 2022. GlobalMLBF. GoogleEarth. ???? Accessed: 2024-08-12. GoogleMap. ???? Accessed: 2024-08-12. Hollein, L.; Boˇziˇc, A.; Muller, N.; Novotny, D.; Tseng, H.-Y.; Richardt, C.; Zollhofer, M.; and Nießner, M. 2024. Viewdiff: 3d-consistent image generation with text-to-image models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 50435052. Hollein, L.; Cao, A.; Owens, A.; Johnson, J.; and Nießner, M. 2023. Text2room: Extracting textured 3d meshes from 2d text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 79097920. Hong, Y.; Zhang, K.; Gu, J.; Bi, S.; Zhou, Y.; Liu, D.; Liu, F.; Sunkavalli, K.; Bui, T.; and Tan, H. 2024. Lrm: Large reconstruction model for single image to 3d. In International Conference on Learning Representations. Hu, Q.; Yang, B.; Khalid, S.; Xiao, W.; Trigoni, N.; and Markham, A. 2021. Towards semantic segmentation of urbanscale 3D point clouds: dataset, benchmarks and challenges. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 49774987. Huang, B.; Yu, Z.; Chen, A.; Geiger, A.; and Gao, S. 2024a. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, 111. Huang, B.; Yu, Z.; Chen, A.; Geiger, A.; and Gao, S. 2024b. 2D Gaussian Splatting for Geometrically Accurate Radiance Fields. In SIGGRAPH 2024 Conference Papers. Association for Computing Machinery. Kerbl, B.; Kopanas, G.; Leimkuhler, T.; and Drettakis, G. 2023. 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Trans. Graph., 42(4): 1391. Kim, S.; Lee, K.; Choi, J. S.; Jeong, J.; Sohn, K.; and Shin, J. 2023. Collaborative score distillation for consistent visual editing. Advances in Neural Information Processing Systems, 36: 7323273257. Labs, B. F. 2024. FLUX. https://github.com/black-forestlabs/flux. Li, X.; Lai, Z.; Xu, L.; Qu, Y.; Cao, L.; Zhang, S.; Dai, B.; and Ji, R. 2024. Director3d: Real-world camera trajectory and 3d scene generation from text. Advances in Neural Information Processing Systems, 37: 7512575151. Li, Y.; Jiang, L.; Xu, L.; Xiangli, Y.; Wang, Z.; Lin, D.; and Dai, B. 2023. Matrixcity: large-scale city dataset for cityscale neural rendering and beyond. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 32053215. Liang, Y.; Yang, X.; Lin, J.; Li, H.; Xu, X.; and Chen, Y. 2024. Luciddreamer: Towards high-fidelity text-to-3d generation via interval score matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 65176526. Lin, L.; Liu, Y.; Hu, Y.; Yan, X.; Xie, K.; and Huang, H. 2022. Capturing, Reconstructing, and Simulating: the UrbanScene3D Dataset. In ECCV, 93109. Liu, M.; Shi, R.; Chen, L.; Zhang, Z.; Xu, C.; Wei, X.; Chen, H.; Zeng, C.; Gu, J.; and Su, H. 2024a. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10072 10083. Liu, M.; Xu, C.; Jin, H.; Chen, L.; Varma T, M.; Xu, Z.; and Su, H. 2023a. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. Advances in Neural Information Processing Systems, 36: 2222622246. Liu, R.; Wu, R.; Van Hoorick, B.; Tokmakov, P.; Zakharov, S.; and Vondrick, C. 2023b. Zero-1-to-3: Zero-shot one image to 3d object. In Proceedings of the IEEE/CVF international conference on computer vision, 92989309. Liu, S.; Yu, C.; Cao, C.; Qian, W.; and Wang, F. 2024b. VCDTexture: Variance Alignment based 3D-2D Co-Denoising for Text-Guided Texturing. In European Conference on Computer Vision, 373389. Springer. Liu, Z.; Lin, Y.; Cao, Y.; Hu, H.; Wei, Y.; Zhang, Z.; Lin, S.; and Guo, B. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, 1001210022. Mildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.; Ramamoorthi, R.; and Ng, R. 2021. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1): 99106. Models, D. E. 2001. Digital Elevation Models. Muller, T.; Evans, A.; Schied, C.; and Keller, A. 2022. Instant Neural Graphics Primitives with Multiresolution Hash Encoding. ACM Trans. Graph., 41(4): 102:1102:15. Nikhila Ravi, J. R.; Novotny, D.; Gordon, T.; Lo, W.-Y.; Johnson, J.; and Gkioxari, G. 2020. Accelerating 3D Deep Learning with PyTorch3D. arXiv:2007.08501. OpenStreetMap. 2004. OpenStreetMap. Oquab, M.; Darcet, T.; Moutakanni, T.; Vo, H.; Szafraniec, M.; Khalidov, V.; Fernandez, P.; Haziza, D.; Massa, F.; ElNouby, A.; et al. 2023. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193. Poole, B.; Jain, A.; Barron, J. T.; and Mildenhall, B. 2023. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations. PyMeshFix. ???? Accessed: 2024-08-12. Qian-Yi Zhou, J. P.; and Koltun, V. 2018. Open3D: Modern Library for 3D Data Processing. arXiv:1801.09847. Ren, X.; Huang, J.; Zeng, X.; Museth, K.; Fidler, S.; and Williams, F. 2024a. Xcube: Large-scale 3d generative modeling using sparse voxel hierarchies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 42094219. Ren, X.; Lu, Y.; Liang, H.; Wu, Z.; Ling, H.; Chen, M.; Fidler, S.; Williams, F.; and Huang, J. 2024b. Scube: Instant large-scale scene reconstruction using voxsplats. Advances in Neural Information Processing Systems. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1068410695. Samuli Laine, J. H.; Karras, T.; Seol, Y.; Lehtinen, J.; and Aila, T. 2020. Modular Primitives for High-Performance Differentiable Rendering. ACM Transactions on Graphics, 39(6). Sargent, K.; Li, Z.; Shah, T.; Herrmann, C.; Yu, H.-X.; Zhang, Y.; Chan, E. R.; Lagun, D.; Fei-Fei, L.; Sun, D.; et al. 2024. Zeronvs: Zero-shot 360-degree view synthesis from single In Proceedings of the IEEE/CVF Conference on image. Computer Vision and Pattern Recognition, 94209429. Shang, Y.; Lin, Y.; Zheng, Y.; Fan, H.; Ding, J.; Feng, J.; Chen, J.; Tian, L.; and Li, Y. 2024. UrbanWorld: An Urban World Model for 3D City Generation. arXiv preprint arXiv:2407.11965. Imagedream: Image-prompt arXiv preprint Shi, R.; Chen, H.; Zhang, Z.; Liu, M.; Xu, C.; Wei, X.; Chen, L.; Zeng, C.; and Su, H. 2023a. Zero123++: Single Image to Consistent Multi-view Diffusion Base Model. arXiv:2310.15110. Shi, Y.; Wang, P.; Ye, J.; Long, M.; Li, K.; and Yang, X. 2023b. Mvdream: Multi-view diffusion for 3d generation. arXiv preprint arXiv:2308.16512. Shriram, J.; Trevithick, A.; Liu, L.; and Ramamoorthi, R. 2024. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion. arXiv preprint arXiv:2404.07199. Tang, J.; Chen, Z.; Chen, X.; Wang, T.; Zeng, G.; and Liu, Z. 2024. Lgm: Large multi-view gaussian model for highresolution 3d content creation. In European Conference on Computer Vision, 118. Springer. Wang, P.; and Shi, Y. 2023. multi-view diffusion for 3d generation. arXiv:2312.02201. Wang, Z.; Lu, C.; Wang, Y.; Bao, F.; Li, C.; Su, H.; and Zhu, J. 2023. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36. Wang Ruisheng, S. Y. H. 2023. Building3D: urban-scale dataset and benchmarks for learning roof structures from point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2007620086. Williams, F.; Huang, J.; Swartz, J.; Klar, G.; Thakkar, V.; Cong, M.; Ren, X.; Li, R.; Fuji-Tsang, C.; Fidler, S.; Sifakis, E.; and Museth, K. 2024a. fVDB: Deep-Learning Framework for Sparse, Large Scale, and High Performance Spatial Intelligence. ACM Transactions on Graphics, 43(4): 115. Williams, F.; Huang, J.; Swartz, J.; Klar, G.; Thakkar, V.; Cong, M.; Ren, X.; Li, R.; Fuji-Tsang, C.; Fidler, S.; et al. 2024b. fvdb: deep-learning framework for sparse, large scale, and high performance spatial intelligence. ACM Transactions on Graphics (TOG), 43(4): 115. Wu, R.; Mildenhall, B.; Henzler, P.; Park, K.; Gao, R.; Watson, D.; Srinivasan, P. P.; Verbin, D.; Barron, J. T.; Poole, B.; et al. 2024. Reconfusion: 3d reconstruction with diffusion priors. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2155121561. Xiang, J.; Lv, Z.; Xu, S.; Deng, Y.; Wang, R.; Zhang, B.; Chen, D.; Tong, X.; and Yang, J. 2025. Structured 3D Latents for Scalable and Versatile 3D Generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Xiao, B.; Wu, H.; Xu, W.; Dai, X.; Hu, H.; Lu, Y.; Zeng, M.; Liu, C.; and Yuan, L. 2023. Florence-2: Advancing Unified Representation for Variety of Vision Tasks. arXiv:2311.06242. Xie, H.; Chen, Z.; Hong, F.; and Liu, Z. 2024. Citydreamer: Compositional generative model of unbounded 3d cities. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 96669675. Xie, H.; Chen, Z.; Hong, F.; and Liu, Z. 2025a. CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities. arXiv 2501.08983. Xie, H.; Chen, Z.; Hong, F.; and Liu, Z. 2025b. GaussianCity: Generative Gaussian splatting for unbounded 3D city genIn Proceedings of the IEEE/CVF Conference on eration. Computer Vision and Pattern Recognition. Xu, H.; Man, Y.; Yang, M.; Wu, J.; Zhang, Q.; and Wang, J. 2023a. Analytical insight of earth: cloud-platform of intelligent computing for geospatial big data. arXiv preprint arXiv:2312.16385. Xu, Y.; Tan, H.; Luan, F.; Bi, S.; Wang, P.; Li, J.; Shi, Z.; Sunkavalli, K.; Wetzstein, G.; Xu, Z.; et al. 2023b. Dmv3d: Denoising multi-view diffusion using 3d large reconstruction model. arXiv preprint arXiv:2311.09217. Yang, Y.; Shao, J.; Li, X.; Shen, Y.; Geiger, A.; and Liao, Y. 2024. Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation. arXiv preprint arXiv:2412.21117. Ye, V.; Li, R.; Kerr, J.; Turkulainen, M.; Yi, B.; Pan, Z.; Seiskari, O.; Ye, J.; Hu, J.; Tancik, M.; and Kanazawa, A. 2025. gsplat: An open-source library for Gaussian splatting. Journal of Machine Learning Research, 26(34): 117. Yu, H.-X.; Duan, H.; Herrmann, C.; Freeman, W. T.; and Wu, J. 2024a. WonderWorld: Interactive 3D Scene Generation from Single Image. arXiv preprint arXiv:2406.09394. Yu, H.-X.; Duan, H.; Hur, J.; Sargent, K.; Rubinstein, M.; Freeman, W. T.; Cole, F.; Sun, D.; Snavely, N.; Wu, J.; et al. 2024b. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 66586667. Zhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang, O. 2018. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, 586595. Zhang, S.; Wang, J.; Xu, Y.; Xue, N.; Rupprecht, C.; Zhou, X.; Shen, Y.; and Wetzstein, G. 2025. Flare: Feed-forward geometry, appearance and camera estimation from uncalibrated sparse views. arXiv preprint arXiv:2502.12138. Zhao, Z.; Lai, Z.; Lin, Q.; Zhao, Y.; Liu, H.; Yang, S.; Feng, Y.; Yang, M.; Zhang, S.; Yang, X.; et al. 2025. Hunyuan3d 2.0: Scaling diffusion models for high resolution textured 3d assets generation. arXiv preprint arXiv:2501.12202. Zhou, S.; Fan, Z.; Xu, D.; Chang, H.; Chari, P.; Bharadwaj, T.; You, S.; Wang, Z.; and Kadambi, A. 2024. Dreamscene360: Unconstrained text-to-3d scene generation with panoramic gaussian splatting. In European Conference on Computer Vision, 324342. Springer. HiFA: High-fidelity Zhu, J.; and Zhuang, P. 2023. Text-to-3D Generation with Advanced Diffusion Guidance. arXiv:2305.18766. Zou, Z.-X.; Yu, Z.; Guo, Y.-C.; Li, Y.; Liang, D.; Cao, Y.-P.; and Zhang, S.-H. 2024. Triplane meets gaussian splatting: Fast and generalizable single-view 3d reconstruction with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1032410335."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "Fudan University",
        "Hupan Lab"
    ]
}