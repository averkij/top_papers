{
    "paper_title": "Harnessing Vision Models for Time Series Analysis: A Survey",
    "authors": [
        "Jingchao Ni",
        "Ziming Zhao",
        "ChengAo Shen",
        "Hanghang Tong",
        "Dongjin Song",
        "Wei Cheng",
        "Dongsheng Luo",
        "Haifeng Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models."
        },
        {
            "title": "Start",
            "content": "Harnessing Vision Models for Time Series Analysis: Survey Jingchao Ni1, Ziming Zhao1, ChengAo Shen1, Hanghang Tong2, Dongjin Song3, Wei Cheng4, Dongsheng Luo5, Haifeng Chen4 1University of Houston, 2University of Illinois at Urbana-Champaign, 3University of Connecticut, 4NEC Laboratories America, 5Florida International University 1{jni7, zzhao35, cshen9}@uh.edu, 2htong@illinois.edu, 3dongjin.song@uconn.edu, 4{weicheng, haifeng}@nec-labs.com, 5dluo@fiu.edu 5 2 0 2 3 1 ] . [ 1 9 6 8 8 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the preand post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models."
        },
        {
            "title": "1 Introduction\nVision models have historically been used for time series\nanalysis. Since 1-dimensional (1D) convolutional neural net-\nworks (CNNs), such as WaveNet [Van Den Oord et al., 2016],\nwere found effective in sequence modeling [Bai et al., 2018],\nthey have been extensively adapted to various time series\ntasks [Koprinska et al., 2018; Zhang et al., 2020]. Recently,\nwith the significant advances of sequence modeling in the\nlanguage domain, growing research attentions on time se-\nries have been drawn to methods ranging from Transformers\n[Wen et al., 2023] to Large Language Models (LLMs) [Zhang\net al., 2024]. Meanwhile, the demands for universal modeling\nhave spurred on an explosion of works on time series foun-\ndation models, such as TimesFM [Das et al., 2024], Chronos\n[Ansari et al., 2024] and Time-MoE [Shi et al., 2024].",
            "content": "Figure 1: The general process of leveraging vision models for time series analysis. The red boxes are two views of taxonomy used in this survey. The dashed boxes denote optional, task-dependent steps. As Large Vision Models (LVMs), such as ViT [Dosovitskiy et al., 2021], BEiT [Bao et al., 2022] and MAE [He et al., 2022], become achieving similar success as LLMs (but in vision domain), great deal of emergent efforts has been invested to explore the potential of LVMs in time series modeling [Chen et al., 2024]. This is inspired by the plenty of ways for visualizing time series as images such as line plots of univariate time series (UTS) and heatmaps of multivariate time series (MTS). Such images provide more straightforward view of time series than the counterpart textual representations to humans and, presumably, AI bots. Taking closer inspection reveals more advantages favoring LVMs over LLMs: (1) There is an inherent relationship between images and time series each row/column in an image (per channel) is sequence of continuous pixel values. By pre-training on massive images, LVMs may have learned important sequential patterns such as trends, periods, and spikes [Chen et al., 2024]. In contrast, LLMs are pre-trained on discrete tokens, thus are less aligned with continuous time series. In fact, LLMs effectiveness on time series modeling is in question [Tan et al., 2024]; (2) Instead of using channelindependence assumption [Nie et al., 2023] to individually model each variate in an MTS, some imaging methods (3.7) can naturally represent MTS, enabling explicit correlation encoding; (3) When prompting LLMs, existing methods often struggle with properly verbalizing long sequence (or matrix) of floating numbers in UTS (or MTS), which may be limited by the context length or induce high API costs. In contrast, existing works find that using LVMs on imaged time series is more prompt-friendly and less API-costly [Daswani et al., 2024]; (4) Some imaging methods can encode long time series in compact manner [Naiman et al., 2024], thus have great potential in modeling long-term dependency. Also, the concurrent developments of LLMs and LVMs for time series pave the way for confluence, i.e., leveraging Large Multimodal Models (LMMs), such as LLaVA [Liu et al., 2023], Gemini [Team, 2023] and Claude-3 [Anthropic, 2024], to consolidate the two complementary modalities, which may revolutionize the way (e.g., visually, linguistically, etc.) that users interact with time series. Despite the significance, thorough review of relevant works is absent in the existing literature to the best of our knowledge. The survey [Zhang et al., 2024] discusses few vision models, but its focus is LLMs for time series. In light of this, in this survey, we comprehensively investigate the traditional and the state-of-the-art (SOTA) methods. Fig. 1 identifies the general process of applying vision models for time series analysis, which also serves as the structure of this survey. Our taxonomy has dual view: (1) in Time Series to Image Transformation (3), we review 5 primary imaging methods including Line Plot, Heatmap, Spectrogram, Gramian Angular Field (GAF), Recurrence Plot (RP), and some other methods; (2) in Imaged Time Series Modeling (4), we discuss conventional vision models, LVMs and the initial efforts in LMMs. To highlight the taxonomy, we defer the discussion on the desiderata of preand post-processing to the end of this survey (5). For comparison, we provide Table 1 to summarize the existing methods. Finally, we discuss future directions in this promising field (6). Github repository1 is also maintained to provide up-to-date resources including our code of the imaging methods in 3. We hope this survey could be an orthogonal complement to the existing surveys on Transformer [Wen et al., 2023], LLMs [Zhang et al., 2024; Jiang et al., 2024] and foundation models [Liang et al., 2024] for time series, and provide complete view on the process of using vision models for time series analysis, so as to be an insightful guidebook to the developers in this area."
        },
        {
            "title": "2 Preliminaries and Taxonomy\nIn this paper, a UTS is represented by x = [x1, ..., xT ] ∈\nR1×T where T is the length of the UTS, xt (1 ≤ t ≤ T ) is the\nvalue at time step t. Suppose there are d variates (or features),\nlet xi ∈ R1×T (1 ≤ i ≤ d) be a UTS of the i-th variate, an\nMTS can be represented by X = [x⊤",
            "content": "d ] RdT . 1 , ..., As illustrated in Fig. 1, this survey focuses on methods that transform time series to images, namely imaged time series, and then apply vision models on the imaged time series for tackling time series tasks, such as classification, forecasting and anomaly detection. It is noteworthy that methods on videos or sequential images (a.k.a. image time series [Tarasiou et al., 2023]) do not belong to this category because they dont transform time series to images. Similarly, methods for spaciotemporal traffic data are out of our scope if the meth1https://github.com/D2I-Group/awesome-vision-time-series ods focus on streams of images (e.g., traffic flows in stream of grid maps [Zhang et al., 2017]), but methods on imaging time-space matrices [Ma et al., 2017] that resemble MTSs are included. For vision models on audios, this survey only discusses some representative works in 3.3 due to space limit. The focus of the survey will remain on general time series."
        },
        {
            "title": "3.1 Line Plot\nLine Plot is a straightforward way for visualizing UTSs for\nhuman analysis (e.g., stocks, power consumption, etc.). As\nillustrated by Fig. 2(a), the simplest approach is to draw a 2D\nimage with x-axis representing time steps and y-axis repre-\nsenting time-wise values, with a line connecting all values of\nthe series over time. This image can be either three-channel\n(i.e., RGB) or single-channel as the colors may not be infor-\nmative [Cohen et al., 2020; Sood et al., 2021; Jin et al., 2023;\nZhang et al., 2023]. ForCNN [Semenoglou et al., 2023] even\nuses a single 8-bit integer to represent each pixel for black-\nwhite images. So far, there is no consensus on whether other\ngraphical components, such as legend, grids and tick labels,\ncould provide extra benefits in any task. For example, ViTST\n[Li et al., 2023b] finds these components are superfluous in a\nclassification task, while TAMA [Zhuang et al., 2024] finds\ngrid-like auxiliary lines help enhance anomaly detection.",
            "content": "In addition to the regular Line Plot, MV-DTSA [Yang et al., 2023] and ViTime [Yang et al., 2024] divide an image into grids, and define function to map each time step of UTS to grid, producing grid-like Line Plot. Also, we include methods that use Scatter Plot [Daswani et al., 2024; Prithyani et al., 2024] in this category because Scatter Plot resembles Line Plot but doesnt connect data points with line. By comparing them, [Prithyani et al., 2024] finds Line Plot could induce better time series classification. For MTSs, we defer the discussion on Line Plot to 3.7."
        },
        {
            "title": "3.2 Heatmap\nAs shown in Fig. 2(b), Heatmap is a 2D visualization of\nthe magnitude of the values in a matrix using color. It has\nbeen used to represent the matrix of an MTS, i.e., X ∈\nRd×T , as a one-channel d × T image [Li et al., 2022;\nYazdanbakhsh and Dick, 2019]. Similarly, TimEHR [Karami\net al., 2024] represents an irregular MTS, where the intervals",
            "content": "Figure 2: An illustration of different methods for imaging time series with sample (length=336) from the Electricity benchmark dataset [Nie et al., 2023]. (a)(c)(d)(e)(f) visualize the same variate. (b) visualizes all 321 variates. Filterbank is omitted due to its similarity to STFT. between time steps are uneven, as Heatmap image by grouping the uneven time steps into even time bins. In [Zeng et al., 2021], different method is used for visualizing 9-variate financial MTS. It reshapes the 9 variates at each time step to 3 3 Heatmap image, and uses the sequence of images to forecast future frames, achieving time series forecasting. In contrast, VisionTS [Chen et al., 2024] uses Heatmap to visualize UTSs. Similar to TimesNet [Wu et al., 2023], it first segments length-T UTS into /P length-P subsequences, where is parameter representing periodicity of the UTS. Then the subsequences are stacked into T /P matrix, with 3 duplicated channels, to produce grayscale image input to an LVM. To encode MTSs, VisionTS adopts the channel independence assumption [Nie et al., 2023] and individually models each variate in an MTS. Remark. Heatmap can be used to visualize matrices of various forms. It is also used for matrices generated by the subsequent methods (e.g., Spectrogram, GAF, RP) in this section. In this paper, the name Heatmap refers specifically to images that use color to visualize the (normalized) values in UTS or MTS without performing other transformations."
        },
        {
            "title": "3.3 Spectrogram\nA spectrogram is a visual representation of the spectrum of\nfrequencies of a signal as it varies with time, which are exten-\nsively used for analyzing audio signals [Gong et al., 2021].\nSince audio signals are a type of UTS, spectrogram can be\nconsidered as a method for imaging a UTS. As shown in Fig.\n2(c), a common format is a 2D heatmap image with x-axis\nrepresenting time steps and y-axis representing frequency,\na.k.a. a time-frequency space. Each pixel in the image repre-\nsents the (logarithmic) amplitude of a specific frequency at a\nspecific time point. Typical methods for producing a spectro-\ngram include Short-Time Fourier Transform (STFT) [Grif-\nfin and Lim, 1984], Wavelet Transform [Daubechies, 1990],\nand Filterbank [Vetterli and Herley, 1992].",
            "content": "STFT. Discrete Fourier transform (DFT) can be used to describe the intensity (w) of each constituent frequency of UTS signal R1T . However, (w) has no time dependency. It cannot provide dynamic information such as when specific frequency appear in the UTS. STFT addresses this deficiency by sliding window function g(t) over the time steps in x, and computing the DFT within each window by (w, τ ) = (cid:88) t=1 xtg(t τ )eiwt (1) where is frequency, τ is the position of the window, (w, τ ) describes the intensity of frequency at time step τ . By selecting proper window function g() (e.g., Gaussian/Hamming/Bartlett window), 2D spectrogram (e.g., Fig. 2(c)) can be drawn via heatmap on the squared values (w, τ )2, with as the y-axis, and τ as the x-axis. For example, [Dixit et al., 2024] uses STFT based spectrogram as an input to LMMs for time series classification. Wavelet Transform. Continuous Wavelet Transform (CWT) uses the inner product to measure the similarity between signal function x(t) and an analyzing function. The analyzing function is wavelet ψ(t), where the typical choices include Morse wavelet, Morlet wavelet, etc. CWT compares x(t) to the shifted and scaled (i.e., stretched or shrunk) versions of the wavelet, and output CWT coefficient by c(s, τ ) = (cid:90) x(t) ψ( 1 τ )dt (2) where denotes complex conjugate, τ is the time step to shift, and represents the scale. In practice, discretized version of CWT in Eq. (2) is implemented for UTS [x1, ..., xT ]. It is noteworthy that the scale controls the frequency encoded in wavelet larger leads to stretched wavelet with lower frequency, and vice versa. As such, by varying and τ , 2D spectrogram (e.g., Fig. 2(d)) can be drawn on c(s, τ ), where is the y-axis and τ is the x-axis. Compared to STFT, which uses fixed window size, Wavelet Transform allows variable wavelet sizes larger size for more precise low frequency information. Thus, the methods in [Du et al., 2020; Namura et al., 2024; Zeng et al., 2023] choose CWT (with Morlet wavelet) to generate the spectrogram. Filterbank. This method resembles STFT and is often used in processing audio signals. Given an audio signal, it firstly goes through pre-emphasis filter to boost high frequencies, which helps improve the clarity of the signal. Then, STFT is applied on the signal. Finally, multiple triangle-shaped filters spaced on Mel-scale are applied to the STFT power spectrum (w, τ )2 to extract frequency bands. The outcome filterbank features ˆf (w, τ ) can be used to yield spectrogram with as the y-axis, and τ as the x-axis. Filterbank was adopted in AST [Gong et al., 2021] with 25ms Hamming window that shifts every 10ms for classifying audio signals using Vision Transformer (ViT). It then becomes widely used in the follow-up works such as SSAST [Gong et al., 2022], MAE-AST [Baade et al., 2022], and AST-SED [Li et al., 2023a], as summarized in Table 1."
        },
        {
            "title": "3.4 Gramian Angular Field (GAF)\nGAF was introduced for classifying UTSs using CNNs by\n[Wang and Oates, 2015a]. It was then extended to an impu-",
            "content": "Method TS-Type Imaging [Silva et al., 2013] UTS [Wang and Oates, 2015a] UTS [Wang and Oates, 2015b] UTS [Ma et al., 2017] MTS [Hatami et al., 2018] UTS [Yazdanbakhsh and Dick, 2019] MTS MSCRED [Zhang et al., 2019] MTS [Li et al., 2020] UTS [Cohen et al., 2020] UTS [Barra et al., 2020] UTS VisualAE [Sood et al., 2021] UTS [Zeng et al., 2021] MTS AST [Gong et al., 2021] UTS TTS-GAN [Li et al., 2022] MTS SSAST [Gong et al., 2022] UTS MAE-AST [Baade et al., 2022] UTS AST-SED [Li et al., 2023a] UTS [Jin et al., 2023] UTS ForCNN [Semenoglou et al., 2023] UTS Vit-num-spec [Zeng et al., 2023] UTS ViTST [Li et al., 2023b] MTS UTS* MV-DTSA [Yang et al., 2023] TimesNet [Wu et al., 2023] MTS ITF-TAD [Namura et al., 2024] UTS [Kaewrakmuk et al., 2024] UTS HCR-AdaAD [Lin et al., 2024] MTS FIRTS [Costa et al., 2024] UTS CAFO [Kim et al., 2024] MTS UTS* ViTime [Yang et al., 2024] ImagenTime [Naiman et al., 2024] MTS TimEHR [Karami et al., 2024] MTS UTS* VisionTS [Chen et al., 2024] InsightMiner [Zhang et al., 2023] UTS [Wimmer and Rekabsaz, 2023] MTS [Dixit et al., 2024] [Daswani et al., 2024] TAMA [Zhuang et al., 2024] [Prithyani et al., 2024] UTS MTS UTS MTS RP GAF GAF Heatmap RP Heatmap Other (3.6) RP LinePlot GAF LinePlot Heatmap Spectrogram Heatmap Spectrogram Spectrogram Spectrogram LinePlot LinePlot Spectrogram LinePlot LinePlot Heatmap Spectrogram GAF RP Other (3.6) RP LinePlot Other (3.6) Heatmap Heatmap LinePlot LinePlot Spectrogram LinePlot LinePlot LinePlot Imaged Time Series Modeling Multi-modal Model K-NN CNN CNN CNN CNN CNN ConvLSTM CNN Ensemble CNN CNN CNN,LSTM DeiT ViT ViT MAE SSAST,GRU CNN CNN ViT Swin CNN CNN CNN CNN CNN,GNN CNN CNN,ViT ViT CNN CNN MAE LLaVA CLIP,LSTM GPT4o,Gemini & Claude3 GPT4o,Gemini GPT4o LLaVA Pre-trained Fine-tune Prompt TS-Recover Task Domain Code Classification General Classification General General Multiple Traffic Forecasting Classification General Classification General [1] General [2] Anomaly General [3] Forecasting Finance Classification Finance Classification Finance Forecasting Finance Forecasting Audio [4] Classification Ts-Generation Health [5] Audio [6] Classification Audio [7] Classification EventDetection Audio Physics Classification General Forecasting Finance Forecasting Classification General [8] General [9] Forecasting General [10] Multiple General Anomaly Sensing Classification General Anomaly Classification General [11] General [12] Explanation General [13] Forecasting Ts-Generation General [14] Ts-Generation Health [15] General [16] Forecasting Txt-Generation General Finance Classification Audio General General Multiple Anomaly Classification Classification General [17] Table 1: Taxonomy of vision models on time series. The top panel includes single-modal models. The bottom panel includes multi-modal models. TS-Type denotes type of time series. TS-Recover denotes recovering time series from predicted images (5). *: the method has been used to model the individual UTSs of an MTS. : new pre-trained model was proposed in the work. : when pre-trained models were unused, Fine-tune refers to train task-specific model from scratch. Model column: CNN could be regular CNN, ResNet, VGG-Net, etc. tation task in [Wang and Oates, 2015b]. Similarly, [Barra et al., 2020] applied GAF for financial time series forecasting. Given UTS R1T , the first step is to rescale each xt to value xt within [0, 1] (or [1, 1]). This range enables mapping xt to polar coordinates by ϕt = arccos(xi), with radius = t/N encoding the time stamp, where is constant factor to regularize the span of the polar coordinates. Two types of GAF, Gramian Sum Angular Field (GASF) and Gramian Difference Angular Field (GADF) are defined as GASF: cos(ϕt + ϕt ) = xtxt (cid:113) 1 x2 (cid:113) 1 x2 GADF: sin(ϕt ϕt ) = xt (cid:113) 1 x2 xt (cid:113) 1 x2 (3) which exploits the pairwise temporal correlations in the UTS. Thus, the outcome is matrix with Gt,t specified by either type in Eq. (3). GAF image is heatmap on with both axes representing time, as illustrated by Fig. 2(e)."
        },
        {
            "title": "3.5 Recurrence Plot (RP)\nRP [Eckmann et al., 1987] encodes a UTS into an image that\ncaptures its periodic patterns by using its reconstructed phase",
            "content": "space. The phase space of R1T can be reconstructed by time delay embedding set of new vectors v1, ..., vl with vt = [xt, xt+τ , xt+2τ , ..., xt+(m1)τ ] Rmτ , 1 (4) where τ is the time delay, is the dimension of the phase space, both are hyperparameters. Hence, = (m 1)τ . With vectors v1, ..., vl, an RP image measures their pairwise distances, results in an image whose element RPi,j = Θ(ε vi vj), 1 i, (5) where Θ() is the Heaviside step function, ε is threshold, and is norm function such as ℓ2 norm. Eq. (5) generates binary matrix with RPi,j = 1 if vi and vj are sufficiently similar, producing black-white image (e.g., Fig. 2(f)). An advantage of RP is its flexibility in image size by tuning and τ . Thus it has been used for time series classification [Silva et al., 2013; Hatami et al., 2018], forecasting [Li et al., 2020], anomaly detection [Lin et al., 2024] and explanation [Kim et al., 2024]. Moreover, the method in [Hatami et al., 2018], and similarly in HCR-AdaAD [Lin et al., 2024], omit the thresholding in Eq. (5) and uses vi vj to produce continuously valued images to avoid information loss. Method Line Plot (3.1) Heatmap (3.2) Spectrogram (3.3) UTS GAF (3.4) UTS RP (3.5) UTS Advantages TS-Type UTS, MTS matches human perception of time series UTS, MTS straightforward for both UTSs and MTSs Limitations limited to MTSs with small number of variates the order of variates may affect their correlation learning limited to UTSs; needs proper choice of window/wavelet encodes the time-frequency space encodes the temporal correlations in UTS limited to UTSs; O(T 2) time and space complexity flexibility in image size by tuning and τ limited to UTSs; information loss after thresholding Table 2: Summary of the five primary methods for transforming time series to images. TS-Type denotes type of time series."
        },
        {
            "title": "3.7 How to Model MTS\nIn the above methods, Heatmap (§3.2) can be used to visual-\nize the variate-time matrices, X, of MTSs (e.g., Fig. 1(b)),\nwhere correlated variates should be spatially close to each\nother. Line Plot (§3.1) can be used to visualize MTSs by plot-\nting all variates in the same image [Wimmer and Rekabsaz,\n2023; Daswani et al., 2024] or combining all univariate im-\nages to compose a bigger image [Li et al., 2023b], but these\nmethods only work for a small number of variates. Spectro-\ngram (§3.3), GAF (§3.4), and RP (§3.5) were designed specif-\nically for UTSs. For these methods and Line Plot, which\nare not straightforward in imaging MTSs, the general ap-\nproaches include using channel independence assumption to\nmodel each variate individually [Nie et al., 2023], or stacking\nthe images of d variates to form a d-channel image [Naiman\net al., 2024; Kim et al., 2024]. However, the latter does not\nfit some vision models pre-trained on RGB images which re-\nquires 3-channel inputs (more discussions are deferred to §5).",
            "content": "Remark. As summary, Table 2 recaps the salient advantages and limitations of the five primary imaging methods that are introduced in this section."
        },
        {
            "title": "4.1 Conventional Vision Models\nFollowing traditional image classification, [Silva et al., 2013]\napplies a K-NN classifier on the RPs of time series, [Cohen\net al., 2020] applies an ensemble of fundamental classifiers\nsuch as SVM and AdaBoost on the Line Plots for time series",
            "content": "classification. As an image encoder, CNNs have been widely used for learning image representations. Different from using 1D CNNs on sequences [Bai et al., 2018], 2D or 3D CNNs can be applied on imaged time series as shown in Fig. 3(a). For example, regular CNNs have been used on Spectrograms [Du et al., 2020], tiled CNNs have been used on GAF images [Wang and Oates, 2015a; Wang and Oates, 2015b], dilated CNNs have been used on Heatmap images [Yazdanbakhsh and Dick, 2019]. More frequently, ResNet [He et al., 2016], Inception-v1 [Szegedy et al., 2015], and VGGNet [Simonyan and Zisserman, 2015] have been used on Line Plots [Jin et al., 2023; Semenoglou et al., 2023], Heatmap images [Zeng et al., 2021], RP images [Li et al., 2020; Kim et al., 2024], GAF images [Barra et al., 2020; Kaewrakmuk et al., 2024], and even mixture of GAF, MTF and RP images [Costa et al., 2024]. In particular, for time series generation tasks, GAN frameworks of CNNs [Li et al., 2022; Karami et al., 2024] and diffusion model with U-Nets [Naiman et al., 2024] have also been explored. Due to their small to medium sizes, these models are often trained from scratch using task-specific training data. Meanwhile, fine-tuning pre-trained vision models have already been found promising in cross-modality knowledge transfer for time series anomaly detection [Namura et al., 2024], forecasting [Li et al., 2020] and classification [Jin et al., 2023]."
        },
        {
            "title": "4.2 Large Vision Models (LVMs)\nVision Transformer (ViT) [Dosovitskiy et al., 2021] has in-\nspired the development of modern LVMs such as Swin [Liu\net al., 2021], BEiT [Bao et al., 2022], and MAE [He et al.,\n2022]. As Fig. 3(b) shows, ViT splits an image into patches\nof fixed size, then embeds each patch and augments it with\na positional embedding. The vectors of patches are pro-\ncessed by a Transformer as if they were token embeddings.\nCompared to CNNs, ViTs are less data-efficient, but have\nhigher capacity. Thus, pre-trained ViTs have been explored\nfor modeling imaged time series. For example, AST [Gong\net al., 2021] fine-tunes DeiT [Touvron et al., 2021] on the\nfilterbank spetrogram of audios for classification tasks and\nfinds ImageNet-pretrained DeiT is remarkably effective in\nknowledge transfer. The fine-tuning paradigm has also been\nadopted in [Zeng et al., 2023; Li et al., 2023b] but with differ-\nent pre-trained models such as Swin by [Li et al., 2023b]. Vi-\nsionTS [Chen et al., 2024] attributes LVMs’ superiority over\nLLMs in knowledge transfer to the small gap between the\npre-trained images and imaged time series. It finds that with\none-epoch fine-tuning, MAE becomes the SOTA time series\nforecasters on some benchmark datasets.",
            "content": "Similar to time series foundation models such as TimesFM [Das et al., 2024], there are some initial efforts in pre-training ViT architectures with imaged time series. Following AST, Figure 3: An illustration of different modeling strategies on imaged time series in (a)(b)(c) and task-specific heads in (d). SSAST [Gong et al., 2022] introduced masked spectrogram patch prediction framework for pre-training ViT on large dataset AudioSet-2M. Then it becomes backbone of some follow-up works such as AST-SED [Li et al., 2023a] for sound event detection. For UTSs, ViTime [Yang et al., 2024] generates large set of Line Plots of synthetic UTSs for pre-training ViT, which was found superior over TimesFM in zero-shot forecasting tasks on benchmark datasets."
        },
        {
            "title": "4.3 Large Multimodal Models (LMMs)\nAs LMMs get growing attentions, some notable LMMs, such\nas LLaVA [Liu et al., 2023], Gemini [Team, 2023], GPT-4o\n[Achiam et al., 2023] and Claude-3 [Anthropic, 2024], have\nbeen explored to consolidate the power of LLMs and LVMs in\ntime series analysis. Since LMMs support multimodal input\nvia prompts, methods in this thread typically prompt LMMs\nwith the textual and imaged representations of time series,\nand instructions on what tasks to perform (e.g., Fig. 3(c)).",
            "content": "InsightMiner [Zhang et al., 2023] is pioneer work that uses the LLaVA architecture to generate texts describing the trend of each input UTS. It extracts the trend of UTS by Seasonal-Trend decomposition, encodes the Line Plot of the trend, and concatenates the embedding of the Line Plot with the embeddings of textual instruction, which includes sequence of numbers representing the UTS, e.g., [1.1, 1.7, ..., 0.3]. The concatenated embeddings are taken by language model for generating trend descriptions. Similarly, [Prithyani et al., 2024] adopts the LLaVA architecture, but for MTS classification. An MTS is encoded by the visual embeddings of the stacked Line Plots of all variates. The matrix of the MTS is also verbalized in prompt as the textual modality. By integrating token embeddings, both methods fine-tune some layers of the LMMs with some synthetic data. Moreover, zero-shot and in-context learning performance of several commercial LMMs have been evaluated for audio classification [Dixit et al., 2024], anomaly detection [Zhuang et al., 2024], and some synthetic tasks [Daswani et al., 2024], where the image and textual representations of query time series are integrated into prompt. For in-context learning, these methods inject the images of few example time series and their labels (e.g., classes) into an instruction to prompt LMMs for assisting the prediction of the query time series."
        },
        {
            "title": "4.4 Task-Specific Heads\nFor classification tasks, most of the methods in Table 1 adopt\na fully connected (FC) layer or multilayer perceptron (MLP)\nto transform an embedding into a probability distribution\nover all classes. For forecasting tasks, there are two ap-\n(1) using a de × W MLP/FC layer to directly\nproaches:",
            "content": "predict (from the de-dimensional embedding) the time series values in future time window of size [Li et al., 2020; Semenoglou et al., 2023]; (2) predicting the pixel values that represent the future part of the time series and then recovering the time series from the predicted image [Yang et al., 2023; Chen et al., 2024; Yang et al., 2024] (5 discusses the recovery methods). Imputation and generation tasks resemble forecasting as they also predict time series values. Thus approach (2) has been used for imputation [Wang and Oates, 2015b] and generation [Naiman et al., 2024; Karami et al., 2024]. When using LMMs for classification, text generation, and anomaly detection, most of the methods prompt LMMs to produce the desired outputs in textual answers, circumventing task-specific heads [Zhang et al., 2023; Dixit et al., 2024; Zhuang et al., 2024]."
        },
        {
            "title": "5 Pre-Processing and Post-Processing\nTo be successful in using vision models, some subtle design\ndesiderata include time series normalization, image align-\nment and time series recovery.",
            "content": "Time Series Normalization. Vision models are usually trained on standardized images. To be aligned, the images introduced in 3 should be normalized with controlled mean and standard deviation, as did by [Gong et al., 2021] on spectrograms. In particular, as Heatmap is built on raw time series values, the commonly used Instance Normalization (IN) [Kim et al., 2022] can be applied on the time series as suggested by VisionTS [Chen et al., 2024] since IN share similar merits as Standardization. Using Line Plot requires proper range of y-axis. In addition to rescaling time series [Zhuang et al., 2024], ViTST [Li et al., 2023b] introduced several methods to remove extreme values from the plot. GAF requires min-max normalization on its input, as it transforms time series values withtin [0, 1] to polar coordinates (i.e., arccos). In contrast, input to RP is usually normalization-free as an ℓ2 norm is involved in Eq. (5) before thresholding. Image Alignment. When using pre-trained models, it is imperative to fit the image size to the input requirement of the models. This is especially true for Transformer based models as they use fixed number of positional embeddings to encode the spacial information of image patches. For 3-channel RGB images such as Line Plot, it is straightforward to meet pre-defined size by adjusting the resolution when producing the image. For images built upon matrices such as Heatmap, Spectrogram, GAF, RP, the number of channels and matrix size need adjustment. For the channels, one method is to duplicate matrix to 3 channels [Chen et al., 2024], another way is to average the weights of the 3-channel patch embedding layer into 1-channel layer [Gong et al., 2021]. For the image size, bilinear interpolation is common method to resize input images [Chen et al., 2024]. Alternatively, AST [Gong et al., 2021] resizes the positional embeddings instead of the images to fit the model to desired input size. However, the interpolation in these methods may either alter the time series or the spacial information in positional embeddings. Time Series Recovery. As stated in 4.4, tasks such as forecasting, imputation and generation requires predicting time series values. For models that predict pixel values of images, post-processing involves recovering time series from the predicted images. Recovery from Line Plots is tricky, it requires locating pixels that represent time series and mapping them back to the original values. This can be done by manipulating grid-like Line Plot as introduced in [Yang et al., 2023; Yang et al., 2024], which has recovery function. In contrast, recovery from Heatmap is straightforward as it directly stores the predicted time series values [Zeng et al., 2021; Chen et al., 2024]. Spectrogram is underexplored in these tasks and it remains open on how to recover time series from it. The existing work [Zeng et al., 2023] uses Spectrogram for forecasting only with an MLP head that directly predicts time series. GAF supports accurate recovery by an inverse mapping from polar coordinates to normalized time series [Wang and Oates, 2015b]. However, RP lost time series information during thresholding (Eq. 5), thus may not fit recoverydemanded tasks without using an ad-hoc prediction head."
        },
        {
            "title": "6 Challenges and Future Directions",
            "content": "Fundamental Understanding. Given the multiple methods for imaging time series, the existing works usually pick their own choice by intuition. There remains gap in both theoretical and empirical understanding of research questions such as which imaging methods fit what tasks and whether LVMs truly learn patterns from the images that make them more suitable than LLMs in time series modeling. Some existing works evaluate multiple imaging methods, but in limited tasks. For example, ImagenTime [Naiman et al., 2024] compares the representation abilities of GAF, STFT, and delay embedding (3.6) in time series generation task. However, thorough understanding that can guide future developments of LVMs and LMMs on top of different imaging methods is absent. This survey provides an initial comparative discussion of these methods in 3. Further investigations with empirical validation and theoretical justification is essential to the synergy between LVMs/LMMs and time series analysis. Modeling the Correlation of Variates in MTS. In 3.7, we discussed the existing methods for imaging MTSs. However, each of them has its limitation. For example, when visualizing variate-time matrix by Heatmap image (e.g., Fig. 2(b)), the row variate locates at matters to the downstream modeling of correlations. This is because vision models only encode the spatial relationships of pixels thus correlated variates should be spatially close to each other. Similarly, Line Plots does not enable explicit modeling of correlated variates by vision models. Stacking images, one per variate, into d-channel input may disable the chance to use pre-trained LVMs due to their fixed 3-channel RGB input. As such, effective methods at either the imaging step or the modeling step (e.g., leveraging graph neural networks (GNNs) on variates) that allow correlation learning from MTSs are in demand. Advanced Imaging for Time Series. In addition to the basic methods introduced in 3, it is promising to explore more advanced image representations. For example, InsightMiner [Zhang et al., 2023] adopts Seasonal-Trend decomposition, which is often used to extract components that can serve as inductive bias for time series models. Generalizing it to decompose images such as Spectrogram, GAF, RP into fine-grained representations may further boost vision models ability in time series analysis. Moreover, mixture of imaging may enable encoding of information from different views, such as frequency (Spectrogram), temporal relationships (GAF) and recurrence patterns (RP). FIRTS [Costa et al., 2024] stacks mixture of images in multiple channels for classification task, but it is limited to images of the same size. Modeling mixture of arbitrary images by methods such as multi-view learning may enable more flexibility. Multimodal Time Series Models and Agents. As can be seen from Table 1, the existing research on multimodal analysis (with vision modality) is much less than unimodal analysis, with limited scope of time series tasks. Given the existing LLMs for time series such as Time-LLM [Jin et al., 2024] and S2IP [Pan et al., 2024], it is appealing to introduce vision modality to further boost the performance in wide tasks such as forecasting, classification and anomaly detection. Furthermore, the visual representation of time series provides the foundation for exploring multimodal AI agents [Xie et al., 2024] for more intricate and nuanced tasks that requires reasoning and interactions with environments, such as root cause analysis in AI for IT Operations (AIOps). Vision-based Time Series Foundation Models. foundation model (FM) is deep learning model trained on vast datasets that is applicable to wide range of tasks. Recent time series FMs, such as TimesFM [Das et al., 2024], MOMENT [Goswami et al., 2024], Chronos [Ansari et al., 2024] and Time-MoE [Shi et al., 2024], are mostly built upon LLM architectures and trained on raw time series. Given the potential of image representation, it is promising to explore vision models as new architecture to revolutionize time series FMs. This research direction not only leverages the advantages of LVMs as introduced in 1 (e.g., the prior knowledge extracted from the vast pre-training images), but also enables future development of vision-language FMs for time series."
        },
        {
            "title": "7 Conclusion\nIn this paper, we present the first survey on leveraging vision\nmodels for time series analysis, whose general process struc-\ntures the survey. We propose a new taxonomy consisting of\nimaging and modeling methods for time series. We discuss\nthe pre- and post-processing steps as well. Each category en-\ncompasses representative methods and relevant remarks. The\nsurvey also highlights the challenges and future directions for\nfurther advancing time series analysis with vision models.",
            "content": "References [Achiam et al., 2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, et al. Gpt-4 technical report. arXiv:2303.08774, 2023. [Ansari et al., 2024] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, et al. Chronos: Learning the language of time series. arXiv:2403.07815, 2024. [Anthropic, 2024] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card, 2024. [Baade et al., 2022] Alan Baade, Puyuan Peng, and David Harwath. Mae-ast: Masked autoencoding audio spectrogram transformer. arXiv:2203.16691, 2022. [Bai et al., 2018] Shaojie Bai, Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv:1803.01271, 2018. [Bao et al., 2022] Hangbo Bao, Li Dong, Songhao Piao, et al. Beit: Bert pre-training of image transformers. In ICLR, 2022. [Barra et al., 2020] Silvio Barra, Salvatore Mario Carta, Andrea Corriga, Alessandro Sebastian Podda, et al. Deep learning and time series-to-image encoding for financial forecasting. IEEE/CAA J. Autom. Sin., 7(3):683692, 2020. [Chen et al., 2024] Mouxiang Chen, Lefei Shen, Zhuo Li, Xiaoyun Joy Wang, Jianling Sun, and Chenghao Liu. Visionts: Visual masked autoencoders are free-lunch zero-shot time series forecasters. arXiv:2408.17253, 2024. [Cohen et al., 2020] Naftali Cohen, Tucker Balch, and Manuela Veloso. Trading via image classification. In ICAIF, 2020. [Costa et al., 2024] Henrique Costa, Andre GR Ribeiro, and Vinicius MA Souza. Fusion of image representations for time series classification with deep learning. In ICANN, 2024. [Das et al., 2024] Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting. In ICML, 2024. [Daswani et al., 2024] Mayank Daswani, Mathias MJ Bellaiche, Marc Wilson, Desislav Ivanov, Mikhail Papkov, Eva Schnider, Jing Tang, et al. Plots unlock time-series understanding in multimodal models. arXiv:2410.02637, 2024. [Daubechies, 1990] Ingrid Daubechies. The wavelet transform, time-frequency localization and signal analysis. IEEE Trans. Inf. Theory, 36(5):9611005, 1990. [Dixit et al., 2024] Satvik Dixit, Laurie Heller, and Chris Donahue. Vision language models are few-shot audio spectrogram classifiers. In NeurIPS Workshop, 2024. [Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [Du et al., 2020] Bairui Du, Delmiro Fernandez-Reyes, and Paolo Barucca. Image processing tools for financial time series classification. arXiv:2008.06042, 2020. [Eckmann et al., 1987] J-P Eckmann, Oliffson Kamphorst, et al. Recurrence plots of dynamical systems. EPL, 4(9):973, 1987. [Gong et al., 2021] Yuan Gong, Yu-An Chung, and James Glass. Ast: Audio spectrogram transformer. Interspeech, 2021. [Goswami et al., 2024] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur Dubrawski. Moment: family of open time-series foundation models. In ICML, 2024. [Griffin and Lim, 1984] Daniel Griffin and Jae Lim. Signal estimaIEEE Trans. tion from modified short-time fourier transform. Acoust., 32(2):236243, 1984. [Hatami et al., 2018] Nima Hatami, Yann Gavet, and Johan Debayle. Classification of time-series images using deep convolutional neural networks. In ICMV, 2018. [He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, et al. Deep residual learning for image recognition. In CVPR, 2016. [He et al., 2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022. [Jiang et al., 2024] Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and Dongjin Song. Empowering time series analysis with large language models: survey. In IJCAI, 2024. [Jin et al., 2023] Shuzu Jin, Soumya Mohanty, Qunying Xie, Hanzhi Wang, and Xue-Hao Zhang. Classification of time series as images using deep convolutional neural networks: application to glitches in gravitational wave data. In ASPAI, 2023. [Jin et al., 2024] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, et al. Time-llm: Time series forecasting by reprogramming large language models. In ICLR, 2024. [Kaewrakmuk et al., 2024] Thossapon Kaewrakmuk, Jakkree Srinonchat, et al. Multi-sensor data fusion and time series to image encoding for hardness recognition. IEEE Sens. J., 2024. [Karami et al., 2024] Hojjat Karami, Mary-Anne Hartley, David Atienza, et al. Timehr: Image-based time series generation for electronic health records. arXiv:2402.06318, 2024. [Kim et al., 2022] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In ICLR, 2022. [Kim et al., 2024] Jaeho Kim, Seok-Ju Hahn, Yoontae Hwang, Junghye Lee, and Seulki Lee. Cafo: Feature-centric explanation on time series classification. In KDD, 2024. [Koprinska et al., 2018] Irena Koprinska, Dengsong Wu, and Zheng Wang. Convolutional neural networks for energy time series forecasting. In IJCNN, 2018. [Li et al., 2020] Xixi Li, Yanfei Kang, and Feng Li. Forecasting with time series imaging. Expert Syst. Appl., 160:113680, 2020. [Li et al., 2022] Xiaomin Li, Vangelis Metsis, Huangyingrui Wang, and Anne Hee Hiong Ngu. Tts-gan: transformer-based timeseries generative adversarial network. In AIME, 2022. [Li et al., 2023a] Kang Li, Yan Song, Li-Rong Dai, et al. Ast-sed: An effective sound event detection method based on audio spectrogram transformer. In ICASSP, 2023. [Li et al., 2023b] Zekun Li, Shiyang Li, and Xifeng Yan. Time series as images: Vision transformer for irregularly sampled time series. NeurIPS, 2023. [Liang et al., 2024] Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin Song, Shirui Pan, and Qingsong Wen. Foundation models for time series analysis: tutorial and survey. In KDD, 2024. [Gong et al., 2022] Yuan Gong, Cheng-I Lai, Yu-An Chung, and James Glass. Ssast: Self-supervised audio spectrogram transformer. In AAAI, 2022. [Lin et al., 2024] Chunming Lin, Bowen Du, Leilei Sun, and Linchao Li. Hierarchical context representation and self-adaptive thresholding for multivariate anomaly detection. TKDE, 2024. [Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, et al. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. [Liu et al., 2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 2023. [Ma et al., 2017] Xiaolei Ma, Zhuang Dai, Zhengbing He, Jihui Ma, Yong Wang, and Yunpeng Wang. Learning traffic as images: deep convolutional neural network for large-scale transportation network speed prediction. Sensors, 17(4):818, 2017. [Naiman et al., 2024] Ilan Naiman, Nimrod Berman, Itai Pemper, Idan Arbiv, Gal Fadlon, and Omri Azencot. Utilizing image transforms and diffusion models for generative modeling of short and long time series. NeurIPS, 2024. [Namura et al., 2024] Nobuo Namura, Yuma Ichikawa, et al. Training-free time-series anomaly detection: Leveraging image foundation models. arXiv preprint arXiv:2408.14756, 2024. [Nie et al., 2023] Yuqi Nie, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Longterm forecasting with transformers. In ICLR, 2023. [Pan et al., 2024] Zijie Pan, Yushan Jiang, Sahil Garg, Anderson Schneider, Yuriy Nevmyvaka, and Dongjin Song. S2ip-llm: Semantic space informed prompt learning with llm for time series forecasting. In ICML, 2024. [Prithyani et al., 2024] Vinay Prithyani, Mohsin Mohammed, Richa Gadgil, Ricardo Buitrago, Vinija Jain, and Aman Chadha. On the feasibility of vision-language models for time-series classification. arXiv:2412.17304, 2024. [Semenoglou et al., 2023] Artemios-Anargyros Semenoglou, ImageEvangelos Spiliotis, and Vassilios Assimakopoulos. based time series forecasting: deep convolutional neural network approach. Neural Netw., 157:3953, 2023. [Shi et al., 2024] Xiaoming Shi, Shiyu Wang, Yuqi Nie, Dianqi Li, et al. Time-moe: Billion-scale time series foundation models with mixture of experts. arXiv:2409.16040, 2024. [Silva et al., 2013] Diego Silva, Vinıcius MA De Souza, and Gustavo EAPA Batista. Time series classification using compression distance of recurrence plots. In ICDM, 2013. [Simonyan and Zisserman, 2015] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [Sood et al., 2021] Srijan Sood, Zhen Zeng, Naftali Cohen, Tucker Balch, and Manuela Veloso. Visual time series forecasting: an image-driven approach. In ICAIF, 2021. [Szegedy et al., 2015] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, et al. Going deeper with convolutions. In CVPR, 2015. [Tan et al., 2024] Mingtian Tan, Mike Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are language models actually useful for time series forecasting? In NeurIPS, 2024. [Tarasiou et al., 2023] Michail Tarasiou, Erik Chavez, and Stefanos Zafeiriou. Vits for sits: Vision transformers for satellite image time series. In CVPR, 2023. [Team, 2023] Gemini Team. Gemini: family of highly capable multimodal models. arXiv:2312.11805, 2023. [Touvron et al., 2021] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, et al. Training data-efficient image transformers & distillation through attention. In ICML, 2021. [Van Den Oord et al., 2016] Aaron Van Den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, et al. Wavenet: generative model for raw audio. arXiv:1609.03499, 12, 2016. [Vetterli and Herley, 1992] Martin Vetterli and Cormac Herley. Wavelets and filter banks: Theory and design. IEEE Trans. Signal Process., 40(9):22072232, 1992. [Wang and Oates, 2015a] Zhiguang Wang and Tim Oates. Encoding time series as images for visual inspection and classification In AAAI Workshop, using tiled convolutional neural networks. 2015. [Wang and Oates, 2015b] Zhiguang Wang and Tim Oates. Imaging time-series to improve classification and imputation. In IJCAI, 2015. [Wen et al., 2023] Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun. Transformers in time series: survey. In IJCAI, 2023. [Wimmer and Rekabsaz, 2023] Christopher Wimmer and Navid Rekabsaz. Leveraging vision-language models for granular market change prediction. arXiv:2301.10166, 2023. [Wu et al., 2023] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, et al. Timesnet: Temporal 2d-variation modeling for general time series analysis. In ICLR, 2023. [Xie et al., 2024] Junlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: survey. arXiv:2402.15116, 2024. [Yang et al., 2023] Luoxiao Yang, Xinqi Fan, et al. Your time series is worth binary image: machine vision assisted deep framework for time series forecasting. arXiv:2302.14390, 2023. [Yang et al., 2024] Luoxiao Yang, Yun Wang, Xinqi Fan, Israel Cohen, et al. Vitime: visual intelligence-based foundation model for time series forecasting. arXiv:2407.07311, 2024. [Yazdanbakhsh and Dick, 2019] Omolbanin Yazdanbakhsh and Scott Dick. Multivariate time series classification using dilated convolutional neural network. arXiv:1905.01697, 2019. [Zeng et al., 2021] Zhen Zeng, Tucker Balch, et al. Deep video prediction for time series forecasting. In ICAIF, 2021. [Zeng et al., 2023] Zhen Zeng, Rachneet Kaur, Suchetha Siddagangappa, et al. From pixels to predictions: Spectrogram and vision transformer for better time series forecasting. In ICAIF, 2023. [Zhang et al., 2017] Junbo Zhang, Yu Zheng, and Dekang Qi. Deep spatio-temporal residual networks for citywide crowd flows prediction. In AAAI, 2017. [Zhang et al., 2019] Chuxu Zhang, Dongjin Song, Yuncong Chen, Xinyang Feng, Cristian Lumezanu, Wei Cheng, Jingchao Ni, et al. deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data. In AAAI, 2019. [Zhang et al., 2020] Xuchao Zhang, Yifeng Gao, Jessica Lin, and Chang-Tien Lu. Tapnet: Multivariate time series classification with attentional prototypical network. In AAAI, 2020. [Zhang et al., 2023] Yunkai Zhang, Yawen Zhang, Ming Zheng, Kezhen Chen, Chongyang Gao, Ruian Ge, et al. Insight miner: time series analysis dataset for cross-domain alignment with natural language. In NeurIPS AI4Science, 2023. [Zhang et al., 2024] Xiyuan Zhang, Ranak Roy Chowdhury, Rajesh Gupta, and Jingbo Shang. Large language models for time series: survey. In IJCAI, 2024. [Zhuang et al., 2024] Jiaxin Zhuang, Leon Yan, Zhenwei Zhang, et al. See it, think it, sorted: Large multimodal models are fewshot time series anomaly analyzers. arXiv:2411.02465, 2024."
        }
    ],
    "affiliations": [
        "Florida International University",
        "NEC Laboratories America",
        "University of Connecticut",
        "University of Houston",
        "University of Illinois at Urbana-Champaign"
    ]
}