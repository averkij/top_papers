{
    "paper_title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
    "authors": [
        "Leqi Shen",
        "Guoqiang Gong",
        "Tianxiang Hao",
        "Tao He",
        "Yifeng Zhang",
        "Pengzhang Liu",
        "Sicheng Zhao",
        "Jungong Han",
        "Guiguang Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is a prominent area of research. While CLIP is focused on image-level vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from image-level to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neglecting language and alignment. In this paper, we propose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 7 8 8 8 0 . 6 0 5 2 : r DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval Leqi Shen1,2,4* Guoqiang Gong5 Tianxiang Hao1,2 Tao He6,7 Yifeng Zhang5 Pengzhang Liu5 Sicheng Zhao2 Jungong Han3 Guiguang Ding1,2 1 School of Software 2 BNRist 3 Department of Automation, Tsinghua University 4 Hangzhou Zhuoxi Institute of Brain and Intelligence 5 JD.com 6 GRG Banking Equipment Co., Ltd. 7 South China University of Technology"
        },
        {
            "title": "Abstract",
            "content": "The parameter-efficient adaptation of the image-text pretraining model CLIP for video-text retrieval is prominent area of research. While CLIP is focused on imagelevel vision-language matching, video-text retrieval demands comprehensive understanding at the video level. Three key discrepancies emerge in the transfer from imagelevel to video-level: vision, language, and alignment. However, existing methods mainly focus on vision while neIn this paper, we proglecting language and alignment. pose Discrepancy Reduction in Vision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all three discrepancies. Specifically, we introduce Image-Video Features Fusion to integrate image-level and video-level features, effectively tackling both vision and language discrepancies. Additionally, we generate pseudo image captions to learn fine-grained image-level alignment. To mitigate alignment discrepancies, we propose Image-to-Video Alignment Distillation, which leverages image-level alignment knowledge to enhance video-level alignment. Extensive experiments demonstrate the superiority of our DiscoVLA. In particular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous methods by 1.5% in R@1, reaching final score of 50.5% R@1. The code is available at https://github.com/LunarShen/DsicoVLA. Method Image-Level PETL==== Video-Level Vision Language Alignment Previous Methods Our DiscoVLA (cid:33) (cid:33) (cid:33) (cid:33) Figure 1. When parameter-efficient transferring image-level CLIP to video-text retrieval, we identify three key discrepancies: vision, alignment, and language. Unlike previous methods, our DiscoVLA aims to address all these discrepancies. More visualization examples are provided in Figure 4. 1. Introduction The rapid expansion of online video content has generated growing demand for effective video-text retrieval, focusing on matching video content with relevant textual descriptions. Inspired by the success of image-text pretraining approaches such as CLIP [42], recent efforts [9, 14, *lunarshen@gmail.com. Work done during an internship at JD.com. Project leader. Corresponding authors. 24, 37, 48, 49, 51] have increasingly focused on extending these powerful capabilities to video-text retrieval tasks. However, fully fine-tuning these pretrained models requires updating numerous parameters for every dataset, resulting in large storage overhead [22, 58]. Therefore, we focus on parameter-efficient transfer learning methods, targeting strong performance with minimal trainable parameters. Although both image-text pretraining and video-text retrieval involve vision-language (VL) matching, they exhibit substantial discrepancies that hinder straightforward knowledge transfer. To be specific, image-text pretraining focuses on matching images with image captions, which represents image-level VL matching. In contrast, video-text retrieval focuses on matching videos with video captions, which represents video-level VL matching. The challenges of videolevel VL matching, which demands understanding motion and time, create significant obstacles for transferring knowledge from image-text to video-text tasks. In this work, we explore the discrepancies between image-level and video-level VL matching across three key aspects: vision, language, and alignment. Videos introduce temporal dimension, which is absent in images. Similarly, the distinction between image captions and video captions arises from the varying levels of granularity, such as isolated image descriptions versus continuous video narratives. Thus, video-level alignment is inherently more complex than image-level alignment, as it must account for intricate spatio-temporal relationships. In Figure 1, the three images correspond to preparing food, pouring, and cutting, while the entire video demonstrates an instructional cooking process. Image-level VL matching focuses on isolated images, while video-level VL matching emphasizes the overall understanding of continuous segments. However, current methods [5, 22, 25, 58] struggle to fully address the discrepancies mentioned above. Some [5, 22, 25, 58] focus on the vision aspect, modeling temporal relationships to extract video-level features, while others [25, 58] attempt to reduce the modality gap between vision and language through shared parameter mechanisms. Despite these advances, these methods remain limited in both language and alignment discrepancies. To tackle these challenges, we introduce Discrepancy Reduction in Vision, Language, and Alignment, termed DiscoVLA, novel parameter-efficient method for videotext retrieval. Unlike previous methods that target single gap, our method aims to simultaneously reduce all significant discrepancies, as illustrated in the table of Figure 1. Specifically, we propose an Image-Video Features Fusion module (IVFusion), which addresses both vision and language discrepancies through unified feature exIVFusion effectively combines both traction approach. imageand video-level features by utilizing lightweight adapter. In addition to video-level alignment, we propose Pseudo Image-level Alignment (PImgAlign) which generates pseudo image captions and learns fine-grained imagelevel alignment. Furthermore, to reduce alignment discrepancy, we propose Image-to-Video Alignment Distillation (AlignDistill), transferring image-level alignment knowledge to improve video-level alignment. All proposed modules prioritize parameter efficiency: IVFusion requires minimal trainable parameters, while PImgAlign and AlignDistill operate without increasing inference parameters. In summary, the main contributions are as follows: We reveal, for the first time, the necessity of addressing all three image-to-video discrepanciesvision, language, and alignmentto achieve parameter-efficient VTR. We introduce IVFusion to fuse imageand video-level features for both vision and language gaps. PImgAlign is introduced for fine-grained image-level alignment. We introduce AlignDistill to minimize alignment gaps. Our DiscoVLA achieves results on MSRVTT, LSMDC, ActivityNet, and DiDeMo. Notably, on MSRVTT with CLIP (ViT-B/16), DiscoVLA reaches 50.5% R@1, surpassing previous methods by 1.5%. state-of-the-art 2. Related Work Video-Text Retrieval. Advances in vision-language pretraining [7, 12, 3234, 42, 50, 52, 55] have achieved significant success across various downstream tasks [2, 10, 11, 18, 26, 45, 47, 54, 56, 57]. Leveraging powerful CLIP [42], recent video-text retrieval (VTR) methods [9, 14, 24, 37, 48, 49, 51] have achieved impressive performance enhancements. CLIP4Clip [37] is pioneering CLIP-based method that aggregates image features via mean-pooling or transformers to obtain video features, inspiring subsequent studies. Additionally, Cap4Video [51] utilizes ZeroCap [46] and GPT-2 [41] to generate video-level captions as supplementary video content during both training and inference. In contrast, our DiscoVLA addresses the image-level and video-level gap by generating image-level captions used only in training to improve video-level alignment. These full fine-tuning methods impose substantial overhead from large trainable parameters. Parameter-Efficient Transfer Learning. Parameterefficient methods aim to adapt pre-trained models to new tasks with minimal fine-tuning. The mainstream methods in natural language processing and image tasks include techniques such as Prompt [23, 27, 28, 61, 62], Adapter [6, 17, 20, 60], and LoRA [21, 60]. Prompt [27] modifies the input space by adding learnable tokens. Adapter [20] introduces lightweight bottleneck neural network consisting of upand down-projection layers with non-linear activation function. LoRA [21] aims to update upand down-projection layers by learning low-rank decomposition matrices. Recently, several studies have explored parameterefficient VTR methods [5, 22, 25, 58] to incorporate temporal modeling into the frozen image-level CLIP backbone. VoP [22] employs trainable BiLSTMs [16] to produce prompt tokens in the vision encoder. DGL [58] introduces weight-sharing layers to generate prompts for both text and video encoders. MV-Adapter [25] introduces shared bottleneck structures in both video and text branches. However, these methods overlook language and alignment discrepancies. We address these limitations with DiscoVLA, framework that facilitates robust video-level understanding. Figure 2. The overall framework of DiscoVLA. Initially, we generate pseudo image captions for each sampled image. In both vision and text encoders, we utilize image-level layers to acquire pretrained image-level knowledge and employ IVFusion layers to enhance spatiotemporal information. The single video caption is encoded through the text encoder, utilizing IVFusion layer as image-level (image-lvl) layer. Finally, AlignDistill is applied to distill image-level alignment to video-level alignment. For fair comparisons with previous methods, we do not generate pseudo image captions during the inference phase. 3. Methodology 3.1. Preliminary Video-text retrieval is task that involves retrieving relevant videos based on given text query, or conversely, retrieving appropriate text descriptions for video. Consistent with previous approaches [5, 22, 25, 58], we utilize the CLIP model, pre-trained on image-text data, as our backbone. To process given video, we first sample sequence of frame images uniformly, which are then input into the CLIP vision encoder to extract image features vimg. These features are then aggregated through temporal average pooling to get video features vvid. For given video caption, it is passed through the CLIP text encoder to extract video caption features tvid. Then, we compute the cross-modal contrastive loss [40], which functions as video-level alignment term, maximizing the video-level similarity Simvid between matched pairs: Simvid ij = tvidT vvid , Lt2v(Simvid) = Lv2t(Simvid) = 1 1 B (cid:88) i=1 (cid:88) i=1 log log exp(Simvid ii )/τ ) j=1 exp(Simvid exp(Simvid ii )/τ ) j=1 exp(Simvid (cid:80)B (cid:80)B ji )/τ ) ij )/τ ) (1) (2) (3) , , LA(Simvid) = 1 [Lt2v(Simvid) + Lv2t(Simvid)], (4) where is the batch size, τ is the temperature parameter. 3.2. DiscoVLA As shown in Figure 2, we propose DiscoVLA to address the challenges of vision, language, and alignment when transferring from image-level to video-level VL matching. First, we introduce Image-Video Features Fusion (IVFusion) in both vision and text encoders to enhance feature extraction. Next, we introduce Pseudo Image-Level Alignment (PImgAlign) for fine-grained image-level alignment, consisting of Pseudo Image Language Generation and ImageLevel Alignment. Finally, Image-to-Video Alignment Distillation (AlignDistill) is proposed to leverage image-level alignment knowledge to enhance video-level alignment. Importantly, PImgAlign is only employed during training. For fair comparison with previous methods and due to its generation inefficiency, PImgAlign is omitted at inference. The final similarity used for retrieval is based on video-level similarity in Eq. (1). We focus on parameter-efficient transfer learning for video-text retrieval, where the large backbone parameters remain fixed, and only small number of additional parameters are trained. The trainable parameters are embedded in the parameter-efficient attention of each encoder layer, as illustrated in Figure 3. 3.3. Image-Video Features Fusion Image-level and video-level variations exist in both vision and language modalities. Our proposed IVFusion fuses these image-level and video-level features in both the vision and language encoders. Below, we illustrate IVFusions application within the vision encoder. Figure 3. Illustration of the encoder layers for vision and text encoders. (a) Image-Level Attn operates on each of the images or image captions individually. (b) Video-Level Attn concatenates tokens across all sampled images or image captions. (c) IVFusion Attn employs lightweight adapter to integrate the efficiency of Image-Level Attn with the effectiveness of Video-Level Attn. Here, we illustrate the application of IVFusion within the vision encoder. The text encoder adopts the same approach for processing pseudo image captions. Image-Level Attn. To achieve parameter-efficient transfer learning, we apply LoRA in the attention module of each encoder layer. As illustrated in Figure 3a, (N + 1) tokens are processed via Image-level Attn, where denotes the number of sampled images, and + 1 represents the combination of the global CLS token and patch tokens. Image-Level Attn processes each of the images individually, using + 1 tokens per image. However, Image-Level Attn does not encode temporal information. Video-Level Attn. straightforward solution is VideoLevel Attn, which concatenates tokens across all sampled images, enabling the attention module to learn spatiotemporal information. As shown in Figure 3b, (N + 1) tokens are simultaneously processed during the attention module. Due to the quadratic complexity of the attention mechanism, Image-Level Attn has complexity of O(F (N )2), while Video-Level Attn has higher complexity of O((F )2). IVFusion Attn. To combine Image-Level Attns efficiency with Video-Level Attns effectiveness, we introduce IVFusion Attn in Figure 3c. We extend Image-Level Attn by incorporating branch that uses all CLS tokens to extract spatio-temporal information, reducing complexity to O(F (N )2 + (F )2N ): [c(1) , ˆEi] = ATTN(QKV = [c(0) 1:F ], KV = [c(0) Wdown)Wup, 1:F ] = ATTN(Q = [c(0) [c(2) + σ(c(1) ˆci = c(2) , E(0) ]), 1:F , E(0) 1:F ]), (5) (6) (7) where ci R1D is CLS token of the ith image and c1:F RF are CLS tokens of all sampled images. Similarly, Ei RN are patch tokens of the ith image and E1:F RF are patch tokens of all samIn Eq. (7), we employ trainable adapter pled images. to merge image-level features c(1) and video-level features c(2) , where Wdown RDr and Wup RrD represent the upand down-projection layers, and σ denotes the nonlinear GELU activation [19]. Image-level features preserve pretrained spatial details, while video-level features capture temporal information. This fusion mechanism strengthens feature extraction. Our parameter-efficient vision encoder comprises both image-level and IVFusion layers. The shallow image-level layers employ Image-Level Attn to capture spatial information, while the top Hv IVFusion layers leverage IVFusion Attn to focus on spatio-temporal information. Consequently, our vision encoder outputs enhanced image features 1 (cid:111) . (cid:110) vimg 3.4. Pseudo Image-Level Alignment Unlike previous work focused on video-level alignment, we introduce PImgAlign to learn fine-grained image-level alignment via the decomposition of video content at the image level. To accomplish this, we first generate pseudo image captions in VTR datasets. In contrast to the straightforward sampling of images from video, sampling image captions is impractical, as video captions generally lack the specific details needed for individual images. As solution, we utilize the multimodal large language model (MLLM) LLaVA-NeXT [35] to generate pseudo image captions. For each sampled image from video, we prompt the model using the relevant video caption for guidance: video-level similarity: The provided image is frame sampled from the video, which describes {video caption}. Based on the videos content, provide caption for the provided image. Unlike EA-VTR [38], which uses an image captioner, our method employs strong MLLM. By incorporating video caption guidance, it generates higher-quality image captions that align with the overall video content. Similarly to visual feature extraction, we adopt our proposed IVFusion to extract pseudo image caption features 1 (cid:111) . (cid:110) timg Given image features and pseudo image caption features from paired video and video caption, we introduce finegrained image-level similarity. Since image captions are typically more concise than the images they describe, single caption may correspond to multiple images, as shown in Figure 4d. Therefore, we calculate the maximum similarity for each image across all image captions and vice versa. Specifically, we computed similarity matrix between image and image caption features, selecting the maximum value from each row and column to obtain the final image-level similarity: Simimg ijt2v = Simimg ijv2t = Simimg ij = 1 2 1 1 (cid:88) n=1 (cid:88) m=1 max 1mF timg in vimg jm, max 1nF timg in vimg jm, [Simimg ijt2v + Simimg ijv2t], (8) (9) (10) ij where Simimg denotes the image-level similarity between the ith video caption and the jth video. timg in denotes the nth pseudo image caption features of the ith video caption and vimg jm denotes the mth image features of the jth video. PImgAlign achieves video-level alignment from the image level, effectively emphasizing spatial details. However, in the inference stage, the lack of ground truth for video captions prevents pseudo image captions generation. Additionally, MLLM introduces computational overhead. For fair comparisons with other methods, we exclude PImgAlign during inference. 3.5. Image-to-Video Alignment Distillation Pretrained CLIP demonstrates strong capability for image-level alignment. In contrast, video-text retrieval relies on comprehensive spatio-temporal alignment. Therefore, we propose AlignDistill to distill image-level alignment knowledge into video-level alignment, thus mitigating alignment discrepancies. Then, we optimize the KullbackLeibler divergence [30] between image-level similarity and t2v = [simg img t2v = [svid vid 1 2 LKL = i1 , , simg iNv i1 , , svid iNv (KL(S img t2v vid ], img ], vid v2t = [simg , , simg Nti], 1i 1i , , svid v2t = [svid Nti], t2v ) + KL(S img v2t vid v2t)). (13) (11) (12) ij is Simvid ij in Eq. (10) and svid ij is Simimg t2v and vid where simg in ij Eq. (1). img t2v denote the probability distributions for the text-to-video task, while img v2t and vid v2t denote those for the video-to-text task. During the inference phase, We employ enhanced video-level similarity for retrieval tasks. Finally, the overall training objective, which includes alignment loss (Eq. (4)) and distillation loss (Eq. (13)), is formulated as follows: = LA(Simvid) + αLA(Simimg) + βLKL, (14) where α and β are used to balance the loss. LA(Simimg) is the image-level alignment loss (see Eq. (4) and (10)). 4. Experiments 4.1. Experimental Settings Following common practice, we perform evaluations on four widely used benchmarks: MSRVTT [53], LSMDC [43], ActivityNet [29], and DiDeMo [1]. We evaluate the performance using common retrieval metrics such as Recall at (R@K and = 1, 5, 10), the sum of these recalls (R@sum), and Mean Rank (MnR). In all experiments, the LoRA dimension and the adapter dimension are set to 8. In Eq. 14, α and β are set to 0.3 and 1.0, respectively. For the number of IVFusion layers, we set HV = 4 for the vision encoder and HL = 2 for the text encoder. Please see Appendix for further details. 4.2. Comparisons with State-of-the-Art Methods We compare our proposed DiscoVLA with state-of-theart methods on popular benchmarks such as MSRVTT, LSMDC, ActivityNet, and DiDeMo. Full fine-tuning refers to CLIP4Clip-meanP [37], which averages image features along the temporal dimension. All parameter-efficient methods follow this temporal average pooling strategy to obtain the final video features. Although Prompt [27], Adapter [20], and LoRA [21] are widely adopted for parameter-efficient image and NLP tasks, they lack effective temporal modeling for video-level understanding. In contrast, methods like RAP [5], VoP [22], DGL [58], and MV-Adapter [25] focus on temporal modeling in the vision modality. As shown in Table 1, on MSRVTT, our DiscoVLA with CLIP (ViT-B/32) as the backbone achieves 47.0% R@1 in Method # Params (M) Text-to-Video Video-to-Text R@1 R@5 R@10 R@sum MnR R@1 R@5 R@10 R@sum MnR 43.1 40.4 41.9 43.7 44.8 43.5 44.7 44.6 46.1 47.0 43.1 42.2 43.6 43.0 44.0 43.6 42.1 44.5 45.6 47.7 80.8 77.3 78.7 80.4 81.5 79.3 79.2 80.3 80.7 82.8 70.4 66.3 69.9 68.9 71.4 69.3 70.5 69.9 71.8 73. 16.2 16.7 14.9 16.0 14.4 14.8 16.2 16.3 14.8 14.1 123.54 0.08 0.26 0.49 1.06 0.4 14.10 0.83 0.50 0.56 Full fine-tuning Prompt [27] Adapter [20] LoRA [21] RAP [5] VoPF+P [22] VoP F+C [22] DGL [58] TempMe [44] DiscoVLA CLIP (ViT-B/32) 194.3 184.0 190.2 193.0 197.7 192.1 194.4 194.8 198.6 202.8 CLIP (ViT-B/16) 200.1 202.4 201.3 202.3 200.7 206.7 209.9 Table 1. Comparisons with state-of-the-art methods on MSRVTT. # Params denotes the number of trainable parameters. denotes higher values represent better performance and denotes lower values represent better performance. The best value for each metric is highlighted in bold. R@sum is defined as the total of R@1, R@5, and R@10. The gray row represents the fully fine-tuned CLIP4Clip-meanP [37]. MV-Adapter [25] RAP [5] VoPF+P [22] VoPF+C [22] DGL [58] TempMe [44] DiscoVLA 194.8 191.1 193.6 195.4 198.3 196.0 192.7 195.8 199.2 204. 12.4 12.4 11.5 12.0 10.1 11.0 13.4 11.5 10.2 10.0 70.5 69.7 69.9 70.2 71.9 71.2 70.0 70.7 72.4 73.6 81.2 79.2 80.1 82.2 82.4 81.2 80.6 80.6 81.2 83.6 3.6 1.06 0.4 14.10 0.83 0.50 0.56 203.4 206.5 - - 202.6 208.3 209.9 - 12.1 12.9 12.0 13.4 11.9 12. 74.0 76.4 - - 74.0 75.3 76.0 83.8 84.8 - - 82.9 85.4 84.7 72.0 73.9 72.4 72.4 71.8 74.4 75.6 45.6 45.3 - - 45.7 47.6 49.2 46.0 46.5 47.1 47.7 48.3 49.0 50.5 - 9.1 - - 10.9 9.0 8. 82.1 82.0 81.8 82.2 80.6 83.3 83.8 Method Full fine-tuning VoPF+P [22] VoPF+C [22] DGL [58] DiscoVLA Method Full fine-tuning RAP [5] VoPF+P [22] VoPF+C [22] DGL [58] DiscoVLA # Params (M) 123.54 0.4 14.10 0.83 0. # Params (M) 123.54 1.06 0.4 14.10 0.83 0.56 Text-to-Video Video-to-Text R@1 R@5 R@10 R@sum MnR R@1 R@5 R@10 R@sum MnR 56.7 20.7 50.8 20.7 51.1 21.1 - 21.4 46.6 23.6 106.8 111.1 111.6 109.2 117.9 Table 2. Comparisons with state-of-the-art methods on LSMDC using CLIP (ViT-B/32). 107.5 112.8 113.3 - 115. 47.5 50.7 50.7 - 51.2 47.2 49.7 49.6 48.4 52.3 39.4 40.6 40.3 - 41.9 38.9 40.7 40.9 39.4 42.0 65.3 59.1 60.1 64.3 52.0 20.6 21.5 22.3 - 22. Text-to-Video Video-to-Text R@1 R@5 R@10 R@sum MnR R@1 R@5 R@10 R@sum MnR 40.5 40.8 36.1 35.1 38.6 41.2 - 194.0 180.1 176.4 189.4 197.2 Table 3. Comparisons with state-of-the-art methods on ActivityNet using CLIP (ViT-B/32). 202.4 - 181.4 179.3 - 199.3 85.8 - 79.2 77.8 - 84. 74.1 - 65.9 65.9 - 72.8 72.4 71.0 65.5 63.7 69.2 72.4 42.5 - 36.3 35.6 - 41.8 7.4 8.3 10.9 11.4 9.0 7.8 - 82.2 78.5 77.6 81.6 83.6 6.6 - 10.1 10.4 - 6. the text-to-video task (t2v) and 47.7% R@1 in the video-totext task (v2t), significantly outperforming previous methods. Furthermore, when using CLIP (ViT-B/16), DiscoVLA achieves improvements of 1.5% in R@1, reaching final score of 50.5% R@1. The comparison results on LSMDC, ActivityNet, DiDeMo are shown in Table 2-4. Our method outperforms all existing approaches across these datasets. Existing approaches overlook the critical image-to-video language and alignment discrepancies. In contrast, our proposed DiscoVLA addresses these discrepancies across all three areas: vision, language, and alignment. 4.3. Ablation Study Ablation study on individual components. We evaluate the proposed components in Table 5. LoRA applies Image-Level Attn (see Figure 3a) in both the vision and text encoders. In B1, IVFusion significantly enhances performance by 8.7% R@sum in t2v and 6.7% R@sum in v2t, effectively integrating image-level and video-level features. In B2 and B3, we find that using PImgAlign or AlignDistill independently yields limited improvements. This is largely due to the notable gap between image-level and video-level (see Figure 1). Image-level PImgAlign struggles to impact video-level tasks, while AlignDistll lacks sufficient imagespecific learning. When combined, however, they complement each other. PImgAlign captures fine-grained imagelevel alignments, and AlignDistill effectively transfers this knowledge to the video-level alignment. This improves 1.1% R@sum in t2v and 2.8% R@sum in v2t over B1. Finally, our full DiscoVLA improves 9.8% R@sum in t2v and 9.5% R@sum in v2t over LoRA. Method Full fine-tuning RAP [5] VoPF+P [22] VoPF+C [22] DiscoVLA # Params (M) 123.54 1.06 0.4 14.10 0.56 Text-to-Video Video-to-Text R@1 R@5 R@10 R@sum MnR R@1 R@5 R@10 R@sum MnR 11.6 43.4 - 42.6 9.9 45.3 9.5 46.4 9.3 48.4 194.2 192.6 198.0 199.8 205.6 Table 4. Comparisons with state-of-the-art methods on DiDeMo using CLIP (ViT-B/32). 193.3 - 197.0 198.0 205. 17.5 18.0 13.8 13.6 14.0 70.2 70.4 72.3 71.9 74.5 70.6 - 71.2 71.8 74.4 80.6 79.6 80.4 81.5 82.7 80.2 - 81.1 81.8 83.8 42.5 - 44.7 44.4 47. Methods Pretraind CLIP LoRA B1 B2 B3 DiscoVLA IVFusion (cid:33) (cid:33) (cid:33) (cid:33) Components PImgAlign AlignDistill (cid:33) (cid:33) (cid:33) (cid:33) # Params (M) 0 0.49 0.54 0.56 0.54 0.56 Text-to-Video Video-to-Text R@1 R@5 R@10 R@sum R@1 R@5 R@10 R@sum 30.8 43.7 46.5 47.0 46.6 47.0 138.7 195.4 202.1 201.0 200.0 204.9 147.9 193.0 201.7 201.4 199.2 202. 63.3 80.4 82.0 81.3 81.0 82.8 50.1 70.2 72.6 72.9 71.9 73.6 62.0 82.2 82.9 82.4 82.2 83.6 26.6 43.0 46.6 45.7 45.9 47.7 53.8 68.9 73.2 73.1 71.6 73.0 Table 5. Ablation study on the contribution of each proposed component on MSRVTT using CLIP (ViT-B/32). Pretrained CLIP denotes the zero-shot performance of CLIP without any additional training. Our proposed methods are built upon LoRA. B1, B2, and B3 denote various combinations of our proposed components. Parameter-Efficient Attention Image-Level Attn Video-Level Attn IVFusion Attn w/o Adapter IVFusion Attn Text-to-Video R@1 R@5 R@10 R@sum 43.7 45.7 46.0 46.5 193.0 197.4 197.4 201.7 80.4 80.8 80.4 82.0 68.9 70.9 71.0 73.2 Table 6. Ablation study on IVFusion on MSRVTT using CLIP (ViT-B/32). We compare several implementations against our proposed IVFusion for Parameter-Efficient Attention (see Figure 3). Image-Level Attn represents the LoRA baseline. Similarity in PImgAlign None Video-Level Paired Image-Level Fine-grained Image-Level Text-to-Video R@1 R@5 R@10 R@sum 46.6 46.3 46.1 47.0 199.2 199.8 201.7 202.8 81.0 81.2 82.4 82.8 71.6 72.3 73.2 73.0 Table 7. Ablation study on similarity in PImgAlign on MSRVTT using CLIP (ViT-B/32). None represents DiscoVLA without image-level alignment optimization, shown as B3 in Table 5. Paired Image-Level is based on matched image-caption pairs. Fine-grained Image-Level represents the full DiscoVLA. Analysis of trainable parameters. The trainable parameters of our DiscoVLA consist of the LoRA and Adapter parts. As shown in Table 5, LoRA in both the vision and text encoders contain 0.49M parameters. In B1, an additional 0.05M parameters are introduced by the Adapter in the HV = 4 IVFusion layers of the vision encoder. In B2, an additional 0.02M parameters are introduced by the Adapter in the HL = 2 IVFusion layers of the text encoder, which are utilized for pseudo-image caption features in PImgAlign. Overall, the total number of trainable parameters of DiscoVLA is 0.56M. Ablation study on IVFusion. In Table 6, we compare different strategies for parameter-efficient attention in Figure 3. Video-Level Attn concatenates all tokens across sampled images to capture spatio-temporal information, Image Caption Generation DiscoVLA W/O VidCap R@1 47.0 46.1 (-0.9) Text-to-Video R@10 82.8 82.1 (-0.7) R@5 73.0 73.0 R@sum 202.8 201.2 (-1.6) Table 8. Ablation study on image caption generation on MSRVTT using CLIP (ViT-B/32). W/O VidCap denotes generating pseudo image captions without video caption guidance. achieving notable 4.4% R@sum improvement. To reduce the computational complexity, IVFusion applies spatiotemporal attention only to global CLS tokens and merges image-level features with video-level features. Without an adapter, averaging imageand video-level features results in modest performance increase. In contrast, with learnable adapter, IVFusion merges the features more effectively, achieving 201.7% R@sum. Ablation study on similarity in PImgAlign. Table 7 demonstrates the effectiveness of our proposed fine-grained image-level similarity in PImgAlign. The similarity is computed between sampled images and pseudo image captions from paired video and video caption. The Video-Level baseline averages the image features and pseudo-image caption features along the temporal dimension to compute the video-level similarity, replacing fine-grained image-level similarity. However, this modification leads to notable decrease in performance, emphasizing the importance of image-level similarity in PImgAlign. In contrast, we introduce the Paired Image-Level baseline, which computes similarity based on matched image-caption pairs. Compared to Video-Level, Paired Image-Level yields 1.9% gain in R@sum. Moreover, we observe in Figure 4d that single image caption may describe multiple images. Consequently, our fine-grained image-level similarity, as defined in Eq. 10, selects the maximum similarity from all available pairs, further enhancing video-level alignment. Figure 4. Visualization of discrepancies between video-level and image-level data. Each example consists of paired video and video caption, with four sampled images from the video. Video captions are highlighted with orange solid lines, and pseudo image captions generated by LLaVA-NeXT [35] are indicated with green dashed lines. While video captions convey the general context of the video, image captions focus on the detailed context of each individual frame. as post-processing methods are provided in the appendix. 4.4. Qualitative Analysis To compare image-level and video-level data, we conduct visualization experiment in Figure 4. In Figure 4a and 4b, image-level data alone fails to capture the complete content of the video. Figure 4c and 4d demonstrate strong correlation between the video-level and image-level content, but differences across images lead to varying image captions. Visually, video is composed of sequence of images, incorporating temporal information. Textually, image captions are specific to individual images, while video captions describe the overall video content. These discrepancies hinder the effective transfer of image-level CLIP. Our approach addresses this issue by utilizing image-level alignment to enhance video-level alignment. 5. Conclusion In this paper, we identify three major discrepancies in the parameter-efficient transfer of image-level CLIP to videotext retrieval: vision, language, and alignment. To address these challenges, we propose DiscoVLA. Specifically, IVFusion fuses image-level and video-level features to tackle the vision and language discrepancies. For alignment discrepancies, we propose PImgAlign and AlignDistill, which learn and distill image-level alignment to enhance videolevel alignment. Our DiscoVLA significantly outperforms existing methods, achieving state-of-the-art performance. Figure 5. Comparison of image caption generation with (W/) and without (W/O) video caption guidance (VidCap) in PImgAlign. Ablation study on image caption generation in PImgAlign. Figure 5 and Table 8 show qualitative and quantitative evaluations of video caption guidance. In Figure 5, video captions serve as the global context that enriches the understanding of individual frames (marked in red). In Table 8, without this guidance, image captions focus only on salient regions while missing globally relevant details, leading to performance drops. In PImgAlign, we employ MLLM to process both frame images and video descriptions, generating image captions that align with the overall video content. These high-quality captions further enhance DiscoVLAs ability to learn robust image-level alignment. More Ablation Analysis. In Appendix B, we conduct additional ablation studies. Specifically, we evaluate the effects of hyperparameters α, β, HV and HV . For fairness, all results in the main paper are reported without post-processing. Complementary evaluations with Q-Norm [4] and DSL [8]"
        },
        {
            "title": "Acknowledgment",
            "content": "This work was supported by Beijing Natural Science Foundation (No. L223023), National Natural Science Foundation of China (Nos. 62441235, 62021002, 62441614), the Key & Program of Xinjiang, China (2022B01006), Zhejiang Provincial Natural Science Foundation of China under Grant (No. LDT23F01013F01), CCF-DiDi GAIA Collaborative Research Funds, China Postdoctoral Science Foundation (2024M750565), Guangdong & Program (2024B0101040008), and the Key Realm Research and Development Program of Guangzhou (No. 2024B01W0007)."
        },
        {
            "title": "References",
            "content": "[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the International Conference on Computer Vision, 2017. 5, 12 [2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the International Conference on Computer Vision, 2015. 2 [3] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. Frozen in time: joint video and image encoder for end-to-end retrieval. In Proceedings of the International Conference on Computer Vision, 2021. 12 [4] Simion-Vlad Bogolin, Ioana Croitoru, Hailin Jin, Yang Liu, and Samuel Albanie. Cross modal retrieval with querybank In Proceedings of the IEEE Conference on normalisation. Computer Vision and Pattern Recognition, 2022. 8, 13 [5] Meng Cao, Haoran Tang, Jinfa Huang, Peng Jin, Can Zhang, Ruyang Liu, Long Chen, Xiaodan Liang, Li Yuan, and Ge Li. RAP: Efficient text-video retrieval with sparse-andIn Association for Computational Lincorrelated adapter. guistics, 2024. 2, 3, 5, 6, 7, 12 [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 2022. 2 [7] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 2023. [8] Xing Cheng, Hezheng Lin, Xiangyu Wu, Fan Yang, and Dong Shen. Improving video-text retrieval by multi-stream arXiv preprint corpus alignment and dual softmax loss. arXiv:2109.04290, 2021. 8, 13 [9] Chaorui Deng, Qi Chen, Pengda Qin, Da Chen, and Qi Wu. Prompt switch: Efficient clip adaptation for text-video retrieval. In Proceedings of the International Conference on Computer Vision, 2023. 1, 2 [10] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. In Proceedings of the International Conference on Computer Vision, 2019. 2 [11] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. 2 [12] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Improving clip training with language Yonglong Tian. rewrites. Advances in Neural Information Processing Systems, 2024. 2 [13] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia In Schmid. Multi-modal transformer for video retrieval. Proceedings of the European Conference on Computer Vision, 2020. 12 [14] Satya Krishna Gorti, Noel Vouitsis, Junwei Ma, Keyvan Golestan, Maksims Volkovs, Animesh Garg, and Guangwei Yu. X-pool: Cross-modal language-video attention for textvideo retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 1, 2 [15] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, large miniYangqing Jia, and Kaiming He. Accurate, arXiv preprint batch sgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017. [16] Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and other neural network architectures. Neural Networks, 2005. 2 [17] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor BergKirkpatrick, and Graham Neubig. Towards unified view In Proceedings of of parameter-efficient transfer learning. the International Conference on Learning Representations, 2021. 2 [18] Tao He, Leqi Shen, Yuchen Guo, Guiguang Ding, and Zhenhua Guo. Secret: Self-consistent pseudo label refinement for unsupervised domain adaptive person re-identification. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022. 2 [19] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. 4 [20] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer In Proceedings of the International Conlearning for nlp. ference on Machine Learning, 2019. 2, 5, 6 [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 2, 5, 6 [22] Siteng Huang, Biao Gong, Yulin Pan, Jianwen Jiang, Yiliang Lv, Yuyuan Li, and Donglin Wang. Vop: Text-video coIn Prooperative prompt tuning for cross-modal retrieval. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 2, 3, 5, 6, 7, [23] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In Proceedings of the European Conference on Computer Vision, 2022. 2 [24] Peng Jin, Jinfa Huang, Pengfei Xiong, Shangxuan Tian, Chang Liu, Xiangyang Ji, Li Yuan, and Jie Chen. Videotext as game players: Hierarchical banzhaf interaction for cross-modal representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 2 [25] Xiaojie Jin, Bowen Zhang, Weibo Gong, Kai Xu, Xueqing Deng, Peng Wang, Zhao Zhang, Xiaohui Shen, and Jiashi Feng. Mv-adapter: Multimodal video transfer learning for video text retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 2, 3, 5, 6, 12 [26] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignIn Proceedings ments for generating image descriptions. of the IEEE Conference on Computer Vision and Pattern Recognition, 2015. 2 [27] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Maple: Multi-modal prompt learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 2, 5, [28] Muhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without forgetting. In Proceedings of the International Conference on Computer Vision, 2023. 2 [29] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the International Conference on Computer Vision, 2017. 5, 12 [30] Solomon Kullback. Information theory and statistics. Courier Corporation, 1997. 5 [31] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for In Provideo-and-language learning via sparse sampling. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021. [32] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 2021. 2 [33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In Proceedings of the International Conference on Machine Learning, 2022. [34] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models. In Proceedings of the International Conference on Computer Vision, 2023. 2 [35] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, 2024. 4, 8 [36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In Proceedings of the International Conference on Learning Representations, 2016. 12 [37] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 2022. 1, 2, 5, [38] Zongyang Ma, Ziqi Zhang, Yuxin Chen, Zhongang Qi, Chunfeng Yuan, Bing Li, Yingmin Luo, Xu Li, Xiaojuan Qi, Ying Shan, et al. Ea-vtr: Event-aware video-text retrieval. In Proceedings of the European Conference on Computer Vision, 2024. 5 [39] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, and Josef Sivic. Ivan Laptev, Howto100m: Learning text-video embedding by watching hundred million narrated video clips. In Proceedings of the International Conference on Computer Vision, 2019. 12 [40] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3 [41] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019. 2 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn Proceedings of the International Conference on vision. Machine Learning, 2021. 1, 2 [43] Anna Rohrbach, Marcus Rohrbach, and Bernt Schiele. The long-short story of movie description. In Pattern Recognition, 2015. 5, 12 [44] Leqi Shen, Tianxiang Hao, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, and Guiguang Ding. Tempme: Video temporal token merging for efficient text-video retrieval. arXiv preprint arXiv:2409.01156, 2024. [45] Leqi Shen, Tao He, Sicheng Zhao, Zhelun Shen, Yuchen Guo, Tianshi Xu, and Guiguang Ding. X-reid: Cross-instance transformer identity-level person refor identification. In IEEE International Conference on Multimedia and Expo, 2024. 2 [46] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf. Zerocap: Zero-shot image-to-text generation for visualsemantic arithmetic. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2022. 2 [47] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et al. Yolov10: Real-time end-to-end object detection. Advances in Neural Information Processing Systems, 2024. 2 [48] Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and Zhiqiang Tao. Text is mass: Modeling as stochastic embedding for text-video retrieval. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024. 1, 2 [49] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xiansheng Hua. Disentangled representation learning for textvideo retrieval. arXiv preprint arXiv:2203.07111, 2022. 1, 2 [50] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. 2 [51] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2023. 1, 2 [52] Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying In Proceedings of the International Conference clip data. on Learning Representations, 2024. [53] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: large video description dataset for bridging video and language. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016. 5, 12 [54] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the International Conference on Machine Learning, 2015. 2 [55] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language alignment. In Proceedings of the International Conference on Learning Representations, 2022. 2 [56] Fan Yang, Sicheng Zhao, Yanhao Zhang, Haoxiang Chen, Hui Chen, Wenbo Tang, Haonan Lu, Pengfei Xu, Zhenyu Yang, Jungong Han, et al. Llmi3d: Empowering llm with arXiv preprint 3d perception from single 2d image. arXiv:2408.07422, 2024. 2 [57] Fan Yang, Ru Zhen, Jianing Wang, Yanhao Zhang, Haoxiang Chen, Haonan Lu, Sicheng Zhao, and Guiguang Ding. Heie: Mllm-based hierarchical explainable aigc image implausibility evaluator. arXiv preprint arXiv:2411.17261, 2024. 2 [58] Xiangpeng Yang, Linchao Zhu, Xiaohan Wang, and Yi Yang. Dgl: Dynamic global-local prompt tuning for text-video retrieval. Proceedings of the AAAI Conference on Artificial Intelligence, 2024. 1, 2, 3, 5, 6, 12 [59] Bowen Zhang, Hexiang Hu, and Fei Sha. Cross-modal and hierarchical modeling of video and text. In Proceedings of the European Conference on Computer Vision, 2018. 12 [60] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv preprint arXiv:2206.04673, 2022. 2 [61] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language modIn Proceedings of the IEEE Conference on Computer els. Vision and Pattern Recognition, 2022. 2 [62] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision, 2022. 2 DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Additional Experimental Settings Datasets. Following common practice, we perform evaluations on four widely used benchmarks for video-text (1) MSRVTT [53] includes 10,000 YouTube retrieval: videos, each with 20 text descriptions. Following previous methods [13, 39], we train on the train+val set with 9,000 video-text pairs and evaluate on the 1K-A test set with 1,000 video-text pairs. (2) LSMDC [43] consists of 118,081 movie clips, each paired with single description. We use 101,079 for training, 7,408 for validation, and 1,000 for testing, reporting results based on the test set. (3) ActivityNet [29] consists of 20,000 YouTube videos. Our evaluation utilizes the val1 split, which includes 10,009 videos for training and 4,917 for testing. Following previous methods [13, 59], we concatenate all sentence descriptions of video into single paragraph. (4) DiDeMo [1] comprises 10,000 videos with total of 40,000 text descriptions. The training set contains 8,395 videos, while the test set contains 1,004 videos. Following previous methods [3, 31], we combine all descriptions of video into single query. Evaluation Metrics. We evaluate the performance using common retrieval metrics such as Recall at (R@K and = 1, 5, 10), the sum of these recalls (R@sum), and Mean Rank (MnR). R@K measures the proportion of relevant items retrieved in the top results for given query. MnR calculates the mean rank of correct items. Note that for R@K, higher score means better performance. Conversely, for MnR, lower score indicates better results. Implementation Details. Following previous parameterefficient research [5, 22, 25, 58], we utilize the pre-trained CLIP model as our backbone. we implement the AdamW optimizer [36] with batch size of 128. For all datasets, the initial learning rate is set to 6e-4, employing cosine learning rate schedule [15] over 5 epochs. For MSRVTT and LSMDC, the max frame and caption length are set to 12 and 32. For ActivityNet and DiDeMo, the max frame and caption length are set to 32 and 64. In all experiments, the LoRA dimension and the adapter dimension are set to 8. In Eq. 14, α and β are set to 0.3 and 1.0, respectively. For the number of IVFusion layers, we set HV = 4 for the vision encoder and HL = 2 for the text encoder. B. Additional Experimental Results Ablation study on α and β in Eq. (14). Figures 6 and 7 present ablation studies on hyperparameters α and β, respectively. These parameters control the trade-off among Figure 6. Ablation study on α in Eq. (14) for text-to-video results on MSRVTT using CLIP (ViT-B/32). α represents the weight of the image-level alignment loss LA(Simimg). All other hyperparameters are kept constant. Figure 7. Ablation study on β in Eq. (14) for text-to-video results on MSRVTT using CLIP (ViT-B/32). β represents the weight of the distillation loss LKL. All other hyperparameters are kept constant. the video-level alignment loss LA(Simvid), the image-level alignment loss LA(Simimg), and the distillation loss LKL. The parameter α represents the weight of LA(Simimg). Increasing α improves both R@1 and R@sum significantly. Our DiscoVLA achieves optimal performance at α = 0.3, beyond which it demonstrates parameter insensitivity. The parameter β represents the weight of LKL. While LKL does not affect R@1, setting β = 1.0 yields marked improvement in R@sum. Consequently, we set α = 0.3 and β = 1.0 in our final implementation. Ablation study on the number of IVFusion layers HV and HL. The number of IVFusion layers in the vision and text encoders is denoted by HV and HL, respectively. IVFusion is applied to the upper layers of CLIP, as these layers extract high-level semantic information essential for cross-frame learning. This learning process requires an adsistent performance gains across both datasets, improving performance by 11.1% R@sum on MSRVTT and 19.6% on ActivityNet. Figure 8. Ablation study on HV for text-to-video results on MSRVTT using CLIP (ViT-B/32). HV represents the number of IVFusion layers in the vision encoder. All other hyperparameters are kept constant. Figure 9. Ablation study on HL for text-to-video results on MSRVTT using CLIP (ViT-B/32). HL represents the number of IVFusion layers in the text encoder. All other hyperparameters are kept constant. Post-processing MSRVTT ActivityNet DiscoVLA +QB-Norm [4] +DSL [8] DiscoVLA +QB-Norm [4] +DSL [8] Text-to-Video R@1 R@5 R@10 R@sum 47.0 47.5 51.3 41.2 45.1 49.9 202.8 204.0 213.9 197.2 205.2 216.8 73.0 73.6 77.1 72.4 74.9 78.8 82.8 82.9 85.5 83.6 85.2 88. Table 9. Effect of post-processing on MSRVTT and ActivityNet using CLIP (ViT-B/32). equate number of IVFusion layers. From Figures 8 and 9, we observe that the best performance is achieved when HV = 4 and HL = 2. This difference in optimal values may be attributed to the inherent distinctions between the vision and text modalities. This configuration HV = 4 and HL = 2 demonstrate superior and robust performance across all benchmarks (see Tables 1-4). Effect of post-processing. As shown in Table 9, we evaluate post-processing methods Q-Norm [4] and DSL [8] on top of our proposed DiscoVLA. While Q-Norm leads to notable 8.0% R@sum increase on ActivityNet, it has minimal effect on MSTVTT. In contrast, DSL provides con-"
        }
    ],
    "affiliations": [
        "BNRist",
        "Department of Automation, Tsinghua University",
        "GRG Banking Equipment Co., Ltd.",
        "Hangzhou Zhuoxi Institute of Brain and Intelligence",
        "JD.com",
        "School of Software",
        "South China University of Technology"
    ]
}