{
    "paper_title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks",
    "authors": [
        "Krithik Vishwanath",
        "Mrigayu Ghosh",
        "Anton Alyakin",
        "Daniel Alexander Alber",
        "Yindalon Aphinyanaphongs",
        "Eric Karl Oermann"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows."
        },
        {
            "title": "Start",
            "content": "1 Krithik Vishwanath1-3, Mrigayu Ghosh4,5, Anton Alyakin, M.S.E.1,6,7 , Daniel Alexander Alber, B.S.1 , Yindalon Aphinyanaphongs, M.D., Ph.D.7-9, Eric Karl Oermann, M.D.1,7,10,11 1Department of Neurological Surgery, NYU Langone Health, New York, New York, USA 2Department of Aerospace Engineering & Engineering Mechanics, The University of Texas at Austin, Austin, Texas, USA 3Department of Mathematics, The University of Texas at Austin, Austin, Texas, USA 4Department of Biomedical Engineering, The University of Texas at Austin, Austin, Texas, USA 5Department of Molecular Biosciences, The University of Texas at Austin, Austin, Texas, USA 6Department of Neurosurgery, Washington University School of Medicine in St. Louis, St. Louis, Missouri, USA 7Global AI Frontier Lab, New York University, New York, New York, USA 8Department of Population Health, NYU Langone Health, New York, New York, USA 9Department of Medicine, NYU Langone Health, New York, New York, USA 10Department of Radiology, NYU Langone Health, New York, New York, USA 11Center for Data Science, New York University, New York, New York, USA Correspondence: Krithik Vishwanath Department of Neurosurgery, NYU Langone Medical Center, New York University, 550 First Ave, MS 3 205, New York, NY10016, USA. Email: krithik.vish@utexas.edu Eric K. Oermann, MD Department of Neurosurgery, NYU Langone Medical Center, New York University, 550 First Ave, MS 3 205, New York, NY10016, USA. Email: eric.oermann@nyulangone.org 3 Abstract Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows. 4 Introduction Large language models (LLMs) are increasingly used in clinical medicine.13 While major frontier models are well-benchmarked1,2, specialized tools marketed for clinical use remain under-studied. OpenEvidence, claimed to be used by 40% of U.S. physicians3 and to ace the USMLE4, has only been evaluated either internally4 or qualitatively5. Similarly, the newly released UpToDate Expert AI, already used by clinicians at an estimated 70% of major enterprise health systems6, still awaits any sort of independent quantitative evaluation. There is substantial gap between marketing claims and rigorous, peer-reviewed assessment of clinical AI performance. This has created situation in which individual clinicians and health systems may adopt AI systems without adequate evidence regarding their limitations, biases, comparative performance, or failure modes. In settings where AI outputs influences diagnosis or management decisions, this lack of transparency may introduce avoidable clinical risk and challenges evidence-based integration of AI into care. It is commonly hypothesized that specialist tools may be superior to generalist LLMs through domain-specific training79 or retrieval augmented generation (RAG)3,6,7. However, specialist systems vary in their design, scope, and implementation. Frontier generalist models benefit from substantially larger training corpora and more advanced alignment, which may enable them to match or exceed specialist performance depending on the task. Independent evaluations capable of comparing these two categories of models on standardized, clinically relevant benchmarks are critical to clarify whether specialization offers measurable advantage. To address this gap, we conducted the first quantitative assessment of specialized clinical AI tools (OpenEvidence, UpToDate Expert AI) and leading generalist models (GPT-5, Gemini 3 Pro Preview, and Claude Sonnet 4.5) on mini-benchmark comprising knowledge-intensive (MedQA1) and expert-alignment (HealthBench2) tasks. Methods We created medical mini-benchmark by randomly sampling 500 USMLE-style questions from MedQA1 to assess factual knowledge, and 500 prompts from HealthBench2 to evaluate alignment with expert judgement. The resulting 1,000 questions were used to assess GPT-5 (snapshot gpt-5-2025-08-07), Gemini 3 Pro Preview (accessed 11/20/25), and Sonnet 4.5 (snapshot claude-sonnet-4-5-20250929) via their respective APIs, while OpenEvidence (accessed 09/25) and UpToDate Expert AI (accessed 11/25) were manually queried via browser interface. OpenEvidence and UpToDate were grouped as clinical tools, whereas the other three (GPT-5, Gemini 3 Pro, and Sonnet 4.5) were considered to be generalist models. HealthBench responses were graded for the proportion of rubric points achieved across five axes: accuracy, completeness, communication quality, context awareness, and instruction following. HealthBench questions were restricted to prompts requiring single, standalone response (i.e., no multi-turn interactions). Responses were also subset into seven themes: emergency referrals, context seeking, global health, health data tasks, expertise-tailored communication, responding under uncertainty, and response depth. 6 Scoring for HealthBench and MedQA were completed by GPT-4.1 (gpt-4.1-2025-04-14) which demonstrated reliability in grading medical Q&A2. HolmBonferroni-adjusted statistical significance was established using exact McNemars and Wilcoxons tests. For grouped comparisons of clinical tools to generalist models, statistical significance was established using Welchs tests and ≈†id√°k-adjusted two-way ANOVA. Results To evaluate model performance on medical question answering, we first assessed accuracy on MedQA. The resultant MedQA accuracies were: GPT-5, 96.2% (95% CI, 94.1%97.6%); Gemini, 94.6% (92.3%96.3%); Sonnet 4.5, 91.4% (88.6%93.6%); OpenEvidence, 89.6% (86.6%92.0%); and UpToDate, 88.4% (85.3%90.9%); with GPT-5 outperforming all models except Gemini (McNemar P<110-4 vs. OpenEvidence and vs. UpToDate, P=0.0008 vs. Sonnet). Gemini outperformed OpenEvidence (P=0.003), UpToDate (P<110-4), and Sonnet (P=0.026). Other differences were not significant (GPT-5 vs. Gemini, P=0.51; OpenEvidence vs. Sonnet, P=0.52; UpToDate vs. Sonnet, P=0.21; OpenEvidence vs. UpToDate, P=0.52) (Figure 1A). OpenEvidence, UpToDate Expert AI, and Sonnet 4.5 perform worse on Medical Knowledge / Scientific Topic questions (P<0.008; Supplemental Figure 1). 7 Figure 1. Performance of generative artificial intelligence (AI) tools on medical benchmarks. A) MedQA accuracy by model. Statistical significance established via exact McNemars tests. C) HealthBench average scores by model, assessed through physician-consensus rubric. Statistical significance established via pairwise Wilcoxons tests. B, D) Comparison of the performance of Clinical (Expert AI, Open Evidence) tools to Generalist (GPT 5, Gemini 3 Pro, and Sonnet 4.5) models on MedQA and HealthBench, respectively. Statistical significance established via Welchs t-tests. Error bars represent 95% confidence intervals in A-D. * < 0.05, ** < 0.01, *** < 0.001, **** < 0.0001. When comparing clinical tools to generalist models in grouped fashion, generalist models (average accuracy of 94.1%) outperformed clinical tools (average accuracy of 89.0%), although this difference was not significant (P=0.056; Figure 1B). Notably, when questions are classified by their competency category, clinical tools perform worse on Systems-based Practice Questions, including Patient Safety (P<110-4; Supplemental Figure 2). 8 To assess model agreement with expert clinicians, we evaluate performance on HealthBench. HealthBench mean consensus scores were: GPT-5, 97.0% (96.098.0%); Gemini, 90.5% (88.792.3%); Sonnet, 87.7% (85.589.9%); UpToDate, 75.2 (72.3%78.1%); OpenEvidence, 74.3% (71.477.2%). GPT-5 outperformed other models (all Wilcoxon P<110-9), while OpenEvidence consistently underperformed (P<110-10), except when compared to UpToDate (P=0.58) (Figure 1C). Furthermore, the generalist models outperformed the clinical tools by about 1.23-fold (91.7% vs. 74.8%; P=0.023; Figure 1D). In axis-level analysis, GPT-5 achieved the highest or tied-for-highest score across all five axes (Figure 2A). OpenEvidence consistently ranked lowest or tied for lowest with UpToDate, with all GPT-5 vs. OpenEvidence and GPT-5 vs. UpToDate comparisons being significant (McNemar P5.510-4). In three out of five axes, OpenEvidence was inferior to all three generalist models (P5.810-4), and in two out of five axes, UpToDate was inferior to all three generalist models (P2.710-4). These trends correspond with the grouped clinical vs. generalist model analysis, which demonstrated superiority of generalist models in completeness (P<10-4), communication quality (P=0.002), and context awareness (P=0.012) (Figure 2C). In theme-level analysis, GPT-5 ranked first or tied for first in all categories, with perfect scores in four. OpenEvidence and UpToDate scored lowest or tied for lowest in all seven themes (Figure 2B). Again, all GPT-5 vs. OpenEvidence and GPT-5 vs. UpToDate comparisons were significant (Wilcoxon P0.005). Grouped analysis revealed that there were significant differences in the scores of clinical tools and generalist models when responding to questions under the cluster themes of emergency referrals (P=0.0002) and expertise-tailored communication (P=0.0392) (Figure 2D). Figure 2. Performance of generative AI tools on HealthBench by axis and by theme. A) Proportion of prompts with axis met in HealthBench by model. Statistical significance established via pairwise exact McNemars tests. B) Mean score in HealthBench by cluster theme by model. Statistical significance established via pairwise Wilcoxons tests. In A-B, error bars represent 95% confidence intervals, and labeled letters indicate significant differences within each theme: models with the same letter are not significantly different whereas models with different letters are significantly different at ùõº=0.05. C-D) Comparison of the performance of clinical tools to generalist models on HealthBench axes and HealthBench cluster themes, respectively. Statistical significance established via two-way ANOVA and ≈†id√°k-corrected post-hoc analysis. In C-D, error bars represent SEM. * < 0.05, ** < 0.01, *** < 0.001, 10 **** < 0.0001. Discussion We present an independent, quantitative comparison of popular clinical AI tools against frontier generalist LLMs. GPT-5 outperformed all models except for Gemini 3 Pro on the MedQA subset of our mini-benchmark, and outperformed all models on the HealthBench subset. Notably, OpenEvidence fell short of its reported 100% accuracy on USMLE-style questions4. Our results reveal consistent pattern in which clinical AI tools lag behind frontier generalist models on numerous dimensions of clinical importance, such as completeness, communication quality, and context awareness. Without knowing the underlying architecture of these systems, it is hard to know why they may underperform generalist models. However research has shown that RAG, which both OpenEvidence and UpToDate Expert AI heavily rely on, can hurt model performance if the wrong material is retrieved or if it is poorly integrated by the base model.7,10,11 Another possibility is simply the strength of frontier LLMs, which excel at the knowledge retrieval and reasoning tasks that characterize many medical questions. These findings have broader implications for the evolving landscape of clinical AI. As generative models become integrated into routine decision-making, discrepancies between advertised claims and real-world performance introduce avoidable clinical risk. Health systems are increasingly deploying AI-driven assistants in documentation support, guideline lookup, triage, and ambulatory care; settings in which even small reliability deficits can meaningfully affect patient 11 outcomes. Importantly, much of todays AI use is emerging bottom-up, as clinicians and patients experiment with these models on their personal devices well before formal institutional adoption. The consistently superior performance of generalist models suggests that scale, alignment, and cross-domain reasoning may outweigh domain-specific tuning as determinants of medical competency. If this trend continues, the perceived distinction between clinical LLMs and general-purpose LLMs may become increasingly artificial, with implications for procurement, reimbursement, and regulatory oversight. This study has several limitations. We were constrained to modest sample sizes due to the need for manual evaluation of OpenEvidence and UpToDate Expert AI. Moreover, standardized benchmarks, while allowing quantitative comparison, have inherent shortcomings.10 It is possible that recently trained models or tools have been trained on the benchmarks themselves, which were incidentally swept up in their training data. Such concerns underscore the need for open datasets, open weights, and greater transparency surrounding AI systems utilized within clinical medicine. Nonetheless, controlled studies and independent benchmarks of AI systems in medicine are essential to ensure the safe clinical use of these technologies. Perhaps most importantly, however, is the pressing need for actual clinical trials looking at the impact of these technologies with patient centered outcomes as their endpoints. Future work should examine performance across broader datasets, contexts, and safety-critical outcomes and the need for regulatory guidance regarding the evaluation of all LLMs in healthcare. 12 Acknowledgements: We would like to acknowledge Nader Mherabi and Dafna Bar-Sagi, Ph.D., for their continued support of medical AI research at NYU. We thank Michael Constantino, Kevin Yie, and the NYU Langone High-Performance Computing (HPC) Team for supporting computing resources fundamental to our work. Disclosures: EKO has equity in Delvi, MarchAI, and Artisight, income from Merck & Co. and Mirati Therapeutics, employment in Eikon Therapeutics, and consulting for Sofinnova Partners and Google. The other authors have no personal, financial, or institutional interest pertinent to this article. Author Contributions: AA and EKO supervised the study. KV, AA, and EKO conceptualized and established the study design. KV designed and performed the LLM evaluations and scoring. MG performed the statistical analysis. KV wrote the initial draft. KV, MG, and AA developed the figures of the manuscript. KV, MG, AA, DAA, YA, and EKO revised and approved the manuscript. Funding: EKO is supported by the National Cancer Institutes Early Surgeon Scientist Program (3P30CA016087-41S1) and the W.M. Keck Foundation. This work was supported by the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No. RS-2019-II190075 Artificial Intelligence Graduate School Program (KAIST); No. RS-2024-00509279, Global AI Frontier Lab). 13 References: 1. Jin, D. et al. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Appl. Sci. (Basel) 11, 6421 (2021). 2. Arora, R. K. et al. HealthBench: Evaluating large language models towards improved human health. arXiv [cs.CL] (2025) doi:10.48550/ARXIV.2505.08775. 3. OpenEvidence. OpenEvidence, the Fastest-Growing Application for Physicians in History, Announces 210 Million Round at 3.5 Billion Valuation. Cision PR Newswire https://www.prnewswire.com/news-releases/openevidence-the-fastest-growing-application-for-physi cians-in-history-announces-210-million-round-at-3-5-billion-valuation-302505806.html (2025). 4. OpenEvidence. OpenEvidence Creates the First AI in History to Score Perfect 100% on the United States Medical Licensing Examination (USMLE). OpenEvidence https://www.openevidence.com/announcements/openevidence-creates-the-first-ai-in-history-to-score -a-perfect-100percent-on-the-united-states-medical-licensing-examination-usmle. 5. Hurt, R. T. et al. The use of an artificial intelligence platform OpenEvidence to augment clinical decision-making for primary care physicians. J. Prim. Care Community Health 16, 21501319251332215 (2025). 6. HLTH 2025: Wolters Kluwer showcases UpToDate Expert AI and workflow innovations. Wolterskluwer https://www.wolterskluwer.com/en/news/uptodate-expert-ai-workflow-hlth-2025 (2025). 7. Vishwanath, K., Stryker, J., Alyakin, A., Alber, D. A. & Oermann, E. K. MedMobile: mobile-sized language model with clinical capabilities. arXiv [cs.CL] (2025) doi:10.48550/arXiv.2410.09019. 8. Alyakin, A. et al. CNS-Obsidian: Neurosurgical Vision-Language Model Built From Scientific Publications. arXiv [cs.AI] (2025). 9. Jiang, L. Y. et al. Generalist foundation models are not clinical enough for hospital operations. arXiv [cs.CL] (2025) doi:10.48550/arXiv.2511.13703. 10. Vishwanath, K. et al. Medical large language models are easily distracted. arXiv [cs.CL] (2025). 11. Wu, E., Wu, K. & Zou, J. ClashEval: Quantifying the tug-of-war between an LLMs internal prior and external evidence. in Advances in Neural Information Processing Systems 37 3340233422 (Neural Information Processing Systems Foundation, Inc. (NeurIPS), San Diego, California, USA, 2024). Supplemental Figures: 15 Supplemental Figure 1. Performance of each model on USMLE-style questions (MedQA), broken down by A) human system category and B) USMLE competency category. Statistical significance established via pairwise exact McNemars tests. Error bars represent 95% confidence intervals, and labeled letters indicate significant differences within each system or within each competency: models with the same letter (or no letter) are not significantly different whereas models with different letters are significantly different at ùõº=0.05. 16 Supplemental Figure 2. Performance of clinical versus generalist models on USMLE-style questions (MedQA), broken down by A) human system category and B) USMLE competency category. Statistical significance established via two-way ANOVA and ≈†id√°k-corrected post-hoc analysis. Error bars represent SEM. * < 0.05, ** < 0.01, *** < 0.001, **** < 0.0001."
        }
    ],
    "affiliations": [
        "Center for Data Science, New York University",
        "Department of Aerospace Engineering & Engineering Mechanics, The University of Texas at Austin",
        "Department of Biomedical Engineering, The University of Texas at Austin",
        "Department of Mathematics, The University of Texas at Austin",
        "Department of Medicine, NYU Langone Health",
        "Department of Molecular Biosciences, The University of Texas at Austin",
        "Department of Neurological Surgery, NYU Langone Health",
        "Department of Neurosurgery, Washington University School of Medicine in St. Louis",
        "Department of Population Health, NYU Langone Health",
        "Department of Radiology, NYU Langone Health",
        "Global AI Frontier Lab, New York University"
    ]
}