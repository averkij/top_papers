{
    "paper_title": "A Pragmatic VLA Foundation Model",
    "authors": [
        "Wei Wu",
        "Fan Lu",
        "Yunnan Wang",
        "Shuai Yang",
        "Shi Liu",
        "Fangjing Wang",
        "Qian Zhu",
        "He Sun",
        "Yong Wang",
        "Shuailei Ma",
        "Yiyu Ren",
        "Kejia Zhang",
        "Hui Yu",
        "Jingmei Zhao",
        "Shuai Zhou",
        "Zhenqi Qiu",
        "Houlong Xiong",
        "Ziyu Wang",
        "Zechen Wang",
        "Ran Cheng",
        "Yong-Lu Li",
        "Yongtao Huang",
        "Xing Zhu",
        "Yujun Shen",
        "Kecheng Zheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 6 2 ] . [ 1 2 9 6 8 1 . 1 0 6 2 : r a"
        },
        {
            "title": "A Pragmatic VLA Foundation Model",
            "content": "Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng Equal Contribution Project Lead Offering great potential in robotic manipulation, capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers throughput of 261 samples per second per GPU with an 8-GPU training setup, representing 1.5 2.8 (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with focus on enabling more challenging tasks and promoting sound evaluation standards. Website: https://technology.robbyant.com/lingbot-vla Github: https://github.com/robbyant/lingbot-vla Checkpoints: https://huggingface.co/collections/robbyant/lingbot-vla"
        },
        {
            "title": "1 Introduction",
            "content": "Vision-Language-Action (VLA) foundation models [5, 6, 27] have emerged as promising method for enabling robots to perform diverse manipulation tasks guided by natural language instructions. Through large-scale pre-training, these models acquire generalizable skills that can be rapidly adapted to diverse tasks and robotic platforms. Despite the significant progress, there remains lack of comprehensive empirical studies on how real-robot performance scales with increasingly vast pre-training datasets. Moreover, the community lacks highly optimized training codebase capable of efficiently conducting these scaling evaluations on massive volumes of data. Consequently, fundamental question that demands investigation in the real-world setting is: How do VLA models truly scale with massive real-world robot data? Understanding the scaling behavior of VLA models is crucial for robotic learning, especially on vast and diverse real-world datasets. In this work, we provide systematic empirical investigation into how success rates scale with respect to data volume and diversity during VLA pre-training By scaling pre-training data from 3,000 hours to 20,000 hours, we demonstrate that downstream success rates improve consistently and substantially. Notably, this scaling behavior shows no signs of saturation even at the 20,000-hour mark, suggesting that VLA performance continues to benefit from increased data volume. These results provide the first empirical evidence of favorable scaling properties in real-world robot learning, offering critical insights for future VLA development and large-scale data curation. While scaling analysis reveals favorable performance trends, translating these insights into reliable, deployable systems necessitates rigorous evaluation on real robotic platforms at large scale. Thanks to GM-100 [29], which provides 100 carefully designed tasks, we conduct systematic assessment across 3 robotic platforms, involving 130 episodes per task per embodiment. By emphasizing task diversity and multi-platform consistency, our evaluation framework provides choice of new standards for sound VLA benchmarking. 1 Figure 1. Overview of LingBot-VLA. We scale dual-arm robot data collected in the real world for pre-training. LingBot-VLA can be easily and efficiently transferred to downstream tasks. Moreover, we conduct systematic assessment across three robotic embodiments, which demonstrates the clear superiority of our model. In this paper, we present LingBot-VLA, pragmatic VLA foundation model trained on about 20,000 hours of real-world manipulation data from 9 robotic platforms. Our systematic evaluation on the comprehensive benchmark, demonstrates that LingBot-VLA achieves state-of-the-art performance and exceptional generalization compared to existing methods. Beyond model capabilities, we emphasize that large-scale robot learning necessitates high computational efficiency. To this end, we have developed an optimized codebase that achieves throughput of 261 samples per second per GPU on an 8-GPU cluster. This efficiency gain substantially shortens training cycles and reduces computational overhead, thereby lowering the overall costs. By combining superior performance, broad generalizability, and computational efficiency, LingBot-VLA is well-positioned for real-world robotic applications. To foster community progress, we provide open access to the code, base model, and benchmark data, with focus on enabling more challenging tasks and promoting sound evaluation standards."
        },
        {
            "title": "2.1 Vision-Language-Action Models",
            "content": "Foundation VLA. Vision-language-action foundation models typically adopt powerful pre-trained vision-language model [2, 3] as the semantic backbone, coupled with diffusion-based action head. Recent VLA foundation models [4 7, 13, 26, 27, 32, 33] have demonstrated enhanced multi-task execution capabilities and superior multi-embodiment adaptability, following pre-training on larger-scale and increasingly diverse datasets. Distinguishing itself from the datasets utilized in preceding VLA foundation models, our model is pre-trained on an extensive corpus approximate 20,000 hours of multi-embodiment data. This massive-scale dataset, characterized by its high behavioral diversity, significantly bolsters the models generalization capabilities across various robotic manipulation tasks. 2 Spatial VLA. While traditional VLA models excel at semantic understanding, they often struggle with precise geometric reasoning and depth perception required for complex spatial manipulation. To address this, several works [9, 11, 14, 21, 23, 25, 32] have integrated spatial representations into the VLA framework. Several research [9, 26, 32] initiatives have focused on bolstering the spatial awareness of VLMs within embodied scenarios to enhance the spatial manipulation capabilities of VLAs in downstream tasks. Others explicitly or implicitly incorporate depth information during the VLA training phase. Spatial Forcing [14] employs streamlined alignment strategy that compels the integration of VLA visual embeddings with spatial representations, thereby significantly improving the models spatial comprehension."
        },
        {
            "title": "2.2 Evaluation on Robot Policy",
            "content": "Current evaluation methodologies for robot policies are primarily bifurcated into two categories: simulation-based [8, 15, 17, 19, 20] and real-world embodiment-based [1, 31]. Simulation-based benchmark provide rapid and convenient means to evaluate the capabilities of policies, enabling large-scale parallel testing across vast and diverse interaction scenarios at very low cost. Although simulation environments typically employ idealized physical models, their results often do not fully represent the complexity of the real physical world. The another real-world evaluations efficiency is often bottlenecked by the requirement for extensive hardware parallelism. Consequently, the majority of prior VLA studies have been confined to comparing limited number of methods across only few tasks. To more comprehensively evaluate the real-world performance of policies, this work conducts an assessment across three distinct robotic platforms, with 100 tasks executed on each platform. We further provide thorough analysis of how mainstream VLA models adapt to the diversity encountered in real-world scenarios."
        },
        {
            "title": "2.3 Efficient VLA Training",
            "content": "The rapid iteration of VLA models has catalyzed the development of specialized training infrastructure. Several well-designed open-source codebases have recently emerged in the community, each catering to different research priorities. For instance, the OpenPI [6] repository provides versatile framework supporting both JAX and PyTorch for training the π series models. StarVLA [22] introduces modular and user-friendly codebase specifically optimized for the co-training of VLAs and VLMs, facilitating the transfer of semantic knowledge to robotic control. Additionally, Dexbotic [30] is designed as unified and efficient solution to streamline the development lifecycle of VLAs, focusing on standardizing the pipeline from data ingestion to model deployment. Despite these advancements, training large-scale VLA models on multi-node clusters remains significant challenge due to data I/O bottlenecks and communication overheads. To bridge this gap, we present LingBot-VLA, high-performance open-source codebase engineered for large-scale VLA training. Unlike existing frameworks, our codebase implements systemic optimizations in data loading, distributed training strategies, and operator-level acceleration. These enhancements lead to comprehensive improvement in training throughput and scalability, providing more efficient foundation for the community to explore the scaling limits of robotic foundation models."
        },
        {
            "title": "3.1 Data Collection",
            "content": "The pre-training dataset is built upon large-scale teleoperated data collected from 9 popular dual-arm robot embodiments, as shown in Fig. 2. We discuss these embodiments below: AgiBot G1. This setup has two 7-DoF arms with three RGB-D cameras. Robot data are collected via VR-based teleoperation on this setup. AgileX. This setup is equipped with three cameras and two 6-DoF arms. Robot control is achieved using isomorphic arms during the data collection process. Galaxea R1Lite. This setup has two 6-DoF arms, with one stereo camera and two wrist cameras. Galaxea R1Pro. Two 7-DoF arms, one stereo camera, and two wrist cameras are used in this setup. Realman Rs-02. This setup uses three cameras and features 16-dimensional configuration and action space: two 7-DoF arms and two parallel grippers. 3 Figure 2. Visualization of pre-training dataset used by LingBot-VLA. Leju KUAVO 4 Pro. This setup features two 7-DoF arms, two parallel grippers, one camera on the head, and two cameras on the wrists. Qinglong. humanoid robot with two 7-DoF arms and three cameras: one on the head and one on each wrist. ARX Lift2. This setup uses three cameras and two 6-DoF arms. Bimanual Franka. This setup uses two 7-DoF arms and two parallel grippers, forming 16-dimensional action space, with three cameras."
        },
        {
            "title": "3.2 Data Labeling",
            "content": "To obtain precise language instructions, we perform the following annotations: (1) Video Segment. Videos from multiple viewpoints, captured by robots, are jointly decomposed into clips by human annotators according to predefined atomic actions. Besides, to reduce the redundant information within videos, static frames at the start and end of the videos are eliminated at this stage. (2) Instruction Annotation. After obtaining videos containing the robots full motion trajectories and video clips for each atomic action, we employ Qwen3-VL-235B-A22B [2] for precise annotation of task and sub-task instructions, as shown in Fig. 1."
        },
        {
            "title": "4.1 Architecture",
            "content": "To leverage well-trained vision-language representations, LingBot-VLA integrates the pre-trained VLM (i.e., Qwen2.5VL [2]) with an initialized action generation module called action expert. These components are organized via Mixture-of-Transformers (MoT) architecture like BAGEL [10], where vision-language and action modalities are processed through distinct transformer pathways, coupled by shared self-attention mechanism for layer-wise unified sequence modeling. This MoT framework ensures that high-dimensional semantic priors from the VLM provide continuous guidance across all layers, while simultaneously mitigating cross-modal interference by maintaining modality-specific processing. The architecture of LingBot-VLA is illustrated in Fig. 1. Multi-view operational images and the related task instruction are uniformly encoded through VLM to establish multimodal conditioning for subsequent action generation. Concurrently, the robots proprioceptive sequences, specifically initial states and action chunks, are fed into the action expert for the prediction of action generation. We employ Flow Matching [16] for continuous action modeling, which facilitates fluid and smooth robotic control, ensuring high-precision execution across complex tasks and diverse robots. 4 In LingBot-VLA, the VLM and the action expert interact through shared self-attention mechanism, facilitating unified layer-wise representation. Consequently, the joint modeling sequence at timestamp is formulated as the concatenation of the observation conditions Ot and the action chunk At. Specifically, the observation context is defined as: Ot = [I1 , I2 , t , Tt, st], (1) which incorporates tokens from three-view operational images I1,2,3 robot state st. The corresponding action sequence is denoted as: of dual-arm robots, the task instruction Tt, and the At = [at, at+1, . . . , at+T 1], (2) where represents the action chunk length, i.e., the temporal horizon of the predicted trajectory, which is set to 50 during our pre-training stage. Therefore, the training objective is to characterize the conditional distribution p(AtOt) through conditional flow matching. For flow timestep [0, 1], we define probability path through linear interpolation between the Gaussian noise ϵ (0, I) and the ground-truth action At, obtaining the intermediate action At,s = sAt + (1 s)ϵ. The conditional distribution of At,s is formulated as: p(At,sAt) = (sAt, (1 s)I). The action expert vθ is trained to predict the conditional vector field by minimizing the Flow Matching objective: LFM = EsU [0,1],At,ϵ vθ(At,s, Ot, s) (At ϵ)2 , (3) (4) where the target velocity is given by the ideal vector field At ϵ derived from the linear probability path. Following π0 [6], we implement blockwise causal attention for modeling the joint sequence [Ot, At]. The sequence can be partitioned into three distinct functional blocks: [I1 , I2 , Tt], [st] and [at, at+1, . . . , at+T 1]. causal mask is applied among these blocks, such that tokens in each block can only attend to themselves and those in preceding blocks. Conversely, all tokens within the same block employ bidirectional attention and can attend to each other. This configuration ensures that the action expert can leverage all available observation knowledge, while preventing information leakage from future action tokens into the current observation representations. , I3 To explicitly capture spatial awareness within manipulation environments and further enhance the robots execution robustness, we adopt vision distillation approach inspired by recent works [12, 28]. Specifically, we apply the learnable queries [Q1 ] corresponding to three-view operational images. To integrate depth information, these queries are , D3 processed by VLM and then aligned with the depth tokens [D1 ] from LingBot-Depth [24]. We align the VLM learnable queries and LingBot-Depth tokens by minimizing the distillation loss Ldistill: , D2 , t , Q3 where Proj() is projection layer that applies cross-attention for dimensional alignment. This integration infuses geometric information into the LingBot-VLA model, enabling precise perception for complex manipulation tasks. Ldistill = EQt Proj(Qt) Dt , (5)"
        },
        {
            "title": "4.2 Training Efficiency Optimization",
            "content": "Given that action data is inherently high-frequency, establishing highly efficient pipeline encompassing distributed training and operator optimization is imperative. Our optimization methodology is structured as follows: Distributed Strategy: While VLA models typically possess moderate parameter count, achieving an optimal trade-off between GPU memory occupancy and training throughput remains essential. We employ Fully Sharded Data Parallel (FSDP)a highly efficient PyTorch implementation of the Zero Redundancy Optimizer (ZeRO)to shard optimizer states, model parameters, and gradients, thereby minimizing memory footprint. Drawing inspiration from the Hybrid Sharded Data Parallel (HSDP) approach proposed in VeOmni [18], we construct specific shard groups exclusively for the action expert modules. This strategy effectively mitigates the communication overhead associated with excessive parameter sharding. Additionally, we implement mixed-precision policy: performing reductions in torch.float32 to ensure numerical stability, while utilizing torch.bfloat16 for storage and communication. Operator-Level Optimization: The multimodal fusion of vision, language, and action within our architecture is fundamentally sparse attention process. To address this, we leverage FlexAttention to optimize computation. Furthermore, we apply operator fusion (via torch.compile) to reduce kernel launch overhead and maximize memory bandwidth utilization. 5 (a) (b) Figure 3. Word cloud of atomic actions in (a) Pre-training datasets and (b) Benchmark."
        },
        {
            "title": "5.1 Large-scale Real-world Benchmark",
            "content": "We conduct large-scale empirical evaluation of LingBot-VLA designed to rigorously assess multi-embodiment generalization and real-world robustness. Our experimental framework comprises three core components: (1) 25 physical robots spanning 3 distinct commercial platforms, (2) GM-100 [29] benchmark featuring 100 diverse manipulation tasks with 39,000 expert demonstrations and (3) controlled evaluation protocol generating 22,500 trials comparing LingBot-VLA against three state-of-the-art baselines under identical training and testing conditions."
        },
        {
            "title": "5.1.1 Hardware Platforms",
            "content": "We conduct experiments across 3 distinct robotic platforms: AgileX, Agibot G1 and Galaxea R1Pro. All three embodiments feature dual-arm configuration equipped with parallel-jaw grippers. To ensure robust perception, each robot is outfitted with multiple cameras: two wrist-mounted cameras and head-mounted camera to capture an egocentric, human-eye perspective. All tasks are tabletop-based, with the embodiments chassis and waist securely fixed in place."
        },
        {
            "title": "5.1.2 Data Collection and Processing",
            "content": "For each GM-100 task [29], we collect expert demonstrations via teleoperation following standardized protocol designed to ensure high data quality and environmental diversity. Trajectory Volume: 150 raw trajectories are collected per task across three platforms. The top 130, ranked by execution quality (task completion, motion smoothness, and protocol adherence), are retained for training. All trajectories strictly follow GM-100 task specifications. Standardized Objects: task objects are standardized and sourced according to GM-100 material specifications to ensure reproducibility across sites. Environmental Diversity: object poses (positions and orientations) are randomized within the workspace for each trajectory to prevent overfitting to specific spatial configurations and encourage learning of task-relevant invariances. Teleoperation Guidelines: (1) maintaining clearance between the end-effector and workspace surfaces to avoid collisions, (2) reducing velocity during object contact phases for smooth manipulation, and (3) ensuring distinct image observations at episode start and termination for reliable policy training. Automated Filtering: an algorithmic screening procedure automatically excludes episodes exhibiting technical anomalies. Manual Review: human reviewers validate the filtered dataset using synchronized multi-view video streams. Episodes are removed if they include extraneous objects or deviate from task protocols. To analyze the semantic distribution and diversity of action categories, we visualized the most prevalent atomic actions in the training and testing sets using word clouds, as shown in Figs. 3a and 3b. Quantitative analysis reveals that approximately 50% of the atomic actions in the test set are absent from the top 100 most frequent training actions. This significant discrepancy underscores the diversity of our test set and ensures rigorous assessment of the models generalization capabilities. 6 Table 1. Experiment results of real-world evaluation on GM-100 [29] benchmark. SR refers to success rate, and PS refers to progress score."
        },
        {
            "title": "Platform",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR π0.5 SR PS Ours w/o depth PS SR Ours w/ depth PS SR Agibot G1 AgileX Galaxea R1Pro 5.23% 12.63% 7.77% 21.98% 12.82% 30.04% 11.98% 30.47% 2.99% 8.75% 3.26% 10.52% 17.20% 34.82% 15.50% 36.31% 18.93% 40.36% 2.26% 8.16% 6.89% 14.13% 14.29% 24.83% 14.10% 26.14% 18.89% 34.71% 20.98% 35.40%"
        },
        {
            "title": "Average",
            "content": "4.05% 10.35% 7.59% 15.99% 13.02% 27.65% 15.74% 33.69% 17.30% 35.41% Table 2. Experiment results of simulation evaluation on RoboTwin 2.0 [8] benchmark. (a). Clean Scenes (b). Randomized Scenes π0.5 Ours w/o depth Ours w/ depth π0.5 Ours w/o depth Ours w/ depth Average SR 82.74% 86.50% 88.56% Average SR 76.76% 85.34% 86.68%"
        },
        {
            "title": "5.1.3 Benchmarking and Evaluation Protocol",
            "content": "We systematically compare LingBot-VLA with three state-of-the-art VLA models: π0.5, GR00T N1.6, and WALLOSS, under strict experimental controls to isolate architectural performance. Standardized Training: All models are fine-tuned from publicly available pre-trained checkpoints using the same post-training pipeline. The verified dataset (130 filtered trajectories per task) and consistent hyperparameters (i.e., batch sizes=256, epochs=20) are applied to ensure fair comparisons. Strict Machine-Task Pairing: To eliminate hardware-induced variance, evaluations are conducted on the exact robot units used during data collection. All models are tested sequentially on the same hardware-task pair in randomized order. For example, in the Stack Bowls task, all models are evaluated on the same unit across AgileX, Agibot G1, and Galaxea R1pro platforms. Controlled Evaluation Setup: Testing conditions follow standardized protocols, mirroring data collection procedures with randomized object positions and orientations while maintaining consistent task specifications. This ensures evaluation of generalization rather than memorization. Inference and Recording: Each model undergoes 15 trials per task-robot pair for statistical robustness. Evaluation environments are kept constant, and comprehensive data (e.g., third-person views, robot states, and model predictions) are recorded in rosbag format for transparency. These recordings will be open-sourced to establish verifiable benchmarks."
        },
        {
            "title": "5.1.4 Evaluation Metrics",
            "content": "We evaluate model performance using two metrics capturing both task completion and partial progress. Success Rate (SR): the proportion of trials where the model completes all task steps within 3-minute time limit. This primary metric reflects the models real-world deployment viability. Progress Score (PS): measures partial task completion by tracking progress through sequential subtask checkpoints: For example, in 6-step Stack Bowls task, completing steps 14 but failing at step 5 results in score of 4 6 0.67. This diagnostic metric highlights failure modes and rewards partial success. Termination Criteria: trial ends if: (1) three consecutive subtask failures occur, or (2) safety-critical events (e.g., collisions) arise. Progress is scored based on subtasks completed before termination. We report overall SR and PS across 100 tasks, and per-platform metrics stratified by robot type to assess cross-embodiment generalization."
        },
        {
            "title": "5.2 Comparison on Real-world Benchmark",
            "content": "As shown in Tab. 1, we compare our two LingBot-VLA variants with three strong baselines across three platforms. On all platforms, LingBot-VLA w/o depth significantly outperforms WALL-OSS and GR00T N1.6 in both SR and PS metrics. By incorporating depth-based spatial information, LingBot-VLA w/ depth achieves an average SR improvement of 4.28% and PS increase of 7.76% over π0.5 across the three embodiments. Notably, GR00T N1.6 performs average on the Agibot G1 and AgileX embodiment but achieves SR and PS comparable to π0.5 on the Galaxea R1Pro platform. This is due to the extensive inclusion of Galaxea R1Pro data during its pre-training, indicating that pre-training can significantly enhance performance on downstream tasks with high structural similarity. The complete and detailed test results can be found in the Appendix Tab. S1S6. 7 (a). Qwen2.5-VL-3B-π model (b). PaliGemma-3B-pt-224-π model Figure 4. Training throughput analysis of the (a) Qwen2.5-VL-3B-π and (b) PaliGemma-3B-pt-224-π models."
        },
        {
            "title": "5.3 Comparison on Simulation Benchmark",
            "content": "In Tab. 2, we evaluate simulation performance across 50 representative manipulation tasks within the RoboTwin 2.0 suite. Starting from pretrained checkpoints, each model was further finetuned on the RoboTwin dataset. To assess multi-task generalization, we train all models on 2, 500 demonstrations from clean scenes (50 per task) and 25, 000 from highly randomized scenes (500 per task). Randomization factors encompass varied backgrounds, table-top clutter, table-height perturbations, and diverse lighting conditions. Compared to the π0.5 baseline, LingBot-VLA demonstrates marked advancements in RoboTwin 2.0 multi-task settings. Specifically, LingBot-VLA w/o depth yields absolute success rate increases of over 3.76% in clean environments and 8.58% in randomized scenarios. By employing learnable query-based alignment, the integration of depth information enables LingBot-VLA to effectively extract rich spatial priors from LingBot-Depth model. The approach surpasses the baseline model by absolute margins of 5.82% and 9.92% in clean and randomized configurations, respectively. Please refer to Tab. S7 of Appendix for detailed results."
        },
        {
            "title": "5.4 Training Throughput Analysis",
            "content": "To comprehensively evaluate the training efficiency of VLA models across different frameworks, we selected three opensource codebases (i.e., StarVLA, Dexbotic, and OpenPI) as the baselines for comparison. To ensure fair comparison, all experiments were conducted on the Libero dataset using standardized π-like model architecture. Given the variations in the VLM implementations across different codebases, we reproduced both Qwen2.5-VL-3B-π and PaliGemma-3B-pt-224-π models within our own codebase to facilitate direct alignment and comparison with the baselines. Regarding the training configuration, the local batch size was standardized to 32 for all experiments. It is worth noting that while StarVLA and Dexbotic default to ZeRO for distributed training, our codebase employs the comparable FSDP2 strategy. In contrast, OpenPI utilizes DDP, which inherently incurs lower communication overhead. We adopted sample throughput (samples/s) as the primary evaluation metric. Figures 4a and 4b illustrate the training efficiency comparison between our codebase and the baselines for the Qwen2.5-VL-3B-π and PaliGemma-3B-pt-224-π models, respectively. The results demonstrate that our codebase achieved the fastest training speeds in both model settings. Furthermore, the figures detail the training throughput across configurations of 8, 16, 32, 128, and 256 GPUs, alongside the theoretical linear scaling limit. The data indicates that our solution not only delivers superior throughput but also exhibits excellent scaling efficiency that closely follows the theoretical limit as the number of GPUs increases."
        },
        {
            "title": "5.5.1 Scaling Experiments",
            "content": "To assess the scaling laws of pre-training data, we conduct experiments on subset of 25 representative tasks drawn from the benchmark. As shown in Figs. 5a and 5b, both the progress rate and success rate demonstrate consistent upward trend as the pre-training data duration increases from 3,000 to 20,000 hours. This indicates that scaling up 8 (a). Progress Rate (PS) (b). Success Rate (SR) Figure 5. Scaling behavior across dataset size. With increased data scale, our model exhibits scaling laws in terms of success rate and progress rate. real-world pre-training data contributes to improved generalization and performance across diverse downstream tasks and embodiments. Furthermore, the individual trends of the three embodiments (i.e., Agibot G1, AgileX, and Galaxea R1Pro) generally align with the aggregated performance, suggesting the observed scaling law is robust and not specific to single platform. These results validate the effectiveness of our scaling approach in enhancing the capabilities of the generalist policy."
        },
        {
            "title": "5.5.2 Data-efficient Analysis",
            "content": "Following the large-scale real-world benchmarking protocols, we selected eight representative tasks from GM-100 dataset to conduct data-efficient post-training experiments on the Agibot G1 platform. As illustrated in Fig. 6, with limited budget of only 80 demonstrations per task, LingBot-VLA outperforms π0.5 using the full 130demonstration set in both Progress Rate and Success Rate. Notably, the performance margin between LingBotVLA and π0.5 widens significantly as the volume of post-training data increases, demonstrating superior data efficiency and scalability."
        },
        {
            "title": "6 Conclusion",
            "content": "Figure 6. Data efficiency of LingBot-VLA post-training. We have introduced LingBot-VLA, foundation model that achieves superior generalizability and training efficiency through large-scale real-world data and an optimized codebase. Our comprehensive evaluation across 100 tasks demonstrates that our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. To foster open science, we release our code, model, and benchmark data. Future research will focus on scaling the model versatility by integrating single-arm and mobile robotic data, paving the way for more diverse and mobile manipulation capabilities in unconstrained environments. Acknowledgment. We thank Zhengyu He, Han Zhang, Haidan Zhou, Chongjun Zhong, Yida Zou, Siyuan Li, Zhikun Luo, Yuanqi Chen, Yingying Zhang, Yijun Zheng, Wanting Xu, Hongfei Niu, Yan Zha, Ka Leong Cheng, Liping Zhang, Zhen Liu, Rundong Zhou, Yuan Guan, Haitao Wang, Weilun Yao, Zhiwei Liang, Jiahao Fan, Jingran Xu, Linyu Su, Haiwei Liang, Yixiang Gao, Yingmin Li, Yongqiang Wen, Jiafei Li, Yuanzhe Guo, Yijun Zheng, Fengrui Zhang, Lin Wang, Min Yao, Fei Lu, Jingyun Tian, Ting Huang, Xinyang Wang, Jianxue Qian and Wenhui Shi for help with data, evaluation experiments, training infrastructure, robot hardware and robot software. We also gratefully acknowledge Galaxea Team, AgileX Robotics, and Leju(Shenzhen) Robotics Technology Co., Ltd. for help with the data collection and benchmark."
        },
        {
            "title": "References",
            "content": "[1] Pranav Atreya, Karl Pertsch, Tony Lee, Moo Jin Kim, Arhan Jain, Artur Kuramshin, Clemens Eppner, Cyrus Neary, Edward Hu, Fabio Ramos, et al. Roboarena: Distributed real-world evaluation of generalist robot policies. arXiv preprint arXiv:2506.18123, 2025. [2] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [3] Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3B VLM for transfer. arXiv preprint arXiv:2407.07726, 2024. [4] Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. GR00T N1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. [5] Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Robert Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, brian ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization. In Conference on Robot Learning, 2025. [6] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. π0: vision-language-action flow model for general robot control. In Proceedings of Robotics: Science and Systems, 2025. [7] Chilam Cheang, Sijin Chen, Zhongren Cui, Yingdong Hu, Liqun Huang, Tao Kong, Hang Li, Yifeng Li, Yuxiao Liu, Xiao Ma, et al. GR-3 technical report. arXiv preprint arXiv:2507.15493, 2025. [8] Tianxing Chen, Zanxin Chen, Baijun Chen, Zijian Cai, Yibin Liu, Zixuan Li, Qiwei Liang, Xianliang Lin, Yiheng Ge, Zhenyu Gu, et al. Robotwin 2.0: scalable data generator and benchmark with strong domain randomization for robust bimanual robotic manipulation. arXiv preprint arXiv:2506.18088, 2025. [9] Xinyi Chen, Yilun Chen, Yanwei Fu, Ning Gao, Jiaya Jia, Weiyang Jin, Hao Li, Yao Mu, Jiangmiao Pang, Yu Qiao, InternVLA-M1: spatially guided vision-language-action framework for generalist robot policy. arXiv preprint et al. arXiv:2510.13778, 2025. [10] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [11] Heyu Guo, Shanmu Wang, Ruichun Ma, Shiqi Jiang, Yasaman Ghasempour, Omid Abari, Baining Guo, and Lili Qiu. OmniVLA: Physically-grounded multimodal vla with unified multi-sensor perception for robotic manipulation. arXiv preprint arXiv:2511.01210, 2025. [12] Xiaohu Huang, Jingjing Wu, Qunyi Xie, and Kai Han. MLLMs need 3D-aware representation supervision for scene understanding. arXiv preprint arXiv:2506.01946, 2025. [13] Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Jiyang Gao, Huazhe Xu, and Hang Zhao. Galaxea open-world dataset and G0 dual-system vla model. arXiv preprint arXiv:2509.00576, 2025. [14] Fuhao Li, Wenxuan Song, Han Zhao, Jingbo Wang, Pengxiang Ding, Donglin Wang, Long Zeng, and Haoang Li. Spatial forcing: Implicit spatial representation alignment for vision-language-action model. arXiv preprint arXiv:2510.12276, 2025. [15] Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. [16] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [17] Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Adv. Neural Inform. Process. Syst., 36:4477644791, 2023. 10 [18] Qianli Ma, Yaowei Zheng, Zhelun Shi, Zhongkai Zhao, Bin Jia, Ziyue Huang, Zhiqi Lin, Youjie Li, Jiacheng Yang, Yanghua Peng, et al. Veomni: Scaling any modality model training with model-centric distributed recipe zoo. arXiv preprint arXiv:2508.02317, 2025. [19] Oier Mees, Lukas Hermann, Erick Rosete-Beas, and Wolfram Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters, 7(3):73277334, 2022. [20] Soroush Nasiriany, Abhiram Maddukuri, Lance Zhang, Adeet Parikh, Aaron Lo, Abhishek Joshi, Ajay Mandlekar, and Yuke Zhu. Robocasa: Large-scale simulation of everyday tasks for generalist robots. arXiv preprint arXiv:2406.02523, 2024. [21] Delin Qu, Haoming Song, Qizhi Chen, Yuanqi Yao, Xinyi Ye, Yan Ding, Zhigang Wang, JiaYuan Gu, Bin Zhao, Dong Wang, et al. SpatialVLA: Exploring spatial representations for visual-language-action model. arXiv preprint arXiv:2501.15830, 2025. [22] starVLA Contributors. StarVLA: lego-like codebase for vision-language-action model developing, 2025. [23] Lin Sun, Bin Xie, Yingfei Liu, Hao Shi, Tiancai Wang, and Jiale Cao. GeoVLA: Empowering 3d representations in visionlanguage-action models. arXiv preprint arXiv:2508.09071, 2025. [24] Bin Tan, Changjian Sun, Xiage Qin, Hanat Adai, Zelin Fu, Tianxiang Zhou, Han Zhang, Yinghao Xu, Xing Zhu, Yujun Shen Shen, and Nan Xue. Masked depth modeling for spatial perception. https://technology.robbyant.com/lingbot-depth, 2026. [25] Gemini Robotics Team, Abbas Abdolmaleki, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Ashwin Balakrishna, Nathan Batchelor, Alex Bewley, Jeff Bingham, et al. Gemini Robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer. arXiv preprint arXiv:2510.03342, 2025. [26] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, et al. Gemini Robotics: Bringing AI into the physical world. arXiv preprint arXiv:2503.20020, 2025. [27] NVIDIA GEAR Team. GR00T N1.6: An improved open foundation model for generalist humanoid robots. https://research. nvidia.com/labs/gear/gr00t-n1_6/, 2025. [28] Yunnan Wang, Fan Lu, Kecheng Zheng, Ziyuan Huang, Ziqiang Li, Wenjun Zeng, and Xin Jin. Vision-centric activation and coordination for multimodal large language models. arXiv preprint arXiv:2510.14349, 2025. [29] Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, and Yong-Lu Li. The Great March 100: 100 detail-oriented tasks for evaluating embodied ai agents, 2026. [30] Bin Xie, Erjin Zhou, Fan Jia, Hao Shi, Haoqiang Fan, Haowei Zhang, Hebei Li, Jianjian Sun, Jie Bin, Junwen Huang, et al. Dexbotic: Open-source vision-language-action toolbox. arXiv preprint arXiv:2510.23511, 2025. [31] Adina Yakefu, Bin Xie, Chongyang Xu, Enwen Zhang, Erjin Zhou, Fan Jia, Haitao Yang, Haoqiang Fan, Haowei Zhang, Hongyang Peng, et al. RoboChallenge: Large-scale real-robot evaluation of embodied policies. arXiv preprint arXiv:2510.17950, 2025. [32] Jianwei Yang, Reuben Tan, Qianhui Wu, Ruijie Zheng, Baolin Peng, Yongyuan Liang, Yu Gu, Mu Cai, Seonghyeon Ye, Joel Jang, et al. Magma: foundation model for multimodal AI agents. In IEEE Conf. Comput. Vis. Pattern Recog., pages 1420314214, 2025. [33] Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, et al. Igniting VLMs toward the embodied space. arXiv preprint arXiv:2509.11766, 2025."
        },
        {
            "title": "A Experiment",
            "content": "This section provides comprehensive breakdown of the experimental results. Specifically, Table S1, Table S2, Table S3, Table S4, Table S5 and Table S6 below present the detailed performance on GM-100 real-world benchmark. Table S7 presents the detailed performance on Robotwin 2.0 benchmark. These tables serve as the basis for the aggregated mean results reported in the main text (see Sec. 5.2 and Sec. 5.3). 11 Table S1. Real-world evaluation on GM-100 [29] benchmark (AgileX, Part I)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #001 #002 #003 #006 #007 #008 #009 #010 #011 #012 #013 #014 #015 #016 #017 #018 #019 #020 #021 #022 #023 #024 #025 #026 #027 #028 #029 #030 #031 #032 #033 #034 #035 #036 #037 #038 #040 #041 #042 #043 #044 #045 #046 #047 #048 #049 #050 #051 #053 #054 #055 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 53% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 17% 3% 1% 3% 16% 8% 20% 2% 15% 0% 0% 3% 53% 0% 11% 0% 12% 3% 12% 0% 3% 0% 0% 0% 0% 0% 15% 0% 0% 13% 0% 0% 0% 11% 1% 0% 14% 0% 2% 18% 0% 0% 0% 6% 7% 0% 3% 0% 16% 3% 0% 20% 0% 0% 0% 0% 0% 27% 0% 7% 0% 0% 0% 67% 7% 20% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 53% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 23% 0% 0% 0% 7% 0% 47% 0% 7% 0% 0% 5% 67% 15% 63% 3% 0% 0% 5% 1% 0% 4% 0% 0% 3% 3% 15% 53% 2% 24% 24% 7% 0% 9% 3% 0% 0% 2% 0% 22% 0% 3% 5% 19% 9% 5% 4% 2% 13% 3% 3% Ours w/o depth PS SR Ours w/ depth PS SR 40% 0% 7% 0% 20% 0% 13% 13% 7% 7% 0% 0% 80% 0% 27% 0% 0% 7% 0% 0% 0% 0% 0% 0% 20% 40% 0% 67% 0% 20% 0% 67% 0% 0% 0% 0% 80% 0% 7% 27% 0% 0% 7% 7% 7% 0% 0% 0% 0% 7% 0% 67% 15% 42% 0% 61% 39% 33% 13% 67% 45% 0% 40% 80% 7% 71% 0% 55% 37% 27% 59% 22% 9% 0% 0% 59% 43% 43% 87% 3% 33% 31% 84% 7% 51% 6% 1% 91% 2% 22% 78% 5% 11% 60% 59% 63% 22% 15% 40% 48% 20% 5% 47% 0% 0% 0% 27% 0% 47% 7% 20% 33% 7% 0% 100% 0% 0% 0% 7% 0% 0% 7% 0% 0% 0% 0% 20% 0% 13% 53% 0% 0% 0% 53% 60% 0% 0% 0% 93% 0% 0% 33% 0% 0% 73% 7% 0% 0% 0% 0% 13% 20% 0% 73% 2% 19% 17% 70% 66% 73% 13% 67% 59% 20% 54% 100% 27% 60% 20% 35% 32% 23% 63% 12% 10% 0% 0% 59% 8% 44% 78% 5% 27% 27% 85% 60% 51% 8% 6% 98% 0% 22% 78% 3% 23% 89% 37% 52% 23% 33% 58% 20% 43% 0% π0.5 PS 57% 23% 53% 20% 41% 1% 0% 0% 77% 0% 15% 2% 40% 18% 75% 2% 57% 27% 25% 26% 3% 15% 5% 4% 68% 60% 67% 62% 35% 0% 53% 96% 0% 75% 16% 10% 96% 8% 65% 58% 14% 22% 29% 40% 39% 18% 25% 2% 60% 33% 13% SR 53% 0% 20% 0% 0% 0% 0% 0% 53% 0% 0% 0% 40% 7% 53% 0% 13% 0% 7% 0% 0% 0% 0% 0% 53% 27% 47% 60% 27% 0% 0% 87% 0% 13% 7% 0% 80% 0% 40% 7% 0% 0% 13% 7% 7% 0% 0% 0% 7% 0% 0% 12 Table S2. Real-world evaluation on GM-100 [29] benchmark (AgileX, Part II)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #056 #057 #058 #060 #061 #062 #063 #064 #065 #066 #067 #068 #069 #070 #071 #072 #073 #074 #075 #076 #077 #078 #079 #080 #081 #082 #083 #084 #085 #086 #087 #088 #089 #090 #091 #092 #093 #094 #095 #096 #097 #098 #099 #100 #102 #103 #104 #105 #106 #107 0% 0% 0% 0% 53% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 7% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 13% 47% 13% 0% 0% 7% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 28% 2% 37% 8% 77% 2% 0% 0% 22% 7% 0% 27% 2% 0% 3% 0% 0% 0% 9% 27% 7% 0% 15% 0% 8% 23% 20% 0% 9% 0% 3% 0% 2% 1% 37% 55% 14% 0% 2% 7% 4% 0% 0% 2% 2% 43% 15% 3% 1% 9% 0% 0% 7% 20% 7% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 7% 0% 0% 0% 0% 0% 0% 0% 13% 0% 0% 0% 0% 0% 13% 0% 27% 0% 0% 0% 0% 0% 0% 13% 0% 0% 0% 0% 0% 0% 7% 1% 44% 42% 33% 0% 7% 2% 12% 3% 2% 0% 10% 0% 5% 0% 55% 3% 13% 43% 7% 0% 20% 0% 3% 15% 0% 16% 40% 10% 0% 5% 22% 1% 25% 17% 27% 0% 18% 3% 0% 0% 3% 18% 13% 27% 7% 0% 4% 0% Ours w/o depth PS SR Ours w/ depth PS SR 73% 0% 7% 27% 93% 0% 0% 0% 33% 13% 7% 20% 0% 0% 0% 0% 27% 0% 0% 7% 47% 87% 47% 0% 0% 20% 7% 40% 13% 100% 20% 20% 0% 0% 33% 60% 27% 7% 20% 0% 27% 0% 0% 0% 0% 73% 7% 0% 0% 27% 93% 19% 63% 57% 97% 4% 0% 0% 55% 55% 20% 30% 17% 3% 0% 9% 57% 22% 4% 53% 47% 87% 73% 7% 26% 67% 28% 58% 36% 100% 72% 55% 28% 19% 42% 68% 27% 23% 38% 0% 44% 15% 0% 31% 33% 77% 40% 15% 13% 62% 80% 0% 20% 13% 93% 0% 0% 0% 20% 0% 0% 60% 0% 7% 0% 13% 13% 0% 13% 33% 40% 73% 33% 0% 13% 27% 53% 33% 27% 60% 60% 7% 13% 0% 7% 93% 87% 0% 13% 13% 7% 7% 0% 13% 27% 47% 20% 0% 7% 27% 92% 27% 72% 50% 97% 1% 0% 5% 62% 43% 27% 63% 12% 7% 5% 38% 23% 21% 56% 67% 42% 77% 80% 10% 50% 43% 77% 62% 42% 73% 78% 40% 41% 24% 17% 97% 87% 12% 39% 17% 24% 15% 0% 27% 60% 60% 53% 20% 31% 58% π0.5 PS 88% 13% 58% 93% 68% 8% 50% 5% 23% 45% 28% 53% 8% 0% 5% 27% 0% 19% 20% 33% 2% 83% 49% 0% 15% 57% 50% 2% 85% 70% 15% 42% 47% 43% 77% 82% 87% 20% 44% 13% 4% 0% 12% 42% 29% 90% 31% 23% 36% 73% SR 67% 0% 7% 93% 47% 0% 13% 0% 0% 7% 0% 47% 7% 0% 0% 7% 0% 0% 0% 7% 0% 80% 27% 0% 0% 20% 7% 0% 67% 60% 13% 13% 20% 0% 33% 67% 87% 0% 20% 7% 0% 0% 0% 27% 13% 80% 13% 0% 0% 53% 13 Table S3. Real-world evaluation on GM-100 [29] benchmark (AgibotG1, Part I)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #001 #002 #003 #006 #007 #008 #009 #010 #011 #012 #013 #014 #015 #016 #017 #018 #019 #020 #021 #022 #023 #024 #025 #026 #027 #028 #029 #030 #031 #032 #033 #034 #035 #036 #037 #038 #040 #041 #042 #043 #044 #045 #046 #047 #048 #049 #050 #051 #053 #054 #055 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 80% 0% 0% 0% 0% 0% 33% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 0% 20% 0% 3% 0% 0% 0% 41% 0% 0% 0% 0% 0% 80% 0% 0% 3% 8% 0% 54% 7% 2% 0% 12% 0% 0% 5% 0% 0% 3% 0% 2% 7% 0% 1% 0% 4% 6% 0% 0% 2% 0% 0% 0% 30% 1% 0% 2% 0% 4% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 0% 20% 0% 100% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 13% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 3% 0% 18% 10% 30% 10% 23% 0% 69% 0% 100% 0% 3% 0% 12% 0% 3% 1% 3% 23% 0% 0% 5% 0% 1% 0% 14% 0% 20% 50% 1% 5% 0% 5% 0% 5% 0% 23% 0% 0% 9% 0% 0% 0% 0% 0% 5% 10% 0% Ours w/o depth PS SR Ours w/ depth PS SR 0% 0% 7% 0% 0% 0% 13% 7% 53% 13% 33% 0% 100% 0% 0% 7% 0% 13% 13% 0% 0% 0% 0% 0% 27% 67% 7% 73% 7% 27% 0% 0% 0% 0% 0% 0% 40% 0% 0% 0% 0% 0% 47% 0% 0% 0% 0% 0% 0% 0% 0% 3% 0% 19% 0% 25% 2% 57% 44% 88% 22% 71% 5% 100% 3% 51% 67% 20% 37% 63% 12% 3% 9% 5% 0% 65% 77% 19% 78% 7% 44% 31% 42% 24% 2% 8% 10% 66% 0% 0% 35% 2% 3% 73% 7% 0% 0% 10% 0% 36% 3% 0% 0% 0% 0% 0% 13% 0% 7% 7% 0% 13% 0% 0% 93% 0% 0% 33% 0% 33% 60% 0% 0% 0% 13% 0% 47% 60% 7% 27% 0% 7% 7% 20% 7% 0% 0% 0% 47% 0% 0% 13% 0% 0% 27% 7% 0% 0% 0% 0% 13% 7% 0% 17% 5% 0% 0% 52% 0% 47% 18% 43% 37% 45% 0% 93% 2% 55% 72% 0% 47% 87% 9% 35% 29% 18% 7% 68% 78% 19% 76% 7% 27% 42% 67% 44% 5% 0% 4% 64% 3% 40% 52% 0% 15% 42% 23% 11% 3% 8% 0% 62% 17% 0% π0.5 PS 23% 35% 35% 0% 44% 7% 33% 11% 43% 28% 71% 0% 93% 20% 45% 8% 5% 10% 17% 13% 10% 18% 2% 2% 24% 43% 17% 47% 12% 7% 16% 45% 20% 11% 17% 34% 24% 0% 49% 13% 7% 0% 35% 5% 3% 0% 5% 0% 19% 3% 0% SR 0% 13% 0% 0% 7% 0% 0% 0% 20% 7% 20% 0% 93% 0% 27% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 47% 0% 0% 0% 13% 0% 0% 0% 13% 0% 0% 47% 0% 7% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 14 Table S4. Real-world evaluation on GM-100 [29] benchmark (AgibotG1, Part II)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #056 #057 #058 #060 #061 #062 #063 #064 #065 #066 #067 #068 #069 #070 #071 #072 #073 #074 #075 #076 #077 #078 #079 #080 #081 #082 #083 #084 #085 #086 #087 #088 #089 #090 #091 #092 #093 #094 #095 #096 #097 #098 #099 #100 #102 #103 #104 #105 #106 #107 0% 0% 0% 7% 47% 0% 0% 0% 0% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 33% 0% 0% 0% 0% 7% 0% 0% 0% 33% 0% 0% 0% 0% 0% 0% 0% 0% 0% 20% 0% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 32% 38% 50% 0% 18% 0% 20% 3% 0% 0% 2% 0% 37% 0% 0% 7% 43% 37% 0% 37% 16% 0% 2% 0% 55% 11% 0% 7% 52% 3% 0% 4% 3% 29% 0% 0% 13% 0% 40% 0% 0% 4% 0% 17% 7% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 20% 0% 0% 73% 0% 0% 0% 0% 0% 7% 0% 0% 53% 0% 0% 0% 0% 0% 60% 0% 67% 0% 0% 0% 0% 0% 0% 0% 87% 0% 7% 0% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 17% 1% 27% 22% 3% 2% 0% 0% 42% 7% 0% 83% 2% 0% 17% 0% 0% 41% 29% 7% 56% 3% 7% 0% 3% 2% 87% 12% 69% 10% 0% 4% 0% 13% 8% 15% 87% 3% 13% 3% 51% 10% 0% 11% 2% 33% 0% 0% 1% 12% Ours w/o depth PS SR Ours w/ depth PS SR 7% 0% 7% 0% 93% 0% 0% 0% 40% 0% 0% 40% 0% 47% 27% 0% 0% 0% 0% 0% 73% 27% 0% 0% 0% 0% 20% 0% 80% 0% 60% 0% 7% 33% 40% 7% 0% 0% 13% 0% 7% 0% 0% 13% 33% 53% 7% 0% 7% 0% 29% 13% 63% 37% 97% 2% 3% 13% 68% 15% 2% 60% 0% 60% 58% 0% 36% 21% 37% 50% 82% 37% 44% 7% 10% 17% 63% 38% 80% 20% 63% 45% 18% 62% 68% 18% 0% 19% 20% 0% 38% 3% 0% 27% 71% 70% 24% 20% 30% 28% 7% 0% 27% 0% 60% 0% 0% 0% 80% 0% 0% 33% 0% 33% 13% 0% 0% 0% 0% 0% 53% 40% 7% 7% 0% 0% 0% 7% 27% 7% 67% 13% 7% 0% 47% 13% 13% 0% 7% 0% 13% 0% 0% 40% 7% 7% 0% 0% 7% 0% 22% 8% 75% 17% 80% 2% 17% 2% 90% 15% 0% 40% 3% 53% 52% 0% 68% 25% 11% 43% 53% 50% 39% 17% 10% 20% 38% 46% 45% 23% 83% 44% 33% 32% 75% 35% 13% 12% 18% 0% 42% 2% 37% 53% 23% 40% 18% 13% 18% 27% π0.5 PS 15% 3% 77% 67% 67% 2% 8% 2% 52% 17% 3% 37% 5% 7% 12% 13% 53% 1% 24% 22% 71% 10% 4% 0% 2% 18% 85% 66% 2% 0% 65% 1% 9% 42% 77% 57% 7% 27% 8% 0% 27% 10% 7% 7% 10% 27% 31% 12% 15% 7% SR 0% 0% 20% 27% 40% 0% 0% 0% 7% 0% 0% 33% 0% 7% 0% 7% 0% 0% 0% 0% 67% 7% 0% 0% 0% 0% 60% 27% 0% 0% 47% 0% 0% 20% 20% 40% 7% 7% 0% 0% 0% 0% 0% 0% 0% 7% 0% 0% 7% 0% 15 Table S5. Real-world evaluation on GM-100 [29] benchmark (Galaxea R1Pro, Part I)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #001 #002 #003 #006 #007 #008 #009 #010 #011 #012 #013 #014 #015 #016 #017 #018 #019 #020 #021 #022 #023 #024 #025 #026 #027 #028 #029 #030 #031 #032 #033 #034 #035 #036 #037 #038 #040 #041 #042 #043 #044 #045 #046 #047 #048 #049 #050 #051 #053 #054 #055 13% 33% 7% 0% 0% 0% 0% 7% 33% 27% 0% 0% 47% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 13% 0% 0% 7% 0% 0% 40% 0% 0% 0% 0% 0% 0% 13% 0% 0% 0% 7% 0% 0% 0% 0% 0% 0% 0% 0% 17% 45% 49% 1% 2% 4% 20% 8% 63% 33% 2% 0% 47% 0% 4% 17% 5% 0% 0% 1% 0% 0% 5% 4% 19% 32% 12% 2% 28% 4% 15% 75% 1% 1% 0% 0% 9% 2% 38% 2% 0% 0% 27% 9% 19% 7% 0% 0% 0% 10% 2% 13% 0% 0% 0% 0% 0% 80% 0% 80% 47% 0% 0% 93% 0% 0% 7% 0% 0% 7% 0% 0% 0% 0% 0% 20% 47% 20% 27% 7% 0% 13% 27% 0% 0% 0% 0% 0% 0% 80% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 40% 0% 20% 2% 0% 12% 8% 14% 83% 2% 82% 62% 0% 3% 93% 0% 7% 20% 0% 0% 28% 16% 8% 5% 0% 7% 43% 57% 45% 55% 22% 2% 29% 53% 3% 6% 15% 0% 16% 12% 91% 0% 0% 4% 24% 15% 28% 5% 0% 0% 2% 63% 7% Ours w/o depth PS SR Ours w/ depth PS SR 20% 0% 73% 7% 0% 0% 33% 0% 100% 33% 87% 0% 53% 0% 0% 0% 0% 0% 20% 0% 0% 13% 0% 0% 7% 60% 0% 53% 0% 0% 27% 73% 0% 0% 0% 0% 7% 0% 60% 7% 0% 0% 7% 0% 7% 0% 0% 0% 0% 20% 0% 27% 10% 89% 28% 0% 43% 63% 15% 100% 35% 96% 25% 53% 8% 3% 10% 0% 0% 68% 18% 30% 32% 7% 2% 35% 73% 7% 56% 37% 15% 33% 82% 0% 11% 34% 1% 41% 5% 73% 45% 0% 1% 59% 14% 49% 2% 15% 17% 0% 37% 7% 27% 0% 33% 0% 33% 0% 27% 13% 80% 0% 80% 7% 47% 0% 0% 73% 0% 0% 0% 0% 0% 0% 0% 0% 13% 27% 0% 73% 20% 0% 33% 67% 0% 0% 0% 0% 7% 0% 60% 0% 0% 0% 33% 0% 0% 0% 0% 0% 20% 0% 0% 47% 8% 44% 6% 39% 45% 53% 43% 95% 2% 80% 35% 47% 38% 15% 87% 0% 0% 10% 28% 23% 16% 0% 0% 45% 55% 7% 78% 58% 0% 33% 71% 0% 17% 29% 0% 27% 0% 71% 48% 0% 3% 71% 6% 52% 5% 15% 10% 28% 33% 2% π0.5 PS 43% 2% 0% 0% 18% 20% 57% 15% 88% 72% 40% 7% 40% 30% 27% 28% 18% 0% 30% 33% 22% 0% 0% 0% 56% 58% 0% 98% 3% 33% 33% 64% 5% 22% 20% 11% 0% 3% 40% 25% 0% 3% 51% 12% 40% 5% 0% 3% 0% 53% 7% SR 20% 0% 0% 0% 13% 0% 13% 7% 73% 53% 40% 7% 40% 13% 0% 7% 0% 0% 20% 7% 7% 0% 0% 0% 47% 33% 0% 93% 0% 0% 7% 47% 0% 0% 7% 7% 0% 0% 27% 0% 0% 0% 13% 0% 7% 0% 0% 0% 0% 47% 0% 16 Table S6. Real-world evaluation on GM-100 [29] benchmark (Galaxea R1Pro, Part II)."
        },
        {
            "title": "Tasks",
            "content": "WALL-OSS PS SR GR00T N1.6 PS SR #056 #057 #058 #060 #061 #062 #063 #064 #065 #066 #067 #068 #069 #070 #071 #072 #073 #074 #075 #076 #077 #078 #079 #080 #081 #082 #083 #084 #085 #086 #087 #088 #089 #090 #091 #092 #093 #094 #095 #096 #097 #098 #099 #100 #102 #103 #104 #105 #106 #107 0% 0% 0% 0% 47% 0% 0% 0% 0% 0% 0% 7% 0% 7% 67% 0% 0% 0% 13% 20% 7% 80% 7% 0% 0% 0% 7% 0% 93% 0% 7% 0% 0% 0% 0% 7% 27% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 0% 53% 8% 0% 8% 15% 57% 5% 7% 0% 17% 13% 0% 10% 3% 7% 73% 9% 3% 17% 13% 50% 7% 80% 9% 0% 17% 2% 12% 23% 93% 7% 18% 4% 0% 7% 0% 20% 27% 31% 22% 0% 0% 0% 3% 11% 2% 13% 2% 3% 1% 57% 7% 0% 0% 7% 60% 0% 0% 0% 53% 0% 13% 20% 0% 73% 7% 0% 7% 0% 0% 0% 87% 80% 67% 0% 0% 0% 0% 13% 60% 0% 0% 0% 0% 0% 40% 27% 47% 0% 60% 0% 33% 0% 0% 20% 7% 0% 40% 0% 0% 7% 32% 4% 15% 47% 63% 2% 23% 0% 70% 13% 13% 30% 8% 73% 20% 7% 41% 1% 11% 50% 96% 80% 81% 0% 22% 5% 22% 40% 60% 23% 2% 3% 0% 3% 70% 63% 47% 29% 67% 3% 58% 0% 0% 38% 16% 17% 51% 3% 5% 12% Ours w/o depth PS SR Ours w/ depth PS SR 0% 0% 20% 20% 87% 0% 7% 0% 73% 0% 47% 13% 0% 73% 33% 0% 7% 20% 20% 13% 47% 87% 0% 0% 0% 47% 27% 20% 93% 13% 40% 53% 7% 0% 13% 47% 67% 0% 13% 0% 27% 0% 0% 27% 0% 33% 0% 0% 0% 47% 52% 38% 30% 58% 93% 0% 32% 0% 80% 15% 48% 23% 33% 73% 65% 2% 12% 52% 37% 57% 53% 87% 9% 23% 50% 67% 57% 50% 93% 47% 67% 88% 10% 15% 23% 73% 67% 16% 29% 0% 29% 3% 3% 33% 33% 67% 2% 28% 25% 48% 47% 0% 20% 20% 93% 0% 7% 0% 67% 0% 47% 0% 13% 87% 33% 0% 7% 0% 0% 13% 80% 80% 0% 40% 33% 0% 27% 13% 87% 40% 27% 73% 0% 20% 93% 0% 73% 0% 73% 0% 20% 7% 0% 60% 0% 33% 13% 0% 0% 0% 68% 3% 33% 68% 97% 1% 22% 0% 80% 17% 47% 10% 45% 87% 63% 0% 40% 2% 16% 47% 82% 80% 9% 57% 62% 48% 57% 60% 93% 53% 37% 93% 3% 35% 93% 32% 73% 8% 87% 0% 33% 7% 10% 60% 18% 63% 20% 7% 24% 0% π0.5 PS 22% 0% 15% 53% 57% 3% 23% 0% 53% 30% 0% 53% 40% 60% 30% 0% 12% 17% 33% 43% 38% 93% 73% 0% 22% 32% 22% 37% 71% 23% 54% 19% 1% 15% 20% 12% 27% 23% 22% 0% 27% 45% 0% 69% 29% 50% 0% 23% 2% 12% SR 0% 0% 7% 7% 47% 0% 0% 0% 53% 0% 0% 40% 13% 60% 27% 0% 0% 7% 20% 0% 33% 93% 60% 0% 0% 20% 0% 7% 67% 7% 20% 0% 0% 0% 13% 0% 27% 0% 7% 0% 27% 27% 0% 53% 0% 20% 0% 0% 0% 7% 17 Table S7. Simulation evaluation on RoboTwin 2.0 [8] benchmark, under clean and randomized settings."
        },
        {
            "title": "Simulation Tasks",
            "content": "Adjust Bottle Beat Block Hammer Blocks Ranking Rgb Blocks Ranking Size Click Alarmclock Click Bell Dump Bin Bigbin Grab Roller Handover Block Handover Mic Hanging Mug Lift Pot Move Can Pot Move Pillbottle Pad Move Playingcard Away Move Stapler Pad Open Laptop Open Microwave Pick Diverse Bottles Pick Dual Bottles Place A2b Left Place A2b Right Place Bread Basket Place Bread Skillet Place Burger Fries Place Can Basket Place Cans Plasticbox Place Container Plate Place Dual Shoes Place Empty Cup Place Fan Place Mouse Pad Place Object Basket Place Object Scale Place Object Stand Place Phone Stand Place Shoe Press Stapler Put Bottles Dustbin Put Object Cabinet Rotate Qrcode Scan Object Shake Bottle Horizontally Shake Bottle Stack Blocks Three Stack Blocks Two Stack Bowls Three Stack Bowls Two Stamp Seal Turn Switch π0.5 Ours w/o depth Ours w/ depth"
        },
        {
            "title": "Clean",
            "content": "Rand."
        },
        {
            "title": "Clean",
            "content": "Rand."
        },
        {
            "title": "Clean",
            "content": "Rand. 100% 91% 91% 73% 26% 19% 92% 99% 83% 98% 27% 99% 84% 94% 99% 49% 96% 75% 86% 95% 83% 77% 93% 89% 96% 78% 100% 99% 83% 100% 79% 78% 91% 90% 93% 93% 94% 88% 92% 86% 84% 97% 100% 100% 99% 100% 83% 95% 86% 65% 100% 92% 92% 76% 97% 43% 97% 100% 83% 94% 34% 100% 89% 92% 98% 74% 98% 91% 88% 99% 89% 80% 95% 90% 98% 75% 100% 99% 87% 100% 92% 86% 90% 90% 93% 90% 99% 86% 92% 85% 86% 92% 99% 100% 96% 100% 71% 90% 74% 67% 100% 89% 91% 70% 43% 36% 97% 100% 95% 99% 53% 100% 87% 90% 100% 48% 96% 92% 85% 90% 85% 80% 93% 92% 94% 72% 98% 100% 86% 100% 87% 79% 88% 88% 88% 87% 99% 93% 93% 88% 82% 96% 98% 99% 95% 99% 77% 97% 77% 63% 100% 96% 92% 49% 98% 99% 92% 100% 66% 98% 18% 96% 51% 84% 96% 56% 90% 34% 81% 93% 87% 87% 77% 85% 94% 62% 94% 99% 75% 100% 87% 60% 80% 86% 91% 81% 92% 87% 84% 80% 89% 72% 99% 99% 91% 97% 77% 95% 79% 62% 100% 87% 92% 66% 93% 32% 97% 100% 80% 94% 32% 100% 79% 93% 96% 74% 96% 91% 79% 82% 86% 74% 92% 90% 95% 68% 97% 99% 80% 100% 91% 82% 90% 84% 97% 92% 99% 90% 88% 92% 93% 91% 100% 99% 92% 100% 72% 92% 76% 61% 99% 93% 85% 26% 89% 66% 97% 100% 57% 97% 17% 85% 55% 61% 84% 42% 96% 77% 71% 63% 82% 84% 64% 66% 87% 62% 84% 95% 75% 99% 85% 39% 76% 80% 85% 81% 93% 83% 79% 79% 87% 65% 99% 97% 76% 100% 71% 96% 55% 54%"
        }
    ],
    "affiliations": []
}