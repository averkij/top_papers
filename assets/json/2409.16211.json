{
    "paper_title": "MaskBit: Embedding-free Image Generation via Bit Tokens",
    "authors": [
        "Mark Weber",
        "Lijun Yu",
        "Qihang Yu",
        "Xueqing Deng",
        "Xiaohui Shen",
        "Daniel Cremers",
        "Liang-Chieh Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked transformer models for class-conditional image generation have become a compelling alternative to diffusion models. Typically comprising two stages - an initial VQGAN model for transitioning between latent space and image space, and a subsequent Transformer model for image generation within latent space - these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to a modernized VQGAN. Secondly, a novel embedding-free generation network operating directly on bit tokens - a binary quantized representation of tokens with rich semantics. The first contribution furnishes a transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves a new state-of-the-art FID of 1.52 on the ImageNet 256x256 benchmark, with a compact generator model of mere 305M parameters."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 1 1 2 6 1 . 9 0 4 2 : r MaskBit: Embedding-free Image Generation via Bit Tokens Mark Weber1,2,4 Xiaohui Shen1 Lijun Yu3 Qihang Yu1 Xueqing Deng1 Daniel Cremers2,4 Liang-Chieh Chen 1 ByteDance 2 Technical University Munich 3 Carnegie Mellon University 4 MCML https://weber-mark.github.io/projects/maskbit.html Figure 1: Generated images by the proposed MaskBit. We showcase samples from MaskBit trained on ImageNet at 256 256 resolution."
        },
        {
            "title": "Abstract",
            "content": "Masked transformer models for class-conditional image generation have become compelling alternative to diffusion models. Typically comprising two stages an initial VQGAN model for transitioning between latent space and image space, and subsequent Transformer model for image generation within latent space these frameworks offer promising avenues for image synthesis. In this study, we present two primary contributions: Firstly, an empirical and systematic examination of VQGANs, leading to modernized VQGAN. Secondly, novel embedding-free generation network operating directly on bit tokens binary quantized representation of tokens with rich semantics. The first contribution furnishes transparent, reproducible, and high-performing VQGAN model, enhancing accessibility and matching the performance of current state-of-the-art methods while revealing previously undisclosed details. The second contribution demonstrates that embedding-free image generation using bit tokens achieves new state-of-the-art FID of 1.52 on the ImageNet 256 256 benchmark, with compact generator model of mere 305M parameters."
        },
        {
            "title": "1 Motivation",
            "content": "Masked transformer models for class-conditioned image [4, 56] or text-to-image [5] generation have emerged as strong alternatives to auto-regressive models [11, 53, 29] and diffusion models [22, 43]. These masked transformer methods typically employ two-stage framework: discrete tokenizer (e.g., VQGAN [11]) projects the input from image space to discrete, compact latent space, while Preprint. Under review. transformer model [50, 9] serves as the generator to create images in latent space from masked token sequence. These methods achieve performance comparable to state-of-the-art auto-regressive and diffusion models, but with the advantage of requiring significantly fewer sampling steps and smaller model sizes, resulting in substantial speed-ups. Despite the success of masked transformer frameworks, the development details of strong tokenizer have been largely overlooked. Moreover, one key aspect of training modern VQGAN-based tokenizers [4, 55, 56] the perceptual loss remains undiscussed. Since latent space-based generation relies on encoding and decoding the input image with the VQGAN tokenizer, the final image quality is influenced by both the generator network and the VQGAN. The importance of publicly available, high-performance VQGAN model cannot be overstated, yet the most widely-used model and codebase still originate from the original work [11] developed over three years ago. Although stronger, closed-source VQGAN variants exist [4, 55, 56], their details are not fully shared in the literature, creating significant performance gap for researchers without access to these advanced tokenizers. While the community has made attempts [3, 38] to reproduce these works, none have matched the performance (for both reconstruction and generation) reported in [4, 55, 56]. In this paper, we undertake systematic step-by-step study to elucidate the architectural design and training process necessary to create modernized VQGAN model, referred to as VQGAN+. We provide detailed ablation of key components in the VQGAN design, and propose several changes to them, including model and discriminator architecture, perceptual loss, and training recipe. As result, we significantly enhance the VQGAN model, reducing the reconstruction FID from 7.94 [11] to 1.66, marking an impressive improvement of 6.28. Moreover, our modernized VQGAN+ improves the generation performance and establishes strong, reproducible foundation for future generation frameworks based on VQGAN. Furthermore, we delve into embedding-free tokenization [56, 39], specifically implementing LookupFree Quantization (LFQ) [56] within our improved tokenizer framework. Our resulting method, employs binary quantization process by projecting latent embeddings into dimensions and then quantizing them based on their sign values. This process produces bit tokens, where each token is represented by bits. We empirically observe that this representation captures high-level structured information, with bit tokens in close proximity being semantically similar. This insight leads us to propose novel embedding-free generation model, MaskBit, which directly generates images using bit tokens, eliminating the need for learning new embeddings (from VQGAN token indices to new embedding values) as required in traditional VQGAN-based generators [11, 4, 56]. Consequently, MaskBit achieves state-of-the-art performance, with an FID score of 1.52 on the popular ImageNet 256 256 image generation benchmark using compact generator model of 305M parameters. All implementations will be made publicly available to support further research in this field. Our contribution can be summarized as follows: 1. We study the key ingredients of modern closed-source VQGAN tokenizers, and develop an open-source and high-performing VQGAN model, called VQGAN+, achieving significant improvement of 6.28 rFID over the original VQGAN developed three years ago. 2. Building on our improved tokenizer framework, we leverage Lookup-Free Quantization (LFQ). We observe that embedding-free bit token representation exhibits highly structured semantics, important for generation. 3. Motivated by these discoveries, we develop novel embedding-free generation framework, MaskBit, which achieves state-of-the-art performance on the ImageNet 256 256 image generation benchmark."
        },
        {
            "title": "2 Revisiting VQGAN",
            "content": "Image generation methods [11, 43] that operate in latent space rather than in the pixel space require networks to project images into this latent space and then back to the pixel space. Such networks are usually trained with reconstruction objective, as is common in VAE-inspired methods [27, 40, 11]. Over time, the training frameworks for these networks have become increasingly complex [11, 53, 4, 29, 55, 56]. Despite this complexity, the exploration and optimization of these frameworks remain significantly underrepresented in the scientific literature. In particular, issues in reproduction arise when crucial details are not transparently discussed, making comparison under fair and consistent 2 conditions hard. Moreover, fully disclosing all details enables the community to scrutinize and validate the true advancements brought by new designs. In response to this challenge, we conduct systematic study on the essential modifications to the popular baseline Taming-VQGAN [11]. For this study, we use the term VQGAN to refer to any network of this type and Taming-VQGAN specifically to refer to the network published by [11]. In the following, we provide an empirical roadmap for developing modernized VQGAN, named VQGAN+, aiming to bridge the performance gap and make advanced image generation techniques more accessible. 2.1 Preliminaries Variational Autoencoders (VAEs) [27] compress high-dimensional data into low-dimensional representations using an encoder network and reverse this process with decoder network. VAEs are typically trained with reconstruction loss, such as the L2 loss. However, minimizing the L2 distance alone is insufficient for achieving visually satisfying results. Therefore, perceptual loss term [25] based on the LPIPS metric [58] is used to reduce noise and improve image quality. Vector-Quantized VAEs (VQ-VAEs) [40] incorporate learnable codebook that acts as lookup table between the encoder and decoder. The codebook is trained with two L2 losses: the commitment loss, which reduces the distance of the encoder output to the codebook entries, and the codebook loss, which minimizes the distance from the codebook entries to the encoder output. VQGANs [11] build on top of VQ-VAEs by incorporating discriminator network to add an adversarial objective [15] to the training framework. The discriminators goal is to distinguish reconstructed images from original ones, while the VQ-VAE is trained to fool the discriminator. 2.2 The Experimental Setup We follow standard practices to train and evaluate the network on ImageNet [8]. Unless specified otherwise, all networks are trained with batch size of 256 for 300,000 iterations. Evaluation is performed using the reconstruction Fréchet Inception Distance (rFID) [18], where lower values indicate better performance. We use Taming-VQGAN [11] as our baseline and starting point, due to its thorough study and open-source availability, ensuring reproducibility of results. Taming-VQGAN is an encoder-decoder network that uses residual blocks [17] with convolutions for low-level feature processing and interleaved blocks of convolutions and self-attention [50] for high-level feature processing. In our experiments, we focus on encoder networks that produce latent embeddings with an output stride of 16, resulting in 16 16 latent embeddings for input images of 256 256 pixels. The learned vocabulary consists of 1,024 entries, each being 256-dimensional vector. 2.3 VQGAN+: Modern VQGAN Starting with the baseline Taming-VQGAN [11] that attains 7.94 rFID on ImageNet-1K 256256 [8], we provide step-by-step walk-through and ablation of all changes in the following paragraphs. summary of the changes and their effects is given in Fig. 2. Basic Training Recipe and Architecture. The initial modifications to the Taming-VQGAN baseline are as follows: (1) removing attention blocks for purely convolutional design, (2) adding symmetry to the generator and discriminator, and (3) updating the learning rate scheduler. Removing the attention layers, as adopted in recent methods [4, 55, 56], reduces computational complexity without sacrificing performance. We also observed discrepancies between the open-source implementation of Taming-VQGAN [11] and its description in the publication. The implementation uses two residual blocks per stage in the encoder but three per stage in the decoder. For our experiments, we adopt symmetric architecture with two residual blocks per stage in both the encoder and decoder, as described in the publication. Additionally, we align the number of base channels in the generator and discriminator. Specifically, we reduce the channel dimension of the generator from 128 to 64 to match the discriminator. Here, base channels refer to the feature dimension in the first stage of the network. The third modification involves the learning rate scheduler. We replace constant learning rate with cosine-learning rate scheduler with warmup [36]. Specifically, we use warmup phase of 5,000 iterations to reach the initial learning rate of 1e 4 and then gradually decay it to 10% of the initial learning rate over time. Result: 7.15 rFID (an improvement of 0.79 rFID) Figure 2: Detailed roadmap to build modern VQGAN+. This overview summarizes the performance gains achieved by each proposed change to the architecture and training recipe. The reconstruction FID (rFID) is computed against the validation split of ImageNet at resolution of 256. The popular and open-source Taming-VQGAN [11] serves as the baseline and starting point. Increasing Model Capacity. Next, we increase the number of base channels from 64 to 128 for both generator and discriminator. This adjustment aligns the generators capacity with that of the Taming-VQGAN baseline, except for the absence of attention blocks. As result, the generator uses 17 million fewer parameters overall compared to the Taming-VQGAN baseline, while the number of parameters in the discriminator increases by only 8.3 million. Result: 4.02 rFID (an improvement of 3.13 rFID) Perceptual Loss. The perceptual loss [25, 58] plays crucial role in further reducing the rFID score. In Taming-VQGAN, the LPIPS [58] score obtained by the LPIPS VGG [46] network is minimized to improve image decoding. However, we noticed in the partially open-source code of recent work on image generation [55]1, the usage of ResNet50 [17] network instead to compute the perceptual loss. Although confirmed by the authors of [55, 56], this change has so far not been discussed in the literature. We believe this change was already introduced in earlier publications of the same group [4], but never transparently documented. Following the open-source code of [55], we apply an L2 loss on the logits of pretrained ResNet50 using the original and reconstructed images. While incorporating intermediate features into this loss can improve reconstruction further, it negatively affects generation performance in our experiments. Therefore, we compute the loss solely on the logits. Result: 2.40 rFID (an improvement of 1.62 rFID) Discriminator Update. The original PatchGAN discriminator [11] employs 4 4 convolutions and batch normalization [24], resulting in an output resolution of 30 30 from 256 256 input and utilizing 11 million parameters (with 128 base channels). We propose to replace the 4 4 convolution kernels with 3 3 kernels and switch to group normalization [52], maintaining the same number of convolutions while producing 32 32 output resolution. We then apply 2 2 max pooling to align the output stride between the generator and discriminator. These adjustments reduce the discriminators parameter count to 1.9 million, absorbing the earlier increase in parameters completely. In the second update, following prior work [56], we replace average pooling for downsampling with precomputed 4 4 Gaussian blur kernel using stride of 2. Additionally, we incorporate LeCAM loss [49] to stabilize adversarial training. Result: 2.00 rFID (an improvement of 0.4 rFID) The Final Changes. Our last addition to the training framework is the use of Exponential Moving Average (EMA) [42]. We found that EMA significantly stabilizes the training and improves convergence, while also providing small performance boost. Orthogonal to the reconstruction 1https://github.com/google-research/magvit/ 4 Figure 3: Bit tokens exhibit structured semantic representations. We visualize robustness test involving bit flipping. Specifically, we utilize our method to encode images into bit tokens, where each token is represented by K-bits (K = 12 in this example). We then flip the i-th bit for all the bit tokens and reconstruct the images as usual. Interestingly, the reconstructed images from these bit-flipped tokens remain semantically consistent to the original image, exhibiting only minor visual modifications such as changes in texture, exposure, smoothness, color palette, or painterly quality. performance, we also add an entropy loss [56] to ease the learning in the generation phase. To achieve the final performance of VQGAN+, we increase the number of training iterations from 300,000 to 1.35 million iterations, following common practices [56, 13]. Result: 1.66 rFID (an improvement of 0.34 rFID) An Embedding-Free Variant. Recent works [56, 39] have shown that removing the lookup table from the quantization [40] improves scalability. Following the Lookup-Free Quantization (LFQ) approach [56], we implement binary quantization process by projecting the latent embeddings to dimensions (K = 12 in this experiment) and then quantizing them based on their sign values. Due to this binary quantization, we can directly interpret the obtained quantized representations as their token numbers. We use the standard conversion between base-2 and base-10 integers (e.g., 10012 = 910). In VQGAN+, the token is merely the index number of the embedding table; however, in this case, the token itself contains its corresponding representation, eliminating the need for an additional embedding. Implicitly, this defines non-learnable codebook of size 2K. Given the compact nature of binary quantization with only feature dimensions, we do not anticipate significant gains in reconstruction. However, we will empirically demonstrate the clear advantages of this representation in the generation stage. Result: 1.61 rFID (an improvement of 0.05 rFID)"
        },
        {
            "title": "3 MaskBit: A New Embedding-free Image Generation Model",
            "content": "Since the typical pipeline of VQGAN-based image generation models [4, 56] includes two stages with Stage-I already discussed in the previous section, we focus on the Stage-II in this section. Stage-II refers to transformer network that learns to generate images in the VQGANs latent space. During Stage-II training, the tokenizer produces latent representations of images, which are then masked according to predefined schedule [4]. The masked tokens are fed into the Stage-II transformer network, which is trained with categorical reconstruction objective for the masked tokens. This process resembles masked language modeling, making language models such as BERT [9] suitable for this stage. These transformer models relearn new embedding for each token, as the input tokens themselves are just arbitrary index numbers. Bit Tokens Are Semantically Structured. Following our study of embedding-free tokenization, which eliminates the lookup table from Stage-I (i.e., removing the need for embedding tables in VQ-VAE [40]) by directly quantizing the tokens into K-bits, we study the achieved representation more carefully. As shown in [56], this representation yields high-fidelity image reconstruction results, 5 Figure 4: High-level overview of the architecture and comparison. Our training framework comprises two stages for image generation. In Stage-I, an encoder-decoder network compresses images into latent representation and decodes them back. Stage-II masks the tokens, feeds them into transformer and predicts the masked tokens. Most prior art uses VQGAN-based methods (top) that learn independent embedding tables in both stages. In VQGAN-based methods, only indices of embedding tables are shared across stages, but not the embeddings. In MaskBit, however, neither Stage-I nor Stage-II utilizes embedding tables. The Stage-I predicts bit tokens by using binary quantization on the encoder output directly. The Stage-II partitions the shared bit tokens into groups (e.g., 2 groups), masks and feeds them into transformer, and predicts the masked bit tokens. indicating that images can be well represented and reconstructed using bit tokens. However, prior arts [56] have used different token spaces for Stage-I and Stage-II, intuitively allowing the Stage-II token to focus more on semantics, while enabling the Stage-I token space to capture all visual details necessary for faithful reconstruction. This raises the question: Are bit tokens also good representation for generation, and can bit tokens eliminate the need for re-learning new embeddings in the typical Stage-II network? To address this, we first analyze the bit tokens learned representations for image reconstruction. Specifically, given latent representation with 256 tokens from our model (where each token is represented by K-bits, and each bit by either 1 or 1), we conduct robustness test involving bit flipping. This test involves flipping the i-th bit (i.e., swapping 1 and 1) for all 256 bit tokens and decoding them as usual to produce images. The bit-flipped tokens each have Hamming distance of 1 from the original bit tokens. Surprisingly, we find that the reconstructed images from these bit-flipped tokens are still visually and semantically similar to the original images. This indicates that the representation of bit tokens has learned structured semantics, where neighboring tokens (within Hamming distance of 1) are semantically similar to each other. We visualize an example of this in Fig. 3 and provide more examples in Sec. D. We note that this behaviour is not present in VQGAN methods. Conducting the same experiment with VQGAN+ leads to non-meaningful output, sharing no semantic or visual similarities with the original image. Since the generation model is usually semantically conditioned on external input such as classes, the model needs to learn the semantic relationships of tokens. Given this evidence of the inherent structured semantic representation in bit tokens, we believe that the learned rich semantic structure in the bit token representation can thus be good representation for the Stage-II generation model. Proposing MaskBit. This motivates us to propose MaskBit for Stage-II, transformer-based generative model that generates images directly using bit tokens, removing the need for the embedding lookup table as required by existing methods [4, 56]. Our training framework is illustrated in Fig. 4. 6 Prior art using VQGAN approaches share only indices of the embedding tables between the two stages, but learn independent embeddings. Taking advantage of the built-in semantic structure of bit tokens, the proposed MaskBit can share the tokens directly between Stage-I and Stage-II. We discuss more detailed differences between the two approaches in Sec. C. In the next paragraph, we provide details on bit token masking. Masked Bits Modeling. The Stage-II training follows the masked modeling framework [9], where certain number of tokens are masked (i.e., replaced with special mask token) before being fed into the transformer, which is trained to recover the masked tokens. This approach requires an additional entry in the embedding table to learn the embedding vector for the special mask token. However, this presents challenge for an embedding-free setup, where images are generated directly using bit tokens without embedding lookup. Specifically, it raises the question of how to represent the masked bit tokens in the new framework. To address this challenge, we propose straightforward yet effective solution: using zeros to represent the masked bit tokens. In particular, bit token is represented as {1, 1}K (i.e., K-bits, with each bit being either 1 or 1), while we set all masked bits to zero. Consequently, these masked bit tokens do not contribute to the image representation. Masked Groups of Bits Modeling. During training, the network is supervised with categorical cross-entropy loss applied to all masked tokens. With increasing number of bits, the categorical cross-entropy gets computed over an exponentially increasing distribution size. Given that bit tokens capture channel-wise binary quantization, we explore masking groups of bits. Specifically, for each bit token {1, 1}K, we split it into groups tn {1, 1}K/N , {1, , }, with each group contains K/N consecutive bits. During the masking process, each group of bits can be independently masked. Consequently, bit token may be partially masked, allowing the model to leverage unmasked groups to predict the masked bits, easing the training process. During the inference phase, the sampling procedure allows to sample some groups and use their values to guide the remaining samplings. However, this approach increases the number of bit token groups to be sampled, posing challenge during inference due to the potential for poorly chosen samples. Empirically, we found that using two groups yields the best performance, striking good balance."
        },
        {
            "title": "4 Experimental Results for Image Generation",
            "content": "Experimental Setup. We evaluate the proposed MaskBit on class-conditional image generation. Specifically, the network generates total of 50,000 samples for the 1,000 ImageNet [8] classes. All Stage-II generative networks are trained and evaluated on ImageNet at resolutions of 256 256. We report the main metric Fréchet Inception Distance for generation (gFID) [18], along with the side metric Inception Score (IS) [44]. All scores are obtained using the official evaluation suite and reference statistics by ADM [10]. We train all Stage-II networks for 1,080 epochs and report results with classifier-free guidance [21]. More training and inference details can be found in Sec. B. 4.1 Main Results Tab. 1 summarizes the generation results on ImageNet 256256. We make the following observations. VQGAN+ as New Strong Baseline for Image Tokenizers. By carefully revisiting the VQGAN design in Sec. 2, we develop VQGAN+, solid new baseline for image tokenizers that achieves an rFID of 1.66, marking significant improvement of 6.28 rFID over the original Taming-VQGAN model [11]. Using VQGAN+ in the generative framework of MaskGIT [4] results in gFID of 2.12, outperforming several recent advanced VQGAN-based methods, such as ViT-VQGAN [53] (by 0.92 gFID), RQ-Transformer [29] (by 1.68 gFID), and even the original MaskGIT [4] (by 1.90 gFID). This result shows that the improvements we propose to the Stage-I model, indeed transfer to the generation model. Furthermore, our result surpasses several recent diffusion-based generative models that utilize the heavily optimized VAE [27] by Stable Diffusion [1], including LDM [43], U-ViT [2], and DiT [41]. We believe these results with VQGAN+ establish new solid baseline for the community. Bit Tokens as Strong Representation for Both Reconstruction and Generation. As discussed in previous sections, bit tokens encapsulate structured semantic representation that retains essential information for image generation, forming the foundation of MaskBit. Consequently, MaskBit Table 1: ImageNet 256 256 generation results. : Tokenizers trained on OpenImages [28]. : Tokenizers trained on OpenImages, LAION-Aesthetics/-Humans [45]. : Concurrent methods. All results use classifier-free guidance or rejection sampling, but not both. #Params: Generators parameters. #Steps: Sampling steps."
        },
        {
            "title": "Model",
            "content": "LDM-4-G [43] U-ViT [2] DiT-XL/2-G [41] MDT [13] DiMR [35] MDTv2 [14] MAR-H [33] Taming-VQGAN [11] MaskGIT [4] RQ-Transformer [29] ViT-VQGAN [53] LlamaGen-3B [47] TiTok-S-128 [57] VAR [48] MAGVIT-v2 [56] MaskGIT (w/ our VQGAN+) MaskBit (ours) MaskBit (ours) MaskBit (ours)"
        },
        {
            "title": "Generator",
            "content": "#tokens codebook gFID IS #Params #Steps - - - - - - - continuous latent representation 40963 3.60 10244 2.29 10244 2.27 10244 1.79 10244 1.70 10244 1.58 25616 1.55 discrete latent representation 5.88 4.02 3.80 3.04 2.18 1.97 1.92 1.78 2.12 1.65 1.62 1.52 1024 1024 16384 8192 16384 4096 4096 262144 4096 4096 16384 16384 256 256 256 1024 576 128 680 256 256 256 256 256 247.7 263.9 278.2 283.0 289.0 314.7 303.7 304.8 355.6 323.7 227.4 263.3 281.8 350.2 319.4 300.8 341.8 338.7 328. 400M 501M 675M 676M 505M 676M 943M 1.4B 177M 3.8B 1.7B 3.1B 287M 2.0B 307M 310M 305M 305M 305M 250 50 250 250 250 250 256 256 8 64 1024 576 64 10 64 64 64 64 256 Table 2: Comparison using similar generator model capacity ( 300M) on ImageNet 256 256. : Tokenizers trained on OpenImages [28]. : Concurrent methods. All results use classifier-free guidance. #Steps: Sampling steps. Model VAR [48] TiTok-S-128 [57] MaskBit (ours) LlamaGen-L [47] MaskBit (ours) MAGVIT-v2 [56] MaskBit (ours) #Params Tokenizer #tokens codebook gFID IS #Params #Steps Generator 109M 72M 54M 72M 54M 116M 54M 680 128 256 256 256 256 256 4096 4096 4096 16384 16384 262144 262144 3.30 1.97 1.65 3.80 1.52 1.78 1. 274.4 281.8 341.8 263.3 328.6 319.4 331.6 310M 287M 305M 343M 305M 307M 305M 10 64 64 256 256 64 64 delivers superior performance, achieving gFID of 1.65 with 12 bits and 1.62 with 14 bits, surpassing prior works. Notably, MaskBit outperforms the recent state-of-the-art MAGVIT-v2 [56] while utilizing an implicit codebook that is 16 to 64 smaller. When the number of sampling steps is increased from 64 to 256matching the steps used by autoregressive modelsMaskBit sets new state-of-the-art with gFID of 1.52, outperforming concurrent works [14, 33, 35, 47, 48, 57], while using compact generator model of just 305M parameters. Remarkably, MaskBit, which operates with discrete tokens, surpasses methods relying on continuous tokens, such as MDTv2 [14] and MAR-H [33]. Comparison under Similar Generator Parameters. In Tab. 2, we present performance comparison across models of similar sizes. Previous works have demonstrated that increasing model size generally leads to superior performance [33, 47, 48]. Therefore, to provide more comprehensive Table 3: Ablation studies on ImageNet 256 256 generation. Our final setting is labeled in gray. To save computations, shorter training schedule has been used for this study. (a) Embedding-free Schemes (b) Number of Groups Tokenizer embedding-free Generator embedding-free gFID 2.12 1.95 1.82 2."
        },
        {
            "title": "Groups",
            "content": "gFID 1 2 4 1.94 1.82 2.01 comparison, we only compare models using similar number of parameters, but vary codebook size and sampling steps. As shown in the table, MaskBit not only achieves state-of-the-art results overall but also outperforms all other models under similar conditions. It is worth noting that all models in the comparison use larger Stage-I (reconstruction) tokenizer than ours. Qualitative Results. We present visualizations of the generated 256256 images using the proposed MaskBit in Fig. 5. The results show the diversity of classes represented in ImageNet. MaskBit is able to generate high-fidelity outputs. We note that these generated results inherit the dataset bias of ImageNet. As classification dataset, the images are centered around objects showing the respective class. Thus, this dataset bias influences the kind of images that can be generated. Methods used in production environments such as Stable Diffusion [1] therefore train on more general large-scale datasets such as LAION-5B [45] to remove such dataset bias. 4.2 Ablation Study Unless stated otherwise, we use fewer training iterations for the ablation studies due to the limited resources and computational costs. Embedding-free Stages Bring Improvements. In Tab. 3a, we investigate embedding-free designs in both Stage-I (tokenizer) and Stage-II (generator). To ensure fair comparison, all models use the same number of tokens, codebook size, and model capacity. In the first row, we employ VQGAN+ with traditional Stage-II model, as in MaskGIT [4], achieving gFID of 2.12. Using bit tokens in Stage-I improves performance to gFID of 1.95, highlighting the effectiveness of structured semantic representation of bit tokens. Further enabling our Stage-II model MaskBit (i.e., both stages now are embedding-free) achieves the best gFID of 1.82, demonstrating the advantages of embedding-free image generation from bit tokens. For completeness, we also explore VQGAN+ with an embeddingfree Stage-II model, yielding gFID of 2.83. This variant performs worse due to the lack of the semantic structure in the tokens. Two Groups of Bits Lead to Better Performance. Tab. 3b presents the performance of MaskBit using different numbers of groups. As shown in the table, when using two groups, each consisting of six consecutive bits, MaskBit achieves the best performance. This configuration outperforms single group because it allows the model to leverage information from the unmasked groups. However, when using four groups, the increased difficulty in sampling during inference due to the need for four times more sampling detracts from performance. Still, all group configurations outperform our strong baseline using VQGAN+ as well as modern approaches like ViT-VQGAN [53] and DiT-XL [41], showing the potential of grouping. We also conduct another experiment, where we apply 2 groups to VQGAN+. The performance drops form 2.12 to 4.4 gFID, due to the missing semantic structure in the tokens. 14 Bits are Enough for ImageNet. In Tab. 4a, we train the Stage-II models with our final training schedule. In each setting, MaskBit leverages different number of bits. Empirically, we find that using 14 bits works the best on ImageNet. We note that all settings outperform prior methods (e.g., [41, 56]) in gFID and the performance differences are marginal across different number of bits. While using 14 bits is sufficient for ImageNet, more general and large-scale datasets such as LAION-5B [45] might benefit from using larger values of (i.e., larger implicit vocabulary sizes). 9 Figure 5: Visualization of generated 256256 images. MaskBit demonstrates the ability to produce high-fidelity images across diverse range of classes. Table 4: Ablation on the effect of different number of bits and different number of sampling steps on ImageNet 256 256. Table (a) uses 64 sampling steps, and Table (b) uses 14 bit model. (a) Number of Bits (b) Number of Sampling Steps Bits #tokens codebook gFID IS Sampling Steps gFID IS 10 12 14 16 256 256 256 256 256 1024 4096 16384 65536 262144 1.68 1.65 1.62 1.64 1.67 353.1 341.8 330.7 339.6 331.6 64 128 256 1.62 1.56 1. 330.7 341.9 328.6 10 MaskBit Scales with More Sampling Steps. MaskBit follows the non-autoregressive sampling paradigm [4, 55], enabling flexibility in the number of sampling steps during inference (up to 256 steps in our ImageNet 256256 experiments). Unlike autoregressive models [11, 47], this approach allows for fewer forward passes through the Stage-II generative model, reducing computational cost and inference time. However, increasing MaskBits sampling steps to match those of autoregressive models can also improve performance. As shown in Tab. 4b, even with 64 steps, MaskBit surpasses prior work and achieves an excellent balance between compute efficiency and performance. Scaling further to 256 steps sets new state-of-the-art gFID of 1.52."
        },
        {
            "title": "5 Related Work",
            "content": "Image Tokenization. Images represented by raw pixels are highly redundant and commonly compressed to latent space with autoencoders [20, 51], since the early days of deep learning. Nowadays, image tokenization still plays crucial role in mapping pixels into discrete [40] or continuous [27, 19] representation suitable for generative modeling. We focus on the stream of vector quantization (VQ) tokenizers, which proves successful in state-of-the-art transformer generation frameworks [11, 4, 54, 56, 31, 32, 29, 30]. Starting from the seminal work VQ-VAE [40], several key advancements have been proposed, such as the perceptual loss [58] and discriminator loss [26] from [11] or better quantization methods [60, 29, 59, 39, 56], which significantly improve the reconstruction quality. Nonetheless, modern transformer generation models [4, 55, 56] utilize an even stronger VQ model, with details not fully discussed in the literature, resulting in performance gap to the wider research community. In this work, we conducted systematic study to modernize widely-used VQGAN model, aiming at demystifying the process to obtain strong tokenizer. Image Generation. Although various types of image generation models exist, mainstream state-ofthe-art methods are usually either diffusion-based [10, 7, 43, 41, 34, 23], auto-regressive-based [6, 11, 53, 12], or masked-transformer-based [4, 30, 56, 31, 32]. Among them, the masked-transformer-based methods exhibit competitive performance, while bringing substantial speed-up, thanks to much fewer sampling steps. This research line begins with MaskGIT [4], which generates images from masked sequence in non-autoregressive manner where at each step multiple token can be predicted [9]. Follow-up work has successfully extend the idea to the video generation domain [55, 16], largescale text-to-image generation [5], along with competitive performance to diffusion models [56] for image generation. Nonetheless, these works only utilize the tokens from the tokenizer to learn new embeddings per token in the transformer network, while we aim at an embedding-free generator framework, making better use of the learned semantic structure from the tokenizer."
        },
        {
            "title": "6 Conclusion",
            "content": "We conducted systematic experiments and provided thorough, step-by-step study towards modernized VQGAN+. Together with bit tokens, we provide two strong tokenizers with fully reproducible training recipes. Furthermore, we propose an embedding-free generation model MaskBit, which makes full use of the rich semantic information from bit tokens. Our method demonstrates state-ofthe-art results on the challenging 256 256 class-conditional ImageNet benchmark."
        },
        {
            "title": "References",
            "content": "[1] Stability AI. Stable diffusion - vae, 2022. URL https://huggingface.co/stabilityai/ sd-vae-ft-ema. [2] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. [3] Victor Besnier and Mickael Chen. pytorch reproduction of masked generative image transformer. arXiv preprint arXiv:2310.14400, 2023. [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, 2022. [5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. In ICML, 2023. 11 [6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. [7] Ting Chen, Ruixiang Zhang, and Geoffrey Hinton. Analog bits: Generating discrete data using diffusion models with self-conditioning. In ICLR, 2023. [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In CVPR, 2009. [9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2018. [10] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021. [11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021. [12] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022. [13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In ICCV, 2023. [14] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Mdtv2: Masked diffusion transformer is strong image synthesizer. arXiv preprint arXiv:2303.14389v2, 2024. [15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 2014. [16] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. [18] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. NeurIPS, 2017. [19] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. In ICLR, 2017. [20] Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786), 2006. [21] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. [23] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. In ICML, 2023. [24] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. [25] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. [26] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, 2019. [27] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. [28] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020. 12 [29] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In CVPR, 2022. [30] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and WOOK SHIN HAN. Draft-and-revise: Effective image generation with contextual rq-transformer. NeurIPS, 2022. [31] José Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with token-critic. In ECCV, 2022. [32] José Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa. Predictorcorrector sampling for discrete diffusion models. In ICLR, 2023. [33] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [34] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, 2023. [35] Qihao Liu, Zhanpeng Zeng, Ju He, Qihang Yu, Xiaohui Shen, and Liang-Chieh Chen. Alleviating distortion in image generation via multi-resolution diffusion models. arXiv preprint arXiv:2406.09416, 2024. [36] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2016. [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. [38] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Openmagvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [39] Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024. [40] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. NeurIPS, 2017. [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. [42] Boris Polyak and Anatoli Juditsky. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4), 1992. [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [44] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 2016. [45] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion5b: An open large-scale dataset for training next generation image-text models. NeurIPS, 2022. [46] Simonyan and Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. [47] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [48] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [49] Hung-Yu Tseng, Lu Jiang, Ce Liu, Ming-Hsuan Yang, and Weilong Yang. Regularizing generative adversarial networks under limited data. In CVPR, 2021. [50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. [51] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In ICML, 2008. [52] Yuxin Wu and Kaiming He. Group normalization. In ECCV, 2018. [53] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. In ICLR, 2022. [54] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3), 2022. [55] Lijun Yu, Yong Cheng, Kihyuk Sohn, José Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, 2023. [56] Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusiontokenizer is key to visual generation. In ICLR, 2024. [57] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. arXiv preprint arxiv: 2406.07550, 2024. [58] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [59] Chuanxia Zheng and Andrea Vedaldi. Online clustered codebook. In ICCV, 2023. [60] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. NeurIPS, 2022."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide additional information as listed below: Sec. provides the dataset information and licenses. Sec. provides all implementation details. Sec. provides more discussion on bit tokens. Sec. provides more reconstructed images for the bit flipping experiments. Sec. discusses the methods limitations. Sec. discusses the broader impacts."
        },
        {
            "title": "A Datasets",
            "content": "ImageNet: used to benchmark image classification, class-conditional image generation, and more. ImageNet [8] is one of the most popular benchmarks in computer vision. It has been License: Custom License, non-commercial. https://image-net.org/accessagreement Dataset website: https://image-net.org/"
        },
        {
            "title": "B Implementation Details",
            "content": "In the following, we provide implementation details for the purpose of reproducibility: B.1 Stage-I For data augmentation, we closely follow the prior arts [55, 56]. Specifically, we use random cropping of 80%-100% of the original image with random aspect ratio of 0.75-1.33. After that, we resize the input to 256 256 and apply horizontal flipping. Base channels: 128 Base channel multiplier per stage: [1,1,2,2,4] Residual blocks per stage: Adversarial loss enabled at iteration: 20000 Discriminator loss weight: 0.02 Discriminator loss: hinge loss Perceptual loss weight: 0.7 LeCam regularization weight: 0.001 L2 reconstruction weight: 4. Commitment loss weight: 0.25 Entropy loss weight: 0.02 Entropy loss temperature: 0.01 Gradient clipping by norm: 1.0 Optimizer: AdamW [37] Beta1: 0. Beta2: 0.999 Weight decay: 1e-4 LR scheduler: cosine Base LR: 1e-4 LR warmup iterations: 5000 Training iterations: 1350000 Total Batchsize: 256 GPU: A100 B.2 Stage-II For data augmentation, we closely follow the prior arts [55, 56]. Specifically, we use random cropping of 80%-100% of the original image. After that, we resize the input to 256 256 and apply horizontal flipping. Hidden dimension: 1024 Depth: 24 Attention heads: 16 MLP dimension: 4096 Dropout: 0.1 Mask schedule: arccos Class label dropout: 0.1 Label smoothing: 0.1 Gradient clipping by norm: 1.0 Optimizer: AdamW [37] Beta1: 0.9 Beta2: 0.96 Weight decay: 0.045 LR scheduler: cosine Base LR: 1e-4 LR warmup iterations: 5000 Training iterations: 1350000 Total Batchsize: 1024 GPU: A100 We use 32 A100 GPUs for training Stage-I models. Training time varies with respect to the number of training iterations between 1 (300K iterations) and 4.5 (1.35M iterations) days. We note that the Stage-I model can also be trained with fewer GPUs without any issues, but this will accordingly increase the training duration. Stage-II models are trained with 64 A100 GPUs and take 4.2 days for the longest schedule (1.35M iterations). B.3 Sampling Procedure Our sampling procedure closely follows prior works for non-autoregressive image generation [13, 4, 56, 55, 5]. By default, the Stage-II network generates images in 64 steps. Each step, number of tokens is unmasked and kept. The number of tokens to keep is determined by an arccos scheduler [3, 4]. Additionally, we use classifier-free guidance and guidance scheduler [13, 55]. Specifically, for models trained on 256 resolution, we use temperature of 7.3, guidance of 7.6 and scale_powof 3.5. B.4 Sensitivity to Sampling Parameters In this subsection, we detail the specific values for the sampling hyper-parameters to ensure reproducibility. However, the reported FID scores can be obtained by large range of hyper-parameters. Due to the inherent randomness in the sampling procedure, results might vary by 0.02 FID. For the resolution of 256 256, we verified several choices of parameters obtaining similar results: temperature [7.3, 8.3], guidance [5.8, 7.8] and scale_pow [2.5, 3.5]."
        },
        {
            "title": "C More Discussion of Bit Tokens",
            "content": "In prior work, only the token indices were shared between Stage-I and Stage-II models. In all cases, the Stage-II model relearns an embedding table. Considering our setting of 12 bits, MaskGIT Stage-II model would learn new embedding table with 212 = 4096 entries. Each of these entries is an (arbitrary) vector in R1024 with 1024 being the hidden dimension. We note that this approach is equivalent to replacing the embedding table with linear layer processing one-hot encoded token indices. The linear layer would in this case do the same mapping as the embedding table and have the same number of parameters, i.e., 4096 1024, omitting the bias for simplicity. In MaskBit, instead of token indices, the bit tokens are shared and processed directly. MaskBit also uses linear layer to map to the models internal representation, but this mapping is from 12 dimensions (when = 12) to 1024, which corresponds to learning just 12 embedding vectors. Thus, the learnable mapping is reduced by factor of 341. This is possible as MaskBit does not use one-hot encoded representation, but bit token representation with 12 bits. The linear layer therefore adds/subtracts the learnable weights depending on the bit coefficients 1. It follows that the internal representation per token in MaskBit is not arbitrary in R1024, but significantly constrained. These constraints require semantically meaningful structure in the bit tokens, while in MaskGIT the representation for each token index is independent and arbitrarily learnable. Example: Given two bit tokens t1 and t2 with only difference in the last bit, t1 = +1 ... 1 +1 +1 1 t2 = +1 ... 1 +1 +1 +1 In MaskBit, the only difference when processing the tokens in the first linear layer is that the last weight vector, corresponding to the last bit in the tokens, is either added (for t2) or subtracted (for t1). Thus, the obtained results are not independent of each other and not arbitrary, which is different from prior work."
        },
        {
            "title": "D More Reconstruction Results of Flipping Bit Tokens",
            "content": "We present additional reconstruction results from the bit flipping analysis. As shown in Fig. 6, the reconstructed images from these bit-flipped tokens remain visually and semantically similar to the original images, consistent with the findings in the main paper."
        },
        {
            "title": "E Limitations",
            "content": "In this study, the focus has been on non-autoregressive, token-based transformer models for generation. Due to the computational costs, we did not explore using the modernized VQGAN model (VQGAN+) in conjunction with diffusion models. For similar reasons, MaskBit was only trained on ImageNet, thus inheriting the dataset bias, i.e., focus on single foreground object. This makes it difficult to do have qualitative comparison to methods trained on large-scale datasets such as LAION [45]. We also note, that the standard benchmark metric FID has received criticism due to it measuring only distribution similarities. However, due to the lack of better and established alternatives, this study uses FID for its quantitative evaluation and comparison."
        },
        {
            "title": "F Broader Impacts",
            "content": "Generative image models have diverse applications with significant social impacts. Positively, they enhance creativity in digital art, entertainment, and design, enabling easier and potentially new forms of expression, art and innovation. However, these models pose risks, including the potential for misuse such as deepfakes, which can spread misinformation and facilitate harassment. 17 Figure 6: More examples demonstrating the structured semantic representation in bit tokens. Similar to the observation in the main paper, the reconstructed images from these bit-flipped tokens remain semantically similar to the original image, exhibiting only minor visual modifications such as changes in texture, exposure, smoothness, color palette, or painterly quality. 18 Additionally, generative models can suffer from social and cultural biases present in their training datasets, reinforcing negative stereotypes. We notice that these risks have become dilemma for open-source and reproducible research. The scope of this work however is purely limited to classconditional image generation. In class-conditional generation, the model will only be able to generate images for predefined, public and controlled set of classes. In this case, the images and class set are coming from ImageNet. As result, we believe that this specific work does not pose the aforementioned risks and is therefore safe to share."
        }
    ],
    "affiliations": [
        "ByteDance",
        "Carnegie Mellon University",
        "MCML",
        "Technical University Munich"
    ]
}