{
    "paper_title": "Benchmark Test-Time Scaling of General LLM Agents",
    "authors": [
        "Xiaochuan Li",
        "Ryan Ming",
        "Pranav Setlur",
        "Abhijay Paladugu",
        "Andy Tang",
        "Hao Kang",
        "Shuai Shao",
        "Rong Jin",
        "Chenyan Xiong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating general-purpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within a unified environment. We introduce General AgentBench, a benchmark that provides such a unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals a substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/General-AgentBench."
        },
        {
            "title": "Start",
            "content": "Benchmark Test-Time Scaling of General LLM Agents Xiaochuan Li 1 Ryan Ming 1 Pranav Setlur 1 Abhijay Paladugu 1 Andy Tang 1 Hao Kang 1 Shuai Shao 2 Rong Jin 2 Chenyan Xiong 1 6 2 0 2 2 2 ] . [ 1 8 9 9 8 1 . 2 0 6 2 : r Abstract LLM agents are increasingly expected to function as general-purpose systems capable of resolving open-ended user requests. While existing benchmarks focus on domain-aware environments for developing specialized agents, evaluating generalpurpose agents requires more realistic settings that challenge them to operate across multiple skills and tools within unified environment. We introduce General AgentBench, benchmark that provides such unified framework for evaluating general LLM agents across search, coding, reasoning, and tool-use domains. Using General AgentBench, we systematically study test-time scaling behaviors under sequential scaling (iterative interaction) and parallel scaling (sampling multiple trajectories). Evaluation of ten leading LLM agents reveals substantial performance degradation when moving from domain-specific evaluations to this general-agent setting. Moreover, we find that neither scaling methodology yields effective performance improvements in practice, due to two fundamental limitations: context ceiling in sequential scaling and verification gap in parallel scaling. Code is publicly available at https://github.com/cxcscmu/Gen eral-AgentBench. 1. Introduction Agents powered by large language models (LLMs) are at turning point, transitioning from domain-specific (Liu et al., 2023b; Yang et al., 2024a; Yue et al., 2024) to generalpurpose (Xi et al., 2023; Luo et al., 2025). Real-world user requests are often open-ended and require LLM agents to operate end-to-end through planning (Wang et al., 2023; Erdogan et al., 2025), reasoning (Wei et al., 2022; Yao et al., 1Language Technologies Institute, School of Computer Science, Carnegie Mellon University 2Meta. All experiments, data collection, and processing activities were conducted by Carnegie Mellon University. Meta was involved solely in an advisory role.. Correspondence to: Xiaochuan Li <xiaochu4@andrew.cmu.edu>. Preprint. February 24, 2026. 1 2022b; Parmar et al., 2025), and tool use (Schick et al., 2023; Patil et al., 2024) under uncertain and evolving conditions. capable general agent is expected to compose multiple skills and tools (e.g., search, coding, computation, and MCP APIs) to handle the diversity of realistic requests(Anthropic, 2026), while exhibiting effective test-time scaling abilities to address increasing task complexity and enhance response quality (Wang et al., 2022; Brown et al., 2024; Snell et al., 2024). This shift raises an important evaluation gap: beyond asking can the model solve task, we must assess whether agents can infer user intent, select specific tools, and scale up their performance under unified evaluation framework across diverse domains. Existing agentic benchmarks typically evaluate LLM agents in domain-specific settings, where the environment and available toolsets are explicitly designed for particular task category (e.g., software engineering with Docker environment and terminal tools (Jimenez et al., 2023; Aleithan et al., 2024), or web navigation with browsing interface (Zhou et al., 2023b; He et al., 2024; Wei et al., 2025)). Conversely, actual user interactions are rarely so constrained; they typically involve multi-turn, open-ended requests spanning disparate domains, requiring agents to remain ready across broad toolset to handle unpredictable queries. As result, while current benchmarks are informative for domainspecific agent development, they may not fully capture the demands of real-world usage and can overestimate robustness under realistic, multi-domain conditions. In this paper, we address this gap by introducing General AgentBench, benchmark designed to evaluate generalpurpose agents across diverse scenarios under unified framework that more closely reflects real-world user interactions. We consolidate tools from all domains into shared interface that is consistently exposed to evaluated agents across different tasks, while domain-specific environments and implementations remain hidden. We evaluate ten leading LLM-based agents, each of which must first interpret the user request, then choose suitable tools from large and diverse tool pool, and iteratively interact with the environment until producing final response. For complex requests that exceed the capabilities of short interaction horizon, agents can benefit from increased Benchmark Test-Time Scaling of General LLM Agents (a) Performace comparsion. (b) Sequential test-time scaling. (c) Parallel test-time scaling. Figure 1. Evaluating general LLM agents under realistic user-interaction scenario. A: GPT-5s performance drop under General AgentBench compared to static, domain-specified evaluation. B: Sequential test-time scaling via longer interaction histories can lead to unstable or degraded performance. C: While correct solutions increasingly appear in the generation space (past@K), agents often fail to select them, revealing verification gap. inference-time computationa strategy known as test-time scaling and extensively studied in the context of non-agentic reasoning (Cobbe et al., 2021; Zelikman et al., 2024; Guo et al., 2025). We focus on two primary scaling strategies: (1) sequential scaling, which extends interaction histories to support continued reasoning, reflection, and exploration; and (2) parallel scaling, which independently samples candidate trajectories and selects single candidate to return. More concretely, effective parallel scaling requires agents not only to generate correct solutions, but also to reliably identify and choose the correct one, since real-world agents cannot present multiple responses simultaneously. Together, these settings enable systematic study of test-time scaling behaviors in general LLM agents. Our results lead to three key conclusions. (1) Across ten leading LLMs, we observe substantial performance drop when moving from domain-specific configurations to the general-agent setting (Figure 1a), with pronounced differences in robustness across model families. Among them, Claude exhibits the strongest robustness, while most other models experience performance drops of approximately 30%. (2) Sequential scaling exhibits an effective context length ceiling (context ceiling): while performance improves within modest range of additional interaction turns, it often fluctuates or degrades thereafter. This suggests that the accumulated history eventually overwhelms the agents reasoning capacity, leading to instability in long-horizon tasks. (Figure 1b). (3) Although parallel scaling increases the theoretical upper bound of performance (past@K), we consistently observe gap between this upper bound and self-choice accuracy, revealing substantial verification gap that ultimately limits achievable performance in realistic settings (Figure 1c). In summary, our contributions are: General AgentBench for Realistic Evaluation. We introduce General AgentBench, benchmark for evaluating whether agents can compose multiple skills and tools to solve open-ended requests from diverse domains under unified framework, more closely reflecting real-world user interactions. Sequential Test-time Scaling in General Agents. We study the sequential test-time scaling behavior of general agents, showing that performance improvements are bounded by an effective context ceiling, beyond which additional computation often leads to instability and performance degradation. This inherent point varies across models and domains. Parallel Test-time Scaling. We analyze parallel testtime scaling and show that, despite increasing the theoretical performance upper bound (past@K), its practical gains are limited by verification gap between generation and model self-choice accuracy. 2. General AgentBench In this section, we introduce the construction of General AgentBench (Section 2.1) and the unified evaluation framework (Section 2.2 ). Detailed prompt templates, tool specifications, and the unified policy are deferred to the Appendix E. 2.1. Domains and Sources Our benchmark spans four task domains: Coding, Search, Tool-use, and Reason. These domains reflect common real-world applications such as software engineering, information seeking, service workflows, and analytical reasoning, positioning General AgentBench as an initial step toward 2 Benchmark Test-Time Scaling of General LLM Agents evaluating general-purpose agents in open-ended and unified settings. Table 1 summarizes the benchmark composition. Table 1. Composition of General AgentBench Domain Dataset Original Sampled Search Coding BrowseComp WebVoyager SWE-Bench Verified Terminal-Bench Reason MathHay Tool-Calling Tau2-Bench MCP-Bench 1266 500 230 602 278 104 124 65 50 80 50 52 Coding We include tasks from SWE-Bench Verified (OpenAI, 2024b) and Terminal Bench, which evaluate an agents ability to analyze production-level software issues, reason over long instructions, and iteratively interact with execution environments to reach correct final state. Search The search domain includes tasks from BrowseComp (Wei et al., 2025) and WebVoyager (He et al., 2024). These benchmarks assess an agents ability to identify missing information, decide when additional search steps are needed, and navigate long, evolving web contexts, going beyond static retrieval or single-turn question answering. Figure 2. Illustration of how General AgentBench covers wide range of task categories while providing unified interface to simulate real-world user interactions. The green region indicates the specific task currently being handled by the agent (e.g., search task). Orange boxes denote other clients and servers that remain active and responsive but are not directly involved in the current interaction. Red indicates that other domain-specific data are excluded. we adopt the Model Context Protocol (MCP) (Anthropic, 2024) as the backbone of our framework. Each benchmark environment is instantiated as an MCP server, while all servers are centrally managed by unified Host. The Host maintains global tool registry that records all available tools and their corresponding server mappings, presenting the agent with single, unified tool space across all domains. Tool-use For tool-use, we adopt Tau2-Bench (Barres et al., 2025) and MCP-Bench (Wang et al., 2025b), both of which provide rich tool suites requiring models to select, invoke, and coordinate multiple tools. These tasks emphasize structured tool calling and multi-step planning in realistic service and workflow scenarios. Centralized interaction abstraction. The Host serves as the sole interaction interface for the general agent, abstracting away individual benchmark implementations. When the agent invokes tool, the Host resolves the call via the tool registry and routes the request to the appropriate server for execution. Reason For long-context reasoning, we use MathHay (Wang et al., 2024a), which constructs queries by embedding relevant mathematical documents into noisy longcontext haystacks. This benchmark isolates sustained reasoning over long inputs without relying on external tool execution, complementing the other domains. 2.2. Unified Realistic Evaluation Framework To support realistic evaluation of general LLM agents, we design unified framework that exposes all tasks and tools through shared interaction interface. These choices reflect three fundamental properties of real-world agent usage: cross-domain task diversity, comprehensive skill requirements, and dynamically evolving multi-turn interactions. An overview of the framework is illustrated in Figure 2. Unified tool interface. In practical deployments, agents must select appropriate tools from large pool without prior knowledge of task domains. To reflect this setting, Evolving interaction context. Because all tools and benchmark environments are exposed simultaneously, the unified tool descriptions alone can span tens of thousands of tokens. When combined with user queries and accumulated multi-turn interaction histories, the resulting context naturally grows into the long-context regime. In this setting, agents must reason over heterogeneous information sources, including task instructions, tool documentation, execution feedback, and their own prior decisions. This distinguishes agentic interaction from many existing long-context benchmarks that focus on static, single-turn question answering or summarization with short outputs. We provide further long-context study in Appendix A. Execution process. For each evaluation instance, the framework provides it to the agent together with the unified policy and toolset as the context. All benchmark servers (e.g., Docker-based environments in the coding domain) are instantiated simultaneously and remain idle while awaiting requests from the agent. When the agent issues tool call, 3 Benchmark Test-Time Scaling of General LLM Agents Table 2. Main results on General AgentBench. Benchmarks are grouped by domain. Avg. denotes the mean score across all available benchmarks for each model. Bold indicates the best score. Models Open-Source GPT-OSS-120B Qwen3-235B-A22B Qwen3-Next DeepSeek-V3.2 DeepSeek-R1 Proprietary Gemini 2.5-Flash Gemini 2.5-Pro Claude Haiku 4.5 Claude Sonnet 4.5 GPTSearch Code Reason Tool-use Avg. BrowseComp WebVoyager SWE-Bench Terminal-Bench MathHay Tau2-Bench MCP-Bench 4.0 8.9 10.5 19.4 9.7 6.5 8.9 17.7 23.1 27.4 27.7 30.8 35.4 46.2 43.1 32.3 46.2 47.7 56.9 61.5 12.0 20.4 18.0 31.8 14.0 14.0 26.0 56.0 54.0 36. 6.3 23.8 8.8 22.2 8.8 20.0 27.5 25.0 45.0 41.3 38.7 32.0 42.0 33.3 46.7 36.0 24.0 34.7 36.0 64.0 26.0 38.3 48.9 54.0 17.1 38.3 46.0 44.0 48.0 32. 63.3 25.4 66.1 31.5 64.6 32.6 66.0 39.0 62.2 28.8 66.6 30.5 67.2 35.1 69.0 42.0 72.9 48.0 59.1 45.9 the Host routes the request to the corresponding server, executes the tool, and returns the result in unified response format. Tool execution is decoupled from task type: even if task is search-oriented, code-related tool calls can still be executed by the environment, returning valid outputs despite having no direct relevance to the final solution. This design intentionally exposes the agent to realistic setting where incorrect or irrelevant tool usage remains possible. The agent interacts with the framework over multiple turns until producing final answer. During interaction, we monitor execution signals (e.g., terminal outputs) and regulate the interaction budget, enabling additional computation or extended reasoning when applicable (Section 4.1). The final answer is then forwarded to the corresponding benchmark server for correctness evaluation. 2.3. Experimental details Our evaluation covers total of ten frontier language models. Among open-source models, we include several highperforming systems such as Qwen3-235B (Yang et al., 2025) and DeepSeek-R1 (Guo et al., 2025), as well as more recent models with novel attention mechanisms, including Qwen3Next (Qwen Team, Alibaba Inc., 2025) and DeepSeek-v3.2 (Liu et al., 2025). For proprietary models, we consider both efficiency-oriented variants (e.g., Gemini 2.5 Flash (Comanici et al., 2025)) and models optimized for complex reasoning (e.g., GPT-5 (OpenAI, 2025a) and Claude Sonnet 4.5 (Anthropic, 2025a)). We access these models via Amazon Bedrock 1 and the Hugging Face Inference API .2 For all evaluations, we fix the decoding temperature to 0.7 and ensure that each models native context length exceeds the maximum context length required by the benchmark. 1https://aws.amazon.com/bedrock/pricing/ 2https://huggingface.co/docs/inference-p roviders/en/index 3. Main Results In this section, we report overall performance on General AgentBench and compare it against evaluations conducted under prior domain-specific settings to quantify the gap between specialized and general-purpose LLM agents. 3.1. Result analysis Table 2 summarizes the results across models and domains on General AgentBench. Claude Sonnet 4.5 achieves the strongest overall performance, driven primarily by its tooluse and coding capabilities, while GPT-5 attains the highest scores in the Search and Reason domains, reflecting its strengths in information retrieval and complex reasoning. Among open-source models, DeepSeek-V3.2 outperforms both Gemini variants, demonstrating the significant scaling potential of efficient, sparse-attention architectures. Across models, performance on BrowseComp remains consistently low, indicating that retrieving rare and precise information beyond in-domain training data is still major bottleneck for current LLM agents. We further examine how performance changes when models transition from specialized agents operating under domainspecific contexts to general agents acting within unified environment with shared toolsets. Figure 4 summarizes the mean degradation aggregated over all domains, while Figure 3 reports the relative performance change for each agent across domains. Most LLM agents experience substantial degradation in the general-agent setting, with average relative drops ranging from 10% to 30%. The magnitude of this degradation varies widely: for example, Gemini 2.5Pro suffers drop exceeding 60% in the Reason domain, falling from top-tier performance in the baseline setting to near-average performance as general agent. In contrast, Claude Sonnet 4.5 remains notably robust, with only 0.2% average degradation. Detailed overall results can be found 4 Benchmark Test-Time Scaling of General LLM Agents Figure 3. Relative performance change across domains from the Baseline (B) specialized agent setting to the general agent (G) setting with unified context and tools. Negative values indicate performance degradation under the General AgentBench. Case Study: Retrieving the Latest HF Model Task. Identify the most recent pre-trained language model on HuggingFace suitable for text classification. Plain Web Search Baseline (6 turns) T1: <search> latest Hugging Face pre-trained language model text classification 2024 </search> T2: <search> Hugging Face text classification models sorted by recent trending 2024 </search> T3: <search> Hugging Face newest text classification models January 2025 recently released </search> ... T6: <answer> tabularisai/multilingual-sentimentanalysis (fine-tuned DistilBERT) </answer> Figure 4. Performance comparison between specialized-agent and general-agent settings.Top: Absolute performance .Bottom: Relative performance degradation under the general-agent setting. General Agent (3 turns) T1: Hugging Face search-models( {\"tags\": \"text-classification\", \"limit\": [{\"id\": \"FutureMa/Qwen3-4B-Evasion\", \"download\": {\"id\": \"BAAI/bge-reranker-v2-m3\", \"download\": 2807076}, ...] 15}, 15}) in Appendix A.2. 3.2. Cross-domain tool usage Interestingly, several models, including Qwen3-Next, Deepseek-R1, and Claude, exhibit performance gains in the Search domains under the general-agent setting. Trajectory-level analysis shows that these improvements arise from effective cross-domain tool usage, where agents repurpose tools beyond their originally intended domains to support reasoning and information retrieval. We take closer look at these behaviors. Analysis of 189 search task traces from Claude Sonnet 4.5 reveals that 26% of tasks (50/189) utilized specialized domain tools beyond plain web search. The most frequently used specialized tools include Google Maps APIs (78 calls), Paper Search across arXiv, PubMed, and Google Scholar (60 calls), and Hugging Face model APIs (36 calls). We present case study demonstrating how domain-specific tools outperform plain web search. T2: search web search(\"ModernBERT HF 2025\") 8192-token context, \"ModernBERT... 139M/395M params, trained on 2T tokens\" T3: Hugging Face get-model-info( {\"model id\": Full model card and architecture details \"answerdotai/ModernBERT-base\"}) the plain search baseline iteratively We observe that refines web queries across 6 turns, ultimately finding tabularisai/multilingual-sentiment-analysis with only surface-level information (fine-tuned DistilBERT). In contrast, the General agent systems specialized Hugging Face search-models API directly queries the model hub with structured filters, returning download counts, tags, and model IDs. The subsequent Hugging Face get-model-info call retrieves comprehensive metadata including architecture specifications, training data scale, and official model cardsinformation unavailable through web search snippets. This behavior reflects an agents ability to dynamically select and compose tools under minimal domain priors, capturing more realistic upper bound on general-agent capability 5 Benchmark Test-Time Scaling of General LLM Agents Figure 5. Test-time scaling behaviors of general LLM agents. Results are reported for five models across four domains on General AgentBench. Top: Parallel scaling expands the solution space through increased sampling. Bottom: Sequential scaling allocates additional computation via longer interaction histories, yet exhibiting unstable or diminishing returns. and highlighting the importance of evaluation settings that approximate real-world tool availability. 4. Test-Time Scaling Evaluation In this section, we present systematic study of test-time scaling behavior of general LLM agents. Section 4.1 introduces the scaling strategies, with results and findings presented in Sections 4.2 and 4.3. 4.1. Scaling Methodology We investigate test-time scaling through two complementary paradigms: Parallel Scaling and Sequential Scaling, which correspond to distinct axes of computational allocationbreadth of exploration versus depth of exploitation. Compared with more sophisticated test-time scaling techniques such as self-correction(Madaan et al., 2023), beam search(Yao et al., 2023), or MCTS(Zhou et al., 2023a), these two strategies are the most commonly adopted and easiest to deploy in real-world agentic systems. evaluating and selecting the best outcome from their own generated trajectories. To assess this ability, we introduce this Self-Choice setting, in which the agent evaluates its parallelly sampled outputs using one of the following strategies: (1) Point-wise choice. The agent independently evaluates each sampled trajectory and assigns binary judgment. Performance is measured by the alignment between the models judgments and oracle labels, averaged over trajectories that are correct under oracle evaluation. (2) Pair-wise choice. The agent compares two sampled trajectories at time and iteratively promotes the superior one through bubble-sort-style selection process. After 1 pairwise comparisons, single trajectory is selected as the final output, and performance is evaluated based on the correctness of this selected trajectory. Overall, self-choice reflects the practical effectiveness of parallel scaling, while past@K serves as an upper bound that reveals the solution potential. Parallel Scaling. In the parallel scaling regime, we independently sample trajectories for each query. Increasing expands the reachable action space, thereby increasing the likelihood that the agent explores at least one trajectory containing correct solution, relative to the single-shot baseline (K = 1). However, in the absence of external oracles or human feedbackparticularly in real-world deploymentsparallel scaling alone is insufficient: agents must also be capable of Sequential Scaling. In contrast, sequential scaling increases computational depth by extending the interaction horizon. As the agent engages with the environment, the conversation context progressively grows. When the agent attempts to terminate an episode (e.g., by emitting an Endof-Turn token), we inject an additional round of environment feedback to encourage further reflection on prior reasoning and exploration of alternative solution paths. To quantify scaling behaviors under both paradigms, we plot 6 Benchmark Test-Time Scaling of General LLM Agents task accuracy against the number of independent samples (K) for parallel scaling, and against cumulative context length for sequential scaling in Figure 5. Due to the cost of API-based inference, each model is sampled at most four times, and context length is scaled up to 196K tokens. detailed cost analysis for reproducibility is provided in the Appendix C. 4.2. Sequential Scaling Figure 6. Instance-level correctness dynamics under sequential scaling. We randomly sample 10 instances from the reasoning domain and track their correctness across increasing context lengths. Red indicates incorrect predictions and blue indicates correct ones. Most instances exhibit stagnant or oscillatory behavior, repeating prior successes while failing on unresolved cases, with some fluctuating between correct and incorrect across steps. Sequential scaling exhibits behavior that departs markedly from our expectations, shown in the bottom of Figure 5. Although several models on certain benchmarks show performance improvements, such as Qwen3-235B on the Search domain and Deepseek-v3.2 on the Reason domain, most show little to no consistent improvement despite allocating additional computational resources through iterative reasoning and reflection. We categorize sequential scaling behaviors into two distinct regimes. (1) Stagnant fluctuation: In domains such as reasoning, performance oscillates within narrow range despite increased computation. This pattern suggests limited capacity for agents to explore novel solution paths within extended interaction traces, coupled with diminished ability to maintain coherencelikely due to the agents finite context processing capacity. (2) Saturation and degradation: In coding domains, models initially benefit from additional reasoning steps; however, beyond an important turning point, performance consistently deteriorates and fails to recover. Figure 6 shows how instance-level correctness for several data entries changes as the context length increases. We observe that agents either repeatedly succeed on queries they already handle well while failing to progress on unsuccessful cases, or exhibit unstable behavior with accuracy fluctuating between 0 and 1 across interaction steps. These observations further validate our summarization regarding the inherent limitations of sequential scaling. 7 Figure 7. Sequential scaling behavior of Gemini 2.5-Flash and Qwen3-235B across domains, with inherent context lengths indicated by the dashed line. Performance scales positively as interaction history approaches and slightly exceeds the inherent context; however, it saturates or degrades once the context extends significantly beyond this threshold. This limit represents the context ceiling of sequential scaling, beyond which further history yields diminishing returns. To better understand how this turning point occurs, we analyze performance relative to an agents inherent context lengthdefined as the total context naturally accumulated when completing task without artificial constraints on interaction depth. By integrating data from Qwen3-235B and Gemini 2.5-Flash (as seen in Figure 5) with new experimental results on smaller context windows, we observed distinct performance ceiling. As shown in Figure 7, both models exhibit an initial upward trend, approaching peak performance as they reach their inherent context limits. However, once the accumulated context passes specific threshold ( for example, approximately 112K for Qwen3-235B and 96K for Gemini 2.5-Flash in the search domain), performance typically plateaus or begins to degrade. In these instances, additional computation and time yield no further gains. This suggests the existence of context ceiling: maximum effective context length under sequential scaling, beyond which raw interaction history offers diminishing or zero practical returns. Notably, this ceiling varies across domains, reflecting the unique demands each task places on context utilization and computational efficiency. Taken together, these results indicate that simply allocating more computation by extending raw interaction histories rarely leads to meaningful performance gains, contradicting previous observations in non-agentic settings (Muennighoff et al., 2025). Long-horizon agentic tasks expose fundamental challenges in context utilization and reasoning stability, highlighting the need for more effective mechanisms for context management and reasoning control under sequential scaling (Zhang et al., 2025c; Anthropic, 2025c). Benchmark Test-Time Scaling of General LLM Agents Figure 8. Verification gap between generation and self-choice. Across four domains, we observe consistent gap between solution generation and verification: as the number of samples increases, correct solutions appear more frequently in the sampled set, yet models often fail to identify and select them. The dashed and dotted curves represent two self-choice strategies, while the diamond denotes stronger evaluator, GPT-5. 4.3. Parallel Scaling We observe monotonic increase in pass@K as the number of sampled trajectories grows (Figure 5, top). Increasing from 1 to 4 yields an average improvement of roughly 50%, with DeepSeek-V3.2 exhibiting the largest gainsapproaching twofold improvement in both coding and reasoning domains. However, pass@K only reflects an idealized upper bound. In practice, the effectiveness of parallel scaling depends on an agents ability to evaluate and select among its own sampled trajectories, which is captured by self-choice performance. As shown by the gap between the solid lines (pass@K) and the dashed or dotted lines (self-choice) in Figure 8, self-choice performance consistently lags behind the pass@K upper bound regardless of the strategy. In some cases, self-choice performance even degrades as increases. While the coding domain exhibits the smallest gap between pass@K and self-choice, in most settings selfchoice gains saturate quickly and fail to track the continued improvement of pass@K. To examine whether this gap stems from limited verifier capability, we replace the models internal self-judgment with an external verifier, GPT-5, and perform point-wise evaluation of sampled trajectories from all models. As shown by the diamond line in Figure 8, GPT-5 generally underperforms models own self-judgment. Notably, even at = 1, GPT-5 occasionally misclassifies correct trajectories as incorrect, introducing non-trivial gap between pass@1 and GPT-5based verification. We hypothesize that this effect arises from solution familiarity: models are better at evaluating their own generations, which align closely with their internal reasoning patterns, whereas external verifiers may struggle to accurately assess unfamiliar execution traces. Overall, these results reveal fundamental gap between models generation capacity and their ability to reliably select correct solutions, which ultimately limits the practical utility of parallel scaling. Benchmark Test-Time Scaling of General LLM Agents 5. Related Work Prior work on LLM agents spans three closely related directions: benchmarking agentic abilities, developing agent models and frameworks, and exploring effective scaling methods. Most agentic benchmarks evaluate specialized agents in domain-specific settings, where tasks, interaction protocols, and tool access are tailored to specific skills. Representative ones cover software engineering (e.g., SWEbench (Jimenez et al., 2023) and its variants (Aleithan et al., 2024; Zhang et al., 2025b; Yang et al., 2024a)), web navigation and interaction (e.g., WebShopcite (Yao et al., 2022a), Mind2Web (Deng et al., 2023), WebArena (Zhou et al., 2023b), WebVoyager (He et al., 2024)), and tool use with curated APIs (e.g., ToolLLM (Qin et al., 2023), API-Bank (Li et al., 2023), BFCL (Patil et al., 2025), StableToolBench (Guo et al., 2024)). Broader suites such as τ -bench (Yao et al., 2024), AgentBench(Barres et al., 2025), and GAIA (Mialon et al., 2023) incorporate multi-turn interaction across multiple task categories , while OSWorld (Xie et al., 2024), OSWorld-G (Xie et al., 2025), and AndroidWorld (Rawles et al., 2024) evaluate agents operating in real desktop and mobile systems. In parallel, substantial effort has focused on building general-purpose agents capable of planning, acting, and invoking tools across heterogeneous tasks. Early methods such as ReAct (Yao et al., 2022b), Reflexion (Shinn et al., 2023), Toolformer (Schick et al., 2023), and HuggingGPT (Shen et al., 2023) introduce structured reasoningaction trajectories, reflection, and learned tool invocation. More recent work emphasizes scalable agent frameworks and deployment platforms supporting multi-agent coordination and rich tool ecosystems, including OpenAgents (Xie et al., 2023), AgentVerse (Chen et al., 2024b), AgentScope (Gao et al., 2024), and OpenCUA (Wang et al., 2025a). Industry systemssuch as agents built on Claude (Anthropic, 2025b), Microsoft (Microsoft, 2025), OpenAI (OpenAI, 2025), Qwen (QwenLM, 2023), Kimi (Moonshot AI, 2026), and Gemini (Deepmind, 2025)further demonstrate the practical importance of general agents in real-world deployments. complementary line of research studies test-time scaling, allocating additional inference-time computation to improve agent performance. Chain-of-thought prompting (Wei et al., 2022) and self-consistency (Wang et al., 2022) show that deeper reasoning and multi-sample decoding can substantially boost accuracy. More explicit approaches perform search or refinement over solution paths, including Tree-of-Thoughts (Yao et al., 2023), MCTS-based agent planning such as LATS (Zhou et al., 2023a), and iterative refinement methods like Reflexion (Shinn et al., 2023) and Self-Refine (Madaan et al., 2023). More recently, internal scaling trains models to autonomously decide how much inference computation to allocate and when to terminate reasoning, shifting control from external orchestration toward model-internal deliberation (Guo et al., 2025; OpenAI, 2024a). Verifier-based inference further augments sampling or search with learned ranking or rejection, with processlevel supervision improving reliability in mathematical reasoning (Lightman et al., 2023; Li et al., 2024; Hosseini et al., 2024; Wang et al., 2024c). 6. Conclusions We present General AgentBench, unified benchmark for evaluating LLM agents under realistic, multi-domain interactions where agents must infer intent, select tools from shared pool, and act end-to-end. Across ten leading models, we find substantial robustness gap when moving from domain-specific to general-agent evaluation. Our test-time scaling analysis reveals two fundamental limits: sequential scaling is bounded by an context ceiling, beyond which longer interactions become unstable, and parallel scaling delivers limited practical gains due to persistent verification gap between generation and self-choice. We hope General AgentBench enables realistic assessment and guides progress toward robust, scalable general agents."
        },
        {
            "title": "Impact Statement",
            "content": "This paper introduces General AgentBench, benchmark for evaluating large language model agents as general-purpose systems under unified and realistic interaction settings. By exposing agents to diverse tasks and tools within single framework, it enables more consistent and transparent assessment of agents abilities to interpret open-ended requests, select appropriate tools, and scale inference-time computation across domains. We expect this benchmark to support the development and diagnosis of more robust general-purpose agents. We also acknowledge potential risks. Unified evaluation and test-time scaling may favor computationally intensive or proprietary models, and unreliable self-choice under parallel scaling may lead to unstable or misleading agent behavior if applied naively in deployment. As general-purpose agents are increasingly integrated into real-world applications, failures in long-horizon reasoning, tool use, or self-verification may affect reliability and user trust. We emphasize that General AgentBench is intended for research and evaluation purposes. The community shares responsibility to interpret results carefully and to pair scaling-based improvements with stronger verification, transparency, and safety considerations."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Singh, A., Zhang, L., Bohnet, B., Rosias, L., Chan, S., Zhang, B., Anand, A., Abbas, Z., Nova, A., 9 Benchmark Test-Time Scaling of General LLM Agents et al. Many-shot in-context learning. Advances in Neural Information Processing Systems, 37:7693076966, 2024. Aleithan, R. et al. Swe-bench+: Enhanced coding benchmark for llms. arXiv preprint arXiv:2410.06992, 2024. Anthropic. Introducing the model context protocol, November 2024. URL https://www.anthropic.com/ news/model-context-protocol. Anthropic. Introducing claude sonnet 4.5. https://www. anthropic.com/news/claude-sonnet-4-5, September 2025a. Accessed: 2026-01-29. Anthropic. Building agents with the Claude agent SDK. https://www.anthropic.com/engineerin g/building-agents-with-the-claude-age nt-sdk, September 2025b. Accessed: 2026-01-29. Anthropic. Effective context engineering for ai agents. ht tps://www.anthropic.com/engineering/ effective-context-engineering-for-a i-agents, September 2025c. Accessed: 2026-01-29. Anthropic. Building agents with skills: Equipping agents for specialized work, January 2026. URL https://cl aude.com/blog/building-agents-with-s kills-equipping-agents-for-specializ ed-work. Bai, Y., Lv, X., Zhang, J., Lyu, H., Tang, J., Huang, Z., Du, Z., Liu, X., Zeng, A., Hou, L., et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers), pp. 31193137, 2024. Barres, V., Dong, H., Ray, S., Si, X., and Narasimhan, K. tau2-bench: Evaluating conversational agents in dualcontrol environment. arXiv preprint arXiv:2506.07982, 2025. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Re, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Chen, J., Lin, H., Han, X., and Sun, L. Benchmarking large language models in retrieval-augmented generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1775417762, 2024a. Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., and Wang, W. Y. Tabfact: large-scale dataset for table-based fact verification. arXiv preprint arXiv:1909.02164, 2019. Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Chan, C.-M., Yu, H., Lu, Y., Hung, Y.-H., Qian, C., et al. Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors. In The Twelfth International Conference on Learning Representations, 2024b. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. Deepmind, G. Gemini, Nov 2025. URL https://deep mind.google/models/gemini/. Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B., Sun, H., and Su, Y. Mind2web: Towards generalist agent for the web. In NeurIPS Datasets and Benchmarks, 2023. Ding, Y., Wang, Z., Ahmad, W., Ding, H., Tan, M., Jain, N., Ramanathan, M. K., Nallapati, R., Bhatia, P., Roth, D., et al. Crosscodeeval: diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36:46701 46723, 2023. Erdogan, L. E., Furuta, H., Kim, S., Lee, N., Moon, S., Anumanchipalli, G., Keutzer, K., and Gholami, A. Planand-act: Improving planning of agents for long-horizon tasks. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview .net/forum?id=ybA4EcMmUZ. Fabbri, A. R., Li, I., She, T., Li, S., and Radev, D. Multi-news: large-scale multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 10741084, 2019. Friel, R., Belyi, M., and Sanyal, A. Ragbench: Explainable benchmark for retrieval-augmented generation systems. arXiv preprint arXiv:2407.11005, 2024. Gao, D., Li, Z., Pan, X., Kuang, W., Ma, Z., Qian, B., Wei, F., Zhang, W., Xie, Y., Chen, D., et al. Agentscope: flexible yet robust multi-agent platform. arXiv preprint arXiv:2402.14034, 2024. Gao, T., Yen, H., Yu, J., and Chen, D. Enabling large language models to generate text with citations. arXiv preprint arXiv:2305.14627, 2023. 10 Benchmark Test-Time Scaling of General LLM Agents Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Guo, Z. et al. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. In Findings of ACL, 2024. He, H., Yao, W., Ma, K., Yu, W., Dai, Y., Zhang, H., Lan, Z., and Yu, D. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. Ho, X., Duong Nguyen, A.-K., Sugawara, S., and Aizawa, A. Constructing multi-hop QA dataset for comprehensive evaluation of reasoning steps. In Scott, D., Bel, N., and Zong, C. (eds.), Proceedings of the 28th International Conference on Computational Linguistics, pp. 66096625, Barcelona, Spain (Online), December 2020. International Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https://aclanthology.org/2020.coling -main.580/. Hosseini, A., Yuan, X., Malkin, N., Courville, A., Sordoni, A., and Agarwal, R. V-star: Training verifiers for selftaught reasoners. arXiv preprint arXiv:2402.06457, 2024. Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., and Ginsburg, B. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654, 2024. Hu, Y., Ganter, T., Deilamsalehy, H., Dernoncourt, F., Foroosh, H., and Liu, F. Meetingbank: benchmark dataset for meeting summarization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 16409 16423, 2023. Huang, L., Cao, S., Parulian, N., Ji, H., and Wang, L. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. Koˇcisk`y, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K. M., Melis, G., and Grefenstette, E. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328, 2018. Kuratov, Y., Bulatov, A., Anokhin, P., Rodkin, I., Sorokin, D., Sorokin, A., and Burtsev, M. Babilong: Testing the limits of llms with long context reasoning-in-a-haystack. Advances in Neural Information Processing Systems, 37: 106519106554, 2024. Kweon, S., Kwon, Y., Cho, S., Jo, Y., and Choi, E. Openwikitable: Dataset for open domain question answering with complex reasoning over table. arXiv preprint arXiv:2305.07288, 2023. Li, M. et al. Api-bank: comprehensive benchIn EMNLP, 2023. mark for tool-augmented llms. arXiv:2304.08244. Li, X., Yu, Z., and Xiong, C. Montessori-instruct: Generate influential training data tailored for student learning. arXiv preprint arXiv:2410.14208, 2024. Lightman, H. et al. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556, 2025. Liu, T., Xu, C., and McAuley, J. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091, 2023a. Liu, X.-Y., Wang, G., Yang, H., and Zha, D. Fingpt: Democratizing internet-scale data for financial large language models. arXiv preprint arXiv:2307.10485, 2023b. Liu, Y., Huang, L., Li, S., Chen, S., Zhou, H., Meng, F., Zhou, J., and Sun, X. Recall: benchmark for llms robustness against external counterfactual knowledge. arXiv preprint arXiv:2311.08147, 2023c. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. Swe-bench: Can language modarXiv preprint els resolve real-world github issues? arXiv:2310.06770, 2023. Luo, J., Zhang, W., Yuan, Y., Zhao, Y., Yang, J., Gu, Y., Wu, B., Chen, B., Qiao, Z., Long, Q., et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. Jin, J., Paladugu, A., and Xiong, C. Beneficial reasoning behaviors in agentic search and effective post-training to obtain them. arXiv preprint arXiv:2510.06534, 2025. Kamradt, G. LLMTest NeedleInAHaystack: Needle in haystack pressure testing llms. https://github.c om/gkamradt/LLMTest_NeedleInAHaystack, 2023. GitHub repository. Accessed: 2026-01-15. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023. Mialon, G., Fourrier, C., Swift, C., Wolf, T., LeCun, Y., and Scialom, T. Gaia: benchmark for general ai assistants. arXiv preprint arXiv:2311.12983, 2023. 11 Benchmark Test-Time Scaling of General LLM Agents Microsoft. Microsoft/agent-framework: framework for building, orchestrating and deploying ai agents and multiagent workflows with support for python and .net., Apr 2025. URL https://github.com/microsoft /agent-framework. Moonshot AI. Kimi k2.5: Visual agentic intelligence. http s://www.kimi.com/blog/kimi-k2-5.html, January 2026. Accessed: 2026-01-28. Muennighoff, N., Tazi, N., Magne, L., and Reimers, N. Mteb: Massive text embedding benchmark. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 2014 2037, 2023. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Cand`es, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. OpenAI. Learning to reason with llms. https://op enai.com/index/learning-to-reason-wit h-llms/, September 2024a. Accessed: 2026-01-28. OpenAI. Introducing SWE-bench verified. https://op enai.com/index/introducing-swe-bench -verified/, August 2024b. OpenAI Blog. Updated: 2025-02-24. Accessed: 2026-01-15. OpenAI. Gpt-5 system card. Technical report, OpenAI, August 2025a. URL https://cdn.openai.com /gpt-5-system-card.pdf. OpenAI. Openai mrcr: Long context multiple needle in haystack benchmark. https://huggingface.co /datasets/openai/mrcr, 2025b. Hugging Face Datasets. Accessed: 2026-01-15. OpenAI. Introducing agentkit, Oct 2025. URL https: //openai.com/index/introducing-agent kit/. Parmar, M., Liu, X., Goyal, P., Chen, Y., Le, L., Mishra, S., Mobahi, H., Gu, J., Wang, Z., Nakhost, H., et al. Plangen: multi-agent framework for generating planning and reasoning trajectories for complex problem solving. arXiv preprint arXiv:2502.16111, 2025. Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37: 126544126565, 2024. Patil, S. G., Mao, H., Yan, F., Ji, C. C.-J., Suresh, V., Stoica, I., and Gonzalez, J. E. The berkeley function calling leaderboard (BFCL): From tool use to agentic evaluation of large language models. In Forty-second International Conference on Machine Learning, 2025. URL https: //openreview.net/forum?id=2GmDdhBdDk. Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Qwen Team, Alibaba Inc. Qwen3-Next: Towards ultimate training & inference efficiency. https://qwen.ai/ blog?id=4074cca80393150c248e508aa629 83f9cb7d27cd, September 2025. Accessed: 2026-0129. QwenLM. Qwenlm/qwen-agent: Agent framework and applications built upon qwen=3.0, featuring function calling, mcp, code interpreter, rag, chrome extension, etc., Sep 2023. URL https://github.com/QwenLM/ Qwen-Agent. Rawles, C., Clinckemaillie, S., Chang, Y., Waltz, J., Lau, G., Fair, M., Li, A., Bishop, W., Li, W., CampbellAjala, F., et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli, M., Hambro, E., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. Shaham, U., Ivgi, M., Efrat, A., Berant, J., and Levy, O. Zeroscrolls: zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023. Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang, Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. arXiv preprint arXiv:2303.17580, 2023. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Thakur, N., Reimers, N., Ruckle, A., Srivastava, A., and Gurevych, I. Beir: heterogenous benchmark for zeroshot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663, 2021. 12 Benchmark Test-Time Scaling of General LLM Agents Tu, Q., Chen, C., Li, J., Li, Y., Shang, S., Zhao, D., Wang, R., and Yan, R. Characterchat: Learning towards conversational ai with personalized social support. arXiv preprint arXiv:2308.10278, 2023. Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. Wang, L., Xu, W., Lan, Y., Hu, Z., Lan, Y., Lee, R. K.-W., and Lim, E.-P. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023. Wang, L., Dong, S., Xu, Y., Dong, H., Wang, Y., Saha, A., Lim, E.-P., Xiong, C., and Sahoo, D. Mathhay: An automated benchmark for long-context mathematical reasoning in llms. arXiv preprint arXiv:2410.04698, 2024a. Wang, M., Chen, L., Cheng, F., Liao, S., Zhang, X., Wu, B., Yu, H., Xu, N., Zhang, L., Luo, R., et al. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 56275646, 2024b. Wang, P., Li, L., Shao, Z., Xu, R., Dai, D., Li, Y., Chen, D., Wu, Y., and Sui, Z. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 94269439, 2024c. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wang, X., Wang, B., Lu, D., Yang, J., Xie, T., Wang, J., Deng, J., Guo, X., Xu, Y., Wu, C. H., et al. Opencua: Open foundations for computer-use agents. arXiv preprint arXiv:2508.09123, 2025a. Wang, Z., Chang, Q., Patel, H., Biju, S., Wu, C.-E., Liu, Q., Ding, A., Rezazadeh, A., Shah, A., Bao, Y., et al. Mcp-bench: Benchmarking tool-using llm agents with complex real-world tasks via mcp servers. arXiv preprint arXiv:2508.20453, 2025b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q. V., and Zhou, D. Chain-of-thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516, 2025. Xie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., et al. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634, 2023. Xie, T., Zhang, D., Chen, J., Li, X., Zhao, S., Cao, R., Hua, T. J., Cheng, Z., Shin, D., Lei, F., et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. Xie, T., Deng, J., Li, X., Yang, J., Wu, H., Chen, J., Hu, W., Wang, X., Xu, Y., Wang, Z., et al. Scaling computer-use grounding via user interface decomposition and synthesis. arXiv preprint arXiv:2505.13227, 2025. Yan, K., Ling, Z., Liu, K., Yang, Y., Fan, T.-H., Shen, L., Du, Z., and Chen, J. Mir-bench: Can your llm recognize complicated patterns via many-shot in-context reasoning? arXiv preprint arXiv:2502.09933, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agentcomputer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024a. Yang, S., Kautz, J., and Hatamizadeh, A. Gated delta networks: Improving mamba2 with delta rule. arXiv preprint arXiv:2412.06464, 2024b. Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022a. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022b. Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Yao, S. et al. tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. 13 Benchmark Test-Time Scaling of General LLM Agents Yue, L., Xing, S., Chen, J., and Fu, T. Clinicalagent: Clinical trial multi-agent system with large language model-based reasoning. In Proceedings of the 15th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics, pp. 110, 2024. Zelikman, E., Harik, G., Shao, Y., Jayasiri, V., Haber, N., and Goodman, N. D. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. Zhang, J., Bai, Y., Lv, X., Gu, W., Liu, D., Zou, M., Cao, S., Hou, L., Dong, Y., Feng, L., et al. Longcite: Enabling llms to generate fine-grained citations in long-context qa. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 50985122, 2025a. Zhang, L. et al. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025b. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., et al. Agentic context engineering: Evolving contexts for self-improving arXiv preprint arXiv:2510.04618, language models. 2025c. Zhou, A., Yan, K., Shlapentokh-Rothman, M., Wang, H., and Wang, Y.-X. Language agent tree search unifies reasoning, acting, and planning in language models. arXiv preprint arXiv:2310.04406, 2023a. Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023b. 14 Benchmark Test-Time Scaling of General LLM Agents A. Additional Experimental Results A.1. Agentic Benchmarks Coding We include tasks from SWE-Bench Verified(OpenAI, 2024b) and Terminal Bench for the code domain. SWEBench Verified consists of real-world GitHub issues that require the model to analyze and propose concrete bug fixes, with evaluation performed through automated test pass rates. Terminal Bench assesses models ability to solve problems within terminal environment, requiring not only command-line common sense but also the ability to plan and reason over long user instructions. It examinzes the score by checking whether the model achieves the expected final state. Reason For this domain, we adopt MathHay(Wang et al., 2024a) as our data source. MathHay collects mathematical information grounded in real web documents. For each group of related documents, specific pieces of information are extracted and linked to form query. The benchmark then constructs long-context haystack by inserting these relevant documents into noisy text placed at the beginning, middle, or end of the sequence. This design provides suitable testbed for evaluating the models long-context reasoning ability. One thing to note is that no raw tools are provided in this benchmark. Most reasoning benchmarks provide all necessary information directly within the query and ask the model to perform complex mathematical or scientific derivations. As result, they naturally lack an interactive environment that can deliver feedback, making them unsuitable for multi-turn evaluation. However, we can simulate multi-turn interaction by explicitly prompting the model to refine its own answers over additional turns, effectively creating self-reflective reasoning cycles. key requirement for such evaluation is that the query context must be sufficiently long to allow meaningful iterative improvement. Search For the search domain, we include tasks from BrowseComp(Wei et al., 2025) and WebVoyager(He et al., 2024), aiming to evaluate models ability to locate accurate information within long, evolving contexts. In contrast to Needle-in-aHaystack or purely textual retrieval tasks, agentic search requires the model to reason about what information is needed, determine whether the current context is sufficient to answer the query, and decide when additional search steps are necessary. Mind2Web and WebVoyager focus on everyday web-browsing taskssuch as shopping, navigation, and entertainmentand rely on external language models or task-specific agents to assess model performance. BrowseComp, in contrast, presents challenging, hard-to-find information queries that require multi-step investigation across the web. The models final answers are compared against expert-curated gold references to produce the evaluation score. Tool-use For the tool-use domain, we include tasks from Tau2-Bench(Barres et al., 2025) and MCP-Bench(Wang et al., 2025b), both of which provide extensive tool suites for evaluating models ability to understand, select, and invoke the appropriate tool to solve problem. Tau2-Bench focuses on customer-service scenarios, where the model acts as customersupport agent responding to simulated user queries using set of synthetic tools tailored to the scenario. MCP-Bench, in contrast, offers collection of real-world tools built on the Model Context Protocol (MCP), requiring models to perform dense tool calling and coordinate across multiple tools to complete complex tasks. Both benchmarks require multi-turn interaction with simulated environment, and the complexity and richness of the task specifications make them nearly unsolvable within short context window. While the coding and search domains can be viewed as specific instances of tool usesince they also involve code-execution tools and search APIsthe tool-use domain in General AgentBench significantly broadens the definition of tools. It requires models to precisely control tool parameters, reason about which tool to invoke and when, and plan multi-step interactions based on previous own generation and external feedback. A.2. Detailed Results Comparison Table 3 reports the detailed performance of all evaluated models on both the original domain-specific benchmarks and our General AgentBench setting. These results serve as the data source for Figure 3 and Figure 4. 15 Benchmark Test-Time Scaling of General LLM Agents Table 3. Main Results on General AgentBench. We report performance in the Baseline (B) and General (G) settings. % represents the relative change. The overall average scores (AvgB, AvgG) and the mean relative degradation are averaged across four domains instead of each benchmark. Bold text indicates the best performance, while underlining denotes the second best; red highlights performance degradation. Models Search Code Reason Tool-Call Overall Avg. % % % % AvgB AvgG Avg. % Open-Source GPT-OSS-120B 17.6 12.1 -31.3 16.9 8.5 -49.7 46.7 38.7 -17.1 65.2 45.0 -31.0 Qwen3-235B-A22B 21.7 16.4 -24.4 36.2 22.5 -37.8 45.3 32.0 -29.4 62.6 52.5 -16.1 -4.5 63.3 56.9 -10.1 Qwen3-Next DeepSeek-V3.2 -4.0 61.8 60.1 -2.8 DeepSeek-R1 -1.3 44.7 40.1 -10.3 16.1 19.1 +18.6 22.3 12.3 -44.8 44.0 42.0 34.6 28.6 -17.3 32.3 25.9 -19.8 34.7 33.3 16.6 21.2 +27.7 23.1 10.8 -52.2 47.3 46.7 Proprietary Gemini 2.5-Flash Gemini 2.5-Pro Claude Haiku 4.5 Claude Sonnet 4.5 GPT- -7.3 26.9 26.9 21.1 15.4 -27.0 30.0 17.7 -41.0 60.0 36.0 -40.0 66.0 52.7 -20.2 0.0 61.3 24.0 -60.8 66.1 56.8 -14.1 23.4 21.7 23.6 28.0 +18.6 41.5 36.9 -11.1 54.7 34.7 -36.6 64.5 56.7 -12.1 -1.4 32.0 36.0 +12.5 73.0 60.7 -16.8 26.1 34.7 +33.0 49.2 48.5 0.0 78.3 45.8 -41.5 55.8 39.1 -29.9 45.4 39.3 -13.4 64.0 64.0 36.6 41.5 36.4 40.9 32.9 44.3 44.4 46.1 45.1 60.9 26.1 30.9 32.6 37.0 29.7 30.5 32.4 39.1 45.0 47. -28.7 -25.5 -10.4 -9.5 -9.8 -31.2 -27.2 -15.2 -0.2 -22.7 Table 4. Performance on Traditional Long-Context Benchmarks. We report scores across three established benchmarks focusing on retrieval and single-turn comprehension. Bold indicates the best performance in each category. Models LongBench HELMET MRCR Open-Source GPT-OSS-120B Qwen3-235B-A22B Qwen3-Next DeepSeek-V3.2 DeepSeek-R1 Frontier Gemini 2.5-Flash Gemini 2.5-Pro Claude Haiku 4.5 Claude Sonnet 4.5 GPT-5 47.8 58.3 53.1 50.3 58.3 62.1 63.3 55.3 61.8 64.6 12.9 40.8 26.3 48.0 36. 27.8 63.1 50.4 49.2 51.6 32.8 40.6 27.9 33.2 39.2 67.4 80.0 33.3 35.5 79.1 A.3. Comparison with related long-context benchmarks With unified toolsets alone already approaching 64K tokens, the addition of user queries and multi-turn interaction histories can easily push the total context length to nearly 128K tokens. As result, long-context processing naturally emerges as core capability required for general agents. However, existing long-context benchmarks differ fundamentally from agentic long-context scenarios. As summarized in Table 5, we identify two key dimensions along which prior benchmarks diverge from agentic settings. (1) Context composition. Most existing long-context benchmarks are dominated by long-document question answering (QA), where the interaction paradigm remains static and single-turn. In contrast, agentic contexts are inherently heterogeneous: beyond long documents, they include environment feedback (e.g., tool execution results) and the models own prior decisions accumulated through multi-turn interactions. (2) Long-output reasoning. Other established benchmark categories, such as many-shot in-context learning and summarization, involve long inputs but require only relatively short outputs. Agentic tasks, however, demand sustained reasoning over extended contexts, including plan generation, iterative reflection, and explicit tool-call specifications, often resulting in long and structured outputs. While retrieval-based tasks and recent citation-grounded generation benchmarks partially resemble agentic scenarios, they still lack the multi-turn, interactive dynamics essential to true agentic settings. These fundamental differences suggest that performance measured on prior non-agentic long-context benchmarks does not directly reflect model behavior under agentic interaction. 16 Benchmark Test-Time Scaling of General LLM Agents Figure 9. Pairwise correlation between static long-context benchmarks and agentic domains. All denotes the average performance across the search, code, reasoning, and tool-use domains. A.4. Transferability of Long-Context Abilities from Static to Agentic Benchmarks Long-context processing is fundamental requirement for general agents, as unified toolsets and accumulated multi-turn interactions can rapidly extend the effective context length during real-world use. However, it remains unclear whether performance measured on existing static, single-turn long-context benchmarks meaningfully transfers to agentic settings, where context evolves dynamically through interaction and decisions must be made sequentially. To study this question, we evaluate models on several representative static long-context benchmarks and examine how their performance correlates with results on General AgenticBench. We choose LongBench, HELMET, and MRCR here. LongBench v2 LongBench v2 is an updated and expanded version of LongBench, designed to evaluate language models under diverse long-context understanding tasks across multiple domains. It covers broad set of settings, including longdocument question answering, multi-document reasoning, summarization, code understanding, and many-shot in-context learning. Compared to the original LongBench, v2 increases both context length and task diversity, and introduces harder examples that stress retrieval accuracy and reasoning robustness under long inputs. Despite its breadth, LongBench v2 remains fundamentally single-turn and static in interaction structure. All necessary information is provided upfront, and models are evaluated on their ability to extract, aggregate, or reason over relevant spans within fixed context window. Outputs are typically short and final, without iterative refinement or environment feedback. As result, LongBench v2 primarily measures long-context comprehension and retrieval, rather than the dynamic decision-making, planning, and self-conditioning behaviors required in agentic multi-turn settings. HELMET HELMET is holistic benchmark for evaluating long-context language models across curated set of real-world and synthetic tasks. It emphasizes robustness, faithfulness, and information utilization under long inputs, and includes task categories such as long-document QA, summarization, citation-grounded generation, and structured information extraction. 17 Benchmark Test-Time Scaling of General LLM Agents Figure 10. Pairwise correlation between models. HELMET is particularly focused on evaluating whether models can correctly attribute claims to source documents and avoid hallucinations when operating over extended contexts. MRCR MRCR focuses on evaluating models ability to perform multi-round coreference and entity tracking under long contexts. The benchmark constructs documents containing repeated, interleaved references to entities across long spans, requiring the model to resolve pronouns, aliases, and implicit references over multiple turns or segments. Tasks are designed to test memory persistence and consistency, particularly in scenarios where earlier mentions are far removed from later queries. Across ten models, we compute Pearson correlations over pairwise absolute performance differences between static longcontext benchmarks and the four agentic domains. As shown in Figure 9, static benchmarks exhibit consistently weak correlation with agentic performance overall, indicating limited transferability from static long-context ability to agentic long-context reasoning. moderate correlation is observed between MRCR and the reasoning domain, which is expected: reasoning tasks primarily involve extracting and computing over long documents without tool interaction, closely aligning with the characteristics of MRCR. Other agentic domains show substantially lower alignment. In particular, coding and tool-use exhibit minimal correlation with static benchmarks, suggesting that agentic performance depends not only on longcontext comprehension, but also on dynamic decision-making and precise execution. These results highlight fundamental gap between static, single-turn long-context evaluation and the requirements of realistic agentic settings. We further provide correlations across different models on the General AgenticBench as shown in Figure 10. B. Attention Behavior Analysis In sequential test-time scaling, we observe that Qwen3-Next demonstrates weaker scaling potential compared to other models, particularly its full-attention counterpart, Qwen3-235B-A22B. Although these models differ in training data composition Benchmark Test-Time Scaling of General LLM Agents Table 5. Taxonomy of Existing Long-Context Benchmarks. Unlike static evaluation paradigms, Agentic LongBench introduces multi-turn dynamics and sequential reasoning over expansive contexts. Task Category Sub-category Representative Benchmarks / Papers Long Document QA Single/Multi-Doc Understanding NarrativeQA (Koˇcisk`y et al., 2018), 2WikiMultihopQA (Ho et al., 2020), Loong (Wang et al., 2024b) Long Dialogue Understanding MeetingBank (Hu et al., 2023), CharacterChat (Tu et al., 2023) Code Understanding RepoBench (Liu et al., 2023a), CrossCodeEval (Ding et al., 2023) Structured Data Understanding Summarization Global Information Aggregation Many-Shot Retrieval In-Context Learning Key-Value Retrieval TabFact (Chen et al., 2019), WikiTableQuestions (Kweon et al., 2023), LongBench (Bai et al., 2024) (L-Data) GovReport (Huang et al., 2021), Multi-News (Fabbri et al., 2019), ZeroSCROLLS (Shaham et al., 2023) MIR-Bench (Yan et al., 2025), Many-Shot ICL (Agarwal et al., 2024) Needle In Haystack (Kamradt, 2023), RULER (Hsieh et al., 2024), MRCR (OpenAI, 2025b), BABILong 2024 Retrieval-Augmented Gen (RAG) RAGBench (Friel et al., 2024), RGB (Chen et al., 2024a), RECALL (Liu et al., 2023c) Reranking Passage Reranking BEIR (Thakur et al., 2021), MTEB (Muennighoff et al., 2023) (Reranking Selection) Citation Gen Generation with Citations ALCE (Gao et al., 2023) (ASQA/ELI5), LongCite (Zhang et al., 2025a) Agentic Multi-turn Reasoning SWE-Bench (Jimenez et al., 2023), BrowseComp (Wei et al., 2025), OSWorld (Xie et al., 2024), General AgentBench (This Work) and parameterization, we analyze this discrepancy primarily from the perspective of the attention mechanismthe most salient architectural difference between them. We first introduce our analysis methodology, followed by empirical results and key findings. B.1. Extracting Top-K Attention Tokens for Reasoning Behavior Our attention analysis is conducted on inference trajectories collected from General AgentBench. For each benchmark domain, we randomly sample 25 trajectories generated by the model under analysis. Because attention patterns depend strongly on the specific text region being examined, we follow the framework of Jin et al. (2025) to extract reasoning-behavior sentences from each trajectory. These sentences correspond to critical decision-making steps, where the model actively reasons over accumulated context. For each reasoning sentence, we iterate over all tokens within the sentence. For every token, we compute and store its attention distribution over the preceding context. We then average the attention scores across tokens in the reasoning sentence, select the top-K attended tokens (K = 128), and record their token indices. This procedure is applied to all layers and attention heads. We evaluate attention behavior using two metrics: Mean Attention Distance. For each of the top-K attended tokens, we compute its token distance from the reasoning sentence and weight it by the corresponding attention score. This metric measures how far back the model attends when making key decision, effectively quantifying the models effective contextual view. Top-K Overlap. We measure the overlap of top-K attended tokens through intra-layer overlap (across heads within the same layer) and inter-layer overlap ( across different layers). Higher overlap indicates that different heads or layers attend to similar contextual tokens, suggesting reduced functional differentiation. For Gated DeltaNet, which does not explicitly expose standard attention weights, we adopt mathematically equivalent reconstruction procedure to recover the effective top-K contributing tokens and compute the corresponding distance statistics. 19 Benchmark Test-Time Scaling of General LLM Agents Figure 11. Comparison of attention behaviors under the General AgentBench setting. Top: Qwen3-235B (full attention). Bottom: Qwen3-Next (hybrid linear attention). B.2. Results and findings In the leftmost panel of Figure 11, we visualize the mean attention distance for each head across layers. Full-attention models exhibit consistently larger mean distances, with only minor exceptions in early layers and around layer 70. In most layers, the majority of heads attend to long-range context, while small subset focuses on local patterns. This behavior is also observed in the gated full-attention layers of Qwen3-Next (appearing every four layers). These results suggest that full attention maintains broader effective contextual view than linear attention, consistent with the convolution-like receptive field constraints imposed by DeltaNet-style linear attention (Yang et al., 2024b). The two rightmost panels present the top-K token overlap statistics. Intra-layer overlap. Full attention exhibits characteristic V-shaped curve: middle-layer heads attend to more diverse patterns, while later layers converge toward similar tokens, reflecting increased certainty near the final decision stage. In contrast, linear attention lacks clear structural pattern and shows higher average intra-layer overlap, suggesting reduced head specialization. Inter-layer overlap. Full attention displays gradual low-to-high trend across depth, indicating that adjacent layers share similar functional roles, while functional divergence accumulates progressively with depth. In linear-attention models, DeltaNet layers show very low overlap with gated full-attention layers. Although DeltaNet layers exhibit high overlap across distant layers (indicating homogeneous behavior), the full-attention layers preserve their characteristic inter-layer structure. Overall, our findings indicate that linear attention demonstrates weaker functional differentiation across heads and layers, along with reduced long-context utilization in agentic reasoning tasks, compared to full-attention mechanisms. 20 Benchmark Test-Time Scaling of General LLM Agents C. Estimated Cost C.1. Pricing and accounting. We report an estimated API budget for reproducing our evaluation under three settings: (i) general (default context) evaluation of each model on each domain once, (ii) parallel scaling that samples multiple independent trajectories per query, and (iii) sequential scaling that extends the interaction horizon over multiple steps. All prices are normalized as USD per 1M tokens and are taken from the corresponding model providers at the time of running the experiments. Due to uncertainty about the model providers caching mechanism and evaluation intervals, we will use only the input unit price in our estimates, even if the provider supplies cache unit price. We compute cost by aggregating token usage from execution logs, using the provider-reported token accounting: input tokens (prompt + tool outputs + intermediate messages fed back to the model), and output tokens (model-generated tokens). For each model, the total cost is estimated as: where pin, pout denote the unit prices, and Tin, Tout are the measured token counts. Cost = pin Tin 106 + pout Tout 106 , C.2. Model cost Table 6 lists the unit API prices used in our cost estimation. Prices for input, cached input, and output are reported in USD per 1M tokens. Tables 79 summarize the aggregated evaluation cost per model under parallel scaling, sequential scaling, and the general (default context) setting, respectively. Each entry corresponds to running the full benchmark split of that dataset/domain under the specified protocol and then summing costs across all queries. Model Input Cached Input Output Gemini-2.5-Flash Gemini-2.5-Pro GPT-5 Claude-Haiku 4.5 Claude-Sonnet 4.5 gpt-oss-120B Deepseek-R1 Deepseek-V3.2 Qwen3-235B Qwen3-Next 0.30 1.25 1.25 1.00 3. 0.15 1.35 0.28 0.22 0.15 0.03 0.125 0.125 0.50 3.75 0.075 0.11 2.50 10.00 10.00 5.00 15.00 0.60 5.40 0.42 0.88 1.50 Table 6. Unit API prices (USD per 1M tokens) used in our cost estimation. Model Search MathHay SWEBench MCPBench Tau2Bench TerminalBench Total Gemini-2.5-Flash DeepSeek-R1 DeepSeek-V3.2 Qwen3-235B Qwen3-Next Total $193 $5188 $369 $1254 $25.3 $7028 $70.9 $157 $38.8 $41.4 $9.52 $317 $12719 $3675 $7.80 $1286 $3.38 $ $122 $492 $88.4 $120 $21.2 $843 $62.2 $202 $54.4 $55.5 $14.9 $389 $826 $1505 $412 $409 $154 $13993 $11218 $970 $3166 $ $3307 $29576 Table 7. Cost for evaluating models under parallel scaling setting (USD) 21 Benchmark Test-Time Scaling of General LLM Agents Model Search MathHay SWEBench MCPBench Tau2Bench TerminalBench Total Gemini-2.5-Flash DeepSeek-R1 DeepSeek-V3.2 Qwen3-235B Qwen3-Next Total $5588 $1267 $902 $1370 $ $9568 $369 $654 $333 $238 $219 $1814 $2024 $892 $191 $436 $392 $3935 $568 $931 $79.1 $716 $ $2445 $852 $1088 $52.3 $162 $251 $2405 $1870 $782 $356 $689 $529 $11271 $5614 $1913 $3610 $1985 $ $24392 Table 8. Cost for evaluating models under sequential scaling setting (USD) Model Search MathHay SWEBench MCPBench Tau2Bench TerminalBench Total Gemini-2.5-Pro GPT-5 Claude-Haiku-4.5 Claude-Sonnet-4.5 OpenAI-oss-120B DeepSeek-V3.2 Qwen3-Next Total $47.5 $87.5 $304 $1248 $1.44 $96.8 $6.05 $1791 $18.6 $14.2 $10.9 $40.9 $1.49 $10.0 $2.42 $98. $3253 $146 $312 $926 $0.52 $1.92 $0.88 $4640 $31.3 $21.0 $29.3 $129 $0.50 $22.0 $5.51 $239 $15.2 $12.4 $14.1 $52.0 $1.35 $14.0 $3.60 $ $203 $60.9 $106 $376 $0.40 $102 $38.2 $3569 $342 $776 $2772 $5.70 $247 $56.7 $886 $7768 Table 9. Cost for evaluating models under general (default context) setting (USD) D. General AgentBench Implementation Details D.1. Tool Management Tool Registration The General AgentBench follows the Model Context Protocol (MCP) architecture with Host-ClientServer design. During server connection, the BenchmarkHost creates transport connections (STDIO/HTTP), initializes ClientSession, and discovers tools via list tools(). All tools are registered to global tool to client routing map. Tool Schema Each tool follows OpenAI function-calling format with name, description, and parameters (JSON Schema), as shown in Box D.1. Tools use Bedrock-compatible naming requirements ([a-zA-Z0-9 -]+): MCP-Bench format uses ServerName tool name (e.g., BioMCP think), while Tau2 uses domain tool name (e.g., airline book reservation). The complete toolset in Host contains 301 tools across 35 servers, including BioMCP (34), Scientific Computing (26), Medical Calculator (22), NASA Data (21), NixOS (18), Unit Converter (16), tau2-retail (15), tau2-airline (14), tau2-telecom (13), and others. An example of tool schema: get-collection-info function from Huggingface server { \"type\": \"function\", \"function\": { \"name\": \"Hugging_Face__get-collection-info\", \"description\": \"Get detailed information about specific collection\", \"parameters\": { \"type\": \"object\", \"properties\": { \"namespace\": { \"type\": \"string\", \"description\": \"The namespace of the collection (user or organization)\" }, \"collection_id\": { \"type\": \"string\", \"description\": \"The ID part of the collection\" } }, \"required\": [ \"namespace\", \"collection_id\" ] 22 Benchmark Test-Time Scaling of General LLM Agents } } } Description Compression and Minimal Strategy To reduce context consumption, we implement multi-level compression. The compress-tools option truncates descriptions to target number of characters and removes parameter defaults, achieving 18.6% token reduction. If the description of the first sentence is truncated in the middle, we will keep the first sentence for tool loading.. For self-choice evaluation, we adopt the minimal-tools strategy: converting JSON schema to plain text format (tool name(params): short desc), retaining only tool names, 50-character descriptions, and parameter names. This achieves 90.1% token reduction, saving approximately 70K tokens. D.2. Benchmark Integration Details User Simulator Implementation in Tau2Bench Tau2Bench requires multi-turn conversational interactions between the agent and simulated user. We implement this through Tau2UserSimulatorAdapter, which wraps the original Tau2 UserSimulator. During each conversation turn, the adapter converts the agents response to Tau2s AssistantMessage format, calls generate next message(message, state) to obtain the user response, and maintains conversation state across turns. User-side tool execution (e.g., checking order status) is routed through MCP internal callbacks, which ensures proper environment initialization. Docker Environment Interaction Both Terminal Bench and SWEBench employ Docker Container Bridge Mode for isolated task execution. The MCP servers run as persistent host processes, interacting with task containers via docker exec. Public tools exposed to agents include execute bash, read file, write file, and finish(SWE-Bench). Each container receives unique UUID-suffixed project name to support parallel experiments. This MCP Process Mode design avoids server restart overhead across tasks while maintaining complete environment isolation through Docker. Evaluators The General Agent system adopts native evaluator delegation strategy, directly invoking each benchmarks original evaluation code rather than reimplementing evaluation logic. For Tau2Bench, we compute rewards as the product of environment state matching (DB CHECK), action sequence validation, and communication checks. SWEBench and Terminal Bench use Docker Bridge Mode: MCP servers run as persistent host processes, executing pytest harnesses in task containers via docker exec, then parsing results using toolsets in original benchmark implementation. Search benchmarks (BrowseComp, Mind2Web, WebVoyager, GAIA) use their original eval scripts with LLM-based semantic equivalence or rubric evaluation. All evaluators produce binary rewards (0/1) except MCPBench (continuous 0-1). E. Detailed Prompts Universal Agent Prompt The agent system prompt instructs the model to solve diverse problems using tools or reasoning. It emphasizes careful tool selection, avoiding redundant calls, and building on previous results. For Tau2-Bench tasks, domain-specific policy documents are appended under Policy section, loaded from policy markdown file. Other benchmarks use the base prompt without policy appended. Universal Agent System Prompt You are helpful AI agent that can solve wide variety of problems, including searching the web for information, writing and running code, performing calculations and logical reasoning, and interacting with external services. You can choose to use tools when helpful, or solve problems through your own reasoning. ## Tool Selection - CAREFULLY read tool names and descriptions before selecting - Choose tools that are DIRECTLY relevant to the current task - AVOID REDUNDANT CALLS: Dont repeat successful tools unless specifically needed - If no tools are needed, solve the problem through reasoning alone ## Execution Strategy - Analyze the task to understand what information or actions are needed - Decide whether to use tools, reason independently, or combine both approaches - If using tools, identify which are most relevant based on their names and descriptions - BUILD ON PREVIOUS RESULTS: Use information from previous tool calls - If tool returns an error, try an alternative approach or tool ## Response Guidelines 23 Benchmark Test-Time Scaling of General LLM Agents - Follow any policies (<policy>) or constraints provided in the task - When you have gathered sufficient information, provide clear final answer - If task cannot be completed with available tools, try solving it through reasoning - Do not guess or make up information - only use data from tool results or verified reasoning Point-wise Self-choice Prompt This prompt is used in point-wise self-choice. Our prompt first positions the model as Lead Researcher auditing agent behavior. Our prompt prioritizes result correctness over reasoning qualityeven flawed processes leading to correct answers receive Correct judgments. The model outputs structured < judgment > Correct/Wrong < /judgment > tags, extracted via regex for downstream evaluation against ground truth scores. Point-wise Self-choice System Prompt You are Lead Researcher specializing in auditing autonomous AI agents. and objective. environment observations) and the original Task Description. You will be provided with trajectory (a history of an agents thoughts, tool calls, and Evaluate whether the answer is correct. You are rigorous, detail-oriented, First, look strictly at the final answer or final state. Second, review the steps. Did the agent make logical errors, hallucinate, or misuse tools? Does it satisfy the users request? Your primary job is to determine if the Final Answer is correct. accidental discovery, wrong logic) but the Final Answer is factually correct, you MUST mark the answer as Correct. observations to support your critique. You should note the process error separately. Meanwhile, dont trust the answers too much. You must cite specific evidences or Even if the reasoning was flawed (e.g., After detailed thinking, return your final judgment within the tag <judgment>YOUR FINAL JUDGMENT</judgment>. You can only choose from Correct or Wrong. ## Available Tools: The agent had access to the following tools: {{ Standard Tool Schema}} -- ## Task Description: --- {{Task Description}} ## Trajectory: --- {{Trajectory}} Please evaluate whether the agents final answer correctly addresses the task. <judgment>YOUR FINAL JUDGMENT</judgment>. You can only choose from Correct or Wrong. Provide your judgment in Pair-wise Self-choice Prompt This prompt is used in pair-wise self-choice. Our prompt compares two trajectories to identify the superior answer. The model evaluates final answers and reasoning quality for both, outputting < ranking > 1/2 < /ranking > to indicate preference. We adopt the bump-sort algorithm in pair-wise self-choice, which performs 1 pairwise comparisons across passes and claims the best response across all responses. Pair-wise Self-choice System Prompt You are Lead Researcher specializing in auditing autonomous AI agents. and objective. consisting of history of an agents thoughts, tool calls, and environment observations). trajectory produced the better answer. You will be provided with the original Task Description and TWO trajectories (each You are rigorous, detail-oriented, Evaluate which First, look strictly at the final answer or final state of each trajectory. request? Second, review the steps of each trajectory. tools? Did the agent make logical errors, hallucinate, or misuse Does it satisfy the users Your primary job is to determine which Final Answer is better. accidental discovery, wrong logic) but the Final Answer is factually superior, you MUST mark that trajectory as the better one. observations to support your critique. You should note the process errors separately. Even if the reasoning was flawed (e.g., You must cite specific evidences or After detailed thinking, return your final preference within the tag <ranking>YOUR FINAL PREFERENCE</ranking>. ## Available Tools: The agent had access to the following tools: You can only choose 1 or 2. {{ Standard Tool Schema}} -- ## Task Description: --- {{Task Description}} ## Trajectory 1: {{Trajectory 1}} 24 Benchmark Test-Time Scaling of General LLM Agents --- ## Trajectory 2: {{Trajectory 2}} --- Please evaluate which trajectory produced the better final answer. preference (1 or 2) in <ranking>YOUR FINAL PREFERENCE</ranking>. Provide your analysis and return your"
        }
    ],
    "affiliations": [
        "Language Technologies Institute, School of Computer Science, Carnegie Mellon University",
        "Meta"
    ]
}