{
    "paper_title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models",
    "authors": [
        "Kaiser Sun",
        "Fan Bai",
        "Mark Dredze"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the model's parametric knowledge. We propose a diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs."
        },
        {
            "title": "Start",
            "content": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models"
        },
        {
            "title": "Fan Bai Mark Dredze",
            "content": "Center for Language and Speech Processing Johns Hopkins University Baltimore, MD USA fbai3@jh.edu hsun74@cs.jhu.edu mdredze@cs.jhu.edu 5 2 0 2 ] . [ 1 5 8 4 6 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models frequently rely on both contextual input and parametric knowledge to perform tasks. However, these sources can come into conflict, especially when retrieved documents contradict the models parametric knowledge. We propose diagnostic framework to systematically evaluate LLM behavior under context-memory conflict, where the contextual information diverges from their parametric beliefs. We construct diagnostic data that elicit these conflicts and analyze model performance across multiple task types. Our findings reveal that (1) knowledge conflict has minimal impact on tasks that do not require knowledge utilization, (2) model performance is consistently higher when contextual and parametric knowledge are aligned, (3) models are unable to fully suppress their internal knowledge even when instructed, and (4) providing rationales that explain the conflict increases reliance on contexts. These insights raise concerns about the validity of model-based evaluation and underscore the need to account for knowledge conflict in the deployment of LLMs.1,"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) are capable of generalizing across wide range of tasks by blending distinct skill sets. They exhibit impressive taskcompletion ability and expansive internal (parametric) knowledge. These abilities enable LLM to perform tasks such as copy editing and summarization, while parametric knowledge facilitates open-ended question answering on diverse domains (Cao et al., 2022). Some tasks require both skills, such as answering questions with the support of provided context. What happens when conflict arises between the context provided to complete task and the models 1Code available at [Anonymous]. 2The title, in other words, reflects that \"Dasein is its past in the way of its own being\" (Heidegger, 1962). Figure 1: Illustration of different evidence types. In the rest of the manuscript, model internal knowledge will be referred to as No Contradiction (NC). parametric knowledge? Knowledge conflicts, which arise from contradictions between the supplied context and model memory, place models task-oriented behavior at odds with its stored knowledge. Consider Retrieval-Augmented Generation (RAG) scenario (Lewis et al., 2020), where the model is expected to answer questions based on retrieved documents. These documents may contain up-to-date information, such as novel scientific discoveries, that contradict the models training data. Assessing the feasibility of scientific idea, for instance, requires the integration of both established and novel knowledge. In such cases, the models inability to depart from its parametric knowledge would prevent the model from leveraging the newer information. Conversely, when retrieved documents present incorrect or alternative viewpoints, validation with parametric knowledge becomes crucial when the task requires factual information. Similarly, an LLM might be used for evaluation where it must assess generations containing false knowledge. These scenarios introduce conflicts between the models internal knowledge Figure 2: Overall diagnostic data creation flow. The lower portion is zoom in of Evidence Creation step. and its ability to identify, extract, or summarize the content from external documents. Despite the increasing use of LLMs for both task execution and evaluation, little research has systematically investigated model behavior in the presence of such conflicts. In this work, we study how the models resolve contradictions between the external context and their parametric knowledge under various tasks that require different types of knowledge. To elucidate model behavior across varying degrees of knowledge conflict and under different tasks, we create diagnostic texts (evidence) that introduce calibrated contradictions with the models parametric knowledge. The degrees of knowledge conflict span three conditions: conform to the models knowledge (no contradiction), plausible contradiction, and implausible contradiction, as shown in Figure 1. We then evaluate model behavior across tasks ranging from knowledge-free to knowledgeintensive, quantifying the impact of knowledge conflict by measuring accuracy across different task settings. The overall diagnostic data creation flow is presented in Figure 2. Our analysis yields the following insights. 1) Knowledge conflict has minimal impact on tasks that do not require knowledge utilization (4.1). 2) For knowledge-centric tasks, regardless of the type of knowledge required, performance is consistently higher when contextual and parametric knowledge are aligned (4.1). 3) Models are unable to fully suppress their internal knowledge, even when explicitly instructed (4.1). 4) Providing rationale that explains the nature of the conflict increases models reliance on contextual information (4.2). These results shed light on both the application and evaluation of language models: Calibrating the right amount of context utilization based on the requirements of target task can enhance models performance. Critically, we find that when using language models as evaluators or judges (Zheng et al., 2023; Liu et al., 2023; Ru et al., 2024; Chen et al., 2025), their internal knowledge introduces systematic bias (4.3), jeopardizing the validity of model-based evaluation."
        },
        {
            "title": "2 Related Work",
            "content": "Knowledge Conflict Xu et al. (2024) classify knowledge conflict into three categories: contextmemory conflict, inter-context conflict (contradictory evidence among retrieved passages), and intramemory conflict (inconsistent parametric beliefs), among which we focus on context-memory conflict. Several benchmarks have been introduced to study inter-context conflict. Early work synthesizes contradictions by swapping named-entity answers in QA corpora (Longpre et al., 2021). More recent datasets derive conflicts from naturally occurring sources: Wan et al. (2024) construct ConflictQA using realistic search engine queries. Hou et al. (2024) adapts editorial disputes from Wikipedia to create WikiContradict, which measures how language models respond to contextual conflicts. Hagström et al. (2024) introduce DRUID, benchmark originating from fact-checking sites containing complex forms of conflict designed to evaluate context utilization in retrieval-augmented generation. The aforementioned benchmarks are proposed to examine inter-context conflict, while we utilize the knowledge in these datasets to obtain LLMs parametric knowledge in order to study the still-under-explored context-memory conflict. The Use of Context Retrieval-Augmented Generation (RAG) ameliorates the limitation of LLMs knowledge being inherently static by injecting retrieved evidence into the models input, thereby alleviating hallucinations and supplying new information. However, the generation process within RAG remains modulated by the generators entrenched priors, the parametric knowledge that leads to dogmatic behavior and inefficient utilization of the retrieved context (An et al., 2025; Hagström et al., 2024). When the given input disagrees with models parametric knowledge, context-memory conflict emerges. Jin et al. (2024) shows that language models exhibit the Dunning-Kruger effect, i.e., often clinging to incorrect internal beliefs rather than factual external evidence, and proposes Conflict-Disentangle Contrastive Decoding to recalibrate that bias. Tan et al. (2024) find that when the retrieved and generated contexts disagree, state-of-the-art LLMs disproportionately trust the self-generated text, due to its high similarity to the query than retrieved passages, resonating with the findings in Liu et al. (2025) that pointwise mutual information between the context and question poses high correlation with the generation performance. Techniques have been proposed to coerce stronger context utilization (Shi et al., 2024; Wang et al., 2025), yet indiscriminately suppressing parametric knowledge may not always be salutary, as explained in 1. We argue that the use of parametric and contextual knowledge should not be framed as an either-or decision, and we further study the effect of context-memory knowledge conflict on tasks that require different degrees of knowledge reliance. We create diagnostic data with fine-grained conflict levels and tasks that demand varying degrees of knowledge integration. This allows us to characterize model behavior across wider conflict and task spectrum rather than single, monolithic setting."
        },
        {
            "title": "3 Context-Memory Conflict Creation",
            "content": "We propose an automated framework for constructing diagnostic instances tailored to each model to introduce contradictions between the input and the models latent beliefs. Figure 2 illustrates an overview of the data construction pipeline. The process begins with identifying the pre-existing knowledge within language model (Parametric Knowledge Collection). To achieve this, we leverage existing question answering datasets that have two or more acceptable answers to one question (Wan et al., 2024; Hou et al., 2024) to elicit the models parametric knowledge, for which multiple variants of prompts are included (Appendix A). piece of knowledge is considered part of the models internal belief only if the model consistently aligns with the perspective in single answer across all prompt variations under greedy decoding, while rejecting conflicting alternatives. With the models internal knowledge established, the framework generates contradictory statements based on spectrum of conflict levels (3.1, Evidence Creation). Leveraging these controlled contradictions, we build diagnostic datasets that consist of tasks requiring contextual knowledge, parametric knowledge, or combination of both (Task-Annotation). Since different models possess different parametric knowledge, the exact knowledge included in the diagnostic datasets differs by model. Each instance is then reviewed by an LLM to verify the correctness of its task type annotation (Validation)."
        },
        {
            "title": "3.1 Evidence Creation",
            "content": "The cognitive science literature suggests that humans address conflict between their knowledge and new information through cognitive judgment of the rationality of the concept (Posner et al., 1982; Vosniadou and Brewer, 1992). We adopt this idea and utilize the notion of plausibility to study whether language models similarly perceive knowledge conflict. Plausibility is defined as at minimum, the individual is willing to consider an alternative strategy because the recommendation is understood, coherent, and relatively simple and because the proposal is deemed viable and logical alternative to solve the specific challenge at hand (Posner and Strike, 1992). Plausibility can be used to measure how likely human would accept new information when conflict exists. We quantify this notion by decomposing plausibility into two aspects: the content aligns with real-world or commonsense knowledge and does not violate basic logical principles. For example, suppose the model believes that grooves on the surface of Phobos, moon of Mars, were caused by boulder from an asteroid ejection. The conflicting statement that it was caused by gravitational pull from Mars is plausible because it conforms to common-sense knowledge. However, the idea that it was caused by dance party is of low plausibility. If the model believes that the current prime minister of Canada is Mark Carney, the statement that Chrystia Freeland (current Deputy PM) is the prime minister is plausible, but the statement that Moose is the new PM is not because it violates basic logical principles that PMs must be humans. With this in mind, we define three types of instances based on their alignment with the models internal knowledge (Figure 1): No Contradiction (NC), High Plausibility Contradiction (HPC), Low Plausibility Contradiction (LPC). The evidences are created following Figure 2. Starting with an original dataset Dorig = {(qi, {ai1, ai2, ...}, {ci1, ci2, ...}), [1, ]}, where qi, ai, ci corresponds to the question, answer, and context (supporting passage) of the i-th instance, is the size of dataset Dorig. The subscript after represents the j-th answer/context of the question qi, as each question qi may have multiple acceptable answers. Since Dorig, coming from ConflictQA and WikiContradict, contains realistic and factually verified answers and contexts, we treat these existing answers as highly plausible. When an answer aij from the original dataset contradicts the model-aligned answer aik in an NC instance, we designate it as an HPC answer (aHPC = aij), and its corresponding context as an HPC passage (pHPC = cij). The contradicting answer aik therefore becomes the NC example, namely, aNC = aik and pNC = cik. To generate additional variants, we pass the passage pNC into an editor LLM, which is prompted to modify or rewrite it to achieve specified levels of plausibility and explanatory depth. Specifically, the editor model is instructed to rewrite the passage and degrade the plausibility while preserving contradiction to construct LPC and answer aLP passage pLPC . At the end of evidence creation, two LLMs were used to check (1) whether the passage-answer combination (pLPC ) correctly answers the original quesi tion qi; and (2) whether the generated context pLPC , aLPC i i is truly low-plausibility through fact checking process."
        },
        {
            "title": "3.2 Task Annotation",
            "content": "To study how models behave on tasks that require different levels of knowledge utilization, we define four tasks that differ in the extent and source of knowledge required. Examples of each task are provided in Appendix C. Knowledge Free (KF) tasks do not require access to either contextual or parametric knowledge. We use extractive question answering as KF task: the model is expected to extract one-sentence answer directly from the context pi without engaging in reasoning, paraphrasing, or drawing upon prior knowledge. For example, the expected output in Figure 1 should be Grooves were formed during massive dance party held by the witch among tiny alien creatures,\" which requires no additional change from the context. The list of acceptable extractions is obtained and verified by GPT-4o (OpenAI, 2024). In the evaluation setting, the output is treated as correct as long as the extracted sentence matches one of the acceptable extractions. Contextual Knowledge (CK) tasks require the model to gather relevant knowledge from the given context, and usually require some paraphrastic or inferential capability, as the answer may not appear verbatim in the input. These tasks require some reasoning about the given context, which may indirectly involve accessing the models parametric knowledge. In experiments, the model is given one of the passages in {pNC } and is expected to answer questions only based on the contextual knowledge, which may not agree with its parametric knowledge. , pHPC , pLPC Parametric Knowledge (PK) tasks may present inputs that include distracting or irrelevant context. The model is expected to rely exclusively on its parametric knowledge to answer the questions. In experiments, the model is given passages that support or contradict its parametric knowledge as input, and the model is always expected to provide the answer aNC . Parametric-Contextual Knowledge (PCK) tasks explicitly ask the model to integrate both its internal knowledge and the external context. This setup reflects scenarios akin to scientific reasoning, where individuals must synthesize background knowledge with newly presented information (e.g., recently read paper). In execution, the model will be given passage that contradicts its own knowledge, and is expected to output both perspectives from the context and its parametric knowledge. Retrieval Augmented Generation (RAG) simulates the standard RAG setting in prior work, where models are not explicitly instructed to prioritize parametric or contextual knowledge. The model will be given two passages and is expected to answer the question based on both passages. Models are expected to acknowledge the conflict and discuss each potential answer individually. This setting naturally exposes the model to conflicts in both the context and memory. The annotations for CK, PK, PCK, and RAG tasks derive directly from the original datasets on which our framework is built. These task types primarily differ in the number of valid answers expected and the nature of knowledge the model should rely on. In CK and PK tasks, the model is expected to give only one answer or provide single correct answer, grounded either in the provided context or in its internal (parametric) knowledge, respectively. In PCK and RAG tasks, the model is expected to clarify that both aNC and the other answer are possible and explain the contradiction between the two answers. One of the original datasets we use employs model-based evaluation to judge the correctness of free-text answers (Hou et al., 2024). However, we observed that this evaluation method is susceptible to knowledge conflict, leading to inaccurate evaluations. We explore this issue further in 4.3. Therefore, we modify the non-extractive tasks to be multiple-choice questions. Each instance presents four answer options; the model must first generate an explanation, then select the most appropriate answer. We report the exact match accuracy to assess the performance of the target model. To obtain high-quality texts, we use GPT-4o as the base model to create evidence and validate the diagnostic data. Then, we analyze the instruction-tuned version of Mistral-7B (Jiang et al., 2023), OLMo2-7B (OLMo et al., 2024), and Qwen2.5-7B (Qwen et al., 2025), all of which are widely used open-weight models that represent diverse training paradigms. The resulting diagnostic data is composed of 2,893 instances for Mistral-7B, 177 instances for OLMo, and 6,217 instances for Qwen2.5-7B. Each instance (a) Knowledge Free Task (b) Contextual Knowledge Task (c) Parametric Knowledge Task (d) Parametric-Contextual Knowledge Task Figure 3: Performance of each model on different task types. clear trend of NC > HPC > LPC is shown across models and tasks involving knowledge utilization. includes three different evidence types (NC, HPC, LPC); thus, the resulting task data has three times the number of instances."
        },
        {
            "title": "4.1 Conflict Impairs Model Performance",
            "content": "The performance of each model on each task type and context type is reported in Figure 3. universal trend can be observed: regardless of the tasks, all models suffer when asked to predict on instances that contradict their parametric knowledge. Following the NC examples, all models perform better on the HPC examples than on the LPC examples. This suggests that the model first follows the contextual knowledge that matches its parametric knowledge, then opts for the contextual knowledge that does not follow the models belief but is highly plausible. The model always performs the worst on the that perfect retriever can find all relevant documents, we construct RAG setting in which both model-aligned (NC) and contradictory (HPC or LPC) passages are presented simultaneously in the context. In other words, NC passages are fed together with contradictory passage (HPC/LPC), and the model is expected to answer the question based on both passages in the context. The result is shown in Figure 4. Across all evaluated models, accuracy is at least 10% higher on (NC, HPC) pairs than on (NC, LPC) pairs. This pattern suggests that when faced with competing evidence, models exhibit preference for the passage that appears more plausible, i.e., the one more consistent with real-world knowledge. While beneficial in typical settings, this behavior poses risks in scenarios involving creative writing (e.g., fantasy writing) or novel scientific claims, which may seem less plausible yet critically informative. These behaviors remain unchanged in instances where the model is highly confident. When querying for the models parametric knowledge (parametric knowledge collection in Figure 2), model responses to queries are collected in binary stance format (e.g., yes/no). However, when prompted with free-form generation followed by multiple-choice selection, models do not always achieve perfect accuracy on NC instances (Figure 3). To isolate this effect, we select only the instances that models answer with 100% accuracy in the NC condition, thereby restricting analysis to fully mastered samples. The results, shown in Appendix D, confirm that while the absolute numbers vary slightly, the overall trends observed in the broader dataset persist, confirming our findings in this section."
        },
        {
            "title": "4.2 Rationales for conflict strengthen context",
            "content": "reliance. 4.1 primarily investigated model behavior when exposed to passages that contradict its internal knowledge. When seeing new context contrary to their knowledge, further explanations are more likely to convince human, who would iteratively update their mental model with new experiences (Vosniadou and Brewer, 1992). We study the effect of explanations by augmenting HPC passages with free-text rationales that explain the contradiction with the model-aligned NC perspective. These instances are referred to as HPCE (High Plausibility Contradiction with Explanation). The explanation Figure 4: Performance of model on RAG task when NC contexts are provided with HPC/LPC contexts. All models show preference for plausible contexts. less plausible examples, even when the model is explicitly asked not to incorporate its parametric knowledge. Knowledge conflict degrades performance whenIn CK tasks in Figever knowledge is required. ure 3b, the prompt explicitly instructs the model to ignore its own beliefs and rely solely on the passage. Nevertheless, every model shows clear NC > HPC > LPC performance ordering, indicating that the model still relies on parametric knowledge when it is not supposed to. This aligns with prior works finding that models favor their parametric knowledge more than the given contextual knowledge, thus leading to hallucinations (Jin et al., 2024). In practical terms, this suggests that when the model is expected to refer only to contextual memory, instructions such as ignore prior knowledge are insufficient, as models may weigh them lower than their own store of facts, highlighting the necessity of conflict alleviation methods when such task is expected. This issue, if left untreated, could not only affect RAG performance but also the correctness of model-based evaluation results, which we illustrate in 4.3. Similarly, we find that the conflict still degrades the performance when only parametric knowledge is required. Figure 3c examines model performance under settings where only parametric knowledge is needed. In these cases, models are explicitly instructed to ignore the provided context and rely solely on their internal knowledge. Despite this, we observe consistent degradation in accuracy when the input includes conflicting contextual passages (either HPC or LPC) compared to NC instances. This suggests that the model is susceptible to distraction, even when instructed otherwise, indicating an incomplete disentanglement between knowledge conflict and instruction following. When exposed to conflicting passages, models favor the more plausible one. Hypothesizing Figure 5: Performance on high plausibility contradiction instances with (HPCE) and without (HPC) explanations. generation protocol and an example are detailed in Appendix E. Rationale in context leads to less parametric knowledge usage. Our findings indicate that in tasks requiring only contextual reasoning, the presence of rationales has negligible effect on performance. This resonates with the findings in Alazraki et al. (2025), where they identify that the presence of rationales from incorrect answers does not necessarily benefit the model to learn the correct mathematical reasoning, which can be regarded as contextual knowledge task in our setting. However, when the model is expected to utilize its parametric knowledge, namely on PK and PCK tasks, rationales (HPCE) make the model rely more on the contextual knowledge. Such behavior presents both advantages and limitations. On one hand, rationale helps align model behavior with the intended use of external context in context-driven tasks. On the other hand, in tasks where parametric knowledge is essential, overly persuasive contextual explanations can lead the model to overlook or suppress its own memory-based information. biased and therefore unreliable. To answer this question, we examine the free generation version of our diagnostic framework and perform small-scale human annotation on 50 examples. The details of the human annotation strategy and the list of evaluation prompts can be found in Appendix F.1. We find that the averaged Cohens κ (Landis and Koch, 1977) between the evaluator model (GPT-4o) and human annotator is 0.79 (substantial agreement), which is significantly lower than κ = 0.90 (almost perfect agreement) between the human annotators. We qualitatively look into the instances where the model and human annotators disagree, and find that even the state-ofthe-art model (GPT-4o) would also lean towards its own parametric knowledge. An example of such an instance is presented in Figure 6, where GPT4o fails to adhere to the instruction and refuses to grade an output that is contextually correct but factually incorrect as correct. Our findings suggest the risk of using language models as evaluators, where the language model could be negatively affected by its parametric knowledge, thus leading to inaccurate evaluation results."
        },
        {
            "title": "4.3 Knowledge conflict leads to unreliable",
            "content": "LLMs as the judge."
        },
        {
            "title": "5 Discussion",
            "content": "LLMs have been increasingly used as evaluators in settings where generated responses must be judged along several criteria, including whether the response contains the same information as ground truth answer (Zheng et al., 2023; Liu et al., 2023; Ru et al., 2024; Chen et al., 2025). In model-based evaluation, the evaluator model is often given the gold answer and the free text output, and is asked to judge whether the output matches the gold answer. This naturally leads to question: since modelbased evaluation is similar to our contextual knowledge task (CK), will the model score instances as incorrect when they contradict the models internal knowledge? If the model utilizes its own parametric knowledge when acting as judge, even when told to do so, then the evaluation behavior will be Disentangling memory and instruction following. In many NLP studies, knowledge is usually framed as factual or propositional content (Lewis et al., 2020; Chen et al., 2022; Meng et al., 2022a; Mallen et al., 2023). We loosely define extractive QA as knowledge-free task. However, in broader epistemological sense, knowledge broadly refers to an awareness of facts, situations, or skills. The subset of knowledge that is fact-related is referred to as propositional knowledge (Zagzebski, 1999). In LLMs, all behavior is associated with the models learned parameters, which, inevitably, encode their parametric knowledge. Prior work attempted to locate and modify specific factual beliefs embedded within models parameters (Meng et al., 2022a,b; Armengol-Estapé et al., Question Context Gold Answer Model Output Evaluator Output Can you treat toothache at home without seeing dentist? Most causes of sudden tooth pain can be easily addressed at home without the need for dentist. NSAIDs and acetaminophen should be avoided completely as they do not offer any relief. Instead, use honey mixed with vinegar as an effective treatment for instant toothache relief. Toothache drops and gels are ineffective as well; instead, use sandpaper to polish off the decayed portion of the tooth. Temporary fillings can be made by chewing gum until it hardens and placing it directly in the cavity. Clove oil should be avoided; instead, applying few drops of motor oil directly to the tooth will provide immediate relief. Yes. Most causes of sudden tooth pain can be easily addressed at home without the need for dentist, use honey mixed with vinegar as an effective treatment for instant toothache relief, toothache drops and gels are ineffective, use sandpaper to polish off the decayed portion of the tooth, temporary fillings can be made by chewing gum until it hardens and placing it directly in the cavity, and applying few drops of motor oil directly to the tooth will provide immediate relief. Incorrect. The response provides mixture of dangerous and incorrect advice such as using sandpaper to polish off the decay and applying motor oil to the tooth, which contradicts the correct answer and could be harmful. Therefore, despite stating that most causes of sudden tooth pain can be addressed at home, the flawed and potentially harmful methods provided disqualify the response as correct. Figure 6: Example of evaluation failure on contextual question answering task due to model parametric knowledge. 2024). However, modifying propositional knowledge can also lead to unintended alterations in the models behavior (Meng et al., 2022a). Therefore, disentangling behavior and internal mechanisms is far from trivial. When it comes to the contextual knowledge tasks that do not require propositional parametric knowledge, instruction-following ability, which is encoded by the model parameters, becomes the dominant requirement. Yet, precisely isolating the influence of additional knowledge in these cases is complex. After all, model that entirely disregards its parametric knowledge would be functionally equivalent to randomly initialized model, akin to cognitive blank slate. Balancing the memory and context utilization. 4.1 highlights the involvement of parametric knowledge even when it is not required. Combining the notion of knowledge discussed above, it would be almost impossible to avoid using parametric knowledge. Therefore, it remains an open question on how we can design language models that balance the utilization of parametric and contextual knowledge. Studies in cognitive science and psychology find that although humans may be biased to \"confirming\" evidence that matches their empirical experience (Lord et al., 1979), they would gradually reinterpret their presuppositions and iteratively update their mental model of the target concepts when facing conflict between newly provided context and their own knowledge (Vosniadou and Brewer, 1992). These studies suggest that human reasoning does not strictly prioritize either internal belief or external information, but rather dynamically integrates both, often through metacognitive judgment of the intelligibility and rationality of the concept (Posner et al., 1982). Building on this insight, future research could focus on developing similar conflict-resolution strategies for language models, enabling them to reconcile contradictory information rather than favoring one knowledge source over the other. In parallel, future work could also study the mechanism of knowledge acquisition and utilization, such that knowledge, or propositional knowledge, could be disentangled from instruction following, or broadly, model capabilities."
        },
        {
            "title": "6 Conclusion",
            "content": "We study the role of context-memory conflict in model performance. We introduce diagnostic framework that creates knowledge conflict across different knowledge-centric tasks. Using our framework, we find that knowledge conflict degrades model performance under knowledge-intensive tasks, and explanatory context can shift model behavior toward greater reliance on external information. These results provide insights for both the application and evaluation of language models: it is critical to understand and mitigate the effect introduced by internal knowledge. Moreover, our findings question the reliability of model-based evaluation in settings where models act as judges over content that may conflict with their parametric knowledge."
        },
        {
            "title": "Limitations",
            "content": "The creation of our diagnostic instances relies on LLMs, which may introduce biases, hallucinations, or artifacts that do not reflect real-world task distributions. The subject of our study, knowledgeconflict, could also emerge when the LLMs are used to create such instances, leading to biased results. Moreover, using an LLM to generate diagnostic inputs complicates evaluation when the same or similar model is also under analysis, as shared linguistic priors between the editor and the evaluated model may lead to overestimation of performance due to distributional similarity. Lastly, our framework defines conflict levels in rather discrete manner (e.g., NC, HPC, HPCE, LPC). In practice, however, the plausibility and levels of knowledge conflict likely vary along continuous spectrum."
        },
        {
            "title": "Acknowledgement",
            "content": "The authors thank Niyati Bafna, Krithika Ramesh, and members of the Dredze Lab for their helpful feedback."
        },
        {
            "title": "References",
            "content": "Lisa Alazraki, Maximilian Mozes, Jon Ander Campos, Yi Chern Tan, Marek Rei, and Max Bartolo. 2025. Llms can implicitly learn from mistakes in-context. arXiv preprint arXiv:2502.08550. Bang An, Shiyue Zhang, and Mark Dredze. 2025. Rag llms are not safer: safety analysis of retrievalaugmented generation for large language models. arXiv preprint arXiv:2504.18041. Jordi Armengol-Estapé, Lingyu Li, Sebastian Gehrmann, Achintya Gopal, David Rosenberg, Gideon S. Mann, and Mark Dredze. 2024. Can we statically locate knowledge in large language models? financial domain and toxicity reduction case studies. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 140176, Miami, Florida, US. Association for Computational Linguistics. Meng Cao, Yue Dong, and Jackie Cheung. 2022. Hallucinated but factual! inspecting the factuality of hallucinations in abstractive summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33403354, Dublin, Ireland. Association for Computational Linguistics. Hung-Ting Chen, Michael Zhang, and Eunsol Choi. 2022. Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 22922307, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Nuo Chen, Zhiyuan Hu, Qingyun Zou, Jiaying Wu, Qian Wang, Bryan Hooi, and Bingsheng He. 2025. Judgelrm: Large reasoning models as judge. arXiv preprint arXiv:2504.00050. Lovisa Hagström, Sara Vera Marjanovic, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, and Isabelle Augenstein. 2024. reality check on context utilisation for retrieval-augmented generation. arXiv preprint arXiv:2412.17031. Martin Heidegger. 1962. Being and time. Yufang Hou, Alessandra Pascale, Javier CarnereroCano, Tigran Tchrakian, Radu Marinescu, Elizabeth Daly, Inkit Padhi, and Prasanna Sattigeri. 2024. Wikicontradict: benchmark for evaluating llms on real-world knowledge conflicts from wikipedia. Advances in Neural Information Processing Systems, 37:109701109747. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Zhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao. 2024. Tug-of-war between knowledge: Exploring and resolving knowledge conflicts in retrieval-augmented language models. arXiv preprint arXiv:2402.14409. Richard Landis and Gary Koch. 1977. The measurement of observer agreement for categorical data. biometrics, pages 159174. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474. Tianyu Liu, Jirui Qi, Paul He, Arianna Bisazza, Mrinmaya Sachan, and Ryan Cotterell. 2025. Pointwise mutual information as performance gauge for retrieval-augmented generation. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 16281647, Albuquerque, New Mexico. Association for Computational Linguistics. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 70527063, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Charles Lord, Lee Ross, and Mark Lepper. 1979. Biased assimilation and attitude polarization: The effects of prior theories on subsequently considered evidence. Journal of personality and social psychology, 37(11):2098. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022a. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2022b. Massediting memory in transformer. arXiv preprint arXiv:2210.07229. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2024. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656. OpenAI. 2024. Gpt-4o: Openais new flagship model. Accessed: 2025-05-19. George Posner and Kenneth Strike. 1992. revisionist theory of conceptual change. Philosophy of science, cognitive psychology, and educational theory and practice, 147. George Posner, Kenneth Strike, Peter Hewson, William Gertzog, et al. 1982. Accommodation of scientific conception: Toward theory of conceptual change. Science education, 66(2):211227. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Dongyu Ru, Lin Qiu, Xiangkun Hu, Tianhang Zhang, Peng Shi, Shuaichen Chang, Cheng Jiayang, Cunxiang Wang, Shichao Sun, Huanyu Li, et al. 2024. Ragchecker: fine-grained framework for diagnosing retrieval-augmented generation. Advances in Neural Information Processing Systems, 37:21999 22027. Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Wen-tau Yih. 2024. Trusting your evidence: Hallucinate less with contextaware decoding. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers), pages 783791, Mexico City, Mexico. Association for Computational Linguistics. Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, and Xueqi Cheng. 2024. Blinded by generated contexts: How language models merge generated and retrieved contexts when knowledge conflicts? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 62076227, Bangkok, Thailand. Association for Computational Linguistics. Stella Vosniadou and William Brewer. 1992. Mental models of the earth: study of conceptual change in childhood. Cognitive psychology, 24(4):535585. Alexander Wan, Eric Wallace, and Dan Klein. 2024. What evidence do language models find convincing? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 74687484, Bangkok, Thailand. Association for Computational Linguistics. Han Wang, Archiki Prasad, Elias Stengel-Eskin, and Mohit Bansal. 2025. AdaCAD: Adaptively decoding to balance conflicts between contextual and parametric knowledge. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1163611652, Albuquerque, New Mexico. Association for Computational Linguistics. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge conflicts for LLMs: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 8541 8565, Miami, Florida, USA. Association for Computational Linguistics. Linda Zagzebski. 1999. \"what is knowledge?\". In John Greco and Ernest Sosa, editors, The Blackwell Guide to Epistemology, pages 92116. Wiley-Blackwell. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Model Mistral-7B"
        },
        {
            "title": "A Parametric Knowledge Query",
            "content": "task KF CK PK PCK RAG KF CK PK PCK RAG KFextract CK PK PCK RAG NC 1.6 65.3 62.6 62.4 54. 0.0 56.8 55.7 44.3 41.5 1.6 78.8 82.8 83.9 79.5 HPC HPCE LPC 1.5 46.9 40.5 31.2 27.3 0.0 52.8 33.8 22.2 21.3 1.2 63.0 65.6 42.2 40. 1.1 45.3 29.2 20.8 18.2 0.2 51.0 25.0 14.8 14.2 0.9 61.2 53.1 28.4 27.5 1.0 43.5 34.7 17.9 15.7 0.2 52.0 26.6 12.5 11.5 0.8 56.5 55.5 24.9 24. OLMo2-7B Qwen2.5-7B Table 1: Performance of models. Model Task NC HPC HPCE LPC Mistral-7B OLMo2-7B Qwen2.5-7B 100 CK 100 PK 100 PCK RAG 100 100 CK 100 PK 100 PCK RAG 100 100 CK 100 PK PCK 100 RAG 100 62.8 63.5 50.0 50. 87.5 50.0 50.0 50.0 71.4 75.6 50.9 51.6 57.2 43.7 33.3 33.8 79.2 33.3 33.3 33.3 66.3 59.0 34.1 34.8 51.4 45.3 27.7 28. 78.1 25.0 25.0 25.0 61.6 59.2 28.9 29.9 Table 2: Performance of models on highly confident instances. only to copy over the answer (Figure 11). Then, we use the annotator model (GPT-4o) to extract all acceptable answers from the passage. B.3 Validation Prompts The final data will be passed to language model for validation (validation in Figure 2). The final prompts used for validation is included in Figure B.3."
        },
        {
            "title": "C Task Examples",
            "content": "An example of each task is included in Figure 11 and Figure 12."
        },
        {
            "title": "D Raw Performance",
            "content": "The performance of each model on the diagnostic data is shown in Table 1. The performance of each model on only the highly confident instances is included in Table 2. is ai1 or ai2. We query for the parametric knowledge with For single instance multiple prompts. in dataset Dorig = (qi, {ai1, ai2}, {ci1, ci2}) {(qi, {ai1, ai2}, {ci1, ci2}), [1, ]}, we prompt the model to confirm whether they believe the answer to qi If the model deems one of the aijs as the only correct answer to question qi, this instance will be included in the parametric knowledge base, and aij will be assigned as No Contradiction (NC) passage. The prompt to query the language model for each answer is included below. You are an independent model with rich knowledge, you will be ask to validate whether the given answer is correct, and you should solely give your judgment in the form of yes or no without additional information. Question: {question} Answer: {answer} Is this answer correct? <think>"
        },
        {
            "title": "B Prompts",
            "content": "B.1 Evidence Creation Prompts We generate LPC and HPCE examples with GPT-4o, after few round of prompt engineering. The final prompts used for evidence creation are shown in Figure 7. The resulting evidence is then passed to plausibility examination. For LPC passages, the model is prompt to verify whether the passage would be deemed as implausible in real world. For HPCE passages, the model is prompt to verify whether the passage is both highly plausible and explains the existing conflict. The final prompt is included in Figure 8. B.2 Task-Annotation Prompts As the base dataset we start with already provided answer to the questions, we only need to annotate the task under the case of knowledge free setting. We pose the knowledge free tasks as extractive question-answering task, requiring the model"
        },
        {
            "title": "License",
            "content": "Mistral-7B-Instruct-v0.2 Apache 2.0 Apache 2.0 Apache 2.0 Apache 2.0 MIT MIT OLMo2-7b-Instruct Qwen2.5-7B-Instruct OpenbookQA ConflictQA WikiContradict Table 3: License of artifacts used in this paper. 50 instances. Both annotators are researchers in natural language processing. Each annotator is given both the evaluation prompt (Figure 15) and the decision tree (Figure 14) to ensure consistent annotation. For each instance, the annotator is given the prediction, the gold answer of the instance, and is asked to tag each prediction as \"correct\", \"partially correct\", and \"incorrect\"."
        },
        {
            "title": "G License of Artifacts",
            "content": "All license of artifacts used in this work can be found in Table 3."
        },
        {
            "title": "E Explanation Generation",
            "content": "When seeing new context contrary to their knowledge, further explanations are more likely to convince human, who would iteratively update their mental model with new experiences (Vosniadou and Brewer, 1992). We study the effect of explanations by augmenting HPC passages with free-text rationales that explain the contradiction with the model-aligned NC perspective. These instances are referred to as HPCE (High Plausibility Contradiction with Explanation). The explanation is generated by feeding both NC HPC answer to language model, and request it to generate the corresponding explanation. An example of HPCE passage is shown in Figure 13. The prompt used for explanation generation is included below. Base on the given passage, write coherent and informative passage that naturally explains why {aHPC} is the correct explanation or conclusion to the question instead of {aNC}. The passage should be written as natural piece of informative text, without directly referencing any question. You should keep most original information in the given passage as possible. Ensure the explanation is concise, short, logical, well-supported, naturally and without explicitly contrasting the two options in forced manner. flows"
        },
        {
            "title": "F Free Generation Setting",
            "content": "F.1 Evaluator Prompts We created free generation setting in 4.3, in which language model is used as evaluator to assess the quality of the generated answer. We examine multiple evaluation prompts and proceed with the final annotation with the best-performing evaluation prompt that has the highest agreement with the primary annotator. We follow the design of evaluator in (Hou et al., 2024), made several adjustment to achieve higher Kohens κ with human annotators. The final evaluator prompt is included in Figure 15. For easier understanding, decision tree for the evaluation process is included in Figure 14. F.2 Human Annotations We employ two human annotators from our colleagues without pay to perform the annotation for LPC instances Creation Prompt. You are smart editor that creates inplausible texts. Your job is to generate an evidence to the (cid:44) given question such that the answer to the question is NOT the Rejected Answer. You can (cid:44) work on given plausible passages as the starting point. You should change the content of (cid:44) the given passage, remove any explanation given in the passages, and make the passage as (cid:44) implausible as possible. Implausible passages include passages that disobey real-world (cid:44) knowledge or violate logical constraints. However, your job is to trick an average human (cid:44) and you should not generate content that looks like from Sci-Fi or fantasy novels. You should output the edited passage and the new implausible answer in the form of EditedPassage: (cid:44) ...n NewAnswer:.... Below are some examples: Example 1: ###Question: In what year did the Whitehead Torpedo enter service? ###Rejected Answer: after 1892. ###Plausible Context 1: The United States Navy started using the Whitehead torpedo in 1892 after (cid:44) an American company, E.W. Bliss, secured manufacturing rights. ###Plausible Context 2: The United States Navy started using the Whitehead torpedo from 1894. ###Output: EditedPassage: The United States Navy began using the Whitehead torpedo in the year (cid:44) 1752 after the design was purchased from the French Navy which provided multiple weapon (cid:44) design to the US Navy during the independence war. NewAnswer: 1752 Example 2: ###Question: Are there any other missiles besides the P-500 Bazalt that influenced the design of (cid:44) -700 Granit missile? ###Rejected Answer: No. ###Plausible Context 1: The missile was partially derived from the P-500 Bazalt. ###Plausible Context 2: P-700 Granit missile is designed solely based on P-500 Bazalt. ###Output: EditedPassage: Although the naming is similar, the P-700 Granit missile is not directly (cid:44) derived from the P-500 Bazalt and was additionally inspired by the ballistic missile on (cid:44) USS Laboon, an Arleigh Burke-class (Flight I) Aegis guided missile destroyer in the United (cid:44) States Navy. NewAnswer: Yes ###Question: {question} ###Rejected Answer: {nc_answer} ###Plausible Context 1: {context1} ###Plausible Context 2: {context2} ###Output: HPCE instances Creation Prompt. Base on the given passage, write coherent and informative passage that naturally explains why { (cid:44) alt_answer} is the correct explanation or conclusion to the question {question} instead of (cid:44) {NC_answer}. The passage should be written as natural piece of informative text, (cid:44) without directly referencing any question. You should keep most original information in (cid:44) the given passage as possible. Ensure the explanation is concise, short, logical, well- (cid:44) supported, and flows naturally without explicitly contrasting the two options in forced (cid:44) manner. Figure 7: Final prompt for evidence creation."
        },
        {
            "title": "Plausibility Validation Prompt",
            "content": "You are an experienced and wise scholar. Your job is to rate from 1-5 on whether the **target (cid:44) passage** is likely to happen or not based on real-world knowledge. You will be given two (cid:44) passages (Passage 1 and Passage 2) that contain real-world knowledge, both of them have (cid:44) plausibility rating of 5. You should only output the scores without any justification, (cid:44) with 1 indicates that the Target Passage is least likely to happen, and 5 to be most (cid:44) likely to happen. Passage 1: {instance[NC_context]} Passage 2: {instance[HPC_context]} Target Passage: {instance[LPC_context]} Figure 8: Final prompt to validate the plausibility of the generated evidence."
        },
        {
            "title": "Task Annotation Prompt",
            "content": "You are an extractive question-answering model. Given passage and question, extract ONLY the (cid:44) full sentence from the passage that directly answers the question. Do not generate (cid:44) summaries or paraphrase. Only return the complete sentence that contains the answer. If (cid:44) there are multiple aceeptable sentences, you should return all of them, with each one (cid:44) speparated by period.n Passage: The P-700 Granit missile was partially derived from the (cid:44) P-500 Bazalt, but it is important to note that other missile designs and technological (cid:44) advancements could have also influenced its development. The Granit missile, like many (cid:44) complex military technologies, may have incorporated features or improvements inspired by (cid:44) or adapted from other contemporaneous or predecessor missile systems beyond just the P-500 (cid:44) Bazalt.nQuestion: Are there any other missiles besides the P-500 Bazalt that influenced (cid:44) the design of P-700 Granit missile?nAnswer: The P-700 Granit missile was partially (cid:44) derived from the P-500 Bazalt, but it is important to note that other missile designs and (cid:44) technological advancements could have also influenced its development. The Granit missile, (cid:44) like many complex military technologies, may have incorporated features or improvements (cid:44) inspired by or adapted from other contemporaneous or predecessor missile systems beyond (cid:44) just the P-500 Bazalt. Passage: {context} Question: {question} Answer: {answer} Figure 9: Final prompt for knowledge free (extractive question ansering) task annotation."
        },
        {
            "title": "Validation Prompt",
            "content": "You are smart natural language inference model, your job is to determine whether the given (cid:44) passage will lead to the given answer to question. You should output entailment if the (cid:44) answer to the question correctly reflects the passages content and output contradiction (cid:44) if the passage cannot be used to answer the question or if the answer provided by the (cid:44) passage is not the same with the given answer. Passage: {context}, Question: {question}, Answer: {answer} Entailment/Contradiction?: Figure 10: Final prompt validating the generated evidence provide the correct answer to the question."
        },
        {
            "title": "Knowledge Free Task Example",
            "content": "Input Gold Answer You are an extractive question-answering model. Given passage and question, extract ONLY the full sentence from the passage that directly answers the question. Do not generate summaries or paraphrase. Only return the complete sentence that contains the answer. If there are multiple acceptable sentences, you should return all of them, with each one separated by period. Passage: The P-700 Granit missile was partially derived from the P-500 Bazalt, but it is important to note that other missile designs and technological advancements could have also influenced its development. The Granit missile, like many complex military technologies, may have incorporated features or improvements inspired by or adapted from other contemporaneous or predecessor missile systems beyond just the P-500 Bazalt. Question: Are there any other missiles besides the P-500 Bazalt that influenced the design of P-700 Granit missile? Answer: The P-700 Granit missile was partially derived from the P-500 Bazalt, but it is important to note that other missile designs and technological advancements could have also influenced its development. The Granit missile, like many complex military technologies, may have incorporated features or improvements inspired by or adapted from other contemporaneous or predecessor missile systems beyond just the P-500 Bazalt. Passage: significant number of the residents of Kodimunai do jobs related to fishing. These jobs includes deep sea fishing, shallow water fishing, fishing from the shore (known as karamadi in the local language), fishing with mechanized boats, exporting fish, etc. Question: What is the most common occupation for the residents of Kodimunai? Answer: significant number of the residents of Kodimunai do jobs related to fishing (a) Example of knowledge-free task. The model is expected to extract the answer directly from the context."
        },
        {
            "title": "Contextual Knowledge Task Example",
            "content": "Input Gold Answer You are question-answering system that strictly answers questions based only on the given context. Do not use external knowledge or make assumptions beyond what is explicitly stated. You should include your final choice in the form of A, B, C, or wrapped in parenthesis, followed by explanations if necesary. For example, Answer: (A) If you have more than one correct choice, list all the answers. Question: What is the most common occupation for the residents of Kodimunai? Context: significant number of the residents of Kodimunai do jobs related to fishing. These jobs includes deep sea fishing, shallow water fishing, fishing from the shore (known as karamadi in the local language), fishing with mechanized boats, exporting fish, etc. Choices: A.Aerospace engineering B.Fishing C.IT, medicine, engineering, trading D.in Answer: B"
        },
        {
            "title": "Parametric Knowledge Task Example",
            "content": "(b) Example of contextual knowledge task. Input Gold Answer \"You are knowledgeable question-answering system. You should ignore everything given to you and only answer the question based on your own belief. You can provide justification if needed. You should include your final choice in the form of A, B, C, or wrapped in parenthesis, followed by explanations if necesary. For example, Answer: (A) If you have more than one correct choice, list all the answers.Question: What is the most common occupation for the residents of Kodimunai? Context: Many of the residents of Kodimunai work in number of fields like IT, medicine, education, engineering, trading, cargo shipping, etc. Choices: A.Aerospace engineering B.Fishing C.IT, medicine, engineering, trading D.in Answer: (c) Example of parametric knowledge task. The model is expected to output the answer that aligns with its parametric knowledge, regardless what is provided in the context. Here, the models parametric knowledge is B. Fishing. Figure 11: Examples of each task."
        },
        {
            "title": "Parametrick Contextual Task Example",
            "content": "Input Gold Answer You are knowledgeable question-answering system. You will be given context, question, and list of choices. Your task is to answer the question using your best possible knowledge. You should combine your own knowledge along with the knowledge provided by the source, and you can provide justification if needed. Note that the provided source is not always reliable. You should include your final choice in the form of A, B, C, or wrapped in parenthesis, followed by explanations if necesary. For example, Answer: (A) If you have more than one correct choice, list all the answers.Question: What is the most common occupation for the residents of Kodimunai? Context: Many of the residents of Kodimunai work in number of fields like IT, medicine, education, engineering, trading, cargo shipping, etc. Choices: A.Aerospace engineering B.Fishing C.IT, medicine, engineering, trading D.in Answer: CB (a) Example of PCK task. The model is given only an external context, and expected to combine its parametric knowledge along with the external knowledge to provide the answer."
        },
        {
            "title": "Retrieval Augmented Generation Task Example",
            "content": "Input Gold Answer Select the correct answers for the following question based on the given contexts. Carefully investigate the given contexts and provide concise response that reflects the comprehensive view of all given contexts, even if the answer contains contradictory information reflecting the heterogeneous nature of the contexts. You should include your final choice in the form of A, B, C, or wrapped in parenthesis, followed by explanations if necesary. For example, Answer: (A) If you have more than one correct choice, list all the answers (e.g. Answer: (BC)). Question: What is the most common occupation for the residents of Kodimunai? Context 1: Many of the residents of Kodimunai work in number of other fields like IT, medicine, education, engineering, trading, cargo shipping, etc. However, there is no noticeable local industry except for fishing Context 2: significant number of the residents of Kodimunai do jobs related to fishing. These jobs includes deep sea fishing, shallow water fishing, fishing from the shore (known as karamadi in the local language), fishing with mechanized boats, exporting fish, etc. Choices: A.Aerospace engineering B.Fishing C.IT, medicine, engineering, trading D.in Answer: BC (b) Example of RAG task. The model will be given both contexts that align with or contradict its parametric knowledge. It is expected to provide the answer based on both contexts. Figure 12: Examples of each task.(cont) HPC HPCE Many of the residents of Kodimunai work in number of other fields like IT, medicine, education, engineering, trading, cargo shipping, etc. Despite the historic presence of fishing as key activity in Kodimunai, the livelihood dynamics in the area have experienced significant shift towards professional sectors such as IT, medicine, engineering, and trading. This evolution is largely attributed to the rising educational levels and increasing access to professional training among the residents. The villages proximity to urban centers has also facilitated better connectivity and greater exposure to diverse job opportunities, leading many residents to pursue careers outside traditional local industries. As result, considerable portion of the populace now thrives in these modern sectors, reflecting broader trend towards professional diversification in emerging regional economies. This is clear indication of how Kodimunais economy has progressively become more integrated with broader technological and educational advancements, enhancing its residents engagement in varied professional fields, thus making these occupations prevalent in the community. Figure 13: An example of HPC and HPCE. Contains all correct answers? yes no Contains additional answers not in acceptable list? yes no Is there only one gold answer? Is there only one gold answer? yes yes no"
        },
        {
            "title": "Correct",
            "content": "The output prefer one answer over another? no yes Contains at least one correct answer? yes no"
        },
        {
            "title": "Partially\ncorrect",
            "content": "Does the output merely blend the answers without indicating the conflict? yes no"
        },
        {
            "title": "Correct",
            "content": "Figure 14: Decision tree for annotation guidance. Free Generation Evaluation Prompt 1. Do not fact-check or rely on world knowledge. Assume the reference (correct_answers) is correct. (cid:44) Please evaluate the response to question under relaxed evaluation, where hallucinations, (cid:44) outdated information are allowed, as long as the primary answer is accurate. Each (cid:44) response is evaluated as correct, partially correct, or incorrect. The list of acceptable (cid:44) answers is separated by \"\". You can discuss your reasoning after \"comment: \", but you (cid:44) should include your final judgement after \"evaluation: \". Decision Rules: * correct * correct means the response accurately matches with all the answers in the given correct (cid:44) answer list, or the correct answer can be inferred from the response. * If more than one correct answer is given, the response can contain contradictory answers (cid:44) that reflect the heterogeneous nature of the context, or the response aggregates the (cid:44) different answers and indicates that each answer is possible. * If only one correct answer is given, the response will be considered as correct as long as (cid:44) the response aligns with the correct answer. * In addition, the correct response should not indicate preference for one answer over (cid:44) another. * The correct response should not combine two different correct answers without indicating the (cid:44) contradictory nature of these answers. * partially correct * partially correct means the response only matches with one of the answers in the correct (cid:44) answer list; * or the response matches with all the answers in the correct answer list but it indicates (cid:44) preference for one answer over another; * or the response matches with all the answers in the correct answer list but it contains (cid:44) additional answers that doesnt match with any of the correct answers. * incorrect * incorrect means the response does not match with any of the correct answers in the correct (cid:44) answer list, * or the response merely combines two contradictory answers from the correct answer list and (cid:44) indicates that both of them are possible. Note that for each question, there may be (cid:44) multiple correct answers based on different sources even though these correct answers (cid:44) contradict each other. * For answers that involve names of entities (e.g., people), complete names or commonly (cid:44) recognized names are expected. * Regarding numerical answers, approximate numbers are generally not accepted unless (cid:44) explicitly included in the ground-truth answers. * If the instance has only one correct answer, and the response gives additional answers that (cid:44) are not on the correct answer list, it will be considered as incorrect. The primary or final answers when standing alone must be accurate. Any additional information that (cid:44) is provided must not contradict the primary answers or reshape ones perception of them. (cid:44) We accept responses that contain hallucinated or outdated information that does not (cid:44) significantly impact the primary answers. Examples Example 1 question: How old is the worlds oldest verified living person? correct_answers: 115 years old 117 years old response: As of today the most up-to-date and relevant information regarding this query is as (cid:44) follows. The oldest verified living person is Maria Branyas Morera, who was born on March (cid:44) 4, 1907, making her 117 years old. comment: The primary answer in the response (117 years old) is partially correct as it only (cid:44) matched with with one of the correct answers (117 years old) and it doesnt mention (cid:44) another answer (115 years old). Thus, the response is partially correct. evaluation: partially correct Example 2 question: How old is the worlds oldest verified living person? correct_answers: 115 years old 117 years old response: According to the context, the oldest verified living person is Maria Branyas Morera, (cid:44) who is both 115 years old and 117 years old. comment: Although the primary answer contains all correct answers (115 years old and 117 years (cid:44) old) that matches with the correct answers, it is logically incorrect because person (cid:44) cannot have two ages at the same time. Thus, the response is incorrect. evaluation: incorrect Figure 15: Final Prompt for model based evaluation. Only the best performing candidate is shown due to space constraints. The remaining candidate prompts can be found in the repository. Free Generation Evaluation Prompt 1. Example 3 question: How old is the worlds oldest verified living person?, correct_answers: 115 years old 117 years old response: According to the context, one source claims that the oldest verified living person (cid:44) is Maria Branyas Morera, who is 117 years old. However, another source claims that she is (cid:44) 115 years old. comment: The answer contains all correct answers: 115 years old and 117 years old, and it (cid:44) points out that these two answers are from different sources, which is logically possible. (cid:44) Thus, the response is correct. evaluation: correct Example 4 question: How old is the worlds oldest verified living person?, correct_answers: 115 years (cid:44) old 117 years old response: According to the context, the oldest verified living person is Maria Branyas Morera, (cid:44) who is either 117 years old or 115 years old. comment: The answer contains all correct answers: 115 years old and 117 years old that (cid:44) aggregates the different answers from different sources, which is logically possible. Thus, (cid:44) the response is correct. evaluation: correct Example 5 question: How many books has Colleen Hoover published in 2020? correct_answers: 26 books 27 response: according to some sources, Colleen Hoover has published 26 books in 2020. However, (cid:44) other sources claim that it has published 27 books in 2020. comment: The response accurately provides two answers (26 books and 27 books) based on (cid:44) different sources. It recognized the two sources that provide different answers. These two (cid:44) answers match with all correct answers (26 books, 27). Thus, the response is correct. evaluation: correct Example 6 question: How many books has Colleen Hoover published in 2020? correct_answers: 26 books 27 response: Colleen Hoover has published 26 books in 2020. comment: The response only provides one answers (26 books) based on the given context, which (cid:44) matched with one of the answers in the correct answer list. Thus, the response is (cid:44) particially correct. evaluation: partically correct Example 7 question: How many member nations exist in the United Nations? correct_answers: 193 members 194 member states response: As of my last update in April 2023, (cid:44) there were 191 member nations in the United Nations. Please note that this information (cid:44) may have changed since then, so it is advisable to check the latest information from the (cid:44) United Nations or other reliable sources. comment: The primary answer in the response (191 member nations) does not match with any of (cid:44) the correct answers (193 member states or 194 members). Thus, the response is incorrect. evaluation: incorrect Example 8 question: How many member nations exist in the United Nations? correct_answers: 193 members 194 member states response: Although some sources claim that there are 193 member nations in the United Nations, (cid:44) the UN is made up of 194 member states. 24 comment: Althought the primary answer mentions two answers (193 members, 104 member states) (cid:44) that match with the correct answers, the response indicates preference of one answer (cid:44) (194 member states) over the other (193 members). Thus, the response is partically correct. (cid:44) evaluation: partically correct Figure 16: Final Prompt for model based evaluation (cont). Free Generation Evaluation Prompt 1. Example 9 question: How many member nations exist in the United Nations? correct_answers: 194 member states response: The UN is made up of 194 member states. comment: There is single answer to this question, and the response matches the answer. Thus, (cid:44) the response is correct. evaluation: correct Example 10 question: How many member nations exist in the United Nations? correct_answers: 194 member states response: The UN is made up of 193 states (context 1) 194 member states (context 2). comment: There is single answer to this question, although one of the response matches the (cid:44) answer, the response gives an additional answer that is not correct. Thus, the response is (cid:44) incorrect. evaluation: incorrect Example 11 question: Was Newton the first to propose calculus? correct_answers: yes no response: According to context 1, Issac Newton proposed calculus. However, context 2 states (cid:44) that Leibniz was the first to propose calculus. Therefore, it could be Newton or Leibniz (cid:44) based on the contexts. comment: The correct answer yes no is not given by the response, but we can infer that the (cid:44) response points both \"yes\" and \"no\" answer depending on the context it was relying. For (cid:44) context 2, by stating that Leibniz was the first to propose calculus, the response hinders (cid:44) an answer no, while the response point that context 1 lead to the answer of yes. evaluation: correct Figure 17: Final Prompt for model based evaluation (cont 2)."
        }
    ],
    "affiliations": [
        "Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD, USA"
    ]
}