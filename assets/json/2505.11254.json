{
    "paper_title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction",
    "authors": [
        "Jeffrey Willette",
        "Heejun Lee",
        "Sung Ju Hwang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The attention mechanism of a transformer has a quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with a troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces a distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to a drop in performance. We propose a simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding a small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills."
        },
        {
            "title": "Start",
            "content": "Attention: Fast and Accurate Sparse Attention Inference by Delta Correction Jeffrey Willette1, Heejun Lee1, Sung Ju Hwang1,2 KAIST1, DeepAuto.ai2 {jwillette, ainl, sjhwang82}@kaist.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "The attention mechanism of transformer has quadratic complexity, leading to high inference costs and latency for long sequences. However, attention matrices are mostly sparse, which implies that many entries may be omitted from computation for efficient inference. Sparse attention inference methods aim to reduce this computational burden; however, they also come with troublesome performance degradation. We discover that one reason for this degradation is that the sparse calculation induces distributional shift in the attention outputs. The distributional shift causes decoding-time queries to fail to align well with the appropriate keys from the prefill stage, leading to drop in performance. We propose simple, novel, and effective procedure for correcting this distributional shift, bringing the distribution of sparse attention outputs closer to that of quadratic attention. Our method can be applied on top of any sparse attention method, and results in an average 36%pt performance increase, recovering 88% of quadratic attention accuracy on the 131K RULER benchmark when applied on top of sliding window attention with sink tokens while only adding small overhead. Our method can maintain approximately 98.5% sparsity over full quadratic attention, making our model 32 times faster than Flash Attention 2 when processing 1M token prefills."
        },
        {
            "title": "Introduction",
            "content": "The main operation that powers modern transformers, self-attention [Vaswani et al., 2017], creates causal pairwise comparisons for every item in sequence. While powerful and expressive, this operation comes with quadratic complexity, leading to the need for large amounts of computation during inference on long sequences. This increases direct costs for hardware and electricity as well as negative externalities such as CO2 emissions. Training-free sparse attention modifications aim to lower the quadratic complexity at inference time, but come with unwanted side effects such as accuracy degradation due to the sparsification of the attention matrix. 5 2 0 2 6 1 ] . [ 1 4 5 2 1 1 . 5 0 5 2 : r Figure 1: RULER 131K Subsets. At long context lengths, sparse attention can degrade performance by large margin. Our simple correction improves performance and only requires an additional 1.5% of the full quadratic attention computation. Preprint. Under review. Recent works on sparse attention have found that sparse sliding window can be added at inference time without total loss of model stability. This is accomplished by saving small number of initial tokens, and applying sliding window on all subsequent tokens (Streaming LLM [Xiao et al., 2023]). Subsequent works such as Star Attention [Acharya et al., 2024] have proposed similar sparse prefill strategy with fully dense decoding procedure to generate new tokens. This strategy has the positive attribute of sparse prefill while still performing attention with all tokens during generation. This should allow the model to accurately recall context buried deep within the prompt. However, we find that this is not the case in practice. For example, there is challenging subset of the RULER [Hsieh et al., 2024] benchmark titled MultiKey-3, which consists entirely of unique UUID keys and values, and the large language model (LLM) must be able to recall the proper value for particular key in order to get correct answer. In this setting, sliding window of 2048 tokens provides more than adequate room for encoding individual key and value pairs together within the window. One would then expect that dense decode procedure would be able to retrieve the proper UUID given user query. However, we find that this is not the case and the dense decode achieves surprisingly low accuracy of 0% as opposed to 62% when using quadratic attention. We find this drop in accuracy arises from distributional shift in the output tokens of each layer due to the sparse prefill. This distributional shift causes problems with the query-key dot products in long contexts and therefore results in an extreme drop in performance as the queries no longer align with the expected keys. We study this problem and found surprisingly simple fix which we dub Attention that improves the accuracy of sliding window attention from 0% to 44% (Figure 1, NIAH MK3) on this challenging subset while maintaining more than 11-fold speedup over plain Flash Attention 2 [Dao, 2023] for processing 131K context lengths (Figure 2). Through evaluations on perplexity, natural language understanding, and synthetic tasks, we demonstrate that our method consistently results in better performance while maintaining the low latency of the sparse prefill. Figure 2: Comparing RULER 131K prefill attention latency and accuracy for sparse attention methods. Our contributions are as follows: We identify distributional shift in tokens when applying an inference-time sparse attention method to pretrained transformers, which interferes with query-key alignment on long contexts and leads to drop in performance. We introduce Delta () Attention, sparse post-processing correction that realigns sparse outputs with full quadratic attention. Our method adds negligible latency overhead compared to plain sparse attention, while drastically increasing performance over purely sparse methods. Our method is designed to work in the attention output space, so it can be seamlessly integrated with existing sparse attention kernels and inference pipelines without major modification."
        },
        {
            "title": "2 Background & Related Work",
            "content": "Rd for RN of individual The self attention mechanism of transformer takes an input sequence Rdd to the tokens xi input to achieve the respective Q, K, matrices, positional encodings such as [Su et al., 2024] are applied to and K. With σ representing the softmax operation over the last dimension, the self-attention operation for an arbitrary layer in transformer is the following, . After applying linear projections WQ, WK, WV } 1..N { AV = σ = σ XWV (1) QK XWQ(XWK) (cid:18) (cid:18) We omit the output projections, attention heads, and post-attention multilayer perceptrons (MLPs). For deeper discussion of these topics in transformers, please see [Vaswani et al., 2017]. The most expensive operation in Equation (1) that arises from the multiplication inside σ() which results in RN which is computationally expensive for the implicit construction of an attention matrix (cid:19) (cid:19) 2 large . Due to the causality condition of language, token xi may only influence another token xj where the index j. In practice, this means that only the lower triangle of is computed. After traversing through the layers of the network, the next token in the sequence xN +1 is generated (predicted) and added to the input sequence to generate the next token and so on until the sequence terminates. In this generation phase, each iteration may use the previously computed tokens, which are stored within cache at each layer, so that we may avoid re-calculating the entire attention matrix which concatenates matrices by adding new rows, and in Equation (1). With union operator considering that K, contain tokens with indices , and the newly generated token has index = + 1, the generative process for the next token proceeds through the attention layers as, 1..N { } K i (av)i = σ (cid:3) (cid:2) (cid:0) (cid:1) { i,j 1 (cid:33) (cid:32) (2) } i,j > Sparse attention prefill methods aim to reduce the quadratic computation in Equation (1) by computing subset of entries within A, forming sparse matrix where the number of computed entries 2 2 with minimal information loss. However, in practice, large portions of the attention matrix are ignored, which may cause unintended differences in the output tokens and (cid:80) lead to unexpected behavior of future query-key dot products, which could degrade performance on downstream tasks. Previous works have studied in-context learning (ICL) processes such as induction heads [Olsson et al., 2022], which are responsible for copying relevant content from earlier tokens into later tokens in the sequence [Musat, 2024]. Induction heads are known to be more prevalent in the lower layers of the network [Yin and Steinhardt, 2025], which implies that distributional mismatch between queries and keys at the lower layers of the network will inhibit ICL processes. Additionally, Wu et al. [2024] showed that these induction or retrieval heads are universal for all transformer model types and further highlighted that interfering with these special attention heads causes catastrophic drop in performance on downstream tasks during inference. Recent works on sparse attention, such as Streaming LLM [Xiao et al., 2023], have shown that pretrained quadratic transformers can be modified on-the-fly at test time into stable sparse attention model by utilizing sink tokens and sliding windows. This has inspired multitude of recent works that utilize this knowledge for inference time adaptations that selectively prune the less important middle tokens from the KV-cache during inference. Two approaches, H2O [Zhang et al., 2024b] and SnapKV [Li et al., 2024] accomplish this by looking at historical attention scores to decide which tokens to prune. However, these works still leave the quadratic prompt in place, which requires computation overhead of (n2). Other recent works have therefore made efforts to lower the complexity of the prompt as well. Big Bird [Zaheer et al., 2020] studies the effect of randomly choosing keys for every new query in the attention matrix. However, random key selection has been shown to underperform more targeted selection of keys in HiP Attention [Lee et al., 2024a,b], which applies tree-based pruning mechanism that masks out less important blocks of keys in order to sparsify the computation of the attention matrix. MInference [Jiang et al., 2024] studies reliably recurring patterns in the attention matrix of specific attention heads, and builds set of sparse kernels which apply sparse attention following these patterns. Star Attention [Acharya et al., 2024] uses sparse strategy akin to that of Streaming LLM with sliding window, initial tokens, and fully dense decode procedure which evaluates the dot product between every past key for new queries during the decoding phase. As we show in our experiments, this scheme does not work for all tasks unless the sliding window represents large percentage of the total context length (see Table 1). Figure 3: Comparing sparse attention methods to quadratic attention. Our correction results in outputs that are more similar to quadratic attention. To illustrate how our findings integrate with these prior works, we provide an example in Figure 3. In this experiment, we use quadratic attention and Streaming LLM to prefill 131K length input from the RULER benchmark. We then compute the cosine similarity cos([AV]i, [AV]i) of the sparse and quadratic outputs, and also construct the last part of the full attention matrix using the 3 Figure 4: Overview of Attention. (Top) Given an arbitrary sparse attention method we calculate the difference between the sparse attention and full attention for small subset of queries. The subset size is controlled by hyperparameter γ. (Bottom) We then repeat the calculated difference for all output tokens and add the result to the full sparse attention output. The result is an approximation to the original quadratic attention. last 128 queries in order to compare the rank correlation coefficient ρ(A i, Ai) in the final rows of the attention matrix. If the sparse attention method does not cause distributional shift, then the attention outputs should have high cosine similarity to quadratic attention, and sorting the rows of the attention matrix should lead to the same sort order, which implies that the relative importance (ranking) between queries and keys has been maintained. As seen in Figure 3, in both dimensions, the sparse attention of Streaming LLM causes drift in the distribution of tokens, which causes the degradation in task performance seen in Figure 1. However, we find we can correct this distributional shift with the addition of term which we will describe in the following section."
        },
        {
            "title": "3 Method",
            "content": "Given the distributional shift shown in Figure 3, our method answers the following question: How may we shift the distribution of attention outputs such that they are closer to the representation which is expected during quadratic attention? Specifically, we wish to add term to the sparse attention output AV such that we recover the attention contribution AV from the places where sparse attention has given zero weight. This region is usually located somewhere inside the lower triangle of the attention matrix and resembles delta shape. We propose to approximate this region by simple difference of attention outputs, as geometrically depicted in Figure 5. Specifically, Figure 5: Intuition for Attention. The difference of attention outputs approximates the missing attention contribution. AV AV AV (3) Note that the softmax normalization of sparse attention methods generally only computes the normalization constant over the nonzero values. Thus, and have different normalization constants, which makes the relation an approximation. We consider and to share the same softmax normalization constant. Let the full attention softmax normalization constant be + H, and the sparse attention normalization constant be . Lemma 1. w.l.o.g. Consider an arbitrary row in the attention matrix and arbitrary column of the values v, with both and being sorted according to rank of such that = (ar(1) ar(2) ar(N )). For top-k sparse attention matrix which only computes the top-k attention scores, one only needs to compute av = av, we may bound the k+1 error of our attention approximation as, (cid:80) ivi. With = av Proof. See Appendix G. ai vi i=1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + max i>N vi We ultimately seek shift in the attention outputs such that AV + we choose = AV AV. Trivially, if AV, we have exact equality; however, calculating requires the full 4 AV, if we further AV quadratic attention procedure that we wish to avoid. As AV N, then we may approximate assume that (AV)i and γ (AV)i + (AV)i+ν. Under this approximation, one only needs to compute every γth (AV)i+ν row of the attention matrix, which maintains sparse computation by only computing subset of rows of A. To do this, we select fixed fraction of row indices from Q, such that, (AV)i+ν for ν 1, . . . , γ { } γ = Qi = mod γ = 0; 1 .. { } (4) (cid:101) AV = σ( QK)V which is sparse in the query dimension, but dense in the key and therefore dimension. One possible approach would be to substitute this representation into the appropriate rows of the sparse output AV such that the final representation A, is the following, (cid:101) (cid:101) make dense output row if mod γ = 0 (cid:98) AV = (AV)i + mod γ = 0 } { 1 (cid:122) (cid:16) (cid:17) AV (cid:125)(cid:124) γ (cid:104) (AV) γ γ ; (cid:123) (cid:105) 1 .. { } (5) (cid:98) (cid:101) We dub this approach as recompute, as we are essentially using the sparse representation with some densely computed output tokens interwoven at regular intervals. However, we find that this approach still does not shift the distribution of attention outputs far enough towards the expected representation under quadratic attention (see Figure 9). Therefore, in order to apply shift to all tokens in the output of AV while maintaining sparse computation, we instead apply the following correction to the sparse attention output, AV (cid:16) (cid:98) (cid:17) = (AV)i + (AV) γ γ = (AV)i + (cid:104) AV (AV) γ correction term γ γ (6) (7) (cid:105) (cid:124) (cid:125) (Q, K, V) Equation 4 (cid:123)(cid:122) Algorithm 1: Attention Algorithm Which is equivalent to swapping in dense row of the attention matrix at every γth row, and applying the difference between the dense and sparse attention for the previous γth row otherwise. visual depiction of this process can be seen in Figure 4, and pseudocode in Algorithm 1. Since our method is applied directly on the attention outputs, we may utilize existing sparse attention kernels to compute AV and make use of minimally modified flash attention kernel to compute our query-sparse attention AV. Assuming that row index of the attention matrix is not evenly divisible by γ, this means that an attention differential from previous row is being applied to the current row j. The intuition from this operation comes from prior works which have studied attention locality [Lee et al., 2024a], finding that the difference between attention scores for neighboring tokens is generally small. Likewise, our conjecture is that the low attention score regions from neighboring rows of the attention matrix also have negligible difference, allowing for the less important part of the row of the attention matrix to be reused multiple times. Specifically, as stated above Equation (4), we assume that (AV)i (AV)i+ν N. To validate this assumption, we examine the average cosine similarity for ν of (AV)i within γ window on an input from the RULER 131K task set for various values of γ in Figure 6b. We find high average cosine similarity within the window, implying that (AV)i may be reused for multiple rows of the attention output. Require: (), () Q, K, V, γ // sparse attention for all of AV // dense attention every γth query AV Q, K, V) (cid:101) // collect proper indices for construction δ // repeat and apply correction AV = AV + repeat(, γ) return (cid:98) { AV mod γ = 0 } (AV)iδ 1, . . . , γ and γ AV { ( (cid:98) (cid:101) }"
        },
        {
            "title": "4 Experiments",
            "content": "We evaluate our method in terms of perplexity (PPL) and long context perplexity using the LongPPL [Fang et al., 2024] metric on QA version of the PG19 [Rae et al., 2019] test set, which was recently proposed as long context understanding dataset [He et al., 2025]. We also provide 5 Table 1: RULER (Llama 3.1 8B Instruct and Mistral NeMo 12B) for sparse attention methods. Adding Attention results in better overall accuracy, with the largest improvement occurring at the longest context length and on the most naive sparse method (Streaming LLM). Colors are relative to each attention method group + Flash Attention 2. Model Attn. Method Wind. 4K 8K 16K 32K 65K 131K Avg. Llama 3.1 8B Instruct Mistral NeMo 12B s t . - . S . M r . S . S . + n . f . + i + 2K 4K 16K 32K 2K 3K 3K 3K 3K a t . - . M r . + i + 2K 2k 3K 3K 96.74 90.52 96.71 96.71 96.71 96.54 96.74 96.71 96.78 96.82 90.60 71.01 90.42 90.36 90.55 93.25 60.53 93.76 93.76 93.76 92.25 93.65 93.69 94.44 94.32 87.67 44.89 85.38 88.36 87.69 90.99 38.13 68.07 91.15 91.15 88.66 92.32 91.34 94.02 94.02 81.82 33.28 78.07 78.07 81.08 85.84 30.25 43.38 56.32 85.83 81.27 86.75 85.96 86.75 91.12 62.54 12.27 34.76 58.76 60.38 85.25 18.59 34.08 41.28 58.35 75.22 84.43 83.67 79.30 82.91 46.89 03.28 16.22 35.87 41.56 73.16 27.45 30.32 40.51 49.17 64.40 65.73 73.31 68.09 72.56 18.09 02.25 01.44 10.10 10.93 87.54 44.25 61.05 69.96 79.16 83.06 86.60 87.44 86.56 88.62 64.60 27.83 51.05 60.25 62.03 evaluations of our method on the RULER [Hsieh et al., 2024] benchmark, which tests models performance under number of long context retrieval tasks. Additionally, we evaluate our Attention on Infinite-Bench [Zhang et al., 2024a], and also provide analysis that evaluates the effect of our correction on the distribution of attention outputs and scores, and overall attention latency. Our work considers that the decoding process shown in Equation (2) is dense along the key dimension and should be able to successfully learn from previously encoded information during the sparse prefill. We apply our method in conjunction with sparse attention methods such as Streaming LLM [Xiao et al., 2023], HiP [Lee et al., 2024a,b], and MInference [Jiang et al., 2024], and also on models from the Llama [Dubey et al., 2024] (3.1 and 4), and Mistral [Jiang et al., 2023] model families. Unless otherwise noted, our standard setting uses γ = 64 which means we calculate every 64th query row which is equivalent to approximately 98.5 sparsity in the attention computation required by Attention. RULER. For baselines on needle-in-a-haystack type tasks, we compare our method in addition to Streaming LLM, HiP, and MInference for both Llama and Mistral models. In all cases, Attention shows large improvement upon the given sparse methods, and especially at the longer context lengths in Table 1. In particular, we note an improvement of nearly 37%pt over Streaming LLM with the same 2K window size for 131K with Llama 3.1. For Streaming LLM, if we adjust for the extra computation needed by our method, we find that the approximate window size of our method is 3072 (see Appendix for calculation). This is due to the fact that we also use sliding window of 2048 and compute every 64th row of the lower triangle in the attention matrix. Therefore, even when Streaming LLM is allowed higher computational budget of 4K window, Attention still results in an increase of 34%pt, more than doubling the accuracy of Streaming LLM (+112%, relative). Even when allowed 32K window, Streaming LLM + still delivers higher accuracy. Method Table 2: Perplexity on PG19 Long QA [He et al., 2025]. Our simple correction results in significant drop in both PPL and Long PPL. Perplexity (PPL) and Long Perplexity (LongPPL). We generated QA dataset based on the PG19 test set according to the procedure outlined by He et al. [2025]. This results in long context task where an entire book is used as context, along with series of LLM-generated questions and answer pairs with total context lengths of approximately 100K. In order to excel at this task, model must be able to retain all information and facts from the text, which may be asked in the follow-up QA session. We evaluate both PPL and LongPPL, where the latter metric selects subset of tokens that are found to rely heavily on long context for the final loss calculation. LongPPL has been shown to have stronger correlation with long context performance over PPL [Fang et al., 2024]. We use Llama 3.1 8B instruction-tuned models for this experiment. Results can be seen in Table 2 and Figure 6. When 7.02 (+1.91) Streaming LLM Streaming LLM + 5.96 (+0.85) HiP Attention HiP Attention + 3.48 (+0.15) 3.37 (+0.04) 6.29 (+1.18) 5.45 (+0.34) 3.54 (+0.21) 3.41 (+0.08) Flash Attention 2 Long PPL 5.11 (-) 3.33 (-) PPL 6 -bench results. Colors are made relative to the best and worst metrics within each Table 3: model group, with Flash Attention being part of every group. Our correction improves overall performance in every case. En.QAR displays recall for the En.QA subset. Model Method Ctx Len. En.MC En.QA En.QAR En.Sum Passkey Number KV Math.F Avg. Flash Attention 126K 64. 35.89 44.69 31.59 99.13 99.83 92. 24.86 61.57 Llama 3.1 8B Instruct HiP HiP + 126K 54.15 126K 61.14 126K 27.95 Str. LLM Str. LLM + 126K 56. 31.49 33.70 07.25 24.93 38.12 43.54 14.67 33.35 31.06 31.30 20.57 26. 75.08 100.0 02.71 96.27 96.10 97.97 01.36 68.81 30.60 69.60 01.20 00. 18.86 25.71 25.14 25.43 46.93 57.87 12.51 41.66 Flash Attention 384K 82. 44.34 48.82 35.30 100.0 100.0 99. 43.14 69.11 Llama 4 Scout 109B HiP HiP + 384K 74.67 384K 78.60 384K 49.78 Str. LLM Str. LLM + 384K 73. 43.19 42.84 15.23 37.82 48.29 48.14 26.11 43.03 34.28 34.06 31.50 30. 100.0 100.0 52.88 94.75 99.83 99.66 08.31 91.36 99.40 97.20 03.40 46. 41.14 44.29 40.57 40.86 67.60 68.10 28.47 57.35 (a) Perplexity and Long Perplexity (b) cos([AV]i, [AV]i+ν ) Figure 6: (a) Perplexity metrics for increasing γ . For PPL and LongPPL, increasing { the query stride shows slight trend towards higher PPL with higher sparsity. (b) Measures the average cosine similarity between the approximate (AV)i and (AV)i+ν for ν } for Streaming LLM and finds high similarity within γ neighborhood of attention outputs. High similarity implies (AV)i can be reused within the γ neighborhood. 23, . . . , 28 1, . . . , γ { } our Attention is applied on top of both HiP and Streaming LLM, we achieve between 50-75% reduction in the PPL performance gap between quadratic attention. This trend holds true for both PPL and LongPPL. Figure 6 shows the effect of varying the γ parameter form 8-256. As γ also controls the sparsity, we find that as the sparsity increases, both perplexity metrics tend to rise. Infinite Bench. [Zhang et al., 2024a] For both LLama 3.1 8B and Llama 4 Scout 109B, results are displayed in Table 3. The display colors are encoded to show the performance difference within each model group, and including flash attention in all groups. For Llama 4 (Streaming LLM), the addition of resulted in an increase of 40%pt, which leads to recapturing 82% of quadratic attention accuracy (up from 41%). Similarly, for Llama 3.1, the addition of increased overall performance by 29%pt, which moves from 20% of full attention accuracy to recovering 67%. The realized performance gains when applying our method to HiP result in 10%pt increase for Llama 3.1 and 0.5%pt increase for Llama 4. Note that HiP with Llama 4 only shows total of 1.5%pt gap in performance, which means that Attention was able to recapture 33% of the total performance gap. 4.1 Ablation & Analysis Latency. For single attention layer, our method shows large reduction in latency when compared to Flash Attention 2 benchmarked at 1M tokens. In Figure 7, HiP + runs more than 8 times faster. For Streaming LLM + this factor increases to over 32, which means that Attention may perform more than 32 attention operations for single quadratic Flash Attention 2 operation. While our method does require more computation than the standalone sparse methods in Figure 7b, the relative increase is modest in comparison to the latency of quadratic attention. MInference has been excluded from these latency results due to the current public implementation not fully utilizing hardware parallelization in this experiment. For further details, please see Appendix E. 7 (a) Latency vs. Flash Attention (b) Latency vs. Sparse Methods (c) Latency for increasing γ Figure 7: (a) shows latency comparisons against flash attention at 1M tokens. Our method maintains most of the large latency reductions of sparse methods. (b) compares latency against plain sparse methods. Our method introduces slight overhead due to requiring computation equivalent to 1.5% of the whole attention matrix. (c) evaluates the effect of different γ parameters on latency. We find that increasing the stride between queries leads to an expected decrease in latency. How does the affect attention outputs and scores? To study the effect of the correction on the attention outputs and scores, we evaluate both attention output cosine similarity and the Spearman rank correlation coefficient [Spearman, 1904] of the attention rows for the last 128 queries of the prefill. For this, we used sample from the MultiKey-3 RULER (131K) benchmark with the Llama3.1 8B instruction tuned model. subset of layers is depicted in Figure 9, where each point in the plot and histogram is random sample from one of the 32 128 (attention heads and queries). Additional plots for all layers in the network can be seen in Figures 13 to 15 in the appendix. At the key lower layers where the induction heads are known to be most prevalent, we find that the correction results in large corrective shift in both the rank correlation and cosine similarity, making both metrics much closer to the ground truth distributions of quadratic attention. Notably, only using Recompute, which densely recomputes some rows of the attention matrix, is not enough to shift the distribution, as it is indistinguishable from the plain Streaming LLM model in Figure 9. In Section 1, we stated that Attention shifts the distribution of attention outputs towards the distribution which would be seen under fully quadratic attention. Figure 9 provides three more examples of lower layers which show the same shift as shown in Figure 3. It is notable, however, that this strong shift towards the distribution of quadratic attention is not present in all layers of the network. Figures 13 to 15 together show all layers. Attention appears to maintain strong similarity to quadratic attention at the lower layers, which gradually dissipates until layer 10, when the three methods become indistinguishable. However, there is sudden rise in attention output cosine similarity again towards the last layers of the network While both the output cosine similarity and the rank correlation are important, the high rank correlation coefficient provides crucial insight as to how the correction aids in improving performance. For sparse methods, the last 128 queries from 131K context have undergone distributional shift induced by the sparse method, which means that they no longer correctly align with the appropriate key tokens during dot-product attention. high rank correlation, however, implies that the ranking (importance order) of dot products across an entire row of the attention matrix remains largely intact and therefore, should result in outputs with higher similarity to quadratic attention outputs. This suggests that dense decoding can now effectively access information buried deep in the prompt, which is something our experiments show sparse attention methods struggle to do. Table 4: RULER ablation for Equation (5) recompute and Equation (6) . Does Equation (5) or Equation (6) Perform Better? In the previous paragraph we gave qualitative examples of the difference between Equation (5) and Equation (6) on the attention output cosine similarity. Now we ask, how does this observed difference affect the performance of the model? Table 4 shows the effect of recompute from Equation (5), which recomputes selected number of queries with dense attention and does not apply the difference to subsequent tokens in the γ neighborhood. Only recomputing tokens results in 37%pt increase over all context lengths and is only 3%pt short of matching . However, at the longest context length, still delivers more than 11%pt increase in accuracy. Str. LLM + Recompute 52.67 72.71 78.39 . . . 79.99 Str. LLM + 64.40 75.22 81.27 . . . 83.06 27.45 18.59 30.25 . . . 44.25 131K 65K 32K . . . Avg. Str. LLM Model Figure 8 shows recompute compared to Attention for individual subsets of the RULER-131K context length. We find that the only case where recompute outperforms our method is on the 8 Figure 9: For RULER with context length of 131K, we look at the final 128 tokens in the attention output and the final 128 queries in the attention matrix. We compare the cosine similarity of the outputs and the rank correlation of the attention rows to quadratic attention. We find that for both measures, Attention is more similar to quadratic attention. variable tracking subset (VT). We are unsure of the cause of this anomaly, although it is important to note that recompute even outperformed flash attention by approximately 15%pt, which implies that there is some structure within this task that happened to benefit from recompute. In general, flash attention should represent an upper bound to sparse attention, which is what we observe in general. Note that the CWE subset of RULER is removed from this plot, as all methods (including flash attention) score 0% on the 131K context length."
        },
        {
            "title": "5 Discussion & Limitations",
            "content": "Our method presented thus far has been simple extension to existing sparse attention methods, which can be applied with minimal addition of overhead and very simple modification to the attention layer. The common way of computing sparse attention in prior work is to compute an attention output that is dense in the queries and sparse in the keys, so that there is at least one output for every input query token. One way to view our Attention extension is that we are mixing key-sparse (and query-dense) attention output with query-sparse (and key-dense) attention output in order to arrive at representation which is closer to the quadratic attention output that is dense in both the queries and keys. The idea of viewing attention sparsity from both dimensions holds the potential for future works to explore novel ways of combining various combinations of sparse methods in order to approximate the full attention operation. With Lemma 1, we were able to show that the difference of attention outputs approximates the missing attention output, however, we only have empirical evidence of the secondary approximation that (AV)i . While this is empirically validated in our experiments } and by the high cosine similarity in Figure 6b, future works may study this approximation further, which could lead to creating smarter selection criteria for the query sparse attention, as our method uses only fixed hyperparameter to set the size of the gap between query tokens. Figure 8: Comparing the effects of Equation (5) recompute and Equation (6) on RULER 131K subsets. (AV)i+ν for ν 1, . . . , γ {"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we first diagnose harmful distributional shift induced by sparse attention prefill methods. We then propose remedy with our lightweight, sparse-kernel agnostic Attention procedure. Attention corrects sparse outputs to align better with full quadratic attention outputs, requiring only small post-processing step that can be integrated seamlessly into existing inference pipelines. Across all benchmarks, and especially at the longest context lengths, our method delivers significant accuracy gains while maintaining high sparsity and low latency."
        },
        {
            "title": "References",
            "content": "S. Acharya, F. Jia, and B. Ginsburg. Star attention: Efficient llm inference over long sequences. arXiv preprint arXiv:2411.17116, 2024. T. Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. L. Fang, Y. Wang, Z. Liu, C. Zhang, S. Jegelka, J. Gao, B. Ding, and Y. Wang. What is wrong with perplexity for long-context language modeling? arXiv preprint arXiv:2410.23771, 2024. L. He, J. Wang, M. Weber, S. Zhu, B. Athiwaratkun, and C. Zhang. Scaling instruction-tuned llms to million-token contexts via hierarchical synthetic data generation. arXiv preprint arXiv:2504.12637, 2025. C.-P. Hsieh, S. Sun, S. Kriman, S. Acharya, D. Rekesh, F. Jia, Y. Zhang, and B. Ginsburg. arXiv preprint Ruler: Whats the real context size of your long-context language models? arXiv:2404.06654, 2024. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. H. Jiang, Y. Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H. Abdi, D. Li, C.-Y. Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. H. Lee, G. Park, Y. Lee, J. Suh, J. Kim, W. Jeong, B. Kim, H. Lee, M. Jeon, and S. J. Hwang. training-free sub-quadratic cost transformer model serving framework with hierarchically pruned attention. arXiv preprint arXiv:2406.09827, 2024a. H. Lee, G. Park, J. Suh, and S. J. Hwang. Infinitehip: Extending language model context up to 3 million tokens on single gpu. arXiv preprint arXiv:2502.08910, 2024b. Y. Li, Y. Huang, B. Yang, B. Venkitesh, A. Locatelli, H. Ye, T. Cai, P. Lewis, and D. Chen. Snapkv: Llm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469, 2024. T. Musat. Mechanism and emergence of stacked attention heads in multi-layer transformers. arXiv preprint arXiv:2411.12118, 2024. C. Olsson, N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022. J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint, 2019. URL https://arxiv.org/abs/1911. 05507. C. Spearman. The proof and measurement of association between two things. American Journal of Psychology, 15:88103, 1904. J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. W. Wu, Y. Wang, G. Xiao, H. Peng, and Y. Fu. Retrieval head mechanistically explains long-context factuality. arXiv preprint arXiv:2404.15574, 2024. G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. K. Yin and J. Steinhardt. Which attention heads matter for in-context learning? arXiv preprint arXiv:2502.14010, 2025. 10 M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. X. Zhang, Y. Chen, S. Hu, Z. Xu, J. Chen, M. K. Hao, X. Han, Z. L. Thai, S. Wang, Z. Liu, et al. Infinity bench: Extending long context evaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718, 2024a. Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. Ré, C. Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36, 2024b."
        },
        {
            "title": "A Additional Experimental Results",
            "content": "Figures 13 to 15 shows individual plots comparing cosine similarities and rank correlation coefficients against quadratic attention for all layers, analogous to Figure 9. Figure 10 shows an additional study on the γ parameter and latency for HiP, analogous to Figure 7c. Appendix discusses details about the implementation of our method. Appendix discusses details regarding latency for MInference. Figure 12 shows bar charts for the full set of datasets for the RULER 131K context length. Appendix states the computing resources that were used for the experiments in this work."
        },
        {
            "title": "B Broader Impact",
            "content": "We are not aware of any negative potential impacts of our work beyond impacts that are general to all machine learning models. However, lowering the computational cost for inference has the potential to lower costs such as electricity consumption, hardware requirements, and latency for end users. If this can effectively be done with minimal degradation in the performance of the underlying model, it will likely be beneficial to both producers and consumers of AI models."
        },
        {
            "title": "C Implementation Details",
            "content": "In addition to the index selection in Equation (4), in practice, we also select block of queries for dense recomputation at the end of the prefill sequence, both for ease of implementation and also to provide the decoding tokens with the most accurate block of recent context. The block of queries at the end of the sequence allows us to simply reshape tensor and project the correction onto every element in the block, as the tensor that needs delta correction will have regular size which is divisible by γ. For further details, please refer to our code in the supplementary file."
        },
        {
            "title": "D Compute Resources",
            "content": "For LLM inference on the benchmark datasets, we use Google Cloud Platforms 8x NVIDIA H100 node. For latency measurements, we use standalone machine with an NVIDIA RTX 4090 in order to have controlled environment. Here, we show the detailed specification of the latency benchmarking machine: CPU RAM GPU PCIe OS GPU Driver AMD Ryzen 7950X, 16 Core, 32 Thread 128GB, DDR5 5600 Mhz Nvidia RTX 4090, VRAM 24GB Gen 4.0 x8 Ubuntu 22.04.4 LTS 535.171."
        },
        {
            "title": "E Latency of MInference with Delta Attention",
            "content": "We did not report the latency of MInference in the main paper, because MInference shows unusually slower latency than other tested methods, including Flash Attention. We think this is due to (1) insufficient optimization of the publicly available kernel 1 and (2) MInference uses for-loop across the head dimension that prevents the head dimension from being parallelized within the GPU. This limitation of the publicly available implementation will cause the latency to suffer if the attention calculation for each head does not fully utilize the hardware. This for-loop structure was likely implemented in this way because MInference uses different sparse attention strategies for each head. Therefore, as MInference is algorithmically faster than flash attention, we do not report the latency in Figure 7, as we feel this would be misleading to readers who are not familiar with the low-level details of the implementations. 1https://github.com/microsoft/MInference 12 Figure 10: Latency measurements for different settings of γ which controls the gap size between queries and also the overall sparsity of the calculation. This figure accompanies the latency ablation for Streaming LLM in the main text, Figure 7c. We capture the kernel latencies and hardware utilization for MInference. In our analysis with Nsight Systems, their vertical slash pattern kernel _triton_mixed_sparse_attn_fwd_kernel, shows around 32 milliseconds latency for single head, while flash attention shows only 462 milliseconds for 32 heads. The MInference kernel shows noticeably low utilization of streaming multiprocessor warps, which is around 9%. However, for completeness, we put the latency measurements of MInference in Table 5. In our measurement using their official codebase without meaningful modification, with pre-compiled model configuration for head-wise sparse method settings, Minference is about 1.377 times slower than Flash Attention. We believe this is only due to the lack of fully parallelized kernel and not the design of the method. Table 5: Prefill latency measurements (ms) that include MInference on RTX 4090 up to 256K context length. 32K 64K 128K 256K FA HiP HiP + Minference 34.27 53.61 55.44 135.28 119.77 118.53 123.49 395.92 462.39 255.05 268.02 1083.66 1858.60 562.24 602.74 2559."
        },
        {
            "title": "F Approx Window Size Calculation",
            "content": "When comparing out method to Streaming LLM, we would like to know how much computation overhead is increased in order to estimate the approximate window size of our method due to the fact that Attention computes extra tokens. We can calculate this as follows with as the window size in single row of the attention matrix, our method will compute every γth row of the attention matrix which would be equivalent to 2γ when amortized into each row calculation. This brings the total calculation per row to + 2γ . In the case of 131K context, window size of 2048, and γ = 64 (our standard setting) this would be evaluated as 2048 + 217 2(26) = 2048 + 210 = 2048 + 1024 = 3072. Restatement and proof of Lemma 1 We want to show that the difference of AV is approximately equal to the missing AV delta-shaped attention output, which is pictured in Figure 5. w.l.o.g., we will consider single arbitrary row of the attention matrix and single column vector from the values v. The following 13 is true regardless of the selected entries in a, however, in order to create tighter error bound, we assume the existence of sparse attention method which chooses the largest attention values in when calculating the sparse dot product va. Specifically, Lemma (Lemma 1). Let = (a1, . . . , ad) satisfies, a2 then any exact top-k sparse attention method which selects the top-k attention scores should select the last elements of a. Fix an integer 1 Rd be the pre-softmax vector which is sorted and . Define a1 aN ,"
        },
        {
            "title": "Set",
            "content": "N H = eai, = eai, = + T. i=1 (cid:88) i=N k+1 (cid:88) ai = eai , 0, i = eai , i > k, k. For any = (v1, . . . , vd) tail-max as, Rd which is sorted according to the rank of elements in a, define the write we have the exact decomposition where the remainder term Mtail = max i>N vi . (cid:12) (cid:12) (cid:12) (cid:12) av, = av k = ai vi + R, i=1 (cid:88) = ai i=N k+1 (cid:88) (cid:2) vi (cid:3) is upper bounded by Therefore, Proof. Split For > k, (cid:12) (cid:12) (cid:12) (cid:12) i=1 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) + Mtail. σ(a)i vi = + Mtail (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = ai vi + ai i=1 (cid:88) i=N k+1 (cid:88) (cid:2) = ai vi + R. vi (cid:3) i=1 (cid:88) ai = eai + = eai + = i + , 14 (8) (9) (10) (11) so"
        },
        {
            "title": "Thus",
            "content": "a = ai = i + 1 (cid:19) (cid:18) = + + . = + i vi, i=N k+1 (cid:88) and since i=N k+1 = 1 and vi Mtail on the tail, (cid:80) completing the proof. = N + (cid:12) i=N k+1 (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) + i=N k+1 (cid:88) Mtail. + vi (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) vi (12) (13) (14) (15) (16) (17) as is the expected outcome with sparse attention, then the bound becomes If we assume that tighter, as the denominator + H. This implies that better sparse top-k approximations will result in lower error bound. We empirically verified this difference in Figure 11, which analyzes both the error bound and the empirical error on real input from the RULER-131K subset. Figure 11a measures the bound and empirical error of an oracle top-k attention while Figure 11b measures the same bound and empirical error for Streaming LLM, which chooses sliding window and attention sink. We find that the bound is generally tighter for the oracle top-k attention, but in both cases, the overall empirical approximation error remains low. (a) Oracle top-k sparse attention. (b) Streaming LLM. Figure 11: Empirically analyzing the approximation and bound from Lemma 1. more precise sparse top-k attention method, such as an oracle (a) maintains tighter bound on the approximation error. Streaming LLM (b) results in looser bound, however the empirical approximation error remains low. Figure 12: All RULER 131K subsets. This is companion to Figure 1. The CWE subset is excluded, as all models, including quadratic attention, scored 0%. 16 Figure 13: Attention output cosine similarity (compared to full attention) for Streaming LLM with our method. Figures 13 to 15 show the results from every layer, and are counterpart to Figure 9 in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention. 17 Figure 14: Attention output cosine similarity (compared to full attention) for Streaming LLM with our method. Figures 13 to 15 show the results from every layer, and are counterpart to Figure 9 in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention. Figure 15: Attention output cosine similarity (compared to full attention) for Streaming LLM with our method. Figures 13 to 15 show the results from every layer, and are counterpart to Figure 9 in the main text. For the lower layers where induction heads are most prevalent, our method shows higher cosine similarity and attention row rank correlation as compared to quadratic attention."
        }
    ],
    "affiliations": [
        "DeepAuto.ai",
        "KAIST"
    ]
}