{
    "paper_title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning",
    "authors": [
        "Chi Ruan",
        "Dongfu Jiang",
        "Yubo Wang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 4 2 8 2 2 . 9 0 5 2 : r a"
        },
        {
            "title": "Work in Progress",
            "content": "CRITIQUE-CODER: ENHANCING CODER MODELS BY CRITIQUE REINFORCEMENT LEARNING Chi Ruan1 Dongfu Jiang1,2 Yubo Wang1,2 Wenhu Chen1,2 1University of Waterloo 2Vector Institute cruan059@uottawa.ca wenhuchen@uwaterloo.ca https://tiger-ai-lab.github.io/Critique-Coder"
        },
        {
            "title": "ABSTRACT",
            "content": "Reinforcement Learning (RL) has emerged as popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and CritiqueGuided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating critique for given (question, solution) pair. The reward is determined solely by whether the final judgment label {True, False} of the generated critique aligns with the ground-truth judgment c. Building on this point, we introduce CRITIQUE-CODER, which is trained on hybrid of RL and CRL by substituting 20% of the standard RL data with CRL data. We fine-tune multiple models (CRITIQUE-CODER) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that CRITIQUE-CODER consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our CRITIQUE-CODER-8B can reach over 60% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, CRITIQUE-CODER also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across broad range of tasks. Hence, we believe that CRL works as great complement to standard RL for LLM reasoning. Figure 1: The effectiveness of our CRITIQUE-CODER, trained with combination of CRL and RL data, compared to baselines and models trained solely on RL data, with both training and evaluation conducted under the think mode setting. EvalPlus denotes the average of 4 benchmarks: HumanEval, MBPP, and their corresponding plus version."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent breakthroughs in complex reasoning across code generation, mathematical problem-solving, and logical deduction have been driven by large language models such as OpenAIs o1-o4 (Jaech et al., 2024a), DeepSeek-R1 (Guo et al., 2025), and Kimi-K1.5 (Team et al., 2025). key factor behind these advances is the combination of reinforcement learning (RL) with chain-of-thought"
        },
        {
            "title": "Work in Progress",
            "content": "(CoT) (Wei et al., 2022), which enables models to iteratively refine intermediate reasoning steps. Building on this foundation, research has increasingly focused on scaling reasoning abilities in code generation. For example, AceCoder (Zeng et al., 2025), HardTests (He et al., 2025), and KodCoder (Xu et al., 2025) developed automated large-scale code data generation pipelines and applied RL using reward models and test cases pass rewards, achieving notable performance gains. SWERL (Wei et al., 2025) pioneered the scaling of RL-driven LLM reasoning for real-world software engineering, leveraging an efficient rule-based reward system. Standard reinforcement learning with verifiable reward (RLVR) has shown strong capabilities in improving models problem-solving abilities. However, this paradigm can hardly elicit the internal critique or reflection behavior on existing solutions. Recently, there has been line of work (Wang et al., 2025c; Gou et al., 2024; Xi et al., 2024; Kapusuzoglu et al., 2025; Wang et al., 2025a; Tang et al., 2024) aiming to explicitly teach LLMs to critique to unleash their reasoning ability. Inspired by this, we propose new learning paradigm, Critique Reinforcement Learning (CRL), which incorporates this critique mechanism into RL and explicitly rewards models for accurate reflection. CRL not only optimizes problem-solving skills but also explicitly incentivizes its critique abilities via rewards for whether it can correctly judge responses correctness. Specially, the model is prompted with questionsolution pair [q; s] to predict binarized judgment label {True, False}, which is compared with the annotated label to derive binary (verifiable) reward. This is in contrast to the standard RLVR algorithm, which incentivizes the model to predict correct solution to given query q, as shown in Figure 2. Different from CFT (Wang et al., 2025c), CRL incentivizes the model based on its self-generated judgment instead of using teacher-provided critique traces. Based on the CRL paradigm, we develop CRITIQUE-CODER, model trained to generate both highquality coding solutions and critiques on existing solutions. We conducted series of experiments with the GRPO algorithm (Shao et al., 2024). Specifically, we train QWEN3-4B and QWEN38B (Yang et al., 2025) on the filtered rStar seed dataset (Liu et al., 2025) using hybrid framework that unifies CRL and standard RLVR. This hybrid approach enables the model to integrate the strengths of both paradigms: CRL helps the model develop critical thinking and reasoning abilities, while RLVR focuses on enhancing its problem-solving performance. We further adopted an iterative context lengthening approach Luo et al. (2025), where the training context length is extended from 16k to 32k tokens to better leverage the potential of long reasoning chains. By iteratively increasing the context length, the model first develops reasoning skills on shorter contexts, which can then be applied to longer ones. CRITIQUE-CODER present consistent improvements across multiple benchmarks, illustrated in Figure 1. From QWEN3-4B, our CRITIQUE-CODER achieves 59.0 accuracy on LiveCodeBench (v5) (Jain et al., 2024), yielding +4.8 points over the base model and +2.4 points over the RL-only variant. Remarkably, it even surpasses QWEN3-8B by +1.5 points. On QWEN3-8B, CRITIQUECODER reaches 35.6 points on Aider-Polyglot, +7.2 points higher than baseline. It also reaches 60.8 points on LiveCodeBench (v5), which outperforms other reasoning models like DeepCoder14B (Luo et al., 2025) and GPT-o1 (Jaech et al., 2024b). This showcases the effectiveness of CRL training. Furthermore, results on the logical reasoning benchmark BIG-Bench Extra Hard (Kazemi et al., 2025) demonstrate that CRITIQUE-CODER achieves strong transferable reasoning ability, surpassing both baseline and RL-trained models and yielding +6.1 improvement over the base model. We also find that CRL is more effectively utilized as complement to RL rather than serving as an alternative. This is because CRL training primarily focuses on critiquing questionsolution pairs without generating actual solutions. Our ablation study in Table 5 confirms 20% mix ratio as best practice. In summary, we introduce CRL, novel reinforcement learning (RL) training framework that incorporates critique learning within the RL paradigm. This novel learning approach enhances the models critique and reasoning abilities, addressing the lack of critique and reflection incentives typically found in standard RL frameworks. Building on this foundation, we introduce CRITIQUECODER, model combining CRL and RL to leverage the strengths of both. CRL fosters critical thinking and reasoning, while RL focuses on optimizing problem-solving. Compared to baseline models and those trained exclusively with RL, CRITIQUE-CODER shows superior performance across coding datasets of varying difficulty. Furthermore, the model demonstrates transferable general reasoning abilities, as evidenced by its strong performance on logic reasoning benchmarks."
        },
        {
            "title": "2 METHOD",
            "content": "Figure 2: Comparison between CRL and Standard RL. Standard RL generates solutions based on input questions and evaluates them by executing test cases, while CRL critiques the solution for the paired question and compares the resulting conclusion with the GT to determine its correctness. Experiment shows that RL+CRL can improve not only accuracy, but also the code quality. 2.1 PRELIMINARY Problem Definition. CRITIQUE-CODER incorporates two complementary training frameworks for LLMs. The first follows the standard RL setting: given question q, the policy πθ samples candidate solutions {si}n i=1; each si is evaluated on the annotated test cases to compute its pass rate, which serves as the reward signal Rrl,i. To complement this solution-level feedback, we introduce Critique Reinforcement Learning (CRL), which provides binary correction signals on questionsolution pairs. Specially, given an annotated dataset = {([qk; sk], k=1, where each pair ([q; s]) consists of question and solution with an associated binary judgment label {0, 1}, the policy πθ is trained to generate predictions {ci}n i=1 indicating whether satisfies the requirement posed by q. The reward Rcrl,i is derived from the comparison between ci and c. k)}N Finally, two reward signals Rrl,i from RL and Rcrl,i from CRL are combined together to update policy parameters θ using GRPO. This unified optimization enables the model to benefit from both critique-guided learning and task-oriented learning, fostering more critical and reflective learning. Group Relative Policy Optimization (GRPO). We now detail GRPO (Shao et al., 2024), the optimization algorithm used to update model parameters. In contrast to PPO (Schulman et al., 2017), GRPO enhances performance by leveraging relative performance-based updates, which yield more stable and efficient policy refinement. The formal definition of GRPO is provided below: (θ) = 1 (cid:88) i=1 1 oi oi (cid:88) t=1 (cid:16) min ρi,t ˆAi,t, clip(ρi,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) β DKL (cid:0)πθπref (cid:1) , (1) where ρi,t = πθ(oi,t x, oi,<t) πθold (oi,t x, oi,<t) . In the above equation, ρi,t denotes the probability ratio of generating output oi,t under the new policy πθ and old policy πθold , ˆAi,t represents the calculated advantage within each output group, and DKL regularizes the optimization by measuring the divergence between πθ and the reference policy πθref , which can prevent the policy from drifting too far away. In our training scenario, the policy input can be either single question from RL or questionsolution pair ([q; s]) from CRL. These two input modalities give rise to distinct reward signals, solution-level rewards Rrl,i and critique-level rewards Rcrl,i. Both signals are aggregated in the"
        },
        {
            "title": "Work in Progress",
            "content": "Table 1: Dataset statistics of rStar-Coder seed dataset before and after test-case filtering to save the verification time. Table 2: Dataset difficulty statistics using QWEN3-4B (thinking) with temperature=0.6, top-p=0.95, and top-k=20 Metric Num of Questions Avg Test Cases Median Test Cases Avg Test Case Input Chars Before After 29,365 87 48 96,208 23,069 24 30 40 Metric Pass@1 Pass@2 Pass@4 Value 43.72% 49.05% 52.98% Avg Tokens / Solution 13,732 computation of the advantage ˆAi,t, which makes the GRPO update in our framework fundamentally different from standard RL: the advantage estimation is jointly shaped by task outcomes and critique guidance, allowing the policy to align with both execution correctness and reflective judgment."
        },
        {
            "title": "2.2 DATASET CONSTRUCTION",
            "content": "To evaluate the efficacy of CRL, the first step is to build reliable CRL dataset. The construction process is detailed in the following steps. RL Dataset Selection. We construct our CRL dataset from the human-seeded RL dataset of rStarCoder (Liu et al., 2025), which contains large number of test cases collected from both humanwritten and verified generated data. To generate these test cases, RStar-Coder employs utility functions to generate test case inputs across wide range of scales, reaching up to 105 for challenging cases. As result, many questions in the original dataset include an excessive number of test cases (often exceeding 100 per problem), and some individual cases are extremely long (over 10,000 tokens). Such characteristics substantially increase verification time during RL training. To improve efficiency and consistency, we filter the data by discarding test cases longer than 200 tokens and randomly sampling 30 cases for each problem. Table 1 reports dataset statistics before and after filtering, showing significant reduction in both test case length and volume. Specifically, the average input characters decrease from 96,208 to 40, and the average number of cases drops from 87 to 24. This reduction greatly shortens test case evaluation time during training, resulting in more efficient learning process. To assess dataset difficulty, we evaluate QWEN3-4B (Yang et al., 2025) on the filtered dataset, shown in Table 2. The model achieves 43.72% at Pass@1 and 52.98% at Pass@4, indicating moderate difficulty levelsolvable in part, yet leaving significant headroom for further progress. This makes the dataset well-suited for RL training under the GRPO algorithm, where advantage is computed within groups. Figure 3: Critique data generation. This process involves generating candidate solutions and annotating their judgment in the CRL dataset based on the pass rate over test cases. Critique Data Generation. Figure 3 illustrates the critique data generation workflow. For each problem, we prompt QWEN3-CODER-30B-A3B-INSTRUCT (Yang et al., 2025) to generate outputs, from which we extract code blocks as candidate solutions. Empty code blocks are discarded to ensure that only valid programs are retained for evaluation. Each candidate solution is then executed on the test cases from the filtered dataset, and its pass rate is computed to determine its judgment. practical challenge is that certain test cases exhibit excessively long execution times, which may cause timeouts and lead to the erroneous classification of correct solutions as failures. To relax this"
        },
        {
            "title": "Work in Progress",
            "content": "constraint, we adopt pass rate threshold of 80%: candidate solutions are labeled as True if their test pass rate exceeds this percentage, and as False otherwise. Hybrid Data Integration. In our case, training exclusively on critique-oriented data biases the model toward overly focusing on evaluative feedback rather than task-oriented solutions, thereby hindering its ability to directly generate answers in evaluation tasks. To mitigate this issue, we construct hybrid dataset that combines both CRL and standard RL data. This approach balances the characteristics of both data types and exposes the model to diverse range of scenarios during training. Concretely, we randomly assign 20% of the data from the dataset to be CRL data, with the remaining 80% consisting of standard RL data. Such configuration not only mitigates the risk of format shift but also improves the robustness of the learning process by jointly exploiting the complementary advantages of both CRL and RL training paradigms."
        },
        {
            "title": "2.3 TRAINING",
            "content": "Training procedure. Algorithm 1 presents the training procedure of CRITIQUE-CODER, which integrates CRL and RL within unified framework. The policy model is initialized from pretrained checkpoint πθinit and optimized on hybrid dataset comprising CRL samples with judgment and RL samples with test cases. For CRL data, the model generates multiple candidate critiques, extracts judgment predictions from the conclusion{} field, and assigns rewards according to their alignment with the GT labels. For RL data, the model produces solution candidates, extracts the code block enclosed by [code], and evaluates them against the given test cases to derive rewards. In both cases, the rewards are converted into advantage estimates, which subsequently guide the policy update through GRPO. Reward function. As specified in Algorithm 1, rewards are computed from two data sources: CRL and RL. For CRL samples, the model is prompted to store the final judgment in conclusion{}, from which the predicted label is extracted. reward of 1 is assigned if matches the ground truth c; otherwise, including the case where the prediction is missing, the reward is 0. For RL samples, the reward is defined as the pass rate across test cases. Formally, Rcrl(c, c) = (cid:26)1, 0, if = c, otherwise, , Rrl(s, ) = (2) where is the set of test cases and denotes the number of cases successfully solved by the models output s. Thus Rrl [0, 1], with larger values indicating more reliable solutions. At the batch level, each instance receives its reward according to its data type: Ri = (cid:40)Rcrl(ci, ), if BCRL, Rrl(si, Ti), if BRL. (3) Here BCRL and BRL denote the CRL and RL subsets within the batch, respectively."
        },
        {
            "title": "3 EXPERIMENT",
            "content": "3.1 TRAINING SETUP We conducted experiments on two models, QWEN3-4B and QWEN3-8B (Yang et al., 2025), in thinking mode. Following two-phase training strategy similar to DeepCoder (Luo et al., 2025), we set the maximum response length to 16k in the first phase and increase it to 32k once the rewards have stabilized. During training, two rule-based rewards are employed, each tailored to different data type. For CRL data, the reward is 1.0 if the prediction matches the ground truth (GT) and 0.0 otherwise; additionally, during the 16k phase, this reward is scaled by factor of 0.8 to reduce its dominance relative to RL signals. For RL data, the reward corresponds to the pass rate over test cases, ranging from 0.0 to 1.0. For standard RL training, we adopt the thinking prompt used in the Qwen3 paper on LiveCodeBench, while the CRL training prompt is provided in Appendix A. Throughout training, we apply the GRPO algorithm (Shao et al., 2024), which provides improved stability and efficiency compared to PPO (Schulman et al., 2017). The hyperparameters are set as follows: batch size of 128, learning rate of 1e-6, and 8 sampled outputs per prompt. To encourage exploration while stabilizing entropy, the clipping ratio is asymmetric, with an upper bound of 0."
        },
        {
            "title": "Work in Progress",
            "content": "j=1, policy πθ )}N1 if = ([q; s], c) i=1 {(qj, tj)}N2 Sample batch for each data instance do Algorithm 1 Training procedure of CRITIQUE-CODER Input dataset = {([qi; si], 1: Initialize policy model πθ πθinit 2: for each step do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for else if = (q, t) i=1 πθ(q) end if Sample outputs {oi}G i=1 πθ([q; s]) Parse each oi to extract judgment ci inside conclusion{} Compute reward Rcrl,i(ci, c) for each ci Sample outputs {oi}G Parse each oi to extract solution si enclosed by [code] Evaluate si on test cases t, obtain reward Rrl,i(si, t) end for Compute ˆAi,t from reward Ri, where Ri = Rcrl,i(ci, c) for CRL or Rrl,i(si, t) for RL. Update the policy model πθ with GRPO ( Equation 1) CRL data with judgment then RL data with test cases then and lower bound of 0.2. We trained the model on the entire dataset for one epoch, and selected the best-performing checkpoint using the LiveCodeBench (v5) as the validation set. 3.2 EVALUATION SETUP To evaluate and compare our training results, we utilized four different benchmarks: EvalPlus (Liu et al., 2023) (an average of HumanEval (Chen et al., 2021), HumanEval+, MBPP (Austin et al., 2021), and MBPP+), BigCodeBench-Instruct (Zhuo et al., 2024), Aider-Polyglot (Aider, 2024), and LiveCodeBench (v5, 2024.102025.02) (Jain et al., 2024). These benchmarks cover diverse range of coding tasks, enabling comprehensive assessment of the models code generation ability. For the sampling configuration, we follow the thinking mode settings reported in the original Qwen3 paper (Yang et al., 2025), with temperature of 0.6, top-p of 0.95, top-k of 20, and maximum output length of 32,768 tokens. The same configuration is applied consistently across all evaluation tasks. For LiveCodeBench specifically, we adopt the official evaluation prompt used in Qwen3 thinking mode. We also compare with some existing strong coding models like DeepSeek-R1distill-14B (Guo et al., 2025), DeepCoder (Luo et al., 2025), DeepSeek-V2.5 (DeepSeek-AI, 2024), and GPT-o1 (Jaech et al., 2024b) with high reasoning effort."
        },
        {
            "title": "3.3 MAIN RESULTS",
            "content": "Gains over Base Models. Compared with the base models, CRITIQUE-CODER leads to consistent and notable improvements across benchmarks of varying difficulty levels. On QWEN3-4B, for example, the LiveCodeBench score of CRITIQUE-CODER rises from 54.2 to 59.0, gain of +4.8, surpassing the larger QWEN3-8B baseline by +1.5 points. On the Aider-Polyglot benchmark, which consists of multiple programming languages, CRITIQUE-CODER still demonstrates strong performance, achieving +7.2 improvement from QWEN3-8B baseline, despite being trained solely on Python data using CRL. These results indicate the effectiveness of our algorithm. Advantages over RL-trained Models. Under identical datasets and training configurations, replacing part of the RL data with CRL consistently yields superior results across all benchmarks. On QWEN3-4B, CRITIQUE-CODER exceeds the Qwen3-4B-RL by +2.4 points on LiveCodeBench and improves the overall benchmark average by +1.5 points. On QWEN3-8B, it outperforms the Qwen38B-RL counterpart by +2.7 points on BigCodeBench-Hard, contributing to an average gain of +1.7 points across all benchmarks. These findings highlight that CRL brings complementary benefits over RL, enabling CRITIQUE-CODER to achieve more robust and consistent improvements. We further analyze the test outputs of CRL and standard RL on LiveCodeBench, as illustrated in Figure 4."
        },
        {
            "title": "Work in Progress",
            "content": "Table 3: CRITIQUE-CODER performance compared with baseline and models trained with standard RL. The RL training was conducted on the filtered rStar-Coder seed dataset, and the CRL training was carried out by converting 20% of the data into CRL for fair comparison. Model EvalPlus BigCodeBench-I Full Hard Aider-Polyglot LiveCodeBench v5 AceCoder-7B DeepSeek-R1-Distill-14B DeepCoder-14B DeepSeek-V2.5-238B GPT-o1 82.7 82.4 85.3 83.8 88.6 43.3 38.1 38.2 48.9 50.4 19.6 20.9 18.2 27.0 28.4 Baseline Qwen3-4B-RL CRITIQUE-CODER (Ours-Baseline) Baseline Qwen3-8B-RL CRITIQUE-CODER (Ours-Baseline) Baseline = Qwen3-4B (Thinking) 85.2 84.9 86.5 +1.3 42.0 40.6 43.1 +1.1 20.9 23.0 23.0 +2.1 Baseline = Qwen3-8B (Thinking) 85.8 86.2 87.7 +1.9 44.6 44.5 46.6 +2.0 23.6 24.3 27.0 +3.4 - 18.6 18.4 17.8 61.7 21.8 23.6 24.4 +2.6 28.4 34.5 35.6 +7. - 53.0 60.6 42.6 59.5 54.2 56.6 59.0 +4.8 57.5 59.6 60.8 +3.3 AVG - 42.6 44.1 44.0 57.7 44.8 45.7 47.2 +2. 48.0 49.8 51.5 +3.5 Figure 4: Analysis of the generations on the LiveCodeBench (v5) problems. Results show that CRL can elicit better reasoning behavior and coding quality. Figure 5: Test-time scaling performance of CRITIQUE-CODER-4B on LiveCodeBench (v5) The results show that CRL generates longer reasoning traces in the think blocks, indicating more extensive deliberation and reflection, and confirming that CRL indeed enhances the models reasoning and critique capabilities. It also incorporates markedly more explanatory comments within the generated code, indicating stronger tendencies toward self-explanation. Comparison with Frontier Models. Our 4B and 8B models are also highly competent in their absolute performance. CRITIQUE-CODER-4B can beat the DeepCoder-14B (Luo et al., 2025) significantly across the board despite having only 28% of its parameters. CRITIQUE-CODER-8B can beat other strong models and only lags behind GPT-o1 (Jaech et al., 2024b) on Aide-Polyglot, which is mainly due to not optimizing other languages beyond Python. On EvalPlus, CRITIQUE-CODER4B scores an impressive 86.5, just behind GPT-o1, while CRITIQUE-CODER-8B takes it step further with an even higher score of 87.7. 3.4 TRANSFERABLE REASONING ABILITY To examine whether the critique and reasoning abilities learned by CRITIQUE-CODER extend beyond coding tasks, we further evaluate the model on the BIG-Bench Extra Hard (BBEH) logic reasoning benchmark (Kazemi et al., 2025). As shown in Table 4, CRITIQUE-CODER achieves"
        },
        {
            "title": "Work in Progress",
            "content": "Table 4: Performance comparison on four BIG-Bench Extra Hard (BBEH) logic reasoning subtasks Model Time Arithmetic DisambiguationQA Zebra Puzzles BoardgameQA AVG Baseline = Qwen3-4B (Thinking) Baseline Qwen3-4B-RL CRITIQUE-CODER (Ours-Baseline) 40.5 45.0 48.5 +8. 43.3 43.3 47.5 +4.2 36.5 40.5 44.0 +7.5 66.5 66.5 71.0 +4.5 46.7 48.8 52.8 +6.1 Table 5: Impact of different CRL data proportion. All datasets are derived from the filtered rStarCoder seed dataset, with varying proportions of RL data converted into CRL data. Model EvalPlus BigCodeBench-I Full Hard Aider-Polyglot LiveCodeBench v5 AVG Baseline = Qwen3-4B (Thinking) 0% of CRL Data 50% of CRL Data 100% of CRL Data 20% of CRL Data (Ours) 84.9 86.5 85.2 86.5 40.6 42.4 41.6 43.1 23.0 22.3 17.6 23.0 23.6 24.0 21.3 24. 56.6 56.0 56.6 59.0 45.7 46.2 44.5 47.2 consistent improvements over both the baseline QWEN3-4B and its RL-trained variant across all four reasoning subtasks. In particular, it outperforms the Qwen3-4B-RL model by +4.5 points on BoardgameQA, leading to an overall average gain of +4.0 points, and surpasses the base model by +6.1 points on average. These results indicate that the critique-enhanced training paradigm not only improves performance in coding benchmarks but also strengthens general reasoning capabilities. 3.5 TEST TIME SCALING We further evaluate CRITIQUE-CODER-4B with sequence test-time scaling (budget forcing) (Muennighoff et al., 2025), which extends the reasoning length at inference. On LiveCodeBench (v5), removing the constraint on reasoning tokens enables the model to achieve score of 62.0 after four iterations, representing +3.0 improvement compared to the setting without test-time scaling, as reported in Figure 5. This result is highly valuable to show that our 4B model can even beat other much larger baselines like DeepCoder-14B. 3.6 ABLATION STUDY To examine how the proportion of CRL data affects training, we perform an ablation study by varying the percentage of RL data replaced with CRL data, while keeping the overall dataset size fixed. As shown in Table 5, introducing moderate amount of CRL data consistently improves performance over the pure RL setting. In particular, using 20% CRL data yields the best overall results, surpassing both the baseline (0%) and higher proportions (50% and 100%) across most benchmarks. Notably, while 50% CRL still maintains comparable performance, fully replacing RL data with CRL leads to clear degradation, suggesting that CRL is most effective as complement to RL rather than complete substitute. These findings highlight the importance of balancing RL and CRL data, with small proportion of CRL providing the most robust gains. Otherwise, there will be mismatch between training outputs and inference behavior during evaluation, as training solely with CRL drives the model to focus excessively on judgment while neglecting solution generation. 3.7 LIMITATIONS IN SELF-CRITIQUE Although incorporating CRL enhances the models reasoning and critique abilities, it does not exhibit self-critique capability. To explore this limitation, we implemented critique-based parallel testtime scaling on CRITIQUE-CODER. Specifically, on LiveCodeBench v5, the CRITIQUE-CODER-4B was prompted to generate 10 candidate solutions for each question. These solutions, along with their corresponding question, were subsequently fed back into the model for critique, with each solution"
        },
        {
            "title": "Work in Progress",
            "content": "being critiqued for 64 samples. We then attempted to identify the best solution by leveraging these critiques. The solutions were scored based on the number of True critiques they received. If multiple solutions received identical scores, the solution with the shortest critique thinking token was selected. However, this approach did not yield performance improvements, indicating that the model lacks genuine self-critical ability."
        },
        {
            "title": "4.1 CRITIQUE LEARNING",
            "content": "The idea of leveraging critiques for improving model reasoning has been explored in several lines of research. One direction is self-correction (Bai et al., 2022; Gou et al., 2023; Madaan et al., 2023; Shinn et al., 2023), where models iteratively evaluate and revise their own outputs. Although such methods are promising, subsequent studies have questioned their robustness and consistency (Huang et al., 2023; Valmeekam et al., 2023). Another line involves reward models (Uesato et al., 2022; Wang et al., 2023; Lightman et al., 2023), which act as learned evaluators that assign quality scores to either final outputs or intermediate reasoning steps, thereby guiding reinforcement learning to enhance reasoning capabilities. More recently, Critique Fine-Tuning (CFT) (Wang et al., 2025c;b) explicitly trains models to critique candidate solutions, demonstrating improved reasoning ability. Our approach is most related to CFT. Unlike CFT, which directly optimizes the model to imitate the critique reasoning process, CRL instead encourages the model to actively explore and learn from the correctness of its final judgments, thereby combining the benefits of critique reasoning with reinforcement feedback. 4.2 REINFORCEMENT LEARNING FOR CODE GENERATION Code generation, core capability of LLMs, has received considerable attention. CodeRL (Le et al., 2022) introduces pioneering RL framework for code generation, employing an actorcritic architecture to encourage functionally correct outputs. Building on this foundation, PPOCoder (Shojaee et al., 2023) incorporates the PPO algorithm to further stabilize and improve training. Moreover, RLEF (Gehring et al., 2024) advances the paradigm by explicitly leveraging execution feedback during synthesis. More recently, AceCoder (Zeng et al., 2025) proposes scalable pipeline that automatically constructs questiontest case pairs from code data to facilitate RL training. 4.3 CHAIN-OF-THOUGHT REASONING. Recent advances in reasoning language models (RLLMs) show that extended chain-of-thought (CoT) reasoning substantially improves performance on tasks like coding and mathematics. OpenAI o1 (Jaech et al., 2024a) and DeepSeek R1 (Guo et al., 2025) exemplify this trend by using inferencetime scaling, where models iteratively explore and reflect before converging on solution. Building on this, KIMI K1.5 (Team et al., 2025) simplifies the reinforcement learning framework while incorporating long-context scaling and enhanced policy optimization, further advancing reasoning efficiency. More recently, Qwen3 (Yang et al., 2025) combines thinking mode for reasoning with non-thinking mode for fast responses, switching between them to balance latency and performance."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduce Critique Reinforcement Learning (CRL), novel reinforcement learning paradigm that integrates critique learning into the RL by incorporating feedback on the correctness of critiques predicted by the model. Building on this foundation, we developed CRITIQUE-CODER, trained on mix of RL and CRL data. On LiveCodeBench v5, CRITIQUE-CODER-4B achieves score of 59.0, outperforming the baseline by +4.8 points and the RL-only model by +2.4 points. In addition to coding tasks, CRL also enhances general reasoning ability. On the BBEH logical reasoning benchmark, CRITIQUE-CODER shows substantial improvements, surpassing the baseline and RL-trained models by +6.1 and +4.0 points on average across four subtasks. These results demonstrate that CRL not only boosts critique and reflection abilities over standard RL but also enables these capabilities to extend beyond coding domains. However, ablation studies reveal that"
        },
        {
            "title": "Work in Progress",
            "content": "training exclusively on CRL data yields poorer performance than RL alone, since CRL focuses on generating critiques rather than task-oriented solutions, leading to mismatch with evaluation requirements. Therefore, rather than substituting RL, CRL serves as powerful complement to it. Taken together, our findings demonstrate that CRL enhances standard RL by endowing models with stronger critique and reasoning abilitiescapabilities that manifest not only in coding tasks but also transfer effectively to broader reasoning domains."
        },
        {
            "title": "REFERENCES",
            "content": "Aider. Aider-polyglot benchmark. https://aider.chat/2024/12/21/polyglot.htm l#the-polyglot-benchmark, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. DeepSeek-AI. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model, 2024. Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Nan Duan, Weizhu Chen, et al. Critic: Large In The Twelfth International language models can self-correct with tool-interactive critiquing. Conference on Learning Representations, 2024. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, and Lei Li. Hardtests: Synthesizing high-quality test cases for llm coding. arXiv preprint arXiv:2505.24098, 2025. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct reasoning yet. arXiv preprint arXiv:2310.01798, 2023. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024a. OpenAI: Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, Alex Iftimie, Alex Karpenko, Alex Tachard Passos, Alexander Neitz, Alexander Prokofiev, Alexander Wei, Allison Tam, Ally Bennett, Ananya Kumar, Andre Saraiva, Andrea Vallone, Andrew Duberstein, Andrew Kondrich, Andrey Mishchenko, Andy Applebaum, Angela Jiang, Ashvin Nair, Barret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin Sokolowsky, Boaz Barak, Bob McGrew, Borys Minaiev, Botao Hao, Bowen Baker, Brandon Houghton, Brandon McKinzie, Brydon Eastman, Camillo Lugaresi, Cary"
        },
        {
            "title": "Work in Progress",
            "content": "Bassin, Cary Hudson, Chak Ming Li, Charles de Bourcy, Chelsea Voss, Chen Shen, Chong Zhang, Chris Koch, Chris Orsinger, Christopher Hesse, Claudia Fischer, Clive Chan, Dan Roberts, Daniel Kappler, Daniel Levy, Daniel Selsam, David Dohan, David Farhi, David Mely, David Robinson, Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Freeman, Eddie Zhang, Edmund Wong, Elizabeth Proehl, Enoch Cheung, Eric Mitchell, Eric Wallace, Erik Ritter, Evan Mays, Fan Wang, Felipe Petroski Such, Filippo Raso, Florencia Leoni, Foivos Tsimpourlas, Francis Song, Fred von Lohmann, Freddie Sulit, Geoff Salmon, Giambattista Parascandolo, Gildas Chabot, Grace Zhao, Greg Brockman, Guillaume Leclerc, Hadi Salman, Haiming Bao, Hao Sheng, Hart Andrin, Hessam Bagherinezhad, Hongyu Ren, Hunter Lightman, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian Osband, Ignasi Clavera Gilaberte, Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina Kofman, Jakub Pachocki, James Lennon, Jason Wei, Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quinonero Candela, Joe Palermo, Joel Parish, Johannes Heidecke, John Hallman, John Rizzo, Jonathan Gordon, Jonathan Uesato, Jonathan Ward, Joost Huizinga, Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Karina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood, Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu, Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad, Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho, Liam Fedus, Lilian Weng, Linden Li, Lindsay McCallum, Lindsey Held, Lorenz Kuhn, Lukas Kondraciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd, Maja Trebacz, Manas Joglekar, Mark Chen, Marko Tintor, Mason Meyer, Matt Jones, Matt Kaufer, Max Schwarzer, Meghan Shah, Mehmet Yatbaz, Melody Y. Guan, Mengyuan Xu, Mengyuan Yan, Mia Glaese, Mianna Chen, Michael Lampe, Michael Malek, Michele Wang, Michelle Fradin, Mike McClay, Mikhail Pavlov, Miles Wang, Mingxuan Wang, Mira Murati, Mo Bavarian, Mostafa Rohaninejad, Nat McAleese, Neil Chowdhury, Neil Chowdhury, Nick Ryder, Nikolas Tezak, Noam Brown, Ofir Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins, Patrick Chao, Paul Ashbourne, Pavel Izmailov, Peter Zhokhov, Rachel Dias, Rahul Arora, Randall Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Miyara, Reimar Leike, Renny Hwang, Rhythm Garg, Robin Brown, Roshan James, Rui Shu, Ryan Cheu, Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer, Sam Toyer, Samuel Miserendino, Sandhini Agarwal, Santiago Hernandez, Sasha Baker, Scott McKinney, Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang, Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji, Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan Clark, Tao Wang, Taylor Gordon, Ted Sanders, Tejal Patwardhan, Thibault Sottiaux, Thomas Degry, Thomas Dimson, Tianhao Zheng, Timur Garipov, Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peterson, Tyna Eloundou, Valerie Qi, Vineet Kosaraju, Vinnie Monaco, Vitchyr Pong, Vlad Fomenko, Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech Zaremba, Yann Dubois, Yinghai Lu, Yining Chen, Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yunyun Wang, Zheng Shao, Zhuohan Li, and et al. Openai o1 system card. Technical Report arXiv:2412.16720, OpenAI, San Francisco, CA, December 2024b. URL https://arxiv.org/abs/2412.16720. arXiv:2412.16720 [cs.AI]. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Berkcan Kapusuzoglu, Supriyo Chakraborty, Chia-Hsuan Lee, and Sambit Sahu. CritiquearXiv preprint Improving supervised fine-tuning via better distillation. guided distillation: arXiv:2505.11628, 2025. Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit Jain, Virginia Aglietti, Disha Jindal, Peter Chen, et al. Big-bench extra hard. arXiv preprint arXiv:2502.19187, 2025. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "Work in Progress",
            "content": "Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. Advances in Neural Information Processing Systems, 36:2155821572, 2023. Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, and Mao Yang. rstar-coder: Scaling competitive code reasoning with large-scale verified dataset. arXiv preprint arXiv:2505.21297, 2025. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75 .notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-min i-Level-1cf81902c14680b3bee5eb349a512a51, 2025. Notion Blog. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and Chandan Reddy. Execution-based code generation using deep reinforcement learning. arXiv preprint arXiv:2301.13816, 2023. Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, et al. Self-evolving critique abilities in large language models. In Second Conference on Language Modeling, 2024. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023. Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, and Wenhu Chen. Unleashing the reasoning potential of pre-trained llms by critique fine-tuning on one problem. Proceedings of EMNLP, 2025a. Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, and Wenhu Chen. Unleashing the reasoning potential of pre-trained llms by critique fine-tuning on one problem. arXiv preprint arXiv:2506.03295, 2025b. Yubo Wang, Xiang Yue, and Wenhu Chen. Critique fine-tuning: Learning to critique is more effective than learning to imitate. arXiv preprint arXiv:2501.17703, 2025c."
        },
        {
            "title": "Work in Progress",
            "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. Enhancing llm reasoning via critique models with test-time and training-time supervision. arXiv preprint arXiv:2411.16579, 2024. Zhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: diverse, challenging, and verifiable synthetic dataset for coding. arXiv preprint arXiv:2503.02951, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, arXiv preprint Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv:2505.09388, 2025. Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025. Terry Yue Zhuo, Minh Chien Vu, Jenny Chim, Han Hu, Wenhao Yu, Ratnadira Widyasari, Imam Nur Bani Yusuf, Haolan Zhan, Junda He, Indraneil Paul, et al. Bigcodebench: Benchmarking code generation with diverse function calls and complex instructions. arXiv preprint arXiv:2406.15877, 2024."
        },
        {
            "title": "A CRL TRAINING PROMPT",
            "content": "We provide the prompt template used for constructing CRL training data. Table 6: Prompt for constructing CRL training data You will be given question (problem specification) and submitted solution. Your task is to determine whether the solution is correct and fully satisfies the specification. Question: {question} Solution: {solution} Conclude with conclusion{T} for correct, conclusion{F} for wrong."
        }
    ],
    "affiliations": [
        "University of Waterloo",
        "Vector Institute"
    ]
}