{
    "paper_title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
    "authors": [
        "Qing Jiang",
        "Xingyu Chen",
        "Zhaoyang Zeng",
        "Junzhi Yu",
        "Lei Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Object referring aims to detect all objects in an image that match a given natural language description. We argue that a robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as a direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, a model that formulates object referring as an explicit CoT reasoning task. Given a referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making a final prediction. To support this paradigm, we construct a large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by GRPO-based RL learning to improve accuracy and generalization. Experiments show that our approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 4 3 0 4 0 . 6 0 5 2 : r Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning Qing Jiang1,2 , Xingyu Chen3 , Zhaoyang Zeng1 , Junzhi Yu3 , Lei Zhang1,2 1International Digital Economy Academy (IDEA) 2South China University of Technology 3Peking University {jiangqing, chenxingyu, leizhang}@idea.edu.cn https://rexthinker.github.io/"
        },
        {
            "title": "Abstract",
            "content": "Object referring aims to detect all objects in an image that match given natural language description. We argue that robust object referring model should be grounded, meaning its predictions should be both explainable and faithful to the visual content. Specifically, it should satisfy two key properties: 1) Verifiable, by producing interpretable reasoning that justifies its predictions and clearly links them to visual evidence; and 2) Trustworthy, by learning to abstain when no object in the image satisfies the given expression. However, most methods treat referring as direct bounding box prediction task, offering limited interpretability and struggling to reject expressions with no matching object. In this work, we propose Rex-Thinker, model that formulates object referring as an explicit Chain-of-Thought (CoT) reasoning task. Given referring expression, we first identify all candidate object instances corresponding to the referred object category. Rex-Thinker then performs step-by-step reasoning over each candidate to assess whether it matches the given expression, before making final prediction. To support this paradigm, we construct large-scale CoT-style referring dataset named HumanRef-CoT by prompting GPT-4o on the HumanRef dataset. Each reasoning trace follows structured planning, action, and summarization format, enabling the model to learn decomposed, interpretable reasoning over object candidates. We then train Rex-Thinker in two stages: cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning in our defined CoT format, followed by GRPO-based reinforcement learning to further improve accuracy and generalization. Experiments show that our CoT-based approach outperforms standard baselines in both precision and interpretability on in-domain evaluation, while also demonstrating improved ability to reject hallucinated outputs and strong generalization in out-of-domain settings. Code is available at https://github.com/IDEA-Research/Rex-Thinker."
        },
        {
            "title": "Introduction",
            "content": "Object Referring, also known as Referring Expression Comprehension (REC) [45, 69, 38, 23, 73, 35, 68, 68, 63, 29], aims to predict the bounding boxes for objects in an image that match given natural language description, which may refer to visual attributes, spatial relations, or interactions. This task has broad applications; however, compared to standard open-vocabulary object detection [20, 46, 47, 32, 65, 19, 18, 25, 46, 27, 9, 40, 70, 56], REC is significantly more challenging, as it requires both fine-grained visual grounding and more complicated language understanding. Equal contributions, work done during internship or academic visit at IDEA. Corresponding author. Preprint. Under review. Figure 1: An example of Rex-Thinker for object referring with CoT reasoning of planning (task decomposition), action (evaluating each candidate), and summarization (final decision). Each step is grounded in specific hint box (as denoted in the left image), enabling interpretable predictions. Benefiting from the strong language comprehension capabilities of large language models (LLMs), multimodal large language models (MLLMs) have demonstrated impressive performance on this task. There are mainly two paradigms: one treats bounding box coordinates as text tokens and predicts them directly [5, 66, 74, 54, 71, 72, 3, 57, 7, 39], while the other adopts retrieval-based strategy [36, 22, 21], where the model is given set of candidate boxes and predicts the box indices that match the expression. Although both approaches have shown promising results, they remain fundamentally implicit, lacking interpretable reasoning steps that reveal how the model arrives at its final prediction. Furthermore, these models are prone to hallucination [21], often producing outputs for objects that do not exist in the image, thereby limiting their reliability in real-world applications. We argue that robust referring system should be grounded, i.e., its predictions must be both explainable and tightly linked to visual evidence. This requires two essential properties: 1) Verifiable, by providing an explicit reasoning process that allows its decisions to be examined and traced to specific image regions; and 2) Trustworthy, by minimizing hallucinated outputs and learning to reject when no object in the image satisfies the given description. To meet these criteria, we draw inspiration from how humans naturally approach referring expressions. For example, when asked to locate the person wearing blue shirt, humans would typically first identify all people in the image, then examine each one to determine whether it matches the described attribute. This step-bystep approach reflects grounded reasoning process, i.e., first localizing relevant object candidates, and then carefully verifying each one against the expression. Motivated by this observation, we propose Rex-Thinker, an MLLM that performs object referring through explicit Chain-of-Thought (CoT) reasoning. Specifically, given an image and referring expression, we first use an open-vocabulary object detector [32] to extract all candidate object boxes corresponding to the referred category. These candidate boxes, along with the image and the expression, are then passed into the model for step-by-step reasoning. Rex-Thinker follows structured CoT framework consisting of three key stages as shown in Figure 1: 1) Planning, where the model decomposes the referring expression into subgoals; 2) Action, where the model examines each candidate box to determine whether it satisfies its current subgoal; 3) Summarization, where it aggregates the intermediate decisions to produce the final prediction. Following DeepSeek-R1 [14], we instruct the model to place its reasoning steps within <think>...</think> block and to output the final prediction inside <answer>...</answer> block. This structured reasoning process not only improves interpretability, but also enables transparent and verifiable predictions, as each reasoning step is grounded in specific candidate region in the image. To support this CoT framework, we construct CoT-style referring dataset named HumanRef-CoT, containing 90,824 samples generated by prompting GPT-4o [16] on the HumanRef [21] dataset. Each example is annotated with structured reasoning trace following the planning, action, and summarization paradigm, enabling explicit supervision for step-by-step reasoning. We train our model in two stages: cold-start supervised fine-tuning phase to teach the model how to perform structured reasoning, followed by reinforcement learning (RL) based on Group Relative Policy 2 Optimization (GRPO) [48] to further improve accuracy and generalization. Experiments demonstrate that our CoT-based approach consistently outperforms direct coordinate prediction baselines. On the in-domain HumanRef benchmark, our model achieves state-of-the-art results with higher detection accuracy and significantly fewer hallucinated outputs, especially on rejection cases. In out-of-domain evaluations on RefCOCOg [38], the model trained only on HumanRef-CoT shows strong zero-shot generalization. Further fine-tuning with GRPO on RefCOCOg yields additional performance gains while preserving the models ability to perform grounded CoT reasoning across arbitrary object categories. To summarize, our contributions are threefold: We formulate the grounded object referring task as planningactionsummarization problem, leveraging Chain-of-Thought reasoning to build verifiable and trustworthy system. We introduce HumanRef-CoT, the first dataset for grounded object referring with step-by-step reasoning annotations, enabling the supervised training of model interpretability. We propose Rex-Thinker, grounded object referring model trained via cold-start SFT and GRPO-based reinforcement learning. Rex-Thinker achieves SOTA performance on the HumanRef benchmark and demonstrates strong generalization on out-of-domain scenes and objects."
        },
        {
            "title": "2 Related Work",
            "content": "MLLM-based Object Referring Methods. Recent progress in multimodal large language models (MLLMs) [43, 3, 57, 7, 2, 24, 11, 1, 53, 26, 28, 31, 62, 76, 4, 50, 15, 4] has led to strong performance in referring expression comprehension. Existing approaches typically follow two paradigms. One line of work treats bounding box coordinates as textual tokens [6] and directly generates them during decoding [5, 66, 54, 71, 74]. The other line formulates the task as retrieval [22, 36, 21], where detector proposes candidate regions and the model selects the best-matching box indices based on the input expression. This decouples localization from semantic understanding and simplifies learning. While both paradigms achieve strong results on standard benchmarks such as RefCOCO/+/g [38, 69], they face key limitations: lack of interpretability and an inability to abstain when no object in the image matches the expression [21]. To address this, we introduce Chain-of-Thought reasoning framework that enables step-by-step evaluation over candidate boxes. This improves interpretability, reduces hallucinations, and grounds the models predictions in the input image. Reasoning-based LLMs and MLLMs. Recent work in large language models [17, 14, 51, 41, 58, 59, 10, 42] has demonstrated that reasoning ability can be significantly enhanced through Chainof-Thought (CoT) training or reinforcement learning-based post-training. OpenAI o1 [17] model demonstrates that inference-time scaling can greatly enhance performance on complex tasks like math and coding. DeepSeek-R1 [14] introduces GRPO [48] as post-training method to improve reasoning without requiring costly critic models. In the multimodal domain, efforts such as LLaVA-CoT [60] and LlamaV-o1 [52] aim to enhance reasoning by constructing CoT-style data or employing multi-step curriculum learning, without relying on reinforcement learning. More recently, inspired by DeepSeek-R1 [14], growing number of works adopt GRPO-based post-training to endow MLLMs with reasoning capabilities. GRPO has been successfully applied to enhance multimodal reasoning across wide range of domains, including mathematical problem solving [64, 44, 75, 12, 55], video understanding [13, 30], and perception tasks [33, 34, 37, 49, 67] such as object detection, segmentation, and referring expression comprehension. Following the DeepSeek-R1 paradigm, we first fine-tune Rex-Thinker on structured CoT data to teach the model how to perform grounded object reasoning. GRPO is then applied in second stage to further improve accuracy and generalization."
        },
        {
            "title": "3 Chain-of-Thought Reasoning Referring Data",
            "content": "High-quality supervision is critical for teaching the model to reason explicitly. To this end, we develop data engine that generates structured referring annotations aligned with our Chain-ofThought formulation. In this section, we introduce the design principles of our CoT reasoning structure and present the data construction pipeline that transforms existing REC annotations into step-by-step reasoning traces suitable for supervised training. 3 Figure 2: Overview of the proposed CoT reasoning referring data engine. We prompt GPT-4o to generate three-step CoT reasoning process, including planning, action, and summarization. 3.1 CoT Formulation The core idea behind our CoT formulation for REC is to transform the task into structured, grounded reasoning process over set of candidate objects. Rather than directly predicting the referred object, the model evaluates each candidate in sequence, guided by input box hints that localize specific regions in the image. We decompose this CoT process into three key stages: Planning: The model analyzes the complexity of the referring expression and determines how many reasoning steps are needed. For simple expressions, it may plan single step to directly match an attribute such as color or size. For more complex expressions, the model generates multi-step plan, where each step focuses on resolving specific sub-aspect. Action: Based on the reasoning plan, the model checks whether each candidate region, grounded via its input box hint, satisfies the current subgoal. This makes the reasoning clear and directly tied to specific regions in the image. Summarization: Finally, the model reviews the reasoning results across all steps and determines which objects best match the overall expression and outputs the final prediction. This structured CoT process improves both interpretability and verifiability. Each candidate is evaluated corresponding to the input box hints, allowing every reasoning step to be explicitly grounded to specific region of the image. This makes the models decisions transparent and easy to trace. Additionally, breaking complex expressions into sub-tasks enables step-by-step reasoning, which enhances accuracy and reflects how humans typically process such tasks. 3.2 Data Engine Pipeline Building on the structured CoT formulation, we develop data engine that leverages GPT-4o [16] to generate high-quality CoT annotations tailored to the referring task. 3.2.1 Data Acquisition We construct our CoT dataset based on HumanRef [21], recently proposed dataset specifically designed for REC in human-centric scenarios. Unlike prior REC datasets such as RefCOCO/+/g [38, 69], HumanRef emphasizes multi-instance referring expressions, where single expression may refer to multiple target persons. It also categorizes expressions into six distinct subsets: attribute, position, interaction, reasoning, celebrity recognition, and rejection. Since the HumanRef dataset provides all person boxes in an image, it can be directly used in our CoT annotation pipeline. 3.2.2 GPT-4o Annotation To generate high-quality CoT annotations, we employ in-context prompting with GPT-4o [16] as shown in Figure 2. Given an image and the bounding boxes of all persons within it, we apply the Setof-Mark [61] strategy: each individual is labeled with an indexed visual marker, where ground-truth targets are marked in green and others in red. This design grounds the answer and guides GPT-4o to reason along the correct path. The prompt includes three key components: 1) meta-information such as the referring question, the number of people, their left-to-right spatial order, and the correct answer; 2) system prompt specifying the desired planningactionsummarization structure; and 4 Figure 3: Overview of the Rex-Thinker architecture and our two-stage training methods <image>. conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Hint: Object and its coordinates in this image: Box Hint. User: Locate Referring. Assistant: Table 1: Prompt Template for Rex-Thinker. Box Hint and Referring will be replaced with the input candidate boxes and the referring expression, respectively. 3) several in-context examples written by humans to illustrate the expected reasoning format. In essence, we provide GPT-4o with both the referring expression and its ground-truth answer, and prompt it to generate step-by-step reasoning in our CoT format. To ensure annotation quality, we retain only examples where GPT-4os final prediction matches the ground-truth label. We construct total of 90,824 high-quality CoT annotations based on the HumanRef dataset, which we refer to as HumanRef-CoT. This diverse and large-scale dataset serves as the foundation for both our initial cold-start SFT and GRPO-based post-training."
        },
        {
            "title": "4 Method",
            "content": "To leverage the CoT-style referring data, we present Rex-Thinker, retrieval-based model that performs object referring through explicit Chain-of-Thought reasoning. 4.1 Retrieval-based Object Referring To support explicit Chain-of-Thought (CoT) reasoning, we reformulate referring expression comprehension as retrieval-based task. As shown in Figure 3, rather than directly regressing bounding boxes, we first use an open-vocabulary detector [32] to extract set of candidate object boxes corresponding to the referred object category. These candidate boxes serve as box hints to guide both the reasoning path and final decision of the model. This retrieval-based formulation brings two key advantages. First, during the reasoning phase, the model evaluates each candidate region in the order they appear in the input box hints (e.g., Person 1 corresponds to the first input box). This alignment ensures that each step in the CoT trace is explicitly grounded to specific region in the image, making the reasoning process interpretable and visually verifiable. Second, during the prediction phase, the model can directly select from the input box hints when producing the final output, thereby easing the challenge of precise coordinate regression. We build Rex-Thinker on top of Qwen2.5-VL-7B [3], preserving its original architecture and using JSON-format bounding box coordinates as the final output. The model input includes the image, the box hint, the referring expression, and system prompt that guides the reasoning process. We adopt similar input prompt format in DeepSeek-R1 [14] as shown in Table 1. 4.2 Training Following DeepSeek-R1 [14], we adopt two-stage training strategy consisting of supervised finetuning for cold start and GRPO-based reinforcement learning for post-training. 5 4.2.1 SFT Cold Start We begin by fine-tuning Rex-Thinker on the HumanRef-CoT dataset to instill the ability to perform structured reasoning following our defined planning, action, and summarization format. We apply cross-entropy loss at the token level to both the reasoning trace and the final answer, providing strong supervision across the entire generation process. This stage teaches the model how to reason stepby-step in CoT manner and also how to utilize the provided box hints to guide its final predictions. 4.2.2 GRPO Post Training While SFT teaches the model to follow our grounded CoT format, its strict token-level supervision may constrain the model to explore alternative reasoning traces and generalize beyond the training data. To enhance generalization beyond the limitations of supervised learning, we employ GRPObased [48] reinforcement learning for post-training. GRPO optimizes model performance by 1) sampling multiple candidate responses for each question and 2) selectively reinforcing responses that achieve higher task-level rewards. In our setting, given an image and referring expression (I, x), the model generates group of complete responses o1, o2, . . . , oG from the current model πθ. Each response contains full reasoning trace and final predicted bounding box set. For each oi, we compute scalar reward ri (detailed in Section 4.2.3), and normalize these rewards to estimate group-relative advantages: Ai = (ri mean(r1, . . . , rG))/std(r1, . . . , rG). (1) Define the token-level advantage estimates ˆAi,t = Ai, and the importance ratio at each decoding step as follows, ρi,t = πθ(oi,t (I, x), x, oi,<t) πθold (oi,t (I, x), x, oi,<t) , (2) where πθold is the model before the current update. Then, the GRPO objective is given as follows, JGRPO(θ) = 1 (cid:88) i= 1 oi oi (cid:88) (cid:104) (cid:16) min t= ρi,t ˆAi,t, clip (ρi,t, 1 ϵ, 1 + ϵ) ˆAi,t (cid:17) (cid:105) βDKL [πθπref] (3) DKL [πθπref] = πθ(oi,t (I, x), x, oi,<t) πref(oi,t (I, x), x, oi,<t) log πθ(oi,t (I, x), x, oi,<t) πref(oi,t (I, x), x, oi,<t) 1, (4) where ϵ is hyperparameter controlling the clipping range, πref is the model fixed after SFT stage, and β is the KL penalty coefficient. We argue that this formulation is suited to policy exploration in our reasoning-driven task. Given that the model is already capable of producing structured reasoning traces after SFT, GRPO allows it to freely explore different reasoning paths. In each iteration, the model generates diverse reasoning strategies that may lead to different predicted object sets. The reward function then guides the model to reinforce reasoning paths that yield accurate predictions. 4.2.3 Reward Modeling Accuracy Reward. We use the F1 score to jointly evaluate the precision and recall of the models predictions. Given set of predicted boxes ˆB and the ground-truth set B, since box hints are provided as input, we define match only when predicted box exactly overlaps with groundtruth box (i.e., IoU = 1), which encourages the model to select final outputs directly from the box hints. Let = ˆB denote the set of matched box pairs under this criterion. We compute precision, recall, and the F1 reward as: Precision = ˆB , Recall = B , rF1 = 2 Precision Recall Precision + Recall . (5) Format Reward. To encourage interpretable and well-structured output, we define format reward rfmt that equals 1 if the output follows the required structure: the reasoning must be enclosed in <think>...</think> and the final result in <answer>...</answer>, and 0 otherwise. The total reward is weighted combination of the accuracy and format rewards, i.e., ri = λ rF1 + (1 λ) rfmt , where λ = 0.9 to emphasize correct detection while still enforcing output structure. 6 Method DINOX [46] InternVL-2.5-8B [8] Ferret-7B [66] Groma-7B [36] ChatRex-7B [22] Qwen2.5-VL-7B [3] DeepSeek-VL2-small [57] Molmo-7B-D [11] RexSeek-7B [21] Rex-Thinker-Plain Rex-Thinker-CoT Rex-Thinker-GRPO 59.5 23.5 27.9 67.5 44.3 49.1 52.3 82.7 87.2 83.0 86.6 88.5 Attribute 28.8 39.0 44.4 47.8 78.0 71.3 78.0 86.4 86.8 88.7 87.7 88.7 DF 20.9 27.1 30.4 38.6 51.8 54.4 57.7 76.3 81.5 81.4 82.7 84.1 78.8 23.0 30.2 63.2 48.0 50.2 56.4 78.0 86.1 82.5 86.5 87.2 Position 28.1 28.0 36.2 43.1 66.7 61.7 66.1 80.6 86.3 83.9 87.0 87.1 Interaction DF1 Reasoning DF1 67.3 27.8 30.8 66.6 49.6 48.2 55.4 69.9 84.8 80.1 79.6 81.5 28.5 40.1 41.8 48.1 74.8 66.3 75.7 77.7 84.6 85.6 81.7 83. 18.9 31.3 31.2 40.6 56.5 53.2 60.7 66.1 80.7 80.2 77.2 79.1 76.2 17.5 19.7 59.1 36.6 34.6 46.6 72.1 87.8 80.5 85.7 87.7 32.1 22.8 33.7 41.4 65.1 61.2 61.7 80.4 84.7 82.2 83.8 85. 22.2 18.9 22.8 34.8 42.8 40.3 50.1 65.5 81.5 77.3 80.3 82.3 94.1 57.4 63.2 73.2 73.7 80.3 85.9 85.9 83.4 86.7 87.6 88.0 DF 17.6 24.3 29.8 37.2 52.5 52.8 58.1 72.4 83.8 81.3 84.3 84.6 Celebrity 48.0 59.3 60.0 63.3 76.5 81.9 74.3 87.5 86.5 88.7 89.5 89.3 DF 37.0 58.0 57.5 59.1 74.2 80.1 70.7 82.9 84.2 86.8 87.2 87.2 75.2 29.8 34.4 65.9 50.4 52.5 59.3 77.7 85.9 82.6 85.2 86.6 Average 33.1 37.8 43.2 48.7 72.2 68.5 71.2 82.5 85.8 85.8 85.9 86.8 DF1 23.3 31.9 34.3 42.1 55.6 56.2 59.5 72.6 82.3 81.4 82.3 83.5 Rejection Score 36.0 54.9 2.0 0.0 0.0 7.1 3.1 68.6 54.1 53.5 67.3 68.2 Table 2: In-domain evaluation results on the HumanRef benchmark. R, P, and DF1 represent Recall, Precision, and DensityF1. The blod and underline fonts indicate the best and second numbers."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we evaluate the effectiveness of our CoT-based reasoning approach for object referring. We first introduce the experimental setup, then present in-domain results on the HumanRef benchmark, followed by out-of-domain evaluation on the RefCOCOg benchmark. Lastly, we conduct ablation studies to analyze key design choices. 5.1 Experimental Setup Model Setting. We use Qwen2.5-VL-7B-Instruct as our base model. Qwen2.5-VL outputs absolute bounding box coordinates rather than quantized tokens, which provides better localization accuracy for detection tasks. We adopt this native decoding format for final bounding box predictions. SFT Training. We fine-tune the model on the full HumanRef-CoT dataset using supervised learning. We use learning rate of 2e-5, weight decay of 0.01, and cosine decay scheduling. The maximum generation length is set to 2048 tokens. During SFT, the vision encoder and MLP projector are frozen, and we update only the LLM parameters. For each training instance, we use all person bounding boxes in the image as box hints. GRPO Training. After SFT, we apply GRPO for reward-driven post-training. We continue training on HumanRef-CoT, but randomly shuffle the box hint order in each training data to create novel input configurations. This leads the model to explore different reasoning paths than those seen during SFT. During this phase, we train only the LLM. We use learning rate of 1e-6, 8 rollout samples per input, batch size of 8, and gradient accumulation steps of 2. The KL penalty coefficient β is set to 0.04, the sampling temperature to 1.0, and the output length remains 2048 tokens. Evaluation Protocol. For in-domain evaluation, we evaluate our model on the HumanRef benchmark, which consists of six subsets: attribute, position, interaction, reasoning, celebrity recognition, and rejection. Following [21], we report Recall (R), Precision (P), and DensityF1 (DF1) scores averaged over IoU thresholds from 0.5 to 0.95. For the rejection subset, we report the rejection score, defined as the proportion of 1,000 images where the model correctly outputs no bounding box when the object described by the referring expression is not present in the image. For out-of-domain evaluation, we evaluate our model on the RefCOCOg dataset and report accuracy at an IoU threshold of 0.5. We compare three variants: 1) Rex-Thinker-Plain, which is trained on HumanRef-CoT using SFT only on the final detection outputs, without reasoning supervision; 2) Rex-Thinker-CoT, which is trained with SFT on both the reasoning process and the final answer; and 3) Rex-Thinker-GRPO, which is initialized from Rex-Thinker-CoT and further optimized with GRPO training. 5.2 In-domain Evaluation Results We begin by evaluating in-domain performance on the HumanRef benchmark to assess referring accuracy within the person domain. As shown in Table 2, Rex-Thinker-CoT, trained with structured CoT supervision, consistently outperforms Rex-Thinker-Plain across most evaluation subsets. Specifically, it achieves average improvements of +2.6 Recall, +0.1 Precision, and +0.9 DensityF1, confirming that step-by-step reasoning leads to more accurate and well-grounded predictions. Most notably, the CoT-trained model shows remarkable 13.8 point improvement in terms of Rejection Score on the rejection subset, indicating substantially reduced hallucination rates and enhanced ability to appropriately abstain from predictions when no valid target exists, which is critical capability for real-world applications requiring high reliability. Additional performance gains are realized through GRPO-based reinforcement learning. RexThinker-GRPO demonstrates consistent improvements over Rex-Thinker-CoT, achieving gains of 7 Figure 4: The out-of-domain result. We use Rex-Thinker-GPRO trained on HumanRef-CoT to infer an unseen category (i.e., fish), resulting in strong generalization. Boxes in the image denote hints. Model RexSeek-7B [21] Grounding DINO [32] QwenVL-2.5-7B [3] ChatRex-7B [22] Rex-Thinker-CoT Rex-Thinker-GRPO Rex-Thinker-GRPO RefCOCOg test val 84.0 86.1 87.2 89.8 81.2 83.2 89. 84.4 87.0 87.2 90.0 80.3 83.3 88.8 Table 3: Out-of-domain evaluation results on RefCOCOg. Fine-tuned on RefCOCOg using GRPO. Figure 5: Predictions from model that was trained with GRPO only, without CoT-based supervised fine-tuning as cold-start initialization. Boxes in the image denote answers. +1.4 Recall, +0.9 Precision, and +1.2 DensityF1. These results demonstrate the effectiveness of reward-based optimization in improving both the models reasoning process and prediction accuracy. While supervised CoT training provides strong guidance and teaches the model how to reason step by step, it may constrain the model to follow fixed patterns and limit its ability to explore more optimal reasoning strategies. In contrast, GRPO enables dynamic exploration of alternative reasoning strategies that better optimize for task-level objectives, leading to improved performance. Among the six subsets in HumanRef benchmark, Rex-Thinker-GRPO achieves leading metrics in most categories. Notably, in the Reasoning subsets, it outperforms Rex-Thinker-Plain by +5 DensityF1, attributed to its CoT capabilities for reasoning tasks. The only subset with relatively weaker performance is Interaction subset, please refer to Section 6 for analysis. 5.3 Out-of-domain Evaluation Results To evaluate the generalization of Rex-Thinker to unseen object categories, we conduct experiments on the out-of-domain RefCOCOg dataset. We first adopt zero-shot setting: the model is trained only on HumanRef-CoT and directly evaluated on RefCOCOg. Given referring expression and its original COCO category label, we use Grounding DINO [32] to detect all instances of the target object category and use the detected results as box hints to Rex-Thinker. As shown in Table 3, the CoT-trained model already performs competitively without any taskspecific tuning. Further gains are achieved by applying GRPO for post training, demonstrating that reward-driven training enhances the models ability to generalize beyond the training domain. Interestingly, we find that Rex-Thinker maintains its structured CoT behavior even on novel categories. As illustrated in Figure 4, Rex-Thinker-GRPO successfully generalizes to detect the fish of manta ray (with fish bounding boxes as hints) while adhering to its planning-action-summarization reasoning paradigm. Notably, the model demonstrates self-correction ability: when provided with an 8 With Box Hint No Yes Attribute 74.3 88.7 66.4 83.0 DF1 67.2 81. 69.3 82.5 Position 71.9 83.9 DF1 69.5 81. Interaction DF1 Reasoning DF1 65.2 80.1 72.1 85.6 66.4 80.2 63.6 80.5 67.5 82.2 62.2 77. Celebrity 84.6 88.7 82.4 86.7 DF1 82.7 86. 69.4 82.6 Average 74.1 85.8 DF1 69.6 81. Rejection Score 71.7 53.5 Table 4: Ablation study on the retrieval-based design of our model. We compare performance with and without box hints to assess their impact on referring accuracy. With Cold Start No Yes Attribute 85.8 88.7 81.4 88.5 DF1 78.1 84.1 80.2 87.2 Position 80.2 87.1 DF1 77.5 84.6 Interaction DF1 Reasoning DF1 79.6 81. 82.6 83.5 78.0 79.1 77.6 87.7 75.0 85.4 70.6 82.3 Celebrity 86.5 89.3 87.3 88.0 DF1 84.8 87.2 81.2 86.6 Average 82.0 86.8 DF1 77.8 83.5 Rejection Score 66.4 68.2 Table 5: Ablation on the impact of CoT-based cold start on final performance after GRPO training. incorrect hint label (e.g., whale was incorrectly labeled as \"fish\" in hint boxes), Rex-Thinker rectifies the error through logical reasoning and explicitly rejects the misclassification. To further explore the upper bound of the model, we fine-tune Rex-Thinker-CoT using GRPO directly on RefCOCOg. This leads to additional performance improvements, achieving results comparable to state-of-the-art referring models. The experiment results highlight the adaptability of our reasoning paradigm across domains and the effectiveness of reward-based optimization in extending CoT reasoning to unseen categories."
        },
        {
            "title": "5.4 Ablations",
            "content": "Effect of Retrieval-based Referring. Our approach adopts retrieval-based formulation of object referring by providing the model with candidate object boxes as box hints. This design serves two key purposes: first, it allows the model to reason over each candidate region individually, aligning each step of the reasoning process with specific image region and thereby ensuring grounded, interpretable outputs; second, it enables the model to reference these box hints when producing the final prediction, reducing the difficulty of direct coordinate regression. To evaluate the impact of this retrieval-based design on referring accuracy, we conduct an ablation study by fine-tuning Qwen2.5-VL-7B on HumanRef-CoT with and without box hints. In this experiment, we do not include CoT supervision, as CoT reasoning inherently depends on the presence of box hints. As shown in Table 6, incorporating box hints as input leads to substantial performance improvements across all major metrics, with average increases of 13.2, 11.7, and 10.8 points in Recall, Precision, and Density F1, respectively. While the model without box hints shows higher performance on the rejection subset, we attribute this phenomenon primarily to its over-rejection behavior. By analyzing the full test set, we observe that the no-hint model incorrectly abstains from prediction on 189 samples across the five non-rejection subsets, compared to only 134 for the boxhint variant. These results indicate that box hints facilitate more accurate predictions by reducing the difficulty of direct coordinate regression. Impact of CoT-based Cold Start on GRPO. In Rex-Thinker, we adopt two-stage training strategy where the model is first supervised using CoT-annotated data, followed by GRPO-based reinforcement learning. To assess the importance of this CoT-based initialization, we compare GRPO training with and without the cold-start SFT stage. As shown in Table 5, the model with CoT-based SFT achieves higher final performance than the direct GRPO model, indicating that the initial exposure to structured reasoning patterns provides more effective starting point for reward-driven learning. Furthermore, as illustrated in Figure 5, models trained without CoT supervision tend to generate unstructured or incoherent reasoning traces, lacking the verifiable and trustworthy qualities we aim to promote. In contrast, CoTpretrained models produce well-formed thinking steps aligned with our planning, action, and summarization framework."
        },
        {
            "title": "6 Conclusion",
            "content": "We have presented Rex-Thinker, novel framework that has reformulated the object referring problem as an explicit Chain-of-Thought reasoning process to achieve grounded and interpretable predictions. Unlike conventional approaches that have treated referring as direct bounding box prediction, our model has first detected candidate objects and then performed step-by-step verification against the referring expression through structured planning-action-summarization reasoning. To support this paradigm, we have constructed HumanRef-CoT, large-scale dataset with reasoning traces that have enabled learning decomposed and interpretable reasoning patterns. Through two-stage train9 ing approach combining SFT and GRPO-based RL, Rex-Thinker has demonstrated superior performance over prior works in both referring accuracy and rejection. Limitation As shown in Table 2, our model has exhibited relatively weaker performance in the interaction subset. This limitation has arisen because the CoT reasoning process must simultaneously model relationships and interactions among multiple objects. Errors in this complex inference chain have propagated, leading to misleading final responses. Please refer to the Appendix for further limitation analysis."
        },
        {
            "title": "References",
            "content": "[1] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Devendra Chaplot, Jessica Chudnovsky, Saurabh Garg, Theophile Gervet, Soham Ghosh, Amélie Héliou, Paul Jacob, et al. Pixtral 12b. arXiv preprint arXiv:2410.07073, 2024. [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. NeurIPS, 35:2371623736, 2022. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Guo Chen, Zhiqi Li, Shihao Wang, Jindong Jiang, Yicheng Liu, Lidong Lu, De-An Huang, Wonmin Byeon, Matthieu Le, Tuomas Rintamaki, et al. Eagle 2.5: Boosting long-context post-training for frontier vision-language models. arXiv preprint arXiv:2504.15271, 2025. [5] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing multimodal llms referential dialogue magic. arXiv preprint arXiv:2306.15195, 2023. [6] Ting Chen, Saurabh Saxena, Lala Li, David Fleet, and Geoffrey Hinton. Pix2seq: language modeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, Lixin Gu, Xuehui Wang, Qingyun Li, Yimin Ren, Zixuan Chen, Jiapeng Luo, Jiahao Wang, Tan Jiang, Bo Wang, Conghui He, Botian Shi, Xingcheng Zhang, Han Lv, Yi Wang, Wenqi Shao, Pei Chu, Zhongying Tu, Tong He, Zhiyong Wu, Huipeng Deng, Jiaye Ge, Kai Chen, Kaipeng Zhang, Limin Wang, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling, 2025. [9] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Realtime open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. [10] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [12] Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative self-improvement. arXiv preprint arXiv:2503.17352, 2025. [13] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [15] Dong Guo, Faming Wu, Feida Zhu, Fuxing Leng, Guang Shi, Haobin Chen, Haoqi Fan, Jian Wang, Jianyu Jiang, Jiawei Wang, et al. Seed1. 5-vl technical report. arXiv preprint arXiv:2505.07062, 2025. [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [17] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [18] Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, and Lei Zhang. T-rex: Counting by visual prompting. arXiv preprint arXiv:2311.13596, 2023. [19] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic In European Conference on Computer Vision, pages object detection via text-visual prompt synergy. 3857. Springer, 2024. [20] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic In European Conference on Computer Vision, pages object detection via text-visual prompt synergy. 3857. Springer, 2025. [21] Qing Jiang, Lin Wu, Zhaoyang Zeng, Tianhe Ren, Yuda Xiong, Yihao Chen, Qin Liu, and Lei Zhang. Referring to any person, 2025. [22] Qing Jiang, Yuqin Yang, Yuda Xiong, Yihao Chen, Zhaoyang Zeng, Tianhe Ren, Lei Zhang, et al. Chatrex: Taming multimodal llm for joint perception and understanding. arXiv preprint arXiv:2411.18363, 2024. [23] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [24] Dongxu Li, Yudong Liu, Haoning Wu, Yue Wang, Zhiqi Shen, Bowen Qu, Xinyao Niu, Guoyin Wang, Bei Chen, and Junnan Li. Aria: An open multimodal native mixture-of-experts model. arXiv preprint arXiv:2410.05993, 2024. [25] Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Jianwei Yang, Chunyuan Li, et al. Visual in-context prompting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1286112871, 2024. [26] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. [27] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1096510975, 2022. [28] Zhiqi Li, Guo Chen, Shilong Liu, Shihao Wang, Vibashan VS, Yishen Ji, Shiyi Lan, Hao Zhang, Yilin Zhao, Subhashree Radhakrishnan, et al. Eagle 2: Building post-training data strategies from scratch for frontier vision-language models. arXiv preprint arXiv:2501.14818, 2025. [29] Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, and Bo Li. real-time cross-modality correlation filtering method for referring expression comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1088010889, 2020. [30] Zhenyi Liao, Qingsong Xie, Yanhao Zhang, Zijian Kong, Haonan Lu, Zhenyu Yang, and Zhijie Deng. Improved visual-spatial reasoning via r1-zero-like training. arXiv preprint arXiv:2504.00883, 2025. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. [32] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. [33] Yuqi Liu, Bohao Peng, Zhisheng Zhong, Zihao Yue, Fanbin Lu, Bei Yu, and Jiaya Jia. Seg-zero: Reasoning-chain guided segmentation via cognitive reinforcement. arXiv preprint arXiv:2503.06520, 2025. 11 [34] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [35] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multitask collaborative network for joint referring expression comprehension and segmentation. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 1003410043, 2020. [36] Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, and Xiaojuan Qi. Groma: Localized visual tokenization for grounding multimodal large language models. arXiv preprint arXiv:2404.13013, 2024. [37] Xinyu Ma, Ziyang Ding, Zhicong Luo, Chi Chen, Zonghao Guo, Derek Wong, Xiaoyi Feng, and Maosong Sun. Deepperception: Advancing r1-like cognitive visual perception in mllms for knowledgeintensive visual grounding. arXiv preprint arXiv:2503.12797, 2025. [38] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L. Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, pages 1120, 2016. [39] Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong Wang, Nan Du, Tao Lei, Sam Wiseman, Guoli Yin, Mark Lee, Zirui Wang, Ruoming Pang, Peter Grasch, Alexander Toshev, and Yinfei Yang. MM1: methods, analysis & insights from multimodal LLM pre-training. arXiv: 2403.09611, 2024. [40] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. Simple openvocabulary object detection. In European conference on computer vision, pages 728755. Springer, 2022. [41] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. [42] OpenAI, :, Ahmed El-Kishky, Alexander Wei, Andre Saraiva, Borys Minaiev, Daniel Selsam, David Dohan, Francis Song, Hunter Lightman, Ignasi Clavera, Jakub Pachocki, Jerry Tworek, Lorenz Kuhn, Lukasz Kaiser, Mark Chen, Max Schwarzer, Mostafa Rohaninejad, Nat McAleese, o3 contributors, Oleg Mürk, Rhythm Garg, Rui Shu, Szymon Sidor, Vineet Kosaraju, and Wenda Zhou. Competitive programming with large reasoning models, 2025. [43] OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023. [44] Yi Peng, Xiaokun Wang, Yichen Wei, Jiangbo Pei, Weijie Qiu, Ai Jian, Yunzhuo Hao, Jiachun Pan, Tianyidan Xie, Li Ge, et al. Skywork r1v: Pioneering multimodal reasoning with chain-of-thought. arXiv preprint arXiv:2504.05599, 2025. [45] Yanyuan Qiao, Chaorui Deng, and Qi Wu. Referring expression comprehension: survey of methods and datasets. IEEE Transactions on Multimedia, 23:44264440, 2020. [46] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for open-world object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. [47] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. arXiv preprint arXiv:2405.10300, 2024. [48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models, 2024. [49] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [50] Andreas Steiner, André Susano Pinto, Michael Tschannen, Daniel Keysers, Xiao Wang, Yonatan Bitton, Alexey Gritsenko, Matthias Minderer, Anthony Sherbondy, Shangbang Long, et al. Paligemma 2: family of versatile vlms for transfer. arXiv preprint arXiv:2412.03555, 2024. 12 [51] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [52] Omkar Thawakar, Dinura Dissanayake, Ketan More, Ritesh Thawkar, Ahmed Heakl, Noor Ahsan, Yuhao Li, Mohammed Zumri, Jean Lahoud, Rao Muhammad Anwer, et al. Llamav-o1: Rethinking step-by-step visual reasoning in llms. arXiv preprint arXiv:2501.06186, 2025. [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [54] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [55] Yichen Wei, Yi Peng, Xiaokun Wang, Weijie Qiu, Wei Shen, Tianyidan Xie, Jiangbo Pei, Jianhao Zhang, Yunzhuo Hao, Xuchen Song, et al. Skywork r1v2: Multimodal hybrid reinforcement learning for reasoning. arXiv preprint arXiv:2504.16656, 2025. [56] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and Chen Change Loy. Aligning bag of regions for open-vocabulary object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1525415264, 2023. [57] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [58] Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, Louis Castricato, Jan-Philipp Franken, Nick Haber, and Chelsea Finn. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-thought, 2025. [59] Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. Self-rewarding correction for mathematical reasoning, 2025. [60] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision language models reason step-by-step. arXiv preprint arXiv:2411.10440, 2024. [61] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023. [62] Jie Yang, ZENG Wang, Sheng Jin, Lumin Xu, Wentao Liu, Chen Qian, and Ruimao Zhang. Kptllm: Unveiling the power of large language model for keypoint comprehension. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2025. [63] Sibei Yang, Guanbin Li, and Yizhou Yu. Dynamic graph attention for referring expression comprehension. In Proceedings of the IEEE/CVF international conference on computer vision, pages 46444653, 2019. [64] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [65] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. Advances in Neural Information Processing Systems, 35:91259138, 2022. [66] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, ShihFu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. arXiv preprint arXiv:2310.07704, 2023. [67] En Yu, Kangheng Lin, Liang Zhao, Jisheng Yin, Yana Wei, Yuang Peng, Haoran Wei, Jianjian Sun, Chunrui Han, Zheng Ge, et al. Perception-r1: Pioneering perception policy with reinforcement learning. arXiv preprint arXiv:2504.07954, 2025. [68] Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara Berg. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13071315, 2018. 13 [69] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. In ECCV, volume 9906, pages 6985, 2016. [70] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1439314402, 2021. [71] Yufei Zhan, Yousong Zhu, Zhiyang Chen, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon: Spelling out all object locations at any granularity with large language models. In European Conference on Computer Vision, pages 405422. Springer, 2025. [72] Yufei Zhan, Yousong Zhu, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Griffon v2: Advancing multimodal perception with high-resolution scaling and visual-language co-referring. arXiv preprint arXiv:2403.09333, 2024. [73] Chao Zhang, Weiming Li, Wanli Ouyang, Qiang Wang, Woo-Shik Kim, and Sunghoon Hong. Referring expression comprehension with semantic visual relationship and word mapping. In Proceedings of the 27th ACM International Conference on Multimedia, pages 12581266, 2019. [74] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferret-v2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. [75] Jingyi Zhang, Jiaxing Huang, Huanjin Yao, Shunyu Liu, Xikun Zhang, Shijian Lu, and Dacheng Tao. R1-vl: Learning to reason with multimodal large language models via step-wise group relative policy optimization. arXiv preprint arXiv:2503.12937, 2025. [76] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        },
        {
            "title": "A Appendix",
            "content": "A.1 More Details on Constructing HumanRef-CoT A.1.1 Prompt for GPT-4o To annotate HumanRef-CoT dataset using GPT-4o, we designed two-part prompting strategy that addresses the diverse reasoning requirements across different subsets. This strategy consists of unified system prompt and set of subset-specific in-context examples. The system prompt is shared across all subsets and instructs the model on how to interpret the input, It also defines which includes an image, referring expression, and candidate bounding boxes. the expected format of the response, including the use of structured reasoning and answer tags. In addition to the system prompt, each of the six subsets in HumanRef-CoT namely attribute, position, interaction, reasoning, celebrity recognition, and rejection, is paired with collection of in-context examples. These examples are carefully curated to reflect the specific annotation challenges and reasoning patterns required for each subset. They guide GPT-4o in producing chain-of-thought (CoT) rationales that are consistent with human annotations in both style and logic. In the following sections, we first present the shared system prompt. Then, for each subset, we provide the corresponding in-context examples and visualization results. Unified System Prompt. The system prompt instructs the model to perform detailed visual reasoning based on either positional or attribute-based referring expressions. It emphasizes step-by-step analysis, beginning with predefined reasoning steps (first attributes, then orientation), and requires the model to explicitly evaluate each candidate object. Special symbols are also used to denote matching, non-matching, and reference entities during analysis. Figure 6: The system prompt used to instruct GPT-4o on visual reasoning for HumanRef-CoT. It specifies output format, reasoning steps, symbol conventions, and the expected alignment between intermediate analysis and final answers. Subset-Specific In-Context Examples. After the system prompt, we provide in-context examples to guide the model toward producing outputs aligned with our CoT structure. These examples help 15 reinforce consistent reasoning patterns. HumanRef-CoT includes six subsets: attribute, position, interaction, reasoning, celebrity recognition, and rejection. Each subset uses its own set of in-context examples tailored to its specific reasoning needs. We show the in-context prompts used for each subset, along with representative outputs generated by GPT-4o. Subset - Prompt Example Figure 29 Figure 30 attribute position interaction reasoning celebrity rejection inner position Figure 31 Figure 32 outer position Figure 33 Figure 34 inner interaction Figure 35 Figure outer interaction Figure 37 Figure 38 inner positon reasoning Figure 39 Figure 40 attribute reasoning Figure 41 Figure 42 - - Figure 43 Figure Figure 45 Figure 46 Table 6: Ablation study on the retrieval-based design of our model. We compare performance with and without box hints to assess their impact on referring accuracy. A.1.2 Evaluate GPT-4o on HumanRef Since we use GPT-4o to annotate HumanRef-CoT, natural question is how well GPT-4o performs directly on the HumanRef benchmark when prompted in similar style. To investigate this, we adopt setup similar to the annotation phase, using the same SoM-style prompt and set of visual marks (with all marks shown in red). However, we remove any hint indicating which objects are correct. We then evaluate GPT-4o on the HumanRef-Benchmark without prompting with groundtruth answers. As shown in Table 7, GPT-4o achieves an average DF1 score of 53.2 without any hint supervision. This result suggests that while GPT-4o can be used to generate annotations when given the correct answer as reference, its standalone performance without answer supervision remains limited. Method 50.2 Rex-Thinker-GRPO 88. GPT-4o-CoT Attribute 56.2 88.7 DF1 50.9 84.1 Position 56.8 87.1 DF1 55.1 84.6 56.1 87. Interaction 56.8 83.5 DF1 53.2 79.1 52.8 81.5 Reasoning 52.9 85.4 DF1 51.1 82.3 53.3 87. Celebrity 54.3 89.3 DF1 53.2 87.2 54.9 88.0 Average 55.2 86.8 DF1 53.2 83.5 54.3 86. Rejection Score 14.8 68.2 Table 7: Evaluation of GPT-4o on the HumanRef-Benchmark test set using SoM-style prompts without answer hints. The model achieves 53.2 average DF1 score, indicating limited standalone performance. A.2 Experiment Details A.2.1 CoT SFT Settings Table 8 summarizes the full training hyperparameters and computational cost used during the CoT SFT stage. These settings were applied in the cold-start phase without prior instruction tuning. batch size gradient accumulation learning rate optimizer warm up ratio 4 4 2e-5 AdamW 0.03 maximum gradient norm learning rate scheduler max length deepspeed weight decay 1 cosine 2048 zero3 0.01 precision epochs times GPU trainable module bf16 2 10.1h 8xA100 LLM Table 8: Training settings and cost statistics for CoT SFT. A.2.2 GRPO Settings We provide the training configurations used during the GRPO stage in Table 10. We did not run full GRPO training on the entire HumanRef-CoT dataset. Instead, training was terminated when the reward signal plateaued, indicating convergence. A.2.3 GRPO Training Analysis We analyze the training logs of the GRPO stage. As shown in Figure 7, we visualize the changes in both reward signals and completion length throughout training. Thanks to the cold-start CoT initialization, the model achieves reasonably high accuracy reward at the beginning of GRPO training. At the same time, the format reward is nearly saturated from the start, indicating that the model has already learned to follow the correct output structure after CoT supervision. Meanwhile, the completion length remains stable at around 560 tokens throughout training. We attribute this to the model having already acquired the basic reasoning skills required batch size gradient accumulation learning rate optimizer warm up ratio 8 2 1e-6 AdamW 0.03 num of rollout β temperature deepspeed weight decay 8 0.04 1.0 zero3 0.01 precision epochs times GPU trainable module bf16 0.25 112h 8xA100 LLM Table 9: Hyperparameters used during the GRPO training stage. Figure 7: GRPO training curves showing accuracy reward, format reward, and completion length over time. for the referring task during the CoT fine-tuning phase, resulting in consistent output lengths with minimal fluctuation. A.3 Limitations and Broader Impacts A.3.1 Inference Speed While the CoT-based design improves both interpretability and performance, it also introduces additional computational overhead at inference time. To quantify this, we randomly selected 100 images from the HumanRef-Benchmark test set and compared the average inference time per image between RexThinker-Plain and RexThinker-GRPO. All experiments were conducted using the vLLM framework on single NVIDIA A100 GPU. As shown in Table 3, RexThinker-GRPO exhibits slower inference due to its longer CoT-style outputs. This observation aligns with the general principle of test-time computation, where improved interpretability and accuracy often come at the cost of slower response time. model average inference time Rex-Thinker-Plain Rex-Thinker-GRPO 1.13s 6.68s Table 10: Comparison of average inference time between RexThinker-Plain and RexThinker-GRPO. The CoT design in GRPO leads to slower inference. A.3.2 Inconsistent Reasoning We observe occasional inconsistencies between the models reasoning process and its final output. For example, as shown in Figure 8, the model identifies nine candidate objects in the reasoning phase but only includes eight in the final predicted coordinates. We attribute such inconsistencies to the lack of explicit supervision enforcing alignment between the reasoning chain and the final answer during GRPO training. While most predictions remain consistent, these rare cases highlight potential gap in our current framework. One possible solution is to introduce consistency reward, which evaluates whether the number of objects summarized in the reasoning matches the number of bounding boxes in the final answer. This direction remains open for future exploration. A.3.3 Broader Impacts Referring models enable more flexible and natural interaction with visual scenes compared to traditional object detection, as they can understand complex, context-dependent language. This makes them valuable in applications such as surveillance, smart cities, and smart homes. However, these models also raise concerns. In particular, they may expose privacy risks when deployed in sensitive environments and can inherit biases from training data or user input. Although 17 Figure 8: Example of reasoninganswer mismatch. The number of predicted objects differs between reasoning and the final output. Figure 9: Attribute referring example. CoT reasoning improves interpretability, ensuring its consistency and robustness remains an open challenge. Future work should address these risks to ensure safe and responsible deployment. A.4 Visualization Results In this section, we present qualitative results of the RexThinker model across different scenarios, as shown in Figures 928. All visualizations are generated by the RexThinker-GRPO model, which is trained solely on the HumanRef-CoT dataset. 18 Figure 10: Attribute referring example. Figure 11: Attribute referring example. Figure 12: Attribute referring example. Figure 13: Attribute referring example. 20 Figure 14: Attribute referring example. Figure 15: Interaction referring example. Figure 16: Interaction referring example. Figure 17: Interaction referring example. 22 Figure 18: Interaction referring example. Figure 19: Position referring example. Figure 20: Position referring example. Figure 21: Position referring example. 24 Figure 22: Celebrity referring example. Figure 23: Reasoning referring example. Figure 24: Reasoning referring example. Figure 25: Rejection referring example. 26 Figure 26: Rejection referring example. Figure 27: Rejection referring example. Figure 28: Reasoning referring example with multi-task chat. 28 Figure 29: In-context prompt for attribute subset in HumanRef-CoT. Figure 30: Visualization of GPT-4os output on the attribute subset. 29 Figure 31: In-context prompt for position (inner) subset in HumanRef-CoT. Figure 32: Visualization of GPT-4os output on the position (inner) subset. 30 Figure 33: In-context prompt for position (outer) subset in HumanRef-CoT. Figure 34: Visualization of GPT-4os output on the position (outer) subset. 31 Figure 35: In-context prompt for interaction (inner) subset in HumanRef-CoT. Figure 36: Visualization of GPT-4os output on the interaction (inner) subset. 32 Figure 37: In-context prompt for interaction (outer) subset in HumanRef-CoT. Figure 38: Visualization of GPT-4os output on the interaction (outer) subset. 33 Figure 39: In-context prompt for reasoning (inner position) subset in HumanRef-CoT. Figure 40: Visualization of GPT-4os output on the reasoning (inner position) subset. . 34 Figure 41: In-context prompt for reasoning (attribute) subset in HumanRef-CoT. Figure 42: Visualization of GPT-4os output on the reasoning (attribute) subset.. 35 Figure 43: In-context prompt for celebrity recognition subset in HumanRef-CoT. Figure 44: Visualization of GPT-4os output on the celebrity recognition subset.. 36 Figure 45: In-context prompt for rejection subset in HumanRef-CoT. Figure 46: Visualization of GPT-4os output on the rejection subset.."
        }
    ],
    "affiliations": [
        "International Digital Economy Academy (IDEA)",
        "Peking University",
        "South China University of Technology"
    ]
}