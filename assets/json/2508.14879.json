{
    "paper_title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "authors": [
        "Bingquan Dai",
        "Li Ray Luo",
        "Qihong Tang",
        "Jie Wang",
        "Xinyu Lian",
        "Hao Xu",
        "Minghan Qin",
        "Xudong Xu",
        "Bo Dai",
        "Haoqian Wang",
        "Zhaoyang Lyu",
        "Jiangmiao Pang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 7 8 4 1 . 8 0 5 2 : r MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds Bingquan Dai1,2, Li Ray Luo1, Qihong Tang1,3, Jie Wang1,4, Xinyu Lian1, Hao Xu1,5, Minghan Qin2, Xudong Xu1, Bo Dai1, Haoqian Wang2, Zhaoyang Lyu1, Jiangmiao Pang1 1Shanghai Artificial Intelligence Laboratory, Shanghai, China 2Tsinghua University, Beijing, China 3Harbin Institute of Technology, Shenzhen, China 4Beijing Institute of Technology, Beijing, China 5AI Thrust, HKUST(GZ), Guangzhou, China (a) (b) Figure 1: (a) MeshCoder can predict codes and reconstruct 41 categories of objects. (b) MeshCoder takes in point clouds and produce part-segmented meshes by executing the predicted code in Blender. For the dishwasher, we apply transparency to the foremost part to showcase the internal structure."
        },
        {
            "title": "Abstract",
            "content": "Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting Equal contribution. Corresponding authors. Preprint. Under review. their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as powerful and flexible solution for programmatic 3D shape reconstruction and understanding. Project homepage: https://daibingquan.github.io/MeshCoder."
        },
        {
            "title": "Introduction",
            "content": "Inferring shape programs from 3D observations is of great importance for reverse engineering, shape editing, and 3D structure understanding. Prior work [1, 2, 3] has explored this problem by defining Domain-Specific Languages (DSLs) to model geometric and structural properties of objects and training neural networks to map 3D observations to shape programs. However, existing methods struggle to generalize to objects with complex geometry and structure. Two key limitations underlie this gap. First, existing DSLs are constrained to modeling simple primitives (e.g., cubes, spheres, cylinders) and cannot represent real-world objects with intricate parts. Second, training shape-to-code inference models demands large-scale paired datasets of 3D objects and their corresponding code, while such datasets are scarce. Prior work often relies on datasets with limited categories, geometric complexity and part count. To address these challenges, we introduce MeshCoder, novel framework for generating Blender Python scripts that reconstruct complex 3D objects into their constituent parts. First, we design set of expressive Blender Python APIs that are capable of synthesizing intricate geometries beyond simple primitives. For instance, our APIs can create complex shapes by translating 2D section curve along specified trajectory, bridging section curves of different shapes, adding bevels or applying Boolean operations on basic shapes, repeating basic shape in one dimension or two dimensions. With these concise yet powerful Blender Python APIs, we can model highly complex shapes, addressing the limitations of prior DSLs. Second, we present novel pipeline to construct large-scale paired object-code dataset. We begin by synthesizing diverse object parts using our APIs with parametrically sampled parameters, yielding part-level dataset. part-to-code inference model is then trained on this dataset to predict code for individual parts. Next, we employ this model to construct holistic object-code dataset. We use Infinigen-Indoor [4] to generate dataset of objects, and each object is decomposed into its constituent parts. We use the part-to-code inference model to predict code for each part of an object, and then carefully design rules to concatenate code of all parts to obtain code of the object. This process yields dataset of approximately 1 million objects spanning 41 categories, with objects up to more than 100 parts. Finally, we train multimodal large language model (LLM) on this dataset to infer code from 3D objects. We use point clouds as 3D shape representations due to their ease of acquisition, and use triplane-based tokenizer to transform the input point cloud to set of fixed-length tokens. These tokens are fed into the LLM to generate Blender Python scripts that replicate input geometries in distinct semantic parts. We evaluate our approach against existing shape-to-code methods, with experimental results and quantitative metrics demonstrating that our framework significantly outperforms prior work. Furthermore, by representing shapes as executable code, our method facilitates intuitive geometric and topological editing through simple code modifications. This capability enables precise alterations to object geometry and mesh topology, enhancing flexibility in downstream applications. Additionally, we conduct experiments on shape structural and geometric understanding tasks, revealing that our code-based representation improves the reasoning capabilities of large language models (LLMs) when interpreting 3D shapes. In summary, our contributions are outlined as follows: 2 We have developed comprehensive set of Blender Python APIs, facilitating the modeling of intricate geometries. This enhanced API suite empowers the procedural generation of complex 3D structures, effectively addressing the limitations of traditional domain-specific languages (DSLs) in representing detailed and varied shapes. We propose pipeline to construct large-scale paired object-code dataset. Using the dataset we constructed, we can train an shape-to-code inference model. We trained MeshCoder, an Object-to-Code inference framework that generates Blender Python scripts to reconstruct 3D meshes from point clouds in structured and editable manner. Our model encodes 3D shapes into part-level code, simplifying mesh editing and enhancing LLMs understanding of 3D objects. Figure 2: Overview of MeshCoder. The input point cloud is first encoded into shape tokens via shape tokenizer. These tokens are then fed into large language model (LLM), which autoregressively generates executable code representing part-based 3D structures. The decoded code specifies objects name, part identities and names, enabling interpretable and modular reconstruction."
        },
        {
            "title": "2 Related Work",
            "content": "Shape programs. Shape programs provide structured and interpretable framework for representing 3D geometry by utilizing domain-specific languages to describe the generative processes of shapes. Early work such as ShapeAssembly [5] introduced explicit shape programs that capture the hierarchical and part-based organization of objects. Subsequent methods, including ShapeCoder [6], PLAD [2], and ShapeLib [7], progressively improved program abstraction, learning efficiency, and scalability with large language models. Other approaches, such as those proposed by Liang [3] and Tian et al. [1], incorporate differentiable rendering or neuro-symbolic reasoning to enhance program inference and execution. While these methods exhibit strong generalization capabilities in composing simple geometric elements like boxes and cylinders, they often struggle to model complex part geometries or generate artist-grade quad meshes, which restricts their application in high-fidelity asset creation. In addition, range of methods in CAD program generation [8, 9, 10, 11, 12, 13] have explored synthesizing code representations for individual CAD parts. However, these approaches are limited to isolated component generation and lack the capability to model complete multi-part objects with coherent structural relationships. Part-based Representation. Part-based representations have proven highly valuable in 3D shape analysis and synthesis. Some approaches [14, 15, 16, 17, 18, 19, 20, 21, 22, 23] take generative approach, assembling objects by combining predefined or learned parts into complete 3D structures. Other methods [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35] focus on segmenting 3D objects 3 into individual parts, enabling more modular and flexible manipulation of shapes. For instance, SAMPart3D [24] introduces scalable zero-shot 3D part segmentation framework that segments any 3D object into semantic parts at multiple granularities without requiring predefined part label sets as text prompts. PartSLIP [28] explores low-shot part segmentation of 3D point clouds by leveraging pretrained image-language model, GLIP, transferring rich knowledge from 2D to 3D through GLIP-based part detection on point cloud rendering and novel 2D-to-3D label lifting algorithm. SATR [26] performs zero-shot 3D shape segmentation via text descriptions by using zero-shot 2D object detector, inferring 3D segmentation from multi-view 2D bounding box predictions by exploiting the topological properties of the underlying surface. Despite these advancements in part segmentation and reconstruction, these methods do not translate segmented parts into executable code representations, limiting their integration into code-driven design workflows."
        },
        {
            "title": "3 Methodology",
            "content": "As shown in Figure 2, we aim to train an object-to-code inference model that takes in point cloud of an object, and then predict the Blender python scripts of each part of the object. When executing the python scripts in Blender, we can obtain the same object in separated parts. To train such an object-to-code inference model, we need dataset of paired objects and the corresponding codes. To obtain such dataset, we first train part-to-code inference model that predicts code for single part on our synthetic dataset of paired parts and the corresponding codes. Then, given dataset of objects separated in different parts, we use the trained part-to-code inference model to predict code for every part of an object. Finally, we concatenate the codes of every part of the object and obtain the code of the object. Now, we have dataset of paired objects and the corresponding codes, and are ready to train the object-to-code inference model. We explain the key steps described above in details in the following sections. First, we explain how to synthesize dataset of paired parts and the corresponding codes in Section 3.1. Then, we describe the training procedure of the part-to-code inference model in Section 3.2. Next, we use the part-to-code inference model to obtain the code of an entire object in Section 3.3. Finally, we train the object-to-code inference model in Section 3.4. 3.1 Part Dataset We aim to generate dataset of paired part shapes and codes. To do so, we implement probabilistic programs to generate Blender Python scripts, and obtain the corresponding shape by executing the code in Blender. We carefully design these probabilistic programs and ensure that the shapes generated are within the range [1, 1]3. There are several types of shapes that we generate, as illustrated in Figure 3. We explain them in the following paragraphs. Primitive. Primitives are set of fundamental geometric shapes, consistent with those defined in Blender. Specifically, we consider five basic shapes: cube, cylinder, UV sphere, cone, and torus. Each primitive is parameterized by three attributes: location (location R3), rotation (rotation H), and scale (scale R3), where denotes the space of unit quaternions. The location specifies the shapes position in 3D space, rotation defines its orientation via quaternions, and scale determines the shapes size along its local axes. Examples of Primitives can be found in the first row of Figure 3. Translation. Translation is defined as the geometry obtained by sweeping 2D cross-sectional shape along 3D trajectory curve. As illustrated in the second row of Figure 3, during this translation process, the tangent direction of the 3D trajectory remains perpendicular to the 2D shape, and the size of the section shape can change along the 3D trajectory.For more detailed explanation, please refer to A.1. To implement this, we first define 2D shape using set of control points (i.e., spatial coordinates), and then specify 3D trajectory curve in similar manner. Specifically, our experiments consider five types of cross-sectional shapes: rectangles, circles, circular arcs, polygons, and Bézier curves. For the trajectories, we define six forms: straight lines, polylines, circles, circular arcs, rectangles, and Bézier curves. Notably, this method also allows 2D shape to rotate around an axis to form solid of revolution, making it suitable for modeling objects such as bottles and plates. Bridge loop. Although the Translation method is capable of generating certain complex objects, it remains constrained in several ways. For instance, in the Translation operation, the 2D cross4 Figure 3: Visualization of basic geometric shape types and their corresponding code. For each shape category, the code shown corresponds to the first example. sectional shape is always orthogonal to the tangent direction of the trajectory. Moreover, the section shape is allowed to change only in scale, without any deformation in its geometry. To address this limitation, we introduce an alternative method for constructing geometries, namely the Bridge Loop. This geometry is constructed by first generating sequence of 2D shapes and then connecting their corresponding vertices to form continuous 3D geometry. Some cases can be seen in the third row of Figure 3. The Bridge Loop approach enables the creation of more complex geometries compared to those achievable via Translation alone. For more detailed explanation, please refer to A.1. Boolean. Boolean geometries refer to geometries formed by applying Boolean operationsnamely union, intersection, and differenceto two or more of the fundamental shape categories defined in Section 3.1. The union operation enables the construction of complex composite geometries, while the difference operation is used to generate geometries with holes or indentations. We can see some examples and their corresponding codes in the fourth row of Figure 3. Array. When particular type of primitive geometry appears repeatedly in regular pattern, we do not invoke the construction function for each primitive individually, as this would result in lengthy code. Instead, we employ an Array method to construct the entire structure collectively. Specifically, we define two types of Arrays: 1D Arrays, where geometry is repeatedly instantiated along curve, and 2D Arrays, where repetition occurs across plane. Cases of this type can be seen in the last row of Figure 3. 3.2 Part-to-code Inference Model After constructing the dataset of paired code and mesh , we sample point cloud RN 3 from each mesh , where is the number of points in the point cloud. We train part-to-code inference model that takes in point cloud and predict the corresponding code y. The inference model consists of two modules: The shape tokenizer model and fintuned LLM. The tokenizer model takes in the point cloud and outputs set of fixed length tokens RLD, where is the number of shape tokens and is the dimension of each token. We set to the same dimension as the word embeddings in the LLM. Thereafter, the LLM takes in the shape tokens and then predict y, the code of the point cloud x. We train the shape tokenizer model and finetune the LLM at the same time using the cross-entropy loss for the prediction of the next token in the shape code y. We use Llama-3.2-1B as the base LLM and finetune it using LoRA. 5 Figure 4: Architecture of the shape tokenizer. We first project the point cloud into the triplane and obtain triplane features. The triplane features are patchified and reshaped into 1D sequence, and fed into transformer blocks to obtain triplane tokens. Finally, we use set of learnable tokens to aggregate information from triplane tokens via cross-attention. The shape tokenizer model. We explain the detailed structure of the shape tokenizer model. As shown in Figure 4, the shape tokenizer model transforms point cloud RN 3 to set of fixed length tokens RLD. We first project the point cloud to triplane and obtain triplane feature R3HW D1, where H, are the height and width of the planes, and D1 is the dimension of the triplane feature. The coordinates of each point is fed to shared MLP and feature of dimension D1 is obtained. We project each points feature to the three perpendicular planes according to the points position. Features projected to the same pixel are aggregated by max-pooling. Pixels that do not correspond to any point are filled with zeros. After obtaining the triplane feature u, we patchify it and reshape it into 1D sequence R(3H/f W/f )D1 , where is the patch size. We then feed the sequence to set of transformer blocks and outputs R(3H/f W/f )D1 . Next, to compress the number of tokens fed into the LLM, we use learnable set of tokens RLD2 to aggregate information from using cross attention: CrossAttn(Transformer(w), v, v), (1) where Transformer denotes transformer block, CrossAttn(Q, K, ) denotes cross attention block, and Q, K, are query, key, value, respectively. By feeding to set of these cross attention blocks, we obtain tokens RLD2 that contain information about the point cloud x. Finally, we use an MLP to transform the dimension of from D2 to and obtain shape tokens RLD, where is the dimension of the word embeddings in the LLM. Now, the shape tokens can be readily fed to the LLM and predict the code corresponding to the point cloud x. 3.3 Assemble Parts to Objects After training the part-to-code inference model h, we can use it to obtain the code of an object. Given dataset of objects, in which each object is separated into its constituent parts = {qii = 1, 2, , }, where qi is the i-th part of object O, and is the number of parts of the object O. We also assume that each part qi has its semantic label. We can use the part-to-code inference model to obtain the code of each part. Specifically, we first normalize each part qi to the cube [1, 1]3 according to its minimum bounding box and obtain the shape i. Then we use the part-to-code inference model to obtain its code = h(q i). We then implement algorithms to transform the relevant numerical parameters in the code to the original location, scale, and pose of and obtain the code yi of the original shape qi. Finally, we concatenate the codes of all parts of the object, add semantic information to the code for each part, and obtain the code of the object = {yii = 1, 2, , }. When concatenating the code, we sort each part based on its spatial position. Specifically, we assign an index to each part following spatial order from bottom to top, left to right, and front to back. An overview of this pipeline is illustrated in Figure 5. During code inference, the part point cloud qi is first transformed into canonical space using rotation matrix R, translation matrix , and scaling factor s, resulting in i. The trained part-to-code inference model generates the code of is then transformed back to the original pose and scale using the inverse of R, , and s, and we obtain the code yi of qi. i. 3.4 Object-to-code Inference Model After obtaining the code of each object in the dataset, we can use them to train an object-to-code inference model. Our object-to-code inference model has the same structure as the part-to-code 6 Figure 5: Pipeline of object-level code dataset construction using the part-to-code inference model. For each part point cloud qi, the code inference module independently predicts its corresponding code yi. All part codes yi are then concatenated to form the complete object code. We also add meaningful semantic information to the object code following the template shown in the figure. The complete code of the example chair is shown in Figure 2. inference model described in Section 3.2. We initialize the weights of the object-to-code inference model as the weights of the trained part-to-code inference model, and use the same training method in Section 3.2 to train the object-to-code inference model. It is worth noting semantic information in the ground-truth code of objects enables the object-to-code inference model to learn the semantic structure of objects, and facilitate 3D shape understanding."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Datasets 4.1.1 Synthetic Part Dataset To facilitate the training of our part-to-inference model, we first constructed synthetic part dataset. Specifically, we utilized functions from our basic shape code library, randomly sampling their parameters based on manually defined distributions to generate paired data of synthetic parts and corresponding code. This process yielded 1.5 million point cloudcode pairs for primitive shapes, 3 million for Translation-based parts, 1.5 million for Bridge Loop structures, 1.5 million for Boolean operations, and 2.4 million for Array-based constructions. In total, our constructed part dataset comprises around 10 million point cloudcode pairs. We partitioned the dataset into 70% for training, 15% for validation, and 15% for testing. 4.1.2 Object Dataset We trained our model on the Infinigen Indoor [4] dataset. Infinigen Indoor is procedural framework for generating synthetic 3D indoor objects, where each generated instance is automatically composed by its corresponding parts. We have made extensive modifications to the original Infinigen codebase to enable it to produce both individual components and their complete assemblies. Using this framework, we constructed synthetic dataset comprising 41 common object categories, generating 1 million object-code pairs in total. We partitioned the dataset into training, validation, and test sets, following the same split strategy as the Synthetic Part Dataset. For more details, please refer to the A.1. 4. Implementation Details We conduct training and evaluation on the Infinigen Indoor datasets [36]. For the part-to-code reconstruction model, we adopt the AdamW optimizer and train it for 20 epochs on NVIDIA A100 7 GPUs with batch size of 512, and learning rate of 104. We evaluate the model at every epoch and select the checkpoint with the lowest L2 Chamfer Distance (CD) loss. Then we initialize the weights of the object-to-code reconstruction model with the weights of the trained part-to-code reconstruction model, and train the model on Infinigen Indoor dataset for 10 epochs, with batch size of 256, and learning rate of 104. The checkpoint with the lowest CD loss is selected. For additional training details and the parameter settings of the models, please refer to A.3 and A.2. 4.3 Reconstruction Performance For reconstruction performance, we compare our method with two representative shape-to-code baselines, Shape2Prog [1] and PLAD [2]. Figure 6 illustrates visualization comparisons of results. We adopt IoU and L2 CD as our evaluation metrics. Specifically, we voxelize the models predicted outputs into 323 grids and compute the IoU between the predicted and ground truth voxel grids. In parallel, we sample point clouds from both the predicted outputs and the ground truth, and calculate the Chamfer Distance between the two point clouds. Regarding the number of points and normalization, please refer to the appendix A.4. In Table 1, we present reconstruction metrics for some specific object categories as well as the overall performance across the entire dataset. It can be observed that our method consistently outperforms the baselines in both IoU and CD metrics. Complete results for all categories in each dataset are provided in A.4. We conducted series of ablation studies to evaluate the impact of various components within our model. For comprehensive details on these experiments, please refer to A.4. Table 1: Quantitative comparison of reconstruction performance between MeshCoder and baselines. Method CD(102) IoU (%) Lamp Chair Sofa TableDining Toilet All Lamp Chair Sofa TableDining Toilet All Shape2Prog PLAD MeshCoder 25.44 1.40 0.004 1.30 2.26 0.060 2.14 1.52 0.027 1.03 5.52 0.024 7.51 2.30 0. 6.01 1.87 0.063 16.96 69.58 86.23 49.68 40.93 81.87 65.29 81.33 93.81 71.26 58.43 88.14 51.14 62.61 89. 45.03 67.62 86.75 Figure 6: Qualitative comparison of reconstruction performance between MeshCoder and baselines. MeshCoder can accurately reconstruct objects with intricate parts and complex structures. 4.4 Shape Editing MeshCoder facilitates the transformation of 3D shapes into high-level, human-readable code representations, significantly enhancing the interpretability and editability of complex geometries. This capability enables intuitive and precise modifications through code-based interventions. Our shape editing encompasses two primary categories: geometric editing and topological editing. As illustrated in Figure 7, geometric editing can be performed by adjusting function calls or modifying specific parameters within the generated code. For instance, we can adjust the parameters of the code to convert square tabletop into larger circular one. Additionally, topological editing, which is illustrated in Figure 8 such as adjusting mesh resolution, can be achieved by modifying designated parameters within the code, allowing for control over the meshs complexity and surface detail. This 8 Figure 7: Parameter modification in the code conveniently to alter the geometric shape. Left: Change tabletop from square to circular. Right: Make the bathtub shallower. Figure 8: Mesh resolution adjustment by modifying the resolution parameters in the code. The figure depicts results with progressively increasing resolution from left to right. code-centric approach streamlines the process of modifying 3D models, making it more accessible and efficient for applications requiring iterative design and customization. Additionally, it empowers users to adjust the model resolution according to their desired balance between storage requirements and mesh quality. For additional results and details, please refer to A.5 in the appendix. 4.5 Shape Undertanding MeshCoder is capable of predicting object codes with rich semantic information. These codes effectively capture structural and geometric details, making them valuable for shape understanding. By inputting the predicted codes into GPT, we can assist it in comprehending object structures. We conduct experiments on shape understanding, with an example illustrated in Figure 9. Additional results and details are given in A.6 in the appendix."
        },
        {
            "title": "5 Limitations",
            "content": "Figure 9: The pipeline of conducting experiments on shape understanding. Although our method achieves significant advancements in category diversity, geometric complexity, and reconstruction accuracy compared to existing approaches, it primarily targets human-made objects. The applicability of code-based representations to organic forms, such as animals and humans, remains underdeveloped. We reserve this as direction for future research."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we present MeshCoder, comprehensive framework that translates 3D point cloud data into editable Blender Python scripts, enabling detailed reconstruction and intuitive editing of complex 3D objects. By developing robust set of Blender Python APIs, we facilitate the modeling of intricate geometries. Leveraging these APIs, we constructed large-scale dataset pairing 3D objects with their corresponding code representations, decomposed into semantic parts. Subsequently, we trained multimodal large language model (LLM) capable of generating executable Blender scripts from point cloud inputs. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also enhances the reasoning capabilities of LLMs in 3D shape understanding. 9 By representing shapes as structured code, MeshCoder offers flexible and powerful solution for programmatic 3D shape reconstruction and editing, paving the way for advanced applications in reverse engineering, design, and analysis."
        },
        {
            "title": "References",
            "content": "[1] Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. Learning to infer and execute 3d shape programs. In International Conference on Learning Representations, 2019. [2] Kenny Jones, Homer Walke, and Daniel Ritchie. Plad: Learning to infer shape programs with pseudo-labels and approximate distributions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 98719880, 2022. [3] Yichao Liang. Learning to infer 3d shape programs with differentiable renderer. arXiv preprint arXiv:2206.12675, 2022. [4] Alexander Raistrick, Lingjie Mei, Karhan Kayan, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, Stamatis Alexandropoulos, Lahav Lipson, Zeyu Ma, and Jia Deng. Infinigen indoors: Photorealistic indoor scenes using procedural generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2178321794, June 2024. [5] Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang, Paul Guerrero, Niloy Mitra, and Daniel Ritchie. Shapeassembly: Learning to generate programs for 3d shape structure synthesis. ACM Transactions on Graphics (TOG), 39(6):120, 2020. [6] Kenny Jones, Paul Guerrero, Niloy Mitra, and Daniel Ritchie. Shapecoder: Discovering abstractions for visual programs from unstructured primitives. ACM Transactions on Graphics (TOG), 42(4):117, 2023. [7] Kenny Jones, Paul Guerrero, Niloy Mitra, and Daniel Ritchie. Shapelib: designing library of procedural 3d shape abstractions with large language models. arXiv preprint arXiv:2502.08884, 2025. [8] Pradeep Kumar Jayaraman, J. Lambourne, Nishkrit Desai, Karl D. D. Willis, Aditya Sanghi, and Nigel Morris. Solidgen: An autoregressive model for direct b-rep synthesis. ArXiv, abs/2203.13944, 2022. URL https://api.semanticscholar.org/CorpusID: 247761924. [9] Mohammad Sadil Khan, Elona Dupont, Sk Aziz Ali, Kseniya Cherenkova, Anis Kacem, and Djamila Aouada. Cad-signet: Cad language inference from point clouds using layer-wise sketch instance guided attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 47134722, June 2024. [10] Rundi Wu, Chang Xiao, and Changxi Zheng. Deepcad: deep generative network for computeraided design models. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 67526762, 2021. doi: 10.1109/ICCV48922.2021.00670. [11] Jingwei Xu, Zibo Zhao, Chenyu Wang, Wen Liu, Yi Ma, and Shenghua Gao. Cad-mllm: Unifying multimodality-conditioned cad generation with mllm, 2024. [12] Xiang Xu, Pradeep Kumar Jayaraman, Joseph Lambourne, Karl DD Willis, and Yasutaka Furukawa. Hierarchical neural coding for controllable cad model generation. In International Conference on Machine Learning, pages 3844338461, 2023. [13] Jianyu Wu, Yizhou Wang, Xiangyu Yue, Xinzhu Ma, Jingyang Guo, Dongzhan Zhou, Wanli Ouyang, and Shixiang Tang. Cmt: cascade mar with topology predictor for multimodal conditional cad generation, 2025. [14] Juil Koo, Seungwoo Yoo, Minh Hieu Nguyen, and Minhyuk Sung. Salad: Part-level latent diffusion for 3d shape generation and manipulation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1444114451, 2023. 10 [15] Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, and Wenping Wang. Part123: part-aware 3d reconstruction from single-view image. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [16] Minghao Chen, Roman Shapovalov, Iro Laina, Tom Monnier, Jianyuan Wang, David Novotny, and Andrea Vedaldi. Partgen: Part-level 3d generation and reconstruction with multi-view diffusion models. arXiv preprint arXiv:2412.18608, 2024. [17] Yuhang Huang, SHilong Zou, Xinwang Liu, and Kai Xu. Part-aware shape generation with latent 3d diffusion of neural voxel fields. arXiv preprint arXiv:2405.00998, 2024. [18] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, and Hao Zhang. Sdm-net: Deep generative network for structured deformable mesh. ACM Transactions on Graphics (TOG), 38(6):115, 2019. [19] Zhijie Wu, Xiang Wang, Di Lin, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Sagnet: Structure-aware generative network for 3d-shape modeling. ACM Transactions on Graphics (TOG), 38(4):114, 2019. [20] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra, and Leonidas Guibas. Structurenet: Hierarchical graph networks for 3d shape generation. arXiv preprint arXiv:1908.00575, 2019. [21] Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, and Baoquan Chen. Pq-net: generative part seq2seq network for 3d shapes. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 829838, 2020. [22] George Kiyohiro Nakayama, Mikaela Angelina Uy, Jiahui Huang, Shi-Min Hu, Ke Li, and Leonidas Guibas. Difffacto: Controllable part-based 3d point cloud generation with cross diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1425714267, 2023. [23] Dmitry Petrov, Matheus Gadelha, Radomír Mˇech, and Evangelos Kalogerakis. Anise: Assemblybased neural implicit surface reconstruction. IEEE Transactions on Visualization and Computer Graphics, 2023. [24] Yunhan Yang, Yukun Huang, Yuan-Chen Guo, Liangjun Lu, Xiaoyang Wu, Edmund Lam, Yan-Pei Cao, and Xihui Liu. Sampart3d: Segment any part in 3d objects. arXiv preprint arXiv:2411.07184, 2024. [25] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1625916268, 2021. [26] Ahmed Abdelreheem, Ivan Skorokhodov, Maks Ovsjanikov, and Peter Wonka. Satr: Zero-shot semantic segmentation of 3d shapes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1516615179, 2023. [27] Zongji Wang and Feng Lu. Voxsegnet: Volumetric cnns for semantic part segmentation of 3d shapes. IEEE transactions on visualization and computer graphics, 26(9):29192930, 2019. [28] Minghua Liu, Yinhao Zhu, Hong Cai, Shizhong Han, Zhan Ling, Fatih Porikli, and Hao Su. Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2173621746, 2023. [29] Yuchen Zhou, Jiayuan Gu, Xuanlin Li, Minghua Liu, Yunhao Fang, and Hao Su. Partslip++: Enhancing low-shot 3d part segmentation via multi-view instance segmentation and maximum likelihood estimation. arXiv preprint arXiv:2312.03015, 2023. [30] Yuheng Xue, Nenglun Chen, Jun Liu, and Wenyun Sun. Zerops: High-quality cross-modal knowledge transfer for zero-shot 3d part segmentation. arXiv preprint arXiv:2311.14262, 2023. [31] Ardian Umam, Cheng-Kun Yang, Min-Hung Chen, Jen-Hui Chuang, and Yen-Yu Lin. Partdistill: In Proceedings of the 3d shape part segmentation by vision-language model distillation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34703479, 2024. [32] George Tang, William Zhao, Logan Ford, David Benhaim, and Paul Zhang. Segment any mesh: Zero-shot mesh part segmentation via lifting segment anything 2 to 3d. arXiv preprint arXiv:2408.13679, 2024. [33] Anh Thai, Weiyao Wang, Hao Tang, Stefan Stojanov, James Rehg, and Matt Feiszli. 3 2: 3d object part segmentation by 2d semantic correspondences. In European Conference on Computer Vision, pages 149166. Springer, 2024. [34] Ziming Zhong, Yanyu Xu, Jing Li, Jiale Xu, Zhengxin Li, Chaohui Yu, and Shenghua Gao. Meshsegmenter: Zero-shot mesh semantic segmentation via texture synthesis. In European Conference on Computer Vision, pages 182199. Springer, 2024. [35] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao Zeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Pointclip v2: Prompting clip and gpt for powerful 3d open-world learning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 26392650, 2023. [36] Kaichun Mo, Shilin Zhu, Angel X. Chang, Li Yi, Subarna Tripathi, Leonidas J. Guibas, and Hao Su. PartNet: large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 12 Appendix of MeshCoder: LLM-Powered Structured Mesh Code"
        },
        {
            "title": "Generation from Point Clouds",
            "content": "A.1 Datasets A.1.1 The principles of Translation and Bridge Loop Figure 10: schematic illustration of the principles of Translation and Bridge Loop. In the Translation module, the wireframe of the resulting mesh is shown as cross-sectional circle is translated along yellow trajectory. In the Bridge Loop module, the wireframe of the mesh is constructed by connecting the vertices of two 2D shapes. As illustrated in the figure 10, in the Translation operation, 2D cross-sectional shape (a circle in this example) and 3D trajectory curve must first be defined. The Translation process generates mesh by sweeping the 2D shape along the 3D trajectory. During this sweep, the cross-section remains perpendicular to the tangent direction of the trajectory at all times, and only uniform scaling (either enlargement or reduction) of the cross-section is permitted. In contrast, the Bridge Loop operation begins with two predefined 2D shapes. By connecting the corresponding vertices of these two shapes, mesh can be constructed. This method places no constraints on the types of 2D shapes usedmeaning the two shapes can differ, such as circle and irregular closed shape in this example. Moreover, it imposes no restrictions on the relative orientations of the shapes. As result, Bridge Loop overcomes the limitations of Translation, which requires the cross-section to align with the trajectorys tangent direction. This enables Bridge Loop to generate more complex geometries that Translation cannot produce. A.1.2 Part datasets Figure 11: The Fill Grid type, Spoon type and Fork type in basic shape code library For certain shapes that are difficult to represent using the method we defined in Section 3.1, we introduce three additional categories: the Fill Grid type, Spoon type and Fork type. As illustrated in the Figure 11. For the Fill Grid type, we first construct closed 3D shape (as opposed to the 2D cross-sectional shape used in Translation), fill it to form surface, and then extrude it along its 13 normal direction to generate the final mesh. For the Spoon and Fork type, we draw inspiration from the implementation in Infinigen Indoor [4] and design dedicated procedural functions tailored for their generation. We present two core functions from our codebase: the complete implementation for creating primitives (Figure 22) and the complete implementation for creating curves (Figure 23). The full codebase can be found in the supplementary materials. More examples of parts and their corresponding complete code implementations are provided in Figures 12, 13, 14, 15, and 16. Figure 12: Examples of Primitive and complete code. And the code corresponds to the first two objects shown in the figure. Figure 13: Examples of Translation and complete code. And the code corresponds to the first two objects shown in the figure. Figure 14: Examples of Boolean and complete code. And the code corresponds to the first two objects shown in the figure. Taking the Primitive type as an example, we describe how to use functions from the basic shape code library to generate synthetic part dataset. We begin by randomly selecting the type of primitive to generate (e.g., cube, cylinder, etc.). Next, for each axis, we independently uniform sample value from the range [2, 2], and then set the corresponding scale as 10x. To determine the orientation of the shape, we uniformly sample direction from unit sphere and roll angle from uniform distribution. Once the orientation is fixed, we scale the shape uniformly along all three axes based on the size of its bounding box. Specifically, we ensure that the longest edge of the bounding box lies within the range [1, 2]. Finally, we assign the shape random position within the 3D space such that the entire shape remains within the [1, 1] bounds. For other shape types beyond Primitive, we follow similar approach by randomly assigning values to the relevant parameters. 14 Figure 15: Examples of Bridge Loop and complete code. And the code corresponds to the first two objects shown in the figure. Figure 16: Examples of Array and complete code. And the code corresponds to the first two objects shown in the figure. A.1.3 Object datasets For assembling part codes into complete program, we provide full example containing the complete code, as shown in Figure 17. Regarding the ordering strategy used when assembling parts into complete object, we adopt consistent spatial heuristic to determine part sequence. Specifically, parts are arranged from bottom to top, left to right, and front to back. To implement this, we divide the 3D space into 32 32 32 grid and assign each part characteristic grid cell that serves as the basis for sorting. The characteristic grid cell of part is defined as follows: among all grid cells that the part occupies, we first select the one with the smallest z-coordinate. If multiple candidates share the same z-value, we choose the one with the smallest x-coordinate. If tie still exists, we select the one with the smallest y-coordinate. Parts are then sorted based on the lexicographic order of these characteristic grid cells, which determines their final sequence within the object. It is important to note that for each object, the prerequisite for successfully constructing its corresponding code lies in the ability of our part-to-code inference model to accurately infer all of its individual parts. We consider part to be successfully inferred if the Chamfer Distance (CD) between the predicted point cloud and the ground truth is below 5 103. Therefore, when constructing the object-code pairs dataset, we only include objects for which all constituent parts meet this criterion. Objects with any part failing to meet this standard are discarded. As result, the number of successfully constructed object-code pairs is smaller than the total number of objects in the original Infinigen dataset. In fact, the original Infinigen dataset we use contains 1.57 million object instances, from which we successfully construct 1 million shape-code pairs. For training and evaluation, we split the full Infinigen dataset into 70% for training, 15% for testing, and 15% for validation. Accordingly, MESHCODER is trained only on the subset of the shape-code pairs that fall within the training portion of the Infinigen dataset. In contrast, the baseline models are trained on the full set of objects in the training split of the original Infinigen dataset. Importantly, all evaluation results for our method and the baselines are reported on the same test set, i.e., the testing split of the complete Infinigen dataset. 15 Figure 17: complete code example of converting part codes into full object program. Figure 18: Detailed configuration of the shape tokenizer. A.2 Model architecture We explain the detailed structure of the shape tokenizer. As illustrated in the Figure 18, we first project the input point cloud of shape Rn3 onto three orthogonal planes to obtain tri-plane features with shape R312812832. With patch size set to 16 16, these tri-plane features are encoded into tokens and fed into Transformer blocks, where the resulting representation is mapped to and used as the key and value (K, ) inputs. Meanwhile, set of learnable tokens with shape R1281024 are used as queries in self and cross attention module. After passing through 12 layers of self and cross attention, we obtain output tokens of shape R1281024, which are then projected to the final representation of shape R1282048 via an MLP. 16 A.3 More training details For the part-to-code reconstruction model, we adopt the AdamW optimizer and train it for 20 epochs on 64 NVIDIA A100 GPUs for about week with batch size of 512, and learning rate of 104. We evaluate the model at every epoch and select the checkpoint with the lowest L2 Chamfer Distance (CD) loss. Then we initialize the weights of the object-to-code reconstruction model with the weights of the trained part-to-code reconstruction model, and train the model on Infinigen Indoor dataset for 10 epochs, with batch size of 256, and learning rate of 104. It is trained on 64 NVIDIA A100 GPUs for about 2 days. The checkpoint with the lowest CD loss is selected. To further enhance the robustness and generalization ability of the object-to-code inference model, we apply data augmentation techniques. Specifically, we perform random rotation and scaling on the objects. Additionally, during training, we randomly sample the number of points in each point cloud within the range of 4096 to 16384, and add Gaussian noise to further perturb the input. MeshCoder is trained and evaluated on unified dataset that aggregates all object categories. A.4 Complete experiment result of Shape Reconstruction For MeshCoder, during inference, each object is represented by point cloud containing 16,384 points. Given the input point cloud, the object-to-code inference model is able to predict the corresponding Blender Python script code. The resulting code is then executed to generate corresponding mesh. We uniformly sample 100,000 points from the generated mesh and compute the Chamfer Distance (CD) to the input point cloud using the L2 norm. Given two point sets and Q, each of size 100,000, the L2 Chamfer Distance is defined as: CD(P, Q) = 1 (cid:88) xP min yQ y2 2 + 1 (cid:88) yQ min xP x2 2. To evaluate IoU, we voxelize both the ground-truth mesh and the predicted mesh into grids of resolution 323, and compute the voxel-based Intersection-over-Union (IoU) as: IoU = Vpred Vgt Vpred Vgt , where Vpred and Vgt denote the sets of occupied voxels in the predicted and ground-truth voxel grids, respectively. For baseline methods, which take voxel grids as input and output voxel grids, we first voxelize the ground-truth mesh into 323 grid and feed it into the baseline models. The predicted voxel grid is then compared to the input voxelized ground truth to compute IoU. Additionally, we extract mesh from the predicted voxel grid using the Marching Cubes algorithm and uniformly sample 100,000 points from the resulting mesh surface. These sampled points, along with the ground-truth point cloud, are then both uniformly scaled to fit within the [1, 1]3 volume. Finally, the Chamfer Distance is computed between the two normalized point clouds using the L2 norm. Its noticed that for each object category, we independently train the baseline models, according to their official code, resulting in category-specific checkpoints. These models are then evaluated on the corresponding test sets for each category. The quantitative comparison of reconstruction metrics between MeshCoder and baseline methods across all object categories is summarized in Table 2 and Table 3. Some additional examples of object reconstruction results and their complete code can be referred to Figure 24, 25, 26. In addition to evaluating our object-to-code inference model, we also perform quantitative assessment of our part-to-code inference model. Specifically, for each category described in Section 3.1, we construct test set consisting of 10,000 samples. We evaluate the models performance using the CD and voxel IoU metrics on these test sets. The results, shown in Table 4, demonstrate strong performance across all categories, with low CD values and high IoU scores, indicating that our part-to-code inference model is highly effective in generating accurate code representations for individual parts. 17 Table 2: Comparison of reconstruction metrics across all categories. Chamfer Distance (CD) and IoU is shown in percentage (%). Category L2 CD(102) Voxel IoU (%) MeshCoder PLAD Shape2prog MeshCoder PLAD Shape2prog ArmChair BarChair Bathtub BeverageFridge Bottle Bowl CeilingClassicLamp CeilingLight CellShelf Chair Chopsticks Cup DeskLamp Dishwasher FloorLamp Fork Hardware Jar Lamp LargeShelf Lid LiteDoor LouverDoor Microwave OfficeChair PanelDoor Plate SidetableDesk SimpleBookcase SimpleDesk Sofa Spoon TableCocktail TableDining Toilet TriangleShelf TV TVStand Vase Window Wineglass All (Avg.) 0.04 0.03 0.09 0.22 0.01 0.02 0.02 0.03 0.01 0.06 0.03 0.06 0.02 0.13 0.00 0.14 0.01 0.03 0.00 0.02 0.05 0.03 0.07 0.07 0.03 0.04 0.04 0.01 0.03 0.01 0.03 0.67 0.02 0.02 0.02 0.01 0.04 0.01 0.30 0.14 0. 0.06 2.31 2.23 1.22 1.12 1.08 1.43 1.98 3.46 1.93 2.26 1.38 1.40 1.76 1.44 2.13 0.34 0.62 0.76 1.40 0.82 1.83 1.36 1.40 1.44 1.44 1.31 0.96 0.67 1.78 2.12 1.52 0.37 2.59 5.52 2.30 2.30 1.53 0.78 0.73 0.59 0.98 1.87 4.44 2.55 2.45 12.63 6.34 6.29 3.94 1.32 9.40 1.30 21.06 7.35 8.77 3.01 22.97 8.40 8.45 1.39 25.44 5.15 2.39 5.75 16.17 11.04 2.63 6.50 1.07 4.50 2.89 25.39 2.14 4.09 5.93 1.03 7.51 12.61 3.41 13.50 19.10 3.73 6.83 6.00 94.33 88.73 78.70 88.03 88.65 89.93 96.13 65.83 94.67 81.87 82.24 85.96 80.28 88.37 85.96 58.86 89.87 79.12 86.23 88.08 73.22 94.75 89.46 91.72 78.41 94.60 72.70 93.23 92.14 88.68 93.81 74.00 88.47 88.14 89.10 88.75 87.80 91.26 72.26 87.36 88. 86.75 78.79 74.96 74.50 82.13 65.58 60.02 76.01 40.61 59.02 40.93 55.68 62.03 64.31 84.44 66.89 89.28 83.96 69.67 69.58 60.70 63.47 36.91 37.43 55.65 55.65 37.18 70.72 91.75 65.14 93.80 81.33 87.04 60.49 58.43 62.61 62.61 72.69 73.78 89.95 84.21 73.96 67.62 62.74 58.23 42.94 39.13 40.24 25.60 59.07 44.97 22.30 49.68 11.25 29.47 25.35 46.69 17.16 11.03 23.56 41.51 16.96 16.81 50.11 18.71 20.94 49.38 46.91 20.94 60.05 35.75 33.79 45.79 65.29 18.92 25.19 71.26 51.14 30.59 34.14 22.57 60.94 64.64 28.56 45.03 A.5 Complete experiment result of Shape Editing We additionally present two examples of shape editing along with their complete code implementations. In Figure 27, we modify the thickness of the chair legs and armrests by adjusting the scale parameter. In Figure 28, we change the mesh resolution of plate by modifying the resolution parameter. A.6 Complete experiment result of Shape Understanding When presented with 3D point cloud of an object as input, MeshCoder can infer the corresponding code for the object. Upon execution of this code in Blender, the geometry of the object can be 18 Table 3: Comparison of standard deviation of reconstruction metrics across all categories. Category ArmChair BarChair Bathtub BeverageFridge Bottle Bowl CeilingClassicLamp CeilingLight CellShelf Lamp Chair Chopsticks Cup DeskLamp Dishwasher FloorLamp Fork Hardware Jar LargeShelf Lid LiteDoor LouverDoor Microwave OfficeChair PanelDoor Plate SidetableDesk SimpleBookcase SimpleDesk Sofa Spoon TableCocktail TableDining Toilet TriangleShelf TV TVStand Vase Window Wineglass All (Std.) MeshCoder 1.51 103 1.82 104 6.93 104 4.84 103 7.56 105 4.83 105 7.33 107 1.79 106 3.37 105 2.20 105 1.09 103 3.64 103 1.59 103 7.62 104 9.66 103 1.23 104 8.81 103 2.20 104 1.40 104 1.79 104 8.89 104 5.79 103 4.67 103 3.92 103 1.72 104 9.05 103 1.73 104 5.11 105 3.66 103 4.29 105 1.35 103 5.64 102 2.03 104 3.31 103 1.09 104 3.60 105 6.25 104 2.59 105 9.97 103 9.57 103 1.40 102 2.92 103 CD PLAD 9.35 103 1.18 102 1.02 102 2.81 103 6.80 103 5.13 103 7.66 104 3.90 103 1.94 102 9.05 103 1.04 102 1.31 102 5.79 103 8.60 103 2.69 103 2.09 102 2.14 103 3.07 103 2.44 103 4.65 103 1.09 102 4.39 103 4.84 103 2.43 102 7.35 103 4.79 103 6.40 103 3.52 103 6.54 103 9.90 102 5.78 103 1.63 103 2.85 102 7.18 102 8.44 103 9.30 103 1.74 103 1.45 103 3.44 103 3.51 103 3.12 103 2.49 102 Shape2prog MeshCoder 4.62 102 1.51 102 8.19 102 9.90 103 1.31 101 6.70 103 1.13 101 3.44 102 1.13 101 6.00 102 8.11 102 8.78 103 3.39 105 9.86 104 3.10 102 4.44 103 9.65 102 6.95 102 1.56 101 2.74 101 1.05 101 4.52 103 1.87 101 1.85 101 9.84 102 3.60 102 1.30 101 4.55 102 1.27 101 2.39 102 1.68 101 2.54 101 2.14 101 8.57 102 1.21 101 4.48 102 1.44 101 6.11 103 1.53 101 5.12 102 1.55 101 1.95 102 1.44 101 2.88 102 1.65 101 9.23 102 7.26 102 3.15 102 8.97 102 2.95 102 1.50 101 3.74 102 1.70 101 5.78 103 9.64 102 5.37 102 1.08 101 7.01 103 1.68 101 1.72 101 6.61 102 6.99 103 2.26 101 4.59 102 1.09 101 3.10 102 1.55 101 6.16 103 4.22 102 1.99 102 1.03 101 9.47 102 1.66 101 1.02 102 1.31 101 5.63 102 2.68 101 1.05 101 1.81 101 6.61 102 1.04 101 3.86 102 1.25 101 7.23 102 IoU PLAD 6.48 102 1.00 101 1.91 101 6.23 102 7.05 102 6.24 102 2.96 103 5.08 102 1.34 101 6.87 102 9.17 102 1.00 101 6.80 102 7.21 102 4.74 102 4.92 102 1.25 101 1.02 101 6.31 102 8.67 102 1.22 101 6.32 102 6.82 102 1.34 101 1.06 101 7.17 102 1.29 101 5.83 102 9.62 102 6.43 102 7.32 102 8.26 102 2.11 101 1.64 101 5.24 102 6.63 102 2.84 102 9.48 102 2.48 102 1.14 101 5.39 102 1.94 10 Shape2prog 5.28 102 8.30 102 8.57 102 6.64 102 6.90 102 2.35 102 3.82 102 6.93 102 9.45 102 1.18 101 6.72 102 1.01 101 6.98 102 6.01 102 8.82 102 1.12 101 6.55 102 1.34 101 8.98 102 7.09 102 1.23 101 9.69 102 1.53 101 1.65 101 1.05 101 1.09 101 1.74 101 1.23 101 6.06 102 8.00 102 6.37 102 9.81 102 8.68 102 9.41 102 5.41 102 1.08 101 6.68 102 5.92 102 4.00 102 1.88 101 6.99 102 1.92 101 obtained. Notably, the comments within the code encompass variety of semantically rich cues, such as the objects identity and the specifics of each component. The primary aim of this experiment is to highlight that our model can assist existing large language models, like GPT - 4, in understanding the structure of 3D objects. We provide the inferred code to GPT - 4 and then inquire about the geometry or structure of the object, as showed in Figure 19, Figure 20 and Figure 21. GPT - 4 is able to generate relevant responses based on the code inferred by our model. This demonstrates that our model possesses capabilities in understanding the geometry and structure of 3D objects and can aid large - scale models such as GPT in addressing such questions. However, our model does have limitations. Currently, the code inferred by our model solely contains geometric information of the object and does not include color information. As result, it is unable to answer questions pertaining to color. 19 Table 4: Quantitative evaluation of the part-to-code inference model across different part categories. CD is reported in 102, and IoU is reported in percentage. CD (102) Category IoU (%)"
        },
        {
            "title": "Primitive\nBoolean\nArray\nBridge Loop\nTranslation",
            "content": "0.18 0.03 0.70 0.14 0.17 94.81 96.13 78.90 89.16 83.45 Figure 19: Experiments on how GPT-4o can understand shape through given dishwasher code. 20 Figure 20: Experiments on how GPT-4o can understand shape through given office chair code. Figure 21: Experiments on how GPT-4o can understand shape through given cell shelf code. 22 Figure 22: Implementation of the function for creating primitives 23 Figure 23: Implementation of the function for creating curves Figure 24: An example of sofa. The input is point cloud of sofa, and the figure shows the code inferred by the object-to-code inference model, as well as the resulting mesh generated by executing the inferred code. Figure 25: An example of bathtub. The input is point cloud of bathtub, and the figure shows the code inferred by the object-to-code inference model, as well as the resulting mesh generated by executing the inferred code. 25 Figure 26: An example of toilet. The input is point cloud of toilet, and the figure shows the code inferred by the object-to-code inference model, as well as the resulting mesh generated by executing the inferred code. 26 Figure 27: By modifying the scale parameters of the leg and arm parts, we adjust their thickness. The highlighted sections indicate the changes made. 27 Figure 28: By modifying the resolution parameter, we change its resolution. The highlighted sections indicate the changes made."
        }
    ],
    "affiliations": [
        "AI Thrust, HKUST(GZ), Guangzhou, China",
        "Beijing Institute of Technology, Beijing, China",
        "Harbin Institute of Technology, Shenzhen, China",
        "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "Tsinghua University, Beijing, China"
    ]
}