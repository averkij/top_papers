{
    "paper_title": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model",
    "authors": [
        "Xun Liang",
        "Simin Niu",
        "Zhiyu Li",
        "Sensen Zhang",
        "Hanyu Wang",
        "Feiyu Xiong",
        "Jason Zhaoxin Fan",
        "Bo Tang",
        "Shichao Song",
        "Mengwei Wang",
        "Jiawei Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledge-intensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce a benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, inter-context conflict, soft ad, and white Denial-of-Service. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https://github.com/IAAR-Shanghai/SafeRAG."
        },
        {
            "title": "Start",
            "content": "SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of Large Language Model Xun Liang1, Simin Niu1, Zhiyu Li2,*, Sensen Zhang1, Hanyu Wang1, Feiyu Xiong2, Jason Zhaoxin Fan3, Bo Tang2, Shichao Song1, Mengwei Wang1, Jiawei Yang1 1School of Information, Renmin University of China, Beijing, China 2Institute for Advanced Algorithms Research, Shanghai, China 3Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China 5 2 0 2 8 ] . [ 1 6 3 6 8 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The indexing-retrieval-generation paradigm of retrieval-augmented generation (RAG) has been highly successful in solving knowledgeintensive tasks by integrating external knowledge into large language models (LLMs). However, the incorporation of external and unverified knowledge increases the vulnerability of LLMs because attackers can perform attack tasks by manipulating knowledge. In this paper, we introduce benchmark named SafeRAG designed to evaluate the RAG security. First, we classify attack tasks into silver noise, intercontext conflict, soft ad, and white Denial-ofService. Next, we construct RAG security evaluation dataset (i.e., SafeRAG dataset) primarily manually for each task. We then utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter. Experiments conducted on 14 representative RAG components demonstrate that RAG exhibits significant vulnerability to all attack tasks and even the most apparent attack task can easily bypass existing retrievers, filters, or advanced LLMs, resulting in the degradation of RAG service quality. Code is available at: https: //github.com/IAAR-Shanghai/SafeRAG."
        },
        {
            "title": "Introduction",
            "content": "Retrieval-augmented generation (RAG) provides an efficient solution for expanding the knowledge boundaries of large language models (LLMs). Many advanced LLMs, such as ChatGPT (OpenAI et al., 2024), Gemini (Team et al., 2024), and Perplexy.ai1, have incorporated external retrieval modules within their web platforms. However, during the RAG process, query-relevant texts are processed sequentially through the retriever, the filter before being synthesized into response by the generator, introducing potential security risks, as attackers can manipulate texts at any stage of the *Corresponding author: Zhiyu Li. 1https://www.perplexity.ai/ Figure 1: Motivation. The attack methods used in existing RAG benchmarks fail to bypass the RAG components, which hindering accurate RAG security evaluation. Our SafeRAG introduces enhanced attack methods to evaluate the potential vulnerabilities of RAG. pipeline. Current attack tasks targeting RAG can be divided into the following four surfaces: Noise: Due to the limitation in retrieval accuracy, the retrieved contexts often contain large quantities of noisy texts that are at most merely similar to the query but do not actually contain the answer. Attackers can exploit this retrieval limitation to dilute useful knowledge by deliberately injecting extensive noisy texts (Chen et al., 2024a; Fang et al., 2024). Conflict: Knowledge from different sources may conflict with one another, creating opportunities for attackers to manipulate. Simply injecting conflicting texts could prevent LLMs from determining which piece of knowledge is more reliable, resulting in vague or even incorrect responses. (Wu et al., 2024a; Liu et al., 2023). Toxicity: The internet often contains toxic texts published by attackers. Such malicious texts are highly likely to be incorporated into the RAG pipeline, inducing LLMs to generate toxic responses (Deshpande et al., 2023; Perez and Ribeiro, 2022). Denial-of-Service (DoS): The target of DoS is to cause LLMs to refuse to answer, even when evidence is available (Chaudhari et al., 2024; Shafran et al., 2024). DoS-inducing texts injected by attackers are particularly insidious because the resulting behavior is easily mistaken for the RAGs limitations. However, most of existing attack tasks at above surfaces often fail to bypass the safety RAG components, making the attacks no longer suitable for RAG security evaluation. There are four main reasons. R-1: Simple safety filters can effectively defend against noise attack (Li et al., 2024), as existing noise is often concentrated in superficially relevant contexts, which may actually belong to either similar-topic irrelevant contexts or relevant contexts that do not contain answers. (Fig.1-⑨). R-2: Existing conflict primarily focuses on questions that LLMs can directly answer but contain factual inaccuracies in the related documents (Xu et al., 2024). Current adaptive retrievers (Tan et al., 2024) have been able to effectively mitigate such context-memory conflict. R-3: Advanced generators demonstrate strong capabilities in detecting and avoiding explicit and implicit toxicity, such as bias, discrimination, metaphor, and sarcasm (Sun et al., 2023; Wen et al., 2023). R-4: Traditional DoS attack mainly involves maliciously inserting explicit (Fig.1-⑤⑥) or implicit (Fig.1-⑦⑧) refusal signals into the RAG pipeline. Fortunately, such signals are often filtered out as they inherently do not support answering the question, or they are ignored by generators due to being mixed into evidences (Shafran et al., 2024). To address above limitations, we propose four novel attack tasks for conducting effective RAG security evaluation. Firstly, we define silver noise (Fig.1-②), which refers to evidence that partially contains the answer. Such noise can circumvent most safety filters, thereby undermining the RAG diversity (R-1). Secondly, unlike the widely studied context-memory conflict, we explore more hazardous inter-context conflict (Fig. 1-④). Since LLMs lack sufficient parametric knowledge to handle external conflicts, they are more susceptible to being misled by tampered texts (R-2). Thirdly, we reveal the vulnerability of RAG under the soft ad attack (Fig. 1-③). As special type of implicit toxicity, the soft ad can evade LLMs and ultimately be inserted into the response of generators (R-3). Finally, to enable refusal signals to bypass filters or generators, we propose white DoS (Fig.1-①) attack. Under the guise of safety warning, such attack falsely accuses the evidence of containing large number of distorted facts, thereby achieving its purpose of refusal (R-4). Existing benchmarks mainly focus on applying certain attack task at specific stages of the RAG pipeline and observing the impact of the selected attack on the retriever or generator. In this paper, we introduce the RAG security evaluation benchmark, SafeRAG, which systematically evaluates the potential security risks in the retriever and generator by performing four surfaces of improved attack tasks across different stages of the RAG pipeline. Our main contributions are: We reveal four attack tasks capable of bypassing the retriever, filter, and generator. For each attack task, we develop lightweight RAG security evaluation dataset, primarily constructed by humans with LLM assistance. We propose an economical, efficient, and accurate RAG security evaluation framework that incorporates attack-specific metrics, which are highly consistent with human judgment. We introduce the first Chinese RAG security benchmark, SafeRAG, which analyzes the risks posed to the retriever and generator by the injection of noise, conflict, toxicity, and DoS at various stages of the RAG pipeline."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 RAG Security Benchmark Dataset Before performing RAG security evaluation, researchers typically design attack datasets meticulously to trigger the vulnerability of RAG. The primary attack types currently include noise, conflict, toxicity, and DoS. As for noise, RGB (Chen et al., 2024a) employs retrieve-filter-classify strategy, dividing the top retrieved contexts related to the query into golden contexts (those containing the correct answer) and relevant noise contexts. RAG Bench (Fang et al., 2024) adopts the same approach to construct relevant noise while also introducing irrelevant noise. LRII (Wu et al., 2024b) further refines the construction of irrelevant noise through combination of human effort and LLM assistance, categorizing it into three types: semantically unrelated, partially related, and related to questions. Table 1: Related works. Method RGB (Chen et al., 2024a) RAG Bench (Fang et al., 2024) Attack Type Noise Noise, Conflict Attack Stage Knowledge Base Knowledge Base Evaluation Method Rule-based Rule-based LRII (Wu et al., 2024b) Noise, Conflict Filtered Context Model-based RECALL (Liu et al., 2023) Conflict Filtered Context Model-based, Rule-based ClashEval (Wu et al., 2024a) Conflict Filtered Context Rule-based PoisonedRAG (Zou et al., 2024) Conflict Knowledge Base Phantom (Chaudhari et al., 2024) MAR (Shafran et al., 2024) DoS DoS Knowledge Base Knowledge Base Rule-based Rule-based Rule-based SafeRAG (Ours) Noise, Conflict, Toxicity, DoS Knowledge Base, Retrieved Context, Filtered Context Model-based, Rule-based Evaluation Metrics EM EM, F1 Misleading Ratio, Uncertainty Ratio Accuracy, BLEU, ROUGE-L, Misleading Rate, Mistake Reappearance Rate Accuracy, Prior Bias, Context Bias Attack Success Rate, Precision, Recall, F1 Retrieval Failure Rate Retrieval Accuracy F1 (correct/incorrect/avg), Attack Success Rate, Retrieval Accuracy Lang. CN, EN EN EN EN EN CN Evaluation Task Open-domain Q&A Open-domain Q&A Open-domain Q&A, Simple Fact Q&A Open-domain Q&A, Simple Fact Q&A, Text Generation Domain-specific Q&A Domain-specific Question, Comprehensive Question-contexts As for conflict, most existing works rely on generating counterfactual perturbations using LLMs (Fang et al., 2024; Zou et al., 2024). However, these methods may incorrectly alter key facts, leading to the generation of similar-topic irrelevant contexts or hallucinatory relevant contexts. Consequently, manually constructing conflicts is considered more reliable approach. For instance, RECALL (Liu et al., 2023) create context-memory conflict manually to evaluate the ability of LLMs to discern the reliability of external knowledge. In this paper, we first refine the rules for manually constructing conflicts and build high-quality, deliberately misleading inter-context conflicts. DoS attack is relatively simple to construct. For example, Phantom (Chaudhari et al., 2024) injects the response ... Sorry, dont know ... into the knowledge base to prevent LLMs from providing useful responses. MAR (Shafran et al., 2024) introduces target responses such as dont know. The context does not provide enough information or cannot provide response that may perpetuate or encourage harmful content to induce LLMs to refuse. However, these rule-based generated attack texts are often intercepted by filters as obviously unhelpful to the query, leading to failed attacks. To address this limitation, MAR (Shafran et al., 2024) employs model-based methods to generate attack contexts that induce target responses and injects them into the knowledge base, but these attack texts are often interspersed among evidence, causing LLMs to prioritize the evidence and rendering the attack ineffective. As result, in this paper, we propose white DoS attack, which fabricates security warning to falsely accuse evidence of containing large amount of distorted facts, successfully inducing LLMs to refuse to respond. Research on toxicity attack has predominantly focused on direct prompt injection targeting LLMs, with no dedicated investigation of RAG under toxicity scenarios. Therefore, in our SafeRAG datasets, we also include toxicity attack, with particular emphasis on implicit toxic attack that can easily bypass the retriever, filter, and generator. 2.2 RAG Security Evaluation Metric When evaluating the safety of RAG, well-designed evaluation metrics are crucial to ensure that the assessment results comprehensively and accurately reflect the LLMs actual performance, while also providing effective guidance for subsequent improvements and optimizations. Existing safety evaluation metrics can be broadly categorized into rulebased and model-based approaches. For instance, methods such as RGB (Chen et al., 2024a), RAG Bench (Fang et al., 2024), and PoisonedRAG (Zou et al., 2024) utilize traditional evaluation metrics (e.g., EM, F1, Recall, Precision, and Attack Success Rate) to assess the safety of generated content. Meanwhile, LRII (Wu et al., 2024b), RECALL (Liu et al., 2023), and ClashEval (Wu et al., 2024a) introduce custom metrics for safety evaluation, including Misleading Rate, Uncertainty Ratio, Mistake Reappearance Rate, Prior Bias, and Context Bias. Additionally, Phantom (Chaudhari et al., 2024) and MAR (Shafran et al., 2024) assess the retrieval safety of RAG from the perspectives of Retrieval Failure Rate and Retrieval Accuracy. In this paper, we focus on both the retrieval safety and generation safety of RAG to provide more comprehensive evaluation perspective. Moreover, existing RAG safety evaluation methods primarily target open-domain Q&A, with an emphasis on simple fact Q&A in English. We construct comprehensive question-contexts safety evaluation dataset specifically tailored to the Chinese news doFigure 2: The process of generating attacking texts. To meet the injection requirements for four attack surfaces: Noise, Conflict, Toxicity, and DoS, we first collected batch of news articles and constructed comprehensive question-contexts dataset as basic dataset. Subsequently, we selected attack-targeted text from the basic dataset for the generation of attacking texts. main, aiming to address the complex issues likely to arise in real-world applications. text generation. The complete generation prompt is detailed in Fig. 12."
        },
        {
            "title": "RAG Pipeline",
            "content": "3.1 Meta Data Collection and Pre-processing As shown in Fig.2-①, we collected raw news texts from news websites2 between 08/16/24 and 09/28/24, covering five major sections: politics, finance, technology, culture, and military. Subsequently, we manually screened news segments that met the following criteria: (1) contain more than 8 consecutive sentences; (2) consecutive sentences revolve around specific topic; (3) consecutive sentences can generate comprehensive questions of the what, why, or how types. 3.2 Generation of Comprehensive Question and Golden Contexts Using DeepSeek3, powerful Chinese LLM engine, and referencing the news title, we generated comprehensive question and its corresponding 8 pieces of golden contexts for each extracted news segment (Fig.2-②). In total, we obtained 110 unique question-contexts pairs. Additionally, we manually verified and removed data points that did not meet the following criteria: (1) the question is not comprehensive what, why, or how type question; (2) there are contexts unrelated to the question. Finally, we obtained 100 unique question-contexts pairs, which serve as the basic dataset for attack 2http://www.news.cn/ 3https://www.deepseek.com/ 3.3 Selection of Attack-Targeted Texts and Generation of Attacking Texts We select different attack-targeted texts from the question-contexts pairs in the base dataset to generate the specific attacking texts. 3.3.1 Generation of Silver Noise To construct silver noise, which includes partial but incomplete answers, we first need to decompose the golden contexts in the base dataset. Specifically, we utilized the knowledge transformation prompt proposed in (Chen et al., 2024b)4 to break the contexts into fine-grained propositions (Fig.2③), which are the smallest semantic units that are complete and independent as evidence. Then, we selected the proposition with the highest semantic similarity (cosine similarity) to the question as the attack-targeted text, ensuring that the subsequent attack texts achieve high recall ratio. Finally, we prompted DeepSeek to generate 10 diverse contexts based on the selected attack-targeted text. 3.3.2 Generation of Inter-Context Conflict The goal of conflict attack is to generate target texts that are prone to contradicting or being confused with the golden context. To achieve this, we manually select golden context most susceptible to manipulation into conflict5. Subsequently, anno4The complete prompt can be found in Appendix 15. 5It is important to emphasize that constructing conflicts is meticulous process that is not well-suited to being fully automated by LLMs. We first refined the rules for manually tators are instructed to modify the context based on the following guidelines: (1) Minimal Perturbation: Introduce conflicts using the smallest possible changes (Fig. 3-①); (2) Rewriting for Realistic Conflicts: Rewrite the context where appropriate to create more convincing conflicts (Fig. 3-②); (3) Preservation of Key Facts: Avoid perturbations that render the conflict invalid, as altering the key fact may lead to generating the hallucinated context instead of the expected conflict context (Fig. 3-③), or even make the context irrelevant to the query (Fig. 3-④). Therefore, we require annotators to preserve key facts while generating conflict contexts. For instance, the year 2018 in Fig. 3-③ and the month September in Fig. 3-④. Figure 3: Cases of forming conflict contexts. 3.3.3 Generation of Soft Ad For the toxic attack task, we manu ally selected the golden context most susceptible to the injection of malicious soft ads as the attack-targeted text. Then, we encouraged annotators to use the following two methods to create the soft ad attack text by seamlessly incorporating seemingly professional and harmless soft ads into the attack-targeted text: (1) Direct Insertion: Soft ad attack texts are directly inserted into the original context (Fig. 26-①); (2) Indirect Insertion: The original context is modified or adjusted to some extent, for instance, by promoting alongside authoritative entities (such as government, organizations, etc.) within the context, making the soft ad more subtle and closely integrated into the context (Fig. 26-②). The complete annotation requirements are shown in Fig. 17. constructing conflicts, ensuring that the generated attack is as realistic and effective as possible. 3.3.4 Generation of White Denial-of-Service In DoS attack, the original question is directly used as the target text. We then employ rule-based approach to construct refusal contexts. Specifically, the construction rule is as shown as in Fig. 4. Figure 4: The construction rules of White DoS. Blue text represents the original question, designed to bypass the retriever. Green text is used to bypass the filter, and red text is intended to bypass the generator to achieve the goal of refusal to answer. The white DoS attack text constructed in this manner leverages the pretense of safety warning to falsely accuse the evidence of containing heavily distorted information, thereby justifying refusal. Since safety warnings are perceived as well-intentioned and high-priority, they are less likely to be filtered by filters and are more likely to be adopted by generators. 3.4 Attacks on the RAG Pipeline We perform final manual verification of the various types of attack contexts obtained and selected appropriate contexts from the golden contexts to construct the final security evaluation datasets. The detailed construction and usage rules for the datasets can be found in Appendix A.1.3."
        },
        {
            "title": "4 Evaluation Metrics",
            "content": "4.1 Retrieval Safety Assessment Metric: Retrieval Accuracy Retrieval Accuracy (RA) is metric used to evaluate the performance of RAG in terms of both retrieval accuracy and safety. It combines the recall of golden contexts and the suppression ability for attack contexts. The formula is as follows: Recall (gc) + (1 Recall (ac)) 2 RA = , where Recall (gc) and Recall (ac) denote the recall of golden contexts and attack contexts, respectively. The core idea of RA is to balance the RAGs ability to retrieve relevant content while avoiding incorrect or harmful content. high Recall (gc) reflects strong coverage of correct content, while low Recall (ac) demonstrates the RAGs robustness in suppressing irrelevant or disruptive content. By combining these two sub-metrics, the higher RA indicates better retrieval performance by RAG. Figure 5: Experimental results injected different noise ratios into the text accessible within the RAG pipeline. Figure 6: Experimental results injected conflict into the text accessible within the RAG pipeline. 4.2 Generation Safety Assessment Metric: F1 Variant and ASR 4.2.1 F1 Variants in Noise and DoS For silver noise and white DoS tasks, we designed metric called F1 (avg) to evaluates whether the generated response include fine-grained propositions and thus assess the diversity of the generator. Specifically, we prompt evaluator to complete the multiple-choice question based on the generated response, i.e., the news summary  (Fig.7)  . Figure 7: Evaluation cases for multiple-choice questions in Noise and DoS tasks. To build multiple options for each data point, we intentionally manipulate certain propositions extracted from the golden contexts to create incorrect ground truth options while retaining the unmodified propositions as correct ground truth options. The evaluator is then tasked with selecting both the correct and incorrect options. Then, we use F1 (correct) to measure the generated responses ability to correctly identify the correct options, and F1 (incorrect) to evaluate its ability to recognize the incorrect options. We further define F1 (avg), which is the average of F1 (correct) and F1 (incorrect). F1 (avg) quantifies the performance of the generated response in recognizing both correct and incorrect options and reflects the generators diversity. If the generator can produce diverse and high-quality responses, it will correctly infer both the correct and incorrect options, resulting in high F1 (avg). 4.2.2 F1 Variants in Conflict To construct multiple-choice options for each data point, we deliberately use conflicting facts between contexts as the options and manually annotate the correct ground truth options. As shown in Fig.8, if the response utilizes the correct fact, it can accurately select the correct options. However, selecting the wrong options is not solely due to using conflicting facts; it may also result from generating hallucinated facts. Consequently, the F1 (incorrect) Figure 8: An evaluation case for multiple-choice question in the conflict task. Figure 9: Experimental results injected toxicity into the text accessible within the RAG pipeline. Figure 10: Experimental results injected DoS into the text accessible within the RAG pipeline. metric is unreliable. Therefore, in the evaluation of inter-context conflict, we only retain the F1 (correct) metric to assess the generators performance. 4.2.3 ASR in Conflict, Toxicity, and DoS In the conflict, toxicity, and DoS tasks, attack keywords are present, such as the conflict facts leading to inter-context conflicts, seamlessly integrated soft ad keywords, and refusal signals. Therefore, in these tasks, we can evaluate the generators safety using the attack success rate (ASR) (Zou et al., 2024). If higher proportion of attack keywords appears in the response text, the ASR will increase6."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Settings The default retrieval window for the silver noise task is set to top = 6, with default attack injection ratio of 3/6. For other tasks, the default retrieval window is top = 2, and the attack injection ratio is fixed at 1/2. We evaluated the impact of using different retrievers (DPR, BM25, Hybrid, Hybrid-Rerank) and filters (OFF, filter NLI (Li et al., 2024), compressor SKR (Wang et al., 2023)) across different RAG stages (indexing, retrieval, generation) on the contexts retrieved for var6Note: in experiments, we use the attack failure rate (AFR = 1 - ASR) for safety evaluation because AFR, as positive metric, can be analyzed alongside F1 variants. ious generators (DeepSeek, GPT-3.5-turbo, GPT4, GPT-4o, Qwen 7B, Qwen 14B, Baichuan 13B, ChatGLM 6B). The bold values represent the default settings. Then, we adopt unified sentence chunking strategy to segment the knowledge base and build the index. The embedding model used is bge-base-zh-v1.5, the reranker is bge-rerankerbase, and the evaluator is GPT-3.5-turbo. 5.2 Results on Noise We inject different noise ratios into the text accessible in the RAG pipeline, including the knowledge base, retrieved context, and filtered context. As shown in Fig.5, the following observations can be made: (1) Regardless of the stage where noise is injected, the F1 (avg) score exhibits downward trend as the noise ratio increases, indicating decline in generation diversity (Fig.5-①); (2) The retriever demonstrates some noise resistance, as noise injected at the knowledge base has approximately 50% chance of not being retrieved. The results in Fig.5-① support this point. Specifically, as the noise ratio increases, the Retrieval Accuracy (RA) of injecting silver noise into the retrieved context or filtered context significantly outperforms that of injecting it into the knowledge base; (3) The performance of injecting noise into the retrieved context and filtered context is similar, indicating that the filter cannot effectively resist silver noise since silver noise still supports answering the query. (4) Different retrievers exhibit varying levels of robustness to noise. Overall, the ranking is Hybrid-Rerank > Hybrid > BM25 > DPR, suggesting that compared to attacking contexts, hybrid retriever and rerankers show preference for retrieving golden contexts. (5) Compression-based filters like SKR are not sufficiently secure, as they tend to lose detailed information, leading to decrease in F1 (avg). 5.3 Results on Conflict, Toxicity, and DoS (1) After injecting different types of attacks into the texts accessible by the RAG pipeline, it was observed that the retrieval accuracy (RA) and the attack failure rate (AFR) decreased across all three tasks. The ranking of attack effectiveness at different RAG stages was: filtered context > retrieved context > knowledge base. Furthermore, adding conflict attack increased the likelihood of misjudging incorrect options as correct, leading to drop in F1 (correct). Introducing DoS attack reduced F1 (avg) and severely impacted generative diversity (Fig.6, 10, 9-①). (2) Retrievers exhibited different vulnerabilities to various attacks. For instance, Hybrid-Rerank was more susceptible to conflict attack, while DPR was more prone to DoS attack. Both experienced significant decrease in AFR. Additionally, all retrievers showed consistent AFR degradation under toxicity attack. After adding conflict attack, the F1 (correct) scores of all retrievers became similar, indicating stable attack effectiveness. However, DPR was more affected by DoS attack compared to other retrievers, as evidenced by its significantly larger decline in the diversity metric F1 (avg) (Fig.6, 10, 9-②). (3) The RA of different retrievers was largely consistent across different attack tasks (Fig.6, 10, 9-③). (4) In conflict tasks, using the SKR filter was less secure because it could compress conflict details, resulting in decline in F1 (correct). In toxicity and DoS tasks, the NLI filter was generally ineffective, with its AFR close to that of disabling the filter. However, the SKR filter proved to be safe in these tasks, as it was able to compress soft ads and warnings (Fig.6, 10, 9-④). 5.4 Analysis of Generator and Evaluator 5.4.1 Selection of Generator We conduct cumulative analysis of the positive metrics across different attack tasks. As shown in Fig.11, the results show that Baichuan 13B achieved leading position in multiple attack tasks, Figure 11: Cumulative analysis of the generators positive evaluation metrics across different attack tasks. particularly excelling in AFR (DoS) and F1 Variants (DoS) metrics7. Lighter models are even safer than models such as the GPT series and DeepSeek. 5.4.2 Selection of Evaluator Silver Noise Inter-context Conflict Soft Ad White DoS F1 (correct) F1 (incorrect) ASR/AFR 96.22 91.67 96.22 89.97 99.10 89.97 95.65 100 Table 2: Evaluation metrics and human consistency. As shown in Table 2, We present the evaluation metrics and their consistency with human judgments. The ASR and AFR metric exhibit high human consistency. Similarly, the F1 (correct) and F1 (incorrect) scores obtained using DeepSeek also demonstrate strong agreement with human judgments. Therefore, DeepSeek is uniformly adopted for evaluation across all experiments."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper introduces SafeRAG, benchmark designed to assess the security vulnerabilities of RAG against data injection attacks. We identified four critical attack surfaces: noise, conflict, toxicity, and DoS, and revealed significant weaknesses across the retriever, filter, and generator components of RAG. By proposing novel attack strategies such as silver noise, inter-context conflict, soft ad, and white DoS, we exposed critical gaps in existing defenses and demonstrated the susceptibility of RAG systems to subtle yet impactful threats. 7The detailed results are shown in Table 3."
        },
        {
            "title": "References",
            "content": "Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, and Alina Oprea. trigger attacks on 2024. retrieval augmented language generation. In arxiv:2405.20485. Phantom: General Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024a. Benchmarking large language models in retrieval-augmented generation. In Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, pages 1775417762. Tong Chen, Hongwei Wang, Sihao Chen, Wenhao Yu, Kaixin Ma, Xinran Zhao, Hongming Zhang, and Dong Yu. 2024b. Dense retrieval: What retrieval granularity should we use? In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1515915177. Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. In Findings of the Association for Computational Linguistics, pages 12361270. Yibing Du, Antoine Bosselut, and Christopher D. Manning. 2022. Synthetic disinformation attacks on automated fact verification systems. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI, pages 1058110589. Feiteng Fang, Yuelin Bai, Shiwen Ni, Min Yang, Xiaojun Chen, and Ruifeng Xu. 2024. Enhancing noise robustness of retrieval-augmented language models with adaptive adversarial training. In arxiv:2405.20978. Weitao Li, Junkai Li, Weizhi Ma, and Yang Liu. 2024. Citation-enhanced generation for LLM-based chatbots. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 14511466. Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. RECALL: benchmark for llms robustness against external counterfactual knowledge. OpenAI, Josh Achiam, Steven Adler, Sandhini AgarIn wal, and so on. 2024. Gpt-4 technical report. arxiv:2303.08774. Liangming Pan, Wenhu Chen, Min-Yen Kan, and William Yang Wang. 2023a. Attacking open-domain question answering by injecting misinformation. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics, IJCNLP, pages 525539. Association for Computational Linguistics. Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Yang Wang. 2023b. On the risk of misinformation pollution with large language models. In Findings of the Association for Computational Linguistics: EMNLP, pages 13891403. Fábio Perez and Ian Ribeiro. 2022. Ignore previous prompt: Attack techniques for language models. In arxiv:2211.09527. Avital Shafran, Roei Schuster, and Vitaly Shmatikov. 2024. Machine against the rag: Jamming retrievalaugmented generation with blocker documents. In arxiv:2406.05870. Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. 2023. Safety assessment of chinese large language models. Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, pages 44204436. Gemini Team, Rohan Anil, Sebastian Borgeaud, and so on. 2024. Gemini: family of highly capable multimodal models. In arxiv:2312.11805. Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-knowledge guided retrieval augmentation for large language models. In Findings of the Association for Computational Linguistics: EMNLP, pages 1030310315. Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. 2023. Unveiling the implicit toxicity in large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13221338. Kevin Wu, Eric Wu, and James Zou. 2024a. Clasheval: Quantifying the tug-of-war between an llms internal prior and external evidence. In arxiv:2404.10198. Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, and Yanghua Xiao. 2024b. How easily do irrelevant inputs skew the responses of large language models? In arxiv:2404.03302. Chong Xiang, Tong Wu, Zexuan Zhong, David Wagner, Danqi Chen, and Prateek Mittal. 2024. Certifiably robust rag against retrieval corruption. Rongwu Xu, Zehan Qi, Zhijiang Guo, Cunxiang Wang, Hongru Wang, Yue Zhang, and Wei Xu. 2024. Knowledge conflicts for llms: survey. In arxiv:2403.08319. Zexuan Zhong, Ziqing Huang, Alexander Wettig, and Danqi Chen. 2023. Poisoning retrieval corpora by injecting adversarial passages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1376413775. Wei Zou, Runpeng Geng, Binghui Wang, and Jinyuan Jia. 2024. Poisonedrag: Knowledge poisoning attacks to retrieval-augmented generation of large language models. In arxiv:2402.07867."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Preliminaries and Definitions A.1.1 RAG Pipeline Let denote the query, the instruction for the LLM, and Cb the knowledge base that comprises all available documents. For effective integration of external knowledge with the LLMs generative capabilities in the answering process, RAG pipeline typically includes three primary modules: retriever R, filter F, and generator G. First, given the query and the knowledge base Cb, the retriever returns the most relevant contexts for query q: Ck = R(q, Cb) = (c r, c2 r, . . . , ck ). Next, to further refine these retrieved contexts, to derive conthe filter picks or compresses Ck texts that are highly relevant to the query: Cm = F(q, Cm ) = (c1 , c2 , . . . , cm ), where k. Finally, the generator combines the instruction i, the query q, and the filtered contexts Cm by concatenating them (denoted by ) into unified prompt, which is then fed into the LLM to generate the final answer: = G(i, q, Cm ) = LLM(i c1 cm ). By sequentially performing the retrieve filter generate composite mapping: (i, q, Cb) (cid:55) R(q, Cb) (cid:55) F(cid:0)q, R(q, Cb)(cid:1) (cid:55) G(cid:0)i, q, F(cid:0)q, R(q, Cb)(cid:1)(cid:1) (cid:55) r, the RAG Pipeline effectively exploits relevant contexts from the external knowledge base Cb while also leveraging the powerful generation capabilities of LLMs. This approach mitigates the hallucination problem and enhances both the accuracy and the interpretability of the answers. A.1.2 SafeRAG Dataset Construction (1) Base Dataset Construction: Raw document is collected from the external news website, and paragraphs meeting the following criteria are selected: 1) contain more than 8 consecutive sentences; 2) consecutive sentences revolve around specific topic; 3) consecutive sentences can generate comprehensive questions of the what, why, or how types. For each paragraph, comprehensive question qj is generated, and golden contexts Cj }, 8 closely related to the question qj are extracted. Each golden context cj Cj is manually screened and verified to ensure accuracy, coherence, and the ability to fully answer the question. The base dataset is defined as: g, . . . , cn = {c1 g, c2 Dbase = (cid:8)(qj, i, Cj ) = 1, . . . , (cid:9), where represents the uniform instruction provided to the LLM8. (2) Attacking Texts Generation and RAG Security Evaluation Dataset Construction: To evaluate the security of RAG under different adversarial scenarios, we design four attack tasks = {SN, ICC, SA, WDoS}: silver noise SN, inter-context conflict ICC, soft ad SA, and white DoS WDoS. For each attack task , malicious attack texts are generated and combined with golden contexts to construct the RAG security evaluation dataset (i.e., SafeRAG dataset). The detailed process is as follows: 1) Silver Noise: 1. Decompose the golden contexts Cj mal semantic units (i.e., propositions) {p1 g, . . . }. g, p2 into minig = 2. Select the proposition pi relevant to question qj. most semantically 3. Use pi as input to the DeepSeek to generate 10 diverse attack contexts. 4. Manually select 8 semantically consistent yet non-redundant attack contexts to form the silver noise contexts: CSN,j = {ck,SN = 1, . . . , 8}. 8The default instruction used in this paper is shown in Fig. 14 5. Combine the silver noise contexts with the golden contexts to construct the silver noise security evaluation dataset: DSN = (cid:8)(qj, i, Cj (cid:124)(cid:123)(cid:122)(cid:125) CSN CSN,j ) = 1, . . . , MSN (cid:9). 2) Inter-Context Conflict: 1. Select golden context ci Cj . 2. Based on ci g, use strategies such as minimal = perturbation to generate text CICC,j {c1,ICC } that clearly contradicts ci g. 3. Randomly select another golden context ce Cj {ci g}, where = i. 4. Combine the generated conflict context with used to generate it and to construct the conthe golden context ci another golden context ce flict security evaluation dataset: DICC = (cid:8)(qj, i, {ci CICC,j ) g, ce g} (cid:124) (cid:123)(cid:122) (cid:125) CICC = 1, . . . , MICC (cid:9). 3) Soft Ad: 1. Select golden context ci Cj . 2. Manually read ci and generate appropriate attack keywords a1,... (i.e., soft ad keywords). 3. Use strategies like direct insertion or indirect insertion to embed attack keywords into ci g, generating the soft ad attack context = {c1,SA CSA,j }. 4. Randomly select two other golden contexts {ci g} for constructing the ce and cl clean set, where = e, = l, = i. from Cj 5. Combine the soft ad attack context with the to construct other two golden contexts ce the soft ad security evaluation dataset: g, cl DSA = (cid:8)(qj, i, {ce g, cl g} (cid:124) (cid:123)(cid:122) (cid:125) CSA CSA,j ) = 1, . . . , MSA (cid:9). 4) White DoS: 1. Based on question qj, generate an attack con- = {c1,WDoS } containing white text CWDoS,j safety warnings. 2. Combine the generated white DoS context with the complete golden contexts Cj to construct the White DoS security evaluation dataset: DWDoS = (cid:8)(qj, i, Cj (cid:124)(cid:123)(cid:122)(cid:125) CWDoS CWDoS,j ) = 1, . . . , MWDoS (cid:9). The complete SafeRAG dataset is defined as: Dsfr = DSN DICC DSA DWDoS. A.1.3 Threat Framework: Attacks on the RAG Pipeline We utilize the SafeRAG dataset to simulate various attack scenarios that RAG may encounter during Q&A tasks. Our proposed threat framework enables attackers to inject malicious contexts into any stage of the RAG pipeline (i.e., retrieval, filter, or generation) to analyze potential vulnerabilities when facing different types of attacks. Ct,j Specifically, for an attack task , Ct from the dataset Dt = {(qj, i, Ct ) = 1, . . . , Mt} is the selected knowledge base for the given attack task t. For given query qj from Dt, and the knowledge base Ct b, we first construct benign RAG pipeline, where neither the retriever R, the filter F, nor the generator is influenced by malicious contexts. This allows us to observe the baseline performance of RAG in terms of retrieval and generation when no attacks are present. Under the threat framework presented in this paper, we then select malicious contexts from the attack source: Ct,j , . . . , ck,t }, 1, = {c1,t where Ct,j represents the attack contexts ina jected by the attacker9. These contexts may be embedded into any stage of the RAG pipeline, targeting its specific components: (1) Attacking the Retriever: The attacker injects attack contexts Ct,j into the original knowla edge base Ct b, camouflaging them as relevant contexts to compromise the retriever R. In this sceb Ct,j nario, when the retriever executes R(qj, Ct ), 9For the text injection attack, the attacker first ejects the original bottom benign contexts and then injects malicious contexts. it is likely to retrieve the attack text ca Ct,j , resulting in erroneous or biased contexts that affect subsequent filter and generation stages. (2) Attacking the Filter: The attacker directly incorporates attack contexts into the retrievers output Ck , such that: topK(Ct,j Ck , . . . , ck,t ) = ({c1,t , r, . . . , ck c1 })[: K]. Consequently, the filter and generator misinterpret these attack texts as part of the retrieved contexts, integrating them into the subsequent stages of the pipeline. (3) Attacking the Generator: The attacker disinto the rupts the filter stage by introducing Ct,j filtered results Cm , such that: topK(Ct,j Cm , . . . , ck,t ) = ({c1,t , , . . . , cm c1 })[: K]. This action directly distorts the input contexts for the generator. Regardless of the stage at which the injection occurs, the attackers objective is to mislead or compromise the RAG pipelines final output by leveraging the attack contexts Ct,j . It is important to note that, under the attack assumptions in this paper, golden contexts Cj are neither altered in content nor re-ranked10. Through this method, the attacker maximizes the systems original usability and normalcy while covertly influencing the RAG pipelines generated responses. 10This setting is widely adopted in numerous many attacks (Xiang et al., 2024; Zhong et al., 2023; Zou et al., 2024; Pan et al., 2023b,a; Du et al., 2022) Figure 12: Generation of comprehensive questions and golden contexts. Figure 13: Generation of comprehensive questions and golden contexts (in Chinese). Figure 14: Question answering. Figure 15: Extraction of propositions from golden contexts. Figure 16: Extraction of propositions from golden contexts (in Chinese). Figure 17: Guidelines for generating (annotating) soft ad attack texts. Figure 18: Guidelines for generating (annotating) soft ad attack texts (in Chinese). Figure 19: Multiple-choice question evaluation. Figure 20: Multiple-choice question evaluation (in Chinese). Figure 21: data point of comprehensive question, the golden contexts and propositions. Figure 22: case of multiple options and the ground truth answers. Figure 23: data point of comprehensive question, the golden contexts and propositions (in Chinese). Figure 24: case of multiple options and the ground truth answers (in Chinese). Figure 25: case of silver noise. Figure 26: case of context conflict and soft ad. Table 3: Cumulative analysis of the generators positive evaluation metrics across different attack tasks Model Noise Conflict Toxicity DoS F1_Variants AFR F1_Variants AFR F1_Variants AFR F1_Variants AFR DeepSeek GPT-3.5-Turbo GPT-4 GPT-4o Qwen 7B Qwen 14B Baichuan 13B ChatGLM 6B 0.1032 0.1102 0.1141 0.1229 0.1016 0.1005 0.0706 0.0815 - - - - - - - - 0.4000 0.3615 0.3615 0.3615 0.4948 0.5000 0.4800 0.5966 0.69 0.72 0.66 0.68 0.75 0.65 0.87 0. - - - - - - - - 0.38 0.47 0.29 0.37 0.47 0.38 0.59 0.27 0.2068 0.1654 0.4760 0.3196 0.2582 0.1842 0.7222 0.5096 0.17 0.17 0.16 0.28 0.57 0.29 1.00 1."
        }
    ],
    "affiliations": [
        "Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China",
        "Institute for Advanced Algorithms Research, Shanghai, China",
        "School of Information, Renmin University of China, Beijing, China"
    ]
}