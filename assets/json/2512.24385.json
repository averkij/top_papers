{
    "paper_title": "Forging Spatial Intelligence: A Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems",
    "authors": [
        "Song Wang",
        "Lingdong Kong",
        "Xiaolu Liu",
        "Hao Shi",
        "Wentong Li",
        "Jianke Zhu",
        "Steven C. H. Hoi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create a unified understanding remains a formidable challenge. This paper presents a comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of a unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose a roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment."
        },
        {
            "title": "Start",
            "content": "Forging Spatial Intelligence: Roadmap of Multi-Modal Data Pre-Training for Autonomous Systems 1, , Lingdong Kong 2, , Xiaolu Liu 1, , Hao Shi1, Wentong Li3 , Jianke Zhu 1, , Song Wang Steven C. H. Hoi4,5 5 2 0 2 0 ] . [ 1 5 8 3 4 2 . 2 1 5 2 : r 1Zhejiang University Astronautics 2National University of Singapore 4Alibaba Group 5Singapore Management University 3Nanjing University of Aeronautics and"
        },
        {
            "title": "WorldBench Team",
            "content": "Equal Contributions Corresponding Author The rapid advancement of autonomous systems, including self-driving vehicles and drones, has intensified the need to forge true Spatial Intelligence from multi-modal onboard sensor data. While foundation models excel in single-modal contexts, integrating their capabilities across diverse sensors like cameras and LiDAR to create unified understanding remains formidable challenge. This paper presents comprehensive framework for multi-modal pre-training, identifying the core set of techniques driving progress toward this goal. We dissect the interplay between foundational sensor characteristics and learning strategies, evaluating the role of platform-specific datasets in enabling these advancements. Our central contribution is the formulation of unified taxonomy for pre-training paradigms: ranging from single-modality baselines to sophisticated unified frameworks that learn holistic representations for advanced tasks like 3D object detection and semantic occupancy prediction. Furthermore, we investigate the integration of textual inputs and occupancy representations to facilitate open-world perception and planning. Finally, we identify critical bottlenecks, such as computational efficiency and model scalability, and propose roadmap toward general-purpose multi-modal foundation models capable of achieving robust Spatial Intelligence for real-world deployment. GitHub Repo: https://github.com/worldbench/awesome-spatial-intelligence"
        },
        {
            "title": "1 Introduction",
            "content": "With the rapid proliferation of autonomous platforms, ranging from self-driving vehicles [72, 122, 166] and aerial drones [162, 302] to unmanned surface vehicles [52, 271], rail-based systems [114, 282, 325], and legged robots [55, 57], the challenge of endowing machines with the capability to perceive and act in the real world has reached an unprecedented level of complexity. As these systems are required to navigate diverse and dynamically evolving scenarios, they demand robust and deeply nuanced understanding of their environment to support critical downstream functions, including navigation [4, 156], interaction [81], and planning [72, 143]. Central to these platforms is sophisticated suite of onboard sensors, primarily comprising cameras, LiDAR, radar, and emerging event cameras, which collectively serve as the foundation for perception [17, 21, 47, 69]. Each modality contributes unique and complementary stream of information, where cameras provide rich visual semantics [122, 244], LiDAR delivers precise 3D geometry [108, 125], and radar captures essential motion cues [179, 272], while event cameras offer microsecond-level temporal precision for high-speed dynamics [56, 100, 152, 266]. The effective integration of these heterogeneous data streams is paramount for achieving the 1 Figure 1 Overview of the paper structure. We systematically structure the landscape of multi-modal data pre-training for forging Spatial Intelligence. This work is organized into four key pillars: (1) Background, introducing onboard sensors and foundational learning paradigms; (2) Platforms & Datasets, analyzing benchmarks across autonomous vehicles, drones, and other robotic systems; (3) Pre-Training Methodologies, categorized into single-modality, cross-modal (Camera/LiDAR-centric), and unified frameworks; and (4) Applications, highlighting downstream tasks from 3D perception to open-world planning. holistic perception required for safe and generalizable autonomy [61, 104, 116, 189, 245]. In response to this demand, the research community has curated numerous large-scale and sensor-centric datasets [22, 47, 199], alongside specialized benchmarks for drones [162, 302] and robotic agents [57]. While these datasets provide an invaluable empirical foundation, they simultaneously highlight fundamental challenge that constitutes the central focus of this work. Most existing datasets heavily rely on costly manual annotations to support supervised learning paradigms [17, 47, 52], creating significant bottlenecks for scalability and generalization [102, 103, 205, 221, 246]. Consequently, there has been growing interest in representation learning methods that aim to distill meaningful features directly from raw sensor data, alleviating the dependence on extensive human supervision [6264, 240]. Particularly noteworthy is the emergence of foundation models, which facilitate large-scale, transferable pre-training across various domains, including vision [96, 182], 3D geometry [140, 298], and multi-modal scenarios [107, 264, 323]. Such foundation models provide unified paradigm for extracting general-purpose representations from diverse sensor inputs [105, 158, 270], significantly enhancing cross-domain adaptability and paving the way for next-generation world models [71, 106, 125, 126, 146, 166, 255, 305]. As result, pre-training strategies tailored specifically to sensor modalities have become an essential research frontier. As depicted in Fig. 1, these strategies form the core techniques to forge what we define as Spatial Intelligence capability that transcends simple detection to encompass holistic scene understanding, reasoning, and future prediction [127, 143, 270]. Current approaches include single-modality methods (e.g., LiDAR-only or camera-only) [3, 65, 108, 288], cross-modal knowledge transfer (e.g., distillation between camera and LiDAR) [101, 189, 270], and unified multi-modal pre-training frameworks [264, 323]. Understanding the overall landscape of these methods, as well as their connections to sensor characteristics, platform constraints, and the development of foundation models, is crucial for advancing robust and efficient perception capabilities 2 in intelligent systems. In this work, we systematically analyze the state-of-the-art techniques in representation learning from onboard sensor data, emphasizing multi-modal interactions and integration with foundation models. We first dissect foundational methodologies such as self-supervised learning, transfer learning, and multimodal learning, evaluating their respective strengths and limitations across various autonomous platforms, including self-driving vehicles, drones, robotic dogs, and rail transportation systems. By structuring and characterizing representative pre-training approaches according to modality composition, sensor interactions, and targeted applications, we highlight their adaptability to diverse sensor configurations and practical scenarios. Furthermore, we investigate key challenges in sensor representation learning, such as data sparsity, sensor noise, multi-modal alignment, and real-time processing demands. Finally, we propose promising directions for future research toward generative world models and embodied reasoning suitable for dynamic real-world environments."
        },
        {
            "title": "1.1 Scope of the Work",
            "content": "Multi-modal representation learning from onboard sensors encompasses various related areas, such as singlemodality pre-training, cross-modal fusion, and foundation model integration. Given the breadth of these topics, it is impractical to exhaustively analyze all relevant methods within single manuscript. Therefore, this work specifically concentrates on recent advances in foundation models for multi-modal representation learning, primarily focusing on onboard camera and LiDAR sensors [192, 264, 270]. Representative methods involving additional sensors such as radar and event cameras are also examined [37, 266, 267, 272]. We emphasize significant progress from the past five years, particularly highlighting influential works published in top-tier conferences and journals. In addition to technical approaches, we analyze widely adopted datasets, evaluation metrics, and sensor configurations. Finally, we investigate key challenges and outline promising future research directions."
        },
        {
            "title": "1.2 Relation to Previous Studies",
            "content": "Several existing studies [238, 240, 260, 318, 321] have recently explored representation learning in autonomous systems, typically focusing on individual sensor modalities, specific autonomous platforms, or particular downstream tasks. While these efforts offer valuable insights into targeted aspects of the field, they often discuss sensors and tasks independently, lacking an integrated perspective on how single-modality and cross-modal approaches collectively advance multi-modal representation learning. In contrast, our work presents comprehensive framework emphasizing the role of foundation models within multi-modal representation learning from onboard sensors. We systematically analyze modality-specific pretraining as well as cross-modal interactions and unified frameworks, clearly highlighting how these methods interconnect and contribute to robust, generalizable perception across various platforms and tasks. By bridging single-modality strategies with unified multi-modal paradigms, this study uniquely facilitates an in-depth understanding of recent advances, emerging trends, and future directions in multi-modal pre-training for autonomous systems."
        },
        {
            "title": "1.3 Organization",
            "content": "To provide clear roadmap of the field, we present comprehensive taxonomy encompassing datasets, pre-training paradigms, and downstream applications, as illustrated in Fig. 3. The remainder of this paper is structured as follows: Section 2 introduces the foundations of data representation learning for onboard sensors, covering sensor characteristics, pre-training paradigms, and the role of foundation models. Section 3 analyzes platform-specific datasets, including those collected from autonomous vehicles, aerial drones, and other robotic systems. Section 4 provides comprehensive analysis of pre-training methods, categorized by sensor modality, interaction level, and downstream application. 3 Section 5 investigates recent progress in open-world perception and planning, focusing on text-assisted understanding and the shift toward generative world models for end-to-end autonomy. Section 6 outlines key challenges in current research and highlights promising future directions. Section 7 concludes the paper with summary of major insights and takeaways."
        },
        {
            "title": "2 Background",
            "content": "Multi-modal pre-training from onboard sensors serves as the bedrock for forging Spatial Intelligence in autonomous systems. It aims to transcend simple feature extraction by distilling compact, discriminative, and semantically rich representations from diverse sensory inputs. By effectively integrating complementary information from modalities such as cameras, LiDAR and radar, these methods enable foundation models to not only perceive geometry and semantics but also reason about dynamics and affordances [127, 270]. In the context of autonomous deployment, developing scalable and reliable multi-modal pre-training approaches is essential for achieving robust open-world generalization and bridging the gap between passive perception and active embodied reasoning."
        },
        {
            "title": "2.1 Onboard Sensors and Data Characteristics",
            "content": "The sensory apparatus of intelligent agents, primarily comprising cameras, LiDAR, radar, and event cameras, presents heterogeneous data landscape characterized by distinct modalities and formats. Cameras provide dense semantic and textural information essential for scene understanding [122, 270], yet they remain susceptible to environmental variations such as illumination changes and adverse weather conditions. In contrast, LiDAR sensors deliver precise 3D geometric structures via point clouds [108, 157, 243], which offer robustness against lighting variations but suffer from inherent sparsity and limited semantic richness. Radar provides robust Doppler velocity cues even in adverse weather, albeit at lower spatial resolution [37, 272]. Complementing these with microsecond-level temporal precision, event cameras capture asynchronous brightness changes to handle high-speed dynamics and motion blur inherent in standard vision [98100, 266, 308]. Understanding the inherent properties of these sensors, specifically the trade-off between the semantic richness of vision and the geometric precision of ranging sensors, is fundamental. Effective representation learning must address these disparities to construct unified and coherent world model."
        },
        {
            "title": "2.2 Paradigms of Representation Learning",
            "content": "To forge spatial intelligence from the heterogeneous data streams described above, robust methodological framework is required. The evolution of this framework has followed clear logical progression, as chronologically illustrated in Fig. 2. Initially, to overcome the immense cost of manual annotation, the field turned to Self-Supervised Learning to extract meaningful features directly from vast quantities of unlabeled data. natural next step was to exploit the complementary nature of different sensors through Cross-Modality Interaction, creating more holistic representation than any single sensor could provide. Concurrently, Knowledge Distillation and Transfer Learning emerged as powerful technique to leverage priors from well-established vision foundation models, accelerating progress in 3D domains. Ultimately, these distinct paradigms are being synthesized under unified vision: the development of Foundation Models and Generative World Models. 2.2.1 Self-Supervised Learning Self-supervised learning (SSL) has emerged as the dominant paradigm for representation learning from unlabeled sensor data [20, 62, 63, 171]. By defining suitable pretext tasks, models leverage supervisory signals inherently present within the data. Classic strategies include Contrastive Learning, which discriminates between augmented views of the same instance, and Masked Modeling, which reconstructs obscured portions of inputs [63, 97, 108]. More recently, Generative Modeling (e.g., next-token prediction or video generation) has gained prominence [8, 305]. By learning to predict future frames or occupancy states, these methods enable models to internalize the physics and dynamics of the environment, serving as precursor to world models. 4 Figure 2 Chronological evolution of representative pre-training methods (20202025). The timeline illustrates the paradigm shift in representation learning for autonomous systems. Early approaches predominantly focused on single-modality self-supervision (e.g., LiDAR-only contrastive learning). In contrast, recent advancements (2023present) demonstrate surge in cross-modal synergy, characterized by Camera/LiDAR-centric methods and Unified pre-training frameworks, ultimately paving the way for generative world models and comprehensive spatial intelligence. 2.2.2 Cross-Modality Interaction Cross-modal interaction methods aim to fuse disparate sensor modalities into unified representation space, enhancing both robustness and semantic depth [166, 192, 256, 270]. For instance, projecting dense visual features from cameras onto sparse LiDAR point clouds allows models to achieve superior spatial-semantic reasoning [6, 250, 275]. Key challenges addressed by these approaches include spatio-temporal alignment, handling modality-specific noise, and maintaining robustness when one modality is degraded or missing. 2.2.3 Knowledge Distillation and Transfer Learning While transfer learning traditionally involves adapting pre-trained weights to new domains [145, 277], in the context of multi-modal autonomous systems, it increasingly takes the form of Knowledge Distillation. Here, powerful 2D vision foundation models (teachers) are used to guide the training of 3D sensor backbones (students). This allows 3D models to inherit the open-vocabulary capabilities and rich semantics of large-scale vision models [96, 171, 182] without requiring massive annotated 3D datasets [158, 189]. This paradigm effectively bridges the data scale gap between the 2D image domain and the 3D robotics domain. 2.2.4 Foundation Models Foundation models represent paradigm shift from specialized pipelines to unified, scalable representation learning [13, 196]. In the vision domain, the trajectory from CNNs [60] to Transformers [38] and generalpurpose encoders like DINO [20, 171] and SAM [96, 184] has established robust, transferable perceptual priors. Recent research integrates these visual priors into non-visual modalities (e.g., LiDAR and radar) via cross-modal alignment, enriching 3D perception with open-world semantics [101, 256, 272]. Crucially, the field is now advancing beyond perception towards Generative World Models [106, 305] and Vision-Language-Action (VLA) models [39, 143, 217]. These next-generation foundation models integrate vision, language, and action into unified reasoning framework [71, 118], enabling systems not just to recognize objects, but to simulate future scenarios and plan actions in complex, dynamic environments. Autonomous Vehicles (Section 3.2.1) Platform-Specific Datasets (Section 3) Drones (UAVs) (Section 3.2.2) Other Robotic Platforms (Section 3.2.3) Single Modality Pre-Training (Section 4.1) Pre-Training Techniques for Sensor Perception (Section 4) LiDAR-Centric Pre-Training (Section 4.2.1) Camera-Centric Pre-Training (Section 4.2.2) KITTI[47]; SemanticKITTI [9]; nuScenes[17]; Lyft L5[68]; Waymo[199]; Argoverse[22, 235]; ONCE[161]; nuPlan[18, 89]; MAN TruckScenes[44]; OminiHD-Scenes[304];AevaScenes [2]; PhysicalAI-AV [170] Campus[188]; CarFusion[185]; VisDrone[317]; DOTA[36, 239]; AU-AIR[16]; SensatUrban[69]; MAVREC[41]; BioDrone[302]; PDT[309]; UAVScenes [220] RailSem19[282]; SRLC[209]; Rail-DB[114]; Rail3D[93]; OSDaR23[93]; WHU-Railway3D[181]; Flow[31]; DartMouth[79]; SeaSAW[90]; WaterScenes[271]; LW3DR[163]; WaterVG[52]; LAQR[57];M3ED[21]; Pi3DET[124] 1.LiDAR-Only: (1) PointContrast[243]; DepthContrast [299]; GCC-3D [128]; ContrastiveSceneContexts [67]; SegContrast[169]; GroupContrast [212]; PatchContrast [194]; AD-PT [280]; E-SSL [64]; ProposalContrast [274]; BEVContrast [190]; 4DContrast [29]; (2) MV-JAR [248]; Occupancy-MAE [164]; Core [211]; MAELi [108]; R-MAE [203]; T-MAE [233]; GD MAE [263]; LSV-MAE [30]; BEV-MAE [135]; AD-L-JEPA [315]; TurboTrain [313]; NOMAE [1] (3) ALSO [14]; 4D Occ [95]; Copilot4D [292]; UnO [3]; PICTURE [251]; UNIT [191]; GPICTURE [252]; CooPre [301]; TREND [24]; 2.Camera Only: INoD[65]; TempO[109]; LetsMap[50]; NeRF-MAE[77]; VisionPAD[288] SimIPU [121]; SLidR [189]; SSPC-Im[78]; ST-SLidR [157]; Rel [158]; I2P-MAE [293]; TriCC [175]; Seal [145]; PRED [262]; LiMA [256]; ScaLR [180]; CSC [23]; GPC [174]; Cross Modal SSL [19]; SuperFlow [253]; HVDistill [294]; CM3D [94]; ImageTo360 [186]; OLIVINE [297]; EUCA-3DP[200]; GASP [151]; BALViT [66] DD3D [176]; DEPT [123]; OccNet [207]; GeoMIM [140]; GAPretrain [73]; ViDAR [270]; MIM4D [322]; SelfOcc [75]; UniScene [165]; MVS3D [230]; DriveWorld [166]; GaussianPretrain [249]; OccFeat [197]; S3PT [236]; UniFuture [127]; GaussianOcc [45]; GaussTR [83]; DistillNeRF [215]; Unified Pre-Training (Section 4.2.3) PonderV2 [316]; UniPAD [264]; UniM2AE [323]; ConDense [295]; NS-MAE [257]; BEVWorld [298]; CLAP [25]; GS3 [138]; Hermes [311]; LRS4Fusion [173]; Gaussian2Scene [141] Incorporating Additional Sensors (Section 4.3) Text-Grounded Understanding (Section 5.2) Unified World Representation for Action (Section 5.3) 1.Radar: RadarContrast [226]; AssociationNet [37]; MVRAE [314]; SSRLD [195]; Radar-Rep[272]; U-MLPNet [258]; 4D-ROLLS [142]; SS-RODNet [319, 320]; Radical [58]; RiCL [33]; RSLM [179]; 2.Event Camera: ECDP [266]; MEM [97]; DMM [76]; STP [129]; ECDDP [267]; EventBind [308]; EventFly [100] CLIP2Scene [26]; OpenScene [177]; CLIP-ZSPCS[228]; CLIP-FO3D [291]; POP-3D [210]; VLM2Scene [130]; IntraCorr3D[88]; SAL [172]; Affinity3D [137]; UOV [198]; OVO [202]; LangOcc [10]; VEON [303]; LOcc [279]; UP-VL [168]; ZPCS-MM[153]; CNS[27]; 3DOV-VLD[242]; CLIP2 [283]; AdaCo [324]; TT-Occ [286]; AutoOcc [310] OccWorld [305]; GenAD [265]; OccSora [214]; OccLLaMA [232]; OccVAR [86]; RenderWorld [261]; Drive-OccWorld [269]; LAW [117]; FSF-Net [53]; DriveX [192]; SPOT [259]; WoTE [119]; FASTopoWM [268]; OccTens [87]; OccVLA [143]; World4Drive [306] Open-World Perception and Planning (Section 5) t s o u f a - a D l M - u Figure 3 Taxonomy of multi-modal pre-training methodologies. We structure the landscape into three pillars: (1) Platformspecific datasets, (2) Core pre-training techniques classified by sensor interaction (single-modality, cross-modal, and unified), and (3) Advanced open-world perception and planning tasks. 6 Table 1 Summary of representative autonomous vehicle datasets. Region: AS (Asia), EU (Europe), NA (North America). Sensor Configuration: Camera, LiDAR, and Radar denote the count of equipped sensors. Data Statistics: Scenes refers to the number of dataset clips/sequences; Frames indicates the total annotated frames. Conditions: Weather captures adverse scenarios; d&n denotes day and night coverage. Symbol - indicates that the specific modality or statistic is unavailable/unsupported. Dataset Year Region Sensor Data Scenes Camera LiDAR Radar Frames Annotation 3D Det. 3D Occ. HD-Map E2E Plan Weather Time KITTI [47] 2012 ApolloScape [74] 2018 EU AS nuScenes [17] 2019 NA/AS SemanticKITTI [9] 2019 Waymo [199] 2019 Argoverse [22] 2019 Lyft L5 [68] 2019 A*3D [178] 2019 KITTI-360 [131] 2020 A2D2 [48] 2020 PandaSet [241] 2020 Cirrus [229] 2020 ONCE [161] 2021 Shifts [159] 2021 nuPlan [18] 2021 NA/AS EU NA NA NA AS EU EU NA - AS AS Argoverse2 [235] 2022 MONA [51] 2022 Dual Radar [296] 2023 MAN TruckScenes [44] 2024 OmniHD-Scenes [304] 2024 AevaScenes [2] NA EU AS EU AS NA 22 103 1000 22 1150 113 366 - 11 - 179 12 - - 3098 1000 3 151 747 1501 100 PhysicalAI-AV [170] 2025 NA/EU 310,895 4 2 6 4 5 7 7 2 4 6 6 1 7 - 8 7 3 1 4 6 6"
        },
        {
            "title": "3 Platform-specific Datasets",
            "content": "164 & 240-Beam 53d - - 53d - - - 164-Beam 164-Beam 516-Beam 264-Beam 264-Beam 140-Beam - 164-Beam 2VUX-1HA 132-Beam 164-Beam 564-Beam 232-Beam 15k 144k 40k - 230k 22k 46k 39k - 80k - 12.5k - 14k - 6285 - 15k - - - - 340 & 220-Beam - 150k - - - 10k 24d 64d 30k 64d 11.9k 10k - 232-Beam - 180-Beam 664-Beam 1128-Beam 6 - 1 - - 11 day day d&n day d&n d&n day d&n day day d&n d&n d&n d&n d&n d&n day d&n d&n d&n d&n d&n The efficacy of multi-modal representation learning is intrinsically linked to the scale, diversity, and fidelity of the underlying data. As the field transitions from supervised learning to self-supervised pre-training and foundation models, the role of datasets has evolved from static benchmarks to dynamic engines for forging Spatial Intelligence. In this section, we systematically evaluate prominent datasets across autonomous vehicles, aerial drones, and other robotic platforms. We analyze not only their sensor configurations and annotation richness but also their suitability for emerging tasks such as open-vocabulary perception and generative world modeling."
        },
        {
            "title": "3.1 Overview of Sensor Modalities and Datasets",
            "content": "Multimodal perception systems integrate heterogeneous suite of onboard sensors, primarily including RGB cameras, LiDAR, radar, event camera, and inertial measurement units (IMUs). Each modality offers distinct perceptual affordances and IMUs enable high-rate ego-motion estimation. Beyond raw sensing, the utility of dataset for modern pre-training is defined by several critical attributes: Sensor Configuration & Coverage: The spatial arrangement and field-of-view (FoV) determine the systems ability to construct holistic 360-degree world representations. Spatio-Temporal Synchronization: Precise calibration is non-negotiable for learning unified representations, especially for fusing high-frequency visual streams with sparse geometric points. Annotation Granularity & Modality: The shift from bounding boxes to dense occupancy grids, and recently to natural language descriptions, reflects the communitys move towards reasoning-centric tasks. Domain Diversity: Variations in weather, lighting, and geography are essential for training robust foundation models capable of zero-shot generalization. 7 Table 2 Chronological overview of state-of-the-art UAV-based datasets (2016Present). Region: Multi denotes data collected across multiple regions/platforms; Sim indicates synthetic simulation data. Viewpoint: (Ground-view), (Aerial-view), and AG (Aerial & Ground joint view). Annotations lists the supported downstream tasks. Dataset Year Region Viewpoint Sensor Configuration Frames Sensor Resolution Annotations Campus [188] 2016 NA Single (A) UAV123 [167] 2016 AS Multi (A) CarFusion[185] 2018 NA UAVDT [40] 2018 AS DOTA[239] 2018 Multi VisDrone[317] 2019 AS DOTA V2.0 [36] 2021 Multi MOR-UAV [160] 2020 AS EU AU-AIR [16] 2020 EU UAVid [155] 2020 Multi Single Single (A) Single (A) Single (A) Single Multi Single 1 Camera 1Camera 22Camera 1Camera Multi-Source 1Camera Multi-Source 1Camera 1Camera 1Camera 3Camera 929,499 1400 2019 110,000 720 720 53,000 1, 280 720 80,000 1080 540 2,806 4000 4000 179,264 3840 2160 11,268 4000 4000 10,948 1280 720, 1920 1080 32,823 1920 1080 300 3840 2160, 5472 3078 10,631 5472 43078, 7360 4192 MOHR [287] 2021 AS Multi (A) SensatUrban [69] 2021 EU UAVDark135 [110] 2023 AS MAVREC [41] 2023 BioDrone [302] 2024 AS PDT [309] 2024 AS UAV3D [273] 2024 Sim Multi (A) IndraEye [54] 2024 AS Multi (A) UAVScenes [220] 2025 AS Multi (A) Single (A) Single 1Camera 1Camera 2Camera 1Camera - 125,466 537,030 EU Multi (AG) 304,000 Single (A) Single (A) 1Camera, 1LiDAR 5,775 20,000 2,000 1Camera, 1LiDAR 120, 5Camera 1Camera 8688 5792 - 1920 1080 2700 1520 1400 1080 5472 3648, 640 640 800 450 1280 720, 640 480 24482048 Target Forecasting/ Tracking UAV Tracking 3D Vehicle Reconstruction 2D object Detection/ Tracking 2D Object Detection 2D Object Detection/ Tracking 2D Object Detection Moving Object Recognition 2D Object Detection 2D Semantic Segmentation 2D Object Detection 3D Segmentation 2D Object Tracking 2D Sup/Semi-Sup Object Detection 2D Object Tracking 2D Object Detection 3D Object Detection/ Tracking 2D Object Detection/ Semantic Segmentation 2D/3D Semantic Segmentation; 6-DoF Visual Localization The following subsections examine datasets from specific platforms, revealing how platform-specific constraints shape data characteristics and subsequent learning paradigms."
        },
        {
            "title": "3.2 Datasets Acquired from Various Platforms",
            "content": "3.2.1 Autonomous Vehicles Autonomous driving serves as the primary testbed for multi-modal spatial intelligence. Vehicles typically deploy redundant sensor suite consisting of surround-view cameras, high-beam LiDARs, and radars to ensure safetycritical perception [104, 222, 244]. The continuous collection of synchronized sensor streams has produced massive-scale datasets [7, 17, 47, 170, 199], which act as the fuel for self-supervised pre-training. Current methodologies leverage these unlabeled streams for pretext tasks such as temporal future prediction [166, 270], cross-modal masked reconstruction [1, 135, 264], and contrastive distillation [189, 256], effectively turning raw data into transferable representations without human labeling. Table 1 summarizes representative datasets. Notably, the evolution from early perception-centric benchmarks (e.g., KITTI [47]) to modern reasoning-centric datasets (e.g., nuPlan [18] and Argoverse 2 [235]) highlights crucial trend: the integration of high-definition maps, long-horizon trajectories, and increasingly, languagebased scenario descriptions [143, 219]. These rich annotations are pivotal for training next-generation End-to-End planners and Vision-Language-Action (VLA) models [71, 118, 119]. 3.2.2 Drones (UAVs) Unmanned Aerial Vehicles (UAVs) present unique perception challenges due to their birds-eye viewpoints, six degrees-of-freedom (6-DoF) motion, and rapid scale changes [69, 155, 220]. While RGB cameras and IMUs remain standard, advanced datasets now incorporate LiDAR to capture 3D structural information for complex environments [69, 273]. Table 2 details key UAV datasets. Unlike ground vehicles, UAV data is characterized by significant perspective distortion and motion blur [155, 302]. Consequently, pre-training in this domain heavily utilizes transfer learning from ground-level or satellite imagery [317], adapting visual foundation models to aerial domains. Recent efforts also explore cross-view geo-localization and self-supervised flow estimation to handle the dynamic nature of flight. The emergence of multi-modal UAV datasets [220, 273, 302] is crucial for extending Spatial Intelligence from 2D ground planes to 3D volumetric spaces. 8 Table 3 Overview of multi-modal datasets for diverse robotic platforms. This table categorizes datasets into three specialized domains: Railways, Unmanned Surface Vehicles (USVs), and Legged Robots. These benchmarks extend spatial intelligence research to constrained tracks, maritime environments, and complex terrains. Dataset Year Region Platform Sensors RailSem19 [282] 2019 FRSign [59] 2020 EU EU RAWPED [208] 2020 EU, AS SRLC [209] 2021 EU Rail-DB [114] 2022 AS EU RailSet [325] 2022 EU OSDaR23 [201] 2023 EU Rail3D [93] 2024 WHU-Railway3D [181] 2024 AS Railway Railway Railway Railway Railway Railway Railway Railway Railway MODS [15] 2022 FloW [31] 2021 AS Unmanned Surface Vehicle DartMouth [79] 2021 NA Unmanned Surface Vehicle EU Unmanned Surface Vehicle SeaSAW [90] 2022 EU, NA Unmanned Surface Vehicle WaterScenes [271] 2023 AS Unmanned Surface Vehicle MVDD13 [216] 2024 AS Unmanned Surface Vehicle SeePerSea [80] 2024 AS, NA Unmanned Surface Vehicle WaterVG [52] 2024 AS Unmanned Surface Vehicle [57] 2024 AS Han et al . [154] 2025 AS Luo et al . QuadOcc [193] 2025 AS Legged Robots Legged Robots Legged Robots Camera 2Camera 1Camera LiDAR Camera 1Camera 9Camera, 6LiDAR, 1Radar 4Camera, 1LiDAR 1LiDAR 2Camera, 14D Radar 3Camera, 1LiDAR 2Camera, 1LiDAR 5Camera 1Camera, 14D Radar Camera x1 1Camera, 1LiDAR 1Camera, 14D Radar Depth Camera Panoramic Camera Panoramic Camera, 1LiDAR M3ED [21] 2023 NA Car, UAV, Legged Robots 3 Camera, 2 Event Camera,1LiDAR Frames 8,500 105,352 26,000 - 7,432 6,600 1,534 - 40 tiles 2,000 - 8,175 1,900,000 Annotations Image Classification, Semantic Segmentation Railway Signaling Reading 2D Object Detection Point Cloud Generation, Semantic Segmentation Rail Detection Railway Anomaly Detection Rail and Object Detection, LiDAR Segmentation LiDAR Semantic Segmentation LiDAR Segmentation 2D Object Detection 2D Object Detection, Semantic Segmentation 2D Object Detection 2D Object Detection, Tracking, Classification 54,120 2D Object Detection, Semantic/ Panoptic Segmentation - 10,906 11,568 - 1,920s 8,000 - 2D Object Detection 2D & 3D Object Detection Multi-Task Visual Grounding Animal Motions 2D Object Tracking 3D Occupancy Depth Estimation, Semantic Segmentation 3D Object Detection Pi3DET [124] 2025 NA Car, UAV, Legged Robots 3 Camera, 2 Event Camera,1LiDAR 51,545 3.2.3 Other Robotic Platforms Beyond cars and drones, diverse robotic platforms such as Unmanned Surface Vehicles (USVs) [52, 271], railway systems [114, 282, 325], and legged robots [21, 55, 57] operate in highly constrained or unstructured environments. These domains challenge pre-training models with unique noise patterns (e.g., water reflections for USVs) and motion dynamics (e.g., non-linear locomotion for quadrupeds). Table 3 lists representative datasets. For instance, datasets for legged robots [57, 154] emphasize egocentric perception under severe camera shake, motivating research into robust, motion-aware representation learning. Similarly, rail and USV datasets focus on long-range, track-constrained perception [181, 271, 325]. growing trend in these specialized domains is the use of Simulation-to-Real transfer and domain adaption [21, 124]. Engines like QuaDreamer [237] generate synthetic training data to supplement scarce real-world samples, training models that can generalize to physical robots via domain randomization. This highlights the increasing role of synthetic data in democratizing foundation models for varied robotic form factors [209]."
        },
        {
            "title": "3.3 Key Dataset Trends and Implications",
            "content": "Analyzing the landscape of platform-specific datasets identifies three evolutionary trends that are reshaping multi-modal pre-training: From Perception to Reasoning and Action. Modern datasets are moving beyond bounding boxes. Benchmarks like nuPlan [18] and OmniDrive [219] introduce planning trajectories, logic-based scenarios, and open-vocabulary language labels. This shift enables the training of models that do not just see but reason and act, laying the groundwork for Embodied AI and VLA models [71, 118, 143, 177]. The Rise of Synthetic and Generative Data. Recognizing the long-tail limitations of real-world data, there is surge in high-fidelity synthetic datasets and simulation environments [162, 209, 237]. This supports the development of Generative World Models, which can simulate infinite what-if scenarios for robust policy learning, effectively closing the loop between perception and simulation [106, 126]. Scale and Diversity for Foundation Models. The explosion in data volume and modality diversity (from LiDAR to Event cameras) has rendered manual annotation obsolete [21, 108, 266]. This reality firmly establishes Self-Supervised Pre-Training as the necessary paradigm. Future progress will depend on data engines that can automatically curate, label, and align these massive multi-modal streams to feed hungry foundation models [101, 264]. These trends collectively signal transition: datasets are no longer just static benchmarks for performance Table 4 Comprehensive summary of LiDAR-based pre-training techniques. The table categorizes methods into LiDAR-only (single-modality) and LiDAR-centric (cross-modal) paradigms. Input Modality: denotes LiDAR input; SC and MC refer to Single-Camera and Multi-Camera data used for cross-modal distillation or alignment. Method Venue Input Modality Proxy Task Downstream Task Dataset Key Contribution PointContrast [243] DepthContrast [299] GCC-3D [128] SimIPU [121] ProposalContrast [274] GD-MAE [263] ALSO [14] BEV-MAE [135] ECCV20 ICCV21 ICCV21 AAAI22 ECCV22 CVPR23 CVPR23 AAAI24 MAELi-MAE [108] WACV24 3DV24 BEVContrast [190] PPKT [147] SLidR [189] ST-SLidR [157] TriCC [175] arXiv21 CVPR22 CVPR23 CVPR23 Seal [145] NeurIPS23 CVPR24 CSC [23] OLIVINE [297] NeurIPS24 IJCV24 HVDistill [294] CVPR24 ScaLR [180] ECCV24 SuperFlow [253] LargeAD [101] arXiv25 CVPR25 LiMoE [254] ICCV25 LiMA [256] L + SC L + MC + MC + MC + MC + MC + MC + MC + MC + MC + MC + MC + MC + MC Spatial Contra. (Point) Spatial Contra. Spatial Contra. Spatial Contra. Spatial Contra. (Region) MAE Occupancy Estimation BEV MAE MAE BEV Contra. Spatial Contra. Spatial Contra. Spatial Contra. Spatial & Temp. Contra. Spatial Contra. Spatial Distill. Spatial Distill. Spatial Distill. Spatial Distill. Spatial & Temp. Contra. Spatial Contra. Spatial & Temp. Distill. Spatial & Temp. Distill. Sem-Seg. Sem-Seg./Det. Det. Sem-Seg. Det. Sem-Seg./Det. Occ. Det. Det. Sem-Seg./Det. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg. Sem-Seg./Det. Sem-Seg. Sem-Seg./Det. Sem-Seg./Det. Sem-Seg./Det. ScanNet, SemanticKITTI Waymo, nuScenes Waymo SemanticKITTI Waymo, nuScenes Waymo nuScenes Waymo Waymo nuScenes Point-wise contrastive learning on augmentations Frame-wise depth consistency learning Geometry-aware contrast with clustering Simple 2D-3D spatial alignment Contrastive learning on detection proposals MAE with generative decoder Occupancy-based self-supervision Masked BEV feature learning MAE for large-scale LiDAR representation learning Contrastive learning in BEV space nuScenes nuScenes nuScenes nuScenes nuScenes nuScenes nuScenes nuScenes nuScenes, KITTI, PandaSet nuScenes nuScenes, KITTI, Waymo nuScenes nuScenes Pixel-to-point contrastive transfer learning Superpixels to guide the image-to-LiDAR pre-training Class-balanced cross-modal contrastive learning Triangle-constrained spatiotemporal contrastive Transfer knowledge from foundation models to 3D Unified baseline for large-scale pretraining Fine-grained contrast with vision features Hybrid-view distillation from images to 3D Directly distill knowledge from image to LiDAR Spatiotemporal contrastive for knowledge transfer Large-scale multi-dataset pre-training MoE-based multi-representation pre-training Cross-view and long-horizon distillation for pre-training evaluation, but active components in the loop of training generative, reasoning-capable Spatial Intelligence agents [71, 219, 304]."
        },
        {
            "title": "4 Pre-Training Techniques for Perception",
            "content": "In this section, we critically examine the methodologies that empower autonomous systems to learn robust representations from raw sensor data. As depicted in the taxonomy  (Fig. 3)  , we structure the landscape based on sensor interaction paradigms: Single-Modality baselines, Multi-Modality synergy (including Camera-Centric and LiDAR-Centric distillation), and Unified frameworks that jointly optimize cross-modal encoders. Beyond the modality-based categorization, we emphasize crucial trend: the integration of Foundation Models and Generative Objectives. Recent approaches are shifting from simple discriminative tasks to generative reconstruction [92, 264](e.g., NeRF, 3DGS) and future forecasting [127, 166, 270], leveraging the rich semantic priors of large-scale vision models to enhance geometric reasoning [189, 279]. We also briefly discuss complementary sensors such as radar and event cameras [179, 266, 267, 272]. Finally, we synthesize benchmark performance to offer holistic evaluation of how these pre-training techniques translate to downstream perception tasks."
        },
        {
            "title": "4.1 Single-Modality Pre-Training",
            "content": "Single-modality pre-training serves as the bedrock of perception, aiming to extract intrinsic semantic and geometric features from individual sensor streams without the aid of cross-modal supervision. Given their ubiquity in autonomous systems, we primarily focus on Camera and LiDAR modalities in this subsection. Mastering these single-modality representations is prerequisite for effective sensor fusion and interaction, as it ensures that each branch of multi-modal system contributes robust, high-quality features to the unified world model. 4.1.1 LiDAR-Only Pre-Training LiDAR sensors provide precise and metric-accurate 3D measurements, making them indispensable for tasks requiring fine-grained geometric perception, such as object detection and occupancy prediction. Unlike cameras, LiDAR data is inherently sparse, unordered, and lacks texture, necessitating specialized pre-training objectives to capture underlying topological structures and temporal dynamics. As illustrated in Fig. 4, current research focuses on three primary paradigms to forge robust 3D representations from unlabeled point 10 clouds: Masked Reconstruction for structural understanding, Contrastive Learning for spatial invariance, and Temporal Forecasting for dynamic world modeling. Masked Reconstruction and Structural Completion. Drawing inspiration from Masked Autoencoders (MAE) in general vision [63] and NLP [35], this paradigm forces the network to infer unseen geometric structures from partial observations, thereby learning holistic spatial priors. To handle the irregularity of point clouds, approaches such as GD-MAE [263] and BEV-MAE [135] leverage regular 2D/3D grids for structured masking, while MAELi [108] explicitly reconstructs intensity values to incorporate surface reflectivity properties. MVJAR [248] and Occupancy-MAE [164] also operate on voxelized features to enforce spatial consistency. Recent advances extend this concept to the temporal dimension. T-MAE [233] and LSV-MAE [30] reconstruct sequence-level motion patterns. Furthermore, AD-L-JEPA [315] moves beyond voxel reconstruction to latent space prediction, focusing on learning abstract relational reasoning rather than low-level details. Contrastive Learning and Spatial Invariance. Contrastive learning aims to learn discriminative feature spaces where semantically similar points or scenes are pulled together. This paradigm has evolved from point-level discrimination to multi-scale hierarchical understanding. PointContrast [243] pioneered this direction by optimizing point-level invariance across augmented views, while DepthContrast [299] utilized singleview depth maps to construct informative pairs. Subsequent research has scaled this objective to various spatial hierarchies: Patch/Proposal-level methods [169, 194, 274] focus on object-centric features; BEV-level approaches [64, 190, 280] align features in the birds-eye view for downstream perception tasks; and Scene-level methods [29, 67] capture global context. This hierarchical evolution demonstrates the versatility of contrastive objectives in encoding geometry at different granularities. Figure 4 Schematic illustration of representative LiDAR-only pre-training paradigms. To learn robust geometric representations from sparse point clouds without annotations, methods typically adopt three strategies: (a) Masked Autoencoding (MAE), which reconstructs missing structures to learn local geometry; (b) Contrastive Learning, which enforces view-invariant feature discrimination; and (c) Temporal Forecasting, which predicts future frames to capture dynamic scene evolution. Temporal Forecasting and Predictive Modeling. Moving beyond static perception, forecasting-based pre-training leverages the sequential nature of LiDAR streams to anticipate future states, serving as precursor to predictive world models. Early works like ALSO [14] and 4D-Occ [95] formulate pre-training as occupancy or flow prediction, enabling the model to fill in future geometric voids. Recent frameworks such as Copilot4D [292] and UnO [3] explicitly predict point cloud sequences, fostering temporally consistent representations. Advanced methods further incorporate complex interactions: PICTURE [251] and UNIT [191] introduce mutual information maximization and spatio-temporal clustering, while CooPre [301] and TREND [24] extend forecasting to multiagent cooperative scenarios. These approaches equip models with the predictive capacity essential for planning in dynamic environments. 4.1.2 Camera-Only Pre-Training Visual data from camera offers the rich semantic information for scene understanding. While supervised pre-training on generic datasets like ImageNet [34] and MS-COCO [133] remains standard initialization strategy for common vision backbones (e.g., ResNet [60] and ViT [38]), it suffers from domain gap when applied to the complex, 3D-centric tasks of autonomous systems. Consequently, the field has pivoted towards Self-Supervised Learning (SSL) on domain-specific onboard data, evolving through three key paradigms: Domain and Temporal Consistency. Handling domain shifts and exploiting temporal continuity are fundamental for robust vision. INoD [65] addresses the domain generalization challenge by formulating dataset affiliation prediction pretext task, interleaving feature maps from disjoint domains to learn invariant representations. Capitalizing on the sequential nature of driving videos, TempO [109] treats region-level feature ordering as sequence prediction problem. By modeling the temporal evolution of features, it enables the visual encoder to capture motion dynamics and causality, which are critical for planning. Geometric Lifting to BEV. Bridging the gap between 2D images and 3D perception is core objective. LetsMap [50] pioneers label-efficient approach for semantic Birds-Eye-View (BEV) mapping. It leverages the spatial constraints inherent in monocular sequences to enforce consistency between perspective and BEV representations, effectively lifting 2D semantics into metric space without relying on expensive dense annotations or LiDAR depth. Neural Fields and Volumetric Reasoning. The most recent frontier involves incorporating implicit 3D representations into visual pre-training. NeRF-MAE [77] represents paradigm shift, adapting Masked Autoencoders (MAE) to Neural Radiance Fields (NeRF). By using posed RGB images to reconstruct masked volumetric tokens, it forces the transformer to internalize 3D spatial layouts and view-dependent effects. Similarly, VisionPAD [288] introduces voxel-centric framework that combines voxel warping with multi-frame photometric consistency. This allows the model to learn fine-grained motion and geometry directly from image streams, offering scalable alternative to depth-supervised methods. Collectively, these methods illustrate trajectory from learning 2D semantics to mastering 3D geometry and temporal dynamics, enabling cameras to function as standalone sensors for spatial intelligence."
        },
        {
            "title": "4.2 Multi-Modality Pre-Training",
            "content": "While single-modality pre-training establishes the foundational feature space, forging true Spatial Intelligence requires the synergy of heterogeneous sensors. The physical world manifests in diverse signals: cameras capture dense semantic texture, while LiDAR and radar provide sparse but metric-accurate geometry and kinematics. Multi-modality pre-training aims to bridge the semantic-geometric gap by learning unified representations that leverage the complementary strengths of these modalities. We categorize these approaches based on the information flow direction: LiDAR-Centric (distilling visual semantics into 3D geometry), Camera-Centric (injecting geometric priors into 2D features), and Unified Frameworks (jointly optimizing modality-agnostic representations). This taxonomy highlights how crossmodal interactions evolve from simple alignment to unified world modeling. 4.2.1 LiDAR-Centric Pre-Training LiDAR sensors excel at capturing precise 3D structures but suffer from inherent semantic sparsity and lack of texture. Conversely, the computer vision community has cultivated powerful foundation models [20, 96, 182] that encapsulate rich, open-world semantic knowledge. LiDAR-centric pre-training aims to bridge this asymmetry by treating visual signals as Privileged Information during training. The goal is to transfer the semantic richness of 2D images into 3D point cloud networks, enabling them to hallucinate semantic features even when cameras are absent during inference. As illustrated in Fig. 5, this paradigm has evolved through four key strategies: Masked Reconstruction with Visual Guidance. Integrating cross-modal cues from camera images into the Masked Autoencoder (MAE) framework [63] enhances structural learning for LiDAR point cloud. I2P-MAE [293] and CM3D [94] condition the reconstruction of masked LiDAR tokens on visible image patches, forcing the network to infer 3D geometry from 2D semantic context. ImageTo360 [186] and EUCA-3DP [200] extend this to full-scene scales, leveraging BEV context to promote holistic spatial reasoning that fuses visual texture with geometric occupancy. Cross-Modal Contrastive Alignment. The foundational approach involves aligning 2D and 3D feature spaces through contrastive learning. By maximizing the similarity between corresponding image pixels and projected LiDAR points, models learn to associate geometric clusters with visual concepts. SimIPU [121] and SLidR [189] pioneered this by constructing point-pixel pairs to enforce local semantic consistency. Recent extensions like ST-SLidR [157] and Cross-Modal SSL [19] incorporate temporal constraints and region-aware affinity, improving the robustness of alignment against calibration errors and dynamic objects. Knowledge Distillation from Foundation Models. Moving beyond simple alignment, recent works leverage 2D foundation models as teachers to distill open-vocabulary semantics into 3D students. Seal [145] and ScaLR [180] utilize the segmentation capability of SAM and vision transformers to generate high-quality pseudo-labels or soft feature targets for point clouds. CSC [23] and OLIVINE [297] further refine this process by incorporating hierarchical clustering and class-aware gating, ensuring that the distilled knowledge respects the geometric boundaries of 3D objects. This strategy effectively imparts sight to blind LiDAR networks. Temporal Dynamics and Motion Transfer. Static cross-modal alignment is insufficient for dynamic autonomous systems in real world. SuperFlow [253] and PRED [262] introduce temporal supervision by transferring motion knowledge from video to point cloud sequences. By aligning the temporal evolution of features across modalities, these methods enable LiDAR backbones to capture long-horizon dynamics [256], serving as stepping stone towards predictive world models. Figure 5 Taxonomy of LiDAR-centric pre-training methodologies. To bridge the semantic gap of point clouds, these approaches leverage images as privileged information during training. The main paradigms involve: (a) Cross-modal MAE-based Pre-Training, which incorporates 2D-guided masking strategies to enhance geometric reconstruction and structural understanding; (b) Cross-modal Contrastive/Distillation PreTraining, which either enforces feature alignment between modalities or directly transfers rich open-vocabulary semantics from pre-trained Vision Foundation Models (VFMs) to 3D encoders; and (c) Temporal Pre-Training, which exploits video-LiDAR sequences to capture motion dynamics and enforce spatiotemporal consistency. In summary, LiDAR-centric pre-training transforms point cloud networks from pure geometric processors into semantically aware perception engines, significantly enhancing performance in detection and semantic segmentation tasks, particularly in data-scarce regimes. Table 4 provides comprehensive taxonomy of these LiDAR-based techniques, categorizing them by input modality, proxy tasks, and downstream applications. 4.2.2 Camera-Centric Pre-Training Camera-centric pre-training addresses the ill-posed nature of monocular perception: recovering 3D structures from 2D projections. While cameras are cost-effective and ubiquitous, they lack intrinsic and accurate depth. To overcome this, recent methods utilize LiDAR data as Geometric Supervisor during pre-training. By injecting precise depth and structural priors into visual backbones, these models learn to hallucinate 3D geometry from images alone, retaining efficient camera-only inference while benefiting from LiDAR-grade supervision. As visually taxonomized in Fig. 6, this domain bifurcates into two primary streams: Geometric Perception (via explicit depth or feature distillation) and Predictive World Modeling (via forecasting or neural rendering). detailed overview of these vision-centric methodologies, including their proxy tasks and key contributions, is summarized in Table 5. 13 Table 5 Overview of camera-centric and unified pre-training methodologies. This table summarizes representative approaches that leverage visual data as the primary input. Input Modality: MC denotes Multi-Camera setups; indicates the use of LiDAR; signifies the integration of temporal information for dynamic modeling. Method Venue Input Modality Proxy Task Downstream Task GeoMIM [140] OccNet [207] ICCV23 ICCV23 UniScene [165] RA-L24 DriveWorld [166] CVPR24 ViDAR [270] CVPR24 IJCV25 MIM4D [322] arXiv24 GaussianPretrain [249] VisionPAD [288] CVPR25 MC MC MC MC MC-T MC-T MC-T MC-T Det./Map/Occ. Det./Map Det./Occ. Reconstruction Forecasting Forecasting Forecasting Det./Map/Occ./E2E Forecasting Det./Map/Occ./E2E Det./Map/Vec. Map Rendering Det./Occ./Vec. Map Rendering Det./Occ. Rendering Dataset nuScene nuScene nuScene nuScene nuScene nuScene nuScene nuScene Key Contribution Leveraging the knowledge of pretrained LiDAR model Utilizing the semantic occupancy as the latent feature supervision Utilizing the geometric occupancy as the latent feature supervision Utilizing 4D occupancy as the latent feature supervision Visual point cloud forecasting Investigating spatial and temporal relations with video Leveraging the Gaussian representation Vision-only pre-training with temporal constraint UniPAD [264] CVPR24 UniM2AE [323] ECCV24 NS-MAE [257] CASE MC & MC & MC & BEVWorld [298] LRS4Fusion [173] arXiv24 MC-T & ICCV25 MC-T & Rendering Rendering Rendering Rendering Forecasting Det./Seg. Det./Map Det./Map Det./Motion Det./Depth nuScene nuScene nuScene nuScene LR & nuScenes Multi-modality pre-training with MAE Multi-modality pre-training with MAE and extra alignment Multi-modality pre-training with differential neural volume rendering Multi-modality with temporal information Self-supervised sparse sensor fusion for long range perception Geometric Perception: From Depth to Distillation. The primary goal here is to equip vision models with spatial awareness by aligning 2D features with 3D structural constraints. Explicit Depth Pre-Training (Fig. 6 (a)) serves as the foundational approach. Early works like DD3D [176] and DEPT [123] leverage pseudo-depth supervision from LiDAR to initialize 3D object detectors, effectively grounding visual features in metric space. Moving beyond simple depth maps, Distillation-based Pre-Training (Fig. 6 (b)) aligns latent representations. OccNet [207] and SelfOcc [75] advance this by learning to predict dense 3D occupancy grids, utilizing LiDAR occupancy as ground-truth supervisor. Furthermore, Masked Image Modeling (MIM) has been adapted for geometric consistency: GeoMIM [140] and MIM4D [322] reconstruct masked image patches by cross-referencing with projected LiDAR points, forcing the network to internalize 3D spatial correspondences within the feature extraction process. Figure 6 Overview of camera-centric pre-training paradigms (LiDARto-Vision). These methods aim to inject 3D geometric priors into 2D visual backbones using LiDAR as supervisor. Key approaches include: (a) Depth Estimation for explicit geometry learning; (b) Feature Distillation to align 2D-3D latent spaces; and (c) Forecasting and (d) Generative Rendering, which empower vision models to hallucinate 3D structures and predict future dynamics from monocular inputs. Predictive World Modeling: Forecasting and Rendering. This stream represents the transition from static perception to dynamic simulation, requiring models to understand temporal evolution and photorealistic synthesis. Forecasting-based Pre-Training (Fig. 6 (c)) compels models to predict future states from current video streams, thereby internalizing the physics of the environment. ViDAR [270] pioneers \"Visual Point Cloud Forecasting,\" treating future LiDAR points as supervision signal for historical visual inputs. Extensions like DriveWorld [166] and UniScene [165] scale this to 4D occupancy, learning spatio-temporal abstractions that facilitate long-term planning. Complementing this, Rendering-based Pre-Training (Fig. 6 (d)) exploits the differentiability of neural fields. Frontier methods like GaussianPretrain [249] and GaussianOcc [45] incorporate 3D Gaussian Splatting (3DGS) [92] into the pre-training loop. By enforcing photometric consistency through differentiable rendering, these models learn continuous, high-fidelity geometric representations that surpass discrete voxels in precision. Finally, generative approaches such as GenAD [265] and OccSora [214] integrate these concepts to function as neural simulators, paving the way for end-to-end agents capable of reasoning about future consequences [232, 269]. 14 4.2.3 Unified Pre-Training Unified pre-training represents the convergence of multi-modal learning. Unlike asymmetric distillation (LiDAR-centric or Camera-centric), which treats one modality as primary, unified frameworks jointly optimize encoders for heterogeneous modalities within shared latent space. As explicitly illustrated in Fig. 7, canonical unified framework processes data through cohesive pipeline encompassing masking, alignment, and reconstruction. This paradigm can be deconstructed into three critical phases: Multi-Modal Masking and Encoding. The pipeline begins by treating raw sensor inputs as discrete tokens. As depicted in the Multi-Modal Masking stage of Fig. 7, methods like UniPAD [264] and UniM2AE [323] apply randomized masking to both LiDAR points and image patches. This forces the encoders to learn robust local features rather than relying on redundant shortcuts. Specifically, the visual branch typically employs Camera Encoder, while the geometric branch utilizes LiDAR Encoder to extract high-dimensional primitives from sparse inputs. View Transformation and Unified Fusion. To bridge the dimensional gap between 2D images and 3D points, the framework transforms heterogeneous features into common coordinate system. As shown in the center of Fig. 7, visual features undergo View Transform [111, 122, 264], while point features are processed via 3D-to-BEV Flattening. These streams converge at the Multi-Modal Fusion and Interaction stage, resulting in Unified Representation manifesting typically as BEV Features or dense Volumetric Features. Approaches like BEVWorld [298] and GS3 [138] leverage this shared latent space to enforce strict geometric consistency between modalities. Figure 7 Illustration of unified multi-modal pre-training frameworks. Unlike asymmetric distillation, unified approaches jointly optimize Camera and LiDAR encoders within shared representation space. This paradigm facilitates the learning of modality-agnostic features that integrate both semantic richness and geometric precision, forging holistic basis for Spatial Intelligence. Generative Reconstruction. The final objective is to validate the understanding of the scene by reconstructing the masked or missing information. The right side of Fig. 7 demonstrates that the unified representation is decoded to simultaneously reconstruct the original LiDAR geometry, Image texture, and often auxiliary Depth Maps. By optimizing for this holistic reconstruction objective, the model learns modality-agnostic features that integrate semantic richness with geometric precision, ensuring robustness even when individual sensors are compromised during inference [25, 257, 295]. In conclusion, unified pre-training moves beyond simple sensor fusion; it forges holistic understanding of the physical world that is independent of the specific sensing apparatus, key characteristic of true Spatial Intelligence."
        },
        {
            "title": "4.3 Incorporating Additional Sensors",
            "content": "In complex open-world environments, reliance solely on cameras and LiDAR can lead to perceptual failures under adverse conditions, such as severe weather, high-speed motion, or extreme lighting changes [100, 104, 105, 244]. To forge robust Spatial Intelligence, incorporating complementary sensors becomes imperative. Millimeter-wave radar offers resilience against fog and rain via Doppler signatures, while Event Cameras (neuromorphic sensors) capture microsecond-level dynamics with high dynamic range. Integrating these modalities into pre-training frameworks not only enhances system reliability but also extends the operational design domain of autonomous agents. In this subsection, we analyze representation learning paradigms specialized for these sensors. 15 4.3.1 Radar Pre-Training Radar point clouds differ significantly from LiDAR in their scarcity, noise characteristics (clutter), and unique velocity channels. Pre-training methods in this domain focus on suppressing noise and extracting meaningful structural features through three key approaches: Cross-Modal Alignment and Supervision. Due to the semantic sparsity of radar returns, aligning them with richer modalities is standard strategy. AssociationNet [37] utilizes well-structured LiDAR point clouds to supervise radar feature learning, enhancing geometric consistency. RadarContrast [226] and RiCL [33] employ contrastive learning to enforce invariance between radar representations and their multi-view or temporal counterparts, effectively grounding radar features in stable metric space. Masked Modeling for Sparse Signals. Adapting masked reconstruction to radar involves dealing with extreme sparsity. MVRAE [314] and RSLM [179] introduce autoencoding frameworks that reconstruct raw radar signals, enabling the model to learn spatiotemporal priors and filter out multi-path noise. Radar-Rep [272] and Radical [58] further refine this by designing radar-specific masking strategies and curriculum learning to handle the high variance in signal quality. Domain Adaptation and Simulation. To bridge the gap between synthetic and real-world radar data, domainadaptive strategies are crucial. SS-RODNet [319, 320] facilitates transfer learning across domains, while U-MLPNet [258] explores lightweight inductive biases to enable efficient radar perception on edge devices. 4.3.2 Event Camera Pre-Training Event cameras capture asynchronous brightness changes, offering paradigm shift for high-speed perception. Pre-training methodologies here must address the non-grid, asynchronous nature of event streams: Spatiotemporal Reconstruction. Reconstructing dense signals from sparse events forces the model to understand scene dynamics. MEM [97] and DMM [76] adapt masked modeling to event streams, reconstructing spatial structures from fragmented temporal triggers. ECDP [266] and ECDDP [267] focus on future frame prediction, leveraging the high temporal resolution of events to forecast motion with exceptional precision. STP [129] introduces specialized transformer architectures to simultaneously model the spatial sparsity and temporal continuity inherent in event data. Cross-Modal Synergy. Integrating events with standard RGB frames combines high dynamic range with semantic texture. EventBind [308] aligns asynchronous event streams with synchronous RGB frames in shared latent space, enabling semantic understanding even in high-motion blur scenarios. EventFly [100] further demonstrates the utility of this synergy for agile navigation in aerial robotics, where latency is critical bottleneck. 4.3.3 Auxiliary Modalities Beyond primary perception sensors, other onboard instruments serve as critical sources of weak supervision or geometric constraints during pre-training [17, 22, 47], rather than just as input modalities: Inertial Measurement Units (IMU): Instead of learning IMU representations in isolation, recent works utilize IMU data to enforce ego-motion consistency. By providing accurate acceleration and orientation priors, IMUs supervise the temporal alignment of vision and LiDAR backbones, essential for learning physically plausible world models. GPS and Localization Signals: Global positioning data provides coarse-grained location context. In large-scale pre-training, GPS traces are often used to retrieve topologically neighboring scenes or to enforce trajectory consistency in long-horizon prediction tasks. Thermal/Infrared Sensors: In safety-critical applications, these sensors provide distinct signatures for living beings (e.g., pedestrians and animals) that are invisible to standard cameras at night. Pretraining on thermal data typically follows domain adaptation paradigms to transfer RGB-based semantic knowledge to the thermal domain. 16 Table 6 Comparative analysis of 3D object detection on the nuScenes benchmark [17]. The table reports the mean Average Precision (mAP) and NuScenes Detection Score (NDS) of various pre-training frameworks. The values in parentheses denote the performance gains relative to the corresponding baseline methods. Method Venue Backbone Image Size Baseline FCOS3D [225] GeoMIM [140] OccNet [207] UniScene [165] DriveWorld [166] ViDAR [270] UniPAD [264] MIM4D [322] GaussianPretrain [249] VisionPAD [288] ICCVW21 ICCV23 ICCV23 RA-L24 CVPR24 CVPR24 CVPR24 IJCV25 arXiv24 CVPR25 SQS [289] NeurIPS25 UniPAD [264] UniM2AE [323] CVPR24 ECCV24 ResNet101 [60] Swin-B [148] ResNet101 [60] ResNet101 [60] ResNet101 [60] ResNet101 [60] ConNeXt-S [150] ResNet50 [60] ResNet50 [60] ResNet101 [60] ResNet101 [60] VoxelNet [312] SST [42] 1600 900 BEVFormer [122] 1408 512 BEVDepth [120] 1600 900 BEVFormer [122] 1600 900 BEVFormer [122] 1600 900 BEVFormer [122] 1600 900 BEVFormer [122] 1600 900 UVTR-CS [115] Sparse4Dv3 [134] 704 256 1600 900 StreamPETR [218] 1600 900 UVTR-CS [115] SparseBEV [136] 1408 - - UVTR-L [115] TransFusion [6] NS-MAE [257] UniPAD [264] UniM2AE [323] VoxelNet [312]+Swin-T [148] arXiv24 CVPR24 VoxelNet [312]+ConNeXt-S [150] 1600 900 SST [42]+Swin-T [148] ECCV24 704 256 BEVFusion [149] UVTR-M [115] 1600 900 FocalFormer3D [28] Auxiliary Data Performance Temporal Pre-Training Others mAP NDS ImageNet [34] ImageNet [34] ImageNet [34] FCOS3D [225] FCOS3D [225] FCOS3D [225] FCOS3D [225] ImageNet [34] ImageNet [34] FCOS3D [225] FCOS3D [225] - - - FCOS3D [225] MMIM [323] - - - - - - - - - - - - - - - - 41.6(+3.9) 51.7 (+4.0) 52.3(+5.7) 60.5 (+5.0) 43.6(+2.0) 53.2 (+1.5) 43.8(+2.2) 53.4 (+1.7) 44.2(+2.6) 53.6 (+1.9) 45.8(+4.3) 54.8 (+4.3) 42.8(+3.6) 50.2 (+1.4) 46.4(+0.1) 57.0 (+0.6) 38.6(+0.6) 48.8 (+0.9) 43.1(+3.9) 50.4 (+1.6) 50.9(+0.8) 60.2 (+1.0) 65.0(+4.1) 70.6 (+2.9) 65.7(+0.7) 70.4 (+0.5) 63.0(+2.2) 65.5 (+1.4) 69.9(+4.5) 73.2 (+3.0) 71.1(+0.6) 73.8 (+0.7)"
        },
        {
            "title": "4.4 Empirical Analysis and Benchmark Performance",
            "content": "To empirically substantiate the efficacy of the discussed pre-training paradigms, we evaluate their impact on core 3D perception tasks: 3D Object Detection and LiDAR Semantic Segmentation. These tasks serve as the definitive litmus test for Spatial Intelligence, assessing whether learned representations can translate pretext objectives (e.g., reconstruction and forecasting) into precise geometric localization and fine-grained semantic understanding. In this subsection, we synthesize key findings from major benchmarks, highlighting how different pre-training strategies reshape the performance landscape. 4.4.1 3D Object Detection 3D object detection requires the model to identify and localize objects within metric space, task that demands both high-level semantics and low-level geometric precision. Quantitative results on the nuScenes benchmark  (Table 6)  provide compelling evidence for the superiority of Unified Pre-Training. As shown in the comparative analysis, frameworks that jointly optimize multi-modal encoders consistently outperform camera-only baselines. Notably, UniM2AE [323] achieves state-of-the-art performance with 71.1 mAP and 73.8 NDS, representing significant gain over the strong FocalFormer3D [28] baseline. Similarly, UniPAD [264] demonstrates remarkable robustness, boosting the UVTR-M [115] baseline by +4.5 mAP to reach 69.9 mAP. This suggests that learning shared latent space for vision and geometry allows the model to capture complementary features that are otherwise lost in late-fusion pipelines, proving that unified multi-modal masking is superior to disjoint training strategies. 4.4.2 LiDAR Segmentation LiDAR semantic segmentation, involving dense point-level classification, is the rigorous testing ground for the Semantic-Geometric Gap. Since point clouds inherently lack texture, performance on this task directly reflects models ability to hallucinate semantics from geometry. The comparisons in Table 7 reveal decisive trend: Camera-to-LiDAR Distillation is indispensable, particularly for Data Efficiency. Approaches utilizing visual priors consistently surpass training-from-scratch baselines, with advantages magnified in data-scarce regimes. For instance, with only 1% of labeled data, the random baseline yields poor mIoU of 30.30. In stark contrast, distillation-based methods like OLIVINE [297] and LiMoE [254] achieve 50.58 and 49.60 mIoU respectively, effectively doubling the performance of the baseline. This indicates that self-supervised pre-training effectively unlocks the latent geometric structure of unlabeled data, significantly reducing the dependency on costly manual annotations. Crucially, the results uncover Scaling Law Transfer phenomenon. Advanced distillation methods like LiMoE [254] not only achieve state-of-the-art results on the full dataset (77.27 mIoU) but also demonstrate 17 Table 7 Benchmark of cross-modal pre-training for LiDAR semantic segmentation on nuScenes [17]. We evaluate the transferability of visual semantics to 3D point clouds via knowledge distillation. The results highlight performance gains across varying data regimes (e.g., 1% vs. 100% labeled data), underscoring the data efficiency of LiDAR-centric pre-training. Method Venue Backbone (2D) Backbone (3D) Random - None MinkUNet-34 PointContrast [243] DepthContrast [299] ALSO [14] BEVContrast [190] ECCV20 ICCV21 CVPR23 3DV PPKT [147] SLidR [189] ST-SLidR [157] TriCC [175] arXiv21 CVPR22 CVPR23 CVPR23 Seal [145] NeurIPS23 CVPR24 CSC [23] OLIVINE [297] NeurIPS24 IJCV24 HVDistill [294] LargeAD [101] arXiv25 None MinkUNet-34 [32] ResNet-50 [60] MinkUNet-34 [32] PPKT [147] SLidR [189] arXiv21 CVPR22 Seal [145] NeurIPS23 CVPR24 ECCV24 arXiv25 CVPR25 ScaLR [180] SuperFlow [253] LargeAD [101] LiMoE [254] ViT-S [171] MinkUNet-34 [32] LP 8.10 1.90 2.10 - - 35.90 38.80 40.48 38.00 44.95 46.00 50.09 39.50 46.13 38.60 44.70 45.16 42.40 46.44 46.58 48.20 nuScenes KITTI Waymo 1% 5% 10% 25% Full 1% 1% 30.30 47.84 56.15 65.48 74. 32.50 31.70 37.70 38.30 37.80 38.30 40.75 41.20 45.84 47.00 50.58 42.70 47.08 40.60 41.16 44.27 40.50 47.81 46.78 49.60 - - - - 53.74 52.49 54.69 54.10 55.64 57.00 60.19 56.60 56.90 52.06 53.65 55.13 - 59.44 57.33 60. - - 59.40 59.60 60.25 59.84 60.75 60.40 62.97 63.30 65.01 62.90 63.74 59.99 61.47 62.46 - 64.47 63.85 65.65 - - - - 67.14 66.91 67.70 67.60 68.41 68.60 70.13 69.30 69.34 65.76 66.71 67.64 - 69.20 68.66 71. - - 72.00 72.30 74.52 74.79 75.14 75.60 75.60 75.70 76.54 76.60 76.03 73.97 74.20 75.58 - 76.54 75.75 77.27 39.50 41.10 41.50 - - 44.00 44.60 44.72 45.90 46.63 47.20 49.38 49.70 49. 43.25 44.67 46.51 - 47.97 50.07 49.53 39.41 - - - - 47.60 47.12 44.93 - 49.34 - - - 50.29 47.44 47.57 48.67 - 49.94 50.83 51.42 that 3D backbones can inherit the rich, open-world semantics of large-scale 2D Foundation Models (utilizing ViT-S teachers). It validates the hypothesis that forging Spatial Intelligence does not require reinventing semantic understanding, but rather effectively transferring it from the vision domain to the 3D physical world."
        },
        {
            "title": "5 Open-World Perception and Planning",
            "content": "The ultimate goal of Spatial Intelligence is not merely to perceive closed-set categories but to generalize to the open world and make robust decisions in unseen scenarios. Traditional perception systems, constrained by fixed ontologies and supervised data, struggle with the long-tail unpredictability of real-world environments. In this section, we explore how multi-modal pre-training is evolving to address these challenges. We first analyze the demands of Open-World Perception (Section 5.1). We then discuss how Text-Grounded Understanding leverages Vision-Language Models (VLMs) to bridge the semantic gap and automate supervision (Section 5.2). Finally, we examine the culmination of these efforts in Unified World Representations, where generative world models and Vision-Language-Action (VLA) architectures are redefining end-to-end planning (Section 5.3)."
        },
        {
            "title": "5.1 Open-World Challenges",
            "content": "Open-world deployment introduces complexity vectors that exceed the capacity of traditional representation learning: Open-Vocabulary Recognition: Systems must identify novel objects (e.g., \"overturned truck\", \"debris\") that were never explicitly annotated during training, requiring shift from ID-based classification to language-driven reasoning. Domain Shifts and Anomalies: Robustness against changing weather, lighting, and sensor degradation is critical. Models must quantify epistemic uncertainty to handle \"unknown unknowns\" safely. Data Scalability: The combinatorial explosion of corner cases makes manual annotation infeasible. Learning from vast, unlabeled, diverse data streams is the only viable path to coverage. 18 Table 8 Performance comparison for self-supervised 3D occupancy prediction on Occ3D-nuScenes [205]. This table assesses the capability of methods to learn dense volumetric representations without manual 3D labels. FM denotes the specific 2D Foundation Model utilized for pseudo-label generation or feature distillation. Method Venue Representation Foundation Model Used Supervision Other Supported Task Performance IoU mIoU SimpleOcc [46] OccNeRF [285] SelfOcc [75] TIV24 TIP25 NeRF NeRF CVPR24 BEV/TPV Feature DistillNeRF [215] NeurIPS24 ICCV25 GaussianOcc [45] CVPR25 GaussTR [75] 3DV25 LangOcc [10] ECCV24 VEON-L [303] arXiv25 TT-OccLiDAR [286] arXiv25 GaussianFlowOcc [11] arXiv25 ShelfOcc [12] arXiv25 ShelfGaussian [300] arXiv25 QueryOcc [132] NeRF Gaussians Gaussians NeRF Occ Gaussians Gaussians Voxel Gaussians Query - Grounding DINO [144] OpenSeeD [290] CLIP [182] & DINOv2 [171] Grounding DINO [144] Metric3D [276] & CLIP [182] & SAM [96] MaskCLIP [307] MiDAS [183] & SAN [247] & CLIP [182] VGGT [213] & OpenSeeD [290] GroundedSAM [187] & Metric3D [276] MapAnything [91] & GroundedSAM [187] DINOv2 [171] & Metric3D [276] Depth Estimation Depth Estimation - 22.81 Novel Depth Synthesi/Depth Estimation 45.01 Novel View Synthesi/Depth Estimation 29.11 - Depth Estimation Video Sequence Video Sequence & FM Video Sequence & FM FM Video Sequence & FM FM Video Sequence & FM FM & LiDAR Video Sequence & FM & LiDAR Video Sequence & FM Video Sequence & FM 7.99 9.53 9.30 8.93 9.94 Open-Vocabulary Occupancy Prediction 45.19 11.70 51.76 11.84 15.14 23.60 46.91 17.08 56.14 22.87 Video Sequence & FM & LiDAR BEV Segmentation / Trajectory Planning 63.25 19.07 55.00 21.30 3D Open Vocabulary Retrieval 3D Open Vocabulary Retrieval Progressive Occupancy Estimation Depth Estimation - Depth Estimation - - Metric3D [276] & GroundedSAM [187] & DinoV3 [196] Video Sequence & FM & LiDAR Addressing these challenges necessitates paradigm shift: from learning specific tasks to learning generalizable world knowledge."
        },
        {
            "title": "5.2 Text-Grounded Understanding",
            "content": "Language serves as the universal interface for open-world knowledge. By aligning 3D sensor data with rich textual semantics, foundation models can read the scene, unlocking zero-shot capabilities. This paradigm manifests in two key directions: Auto-Labeling Data Engines and Open-Vocabulary Representation Learning. Auto-Labeling as Scalable Data Engine. The most immediate impact of foundation models is breaking the annotation bottleneck. Instead of relying on human labelers, recent works utilize pre-trained VLMs [182, 217] to generate high-quality pseudo-labels for sensor data. CLIP2Scene [26] and OpenScene [177] pioneered the distillation of 2D vision-language features into 3D point clouds, effectively automating semantic segmentation. Advanced frameworks like Affinity3D [137] and VLM2Scene [130] further refine this process by enforcing multi-view consistency, ensuring that the hallucinated labels are geometrically coherent for downstream supervised training. Text-Assisted Representation Learning. Beyond generating discrete labels, recent research focuses on SelfSupervised 3D Occupancy Prediction, treating text-aligned 2D features as continuous supervision signals. Methods like LangOcc [10] and LOcc [279] leverage knowledge distillation from diverse teacher models [5, 171, 184] to directly guide the learning of dense volumetric semantics. As shown in Table 8, these self-supervised approaches now rival supervised baselines, proving that foundation model-driven supervision can replace manual effort. Furthermore, the trend towards 3D Gaussian Splatting [11, 83, 300] illustrates the push for representations that are not only semantically rich but also geometrically continuous and renderable, facilitating better alignment with 2D VLMs."
        },
        {
            "title": "5.3 Unified World Representation for Action",
            "content": "Perception serves as the foundation for decision-making, while the ultimate manifestation of Spatial Intelligence is Action. The field is transitioning from modular perception-planning pipelines to unified World Models that can simulate future states and plan end-to-end within shared space. From Discriminative to Generative Planning. Traditional end-to-end planning often relied on explicit perception outputs (e.g., bounding boxes and vectorized maps) or decoupled feature maps. Recent breakthroughs, however, are driven by Generative World Models [106, 269, 306]. Moving beyond discrete label prediction, models like OccWorld [305] and GenAD [265] learn to predict the future evolution of the 3D world (e.g., 4D Occupancy flow) conditioned on ego-actions. This predictive learning objective forces the model to internalize scene dynamics, causal relationships, and object interactions. As evidenced in Table 9, these generative planners significantly outperform discriminative baselines in both collision rates and open-loop planning metrics. Unified End-to-End Architectures: VA and VLA. The convergence of generative modeling and autonomous driving has bifurcated into two powerful paradigms for action generation: Vision-Action (VA) latent models and Vision-Language-Action (VLA) reasoning frameworks. The first paradigm focuses on pure decision-making 19 Table 9 Evaluation of end-to-end autonomous driving planning on the nuScenes benchmark [17]. The table compares the planning fidelity of state-of-the-art methods, contrasting traditional pipelines with emerging Generative World Models. Performance is measured by planning L2 error (L2) and Collision Rate (CR), where lower values indicate better safety and precision. Method Venue Input Representation Supported Task Auxiliary Supervision Performance L2 Avg. (m) CR Avg. FPS ST-P3 [70] ECCV22 Image BEV Feature UniAD [72] CVPR23 Image BEV Feature BEV Seg. Map & Box & Depth Track./Map/Motion Fore./Occ. Map & Box & Motion & Tracklets & Occ VAD-Tiny [82] VAD-Base [82] OccNet [207] ICCV23 Image Vectorized BEV Scene ICCV23 Image Vectorized BEV Scene ICCV23 Image 3D Occupancy Vectorized Map Vectorized Map Semantic Occ. Pred. OccWorld [305] ECCV24 Image 3D Occupancy OccWorld [305] ECCV24 Image 3D Occupancy 3D Occupancy OccWorld[305] ECCV24 Occ ICRA25 Image 3D Occupancy ICRA25 Occ 3D Occupancy arXiv24 Image 3D Occupancy arXiv24 Occ 3D Occupancy arXiv24 Image 3D Occupancy arXiv24 Occ 3D Occupancy ICLR25 Image Latent Feature ICLR25 Image BEV Feature arXiv24 Occ RenderWorld [261] RenderWorld [261] OccLLaMA [232] OccLLaMA [232] OccVAR [86] OccVAR [86] LAW [117] SSR [112] FSF-Net [53] 3D Occupancy Drive-OccWorld [269] AAAI25 Image 3D Occupancy 3D Occupancy arXiv25 Occ arXiv25 Occ 3D Occupancy ICCV25 Image Latent Feature OccTens [87] OccVLA [143] World4Drive [306] 4D Occ. Fore. 4D Occ. Fore. 4D Occ. Fore. 4D Occ. Fore. 4D Occ. Fore. 4D Occ. Fore./VQA 4D Occ. Fore./VQA 4D Occ. Fore. 4D Occ. Fore. Latent Prediction Fore. BEV Feature Fore. 4D Occ. Fore. 4D Occ. Fore./Generation 4D Occ. Fore./Generation 3D Occ. Generation Latent Prediction Fore. Map & Box & Motion Map & Box & Motion 3D-Occ & Map & Box 3D-Occ None None None None 3D-Occ None 3D-Occ None None None None 3D-Occ 3D-Occ 3D-Occ Open-vocabulary Semantics 2.11 1.03 1.30 1.22 2.14 1.34 1.83 1.17 1.48 1.03 1.20 1.14 1.35 1.21 0.61 0.39 0.82 0.85 1.12 0.28 0.50 0.71 0.31 0.72 0.53 0.72 0.73 2.02 0.60 0.97 0.61 0.70 0.49 0.83 0.78 0.30 0.06 0.01 0.29 0.48 - 0. 1.6 1.8 16.8 4.5 2.6 2.8 2.8 18.0 - - - - - - 19.5 19.5 - - - - - efficiency by constructing Latent World Models. Unlike traditional pipelines that rely on explicit perception supervision, methods like LAW [117] and SSR [112] bypass human annotations entirely. By abstracting the environment into high-dimensional latent states, these models learn to predict future rewards and control signals directly from sensor inputs without the need for perception labels. Parallel to pure latent modeling, the integration of Large Language Models (LLMs) has catalyzed the emergence of VLA frameworks that emphasize interpretability and open-world reasoning [5, 139, 224]. Approaches like OccVLA [143] and DriveVLA-W0 [118] tokenize visual input and project them into the LLMs context window alongside text. This enables the system to not only generate control actions but also to perform causal reasoning (Why is the car stopping? ) and handle complex social interactions (Yield to the aggressive merger ) in unified autoregressive process. In summary, the trajectory is clear: from detecting objects to simulating latent futures (VA), and finally to reasoning with language (VLA). This evolution underscores the pivotal role of multi-modal pre-training in constructing the next generation of embodied intelligent systems."
        },
        {
            "title": "6 Challenges and Future Directions",
            "content": "As demonstrated in this work, the pursuit of Spatial Intelligence has evolved from task-specific supervision to paradigm dominated by large-scale, multi-modal pre-training. While the techniques analyzed in Section 4 and Section 5 demonstrate immense progress, the rapid emergence of generative AI and foundation models introduces new frontiers. In this section, we synthesize critical remaining obstacles and outline forwardlooking research agenda centered on generative world modeling and embodied reasoning."
        },
        {
            "title": "6.1 Current Challenges\nThe Semantic-Geometric Gap. A fundamental dissonance remains between the rich semantic knowledge\nencapsulated in Vision-Language Models (VLMs) and the precise metric requirements of autonomous control.\nWhile VLMs excel at open-vocabulary recognition [5, 139, 182, 217], they often lack the fine-grained spatial\ngrounding necessary to localize it with centimeter-level accuracy. Bridging the gap between high-level semantic\nreasoning and low-level geometric constraints without compromising either remains a formidable theoretical\nand engineering challenge [143, 232].",
            "content": "Data-Centric Bottlenecks and Corner Cases. The scaling laws of foundation models are increasingly hitting diminishing returns regarding data quality. The primary challenge has shifted from acquiring more data to mining valuable dataspecifically, long-tail corner cases and safety-critical scenarios [49, 204, 281]. Current 20 pre-training objectives treat all data samples equally, often wasting computation on repetitive driving patterns while under-weighting rare, high-value events [264, 270]. Furthermore, utilizing foundation models for auto-labeling introduces epistemic uncertainty that is difficult to filter from the training pipeline. Real-Time Inference of Foundation Models. There is growing disparity between the computational demands of state-of-the-art pre-trained models and the strict latency/power constraints of onboard edge devices [85, 234]. While cloud-based pre-training leverages unlimited resources, distilling these massive teacher models into lightweight, real-time student networks without catastrophic performance drops is an ongoing bottleneck for deployment [253, 256, 294]."
        },
        {
            "title": "6.2 Future Directions",
            "content": "While recent advancements have laid the foundation for spatial intelligence, several critical frontiers remain to be conquered to achieve robust, human-level autonomy. Physically Consistent World Simulators. Although emerging Generative World Models [106, 118, 306] can synthesize plausible futures, they often suffer from hallucinations that violate physical laws [126]. key future direction is to enforce Physical Consistency within the pre-training objective. By integrating differentiable physics engines or explicit geometric constraints into the generation process [8, 118, 143, 227], future models must evolve from merely generating visual pixels to simulating realistic physical interactions, thereby serving as reliable training environments for safety-critical policies. Trustworthy and Real-Time Embodied VLA. Current Vision-Language-Action (VLA) models [71, 84, 118, 143] demonstrate promise but face significant hurdles in real-world deployment: high inference latency and lack of interpretability. Future research should bridge the gap between heavy foundation models and the millisecondlevel reaction requirements of autonomous systems. This necessitates exploring lightweight VLA architectures, efficient tokenization strategies, and mechanisms for uncertainty quantification to ensure that end-to-end decision-making is not only intelligent but also trustworthy and verifiable [43, 113, 223, 278]. 4D Semantic-Geometric Unification. The transition from discrete voxels to continuous representations like 3D Gaussian Splatting (3DGS) [83, 92, 141] is underway. However, current 3DGS methods largely focus on visual rendering quality rather than semantic understanding. The next frontier lies in Semantic Liftingimbuing these continuous geometric primitives with dense semantic and instance-level attributes over time. Pre-training tasks that enforce spatiotemporal consistency on Gaussian attributes [11, 249] will be pivotal for enabling agents to not just view the scene, but to manipulate and interact with specific objects in dynamic 4D world. System 2 Reasoning for Long-Tail Safety. Existing pre-training paradigms excel at pattern recognition (System 1 ) but struggle with rare, complex scenarios requiring logical deduction. Future systems will integrate System 2 capabilities [71, 206], potentially via Chain-of-Thought (CoT) distillation from LLMs [219, 224, 231, 284]. The goal is to move beyond passive explanation to active Causal Reasoningenabling the vehicle to counterfactually simulate what if scenarios and override reactive policies when facing novel, long-tail safety hazards."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we have presented systematic analysis of multi-modal pre-training for autonomous systems, characterizing the evolution from modality-specific pre-training to unified foundation models as the cornerstone of Spatial Intelligence. By structuring datasets and methodologies across autonomous vehicles, drones, and other robotic systems, we demonstrated how integrating complementary sensor modalities (specifically camera and LiDAR) creates representations that are both semantically rich and geometrically precise. Our analysis confirms that leveraging pre-trained foundation models is no longer optional but essential for achieving open-world generalization and mitigating the scarcity of annotated 3D data. Looking ahead, the field stands at critical inflection point. As demonstrated, the paradigm is shifting from passive perception to active, embodied reasoning. Future breakthroughs will likely stem from bridging the semantic-geometric gap through Generative World Models that serve as neural simulators, and from the development of end-to-end Vision-Language-Action (VLA) frameworks that unify perception with decisionmaking. Furthermore, equipping these systems with explicit reasoning capabilities will be pivotal for handling 21 the long-tail unpredictability of real-world environments. Ultimately, the transition from seeing to acting and reasoning represents the next frontier. Continued advancements in these generative and embodied pre-training paradigms will be instrumental in forging autonomous systems that are not only robust and scalable but possess true Spatial Intelligence for safe and real-world deployment."
        },
        {
            "title": "References",
            "content": "[1] Mohamed Abdelsamad et al. Multi-scale neighborhood occupancy masked autoencoder for self-supervised learning in LiDAR point clouds. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2223422243, 2025. [2] Inc. Aeva Technologies. AevaScenes: Open-access 4D FMCW LiDAR and camera dataset, 2025. URL https: //scenes.aeva.com. [3] Ben Agro, Quinlan Sykora, Sergio Casas, Thomas Gilles, and Raquel Urtasun. UnO: Unsupervised occupancy fields for perception and forecasting. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 14487 14496, 2024. [4] Raghad Alqobali et al. survey on robot semantic navigation systems for indoor environments. Applied Sciences, 14(1):89, 2023. [5] Shuai Bai et al. Qwen2.5-VL technical report. arXiv preprint arXiv:2502.13923, 2025. [6] Xuyang Bai et al. TransFusion: Robust LiDAR-camera fusion for 3D object detection with transformers. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 10901099, 2022. [7] Bifta Sama Bari et al. Datasets in vehicular communication systems: review of current trends and future prospects. SN Computer Science, 6(3):125, 2025. [8] Bartoccioni et al. VaViM and VaVAM: Autonomous driving through video generative modeling. arXiv preprint arXiv:2502.15672, 2025. [9] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. SemanticKITTI: dataset for semantic scene understanding of lidar sequences. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 92979307, 2019. [10] Simon Boeder, Fabian Gigengack, and Benjamin Risse. LangOcc: Self-supervised open vocabulary occupancy estimation via volume rendering. arXiv preprint arXiv:2407.17310, 2024. [11] Simon Boeder et al. GaussianFlowOcc: Sparse and weakly supervised occupancy estimation using gaussian splatting and temporal flow. arXiv preprint arXiv:2502.17288, 2025. [12] Simon Boeder et al. ShelfOcc: Native 3D supervision beyond LiDAR for vision-based occupancy estimation. arXiv preprint arXiv:2511.15396, 2025. [13] Daniel Bolya et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. [14] Alexandre Boulch, Corentin Sautier, Bjrn Michele, Gilles Puy, and Renaud Marlet. ALSO: Automotive LiDAR self-supervision by occupancy estimation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1345513465, 2023. [15] Borja Bovcon, Jon Muhovi, Duko Vranac, Dean Mozeti, Janez Per, and Matej Kristan. MODSa USVoriented object detection and obstacle segmentation benchmark. IEEE Trans. Intell. Transport. Sys., 23(8): 1340313418, 2021. [16] Ilker Bozcan and Erdal Kayacan. AU-Air: multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance. In Proc. IEEE Int. Conf. Robot. Automat., pages 85048510, 2020. [17] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: multimodal dataset for autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1162111631, 2020. [18] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit Fong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom, and Sammy Omari. nuPlan: closed-loop ML-based planning benchmark for autonomous vehicles. arXiv preprint arXiv:2106.11810, 2021. 22 [19] Mu Cai, Chenxu Luo, Yong Jae Lee, and Xiaodong Yang. Cross-modal self-supervised learning with effective contrastive units for LiDAR point clouds. arXiv preprint arXiv:2409.06827, 2024. [20] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 96509660, 2021. [21] Kenneth Chaney, Fernando Cladera, Ziyun Wang, Anthony Bisulco, Ani Hsieh, Christopher Korpela, Vijay Kumar, Camillo Taylor, and Kostas Daniilidis. M3ED: Multi-robot, multi-sensor, multi-environment event dataset. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog. Worksh., pages 40164023, 2023. [22] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argoverse: 3D tracking and forecasting with rich maps. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019. [23] Haoming Chen, Zhizhong Zhang, Yanyun Qu, Ruixin Zhang, Xin Tan, and Yuan Xie. Building strong pre-training baseline for universal 3D large-scale perception. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1992519935, 2024. [24] Runjian Chen, Hyoungseob Park, Bo Zhang, Wenqi Shao, Ping Luo, and Alex Wong. TREND: Unsupervised 3D representation learning via temporal forecasting for LiDAR perception. arXiv preprint arXiv:2412.03054, 2024. [25] Runjian Chen, Hang Zhang, Avinash Ravichandran, Wenqi Shao, Alex Wong, and Ping Luo. CLAP: Unsupervised 3D representation learning for fusion 3D perception via curvature sampling and prototype learning. arXiv preprint arXiv:2412.03059, 2024. [26] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. CLIP2Scene: Towards label-efficient 3D scene understanding by CLIP. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 70207030, 2023. [27] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wenping Wang. Towards label-free scene understanding by vision foundation models. In Adv. Neural Inf. Process. Syst., volume 36, 2024. [28] Yilun Chen et al. FocalFormer3D: Focusing on hard instance for 3D object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 83948405, 2023. [29] Yujin Chen, Matthias Niener, and Angela Dai. 4DContrast: Contrastive learning with dynamic correspondences for 3D scene understanding. In Proc. Eur. Conf. Comput. Vis., pages 543560. Springer, 2022. [30] Nuo Cheng, Chuanyu Luo, Xinzhe Li, Ruizhi Hu, Han Li, Sikun Ma, Zhong Ren, Haipeng Jiang, Xiaohan Li, Shengguang Lei, et al. Rethinking masked-autoencoder-based 3D point cloud pretraining. In IEEE Intell. Veh. Symposium, pages 27632768, 2024. [31] Yuwei Cheng, Jiannan Zhu, Mengxin Jiang, Jie Fu, Changsong Pang, Peidong Wang, Kris Sankaran, Olawale Onabola, Yimin Liu, Dianbo Liu, et al. Flow: dataset and benchmark for floating waste detection in inland waters. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1095310962, 2021. [32] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D spatio-temporal ConvNets: Minkowski convolutional neural networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 30753084, 2019. [33] Colin Decourt, Rufin VanRullen, Didier Salle, and Thomas Oberlin. Leveraging self-supervised instance contrastive learning for radar object detection. arXiv preprint arXiv:2402.08427, 2024. [34] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: large-scale hierarchical image database. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 248255, 2009. [35] Jacob Devlin et al. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. Conf. of the North American Chapter of the Association for Computational Linguistics, pages 41714186, 2019. [36] Jian Ding, Nan Xue, Gui-Song Xia, Xiang Bai, Wen Yang, Michael Ying Yang, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, et al. Object detection in aerial images: large-scale benchmark and challenges. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):77787796, 2021. [37] Xu Dong, Binnan Zhuang, Yunxiang Mao, and Langechuan Liu. Radar camera fusion via representation learning in autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 16721681, 2021. 23 [38] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2021. [39] Danny Driess et al. PaLM-E: An embodied multimodal language model. 2023. [40] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In Proc. Eur. Conf. Comput. Vis., pages 370386. Springer, 2018. [41] Aritra Dutta, Srijan Das, Jacob Nielsen, Rajatsubhra Chakraborty, and Mubarak Shah. Multiview aerial visual recognition (MAVREC): Can multi-view improve aerial visual perception? In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2267822690, 2024. [42] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang. Embracing single stride 3D object detector with sparse transformer. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 84588468, 2022. [43] Sicheng Feng, Song Wang, Shuyi Ouyang, Lingdong Kong, Zikai Song, Jianke Zhu, Huan Wang, and Xinchao Wang. Can mllms guide me home? benchmark study on fine-grained visual reasoning from transit maps. arXiv preprint arXiv:2505.18675, 2025. [44] Felix Fent, Fabian Kuttenreich, Florian Ruch, Farija Rizwin, Stefan Juergens, Lorenz Lechermann, Christian Nissler, Andrea Perl, Ulrich Voll, Min Yan, et al. MAN TruckScenes: multimodal dataset for autonomous trucking in diverse conditions. arXiv preprint arXiv:2407.07462, 2024. [45] Wanshui Gan, Fang Liu, Hongbin Xu, Ningkai Mo, and Naoto Yokoya. GaussianOcc: Fully self-supervised and efficient 3D occupancy estimation with gaussian splatting. arXiv preprint arXiv:2408.11447, 2024. [46] Wanshui Gan et al. comprehensive framework for 3D occupancy estimation in autonomous driving. IEEE Trans. Intell. Veh., 2024. [47] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 33543361, 2012. [48] Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian Mhlegg, Sebastian Dorn, et al. A2D2: Audi autonomous driving dataset. arXiv preprint arXiv:2004.06320, 2020. [49] Anurag Ghosh et al. ROADWork dataset: Learning to recognize, observe, analyze and drive through work zones. arXiv preprint arXiv:2406.07661, 2024. [50] Nikhil Gosala, Krsat Petek, Ravi Kiran, Senthil Yogamani, Paulo Drews-Jr, Wolfram Burgard, and Abhinav Valada. LetsMap: Unsupervised representation learning for label-efficient semantic BEV mapping. In Proc. Eur. Conf. Comput. Vis., pages 110126. Springer, 2025. [51] Luis Gressenbuch, Klemens Esterle, Tobias Kessler, and Matthias Althoff. Mona: The munich motion dataset of natural driving. In Proc. IEEE Int. Conf. Intell. Transport. Syst., pages 20932100, 2022. [52] Runwei Guan, Liye Jia, Fengyufan Yang, Shanliang Yao, Erick Purwanto, Xiaohui Zhu, Eng Gee Lim, Jeremy Smith, Ka Lok Man, Xuming Hu, et al. WaterVG: Waterway visual grounding based on text-guided vision and mmWave radar. arXiv preprint arXiv:2403.12686, 2024. [53] Erxin Guo, Pei An, You Yang, Qiong Liu, and An-An Liu. FSF-Net: Enhance 4D occupancy forecasting with coarse BEV scene flow for autonomous driving. arXiv preprint arXiv:2409.15841, 2024. [54] Prajwal Gurunath, Sumanth Udupa, Aditya Gandhamal, Shrikar Madhu, Aniruddh Sikdar, Suresh Sundaram, et al. IndraEye: Infrared electro-optical UAV-based perception dataset for robust downstream tasks. arXiv preprint arXiv:2410.20953, 2024. [55] Sehoon Ha, Joonho Lee, Michiel van de Panne, Zhaoming Xie, Wenhao Yu, and Majid Khadiv. Learning-based legged locomotion; state of the art and future perspectives. arXiv preprint arXiv:2406.01152, 2024. [56] Haiqian Han et al. Learning to remove lens flare in event camera. arXiv preprint arXiv:2512.09016, 2025. [57] Lei Han, Qingxu Zhu, Jiapeng Sheng, Chong Zhang, Tingguang Li, Yizheng Zhang, He Zhang, Yuzhen Liu, Cheng Zhou, Rui Zhao, et al. Lifelike agility and play in quadrupedal robots using reinforcement learning and generative pre-trained models. Nature Machine Intelligence, pages 112, 2024. 24 [58] Yiduo Hao, Sohrab Madani, Junfeng Guan, Mohammed Alloulah, Saurabh Gupta, and Haitham Hassanieh. Bootstrapping autonomous driving radars with self-supervised learning. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1501215023, 2024. [59] Jeanine Harb, Nicolas Rbna, Raphal Chosidow, Grgoire Roblin, Roman Potarusov, and Hatem Hajri. FRSign: large-scale traffic light dataset for autonomous trains. arXiv preprint arXiv:2002.05665, 2020. [60] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 770778, 2016. [61] Kaiming He, Ross Girshick, and Piotr Dollr. Rethinking ImageNet pre-training. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 49184927, 2019. [62] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 97299738, 2020. [63] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollr, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1600016009, 2022. [64] Deepti Hegde, Suhas Lohit, Kuan-Chuan Peng, Michael Jones, and Vishal Patel. Equivariant spatio-temporal self-supervision for LiDAR object detection. In Proc. Eur. Conf. Comput. Vis., pages 475491. Springer, 2024. [65] Julia Hindel, Nikhil Gosala, Kevin Bregler, and Abhinav Valada. INOD: Injected noise discriminator for self-supervised representation learning in agricultural fields. IEEE Robotics Autom. Letters, 2023. [66] Julia Hindel et al. Label-efficient LiDAR scene understanding with 2D-3D vision transformer adapters. In IEEE Int. Conf. Robotics Autom. Worksh., 2025. [67] Ji Hou, Benjamin Graham, Matthias Niener, and Saining Xie. Exploring data-efficient 3D scene understanding with contrastive scene contexts. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1558715597, 2021. [68] John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one hours: Self-driving motion prediction dataset. In Conf. Robot Learn. PMLR, 2020. [69] Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki Trigoni, and Andrew Markham. Towards semantic segmentation of urban-scale 3D point clouds: dataset, benchmarks and challenges. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 49774987, 2021. [70] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. ST-P3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In Proc. Eur. Conf. Comput. Vis., pages 533549. Springer, 2022. [71] Tianshuai Hu, Xiaolu Liu, Song Wang, et al. Vision-language-action models for autonomous driving: Past, present, and future. arXiv preprint arXiv:2512.16760, 2025. [72] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1785317862, 2023. [73] Linyan Huang et al. LiDAR-guided geometric pretraining for vision-centric 3D object detection. Int. J. Comput. Vis., 133(7):38773890, 2025. [74] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The ApolloScape open dataset for autonomous driving and its application. IEEE Trans. Pattern Anal. Mach. Intell., 42(10): 27022719, 2019. [75] Yuanhui Huang, Wenzhao Zheng, Borui Zhang, Jie Zhou, and Jiwen Lu. SelfOcc: Self-supervised vision-based 3D occupancy prediction. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1994619956, 2024. [76] Zhenpeng Huang, Chao Li, Hao Chen, Yongjian Deng, Yifeng Geng, and Limin Wang. Data-efficient event camera pre-training via disentangled masked modeling. arXiv preprint arXiv:2403.00416, 2024. [77] Muhammad Zubair Irshad, Sergey Zakharov, Vitor Guizilini, Adrien Gaidon, Zsolt Kira, and Rares Ambrus. NeRF-MAE: Masked autoencoders for self-supervised 3D representation learning for neural radiance fields. In Proc. Eur. Conf. Comput. Vis., pages 434453. Springer, 2024. 25 [78] Andrej Janda, Brandon Wagstaff, Edwin Ng, and Jonathan Kelly. Self-supervised pre-training of 3D point cloud networks with image data. arXiv preprint arXiv:2211.11801, 2022. [79] Mingi Jeong and Alberto Quattrini Li. Efficient LiDAR-based in-water obstacle detection and segmentation by autonomous surface vehicles in aquatic environments. In Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., pages 53875394, 2021. [80] Mingi Jeong, Arihant Chadda, Ziang Ren, Luyang Zhao, Haowen Liu, Monika Roznere, Aiwei Zhang, Yitao Jiang, Sabriel Achong, Samuel Lensgraf, et al. Multi-modal perception dataset of in-water objects for autonomous surface vehicles. arXiv preprint arXiv:2404.18411, 2024. [81] Aleksandar Jevti et al. Comparison of interaction modalities for mobile indoor robot guidance: Direct physical interaction, person following, and pointing control. IEEE Trans. Human-Machine Sys., 45(6):653663, 2015. [82] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. VAD: Vectorized scene representation for efficient autonomous driving. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 83408350, 2023. [83] Haoyi Jiang, Liu Liu, Tianheng Cheng, Xinjie Wang, Tianwei Lin, Zhizhong Su, Wenyu Liu, and Xinggang Wang. GaussTR: Foundation model-aligned gaussian transformer for self-supervised 3D spatial understanding. arXiv preprint arXiv:2412.13193, 2024. [84] Sicong Jiang et al. survey on vision-language-action models for autonomous driving. arXiv preprint arXiv:2506.24044, 2025. [85] Xie Jihong et al. Edge computing for real-time decision making in autonomous driving: Review of challenges, solutions, and future trends. Int. J. Adv. Comput. Sci. & Appl., 15(7), 2024. [86] Bu Jin, Xiaotao Hu, Yupeng Zheng, Xiaoyang Guo, Qian Zhang, Yao Yao, Diming Zhang, Xiaoxiao Long, Wei Yin, et al. OccVAR: Scalable 4D occupancy prediction via next-scale prediction. 2024. [87] Bu Jin et al. OccTENS: 3D occupancy world model via temporal next-scale prediction. arXiv preprint arXiv:2509.03887, 2025. [88] Xin Kang, Lei Chu, Jiahao Li, Xuejin Chen, and Yan Lu. Hierarchical intra-modal correlation learning for labelfree 3D semantic segmentation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2824428253, 2024. [89] Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, et al. Towards learningbased planning: The nuPlan benchmark for real-world autonomous driving. arXiv preprint arXiv:2403.04133, 2024. [90] Parneet Kaur, Arslan Aziz, Darshan Jain, Harshil Patel, Jonathan Hirokawa, Lachlan Townsend, Christoph Reimers, and Fiona Hua. Sea situational awareness (SeaSaw) dataset. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 25792587, 2022. [91] Nikhil Keetha et al. MapAnything: Universal feed-forward metric 3D reconstruction. arXiv preprint arXiv:2509.13414, 2025. [92] Bernhard Kerbl et al. 3D gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4): 1391, 2023. [93] Abderrazzaq Kharroubi, Zouhair Ballouch, Rafika Hajji, Anass Yarroudh, and Roland Billen. Multi-context point cloud dataset and machine learning for railway semantic segmentation. Infrastructures, 9(4):71, 2024. [94] Mehar Khurana, Neehar Peri, James Hays, and Deva Ramanan. Shelf-supervised cross-modal pre-training for 3D object detection. In Conf. Robot Learn. PMLR, 2024. [95] Tarasha Khurana, Peiyun Hu, David Held, and Deva Ramanan. Point cloud forecasting as proxy for 4D occupancy forecasting. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 11161124, 2023. [96] Alexander Kirillov et al. Segment anything. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 40154026, 2023. [97] Simon Klenk, David Bonello, Lukas Koestler, Nikita Araslanov, and Daniel Cremers. Masked event modeling: Self-supervised pretraining for event cameras. In Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis., pages 23782388, 2024. [98] Lingdong Kong, Youquan Liu, Lai Xing Ng, Benoit R. Cottereau, and Wei Tsang Ooi. OpenESS: Event-based semantic scene understanding with open vocabularies. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1568615698, 2024. [99] Lingdong Kong, Dongyue Lu, Ao Liang, Rong Li, Yuhao Dong, Tianshuai Hu, Lai Xing Ng, Wei Tsang Ooi, and Benoit R. Cottereau. Talk2Event: Grounded understanding of dynamic scenes from event cameras. In Adv. Neural Inf. Process. Syst., volume 38, 2025. [100] Lingdong Kong, Dongyue Lu, Xiang Xu, Lai Xing Ng, Wei Tsang Ooi, and Benoit Cottereau. EventFly: Event camera perception from ground to the sky. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 14721484, 2025. [101] Lingdong Kong, Xiang Xu, Youquan Liu, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Liu Ziwei. LargeAD: Large-scale cross-sensor data pretraining for autonomous driving. arXiv preprint arXiv:2501.04005, 2025. [102] Lingdong Kong et al. LaserMix for semi-supervised LiDAR semantic segmentation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 2170521715, 2023. [103] Lingdong Kong et al. Rethinking range view representation for LiDAR segmentation. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 228240, 2023. [104] Lingdong Kong et al. Robo3D: Towards robust and reliable 3D perception against corruptions. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1999420006, 2023. [105] Lingdong Kong et al. RoboDepth: Robust out-of-distribution depth estimation under corruptions. In Adv. Neural Inf. Process. Syst., volume 36, pages 2129821342, 2023. [106] Lingdong Kong et al. 3D and 4D world modeling: survey. arXiv preprint arXiv:2509.07996, 2025. [107] Lingdong Kong et al. Multi-modal data-efficient 3D scene understanding for autonomous driving. IEEE Trans. Pattern Anal. Mach. Intell., 47(5):37483765, 2025. [108] Georg Krispel, David Schinagl, Christian Fruhwirth-Reisinger, Horst Possegger, and Horst Bischof. MAELi: Masked autoencoder for large-scale LiDAR point clouds. In Proc. IEEE/CVF Winter Conf. Appl. Comput. Vis., pages 33833392, 2024. [109] Christopher Lang, Alexander Braun, Lars Schillingmann, Karsten Haug, and Abhinav Valada. Self-supervised representation learning from temporal ordering of automated driving sequences. IEEE Robotics Autom. Letters, 2024. [110] Bowen Li, Changhong Fu, Fangqiang Ding, Junjie Ye, and Fuling Lin. All-day object tracking for unmanned aerial vehicle. IEEE Trans. Mobile Comput., 22(8):45154529, 2022. [111] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Jia Zeng, Zhiqi Li, Jiazhi Yang, Hanming Deng, et al. Delving into the devils of birds-eye-view perception: review, evaluation and recipe. IEEE Trans. Pattern Anal. Mach. Intell., 46(4):21512170, 2023. [112] Peidong Li and Dixiao Cui. Does end-to-end autonomous driving really need perception tasks? arXiv preprint arXiv:2409.18341, 2024. [113] Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. TokenPacker: Efficient visual projector for multimodal llm. International Journal of Computer Vision, pages 119, 2025. [114] Xinpeng Li and Xiaojiang Peng. Rail detection: An efficient row-based network and new benchmark. In Proc. ACM Int. Conf. Multimedia, pages 64556463, 2022. [115] Yanwei Li et al. Unifying voxel-based representation with transformer for 3D object detection. In Adv. Neural Inf. Process. Syst., volume 35, pages 1844218455, 2022. [116] Ye Li et al. Is your LiDAR placement optimized for 3D scene understanding? In Adv. Neural Inf. Process. Syst., volume 37, pages 3498035017, 2024. [117] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model. arXiv preprint arXiv:2406.08481, 2024. [118] Yingyan Li et al. DriveVLA-W0: World models amplify data scaling law in autonomous driving. arXiv preprint arXiv:2510.12796, 2025. 27 [119] Yingyan Li et al. End-to-end driving with online trajectory evaluation via BEV world model. arXiv preprint arXiv:2504.01941, 2025. [120] Yinhao Li et al. BEVDepth: Acquisition of reliable depth for multi-view 3D object detection. In Proc. AAAI Conf. Artifi. Intell., volume 37, pages 14771485, 2023. [121] Zhenyu Li, Zehui Chen, Ang Li, Liangji Fang, Qinhong Jiang, Xianming Liu, Junjun Jiang, Bolei Zhou, and Hang Zhao. SimIPU: Simple 2D image and 3D point cloud unsupervised pre-training for spatial-aware visual representations. In Proc. AAAI Conf. Artifi. Intell., volume 36, pages 15001508, 2022. [122] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. BEVFormer: Learning birds-eye-view representation from multi-camera images via spatiotemporal transformers. In Proc. Eur. Conf. Comput. Vis., pages 118. Springer, 2022. [123] Zhuoling Li, Chuanrui Zhang, En Yu, and Haoqian Wang. Delving into the pre-training paradigm of monocular 3D object detection. arXiv preprint arXiv:2206.03657, 2022. [124] Ao Liang, Lingdong Kong, Dongyue Lu, Youquan Liu, Jian Fang, Huaici Zhao, and Wei Tsang Ooi. Perspectiveinvariant 3D object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 2772527738, 2025. [125] Ao Liang, Youquan Liu, Yu Yang, et al. LiDARCrafter: Dynamic 4D world modeling from LiDAR sequences. arXiv preprint arXiv:2508.03692, 2025. [126] Ao Liang et al. WorldLens: Full-spectrum evaluations of driving world models in real world. arXiv preprint arXiv:2512.10958, 2025. [127] Dingkang Liang et al. Seeing the future, perceiving the future: unified driving world model for future generation and perception. arXiv preprint arXiv:2503.13587, 2025. [128] Hanxue Liang, Chenhan Jiang, Dapeng Feng, Xin Chen, Hang Xu, Xiaodan Liang, Wei Zhang, Zhenguo Li, and Luc Van Gool. Exploring geometry-aware contrast and clustering harmonization for self-supervised 3D object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 32933302, 2021. [129] Quanmin Liang, Qiang Li, Xinzi Cao, Jinyi Lu, Mingyue Cui, Feidiao Yang, Wei Zhang, Kai Huang, and Yonghong Tian. Enhancing event camera data pretraining via prompt-tuning with visual models. 2024. [130] Guibiao Liao, Jiankun Li, and Xiaoqing Ye. VLM2Scene: Self-supervised image-text-LiDAR learning with foundation models for autonomous driving scene understanding. In Proc. AAAI Conf. Artifi. Intell., volume 38, pages 33513359, 2024. [131] Yiyi Liao, Jun Xie, and Andreas Geiger. KITTI-360: novel dataset and benchmarks for urban scene understanding in 2D and 3D. arXiv preprint arXiv:2109.13410, 2021. [132] Adam Lilja, Ji Lan, Junsheng Fu, and Lars Hammarstrand. QueryOcc: Query-based self-supervision for 3D semantic occupancy. arXiv preprint arXiv:2511.17221, 2025. [133] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollr, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proc. Eur. Conf. Comput. Vis., pages 740755. Springer, 2014. [134] Xuewu Lin et al. Sparse4D v3: Advancing end-to-end 3D detection and tracking. arXiv preprint arXiv:2311.11722, 2023. [135] Zhiwei Lin, Yongtao Wang, Shengxiang Qi, Nan Dong, and Ming-Hsuan Yang. BEV-MAE: Birds eye view masked autoencoders for point cloud pre-training in autonomous driving scenarios. In Proc. AAAI Conf. Artifi. Intell., volume 38, pages 35313539, 2024. [136] Haisong Liu, Yao Teng, Tao Lu, Haiguang Wang, and Limin Wang. SparseBEV: High-performance sparse 3D object detection from multi-camera videos. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1858018590, 2023. [137] Haizhuang Liu, Junbao Zhuo, Chen Liang, Jiansheng Chen, and Huimin Ma. Affinity3D: Propagating instancelevel semantic affinity for zero-shot point cloud semantic segmentation. In Proc. ACM Int. Conf. Multimedia, pages 90199028, 2024. [138] Hao Liu, Minglin Chen, Yanni Ma, Haihong Xiao, and Ying He. Point cloud unsupervised pre-training via 3D gaussian splatting. arXiv preprint arXiv:2411.18667, 2024. [139] Haotian Liu et al. Visual instruction tuning. In Adv. Neural Inf. Process. Syst., volume 36, pages 3489234916, 2023. [140] Jihao Liu, Tai Wang, Boxiao Liu, Qihang Zhang, Yu Liu, and Hongsheng Li. GeoMIM: Towards better 3D knowledge transfer via masked image modeling for multi-view 3D understanding. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1783917849, 2023. [141] Keyi Liu et al. Gaussian2Scene: 3D scene representation learning via self-supervised learning with 3D gaussian splatting. arXiv preprint arXiv:2506.08777, 2025. [142] Ruihan Liu et al. 4D-ROLLS: 4D radar occupancy learning via LiDAR supervision. arXiv preprint arXiv:2505.13905, 2025. [143] Ruixun Liu et al. OccVLA: Vision-language-action model with implicit 3D occupancy supervision. arXiv preprint arXiv:2509.05578, 2025. [144] Shilong Liu et al. Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection. In Proc. Eur. Conf. Comput. Vis., pages 3855. Springer, 2024. [145] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. Adv. Neural Inf. Process. Syst., 36, 2024. [146] Youquan Liu et al. La La LiDAR: Large-scale layout generation from LiDAR data. arXiv preprint arXiv:2508.03691, 2025. [147] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang, Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu Tseng, and Winston Hsu. Learning from 2D: Contrastive pixel-to-point knowledge transfer for 3D pretraining. arXiv preprint arXiv:2104.04687, 2021. [148] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1001210022, 2021. [149] Zhijian Liu et al. BEVFusion: Multi-task multi-sensor fusion with unified birds-eye view representation. In IEEE Int. Conf. Robotics Autom., pages 27742781, 2023. [150] Zhuang Liu et al. convnet for the 2020s. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1197611986, 2022. [151] William Ljungbergh et al. GASP: Unifying geometric and semantic self-supervised pre-training for autonomous driving. arXiv preprint arXiv:2503.15672, 2025. [152] Dongyue Lu, Lingdong Kong, Gim Hee Lee, Camille Simon Chane, and Wei Tsang Ooi. FlexEvent: Towards flexible event-frame object detection at varying operational frequencies. In Adv. Neural Inf. Process. Syst., volume 38, 2025. [153] Yuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge Zhu, and Yuexin Ma. See more and know more: Zero-shot point cloud segmentation via multi-modal visual data. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 2167421684, 2023. [154] Kai Luo et al. Omnidirectional multi-object tracking. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2195921969, 2025. [155] Ye Lyu, George Vosselman, Gui-Song Xia, Alper Yilmaz, and Michael Ying Yang. UAVid: semantic segmentation dataset for UAV imagery. ISPRS J. Photogrammetry Remote Sensing, 165:108119, 2020. [156] Hichem Maaref and Claude Barret. Sensor-based navigation of mobile robot in an indoor environment. Robotics Autonom. Sys., 38(1):118, 2002. [157] Anas Mahmoud, Jordan SK Hu, Tianshu Kuai, Ali Harakeh, Liam Paull, and Steven Waslander. Self-supervised image-to-point distillation via semantically tolerant contrastive loss. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 71027110, 2023. [158] Anas Mahmoud, Ali Harakeh, and Steven Waslander. Image-to-LiDAR relational distillation for autonomous driving data. In Proc. Eur. Conf. Comput. Vis., pages 459475. Springer, 2024. [159] Andrey Malinin, Neil Band, German Chesnokov, Yarin Gal, Mark JF Gales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal Raina, et al. SHIFTS: dataset of real distributional shift across multiple large-scale tasks. arXiv preprint arXiv:2107.07455, 2021. [160] Murari Mandal, Lav Kush Kumar, and Santosh Kumar Vipparthi. MOR-UAV: benchmark dataset and baselines for moving object recognition in UAV videos. In Proc. ACM Int. Conf. Multimedia, pages 26262635, 2020. [161] Jiageng Mao, Minzhe Niu, Chenhan Jiang, Xiaodan Liang, Yamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Chunjing Xu, et al. One million scenes for autonomous driving: ONCE dataset. arXiv preprint arXiv:2106.11037, 2021. [162] Johannes Meier, Luca Scalerandi, Oussema Dhaouadi, Jacques Kaiser, Araslanov Nikita, and Daniel Cremers. CARLA Drone: Monocular 3D object detection from different perspective. GCPR, 2024. [163] Takahiro Miki, Joonho Lee, Lorenz Wellhausen, and Marco Hutter. Learning to walk in confined spaces using 3D representation. arXiv preprint arXiv:2403.00187, 2024. [164] Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, and Bin Dai. Occupancy-MAE: Self-supervised pre-training large-scale LiDAR point clouds with masked occupancy autoencoders. IEEE Trans. Intell. Veh., 9(7):51505162, 2024. [165] Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, and Bin Dai. Multi-camera unified pre-training via 3D scene reconstruction. IEEE Robotics Autom. Letters, 9(4):32433250, 2024. [166] Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng Zhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, et al. DriveWorld: 4D pre-trained scene understanding via world models for autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1552215533, 2024. [167] Matthias Mueller, Neil Smith, and Bernard Ghanem. benchmark and simulator for UAV tracking. In Proc. Eur. Conf. Comput. Vis., pages 445461. Springer, 2016. [168] Mahyar Najibi, Jingwei Ji, Yin Zhou, Charles Qi, Xinchen Yan, Scott Ettinger, and Dragomir Anguelov. Unsupervised 3D perception with 2D vision-language distillation for autonomous driving. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 86028612, 2023. [169] Lucas Nunes, Rodrigo Marcuzzi, Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. SegContrast: 3D point cloud feature representation learning through self-supervised segment discrimination. IEEE Robotics Autom. Letters, 7(2):21162123, 2022. [170] NVIDIA Corporation. PhysicalAI autonomous vehicles dataset, 2025. URL https://huggingface.co/datasets/ nvidia/PhysicalAI-Autonomous-Vehicles. Dataset release. [171] Maxime Oquab, Timothe Darcet, Tho Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. Trans. Machine Learn. Research, 2024. [172] Aljoa Oep, Tim Meinhardt, Francesco Ferroni, Neehar Peri, Deva Ramanan, and Laura Leal-Taix. Better call SAL: Towards learning to segment anything in LiDAR. In Proc. Eur. Conf. Comput. Vis., pages 7190. Springer, 2024. [173] Edoardo Palladin et al. Self-supervised sparse sensor fusion for long range perception. arXiv preprint arXiv:2508.13995, 2025. [174] Tai-Yu Pan, Chenyang Ma, Tianle Chen, Cheng Perng Phoo, Katie Luo, Yurong You, Mark Campbell, Kilian Weinberger, Bharath Hariharan, and Wei-Lun Chao. Pre-training LiDAR-based 3D object detectors through colorization. In Int. Conf. Learn. Represent., 2024. [175] Bo Pang, Hongchi Xia, and Cewu Lu. Unsupervised 3D point cloud representation learning by triangle constrained contrast for autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 52295239, 2023. [176] Dennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon. Is pseudo-LiDAR needed for monocular 3D object detection? In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 31423152, 2021. [177] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. OpenScene: 3D scene understanding with open vocabularies. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 815824, 2023. 30 [178] Quang-Hieu Pham, Pierre Sevestre, Ramanpreet Singh Pahwa, Huijing Zhan, Chun Ho Pang, Yuda Chen, Armin Mustafa, Vijay Chandrasekhar, and Jie Lin. A*3D dataset: Towards autonomous driving in challenging environments. In Proc. IEEE Int. Conf. Robot. Automat., pages 22672273, 2020. [179] Mariia Pushkareva, Yuri Feldman, Csaba Domokos, Kilian Rambach, and Dotan Di Castro. Radar spectralanguage model for automotive scene parsing. arXiv preprint arXiv:2406.02158, 2024. [180] Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane Simoni, Corentin Sautier, Patrick Prez, Andrei Bursuc, and Renaud Marlet. Three pillars improving vision foundation model distillation for lidar. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2151921529, 2024. [181] Bo Qiu, Yuzhou Zhou, Lei Dai, Bing Wang, Jianping Li, Zhen Dong, Chenglu Wen, Zhiliang Ma, and Bisheng Yang. WHU-RailWay3D: diverse dataset and benchmark for railway point cloud semantic segmentation. IEEE Trans. Intell. Transport. Sys., 25(12):2090020916, 2024. [182] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In Int. Conf. Machine Learn., pages 87488763. PMLR, 2021. [183] Ren Ranftl et al. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Trans. Pattern Anal. Mach. Intell., 44(3):16231637, 2020. [184] Nikhila Ravi et al. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [185] Dinesh Reddy, Minh Vo, and Srinivasa Narasimhan. CarFusion: Combining point tracking and part detection for dynamic 3D reconstruction of vehicles. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 19061915, 2018. [186] Laurenz Reichardt, Nikolas Ebert, and Oliver Wasenmller. 360 from single camera: few-shot approach for LiDAR segmentation. In Proc. IEEE/CVF Int. Conf. Comput. Vis. Worksh., pages 10671075, 2023. [187] Tianhe Ren et al. Grounded SAM: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. [188] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio Savarese. Learning social etiquette: Human trajectory understanding in crowded scenes. In Proc. Eur. Conf. Comput. Vis., pages 549565. Springer, 2016. [189] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Imageto-LiDAR self-supervised distillation for autonomous driving data. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 98919901, 2022. [190] Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, and Vincent Lepetit. BEVContrast: Selfsupervision in BEV space for automotive LiDAR point clouds. In Proc. IEEE Int. Conf. 3D Vis., pages 559568, 2024. [191] Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud Marlet, and Vincent Lepetit. UNIT: Unsupervised online instance segmentation through time. arXiv preprint arXiv:2409.07887, 2024. [192] Chen Shi et al. DriveX: Omni scene modeling for learning generalizable world knowledge in autonomous driving. arXiv preprint arXiv:2505.19239, 2025. [193] Hao Shi et al. OneOcc: Semantic occupancy prediction for legged robots with single panoramic camera. arXiv preprint arXiv:2511.03571, 2025. [194] Oren Shrout, Ori Nitzan, Yizhak Ben-Shabat, and Ayellet Tal. PatchContrast: Self-supervised pre-training for 3D object detection. arXiv preprint arXiv:2308.06985, 2023. [195] Lingyu Si, Gang Li, Changwen Zheng, and Fanjiang Xu. Self-supervised representation learning for the object detection of marine radar. In Proc. Int. Conf. Comput. Artifi. Intell., pages 751760, 2022. [196] Oriane Simoni et al. DINOv3. arXiv preprint arXiv:2508.10104, 2025. [197] Sophia Sirko-Galouchenko, Alexandre Boulch, Spyros Gidaris, Andrei Bursuc, Antonin Vobecky, Patrick Prez, and Renaud Marlet. OccFeat: Self-supervised occupancy feature prediction for pretraining BEV segmentation networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 44934503, 2024. [198] Boyi Sun, Yuhang Liu, Xingxia Wang, Bin Tian, Long Chen, and Fei-Yue Wang. 3D unsupervised learning by distilling 2D open-vocabulary segmentation models for autonomous driving. arXiv preprint arXiv:2405.15286, 2024. 31 [199] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Chen Zhifeng, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 24462454, 2020. [200] Tianfang Sun, Zhizhong Zhang, Xin Tan, Yanyun Qu, and Yuan Xie. Exploring the untouched sweeps for conflict-aware 3D segmentation pretraining. arXiv preprint arXiv:2407.07465, 2024. [201] Rustam Tagiew, Pavel Klasek, Roman Tilly, Martin Kppel, Patrick Denzler, Philipp Neumaier, Tobias Klockau, Martin Boekhoff, and Karsten Schwalbe. Osdar23: Open sensor data for rail 2023. In Int. Conf. Robotics Autom. Engineer., pages 270276. IEEE, 2023. [202] Zhiyu Tan, Zichao Dong, Cheng Zhang, Weikun Zhang, Hang Ji, and Hao Li. OvO: Open-vocabulary occupancy. arXiv preprint arXiv:2305.16133, 2023. [203] Sina Tayebati, Theja Tulabandhula, and Amit Trivedi. Sense less, generate more: Pre-training LiDAR perception with masked autoencoders for ultra-efficient 3D sensing. arXiv preprint arXiv:2406.07833, 2024. [204] Ran Tian et al. Tokenize the world into object-level knowledge to address long-tail events in autonomous driving. In Conf. Robot Learn., pages 36563673. PMLR, 2025. [205] Xiaoyu Tian, Tao Jiang, Longfei Yun, Yucheng Mao, Huitong Yang, Yue Wang, Yilun Wang, and Hang Zhao. Occ3D: large-scale 3D occupancy prediction benchmark for autonomous driving. In Adv. Neural Inf. Process. Syst., volume 36, 2024. [206] Xiaoyu Tian et al. DriveVLM: The convergence of autonomous driving and large vision-language models. In Conf. Robot Learn., pages 46984726. PMLR, 2025. [207] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei Wu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin, et al. Scene as occupancy. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 84068415, 2023. [208] Tugce Toprak, Burak Belenlioglu, Burak Aydn, Cuneyt Guzelis, and Alper Selver. Conditional weighted ensemble of transferred models for camera based onboard pedestrian detection in railway driver support systems. IEEE Trans. Veh. Tech., 69(5):50415054, 2020. [209] Gustaf Uggla and Milan Horemuz. Towards synthesized training data for semantic segmentation of mobile laser scanning point clouds: Generating level crossings from real and synthetic point cloud samples. Automation in Construction, 130:103839, 2021. [210] Antonin Vobecky, Oriane Simoni, David Hurych, Spyridon Gidaris, Andrei Bursuc, Patrick Prez, and Josef Sivic. POP-3D: Open-vocabulary 3D occupancy prediction from images. Adv. Neural Inf. Process. Syst., 36, 2024. [211] Binglu Wang, Lei Zhang, Zhaozhong Wang, Yongqiang Zhao, and Tianfei Zhou. Core: Cooperative reconstruction for multi-agent perception. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 87108720, 2023. [212] Chengyao Wang, Li Jiang, Xiaoyang Wu, Zhuotao Tian, Bohao Peng, Hengshuang Zhao, and Jiaya Jia. GroupContrast: Semantic-aware self-supervised representation learning for 3D understanding. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 49174928, 2024. [213] Jianyuan Wang et al. VGGT: Visual geometry grounded transformer. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 52945306, 2025. [214] Lening Wang, Wenzhao Zheng, Yilong Ren, Han Jiang, Zhiyong Cui, Haiyang Yu, and Jiwen Lu. OccSora: 4D occupancy generation models as world simulators for autonomous driving. arXiv preprint arXiv:2405.20337, 2024. [215] Letian Wang, Seung Wook Kim, Jiawei Yang, Cunjun Yu, Boris Ivanovic, Steven Waslander, Yue Wang, Sanja Fidler, Marco Pavone, and Peter Karkus. DistillNeRF: Perceiving 3D scenes from single-glance images by distilling neural fields and foundation model features. arXiv preprint arXiv:2406.12095, 2024. [216] Ning Wang, Yuanyuan Wang, Yi Wei, Bing Han, and Yuan Feng. Marine vessel detection dataset and benchmark for unmanned surface vehicles. Applied Ocean Research, 142:103835, 2024. [217] Peng Wang et al. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 32 [218] Shihao Wang et al. Exploring object-centric temporal modeling for efficient multi-view 3D object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 36213631, 2023. [219] Shihao Wang et al. OmniDrive: holistic vision-language dataset for autonomous driving with counterfactual reasoning. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 2244222452, 2025. [220] Sijie Wang et al. UAVScenes: multi-modal dataset for UAVs. arXiv preprint arXiv:2507.22412, 2025. [221] Song Wang, Jiawei Yu, Wentong Li, Hao Shi, Kailun Yang, Junbo Chen, and Jianke Zhu. Label-efficient semantic scene completion with scribble annotations. In Proc. Int. Joint Conf. Artifi. Intell., pages 13981406, 2024. [222] Song Wang et al. Not all voxels are equal: Hardness-aware semantic scene completion with self-distillation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 1479214801, 2024. [223] Song Wang et al. ReliOcc: Towards reliable semantic occupancy prediction via uncertainty learning. arXiv preprint arXiv:2409.18026, 2024. [224] Song Wang et al. PixelThink: Towards efficient chain-of-pixel reasoning. arXiv preprint arXiv:2505.23727, 2025. [225] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully convolutional one-stage monocular 3D object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 913922, 2021. [226] Wenpeng Wang, Bradford Campbell, and Sirajum Munir. Self-supervised contrastive learning for camera-to-radar knowledge distillation. In Int. Conf. Distributed Comput. in Smart Sys. Internet of Things, pages 154161. IEEE, 2024. [227] Xiaofeng Wang et al. DriveDreamer: Towards real-world-drive world models for autonomous driving. In Proc. Eur. Conf. Comput. Vis., pages 5572. Springer, 2024. [228] Yuanbin Wang, Shaofei Huang, Yulu Gao, Zhen Wang, Rui Wang, Kehua Sheng, Bo Zhang, and Si Liu. Transferring CLIPs knowledge into zero-shot point cloud semantic segmentation. In Proc. ACM Int. Conf. Multimedia, pages 37453754, 2023. [229] Ze Wang, Sihao Ding, Ying Li, Jonas Fenn, Sohini Roychowdhury, Andreas Wallin, Lane Martin, Scott Ryvola, Guillermo Sapiro, and Qiang Qiu. Cirrus: long-range bi-pattern LiDAR dataset. In Proc. IEEE Int. Conf. Robot. Automat., pages 57445750, 2021. [230] Zichen Wang, Zhuokun Yao, Jianwei Zhang, Ye Zheng, Zhengyuan Zhang, Shuang Deng, Yajing Liu, and Hao Liu. Focus on your geometry: Exploiting the potential of multi-frame stereo depth estimation pre-training for 3D object detection. In Int. Joint Conf. Neural Networks, pages 18, 2024. [231] Jason Wei et al. Chain-of-thought prompting elicits reasoning in large language models. In Adv. Neural Inf. Process. Syst., volume 35, pages 2482424837, 2022. [232] Julong Wei, Shanshuai Yuan, Pengfei Li, Qingda Hu, Zhongxue Gan, and Wenchao Ding. OccLLaMA: An occupancy-language-action generative world model for autonomous driving. arXiv preprint arXiv:2409.03272, 2024. [233] Weijie Wei, Fatemeh Karimi Nejadasl, Theo Gevers, and Martin Oswald. T-MAE: temporal masked autoencoders for point cloud representation learning. In Proc. Eur. Conf. Comput. Vis., pages 178195. Springer, 2024. [234] Xinshuo Weng et al. PARA-Drive: Parallelized architecture for real-time autonomous driving. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., pages 1544915458, 2024. [235] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In Adv. Neural Inf. Process. Syst., 2021. [236] Maciej Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, Ravi Kiran, and Senthil Yogamani. S3PT: Scene semantics and structure guided clustering to boost self-supervised pre-training for autonomous driving. arXiv preprint arXiv:2410.23085, 2024. [237] Sheng Wu et al. QuaDreamer: Controllable panoramic video generation for quadruped robots. In Conf. Robot Learn., 2025. 33 [238] Xiongfei Wu, Mingfei Cheng, Qiang Hu, Jianlang Chen, Yuheng Huang, Manabu Okada, Michio Hayashi, Tomoyuki Tsuchiya, Xiaofei Xie, and Lei Ma. Foundation models for autonomous driving system: An initial roadmap. arXiv preprint arXiv:2504.00911, 2025. [239] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. DOTA: large-scale dataset for object detection in aerial images. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 39743983, 2018. [240] Aoran Xiao, Jiaxing Huang, Dayan Guan, Xiaoqin Zhang, Shijian Lu, and Ling Shao. Unsupervised point cloud representation learning with deep neural networks: survey. IEEE Trans. Pattern Anal. Mach. Intell., 45(9): 1132111339, 2023. [241] Pengchuan Xiao, Zhenlei Shao, Steven Hao, Zishuo Zhang, Xiaolin Chai, Judy Jiao, Zesong Li, Jian Wu, Kai Sun, Kun Jiang, et al. PandaSet: Advanced sensor suite dataset for autonomous driving. In IEEE Int. Conf. Intell. Transport. Sys., pages 30953101, 2021. [242] Zihao Xiao, Longlong Jing, Shangxuan Wu, Alex Zihao Zhu, Jingwei Ji, Chiyu Max Jiang, Wei-Chih Hung, Thomas Funkhouser, Weicheng Kuo, Anelia Angelova, et al. 3D open-vocabulary panoptic segmentation with 2D-3D vision-language distillation. In Proc. Eur. Conf. Comput. Vis., pages 2138. Springer, 2025. [243] Saining Xie, Jiatao Gu, Demi Guo, Charles Qi, Leonidas Guibas, and Or Litany. PointContrast: Unsupervised pre-training for 3D point cloud understanding. In Proc. Eur. Conf. Comput. Vis., pages 574591. Springer, 2020. [244] Shaoyuan Xie et al. RoboBEV: Towards robust birds eye view perception under corruptions. arXiv preprint arXiv:2304.06719, 2023. [245] Shaoyuan Xie et al. Benchmarking and improving birds eye view perception robustness in autonomous driving. IEEE Trans. Pattern Anal. Mach. Intell., 47(5):38783894, 2025. [246] Shaoyuan Xie et al. Are VLMs ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 65856597, 2025. [247] Mengde Xu et al. Side adapter network for open-vocabulary semantic segmentation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 29452954, 2023. [248] Runsen Xu, Tai Wang, Wenwei Zhang, Runjian Chen, Jinkun Cao, Jiangmiao Pang, and Dahua Lin. MV-JAR: Masked voxel jigsaw and reconstruction for LiDAR-based self-supervised pre-training. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1344513454, 2023. [249] Shaoqing Xu, Fang Li, Shengyin Jiang, Ziying Song, Li Liu, and Zhi-xin Yang. GaussianPretrain: simple unified 3D gaussian representation for visual pre-training in autonomous driving. arXiv preprint arXiv:2411.12452, 2024. [250] Shaoqing Xu et al. FusionPainting: Multimodal fusion with adaptive attention for 3D object detection. In IEEE Int. Conf. Intell. Transport. Sys., pages 30473054, 2021. [251] Weichen Xu, Jian Cao, Tianhao Fu, Ruilong Ren, Zicong Hu, Xixin Cao, and Xing Zhang. Point cloud reconstruction is insufficient to learn 3D representations. In Proc. ACM Int. Conf. Multimedia, pages 84718479, 2024. [252] Weichen Xu, Tianhao Fu, Jian Cao, Xinyu Zhao, Xinxin Xu, Xixin Cao, and Xing Zhang. Mutual informationdriven self-supervised point cloud pre-training. Knowledge-Based Systems, page 112741, 2024. [253] Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, and Qingshan Liu. 4D contrastive superflows are dense 3D representation learners. In Proc. Eur. Conf. Comput. Vis., pages 5880. Springer, 2024. [254] Xiang Xu, Lingdong Kong, Hui Shuai, , Liang Pan, Liu Ziwei, and Qingshan Liu. LiMoE: Mixture of LiDAR representation learners from automotive scenes. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2736827379, 2025. [255] Xiang Xu, Ao Liang, Youquan Liu, et al. U4D: Uncertainty-aware 4D world modeling from LiDAR sequences. arXiv preprint arXiv: 2512.02982, 2025. [256] Xiang Xu et al. Beyond one shot, beyond one perspective: Cross-view and long-horizon distillation for better LiDAR representations. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 2550625518, 2025. 34 [257] Xiaohao Xu, Ye Li, Tianyi Zhang, Jinrong Yang, Matthew Johnson-Roberson, and Xiaonan Huang. Learning shared RGB-D fields: Unified self-supervised pre-training for label-efficient LiDAR-camera 3D perception. arXiv preprint arXiv:2405.17942, 2024. [258] Hang Yan, Yongji Li, Luping Wang, and Shichao Chen. Learning omni-dimensional spatio-temporal dependencies for millimeter-wave radar perception. Remote Sensing, 16(22):4256, 2024. [259] Xiangchao Yan et al. SPOT: Scalable 3D pre-training via occupancy prediction for learning transferable 3D representations. IEEE Trans. Pattern Anal. Mach. Intell., 2025. [260] Xu Yan, Haiming Zhang, Yingjie Cai, Jingming Guo, Weichao Qiu, Bin Gao, Kaiqiang Zhou, Yue Zhao, Huan Jin, Jiantao Gao, et al. Forging vision foundation models for autonomous driving: Challenges, methodologies, and opportunities. arXiv preprint arXiv:2401.08045, 2024. [261] Ziyang Yan, Wenzhen Dong, Yihua Shao, Yuhang Lu, Liu Haiyang, Jingwen Liu, Haozhe Wang, Zhe Wang, Yan Wang, Fabio Remondino, et al. RenderWorld: World model with self-supervised 3D label. arXiv preprint arXiv:2409.11356, 2024. [262] Hao Yang, Haiyang Wang, Di Dai, and Liwei Wang. PRED: pre-training via semantic rendering on LiDAR point clouds. Adv. Neural Inf. Process. Syst., 36, 2024. [263] Honghui Yang, Tong He, Jiaheng Liu, Hua Chen, Boxi Wu, Binbin Lin, Xiaofei He, and Wanli Ouyang. GD-MAE: generative decoder for MAE pre-training on LiDAR point clouds. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 94039414, 2023. [264] Honghui Yang, Sha Zhang, Di Huang, Xiaoyang Wu, Haoyi Zhu, Tong He, Shixiang Tang, Hengshuang Zhao, Qibo Qiu, Binbin Lin, et al. UniPAD: universal pre-training paradigm for autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1523815250, 2024. [265] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, et al. Generalized predictive model for autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1466214672, 2024. [266] Yan Yang, Liyuan Pan, and Liu Liu. Event camera data pre-training. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1069910709, 2023. [267] Yan Yang, Liyuan Pan, and Liu Liu. Event camera data dense pre-training. In Proc. Eur. Conf. Comput. Vis., pages 292310. Springer, 2024. [268] Yiming Yang et al. FASTopoWM: Fast-slow lane segment topology reasoning with latent world models. arXiv preprint arXiv:2507.23325, 2025. [269] Yu Yang, Jianbiao Mei, Yukai Ma, Siliang Du, Wenqing Chen, Yijie Qian, Yuxiang Feng, and Yong Liu. Driving in the occupancy world: Vision-centric 4D occupancy forecasting and planning via world models for autonomous driving. arXiv preprint arXiv:2408.14197, 2024. [270] Zetong Yang, Li Chen, Yanan Sun, and Hongyang Li. Visual point cloud forecasting enables scalable autonomous driving. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2024. [271] Shanliang Yao, Runwei Guan, Zhaodong Wu, Yi Ni, Zile Huang, Ryan Wen Liu, Yong Yue, Weiping Ding, Eng Gee Lim, Hyungjoon Seo, et al. WaterScenes: multi-task 4D radar-camera fusion dataset and benchmarks for autonomous driving on water surfaces. IEEE Trans. Intell. Transport. Sys., 25(11):1658416598, 2024. [272] Shanliang Yao, Runwei Guan, Zitian Peng, Chenhang Xu, Yilu Shi, Yong Yue, Eng Gee Lim, Hyungjoon Seo, Ka Lok Man, Xiaohui Zhu, et al. Exploring radar data representations in autonomous driving: comprehensive review. IEEE Trans. Intell. Transport. Sys., 26(6):74017425, 2025. [273] Hui Ye, Rajshekhar Sunderraman, and Shihao Ji. UAV3D: large-scale 3D perception benchmark for unmanned aerial vehicles. arXiv preprint arXiv:2410.11125, 2024. [274] Junbo Yin, Dingfu Zhou, Liangjun Zhang, Jin Fang, Cheng-Zhong Xu, Jianbing Shen, and Wenguan Wang. ProposalContrast: Unsupervised pre-training for LiDAR-based 3D object detection. In Proc. Eur. Conf. Comput. Vis., pages 1733. Springer, 2022. [275] Tianwei Yin et al. Multimodal virtual point 3D detection. In Adv. Neural Inf. Process. Syst., volume 34, pages 1649416507, 2021. 35 [276] Wei Yin et al. Metric3D: Towards zero-shot metric 3D prediction from single image. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 90439053, 2023. [277] Jason Yosinski et al. How transferable are features in deep neural networks? In Adv. Neural Inf. Process. Syst., volume 27, 2014. [278] Hanxun Yu, Wentong Li, Song Wang, Junbo Chen, and Jianke Zhu. Inst3D-LMM: Instance-aware 3D scene understanding with multi-modal instruction tuning. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1414714157, 2025. [279] Zhu Yu, Bowen Pang, Lizhe Liu, Runmin Zhang, Qihao Peng, Maochun Luo, Sheng Yang, Mingxia Chen, Si-Yuan Cao, and Hui-Liang Shen. Language driven occupancy prediction. arXiv preprint arXiv:2411.16072, 2024. [280] Jiakang Yuan, Bo Zhang, Xiangchao Yan, Botian Shi, Tao Chen, Yikang Li, and Yu Qiao. AD-PT: Autonomous driving pre-training with large-scale point cloud dataset. Adv. Neural Inf. Process. Syst., 36, 2023. [281] Mahmut Yurt et al. LTDA-Drive: LLMs-guided generative models based long-tail data augmentation for autonomous driving. arXiv preprint arXiv:2505.18198, 2025. [282] Oliver Zendel, Markus Murschitz, Marcel Zeilinger, Daniel Steininger, Sara Abbasi, and Csaba Beleznai. RailSem19: dataset for semantic rail scene understanding. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. Worksh., 2019. [283] Yihan Zeng, Chenhan Jiang, Jiageng Mao, Jianhua Han, Chaoqiang Ye, Qingqiu Huang, Dit-Yan Yeung, Zhen Yang, Xiaodan Liang, and Hang Xu. CLIP2: Contrastive language-image-point pretraining from real-world point cloud data. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 1524415253, 2023. [284] Mingliang Zhai et al. World knowledge-enhanced reasoning using instruction-guided interactor in autonomous driving. In Proc. AAAI Conf. Artifi. Intell., volume 39, pages 98429850, 2025. [285] Chubin Zhang, Juncheng Yan, Yi Wei, Jiaxin Li, Li Liu, Yansong Tang, Yueqi Duan, and Jiwen Lu. OccNeRF: Self-supervised multi-camera occupancy prediction with neural radiance fields. arXiv preprint arXiv:2312.09243, 2023. [286] Fengyi Zhang et al. TT-Occ: Test-time compute for self-supervised occupancy via spatio-temporal gaussian splatting. arXiv preprint arXiv:2503.08485, 2025. [287] Haijun Zhang, Mingshan Sun, Qun Li, Linlin Liu, Ming Liu, and Yuzhu Ji. An empirical study of multi-scale object detection in high resolution UAV images. Neurocomputing, 421:173182, 2021. [288] Haiming Zhang, Wending Zhou, Yiyao Zhu, Xu Yan, Jiantao Gao, Dongfeng Bai, Yingjie Cai, Bingbing Liu, Shuguang Cui, and Zhen Li. VisionPAD: vision-centric pre-training paradigm for autonomous driving. arXiv preprint arXiv:2411.14716, 2024. [289] Haiming Zhang et al. SQS: Enhancing sparse perception models via query-based splatting in autonomous driving. arXiv preprint arXiv:2509.16588, 2025. [290] Hao Zhang et al. simple framework for open-vocabulary segmentation and detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 10201031, 2023. [291] Junbo Zhang, Runpei Dong, and Kaisheng Ma. CLIP-FO3D: Learning free open-world 3D scene representations from 2D dense clip. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 20482059, 2023. [292] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and Raquel Urtasun. Copilot4D: Learning unsupervised world models for autonomous driving via discrete diffusion. In Int. Conf. Learn. Represent., 2024. [293] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hongsheng Li. Learning 3D representations from 2D pre-trained models via image-to-point masked autoencoders. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 2176921780, 2023. [294] Sha Zhang, Jiajun Deng, Lei Bai, Houqiang Li, Wanli Ouyang, and Yanyong Zhang. HVDistill: Transferring knowledge from images to point clouds via unsupervised hybrid-view distillation. Int. J. Comput. Vis., pages 115, 2024. [295] Xiaoshuai Zhang, Zhicheng Wang, Howard Zhou, Soham Ghosh, Danushen Gnanapragasam, Varun Jampani, Hao Su, and Leonidas Guibas. ConDense: Consistent 2D/3D pre-training for dense and sparse features from multi-view images. In Proc. Eur. Conf. Comput. Vis., pages 1938. Springer, 2024. 36 [296] Xinyu Zhang, Li Wang, Jian Chen, Cheng Fang, Lei Yang, Ziying Song, Guangqi Yang, Yichen Wang, Xiaofei Zhang, and Jun Li. Dual radar: multi-modal dataset with dual 4D radar for autononous driving. arXiv preprint arXiv:2310.07602, 2023. [297] Yifan Zhang and Junhui Hou. Fine-grained image-to-LiDAR contrastive distillation with visual foundation models. arXiv preprint arXiv:2405.14271, 2024. [298] Yumeng Zhang, Shi Gong, Kaixin Xiong, Xiaoqing Ye, Xiao Tan, Fan Wang, Jizhou Huang, Hua Wu, and Haifeng Wang. BEVWorld: multimodal world model for autonomous driving via unified bev latent space. arXiv preprint arXiv:2407.05679, 2024. [299] Zaiwei Zhang, Rohit Girdhar, Armand Joulin, and Ishan Misra. Self-supervised pretraining of 3D features on any point-cloud. In Proc. IEEE/CVF Int. Conf. Comput. Vis., pages 1025210263, 2021. [300] Lingjun Zhao et al. ShelfGaussian: Shelf-supervised open-vocabulary gaussian-based 3D scene understanding. arXiv preprint arXiv:2512.03370, 2025. [301] Seth Zhao, Hao Xiang, Chenfeng Xu, Xin Xia, Bolei Zhou, and Jiaqi Ma. CooPre: Cooperative pretraining for V2X cooperative perception. arXiv preprint arXiv:2408.11241, 2024. [302] Xin Zhao, Shiyu Hu, Yipei Wang, Jing Zhang, Yimin Hu, Rongshuai Liu, Haibin Ling, Yin Li, Renshu Li, Kun Liu, et al. BioDrone: bionic drone-based single object tracking benchmark for robust vision. Int. J. Comput. Vis., 132(5):16591684, 2024. [303] Jilai Zheng, Pin Tang, Zhongdao Wang, Guoqing Wang, Xiangxuan Ren, Bailan Feng, and Chao Ma. VEON: Vocabulary-enhanced occupancy prediction. In Proc. Eur. Conf. Comput. Vis., pages 92108. Springer, 2024. [304] Lianqing Zheng, Long Yang, Qunshu Lin, Wenjin Ai, Minghao Liu, Shouyi Lu, Jianan Liu, Hongze Ren, Jingyue Mo, Xiaokai Bai, et al. OmniHD-Scenes: next-generation multimodal dataset for autonomous driving. arXiv preprint arXiv:2412.10734, 2024. [305] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang, Yueqi Duan, and Jiwen Lu. OccWorld: Learning 3D occupancy world model for autonomous driving. arXiv preprint arXiv:2311.16038, 2023. [306] Yupeng Zheng et al. World4Drive: End-to-end autonomous driving via intention-aware physical latent world model. arXiv preprint arXiv:2507.00603, 2025. [307] Chong Zhou et al. Extract free dense labels from CLIP. In Proc. Eur. Conf. Comput. Vis., pages 696712. Springer, 2022. [308] Jiazhou Zhou, Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. EventBind: Learning unified representation to bind them all for event-based open-world understanding. In Proc. Eur. Conf. Comput. Vis., pages 477494. Springer, 2024. [309] Mingle Zhou, Rui Xing, Delong Han, Zhiyong Qi, and Gang Li. PDT: UAV target detection dataset for pests and diseases tree. In Proc. Eur. Conf. Comput. Vis., pages 5672. Springer, 2025. [310] Xiaoyu Zhou et al. AutoOcc: Automatic open-ended semantic occupancy annotation via vision-language guided gaussian splatting. arXiv preprint arXiv:2502.04981, 2025. [311] Xin Zhou et al. HERMES: unified self-driving world model for simultaneous 3D scene understanding and generation. arXiv preprint arXiv:2501.14729, 2025. [312] Yin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud based 3D object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., pages 44904499, 2018. [313] Zewei Zhou et al. TurboTrain: Towards efficient and balanced multi-task learning for multi-agent perception and prediction. arXiv preprint arXiv:2508.04682, 2025. [314] Haoran Zhu, Haoze He, Anna Choromanska, Satish Ravindran, Binbin Shi, and Lihui Chen. Multi-view radar autoencoder for self-supervised automotive radar representation learning. In IEEE Intell. Veh. Symposium, pages 16011608, 2024. [315] Haoran Zhu, Zhenyuan Dong, Kristi Topollai, and Anna Choromanska. AD-L-JEPA: Self-supervised spatial world models with joint embedding predictive architecture for autonomous driving with LiDAR data. arXiv preprint arXiv:2501.04969, 2025. 37 [316] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha Zhang, Xianglong He, Tong He, Hengshuang Zhao, Chunhua Shen, Yu Qiao, et al. Ponderv2: Pave the way for 3D foundation model with universal pre-training paradigm. arXiv preprint arXiv:2310.08586, 2023. [317] Pengfei Zhu, Longyin Wen, Dawei Du, Xiao Bian, Heng Fan, Qinghua Hu, and Haibin Ling. Detection and tracking meet drones challenge. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):73807399, 2021. [318] Zheng Zhu, Xiaofeng Wang, Wangbo Zhao, Chen Min, Nianchen Deng, Min Dou, Yuqi Wang, Botian Shi, Kai Wang, Chi Zhang, et al. Is Sora world simulator? comprehensive survey on general world models and beyond. arXiv preprint arXiv:2405.03520, 2024. [319] Long Zhuang and Tiezhen Jiang. Pre-training for mmWave radar object detection through masked image modeling. In Int. Conf. Image Process., Comput. Vis. Machine Learn., pages 546550. IEEE, 2023. [320] Long Zhuang, Tiezhen Jiang, Jianhua Wang, Qi An, Kai Xiao, and Anqi Wang. Effective mmwave radar object detection pre-training based on masked image modeling. IEEE Sensors J., 2023. [321] Yongshuo Zong, Oisin Mac Aodha, and Timothy Hospedales. Self-supervised multimodal learning: survey. arXiv preprint arXiv:2304.01008, 2023. [322] Jialv Zou, Bencheng Liao, Qian Zhang, Wenyu Liu, and Xinggang Wang. MIM4D: Masked modeling with multi-view video for autonomous driving representation learning. arXiv preprint arXiv:2403.08760, 2024. [323] Jian Zou, Tianyu Huang, Guanglei Yang, Zhenhua Guo, Tao Luo, Chun-Mei Feng, and Wangmeng Zuo. UniM2AE: Multi-modal masked autoencoders with unified 3D representation for 3D perception in autonomous driving. In Proc. Eur. Conf. Comput. Vis., pages 296313. Springer, 2024. [324] Pufan Zou, Shijia Zhao, Weijie Huang, Qiming Xia, Chenglu Wen, Wei Li, and Cheng Wang. AdaCo: Overcoming visual foundation model noise in 3D semantic segmentation via adaptive label correction. arXiv preprint arXiv:2412.18255, 2024. [325] Arij Zouaoui, Ankur Mahtani, Mohamed Amine Hadded, Sbastien Ambellouis, Jacques Boonaert, and Hazem Wannous. RailSet: unique dataset for railway anomaly detection. In Proc. IEEE Int. Conf. Image Process. Appl. Sys., pages 16, 2022."
        }
    ],
    "affiliations": [
        "Alibaba Group",
        "Nanjing University of Aeronautics",
        "National University of Singapore",
        "Singapore Management University",
        "Zhejiang University"
    ]
}