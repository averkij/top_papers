{
    "paper_title": "DiffSeg30k: A Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection",
    "authors": [
        "Hai Ci",
        "Ziheng Peng",
        "Pei Yang",
        "Yingxin Xuan",
        "Mike Zheng Shou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, a publicly available dataset of 30k diffusion-edited images with pixel-level annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild images--we collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion models--local edits using eight SOTA diffusion models; 3) Multi-turn editing--each image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenarios--a vision-language model (VLM)-based pipeline automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/Chaos2629/Diffseg30k"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 1 1 1 9 1 . 1 1 5 2 : r DiffSeg30k: Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection Hai Ci1*, Ziheng Peng2, Pei Yang1, Yingxin Xuan1, Mike Zheng Shou1 1. Show Lab, National University of Singapore 2. South China University of Technology {cihai03,mike.zheng.shou}@gmail.com Figure 1. Motivation. Images shared online may be independently edited multiple times by different users using different models. DiffSeg30k simulates this real-world scenario through multi-turn diffusion-based edits, enabling novel fine-grained AIGC detection - simultaneous edit localization and model attribution."
        },
        {
            "title": "Abstract",
            "content": "Diffusion-based editing enables realistic modification of local image regions, making AI-generated content harder to detect. Existing AIGC detection benchmarks focus on classifying entire images, overlooking the localization of diffusion-based edits. We introduce DiffSeg30k, publicly available dataset of 30k diffusion-edited images with pixellevel annotations, designed to support fine-grained detection. DiffSeg30k features: 1) In-the-wild imageswe collect images or image prompts from COCO to reflect real-world content diversity; 2) Diverse diffusion modelslocal edits using eight SOTA diffusion models; 3) Multi-turn editingeach image undergoes up to three sequential edits to mimic real-world sequential editing; and 4) Realistic editing scenariosa vision-language model (VLM)-based pipeline *Equal Contribution Corresponding Author automatically identifies meaningful regions and generates context-aware prompts covering additions, removals, and attribute changes. DiffSeg30k shifts AIGC detection from binary classification to semantic segmentation, enabling simultaneous localization of edits and identification of the editing models. We benchmark three baseline segmentation approaches, revealing significant challenges in semantic segmentation tasks, particularly concerning robustness to image distortions. Experiments also reveal that segmentation models, despite being trained for pixel-level localization, emerge as highly reliable whole-image classifiers of diffusion edits, outperforming established forgery classifiers while showing great potential in cross-generator generalization. We believe DiffSeg30k will advance research in fine-grained localization of AI-generated content by demonstrating the promise and limitations of segmentation-based methods. DiffSeg30k is released at: https://huggingface.co/datasets/ Chaos2629/Diffseg30k 1 1. Introduction Detecting AI-generated content (AIGC) holds significant societal importance due to its implications for trust, misinformation, privacy, and copyright protection [10, 11, 67, 69]. With the advancement of generative AI technologies, particularly diffusion-based editing techniques [4, 5, 24, 43, 54], it has become possible to produce highly realistic modifications in localized image regions [37, 46]. Such capabilities raise serious concerns, as these subtle yet realistic edits pose considerable challenges to content verification and digital forensics [34, 47, 52, 53, 73]. Current methods and benchmarks for AIGC detection predominantly focus on classifying entire images [6, 48, 59, 80], with only limited consideration for localized editing scenarios. This has resulted in notable gap: the lack of systematic benchmarks specifically designed to evaluate precise detection and localization of AI-generated edits, especially those produced using increasingly popular diffusion-based methods [5, 54]. Traditional related tasks, such as Image Forgery Localization (IFL) [20, 21, 32, 60, 76, 79], typically address edits made using conventional techniques like copy-move or splicing, yet existing IFL benchmarks rarely cover advanced diffusion-based editing approaches. In this paper, we introduce DiffSeg30k, publicly available dataset comprising 30k diffusion-edited images with pixel-level annotations, specifically aimed at facilitating the detection and localization of diffusion-based edits. To reflect realistic, practical scenarios, DiffSeg30k incorporates four key features: 1) In-the-wild imageswe collect natural images from COCO dataset [31] to ensure real-world content diversity. In addition, we leverage COCO-derived prompts with diffusion models to generate complementary AI-based images; 2) Diverse diffusion modelslocal editing is conducted using eight state-of-the-art diffusion models; 3) Realistic user-driven editsan automated annotation pipeline leveraging vision-language models (VLMs) [3] identifies semantically meaningful regions and generates context-aware editing prompts, including object addition, removal, and attribute modification; and 4) Multi-turn editingeach image undergoes up to three sequential edits using different diffusion models, simulating realistic scenarios in which online-shared images are iteratively modified (see Fig. 1). Beyond pixel-level annotations of edited areas, the specific diffusion model used for each edit is annotated, defining novel semantic segmentation task for fine-grained localization that simultaneously identifies the edited regions and the corresponding editing models. Tab. 1 compares DiffSeg30k with existing benchmarks containing local editing data. Leveraging DiffSeg30k, we benchmark three baseline segmentation modelsFCN [35], SegFormer [68], and Deeplabv3+ [8, 9]on both binary and semantic segmentation tasks. Our preliminary experiments reveal several key findings: (1) model capacity matters; (2) semantic segmentation in multi-turn editing scenarios remains highly challenging; (3) baseline models are sensitive to post-hoc image transformations such as resizing and JPEG compression; (4) segmentation models can serve as strong whole-image classifiers and largely outperform established classifiers; and (5) segmentation models exhibits strong generalization to unseen generators, indicating promising potential for building more generalizable detection models. We hope the release of DiffSeg30k and these initial results will draw community attention to the important problem of diffusion-based editing localization and attribution, shifting AIGC detection from whole-image classification paradigm to more universe and powerful segmentation paradigm. 2. Related Work 2.1. Image Forgery Localization This task focuses on detecting image regions modified by traditional editing techniques such as copy-move and splicing. Representative benchmarks include CASIA [15], Columbia [39], NIST16 [1], IMD20 [41], Coverage [66], FantasticReality [28], PSCC-Net [32], OpenForensics [29], etc. Based on these datasets, various approaches have been developed, leveraging transformer-based feature extraction [20, 60], progressive learning [32, 79], hierarchical mechanisms [21], and diffusion priors [76] to localize edits. 2.2. AIGC Detection The task of AIGC detection involves identifying content created by generative AI models such as GANs and diffusion models. Current datasets [6, 21, 23, 38, 48, 59, 64, 80] and methods [12, 13, 26, 55, 62, 63, 78] predominantly focus on classifying entire images. Existing datasets can be categorized into two types according to their image content: facial images [14, 17, 23, 38, 61, 74] and general images [6, 48, 59, 64, 80]. Several datasets [16, 20, 33, 82] include limited numbers ( 3) of diffusion models and only involving single local edit per image, as detailed in Tab. 1. In contrast, our proposed DiffSeg30k dataset focuses on general images, covers eight different diffusion models, and incorporates multi-turn editing by different models on each image. This comprehensive dataset provides systematic support for more general semantic segmentation tasks beyond conventional first-classify-then-binary-segmentation paradigm [21, 70] used in single-turn editing datasets. 3. DiffSeg30k benchmark 3.1. Dataset construction We developed an automated image editing and annotation pipeline based on vision-language models (VLM), illustrated in Fig. 2. The pipeline comprises two stages: 1) generating candidate editing regions and 2) creating suitable editing prompts for these regions, followed by image editing. 2 Table 1. Comparison with previous AIGC detection benchmarks. We just compare datasets that includes local editing data. Datasets Image Availability HIFI-Net [21] COCO Glide [20] OnlineDet [16] DA-HFNet [33] BR-Gen [7] WFake [82] General General General General General Facial DiffSeg30k(Ours) General # Max Edit Turns 1 1 1 1 1 # Total 0 1 3 2 3 2 3 8 Diffusion Editing Models Name - Glide [40] SD1 [46], SD2 [46], Adobe Firefly [2] InpaintAnything [75], Paint by Example [72] SDXL [43], BrushNet [27], PowerPaint [81] Repaint-p2 [36], Repaint-LDM [46] SD2 [46], SD3.5 [54], SDXL [43], Flux.1 [5], Glide [40], Kolors [58], HunyuanDiT1.1 [30], Kandinsky 2.2 [49] Figure 2. Automatic data collection framework. The pipeline consists of two stages: (1) identifying editable regions using VLMs [3] and Grounded-SAM [44] to generate object masks; (2) generating context-appropriate editing prompts with VLMs and applying sequential diffusion-based edits to the selected regions. Stage 1generating candidate editing regions. As shown in Fig. 2, given an arbitrary image, we first use Qwen2.5VL [3] to identify distinct objects present in the image. The identified object categories are then provided to GroundedSAM [44] to generate corresponding masks for each object. Subsequently, we calculate the Intersection-over-Union (IoU) among object masks and eliminate redundant masks with IoU greater than 70% to avoid repetitive editing of the same object. Finally, we randomly select between one to three object masks for editing. Stage 2editing prompt generation & inpainting. As depicted in Fig. 2, each selected object mask from Stage 1 is paired with the original image and fed into the Qwen2.5-VL model, prompting it to generate contextually appropriate editing prompts. For inpainting, the generated editing prompt, corresponding object mask, and the original image are simultaneously provided to diffusion model to perform localized inpainting. For multi-turn editing, the edited image can form new pair with the remaining object masks, repeating the prompt generation and inpainting processes. Thus, we obtain images edited sequentially up to three times, depending on the number of masks selected in Stage 1. Data diversity & balance. To ensure the diversity and balance of the edited images, we specifically considered the following six aspects: 1) Balanced Candidate Object Types: We prompted the VLM to prioritize selecting humans, as human-centric edits are significant in practical applications, yet VLMs tend to under-select humans naturally. 2) Balanced Candidate Mask Area: We encouraged VLM to prioritize larger objects for editing, counteracting the empirical tendency of VLM to select smaller objects. 3) Balanced Edit Types: During editing prompt generation, we prompted VLM to randomly choose from three edit types: attribute changes, object additions, and object removals. For object additions, we followed the approach of SEED-Data-Edit [18], first removing an object and then adding new object in the same position to ensure natural placement. 4) Balanced Edit Models: For inpainting, we randomly selected one diffusion model from eight state-of-the-art diffusion models to execute an editing prompt. 5) Enhanced Multi-turn Editing: We Figure 3. DiffSeg30k Statistics. The distributions reflect our balanced data collection strategy: (a) Object category distribution shows Food (14,308) are most frequently edited, with an overall balanced distribution across categories. (b) Mask area ratio distribution reveals decreasing trend from smaller to larger regions. (c) Editing count per model indicates uniform participation of all eight diffusion models across first, second, and third editing rounds. (d) We manually increased the frequency of multi-turn editing, with the first, second, and third rounds occurring at an approximate 1:4:5 ratio, as multi-turn editing subsumes single-turn cases and presents more challenging scenario. (e) The base image ratio is approximately balanced between real and AI-generated images. manually increased the frequency of multi-turn editing, with the first, second, and third turns occurring at an approximate 1:4:5 ratio, as multi-turn editing subsumes single-turn cases and presents more challenging scenario. 6) Balanced Real and AI Bases: About half of the base images were real, collected from COCO, while the other half were AI-generated using COCO prompts with random diffusion models. 3.2. Sanity check Our automated pipeline occasionally produces low-quality edits due to (i) limited diffusion editing capability (e.g., incomplete object removal) or (ii) mask errors from GroundedSAM [44]. These issues largely stem from our frameworks ambition to simulate diverse and realistic editing scenarios. In contrast, existing datasets such as OnlineDet [16] rely on random masks with potentially empty prompts, while wFake [82] focuses only on predefined facial regions (e.g., eyes, nose). Our approach covers broader and more complex range of edits, though at the cost of occasional artifacts. To improve dataset quality, we employ Qwen2.5-VL [3] to assign image-level quality scores (05) and automatically discard edits with severe artifacts (score < 3). Details of filtering are provided in supplementary. 3.3. Dataset statistics We conducted statistical analysis of DiffSeg30k, illustrated in Fig. 3. It reveals that smaller objects are edited more frequently, whereas larger objects tend to be edited less often, with editing frequency smoothly decreasing as object area increases. Objects such as food, vehicles, furniture, nature scenery, animals, humans, and signs are among the most frequently edited. The dataset maintains balanced distribution across editing models and base image distribution. Fig. 4 shows some editing results. 4. Benchmarking baselines 4.1. Experimental setup Editing data preparation. To match the input requirements of diffusion editing models as well as achieving high editing quality, COCO images are first resized so that the shorter side is 1024 pixels, followed by center crop to 10241024. For model-specific compatibility, images are resized to 256 for Glide during editing. Training details. Due to the lack of existing baseline segmentation models on general AIGC localization to handle semantic segmentation, we trained two CNN baselines based 4 Table 2. Performance of baseline segmentation models. Results of SegFormer and Deeplabv3+ on binary segmentation (localizing edited regions) and semantic segmentation (localizing and attributing edits to specific diffusion models). SegFormer [68] Deeplab v3+ [8] FCN-8s [35] Binary segmentation mIoU bF1 Acc 0.989 0.961 0.974 0.984 0.699 0.838 0.678 0.761 0.457 Semantic segmentation Acc 0.952 0.916 0.331 mIoU bF1 0.825 0.760 0.203 0.532 0.431 0.024 Table 3. Robustness to image transformations. We evaluate DeepLabv3+and SegFormer under JPEG compression with varying transformations to assess their sensitivity to common post-processing operations. Baseline indicates no distortions. SegFormer [68] Deeplab v3+ [8] Distortion Baseline JPEG 60 JPEG 80 resize 256 resize 1024 Baseline JPEG 60 JPEG 80 resize 256 resize 1024 Binary segmentation mIoU bF1 Acc 0.961 0.989 0.260 0.705 0.291 0.775 0.360 0.565 0.898 0.966 0.974 0.984 0.782 0.867 0.847 0.907 0.751 0.811 0.729 0.816 0.678 0.257 0.338 0.286 0.548 0.761 0.556 0.613 0.631 0.499 Semantic segmentation Acc 0.953 0.665 0.724 0.616 0.821 0.916 0.184 0.190 0.021 0. mIoU bF1 0.825 0.032 0.188 0.299 0.414 0.760 0.013 0.014 0.012 0.168 0.532 0.197 0.305 0.338 0.296 0.431 0.019 0.019 0.021 0.014 on classic semantic segmentation frameworks: FCN-8s [35] and Deeplabv3+[8, 9] with ResNet50[22] and 1 transfomer baseline SegFormer-B2 [68]. We evaluated these models on two tasks: binary segmentation (localizing edited areas) and semantic segmentation (localizing edited areas while distinguishing among different editing models). The dataset is split into training and validation sets in an 8:2 ratio, and all results are reported on the validation set. Models are trained on 512512 resolution (for acceleration) using default hyperparameters from open-source implementations. We additionally include 5k unedited real COCO images in the training set to help suppress false positives on real images. Training FCN-8s, SegFormer-B2, and Deeplabv3+ takes approximately 8, 6, and 3 hours, respectively, on an NVIDIA 4090 GPU. Evaluation metric. To evaluate classification performance, we report accuracy (Acc) and mean average precision (mAP). For segmentation, we report pixel-wise accuracy (Acc), mask Intersection-over-Union (mIoU) and boundary-F1 score (bF1) between predicted and ground truth masks. The background class is excluded from mIoU and bF1 calculations to specifically reflect models abilities to detect edited areas. 4.2. Results Model capacity matters. As shown in Tab. 2, the older FCN-8s struggles with both binary and semantic localization of diffusion-edited regions. In contrast, more powerful segmentation models Deeplabv3+ and SegFormer , performs substantially better, achieving strong binary localization with an mIoU of 0.974 and 0.961. These results highlight the importance of model representational capacity. Semantic segmentation is challenging. Simultaneously localizing edited regions and attributing them to the correct diffusion model is substantially more difficult than binary localization. Even for the best performed model SegFormer, the mIoU for semantic segmentation drops to 0.825, leaving significant room for improvement. For Deeplabv3+ and FCN, this number further drops significantly to 0.76 and 0.23. Fig. 5 shows the confusion matrices: FCN-8s tends to misclassify edits from all models as background pixels, while SegFormer struggles particularly with Flux. Deeplab v3+ struggles with Kolors and SDXL. It also often misclassifies SD2 and Flux. Fig. 6 further illustrates representative results. FCN-8s produces largely random outputs in both tasks, whereas SegFormer and Deeplab v3+ achieves more coherent segmentations but occasionally misclassifies the editing model. In the following detailed ablation, we only evaluate Deeplabv3+ and SegFormer. Baseline models are sensitive to image transformations. We evaluated the robustness of SegFormer and DeepLabv3+ under common image transformations, including JPEG compression and resizing. As shown in Tab. 3, image transformations substantially degraded the performance 5 Figure 4. Example edited images. For each image pair, the left shows the original image with red contours marking the regions to be edited, and the right presents the corresponding editing result. of both Deeplabv3+ and SegFormer, with severe mIoU dropssemantic segmentation almost failed under JPEG compression. This highlights the need for improved network architectures or data augmentation strategies to achieve deployable localization models. Interestingly, DeepLabv3+ exhibits partial resilience in binary segmentation settings, while SegFormer demonstrates certain robustness when resized to higher resolutions. 6 Figure 5. Confusion matrix on the semantic segmentation task. Table 4. Generalization to unseen diffusion generators. Segmentation models are trained on images edited by 6 diffusion generators and evaluated on edits from 2 unseen generators to assess crossgenerator generalization. Baseline indicates training and testing on 8 generators. SegFormer [68] Deeplab v3+ [8] Generalize to Baseline Flux Hunyuan SD2 SD3.5 Glide Kolors Baseline Flux Hunyuan SD2 SD3.5 Glide Kolors Binary segmentation mIoU bF1 Acc 0.961 0.989 0.930 0.935 0.926 0.927 0.951 0.957 0.974 0.984 0.867 0.920 0.931 0.833 0.927 0.953 0.678 0.491 0.497 0.578 0.761 0.526 0.372 0.648 Segmentation models achieve strong cross-generator generalization. Tab. 4 presents the cross-generator generalization results of SegFormer and Deeplabv3+. Training was conducted on images edited by six diffusion models and tested on edits from two unseen models, with three different combinations evaluated. Both SegFormer and Deeplabv3+ demonstrate strong cross-generator generalization, with mIoU consistently above 0.9 and 0.8, respectively. Notably, both models were not specifically designed for such generalization, suggesting that segmentation-based methods hold great potential for transferable AIGC detection. Segmentation models as strong generalizable wholeimage classifiers. Given the strong performance of SegFormer and DeepLabv3+ in binary segmentation, we further investigate whether they can serve as powerful wholeimage AIGC classifiers. Unlike standard AIGC classification, which determines whether an entire image is generated by diffusion model, our task is more challenging: models must identify whether diffusion-edited regions exist within Table 5. Binary classification results of estabilished AIGC detectors. SegFormer and Deeplabv3+ are adapted to classification by thresholding. CNNSpot [62] UniversalFakeDet [42] SegFromer [68] Deeplab v3+ [8] Acc 0.942 0.860 0.997 0.980 mAP 0.979 0.934 0.999 0.994 Table 6. Cross-generator generalization of differnet classifiers. The model is trained on images edited by 6 diffusion generators and evaluated on images edited by 2 unseen diffusion generators to assess its ability to generalize across different editing sources. Baseline indicates training and testing on 8 generators. Segmentation models are adapted by thresholding. UniFakeDet [42] SegFromer [68] Deeplab V3+ [8] Generalize to Baseline Flux Hunyuan SD2 SD3.5 Glide Kolors Baseline Flux Hunyuan SD2 SD3.5 Glide Kolors Baseline Flux Hunyuan SD2 SD3.5 Glide Kolors Acc 0.860 0.768 0.714 0.749 0.997 0.978 0.982 0.987 0.980 0.935 0.942 0.964 mAP 0.934 0.877 0.817 0.844 0.999 0.986 0.986 0.996 0.994 0.967 0.965 0.985 an image. We set threshold such that if the non-background mask area exceeds minimal threshold 1% of the image area, the image is considered to contain edited content. We compare with two established AIGC classification models, 7 Figure 6. Segmentation results. SEG and DPL denote SegFormer and Deeplab v3+, respectively. -B and -S refer to binary and semantic segmentation, respectively. In each image group, the first column shows edited images, the second column shows the ground truth masks, and the third column shows the predicted masks. In each mask, Crimson corresponds to Stable Diffusion 2, ForestGreen corresponds to Kolors, Yellow corresponds to Stable Diffusion 3.5 Medium, RoyalBlue corresponds to Flux, MediumPurple corresponds to Stable Diffusion XL 1.0, TealBlue corresponds to Glide, Gray corresponds to Hunyuan-DiT, and red corresponds to Kandinsky 2.2. CNNSpot [62] and UniversalFakeDet [42]. During training, we additionally collect 30k real images from COCO [31] as negative samples to train the classification baselines. Results in Tab. 5 reveal striking observation: when repurposed for whole-image classification, segmentation models outperform dedicated AIGC classifiers by significant margin. We attribute this advantage to the fine-grained, per-pixel supervision employed during segmentation training. Our dataset includes images containing multiple, interleaved edited regions from different generators, allowing segmentation models to develop deeper understanding of localized generative cues. Notably, although UniversalFakeDet was originally designed for cross-model generalization in whole-image classification, it performs markedly worse than general-purpose segmentation models in cross-model evaluation (Tab. 6). Cross-model generalization has long been challenge in the AIGC classification community. Through this experiment, we demonstrate that segmentation models trained on multi-turn edited data exhibit strong potential to address this issue, suggesting paradigm shift from AIGC classification toward more robust AIGC segmentation. 5. Conclusion We introduce DiffSeg30k, multi-turn diffusion editing dataset that provides robust foundation for systematically studying diffusion-based editing localization and model attribution. Through extensive experimental analyses, we highlight both the strengths and current limitations of segmentation methods, laying the groundwork for further research in robust AIGC localization techniques. License. This dataset is derived from COCO and is distributed under the COCO license without additional restrictions."
        },
        {
            "title": "References",
            "content": "[1] Nist nimble 2016 datasets, 2016. 2 [2] Adobe. Adobe firefly. https://www.adobe.com/sg/ products/firefly.html, 2024. 3 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5vl technical report. arXiv preprint arXiv:2502.13923, 2025. 2, 3, 4, 13 [4] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. 2 [5] Bfl.ai. Flux 1.1. https://huggingface.co/blackforest-labs/FLUX.1-dev, 2024. 2, 3 [6] Jordan Bird and Ahmad Lotfi. Cifake: Image classification and explainable identification of ai-generated synthetic images. IEEE Access, 12:1564215650, 2024. 2 [7] Lvpan Cai, Haowei Wang, Jiayi Ji, YanShu ZhouMen, Yiwei Ma, Xiaoshuai Sun, Liujuan Cao, and Rongrong Ji. Zooming in on fakes: novel dataset for localized ai-generated image detection with forgery amplification approach. arXiv preprint arXiv:2504.11922, 2025. 3 [8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 2, 5, 7, 13 [9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801818, 2018. 2, 5 [10] Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, and Mike Zheng Shou. Wmadapter: Adding watermark control to latent diffusion models. arXiv preprint arXiv:2406.08337, 2024. 2 [11] Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. In European Conference on Computer Vision, pages 338354. Springer, 2024. [12] Davide Cozzolino, Giovanni Poggi, Riccardo Corvi, Matthias Nießner, and Luisa Verdoliva. Raising the bar of ai-generated image detection with clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 43564366, 2024. 2 [13] Davide Cozzolino, Giovanni Poggi, Matthias Nießner, and Luisa Verdoliva. Zero-shot detection of ai-generated images. In European Conference on Computer Vision, pages 5472. Springer, 2024. 2 [14] Hao Dang, Feng Liu, Joel Stehouwer, Xiaoming Liu, and Anil Jain. On the detection of digital face manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition, pages 57815790, 2020. 2 [15] Jing Dong, Wei Wang, and Tieniu Tan. Casia image tampering detection evaluation database. In 2013 IEEE China summit and international conference on signal and information processing, pages 422426. IEEE, 2013. 2 [16] David Epstein, Ishan Jain, Oliver Wang, and Richard Zhang. Online detection of ai-generated images. In Proceedings of the IEEE/CVF international conference on computer vision, pages 382392, 2023. 2, 3, 4 [17] Apurva Gandhi and Shomik Jain. Adversarial perturbations fool deepfake detectors. In 2020 International joint conference on neural networks (IJCNN), pages 18. IEEE, 2020. [18] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 3 [19] Google DeepMind. Nano banana (gemini 2.5 flash image) ai image editing tool, 2025. Integrated in Gemini App and Google Photos; allows textand image-based editing of photographs. 13 [20] Fabrizio Guillaro, Davide Cozzolino, Avneesh Sud, Nicholas Dufour, and Luisa Verdoliva. Trufor: Leveraging all-round clues for trustworthy image forgery detection and localization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2060620615, 2023. 2, 3 [21] Xiao Guo, Xiaohong Liu, Zhiyuan Ren, Steven Grosz, Iacopo Masi, and Xiaoming Liu. Hierarchical fine-grained image forgery detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31553165, 2023. 2, 3 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 5 [23] Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. Forgerynet: versatile benchmark for comprehensive forgery analysis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 43604369, 2021. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 13 [26] Zhengyuan Jiang, Yuyang Zhang, Moyang Guo, and Neil Zhenqiang Gong. Edittrack: Detecting and attributing ai-assisted image editing. arXiv preprint arXiv:2510.01173, 2025. 2 [27] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting model with decomposed dual-branch diffusion. In European Conference on Computer Vision, pages 150168. Springer, 2024. 3 [28] Vladimir Kniaz, Vladimir Knyaz, and Fabio Remondino. The point where reality meets fantasy: Mixed adversarial generators for image splice detection. Advances in neural information processing systems, 32, 2019. 2 [29] Trung-Nghia Le, Huy Nguyen, Junichi Yamagishi, and Isao Echizen. Openforensics: Large-scale challenging dataset for multi-face forgery detection and segmentation in-the-wild. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1011710127, 2021. 2 [30] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding, 2024. 3 [31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 2, 8 [32] Xiaohong Liu, Yaojie Liu, Jun Chen, and Xiaoming Liu. Psccnet: Progressive spatio-channel correlation network for image manipulation detection and localization. IEEE Transactions on Circuits and Systems for Video Technology, 32(11):7505 7517, 2022. 2 [33] Yang Liu, Xiaofei Li, Jun Zhang, Shengze Hu, and Jun Lei. Da-hfnet: Progressive fine-grained forgery image detection and localization based on dual attention. In 2024 3rd International Conference on Image Processing and Media Computing (ICIPMC), pages 5158. IEEE, 2024. 2, 3 [34] Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, and Yuheng Bu. Image watermarks are removable using controllable regeneration from clean noise. arXiv preprint arXiv:2410.05470, 2024. 2 [35] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 34313440, 2015. 2, [36] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. 3 [37] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [38] Changtao Miao, Yi Zhang, Weize Gao, Man Luo, Weiwei Feng, Zhiya Tan, Jianshu Li, Ajian Liu, Yunfeng Diao, Qi Chu, et al. Ddl: dataset for interpretable deepfake detection and localization in real-world scenarios. arXiv preprint arXiv:2506.23292, 2025. 2 [39] Tian-Tsong Ng, Jessie Hsu, and Shih-Fu Chang. Columbia image splicing detection evaluation dataset. DVMM lab. Columbia Univ CalPhotos Digit Libr, 2009. 2 [40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [41] Adam Novozamsky, Babak Mahdian, and Stanislav Saic. Imd2020: large-scale annotated dataset tailored for detecting manipulated images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision workshops, pages 7180, 2020. [42] Utkarsh Ojha, Yuheng Li, and Yong Jae Lee. Towards universal fake image detectors that generalize across generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2448024489, 2023. 7, 8 [43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. 2, 3, 13 [44] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 3, 4 [45] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis, 2024. 13 [46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2, 3 [47] Tom Sander, Pierre Fernandez, Alain Durmus, Teddy Furon, and Matthijs Douze. Watermark anything with localized messages. arXiv preprint arXiv:2411.07231, 2024. 2 [48] Zeyang Sha, Zheng Li, Ning Yu, and Yang Zhang. De-fake: Detection and attribution of fake images generated by textIn Proceedings of the 2023 to-image generation models. ACM SIGSAC conference on computer and communications security, pages 34183432, 2023. 2 [49] Arseniy Shakhmatov, Anton Razzhigaev, Aleksandr Nikolich, Vladimir Arkhipkin, Igor Pavlov, Andrey Kuznetsov, and Denis Dimitrov. kandinsky 2.2, 2023. [50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 13 [51] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. 13 [52] Yiren Song, Shengtao Lou, Xiaokang Liu, Hai Ci, Pei Yang, Jiaming Liu, and Mike Zheng Shou. Anti-reference: Universal and immediate defense against reference-based generation. arXiv preprint arXiv:2412.05980, 2024. 2 10 [53] Yiren Song, Pei Yang, Hai Ci, and Mike Zheng Shou. Idprotector: An adversarial noise encoder to protect against idpreserving image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 30193028, 2025. 2 [54] Stability.ai. Stable diffusion v3.5. https://stability. ai/news/introducingstablediffusion35, 2024. 2, [55] Zhen Sun, Ziyi Zhang, Zeren Luo, Zeyang Sha, Tianshuo Cong, Zheng Li, Shiwen Cui, Weiqiang Wang, Jiaheng Wei, Xinlei He, et al. Fragfake: dataset for fine-grained detection of edited images with vision language models. arXiv preprint arXiv:2505.15644, 2025. 2 [56] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1494014950, 2025. 13 [57] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 13 [58] Kolors Team. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. arXiv preprint, 2024. 3 [59] Luisa Verdoliva, Davide Cozzolino, and Koki Nagano. 2022 ieee image and video processing cup synthetic image detection, 2022. 2 [60] Junke Wang, Zuxuan Wu, Jingjing Chen, Xintong Han, Abhinav Shrivastava, Ser-Nam Lim, and Yu-Gang Jiang. Objectformer for image manipulation detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23642373, 2022. [61] Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian Wang, and Yang Liu. Fakespotter: simple yet robust baseline for spotting ai-synthesized fake faces. arXiv preprint arXiv:1909.06122, 2019. 2 [62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei Efros. Cnn-generated images are surprisingly easy to spot... for now. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 86958704, 2020. 2, 7, 8 [63] Wenhao Wang, Longqi Cai, Taihong Xiao, Yuxiao Wang, and Ming-Hsuan Yang. Scaling laws for deepfake detection. arXiv preprint arXiv:2510.16320, 2025. 2 [64] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. Benchmarking deepart detection. arXiv preprint arXiv:2302.14475, 2023. 2 [65] Cong Wei, Zheyang Xiong, Weiming Ren, Xeron Du, Ge Zhang, and Wenhu Chen. Omniedit: Building image editing generalist models through specialist supervision. In The Thirteenth International Conference on Learning Representations, 2024. 13 [66] Bihan Wen, Ye Zhu, Ramanathan Subramanian, Tian-Tsong Ng, Xuanjing Shen, and Stefan Winkler. Coveragea novel In 2016 IEEE database for copy-move forgery detection. international conference on image processing (ICIP), pages 161165. IEEE, 2016. [67] Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023. 2 [68] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers, 2021. 2, 5, 7 [69] Jiazheng Xing, Hai Ci, Hongbin Xu, Hangjie Yuan, Yong Liu, and Mike Zheng Shou. Optmark: Robust multi-bit diffusion watermarking via inference time optimization. arXiv preprint arXiv:2508.21727, 2025. 2 [70] Haozhen Yan, Yan Hong, Suning Lang, Jiahui Zhan, Yikun Ji, Yujie Gao, Jun Lan, Huijia Zhu, Weiqiang Wang, and Jianfu Zhang. Gamma: Generalizable alignment via multitask and manipulation-augmented training for ai-generated image detection. arXiv preprint arXiv:2509.10250, 2025. 2 [71] Haozhen Yan, Yan Hong, Jiahui Zhan, Yikun Ji, Jun Lan, Huijia Zhu, Weiqiang Wang, and Jianfu Zhang. Coco-inpaint: benchmark for image inpainting detection and manipulation localization. arXiv preprint arXiv:2504.18361, 2025. 13 [72] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1838118391, 2023. 3 [73] Pei Yang, Hai Ci, Yiren Song, and Mike Zheng Shou. Can simple averaging defeat modern watermarks? Advances in Neural Information Processing Systems, 37:5664456673, 2024. 2 [74] Xin Yang, Yuezun Li, and Siwei Lyu. Exposing deep fakes using inconsistent head poses. In ICASSP 2019-2019 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 82618265. IEEE, 2019. 2 [75] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Inpaint anything: SegarXiv preprint Wenjun Zeng, and Zhibo Chen. ment anything meets image inpainting. arXiv:2304.06790, 2023. 3 [76] Zeqin Yu, Jiangqun Ni, Yuzhen Lin, Haoyi Deng, and Bin Li. Diffforensics: Leveraging diffusion prior to image forgery detection and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1276512774, 2024. 2 [77] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19513 19524, 2025. 13 [78] Nan Zhong, Yiran Xu, Sheng Li, Zhenxing Qian, and Xinpeng Zhang. Patchcraft: Exploring texture patch for efficient aigenerated image detection. arXiv preprint arXiv:2311.12397, 2023. 2 [79] Haochen Zhu, Gang Cao, and Xianglin Huang. Progressive feedback-enhanced transformer for image forgery localization. arXiv preprint arXiv:2311.08910, 2023. 2 [80] Mingjian Zhu, Hanting Chen, Qiangyu Yan, Xudong Huang, Guanyu Lin, Wei Li, Zhijun Tu, Hailin Hu, Jie Hu, and Yunhe 11 Wang. Genimage: million-scale benchmark for detecting aigenerated image. Advances in Neural Information Processing Systems, 36:7777177782, 2023. 2 [81] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211. Springer, 2024. 3 [82] Dragos, -Constantin T, ˆant,aru, Elisabeta Oneat, a, and Dan Oneat, a. Weakly-supervised deepfake localization in diffusiongenerated images. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 6258 6268, 2024. 2, 3, 4 12 DiffSeg30k: Multi-Turn Diffusion Editing Benchmark for Localized AIGC Detection"
        },
        {
            "title": "Supplementary Material",
            "content": "6. Relation to global editing line of work focuses on global image editing [19, 56, 57, 65, 71], where the entire output image is synthesized from scratch. From the perspective of AIGC detection, these methods can be grouped together with standard (vanilla) image generation approaches, as they lack the notion of local editing regionall pixels are fully regenerated. In contrast, our dataset is intentionally designed around localized inpaintingstyle editing, motivating shift in AIGC detection from whole-image classification to per-pixel attribution, more general and fine-grained paradigm. This formulation also lays foundation for improving global-editing detection, as our experiments show that segmentation models can serve as strong whole-image classifiers. 7. Do LoRAs affect localization? Diffusion models can be enhanced with Low-Rank Adaptation (LoRA) modules [25] to improve efficiency [45] or expressiveness [51, 77]. In this experiment, we investigate whether the use of LoRAs impacts the localization performance of baseline segmentation models. Specifically, we apply the Hyper-SD LoRA [45] (8 steps, DDIM scheduler [50]) to SDXL [43], which reduces the number of inference steps from 50 to 8. We re-edit all regions in the validation set that were previously modified by SDXL using the LoRAaccelerated version (SDXL-LoRA). Then, we re-evaluate the baseline semantic segmentation model (trained on 8 vanilla generators from the main text, which just saw SDXL-edited images during training). In this setup, edits made by SDXLLoRA are considered correctly predicted if classified as SDXL, allowing us to evaluate the segmentation models generalization ability to LoRA-based variants. As shown in Tab. 7, the segmentation model exhibits strong generalization to LoRA variants, with only minor mIoU drop of approximately 0.03. Table 7. Effect of LoRA on localization performance. We evaluate semantic segmentation model Deeplabv3+, trained on SDXLedited images and validated on SDXLand SDXL-LoRA-edited images. We report mIoU only on masks from SDXL or SDXLLoRA edits to isolate LoRAs impact. Gray colors the baseline. Deeplab v3+ [8] Edit model (in val) mIoU (SDXL) SDXL SDXL-LoRA 0.657 0.630 Table 8. Performance of Deeplab v3+ on realand AIbased images. We report binary segmentation (localizing edited regions) and semantic segmentation (localizing and attributing edits to specific diffusion models)."
        },
        {
            "title": "Subset",
            "content": "Binary segmentation mIoU bF1 Acc"
        },
        {
            "title": "Semantic segmentation\nAcc",
            "content": "mIoU bF1 Real-base AI-base 0.964 0.999 0.911 0.999 0.506 0.999 0.918 0. 0.731 0.760 0.339 0.516 8. Quality assessment To remove severely low-quality outputs generated by the automated editing framework, we employ Qwen2.5-VL [3] to evaluate the quality of each edited image. Fig. 7 shows two chain-of-thought demonstration examples provided to Qwen2.5-VL. Images receiving score below 3 are discarded, resulting in the removal of roughly 50% of the generated samples. This threshold is intentionally strict, leading to high rejection rate, but it ensures higher-quality resulting dataset. Fig. 8 shows typical low-quality edits, usually caused by insufficient diffusion editing capability or inaccurate masks. In the third example, we observe that Qwen2.5-VL occasionally assigns quality scores based on the entire image rather than the edited region, which may filter out some higher-quality edits. However, this does not harm the overall dataset quality, as our guiding principle is to err on the side of stricter filteringprefer removing few good samples rather than keeping any low-quality ones. 9. Segmentation on different base images Here we provide more detailed results on the realand AIbased subsets. As shown in Tab. 8, Deeplabv3+ performs generally better on AI-based images than on real-image bases. 10. Additional visualization of edited images Figs. 9 and 10 present additional visualizations of edited images from the DiffSeg30k dataset, showcasing the diversity of edit types and visual characteristics across different diffusion models. 13 Figure 7. Chain-of-thought examples for image quality assessment. Figure 8. Typical low quality edits and VLM ratings. 15 Figure 9. More examples in DiffSeg30k. For each pair of images, the left is the original image with red contours highlighting areas to be edited. The right is the editing results. 16 Figure 10. More examples in DiffSeg30k. For each pair of images, the left is the original image with red contours highlighting areas to be edited. The right is the editing results."
        }
    ],
    "affiliations": [
        "Show Lab, National University of Singapore",
        "South China University of Technology"
    ]
}