{
    "paper_title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT Distillation",
    "authors": [
        "Ruichen Zhang",
        "Rana Muhammad Shahroz Khan",
        "Zhen Tan",
        "Dawei Li",
        "Song Wang",
        "Tianlong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data-centric distillation, including data augmentation, selection, and mixing, offers a promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks a comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with a focus on in-distribution (IID) and out-of-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is shared in https://anonymous.4open.science/r/DC-COT-FF4C/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 9 5 7 8 1 . 5 0 5 2 : r The Quest for Efficient Reasoning: Data-Centric Benchmark to CoT Distillation Ruichen Zhang 1, Rana Muhammad Shahroz Khan1, Zhen Tan2, Dawei Li2, Song Wang3, Tianlong Chen1 1University of North Carolina at Chapel Hill,2Arizona State University, 3University of Virginia"
        },
        {
            "title": "Abstract",
            "content": "Data-centric distillation, including data augmentation, selection, and mixing, offers promising path to creating smaller, more efficient student Large Language Models (LLMs) that retain strong reasoning abilities. However, there still lacks comprehensive benchmark to systematically assess the effect of each distillation approach. This paper introduces DC-CoT, the first data-centric benchmark that investigates data manipulation in chain-of-thought (CoT) distillation from method, model and data perspectives. Utilizing various teacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student architectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of these data manipulations on student model performance across multiple reasoning datasets, with focus on in-distribution (IID) and outof-distribution (OOD) generalization, and cross-domain transfer. Our findings aim to provide actionable insights and establish best practices for optimizing CoT distillation through data-centric techniques, ultimately facilitating the development of more accessible and capable reasoning models. The dataset can be found here, while our code is shared in this repository."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) achieve strong reasoning performance when combined with chain-of-thought (CoT) prompting [66], but the best performance typically comes from expensive models with tens or hundreds of billions of parameters. To address it, knowledge distillation (KD) stands out to transfer reasoning skills to lighter students (e.g. 38 B) at low inference cost [19, 20, 45, 64]. Among various KD strategies for CoT [69, 55], data-centric methodssuch as augmentation, selection, and mixinghave gained popularity for being architecture-agnostic and cost-efficient [68]. However, systematic assessment is still lacking to evaluate the effectiveness of these data manipulation techniques. Figure 1: Overview of DC-CoT pipeline. To address this, building data-centric benchmark is essential. Such benchmark will provide clearer understanding of the performance of existing data-centric methods by systematically Equal contribution. Preprint. Under review. evaluating and answering fundamental questions, such as how to effectively synthesize, select, and mix various CoT samples to robustly boost the student models performance. Furthermore, datacentric benchmark will serve as valuable and controlled evaluation resource for future research and the development of new techniques in this area. In this work, we introduce DC-CoT, the first benchmark designed to investigate data-centric CoT distillation systematically, answering the following research questions from various perspectives: ❶ Method Perspective: How can various data-centric CoT distillation methods for LLMs be categorized, and what is their comparative performance in enhancing student model reasoning? ❷ Model Perspective: How do the relative sizes and architectures of teacher and student models influence the effectiveness of data-centric CoT distillation? ❸ Data Perspective: How do different data characteristics and settings, such as in-distribution (IID) versus out-of-distribution (OOD) data, easy-to-hard generalization, and data availability, impact the outcomes of Chain-of-Thought distillation? Regarding the Method Perspective, DC-CoT investigates various data manipulation strategies across three core axes: (i) Augmentation: We study techniques like reverse reasoning and question/answer rephrasing beyond vanilla CoT. (ii) Selection: We compare heuristics such as teacher-correct filtering, student-error prioritization, and LLM-based quality judges. (iii) Mixing: We explore blending CoT data based on length, domain, and teacher origin. To explore the Model Perspective, DCCoT incorporates diverse teacher models (e.g., GPT-4o, Claude 3.5, Gemini-1.5-Pro) and various open-source student model families and sizes (e.g., LLaMA, Qwen, Gemma at 3-8B parameters). To address the Data Perspective, evaluations are conducted across multiple reasoning datasets, specifically examining performance in in-distribution (IID) and out-of-distribution (OOD) settings. Through extensive experiments, we present key findings and insights guided by research questions across multiple perspectives. From the Method Perspective, we find data augmentation to be generally the most effective approach and provide fine-grained analyses for each manipulation method across task types. From the Model Perspective, we confirm the roles of compatibility and learnability, highlighting their non-trivial impact on distillation and explaining why certain teacherstudent pairs may fail. From the Data Perspective, we reveal distinct scaling behaviors across augmentation methods and quantify the generalization capabilities of student LLMs across datasets. All these insights will help guide future research toward more effective and efficient CoT distillation paradigms. To summarize, our contributions in this work are as follows: ① We present DC-CoT, unified, data-centric benchmark that explores data manipulation in distillation from method, model and data perspectives. ② We conduct extensive experiments across diverse teacherstudent pairs, tasks, and datasets, offering the first large-scale empirical overview of CoT distillation. ③ We distill actionable guidelinese.g., which augmentation boosts generalization, which filtering criterion balances quality and coverage, and when heterogeneous teacher mixtures helpthereby charting path toward smaller yet more capable reasoning models."
        },
        {
            "title": "2 Related Works",
            "content": "Reasoning in LLMs. Chain-of-Thought (CoT) elicits explicit intermediate reasoning steps, making LLM inference more transparent and markedly more accurate on multi-step tasks [66, 27]. Based on this, newer long-CoT methodse.g., Tree-of-Thought, iterative self-reflection, and self-correctionscale CoT by exploring multiple paths and refining answers through critique [72, 40, 76, 36]. Knowledge Distillation in LLMs. Knowledge distillation transfers the behaviour of large teacher LLM to smaller, cheaper student. Beyond the original soft-label paradigm [3, 19], recent work treats LLM-generated instructions, responses, and rationales as synthetic supervision for supervised fine-tuning or alignment tuning [26, 59, 50, 79, 63]. particularly effective variant is reasoning or chain-of-thought (CoT) distillation: instead of imitating only the final answer, the student is trained to follow the intermediate reasoning produced by the teacher, which has proved crucial when capacity or architectural gaps exist [21, 45, 29, 75]. Despite promising gains, the field still lacks principled guidance on (i) which teachers, (ii) which rationales, and (iii) what selection or mixing strategies yield 2 maximal benefit for given student, motivating more data-centric exploration of CoT distillation. More detailed related work is given in Appendix D."
        },
        {
            "title": "3 Methodology: A Data-Centric CoT Distillation Benchmark",
            "content": "3.1 Data-centric Manipulation The central theme of our DC-CoT benchmark is the systematic evaluation of datacentric manipulations applied to CoT exemplars for knowledge distillation. These manipulations encompass various strategic operations to transfer the initial dataset Dsource to the target dataset Dsource for small student model training, potentially guided by set of parameters or rules Θ: Dtarget = M(Dsource, Θ). In this work, DC-CoT is designed to deconstruct and analyze the impact of instantiating through three primary types of data-centric operations: ❶ Data Augmentation (Section 3.1.1), ❷ Data Filtering (Section 3.1.2), and ❸ Data Mixing (Section 3.1.3) 3.1.1 Data Augmentation Figure 2: Data-centric augmentation flow. Teacher CoT traces are independently transformed by four operations: Rephrase Question, Question Augmentation, Answer Augmentation, and Reverse Thinking. Data augmentation is crucial in CoT distillation by enriching and diversifying the training data (Dsource) available for the student model, to expose the student to various reasoning patterns, question formulations, and explanatory styles for enhancing their reasoning capabilities and generalization. Within the DC-CoT benchmark, we investigate several data augmentation strategies as follows: ❶ Question Rephrasing: This method, introduced in MetaMath [75], aims to increase question diversity by having the teacher LLM paraphrase an existing question Qi while preserving its underlying meaning and original answer j=1. Here Qi and Preph are the original question and rephrasing prompt. For each rephrased question ˆQj , the teacher generates CoT rationale ˆRj . one augmentation is retained if ˆAj matches the original answer. ❷ Question Augmentation: This strategy focuses on creating entirely new questions Qnew, to broaden the topical coverage or complexity of the training data, based on set of seed questions [30]: Qnew = (Q, PQA). PQA here is prompt for generating novel questions. After that, the same generation-then-filter process will be adopted to produce new answers and CoTs for the augmented questions, as we introduced in the Question Rephrasing method. = (Qi, Preph)}L and answer ˆAj : { ˆQj [75]. Given (Qi, ❸ Answer Augmentation: It involves prompting the teacher LLM to generate multiple diverse CoT rationales that all lead to the same correct ground-truth answer ) Dsource, and using CoT generation prompt PAA, the teacher model generates candidate rationales and answers as follows: ) = (Qi, PAA, temp)}L {(Rk ❹ Reverse Thinking Augmentation Reverse Thinking was introduced in the RevThink [6]. The goal is to enrich the data by generating forward CoT reasoning Rf , corresponding backward question Qb, and backward reasoning Rb. For each (Qi, Ai) Dsource we do the following: , Ak k=1. Figure 3: Reverse-Thinking augmentation pipeline: from each (question, answer) pair, generate forward reasoning, synthesize backward question with its reasoning, then keep only examples whose forward-backward chains pass consistency check. 3 Generate Forward Reasoning: Ri = (Qi, Pf ) for some prompt Pf . This is filtered to ensure that the outcome of Ri is the ground truth Ai. Generate Backward Question: Using prompt Pbq, the teacher generates question that inverts the original problem: Qi = (Qi, Ai, Pbq). Generate Backward Reasoning: The teacher then generates the CoT for this backward question: Ri = (Qi b, Pbr) for some prompt Pbr. Consistency Filtering: consistency check = (Qi, Ai, Qi b, Pcon) is performed for making sure the backward and the forward questions are related and consistent with each other [71]. Only consistency quadruplets (Qi, Ri b) where = 1 are retained. , Qi b, Ri b, Ri 3.1.2 Data Filtering Data filtering, or selection, is critical step applied to either initial source data Dsource or augmented data to create highquality training set Dtrain for the student model. Since not all CoT instances are equally beneficial, as some are noisy or incorrect, filtering aims to identify and retain the most valuable exemplars to optimize student learning. Our DC-CoT investigates the following data selection strategies: ❶ Filtering by Teacher Correctness: This strategy used in [20], retains CoT instances where the teachers final answer Ai matches the ground-truth answer : Dtarget = {(Qi, Ri, Ai)Ai = }. This ensures the student learns from CoTs lead to correct outcomes. ❷ Filtering by Student Error: This filtering strategy focuses student learning on its weaknesses by selecting instances where the student model yields an incorrect answer: Dtarget = {(Qi, Ri, Ai) ˆAi = }. This concentrated learning can focus on students underperformed areas. ❸ LLM-as-a-Judge Filtering: Inspired by I-SHEEP [37], this method uses an external LLM Ljudge to assess CoT instance quality based on criteria like coherence, correctness, and clarity, allowing for nuanced quality assessment [31, 32]: Scorei = Ljudge(Ai, Ri, Qi, Peval). Instances are retained if their score meets threshold τ , making the final dataset become: Dsource = {(Qi, Ri, Ai)Scorei τ }. Figure 4: Data-filtering pipeline in DC-CoT. teachergenerated CoT pool is refined through three selectors. 3.1.3 Data Mixing Beyond augmentation and selection, data mixing offers another avenue for data-centric manipulation in CoT distillation. This strategy involves strategically combining CoT instances from different distributions or with varying characteristics to create more diverse training dataset Dtarget for the student model. The core idea is that blend of reasoning styles, complexities, or teacher provenances can lead to student model with more robust and generalizable reasoning capabilities. ❶ Length-based CoT Mixing: Length-based mixing, introduced in [35], combines CoT examples of varying reasoning length to help bridge this learnability gap for smaller models and offers complexity for larger models. This mix, controlled by ratio α, aims to provide balanced curriculum, exposing students to detailed and concise reasoning. ❷ Teacher-based CoT Mixing: This method blends CoTs generated by different teachers [35]. The mixed dataset is again guided by ratio α, providing balanced set of reasoning examples and preventing smaller students from being overwhelmed while still offering sophisticated examples. 3.2 Tasks Textual Reasoning: It assesses models ability to make logical inferences from text, often through multi-step reasoning. Each instance includes question Q, rationale R, and answer A. The student model Sθ learns to predict using and R. Tasks span commonsense, science, math, and table reasoning, with performance measured by answer accuracy. 4 Table 1: Accuracy of augmentation, selection and mixing strategies on Llama-3.1-8B. Textual scores are the mean of three independent runs. Agentic Visual Textual AVG. WebArena Visuan-CoT SQA CSQA ARC MATH GSM8K ANLI Data Augmentation Date Visual+Agentic Textual Zero Shot Zero Shot CoT No CoT Vanilla CoT Rephrase Question Question Aug Answer Aug Reverse Thinking No Selection Filtering with Teacher Filtering with Student Judge LLM No Mixing Length Mixing Teacher Mixing 5.66 8.25 30.05 22.78 - - - - 22.78 14.66 27.59 15.64 22.78 - 21.18 42.10 44.52 46.66 45.44 - - - - 44.52 45.50 45.90 46.54 44.52 - 45.48 57.64 65.55 59.89 58.08 59.73 60.40 64.49 72. 59.89 61.43 60.29 54.83 59.89 58.58 56.75 43.08 53.56 65.36 69.37 62.95 61.47 64.57 78.46 48.46 67.41 60.41 55.63 67.01 70.37 81.61 82.17 Data Selection 65.36 70.72 70.85 62. 60.41 62.86 60.30 57.46 Data Mixing 65.36 68.04 66.94 60.41 54.79 62.82 9.32 11.76 7.39 4.38 16.52 20.31 36.84 35.52 7.39 5.04 5.21 3. 7.39 4.64 5.96 19.64 21.00 20.74 24.30 38.86 44.03 53.48 76.35 20.74 30.27 26.97 22.72 20.74 21.84 19.57 33.83 39.92 35.42 23.92 42.47 41.26 40.29 49.75 35.42 24.11 25.40 26. 35.42 22.50 29.46 49.70 62.13 50.37 57.02 59.41 61.07 61.80 70.41 50.37 58.69 58.04 59.85 50.37 59.63 52.30 23.88 26.39 38.36 34.11 - - - - 33.65 30.08 36.75 31. 33.65 - 33.33 37.38 45.90 42.80 41.81 49.56 51.27 57.58 66.45 42.80 44.73 43.87 41.04 42.80 41.43 41.97 Agentic Reasoning: This task tests an LLM agent πθ in the WEBARENA browser sandbox, where it must follow an instruction by navigating real websites. At each step, the agent observes o, takes an action a, and explains its reasoning r. large LLM (ML) selects actions based on the interaction history. Performance is measured by Success Rate (SR)the fraction of tasks where the agent reaches the correct goal state. Visual Reasoning: Extends chain-of-thought to multi-modal inputs, requiring models to interpret visual content and answer related questions. Each instance is tuple (v, q, a, r): an image v, question q, an answer a, and rationale outlining reasoning steps linking to a. Unlike text-only reasoning, visual reasoning demands interpretable groundingr often points to specific image regions that justify the answer. This keeps the reasoning process transparent, testing the models ability to connect visual cues with logical steps across multiple reasoning hops."
        },
        {
            "title": "4 Experiment Result & Analysis",
            "content": "4.1 Benchmark Setup Teacher Models. We use state-of-the-art LLMs known for strong reasoning to generate CoT rationales: (1) Gemini-1.5-Pro [57], (2) GPT-4 [1], (3) Claude-3.5 Sonnet [2], (4) GPT-4.1 mini [48], (5) o4 mini [49]. Utilizing multiple teachers allows us to study the impact of teacher diversity. Data filtering is performed with task-specific Judge LLMs: LLama-2-70B for textual tasks, GPT-4o-mini [1] for agentic tasks, and GPT-4/4.1-mini for visual tasks. Student Models. We test these open-source models as students: (1) LLama-3.1-8B [15], (2) LLama3.1-8B-R1 Distilled [16], (3) Mistral-7B [23], (4) Gemma-7B [58], and (5) Qwen-2.5-7B [70]. Baselines. For Baseline comparison, we evaluate the models for (1) Zero Shot performance on the tasks, (2) Generate Zero-Shot CoT [27], (3) Fine-tune the model on the dataset without any CoT, and (4) Vanilla CoT generated by the teacher model with no augmentation/filtering/mixing. Datasets. Student performance is assessed on diverse reasoning datasets covering various skills and complexities. We evaluate textual reasoning tasks on: Commonsense Reasoning Tasks: StrategyQA (SQA; [14]), CommonsenseQA (CSQA; [54]), ARC-challenge (ARC; [9]). Math Reasoning: GSM8K (GSM8K; [10]), MATH (MATH; [18]). Natural Language Inference: ANLI (ANLI; [46]). Logical Reasoning: Date Understanding (Date; [53]). We evaluate agentic reasoning tasks on WEBARENA [84], and evaluate visual reasoning tasks on Visual-CoT [51] and OK-VQA [42]. We classify Shopping, Map, and Reddit as webarena-easy, and all others as webarena-hard. 4.2 Method-Level Results This section delves into the performance of various data-centric manipulation strategies by posing key questions and deriving insights from our experimental findings. The analysis primarily references Table 1. It is important to note that the results discussed in Table 1 all pertain to the Llama-3.1-8B student model. Furthermore, the teacher model for visual tasks was GPT-4-mini [1], for agentic tasks 5 Table 2: Reverse-augmented distillation results for different teacher / student combinations on textual tasks; numbers are three-run averages. Student Model Llama-3.1-8B Llama-3.1-8B-R1 Mistral-7B Gemma-7B Teacher Model Gemini-1.5-Pro GPT-4 Gemini-1.5-Pro GPT-4 Gemini-1.5-Pro GPT-4 Gemini-1.5-Pro GPTSQA 72.49 70.74 69.43 70.95 72.05 71.08 68.12 69.08 CSQA ARC MATH GSM8K ANLI 78.46 49.75 35.52 51.37 34.60 71.93 36.82 47.08 71.74 50.58 36.27 68.40 45.00 16.12 75.53 45.62 15.39 72.63 40.92 16.54 74.86 41.65 16.18 73.81 76.35 70.72 69.45 70.94 59.21 58.86 53.45 54.49 82.17 83.64 74.23 76.84 76.96 76.85 73.46 75.60 Date 70.41 68.51 70.41 67.80 59.17 60.19 31.36 30.57 AVG. 66.45 64.50 62.74 63.11 57.72 57.23 51.24 51. it was Claude-3.5 [2], and for textual tasks, Gemini-1.5-Pro-001 [57] was used. For the mixing, we use the models as described in Table 2 and 3. Q1: How do the broad categories of data-centric manipulation compare in terms of overall effectiveness? Table 1 shows that Data Augmentation strategies yield the most substantial average performance uplift over the Vanilla CoT baseline. For instance, Reverse improves average accuracy on all eight tasks by 24.64%. Filtering with Teacher Correctness (Textual Average: 44.7%) improves by +1.93 over Vanilla CoT. The best mixing strategy, Teacher Mixing (Textual Average: 41.97%), shows marginal decrease of 0.83% over Vanilla CoT. This confirms that for moderately sized student (7-8B), creating diverse rationales is more impactful than selecting or reshuffling existing ones. Data selection is vital for quality control, and data mixing helps tailor its composition. Q2: Which techniques are most effective for each data manipulation? From Table 1, Reverse consistently excels, especially for structure logical deduction (MATH, GSM8K, Date). It likely fosters deeper understanding by teaching bi-directional reasoning. Answer Augmentation also performs robustly, particularly for commonsense reasoning (SQA, CSQA), by exposing the student to varied solution paths, enhancing flexibility. While Question Augmentation and Rephrasing increase diversity, the more profound alterations from Reverse and Answer Augmentation generally yield larger gains. Among the selection techniques, LLM-as-a-Judge filtering is highly effective, often surpassing simpler heuristics due to its nuanced assessment of rationale quality (coherence, soundness) beyond mere answer correctness. However, filtering by Teacher Correctness is strong baseline, ensuring students learn from factually accurate paths and consistently improve over no selection or other methods. When compared to the No Mixing baseline, data mixing strategies show varied effects. Length Mixing (Average: 41.43%) results in slight decrease of 1.37% on average for textual tasks. However, while underperforming on others, it shows improvements on specific textual datasets like CSQA, GSM8K, and Date. Teacher Mixing also shows slight decrease of 0.83% on average for textual tasks compared to No Mixing. These results suggest that the benefits of the tested mixing strategies are not universally additive over strong No Mixing baseline for textual tasks on average, but they can offer advantages for specific datasets or modalities, likely by tailoring the data complexity or teacher style to particular student needs or task characteristics. Q3: Which data-centric methods show particular strengths for specific reasoning tasks?Optimal strategies vary by task demands, and combining effective augmentation with suitable filtering or mixing can yield further improvements: 1. Textual Reasoning (SQA, CSQA, ANLI): Answer Augmentation and Question Rephrasing enhance linguistic diversity. These should be combined with LLM-as-a-Judge filtering to ensure the high quality and coherence of the textual rationales. Teacher Mixing could also be beneficial after augmentation for tasks with varying teacher capabilities. 2. Mathematical Reasoning (GSM8K, MATH, Date): Reverse Thinking excels due to the need for backward deduction. Answer Augmentation is also valuable. These augmented datasets should then be rigorously filtered using Filtering by Teacher Correctness to eliminate any incorrect mathematical procedures. Subsequently, Length Mixing can be applied to balance the complexity of CoTs presented to the student. 3. Agentic Reasoning (WebArena): Given the complexity and potential for action chain errors, the augmented data should be curated using LLM-as-a-Judge filtering to enhance correctness. 4. Visual Reasoning (Visual-Cot): It is critical to use LLM-as-Judge filtering to ensure rationales are not only logically sound but also accurately reflect and reference the visual content. 6 4.3 Model-Level Results 4.3.1 Teacher Model Analysis We investigate the interplay between teacher and student models, summarized in Tables 2 and 3. For textual reasoning tasks, we utilize the best-performing augmentation approach, Reverse, and for visual as well as agentic tasks, we report the performance on vanilla CoT. Llama-3.1-8B Student Model Table 3: Impact of teacher model on agentic (WebArena) and visual (Visual-CoT) performance. Q4. How does the choice of teacher model impact the performance of different student models on textual reasoning tasks? Is there universally best teacher for all students? Table 2 reveals that for textual reasoning, stronger models like Gemini-1.5-Pro and GPT-4 generally yield better results when distilling to capable student models such as Llama-3-8.1 B. For instance, Llama-3.1-8B achieves high average textual score for both teachers, suggesting that as long as the teacher is powerful enough and the student has adequate capacity, transferring complex reasoning using Knowledge Distillation is quite effective. However, universally best teacher is not apparent. While Gemini-1.5 shows slight edge for LLama-3.1-8B on average, GPT-4 can be comparable or better on specific datasets (e.g., ARC for Llama-3.1-8B). For Mistral Gemini-1.5, it slightly outperforms GPT-4, whereas for Gemma-7B, GPT-4 is marginally better than the other. This variability indicates that optimal teacher-student pairings are nuanced, likely influenced by factors like architectural alignment or specific knowledge domains. Teacher Model WebArena Visual-CoT Claude-3.5 GPT-4o Claude-3.5 GPT-4o GPT-4 GPT-4-mini o4-mini - - - - 42.92 45.44 45.20 22.78 24.51 11.33 13.79 - - - Llama-3.1-8B-R1 Qwen-2.5-VL-3B Q5. What does performance on agentic and visual tasks indicate about teacher model suitability? Table 3, which examines agentic and visual tasks, provides strong support for the small model learnability gap. This concept suggests that smaller student models (e.g., 3B parameters) may not always benefit most from the largest available teachers, as they might learn more effectively from slightly smaller teachers whose reasoning complexity better matches their own capacity. Our results for the Qwen-2.5-VL3B student on Visual-CoT clearly demonstrate this: distillation from smaller, capable teachers like GPT-4-mini (45.44% acc.) and o4-mini (45.20% acc.) leads to superior performance when compared to the largest GPT-4 (42.92% acc.). This implies that the CoTs from very large models like GPT-4 might be overly complex for smaller, specialized model like Qwen-2.5-VL-3B to internalize effectively. The more digestible reasoning patterns of GPT-4-mini and o1-mini likely facilitate better knowledge transfer, highlighting that sheer teacher strength does not guarantee optimal distillation if the student struggles with the complexity. Figure 5: Qwen-2.5 (0.5B-7B) distilled with Vanilla-CoT vs Reverse Q6. Considering textual, agentic, and visual tasks, what general principles can be inferred for selecting an optimal teacher? Several interesting observations lead to emerging principles: 1. Student Capacity is Crucial. Larger student models generally leverage stronger teachers more effectively for complex textual reasoning due to their higher capacity to absorb intricate patterns. 2. The \"Learnability Gap\" Affects Smaller/Specialized Students. For smaller or specialized students, the strongest teacher is not always the best. teacher with more aligned reasoning complexity, even if smaller, can yield better results. 3. Students Prior Distillation History Impacts Receptiveness. The Llama-3.1-8B-R1 model, previously distilled from DeepSeek-R1, shows slightly lower average performance on textual tasks compared to base Llama-3.1-8B when further distilled by either Gemini-1.5-Pro or GPT-4. This suggests that students prior specializations or distillation experiences can hinder learning from new teachers if their strengths dont align, leading to less effective knowledge transfer. 7 4.3.2 Impact of Student Model Size This subsection examines how the scale of the student model influences the efficacy of CoT distillation, with particular focus on learnability from different augmentation strategies. The experiments, summarized in Figure 5, are performed on Qwen-2.5 models of varying sizes (0.5B, 1.5B, 3B, 7B) when distilled with Vanilla CoT and Reverse augmentation, using Gemini-1.5-Pro as the teacher. Q7. How does the student model size generally affect reasoning performance with standard Vanilla CoT, and how does it interact with more complex augmentations like Reverse? As shown in Figure 5, performance with Vanilla CoT clearly scales with student model size: Qwen-2.5-0.5B achieves an average of 32.86%, which improves to 45.72% for 1.5B, 50.89% for 3B, and 55.58% for the 7B model. This confirms that larger models better leverage standard teacher CoTs. The introduction of Reverse presents more nuanced picture. On average across all four textual tasks, the impact is mixed; the 1.5B model shows modest gain, while others see slight average decreases. However, these averages mask strong task-specific effects. Reverse significantly boosts performance on SQA and Date for all student sizes. Conversely, it markedly degraded performance on ARC and GSM8K compared to Vanilla CoT. This demonstrates that the utility of this complex augmentation is highly task-dependent in our specific student-teacher setup, instead of being universal benefit. Q8. Do smaller student models (0.5B, 1.5B) exhibit the small model learnability gap when faced with complex augmentations like Reverse? The small model learnability gap suggests smaller models struggle with overly complex reasoning. Analyzing our results: On tasks where Reverse is beneficial, smaller models (0.5B, 1.5B) achieve substantial gains. However, their absolute scores remain below those of larger students, indicating capacity limitation in reaching peak performance. Figure 6: Accuracy of different seed-data sizes and teachers for WebArena and Visual-CoT. 4.4 Data-Level Results 4.4.1 Effect of Data Volume We investigate the relationship between the volume of seed data used for distillation and the resulting student model performance, referencing Table 4 for the textual reasoning task with Gemini-1.5Pro as the teacher and Reverse augmentation, and Figure 6 for agentic and visual tasks with Claude 3.5 as the teacher and Vanilla CoT. Table 4: Performance of Llama-3.1-8B and Mistral-7B when varying the percentage of seed data. ARC GSM8K 48.46 19.64 Student Model Seed Data % Augmentation Type None Zero-Shot 25% Llama-3.1-8B 50% 75% 100% Vanilla CoT Reverse Vanilla CoT Reverse Vanilla CoT Reverse Vanilla CoT Reverse SQA 57.64 68.12 60.70 73.80 62. 67.69 68.12 58.08 72.49 79.95 77.82 80.12 79.95 71.78 80.79 55.34 82. Zero-Shot None 25% Vanilla CoT Reverse 55.02 50. 70.46 71.18 69.52 73.98 Mistral-7B 50% Vanilla CoT Reverse 64.91 68. 70.04 76.11 42.99 30.02 36.39 47.01 26.61 59.67 24.30 76.35 20. 44.09 54.13 39.25 53.90 Date 49.70 66.86 74.56 71.01 68.64 65.89 73. 59.41 70.41 AVG. 43.86 64.48 60.78 65.33 64.62 57.99 70.64 49.28 75. 46.75 43.24 63.58 62.72 61.91 65.50 58.53 64.59 58.18 65. 75% 100% 60.84 72.05 64.02 77.30 62.14 71.98 50.69 61. 26.94 54.44 50.95 66.28 Vanilla CoT Reverse Vanilla CoT Reverse Q9. How does increasing the percentage of seed data generally impact stu44.55 dent model performance for Vanilla 66.85 CoT and Reverse on textual tasks? How do these two methods compare? On textual tasks, increasing seed data for Vanilla CoT does not consistently yield linear performance improvements. For Llama-3.1-8B, Vanilla CoT performance peaks at 50% seed data, then declines. Mistral with Vanilla Cot shows similar non-linear trend, peaking earlier at 25%. This suggests that additional raw teacher traces might introduce noise or less informative examples beyond an optimal point, potentially hindering learning. In contrast, Reverse augmentation generally shows more consistent benefits with increased data. For both models, Reverse results in better performance at higher data volumes. This indicates that the richer signal from Reverse is more effectively leveraged as data volume increases. Furthermore, Reverse generally outperforms Vanilla CoT for textual reasoning, particularly as more seed data becomes available. This suggests that the benefits of sophisticated augmentation like Reverse become more pronounced with larger datasets. 51.40 76. 46.41 59.17 19.55 59.21 8 SQA ARC ANLI BoolQ ESNLI MATH OBQA GSM8K Testing Data GSM8K-Rev Training Data ACC. 54.75 64.16 74.58 81.60 49.74 59.75 16.74 38.89 Setting Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Q10. Does the \"more data always leads to better results\" scaling law hold true across these experiments? The traditional scaling law does not universally hold in our experiments. This is particularly evident for Vanilla CoT on textual tasks, where performance can degrade with excessive data. However, more data tends to be beneficial up to the tested volumes for more sophisticated augmentations like Reverse on textual data, and generally for agentic tasks. In our setup, performance seems to plateau quickly for visual tasks with increasing data volume. 4.4.2 Generalization Capability Analysis We investigate how well reasoning skills learned through CoT distillation on source dataset transfer to related but distinct target datasets. The analysis primarily references Table 4, which compares Zero-Shot performance on target task with the OOD performance achieved after fine-tuning on source task. For all experiments, we use Llama-3.1-8B as our student model. OOD Datasets were chosen as follows: BoolQ [8] was used for SQA, OBQA [43] for ARC, ESNLI [4] for ANLI, GSM8K-Rev [17] and MATH for GSM8K, GSM8K for MATH, Webarena-easy for Webarena-hard and vice versa, Ok-VQA for Visual-CoT and vice versa. Q11. How does fine-tuning on source dataset generTable 5: Zero-shot versus OOD fineally impact Out-of-Distribution (OOD) performance tuning accuracy with Llama-3.1-8B. compared to Zero-Shot performance on the target dataset? Table 5 consistently shows that fine-tuning on source dataset, even if different from the target, generally leads to substantial improvements in OOD performance on the target dataset compared to its Zero-Shot accuracy. For instance, after training on SQA, OOD performance on BoolQ improves from 54.75% to 64.16%. Similarly, training on ARC boosts OBQA performance. This trend holds across textual, mathematical, agentic, and even some visual task pairings, indicating that the reasoning skills learned via CoT distillation possess notable degree of transferability. 9.32 8.75 19.64 80.74 14.18 19.90 Q12. Are there specific task categories or pairings 2.44 11.95 where OOD generalization is particularly strong or 42.10 weak? Does fine-tuning on source task always guaran38.90 tee better OOD performance than its Zero-Shot coun44.52 44.62 terpart on the target task? The degree of generalization varies across task categories and specific pairings as observed in Table 5. Strong generalization is evident when transferring between similar textual reasoning tasks. For example, training on SQA significantly boosts BoolQ, and ARC training enhances OBQA performance. Mathematical reasoning also shows strong positive transfer, particularly when training on the more complex MATH dataset and testing on GSM8K, and also from GSM8K to its reversed version, GSM8K-Rev. Agentic tasks within WebArena also demonstrate good generalization across difficulty levels, such as training on \"easy\" instances improving \"hard\" instance performances from 2.44% to 11.95%. However, generalization can be mixed or weak in other scenarios. For instance, while MATH to GSM8K is strong, the reverse (GSM8K to MATH) shows decrease. Visual tasks also present varied results; training on OK-VQA improves Visual-Cot, but training on Visual-Cot leads to drop on OK-VQA. 5 Conclusion This paper addressed the critical challenge of transferring the powerful reasoning capabilities of large, computationally expensive Language Models to smaller, more accessible student models via Chainof-Thought (CoT) distillation, domain where the impact of data-centric strategies has been largely underexplored. We introduced DC-CoT, comprehensive benchmark designed to systematically investigate how data augmentation, selection, and mixing influence CoT distillation efficacy across diverse teacher-student pairs and reasoning tasks. Our findings reveal that data-centric manipulations significantly enhance distillation: data augmentation offers the most substantial performance gains by enriching reasoning trace diversity and complexity. Strategic data selection, using methods such as LLM-as-a-Judge or filtering by teacher correctness, is crucial for dataset quality. In contrast, data mixing provides nuanced benefits in tailoring data to student capacities and addressing learning gaps. By providing unified framework, empirical insights, and initial best practices, DC-CoT charts path towards developing more efficient yet potent reasoning models, paving the way for future explorations into more sophisticated data strategies and the interplay between data-centric and model-centric approaches to democratize advanced AI reasoning. Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Zero-Shot OOD Webarena-easy Webarena-hard Webarena-hard Webarena-easy Visual-CoT Visual-CoT OK-VQA OK-VQA GSM8K MATH"
        },
        {
            "title": "Acknowledgement",
            "content": "This work is generously supported by Amazon Research Award, Cisco Faculty Award, UNC Accelerating AI Awards, NAIRR Pilot Award, OpenAI Researcher Access Award, and Gemma Academic Program GCP Credit Award."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. Introducing claude 3.5 sonnet, June 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. Accessed: 2025-05-15. [3] Cristian Buciluˇa, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535541. ACM, 2006. [4] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31, 2018. [5] Justin Chen, Swarnadeep Saha, Elias Stengel-Eskin, and Mohit Bansal. MAGDi: Structured distillation of multi-agent interaction graphs improves reasoning in smaller language models. In Ruslan Salakhutdinov, Zico Kolter, Katherine Heller, Adrian Weller, Nuria Oliver, Jonathan Scarlett, and Felix Berkenkamp, editors, Proceedings of the 41st International Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 7220 7235. PMLR, 2127 Jul 2024. URL https://proceedings.mlr.press/v235/chen24ah. html. [6] Justin Chih-Yao Chen, Zifeng Wang, Hamid Palangi, Rujun Han, Sayna Ebrahimi, Long Le, Vincent Perot, Swaroop Mishra, Mohit Bansal, Chen-Yu Lee, et al. Reverse thinking makes llms stronger reasoners. arXiv preprint arXiv:2411.19865, 2024. [7] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. Advances in neural information processing systems, 33:2224322255, 2020. [8] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. [9] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. [10] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [11] Yangruibo Ding, Jinjun Peng, Marcus Min, Gail Kaiser, Junfeng Yang, and Baishakhi Ray. Semcoder: Training code language models with comprehensive semantics reasoning. Advances in Neural Information Processing Systems, 37:6027560308, 2024. [12] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. Advances in Neural Information Processing Systems, 36: 7029370332, 2023. 10 [13] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 1042110430. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/fu23d.html. [14] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. doi: 10.1162/ tacl_a_00370. URL https://aclanthology.org/2021.tacl-1.21/. [15] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [16] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [17] Pei Guo, WangJie You, Juntao Li, Yan Bowen, and Min Zhang. Exploring reversal mathematical reasoning ability for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 13671 13685, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.811. URL https://aclanthology.org/2024.findings-acl. 811/. [18] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [19] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. [20] Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. arXiv preprint arXiv:2212.10071, 2022. [21] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 80038017, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-acl.507. URL https://aclanthology.org/ 2023.findings-acl.507/. [22] Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 10511068, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.67. URL https://aclanthology.org/2023. emnlp-main.67/. [23] Albert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lelio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b. ArXiv, abs/2310.06825, 2023. URL https://api.semanticscholar.org/CorpusID:263830494. [24] Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, and Xiang Ren. Are machine rationales (not) useful to humans? measuring and improving human utility of free-text rationales. arXiv preprint arXiv:2305.07095, 2023. 11 [25] Martin Josifoski, Marija Sakota, Maxime Peyrard, and Robert West. Exploiting asymmetry for synthetic training data generation: SynthIE and the case of information extraction. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15551574, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.96. URL https://aclanthology.org/2023.emnlp-main.96/. [26] Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Donghyun Kwak, Kang Yoo, and Minjoon Seo. Aligning large language models through synthetic feedback. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1367713700, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.844. URL https://aclanthology.org/2023.emnlp-main.844/. [27] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [28] Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, et al. Measuring faithfulness in chain-of-thought reasoning. arXiv preprint arXiv:2307.13702, 2023. [29] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:38433857, 2022. [30] Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng Zhang, and Houwen Peng. Common 7b language models already possess strong math capabilities. arXiv preprint arXiv:2403.04706, 2024. [31] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. [32] Dawei Li, Renliang Sun, Yue Huang, Ming Zhong, Bohan Jiang, Jiawei Han, Xiangliang Zhang, Wei Wang, and Huan Liu. Preference leakage: contamination problem in llm-as-a-judge. arXiv preprint arXiv:2502.01534, 2025. [33] Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Symbolic chain-of-thought distillation: Small models can also think step-by-step. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 26652679, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.150. URL https://aclanthology.org/2023.acl-long.150/. [34] Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Anna Rogers, Jordan BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 53155333, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.291. URL https://aclanthology.org/2023.acl-long.291/. [35] Yuetai Li, Xiang Yue, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Bill Yuchen Lin, Bhaskar Ramasubramanian, and Radha Poovendran. Small models struggle to learn from strong reasoners. arXiv preprint arXiv:2502.12143, 2025. [36] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025. [37] Yiming Liang, Ge Zhang, Xingwei Qu, Tianyu Zheng, Jiawei Guo, Xinrun Du, Zhenzhu Yang, Jiaheng Liu, Chenghua Lin, Lei Ma, et al. I-sheep: Self-alignment of llm from scratch through an iterative self-enhancement paradigm. arXiv preprint arXiv:2408.08072, 2024. [38] Jianqiao Lu, Wanjun Zhong, Wenyong Huang, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Weichao Wang, Xingshan Zeng, Lifeng Shang, et al. Self: Self-evolution with language feedback. arXiv preprint arXiv:2310.00533, 2023. [39] Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. [40] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [41] Lucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. Teaching small language models to reason. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 17731781, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-short.151. URL https://aclanthology.org/2023.acl-short.151/. [42] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [43] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. In EMNLP, 2018. [44] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045, 2023. [45] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707, 2023. [46] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial NLI: new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 48854901, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.441. URL https://aclanthology.org/2020.acl-main.441/. [47] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. 2021. [48] OpenAI. Gpt-4.1, May 2024. URL https://openai.com/index/gpt-4-1/. Accessed: 2025-05-15. [49] OpenAI. Introducing o3 and o4 mini, May 2024. URL https://openai.com/index/ introducing-o3-and-o4-mini/. Accessed: 2025-05-15. [50] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [51] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. 13 [52] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [53] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. [54] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https:// aclanthology.org/N19-1421/. [55] Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data annotation and synthesis: survey. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 930957, 2024. [56] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. [57] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [58] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. [59] Yongqi Tong, Sizhe Wang, Dawei Li, Yifan Wang, Simeng Han, Zi Lin, Chengsong Huang, Jiaxin Huang, and Jingbo Shang. Optimizing language models reasoning abilities with weak supervision. arXiv preprint arXiv:2405.04086, 2024. [60] Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 27172739, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.153. URL https://aclanthology.org/2023.acl-long.153/. [61] Hongru Wang, Boyang Xue, Baohang Zhou, Tianhua Zhang, Cunxiang Wang, Huimin Wang, Guanhua Chen, and Kam-Fai Wong. Self-DC: When to reason and when to act? self divide-and-conquer for compositional unknown questions. In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 65106525, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https: //aclanthology.org/2025.naacl-long.331/. [62] Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. Pinto: Faithful language reasoning using prompt-generated rationales. arXiv preprint arXiv:2211.01562, 2022. [63] Sizhe Wang, Yongqi Tong, Hengyuan Zhang, Dawei Li, Xin Zhang, and Tianlong Chen. Bpo: Towards balanced preference optimization between knowledge breadth and depth in alignment. arXiv preprint arXiv:2411.10914, 2024. [64] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. 14 [65] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023. acl-long.754/. [66] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [67] Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 46024625, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.341. URL https://aclanthology.org/2022. naacl-main.341/. [68] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. [69] Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models. arXiv preprint arXiv:2402.13116, 2024. [70] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [71] Shiping Yang, Jie Wu, Wenbiao Ding, Ning Wu, Shining Liang, Ming Gong, Hengyuan Zhang, and Dongmei Zhang. Quantifying the robustness of retrieval-augmented language models against spurious features in grounding data. arXiv preprint arXiv:2503.05587, 2025. [72] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [73] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [74] Shan You, Chang Xu, Chao Xu, and Dacheng Tao. Learning from multiple teacher networks. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 17, page 12851294, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450348874. doi: 10.1145/3097983.3098135. URL https: //doi.org/10.1145/3097983.3098135. [75] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [76] Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Ziyi Yang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, et al. Chain-of-reasoning: Towards unified mathematical reasoning in large language models via multi-paradigm perspective. arXiv preprint arXiv:2501.11110, 2025. [77] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. 15 [78] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [79] Hengyuan Zhang, Yanru Wu, Dawei Li, Sak Yang, Rui Zhao, Yong Jiang, and Fei Tan. Balancing speciality and versatility: coarse to fine framework for supervised fine-tuning large language model. In Findings of the Association for Computational Linguistics ACL 2024, pages 7467 7509, 2024. [80] Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, et al. Huatuogpt, towards taming language model to be doctor. arXiv preprint arXiv:2305.15075, 2023. [81] Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng, Lingpeng Kong, and Qi Liu. Gimlet: unified graph-text model for instruction-based molecule zero-shot learning. Advances in neural information processing systems, 36:58505887, 2023. [82] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. [83] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403.13372. [84] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Limitations",
            "content": "Budget Constraints: Due to budget constraints, models like GPT-o4 were not included in our experiments. Moreover, migrating to other benchmarks also incurs substantial API costs. Therefore, for agentic task similar to many related papers Zhou et al. [84], we focus solely on the WEBARENA Zhou et al. [84] framework. However, our method is simple and efficient, without any benchmark-specific optimizations, making it easily transferable to other models. Hardware and Time Constraints: Extending distillation to more and larger models is highly challenging due to hardware and time limitations. Therefore, we selected some student models for our distillation experiments."
        },
        {
            "title": "B Broader Impact",
            "content": "The DC-CoT benchmark is poised to significantly impact AI by fostering the development of smaller, more accessible, and powerful reasoning models. By systematically evaluating data-centric CoT distillation strategies, DC-CoT offers crucial insights and standardized testbed, steering research towards resource-efficient AI and enabling advanced reasoning in computationally constrained environments. This research can yield broad societal and technological benefits: 1. Democratization of AI: Lowering computational barriers allows wider access to innovate with state-of-the-art AI. 2. Educational Advancements: Accessible reasoning models can be integrated into educational tools, supporting personalized learning. 3. Application of AI: Broader deployment of reasoning AI can aid complex problem-solving in research, healthcare, finance, and other industries. The insights from DC-CoT will also guide practitioners in optimizing distillation pipelines, promoting data-aware and sustainable AI by reducing the computational footprint of large models. By facilitating more efficient reasoning systems, DC-CoT contributes to future of more equitably accessible and sustainably developed advanced AI."
        },
        {
            "title": "C Experiment Setting",
            "content": "C.1 Distillation Training We conduct the distillation training on 8 A100 GPUs and 16 A6000 GPUs, using LoRA fine-tuning for the student models. The LoRA rank we set is 32, and the lora alpha we set is 64. For an agentic task, the training process spans 5 epochs, with learning rate of 5 105 and context length of 10, 000. For the visual task, the training process spans 1 epoch, with learning rate of 5 105. The distillation methodology follows the guidelines provided in Llama Factory[83]. For Textual tasks, we train for 10 epochs for each dataset. C.2 Inference Pipeline For inference, we employ the vLLM framework, running on 8 A100 GPUs. The WEBARENA framework is deployed on 4 CPU machines. To enhance efficiency, we leverage the official taskparallel Bash script for parallel execution, rather than processing tasks sequentially by task ID."
        },
        {
            "title": "D Related Work",
            "content": "D.1 Reasoning in LLMs The ability of LLMs to perform complex reasoning has been significantly enhanced by techniques that encourage explicit, step-by-step thinking. Foremost among these is Chain-of-Thought (CoT) prompting [66, 27, 47], which elicits intermediate reasoning steps from LLMs before arriving at final answer. This approach makes the models inference process more transparent by providing 17 human-readable explanations [24, 28] and substantially improves performance on tasks requiring multi-step deduction, such as arithmetic, commonsense, and symbolic reasoning [66]. By breaking down complex problems into manageable intermediate computations, CoT helps LLMs navigate intricate logical pathways and arrive at more accurate conclusions [39, 60, 12]. Integrating selfgenerated rationales through CoT effectively boosts the reasoning capabilities inherent in these models [27]. Building upon the foundational CoT paradigm, recent research has explored more sophisticated \"deep-thinking\" or \"long-CoT\" approaches to push the boundaries of LLM reasoning further. These methods often involve generating more elaborate or structured reasoning pathways. For example, Tree-of-Thought [72] prompting allows models to explore multiple reasoning paths in parallel, evaluating intermediate thoughts to decide the most promising direction. Other techniques focus on iterative refinement [64] and self-correction, such as Self-Reflection [40, 73, 52], where models critique and improve their own generated thoughts.improve their own generated thoughts. D.2 Knowledge Distillation in LLMs Knowledge distillation is potent technique for transferring knowledge from large, often cumbersome, \"teacher\" model to smaller, more efficient \"student\" model. This process is increasingly relevant in the context of LLMs due to their substantial size and computational demands. The fundamental concept, as introduced in early works [3, 19], involves training the student model to mimic the teacher models output distribution (soft labels), thereby minimizing the divergence between their respective distributions. This approach has found applications across various tuning techniques for LLMs. For instance, LLM-generated annotations, including instructions, responses, and rationales, are leveraged in supervised fine-tuning, i.e., where smaller model learns from the synthetic data produced by larger teacher LLM [26, 59, 22, 65, 61, 38]. This is particularly useful for enhancing specific capabilities [25, 80, 81] or imparting domain-specific knowledge efficiently [56, 68, 82, 62]. Furthermore, distillation techniques are employed in alignment tuning. One example includes Reinforcement Learning from Human Feedback (RLHF) [50], where synthetic data from LLMs can aid in reward modeling and policy training to align model outputs with human preferences and intentions. While classical knowledge distillation learns from the teacher models distributions, and the objective is to minimize the difference between the students distribution and the teachers [7], recent advancements in LLMs have brought particular focus to distilling their complex reasoning capabilities, especially CoT processes, into smaller student models [27]. CoT is also crucial when addressing architectural differences or significant capacity gaps between teacher and student LLMs, as merely mimicking the final output might be insufficient for the student to learn effectively. Teacher models provide CoT rationales in various ways: (1) Sampled directly from the teacher [21, 13, 33, 67, 41, 45, 44], (2) Generated via bootstrapping [34, 11, 78, 29, 75, 30, 77, 17, 6], or (3) Obtained via multiple teacher models [74, 5]. The rationale, reflecting the detailed thought process and reasoning pathway, serves as valuable auxiliary information for the student model to predict the final answer more accurately and robustly. While CoT distillation shows promise [45, 20], it remains unclear which methods, teacher models are most effective for specific student model and how they perform in various settings. This calls for data-centric study of how the generation, selection, and combination of distillation data impact student reasoning and generalization."
        },
        {
            "title": "E Thinking Example",
            "content": "Instruction : You need to issue an action,interaction history summary for this step. When you thinking need have OBSERVATION DESCRIPTION, OBSERVATION HIGHLIGHT, REASON. You are ONLY allowed to use the following action commands. Strictly adheres to the given format. {click [id]: To click on an element with its numerical ID on the webpage. E.g., `click [7]` ; type [id] [content] [press_enter_after=01]: To type content into field with specific ID. By default, the \"Enter\" key is pressed after typing unless `press_enter_after` is set to 0. E.g., `type [15] [ Carnegie Mellon University] [1]` .; stop [answer]: To stop interaction and return response. Present your answer within the brackets. If the task doesn't require textual answer or appears insurmountable, additional reasons and all relevant information you gather as the answer. E.g., `stop [N/A ...]`. ; note [content]: To take note of all important info w.r.t. completing the task to enable reviewing it later. E.g., `note [Spent $10 on 4/1/2024]`; go_back: To return to the previously viewed page. Input : OBJECTIVE: Which US states border Vermont? LASTACTION: first step, no last action LAST INTERACTION HISTORY SUMMARY: This is the initial interaction. We are on the OpenStreetMap homepage and need to search for Vermont to begin our task of identifying its bordering states. CURRENT OBSERVATION: RootWebArea \"OpenStreetMap\" link [41] 'OpenStreetMap logo OpenStreetMap' link [402] 'Edit' button [403] link [373] 'History' link [374] 'Export' link [407] 'GPS Traces' link [408] 'User Diaries' link [409] 'Communities' link [410] 'Copyright' link [411] 'Help' link [412] 'About' link [382] 'Log In' link [383] 'Sign Up' link [515] 'Where is this?' textbox [35] 'Search' [required: False] button [516] 'Go' link [503] 'Find directions between two points' heading 'Welcome to OpenStreetMap!' button [469] 'Close' text 'OpenStreetMap is map of the world, created by people like you and free to use under an open license.' text 'Hosting is supported by' 19 link [475] 'UCL' link [477] 'Fastly' link [479] 'Bytemark Hosting' text ', and other' link [481] 'partners' link [485] 'Learn More' link [486] 'Start Mapping' generic [15] 'Zoom In Zoom Out Show My Location Layers Share 50 km 50 mi OpenStreetMap contributors Make Donation. Website and API terms' link [19] 'Zoom In' link [20] 'Zoom Out' button [22] 'Show My Location' link [24] 'Layers' link [296] link [26] 'Share' link [309] 'OpenStreetMap contributors' link [311] 'Make Donation' link [313] 'Website and API terms' Output : \"<think> OBSERVATION DESCRIPTION: The current page is the OpenStreetMap homepage. It contains search box (ID: 35) where we can enter our query. There are various navigation links and buttons, but the most relevant element for our task is the search functionality. The map view (ID: 15) is also present, which will be crucial for visualizing Vermont and its neighboring states once we perform the search. OBSERVATION HIGHLIGHT: 35, 15, 516, 19, 20, 24, 26 REASON: To locate Vermont on the map, we need to use the search functionality. </think> ACTION: type [35] [Vermont] [1] INTERACTION HISTORY SUMMARY: We have searched for \"Vermont\" on OpenStreetMap and are now looking at the search results page with map view of Vermont.\""
        },
        {
            "title": "F Dataset Statistics",
            "content": "Dataset Domain License Train (Original) Train (Filtered) Test Commonsense SQA [14] Commonsense CSQA [54] Commonsense ARC [9] BoolQ [8] Commonsense OpenbookQA [43] Commonsense MATH [18] GSM8K [10] GSM8K-Rev [17] ANLI (r3) [46] e-SNLI [4] Date [53] Webarena [84] Visual-CoT [51] OK-VQA [42] Math Math Math NLI NLI Logic Agentic Visual Visual MIT MIT CC BY-SA 4.0 CC BY-SA 3.0 Apache MIT MIT Apache CC BY-NC 4.0 CC BY-NC 4.0 Apache Apache Apache CC BY 4.0 2,061 9,741 1,199 9,427 4957 7,500 7,379 - 100,459 549,367 - - 132,000 5,046 1,544 6,478 1,035 0 0 2,511 4,293 0 883 0 200 0 943,000 9,009 229 1,140 1,172 3,270 500 5,000 1,339 777 1,200 9,824 169 812 12,500 5, Table 6: The datasets used in our Experimental Setup."
        }
    ],
    "affiliations": [
        "Arizona State University",
        "University of North Carolina at Chapel Hill",
        "University of Virginia"
    ]
}