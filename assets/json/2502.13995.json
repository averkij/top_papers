{
    "paper_title": "FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation",
    "authors": [
        "Yunpeng Zhang",
        "Qiang Wang",
        "Fan Jiang",
        "Yaqi Fan",
        "Mu Xu",
        "Yonggang Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Tuning-free approaches adapting large-scale pre-trained video diffusion models for identity-preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present a novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, a multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing cross-attention to inject guidance cues into DiT layers, a learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our model's superiority over the current tuning-free IPT2V methods."
        },
        {
            "title": "Start",
            "content": "FantasyID: Face Knowledge Enhanced ID-Preserving Video Generation Yunpeng Zhang1,2 , Qiang Wang1 , Fan Jiang1 , Yaqi Fan2 , Mu Xu1 , Yonggang Qi2 1AMAP, Alibaba Group 2Beijing University of Posts and Telecommunications {yijing.wq, frank.jf, xumu.xm}@alibaba-inc.com, {bryan2233, yqfan, qiyg}@bupt.edu.cn 5 2 0 2 9 1 ] . [ 1 5 9 9 3 1 . 2 0 5 2 : r Figure 1: Examples of FantasyID. Given human face image, FantasyID generates ID-preserving videos with enhanced motion dynamics and more stable facial structures."
        },
        {
            "title": "Abstract",
            "content": "Tuning-free approaches adapting large-scale preidentitytrained video diffusion models for Equal Contribution Project Leaders Corresponding authors preserving text-to-video generation (IPT2V) have gained popularity recently due to their efficacy and scalability. However, significant challenges remain to achieve satisfied facial dynamics while keeping the identity unchanged. In this work, we present novel tuning-free IPT2V framework by enhancing face knowledge of the pre-trained video model built on diffusion transformers (DiT), dubbed FantasyID. Essentially, 3D facial geometry prior is incorporated to ensure plausible facial structures during video synthesis. To prevent the model from learning copy-paste shortcuts that simply replicate reference face across frames, multi-view face augmentation strategy is devised to capture diverse 2D facial appearance features, hence increasing the dynamics over the facial expressions and head poses. Additionally, after blending the 2D and 3D features as guidance, instead of naively employing adapter to inject guidance cues into DiT layers, learnable layer-aware adaptive mechanism is employed to selectively inject the fused features into each individual DiT layers, facilitating balanced modeling of identity preservation and motion dynamics. Experimental results validate our models superiority over the current tuning-free IPT2V methods. Our project page: https://fantasy-amap.github. io/fantasy-id/."
        },
        {
            "title": "1 Introduction\nIdentity-preserving text-to-video generation (IPT2V) aims to\ngenerate videos from a reference image while consistently\nmaintaining the identity across frames [44, 34, 40, 42]. Solv-\ning this problem provides valuable insights into developing\ncompelling applications, such as personalized avatars, im-\nmersive try-ons, interactive storytelling, and more. Upon\nthe powerful generative capability of large-scale pre-trained\nvideo diffusion models, recent works on IPT2V, such as ID-\nAnimator [37] and ConsisID [47], shifted to model adapta-\ntion, thus avoiding case-by-case tuning during inference.",
            "content": "Despite notable advancements, current identity-preserving video diffusion models face three critical challenges rooted in their architectural and training paradigms. First, current methods exhibit limited knowledge of facial structures, making them vulnerable when confronted with intricate facial movements. Besides, tuning pre-trained T2V models with single-view reference face may encounter the copy-paste issue [42, 37], that excessive reliance on monocular static image could restrict the desired diversity of facial expressions in the video. Moreover, the intrinsic hierarchical nature of DiT causes layer-specific sensitivity to control signals [46, 47], calling for dedicated conditioning strategies to harmonize identity preservation and temporal coherence throughout generation. To tackle the first challenge, we propose integrating 3D facial geometry priors into our model to ensure stable and consistent structures of the human face during generation. Specifically, DECA [14] is employed to extract the essential identity-specific 3D feature, i.e., shape point cloud, The which is found effective for identity preservation. identity-irrelevant features, e.g., pose and expression, are discarded. Moreover, this introduced 3D shape prior is conveniently manageable by varying the 3D points locations so the generated human face could change accordingly. To mitigate the issue of static motion, we devise multiview face adaptation strategy to avoid learning shortcuts that directly replicate the static face across frames in the generated video. Namely, we augment monocular reference face image with its variants from different viewpoints, forming face pool for the same identity obtained from the training human video. We then randomly select any of them as input for pre-trained video model adaptation. It turns out that this can enforce the adapted IPT2V model to capture detailed diverse 2D facial appearance features, thereby improving the dynamics performance of the generated video. Given the obtained 3D and 2D facial features from the reference face image, we further blend them together using transformer-based feature fusion module to guide the pretrained video diffusion model to produce identity-specific human video. However, we figure that it is inefficient to inject the fused feature into the pre-trained DiT layers naively by adapter as it is known that the DiT lower layers tend to capture the overall structure and upper layers for details. Therefore, we introduce layer-aware injection mechanism to allow the model to adaptively select the most beneficial cues from the fused features. To this end, our contributions are three-fold. (i) To our best knowledge, this is the first attempt that 3D facial priors extracted from single-view reference image are employed to enhance the facial structure stability, thus benefiting ID preservation throughout video generation. (ii) By employing multi-view facial augmentation strategy, we can significantly enhance the perception of 2D facial appearance across wide range of viewpoints, thus benefiting the motion dynamics associated with facial expression and head pose. (iii) learnable layer-aware feature guidance mechanism is devised to facilitate precise control for balanced IDpreserving and dynamics modeling, offering high-fidelity human video with better temporal coherence and identity consistency."
        },
        {
            "title": "2 Related Work",
            "content": "Personalized Diffusion Models. Recent advancements in identity-preserving image generation have seen rapid development, with several approaches employing Textual Inversion [18], DreamBooth [28], and LoRA [15] for fine-tuning models on specific IDs, achieving impressive results. However, these methods lack flexibility and the capability for real-time inference. In contrast, training-free methods effectively eliminate the dependency on parameter fine-tuning. For instance, the IP-Adapter [16] leverages CLIP features [16] to guide pre-trained models, ensuring identity preservation. PuLID [35] adopts EvaClip [29] to maintain identity consistency, while InstantID [43] integrates ArcFace [7] with pose networks to achieve flexible ID retention. Within the realm of identity-preserving video generation, maintaining smooth character motion alongside accurate preservation of identity features represents key challenge. The ID-Animator [37], built upon the AnimateDiff [24] base model, has successfully preserved identity characteristics but exhibits noticeable limitations in the fluidity of character movements. The emerging DiT architecture [45, 48] shows promise for enhancing Figure 2: Overview of FantasyID. The framework constructs multi-view face collection, randomly selects one face as the reference input, and employs face abstractor to extract 2D visual tokens while using DECA to extract 3D face vertex tokens. We fuse both the 2D and 3D tokens with fusion transformer layers and guide DiT-based model via layer-aware signal injection method. consistency in video output. ConsisID [47], which builds on CogVideoX[45], employs frequency-aware control scheme to enhance identity consistency without the need for identityspecific tuning. However, existing methods still face challenges such as low motion amplitude and facial instability during movement, which can result in limited dynamic movements and unnatural changes or distortions in facial features across frames. Portrait Animation. Portrait animation techniques have made significant strides in animating static images by mapping motions and expressions from guiding video while preserving the portraits identity and background. [19, 6, 12, 26, 17] Research primarily focuses on 3D morphable models [22], such as DECA [13] and FLAME [4], which excel in detailed 3D face modeling but largely concentrate on facial features without extending to full-body animations or scene elements. In rendering, volumetric methods provide high detail but are computationally intense, while CVTHead [39] offers more efficient, yet still facially focused, approach. Animation methods like EchoMimic [33], which relies on Mediapipe [9], and SadTalker [31], which uses audio inputs to generate 3D motion coefficients, also emphasize facial regions. Despite their advancements, these methods generally lack the ability to generate or control complete scenes through text-based inputs, highlighting gap in creating broader narrative or environmental elements through such interactions."
        },
        {
            "title": "3 Methodology\nGiven a reference face image, FantasyID is designed to gen-\nerate a video that faithfully preserves the individual’s iden-\ntity characteristics. An overview of FantasyID is illustrated\nin Figure 2. For each training video, we construct a multi-\nview face collection and randomly select a reference image\nas the input condition. Then, we utilize face abstractor to ex-\ntract 2D clip tokens (Sec. 3.2), employ DECA to disentangle",
            "content": "features unrelated to the core ID (such as expressions, pose) and to extract 3D structural information (Sec. 3.3), and use fusion transformer to fuse the 2D tokens and 3D tokens into face descriptor embeddings (Sec. 3.4). Additionally, we exert control over the DiT-based model by employing layer-aware signal injection method, ensuring precise modulation at each layer (Sec. 3.5). The following section (Sec. 3.1) elaborates on the diffusion model and the preliminaries of our method."
        },
        {
            "title": "3.1 Preliminary\nLatent Diffusion Models. Latent diffusion models are effi-\ncient diffusion models that operate in the latent space rather\nthan the pixel space. We use an encoder ε from a pre-trained\nvariational autoencoder to compress video data x into a latent\ncode z = ε(x). The encoder ε is a video compression mod-\nule based on 3D variational autoencoders [30]. It incorporates\nthree-dimensional convolutions to compress videos both spa-\ntially and temporally. During the diffusion stage, Gaussian\n1 − αtϵ, with\nnoise ϵ is added to z to create zt =\nϵ ∼ N (0, I), over T stages. Here, αt serves as the noise\nscheduler, while t represents the timestep. The denoising\nprocess employs the conditional probability pθ(zt−1|zt) =\nN (µθ(zt), Σθ(zt)) to predict the previous state zt−1. Here,\nµθ implemented using a denoising model ϵθ, while Σθ rep-\nresents the learned covariance of the reverse diffusion pro-\ncess. The training objective typically involves a reconstruc-\ntion loss that aims to minimize the discrepancy between the\nadded noise and the network’s predicted noise:",
            "content": "αtz + = Et,zp(z),ϵN (0,I) (cid:2)ϵθ(zt, t) ϵ2 (cid:3) (1) Diffusion Transformer(DiT). The Diffusion Transformer [27] is transformer-based architecture designed for efficient denoising in latent diffusion models. In contrast to to UNet [2] for processing spatiotemporal latent representations,DiTbased models demonstrate superior capabilities in modeling long-range dependencies and ensuring cross-temporal coherence. This has led to significant advancements in motion coherence and overall video quality [45, 38, 48]. For our denoising model ϵθ, we opted for the MM-DiT from CogVideoX [45]."
        },
        {
            "title": "3.2 Multi-View Collection and Face Abstractor\nMulti-View Face Collection. Obtaining effective ID refer-\nences during the training stage is crucial. To ensure the model\nfocuses on critical areas, we first crop the facial region from\neach frame of the video, eliminating the background distrac-\ntions. Following MovieGen [42], a single reference image\ncan lead the model to learn shortcuts that involve directly\nreplicating the face, so we have constructed a collection of\nface images I from different viewpoints in the training stage.\nWe utilize RetinaFace [11] to extract geometric relationships\namong facial landmarks to calculate the head pose angles and\nselect six images with the most significant viewpoint differ-\nences to form a multi-view face set. By providing the model\nwith a diverse range of perspectives, it gains a more compre-\nhensive understanding of the subject, thereby enhancing its\nability to maintain identity consistency across various poses\nand expressions.\nFace Abstractor. After acquiring the face image from the\nmulti-view face collection, we aim to effectively extract fa-\ncial features from it. Previous works [47, 36] on identity-\npreserving generation has employed Q-Former [25] to trans-\nform the face clip embeddings, but this approach can disrupt\nthe spatial structure among features [32]. Recognizing the\ncritical importance of local correlations for face characteris-\ntics, we introduce the use of C-Abstractor [32] to transform\nface clip embeddings to Xf ∈ Rh×w×C. This module, com-\nposed of two-dimensional convolutions and average pooling,\nleverages spatial locality to enhance feature extraction. By\neffectively capturing comprehensive facial information while\npreserving the key spatial relationships essential for accurate\nvideo generation.",
            "content": "3D Constraints"
        },
        {
            "title": "3.3\nFace Vertex. A stable facial geometric constraint is critical\nto ensuring high-quality generation. We utilize the priors ob-\ntained from 3D reconstruction to both constrain and enhance\nthe model’s understanding of the reference image, thereby\nimproving the quality and fidelity of the generated output.\nWe employ the DECA framework [14] to capture the three-\ndimensional geometric structure of faces, which provides ver-\ntex coordinates Vs ∈ R(N ×3) for the reference image, where\nN is the number of vertices in FLAME model [4]. This ap-\nproach distinguishes between intrinsic facial features and ex-\ntrinsic factors such as pose, expression, and lighting. This\nseparation mechanism enhances the model’s comprehension\nof identity-specific features while effectively suppressing in-\nterference from non-identity characteristics.\n3D Vertex Representation. We employ SpiralNet++ [8] to\nV ∈ RN ′×C′\nextract 3D features from Vs, represented as X ′\n,\nwhere N ′ denotes the number of vertices after downsampling\nand C ′ indicates the channel dimension of the feature de-\nscriptor. To encode positional information from the 3D point\ncloud, we incorporate a positional encoding Edep, derived",
            "content": "from the depth map of the projected vertices. This results in the enhanced vertex features is XV = + Edep."
        },
        {
            "title": "3.5 Layer-Aware Control Signal Injection\nInspired by the observation that each layer in the DiT archi-\ntecture contributes uniquely to the overall performance [46],\nwe adopt a similar approach for controlling facial video gen-\neration using DiT. Specifically, we recognize that different\nlayers exhibit varying sensitivities to control signals. To ad-\ndress this, we propose a layer-aware control signal injection\nmechanism that dynamically adjusts the integration of control\nsignals based on the role of each layer.",
            "content": "Particularly, for each MM-DiT block, we employ lightweight model Fl to learn the optimal feature representation. This lightweight network comprises convolution block Independent weights for each followed by normalization. layer enhance fidelity and diversity, aligning control signals precisely with the needs of each layer. This ensures stability and expressive potential in outputs. The process is defined by the formula: ˆZl = Zl + Fl(Attention(l, Ql, id , id )) (2) where Ql, id , id ces of the attention operation, Ql = ZlW = VF id the layer index of the MM-DiT block, and trainable weights. are the query, key, and values matril = VF , . Here, Zl represents the hidden states, and is are , id , , v"
        },
        {
            "title": "4 Experiments\n4.1 Setups\nImplementation Details.\nIn our experiments, we utilize a\ndiverse dataset comprising full body and portrait data from\nConsisID-Data [47], CelebV-HQ[21], and Open-Vid[41].\nSubsequently, following the approach in SVD [23], we em-\nployed PaddleOCR [5, 20] to eliminate any videos contain-\ning subtitles. Furthermore, we used InsightFace [7, 10] to ex-\nclude videos with a face confidence score below 0.9, resulting\nin a final selection of approximately 50,000 clips. We opti-\nmize with a batch size of 16 and a learning rate of 3 × 10−6,",
            "content": "Figure 3: Qualitiative Comparision between our methods and ConsisID, ID-Animator. Please refer our supplementary materials for the video results. completing 90,000 training steps, which takes approximately 36 hours using 16 A100 GPUs. Our methodology incorporates classification-free guidance with random null text ratio of 0.1, utilizing AdamW as the optimizer, and employs cosine with restarts as the learning rate scheduler. During the inference stage, we utilize DECAs coarse FLAME [4] parameters to construct 3D point cloud from the input image. The denoising step is set to 50. The fusion transformer is designed to 6 layers, and the ResConv1D comprises 4 residual 1D convolutional blocks. The downsampling factor of the face abstractor is set to 4, and the number of 3D tokens is 314. Evaluation Metrics. We employ ArcFace [7] embedding similarity to assess two key aspects. First, Reference Similarity(RS) calculates the similarity between the reference image and frames to evaluate identity preservation. Second, InterFrame Similarity(IFS) measures the similarity between consecutive video frames to evaluate the stability of identity features during motion. Additionally, we analyze the Frechet Inception Distance (FID) [3] of the face region to assess video quality and utilize Face Motion (FM), measured by average dense optical flow [1], to evaluate the degree of motion. We used 50 richly detailed portrait reference images. To more accurately measure the identity preservation capability, we cropped the facial regions from each video for quantitative evaluation."
        },
        {
            "title": "4.2 Qualitative Analysis\nFor the qualitative evaluation, we present comparison results\nwith diffusion-based identity preservation models, ConsisID\nand ID-Animator. Other models, including VideoMaker [44]\nand MagicMirror [34], are not open-source and therefore not\nincluded in our direct comparisons.",
            "content": "Figure 3 demonstrates that ID-Animator struggles with generating human body parts beyond the face and exhibits noticeable copy-paste artifacts. Moreover, the generated content often appears overly static, lacking natural motion. These limitations significantly restrict its practical application in scenarios requiring dynamic and realistic human behavior or interactions. Regarding ConsisID, while the overall visual quality is high, there are still issues with structural instability during facial movements, as seen in Case 1. Although ConsisID retains features such as skin texture from the reference image, it fails to accurately reproduce the overall facial structure in Case 3 and 4. In contrast, our method achieves the best results in terms of visual quality, preservation of the subjects identity from the reference image, and maintaining consistent facial structure across frames during motion. To further validate the effectiveness of our proposed method, we conducted comprehensive user study involving 32 participants. Each participant was tasked with assessing four critical aspects: Overall Quality(OQ), Face Similarity(FSim), Facial Structure(F-Str), and Facial Dynamics(FD), rating each aspect on scale from 0 to 10. As shown in Table 1, the scores indicate that FantasyID consistently outperforms baseline methods across all evaluated dimensions, demonstrating its superior perceived quality in human assessments. ID-Animator ConsisID Ours OQ 4.38 7.85 8.39 F-Sim F-Str 6.20 7.79 8.68 5.82 6.44 8.10 FD 3.28 7.12 7.94 Table 1: User Study results. Higher scores indicate better performance."
        },
        {
            "title": "4.3 Quantitative Analysis\nTable 2 presents a comprehensive quantitative evaluation\nof various face video generation methods.\nID-Animator\nachieves an impressive FID score and a higher IFS score.\nHowever, this performance can be attributed to its tendency\nto generate more static content, thereby ensuring high quality\nand excellent identity consistency. This focus on static repre-\nsentations likely limits its ability to produce diverse and dy-\nnamic facial motions. In contrast, while our method provides\na slightly highest FID score, it excels in capturing dynamic\nexpressions, as evidenced by the leading face motion score of\n0.61, and achieves the highest RS score of 0.57, reflecting su-\nperior identity preservation. Notably, our model outperforms",
            "content": "Figure 4: Effect of 3D Constraints. By altering the facial widths and jawline sharpness of face vertex, the generated facial videos exhibit noticeable structural changes. ConsisID across all metrics, reflecting superior ability in dynamism and identity preservation. ID-Animator ConsisID Ours FID 138.27 149.70 142.50 RS 0.35 0.47 0.57 IFS 0.98 0.93 0.95 FM 0.18 0.54 0.61 Table 2: Quantitative evaluation of different methods. The best results are highlighted in bold."
        },
        {
            "title": "4.4 Ablation Studies\nTo comprehensively evaluate the contribution of each module\nwithin the FantasyID framework, we conducted a series of\nablation studies. These experiments systematically removed\nindividual components to assess their impact on the overall\nperformance of the model in Table 3. Specifically, we ex-\namined the effects of excluding the Multi-View Face Col-\nlection(MFC), Face Abstractor(FA), Face Vertex(FV), and\nLayer-Aware Control Signal Injection (LACSI). Additionally,\nwe modify the face vertex data of different inputs to validate\nthe effectiveness of 3D constraints.",
            "content": "w/o FA w/o MFC w/o FV w/o LACSI FantasyID FID 145.82 130.77 172.51 235.46 142.50 RS IFS FM 0.55 0.54 0.42 0.33 0.57 0.93 0.98 0.90 0.93 0.95 0.49 0.33 0.36 0.22 0.61 Table 3: Quantitative results of removing individual modules from FantasyID framework."
        },
        {
            "title": "4.5 Qualitative Analysis\nEffect of 3D Constraints. To verify the efficacy of our 3D\nconstraint control mechanism, we modify the 3D face ver-\ntex to generate videos with different facial widths and jaw-\nline sharpness. The qualitative results presented in Figure 4\nshowcase the significant variations in facial structure, thereby",
            "content": "Figure 5: Ablation study on Face Vertex(FV). Without the face vertex leading to distorted facial structures during motion. These results indicate that the critical role of 3D vertex integration in preserving structural integrity and ensuring smooth facial dynamics. w/o Multi-View Face Collection. We replace the multi-view face collection with single face image during the training stage. As shown in Figure 6, this approach significantly reduces the range of captured facial motions, limiting the models ability to understand and represent different angles. However, this approach achieves the best FID and IFS scores as shown in Table 3. This performance can be attributed to the models tendency to take shortcut by prioritizing higher similarity to the reference image, thereby maintaining consistency at the expense of dynamic range. w/o Face Abstractor. We replaced the face abstractor with Q-Former, which, as shown in Figure 6, leads to some facial distortions. These distortions are likely due to Q-Formers tendency to disrupt the spatial characteristics of face CLIP features. Additionally, the results presented in Table 2 indicate that this approach achieves lower FID, RS, and IFS scores. This suggests that Face Abstractor is more effective at capturing comprehensive and spatially coherent facial information. w/o Layer-Aware Control Signal Injection. By removing the layer-aware control signal injection module Fl, we observed significant decrease in face similarity, as shown in Figure 6, along with decline in all metric scores, as detailed in Table 3. These results indicate decline in both video quality and identity preserving. In contrast, the layeraware control method adapts more effectively to the unique feature distributions between different DiT blocks by learning the most suitable feature control signals for each layer. This approach ensures optimal performance and fidelity in generating ID features."
        },
        {
            "title": "5 Conclusion",
            "content": "FantasyID presents groundbreaking approach for identitypreserving human video generation, overcoming the limitations of traditional methods. By employing multi-view face Figure 6: Collction(MFC), Face Abstractor(FA) Aware Control Signal Injection(LACSI). Ablation studies on Multi-View Face and Layerconfirming that our 3D constraints effectively guide the generation of facial features. This demonstrates the flexibility and precision of our approach in controlling facial characteristics. w/o Face Vertex. We evaluated the importance of 3D constraints by excluding the face vertex of our framework. The qualitative results in Figure 5 demonstrates that the absence of 3D face vertex data causes the model to rely solely on 2D feature extraction, leading to distortions in facial structure during motion. The quantitative results in Table 2 show decline across all metrics, which suggest more erratic facial motion. collection, face abstractor, 3D constraints, and layer-aware control signal injection, it significantly enhances video quality, identity preserving, and temporal coherence. This scalable, training-free solution maintains high-fidelity representations during complex motions. Future work will focus on optimizing multi-identity retention and expanding FantasyIDs role in dynamic video production and personalized content creation. References [1] Gunnar Farneback. Two-frame motion estimation based on polynomial expansion. In: Image Analysis: 13th Scandinavian Conference, SCIA 2003 Halmstad, Sweden, June 29July 2, 2003 Proceedings 13. Springer. 2003, pp. 363370. [2] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In: Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18. Springer. 2015, pp. 234241. [3] Martin Heusel et al. Gans trained by two time-scale update rule converge to local nash equilibrium. In: Advances in neural information processing systems 30 (2017). [4] Tianye Li et al. Learning model of facial shape and expression from 4D scans. In: ACM Trans. Graph. 36.6 (2017), pp. 1941. [5] Xinyu Zhou et al. East: an efficient and accurate scene text detector. In: Proceedings of the IEEE conference on Computer Vision and Pattern Recognition. 2017, pp. 55515560. [6] Olivia Wiles, Koepke, and Andrew Zisserman. X2face: network for controlling face generation using images, audio, and pose codes. In: Proceedings of the European conference on computer vision (ECCV). 2018, pp. 670686. Jiankang Deng et al. Arcface: Additive angular margin loss for deep face recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019, pp. 46904699. [7] [8] Shunwang Gong et al. Spiralnet++: fast and highly efficient mesh convolution operator. In: Proceedings of the IEEE/CVF international conference on computer vision workshops. 2019, pp. 00. [9] Camillo Lugaresi et al. Mediapipe: framework for building perception pipelines. In: arXiv preprint arXiv:1906.08172 (2019). Jiankang Deng et al. RetinaFace: Single-Shot MultiLevel Face Localisation in the Wild. In: CVPR. 2020. Jiankang Deng et al. Retinaface: Single-shot multilevel face localisation in the wild. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020, pp. 52035212. [10] [11] [12] Guangming Yao et al. Mesh guided one-shot face reenactment using graph convolutional networks. In: Proceedings of the 28th ACM international conference on multimedia. 2020, pp. 17731781. [13] Yao Feng et al. Learning an Animatable Detailed 3D Face Model from In-The-Wild Images. In: vol. 40. 8. 2021. URL: https://doi.org/10.1145/3450626.3459936. [14] Yao Feng et al. Learning an animatable detailed 3D face model from in-the-wild images. In: ACM Transactions on Graphics (ToG) 40.4 (2021), pp. 113. [15] Edward Hu et al. Lora: Low-rank adaptation of large language models. In: arXiv preprint arXiv:2106.09685 (2021). [16] Alec Radford et al. Learning transferable visual models from natural language supervision. In: International conference on machine learning. PMLR. 2021, pp. 87488763. [17] Ting-Chun Wang, Arun Mallya, and Ming-Yu Liu. One-shot free-view neural talking-head synthesis for video conferencing. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021, pp. 1003910049. [18] Rinon Gal et al. An image is worth one word: Personalizing text-to-image generation using textual inversion. In: arXiv preprint arXiv:2208.01618 (2022). [19] Taras Khakhulin et al. Realistic one-shot mesh-based head avatars. In: European Conference on Computer Vision. Springer. 2022, pp. 345362. [20] Minghui Liao et al. Real-time scene text detection with differentiable binarization and adaptive scale fusion. In: IEEE transactions on pattern analysis and machine intelligence 45.1 (2022), pp. 919931. [21] Hao Zhu et al. CelebV-HQ: Large-Scale Video Facial Attributes Dataset. In: ECCV. 2022. [22] Volker Blanz and Thomas Vetter. morphable model for the synthesis of 3D faces. In: Seminal Graphics Papers: Pushing the Boundaries, Volume 2. 2023, pp. 157164. [23] Andreas Blattmann et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. In: arXiv preprint arXiv:2311.15127 (2023). [25] [24] Yuwei Guo et al. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. In: arXiv preprint arXiv:2307.04725 (2023). Junnan Li et al. Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models. In: International conference on machine learning. PMLR. 2023, pp. 1973019742. [26] Youxin Pang et al. Dpe: Disentanglement of pose and expression for general video portrait editing. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 427436. [27] William Peebles and Saining Xie. Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023, pp. 41954205. [28] Nataniel Ruiz et al. Dreambooth: Fine tuning text-toimage diffusion models for subject-driven generation. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023, pp. 22500 22510. [29] Quan Sun et al. Eva-clip: techniques for clip at scale. arXiv:2303.15389 (2023). Improved training In: arXiv preprint [30] Lijun Yu et al. Language Model Beats Diffusion Tokenizer is Key to Visual Generation. In: arXiv preprint arXiv:2310.05737 (2023). [31] Wenxuan Zhang et al. Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023, pp. 86528661. Junbum Cha et al. Honeybee: Locality-enhanced projector for multimodal llm. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024, pp. 1381713827. [32] [33] Zhiyuan Chen et al. Echomimic: Lifelike audiodriven portrait animations through editable landmark In: arXiv preprint arXiv:2407.08136 conditions. (2024). [47] Shenghai Yuan et al. Identity-Preserving Text-toVideo Generation by Frequency Decomposition. In: arXiv preprint arXiv:2411.17440 (2024). [48] Zangwei Zheng et al. Open-Sora: Democratizing Efficient Video Production for All. Mar. 2024. URL: https: //github.com/hpcaitech/Open-Sora. [34] Armand Comas-Massague MagicMirror: et and High-Quality Avatar Generation with In: arXiv preprint Fast Constrained Search Space. arXiv:2404.01296 (2024). al. [35] Zinan Guo et al. Pulid: Pure and lightning id customization via contrastive alignment. In: arXiv preprint arXiv:2404.16022 (2024). Junjie He, Yifeng Geng, and Liefeng Bo. UniPortrait: Unified Framework for Identity-Preserving Singleand Multi-Human Image Personalization. In: arXiv preprint arXiv:2408.05939 (2024). [36] [37] Xuanhua He et al. Id-animator: Zero-shot identityIn: arXiv preserving human video generation. preprint arXiv:2404.15275 (2024). [38] Weijie Kong et al. HunyuanVideo: Systematic Framework For Large Video Generative Models. In: arXiv preprint arXiv:2412.03603 (2024). [39] Haoyu Ma et al. CVTHead: One-shot Controllable Head Avatar with Vertex-feature Transformer. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2024, pp. 6131 6141. [40] Ze Ma et al. Magic-me: video customized diffusion. arXiv:2402.09368 (2024). Identity-specific In: arXiv preprint [41] Kepan Nan et al. OpenVid-1M: Large-Scale HighQuality Dataset for Text-to-video Generation. In: arXiv preprint arXiv:2407.02371 (2024). [42] Adam Polyak et al. Movie gen: cast of media foundation models. In: arXiv preprint arXiv:2410.13720 (2024). [43] Qixun Wang et al. Instantid: Zero-shot identitypreserving generation in seconds. In: arXiv preprint arXiv:2401.07519 (2024). [44] Tao Wu et al. VideoMaker: Zero-shot Customized Video Generation with the Inherent Force In: arXiv preprint of Video Diffusion Models. arXiv:2412.19645 (2024). [45] Zhuoyi Yang et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In: arXiv preprint arXiv:2408.06072 (2024). [46] Sihyun Yu et al. Representation alignment for generation: Training diffusion transformers is easier than you think. In: arXiv preprint arXiv:2410.06940 (2024)."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Beijing University of Posts and Telecommunications"
    ]
}