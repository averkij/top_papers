{
    "paper_title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy",
    "authors": [
        "Zihan Liu",
        "Zhuolin Yang",
        "Yang Chen",
        "Chankyu Lee",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Wei Ping"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does a stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for a given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, a setting that strikes a good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging a strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B"
        },
        {
            "title": "Start",
            "content": "2025-06-13 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Zihan Liu, Zhuolin Yang, Yang Chen, Chankyu Lee, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping"
        },
        {
            "title": "Abstract",
            "content": "In this work, we investigate the synergy between supervised fine-tuning (SFT) and reinforcement learning (RL) in developing strong reasoning models. We begin by curating the SFT training data through two scaling strategies: increasing the number of collected prompts and the number of generated responses per prompt. Both approaches yield notable improvements in reasoning performance, with scaling the number of prompts resulting in more substantial gains. We then explore the following questions regarding the synergy between SFT and RL: (i) Does stronger SFT model consistently lead to better final performance after large-scale RL training? (ii) How can we determine an appropriate sampling temperature during RL training to effectively balance exploration and exploitation for given SFT initialization? Our findings suggest that (i) holds true, provided effective RL training is conducted, particularly when the sampling temperature is carefully chosen to maintain the temperature-adjusted entropy around 0.3, setting that strikes good balance between exploration and exploitation. Notably, the performance gap between initial SFT models narrows significantly throughout the RL process. Leveraging strong SFT foundation and insights into the synergistic interplay between SFT and RL, our AceReason-Nemotron-1.1 7B model significantly outperforms AceReason-Nemotron-1.0 and achieves new state-of-the-art performance among Qwen2.5-7B-based reasoning models on challenging math and code benchmarks, thereby demonstrating the effectiveness of our post-training recipe. We release the model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B. 5 2 0 2 6 1 ] . [ 1 4 8 2 3 1 . 6 0 5 2 : r Figure 1: Benchmark accuracy of AceReason-Nemotron-1.1-7B on AIME 2024/2025 (avg@64), HMMT 2025 (avg@64), LiveCodeBench v5 (2024/08/01-2025/02/01, avg@8), and v6 (2025/02/01-2025/05/01, avg@8) using 32,768 output length. Leads the effort. Correspondence to: Zihan Liu <zihanl@nvidia.com>, Wei Ping<wping@nvidia.com>. 2025 NVIDIA. All rights reserved. AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy"
        },
        {
            "title": "4.1 Benchmark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Baselines\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.3 Main Results\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.4 SFT Analyses\n4.4.1 Scaling of SFT data consistently improves performance . . . . . . . . . . . . . . . . . .\n. . . . . . . . . . . . . . . . . . . . . . . .\n4.4.2 Which data scaling factor has larger impact\n. . . . . . . . . . . . . . . . . . . . .\n4.4.3 Performance improves progressively over epochs\n4.5 RL Analyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5.1 RL starting from different SFT models\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5.2 How training temperature affects the progress of RL . . . . . . . . . . . . . . . . . . .\n4.5.3 At which stage should we apply overlong filtering? . . . . . . . . . . . . . . . . . . . .\n4.5.4 Importance of Stage-1 (8K) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5.5 How long should we train Stage-1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.5.6 Math-only RL significantly improves code reasoning . . . . . . . . . . . . . . . . . . . .\n4.5.7 RL improves upon the SFT model in terms of pass@K even when K is large . . . . . . .\n. . . . . . . . . . . . . .\n4.5.8 RL improves over strong SFT model by solving hard problems",
            "content": "5 Conclusion 6 Acknowledgement Instruction for evaluation RL training from different SFT models on AIME25 Pass@k Accuracy on Math-Only RL Models Problem-Level Solving Rates on Math-Only RL Models 3 4 5 5 5 5 6 6 6 7 8 8 8 9 9 9 10 10 11 11 11 12 13 14 15 15 15 16 21 21 22 22 2 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy 1. Introduction Math and code reasoning with large language models (LLMs) has been an active area of research for years (Chen et al., 2021; Cobbe et al., 2021). Previous work has primarily focused on enhancing short chain-of-thought (CoT) reasoning (Wei et al., 2022), which is typically acquired through pretraining and supervised finetuning (SFT) (e.g., Hui et al., 2024; Liu et al., 2024; Shao et al., 2024; Yang et al., 2024). Since the introduction of OpenAI o1 (OpenAI, 2024) and DeepSeek-R1 (Guo et al., 2025; Liu et al., 2024), long chain-of-thought (CoT) reasoning, which is acquired through large-scale reinforcement learning (RL), has emerged as key driver of the remarkable progress in the reasoning capabilities of frontier LLMs (Guo et al., 2025; Qwen-Team, 2025; Yang et al., 2025), with reward signals typically provided by rule-based verifiers. Subsequently, much of the follow-up work has focused on distilling these large frontier models into smaller or mid-sized models through synthetic data generation and SFT-only approach (Ahmad et al., 2025; Bercovich et al., 2025; Moshkov et al., 2025; Yang et al., 2025). Several recent efforts have sought to replicate the success of large-scale RL on smaller base or SFT models (i.e., 7B and 14B ) (e.g., He et al., 2025; Liu et al., 2025; Luo et al., 2025; Wen et al., 2025), often leveraging DeepSeek-R1-Distill-Qwen2.5 (Chen et al., 2025) as the initialization checkpoints. However, systematic study of the synergy between SFT and RL has been limited within the research community and is notably absent from the technical reports of frontier models. In our previous study (Chen et al., 2025), we demonstrated that AceReason-Nemotron-1.0-7B and 14B can outperform the leading math and code models built on Qwen2.5-Math 7B and 14B. In particular, the proposed stage-wise RL approach on math-only and code-only prompts has proven to be both effective and efficient. In this work, we take step further by integrating supervised fine-tuning (SFT) and reinforcement learning (RL), probing their training dynamics and the synergy between them to provide holistic perspective on these post-training techniques for building state-of-the-art reasoning models. Specifically, we make the following contributions: 1. We begin by scaling SFT training through collecting large number of prompts, increasing the number of generated responses per prompt, and increasing the number of training epochs. We find that: (i) Scaling both the number of prompts and the number of generated responses per prompt leads to substantial improvements in reasoning performance on math and code benchmarks, although scaling the number of prompts yields more significant gains. (ii) We observe consistent performance gains from the first to the fifth epoch, with improvements plateauing between the fifth and sixth epochs, regardless of the specific SFT blend used. This suggests that certain degree of \"overfitting\" actually enhances test accuracy with long CoT generation, likely due to exposure bias in autoregressive models. 2. We initiate RL training from various SFT models and make the following key observations: (i) Stronger SFT models continue to produce consistently better results after large-scale RL, although the performance gap narrows during RL training. (ii) For given initial SFT model, selecting an appropriate temperature during RL training is crucial for achieving good balance between exploration and exploitation. We provide rule of thumb for setting the sampling temperature such that the temperature-adjusted entropy remains around 0.3, which typically leads to effecive RL training. 3. We systematically study the best strategy during RL training when the final answer is not produced within specific response length budget (e.g., 24K). Whether we should assign negative reward or mask out the entire sample (i.e., overlong filtering). Our findings show that overlong filtering provides clear benefits when the token limit is short (e.g., 8K or 16K). However, this advantage diminishes at 24K token budget, and at 32K, overlong filtering can even degrade model performance. 4. We confirm that the stage-wise RL approach on math-only and code-only prompts remains effective when applied to range of much stronger SFT models beyond DeepSeek-R1-Distill-Qwen models, demonstrating the broad applicability of this method (Chen et al., 2025). We also reaffirm that performing RL on math-only prompts not only improves math results but also significantly boosts code benchmark performanceeven 3 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy when starting from very strong SFT models, highlighting the cross-domain generalization capability of the RL approach. Building on our strong SFT model and insights into the synergy between SFT and RL, our AceReason-Nemotron-1.1 7B model achieves record-high performance among Qwen2.5-7B-based reasoning models on challenging math and code reasoning benchmarks, demonstrating the superiority of our post-training recipe. We organize the rest of this paper as follows. In 2, we introduce the related work. In 3, we present the SFT and RL methods, along with details on training procedures and data curation. In 4, we provide the main results of our model and in-depth analyses of both SFT and RL training. We conclude the paper in 5. 2. Related Work The capacity for reasoning is essential to AI and manifests in both the text domain (e.g., Cobbe et al., 2021; Hui et al., 2024; Wei et al., 2022) and the multimodal space (e.g., Dai et al., 2024; Ghosh et al., 2025; Zhu et al., 2025). DeepSeek-R1 (Guo et al., 2025) has demonstrated the effectiveness of verification-based reinforcement learning (RL), in which an external verifier provides reward signals by evaluating the correctness of the models outputs against oracle answers. This approach has proven particularly useful in domains with structured outputs and well-defined verification criteria. For math problems, rule-based verification is facilitated by training large language models (LLMs) to produce final answers in structured and easily parsed format, such as enclosing the final answer in box, to enable automatic checking (e.g., Chen et al., 2025; Liu et al., 2024; Yang et al., 2024). For code generation tasks, the reward signal is derived from functional correctness, which is assessed by compiling the generated code and executing it against suite of predefined test cases (e.g., Chen et al., 2025; Guo et al., 2025; Luo et al., 2025). Such rule-based verification eliminates the need for reward modeling, enhances the accuracy of the reward signal, and helps prevent potential reward hacking issues. Several follow-up studies have explored various RL training techniques to improve model performance on math and code reasoning tasks. Among these algorithms, GRPO (Shao et al., 2024) has gained particular popularity for its simplicity, robustness, and effectiveness. DeepScaleR (Luo et al., 2025) introduced multistage RL training approach that achieved strong results on math with 1.5B model. This approach was subsequently adopted by many other works, including AceReason-Nemotron (Chen et al., 2025), Skywork Open Reasoner (He et al., 2025), and DeepCoder (Luo et al., 2025), across both math and code domains. AceReason-Nemotron (Chen et al., 2025) proposed stage-wise RL method that trains sequentially on math and code prompts, and advocated for strict on-policy GRPO training, both of which are adopted in this work. DAPO (Yu et al., 2025) investigates the use of an overlong penaltywhich assigns negative rewards to truncated generations within the response lengthin the math domain, and finds that removing it is beneficial. This approach is also adopted by DeepCoder (Luo et al., 2025). In this work, we conduct more systematic analysis and find that the overlong penalty should be applied during the later stages of RL training, rather than at the early stages. Another line of work focuses on supervised fine-tuning (SFT) via distillation from frontier reasoning models trained with reinforcement learning. Notable examples include DeepSeek-R1-Distill-Qwen (Guo et al., 2025), Light-R1 (Wen et al., 2025), OpenMathReasoning (Moshkov et al., 2025), OpenCodeReasoning (Ahmad et al., 2025), and Llama-Nemotron (Bercovich et al., 2025). These models leverage large-scale math and code reasoning samples generated by DeepSeek-R1 (Guo et al., 2025) or QwQ (Qwen-Team, 2025), demonstrating strong downstream performance on reasoning tasks. Some works describe the use of pretraining, supervised fine-tuning (SFT), and reinforcement learning (RL) in building strong reasoning models. However, systematic study of the interplay and integration between SFT and RL is often lacking in technical reports (e.g., Seed et al., 2025; Xia et al., 2025; Yang et al., 2025). 4 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 2: Training Pipeline of AceReason-Nemotron 1.1. We start by performing math and code SFT on base pretrained model. Next, we conduct three stages of math-only RL training with progressively growing response length, i.e., Stage-1 (8K), Stage-2 (16K), and Stage-3 (24K), to develop math-specialized RL model. We then apply code-only RL training to enhance models coding capability. Lastly, we carry out final stage of math-only RL to produce AceReason-Nemotron 1.1. Figure 3: Response token length distributions for the math SFT dataset (left) and the code SFT dataset (right). 3. Method In this section, we present the details of the supervised fine-tuning and reinforcement learning processes used to train AceReason-Nemotron 1.1. The overall training pipeline is illustrated in Figure 2. 3.1. Supervised Fine-Tuning 3.1.1. Prompt collection and filtering We collect math and code reasoning datasets from high-quality data sources. For math, we collect prompts from AceMath dataset (Liu et al., 2024), NuminaMath (Li et al., 2024), and OpenMathReasoning (Moshkov et al., 2025). For coding, we collect prompts from TACO (Li et al., 2023), APPs (Hendrycks et al., 2021), OpenCoder-Stage2 (Huang et al., 2024), and OpenCodeReasoning (Ahmad et al., 2025). We conduct the dataset deduplication to ensure that each prompt is unique. After that, we conduct data decontamination and filter the sample that has 9-gram overlap with any test sample in math and coding benchmarks (Muennighoff et al., 2025). We use DeepSeek-R1 (Guo et al., 2025) to generate responses for the collected prompt set. We aim for our SFT dataset to encompass diverse range of challenging samples. Intuitively, longer model responses often correspond to more difficult questions. Based on this observation, we found that large portion of the collected prompts are relatively simple, with many responses around or below 2,000 tokens in length. To achieve better balance across difficulty levels, we randomly filtered out subset of these simpler prompts and adjusted the proportions of other difficulty levels through additional random sampling. This resulted in final dataset of 247K math prompts and 136K code prompts, totaling 383K prompts. Figure 3 presents the data statistics for math and code prompts, as well as the average number of responses per prompt. 3.1.2. Scaling of SFT Data We investigate how scaling the SFT dataset impacts model performance along two axes: (1) increasing the number of unique prompts, and (2) increasing the number responses per prompt. Expanding the set of prompts enriches the coverage of problem types and topics, while adding more responses per prompt allows the model to observe diverse reasoning paths for the same input. 5 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy To explore these scaling strategies, we construct seven SFT datasets (v1 through v7), each maintaining similar distribution of response token lengths as shown in Figure 3. The dataset size scales increases from 36K samples in v1 to 2.2M samples in v7. In Section 4.4.1, Figure 4 presents the number of prompts and the corresponding average number of responses per prompt for each dataset. For all datasets, SFT training is initialized from the base model Qwen2.5-Math-7B (Yang et al., 2024), which is also the starting point for DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025). Since Qwen2.5-Math-7B only supports context length of 4,096, we modify the rope_theta parameter from 10,000 to 1,000,000 enable support for context length of 128K. In Section 4.4, we investigate and address the following questions with respect to SFT: i) How does scaling SFT data improve model performance on math and code benchmarks? ii) Is it more effective to scale the number of unique prompts or the number of responses per prompt? What is the best strategy in practice? iii) How does training for more epochs improve performance? What is the stopping criterion? 3.2. Reinforcement Learning 3.2.1. Overview We apply the stage-wise RL approach on math-only and code-only prompts in sequence, as described in Chen et al. (2025), to our SFT models. Specifically, we employ the GRPO algorithm (Shao et al., 2024) and strictly adhere to on-policy training by generating 洧냨 = 8 or 16 rollouts {洧녶洧녰}洧냨 洧녰=1 for each question 洧 in global batch of 128 prompts, followed by single policy gradient update. The motivation for using on-policy training is to stabilize RL and prevent entropy collapse, as demonstrated in Chen et al. (2025). We utilize the token-level policy gradient loss, which assigns greater rewards to longer samples when the answer is correct and harsher penalties when it is incorrect. The intuition is that learning to generate longer samples plays more critical role in enhancing reasoning capabilities. We remove KL divergence term as well. As result, the GRPO objective can be reduced to, 洧눤GRPO(洧랚) = (洧,洧녩)洧, {洧녶洧녰}洧냨 洧녰=1洧랢洧랚(洧) [ 1 洧녰=1 洧녶洧녰 洧냨 洧냨 洧녶洧녰 ] . 洧냢洧녰,洧노 洧녰=1 洧노= (1) where question-answer pair (洧, 洧녩) is sampled from the training dataset 洧, {洧녶洧녰}洧냨 for 洧 by the current policy 洧랢洧랚( 洧), and token-level advantages 洧냢洧녰,洧노 are estimated as, 洧녰=1 are responses generated 틙洧냢洧녰,洧노 = 洧녡洧녰 mean({洧녡洧녰}洧냨 std({洧녡洧녰}洧냨 洧녰=1) 洧녰=1) , (2) therein {洧녡洧녰}洧냨 洧녰=1 denotes the reward scores of the group of responses, computed via rule-based verification against the ground-truth answer 洧녩. We use the RL framework veRL (Sheng et al., 2024) with implementation of token-level lossof GRPO. 3.2.2. Data curation We utilize the high-quality math and code RL data from AceReason-Nemotron-1.0 (Chen et al., 2025). In particular, we find that the difficulty level of prompts and the accuracy of answers are the most critical factors for effective RL training. Prompts should be neither too easy nor too difficult, enabling the model to receive balanced mix of positive and negative rewards across group of rollouts. In all cases, answers must be as accurate as possible, as this is the only way for the model to receive meaningful signals during verification-based RL. Taken together, these findings highlight that the quality of RL data outweighs its quantity. For code RL training, the quality and coverage of test cases are crucial, as incorrect test cases can lead to false negative rewards, while overly simple test cases may result in false positives. AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy 3.2.3. Training process We follow the math-only and code-only RL training curriculum introduced in AceReason-Nemotron (Chen et al., 2025). We begin with math-only RL using stage-wise response length extension from 8K to 16K, and then to 24K. Next, we apply code-only RL with response lengths of 24K and 32K. Finally, we conduct an additional round of math-only RL with 32K response length budget. For each stage, we use the same RL training dataset as AceReason-Nemotron-1.0. The entire training process and key takeaways are summarized below. 1. Math-only Stage-1 (8K): This initial stage with 8K response length budget serves as warm-up phase for RL training. We use relatively simple questions sampled from our collected RL dataset for training. Most of these questions elicit responses from DeepSeek-R1 with token lengths predominantly between 2K and 4K. During this stage, we observe an initial decline in model performance, followed by recovery to nearly the original level. Although this stage does not yield net performance gain by itself, our experiments show that it is essentialskipping directly to Stage-2 (16K) results in suboptimal outcomes. We hypothesize that the model uses this stage to facilitate the transition from imitation learning (SFT) to reinforcement learning. In particular, it learns to compress its reasoning paths into more compact forms, which is beneficial in later stages where larger response length budget is available. See Section 4.5.4 for detailed analysis. 2. Math-only Stage-2 (16K): At this stage of training, we increase the proportion of more challenging questions compared to Stage 1. As result, the models average response length gradually increases, and we observe substantial performance improvementsimilar to what was seen in AceReason-Nemotron-1.0, even though we start from much stronger SFT model. 3. Math-only Stage-3 (24K): We filter out most of the simple questions and keep around 2500 hard ones for the training of this stage. Our model shows significant performance improvement on math benchmarks in this stage. 4. Code-only Stage-I (24K): This stage marks the beginning of code RL training, which is initiated after math RL training to ensure greater stability. False positive and false negative rewards are generally more prevalent in the code domain than math domain due to the nature and lower quality of test cases. Conducting math-only RL beforehand helps to enhance the models reasoning capabilities, facilitates generalization from math to code, and better prepares the model for the relatively noisier code-only RL training that follows. In practice, we found that starting code RL training after math RL largely stabilizes the overall training process. 5. Code-only Stage-II (32K): In this stage, we apply the epoch-wise filtering strategystarting after the first epochas in AceReason-Nemotron-1.0. Specifically, we remove easy problems that can be fully solved by the previous epochs checkpoint, i.e., problems for which every rollout passes all test cases. 6. Math-only Stage-4 (32K): As in the math-only Stage-3 (24K) setup, we filter out most of the simple questionsthose that can be solved by every rolloutand retain only the challenging ones for training in this final stage. In this work, we explore the following questions related to the RL process, with particular focus on the synergy between SFT and RL, which will be discussed in Section 4.5: i) How does initializing RL from different SFT models affect final model performance? Does starting from stronger SFT model lead to better overall performance? ii) How does the sampling temperature of policy LLM affect RL training, particularly in balancing the explorationexploitation trade-off? iii) Is the Math-only Stage-1 (8K) truly necessary, given that it initially lowers benchmark performance? If the reasoning process compression from this stage is essential, how long should we train? Do we need to wait until benchmark accuracies fully recover? iv) During RL training, what is the best strategy when the final answer is not generated within specified response length budget (e.g., 24K)? Should we assign negative reward or mask out the entire sample (i.e., 7 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Models SFT (distillation) based models: Light-R1-7B Llama-Nemotron-Nano-8B-v1 OpenMath-Nemotron-7B OpenCodeReasoning-Nemotron-7B RL based models: AReal-boba-RL-7B Skywork-OR1-Math-7B Skywork-OR1-7B OlympicCoder-7B MiMo-7B-RL o3-mini (low) Magistral Small (24B) DeepSeek-R1-Distill-Qwen-7B AceReason-Nemotron-1.0-7B Our SFT-7B (starting point of RL) AceReason-Nemotron-1.1-7B AIME 2024 avg@64 2025 avg@64 MATH 500 avg@4 HMMT 2025 avg@64 BRUMO 2025 avg@ LiveCodeBench v5 avg@8 v6 avg@8 EvalPlus avg@4 59.1 61.3 74.8 61.9 69.8 70.2 68.2 60.0 70.7 55.5 69.0 62.0 72.6 44.3 47.1 61.2 48.3 52.3 54.6 55.4 48.3 62.8 39.0 53.6 48.4 64.8 92.4 95.4 93.8 94.4 94.4 95.8 95.8 95.9 92.8 94.1 94.1 95.3 27.6 29.4 31.4 32.0 35.7 28.3 26.3 33.9 31.1 42.9 52.8 58.9 60.6 59.7 65.1 66.7 51.2 62.2 59.4 69.8 40.6 46.6 51.3 34.3 43.6 47.6 40.7 57.8 60.9 55.8 37.6 51.8 48.8 57.2 36.4 46.2 46.1 42.7 37.1 49.3 47.4 34.1 44.1 43.8 52.1 81.2 83.4 79.8 80.4 84.6 83.4 84.8 Table 1: Evaluation of reasoning models primarily based on Qwen2.5-Math 7B and Llama-3.1 8B to disentangle the impact of pretraining. We report pass@1 averaged over 洧녵 generations (avg@洧녵) following the DeepSeek-R1 evaluation framework (same template, temperature=0.6, top_p=0.95, max response length=32,768). By default, we include official numbers from the model developers if they are available. Otherwise, we evaluate the model using the official template and same evaluation setting as above. Note that, unlike the base model Qwen2.5-Math, MiMo-7B-RL is developed from base model pretrained with extensive synthetic reasoning data from advanced reasoning model (Xia et al., 2025). overlong filtering)? v) Does RL still improve upon the SFT model in terms of pass@K when is large, even when the SFT model is substantially stronger than DeepSeek-R1-Distill-Qwen used in Chen et al. (2025)? 4. Evaluation 4.1. Benchmark For math tasks, we evaluate our models on AIME24, AIME25, Math500 (Hendrycks et al., 2021), as well as HMMT2025 Feb and BRUMO2025 from MathArena (Balunovic et al., 2025). For coding tasks, we evaluate our models on EvalPlus (Liu et al., 2023, 2024), and LiveCodeBench v5 (2024/08/01-2025/02/01) and v6 (2025/02/01-2025/05/01) (Jain et al., 2024). Unless otherwise specified, all benchmarks use the default inference settings: temperature of 0.6, top-p of 0.95, and maximum sequence length of 32,768 (32K). Because reasoning models produce highly variable outputs when sampling is used, we report pass@1 performance averaged over 洧녵 runs (denoted as avg@n). For all reported numbers, we set 洧녵 = 64 for AIME24, AIME25, HMMT2025 Feb, and BRUMO2025; 洧녵 = 8 for LiveCodeBench V5 and V6; and 洧녵 = 4 for MATH500 and EvalPlus. Note that using sufficiently large is crucial for obtaining reliable metricthat is, achieving low standard deviation in average pass@1 accuracy, which decreases at rate of 1/ 洧녵especially for benchmarks with small number of problems. For example, on AIME2024, using avg@16, avg@32, and avg@64 yields standard deviations of pass@1 accuracy of 1.8, 1.2, and 0.7, respectively. 4.2. Baselines Since our training begins with the base model Qwen2.5-Math-7B (Yang et al., 2024), we primarily compare against state-of-the-art reasoning models built on either Qwen2.5 or Llama-3.1 (Grattafiori et al., 2024) of comparable parameter sizes to isolate the effects of pre-training and ensure fair comparison. Our baselines include SFT models distilled from much larger frontier models (e.g., DeepSeek-R1), including Light-R1-7B (Wen et al., 2025), DeepSeek-R1-Distill-Qwen-7B (Guo et al., 2025), OpenMathReasoning-7B (Moshkov et al., 2025), and Llama-Nemotron-Nano-8B (Bercovich et al., 2025), as well as math-only RL-based models such as AReal8 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy boba-RL-7B (RL Lab, 2025), Skywork-OR1-Math-7B (He et al., 2025), and AceReason-Nemotron-7B (Chen et al., 2025). We also compare our models to state-of-the-art 7B code-specialized LLMs, including OlympicCoder7B (HuggingFace, 2025) and OpenCodeReasoning-7B (Ahmad et al., 2025). 4.3. Main Results Table 1 shows the evaluation results on math and code benchmarks. Compared to other SFT models through distillation, our SFT model achieves slightly better results than Llama-Nemotron-Nano-8B-v1, and achieves much better performance compared to Light-R1 and DeepSeek-R1-Distill-Qwen-7B. It is worth mentioning that, DeepSeek-R1-Distill-Qwen-7B is trained from Qwen2.5-Math-7B as our SFT model, suggesting the high quality of our collected SFT samples. For AceReason-Nemotron-1.1-7B, we observe that the same AceReason RL training recipe (the same training method applied on the same training data) from Chen et al. (2025) substantially improves the performance of our strong SFT model, yielding absolute score gains of 10.6% on AIME24, 16.4% on AIME25, 8.4% on LiveCodeBench v5, and 8.3% on LiveCodeBench v6. As result, our RL model, AceReason-Nemotron-1.1-7B, demonstrates superior performance on math and code reasoning tasks, achieving the highest accuracy among 7B-scale models on AIME25 and LiveCodeBench v6benchmarks that carry lower risk of contamination compared to their earlier versions. As reference, for AceReason-Nemotron-1.0-7B, the same RL training recipe improves its starting SFT model, DeepSeek-R1-Distill-Qwen-7B, 13.5% on AIME24, 14.6% on AIME25, 14.2% on LiveCodeBench v5, and 10.0% on LiveCodeBench v6. This demonstrates that well-curated RL recipe can still largely boost the models reasoning capability, even when starting from much stronger SFT model. 4.4. SFT Analyses 4.4.1. Scaling of SFT data consistently improves performance Figure 4: Log-scaled data statistics for the number of math and code prompts and the average number of responses per prompt. Each SFT dataset consist of both math and code SFT samples. Figure 5: Accuracies on AIME24, AIME25, and LiveCodeBench V5 and V6 for different SFT datasets. For each SFT blend, the model is trained until the accuracy plateaus. Figure 4 presents the number of prompts, total samples, and average number of responses per prompt for each SFT dataset. Figure 5 demonstrates the benefits of scaling SFT datasets, from 18K math and 18K code samples AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy in version v1 to 1.2M math and 1.0M code samples in version v7. We observe that both scaling strategies, namely increasing the diversity of prompts and adding more responses per prompt, significantly enhance model performance. From the SFT dataset v1 to v4, we focus on expanding the number of unique prompts while limiting each prompt to single response. Even modest number of unique prompt increases, such as adding 15K to 20K math or code samples, result in noticeable gains. For instance, expanding the math dataset by 16K samples from v3 to v4 leads to 4% improvement in AIME24 and 2% increase in AIME25. Starting with v5, we scale both the number of unique prompts and the number of responses per prompt. At this stage, the SFT v5 model matches the math performance of DeepSeek-R1-Distill-Qwen-7B and surpasses it in coding benchmarks. In our final dataset, v7, we maintain similar number of prompts but further increase the number of responses per prompt. This additional scaling yields another performance boost, with AIME25 improving by 8% (from 41.3 to 49.3). 4.4.2. Which data scaling factor has larger impact We further analyze which SFT data scaling factor contributes more significantly to performance improvements: increasing the number of unique prompts or increasing the number of responses per prompt. To this end, we perform multiple linear regression analysis to model the relationship between overall accuracy (洧녾) and the two independent variables: the number of unique prompts (洧논) and the number of responses per prompt (洧녽), as described by the following equation: 洧녾 = 洧녩 log2 洧논 + 洧녪 log2 洧녽 + 洧녫 We apply the least squares method to fit this model using seven data points (Figure 5) and estimate the regression coefficients 洧녩 and 洧녪, and bias 洧녫. Prior to fitting, 洧논 and 洧녽 are tranformed to log base-2 scale to reflect the exponential growth in total samples versus the linear trend in accuracy. Then, 洧논 and 洧녽 are standardized (zero mean and unit variance) to ensure they are on the same scale. The dependent variable 洧녾 is defined as the average accuracy across AIME24, AIME25, and LiveCodeBench V5 and V6. The resulting estimates are 洧녩 = 4.831 and 洧녪 = 2.635, with the coefficient of determination 洧녠2 = 0.989, indicating strong fit. The larger value of 洧녩 compared to 洧녪 suggests that increasing the number of unique prompts may have greater impact on SFT model performance than increasing the number of responses per prompt. Given that when the number of unique prompts reaches certain amount, increasing it becomes generally harder due to the difficulties of collecting more diverse data sources. In this case, increasing the number of responses for each prompt serves as practical alternative to boost the performance of SFT model. 4.4.3. Performance improves progressively over epochs Figure 6: Accuracies over different epochs of training for SFT dataset v6 and v7. Figure 6 illustrates the accuracies AIME24 and LiveCodeBench V5 over different epochs of training for SFT dataset v6 and v7. We observe that the models performance gradually improves from the 1st to the 5th epoch, and begins to plateau around the 5th to 6th epoch. This pattern is consistent across different versions of the AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy SFT dataset. This suggests that certain degree of overfitting may actually enhance test accuracy in long chain-of-thought (CoT) generation, likely due to exposure bias in autoregressive models. Additionally, we find that models trained on dataset v7 consistently outperform those trained on dataset v6 throughout training. While both versions share similar set of prompts, dataset v7 contains nearly twice as many responses per prompt on average. Despite being exposed to the same prompts more frequently, models trained on v7 also reach performance plateau around the fifth to sixth epoch. This suggests that increasing the number of responses per prompt in the SFT dataset also greatly benefits from multi-epoch training. 4.5. RL Analyses 4.5.1. RL starting from different SFT models Figure 7: Math-only RL training starting from different SFT (distillation) models. The AIME24 accuracy at step-0 reflects the performance of the initial SFT checkpoints. The subsequent numbers in the figure show the final accuracy achieved at the end of each training stage: Math-Only Stage-1 (8K), Stage-2 (16K), Stage-3 (24K), and Stage-4 (32K). Figure 7 presents RL experiments initialized from different SFT models, including two trained on our SFT datasets v5 and v7, as well as DeepSeek-R1-Distill-Qwen-7B. We generally observe significant performance gains at stage-2 (16K) and stage-3 (24K). The performance begins to plateau at stage-4 (32K) for the model initialized from SFT-7B v7, as further gains become more challenging on top of an already strong model. It is worth mentioning that these performance gains are accompanied by an increase in the average response token length at stage-2 and stage-3, suggesting that the model engaged in longer reasoning processes to tackle more difficult problems. Interestingly, we observe that while some SFT models show substantial performance gaps (e.g., between SFT-7B v5 and v7), these differences become much smaller after applying RL training over more steps. For instance, the performance gap decreases from an initial 6.6% to 1.6% on AIME24. This underscores the potential of RL to effectively enhance model performance and bridge the gap between different starting points. Additionally, when RL is applied to SFT models with similar starting performance, such as SFT-7B v5 and DeepSeek-R1-Distill-Qwen-7B, the resulting performance on AIME24 reaches to similar level. We put the results for AIME25 in Appendix B. 4.5.2. How training temperature affects the progress of RL Figure 8 illustrates how different training temperature settings affect the trajectory of entropy during RL training and model performance after training. We make the following two observations: For given model, the temperature in RL training should be carefully tunednot set too low or too high. low temperature (e.g., 0.6 in Figure 8) leads to over-exploitation and limited exploration, which can ultimately result in sub-optimal performance. In contrast, high temperature (e.g., 1.0 in Figure 8) 11 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy AIME24 AIME25 avg@64 avg@64 LCB V5 avg@8 LCB V6 avg@8 SFT Model (Step-0) Inference using temperature 0.6 Inference using temperature 0.85 Inference using temperature 1.0 Math-Only Stage-2 RL Models Trained with temperature 0.6 Trained with temperature 0.85 Trained with temperature 1.0 62.0 62.0 61.2 64.6 67.6 65.3 48.4 46.3 45.4 52.4 56.8 56.7 48.8 48.9 48. 50.1 52.1 51.6 43.8 44.1 43.6 45.6 47.1 45.7 Figure 8: Left: Trajectories of temperature-adjusted entropy during RL training with different policy LLM temperature settings. Right: Impact of varying temperatures for inference and RL training. We observe that using temperature of 0.6 for inference consistently yields better average results, and thus adopt 0.6 as the default inference temperature unless otherwise specified. causes excessive exploration and low initial rewards, followed by reduction in entropy and hindered learning progress. Through multiple trials, we observed useful rule of thumb: setting the training temperature such that the temperature-adjusted entropy remains around 0.3 typically leads to effective RL training, as it strikes good balance between exploration and exploitation. Note that using temperature of 0.6 for inference consistently yields better average results; therefore, we adopt 0.6 as the default inference temperature across experiments unless otherwise specified. However, with training temperature of 0.6, RL training begins with low entropy (around 0.15), causing the policy LLM to favor exploitation over exploration. This is evidenced by entropy remaining below 0.2 throughout the training process. Such conservative learning behavior ultimately leads to sub-optimal performance. With training temperature of 1.0, training begins with the high entropy (around 0.4), which gradually declines to approximately 0.22. We conjecture that this behavior may be attributed to the poor performance of the SFT model when sampling with temperature of 1.0. As shown in Figure 8 (right table), we can see that using temperature 1.0 results in the poorest performance compared to 0.85 and 0.6. At the early RL stage, we also observe that using temperature of 1.0 results in average rewards that are roughly 34% worse than those achieved with temperature of 0.85. While higher initial temperature encourages greater exploration, relatively low reward signal dampens this tendency, leading the model to favor exploitation. This shift is reflected in sharper logit distribution and consequent drop in entropy. In contrast, moderate temperature of 0.85 starts with an entropy of approximately 0.26, which gradually increases to around 0.38 over the course of training. We hypothesize that the relatively higher rewards associated with this settingalso reflected in the table on the right (inference with 0.85 performs only marginally worse than 0.6, but significantly better than 1.0)promote continued exploration, thereby driving the steady increase in entropy during training. This improved explorationexploitation trade-off results in the highest benchmark performance. 4.5.3. At which stage should we apply overlong filtering? During our curriculum RL training, model responses are sampled within fixed response length budget (e.g., 8K or 16K tokens), and any outputs exceeding this limit are truncated. When response surpasses this predefined length, key training consideration emerges: should we mask the whole sample without assigning any reward (i.e., \"w/ overlong filtering\") or should such overlong outputs be penalized with negative reward (i.e., \"w/o overlong filtering\")? In previous studies, DAPO (Yu et al., 2025) shows that under 16K token limit, penalizing truncated responses could confuse the model, as well-reasoned answer might be unfairly penalized simply for its excessive length. Instead, applying overlong filtering helps stabilize training and improves performance. In contrast, 12 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 9: Ablation Studies on Math-Only RL training to assess the impact of overlong filtering. In both settings, Stage-1 starts with the same SFT model, and each subsequent stage begins with the same RL model from the previous stage trained under the best-performing setting (i.e., w/ overlong filtering). Notably, in the final stage (Stage-4), RL training without overlong filtering leads to superior performance. Evaluations on AIME24 and AIME25 are performed with maximum sequence length of 32K. AIME24 AIME25 avg@64 avg@64 LCB V5 avg@8 LCB V6 avg@ Math-Only Stage-4 RL w/ Overlong Filtering Inference using 32K Maximum Length Inference using 64K Maximum Length Math-Only Stage-4 RL w/o Overlong Filtering Inference using 32K Maximum Length Inference using 64K Maximum Length 70.2 72.4 71.4 73.0 62.3 64.5 63.5 64. 52.0 53.5 53.5 54.5 45.1 45.7 48.0 48.7 Table 2: Comparisons of the effects of increasing the maximum output length to 64K, with and without applying the overlong filtering at the last stage of Math-Only RL. Skywork-OR1 (He et al., 2025) observes no clear performance benefit from overlong filtering, although their findings are limited to stage-1 (8K) RL training. Compared to previous studies, we conduct more systematic study for overlong filtering across all RL stages. Figure 9 illustrates the effects of overlong filtering. Unlike Skywork-OR1, we observe notable benefit from applying overlong filtering in Stage-1 (8K). This is largely due to the relatively short 8K token limit, where at the beginning of the training, approximately 30% of sample outputs exceed this boundary. Without overlong filtering, the negative rewards on these truncated samples will introduce significant noise into training. As the token limit increases, this noise diminishesresulting in smaller gains from overlong filtering in Stage-2 (16K) and nearly equivalent performance in Stage-3 (24K). In Stage-4 (32K), RL training without overlong filtering outperforms the alternative. This is because removing overlong filtering makes the inference more token efficient and enables the model to produce more concise generation within the 32K token budget. Table 2 further evaluates models under 64K maximum inference length. Interestingly, the model trained without overlong filteringdespite generating more concise outputsis still able to outperform the model trained with overlong filtering, particularly on coding benchmarks. These findings contrast with the conclusions of DeepCoder (Luo et al., 2025), which suggest that overlong filtering improves generalization to longer response lengths (e.g., 64K) during inference. 4.5.4. Importance of Stage-1 (8K) We can find in Figure 7 that during Math-Only Stage-1 (8K), the model exhibits an initial drop in performance followed by recovery. Nevertheless, its final performance at the end of Stage-1 may remain below its initial level. Given the lack of clear improvement during Stage-1, natural question arises: should Stage-1 be omitted? 13 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 10: Left: Ablation study comparing models trained with and without Math-Only Stage-1. For w/o Stage-1, the step-0 accuracy reflects the performance of the our SFT model on AIME25. In contrast, for \"w/ Stage-1\", the step-0 accuracy represents the final performance of Stage-1 RL initialized from the same SFT model. Right: Average response token length during Math-Only Stage-1 (8K) RL training. To investigate this, we compare Stage-2 (16K) training initialized from the SFT checkpoint (omit Stage-1 (8K)) versus from the final step of Stage-1, as shown on the left side of Figure 10. We observe that although Stage-1 training causes an initial drop in AIME25 performance from 48.4 to 44.6, it enables more rapid and consistent improvement during Stage-2. In contrast, skipping Stage-1 results in slower gains and an eventual plateau at lower performance level (56.7 vs. 51.8). As shown on the right side of Figure 10, we observe that the average response length sharply declines from over 5000 tokens to around 4000 tokens at approximately step 600 during stage-1 training. This reduction, occurring between steps 0 and 600, is attributed to the 8K token length limit and coincides with period where the model performance drops. Although continued training after 600 steps improves performance, the average response length does not go up. We conjecture that the 8K token constraint compels the reasoning model to develop more concise thinking process in order to complete responses within the limit and successfully solve the tasks. One motivation for compressing the reasoning process is that the teacher-forced responses used during SFT are generated by DeepSeek-R1-671B, and may be too lengthy and complex for smaller models to effectively learn from or reproduce on their own. This reasoning compression becomes especially valuable during subsequent RL training, where the model maintains conciseness but benefits from longer sequence length, allowing it to tackle more complex problems. In contrast, training an SFT model directly from stage-2 (with 16K limit) bypasses this reasoning compression stage and thus fails to learn such capability. 4.5.5. How long should we train Stage-1 AIME24 avg@64 AIME25 avg@ LCB V5 avg@8 LCB V6 avg@8 Our SFT Model 62.0 48.4 Stage-1 1200 steps + Stage-2 (16K) 61.3(0.7) 65.3(3.3) 44.6(3.8) 56.7(8.3) Stage-1 1600 steps + Stage-2 (16K) 61.6(0.4) 66.3(4.3) 46.5(1.9) 56.7(8.3) Stage-1 2300 steps + Stage-2 (16K) 63.2(1.2) 66.1(4.1) 46.8(1.6) 54.8(6.4) 48.8 48.4 51.6 49.4 51.8 50.1 51. 43.8 43.7 45.7 44.5 45.6 44.9 45.6 Table 3: Studies examining how varying the number of Stage-1 training steps (e.g., 8K) impacts subsequent RL training. The arrow indicates the accuracy comparison between our RL models and initial SFT model on math benchmarks. From Section 4.5.4, we learn that the stage-1 (8K) plays an important role in RL training. Building on this insightand noting that model performance tends to recover and improve with continued stage-1 trainingwe investigate whether extending the duration of stage-1 training further enhances the subsequent RL stages. 14 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Table 3 presents the results of this investigation, showing how varying the number of stage-1 training steps affects final performance at stage-2. We observe that continuing stage-1 training beyond 1200 steps yields slight additional improvements, with models gradually matching or even surpassing the original SFT model on math and coding benchmarks. However, these gains do not consistently translate to better outcomes at the final performance of stage-2. Specifically, models initialized from the stage-1 checkpoints at 1200, 1600, or 2300 steps result in comparable stage-2 performance. We also find that all stage-2 training plateau at similar training steps. This suggests that we could stop stage-1 training early and transition to stage-2, which leads to more substantial improvements. 4.5.6. Math-only RL significantly improves code reasoning Figure 11: LiveCodeBench V5 accuracy over different Math-Only RL stages. In AceReason-Nemotron-1.0 (Chen et al., 2025), we found that math-only RL significantly improves performance on code reasoning benchmarks. We reaffirm this finding using different SFT model as initialization, which are stronger than DeepSeek-R1-Distill-Qwen used in AceReason-Nemotron-1.0. To explore this further, we examine the impact of each math-only RL training stage on coding performance. Figure 11 reveals that the majority of the improvement stems from Stage-2, and Stage-1 can even potentially help improve coding capability for relatively weaker SFT model. Similar to our observations on math benchmarks, Stage-4 yields minor gains in coding performance. Notably, the performance gap in coding narrows considerably over the course of RL trainingfrom an initial 5.5% at the SFT starting point to just 1.6% by Stage-3. This trend mirrors the results observed on math benchmarks, further underscoring the powerfulness of RL training in enhancing model capabilities. 4.5.7. RL improves upon the SFT model in terms of pass@K even when is large The pass@K results from AceReason-Nemotron (Chen et al., 2025) indicate that RL can consistently enhance pass@K with increasing K. As shown in Figure 12, our experiments on math and code benchmarks reveal that RL training consistently improves pass@k accuracy across the range of = 8 to = 128. These findings align with those of AceReason-Nemotron, despite our RL model being initialized from considerably stronger SFT baseline (e.g., SFT-7B v7) compared to the DeepSeek-R1-Distill-Qwen-7B model used in their study. In addition, we observe that for the math benchmark AIME25, the improvement from RL diminishes as increases. For instance, the performance gain decreases from 8.3% at = 8 to just 1.2% at = 128. This is due to AIMEs answer space is quite limited, consisting solely of positive integers with maximum value in the hundreds. In contrast, the gains on coding benchmarks such as LiveCodeBench V5 and V6 remain substantial, with LiveCodeBench V6 still showing 5% improvement at = 128. Pass@k accuracies on math-only RL models from different SFT models can be found in Appendix C. 4.5.8. RL improves over strong SFT model by solving hard problems AceReason-Nemotron (Chen et al., 2025) find that RL training is able to unlock long tail of hard coding problems that the starting SFT model, DeepSeek-R1-Distill-Qwen-7B, fails to solve within 64 or even 15 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 12: Comparison of pass@K scores between AceReason-Nemotron-1.1-7B and the SFT-7B v7 model it is trained from. To compute pass@K, we generate 256 outputs per sample for AIME24 and AIME25, and 128 outputs for LiveCodeBench V5 and V6. We then randomly select outputs, and evaluate pass@K. This procedure is repeated 100 times, and the final pass@K score is computed by averaging the results across all repetitions. attempts. In particular, RL training notably boosts performance on problems where the SFT model achieves less than 20% accuracy. As shown in Figure 13, we observe that this finding still holds true, even though our initial SFT model is substantially stronger than the one used in AceReason-Nemotron-1.0 (Chen et al., 2025). Notably, on LiveCodeBench, we observe AceReason-Nemotron-1.1-7B is able to tackle long tail of hard coding problems that the SFT model fails to solve within 128 attempts, leading to over ten additional problems solved on both LiveCodeBench V5 and V6. Analyses on math-only RL models can be found in Appendix D. 5. Conclusion In this work, we study the training dynamics of supervised fine-tuning (SFT) and reinforcement learning (RL). We begin by analyzing SFT, examining the effects of scaling both the number of unique prompts and the number of responses per prompt. Our results show that both scaling strategies substantially improve the reasoning abilities of large language models (LLMs). Notably, performance consistently improves from the first to the fifth epoch during SFT, with gains plateauing around the fifth or sixth epocheven when scaling the number of responses per prompt. We then conduct systematic study of applying RL across different SFT models. Despite large initial performance differences among these models, RL training significantly reduces the performance gap. We observe that the first stage of RL training may not yield immediate improvements and can sometimes degrade performance. However, it plays critical role in encouraging models to generate more concise reasoning, which proves beneficial in later stages of RL training. Interestingly, even strong SFT models with robust coding abilities benefit substantially from math-only RL training. This leads to further gains in coding performance. Our final 7B model, enhanced with math-focused reinforcement learning, achieves stateof-the-art results among Qwen2.5-based 7B models, scoring 63.2% on AIME25 and 52.8% on LiveCodeBench V5demonstrating strong performance on challenging math and code benchmarks. 16 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 13: Comparison of problem-level solving rates between AceReason-Nemotron1.1-7B and the SFT-7B v7 model it is trained from. For each problem, accuracy is averaged over 256 outputs for AIME24 and AIME25, and over 128 outputs for LiveCodeBench V5 and V6. 6. Acknowledgement We would like to extend our gratitude to the NVIDIA Nemo team for the valuable discussion and collaboration on building reasoning models. We especially wish to thank Boris Ginsburg, Oleksii Kuchaiev, Igor Gitman, Wei Du, Somshubra Majumdar, Siddhartha Jain, Jiaqi Zeng, Yi Dong, Alexander Bukharin, Olivier Delalleau, Tugrul Konuk, Vahid Noroozi, and Jonathan Cohen. AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy"
        },
        {
            "title": "References",
            "content": "[1] Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. 3, 4, 5, 9 [2] Mislav Balunovic, Jasper Dekoninck, Ivo Petrov, Nikola Jovanovic, and Martin Vechev. Matharena: Evaluating llms on uncontaminated math competitions, february 2025. URL https://matharena. ai, 2025. 8 [3] Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-Nemotron: Efficient Reasoning Models. arXiv preprint arXiv:2505.00949, 2025. 3, 4, 8 [4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. 3 [5] Yang Chen, Zhuolin Yang, Zihan Liu, Chankyu Lee, Peng Xu, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acereason-nemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025. 3, 4, 6, 7, 8, 9, 15, 16 [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. 3, [7] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. NVLM: Open frontier-class multimodal LLMs. arXiv preprint arXiv:2409.11402, 2024. 4 [8] Sreyan Ghosh, Zhifeng Kong, Sonal Kumar, Sakshi, Jaehyeon Kim, Wei Ping, Rafael Valle, Dinesh Manocha, and Bryan Catanzaro. Audio flamingo 2: An audio-language model with long-audio understanding and expert reasoning abilities. arXiv preprint arXiv:2503.03983, 2025. 4 [9] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 8 [10] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 3, 4, 5, 6, 8 [11] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, Siyuan Li, Liang Zeng, Tianwen Wei, Cheng Cheng, Bo An, Yang Liu, and Yahui Zhou. Skywork open reasoner series, 2025. Notion Blog. 3, 9 [12] Jujie He, Jiacai Liu, Chris Yuhao Liu, Rui Yan, Chaojie Wang, Peng Cheng, Xiaoyu Zhang, Fuxiang Zhang, Jiacheng Xu, Wei Shen, et al. Skywork open reasoner 1 technical report. arXiv preprint arXiv:2505.22312, 2025. 4, [13] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song, and Jacob Steinhardt. Measuring coding challenge competence with apps. NeurIPS, 2021. 5 18 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. Sort, 2(4):06, 2021. 8 [15] Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024. 5 [16] HuggingFace. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. 9 [17] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2.5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 3, 4 [18] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. 8 [19] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. Numinamath. [https://huggingface.co/AI-MO/NuminaMath-CoT](https: //github.com/project-numina/aimo-progress-prize/blob/main/report/numina_ dataset.pdf), 2024. 5 [20] Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852, 2023. [21] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-V3 technical report. arXiv preprint arXiv:2412.19437, 2024. 3 [22] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum? id=1qvx610Cu7. 8 [23] Jiawei Liu, Songrun Xie, Junhao Wang, Yuxiang Wei, Yifeng Ding, and Lingming Zhang. Evaluating language models for efficient code generation. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=IBCBMeAhmC. 8 [24] Mingjie Liu, Shizhe Diao, Ximing Lu, Jian Hu, Xin Dong, Yejin Choi, Jan Kautz, and Yi Dong. Prorl: Prolonged reinforcement learning expands reasoning boundaries in large language models. arXiv preprint arXiv:2505.24864, 2025. 3 [25] Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. AceMath: Advancing frontier math reasoning with post-training and reward modeling. arXiv preprint arXiv:2412.15084, 2024. 3, 4, 5 [26] Michael Luo, Sijun Tan, Roy Huang, Xiaoxiang Shi, Rachel Xin, Colin Cai, Ameen Patel, Alpay Ariyak, Qingyang Wu, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. 3, 4, [27] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL, 2025. Notion Blog. 4 19 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy [28] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. 3, 4, 5, 8 [29] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand칟s, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. 5 [30] OpenAI. Learning to reason with LLMs, 2024. [31] Qwen-Team. QwQ-32B: Embracing the Power of Reinforcement Learning, 2025. URL https://qwenlm. github.io/blog/qwq-32b/. 3, 4 [32] Ant Research RL Lab. Areal: Ant reasoning rl. https://github.com/inclusionAI/AReaL, 2025. 9 [33] ByteDance Seed, Yufeng Yuan, Yu Yue, Mingxuan Wang, Xiaochen Zuo, Jiaze Chen, Lin Yan, Wenyuan Xu, Chi Zhang, Xin Liu, et al. Seed-thinking-v1. 5: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. 4 [34] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. DeepseekMath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 3, 4, [35] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. 6 [36] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 3, 4 [37] Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond. arXiv preprint arXiv:2503.10460, 2025. 3, 4, 8 [38] Bingquan Xia, Bowen Shen, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, et al. Mimo: Unlocking the reasoning potential of language modelfrom pretraining to posttraining. arXiv preprint arXiv:2505.07608, 2025. 4, 8 [39] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. 8 [40] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-Math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. 3, 4, [41] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. 3, 4 [42] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. DAPO: An open-source LLM reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. 4, 12 [43] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 4 20 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy A. Instruction for evaluation"
        },
        {
            "title": "Math",
            "content": "Please place your final answer inside boxed{}. No Starter Code (Python) Write Python code to solve the problem. Please place the solution code in the following format: python # Your solution code here"
        },
        {
            "title": "Has Starter Code",
            "content": "Solve the problem starting with the provided function header. Function header: <starter_code> Please place the solution code in the following format: python # Your solution code here B. RL training from different SFT models on AIME25 Figure 14: Math-only RL training starting from different SFT (distillation) models. The AIME25 accuracy at step-0 reflects the performance of the initial SFT checkpoints. The subsequent numbers in the figure show the final accuracy achieved at the end of each training stage: Math-Only Stage-1 (8K), Stage-2 (16K), Stage-3 (24K), and Stage-4 (32K). Consistent with the results shown in Figure 7, we observe from Figure 14 that the final performance gaps between RL models initialized from SFT-7B-v5 and SFT-7B-v7 narrows after training. However, substantial gap remains when comparing RL training from SFT-7B-v7 to DeepSeek-R1-Distill-Qwen-7B. This suggests that when the initial SFT models differ significantly in quality, RL has limited ability to further close the performance gap. AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy C. Pass@k Accuracy on Math-Only RL Models Figure 15: Pass@k results on AIME24, AIME25, LiveCodeBench V5, and V6, showcasing two SFT models and their subsequent Math-Only RL-trained versions. To compute pass@k, we generate 256 outputs per sample for AIME24 and AIME25, and 128 outputs for LiveCodeBench V5 and V6. We then randomly select outputs, and evaluate pass@k. This procedure is repeated 100 times, and the final pass@k score is computed by averaging the results across all repetitions. In Figure 15, we demonstrate the pass@k results on two SFT models and their subsequent math-only RL-trained models. We find that the results are consistent with the observations in Section 4.5.7. Similar to the pass@1 results, we observe that RL from weaker SFT model (i.e., SFT v5) leads to greater improvements on pass@k, which further highlights the effectiveness of RL training. Interestingly, we find that even though the RL training is performed solely on math-specific prompts, the resulting models exhibit consistent gains on coding benchmarks (LiveCodeBench V5 and V6) as increases. D. Problem-Level Solving Rates on Math-Only RL Models In Figure 16 and Figure 17, we show the problem-level solving rates between two SFT models and their subsequent math-only RL-trained models. Interestingly, despite being trained solely on math tasks, our RL model also performs well on coding benchmarks, successfully solving number of problems that the initial SFT model could not. 22 AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT and RL Synergy Figure 16: Comparison of problem-level solving rates between the SFT model (v7) and the model after Math-Only RL training. For each problem, accuracy is averaged over 256 outputs for AIME24 and AIME25, and over 128 outputs for LiveCodeBench V5 and V6. Figure 17: Problem-level solving rates comparison between SFT model (v5) and after Math-Only RL training. For each problem, accuracy is averaged over 256 outputs for AIME24 and AIME25, and over 128 outputs for LiveCodeBench V5 and V6."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}