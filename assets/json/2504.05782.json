{
    "paper_title": "MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models",
    "authors": [
        "Pengfei Zhou",
        "Fanrui Zhang",
        "Xiaopeng Peng",
        "Zhaopan Xu",
        "Jiaxin Ai",
        "Yansheng Qiu",
        "Chuanhao Li",
        "Zhen Li",
        "Ming Li",
        "Yukang Feng",
        "Jianwen Sun",
        "Haoquan Zhang",
        "Zizhen Li",
        "Xiaofeng Mao",
        "Wangbo Zhao",
        "Kai Wang",
        "Xiaojun Chang",
        "Wenqi Shao",
        "Yang You",
        "Kaipeng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is a fundamental aspect of human intelligence and a crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12-Bench, a multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on a well-organized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing a robust platform for comprehensive evaluation. Additionally, we present a novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 2 8 7 5 0 . 4 0 5 2 : r MDK12-Bench: Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models Pengfei Zhou1*, Fanrui Zhang2,3, Xiaopeng Peng4, Zhaopan Xu5,1, Jiaxin Ai6,2, Yansheng Qiu6,1, Chuanhao Li1, Zhen Li1, Ming Li1, Yukang Feng2, Jianwen Sun2, Haoquan Zhang1, Zizhen Li2, Xiaofeng Mao1, Wangbo Zhao8, Kai Wang8, Xiaojun Chang3,7, Wenqi Shao1, Yang You8, Kaipeng Zhang1,2 1Shanghai AI Laboratory 2Shanghai Innovation Institute 3USTC 4RIT 5HIT 6WHU 7MBZUAI 8NUS"
        },
        {
            "title": "Abstract",
            "content": "Multimodal reasoning, which integrates language and visual cues into problem solving and decision making, is fundamental aspect of human intelligence and crucial step toward artificial general intelligence. However, the evaluation of multimodal reasoning capabilities in Multimodal Large Language Models (MLLMs) remains inadequate. Most existing reasoning benchmarks are constrained by limited data size, narrow domain coverage, and unstructured knowledge distribution. To close these gaps, we introduce MDK12Bench, multi-disciplinary benchmark assessing the reasoning capabilities of MLLMs via real-world K-12 examinations. Spanning six disciplines (math, physics, chemistry, biology, geography, and information science), our benchmark comprises 140K reasoning instances across diverse difficulty levels from primary school to 12th grade. It features 6,827 instance-level knowledge point annotations based on wellorganized knowledge structure, detailed answer explanations, difficulty labels and cross-year partitions, providing robust platform for comprehensive evaluation. Additionally, we present novel dynamic evaluation framework to mitigate data contamination issues by bootstrapping question forms, question types, and image styles during evaluation. Extensive experiment on MDK12-Bench reveals the significant limitation of current MLLMs in multimodal reasoning. The findings on our benchmark provide insights into the development of the next-generation models. Our data and codes are available at https://github.com/LanceZPF/MDK12. 1. Introduction Reasoning is fundamental to human intelligence, enabling logical, rational, and deliberate thought, inference, and deduction beyond prior knowledge [23, 37]. By integrating vision, language, and symbols, multimodal reasoning improves problem-solving and decision-making abilities based *Equal contribution Corresponding author on diverse information sources. Replicating sophisticated and context-aware multimodal reasoning capabilities is crucial to achieving Artificial General Intelligence (AGI) [27]. With the rapid advancement of Multimodal Large Language Models (MLLMs) [2, 18, 20, 33], reliable benchmarks are needed to assess their real-world reasoning capabilities. Existing multimodal benchmarks focus mainly on basic understanding and low-order reasoning tasks [15, 17, 34], such as numerics [39], common sense [22, 49] and image quality judgment [47]. Unlike low-order reasoning, which relies on common knowledge, high-order reasoning requires stepby-step thinking and systematic analysis capabilities. While most existing benchmarks are restricted to single isolated disciplines (e.g., mathematics [5, 24, 43] and medicine [38, 46]) or common knowledge [13, 50], the evaluation of high-order reasoning has not been fully explored. While several early attempts have been made to evaluate the complex reasoning performance of MLLMs [14, 19], the previous benchmarks still have limitations in data scope, data size, data granularity, or systematic knowledge structuring. As result, these benchmarks lack the breadth and depth to challenge the reasoning capabilities of MLLMs in complicated real-world reasoning tasks. In addition, due to the absence of fine-grained key-point annotations and structured knowledge, these benchmarks fail to trace how MLLMs fail in solving certain problems. To address these challenges, we introduce MDK12Bench, multi-disciplinary benchmark assessing reasoning capabilities of MLLMs at the K-12 level (defined for simplicity as Grades 1 to 12 excluding kindergarten). Spanning from Grade 1 to Grade 12, K-12 education is deeply interwoven with disciplinary examinations for testing knowledge comprehension and high-order thinking skills [7, 21]. In contrast to higher education, where individuals receive in-depth knowledge for professional development through self-guided learning, K-12 offers broad spectrum of subjects that are more structured, well-defined and interconnected. These characteristics make the K-12 domains an ideal testbed for Benchmarks MMBench [22] MMIU [25] MMT-Bench[49] EMMA [13] MMMU [50] DrawEduMath [5] Data Coverage Level #Instances #Images Question Type Low-order Low-order Low-order College College K12-Math 3.2K 11.6K 31.2K 2.7K 11.5K 44K - 77K 31.2K 3K 12.3K 2.3K MC MC, Open MC MC, Open MC, Open Open MDK12-Bench K-12 141.3K 105.2K MC, Fill, T/F, Open T, I+T Modality Explanation Annotation Structured Knowlege Dynamic Evaluation I+T I+T I+T I+T I+T I+T Table 1. Comparison between our MDK12-Bench and existing multimodal reaonsing benchmarks. MDK12-Bench includes more comprehensive data and question coverage. The systematic knowledge structuring and dynamic test-time augmentation also provide more reliable and fair evaluation of MLLMs. T: Text; I: Image. MC: multiple-choice; Open: open-ended; Fill: fill-in-the-blank; T/F: true or false; Low-order reasoning: commonsense, image quality judgement, relation, attribute reasonings, etc. systematically evaluating the knowledge coverage, reasoning, and problem-solving abilities of MLLMs. As shown in Table 1 and Fig. 1, our MDK12-Bench consists of 141.3K reasoning questions and spans 6 reasoningoriented K-12 disciplines: Mathematics, Physics, Chemistry, Biology, Geography, and Information Science. For each discipline, we provide fine-grained annotations including difficulty levels, instance-level key knowledge points, and detailed answer explanations. With cross-year partitions, the MDK12-Bench allows various breakdown analyses, cross-validations, and dynamic updates. Moreover, the instance-level key knowledge point annotation is linked with our constructed knowledge tree, enabling deeper model performance analysis at each knowledge level. Compared to existing benchmarks, our MDK12-Bench potentially provides more systematic evaluation of MLLMs multimodal reasoning capabilities in real-world academic tasks. In addition, current reasoning model evaluations are typically static, which can be biased due to data contamination, i.e., test items appearing in the MLLMs large-scale training data. To address this, we introduce novel dynamic evaluation framework that automatically transforms both the textual and visual parts of questions via different bootstrapping strategies, including word substitution, paraphrasing and question type, permuting for textual bootstrapping and image expansion, color shift, and style transfer for visual bootstrapping. Based on MDK12-Benchs diverse content, this dynamic framework offers more robust and fair platform for evaluating high-order multimodal reasoning in MLLMs. We evaluate various classic and state-of-the-art MLLMs on our MDK12-Bench using the proposed dynamic evaluation method. Extensive experiment results demonstrate that large models trained with reasoning-related data, such as Gemini2.0-flash-thinking and QVQ-72B, generally perform better than the smaller common models. It has also been proven that model performance under the dynamic evaluation setting can face more challenges than the original benchmarks. Our contributions are summarized as follows: Comprehensive Multi-Discipline Benchmark. We present MDK12-Bench, systematically curated, largescale K12-based benchmark supporting the comprehensive evaluation of the reasoning capability of MLLMs. Dynamic Evaluation Framework. practical framework mitigating data contamination and providing flexible multimodal data transformation to challenge MLLMs with bootstrapped unseen data. Comprehensive Leaderboard. We provide detailed analysis of current MLLMs in our leaderboard. Our studies suggest that both our subsets and dynamic evaluation benchmark challenge the reasoning capabilities of current MLLMs, showing that our work can support robust platform for evaluating reasoning-oriented models and challenging future artificial general intelligence attempts. 2. Related Works MLLM Reasoning. Based on the rapid advances of Large Language Models (LLMs) [1, 40, 42], multimodal LLMs (MLLMs) have emerged to address complex multimodal tasks, including interleaved image-text generation [3, 28, 40, 40, 42, 55]. Recent studies have also proposed domain-specific MLLMs utilizing multimodal pretraining, vision instruction tuning, and reinforcement learning, such as Math-LLaVA [36] and MultiMath [32] for mathematical tasks, and Med-Flamingo [26], LLaVA-Med [20], and MedMoE [16] in the biomedical domain. Furthermore, methods like Chain-of-Thought (CoT) prompting [45], iterative bootstrapping techniques and reinforce learning [54], such as STaR [52], Quiet-STaR [51] and DeepSeek-E1 [12], have further improved interpretability and response quality. MLLM Evaluation. With the rapid advancement of MLLMs, various benchmarks have been proposed to assess their performance [24, 25, 35, 53]. However, most benchmarks focus primarily on fundamental perceptual skills, and lack the evaluation of expert-level domain knowledge Figure 1. Overview of MDK12-Bench. It comprises 140K instances and spans 6 disciplines in K-12 education. The knowledge system of our bench is structured into six fine-grained levels: discipline, grade, curriculum, topic, meta-knowledge, and key knowledge points, where the three rings showcase the first three levels. Examples illustrate the representative grades (elementary, middle, and high schools), difficulty levels (easy, medium, and high), questions and answers, and key knowledge points of each discipline. The diverse question forms (singleand multiple-choice, open-ended question, fill-in-blank, true-or-false) and detailed answer explanations are also demonstrated. and deep reasoning, or include reasoning only in limited contexts. For instance, MathVerse [53] emphasizes visual mathematical comprehension, while MMBench examines basic visual understanding and cross-modal fusion [22]. Recently, more comprehensive evaluations have emerged. For instance, MMMU [50] proposed large-scale expert AI challenge across domains like medicine and law. GSM-8K [9] emphasized logical consistency in mathematical reasoning. EXAMS-V [10] extended multilingual, multimodal exam-style assessments. Despite these advances, existing benchmarks still face limitations in data size, knowledge system completeness, and domain diversity, as they often focus on mathematics or narrow set of specialized knowledge. Our benchmark aims to overcome these limitations by evaluating complex reasoning and multi-domain proficiency. Dyanmic Evaluation. Growing doubts about the true capabilities of LLMs in public benchmarks often attribute spurious performances to data contamination. To keep pace with evolving model capabilities and mitigate contamination, researchers have turned to dynamic or adaptive evaluations. Zhu et al. [56] introduced meta-probing agent that continually adjusts test content and difficulty during fine-tuning or domain adaptation, while Yang et al. [48] dynamically modifies visual and textual context to verify the influence of data contamination. These dynamic methods indicate promising future for benchmarks, which must evolve as models expand their training corpora. More robust protocols are needed to distinguish genuine reasoning improvements from mere pattern matching. Therefore, we propose dynamic testing framework for MDK12-Bench to counter data leakage and maintain sustained benchmark integrity. 3. MKD12-Benchmark To address the scarcity of high-quality multimodal academic reasoning benchmarks, We created MKD12-Bench in around two months with the participation of over 20 researchers and several K-12 educators. The data curation of our benchmark involves four stages, as shown in Fig. 2(a). Data Collection. The data collection involves an extensive search of online open-source exam paper repositories. The team established and refined an efficient acquisition workFigure 2. Data curation and statistics of our MDK12-Bench. (a) The data curation pipeline consists of four stages: data collection, screening, parsing, and processing. The knowledge in our benchmark is structured into six interconnected levels: Level 1 - discipline, Level 2 - grade, Level 3 - curriculum, Level 4 - topics, Level 5 - meta-knowledge, and Level 6 - key knowledge point. Statistics of knowledge coverage of our bench is illustrated in terms of the number of instance occurrences at (b) discipline and grade levels; (c) high-school curriculum level; and (d) elementaryand middle-school curriculum level. Examples of curriculum-level knowledge points are also demonstrated. flow to ensure both reliability and broad coverage of the collected questions, laying robust foundation for subsequent screening and parsing. We accumulated large-scale question sets from multiple grade levels, regions, and multimodal formats to form the metadata source for MDK12-Bench. Data Screening. After the initial collection, we utilized combination of GPT-4o-based automated review and human inspection, based on predefined checklist, to filter the invalid metadata. We removed questions containing low-quality images or without specific knowledge points. Additionally, we preserved the question analysis section in each instance to support chain-of-thought studies, along with partition information covering exams from 2016 to 2025, thus allowing examination of potential knowledge evolution over time. Each question instance was assigned difficulty level, facilitating dynamic evaluations and enabling model performance analysis under varying complexity levels. Data Parsing. Following the screening stage, we performed rule-based parsing to transform each question into structured format. This process extracted fields such as Year, Question, Grade Level, Image, Difficulty Level, Answer, Question Type, Knowledge, Course, Analysis, and Modality, all of which were systematically stored. By aligning questions, solutions, and annotations in standardized structure, the resulting dataset significantly enhances retrieval efficiency, allowing models or analysis tools to readily access specific subjects, difficulty levels, or knowledge points. Data Processing. Once the data was parsed, we carried out additional post-processing steps to ensure linguistic and formatting consistency. Leveraging the GPT-4o API, we translated all Chinese text into English, followed by meticulous domain-expert reviews to verify technical accuracy. For images containing Chinese text, we utilized an image translation tool and performed manual checks. Furthermore, we built comprehensive knowledge structure encompassing discipline, grade/year, curriculum, chapter/unit, lesson/topic, and key knowledge points, linking each translated question to this framework to enrich its academic context. Table 2. Key Statistics of MDK12-Bench Overall Statistics Total instances Text-only instances Multimodal instances Total images Exam years coverage 141,320 77,857 63,463 105,218 10 Knowledge Structure Knowledge levels Total knowledge points Level 1&2 knowledge points Level 3 knowledge points Level 4 knowledge points Level 5&6 knowledge points 6 6,827 13 90 499 6, Mini-Subsets Total instances Easy-level instances Medium-level instances Hard-level instances 14,595 4.951 4,692 4,952 Data Statistics. As shown in Table 2 and Fig. 2(b), our Figure 3. The proposed dynamic MLLMs evaluation pipeline. It includes an image and text bootstrapping module to mitigate data contamination and two-stage answer evaluation module comparing the model answers with ground truth. benchmark comprises 141,320 instances, including 77,857 text-only and 63,463 multimodal instances, covering total of 105,218 images over 12 years (2016-2025). Additionally, we define four question formats: multiple-choice (singleanswer and multi-answer), fill-in-the-blank, true-or-false, and open-ended (primarily mathematical calculation). For accelerating assessment, we introduce MDK12-Mini, which includes three datasets. Each consists of 10% of the data from MDK12-Bench sampled at easy, medium, and hard levels respectively. Key knowledge points are uniformly sampled to ensure that each subset includes at least one instance per key knowledge point (each instance may cover multiple knowledge points). As MLLMs reasoning capabilities can also be challenged by text-only prompts, we include both single-modal and multimodal instances in MDK12-Bench. 4. Dynamic MLLM Evaluation 4.1. Bootstrapping Methods As illustrated in Fig. 3, we propose dynamic evaluation framework that introduces controlled perturbations to texts and images during the evaluation of MLLMs by creating new test samples while preserving the accuracy of the answers. Image Bootstrapping. Three strategies are proposed to improve image diversity without changing image semantics: Spatial Transformation. We pad the original image with colors uniformly sampled from black, white, and grey. The padding width is proportional to the image dimension along each side, with the ratios uniformly sampled in the range between 10% and 20%. The image padding allows the evaluation of the models recognition and localization performance in varying spatial visual contexts. Color Transformation. In this step, the colors of the original image were inverted. Salt-and-pepper noise of random noise density is also added. This transformation assesses the models resilience to significant color distortions and random visual artifacts, ensuring it can still accurately identify and reason about objects. Style Transformation. We apply mild style transformations using the Flux-Dev [6] model, introducing subtle style variations without significantly altering its key visual elements and semantics that relate to the question. This tests the models robustness to image style shifts. Textual Bootstrapping. We introduce three methods to modify questions while preserving the answers correctness: Word Substitution. We replace certain keywords with synonyms or contextually related expressions. This tests how well model can maintain an accurate understanding when familiar terms are changed, thus assessing vocabulary sensitivity and semantic generalization. Sentence Paraphrasing. We rephrase entire sentences through variations in sentence structure, word order, or style. This checks whether model can consistently capture the underlying meaning even when the surface form of the text is altered. Question Type Permutation. We convert question from one format to another, such as turning multiple-choice problem into fill-in-the-blank. By changing the required style of the answer, we can see if the model retains key information under different response formats. Throughout the generation process, we apply GPTbased judge to reject sampling wrong adapted instances, guaranteeing that each dynamically altered text and image still aligns with the questions original correct answer. By this framework, we construct diverse versions of the samModels Overall Math Phys ChemBio Geo IS Math Phys ChemBio Geo IS Math Phys ChemBio Geo IS Easy Medium Hard Gemini2-thinking Gemini2-flash Claude-3.7 GPT-o1-mini GPT-4o QVQ-72B Qwen2.5-VL-72B Qwen2.5-VL-7B Qwen2-VL-7B-GRPO Qwen2-VL-7B InternVL2.5-MPO InternVL2.5-78B InternVL2.5-8B 59.4 57.2 49.8 53.1 50.0 53.2 51.9 47.9 44.1 43.8 51.7 48.2 37.7 60.9 56.1 70.3 69.8 59.1 65.3 52.8 52.0 67.0 68.8 57.2 59.3 48.0 55.0 62.7 58.0 64.1 67.2 51.8 53.8 66.2 66.0 55.3 62.0 48.9 48.6 63.2 65.0 53.5 55.4 44.8 51.2 59.0 54.8 60.4 63.3 54.3 47.1 59.8 63.3 50.9 55.0 44.8 43.7 56.4 52.9 48.2 51.1 38.3 44.0 49.2 48.6 45.2 49.9 53.0 53.8 42.3 55.7 55.2 63.1 47.6 44.6 46.9 55.1 50.9 64.8 40.9 54.4 47.5 52.7 64.7 64.6 51.6 55.3 61.4 55.3 46.5 57.6 44.7 46.5 50.1 56.7 49.8 40.7 36.1 46.1 58.3 54.2 49.5 49.7 45.0 51.5 69.3 58.4 48.6 56.4 46.9 43.0 49.2 55.7 57.9 59.0 45.8 63.2 54.1 60.2 58.0 58.3 44.7 48.8 54.9 63.7 57.9 64.8 40.2 43.0 50.8 57.9 47.9 56.9 43.0 45.7 50.4 53.1 53.0 64.6 44.9 54.6 50.8 62.0 44.4 31.9 41.0 46.0 49.3 59.7 41.0 26.6 38.4 48.0 41.5 57.1 49.5 31.0 37.6 48.7 46.2 52.1 45.8 38.7 40.7 45.0 45.2 48.9 44.7 35.7 42.1 46.3 43.1 47.8 45.2 38.4 38.2 55.4 45.7 58.7 44.2 35.4 31.6 45.0 46.6 54.5 39.4 18.8 33.3 45.4 41.1 54.0 42.2 31.2 51.3 51.2 63.1 80.0 62.4 69.9 36.4 38.8 45.5 52.5 47.4 44.1 38.2 42.6 55.9 41.0 53.9 55.5 43.1 54.8 42.2 60.3 58.6 42.7 38.2 47.2 43.7 46.1 57.3 38.9 35.5 45.3 40.9 43.0 54.2 40.8 33.2 47.4 35.0 50.0 51.5 35.9 27.8 38.5 36.9 39.8 50.7 32.0 27.8 36.7 31.5 33.9 44.6 33. Table 3. Performance of MLLMs on six disciplines (mathematics, physics, chemistry, biology, geography, and information science) across three difficulty levels. The overall performance indicates the average accuracy across all grades, difficulty levels and disciplines. Figure 4. Knowledge points (Level 5 - Meta Knowledge) ranked by mean accuracy of Gemini2-thinking on MDK12-Mini dataset. pled data, probing reasoning skills under varying degrees of complexity and avoiding serious data contamination. 4.2. Evaluation Procedures Our evaluation process assesses model responses through multiple steps, as illustrated in Fig. 3. 1) We input either the original or dynamically augmented question into the model. The model then generates response based on textual and visual information. 2) The output is parsed using GPT as an interpreter to extract the final predicted answer from the models response. 3) We compare the extracted answer If they match exactly, the model with the ground truth. receives full credit (a score of 1.0 for that question). If the answer does not match perfectly, we conduct more finegrained check with GPT and pre-defined scoring rules. If question has multiple sub-questions or answers, we count how many elements are correct. For instance, if fill-in-theblank question has two blanks and the model only fills one correctly, we assign score of 0.5. Figure 5. Breakdown of accuracy on MDK12-Mini across different exam years. Figure 7. Test logic of using subsets and the fullset data of MDK12-Bench progressively. 5.2. Results on MDK12-mini the performance of baselines across easy, We present medium, and hard subsets in Table 3. Gemini2-thinking achieves the highest overall accuracy of 59.4%, notably excelling in Chemistry and Biology. Gemini2-flash follows closely with an overall accuracy of 57.2%, surpassing Claude-3.7 and GPT-based series. QVQ-72B achieves the best performance (53.2% overall accuracy) among opensource models, showing superior reasoning performance. InternVL2.5-MPO shows exceptional results in Biology at medium difficulty, though performance in other disciplines is mixed. InternVL2.5-8B consistently underperforms compared with other models. Overall, larger models generally perform better, while variability across different subjects is also observed, implying varied domain challenges. These results highlight the necessity for targeted improvements in discipline-specific multimodal reasoning. Fig. 4 presents the accuracy distribution with respect to specific knowledge points, aggregated across all subjects. Each question is annotated with one or more fine-grained knowledge labels. We highlight that models consistently achieve higher accuracy on frequently covered knowledge points in the training data, while systematically underperforming in areas such as advanced geometry and biochemical processes. The results highlight that our benchmark effectively helps identify specific knowledge gaps within models, enabling targeted in-depth evaluation of the full dataset and facilitating focused improvements in these weaker knowledge areas. Furthermore, Fig. 5 provides year-by-year Figure 6. Accuracy of MLLMs on full-set of MDK12-Bench. We demonstrate the results across six disciplines (mathematics, physics, chemistry, biology, geography, and information science) and three grade levels (elementary, middle school, and high school). 5. Experiments 5.1. Baselines and Setup We evaluate set of both closed-source and opensource MLLMs, including: 1) Closed-source MLLMs: Gemini-2.0-flash-exp [40] (Gemini2-flash), Gemini-2.0flash-thinking-exp [40] (Gemini2-thinking), GPT-4o [29], GPT-o1-mini [30], GPT-o3-mini [31], Claude-3.7-Sonnet (Claude-3.7) [3]. 2) Open-source MLLMs: Qwen2.5-VL [4], InternVL2.5 [8], QVQ-72B-preview (QVQ-72B) [41], InternVL2.5-78B-MPO (InternVL2.5-MPO) [44], etc. 3) Reasoning-oriented R1 variants: Qwen2-VL-7B-GRPO8K [11]. We mainly test the largest available models with certain reasoning capabilities, with smaller ones included for comparison if resources allow. All models are evaluated on three difficulty subsets (easy, medium, and hard). Further technical details will be discussed in the supplement. Model Gemini2-thinking Gemini2-flash Claude-3.7 GPT-4o Qwen2-VL-7B-GRPO Qwen2-VL-7B InternVL2.5-8B Overall Easy Medium Hard Original Dynamic Original Dynamic Original Dynamic Original Dynamic 15.3 4.4 15.3 0.6 6.6 5.0 7.7 51.5 48.9 40.5 35.4 26.5 25.6 38.4 12.2 8.6 13.9 2.4 1.4 0.0 16. 57.0 54.7 50.2 53.7 26.3 25.5 44.1 22.9 16.4 16.9 18.5 3.3 -2.8 25.0 58.1 56.4 46.7 51.2 28.2 27.3 41.7 66.7 66.6 49.2 54.1 32.7 31.8 48.5 16.5 9.4 15.3 10.3 2.2 1.2 15.6 36.2 44.5 25.2 34.8 19.9 20.5 30. 41.6 47.0 31.4 40.9 26.0 26.1 26.1 44.8 46.1 36.3 51.3 24.9 25.4 27.5 43.8 50.1 32.3 35.7 29.4 34.6 23.5 Table 4. Accuracy on the original subset and the corresponding dynamic bootstrapped set. The denotes the accuracy fluctuation. Figure 8. Accuracy of InternVL2.5-8B on the sampled subset using different combinations of dynamic bootstrapping strategies. accuracy breakdown across different difficulty levels, highlighting temporal trends and potential shifts in model performance over time. Findings: The results suggest that there is possibility that earlier exam data is included more in the training set, therefore improving accuracy on these memorized similar questions. 5.3. Results on Full MDK12-Bench We also evaluate each baseline on the entire dataset of 141.3K questions. Fig. 6 summarizes the main results. Running on the full benchmark is computationally intensive; hence we prioritize the representative checkpoints from each model family. Results confirm that the trends seen in the subsets align well with the full set. The Test Logic: As illustrated in Fig. 7, it is noted that our benchmark can serve as an additional testbed after detecting which knowledge the model failed. That is, the new proposed models are first tested on three subsets of MDK12-Bench, and provide knowledge-level performance. After gathering the key knowledge points that models do not perform well, the corresponding full-set data related to these key points can be extracted as targeted subset for an enhanced evaluation. This is preliminary dynamic evaluation process. 5.4. Dynamic Evaluation Results Main dynamic evaluation results. We sampled 50% multimodal instances from MDK12-mini as the original set (including 695 easy-level instances, 818 medium-level instances and 1124 hard-level instances. Then we applied all bootstrapping methods (3 textual and 3 visual) to create augmented test queries. Table 4 summarizes the results for each model under original vs. bootstrapped queries. The main insights are introduced as follows: 1) Models show clear vulnerability to combined textual and visual bootstrapping: The accuracy of MLLMs consistently dropped under combined bootstrapping, highlighting weaknesses in handling simultaneous context shifts. 2) Higher-performing models exhibit greater sensitivity to dynamic perturbations: GPT-4o showed significant accuracy drops (10.3%), emphasizing their heavy reliance on contextual reasoning rather than purely memorized knowledge. In contrast, Qwen2.5-VL-7B showed relatively minor reduction (1.20%), indicating reliance on certain question format distribution and, thus, greater stability under dynamic conditions. 3) High-order reasoning models are particularly sensitive to easier tasks under dynamic conditions: Gemini2-think experienced substantial accuracy decline (22.9%) on easy tasks, suggesting these models rely significantly on precise contextual comprehension. It is easy to be disturbed by altered context outside the distribution. Ablation Study on Bootstrapping Combinations. We conduct an ablation study to analyze various random combinations of bootstrapping methods. Fig. 8 reports the average accuracy fluctuation as we add different transformations. Results suggest the following findings: 1) The composition bootstrapping strategy appears to have the strongest negative effect on model accuracy: Significant reduction in accuracy is observed when multiple image (I) or textual (T) perturbations are combined. Particularly, the combination of I1+I2+I3 and T1+T2+T3 contributed to the strongest overall reductions, including 12.7% and 18.6% respective reductions in image and text. 2) Transformations in text produce stronger reduction in accuracy than images: single textual perturbation (e.g., T1/T2/T3) consistently produces lower accuracy than single visual perturbation (I1/I2/I3) indicating models greater reliance on the textual context during reasoning. 3) The hard tasks are less affected by the combined perturbations compared to easier tasks. For example, the accuracy on hard tasks typically remains stable or even slightly increases with certain compositions (e.g., I1+T1). It is possible that models inherently reason harder to solve hard tasks than simple memorization. These observations indicate the critical need for developing models that are capable of maintaining robust multimodal comprehension and reasoning under dynamic and diverse conditions. 6. Conclusion We introduce MDK12-Bench, comprehensive multimodal benchmark designed to evaluate the reasoning abilities of MLLMs across diverse real-world K-12 tasks. By covering 141K questions that span multiple disciplines and grade levels, structuring the knowledge system through fine-grained knowledge-point annotations and detailed answer explanations, MDK12-Bench fills critical gaps present in existing benchmarks, such as limited data scale, narrow domain coverage, and unstructured knowledge representation. We further proposed dynamic evaluation framework employing multiple textual and visual bootstrapping strategies to mitigate data contamination, ensuring robust, reliable, and fair assessment. Experimental results revealed significant limitations in current state-of-the-art MLLMs, particularly highlighting their sensitivity to contextual changes and task complexity. Our insights highlight the necessity for enhanced multimodal contextual comprehension and reasoning abilities. Future work may focus on improving models resilience to dynamic perturbations of question formats, thereby paving the way toward more robust multimodal reasoning models."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2 [2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 1 [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. 2023. 2, 7 [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [5] Sami Baral, Li Lucy, Ryan Knight, Alice Ng, Luca Soldaini, Neil Heffernan, and Kyle Lo. Drawedumath: Evaluating vision language models with expert-annotated students handdrawn math images. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. 1, [6] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. Accessed: 202411-05. 5 [7] Eason Chen, Danyang Wang, Luyi Xu, Chen Cao, Xiao Fang, and Jionghao Lin. systematic review on prompt engineering in large language models for k-12 stem education. arXiv preprint arXiv:2410.11123, 2024. 1 [8] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. 7 [9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training arXiv preprint verifiers to solve math word problems. arXiv:2110.14168, 2021. 3 [10] Rocktim Jyoti Das, Simeon Emilov Hristov, Haonan Li, Dimitar Iliyanov Dimitrov, Ivan Koychev, and Preslav Nakov. Exams-v: multi-discipline multilingual multimodal exam benchmark for evaluating vision language models. arXiv preprint arXiv:2403.10378, 2024. 3 [11] EvolvingLMMs-Lab. open-r1-multimodal: fork to add multimodal model training to open-r1. https://github. com/EvolvingLMMsLab/openr1multimodal. Accessed: 2025-03-08. 7 [12] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [13] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. arXiv preprint arXiv:2501.05444, 2025. 1, 2 [14] Jiaxing Huang and Jingyi Zhang. survey on evaluation of multimodal large language models. arXiv preprint arXiv:2408.15769, 2024. 1 [15] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 1 [16] Songtao Jiang, Tuo Zheng, Yan Zhang, Yeying Jin, Li Yuan, and Zuozhu Liu. Med-moe: Mixture of domain-specific experts for lightweight medical vision-language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 38433860, 2024. 2 [17] Jihyung Kil, Zheda Mai, Justin Lee, Arpita Chowdhury, Zihe Wang, Kerrie Cheng, Lemeng Wang, Ye Liu, and Wei-Lun Chao. Mllm-compbench: comparative reasoning benchmark for multimodal llms. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 1 [18] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified visionIn International language understanding and generation. conference on machine learning, pages 1288812900. PMLR, 2022. [19] Lin Li, Guikun Chen, Hanrong Shi, Jun Xiao, and Long Chen. survey on multimodal benchmarks: In the era of large ai models. arXiv preprint arXiv:2409.18142, 2024. 1 [20] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 1, 2 [21] Jun Liu, Zile Liu, Cong Wang, Yanhua Xu, Jiayu Chen, and Yichun Cheng. K-12 students higher-order thinking skills: Conceptualization, components, and evaluation indicators. Thinking Skills and Creativity, 52:101551, 2024. 1 [22] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an In European conference on computer all-around player? vision, pages 216233. Springer, 2024. 1, 2, 3 [23] David Lohman and Joni Lakin. Intelligence and reasoning. The Cambridge handbook of intelligence, pages 419441, 2011. 1 [24] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2023. 1, [25] Fanqing Meng, Chuanhao Li, Jin Wang, Quanfeng Lu, Hao Tian, Tianshuo Yang, Jiaqi Liao, Xizhou Zhu, Jifeng Dai, Yu Qiao, et al. Mmiu: Multimodal multi-image understanding for evaluating large vision-language models. In The Thirteenth International Conference on Learning Representations. 2 [26] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. 2 [27] Meredith Ringel Morris, Jascha Sohl-Dickstein, Noah Fiedel, Tris Warkentin, Allan Dafoe, Aleksandra Faust, Clement Farabet, and Shane Legg. Levels of agi: Operationalizing progress on the path to agi. arXiv preprint arXiv:2311.02462, 2023. 1 [28] OpenAI. Gpt-4v(ision) system card. 2023. 2 [29] OpenAI. Gpt-4o: multimodal language model, 2024. Accessed: 2025-03-08. 7 [30] OpenAI. Gpt-o1-mini: multimodal language model, 2024. Accessed: 2025-03-08. 7 [31] OpenAI. Gpt-o3-mini: cost-effective reasoning model, 2025. Accessed: 2025-03-08. 7 [32] Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, and Zhi Tang. Multimath: Bridging visual and mathematical reasoning for large language models. arXiv preprint arXiv:2409.00147, 2024. 2 [33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 1 [34] Navid Rajabi and Jana Kosecka. Gsr-bench: benchmark for grounded spatial reasoning evaluation via multimodal llms. In NeurIPS 2024 Workshop on Compositional Learning: Perspectives, Methods, and Paths Forward. [35] Tanik Saikh, Tirthankar Ghosal, Amish Mittal, Asif Ekbal, and Pushpak Bhattacharyya. Scienceqa: novel resource for question answering on scholarly articles. International Journal on Digital Libraries, 23(3):289301, 2022. 2 [36] Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See Kiong Ng, Lidong Bing, and Roy Lee. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 46634680, 2024. 2 [37] Robert Sternberg. Reasoning, problem solving, and intelligence. Handbook of human intelligence, pages 225307, 1982. 1 [38] Yuxuan Sun, Hao Wu, Chenglu Zhu, Sunyi Zheng, Qizi Chen, Kai Zhang, Yunlong Zhang, Dan Wan, Xiaoxiao Lan, Mengyue Zheng, et al. Pathmmu: massive multimodal expert-level benchmark for understanding and reasoning in In European Conference on Computer Vision, pathology. pages 5673. Springer, 2024. 1 [39] Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1363613645, 2023. 1 [40] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 7 [41] Qwen Team. Qvq: To see the world with wisdom, 2024. 7 [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. ppo: Reinforced token optimization for rlhf. arXiv preprint arXiv:2404.18922, 2024. 2 [55] Pengfei Zhou, Xiaopeng Peng, Jiajun Song, Chuanhao Li, Zhaopan Xu, Yue Yang, Ziyao Guo, Hao Zhang, Yuqi Lin, Yefei He, et al. Gate opening: comprehensive benchmark for judging open-ended interleaved image-text generation. arXiv preprint arXiv:2411.18499, 2024. 2 [56] Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, and Xing Xie. Dynamic evaluation of large language models by meta probing agents. In Forty-first International Conference on Machine Learning, 2024. 3 [43] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2025. 1 [44] Weiyun Wang, Zhe Chen, Wenhai Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Jinguo Zhu, Xizhou Zhu, Lewei Lu, Yu Qiao, and Jifeng Dai. Enhancing the reasoning ability of multimodal large language models via mixed preference optimization. arXiv preprint arXiv:2411.10442, 2024. 7 [45] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-ofthought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837, 2022. 2 [46] Peng Xia, Ze Chen, Juanxi Tian, Gong Yangrui, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. In The Thirtyeight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2025. [47] Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, and Yingcong Chen. Seed-story: Multimodal long story generation with large language model. arXiv preprint arXiv:2407.08683, 2024. 1 [48] Yue Yang, Shuibai Zhang, Wenqi Shao, Kaipeng Zhang, Yi Bin, Yu Wang, and Ping Luo. Dynamic multimodal evaluation with flexible complexity by vision-language bootstrapping. arXiv preprint arXiv:2410.08695, 2024. 3 [49] Kaining Ying, Fanqing Meng, Jin Wang, Zhiqian Li, Han Lin, Yue Yang, Hao Zhang, Wenbo Zhang, Yuqi Lin, Shuo Liu, et al. Mmt-bench: comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi. In Forty-first International Conference on Machine Learning. 1, 2 [50] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. 1, 2, 3 [51] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-STaR: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. 2 [52] Eric Zelikman, YH Wu, Jesse Mu, and Noah Goodman. Star: Self-taught reasoner bootstrapping reasoning with reasoning. In Proc. the 36th International Conference on Neural Information Processing Systems, 2024. [53] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly In European see the diagrams in visual math problems? Conference on Computer Vision, pages 169186. Springer, 2024. 2, 3 [54] Han Zhong, Guhao Feng, Wei Xiong, Xinle Cheng, Li Zhao, Di He, Jiang Bian, and Liwei Wang. Dpo meets"
        }
    ],
    "affiliations": [
        "HIT",
        "MBZUAI",
        "NUS",
        "RIT",
        "Shanghai AI Laboratory",
        "Shanghai Innovation Institute",
        "USTC",
        "WHU"
    ]
}