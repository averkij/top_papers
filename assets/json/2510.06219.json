{
    "paper_title": "Human3R: Everyone Everywhere All at Once",
    "authors": [
        "Yue Chen",
        "Xingyu Chen",
        "Yuxuan Xue",
        "Anpei Chen",
        "Yuliang Xiu",
        "Gerard Pons-Moll"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a single forward pass (\"all-at-once\"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream applications.Code available in https://fanegg.github.io/Human3R"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 9 1 2 6 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "HUMAN3R: EVERYONE EVERYWHERE ALL AT ONCE Yue Chen1 Xingyu Chen1 Yuxuan Xue2 Anpei Chen1 Yuliang Xiu1 Gerard Pons-Moll2,3 1Westlake University 2Uni of Tubingen, Tubingen AI Center Project Lead Corresponding Author 3Max Planck Institute for Informatics Figure 1: Given stream of RGB images as input, Human3R enables human-scene reconstruction in an online, continuous manner, estimating global multi-person meshes, camera parameters, and dense scene geometry with each incoming frame in real time."
        },
        {
            "title": "ABSTRACT",
            "content": "We present Human3R, unified, feed-forward framework for online 4D humanscene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contactaware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (everyone), dense 3D scene (everywhere), and camera trajectories in single forward pass (all-at-once). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3Rs rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in one-shot manner, along with 3D scenes, in one stage, in real-time (15 FPS) with low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with single unified model. We hope that Human3R will serve as simple yet strong baseline, which can be easily adapted for downstream applications. Code available in fanegg.github.io/Human3R."
        },
        {
            "title": "INTRODUCTION",
            "content": "Humans do not exist in isolation but constantly move in, interact with, and manipulate the world around us. Thus, understanding human behaviors requires putting them within 3D world context, ideally in an online manner, as indicated in Fig. 2. In the field of 3D vision, this necessitates the 3D reconstruction of both global human motions and the surrounding scene from visual data [30], which is challenging, but fundamental for various downstream applications, including AR/VR, autonomous navigation, humanoid policy learning, and human-scene interaction. Prior global human motion estimators typically follow one of two strategies: 1) directly estimating the global human motions aided with learned motion priors [72, 110]; 2) transforming human motion to world coordinates with SLAM-based [91] estimated global camera [44, 48, 80, 82, 89, 99, 109]. Considering the surrounding 3D scene, which is crucial for contextualizing human actions, recent advances attempt to jointly reconstruct 3D humans, scene, and cameras, either from multi-view images [16, 56, 74] or monocular videos [53]."
        },
        {
            "title": "Preprint",
            "content": "However, these methods have two main limitations: 1) Multi-stage/model/shot: They [53, 56] reconstruct the scene and humans separately, then jointly refine them under contact constraints. The entire pipeline takes hours. In addition, top-down multi-person mesh regressor is used, which requires off-the-shelf human detection and human tracking models [15, 41, 50, 71, 73, 106] to crop and associate each person before feeding into the single-person mesh regressor [31, 99], thus the inference speed considerably drops for images with multiple people. 2) Heavy dependencies: Apart from the modules mentioned above, numerous off-the-shelf dependencies are needed to preprocess the input images, including but not limited to metric depth estimators [4], generic 3D reconstruction models [24, 46, 91, 98] to obtain the 3D scene pointcloud, camera pose and intrinsics. Both limitations hinder real-time online inference, end-to-end learning, effortless deployment, and scalability to long sequences. We seek unified one-stop solution. Figure 2: Human behaviors (i.e., grocery shopping) become clearer when viewed within their surrounding environment. (a) w/o vs. w/ scene context (b) Raw capture We introduce Human3R, an all-at-once model for 4D human-scene reconstruction. The term all-atonce reflects several key aspects: 1) One Model: unified model jointly reasons about humans, scene, and camera, rather than relying on separate off-the-shelf models for each component. 2) One Stage: In contrast to prior work with iterative refinement, our method runs in an online fashion, operates on streaming video at real-time speed (15 FPS) without compromising accuracy. 3) One Shot: With bottom-up multi-person SMPL-X regressor, our model can reconstruct multiple persons in single forward pass. 4) One GPU, One Day: Our model is parameter efficient, requiring only one day of training on single NVIDIA 48GB GPU, still yielding state-of-the-art performance. The main challenge in building such unified model lies in the lack of large-scale video datasets with reliable annotations of global human motion, 3D scene, and camera pose. Existing real datasets [21, 33, 39, 94] are limited in scale, while synthetic ones, like BEDLAM [5], are limited in scene variations. Our key idea is to leverage the strong spatiotemporal priors [13, 27, 108] learned by 4D reconstruction foundation model [97], and extend it through minimal tuning on relatively small-scale human-scene dataset, achieving both data and parameter efficiency. This approach enables us to advance from point-only reconstruction to the joint reconstruction of dense scene point clouds and sequential SMPL-X body meshes [64] for multiple individuals in the scene. Specifically, we build upon CUT3R [97], recurrent 4D reconstruction foundation model for online metric-scale reconstruction, which maintains persistent internal state that encodes everywhere and everyone, and incrementally updates it with new observations. We finetune CUT3R via visual prompt tuning (VPT) [35], with minimal learnable parameters prepended into the input space while the entire CUT3R backbone is kept frozen. BEDLAM [5] serves as our training data, which is small-scale yet high-quality, with 6k sequences featuring 3D scene depth, camera poses, and SMPL-X meshes of multiple persons in the world coordinates. Instead of naively prepending random initialized learnable tokens as visual prompts [35], we detect the human head tokens from CUT3Rs image feature, complement it with human prior tokens [3] learned from human-specific datasets, and project them to human prompts using learnable MLP, as shown in Fig. 4. Our proposed human prompts are highly informative, as the head is the most discriminative keypoint on human bodies [3, 116]. As anchors (i.e., SMPL-X queries), these human prompts provide strong spatial priors for localizing and reconstructing the full human body. They self-attend to image tokens for spatial whole-body information aggregation, and cross-attend to the persistent internal state to make 3D human estimates scene-aware. Remarkably, Fig. 8 shows that the 3D scene reconstruction is also improved after finetuning for human reconstruction, demonstrating the mutual benefits of joint reasoning about humans and scene. Simple yet effective, Human3R leverages the spatial and temporal priors learned by CUT3R to reason about humans, scene, and camera in unified framework, efficiently processes long sequences with linear computational complexity (8 GB GPU memory footprint, 15 FPS inference speed), and supports scalable sequence length (thousands of frames) beyond training length (4 frames) by simply rolling out the state. Across various 4D tasks including video depth estimation, camera pose estimation, human mesh recovery, and global human motion estimation our method achieves superior performance over task-specific baselines while offering unified and real-time solution."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Local Human Mesh Recovery. Previous works on human mesh recovery (HMR) primarily focus on estimating the pose and shape parameters of parametric body model, like SMPL [54], SMPLX [64], and GHUM [105], in the camera frame. Early optimization-based methods fit SMPL model to IMU trajectories [94, 103] or to 2D landmarks by minimizing reprojection errors [6, 65]. In contrast, learning-based approaches, trained on large-scale image-body pairs, can regress SMPL parameters from images [37, 59] in single pass. Progress in this field spans improvements in network architectures [25, 31, 52, 111], training and testing paradigms [19, 45, 76], kinematics designs [47, 49], camera models [43, 62], datasets [5, 25, 29, 36, 63], expressive body models [18, 28, 49, 64, 112], temporal consistency [17, 38, 42], and etc. For multi-person scenarios, most prior works adopt top-down multi-stage approach: detect and crop each person before running single-person HMR. This is computationally expensive, scales poorly with more people, and often fails in crowded scenes due to occlusion and truncation. To overcome this, bottom-up methods [3, 87, 88, 100] recover multiple human meshes from full image in one-shot scheme. Multi-HMR, for example, finetunes DINOv2 [60] on synthetic datasets [5, 63], and achieves strong performance. Our goal is even more ambitious: to reconstruct both the 3D scene and multiple humans in the world frame from monocular videos, using one unified model, in one forward pass, and in real-time. (a) Before Global Human Motion Estimation. Reconstructing world-grounded humans from long video sequences is an ill-posed problem, typically requiring additional priors or constraints. GLAMR [110] leverages the learned motion prior HuMoR [72] to infill occluded human motions and directly predict global trajectories from them. With SLAM (Simultaneous Localization and Mapping) [91], world-frame camera poses can be estimated, allowing local human meshes recovered via HMR to be transformed into the world frame [48, 109]. TRAM [99] robustifies and metrifies SLAMs camera estimation via masking the dynamic regions and estimating metric depth via ZoeDepth [4], which then serve as reference frame to recover the global human motion. GVHMR [80] introduces gravity and view-in direction constraints to further stabilize global human motions. Beyond these offline solutions, several online methods [82, 89] recurrently reconstruct global human meshes, maintaining consistently low memory and computation costs as the number of input frames increases. However, even excluding the SLAM step, most of these approaches still depend on multiple off-the-shelf estimators such as human detection [48, 80, 82, 99, 109, 110], tracking [48, 80, 82, 99, 109, 110], segmentation [99], 2D keypoint detection [48, 80, 82, 109, 110], optical flow [89], camera-frame HMR [48, 99, 109, 110], and etc. Synchronization barriers between these branches often lead to cumulative errors and high computational overhead. In contrast, Human3R is an all-in-one model that not only online recovers human motions and root trajectories in the world frame, but also simultaneously reconstructs the surrounding 3D scene and estimates camera motions an versatile framework not explored in prior works. (b) Ours Figure 3: Multi-stage vs. One-stage. Human-Scene Reconstruction. Existing methods for joint human-scene 3D reconstruction typically perform global optimization over camera poses, pre-reconstructed scenes [24, 51, 79, 98] (please checkout related work about generic 3D reconstruction in Sec. of Sup.Mat.), and SMPL mesh parameters inferred from multi-view images [56, 66], often regularized by learned motion priors [2, 53, 115]. Recently, optimization-free approaches have emerged: HAMSt3R [74], for example, jointly reconstructs the scene and DensePose [32] from multi-view images in feed-forward manner, then fits SMPL meshes to the DensePose outputs. The most relevant work, JOSH3R [53], jointly reconstructs scene and human meshes from monocular videos with dynamic humans, but depends on camera-frame human meshes, detection, segmentation, and tracking, limiting scalability and efficiency. We eliminate all these dependencies, resulting in lightweight yet unified model that directly predicts metric-scale dense scenes, global human motions, and camera poses from monocular video in single forward pass. This unified approach distinguishes our method from previous works and opens up new possibilities for real-time applications in humanoid policy learning, autonomous navigation, and human-robot interaction."
        },
        {
            "title": "3 METHODS",
            "content": "t RV 3}N n=1 in the world coordinate system, where each Mn Our approach operates on continuous stream of images in an online manner. At each timestep t, given an input image It RW H3, our goal is to estimate: 1) set of human meshes {Mn is parameterized by the SMPL-X body model with = 10,475 vertices and = 54 joints; 2) the camera extrinsic pose Tt R34, and intrinsic Ct R33; 3) the canonical point cloud Xt RW H3. Our feedforward inference operates online in real time. We first introduce preliminaries of the 3D human parametric model and the 4D reconstruction foundation model CUT3R [97] in Section 3.1. Then, in Section 3.2, we describe our proposed Human3R, which fine-tunes CUT3R to regress SMPL-X parameters for multiple 3D human bodies."
        },
        {
            "title": "3.1 PRELIMINARIES",
            "content": "Human Mesh Representation SMPL-X [64]. We represent the 3D human body with the SMPL-X [54, 64], which is low-dimensional parametric model of the human body mesh. Given the parameters of the local human pose (relative axis-angle rotations) θ R523, body shape β R10, facial expression α R10, and global human root transformation = [R t] SE(3) parametrized by global orientation SO(3) and global translation R3, it outputs an expressive 3D human RV 3, with = 10,475 vertices. For brevity, we omit the timestep subscript and the mesh Mn id superscript n, as Mn M: = SMPL-X(θ, β, α, P) = TPcam (1) where the global human root transformation P, in the world frame, is decomposed into the camera pose and the local root transformation Pcam in the camera frame. 4D Reconstruction Foundation Model CUT3R [97]. To overcome the scarcity of world-grounded 4D human-scene datasets, we exploit the 4D reconstruction foundation model CUT3R [97], which is 4D-aware, and encodes rich 4D priors of real-world dynamics, including both scene (everywhere) and human (everyone), learned from large-scale 3D point cloud datasets. However, instead of explicitly separating the unstructured point clouds of humans from the scene, Human3R directly reads out global human bodies. CUT3R performs recurrent reconstruction of metric-scale point maps (pixel-aligned point clouds in the world coordinate system) and camera poses in an online fashion, maintaining fixed-size memory state that encodes everything that camera captures. This state enables the retrieval of past observations, while being continuously updated with new observations. Specifically, to transform current image It into pixel-aligned point maps, the input image is encoded into set of image tokens Ft R(hw)c through the ViT image tokenizer [23]: Ft = Encoder(It). The image tokens then interact with the state in the following formulation: [F t, t], St = Decoders([Ft, z], St1) (2) where the init state representation is represented as set of tokens S0 R768768, which are learnable parameters and are shared by all scenes. As the set of image tokens Ft is fed into the decoder, the previous state St1 is updated with new observations to produce an updated state St, which encodes the spatial and temporal history of the scene, namely context. Then, through the decoder, the image token Ft and camera token zt, attend with the context in current state St, will be refined as t. The camera token, designed to capture the image-level ego motion related to the scene, is prepended to the image tokens and is initialized as learnable parameter z. This bidirectional state-token interaction is implemented using two interconnected transformer decoders [98, 101, 102]. and After the state-token interaction, the corresponding pixel-aligned metric scale (i.e., meters) 3D pointmaps in the camera and world coordinate systems are extracted via dense prediction head [70]: t, Xcam t). The camera pose Tt is then regressed from camera tokens through an MLP network: Tt = Headpose(z t), and the camera intrinsic Ct is solved using Weiszfeld [68] algorithms with predicted pointmaps, respectively. = Headcam(F = Headworld(F t), Xworld"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Method Overview. Human3R enables online human-scene reconstruction from video streams. Each frame is encoded into image tokens, with patch-level detection. Each detected head token, concatenated with human prior token from Multi-HMR [3] ViT-DINO feature, is projected into human prompt. The human prompts serve as discriminative human-ID queries for the decoder: they self-attend with image tokens to aggregate spatial whole-body information and cross-attend with the scene state to retrieve temporally consistent human tokens within the 3D scene context. Only human-related layers are fine-tuned, other parameters remain frozen and are initialized from CUT3R [97]."
        },
        {
            "title": "3.2 HUMAN3R",
            "content": "One-stage Global Human-Scene Reconstruction. To preserve the rich 4D priors encoded by CUT3R, we adopt parameter-efficient visual prompt tuning (VPT) [35] for fine-tuning. Specifically, we introduce small set of trainable parameters prepended as visual prompts into the input space to enable the readout of global human meshes, while keeping the entire CUT3R backbone frozen. Unlike standard VPT, where additional parameters are randomly initialized learnable tokens, we instead detect human head tokens and transform them into human prompts using learnable projection layers. Specifically, we follow previous work [3] to detect the human head (defined by the head joint of SMPL-X model) as the primary keypoint of human. For each patch index (i, j) {1, . . . , h} {1, . . . , w}, we predict whether the patch i,j contains the primary keypoint by computing confidence score from the associated image feature token Fi,j Rc using an MLP followed by sigmoid activation σ(), formulated as si,j = σ (cid:0)MLPhead(Fi,j)(cid:1). We apply threshold τ on si,j to collect detected head token indexes, denoted as (cid:8)u i,j (cid:12) n. We then predict the human mesh parameters Yt = {(θ, β, α, Pcam)t}n for all people with detected head tokens Fu (i, j) {ut}n} in parallel: Figure 5: Detection and Segmentation. (cid:12) si,j τ (cid:9) = {Fi,j Ht = Headprojection(Fu ) [F t, t, t], St = Decoders([Ft, z, Ht], St1) (3) Yt = Headhuman(H t) where human prompts Ht is transformed from detected head tokens Fu via the projection MLP, and the SMPL-X parameters Yt are predicted by the human MLP from the refined human token t. Ht is inserted into the input space of the decoder. The colors and indicate learnable and frozen parameters, respectively. During fine-tuning, only the human-related MLP layers are updated, while all other parameters remain frozen. The human prompts serve as discriminative human ID queries: they self-attend with image tokens to aggregate spatial whole-body information and cross-attend with the scene state to retrieve temporal SMPL-X mesh parameters within the 3D scene context. Human Prior. In practice, we found that CUT3R, trained on large-scale scene-centric datasets, lacks detailed human priors, leading to suboptimal performance in reconstructing fine-grained human poses and shapes. Thus, we enhance the head tokens Fu with extra human-specific features from humancentric image encoder. Particularly, we use another image tokenizer, the Multi-HMR [3] ViT image encoder, denoted as EncoderHMR, which fully fine-tuned the pretrained DINO [10, 60] on humanspecific datasets. Same as previous index-based query, we still use {u}n to obtain the corresponding Multi-HMR ViT image tokens FHMR = EncoderHMR(I), to produce Fu HMR (i, j) {u}n}, which are subsequently concatenated with CUT3R head tokens Fu and translated into human prompts by the projection MLP as: = Headprojection(Fu Fu HMR), where denotes concatenation along the channel axis. Notably, EncoderHMR is frozen during training. Concatenating Multi-HMR and CUT3R head tokens injects detailed human priors for improved body pose and shape prediction. And with additional training-free designs, Human3R also supports human segmentation and tracking. HMR = {Fi,j"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Tracking. Human Segmentation and Tracking. For segmentation, we predict whether each patch (i, j) contains human parts by generating score vector mi,j R(1616)1 from the corresponding image token Fi,j Rc. This is achieved by passing Fi,j through an MLP, applying sigmoid activation, and then using pixel shuffle [81] to produce pixel-aligned dense mask: mi,j = PixelShuffle (cid:0)σ (cid:0)MLPmask(Fi,j)(cid:1)(cid:1). We perform human tracking by leveraging the discriminative features encoded in the refined human token H, which encapsulates both human identity and human parameters. This enables us to formulate human tracking as feature matching problem [77], where tracklet association is achieved by matching the refined tokens across timesteps. We maintain human token tracklet [69] indexed by = {1, . . . , } after each step of the online processing, which allows us to build memory bank for all observed humans, and derive soft assignment matrix [0, 1]M for current detections indexed by = {1, . . . , }. To estimate the likelihood of given tracklet-detection pair, we use the pairwise L2 distance Dm,n = Hm 2, (m, n) to obtain the cost matrix RM . To suppress unmatched human tokens, we augment the cost to R(M +1)(N +1) by appending new row and column dustbin with threshold, so that unmatched human tokens are explicitly assigned to it. The assignment with dustbin can be solved by optimal transport [67] with the Sinkhorn algorithm [20] to minimize the total cost (cid:80) m,n Dm,n Am,n under the constraints of 1N +1 = and 1M +1 = b, where = [1 ], denote the number of expected matches for each human token and dustbin in and B. ] and = [1 Hn Training Strategy. We finetune CUT3R on synthetic dataset, BEDLAM [5], which is smallscale yet high-quality, with 6k sequences featuring 3D scene depth, camera poses, and SMPL-X meshes [64] of multiple persons in the world coordinates. Following CUT3R and MASt3R, we apply confidence-aware 3D regression loss Lpointmap to the metric-scale pointmaps, as well as camera pose loss Lpose to the ground-truth camera poses. This helps prevent CUT3R from forgetting the rich spatial and temporal priors learned from large-scale 3D scene datasets. To readout human from CUT3R, we follow Multi-HMR to minimize binary cross-entropy loss Ldetection on si,j, L1 regression losses Lsmpl to human parameter Yt, Lmesh to explicit human meshes, and reprojection loss Lreproj.. With our efficient human prompt tuning protocol, Human3R requires just one day of training on single NVIDIA 48GB GPU, and still achieves state-of-the-art performance. Please checkout more training details in Sec. of Sup.Mat. Test-Time Sequence Length Adaptation. Trained with sequences of only 4 images, we observe that performance of Human3R degrades when the inference sequence length exceeds the training context. This is common issue for RNN-based methods [9, 14, 95, 104], including CUT3R [97], where the state tends to forget earlier frames, resulting in significant performance drops as the number of input views increases. To address this limitation and support longer sequence, we adopt TTT3R [12], which parameterizes the state as fast weight [78] and updates it using gradient descent: St = St1βt(St1, Ft, z), where (St1, Ft, z) denotes the gradient function and βt is the learning rate. Intuitively, this Test-Time Training (TTT) [90] procedure adaptively encodes the current observation into the memory state using dynamic learning rate, enabling online adaptation. This approach effectively balances the retention of historical context with the integration of new observations. We follow TTT3R to use the spatial average of the attention values as closed-form update rule for online associative recall in test time, and formulate the state update as: St = St1 βt(St1, Ft, z, Ht). Inspired by the correlation between length generalization and unexplored state distributions [75], we further propose state reset process: the state is reset every 100 frames, using the global camera pose as cue to align the resulting chunks."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We unfold the validation of Human3R and the baselines on human mesh recovery in the camera coordinates (Sec. 4.1) and the world coordinates (Sec. 4.2) respectively, and then compare our model with current state-of-the-art genetic 3D reconstruction methods in camera pose estimation and video depth estimation (Sec. 4.3). We also analyze the components of Human3R in Sec. 4.4."
        },
        {
            "title": "Preprint",
            "content": "Category Method Crop-free Detection-free Multi-stage One-stage CLIFF [52] HMR2.0a [31] TokenHMR [25] CameraHMR [62] NLF [76] PromptHMR [100] BEV [88] Multi-HMR [3] Human3R Intrinsic-free 3DPW (14) PA-MPJPE MPJPE 43.0 44.4 44.3 38.5 37.3 36.6 46.9 45.9 44.1 69.0 69.8 71.0 62.1 60.3 58.7 78.5 73.1 71. EMDB-1 (24) PA-MPJPE MPJPE 68.3 61.5 55.6 43.7 41.2 41.0 70.9 50.1 48.5 103.3 97.8 91.7 73.0 69.6 71.7 112.2 81.6 73.9 PVE 123.7 120.0 109.4 85.4 82.4 84.5 133.4 95.7 86.0 PVE 81.2 82.2 84.6 72.9 71.4 69.4 92.3 87.1 84.9 Table 1: Evaluation of local human mesh reconstruction on 3DPW [94] and EMDB-1 [39] datasets. Preprocessed Input ( = not required) Output EMDB-2 (24) RICH (24) Category Offline Online Method GLAMR [110] SLAHMR [109] COIN [48] GVHMR [80] TRAM [99] JOSH [53] TRACE [89] WHAM [82] JOSH3R [53] Human3R Detection Tracking LocalHuman Camera Mask Depth Contact GlobalHuman CameraPose Scene WA-MPJPE W-MPJPE RTE WA-MPJPE W-MPJPE RTE 726.6 776.1 407.3 276.5 222.4 174.7 1702.3 354.8 661.7 267.9 236.2 237.1 254.5 126.3 238.0 132.5 925.4 196.1 - 184.9 280.8 326.9 152.8 111.0 76.4 68.9 529.0 135.6 220.0 112. 3.8 6.4 - 2.4 6.0 3.0 610.4 4.5 - 3.3 129.4 132.2 169.5 78.8 127.8 89.0 238.1 108.4 - 110.0 11.4 10.2 3.5 2.0 1.4 1.3 17.7 6.0 13.1 2.2 Table 2: Evaluation of global human motion estimation on EMDB-2 [39] and RICH [33] datasets. 4.1 LOCAL HUMAN MESH RECONSTRUCTION We evaluate human pose and shape reconstruction in camera coordinates on 3DPW [94] and EMDB (subset 1) [39], and follow the commonly used local human mesh reconstruction metrics as prior works [3, 100]: mean per-joint position error (MPJPE), Procrustes-aligned per-joint position error (PA-MPJPE), and per-vertex error (PVE) measured in millimeters (mm). We compare with both multi-stage and one-stage leading methods in Tab. 1. Most multi-stage methods rely on human detection and cropping, processing each detected person individually. Without additional cropping, PromptHMR [100] takes the full image as input and prompt it with bounding-box prompts, and achieves strong performance. Among one-stage models, Multi-HMR [3] eliminates the need for off-the-shelf human detectors, but still requires ground-truth camera intrinsics. BEV [88] removes the dependency on ground-truth intrinsics, aligning with our experimental setting. Our approach surpasses these methods across all metrics, demonstrating substantial performance improvements (10% improvement on MPJPE and PVE on EMDB-1), which we attribute to the spatiotemporal awareness provided by CUT3R as generic 4D reconstruction model. 4.2 GLOBAL HUMAN MOTION ESTIMATION We evaluate motion and trajectory estimation accuracy in world coordinates on EMDB (subset 2) [39] and RICH [33], both feature long sequences with ground-truth global human trajectories and meshes. Following previous work [82, 99], we divide each sequence into 100-frame segments and evaluate 3D joint errors using two metrics: W-MPJPE, which aligns the first two frames, and WA-MPJPE, which aligns the entire segment. Both metrics are reported in millimeters (mm). To comprehensively assess trajectory accuracy over long sequences, we additionally report the root translation error (RTE, in %) after rigid alignment (without scaling), normalized by the total displacement. We compare with both offline and online methods in Tab. 2. Given multiple offline pre-cached conditions, GVHMR [80] and JOSH [53] respectively achieve strong performance on sequences with static cameras (RICH) and long human trajectories (EMDB-2). JOSH3R [53], trained with multi-stage pseudo ground truth from JOSH, removes the need for pre-cached camera poses, depth, contact, and iterative refinement. It enables online prediction of global human trajectories, scene points, and camera poses, but with 2 drop in accuracy compared to WHAM and still requires precomputed human detection, segmentation, and meshes in camera coordinates. TRACE [89] takes only RGB video as input, matching our experimental setting, but outputs only global human meshes. In contrast, our method also reconstructs scene geometry and estimates camera poses. In summary, Human3R jointly reconstructs multiple human meshes and trajectories in world space, scene geometry, and camera poses, achieving notable gains (20% lower W-MPJPE and 60% lower RTE against WHAM on EMDB-2), while enabling online inference and end-to-end training. We visualize the global human motion estimation within the dense scene, together with the predicted camera trajectory, in Fig. 7."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Qualitative 4D human-scene reconstruction results. Given video captured from single camera, Human3R performs online reasoning about global human motion, the surrounding environment, and camera poses all at once. Check our website for video results. (a) Camera pose estimation. (b) Video depth estimation in metric scale. Figure 8: Evaluation of generic 3D reconstruction with camera pose estimation on TUM-D [86] and video depth estimation on Bonn [61]. 4.3 GENERIC 3D RECONSTRUCTION Camera Pose Estimation. Following prior works [12, 97], we evaluate camera pose estimation accuracy on TUM dynamics [86] dataset with dynamic humans. We report the Absolute Translation Error (ATE) after applying the Sim(3) alignment [92] on the estimated camera trajectory to the groundtruth. We compare with current leading 3D reconstruction foundation models [12, 96, 97, 104, 117]. We include VGGT, an offline method utilizing full attention, as an upper bound for online approaches, since it retains complete historical context without forgetting. VGGT and StreamVGGT rely on full attention, making them relatively slow and prone to running out of memory (OOM). In contrast, CUT3R maintains consistently low GPU usage and enables online inference, but struggles to remember long sequences, resulting in less accurate pose estimation. TTT3R [12] introduces closed-form state transition rule as training-free intervention to mitigate the catastrophic forgetting observed in CUT3R. As shown in Fig. 8a, integrating TTT3R with Human3R leads to further improvements in camera pose estimation after human prompt tuning compared to the original TTT3R. Video Depth Estimation. Following common practice [12, 97], we evaluate video depth estimation on Bonn [61] datasets with dynamic humans. We use Absolute Relative Error and δ<1.25 (percentage of predicted depths within 1.25-factor of true depth) as metrics. Metric scale video depth estimation evaluates per-frame depth quality and inter-frame depth consistency without per-sequence scale or shift alignment, which measures the absolute depth accuracy. Fig. 8b presents the quantitative comparison between our method and the online baselines, and still Human3R+TTT3R achieves more acccurate depth estimation over naive TTT3R. We do not plot VGGT [96] and StreamVGGT [117] for the evaluation of the metric depth, as they can only predict the relative depth without metric scale. By integrating TTT3R and fine-tuning with human prompt tuning on human-scene 4D datasets, our approach achieves SOTA human mesh recovery and also slightly improves generic 3D reconstruction. This highlights the mutual benefits of jointly reasoning about humans and scenes."
        },
        {
            "title": "Preprint",
            "content": "(a) Naive (b) Ours Figure 9: Comparison with naive CUT3R+Multi-HMR combination in global human motion, 3D scene reconstruction, and camera poses estimation. The colors and indicates Prediction and Ground-truth, respectively. (cid:252) See Fig. 12 in Sup.Mat. for zoomed-in visualization."
        },
        {
            "title": "4.4 ANALYSIS",
            "content": "1) Human3R benefits from the 3D awareness of CUT3R. We use the Mean Root Position Error (MRPE) [3] between the predicted and ground-truth pelvis locations to evaluate the quality of spatial location estimation. As shown in Fig. 10, Multi-HMR performance varies when processing images at different aspect ratios, while Human3R performs consistently well without requiring camera intrinsics. The metric-scale 3D scene context guides multi-human recovery by capturing their relative spatial relationships, thereby improving our intrinsic robustness. This enables Human3R to recover coherent 3D humans from intrinsic-agnostic internet images. See more in-the-wild examples on our website. 2) Human3R benefits from the human awareness of Multi-HMR. To enhance the details of reconstructed human pose and shape, we introduce Multi-HMR [3] ViT DINO encoder that fine-tuned on human-specific datasets as human prior. As shown in Tab. 3, Human3R reconstructs more fine-grained human pose and shape when injecting human priors in better detail. 3) Human3R takes the best of both worlds. Human3R predicts better camera poses and scenes than CUT3R  (Fig. 8)  , better local humans than Multi-HMR (Tab. 1 and Fig. 10), and better global humans than the naive combinations of Multi-HMR and CUT3R (Tab. 3), all-at-once."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Figure 10: Evaluation of intrinsic robustness. MultiHMR w/ GT intrinsics and Multi-HMR w/o GT intrinsics are sensitive to image aspect ratios, Human3R performs consistently well without camera intrinsics. WA-MPJPE W-MPJPE RTE 221.2 129.9 122.1 113.6 112.2 455.4 401.3 124.3 112.2 808.4 314.2 292.9 291.7 267.9 1263 1173.9 292.3 267.9 Ablations Human3R w/o Prior Human3R w/ ViT-S/672 Human3R w/ ViT-B/672 Human3R w/ ViT-L/672 Human3R w/ ViT-L/896 Naive w/o TTT3R Naive w/ TTT3R Human3R w/o TTT3R Human3R w/ TTT3R Table 3: Ablation of human prior and naive baselines in global human motion on EMDB-2 dataset, using different Multi-HMR ViT-DINO encoders and simple combination of Multi-HMR and CUT3R as the naive baseline. (cid:252) Please check more detailed analyses in Sec. A.1, Sec. A.2, and Sec. A.3 of Sup.Mat. 2.2 2.2 2.2 2.2 2.2 14.3 12.2 2.5 2. We presented Human3R, one-stage method for 4D human-scene reconstruction, providing feasible strategy for both efficient finetuning and real-time inference. Our method demonstrates competitive or state-of-the-art performance in both human motion recovery and general 3D reconstruction benchmark, and generalizes to casually captured videos. Limitations & Future Work. Human3R represents an important first step towards feed-forward 3D human and scene reconstruction, but several limitations remain. First, our method relies on the head as the discriminative keypoint for detecting humans, which leads to failures when the head is not visible. Incorporating pixel-aligned body point localizers [40, 76] could mitigate this issue. Second, we currently represent humans using proxy SMPL meshes that do not model clothing or appearance. Extending the framework with 3DGS anchored on SMPL would enable richer, more holistic reconstructions. Third, while Human3R is designed as an online method for real-time applications, it can also serve as an effective initialization for optimization-based approaches [53] to improve accuracy at the cost of additional computation. Beyond these limitations, Human3R opens avenues for broader applications. Although our focus is on reconstructing humans from monocular videos, the underlying principles can extend to other dynamic entities. By leveraging spatial and temporal cues, the framework could be adapted to reconstruct animals, vehicles, or other moving objects with full 6D poses (see limitations Sec. of Sup.Mat.). Such extensions would enable applications in wildlife monitoring, traffic analysis, human-object interaction, and robotics."
        },
        {
            "title": "6 ACKNOWLEDGE",
            "content": "Thank all members of Endless AI, Inception3D and RVH Group for help, and Yiru for creating the fantastic logo love it! Yue and Xingyu are funded by the Westlake Education Foundation. Gerard and Yuxuan are funded by the Carl Zeiss Foundation, the Deutsche Forschungsgemeinschaft - 409792180 (EmmyNoether Programme, project: Real Virtual Humans), and the German Federal Ministry of Education and Research: Tubingen AI Center, FKZ: 01IS18039A. Gerard is member of the Machine Learning Cluster of Excellence, EXC number 2064/1 Project number 390727645."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven Seitz, and Richard Szeliski. Building rome in day. ACM Communications, 2011. 20 [2] Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, and Angjoo Kanazawa. Visual imitation enables contextual humanoid control. arXiv preprint arXiv:2505.03729, 2025. 3 [3] Fabien Baradel, Matthieu Armando, Salma Galaaoui, Romain Bregier, Philippe Weinzaepfel, Gregory Rogez, and Thomas Lucas. Multi-hmr: Multi-person whole-body human mesh recovery in single shot. In Proc. of the European Conf. on Computer Vision (ECCV), 2024. 2, 3, 5, 7, 9, 19 [4] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Muller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. 2, 3 [5] Michael Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. Bedlam: synthetic dataset of bodies exhibiting detailed lifelike animated motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 87268737, 2023. 2, 3, 6, [6] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael Black. Keep it smpl: Automatic estimation of 3d human pose and shape from single image. In European conference on computer vision, pp. 561578. Springer, 2016. 3 [7] Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 66846692, 2017. 21 [8] Eric Brachmann, Jamie Wynn, Shuai Chen, Tommaso Cavallari, Aron Monszpart, Daniyar Turmukhambetov, and Victor Adrian Prisacariu. Scene coordinate reconstruction: Posing of image collections via incremental learning of relocalizer. In Proc. of the European Conf. on Computer Vision (ECCV), 2024. 21 [9] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. Must3r: Multi-view network for stereo 3d reconstruction. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025. 6, 21 [10] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 5 [11] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Easi3r: Estimating disentangled motion from dust3r without training. arXiv preprint arXiv:2503.24391, 2025. [12] Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, and Anpei Chen. Ttt3r: 3d reconstruction as test-time training. arXiv preprint arXiv:2509.26645, 2025. 6, 8 [13] Yue Chen, Xingyu Chen, Anpei Chen, Gerard Pons-Moll, and Yuliang Xiu. Feat2gs: Probing visual foundation models with gaussian splatting. arXiv, 2412.09606, 2024."
        },
        {
            "title": "Preprint",
            "content": "[14] Zhuoguang Chen, Minghui Qin, Tianyuan Yuan, Zhe Liu, and Hang Zhao. Long3r: Long sequence streaming 3d reconstruction. arXiv preprint arXiv:2507.18255, 2025. 6, 21 [15] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Alexander Schwing, and Joon-Young Lee. Tracking anything with decoupled video segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 13161326, 2023. 2 [16] Changwoon Choi, Jeongjun Kim, Geonho Cha, Minkwan Kim, Dongyoon Wee, and Young Min Kim. Humans as calibration pattern: Dynamic 3d scene reconstruction from unsynchronized and uncalibrated videos. arXiv preprint arXiv:2412.19089, 2024. 1 [17] Hongsuk Choi, Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. Beyond static features for temporally consistent 3d human pose and shape from video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19641973, 2021. 3 [18] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Monocular expressive body regression through body-driven attention. In European Conference on Computer Vision, pp. 2040. Springer, 2020. 3 [19] Enric Corona, Gerard Pons-Moll, Guillem Alenya, and Francesc Moreno-Noguer. Learned vertex descent: new direction for 3d human model fitting. In European Conference on Computer Vision, pp. 146165. Springer, 2022. [20] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. Advances in Neural Information Processing Systems (NIPS), 2013. 6 [21] Yudi Dai, YiTai Lin, XiPing Lin, Chenglu Wen, Lan Xu, Hongwei Yi, Siqi Shen, Yuexin Ma, and Cheng Wang. Sloper4d: scene-aware dataset for global 4d human pose estimation in urban environments. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 682692, 2023. 2 [22] Andrew Davison, Ian Reid, Nicholas Molton, and Olivier Stasse. Monoslam: Real-time single camera slam. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 2007. 20 [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 4 [24] Bardienus Pieter Duisterhof, Lojze Zust, Philippe Weinzaepfel, Vincent Leroy, Yohann Cabon, and Jerome Revaud. Mast3r-sfm: fully-integrated solution for unconstrained structure-frommotion. In 2025 International Conference on 3D Vision (3DV), pp. 110. IEEE, 2025. 2, [25] Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, Yao Feng, and Michael Black. Tokenhmr: Advancing human mesh recovery with tokenized pose representation. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 7 [26] Jakob Engel, Thomas Schops, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In Proc. of the European Conf. on Computer Vision (ECCV), 2014. 20 [27] Zhiwen Fan, Jian Zhang, Renjie Li, Junge Zhang, Runjin Chen, Hezhen Hu, Kevin Wang, Huaizhi Qu, Dilin Wang, Zhicheng Yan, et al. Vlm-3r: Vision-language models augmented with instruction-aligned 3d reconstruction. arXiv preprint arXiv:2505.20279, 2025. 2 [28] Yao Feng, Vasileios Choutas, Timo Bolkart, Dimitrios Tzionas, and Michael Black. Collaborative regression of expressive bodies using moderation. In 2021 International Conference on 3D Vision (3DV), pp. 792804. IEEE, 2021. 3 [29] Yao Feng, Jing Lin, Sai Kumar Dwivedi, Yu Sun, Priyanka Patel, and Michael Black. Chatpose: Chatting about 3d human pose. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 20932103, 2024."
        },
        {
            "title": "Preprint",
            "content": "[30] James Gibson. The perception of the visual world. 1950. 1 [31] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran, Angjoo Kanazawa, and Jitendra Malik. Humans in 4d: Reconstructing and tracking humans with transformers. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2023. 2, 3, 7 [32] Rıza Alp Guler, Natalia Neverova, and Iasonas Kokkinos. Densepose: Dense human pose estimation in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 72977306, 2018. 3 [33] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael Black. Resolving 3d human pose ambiguities with 3d scene constraints. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 22822292, 2019. 2, 7 [34] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. [35] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European conference on computer vision, pp. 709727. Springer, 2022. 2, 5 [36] Hanbyul Joo, Natalia Neverova, and Andrea Vedaldi. Exemplar fine-tuning for 3d human model fitting towards in-the-wild 3d human pose estimation. In 2021 International Conference on 3D Vision (3DV), pp. 4252. IEEE, 2021. 3 [37] Angjoo Kanazawa, Michael Black, David Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 71227131, 2018. 3 [38] Angjoo Kanazawa, Jason Zhang, Panna Felsen, and Jitendra Malik. Learning 3d human dynamics from video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 56145623, 2019. 3 [39] Manuel Kaufmann, Jie Song, Chen Guo, Kaiyue Shen, Tianjian Jiang, Chengcheng Tang, Juan Jose Zarate, and Otmar Hilliges. Emdb: The electromagnetic database of global 3d human pose and shape in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1463214643, 2023. 2, 7, 22, 24 [40] Ben Kaye, Tomas Jakab, Shangzhe Wu, Christian Ruprecht, and Andrea Vedaldi. Dualpm: dual posed-canonical point maps for 3d shape and pose reconstruction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 64256435, 2025. [41] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. 2 [42] Muhammed Kocabas, Nikos Athanasiou, and Michael Black. Vibe: Video inference for human body pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 52535263, 2020. 3 [43] Muhammed Kocabas, Chun-Hao Huang, Joachim Tesch, Lea Muller, Otmar Hilliges, and Michael Black. Spec: Seeing people in the wild with an estimated camera. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1103511045, 2021. 3 [44] Muhammed Kocabas, Ye Yuan, Pavlo Molchanov, Yunrong Guo, Michael Black, Otmar Hilliges, Jan Kautz, and Umar Iqbal. Pace: Human and camera motion estimation from in-the-wild videos. In 2024 International Conference on 3D Vision (3DV), pp. 397408. IEEE, 2024. 1 [45] Nikos Kolotouros, Georgios Pavlakos, Michael Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 22522261, 2019."
        },
        {
            "title": "Preprint",
            "content": "[46] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with MASt3R. In Proc. of the European Conf. on Computer Vision (ECCV), 2024. 2 [47] Jiefeng Li, Chao Xu, Zhicun Chen, Siyuan Bian, Lixin Yang, and Cewu Lu. Hybrik: hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 33833393, 2021. 3 [48] Jiefeng Li, Ye Yuan, Davis Rempe, Haotian Zhang, Pavlo Molchanov, Cewu Lu, Jan Kautz, and Umar Iqbal. Coin: Control-inpainting diffusion prior for human and camera motion estimation. In Proc. of the European Conf. on Computer Vision (ECCV), 2024. 1, 3, 7 [49] Jiefeng Li, Siyuan Bian, Chao Xu, Zhicun Chen, Lixin Yang, and Cewu Lu. Hybrik-x: Hybrid analytical-neural inverse kinematics for whole-body mesh recovery. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 3 [50] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In European conference on computer vision, pp. 280296. Springer, 2022. [51] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa, Aleksander Holynski, and Noah Snavely. MegaSaM: accurate, fast, and robust structure and motion from casual dynamic videos. 2025. 3, 20 [52] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu, and Youliang Yan. Cliff: Carrying location information in full frames into human pose and shape estimation. In Proc. of the European Conf. on Computer Vision (ECCV), 2022. 3, 7 [53] Zhizheng Liu, Joe Lin, Wayne Wu, and Bolei Zhou. Joint optimization for 4d human-scene reconstruction in the wild. arXiv preprint arXiv:2501.02158, 2025. 1, 2, 3, 7, 9, 21 [54] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. In Seminal Graphics Papers: Pushing the Smpl: skinned multi-person linear model. Boundaries, Volume 2, pp. 851866. 2023. 3, [55] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 21 [56] Lea Muller, Hongsuk Choi, Anthony Zhang, Brent Yi, Jitendra Malik, and Angjoo Kanazawa. Reconstructing people, places, and cameras. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2194821958, 2025. 1, 2, 3 [57] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan Tardos. Orb-slam: versatile and accurate monocular slam system. IEEE transactions on robotics, 2015. 20 [58] Richard Newcombe, Steven Lovegrove, and Andrew Davison. Dtam: Dense tracking and mapping in real-time. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2011. 20 [59] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, and Bernt Schiele. Neural body fitting: Unifying deep learning and model based human pose and shape estimation. In 2018 international conference on 3D vision (3DV), pp. 484494. IEEE, 2018. [60] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3, 5 [61] Emanuele Palazzolo, Jens Behley, Philipp Lottes, Philippe Giguere, and Cyrill Stachniss. Refusion: 3d reconstruction in dynamic environments for rgb-d cameras exploiting residuals. In IROS, 2019. 8 [62] Priyanka Patel and Michael Black. Camerahmr: Aligning people with perspective. In Proc. of the International Conf. on 3D Vision (3DV), 2025. 3,"
        },
        {
            "title": "Preprint",
            "content": "[63] Priyanka Patel, Chun-Hao Huang, Joachim Tesch, David Hoffmann, Shashank Tripathi, and Michael Black. Agora: Avatars in geography optimized for regression analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1346813478, 2021. 3 [64] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1097510985, 2019. 2, 3, 4, 6, 21 [65] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1097510985, 2019. 3 [66] Georgios Pavlakos, Ethan Weber, Matthew Tancik, and Angjoo Kanazawa. The one where they reconstructed 3d humans and environments in tv shows. In European Conference on Computer Vision, pp. 732749. Springer, 2022. 3 [67] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. Foundations and Trends in Machine Learning, 11(5-6):355607, 2019. [68] Frank Plastria. The weiszfeld algorithm: proof, amendments, and extensions. Foundations of location analysis, pp. 357389, 2011. 4 [69] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo Kanazawa, and Jitendra Malik. Tracking people by predicting 3d appearance, location and pose. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 6 [70] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2021. 4 [71] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [72] Davis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas Guibas. Humor: 3d human motion model for robust pose estimation. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1148811499, 2021. 1, 3 [73] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 2 [74] Sara Rojas, Matthieu Armando, Bernard Ghamen, Philippe Weinzaepfel, Vincent Leroy, and Gregory Rogez. Hamst3r: Human-aware multi-view stereo 3d reconstruction. arXiv preprint arXiv:2508.16433, 2025. 1, 3 [75] Ricardo Buitrago Ruiz and Albert Gu. Understanding and improving length generalization in recurrent models. arXiv preprint arXiv:2507.02782, 2025. 6 [76] Istvan Sarandi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. Advances in Neural Information Processing Systems (NeurIPS), 2024. 3, 7, [77] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2020. 6 [78] Jurgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 1992. 6 [79] Johannes Lutz Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR, 2016. 3,"
        },
        {
            "title": "Preprint",
            "content": "[80] Zehong Shen, Huaijin Pi, Yan Xia, Zhi Cen, Sida Peng, Zechen Hu, Hujun Bao, Ruizhen Hu, and Xiaowei Zhou. World-grounded human motion recovery via gravity-view coordinates. In SIGGRAPH Asia, 2024. 1, 3, 7 [81] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2016. 6 [82] Soyong Shin, Juyong Kim, Eni Halilaj, and Michael Black. Wham: Reconstructing worldgrounded humans with accurate 3d motion. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2024. 1, 3, 7 [83] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGB-D images. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2013. 21 [84] Noah Snavely, Steven Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In SIGGRAPH. 2006. [85] Noah Snavely, Steven Seitz, and Richard Szeliski. Modeling the world from internet photo collections. International journal of computer vision, 2008. 20 [86] Jurgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. benchmark for the evaluation of rgb-d slam systems. In 2012 IEEE/RSJ international conference on intelligent robots and systems, 2012. 8 [87] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael Black, and Tao Mei. Monocular, one-stage, regression of multiple 3d people. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1117911188, 2021. 3, 19 [88] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael Black. Putting people in their place: Monocular regression of 3d people in depth. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 3, 7, 19 [89] Yu Sun, Qian Bao, Wu Liu, Tao Mei, and Michael Black. Trace: 5d temporal regression of avatars with dynamic cameras in 3d environments. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 3, [90] Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. 6 [91] Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in neural information processing systems, 34:1655816569, 2021. 1, 2, 3 [92] Shinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Trans. on Pattern Analysis and Machine Intelligence (PAMI), 1991. 8 [93] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems (NeurIPS), 2017. 21 [94] Timo Von Marcard, Roberto Henschel, Michael Black, Bodo Rosenhahn, and Gerard PonsMoll. Recovering accurate 3d human pose in the wild using imus and moving camera. In Proceedings of the European conference on computer vision (ECCV), pp. 601617, 2018. 2, 3, 7, 23 [95] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv, 2408.16061, 2024. 6, 21 [96] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and In Proc. IEEE Conf. on David Novotny. Vggt: Visual geometry grounded transformer. Computer Vision and Pattern Recognition (CVPR), 2025. 8,"
        },
        {
            "title": "Preprint",
            "content": "[97] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A. Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. 2025. 2, 4, 5, 6, 8, 19, 21 [98] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. DUSt3R: geometric 3d vision made easy. In Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 4, 21 [99] Yufu Wang, Ziyun Wang, Lingjie Liu, and Kostas Daniilidis. Tram: Global trajectory and motion of 3d humans from in-the-wild videos. In Proc. of the European Conf. on Computer Vision (ECCV), 2024. 1, 2, 3, 7 [100] Yufu Wang, Yu Sun, Priyanka Patel, Kostas Daniilidis, Michael Black, and Muhammed Kocabas. Prompthmr: Promptable human mesh recovery. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025. 3, 7 [101] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Bregier, Yohann Cabon, Vaibhav Arora, Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jerˆome Revaud. Croco: Self-supervised pre-training for 3d vision tasks by cross-view completion. Advances in Neural Information Processing Systems (NeurIPS), 2022. [102] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Bregier, Gabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and Jerˆome Revaud. Croco v2: Improved cross-view completion pre-training for stereo matching and optical flow. In Proc. of the IEEE International Conf. on Computer Vision (ICCV), 2023. 4 [103] Alexander Weiss, David Hirshberg, and Michael Black. Home 3d body scans from noisy image and range data. In 2011 International Conference on Computer Vision, pp. 19511958. IEEE, 2011. 3 [104] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 6, 8, 21 [105] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Ghum & ghuml: Generative 3d human shape and articulated pose models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 61846193, 2020. 3 [106] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35: 3857138584, 2022. [107] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025. 21 [108] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [109] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo Kanazawa. Decoupling human and camera motion from videos in the wild. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2023. 1, 3, 7 [110] Ye Yuan, Umar Iqbal, Pavlo Molchanov, Kris Kitani, and Jan Kautz. Glamr: Global occlusionaware human mesh recovery with dynamic cameras. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2022. 1, 3, 7 [111] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang, Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1144611456, 2021."
        },
        {
            "title": "Preprint",
            "content": "[112] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng Li, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(10):1228712303, 2023. 3 [113] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing Sun, and Ming-Hsuan Yang. MonST3R: simple approach for estimating geometry in the presence of motion. 2025. 21 [114] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Noah Snavely, Michael Rubinstein, and William T. Freeman. Structure and motion from casual videos. In Proc. of the European Conf. on Computer Vision (ECCV), 2022. 20 [115] Yizhou Zhao, Tuanfeng Yang Wang, Bhiksha Raj, Min Xu, Jimei Yang, and Chun-Hao Paul Huang. Synergistic global-space camera and human reconstruction from videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12161226, 2024. 3 [116] Xingyi Zhou, Dequan Wang, and Philipp Krahenbuhl. Objects as points. arXiv preprint arXiv:1904.07850, 2019. [117] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry transformer. arXiv preprint arXiv:2507.11539, 2025. 8, 21 17 19 19 19 20 20 20"
        },
        {
            "title": "Table of Contents",
            "content": "A Analysis"
        },
        {
            "title": "Appendix",
            "content": "A.1 Human3R benefits from the 3D awareness of CUT3R . . A.2 Human3R benefits from the human awareness of Multi-HMR . . A.3 Human3R takes the best of both worlds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Related Works B.1 Generic 3D Reconstruction. . . . . . . . . . . . . . . . . . . . . . . . . . . . Training Details Failure Cases & Future Work Use of Large Language Models"
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Evaluation of intrinsic robustness in human mesh reconstruction. Multi-HMR [3] performance varies when processing images at different ratios, while Human3R performs consistently well without requiring camera intrinsics, benefiting from the 3D awareness of CUT3R. Human Mesh Reconstruction Global Human Motion 3DPW (14) EMDB-1 (24) EMDB-2 (24) RICH (24) Ablations Human3R w/o Prior Human3R w/ ViT-S/672 Human3R w/ ViT-B/672 Human3R w/ ViT-L/672 Human3R w/ ViT-L/896 PA-MPJPE MPJPE PVE PA-MPJPE MPJPE PVE WA-MPJPE W-MPJPE RTE WA-MPJPE W-MPJPE RTE FPS 221.2 129.9 122.1 113.6 112.2 252.7 106.5 96.4 95.0 86.0 200.4 103.1 94.3 96.7 84.9 399.5 208.3 188.3 185.0 184. 145.8 66.9 56.6 54.1 48.5 808.4 314.2 292.9 291.7 267.9 226 131.8 119.2 110.3 110.0 173.5 87.8 79.6 83.1 71.2 102.1 56.1 49.3 48.5 44.1 214 93.6 84.1 82.9 73. 3.4 3.3 3.3 3.3 3.3 2.2 2.2 2.2 2.2 2.2 18 15 11 7 5 Table 4: Ablation of human prior in human mesh reconstruction and global human motion estimation. To enhance the details of reconstructed human pose and shape, we introduce Multi-HMR [3] ViT DINO encoder that fine-tuned on human-specific datasets as human prior."
        },
        {
            "title": "A ANALYSIS",
            "content": "A.1 HUMAN3R BENEFITS FROM THE 3D AWARENESS OF CUT3R As shown in Fig. 11, Multi-HMRs performance varies with image aspect ratios, whereas Human3R remains consistently strong without requiring camera intrinsics. Specifically, Multi-HMR w/ GT intrinsics is substantially more robust than Multi-HMR w/o GT intrinsics. This demonstrates that integrating camera intrinsics helps recover and place human 3D meshes more accurately in the scene [3]. However, training with ground-truth intrinsics and testing without themfollowing the fixed 60 field-of-view (FOV) assumption used in previous work [3, 87, 88]induces an outof-distribution (OOD) shift and markedly degrades performance. Conditioning on ground-truth intrinsics improves person-centered reconstruction metrics: mean per-joint position error (MPJPE; root-centered), Procrustes-aligned MPJPE (PA-MPJPE; root-centered and rotation-aligned), and per-vertex error (PVE; root-centered). In contrast, the Mean Root Position Error (MRPE) [3], which evaluates absolute pelvis location in metric scale (millimeters), is notably more sensitive to changes in image aspect ratio. These findings underscore the benefit of using CUT3R [97] as 4D foundation model: leveraging metric-scale scene context enhances the intrinsic robustness of Human3R and enables coherent recovery of 3D humans from intrinsic-agnostic in-the-wild images. A.2 HUMAN3R BENEFITS FROM THE HUMAN AWARENESS OF MULTI-HMR To enhance fine-grained details in reconstructed human pose and shape, we incorporate the MultiHMR [3] ViT DINO encoderfine-tuned on human-centric datasetsas human prior. As shown in Tab. 4, injecting this prior enables Human3R to recover more detailed human pose and shape. We ablate Human3R without the prior (Human3R w/o Prior) and evaluate the impact of input image resolution (672 and 896) across Multi-HMR ViT DINO backbone sizes (ViT-S, ViT-B, ViT-L). Increasing the input resolution and model size consistently improves performance, at the cost of higher inference time, as reported on the right of Tab. 4 in frames per second (FPS). For global human motion estimation, ViT-S backbone with 672 672 inputs offers good performancespeed trade-off (approximately 100 WA-MPJPE and 2 RTE at 15 FPS). Higher resolutions and larger backbones can be more beneficial for detailed human-mesh reconstruction. This is expected, since fine detailssuch as facial expressions and hand posesare better captured at higher resolution and by larger models with richer priors and higher-dimensional features. The largest backbone (ViT-L) at 896 896 runs at 5 FPSwithout extra compression or quantization effortswhile achieving accuracy even competitive with multi-stage methods."
        },
        {
            "title": "Preprint",
            "content": "(a) Naive (b) Ours Figure 12: Comparison with naive CUT3R+Multi-HMR combination in global human motion, 3D scene reconstruction, and camera poses estimation. The colors and indicates Prediction and Ground-truth, respectively. A.3 HUMAN3R TAKES THE BEST OF BOTH WORLDS Human3R predicts camera poses and scenes more accurately than CUT3R  (Fig. 8)  , reconstructs local human details better than Multi-HMR (Tab. 1, Fig. 10), and outperforms naive combinations of MultiHMR and CUT3R on global human reconstruction (Tab. 3)all at once. We visualize reconstruction results in Fig. 12 and Fig. 14. Beyond offering unified model that jointly reasons about humans, the scene, and the camera in an online manner, Human3R runs on streaming video in real time (15 FPS), eliminating the need for separate off-the-shelf components and iterative refinement. Crucially, this efficiency does not come at the expense of accuracy. With human prompt tuning, our model reconstructs multiple people in single forward pass, while implicitly reasoning about human-scene interaction (more examples on Fig. 15 and Fig. 16). Trained with only one 48G GPU for one day, it delivers substantially improved reconstructions than naive combinations of Multi-HMR and CUT3R and achieves state-of-the-art performance over task-specific baselines."
        },
        {
            "title": "B RELATED WORKS",
            "content": "B.1 GENERIC 3D RECONSTRUCTION. 3D reconstruction from RGB images has long been fundamental challenge in computer vision. Structure-from-Motion (SfM) [1, 79, 84, 85] and SLAM [22, 26, 51, 57, 58, 114] are foundational"
        },
        {
            "title": "Preprint",
            "content": "approaches for simultaneously recovering 3D structure and camera poses. However, these methods often struggle in scenarios with small camera parallax, textureless surface, or dynamic elements, like the moving humans, and typically produce only sparse point clouds, which constrains detailed scene understanding. Moreover, their optimization-based pipelines are computationally intensive and slow, making them less suitable for real-time applications. major breakthrough in feedforward 3D reconstruction was achieved by DUSt3R [98], which introduced an end-to-end approach that directly predicts two pixel-aligned pointmaps [7, 8, 83] from an image pair. Subsequent methods [96, 107] extended this framework to handle multiview inputs using large global attention [93], achieving stateof-the-art results in 3D point and camera pose reconstruction. However, these approaches suffer from quadratic growth in computational and memory costs, making them inherently offline: inference must be re-run over all images whenever new frame is added. To enable online reconstruction, several works [9, 14, 95, 104, 117] introduce memory mechanisms that compress and retain information from past frames, allowing for incremental 3D reasoning. However, since these methods are not trained on dynamic datasets [113] or do not explicitly disentangle static scenes from dynamic humans [11], their performance degrades when processing videos with moving people. promising advance is the recurrent deep 4D reconstruction foundation model, CUT3R [97], which is trained on both static and dynamic datasets. CUT3R achieves feed-forward 4D reconstruction by maintaining persistent internal state that encodes the spatiotemporal history of both scenes and humans, incrementally updating this state as new observations arrive. This recurrent formulation enables efficient processing of long sequences with linear computational complexity, while keeping inference memory usage consistently low. Building on this success, we leverage the spatiotemporal priors learned by CUT3R to enable online holistic 4D reconstruction, reasoning jointly not only the 3D scene and camera poses, but also the multi-person human body mesh sequences (parameterized with SMPL-X [64]), in the world frame, at real-time inference speed."
        },
        {
            "title": "C TRAINING DETAILS",
            "content": "We freeze all weights of pretrained CUT3R and Multi-HMR encoder, and fine-tune the humanrelated modules (i.e., Headprojection, Headhuman, MLPhead and MLPmask) on BEDLAM [5]. This dataset provides 3D scene depth and SMPL-X meshes, with 110 people per scene, captured from diverse known camera viewpoints. Following CUT3R, we exclude BEDLAM sequences where the environment is represented by panoramic HDRI image, resulting in 5,000 sequences for training and 1,000 for validation, with each sequence averaging 30 frames. For each iteration, we randomly sample 4 frames from each sequence and train Human3R with batch size of 8, using variable aspect ratios and resizing images so that the longer side is 512 pixels. All MLP networks are implemented as 2 linear layers with GELU activation [34]. Each human prompt, 768-dimensional vector, is concatenated with the camera and image tokens along the token dimension. We use the AdamW optimizer [55] with an initial learning rate of 1 104, employing linear warmup followed by cosine decay. We train our model on single NVIDIA 48GB GPU within one day. FAILURE CASES & FUTURE WORK Human3R implicitly models human interactions (Fig. 13, left) but does not yet resolve them (Fig. 13, middle), and it has not matched strong offline methods (e.g., JOSH [53]) in reconstruction accuracy. Iterative optimizationthough slower and more memory-intensivebetter constrains interpenetration, physics, and contacts. Human3R can therefore serve as an effective initialization for applications that demand high accuracy. While Human3R shows clear boost in real-time human-scene reconstruction, its (b) Interaction (c) Dynamic object design space remains largely unexplored. Fig. 13 (right) Figure 13: Failure cases. (a) Successful human-human highlights vast opportunity to develop more expressive interaction; (b) Interaction failures with human-human interpenetration; (c) Inability to model dynamic objects. architectures for handling human-object interactions and moving toward everything. We hope that this work will motivate future research to revisit the task of dynamic human, animal, and object from real-time, online, end-to-end perspective. (a) Example"
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Qualitative comparison in global human motion, 3D scene reconstruction, and camera poses estimation of our prediction against ground-truth on EMDB dataset (subset 1) [39]."
        },
        {
            "title": "E USE OF LARGE LANGUAGE MODELS",
            "content": "We used large language model to assist with copy editinggrammar checking, wording suggestions, and minor style and clarity improvementsafter the scientific content, methodology, analyses, and conclusions had been written by the authors."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Qualitative 4D human-scene reconstruction results on the 3DPW dataset [94]. Given video captured from moving camera, Human3R performs online reasoning about global human motion, the surrounding environment, and camera poses all at once."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: Qualitative 4D human-scene reconstruction results on the EMDB dataset [39]. Given video captured from single camera, Human3R performs online reasoning about global human motion, the surrounding environment, and camera poses all at once."
        }
    ],
    "affiliations": [
        "Max Planck Institute for Informatics",
        "Uni of Tubingen, Tubingen AI Center",
        "Westlake University"
    ]
}