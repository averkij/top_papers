{
    "paper_title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals",
    "authors": [
        "Davide Lobba",
        "Fulvio Sanguigni",
        "Bin Ren",
        "Marcella Cornia",
        "Rita Cucchiara",
        "Nicu Sebe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While virtual try-on (VTON) systems aim to render a garment onto a target person image, this paper tackles the novel task of virtual try-off (VTOFF), which addresses the inverse problem: generating standardized product images of garments from real-world photos of clothed individuals. Unlike VTON, which must resolve diverse pose and style variations, VTOFF benefits from a consistent and well-defined output format -- typically a flat, lay-down-style representation of the garment -- making it a promising tool for data generation and dataset enhancement. However, existing VTOFF approaches face two major limitations: (i) difficulty in disentangling garment features from occlusions and complex poses, often leading to visual artifacts, and (ii) restricted applicability to single-category garments (e.g., upper-body clothes only), limiting generalization. To address these challenges, we present Text-Enhanced MUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a dual DiT-based backbone with a modified multimodal attention mechanism for robust garment feature extraction. Our architecture is designed to receive garment information from multiple modalities like images, text, and masks to work in a multi-category setting. Finally, we propose an additional alignment module to further refine the generated visual details. Experiments on VITON-HD and Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the VTOFF task, significantly improving both visual quality and fidelity to the target garments."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 2 6 0 1 2 . 5 0 5 2 : r Inverse Virtual Try-On: Generating Multi-Category Product-Style Images from Clothed Individuals Davide Lobba1,2 Fulvio Sanguigni2,3 Bin Ren1,2 Marcella Cornia3 Rita Cucchiara3 Nicu Sebe1 2University of Pisa, Italy 1University of Trento, Italy 3University of Modena and Reggio Emilia, Italy Figure 1: Visual results produced by our proposed text-enhanced multi-category virtual try-off architecture, i.e., TEMU-VTOFF. Given clothed input person image, the proposed model reconstructs the clean, in-shop version of the worn garment. Our model handles various garment types and preserves both structural fidelity and fine-grained textures, even under occlusions and complex poses, thanks to its multimodal attention and garment-alignment design."
        },
        {
            "title": "Abstract",
            "content": "While virtual try-on (VTON) systems aim to render garment onto target person image, this paper tackles the novel task of virtual try-off (VTOFF), which addresses the inverse problem: generating standardized product images of garments from realworld photos of clothed individuals. Unlike VTON, which must resolve diverse pose and style variations, VTOFF benefits from consistent and well-defined output format typically flat, lay-down-style representation of the garment making it promising tool for data generation and dataset enhancement. However, existing VTOFF approaches face two major limitations: (i) difficulty in disentangling garment features from occlusions and complex poses, often leading to visual artifacts, and (ii) restricted applicability to single-category garments (e.g., upper-body clothes only), limiting generalization. To address these challenges, we present TextEnhanced MUlti-category Virtual Try-Off (TEMU-VTOFF), novel architecture featuring dual DiT-based backbone with modified multimodal attention mechanism for robust garment feature extraction. Our architecture is designed to receive garment information from multiple modalities like images, text, and masks to work in multi-category setting. Finally, we propose an additional alignment module to further refine the generated visual details. Experiments on VITON-HD and Dress Code datasets show that TEMU-VTOFF sets new state-of-the-art on the VTOFF task, significantly improving both visual quality and fidelity to the target garments. Our project page is available at https://temu-vtoff-page.github.io/. Equal contribution. Correspondence to: davide.lobba@unitn.it Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Unlike virtual try-on (VTON), whose goal is to dress given clothing image on target person image, in this paper, we focus exactly on the opposite, virtual try-off (VTOFF), whose purpose is to generate standardized product images from real-world clothed individual photos. Compared to VTON, which often struggles with the ambiguity and diversity of valid outputs, such as stylistic variations in how garment is worn, VTOFF benefits from clearer output objective: reconstructing consistent, lay-down-style image of the garment. This reversed formulation facilitates more objective evaluation of garment reconstruction quality. The fashion industry, trillion-dollar global market, is increasingly integrating AI and computer vision to optimize product workflows and enhance user experience. VTOFF, in this context, offers substantial value: it enables the automatic generation of tiled product views, which are essential for tasks such as image retrieval, outfit recommendation, and virtual shopping. However, acquiring such lay-down images is expensive and time-consuming for retailers. VTOFF provides scalable alternative by leveraging images of garments worn by models or customers, transforming them into standardized catalog views through image-to-image translation techniques. Despite the recent success of GANs [20] and Latent Diffusion Models (LDMs) [44] in image translation tasks [45, 43, 27, 47], current VTOFF solutions face notable limitations. Existing models [48, 51] struggle to accurately reconstruct catalog images from dressed human inputs. This limitation arises from fundamental architectural mismatch these approaches repurpose VTON pipelines by merely reversing the input-output roles, without addressing the unique challenges of the VTOFF task. Moreover, the high visual variability of real-world images due to garment wear category (e.g., upper-body), pose changes, and occlusions makes it difficult for these models to robustly extract garment features while preserving fine-grained patterns. On the opposite side, we design dedicated architecture tailored for the VTOFF task, accounting for the unique challenges of complex positions and occlusions in person images as opposed to flat catalog garment. Recent advances in diffusion models demonstrate that DiT-based architectures [38], especially when combined with flow-matching objectives [33], surpass traditional U-Net and DDPM-based approaches [44]. Inspired by these findings, we propose TEMU-VTOFF, Text-Enhanced MUlticategory Virtual Try-Off architecture based on dual-DiT framework. Specifically, we exploit the representational strength of DiT in two distinct ways: (i) the first Transformer component focuses on extracting fine-grained garment features from complex, detail-rich person images; and (ii) the second DiT is specialized for generating the clean, in-shop version of the garment. To support this design, we further adapt the base DiT architecture to accommodate the task-specific input modalities. To further enhance alignment, we introduce an external garment aligner module and novel supervision loss that leverages clean garment references as guidance, further improving quality of generated images. Our contribution can be summarized as follows: Multi-Category Try-Off. We present unified framework capable of handling multiple garment types (upper-body, lower-body, and full-body clothes) without requiring category-specific pipelines. Multimodal Hybrid Attention. We introduce novel attention mechanism that integrates garment textual descriptions into the generative process by linking them with person-specific features. This helps the model synthesize occluded or ambiguous garment regions more accurately. Garment Aligner Module. We design lightweight aligner that conditions generation on clean garment images, replacing conventional denoising objectives. This leads to better alignment consistency on the overall dataset and preserves more precise visual retention. Extensive experiments on the Dress Code and VITON-HD datasets demonstrate that TEMUVTOFF outperforms prior methods in both the quality of generated images and alignment with the target garment, highlighting its strong generalization capabilities."
        },
        {
            "title": "2 Related Work",
            "content": "Virtual Try-On. As one of the most popular tasks within the fashion domain, VTON has been widely studied over the past decades by the computer vision and graphics communities due to its interesting research challenges and the practical potential [2, 11, 15, 42, 18]. Existing methods are broadly categorized into warping-based [6, 52, 54] and warping-free approaches [59, 35, 3, 56, 10], 2 with growing shift from GAN-based [21] to diffusion-based [24, 46] frameworks for better image fidelity and stability. Warping-based methods usually follow two-stage pipeline: first warping the garment using TPS [5], flow, or landmarks to align it with the body, then fusing it with the person image. VITON [23], CP-VTON [49], and their variants improve garment alignment and synthesis quality, but often produce artifacts due to imperfect warping. To mitigate this, warping-free methods leverage diffusion models to bypass explicit deformation [59, 35, 53, 9] employing modified cross-attention or self-attention to directly condition generation on garment features, often using CLIP-based encoders [39]. However, these pre-trained encoders tend to lose fine-grained texture details, prompting methods like StableVITON [29] to introduce dedicated garment encoders and attention mechanisms, albeit at higher computational cost. Lately, DiT-based works [28] show the benefits of Transformer-based diffusion models for high-fidelity garment to person transfer. While most existing works focus on generating dressed images from separate garment and person inputs, the inverse problem (i.e., reconstructing clean, standalone garment representations from worn images) remains underexplored. Virtual Try-Off. While VTON has been extensively studied for synthesizing images of person wearing target garment, the recently proposed VTOFF task shifts the focus toward garment-centric reconstruction, aiming to extract clean, standardized image of garment worn by person. TryOffDiff [48] introduces this task by leveraging diffusion-based model with SigLIP [57] conditioning to recover high-fidelity garment images. Building on this direction, TryOffAnyone [51] addresses the generation of tiled garment images from dressed photos for applications like outfit composition and retrieval. By integrating garment-specific masks and simplifying the Stable Diffusion pipeline through selective Transformer tuning, it achieves both quality and efficiency. In both cases, these works have been designed for single-category scenarios, thus limiting their potential application to generate wider, more diverse data collections. On different line, Any2AnyTryon [22] is not native VTOFF method, but it leverages LoRA-based module [25] to fine-tune FLUX [31] for this task. Though these works collectively reflect growing shift from person-centric synthesis to garment-centric understanding, there are still limitations like frequent garment structural artifacts (e.g., in shape, neckline, waist) and on colors and textures of generated outputs. We hypothesize that this mismatch is due to too generic architectural choice, untailored for the specific needs of the VTOFF setting. In this work, we focus on existing VTOFF open problems, such as multi-category adaptation, occlusions, and complex human poses, and propose novel VTOFF-specific architecture enhanced with text and fine-grained mask conditioning and optimized with garment aligner component that can improve the quality of generated garments."
        },
        {
            "title": "3 Methodology",
            "content": "Preliminary. The latest diffusion models are family of generative architectures that work by corrupting ground-truth image z0 following flow-matching schedule [33] defined as zt = (1 t)z0 + tϵt ϵ (0, 1), [0, 1]. (1) 8 Then, diffusion model estimates back the injected noise ϵt through Diffusion Transformer (DiT) [38], obtaining prediction ˆz0. In Stable Diffusion 3 (SD3) [14], the 16-channel latent zt 8 16 is obtained projecting the original RGB image RHW 3 with variational autoencoder [30], obtaining = E(x), with H, being height and width of the image, and = 8 the spatial compression ratio of the autoencoder. Finally, the model is trained according to an MSE loss function Ldiff: Ldiff = Ez0,ϵt,t (cid:104) ϵt ϵθ(zt, t)2(cid:105) . (2) Overview. An overview of our method is shown in Fig. 2. The purpose of this framework is to generate an in-shop version of the garment worn by the person. The critical design choice is to process correctly the dressed person image in order to extract meaningful information to be injected in the denoising process. For this reason, we encompass dual-DiT architecture, based on SD3, whose models are deputed to two different purposes. Firstly, we design the first DiT as feature extractor FE that encodes the model image xmodel and outputs its intermediate layer features at timestep = 0 and not from subsequent timesteps, as we are interested in extracting clean features from FE. This block is trained with diffusion loss to generate the person image. Once trained, this model outputs meaningful key and value features of the dressed person. Secondly, the main DiT generates the 3 extractor,V Figure 2: Overview of our method. The feature extractor FE processes spatial inputs (noise, masked image, binary mask), and global inputs (model image via AdaLN). The intermediate keys and values Kl extractor are injected into the corresponding hybrid blocks of the garment generator FD. Then, the main DiT model generates the final garment leveraging the proposed MHA module. We align our model with diffusion loss for the noise estimate and an alignment loss with clean, DINOv2 features of the target garment. garment xg leveraging the intermediate features from FE in modified textual-enhanced attention module. The two DiT architectures can be manipulated along three axes: (i) the Transformer projector before the DiT to account for additional inputs; (ii) the Modulation Space (of every DiT block) to account for different input modalities (such as text and images); (iii) the Attention operator (in all layers) for fine-grained conditioning. 3.1 DiT Feature Extractor We design the feature extractor FE as DiT, working with two different input types: global input with the person image xmodel RHW 3 leveraged by the modulation layers of FE, and local = [zt, M, xM ] Rhw33 of the latent zt, the spatial input as the channel-wise concatenation encoded latent of the masked person image xM = E(IM ) Rhw16 and the interpolated binary mask Rhw1 encoded through the Transformer projector : Rhw33 RSd, with as sequence length and as embedding dimension. We train it as single DiT separated from the garment generator FD. We extract the keys and values Kl extractor from every attention block Al of FE, with = 1, . . . , , as shown in Fig. 2, because they bring valuable visual information about the garment features in the person image. extractor, Mask Conditioning with modified Transformer Projector. We investigate whether fine-grained clothing attributes such as garment shape, edge refinement, and neckline structure can be inferred from text alone. These attributes are inherently structural and spatial, which poses challenge for language-based conditioning due to its inherently diffuse nature. We posit that segmentation mask can act as hard discriminator, providing precise spatial constraints that complement the soft and distributed cues present in text. To this end, we propose to use the mask and the associated masked person image xM as an = [zt, M, xM ] Rhw33. additional conditioning of the extractor FE and we get new latent We modify the original SD3 DiT projector Porig : Rhw16 RSd by adding zero-initialized convolutional layers on the channel dimension, thus accounting for the increased channel-size of our input and obtaining the final projector P. We train the module FE alone, detached from the dual DiT FD, according to the diffusion loss Ldiff defined as follows: Lextractor = Ez0,ϵt,t (cid:104) ϵt FE(z t, xmodel, t)2(cid:105) . (3) 4 where we align the generated features with the person image, thus encouraging FE attention keys and values to retain valuable information of the garment worn by the model. Visual-only Modulation Space and Attention. The modulation space of the feature extractor FE follows AdaLN [26] and receives features encoded with CLIP ViT-L [39] and Open-CLIP bigG/14 [7]. For simplicity, we will refer to this operation as CLIP(c), with being textual or visual input. The feature extractor FE does not necessitate text as input, because we want to capture the garment details from xmodel, without steering the output process with text. For this reason, we encode the visual pool = CLIP(xmodel) R2048, which is subsequently used to modulate the latent zt from projection ev AdaLN estimated scale γ and shift β as follows: yt = MLP(t, ev zt γ(yt)zt + β(yt) pool) (4) using an MLP to jointly encode the pooled vector ev pool together with the timestep (0, 1). From Eq. 4 we notice how AdaLN modulation can shift the distribution zt according to appearance or style of our conditioning pooled input. This justifies our architectural choice, as the latent zt in FE inherits the garment information from ev pooled through AdaLN and then propagates it through its layers. Following the reasoning above, we stick to visual-only self-attention modules for FE DiT blocks with queries, keys and values as = Qzt, = Kzt, = Vzt. Although this design strategy proves effective for FE, we show in Sec. 3.2 that our main DiT FD needs to incorporate three diverse types of input, leading to new Multimodal Hybrid Attention (MHA) module. 3.2 Text-Enhanced Garment Try-Off Previous VTON and VTOFF works leverage visual-only inputs, as they claim to have better transfer of information from the conditioning input to the DiT generation stream. While this approach turns out to be effective for VTON tasks, it has two key shortcomings in the VTOFF setting. First, it prevents our method from generating occluded garments (e.g., bodysuit worn under pair of pants). Second, it limits the application of VTOFF to single-category data collections (e.g., upper-body only). Instead, we propose new, text-enhanced DiT FD to solve these two issues. Multimodal Hybrid Attention. For the first problem, we assume that it can be addressed by captions focused on structural details and hidden parts. To validate our claim, we propose new MHA module to seamlessly mix text information, intermediate features from FE and latent features of the denoising DiT. Inspired by the key findings in SD3 [14], we concatenate the text features with the visual inputs along the sequence length dimension, thus obtaining: = [Qzt, Qtext] = [Kzt, Kextractor, Ktext] = [Vzt , Vextractor, Vtext]. (5) This module allows the features Qtext to attend both the latent projection Kzt and the extractor features Kextractor. The resulting attention matrix AMHA captures three key interactions: (i) Atextzt, preserving pre-trained alignment between language and latent image tokens, (ii) Aztextractor, facilitating transfer between the input garment and the person representation, and (iii) Atextextractor, grounding the text in the structural features provided by the extractor. Text embeddings are constructed via the concatenation of CLIP [39] and T5 [40] encoders applied to the input caption as follows: etext = [CLIP(c), T5(c)], with etext R774096. (6) Visual Category Conditioning with Text Modulation. While some datasets are restricted to upperbody garments [8], others, such as Dress Code [12], encompass broader range, including lower-body items and full-body dresses. This variability introduces ambiguity in garment structure and scale, motivating the need for an explicit encoding of category-level priors. To address this, we employ high-level conditioning via AdaLN modulation [26]. As shown in previous works [17], these layers can be successfully leveraged to adapt appearance or style information into existing Transformer-based architectures. For this reason, we extract pooled textual representation epooled R2048 of CLIP textual features of the caption and inject them into the model As previously mentioned, we consider the combined embedding from CLIP ViT-L and Open-CLIP bigG/14. through the modulation layers, following Eq. 4. The pooled vector epooled R2048 encapsulates coarser representation than the full textual embeddings etext R774096, thus being suitable for high-level information conditioning. We generate textual descriptions using Qwen2.5-VL [1] in zero-shot setting. We select this model due to its state-of-the-art performance among open-source multimodal large language models. To align with our objective of capturing fine-grained structural attributes, we steer the captioning process to emphasize garment-level semantics, such as garment type, cut, or sleeve length, while deliberately omitting low-level visual features (e.g., color or texture). Empirically, we find that our proposed MHA module is sufficient for accurate color transfer without explicit textual supervision, allowing us to focus the text modality on structural conditioning. We train the main DiT module (i.e., FD) following diffusion loss with multiple conditioning signals: zg FD(zt, epooled, FE(z 0, xmodel, 0), t)2(cid:105) LDiT = Ezg,ϵt,t (7) (cid:104) , with FE(z 0, xmodel, 0) being the list of keys and values extracted from FE at timestep = 0. We extract this list from FE at = 0 and re-use them in FD for all subsequent timesteps, as we want to use key/values from clean data."
        },
        {
            "title": "3.3 Garment Aligner",
            "content": "While our model is effective at generating realistic and structurally coherent garments, we observe occasional failures in preserving high-frequency details such as fine-grained textures and logos. We hypothesize two primary contributing factors: (i) the diffusion loss Ldiff, defined in the noise space, optimizes over perturbed latents rather than directly over image-space reconstructions, limiting its sensitivity to fine-grained patterns; and (ii) the inherent generation dynamics of diffusion models, where errors introduced in early timesteps typically encoding low-frequency content can accumulate and degrade the fidelity of high-frequency details in later stages. To mitigate this, we draw inspiration from REPA [55], and propose to explicitly align the internal feature representation of our DiT with that of pre-trained vision encoder. Specifically, we encourage patch-wise consistency between the eighth Transformer block of our main DiT model FD and the corresponding features extracted from DINOv2 [36]. Let hDiT R3072d denote the token sequence obtained from the eighth Transformer block of the DiT decoder FD, corresponding to 64 48 patch grid with embedding dimension d. Separately, let henc R1024d be the 32 32 token grid extracted from frozen DINOv2 encoder, with embedding dimension (where = d). To bridge this mismatch, we introduce lightweight garment aligner module composed of convolutional neural network ϕCNN : R6448d R3232d which is used to downsample the spatial token grid while preserving local structure and to project the token embeddings into the DINOv2 feature space. The aligned tokens are defined as hDiT = ϕCNN(hDiT) R1024d . We then enforce feature-level consistency via cosine similarity loss: Lalign = Ezg,ϵt,t (cid:34) 1 (cid:88) i= (cid:16)hDiT , henc cos (cid:35) (cid:17) , (8) where hDiT the total number of tokens, and cos is the cosine similarity. and henc are the i-th aligned and reference tokens, respectively, is the patch index, is Overall Loss Function. Our final training objective combines the standard diffusion loss Ldiff with the garment alignment loss Lalign previously introduced. The overall objective is thus defined as: Ltotal = Ldiff + λ Lalign, (9) where λ is hyperparameter that balances the contribution of the two loss components."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Settings and Datasets We conduct our experiments using two publicly available fashion datasets: VITON-HD [8] and Dress Code [12]. Both datasets provide images at resolution of 1024 768. VITON-HD contains 6 Table 1: Quantitative results on the Dress Code dataset, considering both the entire test set and the three category-specific subsets. indicates higher is better, lower is better. All Upper-Body Method SSIM LPIPS DISTS FID KID SSIM LPIPS DISTS FID KID TryOffDiff [48] Any2AnyTryon [22] TEMU-VTOFF (Ours) 75.79 77.56 75.95 39.70 35.17 31.46 29.88 25.17 18. 70.02 12.32 5.74 42.80 3.65 0.65 76.59 76.61 74.54 40.62 38.99 35.48 29.04 25.78 19.75 37.97 17.30 3.22 15.77 0.76 10. Lower-Body Dresses Method SSIM LPIPS DISTS FID KID SSIM LPIPS DISTS FID KID TryOffDiff [48] Any2AnyTryon [22] TEMU-VTOFF (Ours) 74.10 78.15 73. 43.35 34.72 34.60 32.54 25.87 19.57 162.02 137.10 12.01 30.06 2.04 13.83 76.79 77.93 79.39 35.29 31.80 24.32 27.88 23.86 16. 72.19 47.58 6.27 19.20 0.59 11.29 Figure 3: Qualitative comparison on the Dress Code dataset between images generated by TEMUVTOFF and those generated by competitors. only upper-body garments and represents single-category setting, while Dress Code includes multiple categories (i.e., dresses, upper-body, and lower-body garments) enabling evaluation of the generalization capabilities of our methods across diverse garment types. For both the feature extractor and the diffusion backbone, we adopt Stable Diffusion 3 medium [14]. All models are trained on single node equipped with 4 NVIDIA A100 GPUs (64GB each), using DeepSpeed ZeRO-2 [41] for efficient distributed training. We use total batch size of 32 and train each model for 30k steps, corresponding to approximately 960k images. Optimization is performed with AdamW [34], using learning rate of 1 104, warmup phase of 3k steps, and cosine annealing schedule. We train separate models per dataset to account for differences in distribution and garment structure. In all experiments, we set the alignment loss weight λ equal to 0.5. 4.2 Comparison with the State of the Art To evaluate the proposed TEMU-VTOFF architecture, we use combination of perceptual, structural, and distributional similarity metrics. Specifically, we report LPIPS [58], SSIM [50], DISTS [13], FID [37], and KID [4]. We compare our approach against recent VTOFF methods, including TryOffDiff [48], TryOffAnyone [51], and Any2AnyTryon [22], the latter being more general framework designed for both VTOFF and VTON tasks. Notably, TryOffDiff and TryOffAnyone are trained exclusively on VITON-HD, which contains only upper-body garments. In contrast, Any2AnyTryon is trained on mixture of datasets including Dress Code, VITON-HD, and DeepFashion2 [19], enabling broader category coverage. For fair comparison on the multi-category Dress Code dataset, we retrain TryOffDiff from scratch using its publicly available code and default settings. Due to the lack of released training code, we are unable to retrain TryOffAnyone. This setup allows us to properly evaluate how well existing methods generalize across multiple categories and to highlight the robustness and flexibility of our approach in handling different garment types. Results on the Dress Code Dataset. Table 1 presents the image quality metrics on the Dress Code dataset. Our method significantly outperforms existing state-of-the-art approaches across all 7 Table 2: Quantitative results on the VITON-HD dataset. indicates higher is better, lower is better. Method SSIM LPIPS DISTS FID KID TryOffDiff [48] TryOffAnyone [51] Any2AnyTryon [22] TEMU-VTOFF (Ours) 75.53 75.90 75.72 77.21 39.56 35.26 37.95 28.44 25.53 23.47 24.32 18.04 17.49 12.74 12.88 8.71 5.30 2.85 3.01 1.11 Figure 4: Qualitative comparison on the VITON-HD dataset between images generated by TEMUVTOFF and those generated by competitors. evaluation metrics. The results are especially pronounced in this dataset, with respect to prior works, because of its multi-category setting. This shows that our method is category-agnostic and benefits from the joint use of textual garment descriptions and fine-grained masks. As result, our model achieves better perceptual quality and better alignment with the ground-truth distribution. In Fig. 3, we provide qualitative results comparing TEMU-VTOFF with competitors. This shows the challenges introduced by the Dress Code different set of categories. TryOffDiff often fails to preserve key visual characteristics such as color, texture and shape. Furthermore, it struggles with the reconstruction of lower-body items. Instead, Any2AnyTryon performs better, but often fails with the generation of the target garment. In contrast, our method is able to closely match the target garment across all categories. Results on the VITON-HD Dataset. In Table 2, we report the quantitative results on the VITONHD dataset. Also in this setting, TEMU-VTOFF achieves consistent improvements over existing approaches across all metrics. However, the improvements are less pronounced compared to those observed on the Dress Code dataset. This discrepancy is primarily due to the more constrained nature of VITON-HD, which focuses exclusively on upper-body garments. By contrast, Dress Code includes different categories, including full-body and lower-body items like dresses, skirts, and pants, which are more prone to occlusions and complex geometries. In these cases, the use of textual descriptions and fine-grained masks becomes crucial, offering strong guidance that significantly enhances garment reconstruction quality. Thus, the improvements of our approach are especially evident in more challenging multi-category scenarios. visual comparison on sample VITON-HD images is shown in Fig. 4, which further demonstrates the improved garment reconstruction quality of our proposed method. 4.3 Ablation Studies To assess the contribution of each component in our pipeline, we conduct detailed ablation study on the Dress Code dataset, as shown in Table 3. Specifically, we evaluate the impact of removing the textual description of the garment, the fine-grained mask, or both. Further, we investigate the impact of our garment aligner. The results show that all the components play crucial role, demonstrating their contribution to improving the final image quality. To better understand the strength of each component proposed with our approach, in Fig. 5 we provide visual comparison on the Dress Code dataset. Our method without textual conditioning relies exclusively on visual features extracted from the person wearing the garment, without any textual guidance. Notably, this often results in failures in challenging cases, especially for garments with 8 Table 3: Ablation study of the proposed components on the Dress Code dataset. All Upper-body Lower-body Dresses SSIM LPIPS DISTS FID KID DISTS FID DISTS FID DISTS FID Effect of Text and Mask Conditioning w/o text and masks w/o text modulation w/o fine-grained masks TEMU-VTOFF (Ours) 71.04 73.88 74.65 75.95 39.68 34.63 32.33 31.46 Effect of Garment Aligner Component w/o garment aligner TEMU-VTOFF (Ours) 76.01 75.95 30.84 31.46 25.20 22.54 20.87 18.66 9.63 7.75 6.58 5.74 3.17 1.52 1.03 0.65 23.71 24.02 20.85 19. 19.75 13.48 11.31 10.94 65.85 24.33 22.34 19.57 49.19 18.13 15.74 13.83 20.12 19.27 19.42 16.67 15.47 13.30 13.62 11.29 20.63 18. 5.91 5.74 0.78 0.65 21.77 19.75 11.26 10.94 22.26 19.57 14.22 13. 17.86 16.67 11.86 11.29 (a) Evaluation of mask and text joint impact. (b) Evaluation of garment aligner impact. Figure 5: Qualitative comparisons validating the effectiveness of the proposed components on the Dress Code dataset. strong occlusions such as bodysuits or garments partially hidden by arms or hair. This limitation has been addressed by conditioning the generation to also use textual structural description of the target garment. This structural guidance enables the model to better capture the structure of the garment, leading to significantly more accurate outputs in occluded scenarios. In addition, using fine-grained masks and the garment aligner further improves the quality of the generated garments. Limitations. While our method shows strong performance and generalization, some limitations remain. First, it struggles to reconstruct fine-grained details such as logos or printed text partly due to the SD3 medium backbone, which is known to handle such elements inconsistently. Second, performance on lower-body garments is less reliable than for upper-body garments and dresses, likely due to class imbalance in the Dress Code dataset. Additional discussion and failure cases are included in the supplementary material."
        },
        {
            "title": "5 Conclusion",
            "content": "We have presented TEMU-VTOFF, novel virtual try-off architecture designed to generate highquality, diverse in-shop garment images. Our approach is the first to effectively extend VTOFF to multi-category scenarios, addressing existing inherent shortcomings of existing methods with ad-hoc modules such as dual-DiT architecture equipped with our multimodal hybrid attention to effortlessly integrate garment, person, and text features. Moreover, we have proposed an external garment aligner module paired with new loss objective for visual detail refinement. Experimental results on standard VTOFF benchmarks demonstrate the robustness and effectiveness of our approach across range of garment types and evaluation metrics."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923, 2025. [2] Shuai Bai, Huiling Zhou, Zhikang Li, Chang Zhou, and Hongxia Yang. Single Stage Virtual Try-On Via Deformable Attention Flows. In ECCV, 2022. [3] Alberto Baldrati, Davide Morelli, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing. In ICCV, 2023. [4] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying MMD GANs. arXiv preprint arXiv:1801.01401, 2018. [5] Fred L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Trans. PAMI, 11(6):567585, 1989. [6] Chieh-Yun Chen, Yi-Chung Chen, Hong-Han Shuai, and Wen-Huang Cheng. Size Does Matter: Size-aware Virtual Try-on via Clothing-oriented Transformation Try-on Network. In ICCV, 2023. [7] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In CVPR, 2023. [8] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. VITON-HD: High-Resolution Virtual Try-On via Misalignment-Aware Normalization. In CVPR, 2021. [9] Yisol Choi, Sangkyung Kwak, Kyungmin Lee, Hyungwon Choi, and Jinwoo Shin. Improving diffusion models for authentic virtual try-on in the wild. In ECCV, 2024. [10] Zheng Chong, Xiao Dong, Haoxiang Li, Shiyue Zhang, Wenqing Zhang, Xujie Zhang, Hanqing Zhao, Dongmei Jiang, and Xiaodan Liang. CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models. In ICLR, 2025. [11] Aiyu Cui, Daniel McKee, and Svetlana Lazebnik. Dressing in Order: Recurrent Person Image Generation for Pose Transfer, Virtual Try-On and Outfit Editing. In ICCV, 2021. [12] Morelli Davide, Fincato Matteo, Cornia Marcella, Landi Federico, Cesari Fabio, and Cucchiara Rita. Dress Code: High-Resolution Multi-Category Virtual Try-On. In ECCV, 2022. [13] Keyan Ding, Kede Ma, Shiqi Wang, and Eero Simoncelli. Image Quality Assessment: Unifying Structure and Texture Similarity. IEEE Trans. PAMI, 44(5):25672581, 2020. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling Rectified Flow Transformers for High-Resolution Image Synthesis. In ICML, 2024. [15] Benjamin Fele, Ajda Lampe, Peter Peer, and Vitomir Struc. C-VTON: Context-Driven ImageBased Virtual Try-On Network. In WACV, 2022. [16] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. In NeurIPS, 2023. [17] Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, and Tali Dekel. TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space. In SIGGRAPH, 2025. [18] Chongjian Ge, Yibing Song, Yuying Ge, Han Yang, Wei Liu, and Ping Luo. Disentangled cycle consistency for highly-realistic virtual try-on. In CVPR, 2021. [19] Yuying Ge, Ruimao Zhang, Xiaogang Wang, Xiaoou Tang, and Ping Luo. DeepFashion2: Versatile Benchmark for Detection, Pose Estimation, Segmentation and Re-Identification of Clothing Images. In CVPR, 2019. [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014. [21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Networks. Communications of the ACM, 63(11):139144, 2020. [22] Hailong Guo, Bohan Zeng, Yiren Song, Wentao Zhang, Chuang Zhang, and Jiaming Liu. Any2AnyTryon: Leveraging Adaptive Position Embeddings for Versatile Virtual Clothing Tasks. arXiv preprint arXiv:2501.15891, 2025. [23] Xintong Han, Zuxuan Wu, Zhe Wu, Ruichi Yu, and Larry Davis. VITON: An Image-Based Virtual Try-On Network. In CVPR, 2018. [24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [25] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR, 2022. [26] Xun Huang and Serge Belongie. Arbitrary Style Transfer in Real-Time With Adaptive Instance Normalization. In ICCV, 2017. [27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. [28] Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, Chengming Xu, Jinlong Peng, Jiangning Zhang, Chengjie Wang, Yunsheng Wu, and Yanwei Fu. FitDiT: Advancing the Authentic Garment Details for High-fidelity Virtual Try-on. In CVPR, 2025. [29] Jeongho Kim, Gyojung Gu, Minho Park, Sunghyun Park, and Jaegul Choo. StableVITON: Learning Semantic Correspondence with Latent Diffusion Model for Virtual Try-On. In CVPR, 2024. [30] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [31] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [32] Sangyun Lee, Gyojung Gu, Sunghyun Park, Seunghwan Choi, and Jaegul Choo. HighResolution Virtual Try-On with Misalignment and Occlusion-Handled Conditions. In ECCV, 2022. [33] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow Matching for Generative Modeling. In ICLR, 2023. [34] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019. [35] Davide Morelli, Alberto Baldrati, Giuseppe Cartella, Marcella Cornia, Marco Bertini, and Rita Cucchiara. LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On. In ACM Multimedia, 2023. [36] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning Robust Visual Features without Supervision. arXiv preprint arXiv:2304.07193, 2023. [37] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On Aliased Resizing and Surprising Subtleties in GAN Evaluation. In CVPR, 2022. 11 [38] William Peebles and Saining Xie. Scalable Diffusion Models with Transformers. In ICCV, 2023. [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. [40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. JMLR, 21(140):167, 2020. [41] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In SC, 2021. [42] Bin Ren, Hao Tang, Fanyang Meng, Ding Runwei, Philip HS Torr, and Nicu Sebe. Cloth Interactive Transformer for Virtual Try-On. ACM TOMM, 20(4):120, 2023. [43] Bin Ren, Hao Tang, Yiming Wang, Xia Li, Wei Wang, and Nicu Sebe. PI-Trans: Parallelconvmlp and implicit-transformation based Gan for cross-view image translation. In ICASSP, 2023. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. [45] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. In NeurIPS, 2019. [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising Diffusion Implicit Models. In ICLR, 2021. [47] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In CVPR, 2023. [48] Riza Velioglu, Petra Bevandic, Robin Chan, and Barbara Hammer. TryOffDiff: VirtualTry-Off via High-Fidelity Garment Reconstruction using Diffusion Models. arXiv preprint arXiv:2411.18350, 2024. [49] Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, and Meng Yang. Toward Characteristic-Preserving Image-based Virtual Try-On Network. In ECCV, 2018. [50] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Processing, 13(4):600612, 2004. [51] Ioannis Xarchakos and Theodoros Koukopoulos. TryOffAnyone: Tiled Cloth Generation from Dressed Person. arXiv preprint arXiv:2412.08573, 2024. [52] Zhenyu Xie, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, and Xiaodan Liang. GP-VTON: Towards General Purpose Virtual Try-on via Collaborative Local-Flow Global-Parsing Learning. In CVPR, 2023. [53] Yuhao Xu, Tao Gu, Weifeng Chen, and Chengcai Chen. OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable Virtual Try-on. In AAAI, 2025. [54] Keyu Yan, Tingwei Gao, Hui Zhang, and Chengjun Xie. Linking garment with person via semantically associated landmarks for virtual try-on. In CVPR, 2023. [55] Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think. In ICLR, 2025. [56] Jianhao Zeng, Dan Song, Weizhi Nie, Hongshuo Tian, Tongtong Wang, and An-An Liu. CATDM: Controllable Accelerated Virtual Try-on with Diffusion Model. In CVPR, 2024. [57] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-Training. In ICCV, 2023. [58] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [59] Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. TryOnDiffusion: Tale of Two UNets. In CVPR, 2023."
        },
        {
            "title": "Contents",
            "content": "A Experimental Protocols A.1 Datasets Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Caption Extraction Details Additional Qualitative Results Limitations Broader Impact"
        },
        {
            "title": "A Experimental Protocols",
            "content": "A.1 Datasets Details 14 14 14 15 16 21 21 Dress Code. In our experiments, we adopt the Dress Code dataset [12], the largest publicly available benchmark for image-based virtual try-on. Unlike previous datasets limited to upper-body clothing, Dress Code includes three macro-categories: Upper-body: 15, 363 pairs (e.g., tops, t-shirts, shirts, sweatshirts) Lower-body: 8, 951 pairs (e.g., trousers, skirts, shorts) Dresses: 29, 478 (e.g., full body dresses) The total number of paired samples is 53, 792, split into 48, 392 training images and 5, 400 test images at resolution of 1024 768. VITON-HD. Following previous literature, we also adopt VITON-HD [8], publicly available dataset widely used in virtual try-on research. It is composed exclusively of upper-body garments and provides high-resolution images at 1024 768 pixels. The dataset contains total of 27, 358 images, structured into 13, 679 garment-model pairs. These are split into 11, 647 training pairs and 2, 032 test pairs, each comprising front-view image of garment and the corresponding image of model wearing it. A.2 Implementation Details We evaluate our method both with distribution-based metrics and per-sample similarity metrics. For the first group, we adopt FID [37] and KID [4] implementations derived from clean-fid PyTorch package. Concerning the second group, we adopt both SSIM [50] and LPIPS [58] as they are the standard metrics adopted in the field to measure structural and perceptual similarity between pair of images. We reuse the corresponding Python packages provided by TorchMetrics. Finally, we adopt DISTS [13] as an additional sample-based similarity metric, as it correlates better with human judgment, as shown in previous works [16]. We stick to the corresponding Python package to compute it for our experiments. https://pypi.org/project/clean-fid/ https://pypi.org/project/torchmetrics/ https://pypi.org/project/DISTS-pytorch/ 14 A.3 Algorithm To provide clear understanding of TEMU-VTOFF, we summarize the core components of our method in Algorithm 1. The pseudo-code outlines the sequential steps involved in training our dual-DiT architecture, including multimodal conditioning, the hybrid attention module, and the garment aligner component. Algorithm 1 TEMU-VTOFF: Virtual Try-Off with Dual-DiT and Garment Alignment Require: Person image xmodel, garment caption c, binary mask , target garment image xg Ensure: Generated garment ˆxg 1: Latent encoding: Encode the target garment: zg E(xg) Sample noise: ϵt (0, 1) Apply flow-matching: zt (1 t)zg + ϵt 2: Prepare masked spatial input: Encode masked person image: xM E(xmodel ) Concatenate inputs: 3: Extract modulation features: ev pool CLIP(xmodel) [zt, M, xM ] 4: Extract keys and values using feature extractor: 0, xmodel, t=0) extractor, 5: Encode text information: l=1 FE(z extractor}N {Kl Get pooled text embedding: epooled CLIP(c) Get full sequence text features: etext [CLIP(c), T5(c)] 6: Noise prediction: ˆϵt FD(zt, epooled, etext, {Kl Compute diffusion loss: Ldiff ˆϵt ϵt2 extractor, extractor}, t) 7: Align internal representations: Extract DiT features: hDiT tokens from 8th block of FD Extract DINOv2 features: henc DINOv2(xg) Align via projection: hDiT ϕCNN(hDiT) (cid:80) Compute alignment loss: Lalign 1 cos(hDiT , henc ) 8: Final objective: Combine losses: Ltotal Ldiff + λ Lalign 9: Decode final garment: Run reverse process: ˆxg D( ˆz0)"
        },
        {
            "title": "B Caption Extraction Details",
            "content": "We leverage Qwen2.5-VL [1] to generate the caption of given garment image, following the chat template provided below: visual_attributes = { \" dresses \" : [ \" Cloth Type \" , \" Waist \" , \" Fit \" , \" Hem \" , \" Neckline \" , \" Sleeve Length \" , \" Cloth Length \" ] , \" upper_body \" : [ \" Cloth Type \" , \" Waist \" , \" Fit \" , \" Hem \" , \" Neckline \" , \" Sleeve Length \" , \" Cloth Length \" ] , \" lower_body \" : [ \" Cloth Type \" , \" Waist \" , \" Fit \" , \" Cloth Length \" ] } category = \" upper_body \" # The category to which the image belongs System: You are Qwen, created by Alibaba Cloud. You are helpful assistant. User: Use only visual attributes that are present in the image. Predict values of the following attributes: {visual_attributes[category]}. Its forbidden to generate the following visual attributes: colors, background, and textures/patterns. Its forbidden to generate unspecified predictions. Its forbidden to generate newline characters. Generate in this way: <cloth type> with <attributes description> Qwen Caption: denim shirt with straight fit, long sleeves, and button-down neckline. The hem is straight and the shirt appears to be of standard length. We decide to generate structural-only attributes because our base model without text can already transfer colors and textures correctly from the person image to the generated garment image. The structural attributes are slightly different according to the three categories of clothing, as specified in visual_attributes. For example, the neckline can be specified for upper body and dresses (whole body garments), but not for lower body items."
        },
        {
            "title": "C Additional Qualitative Results",
            "content": "We show an extended version of the qualitative results presented in our main paper. We report additional visual results on the competitors in Fig. on the Dress Code [12] dataset and in Fig. on the VITON-HD [8] dataset. Furthermore, Fig. presents additional ablation results to analyze the impact of textual and mask conditioning. Finally, we include in Fig. the full set of inputs used for generating the target garment, including the model input, the segmentation mask, and the caption. 16 Figure F: Additional qualitative results of TEMU-VTOFF and competitors on Dress Code [12]. 17 Figure G: Additional qualitative results of TEMU-VTOFF and competitors on VITON-HD [8]. Figure H: Additional qualitative results showing the contribution of each component in TEMUVTOFF on Dress Code [12] images. 19 Figure I: Inputs used to generate the target garment with TEMU-VTOFF, using sample images from Dress Code [11]"
        },
        {
            "title": "D Limitations",
            "content": "Our model inherits some inner problems of foundational models like Stable Diffusion 3 [14]. Even though our method improved the generation of big logos and text, it is limited in its scope concerning fine-grained details like complex texture patterns and small written text. Moreover, it sometimes fails to render the correct number of small objects like buttons. We show set of failure cases in Fig. and Fig. K, on sample images from Dress Code [11] and VITON-HD [32] respectively."
        },
        {
            "title": "E Broader Impact",
            "content": "Our method addresses the VTOFF task by generating flat, in-shop garment images from photos of dressed individuals. This enables novel form of data augmentation in the fashion domain, allowing clean garment representations to be synthesized without manual segmentation or dedicated photoshoots. By bridging the gap between worn and catalog-like appearances, our approach can improve scalability for fashion datasets and support downstream applications such as retrieval, recommendation, and virtual try-on. However, as with any generative technology, there are important ethical and legal considerations. In particular, our model could be used to reconstruct garments originally designed by third parties, potentially raising issues of copyright and intellectual property infringement. We emphasize that our framework is intended for research and responsible use, and any deployment in commercial settings should ensure compliance with applicable copyright laws and respect for designer rights. 21 Figure J: An overview of failure cases on the Dress Code [12] dataset. Figure K: An overview of failure cases on the VITON-HD [8] dataset."
        }
    ],
    "affiliations": [
        "University of Modena and Reggio Emilia, Italy",
        "University of Pisa, Italy",
        "University of Trento, Italy"
    ]
}