{
    "paper_title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space",
    "authors": [
        "Zhenyi Shen",
        "Junru Lu",
        "Lin Gui",
        "Jiazheng Li",
        "Yulan He",
        "Di Yin",
        "Xing Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability."
        },
        {
            "title": "Start",
            "content": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space"
        },
        {
            "title": "SSA",
            "content": "Zhenyi Shen*1,2 Junru Lu*2 Lin Gui1 1Kings College London Jiazheng Li1 Yulan He1 Di Yin2 Xing Sun2 2Tencent Youtu Lab 5 2 0 2 5 ] . [ 1 2 0 1 0 2 . 1 1 5 2 : r The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit critical paradox: they produce lower attention sparsity than full attention modelsdespite aiming to approximate full attentionwhich may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked keyvalue pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgetsperformance improves consistently as more tokens are allowed to attendsupporting flexible computeperformance trade-offs at inference time. Finally, we show that native sparse attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in the sink areas, with SSA demonstrating the strongest extrapolation capability."
        },
        {
            "title": "1 Introduction",
            "content": "With the rapid advancement of large language models (LLMs), the demand for efficient long-context processing has grown substantially across diverse scenarios, including long-document understanding [Zhang et al., 2023, Jimenez et al., 2024], extended reasoning trajectories [OpenAI et al., 2024, DeepSeek-AI et al., 2025], and deep research workflows [Zheng et al., 2025]. The context length of LLMs has progressively expanded from 4K to 32K, 128K, and even up to 1M tokens [Yang et al., 2025]. However, the full self-attention mechanism in vanilla transformers [Vaswani et al., 2023] exhibits quadratic computational complexity with respect to context length, rendering training and inference with native full attention (Full-Full) computationally prohibitive for such extended contexts. To address this challenge, prior studies have investigated sparse attention mechanisms, which selectively restrict the number of tokens each query attends to. Early approaches employ training-free heuristics that directly apply sparse attention to models originally trained with full attention (Full-Sparse) [Xiao et al., 2024,, Jiang et al., 2024], exploiting the inherent sparsity patterns observed in full attention to accelerate inference. While computationally efficient, these training-free Full-Sparse methods frequently incur substantial performance degradation. To mitigate this limitation, native sparse attention methods have been proposed (e.g., NSA [Yuan et al., 2025], MoBA [Lu et al., 2025]), where models are trained and deployed with sparse attention (Sparse-Sparse) to narrow the performance gap with Full-Full approaches. *Equal contribution. Work done during the internship at Tencent Youtu Lab."
        },
        {
            "title": "SSA",
            "content": "(a) (b) (c) (d) Figure 1. Preliminary results using 300M-parameter models trained on 50B tokens. The FA model (full attention training) exhibits higher attention sparsity and lower attention entropy than the SA model (sparse attention training), and each performs best under its native inference mode. In contrast, SSA attains the highest attention sparsity and achieves the strongest performance under both fulland sparse-attention inference. (a) Perplexity on WikiText @8k; (b) Commonsense reasoning benchmark average scores; (c) Attention entropy. (d) Attention sparsity; Despite these advances, we identify critical paradox: Sparse-Sparse models, designed explicitly for sparsity, unexpectedly exhibit insufficient attention sparsity compared to full attention models (Figure 1d). This counterintuitive phenomenon fundamentally challenges the core premise of sparse attention, that only small subset of keys contributes substantially to the attention output, while the remainder can be safely omitted. It further implies that sparse-trained models learn attention patterns fundamentally different from those learned under full attention, which we hypothesize contributes to performance degradation during sparse inference. We attribute this reduced attention sparsity to gradient update deficiency inherent in sparse training: low-ranked keyvalue (KV) pairs are systematically excluded from selection and therefore receive no gradient updates, preventing the model from learning to effectively suppress uninformative tokens. To handle this, we propose SSA, unified training framework that jointly incorporates native sparse attention and native full attention while implementing explicit sparsity regularization through attention output alignment (Figure 2). Specifically, during training, SSA stochastically selects either full attention or sparse attention for the primary language modeling objective with equal probability (50%). This hybrid design enables the model to internalize sparse attention patterns while maintaining gradient updates for all keyvalue pairs through full attention streams, thereby enhancing its capacity to suppress uninformative tokens. Meanwhile, we introduce counterpart attention alignment mechanism at each layer to reinforce attention sparsity: full-attention outputs are further encouraged to align with their sparse-attention counterparts to promote learning of sparse representations, while sparse-attention outputs are simultaneously regularized toward full-attention outputs to prevent excessive drift from the full attentions behavior. Through this design, SSA attains substantially higher attention sparsity than both Full-Full and SparseSparse baselines. This improvement translates into strong performance under both sparse and full inference. Moreover, we observe that sparse-trained models consistently generalize better to longer context windows than full-attention models, with SSA demonstrating the most robust extrapolation. Our contributions are summarized as follows: We propose SSA, unified training framework that enhances the attention sparsity of sparse attention trained models by integrating both sparse and full attention during training and enforcing bidirectional attention output alignment. We show that increasing models inherent attention sparsity improves performance in both sparse and full attention inference modes. Extensive experiments demonstrate that SSA achieves state-of-the-art performance across multiple benchmarks, supports flexible inference under different sparsity levels (e.g, various inference budgets), and delivers superior long-context extrapolation."
        },
        {
            "title": "2 Related Work",
            "content": "Training-Free Sparse Attention. Training-free sparse attention leverages the intrinsic sparsity in the attention distribution, allowing it to closely approximate full attention while significantly reducing computational cost. widely used form is sliding-window attention, which restricts each token to attend only to its local neighborhood [Child et al., 2019, Beltagy et al., 2020, Brown et al., 2020]. StreamingLLM [Xiao et al., 2024] further observes an attention sink phenomenon, where substantial attention mass is placed on the initial tokens, and incorporates these early tokens into the sparse attention set. Beyond fixed patterns, dynamic sparse attention selects informative positions based on the relevance between the query and previous keys. common approach is block-sparse attention [Xiao et al., 2024, Jiang et al., 2024, Xu et al., 2025, Tang et al., 2024, Zhang et al., 2025], which partitions the context into blocks, estimates block importance, and selects the top-k blocks for computation. In nutshell, we categorize all training-free sparse attention methods as Full-Sparse type. Trainable Sparse Attention. To further improve the effectiveness of sparse attention, MoBA [Lu et al., 2025] employs block-sparse attention during training, achieving performance comparable to full attention while being substantially more efficient. NSA [Yuan et al., 2025] integrates three complementary attention patterns: coarse global attention, block-sparse attention, and sliding-window attention, combined via gating module. Although NSA reports performance surpassing full-attention models, subsequent analysis suggests that the improvement is largely attributable to the additional gated-attention mechanism rather than sparsity alone [Qiu et al., 2025]. Moreover, its multi-component structure makes it less flexible to adjust sparsity levels or revert to full attention at inference time. Concurrent to our work, InfLLM-v2 [Zhao et al., 2025] refines block selection through two-level hierarchical mechanism. DSA [DeepSeek-AI, 2025] advances token retrieval by selecting top-k keys at token-level granularity rather than block-level. Although this approach still exhibits nominal O(n2) complexity, its cost is largely mitigated by highly optimized system implementations. In this work, we define native sparse attention approaches as Sparse-Sparse paradigms. In contrast to aforementioned Full-Sparse and Sparse-Sparse approaches, our work focuses on natively training models under sparse attention while explicitly encouraging higher attention sparsity, enabling the sparse-attention pathway to serve as closer proxy to full attention, particularly when dropping an equivalent number of KV cache entries. Meanwhile, the full-attention pathway of the SSA model still maintains effective performance and good extrapolation."
        },
        {
            "title": "3.1 Background",
            "content": "Full Attention. In the standard softmax attention mechanism, each token attends to all preceding tokens through learned, token-specific weighted aggregation: yt = softmax(qtKT :t )VT :t (1) Sparse Attention. In contrast, sparse attention restricts each query to attend only to subset of preceding keys and values. Specifically, we focus on block-sparse attention. Following NSA [Yuan et al., 2025] and MoBA [Lu et al., 2025], we divide the input sequence into multiple blocks and obtain block representations by mean-pooling the token embeddings within each block. For each query, we compute its similarity with all preceding block representations using dot products, since an additional softmax normalization does not affect the ranking of blocks, and select the top-k most relevant ones. The use of mean pooling ensures that"
        },
        {
            "title": "SSA",
            "content": "Figure 2. Illustration of the SSA training framework. At each iteration, the model has an equal probability of following either the Sparse Attention (SA) stream or the Full Attention (FA) stream. In the SA stream, the model learns sparse attention while aligning its output with full-attention counterpart computed on the fly. Conversely, in the FA stream, the model learns full attention constrained by alignment with the corresponding sparse-attention output. For clarity, skip connections, normalization, and dropout layers are omitted from the figure. the block-level similarity preserves the relative ranking of token-level attention scores, as shown below: Mean(qK) = Mean(K) (2) This property allows block selection to approximate the token-level attention ranking effectively. The selected blocks are then concatenated to form the reduced key and value sets, denoted as and V, which are used for attention computation: = { ki Top-k } :t ) VT yt = softmax(qt KT :t (3) (4) If the total number of blocks is and each block contains tokens, the resulting sparsity ratio is approximately k/n, and the computational complexity is bounded by O((ks)2). This reduces the quadratic cost of standard self-attention to sub-quadratic regime while preserving most of the relevant contextual information."
        },
        {
            "title": "3.2 Observations",
            "content": "We pre-train two models with full and sparse attention, respectively, which we refer to as the FA model and the SA model, and evaluate each using both full and sparse attention during inference to examine their behavior in different modes. Setup. We follow the same experimental configuration as Section 4, but reduce the model size to 300M parameters and the pre-training corpus to 50B tokens. For evaluation, we report WikiText word perplexity as the primary validation metric and additionally measure performance on commonsense reasoning benchmarks. To quantify attention sparsity, we introduce two complementary metrics: attention entropy and"
        },
        {
            "title": "SSA",
            "content": "attention sparsity. Attention entropy measures the dispersion of attention weights via Shannon entropy, where lower values indicate more concentrated (i.e., sparser) attention. Attention sparsity provides more direct measure by computing the fraction of total attention mass contained within the top-k tokens selected by the block-sparse mechanism; thus, higher values indicate closer approximation to full-attention behavior. In our experiments, we adopt top-16 with block size of 16 (i.e., = 256), and compute AttnSparsity(8192) over 100 PG19 samples [Rae et al., 2019]. AttnEntropy(t) = j<t at,j log at,j AttnSparsity(t) = j<t at,j 1(cid:2)blk(j) Tk(t)(cid:3) (6) where denotes the position of the query token, at, is the normalized attention weight between the tokens and j, blk(j) represents the block index of the token j, Tk(t) is the set of top-k blocks ranked by block-level attention, and 1[] is the indicator function. (5) Observation 1: The SA model achieves better sparse-attention performance than the FA model. Although the benchmark scores are similar, SA-Sparse exhibits lower perplexity than FA-Sparse (Figure 1b). This suggests that end-to-end training with sparse-attention enables the model to adapt to sparsity patterns and better utilize limited attention capacity. Observation 2: The SA model performs poorly under full attention. SA-Full shows benchmark performance comparable to SA-Sparse, yet its perplexity is substantially higher than FA-Full. This degradation stems from the SA models high entropy and low sparsity in the attention distribution (Figure 1c, Figure 1d): instead of concentrating attention on informative keys, the model assigns disproportionately high weights to many irrelevant ones, harming full-attention inference. The underlying cause is that low-ranked keyvalue tokens are omitted during sparse-attention training and thus receive neither forward contribution nor gradient updates. As result, the model never learns to downweight these tokens, limiting its ability to generalize or extrapolate to larger receptive fields. Observation 3: Sparse attention is an imperfect approximation of full attention in the FA model. Sparse attention can be leveraged by assuming that discarded keys contribute negligible mass. However, Figure 1d shows that an average of 47% of the total attention mass is dropped at each layer; this approximation error accumulates across layers and leads to clear degradation in downstream performance (Figure 1b). Collectively, these observations provide two key takeaways: (1) and (2): While sparse-attention training improves sparse-attention inference, it reduces the models inherent attention sparsity; (3): Models exhibiting higher attention sparsity enable sparse attention to better serve as proxy for full attention, thereby yielding stronger performance. This leads to central question: If the inherent attention sparsity of model trained with sparse attention can be increased, will its sparse-attention inference performance correspondingly improve and approach that of full attention trained model?"
        },
        {
            "title": "3.3 SSA: Sparse Sparse Attention",
            "content": "Inspired by previous observations, we propose SSA (Sparse Sparse Attention), which incorporates native sparse attention while explicitly encouraging sparsity-centric attention distribution (Figure 2). SSA optimizes two targets: the first is the standard cross-entropy loss for next-token prediction, applied under either sparse or full attention with equal probability; the second is bi-directional alignment loss that enforces consistency between the sparse-attention and full-attention outputs at each layer (detailed in Algorithm 1). = mode{full,sparse}[Lmode] + αLalignment (7) where Lmode denotes the cross-entropy loss computed under the sampled attention mode (full or sparse), α is weighting coefficient, and Lalignment is the bi-directional alignment loss encouraging consistency between full and sparse attention outputs. Algorithm 1 SSA Dual-Stream Training with Symmetric Alignment Require: x0 (embeddings), optional y, number of blocks L, alignment weight α, routing prob. pFA if training then goFA Bernoulli(pFA) else goFA False"
        },
        {
            "title": "SSA",
            "content": "end if MAIN_ATTN FULL_ATTN if goFA else SPARSE_ATTN AUX_ATTN SPARSE_ATTN if goFA else FULL_ATTN x0; for = 1 to do loss_alignment 0 a_aux AUX_ATTN(q,k,v) NORM1_l(x) Q_l(h); K_l(h); V_l(h) a_main MAIN_ATTN(q,k,v); loss_sparsity a_mainstop_gradient(a_aux) loss_commitment a_auxstop_gradient(a_main) loss_alignment loss_sparsity + loss_commitment + O_l(GATE_l(h) * a_main) NORM2_l(x) + FFN_l(h) end for loss_alignment loss_alignment/L; logits LM_HEAD(x) if training then loss_ce CrossEntropy(logits, y) return logits, loss_ce + α loss_alignment LN_F(x) else return logits end if Sparse or Full Attention Modes. During training, we alternate between full and sparse attention streams with equal probability, as illustrated in Figure 2. We adopt both modes because full attention naturally produces more contrastive and inherently sparse attention distribution, while sparse attention aligns better with the inference-time operation. Instead of jointly optimizing the two losses, we alternate between them to reduce computational cost and ensure that the model processes an equal number of tokens as the baseline, thereby maintaining fair training efficiency. Counterpart Attention Alignment. To further promote sparsity and maintain consistency between the two attention modes, we introduce counterpart attention alignment mechanism. At each layer, we compute an auxiliary attention output from the opposite attention mode (e.g., if the current stream uses full attention, we additionally compute the sparse attention output). This auxiliary computation is used only for the alignment objective and is not propagated to the next layer. The alignment objective consists of two complementary components. The first is sparsity loss, which encourages the full-attention output to mimic the sparse-attention output, thereby promoting sparser and more selective attention distribution: where sg[] denotes the stop-gradient operator, with afull and asparse referring to the full and sparse attention outputs, respectively. In practice, we use the SmoothL1 loss [Girshick, 2015, Shen et al., 2025]. Lsparsity = afull sg[asparse] (8) The second component is commitment loss, which regularizes the sparse-attention outputs to remain close"
        },
        {
            "title": "SSA",
            "content": "to the full-attention outputs, analogous to the KL-divergence term used in RLHF [Ouyang et al., 2022]: Lcommitment = asparse sg[afull] (9) This objective is also conceptually similar to the commitment loss in VQ-VAE [van den Oord et al., 2017], encouraging the sparse-attention branch to align with the representational space learned by full attention. By enforcing such alignment, the model achieves more stable joint optimization of the two attention modes. The total alignment loss combines both components: Lalignment = Lsparsity + Lcommitment (10) Together, this bi-directional alignment encourages full attention to become inherently sparser while ensuring that the sparse-attention pathway remains stable and consistent with its full-attention counterpart during training. Conceptually, this loss aligns the two attention distributions in value-aware manner. Compared to directly aligning full attention distributions, it is substantially more efficient: the latter requires materializing dense attention maps, which are incompatible with online softmax implementations such as FlashAttention and incur prohibitive memory and computational costs. Other Setups. We define the Receptive Field of sparse model as the maximum number of keys available for attention. For instance, with block size of 16 and top-16 block selection, the receptive field is 16 16 = 256. For an 8k-token context, this corresponds to sparsity of 1 256 8192 = 96.9%. During training, the probability of sampling either full or sparse attention is fixed at 50%. More configurations can be found in Appendix A. Efficiency Analysis. During inference, the sparse attention operation of SSA is identical to that of MoBA [Lu et al., 2025]. Thus, it is extremely efficient for long context inference because it decomposes sequence of length into two steps: fast dot product over blocks, and an attention computation over the concatenated top-k blocks of size n/m. The only extra cost comes from splitting the original KV cache into blocks. During training, although we compute full attention on the fly, we do not use it for subsequent computations like feedforward or output softmax layers. Thus, the training cost is not doubled but is marginally increased."
        },
        {
            "title": "4 Experimental Setup",
            "content": "Pretraining Setup. We follow the architecture and configuration of Llama-3.2-1B [Grattafiori et al., 2024], with two key modifications. First, we reduce the number of keyvalue heads (num_head_kv) to 2 in order to accommodate the implementation of block-sparse attention and speed up the training. Second, we adopt Gated Attention [Qiu et al., 2025], which effectively mitigates the attention-sink phenomenon, particularly detrimental to training-free sparse-attention methods, and improves overall performance (see Appendix for further discussion). This mechanism is also implicitly employed in NSA [Yuan et al., 2025], so we adopt this to all methods for fair comparison. The model is pre-trained on the SmolLM corpus [Allal et al., 2025] for 100B tokens with context length of 8k. We use learning rate of 1e-3, decayed to 0 using cosine annealing, and global batch size of 3.15M tokens. Baselines. We compare SSA against the following baselines: (1) FullAttn: the standard full attention mechanism. (2) MoBA [Lu et al., 2025]: trainable sparse attention method conceptually aligned with SSAs Sparse Attention Stream. (3) NSA [Yuan et al., 2025]: more complex sparse attention framework incorporating three components: compression, selection, and sliding window, where the selection module is analogous to MoBA. For each baseline, we train two configurations: one with receptive field of 1024 (block size 32, top-32 blocks, which is ablated in Appendix E), and one with receptive field of 256 (block size 16, top-16 blocks). Note that NSA effectively has 1.5 larger receptive field than other sparse-attention models because its sliding-window module contributes an additional 0.5 view. For SSA, we report the 1024-"
        },
        {
            "title": "SSA",
            "content": "receptive-field results obtained by extrapolating the 256-receptive-field model, as that model consistently performs better, this is likely due to inducing higher sparsity regularisation (see Section 5.5 for details). Evaluation. We evaluate our models along two major dimensions. First, we assess performance on classical commonsense-reasoning benchmarks: PIQA [Bisk et al., 2019] (3 shots), Hellaswag [Zellers et al., 2019] (10 shots), ARC-Easy [Clark et al., 2018] (25 shots), and ARC-Challenge [Clark et al., 2018] (25 shots). We additionally measure word perplexity on WikiText with the context length capped at 8k. Second, we evaluate the models length-extrapolation ability despite being trained only on 8k-context data. Specifically, we use LongBench [Bai et al., 2024] (16 English benchmarks) to measure long-context understanding, Needle-inA-Haystack from RULER [Hsieh et al., 2024] to assess retrieval, and PG19 [Rae et al., 2019] to compute long-context perplexity, where PPL is obtained via sliding-window evaluation with stride of 256 [Press et al., 2021, 2022]. All benchmarks are run using lm-evaluate-harness [Gao et al., 2024], and normalized accuracy is reported when applicable. Except for NSA, all models can be evaluated under both sparseand full-attention modes at inference, enabling us to measure how well they generalize when given full KV-cache access. Unless otherwise specified, we use their 1616trained variants for extrapolation."
        },
        {
            "title": "5.1 Language Modeling",
            "content": "As shown in Table 1, SSA achieves the lowest perplexity among all sparse-attention baselines under the same sparsity budget, and notably matches the FullAttn model when evaluated with full attention. This confirms that introducing the sparse-attention training stream and the alignment loss does not weaken the models capability under full attention. More importantly, SSA provides clear evidence relevant to our research hypothesis: increasing the inherent sparsity of models attention distribution improves its sparse-attention inference quality. Because sparse attention cannot perfectly replicate full attention due to its limited view of the KV cache, the alignment loss encourages the full-attention pathway toward more intrinsically sparse distribution  (Table 6)  . As full-attention becomes sparser, its behavior moves closer to what sparse attention can express, thereby narrowing the performance gap between the two modes. This mechanism directly manifests in the PPL results: SSA exhibits substantially smaller discrepancy between its sparse and full attention perplexities, consistent with the significantly lower KL divergence at the output logits  (Table 6)  . Thus, by increasing the models inherent attention sparsity without compromising modeling capacity, SSA brings sparse-attention inference closer to the performance of full-attentiontrained model, providing affirmative evidence for our research question."
        },
        {
            "title": "5.2 Commonsense Reasoning",
            "content": "As shown in Table 1, SSA consistently outperforms all other baselines under the same sparsity budget, and notably even surpasses the FullAttn model while using receptive field of only 256. Although SSA attains the same full-attention PPL as the FullAttn baseline, it achieves substantially higher benchmark performance. Since the only differences between them lie in SSAs sparser attention distribution and its ability to run sparse-attention inference, the performance gains are most plausibly attributed to the former. This interpretation is further supported by our ablation results (NoAlignmentLoss and FullRatio=1 in Table 3). Removing the alignment loss, which does not encourage sparser attention distribution, significantly degrades benchmark performance. In contrast, training with only the full-attention stream but retaining the alignment loss (FullRatio=1), which does push full attention toward sparser distribution, leads to improved benchmark results. Therefore, SSAs benchmark advantage provides empirical support for our"
        },
        {
            "title": "SSA",
            "content": "Method PIQA/% HellaSwag/% ARC-E/% ARC-C/% Average/% Wikitext PPL FullAttn MoBA SSA FullAttn MoBA NSA SSA FullAttn MoBA NSA SSA Full Attention Inference 73.50 73.39 74.10 73.39 73.45 73.88 73.94 73.29 73.07 74.32 74.21 58.09 56.08 58.15 69.11 68.31 69.82 37.20 36.52 38. 59.48 58.58 60.22 Sparse Attention Inference (Receptive Field = 256) 57.68 56.38 57.44 58.01 68.73 67.97 68.22 69.28 36.43 36.60 37.29 38.23 59.06 58.60 59.21 59. Sparse Attention Inference (Receptive Field = 1024) 58.09 56.11 57.56 58.14 69.11 68.01 68.73 69.91 37.46 36.6 36.6 38.82 59.49 58.45 59.3 60.27 15.18 16.88 15. 17.18 16.69 15.92 15.88 15.7 15.89 15.48 15.39 Table 1. Comparison of different attention training methods under both full and sparse attention inference. The receptive field denotes the maximum number of accessible tokens during sparse-attention inference. SSA consistently outperforms or matches all other methods across all levels of sparsity. (a) (b) (c) (d) Figure 3. Performance versus receptive-field size. SSA and FullAttn extrapolate well, consistently improving as more tokens become visible, whereas MoBA exhibits poor extrapolation. research hypothesis: models with higher inherent attention sparsity not only improve sparse-attention inference but also generalize better on reasoning tasks. These findings collectively indicate that sparser attention distribution directly benefits downstream reasoning tasks. plausible explanation is that models with sparser attention allocate more weight to informative tokens and are less influenced by irrelevant positions, enabling them to learn less noisy representations during pre-training and make more precise decisions at inference time."
        },
        {
            "title": "5.3 Extrapolation between Different levels of Sparsity",
            "content": "We observe that SSA extrapolates effectively across different sparsity levels, exhibiting largely monotonic performance improvement across all four tasks as more tokens are included in sparse-attention computation (Figure 3). In contrast, MoBA fails to extrapolate, likely because its attention distribution is insufficiently sparse, which appears essential for stable extrapolation. Moreover, MoBA lacks mechanisms that encourage consistency between its sparse-attention and full-attention behaviors. As such, its sparse predictions may therefore diverge rather than improve as more tokens are added. FullAttn displays qualitatively similar extrapolation behavior to SSA, but performs worse at nearly all sparsity levels."
        },
        {
            "title": "SSA",
            "content": "Method Needle in Haystack / % Perplexity LongBench / % 4k 8k 16k 32k 4k 8k 16k 32k 32k Full Attention Inference FullAttn MoBA SSA 100 100 100 100 20.8 100 0 0 58. 0 0 31.6 15.56 16.39 15.69 15.07 16.23 15.20 68.51 22.32 14.96 149.1 27.43 15.40 Sparse Attention Inference (Receptive Field = 256) FullAttn MoBA NSA SSA FullAttn MoBA NSA SSA 25.6 87.8 83.8 89.0 43.4 82.2 96.2 94.6 4.6 37.2 31.8 51.8 0 10.8 7 8. 0 2.2 6.4 9.2 16.85 16.34 16.00 16.12 16.72 16.01 15.65 15.78 56.81 17.30 15.56 15.69 118.44 17.08 15.70 16.40 Sparse Attention Inference (Receptive Field = 1024) 13.8 45.8 55 50.0 0 12.2 17.2 23.6 0 0 8.8 7.8 15.81 16.09 15.69 15.78 15.54 15.71 15.29 15.38 62.45 46.04 15.12 15. 130.6 115.04 15.05 15.8 14.58 10.17 20.01 10.91 15.07 18.01 18.56 12.71 12.78 18.21 20.75 Table 2. Evaluation across multiple context lengths using models pretrained up to 8K tokens. Native sparse-attention models demonstrate better extrapolation than FullAttn. SSA and NSA perform the best, with SSA surpassing NSA on LongBench, non-synthetic long-context understanding benchmark. The best results in each inference mode are bolded. (a) (b) (c) (d) Figure 4. (a) Perplexity across context lengths. FullAttn and SparseAttn in parentheses indicate full-attention and sparse-attention (receptive field = 256) inference, respectively. (b) Increasing the proportion of sparse-attention training in SSA improves length extrapolation. (c) SSA produces higher local-logit weights than MoBA and FullAttn. (d) FullAttn allocates substantial attention mass to tokens beyond 8k. Panel (a) uses 1B models, while panels (bd) use 300M models, and all of them use full attention for inference."
        },
        {
            "title": "5.4 Long Context Evaluation",
            "content": "Needle-in-a-Haystack (NIAH). In Table 2, SSA is the strongest sparse-attention method at nearly all receptive fields (except 1024) and attains 100% accuracy under full-attention inference. Notably, beyond the training length (8K), FullAttn collapses to 0% accuracy, whereas sparse-attention-trained models maintain non-zero retrieval accuracy and can recover non-trivial performance in full-attention mode (e.g., 58.8% at 16K). Perplexity (PPL). We observe that both FullAttn and MoBA exhibit PPL explosion once the context length exceeds their pre-training window, with MoBA degrading more slowly at receptive field of 256 but eventually diverging at 1024. In contrast, SSA and NSA maintain stable, low PPL even at 32k. Although SSA is slightly outperformed by NSA in terms of PPL, this is likely because NSA benefits from an additional sliding-window module (see Appendix F.1). However, NSA introduces substantially greater architectural complexity, and its formulation prevents extrapolation to full-attention inference. SSA, by contrast, preserves its PPL stability even under full-attention evaluation, highlighting its simplicity and robustness."
        },
        {
            "title": "SSA",
            "content": "LongBench. While NIAH and perplexity offer useful long-context diagnostics, they may not fully capture models true long-context understanding. LongBench provides more comprehensive evaluation, and sparse-attention-trained models continue to outperform their counterparts. Notably, SSA consistently achieves the best results across all inference modes. Discussion. Sparse-attentiontrained models, including both MoBA and SSA, exhibit superior length extrapolation not only when using sparse attention at inference, but even under full attention evaluation (Figure 4a). This suggests that the improvement does not stem from the sparse inference pattern itself; rather, training with sparse attention fundamentally reshapes the models inductive bias, enabling better generalization to longer contexts. From Figure 4b, we observe that applying the alignment loss solely to the full-attention stream (SSA (FullRatio=1)) already improves length extrapolation. Furthermore, training full-attention model with an additional sparse-attention stream, even without alignment loss (SSA (NoAlignment)), also yields strong length generalization. This indicates that either (1) encouraging full-attention outputs to match sparse-attention outputs, or (2) introducing sparse-attention auxiliary task with shared parameters, significantly enhances models extrapolation capability. We attribute this effect to the attention sink phenomenon, also referred to as the bright-band pattern [Xiong et al., 2025], that arises in full-attention training. small subset of low-frequency rotary bands develops abnormally large norms, causing the model to over-concentrate attention on the earliest tokens. We quantify this sink by measuring the total attention mass assigned to the first 30% tokens of the sequence. As shown in Figure 4c, this pattern strongly correlates with the perplexity explosion in long-context settings (More details are discussed in Appendix F.2). One possible explanation for the emergence of attention sink is that softmax enforces the attention weights to sum to one [Xiao et al., 2024], causing large positive logits in few data-independent positions to dominate the distribution. Sparse attention naturally mitigates this issue by limiting the number of visible tokens during training, effectively enforcing form of length extrapolation during training, which prevents runaway concentration on early positions. Thus, incorporating mechanisms that pull full attention closer to sparse attention, either through alignment of intermediate representations or through multitask training, can substantially reduce the attention sink and improve length extrapolation. Finally, this phenomenon is reflected in the attention sparsity curves (Figure 4d): the scattered high attention scores caused by the attention sink substantially reduce overall attention sparsity. The same figure also clarifies why MoBA performs poorlyits attention sparsity does not extrapolate well under full-attention evaluation, leading to degraded performance in long-context settings."
        },
        {
            "title": "5.5 Ablation Studies",
            "content": "Sparsity Levels. We observe that training with larger receptive field (e.g., 16x32 or 16x64) does not improve SSA performance  (Table 3)  ; these models even underperform those trained with smaller receptive field, despite being evaluated under the same inference setting. We hypothesize that smaller receptive field imposes stronger structural constraints, providing more effective regularization for learning sparse attention patterns. Conversely, shrinking the receptive field too aggressively (e.g., 8x16) also fails to yield further benefits. These results suggest that SSA requires balanced design, and that an optimal receptive-field sweet spot is needed for the best performance. Sampling Ratio to the Full Attention Stream. We vary the mixing ratio between the FA and SA streams. Moderate inclusion of the SA stream (e.g., FullRatio = 0.75) provides near-optimal perplexity, while placing more weight on the FA stream generally yields better downstream benchmark results. Eliminating either stream leads to noticeable performance degradation. Alpha. We observe that different weightings α affect performance, as they control the relative strength of the two loss terms. Careful tuning is required to balance the objectives effectively. 11 Method PPL Commonsense Avg. Method PPL Commonsense Avg."
        },
        {
            "title": "SSA",
            "content": "Baseline (SSA): MoBA: PPL = 24.20 Commonsense Avg. = 49.69 PPL = 24.51 Commonsense Avg. = 48.81 Sparsity Level Sampling Ratio to the Full Attention Stream Baseline(inf816) train816 Baseline(inf1632) train1632 Baseline(inf1664) train16 α = 5 α = 10 (Baseline) α = 20 25.26 25.97 23.52 23.60 23.23 23.22 Alpha 24.16 24.20 24.31 49.69 48.88 49.78 49.32 49.96 49.16 FullRatio=1 FullRatio=0.75 FullRatio=0.5 (Baseline) FullRatio=0.25 FullRatio=0 Random Each Layer 25.16 24.27 24.20 24.32 24.40 NaN Alignment Loss 48.71 Only FullSparse 49.69 Only SparseFull 49.45 No Alignment Loss NaN NaN 24.48 49.58 49.66 49.69 49.25 49.00 0 0 0 48. Table 3. Ablation studies. trainAB denotes training with receptive field size and block size B. FullRatio indicates the sampling ratio of the Full Attention Stream in Figure 2. Only FullSparse applies alignment only from full attention to sparse attention, while Only SparseFull applies alignment only in the reverse direction. Alignment Loss. Without the alignment loss Lalignment performance drops considerably. We hypothesize this is because Full Attention and Sparse Attention prefer different weighting patterns for best performance, and abruptly switching between them harms stability. Using only one direction of the alignment loss also results in unstable training. We speculate this is due to asymmetric over-distillation: FullSparse forces the full attention path to overfit sparse patterns, degrading full-attention capability, while SparseFull has the opposite issue. Bi-directional alignment is therefore necessary to stabilize training."
        },
        {
            "title": "6 Conclusion",
            "content": "We identified critical paradox in sparse-attention training: native sparse-attention methods unexpectedly exhibit insufficient attention sparsity due to gradient-update deficiency on excluded keyvalue pairs. To address this, we proposed SSA, unified framework that jointly trains sparse and full attention with bidirectional output alignment. SSA achieves the highest attention sparsity among all methods, delivers superior performance on perplexity and commonsense-reasoning tasks in both sparse and full inference modes, and demonstrates strong robustness across different sparsity levels. Moreover, we show that sparseattentiontrained models exhibit stronger long-context extrapolation than full-attention models, with SSA achieving the best results on long-context understanding. Our work reveals that encouraging high attention sparsity during training benefits not only sparse-attention inference but also full-attention inference, opening new directions for designing scalable, efficient long-context LLMs that maintain high utility under diverse computational budgets."
        },
        {
            "title": "7 Contact Information",
            "content": "Emails: Zhenyi Shen: zhenyi.shen@kcl.ac.uk, Junru Lu: junrulu@tencent.com"
        },
        {
            "title": "References",
            "content": "[1] Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. RepoCoder: Repository-level code completion through iterative retrieval and generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 24712484, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.151. URL https: //aclanthology.org/2023.emnlp-main.151/. [2] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues?, 2024. URL https: //arxiv.org/abs/2310.06770. [3] OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, and et al. Openai o1 system card, 2024. URL https://arxiv.org/ abs/2412.16720. [4] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, and et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [5] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments, 2025. URL https://arxiv.org/abs/2504.03160. [6] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report, 2025. URL https: //arxiv.org/abs/2501.15383. [7] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/abs/1706. 03762. [8] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=NG7sS51zVF. [9] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. InfLLM: Training-free long-context extrapolation for LLMs with an efficient context memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=bTHFrqhASY. [10] Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir H. Abdi, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention, 2024. URL https://arxiv.org/abs/ 2407.02490. [11] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei, Lean Wang, Zhiping Xiao, Yuqing Wang, Chong Ruan, Ming Zhang, Wenfeng Liang, and Wangding Zeng. Native sparse attention: Hardware-aligned and natively trainable sparse attention. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2307823097, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1126. URL https://aclanthology.org/2025.acl-long.1126/."
        },
        {
            "title": "SSA",
            "content": "[12] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, and Jiezhong Qiu. Moba: Mixture of block attention for long-context llms, 2025. URL https://arxiv.org/abs/2502.13189. [13] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers, 2019. URL https://arxiv.org/abs/1904.10509. [14] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer, 2020. URL https://arxiv.org/abs/2004.05150. [15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https://arxiv.org/abs/2005.14165. [16] Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. XAttention: Block sparse attention with antidiagonal scoring. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/forum?id=KG6aBfGi6e. [17] Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Queryaware sparsity for efficient long-context llm inference, 2024. URL https://arxiv.org/abs/2406. 10774. [18] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattention: Accurate and training-free sparse attention accelerating any model inference, 2025. URL https://arxiv.org/abs/2502.18137. [19] Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Gated attention for large language models: Non-linearity, sparsity, and attention-sink-free, 2025. URL https://arxiv.org/abs/ 2505.06708. [20] Weilin Zhao, Zihan Zhou, Zhou Su, Chaojun Xiao, Yuxuan Li, Yanghao Li, Yudi Zhang, Weilun Zhao, Zhen Li, Yuxiang Huang, Ao Sun, Xu Han, and Zhiyuan Liu. Infllm-v2: Dense-sparse switchable attention for seamless short-to-long adaptation, 2025. URL https://arxiv.org/abs/2509.24663. [21] DeepSeek-AI. Deepseek-v3.2-exp: Boosting long-context efficiency with deepseek sparse attention, 2025. [22] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling, 2019. URL https://arxiv.org/abs/1911.05507. [23] Ross Girshick. Fast r-cnn, 2015. URL https://arxiv.org/abs/1504.08083. [24] Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, and Yulan He. Codi: Compressing chainof-thought into continuous space via self-distillation, 2025. URL https://arxiv.org/abs/2502.21074. [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc., 2022. URL https://proceedings.neurips. cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf."
        },
        {
            "title": "SSA",
            "content": "[26] Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/file/ 7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf. [27] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, and et al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. [28] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https: //arxiv.org/abs/2502.02737. [29] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language, 2019. URL https://arxiv.org/abs/1911.11641. [30] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence?, 2019. URL https://arxiv.org/abs/1905.07830. [31] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL https://arxiv.org/abs/1803.05457. [32] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. LongBench: bilingual, multitask benchmark for long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31193137, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.172. URL https://aclanthology.org/2024.acl-long.172/. [33] Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, Yang Zhang, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models?, 2024. URL https://arxiv.org/abs/2404.06654. [34] Ofir Press, Noah A. Smith, and Mike Lewis. Shortformer: Better language modeling using shorter inputs. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 54935505, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.427. URL https://aclanthology.org/ 2021.acl-long.427/. [35] Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation, 2022. URL https://arxiv.org/abs/2108.12409. [36] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. [37] Jing Xiong, Liyang Fan, Hui Shen, Zunhai Su, Min Yang, Lingpeng Kong, and Ngai Wong. Dope: Denoising rotary position embedding, 2025. URL https://arxiv.org/abs/2511.09146. [38] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, Yanning Chen, and Zhipeng Wang. Liger-kernel: Efficient triton kernels for LLM training. In Championing Open-source DEvelopment in ML Workshop @ ICML25, 2025. URL https://openreview.net/forum?id=36SjAIT42G."
        },
        {
            "title": "A Implementation Details",
            "content": "The model configurations are summarized in Table 4. We adopt the open-source NSA implementation1 and use the Liger Kernel [38] for acceleration. Config Field Block Size Block Counts Hidden Size Intermediate Size Num Hidden Layers Num Attention Heads Num KV Heads Head Dim Max Position Embeddings Vocabulary Size BOS Token EOS Token RMSNorm Eps Hidden Activation Attention Bias MLP Bias Attention Dropout Initializer Range Pretraining TP Tie Word Embeddings Torch Dtype RoPE Base θ 1B Model 300M Model 16 16 2048 8192 16 32 2 64 131072 128256 128000 128001 1e-5 SiLU false false 0.0 0.02 1 true bfloat16 500,000 16 16 1024 4096 16 16 1 64 131072 128256 128000 128001 1e-5 SiLU false false 0.0 0.02 1 true bfloat16 500,000 Table 4. Model configurations for the 1B and 300M parameter models."
        },
        {
            "title": "B Gated Attention",
            "content": "From Table 5, we observe that gated attention noticeably affects performance. While the impact on FullAttn300M is relatively small, the improvement becomes substantial when scaling to 1B. This effect may partly explain the performance gains of NSA over FullAttn reported in Yuan et al. [11], suggesting that the comparison may not be entirely fair. 1https://github.com/fla-org/native-sparse-attention"
        },
        {
            "title": "SSA",
            "content": "Model Commonsense Avg. /% PPL FullAttn FullAttn-gated MoBA MoBA-gated FullAttn FullAttn-gated 300M 49.11 49.14 48.26 48.81 58.40 59.48 1B 22.89 22.49 25.40 24.51 15.97 15.62 Table 5. Performance comparison with and without gated attention. AttnSparsity PPL Commonsense Avg. /% Method KL Divergence Sparse Full Full Sparse Full Sparse SSA MoBA FullAttn 0.0656 0.0932 0. 0.658 0.519 0.590 0.711 0.498 0.604 15.19 16.88 15.18 15.88 16.69 17.18 60.22 58.58 59.48 59.87 58.60 59. Table 6. Comparison of KL divergence, attention sparsity, perplexity, and benchmark accuracy for SSA, MoBA, and FullAttn under 1B-parameter settings."
        },
        {
            "title": "C KL Divergence",
            "content": "We measure the KL Divergence and Attention sparsity for SSA, MoBA, and FullAttn with the PPL and the average scores in the commonsense reasoning benchmarks to measure the influence of the two in Table 6."
        },
        {
            "title": "D Full Longbench Results",
            "content": "Only the average scores are reported in Table 2. The full results evaluated in Longbench is in Table 7. Different Configuration for Receptive Field of 1024 There are multiple combinations of top-k and block size that produce receptive field of 1024. We evaluate two representative configurations, 1664 and 3232, under the 1B-scale setup. As shown in Table 8, larger top-k (32) generally improves retrieval performance in the NIAH tests. This is expected, since doubling the number of retrieved blocks increases accessible context, while block size is not limiting factor at these lengths. Interestingly, on other tasks, MoBA shows the opposite trend from FullAttn, NSA, and SSA: MoBA performs worse with 3232, whereas FullAttn, NSA, and SSA all benefit from this configuration. Because most methods perform better with 3232, we adopt this setting for our main experiments. Notably, SSA maintains its advantage under both configurations."
        },
        {
            "title": "SSA",
            "content": "Category Dataset Full Receptive Field = 1024 Receptive Field = 256 FA MoBA SSA FA MoBA NSA SSA FA MoBA NSA SSA Single Doc Multi Doc Summary Few-shot Synthetic Code Average NarQA Qasper MFQA HotpotQA 2WikiQA MuSiQue GovReport QMSum MultiNews TREC TriviaQA SAMSum PsgCount PsgRe-en LCC RepoBen 1.21 8.68 14.01 2.88 8.70 0.81 8.94 5.79 11.17 54.50 16.75 18.96 2.36 4.30 39.06 35. 14.58 0.43 5.59 8.64 4.19 7.83 0.68 5.66 0.65 11.37 26 17.58 11.32 0.53 36.38 25.94 2.66 8.39 17.5 6.28 10.03 4.2 14.86 17.04 13.96 46 52.29 35.36 3.08 3. 42.13 43.36 0.83 6.28 13.45 2.05 9.52 0.94 5.04 6.51 10.1 36.5 13.87 17.6 0.85 4. 39.11 39.5 7.38 7.11 12.08 7.91 9.37 1.82 11.46 7.38 13.25 40 36.36 22.12 1.80 2. 36.82 31.81 5.64 7.78 16.07 6.04 10.14 3.49 10.26 18.83 11.66 51 38.31 28.83 2.20 2. 27.31 35.24 2.26 7.45 16.57 5.98 10.07 3.9 12.89 17.22 15.55 44 46.16 34.13 2.85 3. 41.85 42.56 0.56 5.48 11.48 1.84 7.82 1 4.22 7.23 8.29 20.5 13.52 17.49 2.25 2. 36.53 33.74 7.14 6.45 13.12 6.26 10.58 4.26 8.74 15.45 8.54 24.5 30.27 20.85 2.55 4. 38.24 39.64 11.19 7.89 17.24 8.12 8.66 4.55 9.73 16.82 10.90 33 48.12 32.93 2.73 4. 34.09 37.92 2.18 7.23 15.62 6.10 9.43 3.89 12.70 16.63 16.53 39.5 44.51 32.56 3.05 3. 40.56 42.47 10.17 20.01 12.57 15.54 17. 19.15 10.91 15.07 18.01 18.56 Table 7. LongBench Evaluation across categories and datasets. Method PPL CommonSns Avg. Longbench NIAH-4k NIAH-8k NIAH-16k NIAH-32k FullAttn-1664 FullAttn-3232 MoBA-1664 MoBA-3232 NSA-1664 NSA-3232 SSA-1664 SSA-3232 15.70 15.70 16.01 15.89 15.56 15.48 15.45 15.39 59.29 59.49 58.73 58.45 59.12 59.30 60.21 60. 12.57 12.71 15.54 12.78 17.23 18.21 19.15 20.75 28.6 43.4 80.4 82.2 83.8 96.2 69.6 94.6 11.8 13.8 33.6 45.8 31.8 55.0 32.0 51.0 0.0 0.0 12.2 12.2 7.0 17.2 15.4 23.6 0.0 0.0 3 0 6.4 8.8 5.6 7.8 Table 8. Comparison of models trained with different top-k and block-size configurations, while each produce same receptive field of 1024."
        },
        {
            "title": "F Length Extrapolation",
            "content": "F.1 NSA NSA achieves better PPL than SSA in very long contexts (32k), although it performs worse on long-context understanding tasks in the Longbench evaluation. To better understand this discrepancy, we conduct ablations (Figure 5) on the NSA architecture and find that the sliding-window module is the primary component responsible for NSAs superior long-context PPL. Notably, the sliding-window module alone produces the most stable PPL curve, though at significantly higher absolute PPL values, indicating that its stability may come at the cost of reduced modeling quality."
        },
        {
            "title": "SSA",
            "content": "Figure 5. Perplexity across different context lengths for NSA architectural ablations, where CMP denotes the compression module, SEL the selection module, and SWA the sliding-window module. NSA relies on the sliding-window component (SWA) to maintain PPL stability at long context lengths, whereas SSA achieves even better PPL stability without requiring sliding-window mechanism. F.2 Attention Sink Comparing the attention score distributions of FullAttn, MoBA, and SSA (1B models and all evaluated under full-attention inference), we observe that FullAttn exhibits clear attention-sink behavior at some layers, most severe at layer 15, while both MoBA and SSA show no attention sink at the 8k context length (Figure 6). MoBA exhibits scattered high-magnitude spikes (most visible in layer 0, 1, 12, and 15), due to its poor attention sparsity extrapolation, whereas SSA maintains clean and stable distribution. When the context length increases to 16k (Figure 7) and 32k (Figure 8), FullAttn develops extremely severe global spikes, which suppress the attention on nearby tokensat 32k, the attention on local tokens becomes even lower than the largest global spike across layers 0, 9, 1115 (Figure 9). This directly explains FullAttns exploding perplexity in long-context evaluation. We further find that these spikes are data-independent (Figure 12). MoBA and SSA also develop spikes at long context, but the magnitudes are substantially smaller, allowing them to retain much better performance. Moreover, because SSA maintains higher attention sparsity (i.e., concentrates its mass more locally), its attention on local tokens remains higher than MoBAs (Figures 10 and 11), leading to its superior long-context robustness."
        },
        {
            "title": "SSA",
            "content": "Figure 6. Attention score distributions for FullAttn, MoBA, and SSA at context length of 8k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 7. Attention score distributions for FullAttn, MoBA, and SSA at context length of 16k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 8. Attention score distributions for FullAttn, MoBA, and SSA at context length of 32k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 9. Attention score distributions for FullAttn, MoBA, and SSA at context length of 32k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 10. Attention score distributions for FullAttn, MoBA, and SSA at context length of 32k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 11. Attention score distributions for FullAttn, MoBA, and SSA at context length of 32k. All sparse-attention models are evaluated using full-attention inference."
        },
        {
            "title": "SSA",
            "content": "Figure 12. Attention score distributions for FullAttn, MoBA, and SSA at context length of 32k. All sparse-attention models are evaluated using full-attention inference."
        }
    ],
    "affiliations": [
        "Kings College London",
        "Tencent Youtu Lab"
    ]
}