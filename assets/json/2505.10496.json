{
    "paper_title": "CheXGenBench: A Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs",
    "authors": [
        "Raman Dutt",
        "Pedro Sanchez",
        "Yongchen Yao",
        "Steven McDonagh",
        "Sotirios A. Tsaftaris",
        "Timothy Hospedales"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce CheXGenBench, a rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and a unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes a standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release a high-quality, synthetic dataset, SynthCheX-75K, comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish a new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 6 9 4 0 1 . 5 0 5 2 : r CheXGenBench: Unified Benchmark For Fidelity, Privacy and Utility of Synthetic Chest Radiographs Raman Dutt1, Pedro Sanchez2, Yongcheng Yao1, Steven McDonagh1 Sotirios A. Tsaftaris1, Timothy Hospedales1,3 1The University of Edinburgh 2Sinkove 3Samsung AI Center, Cambridge {raman.dutt,pedro.sanchez,yc.yao,s.mcdonagh,s.tsaftaris,t.hospedales}@ed.ac.uk"
        },
        {
            "title": "Abstract",
            "content": "We introduce CheXGenBench, rigorous and multifaceted evaluation framework for synthetic chest radiograph generation that simultaneously assesses fidelity, privacy risks, and clinical utility across state-of-the-art text-to-image generative models. Despite rapid advancements in generative AI for real-world imagery, medical domain evaluations have been hindered by methodological inconsistencies, outdated architectural comparisons, and disconnected assessment criteria that rarely address the practical clinical value of synthetic samples. CheXGenBench overcomes these limitations through standardised data partitioning and unified evaluation protocol comprising over 20 quantitative metrics that systematically analyse generation quality, potential privacy vulnerabilities, and downstream clinical applicability across 11 leading text-to-image architectures. Our results reveal critical inefficiencies in the existing evaluation protocols, particularly in assessing generative fidelity, leading to inconsistent and uninformative comparisons. Our framework establishes standardised benchmark for the medical AI community, enabling objective and reproducible comparisons while facilitating seamless integration of both existing and future generative models. Additionally, we release high-quality, synthetic dataset, SynthCheX-75K , comprising 75K radiographs generated by the top-performing model (Sana 0.6B) in our benchmark to support further research in this critical domain. Through CheXGenBench, we establish new state-of-the-art and release our framework, models, and SynthCheX-75K dataset at https://raman1121.github.io/CheXGenBench/"
        },
        {
            "title": "Introduction",
            "content": "Recent advances in multi-modal generative models, particularly Text-to-Image (T2I) systems [1, 2, 3, 4], have demonstrated remarkable capabilities in producing high-fidelity synthetic images that closely adhere to natural-language prompts. Central to this progress is the development of comprehensive, well-designed benchmarks that evaluate various aspects of generative performance. These benchmarks drive innovation by establishing standardised evaluation protocols and creating an equitable foundation for model comparison. The natural imaging domain has benefited from numerous such benchmarks, each meticulously assessing specific aspects and identifying limitations that researchers subsequently address in developing next-generation models. For example, MSCOCO dataset [5] has been established as seminal benchmark for evaluating general performance across multiple tasks, with particular emphasis on text-guided image generation. Building upon this foundation, more specialised benchmarks have emerged to assess specific attributes such as compositional understanding [6, 7, 8], enabling more nuanced analysis of model capabilities. Despite the advancements in the natural imaging domain, there remains significant gap in medical image analysis, and specifically in terms of benchmarking specialised tasks such as text-to-image generation. Preprint. Under review. Figure 1: Figure illustrating the overall schematic of the CheXGenBench benchmark for evaluating text-to-image models in synthetic radiograph generation. CheXGenBench organises evaluation into three dimensions: Fidelity (measuring generative quality and mode coverage), Privacy (assessing memorisation and re-identification risks), and Utility (evaluating practical value of synthetic samples through image classification and radiology report generation metrics) through 20+ metrics across 11 widely-adopted T2I models. Benchmarking Medical Imaging AI: Medical image analysis has made significant strides by benefiting from rapid advancements in artificial intelligence; however, its progress faces substantial constraints due to what has be characterised as benchmarking crisis [9]. The ultimate goal of AI applications in medicine remains the development of intelligent systems capable of supporting and potentially transforming clinical decision-making processes. Progress toward this objective is frequently hindered by insufficient transparency in training and evaluation protocols, coupled with fragmented assessment criteria. Consequently, claims of state-of-the-art performance are often premature, non-reproducible, or reflect narrow contextual improvements rather than demonstrating genuine translational capability to clinical practice. The challenges of benchmarking in medical AI diagnostic tasks such as classification, localisation, segmentation and report generation have been discussed, and some improvements have been proposed [10, 11, 12, 13]. However, despite opportunities for impactful societal applications from diagnostic training to rare condition simulation, benchmarking in medical image generation remains in an even more nascent and unsatisfactory state. This is exacerbated by unique challenges such as unclear evalution metrics, data scarcity due to privacy constraints, and long-tailed distributions of rare pathologies. While medical AI diagnostic tasks have been long studied, the generation of synthetic data through text prompts (medical descriptions) has recently emerged as critical research focus. Besides being an interesting academic measure of medical AI capability, the ultimate motivation is that progress in diagnostic tasks is usually bottlenecked by data scarcity in the medical domain. High-fidelity generative models for synthetic data offer the promise to alleviate this bottleneck and ultimately facilitate clinical impact [14]. Chest radiographs are the most commonly used frontline modality in medical imaging. Although the majority of collected clinical data remain inaccessible behind institutional firewalls and compliances [9], several data repositories have been opened [15, 16, 10, 17], facilitating the development of generative models capable of synthesizing radiographs with potential relevance to downstream clinical utility [18, 19, 20]. Current research on Text-to-Image generation of radiographs can be broadly categorized into two primary streams: (1) studies prioritizing the enhancement of image fidelity and generative performance [21, 22, 20, 23, 19, 24, 25, 26], and (2) investigations focusing on the mitigation of privacy concerns and patient re-identification risks [27, 28, 29, 30, 31] that could undermine synthetic data utility. Despite constituting an active area of research with significant contributions, we have identified several critical benchmarking dimensions in which existing studies demonstrate consistent limitations. Minimal or Absent Comparative Baselines: Several notable studies either include minimal (self2 proposed) [23, 30] or no baselines [22] for evaluation. [24, 20] limit their comparative evaluations to only two competing methodologies. Reporting Inadequate Metrics: Studies throughout the existing literature predominantly report Fr√©chet Inception Distance (FID) [32], specifically adopting in-domain image encoders based on primitive architectures [33]. Furthermore, no studies adequately report metrics characterising mode-coverage, criterion of paramount importance in synthetic image generation for capturing the diversity of the underlying data distribution. Finally and most importantly, all metrics are reported as micro-averages across entire datasets without accounting for conditioning, let alone the long-tailed nature of medical data distributions. For example, excellent average generation fidelity could be reflective of the ability to generate more common images of healthy individuals, and the inability to generate images with pathologies of interest. This situation would offer limited clinical utility for pathology diagnosis. Restriction to Early-Stage Architectures: Existing studies [19, 22, 23, 34] have predominantly confined themselves to outdated T2I model architectures [35], failing to address the crucial question of how recent advancements in generative modelling [36, 4, 37] from the natural imaging domain translate to the specialised requirements of medical imaging contexts. Lack of Unified Evaluation: Existing studies are fragmented between evaluating generative fidelity [19, 22, 20] or privacy and re-identification risks [27, 29, 31], failing to conduct and provide unified evaluation of the two key aspects of synthetic radiograph generation research. Limited Evaluation on Synthetic Data Utility: Most studies fail to comment on the downstream utility of synthetic radiographs [22, 20] often presenting generation results without rigorously assessing their potential impact on medical image analysis tasks such as classification, segmentation, or diagnostic reasoning. To address these critical limitations, we introduce CheXGenBench, comprehensive benchmark designed for rigorous evaluation of frontier generative models across diverse spectrum of metrics encompassing: (1) generation fidelity and mode coverage, (2) privacy and re-identification risk assessment, and (3) downstream clinical utility through an extensive suite of 20+ quantitative and interpretable metrics. We establish standardised training and evaluation protocols to enable equitable comparison between diverse model architectures. CheXGenBench prioritises usability and adaptability, facilitating seamless plug-and-play integration of both existing and emerging generative frameworks. Through systematic evaluation, we present several T2I models previously unexplored for radiograph generation and establish new state-of-the-art (SoTA) performance. Furthermore, we release synthetic dataset, SynthCheX-75K , comprising 75K high-quality radiographs generated by our benchmark-leading model to facilitate advancement in medical image analysis research."
        },
        {
            "title": "2 CheXGenBench Design",
            "content": "CheXGenBench is designed to evaluate each text-to-image model across three dimensions comprehensively: (1) generative fidelity and mode coverage (Sec 2.2), (2) privacy and patient re-identification risks (Sec 2.2), and (3) synthetic data utility for downstream tasks (Sec 2.4), which we elucidate in detail in the following sub-sections. Design Principles: CheXGenBench is designed for usability, featuring decoupled training and evaluation pipelines. This allows researchers to use preferred training frameworks (Hugging Face Diffusers [38], ai-toolkit1, etc.) while adhering to standardised evaluation. Users only need to provide synthetic images and metadata file for the benchmark for automatic assessment using over 20 metrics. Pre-defined data splits further simplify adoption, balancing comprehensive assessment with practical use for medical imaging research. 2.1 Training Dataset and Protocol Criteria for Model Inclusion: Our model selection was guided by two main criteria. First, we included models previously used for synthetic radiograph generation in existing literature [35, 19, 20, 39] to provide continuity and comparability with prior work. Second, we incorporated recent state-of-the-art models (both diffusion and auto-regressive) [36, 40, 4, 41, 37] from the natural imaging domain that had not yet been evaluated for chest X-ray synthesis. Thus we both benchmark established methods and assess the potential of newer architectures for medical image generation. 1https://github.com/ostris/ai-toolkit Training Dataset with Enhanced Captions: We conduct all evaluations on the MIMIC-CXR dataset [15], which has emerged as the de-facto data repository for chest radiograph text-to-image generation [19, 22, 39, 20, 23]. While each CXR image in the dataset is associated with corresponding free-text radiology report, conventional approaches have relied on rule-based methods to derive abbreviated descriptions for T2I model training [39, 19]. These traditional approaches, however, prove inadequate due to inconsistencies in report structure and clinical terminology [42]. Recent advances have shifted toward deep learning-based techniques that generate more comprehensive and clinically accurate summaries from radiology reports, contributing to state-of-the-art medical foundation models [42]. Building on evidence that more descriptive captions enhance generative fidelity [43], we adopt the LLaVA-Rad annotations in place of the traditional captions available in MIMIC-CXR. Fig. 2 illustrates the distribution of character lengths in the two annotations. To our knowledge, this represents the first study to use enhanced descriptive captions for the generation of synthetic radiographs with MIMIC-CXR data. In Appendix Sec. A, we empirically show that LLaVA-Rad annotations improve fidelity and reduce re-identification risks, and strongly recommend for future research in this domain. Training Protocol: Our analysis revealed that prior studies employed inconsistent training budgets, undermining valid cross-model comparison. To establish level evaluation framework, we implemented standardised training protocol across all T2I models. Each architecture was trained for precisely 20 epochs on an identical training split of 237,388 samples annotated with LLaVA-Rad annotations. We release the training and evaluation data splits along with the benchmark. The models in our benchmark were stratified into two categories based on parameter count: (1) models with fewer than 1B parameters, and (2) models exceeding 1B parameters. For the smaller models, we employed full fine-tuning (FFT) of all parameters. For larger architectures, we implemented Low-Rank Adaptation (LoRA) [44] with rank 32 on standard query, key, and value layers in the attention blocks to address both computational constraints and reflect realistic training scenarios. 2.2 Evaluation Protocol: Generative Fidelity, Mode Coverage, and Conditional Analysis Limitations of Current Fidelity Assessment: The Fr√©chet Inception Distance (FID) score [32] is widely used for evaluating synthetic radiograph fidelity. Standard FID uses InceptionV3 [45] features trained on natural images, creating domain mismatch for medical images [46]. While some research employs domain-specific encoders like DenseNet-121 [47] trained on radiographs [33], we argue these adaptations remain inadequate. DenseNet-121, despite domain alignment, represents an outdated backbone that fails to capture critical nuances, reducing FID reliability for radiograph quality assessment. We perform an extensive ablation on this in Appendix Section B. Additionally, studies using natural image-text pre-trained CLIP [48, 19] further compromise evaluation integrity due to high domain misalignment. Improving metric Reliability in CheXGenBench : We address the aforementioned limitations and enable more nuanced evaluation of synthetic radiographs in CheXGenBench . For robust image fidelity assessment using Fr√©chet Inception Distance (FID), CheXGenBench employs features from RadDino [49], model with SoTA performance on radiology classification and report generation tasks. To evaluate image-text alignment, we integrate BioViL-T [50], vision-language model specialized in the biomedical domain. Expanding the Metric Suite with Density and Mode Coverage: All prior studies have reported generation fidelity without systematic evaluation of how effectively generated samples capture critical distributional characteristics, notably the density of the resulting sample distribution and coverage of distinct modes from the true data. This is of particular importance in medical datasets since not all pathologies are distributed equally. To address this, CheXGenBench also supports Precision, Recall, Density and Coverage [51]. Precision, Recall, Density, and Coverage (PRDC) metrics are essential for evaluating mode coverage, particularly in long-tailed distributions where FID scores can be skewed by majority classes. This is evident in the MIMIC-CXR dataset, where No Finding radiographs, representing healthy X-rays, predominate despite pathological images offering greater clinical value for synthesis. Within the PRDC framework, Precision quantifies generated sample realism, Recall measures how well the real distribution is captured, Density assesses feature space concentration, and Coverage determines the 4 proportion of modes of real data successfully generated, together providing more comprehensive assessment than global metrics alone. Conditional Analysis for Individual Pathologies: CheXGenBench extends evaluation capabilities through pathology-specific conditional analysis, wherein we compute each generation fidelity metric independently across individual pathologies in the MIMIC-CXR dataset. This granular assessment approach provides critical insights that enable developers to precisely evaluate generative performance for specific medical conditions. Such an analysis becomes particularly valuable in scenarios requiring selective augmentation of underrepresented pathologies through generative model, which is the most desired use case. Our framework calculates FID, KID, image-text alignment, and PRDC metrics for each distinct pathology, facilitating comprehensive performance evaluation at the condition-specific level. To the best of our knowledge, we are the first study to include pathology-conditional evaluation. 2.3 Evaluation Protocol: Privacy and Patient Re-Identification Deep generative models can inadvertently memorise distinctive training examples, allowing an attacker to reverseengineer sensitive patient information from seemingly synthetic images [52, 53]. In the medical domain, even coarse anatomical cues may be enough to link generated radiograph back to an individual, breaching data-protection regulations such as HIPAA2 and the EU GDPR3. Consequently, to assess clinical relevance claims, benchmark that must characterise (i) how much model memorises, and (ii) how easily real patient can be re-identified from its outputs. Re-identification risk formulation. To evaluate privacy and patient re-identification risks, we implement established metrics from the existing literature [27, 29, 30, 31]. Let Dreal = {xi, ci}N i=1 be the training set of chest radiographs xi with corresponding captions ci, and let GŒ∏ denote text-to-image model producing synthetic images ÀÜx = GŒ∏(c). We assess whether generated sample ÀÜx memorizes any training image xi via learned similarity function ‚Ñì(ÀÜx, xi) with Memorised(xi) ‚Ñì(ÀÜx, xi) > Œ¥, where Œ¥ is fixed safety margin. We evaluate similarity through three distinct lenses: (ii) Latent distance (‚Ñìlat): (i) Pixel distance (‚Ñìpix): ÀÜx xi2, to detect near-duplicates [52]. Normalized Euclidean distance in the embedding space of RadDino [49]. (iii) Re-identification score (sreid): Probability that ÀÜx and xi are from the same patient, as estimated by Siamese neural network [54]. Assessing re-identification. Instead of relying solely on pixel-based [52] or structural-based [55] similarity, we adopt the deep learning-based metric ‚Ñì = re-id is Siamese network with ResNet-50 [56] backbone trained to classify whether two chest X-ray images originate from the same patient. For any pair (ÀÜx, x) of generated and real image, re-id outputs reidentification score sreid [0, 1] after sigmoid layer. synthetic image ÀÜx is considered re-identified if sreid Œ¥ for any training image x. [54]. The model re-id Œ∏ Œ∏ Œ∏ Privacy metrics summary. Given dataset of generated images {ÀÜx(j)}M j=1 (across different random seeds), let s(j) pix denote, respectively, the Re-ID score, latent-space distance, and pixel-space distance of sample to its closest training image. We report the following datasetlevel statistics: lat , and ‚Ñì(j) reid, ‚Ñì(j) Avg. Re-ID Score () : sreid = Avg. Pixel Distance () : ‚Ñìpix = 1 (cid:88) j=1 s(j) reid, 1 M (cid:88) j=1 ‚Ñì(j) pix , Avg. Latent Distance () : ‚Ñìlat = 1 (cid:88) j=1 ‚Ñì(j) lat , Max. Re-ID Score () : smax reid = max 1jM s(j) reid, Count[sreid > Œ¥] () : CŒ¥ = (cid:88) j=1 1(cid:2)s(j) reid > Œ¥(cid:3). 2https://www.hhs.gov/hipaa/for-professionals/privacy/index.htm 3http://gdpr.eu/what-is-gdpr/ Table 1: Table comparing the results for generative fidelity for 11 different T2I models in the benchmark. The best result for each metric is indicated with bold, while the second-best result is underlined. The overall best performing model (See Appendix Tab. 8) is highlighted in green . Model SD V1-4 [35] SD V1-5 [35] SD V2 [35] SD V2-1 [35] RadEdit [39] Pixart Sigma [36] Sana [4] SD V3.5 Medium [40] Lumina 2.0 [41] Flux.1-Dev [37] LLM-CXR [20] Size 0.86B 0.86B 0.86B 0.86B 0.86B 0.60B 0.60B 2.50B 2.60B 12B 12B Default Resolution Prev. Available For CXR? Fine-Tuning FID (RadDino) KID (RadDino) Alignment Score Precision Recall Density Coverage 512 512 512 512 512 512 1024 1024 1024 256 FFT FFT FFT FFT N/A FFT FFT LoRA(r=32) LoRA(r=32) LoRA(r=32) N/A 125.186 118.932 194.724 186.530 69.695 60.154 54.225 91.302 101.198 122.400 71.243 0.172 0.147 0.376 0. 0.033 0.023 0.016 0.103 0.110 0.144 0.061 0.357 0.326 0.311 0.197 0.677 0.697 0.695 0.044 0.121 0.036 0.319 0.488 0.536 0.480 0. 0.397 0.666 0.674 0.632 0.574 0.420 0.782 0.301 0.473 0.086 0.049 0.544 0.522 0.614 0.205 0.014 0.008 0.041 0.236 0.242 0.166 0. 0.150 0.506 0.520 0.401 0.256 0.125 0.671 0.217 0.256 0.057 0.038 0.285 0.506 0.548 0.244 0.170 0.326 0.459 2.4 Evaluation Protocol: Synthetic Data Utility for Downstream Tasks significant application of synthetic data in radiology lies in enhancing downstream model performance, potentially circumventing the stringent sharing restrictions imposed on medical datasets. For our utility assessment framework, we have strategically selected two prevalent downstream tasks previously established in radiological evaluation [19]: (1) Image Classification and (2) Radiology Report Generation (RRG). These tasks were deliberately chosen for their distinct complexities. Image classification serves as unimodal assessment, directly evaluating the intrinsic quality of synthetic radiographs, while RRG functions as more demanding multimodal evaluation, assessing the factual correctness between synthetic images and their corresponding clinical descriptions. Downstream Image Classification: We adopt the experimental setting previously utilised [19] and measure classification performance when classifier [56] is trained exclusively on synthetic data (20K samples) (Dsyn) (for 20 epochs). This provides us with an idea of the stand-alone clinical value of the synthetic data from generative model. The performance metrics are calculated on held-out real test set (Dtest) to ensure clinical relevance and generalizability of our findings. We quantify classification performance through standard accuracy, F1-Score, and AUROC metrics. Downstream Report Generation: We choose an existing multimodal model LLaVA-Rad [42] with SoTA radiograph understanding abilities and continue to fine-tune it with 20,000 synthetic samples. The performance is reported on real test set (Dtest). We quantify RRG performance using the standard metrics adopted in literature [19, 42]: BLEU [57], ROUGE-L [58], F1-RadGraph [59], and F1-Score. BLEU-1 and BLEU-4 measure word and short phrase overlap with reference reports to gauge basic fluency and word choice. ROUGE-L focuses on sentence-level structure and recall of important, potentially non-contiguous, phrases. F1-RadGraph evaluates for factual correctness, while F1-Scores (F1-5 and F1-14) assess the accuracy of identifying the presence or absence of predefined set of 5 or 14 specific radiological findings within the generated report."
        },
        {
            "title": "3 Experiments and Results",
            "content": "Training Setting: Our evaluation used various T2I models with different training approaches. We used existing radiograph generation models (RadEdit [39], LLM-CXR [20]) unmodified. For models under 1B parameters, we performed full fine-tuning (FFT), while larger models used ParameterEfficient Fine-Tuning (PEFT) with LoRA [44] (rank 32) on query, key, and value layers. StableDiffusion variants utilised Huggingface Diffusers [38], while Sana and Pixart-Sigma were trained using their official repositories. Larger models employed the ai-toolkit package4. Training followed officially recommended hyperparameters with consistent total batch size of 128 across 4 Nvidia H200 GPUs, using 237,388 training samples and 5,034 test samples. Downstream evaluation experiments were conducted on Nvidia A100 GPUs. 3.1 Fidelity of Synthetic Generations Results on Global Assessment 4https://github.com/ostris/ai-toolkit 6 Table 2: Comparison of FID (RadDino) scores () across individual pathologies in the MIMICCXR dataset. Lower values indicate superior performance, with the best results for each pathology highlighted in bold. The most challenging pathology (highest average FID across models) is highlighted in red , while the best-performing model overall is highlighted in green . We also highlight the best performing pathology for each model in blue . Model Atelectasis Cardiomegaly Consolid. Edema Fracture LO EC LL NF PE PO PN PT SD SD V1-4 SD V1-5 SD V2 SD V2-1 RadEdit Pixart Sigma Sana SD V3.5 Medium Lumina 2.0 Flux.1-Dev LLM-CXR 134.11 125.67 188.72 179.20 63.38 59.27 51.03 94.94 109.39 137.10 71. 131.04 124.75 193.91 181.79 62.79 60.39 54.68 94.84 111.11 133.60 71.37 184.30 181.25 241.24 228.43 136.59 133.96 127.46 149.05 162.36 176.76 136. 144.84 139.94 214.40 193.62 76.94 73.93 67.84 111.94 131.18 152.91 83.18 217.75 213.94 214.40 242.65 155.97 155.53 147.00 168.48 182.35 191.48 148. 238.78 243.17 253.91 263.01 197.58 179.44 172.32 184.75 191.83 191.02 168.50 225.99 123.13 268.28 260.15 184.11 174.63 163.14 173.37 182.22 194.97 163. 129.38 255.64 280.11 185.00 61.90 56.83 49.23 86.72 99.53 133.37 66.93 106.34 167.81 193.99 192.30 67.88 48.74 44.60 89.60 95.66 100.58 64. 128.16 193.75 299.48 178.84 60.60 59.05 49.80 91.92 105.25 137.66 67.83 255.84 243.17 223.43 287.27 215.92 210.90 199.45 203.62 213.50 221.23 200. 163.82 101.08 250.96 213.26 114.66 108.42 99.52 124.07 134.58 156.59 108.04 212.48 119.91 183.34 242.60 151.34 150.55 141.99 163.27 165.09 190.93 147. 135.10 123.64 193.99 176.99 53.10 51.61 46.51 86.99 102.78 127.03 67.54 The results are presented in Tab. 1, where we showcase both fidelity and mode coverage metrics. Sana [4] delivers superior overall performance across key metrics, achieving the lowest FID and KID scores, indicating exceptional generation fidelity, while simultaneously attaining the highest Recall and Coverage, demonstrating its capacity to capture diverse modes (distributions) throughout the dataset. Pixart Sigma [36] emerges as strong contender, exhibiting the highest image-text alignment alongside second-best FID, KID, and Coverage scores. LLM-CXR [20] exhibits specialised capabilities, substantially outperforming all other models in Precision; however, its notably low Recall suggests limited generative scope, primarily effective for specific distributions (pathologies). This also highlights that conventional fidelity metrics like FID do not present complete picture of the model performance. Earlier Stable-Diffusion variants (SD V-1.x, V2-x) demonstrate consistently suboptimal scores across all metrics despite full fine-tuning, particularly significant finding given their prevalent adoption in the synthetic radiograph generation literature [19, 34, 27, 23]. Larger architectural models (SD V3-5 [40], Lumina 2.0 [41], Flux.1-Dev [37]), with the exception of LLMCXR, yield predominantly inferior performance across evaluation metrics. SD V3-5 exhibits high Precision but low Recall, Density, and Coverage scores, mirroring trends observed in LLM-CXR. We hypothesise that this stems from the inability of LoRA to provide sufficient adaptation for the medical domain, limitation previously observed in [23]. Performance improvements might be achievable by extending LoRA to other linear layers beyond attention (Q,K,V) layers, however, we leave this exploration to future work. Overall, Sana achieves the optimal performance-efficiency trade-off among all evaluated models. Notably, Sana has been adapted for synthetic radiograph generation for the first time through this work. Results on Conditional Assessment Results for conditional analysis on individual pathologies are presented in Tab. 2. Consistent with trends observed in Tab. 1, Sana demonstrates superior performance, achieving the lowest FID scores across 12 of the 14 pathology categories. This indicates Sanas robust capability to generate highfidelity radiographs across diverse pathological conditions. Pixart Sigma maintains its position as the second-best performing model, while RadEdit frequently secures the third-best scores across multiple categories. LLM-CXR demonstrates competitive performance for specific pathologies, notably achieving strong results for Edema (83.18) and No Finding (64.62), frequently outperforming both earlier Stable Diffusion models and certain large-scale models (SD V3.5, Lumina, Flux.1-Dev). Concerning Observations: This analysis reveals concerning patterns. Substantial performance variation exists across pathologies for each model, regardless of overall performance. For instance, Sana exhibits FID scores ranging from 44.60 for No Finding (NF) to 199.45 for Pleural Other (PO). Notably, five of the eleven models achieve optimal performance on No Finding cases, which represent healthy radiographs with limited clinical utility from synthetic X-rays, while all models consistently perform poorly on Pleural Other (PO) pathology. In Appendix and Tab. 10, we empirically demonstrate that model performance strongly correlates with pathology prevalence in the training dataset (correlation coefficient: 0.947), suggesting that current models primarily 1Note: EC = Enlarged Cardiomediastinum, LL = Lung Lesion, LO = Lung Opacity, NF = No Finding, PE = Pleural Effusion, PO = Pleural Other, PN = Pneumonia, PT = Pneumothorax, SD = Support Devices. 7 Table 3: Results on Re-Identification Risk and Patient Privacy Metrics. We present the average scores across 2000 samples and individual scores with maximum privacy risks. SD V2 Pixart-Œ£ SD V3-5 Lum. 2.0 SD V2-1 RadEdit LLM-CXR SD V1-5 SD V1-4 Model Sana Flux Avg. Re-ID Score () Avg. Latent Distance () Avg. Pixel Distance () 0.539 0.592 143.441 Max. Re-ID Score () Count Re-ID > Œ¥ () 0.996 434 0.572 0.583 143.634 0.996 0.533 0.588 143.936 0.996 454 0.503 0.592 145.652 0.997 392 0.481 0.560 145.272 0.992 0.551 0.540 162.901 0.996 442 0.548 0.561 159.864 0.994 442 0.365 0.601 147.098 0.997 0.513 0.591 145.612 0.993 223 0.404 0.595 155.926 0.992 196 0.537 0.557 149.626 0.994 reflect dataset distribution characteristics rather than achieving balanced clinical utility. We hope this analysis encourages model developers to incorporate training strategies tailored for long-tailed distributions [60]. 3.2 Results on Privacy and Re-Identification Risks , xtxt , . . . , ÀÜximg,N Experimental Setting: To quantify re-identification risks, we select subset of 2000 image-text pairs (ximg ) from the training set and generate (= 10) synthetic samples , ÀÜximg,2 ÀÜximg,1 using 10 different random seeds for each training prompt. Next, we , ÀÜximg,n calculate Re-ID scores, Pixel and Latent Distances between each real-synthetic pair (ximg ) for all {1, 2, . . . , }. Finally, we report the maximum Re-ID score maxj s(j) reid, minimum pixel distance minj ‚Ñì(j) lat across generations for each sample. This approach enables us to identify the greatest privacy risk posed for each training sample across multiple generations. pix and minimum latent distance minj ‚Ñì(j) The privacy risk assessment results are detailed in Tab. 3. Most models exhibit Average Re-ID scores within comparable range, with SD V3-5 notably achieving the lowest (most favorable) score. For latent and pixel distances, similar pattern emerges, where SD V3-5 and Sana demonstrate superior performance (i.e., lower average distances), respectively. Concerning Observations: We conducted detailed analysis of individual scores across 2,000 samples, with particular attention to two key metrics: (1) the maximum Re-ID score, which represents the highest potential for re-identification, and (2) the frequency of samples exceeding high-risk threshold (Œ¥ = 0.85). Our results reveal that all models, regardless of their fidelity performance, generate samples that can be re-identified with high confidence. The proportion of samples presenting significant reidentification risk remains substantial across all models, ranging from 10% to 25%. Notably, models trained with LoRA demonstrate relatively lower incidence of high-risk samples, potentially due to their reduced capacity for memorization [30]. These findings underscore critical insight: generative models pose substantial privacy risks irrespective of their generative capabilities. 3.3 Utility of Synthetic Samples for Downstream Tasks Downstream Task: Image Classification Results: We present the results in Tab. 4. Sana emerges as exceptionally effective, with its synthetic images enabling classifiers to match or exceed the original data baseline on an impressive 10 out of 13 pathologies. Interestingly, it surpasses the baseline on Fracture, an underrepresented class in MIMIC-CXR. Other models, such as RadEdit, Pixart-Sigma, and LLM-CXR, show limited success by matching the baseline for at most two pathologies, failing to surpass it. Models like SD V1-4, SD V3-5, Lumina 2.0, and Flux.1-Dev generally produce synthetic data that leads to classifiers significantly underperforming the Original Data baseline across most or all pathologies. Viability: The results from Sana strongly suggest that high-quality synthetic data can, in some cases, be viable standalone replacement for real data for training medical image classifiers. This is powerful finding with implications for data privacy, scarcity, and augmentation. In Appendix C, we expand on the correlation between generative fidelity and downstream utility for classification. Downstream Task: Radiology Report Generation The results are presented in Tab. 5. Firstly, we observe that additional fine-tuning with synthetic data, irrespective of the model, leads to performance degradation as compared to the original baseline (trained with real data). In terms of the models, RadEdit and Sana emerge as leading performers. 8 Table 4: Performance Comparison (AUC ) of ResNet50 classifier trained only on synthetic data vs. Original (Real) Data Baseline for all pathologies in the MIMIC dataset. Results matching or exceeding the Original Data baseline are bolded and within 0.01 AUC are underlined. Model Original (Real) SD V1-4 SD V1-5 SD V2 SD V2-1 RadEdit Pixart Sigma Sana SD V3-5 Lumina 2.0 Flux.1-Dev LLM-CXR Atelectasis Cardiomegaly Consolidation Edema EC Fracture LL LO NF PE PO PN PT SD 0.75 0.70 0.72 0.66 0.63 0.73 0.74 0.74 0.55 0.46 0.41 0. 0.76 0.70 0.72 0.69 0.67 0.73 0.73 0.76 0.55 0.48 0.41 0.69 0.72 0.67 0.69 0.66 0.65 0.72 0.70 0.72 0.56 0.52 0.44 0.70 0.85 0.81 0.81 0.78 0.71 0.84 0.84 0.85 0.55 0.51 0.40 0.81 0.61 0.56 0.60 0.61 0.55 0.61 0.61 0.61 0.47 0.46 0.44 0.61 0.58 0.57 0.53 0.53 0.59 0.56 0.58 0.62 0.47 0.57 0.52 0.57 0.63 0.63 0.66 0.55 0.62 0.60 0.61 0.63 0.47 0.53 0.48 0. 0.70 0.67 0.67 0.63 0.62 0.69 0.69 0.70 0.53 0.52 0.42 0.65 0.84 0.80 0.82 0.75 0.75 0.81 0.83 0.83 0.60 0.59 0.40 0.80 0.84 0.77 0.79 0.76 0.74 0.82 0.81 0.84 0.54 0.55 0.38 0.77 0.74 0.65 0.68 0.50 0.57 0.72 0.68 0.73 0.58 0.57 0.50 0.66 0.67 0.60 0.62 0.61 0.56 0.66 0.63 0.64 0.49 0.49 0.48 0.61 0.71 0.65 0.70 0.64 0.61 0.66 0.70 0.72 0.55 0.50 0.44 0. 0.83 0.80 0.83 0.78 0.75 0.77 0.80 0.83 0.71 0.71 0.67 0.73 Table 5: Radiology Report Generation (RRG) performance metrics for various generative models. SD V2-1 RadEdit Pixart-Œ£ SD V3-5 Lumina 2.0 Flux.1-Dev LLM-CXR Metric Original SD V1-4 SD V1-5 SD Sana BLEU-1 () BLEU-4 () ROUGE-L () F1-RadGraph () Micro F1-5 () Micro F1-14 () 38.16 15.38 0.31 0.29 0.57 0.57 25.85 6.76 0.23 0.21 0.56 0.53 26.02 7.50 0.24 0.24 0.54 0.51 26.62 7.35 0.24 0.23 0.57 0. 26.76 7.38 0.24 0.22 0.57 0.53 30.55 8.36 0.24 0.24 0.55 0.55 31.25 7.91 0.23 0.22 0.50 0.52 31.11 7.70 0.24 0.23 0.57 0.55 23.49 4.91 0.20 0.17 0.31 0.35 17.96 4.27 0.20 0.18 0.41 0. 18.19 3.36 0.18 0.14 0.32 0.36 29.78 7.93 0.23 0.21 0.55 0.53 RadEdit excels in BLEU-4 (fluency) and is top contender in F1-RadGraph (semantic accuracy of clinical entities) and Micro F1-14 (specific findings detection), while Sana demonstrates strengths in ROUGE-L (sentence structure) and Micro F1 scores for identifying specific findings. LLM-CXR also gives strong performance, often giving secondor third-best scores. Pixart Sigma shows the best individual word usage (BLEU-1), and models like SD V2 also perform well in Micro F1 scores. Overall, no single model dominates in all metrics. These results reflect that current T2I models might show high fidelity, but their utility for multimodal tasks such as report generation is still limited under the current setting. Potentially, generations with stronger image-text alignment or training VQA models on collection of real and synthetic data from scratch can alleviate this."
        },
        {
            "title": "4 The SynthCheX-75K Dataset",
            "content": "We introduce SynthCheX-75K , comprehensive synthetic dataset for medical image analysis research. We generated this dataset using Sana [4], our benchmarks top-performing model, after additional fine-tuning from 20 to 50 epochs. Analysis in Appendix Fig. 4 shows that extended fine-tuning yields modest FID improvements and more significant Recall improvements across all pathologies. Using highly-capable medical VLM, HealthGPT [61], we first filter out all the lowquality generations (0.83%), and include only images with high (39%) or medium quality (60.16%). The dataset is publicly available here. Data filtration is further discussed in Appendix F."
        },
        {
            "title": "5 Conclusion",
            "content": "We identified critical gaps in synthetic radiograph generation research and introduced CheXGenBench to assess generation fidelity, privacy preservation, re-identification risk, and downstream utility in single framework. Our findings reveal that: (1) models, even SoTA, struggle with long-tailed medical data distributions significantly limiting their potential utility, (2) models, even with poor fidelity, pose high privacy risks, and (3) synthetic data offers limited utility in downstream multimodal tasks. These limitations highlight substantial opportunities for improvement in future research. With our benchmark-leading Sana (0.6B) model, we produced SynthCheX-75K , high-quality synthetic radiograph dataset. We have open-sourced our benchmark, model checkpoints, and dataset to provide standard evaluation protocol for comprehensive assessment of medical image generation models and expect the benchmark to grow with new generative models and paradigms."
        },
        {
            "title": "References",
            "content": "[1] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. [2] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [3] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [4] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In The Thirteenth International Conference on Learning Representations, 2025. [5] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [6] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36:7872378747, 2023. [7] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. [8] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [9] Faisal Mahmood. benchmarking crisis in biomedical machine learning. Nature Medicine, pages 11, 2025. [10] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund, Behzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: large chest radiograph dataset with uncertainty labels and expert comparison. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 590597, 2019. [11] Yongshuo Zong, Yongxin Yang, and Timothy Hospedales. Medfair: Benchmarking fairness for medical imaging. In ICLR, 2023. [12] Alexandros Karargyris, Renato Umeton, Micah Sheller, Alejandro Aristizabal, Johnu George, Anna Wuest, Sarthak Pati, Hasan Kassem, Maximilian Zenk, Ujjwal Baid, et al. Federated benchmarking of medical artificial intelligence with medperf. Nature Machine Intelligence, 5(7):799810, 2023. [13] Xiaoman Zhang, Hong-Yu Zhou, Xiaoli Yang, Oishi Banerjee, Juli√°n Acosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. Rexrank: public leaderboard for ai-powered radiology report generation. arXiv preprint arXiv:2411.15122, 2024. [14] Mauro Giuffr√® and Dennis Shung. Harnessing the power of synthetic data in healthcare: innovation, application, and privacy. NPJ digital medicine, 6(1):186, 2023. [15] Alistair EW Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger Mark. Mimic-iii, freely accessible critical care database. Scientific data, 3(1):19, 2016. 10 [16] Alistair EW Johnson, Tom Pollard, Nathaniel Greenbaum, Matthew Lungren, Chihying Deng, Yifan Peng, Zhiyong Lu, Roger Mark, Seth Berkowitz, and Steven Horng. Mimic-cxr-jpg, large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019. [17] Xiaoman Zhang, Juli√°n Acosta, Josh Miller, Ouwen Huang, and Pranav Rajpurkar. Rexgradient-160k: large-scale publicly available dataset of chest radiographs with freetext reports. arXiv preprint arXiv:2505.00228, 2025. [18] Sahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena, Robert Stanforth, Jamie Hayes, Soham De, Samuel Smith, Olivia Wiles, and Borja Balle. Differentially private diffusion models generate useful synthetic images. arXiv preprint arXiv:2302.13861, 2023. [19] Christian Bluethgen, Pierre Chambon, Jean-Benoit Delbrouck, Rogier van der Sluijs, Ma≈Çgorzata Po≈Çacin, Juan Manuel Zambrano Chaves, Tanishq Mathew Abraham, Shivanshu Purohit, Curtis Langlotz, and Akshay Chaudhari. visionlanguage foundation model for the generation of realistic chest x-ray images. Nature Biomedical Engineering, pages 113, 2024. [20] Suhyeon Lee, Won Jun Kim, Jinho Chang, and Jong Chul Ye. LLM-CXR: Instruction-finetuned LLM for CXR image understanding and generation. In The Twelfth International Conference on Learning Representations, 2024. [21] Hyungyung Lee, Da Young Lee, Wonjae Kim, Jin-Hwa Kim, Tackeun Kim, Jihang Kim, Leonard Sunwoo, and Edward Choi. Vision-language generative model for view-specific chest x-ray generation. arXiv preprint arXiv:2302.12172, 2023. [22] Tobias Weber, Michael Ingrisch, Bernd Bischl, and David R√ºgamer. Cascaded latent diffusion models for high-resolution chest x-ray synthesis. In Pacific-Asia conference on knowledge discovery and data mining, pages 180191. Springer, 2023. [23] Raman Dutt, Linus Ericsson, Pedro Sanchez, Sotirios A. Tsaftaris, and Timothy Hospedales. Parameter-efficient fine-tuning for medical image analysis: The missed opportunity. In Medical Imaging with Deep Learning, 2024. [24] Woojung Han, Chanyoung Kim, Dayun Ju, Yumin Shim, and Seong Jae Hwang. Advancing text-driven chest x-ray generation with policy-based reinforcement learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 5666. Springer, 2024. [25] Peng Huang, Xue Gao, Lihong Huang, Jing Jiao, Xiaokang Li, Yuanyuan Wang, and Yi Guo. Chest-diffusion: light-weight text-to-image model for report-to-cxr generation. In 2024 IEEE International Symposium on Biomedical Imaging (ISBI), pages 15. IEEE, 2024. [26] Daniel Mor√≠s, Joaquim de Moura, Jorge Novo, and Marcos Ortega. Adapted generative latent diffusion models for accurate pathological analysis in chest x-ray images. Medical & Biological Engineering & Computing, 62(7):21892212, 2024. [27] Virginia Fernandez, Pedro Sanchez, Walter Hugo Lopez Pinaya, Grzegorz Jacenk√≥w, Sotirios Tsaftaris, and Jorge Cardoso. Privacy distillation: reducing re-identification risk of multimodal diffusion models. arXiv preprint arXiv:2306.01322, 2023. [28] Salman Ul Hassan Dar, Arman Ghanaat, Jannik Kahmann, Isabelle Ayx, Theano Papavassiliu, Stefan Schoenberg, and Sandy Engelhardt. Investigating data memorization in 3d latent diffusion models for medical image synthesis. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 5665. Springer, 2023. [29] Muhammad Usman Akbar, Wuhao Wang, and Anders Eklund. Beware of diffusion models for synthesizing medical imagesa comparison with gans in terms of memorizing brain mri and chest x-ray images. Machine Learning: Science and Technology, 6(1):015022, 2025. [30] Raman Dutt, Ondrej Bohdal, Pedro Sanchez, Sotirios Tsaftaris, and Timothy Hospedales. Memcontrol: Mitigating memorization in diffusion models via automated parameter selection. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 44914501. IEEE, 2025. 11 [31] Raman Dutt. The devil is in the prompts: De-identification traces enhance memorization risks in synthetic chest x-ray generation. arXiv preprint arXiv:2502.07516, 2025. [32] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [33] Joseph Paul Cohen, Joseph D. Viviano, Paul Bertin, Paul Morrison, Parsa Torabian, Matteo Guarrera, Matthew Lungren, Akshay Chaudhari, Rupert Brooks, Mohammad Hashir, and Hadrien Bertrand. TorchXRayVision: library of chest X-ray datasets and models. In Medical Imaging with Deep Learning, 2022. [34] Gian Mario Favero, Parham Saremi, Emily Kaczmarek, Brennan Nichyporuk, and Tal Arbel. Conditional diffusion models are medical image classifiers that provide explainability and uncertainty for free. In Medical Imaging with Deep Learning, 2025. [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, June 2022. [36] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-œÉ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [37] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [38] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and Thomas Wolf. Diffusers: State-of-the-art diffusion models. https://github.com/huggingface/ diffusers, 2022. [39] Fernando P√©rez-Garc√≠a, Sam Bond-Taylor, Pedro Sanchez, Boris van Breugel, Daniel Castro, Harshita Sharma, Valentina Salvatelli, Maria TA Wetscherek, Hannah Richardson, Matthew Lungren, et al. Radedit: stress-testing biomedical vision models via diffusion image editing. In European Conference on Computer Vision, pages 358376. Springer, 2024. [40] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [41] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Will Beddow, Erwann Millon, Wenhai Wang Victor Perez, Yu Qiao, Bo Zhang, Xiaohong Liu, Hongsheng Li, Chang Xu, and Peng Gao. Lumina-image 2.0: unified and efficient image generative framework, 2025. [42] Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, Hany Awadalla, Julia Gong, Houdong Hu, Jianwei Yang, Chunyuan Li, Jianfeng Gao, Yu Gu, Cliff Wong, Mu Wei, Tristan Naumann, Muhao Chen, Matthew P. Lungren, Akshay Chaudhari, Serena Yeung-Levy, Curtis P. Langlotz, Sheng Wang, and Hoifung Poon. clinically accessible small multimodal radiology model and evaluation metric for chest x-ray findings. Nature Communications, 16(1):3108, Apr 2025. [43] Eyal Segalis, Dani Valevski, Danny Lumen, Yossi Matias, and Yaniv Leviathan. picture is worth thousand words: Principled recaptioning improves image generation. arXiv preprint arXiv:2310.16656, 2023. [44] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. 12 [45] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. [46] Tuomas Kynk√§√§nniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in fr√©chet inception distance. In The Eleventh International Conference on Learning Representations, 2023. [47] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 47004708, 2017. [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [49] Fernando P√©rez-Garc√≠a, Harshita Sharma, Sam Bond-Taylor, Kenza Bouzid, Valentina Salvatelli, Maximilian Ilse, Shruthi Bannur, Daniel Castro, Anton Schwaighofer, Matthew Lungren, et al. Exploring scalable medical image encoders beyond text supervision. Nature Machine Intelligence, pages 112, 2025. [50] Shruthi Bannur, Stephanie Hyland, Qianchu Liu, Fernando Perez-Garcia, Maximilian Ilse, Daniel Castro, Benedikt Boecking, Harshita Sharma, Kenza Bouzid, Anja Thieme, et al. Learning to exploit temporal structure for biomedical vision-language processing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1501615027, 2023. [51] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. 2020. [52] Nicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito, and Eric Wallace. Extracting training data from diffusion models. In 32nd USENIX Security Symposium (USENIX Security 23), pages 52535270, 2023. [53] M. Jegorova, C. Kaul, C. Mayor, A. Q. ONeil, A. Weir, R. Murray-Smith, and S. A. Tsaftaris. Survey: Leakage and privacy at inference time. IEEE Transactions on Pattern Analysis & Machine Intelligence, pages 120, 2023. [54] Kai Packh√§user, Sebastian G√ºndel, Nicolas M√ºnster, Christopher Syben, Vincent Christlein, and Andreas Maier. Deep learning-based patient re-identification is able to exploit the biometric nature of medical chest x-ray data. Scientific Reports, 12(1):14851, 2022. [55] Kuldeep Kumar, Christian Desrosiers, Kaleem Siddiqi, Olivier Colliot, and Matthew Toews. Fiberprint: subject fingerprint based on sparse code pooling for white matter fiber analysis. NeuroImage, 158:242259, 2017. [56] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [57] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [58] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. [59] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew Lungren, Andrew Ng, et al. Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463, 2021. 13 [60] Yiming Qin, Huangjie Zheng, Jiangchao Yao, Mingyuan Zhou, and Ya Zhang. Class-balancing diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1843418443, 2023. [61] Tianwei Lin, Wenqiao Zhang, Sijing Li, Yuqian Yuan, Binhe Yu, Haoyuan Li, Wanggui He, Hao Jiang, Mengze Li, Xiaohui Song, et al. Healthgpt: medical large vision-language model for unifying comprehension and generation via heterogeneous knowledge adaptation. arXiv preprint arXiv:2502.09838, 2025. 14 Benefits of Adopting LLaVA-Rad Annotations Figure 2: Figure depicting distribution of character lengths for original MIMIC captions and LlaVARad annotations. We also illustrate the text-encoder token length limits for all SD variants and Flux (77 tokens), RadEdit (128 tokens), and Pixart Sigma (300 tokens). Note: We treat 1 token 4 characters2 Table 6: Comparison of generation fidelity (left) and privacy preservation (right) metrics across different stable diffusion models. LlaVA-Rad annotations consistently outperform original MIMIC impressions, yielding improved image quality (FID/KID (), higher alignment ()) and enhanced privacy protection (Re-ID (), latent/pixel distances ()). Model Prompt Type FID (RadDino) KID (RadDino) Alignment Score SD-V1-4 Original MIMIC SD-V1-4 LlaVA-Rad 147.298 125.186 SD-V1-5 Original MIMIC SD-V1-5 LlaVA-Rad 144.661 118.932 SD-V2 SD-V Original MIMIC LlaVA-Rad 214.202 194.724 0.198 0.172 0.201 0.147 0.496 0.376 0.272 0. 0.257 0.326 0.145 0.311 Model Prompt Type Max. Re-ID Distance Min Latent Distance Min. Pixel Distance SD-V1-4 Original MIMIC 0.725 0.71 0.539 0.31 SD-V1-4 LlaVA-Rad 0.482 0.66 0.592 0.05 132.24 4.6 143.44 4.6 SD-V1-5 Original MIMIC 0.721 0.47 0.572 0.29 SD-V1-5 LlaVA-Rad 0.476 0.41 0.583 0.04 131.44 4.2 143.634 4.2 SD-V2 SD-V2 Original MIMIC 0.687 0.23 0.533 0.32 LlaVA-Rad 0.483 0.34 0.588 0.05 132.64 4.3 143.936 4. (a) Generation fidelity metrics. (b) Privacy and memorisation risk metrics. In this section, we demonstrate that LLaVA-Rad Annotations lead to substantial improvements in both fidelity performance and reduction of re-identification risks  (Table 6)  . We attribute these improvements to two key factors: the enhanced descriptiveness of the annotations and the removal of certain tokens from the original MIMIC annotations known to increase privacy risks [31]. Figure 2 displays the distribution of average character lengths across both annotation types. MIMIC annotations cluster around significantly smaller values, while LLaVA-Rad Annotations exhibit wider distribution, indicating greater descriptive detail. Table 6a reveals that LLaVA-Rad Annotations significantly enhance all three fidelity metrics (FID, KID, and Image-Text Alignment) compared to the original MIMIC annotations. Additionally, in Tab. 6b, we observe substantial improvement in privacy risk mitigation, further validating the superiority of the LLaVA-Rad annotation approach. 2https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them"
        },
        {
            "title": "B Unreliability in Fidelity Estimates with Outdated Backbones",
            "content": "Table 7: Comparison of FID and KID metrics across models using two distinct in-domain encoders. While the conventional DenseNet-121 encoder [33] exhibits minimal variance (0.001 - 0.075) across models, indicating limited discriminative power, the RadDino encoder [49] demonstrates substantially greater metric differentiation (54.22 - 194.72), providing more meaningful evaluation of model performance. Metric SD V3-5 Lumina 2.0 Flux.1-Dev LLM-CXR SD V2-1 RadEdit Pixart Sigma SD V1-4 SD V1-5 SD Sana FID KID DenseNet RadDino DenseNet RadDino 0.025 125. 0.075 118.930 0.053 194.720 0.056 186.530 0.004 0.172 0.004 0.147 0.005 0. 0.005 0.413 0.001 69.690 0.001 0.033 0.001 60.150 0.001 0.023 0.001 54. 0.001 0.016 0.002 91.300 0.004 0.103 0.001 101.190 0.003 0.110 0.001 122. 0.006 0.144 0.025 71.240 0.003 0.061 We demonstrate that existing protocols for evaluating generative fidelity in radiographic imaging are unreliable. Current approaches [19, 23, 20] calculate image fidelity (FID Score) using an in-domain DenseNet-121 model trained on the MIMIC-CXR dataset. We argue this model lacks sufficient discriminative power, resulting in less meaningful fidelity assessments. In our CheXGenBench benchmark, we address this limitation by leveraging features from RadDino, state-of-the-art model specifically designed for radiographs. As shown in Table 7, FID evaluation with DenseNet-121 shows minimal variance across models, often ranking several models at the same position. In contrast, the RadDino encoder significantly enhances evaluation quality by providing more meaningful feature representations that better differentiate between model performances."
        },
        {
            "title": "C Correlation between Fidelity and Downstream Tasks",
            "content": "Image Fidelity: We present the rank for each T2I model across each individual fidelity and mode coverage metric in Tab. 8. We also present the combined rank averaged across all the metrics resulting in Sana [4], Pixart Sigma [36], and LLM-CXR [20] as the top-three performers across all models. Downstream Image Classification: We present the rank for each T2I model across each individual pathology in Tab. 9. The Pearson correlation between fidelity and classification rank is 0.70. Based on the correlation coefficient of 0.70 between image fidelity and downstream classification performance, we can derive several important conclusions: 1. Strong Positive Correlation: correlation of 0.70 indicates strong positive relationship between image fidelity and downstream classification performance. This means that as image fidelity increases, classification performance tends to increase substantially as well. This also supports the value of developing high-fidelity synthetic image generators for medical applications. 2. Substantial Explained Variance: The coefficient of determination (r¬≤) would be approximately 0.49, suggesting that about 49% of the variance in classification performance can be explained by image fidelity. 3. Model Selection Guidance: When choosing models for generating synthetic medical images for training purposes, prioritizing those with higher fidelity metrics would be data-driven approach thats likely to yield better downstream performance. 4. Not Perfect Relationship: While strong, the correlation of 0.70 still leaves about 51% of the variance unexplained. This suggests other factors beyond simple image fidelity also influence classification performance, such as: (a) Diversity of the generated images (b) Representation of edge cases (c) Specific features that are diagnostically relevant but might not contribute heavily to overall fidelity metrics For medical imaging applications specifically, this correlation supports the hypothesis that realisticlooking synthetic images translate to better diagnostic model performance, though the relationship 16 isnt perfect. This finding could help justify investments in more sophisticated image generation techniques that prioritize visual fidelity. Table 8: Performance ranking of generative models for image fidelity across multiple evaluation metrics (lower rank indicates better performance). The top-3 performers are (1) Sana [4], (2) Pixart Sigma [36], and (3) LLM-CXR [20]. KID FID Model Inception RadDino Inception RadDino Alignment Precision Recall Density Coverage Average Normalized Score Rank Rank SD V1-4 SD V1-5 SD V2 SD V2-1 RadEdit SD V3-5 Lumina 2.0 Flux.1-Dev LLM-CXR Pixart Sigma Sana 9 8 10 11 3 5 6 7 4 2 1 9 7 11 10 3 5 6 8 4 2 1 9 8 10 11 3 5 6 4 2 1 9 8 10 11 3 5 6 7 4 2 1 4 5 7 8 3 10 9 11 6 1 2 8 6 9 7 11 2 5 1 4 3 5 4 6 7 2 11 9 10 8 3 1 7 6 9 8 10 4 5 11 1 3 2 8 6 10 11 5 7 9 3 2 1 7.55 6.44 9.11 9.33 4.78 6.00 6.78 8.33 3.89 2.33 1.44 8 6 10 11 4 5 7 9 3 2 1 Note: Lower rank numbers indicate better performance. Top three models highlighted based on normalized rank. Table 9: Ranking each T2I Model for synthetic data utility (image classification) across all 14 pathologies. Model Atel. Card. Cons. Edema EC Fract. SD Avg. LO PO LL NF PN PT PE SD V1-4 SD V1-5 SD V2 SD V2-1 RadEdit Pixart Sigma Sana SD V3-5 Lumina 2.0 Flux.1-Dev LLM-CXR 5.5 4.0 7.0 8.0 3.0 1.5 1.5 9.0 10.0 11.0 5.5 5.0 4.0 6.5 8.0 2.5 2.5 1.0 9.0 10.0 11.0 6.5 6.0 5.0 7.0 8.0 1.5 3.5 1.5 9.0 10.0 11.0 3.5 5.0 5.0 7.0 8.0 2.5 2.5 1.0 9.0 10.0 11.0 5.0 7.0 6.0 3.0 8.0 3.0 3.0 3.0 9.0 10.0 11.0 3.0 5.0 8.5 8.5 2.0 7.0 3.0 1.0 11.0 5.0 10.0 5. 2.5 1.0 7.0 4.0 6.0 5.0 2.5 11.0 9.0 10.0 8.0 4.5 4.5 7.0 8.0 2.5 2.5 1.0 9.0 10.0 11.0 6.0 5.5 3.0 7.5 7.5 4.0 1.5 1.5 9.0 10.0 11.0 5.5 5.5 4.0 7.0 8.0 2.0 3.0 1.0 10.0 9.0 11.0 5.5 6.0 3.5 10.5 8.5 2.0 3.5 1.0 7.0 8.5 10.5 5.0 7.0 4.0 5.5 8.0 1.0 3.0 2.0 9.5 9.5 11.0 5. 5.0 2.5 6.0 8.0 4.0 2.5 1.0 9.0 10.0 11.0 7.0 3.5 1.5 5.0 7.0 6.0 3.5 1.5 9.5 9.5 11.0 8.0 5.0 4.0 7.0 7.0 3.0 3.0 1.0 9.0 9.0 11.0 6.0 Legend: Atel. = Atelectasis, Card. = Cardiomegaly, Cons. = Consolidation, EC = Enlarged Cardiomediastinum, Fract. = Fracture, LL = Lung Lesion, LO = Lung Opacity, NF = No Finding, PE = Pleural Effusion, PO = Pleural Other, PN = Pneumonia, PT = Pneumothorax, SD = Support Devices, Avg. = Average Ranks"
        },
        {
            "title": "D Correlation Between Fidelity and Disease Distribution",
            "content": "In this section, we examine whether generative fidelity performance for individual pathologies, as reported in Tab. 2, correlates with the frequency of pathology occurrence in the training dataset. Fig. 3 illustrates the occurrence frequency distribution of 14 distinct pathologies in the training set. Conditions such as No Finding (NF), Pleural Effusion (PE), Support Devices (SD), and Lung Opacity (LO) represent the most frequently observed pathologies in the training data. Conversely, Fracture and Pleural Other exhibit significantly lower occurrence frequencies. Tab. 10 presents the comparative rankings according to occurrence frequency and FID scores. Analysis reveals remarkably strong positive correlation coefficient of 0.947 between these rankings, providing compelling evidence that generative fidelity demonstrates substantial dependence on pathology occurrence frequency in the training distribution. 17 Figure 3: Figure depicting the distribution of pathology counts for the 14 different conditions present in the MIMIC dataset. We indicate pathologies with there abbreviations. Note: AT (Atelectasis), CM (Cardiomegaly), CD (Consolidation), ED (Edema), EC (Enlarged Cardiomediastinum), Frac. (Fracture), LL (Lung Lesion), LO (Lung Opacity), NF (No Finding), PE (Pleural Effusion), PO (Pleural Other), PN (Pneumonia), PT (Pneumothorax), SD (Support Devices). Table 10: Occurrence Frequency and Generative Fidelity for Different Pathologies with rankings. We calculate the \"Fidelity Rank\" across models from Tab. 2 Pathology Code NF SD PE LO AT CM ED PN CD PT EC LL Frac. PO Count (n) 78,939 71,537 56,433 53,513 47,704 46,602 28,601 16,832 11,290 10,971 7,454 6,491 4,671 2,024 Prevalence Rank FID (RadDino) Fidelity Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 110.75 109.77 130.45 133.77 114.28 114.89 130.75 146.69 172.14 172.15 188.95 194.99 211.58 227.43 2 1 5 7 3 4 6 8 9 10 11 12 13 14 Note: Lower FID (Fr√©chet Inception Distance) scores indicate better generative fidelity. The table shows correlation between pathology prevalence and generative quality. Pathology codes: NF = No Finding, SD = Support Devices, PE = Pleural Effusion, LO = Lung Opacity, AT = Atelectasis, CM = Cardiomegaly, ED = Edema, PN = Pneumonia, CD = Consolidation, PT = Pneumothorax, EC = Enlarged Cardiomediastinum, LL = Lung Lesion, Frac. = Fracture, PO = Pleural Other. Effect of Additional Fine-Tuning of Sana In this section, we analyze the impact of extended fine-tuning on our benchmark-leading model, Sana (0.6B), by increasing training from 20 epochs (as reported in the main benchmark) to 50 epochs. Our analysis reveals nuanced picture of how prolonged training affects different performance dimensions. In Tab. 11, we show the improvements from the 20 epoch checkpoint on Report Generation task. Fidelity Improvements: As illustrated in Figure 4a, extended fine-tuning yields modest but consistent improvements in FID scores across all pathologies. The most significant gains were observed in No Finding, which stands as the class with the most number of samples in the MIMIC dataset. Improvement in Recall Scores: Fig. 4b demonstrates that recall scores show more substantial improvements than fidelity metrics. This pattern indicates that extended training primarily enhances the models ability to reproduce larger spectrum of pathological variations rather than incrementally 18 improving visual quality. Interestingly, in this scenario, all classes (majority and minority) show significant improvement. Takeaway: Despite the additional training epochs, performance improvements on rare pathologies remain disproportionately small compared to common conditions. This observation shows that addressing long-tailed distribution challenges cannot be solved through extended fine-tuning alone and would require specialised algorithmic changes. (a) Comparing FID () for fidelity (b) Comparing Recall Scores () for mode coverage Figure 4: Extended Training Impact on Sana Model Performance. Comparison of generative quality metrics after standard (20 epochs) and extended (50 epochs) fine-tuning of the Sana model. While FID scores show modest improvement, the Recall metric exhibits substantial enhancement across all pathologies, indicating significantly improved sample diversity without compromising fidelity. E.1 Training Settings and Hyperparameters The hyperparameters (learning rates) for Text-to-Image model training are provided in Tab. ??. Existing foundation models (RadEdit, LLM-CXR) were not re-trained and used as is. For evaluating downstream utility, the learning rates are provided in Tab. ??. Hyper-Params SD V1SD V1-5 SD V2 SD V2-1 RadEdit Pixart Sigma Sana SD V3-5 Lumina 2. Flux.1-Dev LLM-CXR Fine-Tuning Learning Rate FFT 5e-6 FFT 5e-6 FFT 5eFFT 5e-6 N/A - FFT 2e-5 FFT 1e-4 LoRA (r-32) LoRA (r-32) LoRA (r-32) 1e-4 1e1e-4 N/A - Model ResNet-50 (Classification) LLaVA-Rad (RRG) Fine-Tuning Learning Rate FFT 1e-4 LoRA 1e-4 Data Filtration for SynthCheX-75K Generative models can lead to both high and low-fidelity generations on different subsets of the dataset. In order to keep the sample quality high in SynthCheX-75K, stringent filtration process was adopted using HealthGPT [61], highly-capable medical VLM with advanced understanding, reasoning and generation. The VLM was provided with the following meta-prompt to classify the quality of each generated sample. META PROMPT: \"You are an expert radiologist and medical annotator. Your task is to assess the quality of an image given its description and classify the image as either High Quality, Medium Quality, or Low Quality. Keep your responses limited to only these three options. If the image is not relevant to the description, respond with Not Relevant.\" After quality label assignment, images with \"Low Quality\" and \"Not Relevant\" labels were removed from the dataset leading to 75,649 samples of high-quality radiographs with pathological annotations. 19 Table 11: Comparing the performance of fine-tuning Sana from 20 to 50 epochs for the RRG task. Additional fine-tuning does provide benefits, however, they are marginal. Model BLEU-1 BLEUROUGE-L F1-RadGraph Micro F1-5 Micro F1-14 Original Sana (Epoch 20) Sana (Epoch 50) 38.16 29.83 30.80 (+0.97) 15.38 7.70 7.91 (+0.21) 0.31 0.24 0.25 (+0.01) 0.29 0.23 0.24 (+0.01) 0.57 0.57 0.58 (+0.01) 0.57 0.55 0.57 (+0.02)"
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The abstract introduces benchmark, CheXGenBench, that evaluates frontier text-to-image models on fidelity, privacy, and utility. The paper further explains the fundamentals of the benchmark along with the results, observations and recommendations. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Yes, the limitations are presented in the Conclusion section of the manuscript. Guidelines: 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [NA] Justification: This work does not introduce any new theories. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, the entire benchmark is fully reproducible. The authors have provided anonymized repository that contains the required files and instructions to run every experiment mentioned in the paper. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Yes, the data and code has been anonymized and released with the manuscript. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, all the data splits, hyperparameters and other details have been released with the manuscript. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? 20 Answer: [Yes] Justification: Yes, error bars have been reported wherever applicable in the results. 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, we have provided the compute cost and the corresponding hardware for the experiments mentioned in the paper. 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Yes, the paper conforms to the NeurIPS Code of Ethics. 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, the broader impact has been discussed in the Conclusions section. 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: Yes, safeguards have been put in place. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: All models used in this work are open-sourced. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Yes, the dataset released along with the paper is well documented. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not applicable. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? 21 Answer: [NA] Justification: Not applicable to this study. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: LLMs are not used in this work."
        }
    ],
    "affiliations": [
        "Samsung AI Center, Cambridge",
        "Sinkove",
        "The University of Edinburgh"
    ]
}