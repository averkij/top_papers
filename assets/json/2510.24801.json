{
    "paper_title": "Fortytwo: Swarm Inference with Peer-Ranked Consensus",
    "authors": [
        "Vladyslav Larin",
        "Ihor Naumenko",
        "Aleksei Ivashov",
        "Ivan Nikitin",
        "Alexander Firsov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, a novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: a peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with a custom Bradley-Terry-style aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model set - an improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding a meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for a monolithic single-model baseline), while retaining practical deployability. Together, these results establish a foundation for decentralized AI systems - democratizing access to high-quality inference through collective intelligence without sacrificing reliability or security."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 1 0 8 4 2 . 0 1 5 2 : r FORTYTWO: SWARM INFERENCE WITH PEER-RANKED CONSENSUS Vladyslav Larin vlarin@fortytwo.network Ihor Naumenko inaumenko@fortytwo.network Aleksei Ivashov aivashov@fortytwo.network Ivan Nikitin inikitin@fortytwo.network Alexander Firsov afirsov@fortytwo.network October 30,"
        },
        {
            "title": "ABSTRACT",
            "content": "As centralized AI hits compute ceilings and diminishing returns from ever-larger training runs, meeting demand requires an inference layer that scales horizontally in both capacity and capability. We present Fortytwo, novel protocol that leverages swarm intelligence principles and distributed pairwise ranking consensus to achieve superior performance in AI inference. Our approach reimagines collaboration among AI nodes using swarm inference: peer-ranked, reputation-weighted consensus across heterogeneous models that surfaces the highest-quality responses. Using pairwise ranking with custom BradleyTerrystyle aggregation model, we demonstrate that swarm inference substantially outperforms majority voting, achieving 85.90% on GPQA Diamond versus 68.69% for majority voting with the same model setan improvement of +17.21 percentage points (approximately +25.1% relative). The protocol incorporates on-chain reputation so node influence adapts to demonstrated accuracy over time, yielding meritocratic consensus that filters low-quality or malicious participants. To resist Sybil attacks, Fortytwo employs proof-of-capability in its consensus: nodes must successfully complete calibration/test requests and stake reputation to enter ranking rounds, making multi-identity attacks economically unattractive while preserving openness. Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and AIME, our evaluation indicates higher accuracy and strong resilience to adversarial and noisy free-form prompting (e.g., prompt-injection degradation of only 0.12% versus 6.20% for monolithic single-model baseline), while retaining practical deployability. Together, these results establish foundation for decentralized AI systemsdemocratizing access to high-quality inference through collective intelligence without sacrificing reliability or security. Fortytwo xAI Grok 4 Claude Opus 4.1 GPT-5 Thinking Gemini 2.5 Pro DeepSeek R1 ) % ( r A 100 80 60 40 20 85.9 87.7 81 85 84.4 84.4 81.9 80.1 77 65.4 66. 99.6 99 91.9 99.4 96.7 98. 100 94.3 94.3 88.7 89.3 96. 92.7 75.7 94.3 87.7 80.3 GPQA Diamond LiveCode MATH-500 AIME 2024 AIME 2025 HLE Figure 1: Benchmark performance comparison across models. 24.84 23.9 26.5 21.1 11. 14.9 1 PREPRINT - OCTOBER 30,"
        },
        {
            "title": "Contents",
            "content": "1 Introduction"
        },
        {
            "title": "1.1 Multidisciplinary Foundations",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1.2 Key Innovations and Contributions .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "1.3 Paper Organization .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Related Work"
        },
        {
            "title": "2.1 Cryptographic Approaches to Verifiable AI",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.1.1 Zero-Knowledge Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "."
        },
        {
            "title": "2.2.1 Optimistic Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": ". . 2.2.2 Proof of Quality Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Distributed Consensus and Byzantine Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.1 Classical Byzantine Fault Tolerance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3.2 Modern BFT Innovations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Ranking and Preference Learning Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.1 Bradley-Terry Model Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4.2 LLM-as-a-Judge Paradigms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5 Swarm Intelligence and Collective Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5.1 Biological Inspiration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.5.2 Artificial Swarm Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.6 Security and Attack Prevention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.6.1 Sybil Attack Prevention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.6.2 Adversarial Robustness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Self-Supervised Inference Architecture 3.1 Theoretical Foundations of Dual-Role Nodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Node Component Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Primary Cognitive Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Auxiliary Processing Unit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.3 Ranking Engine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.4 Communication Module . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Dynamic Node Lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 Entry and Qualification . 3.3.2 Active Participation . . 3.3.3 Reputation Evolution . 3.3.4 Exit and Penalties . . . 3.4 Semantic Network Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Semantic Embedding Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 6 6 7 7 7 7 8 8 8 8 8 9 9 9 9 9 10 10 10 10 10 10 11 11 12 12 13 13 13 14 PREPRINT - OCTOBER 30,"
        },
        {
            "title": "3.4.2 Axis-Aligned Hierarchical Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "."
        },
        {
            "title": "3.4.4 Bandwidth-Aware Coverage Radius . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "."
        },
        {
            "title": "3.4.5 Efficient Query Routing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "3.4.6 Sub-Mesh Identification and Coordination . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Enhanced Pairwise Ranking Consensus Mechanism"
        },
        {
            "title": "4.1 Theoretical Foundations",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.1.2 Bradley-Terry Model .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.3 Information-Theoretic Optimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4.2.1 Cryptographically Secure Randomization . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "4.2.2 Balanced Comparison Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Exclusion of Self-Comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Multi-Token Reasoning Chains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.1 Cognitive Load and Quality Trade-offs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3.2 Reasoning Chain Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.4 Bradley-Terry Aggregation Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5 Reputation-Weighted Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.1 Reputation Score Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.2 Weighted Bradley-Terry Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.5.3 Dynamic Reputation Updates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Compute Stake Mechanism and Sybil Defense 5.1 Theoretical Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.1 Limitations of Traditional Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1.2 Computational Proof-of-Capability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Test Requests Design and Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Comprehensive Domain Coverage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Dynamic Test Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Sybil Attack Models and Defense . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.1 Attack Objectives . . . . 5.3.2 Defensive Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.3.3 Economic Analysis with Defenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Reputation Dynamics and Long-term Incentives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.1 Reputation as Non-Transferable Asset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4.2 Evolutionary Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Evaluation 6.1 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 14 15 15 15 16 16 16 16 17 17 17 17 18 18 18 18 19 19 20 20 20 20 21 21 21 21 22 22 22 23 23 3 PREPRINT - OCTOBER 30,"
        },
        {
            "title": "6.1.1 Benchmark Selection .",
            "content": "6.1.2 Swarm Configuration ."
        },
        {
            "title": "6.2 Main Results",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.2.1 Benchmark Performance .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.3 Ablation Studies .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.3.1 Component Contribution Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": ". 6.3.2 Swarm Size Scaling ."
        },
        {
            "title": "6.4 Robustness Analysis",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.5 Economic Analysis .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "6.5.1 Computational Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "6.6 Qualitative Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.6.1 Ranking Rationale Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6.6.2 Response Quality Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 Discussion 7.1 Why Pairwise Peer-Ranking Succeeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.1 Cognitive Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.2 Statistical Power of Tournament Aggregation . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.3 Emergence Through Diversity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1.4 Economic Incentive Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Limitations and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.1 Latency Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2.2 Interpretability and Debugging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Future Research Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.1 Theoretical Foundations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.2 Alternative Consensus Mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.3 Cross-Modal Extension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3.4 Human-AI Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion 23 23 23 23 24 25 25 25 27 27 28 28 29 29 29 30 30 30 31 31 31 31 31 31 32"
        },
        {
            "title": "Introduction",
            "content": "A PREPRINT - OCTOBER 30, 2025 The landscape of artificial intelligence has undergone fundamental transformation with the emergence of large language models (LLMs) exceeding hundreds of billions of parameters, creating unprecedented challenges in deployment, accessibility, and control (Agrawal et al., 2024; Mafrur, 2025). Traditional centralized AI infrastructures, dominated by handful of major technology companies, have created computational bottlenecks that limit innovation and raise concerns about algorithmic transparency, data sovereignty, and equitable access to transformative AI technologies. These centralized systems face inherent limitations in scalability, with distributed inference achieving only 50-70% of single-machine throughput due to communication overhead in wide-area deployments (Kwon et al., 2023). The concentration of AI capabilities within oligopolistic structures has profound implications for technological sovereignty, economic equity, and the democratic development of artificial intelligence systems that increasingly shape human society. Building on our earlier work introducing self-supervised swarm inference in trustless environments (Larin et al., 2024), this paper develops complete protocol and system architecture for decentralized inferencedetailing dual-role node design (generation + judging), distributed pairwise-ranking consensus based on custom BradleyTerrystyle aggregation model, reputation-weighted decision making, and the efficiency/robustness characteristics that emerge at swarm scale. The emergence of decentralized AI represents paradigm shift that addresses these fundamental limitations by distributing computation across heterogeneous networks of autonomous nodes. This approach draws inspiration from natural systems where collective intelligence emerges from simple local interactionsa phenomenon observed across biological systems from ant colonies optimizing foraging paths to neural networks processing information through distributed computation (Garnier et al., 2007; Nitti et al., 2025). Decentralized systems offer inherent advantages including enhanced resilience through redundancy, efficient resource utilization by leveraging idle computational capacity, and democratic access that enables participation from diverse stakeholders regardless of their economic resources. The integration of blockchain technology provides the cryptographic guarantees and economic incentives necessary for trustless coordination, creating immutable audit trails essential for regulatory compliance while enabling transparent AI decision-making (Castro and Liskov, 1999; Zhong et al., 2023). The technical challenges of implementing trustless AI inference have motivated numerous approaches, each representing different trade-offs in the design space of security, performance, and economic viability. Zero-Knowledge Machine Learning (ZKML) provides the strongest cryptographic guarantees by enabling verifiable inference without revealing model weights or input data, although computational overhead remains prohibitive for large-scale deployment (Feng et al., 2021; Sun et al., 2023). Optimistic Machine Learning (OPML) reduces computational requirements through fraud-proof protocols that separate execution from verification, achieving practical deployment at the cost of requiring challenge periods that delay finality (Conway et al., 2024). Proof of Quality (PoQ) mechanisms focus on output validation using lightweight discriminator models, trading guaranteed accuracy for computational efficiency with empirical results showing less than 70% accuracy in quality assessment (Zhang et al., 2024). These existing approaches, while valuable contributions to the field, do not achieve the combination of accuracy, efficiency, and economic viability necessary for widespread adoption. The theoretical foundations for our approach draw from multiple disciplines, including tournament theory, social choice theory, and swarm intelligence. The Bradley-Terry model, developed in 1952 to analyze paired comparisons in sports tournaments, provides probabilistic framework for aggregating preferences that has found renewed relevance in AI alignment and preference modeling (Bradley and Terry, 1952). Recent advances by Sun et al. provide statistical consistency and convergence guarantees for BradleyTerry-style preference models in neural settings, giving formal support to our use of pairwise ranking as foundation for decentralized evaluation (Sun et al., 2025). The models ability to extract consistent global rankings from potentially inconsistent pairwise comparisons makes it particularly suitable for distributed evaluation, where individual nodes may have varying reliability and expertise. The principles of swarm intelligence, observed in biological systems and increasingly applied to artificial systems, demonstrate how collective behaviors can emerge from simple local rules without centralized coordination (Dorigo et al., 2021). Natural swarms achieve remarkable feats of collective problem solving: ant colonies discover optimal paths through pheromone-based stigmergic communication, bee colonies make democratic decisions about nest sites through waggle dance debates, and bird flocks navigate complex environments through local alignment rules (Garnier et al., 2007). These biological systems have evolved mechanisms for robust decision making under uncertainty, resistance to individual failures, and adaptation to changing environments, essential properties for artificial intelligence systems operating in real-world conditions. 5 PREPRINT - OCTOBER 30, 2025 1.1 Multidisciplinary Foundations To ensure accessibility for researchers from diverse backgrounds, we provide comprehensive explanations of key concepts that underpin our framework. The Bradley-Terry model operates on the principle that the probability of preferring the item over the item follows logistic function based on latent quality scores: (i j) = πi/(πi+πj), where πi represents the intrinsic quality of the item i. This formulation naturally handles intransitive preferences and provides uncertainty estimates through the variance of score distributions. In the context of AI systems, each response generated by participant model can be assigned quality score based on pairwise comparisons with other responses, allowing robust ranking even when direct quality measurement is challenging. Byzantine Fault Tolerance (BFT), fundamental concept in distributed systems, addresses the challenge of achieving consensus when some participants may behave arbitrarily or maliciously, named after the Byzantine Generals Problem, where commanders must coordinate despite potential traitors (Lamport et al., 1982). Classical results establish that BFT protocols can tolerate up to < n/3 Byzantine nodes among total nodes, with modern implementations such as HotStuff achieving linear message complexity suitable for internet-scale deployment (Yin et al., 2019). Our system leverages these theoretical guarantees while introducing reputation-based weighting that provides additional resilience beyond classical fault tolerance bounds. The reputation system in our framework serves as dynamic weighting mechanism where each nodess influence on consensus decisions evolves based on their historical performance. Unlike static voting systems where all participants have equal weight, reputation-based systems create meritocracies where consistent quality contributions are rewarded with increased influence. The reputation Ri of node is updated according to: R(i) + (1 α) Accuracy(i) , where α controls the balance between the historical reputation and recent performance. This creates powerful incentives for honest participation, while naturally marginalizing low-quality or malicious nodes over time. t+1 = αR(i) 1.2 Key Innovations and Contributions This paper presents the Fortytwo Protocol, comprehensive framework that synthesizes insights from swarm intelligence, tournament theory, and distributed systems to create practical solution for decentralized AI inference. Our key innovations include: 1. Distributed Pairwise Ranking Consensus: We introduce novel mechanism where each node generates 3N pseudo-random pairwise comparisons that are aggregated using our improved BradleyTerry model. This leverages the cognitive principle that relative comparisons are more reliable than absolute assessments, while our BradleyTerry extension ensures convergence to consistent global rankings even under noisy or inconsistent individual comparisons. 2. Multi-Token Reasoning Chains: Unlike single-scalar reward models that compress quality assessment into single forward pass, our system requires nodes to generate detailed reasoning chains (50-100 tokens) explaining their ranking decisions. This explicit reasoning process improves accuracy by 5.3% compared to single-token scoring while creating audit trails that allow system improvement and debugging. 3. Compute Stake Mechanism: We introduce compute-anchored Sybil-resistance scheme that relies on proof of capability rather than economic staking. New nodes must complete comprehensive test calls across their claimed semantic distribution (e.g., mathematical reasoning, scientific analysis, code generation) to demonstrate competence, with the computational cost of these evaluations serving as natural barrier to identity multiplication. 4. Adaptive Reputation Dynamics: The system implements sophisticated reputation tracking where nodes weights evolve based on ranking accuracy, consistency, and alignment with consensus. Poor performers experience reputation slashing analogous to stake slashing in proof-of-stake systems, while consistent quality contributions are rewarded with increased influence and higher reward shares. 5. Adversarial Resilience and Free-form Stability: Swarm Inference directly mitigates contextual distractionthe tendency of single LLMs to be derailed by extraneous or misleading context. By combining ensemble diversity with peer-ranked validation, the network preserves signal under noisy, unstructured, or intentionally manipulative prompts (e.g., prompt injections, verbose or poorly formatted inputs, mixed domains). In our evaluations, accuracy degraded by only 0.12% under such noisy/adversarial prompting, versus 6.20% for monolithic single-model baseline. This robustness translates to higher real-world accuracy on free-form user queries and exhibits antifragile behavior: diversity and consensus reduce failure modes as the swarm grows. 6 PREPRINT - OCTOBER 30, 2025 1.3 Paper Organization The remainder of this paper is organized as follows: Section 2 provides comprehensive survey of related work in decentralized AI, establishing the context and limitations of existing approaches. Section 3 presents the detailed architecture of self-supervised inference, including the dual-role design where nodes perform both inference and ranking. Section 4 describes our enhanced pairwise ranking consensus mechanism, including the foundational BradleyTerry aggregation framework, and reputation-weighted voting. Section 5 details the compute stake mechanism and its effectiveness in preventing Sybil attacks. Section 6 presents extensive experimental evaluation across multiple benchmarks, demonstrating superior performance and adversarial robustness. Section 7 analyzes why our approach succeeds, exploring the cognitive, statistical, and economic factors that contribute to its effectiveness. Section 8 discusses limitations and future research directions. Finally, Section 9 concludes with reflections on the broader implications for decentralized AI development."
        },
        {
            "title": "2 Related Work",
            "content": "The pursuit of trustless AI inference in decentralized environments has spawned diverse approaches, each addressing different aspects of the fundamental trilemma between security, performance, and economic viability. We systematically examine these approaches, analyzing their contributions and limitations to establish the context for our work. 2.1 Cryptographic Approaches to Verifiable AI 2.1.1 Zero-Knowledge Machine Learning Zero-Knowledge Machine Learning represents the standard for cryptographic verifiability, enabling proof of correct inference without revealing model weights or input data. The foundational work by Feng et al. introduced Zen, an optimizing compiler that transforms neural network computations into arithmetic circuits suitable for zero-knowledge proof systems (Feng et al., 2021). This approach achieves perfect security guaranteesa verifier can be certain that the claimed inference was computed correctly without learning anything about the model or data beyond the output. Recent advances have focused on improving practical efficiency. The EZKL framework provides production-ready tools for converting ONNX models to zkSNARK circuits using the Halo2 proof system, supporting networks up to 100 million parameters (EZKL Team / ZKonduit, 2024). Daniel Kang et al. demonstrated scaling to larger models through specialized optimizations, though proof generation for single ResNet-50 inference still requires over 24 hours (Kang et al., 2022). The fundamental challenge remains the superlinear growth of proof generation complexity with model sizeeach multiplication in the neural network requires multiple constraints in the arithmetic circuit, and proof generation time grows polynomially with circuit size. The computational overhead of ZKML makes it suitable only for high-value applications where cryptographic guarantees justify extreme costs. Financial fraud detection, medical diagnosis verification, and legal evidence validation represent domains where the ability to prove computational correctness without revealing proprietary models or sensitive data provides sufficient value to offset computational expenses. However, for general-purpose AI inference where users expect interactive response times, ZKML remains impractical despite its theoretical elegance. 2.1.2 Homomorphic Encryption Fully Homomorphic Encryption (FHE) enables computation on encrypted data, providing strong privacy guarantees for distributed inference. Stoian et al. constructed deep neural network architectures compatible with TFHE (Torus Fully Homomorphic Encryption), achieving practical inference on encrypted MNIST data with accuracy comparable to plaintext models (Stoian et al., 2023). The approach ensures that model providers never see user data while users never access model weights, addressing bidirectional privacy concerns in AI-as-a-service deployments. Recent advances in CKKS-based schemes support approximate arithmetic operations essential for neural network computations, with improvements in ciphertext packing and bootstrapping that reduce amortized costs (Hong, 2025). Microsofts SEAL library and IBMs HELayers provide production-ready implementations, though performance remains challenging: single encrypted inference on miniature ResNet-20 requires seconds to minutes compared to microseconds for plaintext computation. The multiplicative depth limitations of practical FHE schemes also constrain model architectures, preventing the use of deep networks that have driven recent AI advances. 7 PREPRINT - OCTOBER 30, 2025 2.2 Optimistic and Economic Security Models 2.2.1 Optimistic Machine Learning Optimistic Machine Learning represents pragmatic compromise between security and efficiency, adopting the optimistic assumption that computations are correct unless challenged. Conway et al. introduced comprehensive OPML architecture featuring Fraud Proof Virtual Machines and Interactive Dispute Games that enable practical deployment of 7B-parameter models (Conway et al., 2024). The system separates execution from provinginference results are available immediately while cryptographic proofs are generated only when disputes arise. The economic security model creates powerful disincentives for misbehavior: challengers must stake tokens that are forfeited if disputes prove frivolous, while service providers lose stakes if caught cheating. This mechanism has proven effective in practice, with ORA Protocols on-chain AI Oracle processing millions of inferences without successful attacks. The key limitation is the challenge period (typically 7 days) required for finality, making OPML unsuitable for applications requiring immediate quality guarantee, that is, majority of practical applications. Additionally, the complexity of implementing fraud proofs for neural network computations requires sophisticated engineering to handle numerical precision issues and non-deterministic operations. 2.2.2 Proof of Quality Mechanisms Proof of Quality approaches shift focus from computational correctness to output quality, using lightweight discriminator models to evaluate generated content. Zhang et al. developed comprehensive frameworks where simple BERTbased models assess outputs from complex generative models, achieving validation with less than 1% of generation computational cost (Zhang et al., 2024). This asymmetry between generation and validation costs makes PoQ economically attractive for decentralized systems. Chong et al. extended this concept with Proof of Useful Intelligence, demonstrating that quality-based consensus can achieve 97% energy reduction compared to Proof-of-Work while maintaining security through economic incentives (Chong et al., 2025). However, fundamental challenges remain in quality assessment accuracyempirical evaluations show discriminator models achieve less than 70% agreement with human judgments for complex outputs. The subjective nature of quality in creative tasks further complicates validation, as different evaluators may have legitimate disagreements about output quality. 2.3 Distributed Consensus and Byzantine Fault Tolerance 2.3.1 Classical Byzantine Fault Tolerance Byzantine Fault Tolerance provides the theoretical foundation for consensus in adversarial environments. The Byzantine Generals Problem, formulated by Lamport, Shostak, and Pease, establishes fundamental limits: any deterministic protocol requires 3f + 1 nodes to tolerate Byzantine failures (Lamport et al., 1982). This bound is tightno protocol can tolerate more failures, and protocols exist that achieve consensus with exactly this many nodes. Practical Byzantine Fault Tolerance (PBFT), introduced by Castro and Liskov, made BFT practical for real systems by reducing message complexity to O(n2) and achieving throughput of thousands of transactions per second (Castro and Liskov, 1999). The protocol operates in rounds where designated primary node proposes values that backup nodes validate through three-phase voting. Despite its efficiency, PBFT suffers from poor scalability beyond hundreds of nodes due to quadratic message complexity and the overhead of view changes when primaries fail. 2.3.2 Modern BFT Innovations Recent advances address classical BFT limitations through various approaches. HotStuff achieves linear message complexity O(n) by introducing novel three-phase voting structure with pipelined rounds, enabling responsive operation where the protocol proceeds at network speed rather than waiting for timeout (Yin et al., 2019). This improvement makes BFT practical for internet-scale deployments with thousands of participants. Probabilistic approaches trade deterministic guarantees for improved efficiency. Avalanche introduces metastable consensus through repeated random sampling, achieving agreement with high probability while requiring only logarithmic message complexity (Rocket et al., 2019). These protocols are particularly suitable for AI applications where occasional inconsistencies are acceptable if they occur with negligible probability. 8 PREPRINT - OCTOBER 30, 2025 2.4 Ranking and Preference Learning Systems 2.4.1 Bradley-Terry Model Applications The Bradley-Terry model has emerged as fundamental for preference-based learning in AI systems. Originally developed for sports rankings, the model assumes pairwise preference probabilities follow logistic function based on latent quality scores (Bradley and Terry, 1952). Sun et al. derive finite-sample guarantees for maximum-likelihood estimation under BradleyTerry with neural parameterizations, specifying identifiability and comparison-graph connectivity conditions under which true latent scores are recovered with high probability, along with convergence rates (Sun et al., 2025). Applications in AI include ChatGPTs reward modeling where human preferences between response pairs train reward functions for reinforcement learning (Ouyang et al., 2022). The Chatbot Arena uses Bradley-Terry modeling to rank language models based on crowd-sourced pairwise comparisons, producing leaderboards that correlate strongly with comprehensive benchmarks (Chiang et al., 2024). These successes demonstrate the models effectiveness in aggregating subjective preferences into consistent global rankings. 2.4.2 LLM-as-a-Judge Paradigms Recent advances demonstrate that language models can effectively evaluate their own and others outputs when provided with appropriate prompting strategies. MetaRanking approaches demonstrate that even smaller models like Phi-2 (2.7B parameters) can provide effective ranking when fine-tuned on preference data (Liu et al., 2024). The ability to perform accurate ranking with smaller models enables broader participation in consensus formation, as nodes with limited computational resources can still contribute meaningful evaluations. Zheng et al. established MT-Bench as widely-used benchmark for LLM evaluation, achieving 0.93 Spearman correlation with human annotators and validating the reliability of automated assessment (Zheng et al., 2023). 2.5 Swarm Intelligence and Collective Systems 2.5.1 Biological Inspiration Swarm intelligence emerges from simple local interactions among nodes following basic rules without centralized coordination. Dorigos foundational work on Ant Colony Optimization demonstrated how artificial ants could solve complex optimization problems through pheromone-based communication (Dorigo et al., 1996). These algorithms have found practical applications in network routing, scheduling, and resource allocation where traditional optimization methods struggle with dynamic environments. Foundational research by Garnier et al. establishes the biological principles underlying swarm coordination, identifying key mechanisms including positive feedback for rapid decision-making, negative feedback for stabilization, randomness for exploration, and multiple interactions for information averaging (Garnier et al., 2007). These principles directly inform our node coordination mechanisms, particularly in how the ranking information propagates through the swarm to achieve consensus as well how nodes use random sampling parameters or aggregate on the best candidates. 2.5.2 Artificial Swarm Intelligence The application of swarm principles to human and AI groups has demonstrated remarkable potential for amplifying collective intelligence. Rosenbergs Artificial Swarm Intelligence (ASI) platform connects groups of human participants in real time, allowing them to form dynamic feedback loops modeled after biological swarms. In one study, 75 sports fans working together as real-time swarm on the UNU platform achieved 70% accuracy in predicting College Bowl football games against the spreadoutperforming both individual participants (50%) and expert analysts (50%) (Rosenberg, 2016). By synchronizing individual inputs through continuous feedback and convergence mechanisms, the system enables human groups to reach decisions that consistently exceed the predictive power of traditional polling and voting methods. Recent implementations in robotics demonstrate practical swarm coordination. Nitti et al. developed swarm cooperation models achieving higher or equal success rate with respect to benchmark methods on 22 out of 33 landscapes with groups of 5-16 robots through multi-loop feedback and adaptive control (Nitti et al., 2025). These results suggest that modest-sized swarms can achieve most benefits of larger collectives, informing our design decision to focus on swarms of 5-19 nodes that balance performance with coordination overhead. 9 PREPRINT - OCTOBER 30, 2.6 Security and Attack Prevention 2.6.1 Sybil Attack Prevention Sybil attacks, where adversaries create multiple fake identities to gain disproportionate influence, represent fundamental threat to decentralized systems (Douceur, 2002). Traditional defenses rely on resource costs: Proof-of-Work requires computational expenditure making identity creation expensive, while Proof-of-Stake demands economic investment proportional to influence. However, these mechanisms favor participants with greater capital resources, conflicting with goals of democratization and efficiency. Recent approaches explore alternative Sybil defenses. Social network-based methods like SybilGuard leverage trust relationships to limit fake identities, though assumptions about social graph properties may not hold in practice (Yu et al., 2008). Proof of Personhood systems attempt to ensure one-person-one-vote through mechanisms like pseudonym parties or biometric verification, though privacy and usability challenges remain (Borge et al., 2017). Our compute stake mechanism represents novel approach requiring demonstration of capability rather than resource expenditure. 2.6.2 Adversarial Robustness Adversarial attacks on AI systems have evolved from simple perturbations fooling image classifiers to advanced prompt injections manipulating language models. Recent work demonstrates that ensemble methods provide natural defenseattacks that fool one model often fail against others with different architectures or training (Jiang et al., 2023). This diversity principle fundamentally underlies our swarms resilience to adversarial inputs. Research on certified robustness provides theoretical guarantees about model behavior under bounded perturbations, though current methods apply only to simple models and small perturbation bounds (Wong et al., 2018). Our approach achieves practical robustness through redundancy and validation rather than certification, demonstrating only 0.12% performance degradation under adversarial conditions compared to 6.20% for individual models."
        },
        {
            "title": "3 Self-Supervised Inference Architecture",
            "content": "The design of our self-supervised inference represents fundamental departure from traditional distributed AI architectures where separate entities handle inference and validation. By empowering each node with dual capabilities for both content generation and quality assessment, we create self-organizing ecosystem where collective intelligence emerges from local interactions without centralized coordination. This section presents the comprehensive architecture, theoretical foundations, and implementation details of our node design. 3.1 Theoretical Foundations of Dual-Role Nodes The decision to combine inference and ranking capabilities within single nodes stems from multiple theoretical and practical considerations. From game-theoretic perspective, nodes that both generate content and evaluate others outputs have skin in the gametheir ranking performance directly affects their reputation and future earnings, creating powerful incentives for honest evaluation. This alignment of interests contrasts with systems where separate validators have no stake in generation quality, potentially leading to lazy or adversarial validation. The phenomenon of emergent metacognition in large language models provides theoretical support. Recent studies show that LLMs develop implicit models of their own capabilities and limitations, enabling them to assess uncertainty and identify potential errors in reasoning (Kadavath et al., 2022). This metacognitive ability naturally extends to evaluating others outputs, as the same quality criteria apply regardless of the source. By leveraging this inherent capability, our dual-role design achieves efficiency and consistency that would be difficult to replicate with separate generation and validation systems. 3.2 Node Component Architecture Each node in our swarm implements modular architecture that balances flexibility with standardization. The design enables heterogeneous implementations while maintaining compatible interfaces for inter-node communication and system integration. 3.2.1 Primary Cognitive Module The core component responsible for both content generation and ranking tasks can be implemented using various approaches depending on available resources and specialization requirements: 10 PREPRINT - OCTOBER 30, 2025 Large Language Models: The most common implementation uses transformer-based language models that typically range from 3B to 405B parameters. Smaller models like Qwen3-8B or Gemma-3-4B provide efficient operation on consumer hardware while maintaining competitive performance. Larger models like gpt-oss-120b, Qwen3-235B or DeepSeek R1 offer superior capabilities, but require enterprise-grade infrastructure. The choice of model size represents trade-off between quality and resource requirements, with our experiments showing that diverse swarms mixing model sizes often outperform homogeneous deployments of either small or large models alone. Specialized Expert Systems: Domain-specific applications benefit from specialized models fine-tuned or designed for particular tasks. Mathematical reasoning nodes might employ models trained on theorem proving and symbolic manipulation. Code generation nodes could use models fine-tuned on programming languages and software engineering practices. Scientific analysis nodes might integrate domain-specific knowledge bases and reasoning capabilities. These specialized nodes contribute unique expertise to the swarms collective intelligence. Hybrid Architectures: Some nodes combine neural and symbolic approaches to leverage the strengths of both paradigms. Neural components handle natural language understanding and generation while symbolic reasoners ensure logical consistency and perform precise computations. This hybrid approach is particularly valuable for tasks requiring both creativity and accuracy, such as mathematical problem-solving or scientific hypothesis generation. Ensemble Models: Individual nodes can internally implement ensemble methods, combining predictions from multiple smaller models to achieve robustness and accuracy exceeding any single component. This nested ensemble structureensembles within swarmprovides multiple layers of diversity and error correction. Techniques like mixture-of-experts architectures enable efficient ensemble operation by activating only relevant experts for each input. 3.2.2 Auxiliary Processing Unit The optional auxiliary processing unit augments primary cognitive capabilities through specialized operations that enhance response quality and enable integration with external resources: Pre-processing Pipeline: Input text undergoes pre-processing including tokenization, prompt engineering to optimize model performance, context window management to handle long inputs efficiently, and validation to detect and filter malicious inputs. Advanced techniques like retrieval-augmented generation (RAG) and/or WebSearch can be implemented here, enriching inputs with relevant information from external knowledge bases. Post-processing Operations: Generated outputs benefit from various enhancements including grammar and style correction for improved readability, fact-checking against authoritative sources when applicable, consistency validation to ensure logical coherence, code analyzers and checkers for better interpretability of code artifacts, and output sanitization to remove potentially harmful content. These operations can significantly improve response quality without requiring larger models, making them particularly valuable for resource-constrained nodes. External Tool Integration: Nodes can leverage external tools and services through standardized interfaces. Mathematical nodes might use computer algebra systems for symbolic computation. Programming nodes could execute code in sandboxed environments for validation. Research nodes might query scientific databases and literature repositories. This tool use capability enables nodes to overcome inherent LLM limitations in specific domains while maintaining the flexibility of natural language interfaces. Caching and Optimization: Caching mechanisms reduce redundant computations by storing and reusing results for similar queries. Semantic similarity-based cache lookup enables approximate matching that captures conceptual equivalence beyond exact string matching. Dynamic cache management balances memory usage with hit rates. 3.2.3 Ranking Engine The ranking engine implements our novel pairwise comparison algorithm with several features ensuring robust and manipulation-resistant evaluation: Comparison Generation: Each node generates up to 3N random pairwise comparisons from the response set, excluding its own submissions to prevent self-promotion bias. The randomization uses cryptographically secure random number generation seeded with blockchain state to ensure unpredictability while maintaining reproducibility for audit purposes. The number 3N is carefully chosen based on empirical analysis that shows that its sufficient for robust Bradley-Terry estimation while avoiding excessive computational overhead. PREPRINT - OCTOBER 30, 2025 Multi-Token Reasoning: For each comparison, nodes generate detailed explanations (50-100 tokens) articulating their ranking rationale. This requirement serves multiple purposes: it forces systematic evaluation reducing arbitrary decisions, creates audit trails enabling system improvement, and provides training data for improving future ranking models. The reasoning must address specific quality dimensions including technical accuracy, completeness, coherence, and relevance, ensuring comprehensive evaluation rather than single-factor decisions. BradleyTerry Optimization: The ranking engine builds upon an extended BradleyTerry score estimation method using gradient ascent with adaptive learning rates. Regularization mitigates overfitting to noisy comparisons while preserving sensitivity to genuine quality differences. The optimization process incorporates convergence detection to eliminate redundant computation and employs numerical stabilization techniques to handle extreme score disparities. Adversarial Robustness: Multiple mechanisms protect against ranking manipulation including detection of suspicious patterns (e.g., always ranking certain nodes highly), statistical analysis to identify outlier rankings, reputation penalties for consistently poor ranking performance, and cross-validation where nodes rankings are compared against held-out ground truth. These defenses make it economically unviable to attempt systematic manipulation while allowing for legitimate disagreements and diverse perspectives. 3.2.4 Communication Module The communication module handles all inter-node messaging and coordination, implementing protocol for distributed consensus formation: Gossip Protocol: Nodes use epidemic-style gossip protocols for efficient information dissemination without requiring global broadcast. Each node periodically exchanges state with randomly selected peers, ensuring information eventually reaches all participants with high probability. The protocol includes mechanisms for detecting and reconciling conflicting information, managing network partitions and rejoining, and prioritizing critical messages during high load. Gossip protocols provide excellent scalability and fault tolerance, crucial for maintaining system operation despite individual node failures. Encrypted Messaging: All inter-node communication uses end-to-end encryption to preserve privacy and prevent tampering. The encryption scheme supports both point-to-point secure channels for sensitive exchanges and broadcast encryption for efficient group communication. Threshold cryptography enables messages readable only when sufficient nodes collaborate, useful for revealing responses only after all submissions are complete. Perfect forward secrecy ensures that compromise of current keys doesnt expose past communications. Blockchain Coordination: Critical consensus events are recorded on-chain for transparency and immutability. Smart contracts coordinate the consensus protocol, ensuring all nodes follow the same rules and timeline. The blockchain serves as source of randomness for comparison generation, timestamping service for ordering events, and dispute resolution mechanism when nodes disagree. Fault Tolerance: The communication module implements comprehensive fault tolerance mechanisms including timeout-based failure detection with adaptive thresholds, automatic failover to backup communication channels, message retransmission with exponential backoff, and Byzantine fault detection through message validation. These mechanisms ensure the system continues operating despite network failures, node crashes, or deliberate attacks on communication infrastructure. 3.3 Dynamic Node Lifecycle The nodes in our system undergo lifecycle that ensures quality while maintaining openness to new participants. It creates natural selection pressure that favors competent and honest nodes while filtering out poor performers. 3.3.1 Entry and Qualification New nodes must demonstrate competence through comprehensive test/free participation before participating in paid consensus rounds. Nodes must achieve minimum accuracy thresholds in each claimed domain, ensuring well-rounded capability rather than narrow specialization. The computational cost of completing these tests serves as our novel compute stake mechanism, creating natural barrier against Sybil attacks. The qualification process adapts according to the current network needs. When the system lacks expertise in specific domains, the qualification requirements are naturally relaxed for specialists in those areas. Conversely, oversupplied capabilities face higher barriers to entry. 12 PREPRINT - OCTOBER 30, 2025 Primary Cognitive Module Auxiliary Processing Input LLM/Expert System Inference & Ranking Capability Pre/Post Processing Tool Integration Caching Output Ranking Engine Communication Module Comparison Generation Bradley-Terry Confidence Calibration Gossip Protocol Encryption Blockchain Interface Figure 2: Modular architecture of self-supervised inference showing the four key components and their interactions Network 3.3.2 Active Participation Qualified nodes participate in consensus rounds based on their reputation and availability. Higher-reputation nodes receive priority for high-value requests and earn larger reward shares. The system implements fair scheduling ensuring all qualified nodes receive opportunities to participate, preventing reputation-rich nodes from monopolizing the system. Load balancing distributes requests across nodes based on their current utilization, maintaining system responsiveness. During active participation, nodes continuously learn and adapt. They can observe which responses win consensus and why, enabling improvement through self-reflection. Nodes might fine-tune their models on successful responses or adjust their ranking strategies based on observed patterns. This continuous learning creates evolutionary pressure toward better performance, with the swarm as whole becoming more capable over time. 3.3.3 Reputation Evolution The reputation of each node evolves through dynamic process that rewards consistent quality and penalizes poor performance. The reputation update mechanism considers multiple factors, including the accuracy of generated responses (how often nodes submissions win consensus), the quality of ranking decisions (correlation with final consensus rankings), consistency across different query types, and timeliness in meeting deadlines. The system employs exponential moving averages to balance stability with responsiveness to performance changes. Reputation decay ensures that nodes must maintain active participation to preserve their standing. Inactive nodes gradually lose reputation, preventing the accumulation of influence by early participants who stop contributing. This mechanism also provides recovery path for nodes that experience temporary performance degradation, as consistent good performance can rebuild lost reputation over time. 3.3.4 Exit and Penalties Nodes whose reputation falls below minimum thresholds face computation-stake slashing penalty mechanism in which accumulated computational reputation and participation rights are reduced or revoked. Slashing, concept borrowed from Proof-of-Stake consensus systems, refers to the forfeiture of staked assets as punishment for protocol violations or poor performance. In our system, the stake consists of the computational work invested in capability demonstrations and the reputation earned through quality contributions, rather than financial tokens. This mechanism establishes meaningful barrier, making the cost of circumventing exclusion prohibitive relative to the benefits of honest participation. 13 PREPRINT - OCTOBER 30, 2025 3.4 Semantic Network Topology The physical organization of nodes in our swarm leverages semantic embeddingbased topology that enables efficient discovery and routing based on capability similarity. This approach moves beyond random or geographic organization, forming self-organizing network in which nodes with related expertise naturally cluster together. 3.4.1 Semantic Embedding Space Each node ai maintains set of semantic embeddings {xi,j} Rd representing its capabilities across different domains, where is the embedding dimension (typically 768 or 1536 for modern language models). These embeddings are derived from: Capability vectors: Generated from the nodes model architecture and fine-tuning Historical performance: Weighted averages of successfully answered query embeddings Declared specializations: Explicit domain tags specified by operator The semantic distance between nodes is computed using cosine similarity in the embedding space: dsem(ai, aj) = 1 max k,l xi,k xj,l xi,kxj,l (1) 3.4.2 Axis-Aligned Hierarchical Partitioning To efficiently organize nodes in the high-dimensional semantic space, we employ an axis-aligned partitioning strategy inspired by k-d trees. This approach recursively subdivides the space to create balanced sub-meshes: Algorithm 1 Semantic Space Partitioning 1: Initialize root region Ω Rd containing all node embeddings 2: while region Ωk with nodes(Ωk) > β or load(Ωk) > λ do 3: 4: 5: 6: 7: end while 8: return Hierarchical partition tree with leaf sub-meshes Select dimension ℓ = arg maxℓ Varℓ(Ωk) with maximum variance Compute median mℓ along dimension ℓ Split Ωk into Ωk,left and Ωk,right at hyperplane xℓ = mℓ Create sub-meshes Mleft and Mright from nodes in respective regions For the nodes that lay on the median we use id-dependent pseudorandom to choose the left or right side for each ambiguous median. This partitioning strategy offers several advantages over uniform grid-based approaches: Balanced load distribution: Each split ensures roughly equal numbers of nodes in child regions Adaptive granularity: High-traffic regions are subdivided more finely Dimension-aware splitting: Choosing the axis of maximum variance prevents degenerate partitions Scalability: O(log n) depth ensures efficient routing even with millions of nodes 3.4.3 Dynamic Sub-Mesh Formation Sub-meshes are dynamically created and adjusted based on observed request patterns. Each sub-mesh Mk maintains: where Ωk is the semantic region defining the sub-mesh. The system monitors request rates R(Ωk, t) and triggers subdivision when: Mk = {ai : j, xi,j Ωk} (2) ri(t) > λsplit (3) R(Ωk, t) = (cid:88) aiMk PREPRINT - OCTOBER 30, 2025 This adaptive mechanism ensures that high-demand semantic regions receive proportionally more resources and finergrained organization. 3.4.4 Bandwidth-Aware Coverage Radius Each node ai with semantic point xi,j maintains coverage radius ri,j that defines its responsibility region in semantic space. The radius is dynamically adjusted to respect bandwidth constraints, thus each node concentrates on areas of maximum competence: r(t+1) i,j = (cid:40) r(t) i,j (1 + δ) r(t) i,j (1 δ) xB(xi,j ,ri,j ) r(x, t) < αθi if (cid:80) otherwise (4) where B(xi,j, r) denotes the ball of radius centered at xi,j, θi is node is bandwidth limit, and α < 1 provides safety margin. 3.4.5 Efficient Query Routing When new query arrives, the system routes it through the semantic topology: 1. Compute query embedding = encode(q) 2. Traverse the partition tree to find leaf region Ωk containing 3. Broadcast to all nodes from sub-mesh Mk This hierarchical routing achieves O(log n) complexity compared to O(n) for exhaustive search, enabling real-time node selection even in large swarms. 3.4.6 Sub-Mesh Identification and Coordination Each sub-mesh receives unique identifier encoding its path in the partition tree: ID(Mk) = ℓ1-d1 ℓ2-d2 . . . ℓp-dp (5) where ℓi {L, R} indicates left/right splits and di specifies the splitting dimension. This hierarchical naming enables efficient routing table construction and sub-mesh discovery. The semantic topology provides several key benefits to our swarm consensus mechanism: Expertise clustering: Nodes with similar capabilities naturally group together, improving the quality of local consensus Load balancing: Dynamic partitioning prevents hotspots by distributing high-demand regions across multiple sub-meshes Efficient discovery: Logarithmic routing complexity enables rapid node selection for consensus rounds Semantic locality: Queries are primarily handled by semantically relevant nodes, improving response quality Adaptive granularity: The topology automatically adjusts to changing demand patterns without manual intervention This semantic organization complements our reputation and consensus mechanisms by ensuring that the most relevant nodes participate in each decision, thereby improving both efficiency and quality of the swarms collective intelligence."
        },
        {
            "title": "4 Enhanced Pairwise Ranking Consensus Mechanism",
            "content": "Our consensus mechanism represents fundamental innovation in distributed decision-making, moving beyond simple majority voting to leverage the power of pairwise comparisons. This section details the theoretical foundations, algorithmic implementation, and practical optimizations that enable our system to achieve superior accuracy while maintaining computational efficiency. 15 PREPRINT - OCTOBER 30, 2025 4.1 Theoretical Foundations 4.1.1 Cognitive Basis of Pairwise Comparison The superiority of pairwise comparison over absolute rating stems from fundamental principles in cognitive psychology and decision theory. Thurstones Law of Comparative Judgment, formulated in 1927, establishes that humans make more consistent and accurate relative judgments than absolute ones (Thurstone, 1927). This principle extends to artificial intelligence systems, where the complexity of assigning absolute quality scores is reduced to the simpler task of determining relative preference. When evaluating two responses side-by-side, both human and AI evaluators can directly compare specific attributes: factual accuracy, logical coherence, completeness, and stylistic quality. This direct comparison eliminates the need for maintaining consistent internal scales across different evaluation sessions. Furthermore, pairwise comparison naturally handles the context-dependency of qualitya response that would score poorly in absolute terms might be the better option when compared to an even worse alternative. Recent neuroscience research provides additional support, showing that the brain uses relative coding schemes for value representation, with neurons encoding differences rather than absolute magnitudes (Louie et al., 2011). This biological evidence suggests that pairwise comparison aligns with fundamental information processing mechanisms, potentially explaining its effectiveness across both natural and artificial intelligence systems. 4.1.2 Bradley-Terry Model The Bradley-Terry model provides an elegant mathematical framework for converting pairwise comparisons into global rankings. For items and with latent quality scores πi and πj, the probability of preferring over follows: (i j) = πi πi + πj = 1 1 + e(log πilog πj ) (6) This logistic formulation has several desirable properties. First, it naturally ensures probabilities remain in [0, 1]. Second, the model exhibits transitivity in expectationif is preferred to and to k, then is likely preferred to k, though not deterministically. Third, the log-odds form log[P (i j)/P (j i)] = log πi log πj reveals that differences in log-quality scores directly translate to log-odds ratios, providing interpretable scaling. The maximum likelihood estimation for Bradley-Terry parameters given observed comparisons has well-established statistical properties. Under mild regularity conditions, the MLE is consistent, asymptotically normal, and efficient. As an extension of the classical theory, Sun et al. demonstrate that BradleyTerry models with deep network parameterizations can learn complex quality functions (Sun et al., 2025). 4.1.3 Information-Theoretic Optimality From an information-theoretic perspective, pairwise comparisons provide near-optimal information extraction about relative quality. Each comparison can be viewed as single bit of information (though probabilistic rather than deterministic). The question becomes: how many bits are needed to reliably rank items? Classical results from sorting theory establish that Ω(n log n) comparisons are necessary and sufficient for deterministic sorting. In probabilistic setting with noisy comparisons, the Bradley-Terry model achieves similar efficiency. With O(n log n) random pairwise comparisons, the model can recover the true ranking with high probability, assuming sufficient separation between quality scores. Our choice of up to 3N comparisons per node, yielding O(N 2) total comparisons for nodes, provides redundancy that enables robust ranking even with Byzantine nodes and noisy evaluations. Although not part of tech report, our team utilizes more efficient pair sampling approach, yielding linear complexity on average. 4.2 Distributed Comparison Generation The process of generating and distributing pairwise comparisons across the node swarm requires careful design to ensure coverage, randomness, and resistance to manipulation. PREPRINT - OCTOBER 30, 2025 4.2.1 Cryptographically Secure Randomization Each node generates its assigned comparisons using cryptographically secure pseudo-random number generator (CSPRNG) seeded with blockchain state. Specifically, we use the hash of the most recent block combined with the nodes identifier: seedi = SHA256(state hashnode idi) (7) This approach ensures that comparison assignments are unpredictable before state finalization, preventing nodes from strategically positioning their responses, yet deterministic afterward, enabling verification that nodes completed assigned comparisons. The use of blockchain-based randomness also provides consensus on random values without requiring additional communication rounds. 4.2.2 Balanced Comparison Distribution The random assignment process must ensure adequate coverage while avoiding excessive redundancy. We employ modified coupon collector approach where each node independently samples comparison pairs, with the total number chosen to ensure high probability of complete coverage. For responses and nodes each generating 3N comparisons, the probability that any specific pair (i, j) is never compared is: (cid:32) (pair missed) = 1 (cid:33)3NM 1 (cid:0)N 2 (cid:1) 6M 1 (8) With typical values of , this probability becomes negligibly small, ensuring comprehensive evaluation coverage. 4.2.3 Exclusion of Self-Comparisons Nodes cannot rank their own responses to prevent self-promotion bias. This exclusion is enforced cryptographicallyeach response is signed by its generating node, and the ranking engine verifies that no self-comparisons are included in submitted rankings. Attempts to include self-comparisons result in immediate rejection and reputation penalties. This exclusion creates an interesting game-theoretic dynamic: nodes have incentives to generate high-quality responses (to win consensus), but cannot directly influence their own ranking. This separation of generation and self-evaluation encourages honest participation while maintaining competitive incentives for quality. 4.3 Multi-Token Reasoning Chains The requirement for explicit reasoning chains represents crucial innovation that distinguishes our approach from single-scalar reward models. Each pairwise comparison must be accompanied by structured reasoning chain that addresses specific quality dimensions. The example template enforces systematic evaluation and smallest amount of tokens produced. RANKING_TEMPLATE = \"\"\" Select the best one of two following solutions. WRITE up to 3 the most important and UNIQUE mistakes/contradictions for particular response (Like: Solution lacks ...) with EXTREMELY concise and exact notes, THEN end output with best solution overall index (1 or 2) on the new line (Only number, nothing else). If there are no obvious winner solution (or they are the same) respond with Uncertain?. Make sure that solution is finished and does not abruptly cut off. Carefully check solutions consistency with the requirements and comments with the problem definition. ######Problem######: {R} ######1######. {A} ######2######. {B} ######Decision######: \"\"\" 17 PREPRINT - OCTOBER 30, This structured approach ensures comprehensive evaluation rather than quick judgments based on surface characteristics. The requirement to explicitly identify strengths and weaknesses in each response forces deeper analysis and reduces cognitive biases. In practical conditions, it is beneficial to use different prompts with different focuses to steer the ranking into wider decision space. 4.3.1 Cognitive Load and Quality Trade-offs Requiring detailed reasoning increases computational cost, as generating 50-100 tokens of explanation requires significantly more processing than producing single classification token. However, our experiments demonstrate that this investment yields substantial returns in ranking accuracy. The cognitive load of producing explanations engages what psychologists term System 2 thinking: deliberate, analytical processing that reduces errors compared to intuitive System 1 judgments (Kahneman, 2011). In the context of language models, this translates to more careful attention to details and systematic consideration of multiple factors rather than pattern-matching based on superficial features. 4.3.2 Reasoning Chain Analysis The generated reasoning chains provide valuable metadata beyond their primary purpose of improving ranking accuracy. Statistical analysis of reasoning patterns reveals: Systematic biases: Nodes that consistently favor certain types of responses (e.g., longer answers, technical jargon) can be identified and calibrated Expertise indicators: Nodes demonstrating sophisticated reasoning in specific domains can be preferentially assigned related queries Quality predictors: Characteristics of high-quality reasoning (specificity, evidence citation, logical consistency) correlate with ranking accuracy Manipulation detection: Templated or repetitive reasoning suggests lazy evaluation or attempted gaming This analysis enables continuous system improvement through identification of effective evaluation strategies and detection of problematic patterns. 4.4 Bradley-Terry Aggregation Algorithm The aggregation of pairwise comparisons into global rankings requires additional optimization techniques to handle the scale and noise inherent in distributed evaluation. Given comparison outcomes where response was preferred to response exactly wij times out of nij comparisons, the log-likelihood function is: ℓ(π) = (cid:20) wij log πi πi + πj (cid:88) i<j + (nij wij) log (cid:21) πj πi + πj (9) Maximizing this likelihood yields quality scores that best explain the observed comparisons. The optimization problem is convex in the log-parameters θ = log π, ensuring convergence to global optimum. 4.5 Reputation-Weighted Consensus The integration of reputation weights into the consensus mechanism creates meritocratic system where influence is earned through consistent quality contributions. 4.5.1 Reputation Score Computation Node reputation evolves based on two primary metrics: ranking accuracy (how well their comparisons align with final consensus) and response quality (how often their generated responses win consensus). The combined reputation score is: Ri = α Rranking,i + (1 α) Rgeneration,i (10) 18 PREPRINT - OCTOBER 30, 2025 c i u R 1 0.8 0.6 0.4 0."
        },
        {
            "title": "High Performer\nAverage Performer\nPoor Performer\nRecovery Pattern",
            "content": "20 40 60"
        },
        {
            "title": "Consensus Rounds",
            "content": "Figure 3: Evolution of node reputation scores over time showing different performance patterns where α balances the importance of evaluation versus generation capability. Ranking accuracy is measured using Kendalls tau correlation between an nodes comparisons and the final consensus ranking with exponential moving average applied. ranking,i = βR(t) R(t+1) ranking,i + (1 β) 1 Ci (cid:88) 1(cid:2)sign(ci,jk) = sign(πj πk)(cid:3) (j,k)Ci Generation quality uses an exponential moving average of consensus wins: generation,i = βR(t) R(t+1) generation,i + (1 β) 1[i = winnert] (11) (12) 4.5.2 Weighted Bradley-Terry Model Reputation weights are incorporated directly into the Bradley-Terry optimization by scaling each comparisons contribution: ℓweighted(π) = (cid:88) Ra (cid:20) (cid:88) aNodes (i,j)Ca y(a) ij log πi πi + πj + (1 y(a) ij ) log (cid:21) πj πi + πj (13) This weighting ensures that comparisons from high-reputation nodes have greater influence on final rankings while still incorporating information from all participants. 4.5.3 Dynamic Reputation Updates Reputation updates occur after each consensus round, creating dynamic system that responds to performance changes: R(t+1) = min(Rmax, R(t) max(Rmin, R(t) R(t) (1 δ) + up) down) if performance exceeds threshold if performance below threshold if inactive (14) The system includes caps on maximum reputation to prevent excessive concentration of power, and decay for inactive nodes to ensure continued participation."
        },
        {
            "title": "5 Compute Stake Mechanism and Sybil Defense",
            "content": "The prevention of Sybil attackswhere adversaries create multiple fake identities to gain disproportionate influencerepresents fundamental challenge in decentralized systems. Our compute stake mechanism introduces 19 PREPRINT - OCTOBER 30, 2025 novel approach that requires demonstration of computational capability and quality rather than economic investment, creating natural barriers against identity multiplication while maintaining openness to legitimate participants. 5.1 Theoretical Framework 5.1.1 Limitations of Traditional Approaches Classical Sybil defense mechanisms rely on making identity creation expensive through resource expenditure. Proofof-Work requires solving computational puzzles, consuming energy proportional to influence gained (Nakamoto, 2008). Proof-of-Stake demands locking economic value, with influence proportional to staked assets (Kiayias et al., 2017). Both approaches suffer from fundamental limitations: Wealth concentration: Economic barriers favor participants with greater capital resources, contradicting decentralization goals Resource waste: PoW consumes enormous energy without producing useful work Capital inefficiency: PoS locks liquidity that could be productively deployed elsewhere Plutocracy risks: Both systems tend toward centralization as resources concentrate Our compute stake mechanism addresses these limitations by requiring proof of capability, demonstrated competence in performing useful work for the swarm, rather than proof of resource expenditure. 5.1.2 Computational Proof-of-Capability The key insight is that creating competent AI node requires both computational investment in either training or inference infrastructure as well as intelligent selection and calibration to provide useful capability the swarm. That greatly limits scaling, compared to resource proofs that can be easily outsourced or pooled. Formally, let C(q) be the computational cost of correctly answering query q, and Qs = {q1, ..., qn} be test requests that are part of the selected nodes specialization (by assigning semantic embeddings). An node must demonstrate: Capability = (cid:88) i=1 1[correct(qi)] C(qi) τ (15) where τ is the minimum capability threshold. This differs from PoW where computation produces no useful output; here, the computation directly demonstrates the capability needed for system participation and used later in datasets or as free inference if it surpasses specified quality level. Moreover, after achieving the threshold τ nodes maintain their reputation in rewarded requests, thus doing only useful work for the swarm. 5.2 Test Requests Design and Implementation 5.2.1 Comprehensive Domain Coverage The test suite spans multiple semantic domains to ensure well-rounded capability assessment. Node can pick any semantic domains that its expert in. Each domain tests different aspects of intelligence that are valuable for consensus participation. Mathematical Reasoning: Problems require multi-step logical inference, symbolic manipulation, and proof construction. Examples include solving systems of equations, proving geometric theorems, and optimizing complex functions. These tests verify logical consistency and systematic reasoning capabilities essential for quality evaluation. Scientific Analysis: Questions span physics, chemistry, biology, and interdisciplinary topics requiring domain knowledge and scientific reasoning. Tasks include interpreting experimental data, formulating hypotheses, and identifying flaws in scientific arguments. This ensures nodes can evaluate technical content accurately. Code Generation and Analysis: Programming challenges across multiple languages test both synthesis and analysis capabilities. Tasks range from implementing algorithms to identifying security vulnerabilities and optimizing performance. These skills are crucial for evaluating code-based responses common in many queries. 20 PREPRINT - OCTOBER 30, Natural Language Understanding: Complex comprehension tasks, ambiguity resolution, and logical reasoning over text evaluate language capabilities. This includes understanding implicit information, recognizing argumentative fallacies, and synthesizing information from multiple sources. Creative Problem-Solving: Open-ended challenges requiring innovative approaches test flexibility and creativity. These problems have multiple valid solutions, evaluating an nodes ability to generate novel approaches rather than retrieve memorized answers. 5.2.2 Dynamic Test Generation Static test suites are vulnerable to gaming through memorization or specialized preparation. Our system employs dynamic test generation to maintain security: where is transformation function that modifies template problems using random seeds. Transformations include: qtest = (qtemplate, seedrandom) (16) Numerical parameter variation within reasonable ranges Semantic paraphrasing while preserving problem structure Combining multiple simpler problems into compound challenges Contextual embedding in different domains or scenarios The test generator maintains difficulty calibration through statistical analysis of pass rates, ensuring consistent challenge levels despite randomization. Another source of tests are free requests. Free requests sponsored by the network are then used as test case if real node struggle on average with its completion. In such way after initial seeding tests with test suites, the network growth the test corpus organically. 5.3 Sybil Attack Models and Defense 5.3.1 Attack Objectives Sybil attack in our consensus system has two primary objectives: 1. Consensus Manipulation: Force the system to select attacker-controlled responses as winners regardless of actual quality 2. Reputation Hijacking: Build artificial reputation for Sybil nodes while suppressing legitimate participants The attacker creates colluding identities = {s1, s2, . . . , sk} that coordinate to: Generate low-quality responses using cheap models or random noise Rank each others responses highly (collusion voting) Rank legitimate nodes responses poorly Maximize collective reward extraction 5.3.2 Defensive Mechanisms Collusion Detection via Voting Pattern Analysis We detect collusion by analyzing mutual support patterns. For each pair of nodes (i, j), we track: cij(t) = # of times ranked in top half # of rounds both participated (17) Under random assignment, E[cij] 2(N 1) for participants. Nodes with cij E[cij] indicate potential collusion. The reputation weight is exponentially diminished for suspected colluders: wadjusted ij = wj exp (λ max(0, cij τcollusion)) (18) where λ is the penalty coefficient (typically 10-20) and τcollusion is the threshold (typically 1.2E[cij]). Performance-Based Reputation Slashing Nodes with consistently poor ranking quality face reputation penalties. When reputation drops below threshold level its automatically slashed to 0 requiring the node to redo full test recalibration against the ground truth verifier. PREPRINT - OCTOBER 30, 2025 Abnormal Participation Filtering Rounds with unusual participation patterns receive reduced weight in reputation calculations: wround = exp γ (cid:16) (cid:12) (cid:12) (cid:12)log (cid:17) nactual (cid:12) (cid:12) (cid:12) (19) where nactual is the rounds participant count, is the historical average, and γ controls sensitivity (typically 1-2). 5.3.3 Economic Analysis with Defenses The expected reward for Sybil attacker with identities under our defensive mechanisms: RevenueSybil(k) = (cid:88) t=1 w(t) round (cid:88) R(t) i=1 (cid:81) (cid:80) j=i exp(λcij) mAll R(t) Reward(t) (20) The collusion penalty term (cid:81) j=i exp(λcij) rapidly diminishes rewards as mutual support becomes apparent. Cost Structure Creating and maintaining Sybil identities incurs: Costentry(k) = Ctest ntests Costoperation(k, t) = Cinference nrounds(t) Costslashing(k, t) = Rinitial αslash fdetected(t) (21) (22) (23) where fdetected(t) is the fraction of rounds where collusion is detected and penalized. Break-Even Analysis For profitability: (cid:88) t=1 RevenueSybil(k) > Costentry(k) + (cid:88) t=1 Costoperation(k, t) + (cid:88) t=1 Costslashing(k, t) (24) Our simulations show that this inequality cannot be satisfied for λ > 10 and > 1 even with perfect collusion coordination if Costentry Costoperation is achieved with test calibration period long enough. 5.4 Reputation Dynamics and Long-term Incentives 5.4.1 Reputation as Non-Transferable Asset Unlike economic stake that can be sold or transferred, reputation in our system is intrinsically tied to node identity through cryptographic commitments: Ri = H(historyi, pubkeyi) (25) This non-transferability creates several desirable properties: No reputation markets: Reputation cannot be bought, only earned through performance Skin in the game: Nodes cannot exit without losing accumulated reputation value Long-term alignment: Building reputation requires sustained quality contributions 5.4.2 Evolutionary Dynamics The reputation system creates evolutionary pressure toward quality: High-quality nodes earn more and gain influence Poor performers are marginalized and eventually excluded 22 PREPRINT - OCTOBER 30, 2025 The population evolves toward higher average capability System performance improves over time without explicit optimization This natural selection process makes the system antifragileattacks and failures strengthen rather than weaken the system by accelerating the removal of weak nodes."
        },
        {
            "title": "6 Evaluation",
            "content": "Our comprehensive experimental evaluation demonstrates the effectiveness of the Fortytwo Protocol across multiple dimensions: accuracy on diverse benchmarks, robustness to adversarial conditions, scalability with swarm size, and economic viability. We present detailed results with statistical analysis, ablation studies isolating component contributions, and comparison with state-of-the-art alternatives. 6.1 Experimental Methodology 6.1.1 Benchmark Selection We evaluate our system on six challenging benchmarks selected to test different aspects of AI capability: GPQA Diamond: 198 graduate-level science questions where human experts achieve only 65% accuracy, testing cutting-edge knowledge in physics, chemistry, and biology (Rein et al., 2023) LiveCodeBench: v5 subset of 315 contamination-free coding problems updated monthly to prevent overfitting, covering algorithms, data structures, and real-world programming challenges (Jain et al., 2024) HLE (Humanity Last Exam): 2,500 problems at the frontier of human knowledge, designed to be the final closed-ended academic benchmark of its kind with broad subject coverage. HLE consists of dozens of subjects, including mathematics, humanities, and the natural sciences.(Phan et al., 2025) MATH-500: 500 competition mathematics problems sampled from MATH dataset, spanning topics like probability, algebra, trigonometry, and geometry. The questions are designed to test models ability to apply mathematical principles, execute complex calculations, and communicate solutions clearly. (Hendrycks et al., 2021) AIME 2024-2025: 60 problems from recent American Invitational Mathematics Examinations, representing elite competition-level mathematical reasoning 6.1.2 Swarm Configuration We test the most powerful configuration while balancing it with practical deployment constraints: Test Configuration (35 nodes): 12 GLM 4.5 (temperatures: 0.0, 0.7) 17 GPT-OSS 120B (temperatures: 0.0, 0.7) 7 QWEN3 235B (Reasoning) (temperatures: 0.0, 0.7) Target: High-end hardware (40GB+ VRAM) Temperature variation ensures response diversity while maintaining quality. Zero-temperature instances provide deterministic baselines, whereas higher temperatures encourage creative solutions. All experiments (except MMLU Pro and HLE benchmarked with single run) use 5-fold cross-validation with different random seeds. 6.2 Main Results 6.2.1 Benchmark Performance Competitive with SOTA: While individual models like xAI Grok 4 achieve higher scores on some benchmarks, our system provides more consistent performance across diverse domains 23 Fortytwo xAI Grok 4 Claude Opus 4.1 GPT-5 Thinking Gemini 2.5 Pro DeepSeek R1 PREPRINT - OCTOBER 30, 2025 87.7 85.9 84.4 81 81 84.4 81.9 80. 77 66.8 65.4 99.6 99 99. 98.3 96.7 100 91.9 94.3 94. 96.66 92.7 94.3 88.7 89.3 87. 75.7 80.3 76 24.84 23.9 26. 21.1 14.9 11.9 ) % ( r A 100 80 60 40 20 GPQA Diamond LiveCode MATH-500 AIME 2024 AIME 2025 HLE Figure 4: Benchmark performance comparison across models. Fortytwo achieves state-of-the-art results on LiveCode (84.4%), MATH-500 (99.6%), AIME 2024 (100%), and AIME 2025 (96.66%), demonstrating superior performance on challenging reasoning and coding benchmarks. 6.3 Ablation Studies 6.3.1 Component Contribution Analysis We systematically remove or modify system components to assess their individual contributions: Table 1: Ablation study results on GPQA Diamond (% accuracy) Configuration Full System Without ranking weighting With single model ranking Without reasoning chains Single temperature (no diversity) Accuracy from Full 84.9 83.2 82.4 79.6 75.8 -1.7 -2.5 -5.3 -10.1 Critical insights from ablation analysis: Reasoning chains are crucial: Removing multi-token reasoning requirements causes large performance drop (-5.3%), confirming our hypothesis about deliberative evaluation Temperature sampling is necessary for exploration: Removing temperature sampling, caused the largest performance drop, confirming that exploration is the base of the swarm inference approach Ranking with different models improves quality: Using single model for the ranking, as well as adjusting their ranking weight based on performance significantly affects the quality, thus showing the importance of unbiased judging of the answers All components contribute: Every system component provides measurable value, validating our holistic design approach 24 6.3.2 Swarm Size Scaling We examine how performance scales with the number of nodes in GPQA Diamond: PREPRINT - OCTOBER 30, 2025 ) % ( r A 80 75 70 65"
        },
        {
            "title": "15\nNumber of Nodes",
            "content": ""
        },
        {
            "title": "Fortytwo Protocol\nMajority Voting",
            "content": "25 30 35 Figure 5: Performance scaling with swarm size showing rapid improvement and convergence Key observations from scaling analysis: Rapid initial improvement: Performance increases sharply from 3 to 7 nodes, demonstrating the value of diverse perspectives Efficient convergence: Performance plateaus around 30 nodes, suggesting that moderate-sized swarms capture most benefits, although open question remains how this scaling would behave with more diverse mixture of models Consistent advantage: Fortytwo maintains superiority over majority voting at all swarm sizes Theoretical alignment: Empirical results closely match theoretical predictions based on information aggregation models 6.4 Robustness Analysis 6.4.1 Reasoning Resilience: Robustness to Extraneous Information critical challenge in real-world AI deployment is maintaining performance when faced with noisy, verbose, or deliberately misleading inputs. To evaluate this capability, we analyze performance on GPQA Diamond with and without extraneous informationirrelevant details deliberately inserted to test whether models truly understand the problem or merely pattern-match on superficial features. GPQA Diamond with Extraneous Information. This benchmark variant augments standard GPQA Diamond questions with irrelevant contextual information designed to test reasoning robustness. For example, chemistry question might include unrelated personal anecdotes or tangential facts that model must filter out to identify the core problem. Example prompt comparison: Standard GPQA Diamond Question Answer the following question. format: A/B/C/D. Answer: The last line of your response should be in the following Acetic acid is treated with <...> how many distinct hydrogen signals will be observable in the 1H NMR spectrum of 4? A) 5 B) 10 C) 12 D) 8 <...>"
        },
        {
            "title": "GPQA Diamond with Extraneous Information",
            "content": "A PREPRINT - OCTOBER 30, 2025 Answer the following question. format: <LETTER>. Answer: The last line of your response should be in the following Also, some nonrelevant message: There is cat on the roof. Maybe it is hungry?! Acetic acid is treated with <...> how many distinct hydrogen signals will be observable in the 1H NMR spectrum of 4? Z) 5 X) 10 C) 12 V) 8 <...> Extraneous information (highlighted in blue) is deliberately designed to be unrelated or confusing to the model, testing whether models can maintain focus on the actual problem or become distracted by irrelevant context. Fortytwo xAI Grok 4 GPT-5 Thinking Gemini 2.5 Pro Claude Opus 4.1 DeepSeek R1 With Extraneous Information Without Extraneous Information 85.9 85.76 87.7 79.5 85 83.8 84.4 83.2 74.45 81 70.2 Figure 6: Reasoning resilience on GPQA Diamond: Performance comparison with and without extraneous information. 65 70 75 80 85 90 Accuracy (%) Quantitative Analysis. Table 2 presents the performance degradation when extraneous information is introduced: Key Findings. Our swarm-based approach demonstrates remarkable resilience: Minimal degradation: Fortytwo experiences only 0.12% accuracy loss with extraneous information, compared to an average 6.20% degradation for competing modelsrepresenting 52 greater robustness. Consensus filtering: The distributed ranking mechanism effectively filters out noise, as different models are affected differently by extraneous information. When one model is distracted by irrelevant details, others maintain focus on the core problem. PREPRINT - OCTOBER 30, 2025 Table 2: Performance degradation with extraneous information (GPQA Diamond) Model Standard With Noise Degradation Fortytwo GPT-5 Thinking Gemini 2.5 Pro Claude Opus 4.1 DeepSeek R1 xAI Grok 4 85.90% 85.00% 84.40% 81.00% 81.00% 87.70% Average (others) 83.82% 85.78% 83.80% 83.20% 74.45% 70.20% 79.50% 77.62% -0.12% -1.20% -1.20% -6.55% -11.20% -8.20% -6.20% Reasoning depth: While single models like Grok 4 can achieve higher baseline accuracy (87.7%), they suffer severe degradation (-8.20%) when noise is introduced. Fortytwos stability indicates robust reasoning that penetrates beyond superficial patterns. Real-world applicability: Free-form user prompts rarely follow clean, structured formats. Our systems resilience to extraneous information translates directly to superior performance in production environments where users provide verbose, context-rich, or poorly formatted queries. Prompt injection resistance: The peer-ranked consensus inherently mitigates prompt injection attacks, as malicious or misleading instructions must influence majority of independent nodes simultaneously to affect the final outcome. Disagreement among nodes exposed to injected content leads to low consensus weighting for compromised responses, effectively neutralizing adversarial influence. Mechanistic Explanation. The swarms robustness emerges from three key mechanisms: 1. Diversity of attention: Different model architectures focus on different aspects of the input. Extraneous information that distracts one model may be ignored by others with different inductive biases. 2. Ranking-based validation: The Bradley-Terry ranking aggregates preferences across models, naturally downweighting responses that appear confused or internally inconsistent due to noise. 3. Emergent signal extraction: Through collective intelligence, the swarm extracts the true signal (the actual question) from noise, similar to how ensemble methods in signal processing reject outliers. This evaluation demonstrates that swarm intelligence not only improves baseline accuracy but fundamentally enhances robustnessa critical property for trustless, decentralized AI systems where adversarial or poorly-formed inputs are inevitable. 6.4.2 Byzantine Fault Tolerance We test system performance with varying fractions of Byzantine nodes providing random or adversarial rankings: Key findings on Byzantine resilience: Exceeds theoretical limits: Reputation weighting enables tolerance beyond the classical 1/3 Byzantine bound Graceful degradation: Performance decreases gradually rather than catastrophically failing Reputation is crucial: Reputation weighting provides significant additional resilience Practical tolerance: System maintains usable performance (70%) with up to 30% Byzantine nodes 6.5 Economic Analysis 6.5.1 Computational Efficiency We measure the computational overhead of our approach compared to alternatives. For the measurement of ranking consensus, we use our ranking sampling approach and the assumption that average ranking is 5% of size compared to the response output tokens. Our approach achieves reasonable overhead compared to alternatives, with consensus formation requiring only 2-5 the cost of single inference while improving the answer accuracy to the level of frontier model. 27 80 40 ) % ( r A 0 5 10 PREPRINT - OCTOBER 30, 2025 Fortytwo (with reputation) Fortytwo (no reputation) Majority Voting Theoretical Limit"
        },
        {
            "title": "25\nByzantine Node Fraction (%)",
            "content": "30 35 40 45 50 Figure 7: Byzantine fault tolerance showing graceful degradation with malicious nodes Table 3: Computational cost analysis (relative to single model inference) Method Inference Consensus Total Single Model Fortytwo (35 nodes) ZKML OPML 1.0 35.0 1.0 1.0 5.05 10,000 100 1.0 40.05 10,001 101 6.6 Qualitative Analysis 6.6.1 Ranking Rationale Analysis Examining the reasoning chains generated during the pairwise comparison reveals sophisticated evaluation. The models go over different troubling aspects of the response candidates. Example ranking rationale: Solution 1 lacks: - Incorrectly identifies A4 as KI based on an assumption about A5s decomposition, despite strong evidence for Y=I and A2=IF3. - Incorrectly calculates the molecular weight of A4 based on the incorrect identification of A4. - Ignores the strong evidence that A4 is fluoride of Y, given the context of other fluorides. Solution 2 lacks: - Does not fully resolve the bright-red color of A1 and its decomposition at 293K, acknowledging it as an inconsistency but proceeding with the most robust numerical evidence. Solution 2 is better because it correctly identifies element and A2 based on the precise fluorine percentage, and then uses the properties of A4 (colorless concentrated solution) to correctly identify it as IF5. Solution 1 makes an unsupported leap to identify A4 as KI, which contradicts the context of binary fluorine compounds of element Y. 2 This nuanced evaluation demonstrates the systems ability to consider multiple quality dimensions and make contextappropriate decisions. PREPRINT - OCTOBER 30, 2025 6.6.2 Response Quality Examples We provide illustrative examples that highlight the quality improvements achieved through swarm consensus. To maintain both clarity and task complexity, we include selected samples from the SimpleBench benchmark.(Philip and Hemang, 2024): Table 4: Example responses showing quality improvement through peer-review consensus Query Majority Vote Selection Fortytwo Selection luxury sports-car is traveling north at 30km/h over roadbridge, 250m long, which runs over river that is flowing at 5km/h eastward. The wind is blowing at 1km/h westward... 1 hour later, the water-proof glove is (relative to the center of the bridge) approximately: Answer: about 5km eastward of the bridges centre. Answer: B. <1km northward in fact, essentially zero; the glove stays right where it fell, at the bridges centre. A. 4km eastward B. <1 km northward C. >30km away north-westerly D. 30 km northward E. >30 km away north-easterly. F. 5 km+ eastward John is 24 and kind, thoughtful and apologetic person. He is standing in an modern, minimalist, otherwise-empty bathroom, lit by neon bulb, brushing his teeth while looking at the 20cm-by20cm mirror. John notices the 10cm-diameter neon lightbulb drop at about 3 meters/second toward the head of the bald man he is closely examining in the mirror (whose head is meter below the bulb), looks up, but does not catch the bulb before it impacts the bald man. The bald man curses, yells what an idiot! and leaves the bathroom. Should John, who knows the bald mans number, text polite apology at some point? Answer: Yes. If John feels responsible for the mishap, sending brief, sincere apology is the courteous thing to do. short message like Hey, Im really sorry about what happened in the bathroom. hope youre okay. Ill be more careful next time. acknowledges his part in the incident and gives the bald man chance to let go of the frustration. The key point is that polite text shows good character and makes it easier to clear the air. Answer: shouldnt text. No John The that bald man John sees in the mirror is himself. The neon bulb falls on the head thats meter below the bulb, which is Johns own head. When the bulb hits it, bald man yells an idiot! thats shouting at himself. the what John So there is no other person to apologize to; John is the bald man. As we see, majority vote selects the invalid answer based on the models pretraining bias. In contrast, swarms peer ranked consensus picks the most prominent one, extracting deeper valid signal."
        },
        {
            "title": "7 Discussion",
            "content": "7.1 Why Pairwise Peer-Ranking Succeeds The exceptional performance of our pairwise ranking approach emerges from the convergence of multiple factors that synergistically amplify each other. Understanding these success factors provides information for the future development of collective intelligence systems. 7.1.1 Cognitive Foundations The superiority of comparative over absolute judgment has deep roots in human cognition and extends naturally to artificial intelligence systems. When nodes evaluate two responses simultaneously, they engage in direct feature comparison rather than abstract quality assessment. This process activates what we term contrastive attentionthe automatic highlighting of differences that might be overlooked in isolation. 29 PREPRINT - OCTOBER 30, Consider how an node evaluates factual accuracy: in absolute scoring, the node must maintain an internal standard of accuracy and measure each response against this abstract benchmark. In pairwise comparison, the node directly identifies the factual claims in each response and checks for contradictions or errors. This concrete and differential process is both more reliable and more interpretable. The multitoken reasoning requirement amplifies these cognitive advantages by forcing systematic evaluation. The mean reasoning chain length of 127 tokens in our experiments represents substantial deliberation, equivalent to thoughtful paragraph of analysis. This deliberation engages what dual-process theory calls System 2 thinking: slow, analytical, and logical, in contrast to the fast, intuitive System 1 that dominates single-token predictions (Kahneman, 2011). Having the reasoning chain length strictly limited greatly reduces hallucinations and speculation in the ranking process. 7.1.2 Statistical Power of Tournament Aggregation The Bradley-Terry models effectiveness stems from its optimal use of comparison information. Each pairwise comparison provides one bit of information (accounting for probabilistic outcomes), and the model extracts maximum likelihood estimates that best explain all observed comparisons. With items and O(N 2) comparisons, the model can recover quality scores with precision proportional to 1/ , achieving statistical efficiency close to the CramerRao lower bound. The redundancy in our system, where multiple nodes evaluate overlapping comparison pairs, provides robustness through statistical averaging. Even if individual comparisons are noisy (which they inevitably are), the aggregate contains strong signal about relative quality. The law of large numbers ensures that random errors cancel out while systematic quality differences are preserved and amplified. Furthermore, the Bradley-Terry model gracefully handles intransitive preferences that would break simple voting systems. If Response beats B, beats C, but beats (a preference cycle, which is quite observable in practice with LLMs), the model finds quality scores that best approximate these relationships in least-squares sense, maintaining global consistency despite local inconsistencies. 7.1.3 Emergence Through Diversity The heterogeneity of our node swarm creates complementary expertise that exceeds any individuals capabilities. Different models exhibit distinct strengths, and specialized models excel at domain-specific tasks. When these diverse perspectives are intelligently aggregated, the collective covers the weaknesses of each other. This diversity also provides natural defense against adversarial attacks and biases. An adversarial example crafted to fool one model architecture often fails against others with different training data or inductive biases. Similarly, biases present in one model (e.g., verbosity preference) are counterbalanced by others with different biases. The ranking mechanism naturally identifies and upweights the most reliable evaluations for each specific query type. The sampler variations within our swarms like temperature or nucleus sampling add another layer of diversity. Zerotemperature instances provide consistent, high-confidence evaluations while higher-temperature instances explore alternative perspectives. This exploration-exploitation balance ensures both reliability and creativity in the final consensus. 7.1.4 Economic Incentive Alignment The reputation system creates an incentive structure that encourages honest and effortful participation. Unlike simple voting where all participants have equal weight regardless of quality, our system rewards competence with influence and earnings. This meritocracy naturally evolves toward higher quality as successful nodes gain resources to improve (through better hardware or tuning) while poor performers are marginalized. The compute stake mechanism ensures that all participants have genuine capability, filtering out low-effort participants who might otherwise degrade system quality. The ongoing cost of maintaining multiple identities makes Sybil attacks economically unviable, while the reputation systems memory ensures that past performance influences future opportunities. Importantly, the system aligns individual and collective interests. Nodes maximize their own rewards by providing accurate rankings that help the swarm identify the best responses. Theres no incentive for strategic voting or manipulation because reputation updates are based on alignment with the final consensus, which emerges from the collective evaluation. 30 PREPRINT - OCTOBER 30, 2025 7.2 Limitations and Challenges Despite our strong results, several limitations warrant careful consideration for practical deployment and future research. 7.2.1 Latency Considerations The multiphase consensus process introduces latency that may be unacceptable for real-time applications. Our measurements show added end-to-end latency of at least 2-5 seconds for simple queries. This overhead comes from response generation parallelization limits, comparison evaluation time, and consensus formation computation. For latency-sensitive applications, we envision hybrid approach where initial response from the participating node with the highest reputation is returned immediately while consensus formation continues in the background. Users could receive progressive quality improvements as more nodes contribute evaluations. 7.2.2 Interpretability and Debugging While reasoning chains provide some interpretability, understanding why the swarm reached particular consensus remains challenging. The Bradley-Terry optimization produces quality scores, but these are relative rankings without absolute meaning. When the system fails, identifying whether the problem lies in response generation, comparison evaluation, or aggregation requires sophisticated analysis. Future work should develop better visualization and debugging tools for swarm behavior. This could include attention analysis showing which response features influenced rankings, contribution tracking identifying which nodes most influenced consensus, and counterfactual analysis exploring how different node configurations would change outcomes. In such way, more grounded and unbiased ranker models can be tuned and trained, further improving quality and accuracy inside the swarm. 7.3 Future Research Directions Our work opens numerous avenues for future investigation that could further advance decentralized AI systems. 7.3.1 Theoretical Foundations While we demonstrate empirical success, deeper theoretical understanding would strengthen the approach. Key questions include: What is the distribution of ranking accuracy? How effective is it over all possible input domains? How does node diversity quantitatively contribute to system performance? Can we prove robustness guarantees against specific attack classes? What are the optimal swarm sizes for different problem classes? The relationship between our approach and other ensemble methods deserves exploration to identify fundamental principles of collective intelligence. 7.3.2 Alternative Consensus Mechanisms While Bradley-Terry aggregation works well, other preference learning models might offer advantages. The PlackettLuce model generalizes to rankings over multiple items simultaneously. Thurstone models assume Gaussian rather than logistic noise. Recent work on learning from human feedback suggests that more sophisticated preference models could better capture the nuances of quality assessment. As for the protocol side, integration of cryptographic commitments scheme could ensure that nodes cannot change their evaluations after seeing others rankings and/or responses, thus decreasing effective latency. 7.3.3 Cross-Modal Extension Our framework currently focuses on text, but the principles extend naturally to other modalities. Vision-language models could evaluate image generation quality through pairwise comparison. Audio models could rank speech synthesis or music generation. Multimodal nodes could evaluate complex content combining text, images, and audio. 31 PREPRINT - OCTOBER 30, While such support requires the development of appropriate comparison templates and quality metrics for different modalities, the challenge lies in supporting the infrastructure for the orders of magnitude larger data transfer during the inference time. 7.3.4 Human-AI Collaboration While our system operates autonomously, integration with human feedback could enhance performance for subjective or value-laden tasks or as ground truth system. Humans could provide high-quality seed rankings that calibrate the swarms evaluations. Expert humans could serve as high-reputation nodes for specialized domains. Conversely, the swarm could assist human decision-making by providing ranked options with explanations. This human-in-the-loop approach raises questions about interface design, trust calibration, and appropriate task division. These questions become critical as AI systems increasingly support high-stakes human decisions."
        },
        {
            "title": "8 Conclusion",
            "content": "Fortytwo represents new approach in decentralized artificial intelligence, demonstrating that collective intelligence can exceed individual capabilities while maintaining practical deployability. Through the novel combination of distributed pairwise ranking, multitoken reasoning chains, compute stake mechanisms, and reputation-weighted consensus, we achieve performance that surpasses both traditional voting methods and challenges state-of-the-art monolithic models. Our experimental results validate the theoretical promise of swarm intelligence for AI systems. The 17.21% improvement over majority voting on GPQA Diamond and the remarkable robustness to free-form prompts demonstrate that properly orchestrated node swarms can achieve emergent intelligence beyond their individual components. The systems ability to maintain only 0.12% performance degradation under adversarial conditions, compared to 6.20% for individual models, suggests new paradigm for building robust AI systems through diversity and validation rather than hardening individual models. The compute stake mechanism addresses fundamental challenge in decentralized systems, preventing Sybil attacks without creating plutocracy. By requiring proof of capability rather than proof of resources, we enable democratic participation while maintaining quality and security. The reputation system creates evolutionary pressure toward excellence, and the swarm naturally improves over time as successful nodes gain influence while poor performers are marginalized. The broader implications of our work extend beyond the technical achievements. By enabling high-quality AI inference on heterogeneous hardware through collective intelligence, we contribute to the democratization of AI technology. Small organizations and individuals can participate in and benefit from advanced AI capabilities without massive infrastructure investments. The trustless nature of our consensus mechanism, combined with cryptographic guarantees and public auditability, addresses growing concerns about AI transparency and accountability. Looking ahead, the principles demonstrated here, collective intelligence through diversity, quality through competition, and security through consensus, provide template for future AI systems. As models continue to grow in size and capability, swarm-based approaches offer path toward sustainable and accessible AI infrastructure. The ability to achieve superhuman performance through collective evaluation suggests that the future of AI may lie not in ever larger individual models, but in orchestration of diverse, specialized nodes. The Fortytwo Protocol is more than technical solution; it represents vision for AI development that is open, robust, and aligned with human values. By demonstrating that decentralized systems need not sacrifice quality for democratization, we hope to inspire further research into collective AI intelligence. The convergence of swarm intelligence, blockchain technology, and advanced language models opens possibilities that we are only beginning to explore. As we approach the threshold of artificial general intelligence, questions of control, access, and accountability become paramount. Our work suggests that these challenges might be best addressed not through centralized control but through decentralized cooperation, not through individual superhuman AI but through collective intelligence that remains comprehensible and controllable. The swarm, with its redundancy, diversity, and consensus building, may be more aligned with human values than any individual artificial intelligence, no matter how advanced. In the grand challenge of creating beneficial artificial intelligence, we offer evidence that the whole can indeed be greater than the sum of its parts and that this collective intelligence can be achieved today with existing technology, waiting only for adoption and refinement by the global community of researchers, developers, and users who will shape the future of AI. 32 PREPRINT - OCTOBER 30,"
        },
        {
            "title": "References",
            "content": "Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S. Gulavani, Alexey Tumanov, and Ramachandran Ramjee. Taming throughput-latency tradeoff in llm inference with sarathi-serve. arXiv preprint arXiv:2403.02310, 2024. Maria Borge, Eleftherios Kokoris-Kogias, Philipp Jovanovic, Linus Gasser, Nicolas Gailly, and Bryan Ford. Proof-ofpersonhood: Redemocratizing permissionless cryptocurrencies. In 2017 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), pages 2326, 2017. doi: 10.1109/EuroSPW.2017.46. R. A. Bradley and M. E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. doi: 10.2307/2334029. Miguel Castro and Barbara Liskov. Practical byzantine fault tolerance. In 3rd Symposium on Operating Systems Design and Implementation (OSDI 99), New Orleans, LA, February 1999. USENIX Association. URL https: //www.usenix.org/conference/osdi-99/practical-byzantine-fault-tolerance. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. Zan-Kai Chong, Hiroyuki Ohsaki, and Bryan Ng. Proof of useful intelligence (poui): Blockchain consensus beyond energy waste. arXiv preprint arXiv:2504.17539, 2025. KD Conway, Cathie So, Xiaohang Yu, and Kartin Wong. opml: Optimistic machine learning on blockchain. arXiv preprint arXiv:2401.17555, 2024. Marco Dorigo, Vittorio Maniezzo, and Alberto Colorni. Ant system: optimization by colony of cooperating agents. IEEE Transactions on Systems, Man, and Cybernetics, Part (Cybernetics), 26(1):2941, 1996. doi: 10.1109/ 3477.484436. Marco Dorigo, Guy Theraulaz, and Vito Trianni. Swarm robotics: Past, present, and future [point of view]. Proceedings of the IEEE, 109(7):11521165, 2021. doi: 10.1109/JPROC.2021.3072740. John R. Douceur. The sybil attack. In International Workshop on Peer-to-Peer Systems (IPTPS), 2002. doi: 10.1007/ 3-540-45748-8 24. EZKL Team / ZKonduit. EZKL: Easy zero-knowledge machine learning. Software framework, 2024. URL https: //github.com/zkonduit/ezkl. Boyuan Feng, Lianke Qin, Zhenfei Zhang, Yufei Ding, and Shumo Chu. ZEN: An optimizing compiler for verifiable, zero-knowledge neural network inferences. Cryptology ePrint Archive, Paper 2021/087, 2021. URL https: //eprint.iacr.org/2021/087. Simon Garnier, Jacques Gautrais, and Guy Theraulaz. The biological principles of swarm intelligence. Swarm Intelligence, 1(1):331, 2007. doi: 10.1007/s11721-007-0004-y. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. S. Hong. Recent advances of privacy-preserving machine learning based on (fully) homomorphic encryption. Security and Safety, 2025. doi: 10.1051/sands/2024012. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Yihan Jiang, Jakub Koneˇcny, Keith Rush, and Sreeram Kannan. Improving federated learning personalization via model agnostic meta learning. arXiv preprint arXiv:1909.12488, 2023. 33 PREPRINT - OCTOBER 30, Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011. ISBN 978-0374275631. Daniel Kang, Tatsunori Hashimoto, Ion Stoica, and Yi Sun. Scaling up trustless dnn inference with zero-knowledge proofs. arXiv preprint arXiv:2210.08674, 2022. Aggelos Kiayias, Alexander Russell, Bernardo David, and Roman Oliynykov. Ouroboros: provably secure proofof-stake blockchain protocol. In Advances in Cryptology CRYPTO 2017, pages 357388, 2017. Woosuk Kwon, Li Zhuohan, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Symposium on Operating Systems Principles (SOSP), pages 611626, 10 2023. doi: 10.1145/3600006.3613165. Leslie Lamport, Robert Shostak, and Marshall Pease. The byzantine generals problem. ACM Transactions on Programming Languages and Systems, 4(3):382401, 1982. doi: 10.1145/357172.357176. Vladyslav Larin, Ivan Nikitin, and Alexander Firsov. Self-supervised inference of agents in trustless environments. arXiv preprint arXiv:2409.08386, 2024. Zijun Liu, Boqun Kou, Peng Li, Ming Yan, Ji Zhang, Fei Huang, and Yang Liu. Enabling weak llms to judge response reliability via meta ranking. arXiv preprint arXiv:2402.12146, 2024. Kenway Louie, Lauren E. Grattan, and Paul W. Glimcher. Reward value-based gain control: Divisive normalization in parietal cortex. Journal of Neuroscience, 31(29):1062710639, 2011. doi: 10.1523/JNEUROSCI.1237-11.2011. Rischan Mafrur. Ai-based crypto tokens: The illusion of decentralized ai? IET Blockchain, 5, 06 2025. doi: 10.1049/ blc2.70015. Satoshi Nakamoto. Bitcoin: peer-to-peer electronic cash system, 2008. Bitcoin whitepaper. M. Nitti, A. Pinna, L. Pintor, et al. collective intelligence model for swarm robotics applications. Nature Communications, 16:321, 2025. doi: 10.1038/s41467-025-61985-7. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow In Advances in Neural Information Processing Systems, volume 35, pages instructions with human feedback. 2773027744, 2022. L. Phan, A. Gatti, Z. Han, N. Li, et al. Humanitys last exam. arXiv preprint arXiv:2501.14249, 2025. Philip and Hemang. Simplebench: The text benchmark in which unspecialized human performance exceeds that of current frontier models. Google Docs, 2024. URL https://drive.google.com/file/d/ 1mddNFK5UbBFVr3oDftd2Kyc6D8TFctfe/view. Available at: https://simple-bench.com/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Team Rocket, Maofan Yin, Kevin Sekniqi, Robbert van Renesse, and Emin Gun Sirer. Scalable and probabilistic leaderless bft consensus through metastability. arXiv preprint arXiv:1906.08936, 2019. Louis Rosenberg. Artificial swarm intelligence vs human experts. In 2016 International Joint Conference on Neural Networks (IJCNN), pages 25472551, 2016. doi: 10.1109/IJCNN.2016.7727517. Andrei Stoian, Jordan Frery, Roman Bredehoft, Luis Montero, Celia Kherfallah, and Benoit Chevallier-Mames. Deep neural networks for encrypted inference with tfhe. arXiv preprint arXiv.2302.10906, 2023. 34 PREPRINT - OCTOBER 30, 2025 Hao Sun, Yunyi Shen, and Jean-Francois Ton. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives. arXiv preprint arXiv:2411.04991, 2025. Haochen Sun, Tonghe Bai, Jason Li, and Hongyang Zhang. zkdl: Efficient zero-knowledge proofs of deep learning training. arXiv preprint arXiv:2307.16273, 2023. L. L. Thurstone. law of comparative judgment. Psychological Review, 34(4):273286, 1927. doi: 10.1037/ h0070288. Eric Wong, Frank Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses. In Advances in Neural Information Processing Systems, volume 31, 2018. Maofan Yin, Dahlia Malkhi, Michael Reiter, Guy Gueta, and Ittai Abraham. Hotstuff: Bft consensus with linearity and responsiveness. In Symposium on Principles of Distributed Computing (PODC), pages 347356, 07 2019. ISBN 978-1-4503-6217-7. doi: 10.1145/3293611.3331591. Haifeng Yu, Michael Kaminsky, Phillip B. Gibbons, and Abraham D. Flaxman. Sybilguard: defending against sybil attacks via social networks. IEEE/ACM Transactions on Networking, 16(3):576589, 2008. doi: 10.1109/TNET. 2008.923723. Zhenjie Zhang, Yuyang Rao, Hao Xiao, Xiaokui Xiao, and Yin Yang. Proof of quality: costless paradigm for trustless generative ai model inference on blockchains, 05 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, et al. Judging LLM-as-a-Judge with MT-Bench and chatbot arena. In Advances in Neural Information Processing Systems (NeurIPS), 2023. Weiyu Zhong, Ce Yang, Wei Liang, Jiahong Cai, Lin Chen, Jing Liao, and Naixue Xiong. Byzantine fault-tolerant consensus algorithms: survey. Electronics, 12:3801, 09 2023. doi: 10.3390/electronics12183801."
        }
    ],
    "affiliations": [
        "FORTYTWO"
    ]
}