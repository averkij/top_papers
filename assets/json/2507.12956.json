{
    "paper_title": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers",
    "authors": [
        "Qiang Wang",
        "Mengchao Wang",
        "Fan Jiang",
        "Yaqi Fan",
        "Yonggang Qi",
        "Mu Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model's ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/."
        },
        {
            "title": "Start",
            "content": "FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers Qiang Wang1*, Mengchao Wang1*, Fan Jiang1, Yaqi Fan2, Yonggang Qi2, Mu Xu1, 1AMAP, Alibaba Group yijing.wq,wangmengchao.wmc,frank.jf,xumu.xm@alibaba-inc.com 2Beijing University of Posts and Telecommunications yqfan,qiyg@bupt.edu.cn 5 2 0 2 7 1 ] . [ 1 6 5 9 2 1 . 7 0 5 2 : r Abstract Producing expressive facial animations from static images is challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both singleand multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the models ability to render fine-grained emotions. For multi-character control, we design masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is https://fantasy-amap.github.io/fantasy-portrait/."
        },
        {
            "title": "Introduction",
            "content": "Portrait animation aims to generate dynamic facial video sequences from static images, enabling rich and natural expressions with broad applications in film production (Gu et al. 2024), virtual communication (Khmel 2021), and gaming (Li et al. 2024). Existing approaches typically rely on driving video inputs and employ generative models (e.g., GANs (Zeng et al. 2023; Drobyshev et al. 2022; Wang et al. 2023a; Deng et al. 2024; Guo et al. 2024), NeRF (Yu et al. 2023; Ye et al. 2024), and Diffusion Models (Ma et al. 2024; Xie et al. 2024; Qiu et al. 2025)) to manipulate facial expressions through geometric priors like facial landmarks (Lu- *These authors contributed equally. Project leader. Corresponding author. garesi et al. 2019) or 3D Morphable Models (3DMM) (Egger et al. 2020). However, these geometry-based methods face two fundamental limitations. First, they struggle with cross reenactment when significant facial geometry differences exist between the source image and the driving video (e.g., across ethnicities, ages, or genders), often leading to facial artifacts, motion distortions, and background flickering. Second, explicit geometric representations are insufficient to capture subtle expression variations and complex emotional nuances, as they require precise alignment between source and target faces. These issues severely hinder performance in cross-identity scenarios. Moreover, prior research primarily focuses on singlecharacter portrait animation, shedding little light on multicharacter collaborative animation. In such setting, features from different individuals can interfere with each other, causing expression leakage, where facial attributes of one character inadvertently transfer to others. This makes it challenging to maintain both expression independence and harmony among characters. The absence of publicly available datasets and standardized evaluation benchmarks for multicharacter portrait animation further impedes progress in this area. In this work, we propose FantasyPortrait, Diffusion Transformer (DiT)-based framework for generating precisely aligned, emotionally expressive multi-character portrait animations. Specifically, we extract implicit expression representations from the driven videos to capture identityagnostic facial dynamics and enhance the models ability to express fine-grained affective nuances through expressionaugmented learning. To enable coordinated yet independent control of multi-character expressions, we introduce masked cross-attention mechanism for avoiding intercharacter interference. To advance the training and evaluation of multi-character portrait animation, we present ExprBench, novel benchmark that captures wide range of expressions, emotions, and head movements across both singleand multi-character settings. Extensive experiments on ExprBench demonstrate that FantasyPortrait consistently outperforms existing methods in both quantitative metrics and qualitative evaluations, particularly in cross-identity reenactment scenarios. In summary, our key contributions are as follows: Figure 1: Given portrait image and reference motion video, FantasyPortrait generates vivid animated portraits during crossreenactment. It achieves high-fidelity facial dynamics and natural head movements for both single-character and multi-character. We propose an expression-augmented implicit facial expression control method that enhances subtle expression dynamics and complex emotions through decomposed implicit representations and an expression-aware learning module. We design masked attention mechanism that enables synchronized multi-character animation while maintaining rigorous identity separation, effectively preventing cross-character feature interference. We construct ExprBench, specialized evaluation benchmark for expression-driven animation, along with multi-character expression Multi-Expr dataset. Extensive experiments demonstrate our method superior performance in both fine-grained controllability and expressive quality."
        },
        {
            "title": "2.1 Diffusion-Based Video Generation\nEarly research on video generation (Chu et al. 2020; Wang\net al. 2020; Clark, Donahue, and Simonyan 2019; Bal-\naji et al. 2019)primarily relied on Generative Adversarial\nNetworks (GANs) (Goodfellow et al. 2020). Recently, the\ngroundbreaking progress of diffusion models (Ho, Jain, and\nAbbeel 2020) in image generation (Dhariwal and Nichol\n2021; Rombach et al. 2022; Podell et al. 2023) has di-\nrectly catalyzed a surge of interest in video generation. This\nfield has recently undergone a significant paradigm shift,\ntransitioning from conventional U-Net architectures (Ron-\nneberger, Fischer, and Brox 2015) to DiTs (Peebles and Xie\n2023). U-Net-based approaches (Blattmann et al. 2023; Guo\net al. 2023; Wang et al. 2023b) typically extend pre-trained",
            "content": "image generation models by incorporating temporal attention layers, thereby equipping them with sequential modeling capabilities for video generation. Although these models have demonstrated remarkable video synthesis performance, the latest DiT architectures (e.g., Wan (Wan et al. 2025) and Hunyuan Video (Kong et al. 2024)) have achieved substantial quality improvements. This is accomplished through the integration of 3D VAEs (Kingma, Welling et al. 2013) as encoder-decoders, while combining the sequential modeling advantages of Transformer architectures with advanced techniques such as rectified flows (Esser et al. 2024; Lipman et al. 2022). Moreover, DiT-based models have been successfully applied to diverse scenarios like camera control (Cao et al. 2025; Zheng et al. 2024), identity-preserving (Yuan et al. 2025; Zhang et al. 2025; Liu et al. 2025), and audio-driven (Wang et al. 2025; Kong et al. 2025; Cui et al. 2025a), demonstrating strong application potential and generalization capabilities."
        },
        {
            "title": "2.2 Human Portrait Animation",
            "content": "Portrait animation generation aims to drive static human portraits into dynamic video sequences by leveraging reference conditions such as video and facial expressions. Early approaches (Guo et al. 2024; Zeng et al. 2023; Drobyshev et al. 2022; Wang et al. 2023a) primarily employed GANs to learned motion dynamics, while more recent methods based on diffusion models (Xu et al. 2025; Ma et al. 2024; Xie et al. 2024; Qiu et al. 2025) have demonstrated significantly stronger generative capabilities. However, most existing approaches rely on explicit intermediate representations as driving signals. For instance, Follow-Your-Emoji (Ma et al. 2024) utilizes facial keypoints, and Skyreels-A1 (Qiu et al. 2025) employs the 3DMM (Retsinas et al. 2024). These methods exhibit two main limitations. Firstly, due to significant variations in facial features among individuals, methods relying on explicit intermediate representations often struggle to achieve precise alignment when there are substantial differences in facial structure between the reference image and the target portrait, leading to degraded generation quality. Secondly, these methods typically require portraitspecific keypoint adaptation, making them difficult to generalize to multi-character portrait animation scenarios. In this study, we propose novel DiT-based model architecture that implements implicit feature-driven multiple portrait animation, surpassing previous methods in generation quality and generalization capability."
        },
        {
            "title": "3.1 Preliminary\nLatent Diffusion Model. Our framework is built upon\nthe Latent Diffusion Model (LDM) (Rombach et al. 2022),\nwhich operates in latent space rather than pixel space to\nenable efficient and stable training. The model employs a\npre-trained VAE to establish bidirectional mapping between\npixel space and latent space. Specifically, the VAE encoder\nE transforms input video data x into latent representations\nz = E(x), and the decoder D reconstructs the latent tokens\nback into video space. During training, Gaussian noise ϵ is\nincrementally added to z through a forward process, produc-\ning noised latents zt = (1 − t)z + tϵ, where t ∈ [0, 1] is sam-\npled from the logit-normal distribution. Furthermore, we in-\ncorporate flow matching (Lipman et al. 2022) to simplify the\ntransformation between complex and tractable probability\ndistributions, facilitating sample generation through learned\ninverse transformations. The LDM’s training objective mini-\nmizes the discrepancy between the velocity vt and the noise\npredicted by the denoising network vθ using the following\nloss function:",
            "content": "(cid:3) (cid:2)vθ(zt, t, c) vt2 = Ezt,vt,t,c (1) where denotes the conditions, z1 denote the latent embedding of the training sample, and z0 represents the initialized noise sampled from the gaussian distribution. The velocity term vt = dzt/dt = z1 z0 serves as the regression target for the models prediction task. 2 Video Diffusion Transformer. Diffusion transformer is an advanced class of diffusion models that employ multilayer transformer architecture as the denoising network uθ, demonstrating exceptional generative capabilities in video synthesis tasks (Seawead et al. 2025; Wan et al. 2025; Kong et al. 2024; Yang et al. 2024). Specifically, we adopt the Wan (Wan et al. 2025) as the foundational architecture, which consists of 40 transformer layers. The model utilizes causal 3D VAE to compress videos both temporally and spatially, while incorporating umT5 (Chung et al. 2023) as multilingual text encoder to effectively integrate textual features via cross-attention mechanisms. Furthermore, Wan enhances conditional generation by integrating CLIP (Radford et al. 2021) image encoder along with masked training strategy for initial frames, enabling more effective conditioning on image inputs."
        },
        {
            "title": "3.2 Expression-Augmented Implicit Control\nImplicit Expression Representations. We\nderive\nidentity-agnostic expression representations from the driv-\ning video through an implicit feature extraction pipeline.\nIn contrast to conventional portrait animation methods that\ndepend on explicit facial landmarks, our approach leverages\nimplicit encoding to better disentangle expression-related\nfeatures from confounding factors such as camera motion\nand subject-specific attributes,\nthereby achieving more\nnatural and adaptable animation results. Specifically, we\ndetect (Huang et al. 2020) and align the facial region in\neach frame. Subsequently, we employ a pretrained implicit\nexpression extractor Ee (Wang et al. 2023a) to encode the\ndriving video into expressive latent features. These features\ninclude lip motion elip, eye gaze and blink eeye, head pose\nehead, and emotional expression eemo.",
            "content": "Expression-Augmented Learning. Facial expression generation involves complex multi-level system, encompassing both relatively simple rigid motion features (e.g., head rotation and eye movements) and highly dynamic non-rigid deformations (e.g., emotion-related muscle activity and lip movements). Simple motions, due to their more regular patterns and well-defined physical constraints, can be relatively easily modeled. In contrast, complex motions involve richer semantic information and subtle muscle synergies, exhibiting stronger nonlinear characteristics. This significant disparity in feature complexity poses considerable challenge for simultaneous learning of both motion types. To address this, we propose an expression-augmented encoder Ea to enhance the learning of subtle and challenging features. Specifically, for eemo and elip, we employ learnable tokens to perform fine-grained decomposition and enhancement, where each sub-feature corresponds to more granular muscle groups or emotional dimensions. Each fine-grained sub-feature then interacts with semantically aligned video tokens via multi-head cross-attention, effectively capturing region-specific semantic relationships. Subsequently, we concatenate the expression-augmented features with ehead and eeye to obtain the motion embedding em, as follows: Figure 2: Overview of FantasyPortrait. em = Concat(Ea(eemo), Ea(elip), ehead, eeye) (2)"
        },
        {
            "title": "3.3 Multi-Portrait Animations\nMulti-Portrait Embeddings. Using implicit expression-\naugmented representations, we derive fine-grained portrait\nmotion embeddings for individual characters. For multi-\nportrait animations, we detect and crop facial regions us-\ning the face recognition model (Huang et al. 2020), then ex-\ntract identity-specific motion embeddings em ∈ Rf ×l×c for\neach character, where f is the number of frames, c denotes\nthe number of channel, and l represents the spatial embed-\nding length. The final multi-portrait motion feature ˆem is ob-\ntained by concatenating all N individual embeddings along\nthe length axis:",
            "content": "ˆem = {e1 m, e2 m, . . . , eN m} Rf (N l)c (3) Masked Cross-Attention. To prevent identity confusion and cross-interference between expression-driven signals from different individuals, we design masked crossattention mechanism to weight the multi-portrait embeddings in all cross-attention layers. We extract the face mask from the video and then apply trilinear interpolation to map it to the latent space, obtaining the latent mask . The multi-portrait motion embedding em interact with each block of the pre-trained DiT through dedicated crossattention layers. The hidden state Zi of each DiT block is re-expressed as: = Zi + sof tmax (cid:18) QiK dK (cid:19) Vi (4) where denotes element-wise multiplication, ndexes the attention block layers, dK denotes the dimension of keys, Qi represents the query matrices, Ki = ˆemW ˆemW . Here, for keys and values. , and Vi = are trainable projection weights and v"
        },
        {
            "title": "4.1 Multi-Expr Datasets",
            "content": "To address the current scarcity of multi-portrait facial expression video datasets, we introduce Multi-Expr, novel dataset specifically designed for this purpose. The dataset is curated from OpenVid-1M (Nan et al. 2024) and OpenHumanVid (Li et al. 2025), and we design comprehensive data processing pipelineincluding multi-portrait filtering, quality control, and facial expression selectionto ensure the quality and suitability of the video datasets. First, we employ YOLOv8 (Reis et al. 2023) to detect the number of individuals present in each video clip, and retain only those containing two or more portrait. Next, we filter out low-quality, blurry, or artifact-ridden clips using aesthetic scoring (Yeh et al. 2013) and dathe Laplacian operator. Finally, leveraging facial landmarks detected by MediaPipe (Lugaresi et al. 2019), we compute the angular and motion variations of key facial points to select clips exhibiting clear and expressive facial movements. The dataset comprises approximately 30,000 high-quality video clips, each annotated with descriptive captions generated by CogVLM2 (Hong et al. 2024)."
        },
        {
            "title": "4.2 ExprBench",
            "content": "Due to the lack of publicly available evaluation benchmarks in the field of multiple expression-driven video generation, we introduce ExprBench to objectively compare the performance of different methods in generating facial animations We evaluate all methods on ExprBench. For selfreenactment evaluation, we use the first frame as the source input image and the driving video as ground truth. To assess the generalization quality and motion accuracy of the generated portrait animations, we employ the Frechet Inception Distance (FID) (Heusel et al. 2017), the Frechet Video Distance (FVD) (Unterthiner et al. 2019), Peak Signalto-Noise Ratio (PSNR), and Structural Similarity Index (SSIM) (Wang et al. 2004). Additionally, to measure expression motion accuracy, we use Landmark Mean Distance (LMD) (Lugaresi et al. 2019), while Mean Angular Error (MAE) (Han et al. 2024) is adopted to evaluate eye movement accuracy. For cross-reenactment evaluation, we utilize Average Expression Distance (AED) (Siarohin et al. 2019), Average Pose Distance (APD) (Siarohin et al. 2019), and MAE. AED and APD are used to assess the accuracy of expression and head pose movements respectively. Quantitative Results. The quantitative comparison results are presented in Table 1. The warping-based approach employed by LivePortrait demonstrates limited accuracy in controlling global head movements, resulting in the lowest APD score among the compared methods. Approaches including FollowYE, and Skyreels-A1 utilize explicit facial landmark to control head or facial movements. However, this methodology inevitably introduces identity leakage in crossreenactment scenarios, consequently degrading performance across AED, APD, and MAE evaluation metrics. HunyuanPortrait utilizes implicit signals for facial expression generation, while employing explicit DWPose (Yang et al. 2023) condition to drive head movements, which still exhibits limited performance. The GAN-based method LivePortrait along with UNet-based architectures including HunyuanPortrait, X-Portrait, and FollowYE exhibit inferior FID and FVD scores, indicating their limitations in generated video quality, especially in preserving fine facial details, compared to advanced DiT-based models including Skyreels-A1 and FantasyPortrait. Our method achieves state-of-the-art performance on expression and head movement similarity metrics including LMD, MAE, AED and APD, demonstrating particularly significant improvements in cross-identity reenactment. These results validate that our fine-grained implicit expression representation combined with expressionaugmented learning effectively captures nuanced facial expressions and emotional dynamics while maintaining superior cross-identity transfer capabilities. In multi-portrait experiments, our approach also yields the best quantitative results, confirming that the masked cross-attention mechanism enables robust and precise control over multiple portraits. Qualitative Results. Figure 4 presents the qualitative results, demonstrating that our method achieves more accurate facial motion transfer and more visually compelling results. In the single-character case, despite significant interference from camera movement and body pose variations in the driving video, our method still outperforms all baselines in terms of visual quality, while the baselines exhibit artifacts and incorrect expressions under such disturbances. This advantage stems from our expression-enhanced implicit facial control approach, which enables more robust and nuanced expresFigure 3: Examples of ExprBench. with rich expressions. ExprBench comprises ExprBenchsingle for single-portrait evaluation and ExprBench-multi for multi-portrait scenarios. Specifically, we meticulously collected 200 single portraits and 100 driving videos from copyright-free sources on Pexels1 to construct ExprBenchSingle. Each driving video was trimmed to 5-second clips containing approximately 125 frames. The portrait images encompass realistic human styles, various anthropomorphic styles (e.g., animals, cartoon characters), and wide range of scenarios (e.g., recording studios, performance stages, live streaming rooms). The driving videos contain diverse facial expressions (e.g., drooping eyelids, eyebrow twitches), emotions (e.g., happiness, sadness, anger), and head movements. To further evaluate the performance of multi-portrait expression-driven generation, we also collected 100 portrait images and 50 driving videos to construct ExprBench-Multi, multi-centric benchmark. ExprBench-Multi is designed to test and compare the performance of different methods in handling video generation tasks involving multiple characters expressions and movements. Figure 3 showcases examples of portrait images and driving videos from ExprBench."
        },
        {
            "title": "4.4 Comparison with Baselines\nBaselines and Metrics. We select several publicly avail-\nable portrait animation methods for comparative evaluation\nin the single-portrait setting, including LivePortrait (Guo\net al. 2024), Skyreels-A1 (Qiu et al. 2025), HunyuanPortrait\n(Xu et al. 2025), X-Portrait (Xie et al. 2024), and FollowYE\n(Ma et al. 2024). We employ the multiple faces version of\nLivePortrait as the multi-portrait baseline.",
            "content": "1https://www.pexels.com/ Dataset Method Self Reenactment Cross Reenactment FID FVD PSNR SSIM LMD MAE AED APD MAE Single LivePortrait Skyreels-A1 HunyuanPortrait X-Portrait FollowYE FantasyPortrait 438.27 79.32 373.98 66.84 409.14 74.86 83.28 445.25 103.75 489.93 358.08 64.66 Multi LivePortrait FantasyPortrait 120.43 416.39 391.12 84. 23.38 24.58 24.54 22.51 21.47 25.76 21.98 24.29 0.789 0.812 0.783 0.739 0.692 0.818 0.7370 0.7652 6.90 4.21 5.73 7.26 9.15 5.08 7.36 5. 9.29 7.59 8.35 9.15 12.63 6.97 10.57 7.42 45.13 36.91 40.41 49.26 54.11 33.45 59.09 34.63 39.79 24.27 26.12 29.15 32.19 23.08 36.14 30. 17.09 16.41 16.65 18.89 21.58 14.55 21.52 16.26 Table 1: Quantitative Results on ExprBench. LMD multiplied by 103, AED multiplied by 102 and APD multiplied by 103. indicates higher is better. indicates lower is better. The best results are in bold. Figure 4: Qualitative Results. Dataset Method LivePortrait Skyreels-A1 HunyuanPortrait X-Portrait FollowYE FantasyPortrait LivePortrait FantasyPortrait Single Multi VQ 7.01 7.93 7.88 6.66 5.87 8. 4.72 7.46 ES MN 6.23 6.68 6.81 4.74 4.29 7.66 5.96 7.17 7.59 8.25 8.13 6.09 5.77 9.03 6.29 8. ER 7.69 7.84 7.58 6.57 6.34 8.21 6.88 8.12 Table 2: User Study Results. sion manipulation. For multi-character scenarios, LivePortrait exhibits noticeable discontinuities between the driven regions and static background areas, as it relies on segmenting and re-compositing the facial regions in the pixel space. In contrast, our method employs masked cross-attention mechanism that allows for thorough integration of expression features from different identities in the latent space, without mutual interference or leakage between individuals expressions, thereby producing more natural results. User Studies. Considering that the cross-reenactment evaluation lacks ground truth, we conduct subjective study to comprehensively assess the generation quality. Specifically, 32 participants are invited to rate each sample on scale from 0 to 10 across four key dimensions: Video Quality (VQ), Expression Similarity (ES), Motion Naturalness (MN), and Expression Richness (ER). As shown in Table 2, FantasyPortrait outperforms all baselines across all evaluated dimensions. Notably, our method achieves significant improvements in expression similarity and expressiveness, demonstrating that our implicit conditional control mechanism and Expression-Augmented Learning framework enable the model to better capture and transfer fine-grained facial expressions across different identities, which highlight the strong generalization capability of our approach. More Visualization Results. Our supplementary materials include extended videos showcasing additional visual results, such as diverse portrait styles (e.g., animals and anime characters), outcomes in various complex real-world scenarios (e.g., glasses occlusion, head accessories, and facial obstructions), identity swapping animation, and multi-portrait animation generated by combining multiple single-portrait video inputs."
        },
        {
            "title": "4.5 Ablation Study and Discussion",
            "content": "Dataset Method AED APD MAE Single Multi Ours Ours(w/o EAL) Ours(all EAL) Ours(w/o MCA) Ours(w/o MED) Ours Ours(w/o EAL) Ours(all EAL) Ours(w/o MCA) Ours(w/o MED) 33.45 42.88 33.38 33.41 34.02 34.63 43.63 34.45 73.18 40.92 23.08 23.10 23.05 23.06 23.15 30.64 30.75 30.69 46.22 37. 14.55 14.57 14.61 14.57 14.63 16.26 16.25 16.29 24.37 22.75 Table 3: Ablation Studies in Cross Reenactment. Figure 5: Qualitative Ablation Results. Ablation on Expression-Augmented Learning (EAL). To validate the effectiveness of our proposed EAL module, we conducted comprehensive comparisons between three configurations: (1) direct concatenation of all implicit features without EAL (Ours(w/o EAL)), (2) applying expression-augmented learning to all implicit features (Ours(all EAL)), and (3) our selective approach focusing only on lip elip and emotional eemo features. As demonstrated in Figure 5 and Table 3, the absence of EAL leads to significantly reduced AED scores, indicating impaired finegrained expression learning capability. Interestingly, both APD and MAE metrics remain relatively stable across all configurations, suggesting that head pose and eye movements follow more rigid, easily-learned motion patterns, and the benefits of augmented learning are inherently limited for these rigid motions. However, for complex non-rigid motions like lip articulation and emotional dynamics, the performance degradation without EAL becomes pronounced. These findings validate our design rationale for selectively applying emotion augmentation to elip and eemo features, as full augmentation provides negligible benefits for rigid motions while unnecessarily increasing computational complexity. Ablation on Masked Cross-Attention (MCA). The results in Table 3 and Figure 5 underscore the critical importance of MCA in multi-portrait applications. Without MCA, the facial driving features of multiple individuals interfere with each other, leading to significant degradation across all evaluation metrics. As illustrated in Figure 5, the absence of MCA results in mutual interference between characters facial expressions, generating conflicting outputs that nearly eliminate the models ability to follow the driving video. In contrast, our designed masked cross-attention mechanism effectively empowers the model to independently control different individuals. Ablation on Multi-Expr Dataset (MED). Our experimental results demonstrate the critical role of multiexpression datasets in portrait animation tasks. As shown in Table 3 and Figure 5, training exclusively on single-portrait datasets maintains comparable performance for single portrait animation, but leads to substantial performance degradation and even visual artifacts in multi-portrait scenarios. These findings demonstrate that while multi-expression datasets may be less essential for single-portrait animation, they are indispensable for achieving high-quality results in complex multi-portrait animation tasks, which facilitates the models capacity to acquire nuanced facial expression representations across multiple individuals. Limitations and Future Works. While our method demonstrates significant advancements, particularly in cross-identity reenactment portrait animation, two key limitations and future works warrant discussion. First, the iterative sampling process required by diffusion models leads to relatively slow generation speeds, which may hinder realtime applications. Future research would explore acceleration strategies to improve computational efficiency for timesensitive scenarios. Second, the high-fidelity nature of our portrait animations raises potential misuse concerns. We advocate for development of robust detection and defense mechanisms to mitigate possible ethical risks associated with this technology."
        },
        {
            "title": "5 Conclusion\nIn this work, we present FantasyPortrait, a novel DiT-based\nframework for generating expressive and well-aligned multi-\ncharacter portrait animations. Our method leverages im-\nplicit facial expression representations to achieve identity-\nagnostic motion transfer while preserving fine-grained af-\nfective details. Additionally, we introduce a masked cross-\nattention mechanism to enable synchronized yet indepen-\ndent control of multiple characters, effectively soluting ex-",
            "content": "pression leakage. To support research in this field, we contribute ExprBench, comprehensive evaluation benchmark, along with multi-character facial expression Multi-Expr dataset. Extensive experiments demonstrate that FantasyPortrait outperforms existing methods in both singleand multi-character animation scenarios, particularly in handling cross-identity reenactment and complex emotional expressions. References Balaji, Y.; Min, M. R.; Bai, B.; Chellappa, R.; and Graf, H. P. 2019. Conditional GAN with Discriminative Filter Generation for Text-to-Video Synthesis. In IJCAI, volume 1, 2. Blattmann, A.; Dockhorn, T.; Kulal, S.; Mendelevitch, D.; Kilian, M.; Lorenz, D.; Levi, Y.; English, Z.; Voleti, V.; Letts, A.; et al. 2023. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127. Cao, C.; Zhou, J.; Li, S.; Liang, J.; Yu, C.; Wang, F.; Xue, X.; and Fu, Y. 2025. Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation. arXiv preprint arXiv:2504.14899. Chu, M.; Xie, Y.; Mayer, J.; Leal-Taixe, L.; and Thuerey, N. 2020. Learning temporal coherence via self-supervision for GAN-based video generation. ACM Transactions on Graphics (TOG), 39(4): 751. Chung, H. W.; Constant, N.; Garcia, X.; Roberts, A.; Tay, Y.; Narang, S.; and Firat, O. 2023. Unimax: Fairer and more effective language sampling for large-scale multilingual pretraining. arXiv preprint arXiv:2304.09151. Clark, A.; Donahue, J.; and Simonyan, K. 2019. Adversarial video generation on complex datasets. arXiv preprint arXiv:1907.06571. Cui, J.; Chen, Y.; Xu, M.; Shang, H.; Chen, Y.; Zhan, Y.; Dong, Z.; Yao, Y.; Wang, J.; and Zhu, S. 2025a. Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation. arXiv preprint arXiv:2505.23525. Cui, J.; Li, H.; Zhan, Y.; Shang, H.; Cheng, K.; Ma, Y.; Mu, S.; Zhou, H.; Wang, J.; and Zhu, S. 2025b. Hallo3: Highly dynamic and realistic portrait image animation with video diffusion transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2108621095. Deng, Y.; Wang, D.; Ren, X.; Chen, X.; and Wang, B. 2024. Portrait4d: Learning one-shot 4d head avatar synthesis using synthetic data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 71197130. Dhariwal, P.; and Nichol, A. 2021. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34: 87808794. Drobyshev, N.; Chelishev, J.; Khakhulin, T.; Ivakhnenko, A.; Lempitsky, V.; and Zakharov, E. 2022. Megaportraits: Oneshot megapixel neural head avatars. In Proceedings of the 30th ACM International Conference on Multimedia, 2663 2671. Egger, B.; Smith, W. A.; Tewari, A.; Wuhrer, S.; Zollhoefer, M.; Beeler, T.; Bernard, F.; Bolkart, T.; Kortylewski, A.; Romdhani, S.; et al. 2020. 3d morphable face modelspast, present, and future. ACM Transactions on Graphics (ToG), 39(5): 138. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y. 2020. Generative adversarial networks. Communications of the ACM, 63(11): 139144. Gu, Y.; Xu, H.; Xie, Y.; Song, G.; Shi, Y.; Chang, D.; Yang, J.; and Luo, L. 2024. Diffportrait3d: Controllable diffuIn Proceedings sion for zero-shot portrait view synthesis. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1045610465. Guo, J.; Zhang, D.; Liu, X.; Zhong, Z.; Zhang, Y.; Wan, P.; and Zhang, D. 2024. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168. Guo, Y.; Yang, C.; Rao, A.; Liang, Z.; Wang, Y.; Qiao, Y.; Agrawala, M.; Lin, D.; and Dai, B. 2023. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725. Han, Y.; Zhu, J.; He, K.; Chen, X.; Ge, Y.; Li, W.; Li, X.; Zhang, J.; Wang, C.; and Liu, Y. 2024. Face-Adapter for Pre-trained Diffusion Models with Fine-Grained ID and Attribute Control. In European Conference on Computer Vision, 2036. Springer. Heusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and Hochreiter, S. 2017. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30. Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33: 68406851. Ho, J.; and Salimans, T. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598. Hong, W.; Wang, W.; Ding, M.; Yu, W.; Lv, Q.; Wang, Y.; Cheng, Y.; Huang, S.; Ji, J.; Xue, Z.; et al. 2024. Cogvlm2: Visual language models for image and video understanding. arXiv preprint arXiv:2408.16500. Huang, Y.; Wang, Y.; Tai, Y.; Liu, X.; Shen, P.; Li, S.; Li, J.; and Huang, F. 2020. Curricularface: adaptive curriculum In proceedings of learning loss for deep face recognition. the IEEE/CVF conference on computer vision and pattern recognition, 59015910. Khmel, I. 2021. Humanization of Virtual Communication: from Digit to Image. Philosophy and Cosmology, 27(27): 126134. Kingma, D. P.; Welling, M.; et al. 2013. Auto-encoding variational bayes. Kong, W.; Tian, Q.; Zhang, Z.; Min, R.; Dai, Z.; Zhou, J.; Xiong, J.; Li, X.; Wu, B.; Zhang, J.; et al. 2024. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603. Kong, Z.; Gao, F.; Zhang, Y.; Kang, Z.; Wei, X.; Cai, X.; Chen, G.; and Luo, W. 2025. Let Them Talk: Audio-Driven arXiv Multi-Person Conversational Video Generation. preprint arXiv:2505.22647. Li, H.; Xu, M.; Zhan, Y.; Mu, S.; Li, J.; Cheng, K.; Chen, Y.; Chen, T.; Ye, M.; Wang, J.; et al. 2025. Openhumanvid: large-scale high-quality dataset for enhancing humanIn Proceedings of the Computer centric video generation. Vision and Pattern Recognition Conference, 77527762. Li, R.; Zhang, H.; Zhang, Y.; Zhang, Y.; Zhang, Y.; Guo, J.; Zhang, Y.; Li, X.; and Liu, Y. 2024. Lodge++: High-quality and long dance generation with vivid choreography patterns. arXiv preprint arXiv:2410.20389. Lipman, Y.; Chen, R. T.; Ben-Hamu, H.; Nickel, M.; and Le, M. 2022. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747. Liu, L.; Ma, T.; Li, B.; Chen, Z.; Liu, J.; Li, G.; Zhou, S.; He, Q.; and Wu, X. 2025. Phantom: Subject-consistent video generation via cross-modal alignment. arXiv preprint arXiv:2502.11079. Lugaresi, C.; Tang, J.; Nash, H.; McClanahan, C.; Uboweja, E.; Hays, M.; Zhang, F.; Chang, C.-L.; Yong, M. G.; Lee, J.; et al. 2019. Mediapipe: framework for building perception pipelines. arXiv preprint arXiv:1906.08172. Ma, Y.; Liu, H.; Wang, H.; Pan, H.; He, Y.; Yuan, J.; Zeng, A.; Cai, C.; Shum, H.-Y.; Liu, W.; et al. 2024. Follow-youremoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, 112. Nan, K.; Xie, R.; Zhou, P.; Fan, T.; Yang, Z.; Chen, Z.; Li, X.; Yang, J.; and Tai, Y. 2024. Openvid-1m: largescale high-quality dataset for text-to-video generation. arXiv preprint arXiv:2407.02371. Peebles, W.; and Xie, S. 2023. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 41954205. Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn, T.; Muller, J.; Penna, J.; and Rombach, R. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Qiu, D.; Fei, Z.; Wang, R.; Bai, J.; Yu, C.; Fan, M.; Chen, G.; and Wen, X. 2025. Skyreels-a1: Expressive portrait animation in video diffusion transformers. arXiv preprint arXiv:2502.10841. Radford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.; et al. 2021. Learning transferable visual models from natIn International conference on ural language supervision. machine learning, 87488763. PmLR. Reis, D.; Kupec, J.; Hong, J.; and Daoudi, A. 2023. Realtime flying object detection with YOLOv8. arXiv preprint arXiv:2305.09972. Retsinas, G.; Filntisis, P. P.; Danecek, R.; Abrevaya, V. F.; Roussos, A.; Bolkart, T.; and Maragos, P. 2024. 3D facial In Proexpressions through analysis-by-neural-synthesis. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 24902501. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Ronneberger, O.; Fischer, P.; and Brox, T. 2015. U-net: Convolutional networks for biomedical image segmentaIn Medical image computing and computer-assisted tion. interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, 234241. Springer. Seawead, T.; Yang, C.; Lin, Z.; Zhao, Y.; Lin, S.; Ma, Z.; Guo, H.; Chen, H.; Qi, L.; Wang, S.; et al. 2025. Seaweed7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685. Siarohin, A.; Lathuili`ere, S.; Tulyakov, S.; Ricci, E.; and Sebe, N. 2019. First order motion model for image animation. Advances in neural information processing systems, 32. Unterthiner, T.; Van Steenkiste, S.; Kurach, K.; Marinier, R.; Michalski, M.; and Gelly, S. 2019. FVD: new metric for video generation. Wan, T.; Wang, A.; Ai, B.; Wen, B.; Mao, C.; Xie, C.-W.; Chen, D.; Yu, F.; Zhao, H.; Yang, J.; et al. 2025. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314. Wang, D.; Deng, Y.; Yin, Z.; Shum, H.-Y.; and Wang, B. 2023a. Progressive disentangled representation learning for fine-grained controllable talking head synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1797917989. Wang, J.; Yuan, H.; Chen, D.; Zhang, Y.; Wang, X.; and Zhang, S. 2023b. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571. Wang, M.; Wang, Q.; Jiang, F.; Fan, Y.; Zhang, Y.; Qi, Y.; Zhao, K.; and Xu, M. 2025. Fantasytalking: Realistic talking portrait generation via coherent motion synthesis. arXiv preprint arXiv:2504.04842. Wang, Y.; Bilinski, P.; Bremond, F.; and Dantcheva, A. 2020. Imaginator: Conditional spatio-temporal gan for video generation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, 11601169. Wang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P. Image quality assessment: from error visibility to 2004. structural similarity. IEEE transactions on image processing, 13(4): 600612. Xie, Y.; Xu, H.; Song, G.; Wang, C.; Shi, Y.; and Luo, L. 2024. X-portrait: Expressive portrait animation with hierarchical motion attention. In ACM SIGGRAPH 2024 Conference Papers, 111. Xu, Z.; Yu, Z.; Zhou, Z.; Zhou, J.; Jin, X.; Hong, F.-T.; Ji, X.; Zhu, J.; Cai, C.; Tang, S.; et al. 2025. Hunyuanportrait: Implicit condition control for enhanced portrait animation. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1590915919. Yang, Z.; Teng, J.; Zheng, W.; Ding, M.; Huang, S.; Xu, J.; Yang, Y.; Hong, W.; Zhang, X.; Feng, G.; et al. 2024. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072. Yang, Z.; Zeng, A.; Yuan, C.; and Li, Y. 2023. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 42104220. Ye, Z.; Zhong, T.; Ren, Y.; Yang, J.; Li, W.; Huang, J.; Jiang, Z.; He, J.; Huang, R.; Liu, J.; et al. 2024. Real3dportrait: One-shot realistic 3d talking portrait synthesis. arXiv preprint arXiv:2401.08503. Yeh, H.-H.; Yang, C.-Y.; Lee, M.-S.; and Chen, C.-S. 2013. Video aesthetic quality assessment by temporal integration of photo-and motion-based features. IEEE transactions on multimedia, 15(8): 19441957. Yu, W.; Fan, Y.; Zhang, Y.; Wang, X.; Yin, F.; Bai, Y.; Cao, Y.-P.; Shan, Y.; Wu, Y.; Sun, Z.; et al. 2023. Nofa: NerfIn ACM SIGbased one-shot facial avatar reconstruction. GRAPH 2023 conference proceedings, 112. Yuan, S.; Huang, J.; He, X.; Ge, Y.; Shi, Y.; Chen, L.; Luo, J.; and Yuan, L. 2025. Identity-preserving text-to-video generation by frequency decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, 1297812988. Zeng, B.; Liu, X.; Gao, S.; Liu, B.; Li, H.; Liu, J.; and Zhang, B. 2023. Face animation with an attribute-guided diffusion In Proceedings of the IEEE/CVF Conference on model. Computer Vision and Pattern Recognition, 628637. Zhang, Y.; Wang, Q.; Jiang, F.; Fan, Y.; Xu, M.; and Qi, Y. 2025. Fantasyid: Face knowledge enhanced id-preserving video generation. arXiv preprint arXiv:2502.13995. Zheng, G.; Li, T.; Jiang, R.; Lu, Y.; Wu, T.; and Li, X. 2024. Cami2v: Camera-controlled image-to-video diffusion model. arXiv preprint arXiv:2410.15957."
        }
    ],
    "affiliations": [
        "AMAP, Alibaba Group",
        "Beijing University of Posts and Telecommunications"
    ]
}