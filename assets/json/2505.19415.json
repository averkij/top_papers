{
    "paper_title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models",
    "authors": [
        "Hang Hua",
        "Ziyun Zeng",
        "Yizhi Song",
        "Yunlong Tang",
        "Liu He",
        "Daniel Aliaga",
        "Wei Xiong",
        "Jiebo Luo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and customized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, a comprehensive Multi-Modal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIG-Bench is equipped with a three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): a VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 1 4 9 1 . 5 0 5 2 : r MMIG-Bench: Towards Comprehensive and Explainable Evaluation of Multi-Modal Image Generation Models Hang Hua1 Ziyun Zeng1 Yizhi Song2 Liu He 2 1 University of Rochester Daniel Aliaga 2 Wei Xiong 3 2 Purdue University Yunlong Tang 1 Jiebo Luo1 3 NVIDIA {hhua2, jluo}@cs.rochester.edu, {zzeng24, ytang37}@ur.rochester.edu, {song630, he425, aliaga}@purdue.edu, wxiong@nvidia.com Figure 1: Overview of MMIG-Bench. We present unified multi-modal benchmark which contains 1,750 multi-view reference images with 4,850 richly annotated text prompts, covering both text-only and image-text-conditioned generation. We also propose comprehensive three-level evaluation framework: low-level of artifacts and identity preservation, mid-level of VQA-based Aspect Matching Score, and high-level of aesthetics and human preferencesdelivers holistic and interpretable scores."
        },
        {
            "title": "Abstract",
            "content": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and Gemini 2.5 Pro excel at following complex instructions, editing images and maintaining concept consistency. However, they are still evaluated by disjoint toolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning, and cusEqual Contribution Corresponding Author Preprint. Under review. tomized image generation benchmarks that overlook compositional semantics and common knowledge. We propose MMIG-Bench, comprehensive MultiModal Image Generation Benchmark that unifies these tasks by pairing 4,850 richly annotated text prompts with 1,750 multi-view reference images across 380 subjects, spanning humans, animals, objects, and artistic styles. MMIGBench is equipped with three-level evaluation framework: (1) low-level metrics for visual artifacts and identity preservation of objects; (2) novel Aspect Matching Score (AMS): VQA-based mid-level metric that delivers fine-grained prompt-image alignment and shows strong correlation with human judgments; and (3) high-level metrics for aesthetics and human preference. Using MMIGBench, we benchmark 17 state-of-the-art models, including Gemini 2.5 Pro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human ratings, yielding in-depth insights into architecture and data design. We will release the dataset and evaluation code to foster rigorous, unified evaluation and accelerate future innovations in multi-modal image generation. Resources are available at: https://hanghuacs.github.io/MMIG-Bench/"
        },
        {
            "title": "Introduction",
            "content": "With the rapid progress in foundational image generation systems, diverse range of models has emerged at the forefront of research and application. These include commercial models such as GPT-4o [35] and Gemini 2.0 Flash, as well as opaen-source models like FLUX [26], Hunyuan-DiT [31], Emu3 [55], and DreamO [34]. Currently, the community primarily evaluates them with separate toolkits: text-to-image (T2I) benchmarks that focus on compositionality and world knowledge; and customized generation benchmarks that emphasize identity preservation of the reference images. However, fine-grained semantic alignment and compositional reasoning included in the T2I evaluation are equally critical for the customization task; conversely, providing reference images with text enhances the flexibility and also broadens the expressive scope of generationenabling style transfer and other capabilities that pure T2I tasks does not contain. Therefore there is pressing need for comprehensive and unified benchmark that treats multi-modal input (both text and image) as an integrated entity. To be more specific, early T2I benchmarks (e.g., PartiPrompts, Gecko) are large sparsely labelled, typically assigning only single category per prompt. Recent benchmarks (T2I-CompBench++ [19], GenEval [10], GenAI-Bench [28], T2I-FactualBench [20]) incorporate dense tags, evaluating nuanced aspects of generated images such as compositionality, common sense, and world knowledge. However, they focus on evaluating generators only conditioned on text, and thus are limited in evaluating newer multi-modal generation models with both images and text as input, such as GPT-4o and Gemini 2.0 Flash. Customization benchmarks [5, 38] are still scarce, most are tiny and lack enough multiview reference images. In addition, the evaluation metrics in T2I benchmarks mostly score prompt following, overlooking visual fidelity. Customization benchmarks often rely on trivial approaches to assess semantic alignment or identity preservation, lacking fine-grained and effective metrics. To address these issues, we build the first comprehensive multi-modal benchmark MMIG-Bench for image generation. we summarize our contributions below and illustrate them in Fig. 1 and Fig. 2. Unified task coverage and multi-modal input. We collect over 380 groups (animal, object, human and style) comprising 1,750 multiview object-centric images enabling rigorous reference-based generation. We also construct 4,850 richly annotated prompts across compositionality (attributes, relations, objects, and numeracy), style (fixed pattern, professional, natural, human-written), realism (imaginative) and common sense (comparisons, negations). The proposed benchmark provides future research with the flexibility to conduct any image generation task. Three-level evaluation suite. We propose multilevel scoring framework for comprehensive evaluation. (1) Low-level metrics assess visual artifacts and identity preservation of objects; (2) At mid-level, we propose the Aspect Matching Score (AMS) : novel VQA-based metric that captures fine-grained semantic alignment, showing strong correlation with human perception; (3) high-level metrics measure aesthetics and human preferences. This multi-level framework expands T2I evaluation beyond prompt adherence and provides customized generation the nuanced semantic assessment it lacks. We validate our metrics with 32k human ratings and benchmark 17 state-of-the-art models, offering design insights on architecture choices and data curation. We will release MMIG-Bench and the evaluation code to accelerate future research on multi-modal generation."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Text-to-Image Generation Recent advancements in text-to-image generation have significantly enhanced models visual synthesis capabilities. FLUX.1-dev [26] employs rectified flow transformer integrated with 3D modeling, enabling precise compositional control. Hunyuan-DiT [31] advances diffusion transformers with multilingual support and multimodal dialogue, enhancing caption accuracy. Lumina-Image 2.0 [42] prioritizes efficiency through unified architectures and progressive training, achieving scalability with compact models. Photon-v1 [40] specifically targets photorealism, effectively rendering challenging visual elements. PixArt-Σ [2] innovates with attention mechanisms, achieving ultra-high-resolution generation. Stable Diffusion variants, including SDXL [41] and SD3.5 [6], leverage advanced multimodal conditioning to enhance image quality and textual fidelity. Janus Pro [3] offers superior multimodal stability through optimized training and extensive datasets. Finally, CogView4 [64], with its large-scale parameters, sets benchmarks in visual fidelity and resolution, highlighting ongoing innovation in generative image synthesis. 2.2 Customized Image Generation Customized image generation techniques have significantly advanced, enabling precise, contextspecific visual content [56]. DreamBooth [44] and HyperDreamBooth [46] established robust frameworks for efficient subject-driven fine-tuning from minimal references. Methods like Imagic [21] and Textual Inversion [9] embed new concepts into pretrained models for semantic editing without extensive retraining. InstantBooth [47] and GroundingBooth [58] streamline personalization, reducing computational costs and training time. Multimodal models such as Kosmos-G [37], UNIMOG [30], and Emu3 [55] expand personalization capabilities through multimodal integration and semantic understanding. BLIP-Diffusion [29] and IP-Adapter [60] enhance visual grounding between textual prompts and personalized features. InstantID [53] specializes in identity-aware personalization with high-fidelity identity preservation. Recently, Personalize Anything [7] and DreamO [34] have further advanced the field, enabling versatile, contextually adaptive image synthesis. 2.3 Benchmarks and Metrics for Image Generation Recent benchmarks and metrics comprehensively evaluate generative image models. DreamBench++ [38] and GenAI-Bench [28] systematically assess generative AI across diverse tasks, while PartiPrompts [61] and Gecko [57] provide specialized datasets for prompt-based generation fidelity. T2I-CompBench and T2I-CompBench++ [19] target compositional complexity and context understanding. DPG-Bench [13] focuses on perceptual metrics, whereas GenEval [10] and HEIM [27] offer robust frameworks for systematic comparison. Q-Eval-100K [63] and T2I-FactualBench [20] specifically evaluate factual consistency and quality alignment. Additionally, LMM4LMM [51] assesses multimodal language models for image generation, and EvalMuse-40K [12] provides extensive benchmarking of image quality and model performance."
        },
        {
            "title": "3 Data Curation",
            "content": "3.1 Overview Multi-modal image generation commonly involves both reference images and text prompts as inputs. Accordingly, our benchmarks data collection is structured into two components: grouped image collection and text prompt generation (as shown in Fig. 3). We begin by extracting entities from prompts used in existing text-to-image (T2I) benchmarks (such as [28, 27, 57]). After collecting over 2,000 distinct entities, we retain the 207 most frequent entities for subsequent use. 3 Figure 2: Statistics of the tags in MMIG-Bench. Top-left: Data distribution of compositional categories and high-level categories for text in T2I task. Bottom-left: Data distribution of text prompts in customization task. Right: Statistics of classes for the reference images. 3.1.1 Prompting GPT for Text Prompt Generation To enable scalable and diverse prompt generation, we use GPT-4o with several predefined instruction templates, as illustrated in Fig. 3. By providing entities and instruction templates as inputs, we generate total of 4,350 synthetic prompts covering both tasks. Furthermore, we manually select 500 human-written prompts from prior work [28, 8]. To ensure broad coverage of semantic aspects, we organize prompts into two main categories: compositional and high-level. The compositional category includes five sub-categories: object, counting, attribute, spatial relations (e.g., next to, atop, behind), and complex relations (e.g., pour into, toss, chase). The high-level category contains seven sub-categories, including style (fixed pattern, natural, professional, human-written), realism (imaginative), and common sense (negation, comparison). To better control the aspects, style, and structure of the prompts, we design eight instruction templates, using the T2I task as an example. When prompts require compositionality and adherence to specific structure, we use the following format: [scene description (optional)] + [number][attribute][entity1] + [interaction (spatial or action)] + [number (optional)][attribute][entity2]. For prompts to resemble natural, human-written language, more flexible instruction is used: Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", (optional) \"attributes / adjective\" that describes the entities, (optional) \"spatial or action interactions\" between entities, and (optional) \"background description. The full set of templates is provided in the Appendix. To ensure the quality and safety of the generated prompts, we further filter out toxic or low-quality content (see Sec. 3.4), and utilize FineMatch [17] to generate dense labels (see Sec. 3.3.1), making the dataset more flexible and suitable for research applications. 3.2 Collecting Grouped Subject Images Grouped reference images which are object-centric and realistic are usually missing from the previous benchmarks. However, multiple reference images have proven effective across various tasks, including image customization [44, 24, 65], video generation [22] and 3D reconstruction [52]. To address this gap, we collect large set of grouped reference images. The target objects are selected from the 207 common entities we previously identified. We employ annotators to curate grouped object images from Pexels [39] following these guidelines: (1) each group contains 35 images of the same object; (2) within each group, the object appears in varying poses or views; and (3) objects with complex logos or textures are prioritized. Additionally, we collect artistic images in 12 styles (e.g., sketch, low-poly, oil painting) to support style transfer tasks. In total, we collect 1,750 images across 386 groups, covering four main categoriesanimals, humans, objects, and stylesas shown in Fig. 2 (right). To ensure quality, we apply filtering and cropping to remove unrelated content from the images. Based on the entities in the collected images, we generate corresponding text prompts using the aforementioned procedure. 4 Figure 3: Our data curation pipeline for multi-modal image generation benchmarking. We begin by extracting 207 frequent entities from public T2I datasets. Using these entities, we generate diverse prompts with GPT-4o by prompting it with set of carefully designed instruction templates, which control the structure and style of the prompts (left). Simultaneously, we collect grouped reference images for each entity from free stock sources, with human annotators selecting 35 object-centric images per group that vary in pose or view (right). We further collect artistic images in 12 visual styles to support style transfer. The resulting dataset includes high-quality, structured text-image pairs for both T2I and customization. 3.3 Data Curation for Mid-Level Evaluation The goal of mid-level evaluation is to analyze the text-image alignment in fine-grained aspects, enabling more interpretable assessment on the generated details. To this end, we follow FineMatch [17] to analyze the fine-grained text-image alignment from the perspective of Object, Relation, Attribute, and Counting. We conduct specific data curation for these aspects by first using GPT-4o to extract all the aspect-related phrases from input prompts and then using in-context learning to prompt GPT-4o to generate the corresponding QA pairs. 3.3.1 Prompt Parsing We follow FineMatch [17] to curate aspect phrases from text prompts, employing GPT-4o for aspect graph parsing due to its superior compositional parsing capabilities. Specifically, GPT-4o is guided by explicit instructions and in-context examples to accurately extract and categorize phrases into four categories: objects, relations, attributes, and counting queries. 3.3.2 QA Pair Generation Following the prior VQA-based evaluation frameworks [59, 15, 4, 16, 49, 14, 32, 18], we proceed to generate high-quality question-answer (QA) pairs corresponding to each aspect phrase. Initially, domain experts manually curate set of exemplar QA pairs for each category (Object, Relation, Attribute, Counting). These manually curated QA pairs serve as contextual examples in the subsequent in-context learning phase. GPT-4o is then prompted with these examples to generate comprehensive set of QA pairs for the extracted aspect phrases, ensuring alignment with the fine-grained evaluation dimensions. This automated generation process is iteratively refined by adjusting instructions and examples based on preliminary outputs to improve coverage, clarity, and consistency. 5 3.4 Human Verification To guarantee dataset quality, interpretability, and reliability, we engage trained human annotators in structured verification process. Annotators perform multiple quality assurance tasks, including: ❶ Toxicity and Appropriateness Filtering: Annotators screen generated QA pairs for toxic, offensive, or inappropriate content, ensuring ethical compliance and usability in research settings.❷ QA Pair Correction and Validation: Each QA pair generated by GPT-4o undergoes meticulous human validation to confirm the logical coherence, accuracy, and relevance to the original aspect phrase. Annotators refine ambiguous questions, corrected factual inaccuracies, and ensure precise correspondence between questions and answers. ❸ Aspect Phrase Refinement: Extract aspect phrases were scrutinized and refined for linguistic clarity and semantic precision. Annotators review each phrase to ensure they correctly and comprehensively represent the intended compositional aspects (Object, Relation, Attribute, Counting). After these rigorous human verification steps, we obtain high-quality dataset consisting of 28,668 (16,819 for T2I tasks and 11,849 for Customization tasks) validated QA pairs, explicitly designed to support detailed analyses of fine-grained text-image alignment."
        },
        {
            "title": "4 Proposed Metrics - MMIG-Bench",
            "content": "4.1 Low-Level Evaluation Metrics The goal of low-level evaluation is to assess artifacts in the generated images and to evaluate the low-level feature similarity between the generated images and the prompt, as well as between the generated images and the reference images. To achieve this, we leverage previous evaluation metrics: CLIP-Text [43]: measures the semantic alignment between the generated image and input prompt; CLIP-Image, DINOv2 [36], and CUTE [23]: measures identity preservation; PAL4VST [62]: measures the amount of generative artifacts using segmentation model. These metrics collectively provide comprehensive assessment of the visual quality and consistency. 4.2 Mid-Level Evaluation Metrics The goal of mid-level evaluation is to assess the fine-grained semantic alignment of generated images with text prompts. We use the collected QA pairs corresponding to the four aspects (as described in Section 3.3) to design new interpretable evaluation framework, Aspect Matching Score (AMS). 4.2.1 Aspect Matching Score Formally, given prompt , we extract set of aspect phrases {A1, A2, . . . , An} and generate corresponding set of VQA pairs {(Q1, Ans1), (Q2, Ans2), . . . , (Qn, Ansn)}. These questions are designed to probe whether the generated image faithfully reflects the semantics of each aspect. To compute the alignment score, we use Qwen-VL2.5-72B [1] to answer each question Qi based on the generated image I, resulting in predicted answers { ˆAns1, ˆAns2, . . . , ˆAnsn}. We then compare each prediction ˆAnsi with the ground truth answer Ansi to assess correctness. We define the Aspect Matching Score as the proportion of correctly answered VQA questions: AMS(I, ) = 1 (cid:88) 1( ˆAnsi = Ansi), (1) i=1 where 1() is an indicator function that returns 1 if the predicted answer exactly matches the ground truth and 0 otherwise. AMS provides direct and interpretable measure of how well the generated image aligns with each semantic component of the prompt. higher AMS indicates better fine-grained alignment, capturing failures that coarse-level metrics often miss. 4.3 High-Level Evaluation Metrics The goal of high-level evaluation is to evaluate image aesthetics and human preference in the generated images. To achieve this, we leverage previous evaluation metrics, such as Aesthetic, HPSv2 and 6 PickScore. These metrics offer comprehensive assessment of the visual appeal and alignment with human preferences in the generated outputs."
        },
        {
            "title": "5 Experiments",
            "content": "Table 1: Quantitative comparison is conducted across images generated by 12 different text-to-image models using 2,100 well-designed prompts. Most models generate images at the default resolution of 1024 1024, except for the two autoregressive models, which produce outputs at 384 384, and GPT-4o and Gemini-2.0-Flash produce images with variable, non-fixed resolutions. indicates higher is better and indicates lower is better. The best and second-best results are in bold and underlined, respectively. Low Level Mid Level High Level Method CLIP-T PAL4VST AMS Human Aesthetic HPSv2 PickScore SDXL [41] Photon-v1 [40] Lumina-2 [42] HunyuanDit-v1.2 [31] Pixart-Sigma-xl2 [2] Flux.1-dev [25] SD 3.5-large [6] HiDream-I1-Full [50] 33.529 33.296 33.281 33.701 33.682 33.017 33.873 33.876 Diffusion Models 14.340 2.947 15.531 8.024 9.283 2.171 6.359 1.522 79.08 77.12 84.11 83.61 83.18 84.44 85.33 89.65 72.29 69.49 73.18 74.89 76.65 76.44 77.04 83. Autoregressive Models JanusFlow [33] Janus-Pro-7B [3] 31.498 33.358 365.663 31.954 70.25 85.35 75.69 80. API-based Models Gemini-2.0-Flash [11] GPT-4o [35] 32.433 32.380 11.053 3.497 85.35 82.57 81.98 81. 6.337 6.391 6.048 6.379 6.409 6.433 6.318 6.457 5.221 6.038 6.102 6.719 0.277 0.284 0.287 0.300 0.304 0.307 0.294 0.321 0.209 0.275 0.275 0. 0.120 0.088 0.116 0.144 0.165 0.210 0.157 0.450 0.031 0.129 0.110 0.263 Table 2: Quantitative comparison is conducted across imagees generated by 6 different multi-modal image generation models using 1,690 samples. Most models generate images 3 times per multi-modal input except GPT-4o at the default resolution of 1024 1024, except for Blip Diffusion, which produce outputs at 512 512, and GPT-4o produce images with variable, non-fixed resolutions. indicates higher is better and indicates lower is better. The best and second best results are in bold and underlined, respectively. Low Level Mid Level High Level Method CLIP-T CLIP-I DINOv2 CUTE PAL4VST BLIPVQA AMS Aesthetic HPSv2 PickScore Diffusion Models BLIP Diffusion[29] DreamBooth [45] Emu2 [48] Ip-Adapter-XL [60] MS Diffusion [54] 26.137 24.227 28.410 28.577 31.446 80.286 88.758 79.026 85.297 77.827 26.232 38.961 31.831 34.177 23.600 69.681 79.780 71.132 74.995 71.306 56.780 43.535 10.461 8.531 4. 0.247 0.108 0.378 0.290 0.496 41.59 28.00 53.13 51.10 71.40 5.830 5.368 5.639 5.840 5.979 0.213 0.179 0.243 0.233 0.271 0.032 0.019 0.066 0.073 0.143 API-based Models GPT-4o [35] 33.527 75.152 25.174 64.776 1. 0.672 90.90 6.368 0.289 0.550 5.1 Human Evaluation To evaluate the semantic preservation of state-of-the-art generation models and compare the human correlation of VQA-based metrics, we conduct five user studies. We assess 12 text-to-image (T2I) models across five aspects: attribute, relation, counting, object, and general prompt following. For each of the first four aspects, 150 prompts are randomly selected; for the last, 300 prompts are used. In each study, users are shown prompt and generated image, and asked to rate semantic alignment on 15 scale based on the target aspect (see Appendix for details). In total, we collect 32.4k ratings from over 8,000 Amazon Mechanical Turk users. Results are reported in Table 3. 7 Figure 4: qualitative study of text-only (top) and text-image-conditioned (bottom) generation methods on MMIG-Bench. 8 Table 3: Comparison of VQA-based metrics: BLIPVQA [20], VQ2 [59], DSG [4], and our AMS . Method BLIPVQA VQ2 DSG AMS Human Diffusion Models SDXL Photon-v1 Lumina-2 HunyuanDiT-v1.2 Pixart-Sigma-xl2 Flux.1-dev SD 3.5-large HiDream-I1-Full 0.433 0.440 0.517 0.513 0.521 0.511 0.525 0.572 69.07 66.84 72.51 73.13 71.51 71.41 73.28 75.09 87.63 86.26 90.12 89.77 89.69 83.33 91.41 92. 79.08 77.12 84.11 83.61 83.18 84.44 85.33 89.65 Autoregressive Models JanusFlow [33] Janus-Pro [3] 0.390 0.530 57.24 67.41 85.43 92. 70.25 85.35 API-based Models Gemini-2.0-Flash GPT-4o 0.495 0.497 72.01 70.34 92.93 89. 85.40 82.57 72.29 69.49 73.18 74.89 76.65 76.44 77.04 83.18 75.69 80.36 81.98 81.02 5.2 Correlation of Automated Metrics with Human Annotations To assess the alignment of automated metrics with human, we compute Spearman correlations against human annotations. As shown in Table 3, our proposed AMS achieves the highest correlation (ρ = 0.699), surpassing DSG (ρ = 0.692), VQ2 (ρ = 0.399), and BLIPVQA (ρ = 0.147). This demonstrates the effectiveness of AMS as reliable metric for compositional T2I evaluation. 5.3 Leaderboard We compare the performance across state-of-the-art models in T2I task (Tab. 1) and customization task (Tab. 2) using our multi-level evaluation framework. Based on the scores, we can derive the following insights: In T2I task: (1) Compared with diffusion models, autoregressive models (JanusFlow and Janus-Pro7B) perform significantly worse in visual quality, as they are more likely to generate artifacts, and have the lowest aesthetic and human preference scores. (2) HiDream-I1, the largest model with 17B parameters, excels all the other generators; it takes advantage of rectified flow and the VAE from FLUX.1-schnell. (3) FLUX.1-dev (the second largest model with 12B parameters) stands at the second place for most metrics. (4) The performance of HiDream-I1 and FLUX.1-dev suggests the importance of scaling generative models. (5) Although GPT-4o is not the best model in all metrics, it shows very robust generation abilities competitive to the best model in each category. In customization task, we draw the following conclusions: (1) In most low-level metrics that evaluates identity preservation, DreamBooth is the strongest model; its multi-view inputs and testtime finetuning greatly enhances the identity learning. (2) GPT-4o cannot preserve the identity well, this ability is even worse than some early models like Emu2 and the two encoder-based models (BLIP Diffusion and IP-Adapter). (3) GPT-4o comes at the first place in visual quality and semantic alignment. (4) MS Diffusion is often the second best in terms of generation quality, validating the effectiveness of the grounding resampler and MS cross-attention. However, it shows an unsatisfactory ability on identity preservation. 5.4 Qualitative Analysis We present qualitative results for multi-modal image generation in Fig. 4. The top six rows illustrate generations conditioned on text only; the bottom three rows show generations conditioned on both image and text. Key observations are as follows: In the T2I task, (1) Hunyuan-DiT-V1.2 struggles with entity generation, frequently missing objects, duplicating them, or generating incorrect ones; (2) Pixart-Sigma-XL2 exhibits stronger visual artifacts (e.g., around benches, chairs, and computers), consistent with its lower PAL4VST scores from Tab. 1. 9 In customization task, (1) Non-rigid objects (e.g., dogs) tend to appear in more diverse poses; (2) MS-Diffusion performs worst in preserving object identity, while DreamBooth performs best; This highly aligns with the CLIP-I and DINOv2 scores in Tab. 2. (3) Despite its strength in identity preservation, DreamBooth often fails to generate the correct scene, actions, or additional entities, indicating poor compositional alignment."
        },
        {
            "title": "6 Discussions and Conclusions",
            "content": "We present MMIG-Bench, the first benchmark to treat multi-modal image generation as single task rather than two disjoint tasks. We demonstrate that by pairing 1,750 multi-view reference images with 4,850 densely annotated prompts, MMIG-Bench enables side-by-side evaluation of pure text-to-image, image-conditioned customization, and every hybrid in between. The proposed three-level evaluation framework provides comprehensive, interpretable assessment that addresses the evaluation gaps in both T2I and customization tasks. The evaluation metrics prove to be well aligned with human preferences by comparing them with 32k human ratings across 17 state-of-the-art models. The in-depth assessments of the image generators on our benchmark provide insights on how the model capacity, model architecture, and other factors influence the image quality. One limitation is that the human ratings do not yet cover visual quality; we plan to expand future studies to such dimensions. We will publicly release the data, code, and leaderboard to encourage transparent comparison and guide future advances in architecture design, data curation, and training strategy."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [2] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-σ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, 2024. URL https://api.semanticscholar.org/CorpusID:268264262. [3] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Janus-pro: Unified multimodal understanding and generation with data and model scaling. ArXiv, abs/2501.17811, 2025. URL https://api.semanticscholar.org/ CorpusID:275954151. [4] Jaemin Cho, Yushi Hu, Roopal Garg, Peter Anderson, Ranjay Krishna, Jason Baldridge, Mohit Bansal, Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained evaluation for text-to-image generation. ArXiv, abs/2310.18235, 2023. URL https://api.semanticscholar.org/CorpusID:264555374. [5] dreambench. Dreambench, 2022. https://github.com/nousr/dream-bench. [6] Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. ArXiv, abs/2403.03206, 2024. URL https://api.semanticscholar.org/CorpusID:268247980. [7] Haoran Feng, Zehuan Huang, Lin Li, Hairong Lv, and Lu Sheng. Personalize anything for free with diffusion transformer. arXiv preprint arXiv:2503.12590, 2025. [8] Xingyu Fu, Muyu He, Yujie Lu, William Yang Wang, and Dan Roth. Commonsense-t2i challenge: Can text-to-image generation models understand commonsense? arXiv preprint arXiv:2406.07546, 2024. [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 10 [10] Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36:5213252152, 2023. [11] Google. Gemini 2.0 flash, 2025. https://blog.google/technology/google-deepmind/ google-gemini-ai-update-december-2024/#gemini-2-0-flash. [12] Shuhao Han, Haotian Fan, Jiachen Fu, Liang Li, Tao Li, Junhui Cui, Yunqiu Wang, Yang Tai, Jingwei Sun, Chunle Guo, and Chongyi Li. Evalmuse-40k: reliable and fine-grained benchmark with comprehensive human annotations for text-to-image generation model evaluation. ArXiv, abs/2412.18150, 2024. URL https://api.semanticscholar.org/CorpusID: 274992412. [13] Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang Yu. Ella: Equip diffusion models with llm for enhanced semantic alignment. arXiv preprint arXiv:2403.05135, 2024. [14] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah Smith, and Jiebo Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699, 2022. [15] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A. Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2034920360, 2023. URL https://api.semanticscholar.org/CorpusID:257636562. [16] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2040620417, 2023. [17] Hang Hua, Jing Shi, Kushal Kafle, Simon Jenni, Daoan Zhang, John Collomosse, Scott Cohen, and Jiebo Luo. Finematch: Aspect-based fine-grained image and text mismatch detection and correction. In European Conference on Computer Vision, pages 474491. Springer, 2024. [18] Hang Hua, Yunlong Tang, Ziyun Zeng, Liangliang Cao, Zhengyuan Yang, Hangfeng He, Chenliang Xu, and Jiebo Luo. Mmcomposition: Revisiting the compositionality of pre-trained vision-language models. arXiv preprint arXiv:2410.09733, 2024. [19] Kaiyi Huang, Chengqi Duan, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2icompbench++: An enhanced and comprehensive benchmark for compositional text-to-image generation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 47:35633579, 2023. URL https://api.semanticscholar.org/CorpusID:259847295. [20] Ziwei Huang, Wanggui He, Quanyu Long, Yandi Wang, Haoyuan Li, Zhelun Yu, Fangxun Shu, Long Chan, Hao Jiang, Leilei Gan, et al. T2i-factualbench: Benchmarking the factuality of text-to-image models with knowledge-intensive concepts. arXiv preprint arXiv:2412.04300, 2024. [21] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. [22] Xianghao Kong, Qiaosong Qi, Yuanbin Wang, Anyi Rao, Biaolong Chen, Aixi Zhang, Si Liu, and Hao Jiang. Profashion: Prototype-guided fashion video generation with multiple reference images. arXiv preprint arXiv:2505.06537, 2025. [23] Klemen Kotar, Stephen Tian, Hong-Xing Yu, Daniel L. K. Yamins, and Jiajun Wu. Are these the same apple? comparing images based on object intrinsics. ArXiv, abs/2311.00750, 2023. URL https://api.semanticscholar.org/CorpusID:264935263. [24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multiconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. [25] Black Forest Labs. Flux.1, 2024. https://bfl.ai/announcements/24-08-01-bfl. [26] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [27] Tony Lee, Michihiro Yasunaga, Chenlin Meng, Yifan Mai, Joon Sung Park, Agrim Gupta, Yunzhi Zhang, Deepak Narayanan, Hannah Teufel, Marco Bellagente, et al. Holistic evaluation of text-to-image models. Advances in Neural Information Processing Systems, 36:6998170011, 2023. [28] Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia, Pengchuan Zhang, Graham Neubig, and Deva Ramanan. Genai-bench: Evaluating and improving compositional text-to-visual generation. ArXiv, abs/2406.13743, 2024. URL https: //api.semanticscholar.org/CorpusID:270619531. [29] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36:3014630166, 2023. [30] Wei Li, Xue Xu, Jiachen Liu, and Xinyan Xiao. Unimo-g: Unified image generation through multimodal conditional diffusion. arXiv preprint arXiv:2401.13388, 2024. [31] Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiao-Ting Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Mengxi Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yang-Dan Tao, Jianchen Zhu, Kai Liu, Si-Da Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Dingyong Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. ArXiv, abs/2405.08748, 2024. URL https://api.semanticscholar.org/CorpusID:269761491. [32] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pages 366384. Springer, 2024. [33] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. ArXiv, abs/2411.07975, 2024. URL https: //api.semanticscholar.org/CorpusID:273969525. [34] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. [35] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. URL https: //arxiv.org/abs/2303.08774. [36] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Q. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russ Howes, Po-Yao (Bernie) Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Huijiao Xu, Hervé Jégou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. ArXiv, abs/2304.07193, 2023. URL https://api. semanticscholar.org/CorpusID:258170077. [37] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, and Furu Wei. Kosmos-g: Generating images in context with multimodal large language models. ArXiv, abs/2310.02992, 2023. URL https://api.semanticscholar.org/CorpusID:263620748. [38] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. ArXiv, abs/2406.16855, 2024. URL https: //api.semanticscholar.org/CorpusID:270702690. 12 [39] Pexels. Pexels, 2014. https://www.pexels.com/. [40] Photon78. Photon-v1. https://civitai.com/models/84728/photon78, 2023. Accessed: 2025-05-06. [41] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. ArXiv, abs/2307.01952, 2023. URL https://api.semanticscholar.org/ CorpusID:259341735. [42] Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, Yiting Lu, Jiakang Yuan, Xinyue Li, Dongyang Liu, Xiangyang Zhu, Manyuan Zhang, Will Beddow, Erwann Millon, Victor Perez, Wen-Hao Wang, Conghui He, Bo Zhang, Xiaohong Liu, Hongsheng Li, Yu-Hao Qiao, Chang Xu, and Peng Gao. Lumina-image 2.0: unified and efficient image generative framework. ArXiv, abs/2503.21758, 2025. URL https://api.semanticscholar.org/ CorpusID:277349538. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. URL https://api.semanticscholar.org/ CorpusID:231591445. [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. 2022. [45] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023. [47] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-toimage generation without test-time finetuning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 85438552, 2024. [48] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1439814409, 2024. [49] Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, Zeliang Zhang, et al. Vidcomposition: Can mllms analyze compositions in compiled videos? arXiv preprint arXiv:2411.10979, 2024. [50] HiDream-AI Team. Hidream-i1: 17b parameter open chinese text-to-image generation model. https://github.com/HiDream-ai/HiDream-I1, 2024. Accessed: 2025-05-14. [51] Jiarui Wang, Huiyu Duan, Yu Zhao, Juntong Wang, Guangtao Zhai, and Xiongkuo Min. Lmm4lmm: Benchmarking and evaluating large-multimodal image generation with lmms. 2025. URL https://api.semanticscholar.org/CorpusID:277741112. [52] Peng Wang, Hao Tan, Sai Bi, Yinghao Xu, Fujun Luan, Kalyan Sunkavalli, Wenping Wang, Zexiang Xu, and Kai Zhang. Pf-lrm: Pose-free large reconstruction model for joint pose and shape prediction. arXiv preprint arXiv:2311.12024, 2023. [53] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [54] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multisubject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. [55] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, Yingli Zhao, Yulong Ao, Xuebin Min, Tao Li, Boya Wu, Bo Zhao, Bowen Zhang, Lian zi Wang, Guang Liu, Zheqi He, Xi Yang, Jingjing Liu, Yonghua Lin, Tiejun Huang, and Zhongyuan Wang. Emu3: Next-token prediction is all you need. ArXiv, abs/2409.18869, 2024. URL https://api.semanticscholar.org/CorpusID: 272968818. [56] Yuxiang Wei, Yiheng Zheng, Yabo Zhang, Ming Liu, Zhilong Ji, Lei Zhang, and Wangmeng Zuo. Personalized image generation with deep generative models: decade survey. arXiv preprint arXiv:2502.13081, 2025. [57] Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajic, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, and Aida Nematzadeh. Revisiting text-to-image evaluation with gecko: On metrics, prompts, and human ratings. arXiv preprint arXiv:2404.16820, 2024. [58] Zhexiao Xiong, Wei Xiong, Jing Shi, He Zhang, Yizhi Song, and Nathan Jacobs. Groundingbooth: Grounding text-to-image customization. arXiv preprint arXiv:2409.08520, 2024. [59] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran. O. Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. ArXiv, abs/2305.10400, 2023. URL https://api.semanticscholar. org/CorpusID:258740893. [60] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [61] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich text-to-image generation. Trans. Mach. Learn. Res., 2022, 2022. URL https://api.semanticscholar.org/CorpusID:249926846. [62] Lingzhi Zhang, Zhengjie Xu, Connelly Barnes, Yuqian Zhou, Qing Liu, He Zhang, Sohrab Amirghodsi, Zhe Lin, Eli Shechtman, and Jianbo Shi. Perceptual artifacts localization for image synthesis tasks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 75797590, October 2023. [63] Zicheng Zhang, Tengchuan Kou, Shushi Wang, Chunyi Li, Wei Sun, Wei Wang, Xiaoyu Li, Zongyu Wang, Xuezhi Cao, Xiongkuo Min, Xiaohong Liu, and Guangtao Zhai. Q-eval-100k: Evaluating visual quality and alignment level for text-to-vision content. ArXiv, abs/2503.02357, 2025. URL https://api.semanticscholar.org/CorpusID:276775486. [64] Wendi Zheng, Jiayan Teng, Zhuoyi Yang, Weihan Wang, Jidong Chen, Xiaotao Gu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogview3: Finer and faster text-to-image generation via relay diffusion. ArXiv, abs/2403.05121, 2024. URL https://api.semanticscholar.org/ CorpusID:268297194. [65] Zhuofan Zong, Dongzhi Jiang, Bingqi Ma, Guanglu Song, Hao Shao, Dazhong Shen, Yu Liu, and Hongsheng Li. Easyref: Omni-generalized group image reference for diffusion models via multimodal llm. arXiv preprint arXiv:2412.09618, 2024."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Qualitative Results of MMIG-Bench Data Figure 5: Word clouds of text prompts for the text-only generation (T2I) task (left) and the multimodal generation task (right). Figure 5 visually summarizes the prominent semantic elements in the benchmark prompts for text-only (T2I) and multimodal generation tasks. The differentiation of the word clouds reflects task-specific features of MMIG-Bench, emphasizing spatial and descriptive details in T2I tasks, while multimodal tasks more frequently involve social and interactive scenarios. A.2 Quantitative and Qualitative Results of AMS Figure 6: Aspect Distribution of the QA pairs of AMS. Table 4: Aspect-level correlation (ρ) between AMS and human scores across four aspects. Aspect Objects Relations Attributes Counting Overall Spearman ρ 0.469 0.909 0. 0.839 0.699 As depicted in Figure 6, the distribution of aspect types differs notably between the text-only generation (T2I) and multi-modal generation tasks. In the T2I setting, Objects dominate with 38.3%, while Attributes and Relations also constitute substantial proportions (33.9% and 25.4%, respectively). In multi-modal generation, Objects and Attributes remain prominent (46.1% and 31.6%, respectively), but the relative proportion of Relations decreases significantly (22.2%). The presence of Counting (0.1%) questions suggests this aspect is less frequent in the customized T2I generation task. 15 Figure 7: The AMS of different models on the text-only generation (T2I) task (left) and the multimodal generation task (right). Figure 7 presents comparative analysis of aspect-wise AMS across different models on the text-only generation (T2I) task and the multimodal generation task, highlighting their performance on four key compositional dimensions: Objects, Relations, Attributes, and Counting. On the T2I task, large-scale foundation models such as HiDream-I1, HunyuanDit-v1.2, and SD 3.5-large consistently achieve high AMS scores across aspects, particularly excelling in Objects and Attributes. Specifically, HunyuanDitv1.2 demonstrates superior Counting performance, underscoring strong numerical understanding In contrast, for the multimodal generation task, GPT-4o significantly in text-driven scenarios. outperforms other diffusion-based models, particularly in complex compositional aspects such as Relations and Counting, highlighting its robust capability in interpreting and synthesizing multimodal inputs. Models like DreamBooth and BLIP-Diffusion show markedly weaker performances, especially in Relations and Counting. These AMS-based comparisons effectively illustrate clear distinctions in compositional understanding capabilities between text-only and multimodal generation settings, emphasizing the metrics sensitivity in capturing fine-grained model differences. Table 4 further provides quantitative evidence of AMSs effectiveness: AMS achieves high Spearman correlation with human judgment, particularly in the Relations (0.909) and Counting (0.839) aspects. This indicates AMS reliably captures complex compositional semantics and aligns closely with human evaluative standards, emphasizing its robustness as metric for fine-grained image-text alignment evaluation. A.3 Experiments Compute Resources We conduct our experiments on 8 Nvidia A100 GPUs. A.4 Broader Impact Multi-modal image generation has wide-ranging applications in areas such as creative design, virtual reality, advertisement, and human-computer interaction. However, the powerful capabilities of these models also pose potential risks, particularly in generating toxic, biased, or harmful visual content. For instance, the human-centric images in our benchmark could be misused to produce misleading or inappropriate material. MMIG-Bench aims to support fair and responsible research by providing diverse and high-quality dataset while actively mitigating these risks. To this end, we apply thorough filtering to remove toxic, sensitive, or low-quality content from our benchmark. Nevertheless, we encourage the community to consider ethical implications when developing and deploying such models and benchmarks. A.5 Instruction Templates for Prompt Generation We carefully design eight instruction templates to generate prompts that encompass compositionality, common sense, and diverse stylistic variations. For example, the first template follows fixed structure: [scene description] + [attribute][entity1] + [interaction (spatial or action)] + [attribute][entity2], which guides GPT-4o to produce prompts that include background context, objects, attributes, and relations. In later templates, we provide GPT4o with detailed instructions and examples to encourage the generation of prompts that are natural, imaginative, professionally written, or that incorporate elements such as negation, comparison, and numeracy. Instruction Template for T2I Prompts Generation (fixed pattern) Please generate natural sentences following format of \"[scene description] + [attribute][entity1] + [interaction (spatial or action)] + [attribute][entity2]\"; follow the rules below: 1. \"entity\" should be common objects; e.g., chair, dog, car, lamp, etc. \"entity2\" is optional. Use \"{entity}\" as entity1 here. 2. \"attribute\" should be an adjective that describes \"shape / color / material / size / condition / etc.\" 3. \"interaction\" should describe the relationship between \"entity1\" and \"entity2\". \"spatial interaction\" can be \"on the left of / on the right of / on / on top of / on the bottom of / beneath / on the side of / neighboring / next to / touching / in front of / behind / with / etc.\"; \"action interaction\" can be any action happening between \"entity1\" and \"entity2\", such as \"play with, eat, sit, place, hold, etc.\" 4. \"scene description\" is the background where the entities appear. It can contain other objects. It is optional. 5. The \"interaction action\" can be either in active or passive voice. 6. The order of these terms should not be fixed, as long as the sentence still looks natural. E.g., \"scene description\" can be put at the end. Instruction Template for T2I Prompts Generation (natural) Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", (optional) \"attributes / adjective\" that describes the entities, (optional) \"spatial or action interactions\" between entities, and (optional) \"background description\". Randomly ignore one or more items from [attributes, interactions, background]. One of the entities should be \"{entity}\". Instruction Template for T2I Prompts Generation (unreal) Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", (optional) \"attributes / adjective\" that describes the entities, (optional) \"spatial or action interactions\" between entities, and (optional) \"background description\". Note that: 1. Randomly ignore one or more items from [attributes, interactions, background]. 2. The description should be imaginative. If imaginative, an example: \"A robot and dolphin dancing under the ocean, surrounded by swirling schools of fish\". 3. Avoid repeating sentences youve already generated. A.6 Text-Image-Conditioned Dataset Overview An overview of our comprehensive MMIG-Bench is shown in Fig. 8. Based on the 207 common entities we curated, we collect 386 reference image groups, each containing 35 multi-view, object17 Instruction Template for T2I Prompts Generation (professional) Imagine that you are professional designer, please write prompt for testing text-to-image diffusion models. The prompts should look like natural sentences. Please do not include descriptions about styles, such as \"minimalism meets hygge vibes / editorial photoshoot style / baroque detail / etc.\". One of the entities/nouns should be \"{entity}\". Instruction Template for T2I Prompts Generation (negation) Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", (optional) \"attributes / adjective\" that describes the entities, (optional) \"spatial or action interactions\" between entities, and (optional) \"background description\". Note that: 1. Randomly ignore one or more items from [attributes, interactions, background]. 2. It should include the logic of \"negation\", such as the examples below: \"The girl with glasses is drawing, and the girl without glasses is singing.\", \"In the supermarket, man with glasses pays man without glasses.\", \"The larger person wears yellow hat and the smaller person does not.\", \"Adjacent houses stand side by side; the left one sports chimney, while the right one has none.\", \"A tailless, not black, cat is sitting.\", \"A smiling girl with short hair and no glasses.\", \"A bookshelf with no books, only single red vase.\". One of the entities/nouns should be \"{entity}\". Instruction Template for T2I Prompts Generation (comparison) Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", (optional) \"attributes / adjective\" that describes the entities, (optional) \"spatial or action interactions\" between entities, and (optional) \"background description\". Note that: 1. Randomly ignore one or more items from [attributes, interactions, background]. 2. It should have the logic of \"comparison\", such as the examples below: \"In magnificent castle, red dragon sits and green dragon flies.\", \"A magician holds two books; the left one is open, the right one is closed.\", \"One cat is sleeping on the table and the other is playing under the table.\". \"A green pumpkin is smiling happily, while red pumpkin is sitting sadly.\", One of the entities/nouns should be \"{entity}\". Instruction Template for T2I Prompts Generation (counting) Please generate prompts in NATURAL format. It should contain one or more \"entities / nouns\", and \"numeracy\" that describes the number of the entity. Follow the six examples below: 1. four dogs played with two toys. 2. two chickens, four pens and one lemon. 3. Five cylindrical mugs beside two rectangular napkins. 4. three helicopters buzzed over two pillows. 5. Three cookies on plate. 6. group of sheep being led by two shepherds across green field. Avoid repeating sentences youve already generated. 18 Instruction Template for T2I Prompts Generation (numeracy in fixed structure) Please generate natural sentences following format of \"[scene description (optional)] + [number][attribute][entity1] + [interaction (spatial or action)] + [number (optional)][attribute][entity2]\"; follow the rules below: 1. \"entity\" should be common objects; e.g., chair, dog, car, lamp, etc. \"entity2\" is optional. Use \"entity\" as entity1 here. 2. \"attribute\" should be an adjective that describes \"shape / color / material / size / condition / etc.\" 3. \"number\" should be \"two/three/four/...\" before the attribute, indicating the number of entities. It is optional for entity2. 4. \"interaction\" should describes the relationship between \"entity1\" and \"entity2\". \"spatial interaction\" can be \"on the left of / on the right of / on / on top of / on the bottom of / beneath / on the side of / neighboring / next to / touching / in front of / behind / with / and / etc.\"; \"action interaction\" can be any action happening between \"entity1\" and \"entity2\", such as \"play with, eat, sit, place, hold, etc.\" 5. \"scene description\" is the background where the entities appear. It can contain other objects. It is optional. 6. The \"interaction action\" can be either in active or passive voice. 7. The order of these terms should not be fixed, as long as the sentence still looks natural. E.g., \"scene description\" can be put at the end. Prompt Template for Text Prompts Aspect Extraction You need to analyze the query to aspect graph that matches all the objects, relations (e.g.,spatial relations, action, complex relation), attributes, and counting (number of objects). Please ignore all the redundant phrases that are irrelevant to the contents of the image in the query, for example, photo/picture of something, something in the background etc., should not appear in the parsed graph. Please also remove all the redundent aspects in the parsed graph. Here are some examples, if there are no such aspect, you can use an empty list to represent: For the counting information, please ignore the object numbers that less than 2 (<2). Context: group of women is playing the piano in the room. {Objects:[woman,room], Other Relations:[play piano], Spatila Relations:[in, (the room)], Attributes:[], Counting:[a group of, (Non-specific quantity of woman)]} Two Chihuahuas run after child on bicycle. {Objects:[Chihuahua,child,bicycle], Other Relations:[runs after, (Chihuahua runs after child),ride, (ride by the child)], Spatila Relations:[on, (child on bicycle)] Attributes:[Chihuahua, (Chihuahua is breed of dog)], Counting:[Two (number of Chihuahua)]} } Delta Boeing 777 taxiing on the runway. {Objects:[Delta Boeing 777,runway], Other Relations:[taxiing on, ( the runway)], Spatial Relations:[on (plane on the runway)], Attributes:[None], Counting:[]} Please extrace all the aspects precisely! 19 Prompt Template for AMS QA Pair Generation Given an image and its corresponding caption, generate Visual Question Answering pairs that assess the presence of specific objects, attributes, relations, and counting information in the image. The questions should be phrased naturally, appropriate, and reasonable. Input: Caption: Two dogs are fighting over red Frisbee that is bent in half. Target Elements: {{\"Objects\": [\"dog\", \"Frisbee\"], \"Relations\": [\"fighting over, (dogs fighting over Frisbee)\"], \"Attributes\": [\"red, (color of Frisbee)\", \"bent in half, (condition of Frisbee)\"], \"Counting\": [\"two, (number of dogs)\"]}} Example Output (JSON): {{\"question\": \"Is there dog in the image?\", \"answer\": \"Yes\", \"Aspect\":Objects}}, {{\"question\": \"Is there Frisbee in the image?\", \"answer\": \"Yes\",\"Aspect\":Objects}}, {{\"question\": \"Are the dogs fighting over Frisbee?\", \"answer\": \"Yes\",\"Aspect\":Relations}}, {{\"question\": \"Is the Frisbee red?\", \"answer\": \"Yes\",\"Aspect\":Attributes}}, {{\"question\": \"Is the Frisbee bent in half?\", \"answer\": \"Yes\",\"Aspect\":Relations}}, {{\"question\": \"Are there two dogs in the image?\", \"answer\": \"Yes\",\"Aspect\":Counting}} If the counting aspect is related to one, (number of something), please ignore it! Please reduce the redundancy of the questions, dont repeat ! If the question includes relational referencessuch as friend, mother, daughter, etc.please specify the associated referent (for example, the womans friend). If the aspect entity has no practical significance, please ignore it. Input: Caption: Target Elements: Output (JSON): centric images, and generate 4,850 text prompts that include these entities. The prompts are densely labeled and exhibit rich, detailed semantics, covering compositionality, common sense, and styles. A.7 More Qualitative Results We show more visual comparisons of the state-of-the-art models in Fig. 9, 10 and 11. A.8 Human Evaluation Interface The Amazon Mechanical Turk interfaces used in the user studies are shown in Fig. 12-16. The study is divided into five categories to assess the compositionality of prompt-image alignment across different aspects: general prompt following  (Fig. 12)  , object  (Fig. 13)  , attribute  (Fig. 14)  , relation  (Fig. 15)  and numeracy  (Fig. 16)  . In each session, randomly selected prompt-image pair is presented to the user, who is then asked to rate the generation quality using 5-point scale. Each question is independently rated by three different workers to ensure reliability. 20 Figure 8: Overview of MMIG-Bench. Figure 9: More qualitative results of text-only generation methods on MMIG-Bench. 22 Figure 10: More qualitative results of text-only generation methods on MMIG-Bench. 23 Figure 11: More qualitative results of text-image-conditioned generation methods on MMIG-Bench. Figure 12: The interface of user study for general prompt following. 25 Figure 13: The interface of user study for prompt following on Object. 26 Figure 14: The interface of user study for prompt following on Attributes. Figure 15: The interface of user study for prompt following on Relations. 28 Figure 16: The interface of user study for prompt following on Numeracy."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "Purdue University",
        "University of Rochester"
    ]
}