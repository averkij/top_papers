{
    "paper_title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios",
    "authors": [
        "Shaohang Wei",
        "Wei Li",
        "Feifan Song",
        "Wen Luo",
        "Tianyi Zhuang",
        "Haochen Tan",
        "Zhijiang Guo",
        "Houfeng Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose a multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News, and TIME-Dial. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-Lite, a human-annotated subset to foster future research and standardized evaluation in temporal reasoning. The code is available at https://github.com/sylvain-wei/TIME , and the dataset is available at https://huggingface.co/datasets/SylvainWei/TIME ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 1 9 8 2 1 . 5 0 5 2 : r TIME: Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios Shaohang Wei1, Wei Li1, Feifan Song1, Wen Luo1 Tianyi Zhuang2, Haochen Tan2, Zhijiang Guo2, Houfeng Wang1 1State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University 2Huawei Noahs Ark Lab shaohang@stu.pku.edu.cn wanghf@pku.edu.cn {zhuangtianyi, haochen.tan}@huawei.com cartusguo@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend the real world. However, existing works neglect the real-world challenges for temporal reasoning: (1) intensive temporal information, (2) fast-changing event dynamics, and (3) complex temporal dependencies in social interactions. To bridge this gap, we propose multi-level benchmark TIME, designed for temporal reasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3 levels with 11 fine-grained sub-tasks. This benchmark encompasses 3 sub-datasets reflecting different real-world challenges: TIME-WIKI, TIME-NEWS, and TIMEDIAL. We conduct extensive experiments on reasoning models and non-reasoning models. And we conducted an in-depth analysis of temporal reasoning performance across diverse real-world scenarios and tasks, and summarized the impact of test-time scaling on temporal reasoning capabilities. Additionally, we release TIME-LITE, human-annotated subset to foster future research and standardized evaluation in temporal reasoning. 1"
        },
        {
            "title": "Introduction",
            "content": "Time serves as the thread that weaves together complex events in the real world. Effective temporal reasoning is crucial for Large Language Models (LLMs) to process and comprehend complex events with human-like understanding, particularly in applications requiring integration of historical data and real-time progress tracking. Despite good capabilities of current LLMs across wide range of reasoning tasks, including mathematical problem-solving [12, 15, 7] and code generation [21, 18, 24, 46], they still face challenges in managing temporal understanding in reality. Temporal reasoning in real-world contexts presents complex challenges: (1) the density of temporal information embedded within world knowledge, (2) the rapid evolution of event details over time, and (3) the complexity of temporal dependencies in social interactions, but existed benchmarks, like TimeBench[6] and TRAM[44] primarily focus onsimplified scenarios, such as basic temporal commonsense and relationships within short texts and simple QA tasks. Consequently, significant gap remains in exploring temporal reasoning in depth. On the other hand, temporal reasoning constitutes hierarchical framework of fine-grained abilities, which is also different from other reasoning tasks that focus on singular capabilities, but is still ignored by current works. For example, TReMu[13] involves only neuro-symbolic temporal reasoning while 1Code is available at https://github.com/sylvain-wei/TIME 2Dataset is available at https://huggingface.co/datasets/SylvainWei/TIME Preprint. neglecting temporal computation, and TCELongBench[56] overlooks fundamental temporal concept understanding. In contrast, robust evaluation framework should encompass both basic temporal abilities and complex event-event temporal reasoning, necessitating the development of new comprehensive benchmark. To address these limitations, we introduce TIME, multi-level comprehensive benchmark for evaluating temporal reasoning in LLMs across diverse real-world scenarios, comprising 38,522 instances. TIME consists of three datasets: TIME-WIKI assesses temporal reasoning in knowledgeintensive scenarios, TIME-NEWS evaluates temporal understanding in rapidly evolving news contexts, and TIME-DIAL examines temporal reasoning in complex interactive settings with extensive temporal dependencies in very-long dialogs. Additionally, we construct TIME-LITE, high-quality lightweight benchmark containing 938 carefully curated instances through manual annotation and verification, enabling efficient and reliable temporal reasoning evaluation. Both TIME and TIME-LITE feature multi-level task structure: (1) basic temporal understanding and retrieval, (2) temporal expression reasoning, and (3) complex temporal relationship reasoning, with each level incorporating multiple fine-grained dimensions to comprehensively assess temporal reasoning capabilities. Our main contributions can be summarized as follows: We introduce an innovative multi-level evaluation framework that systematically assesses temporal reasoning capabilities across different granularities in LLMs. We construct TIME, comprehensive benchmark that captures the complexity of temporal reasoning in diverse real-world scenarios, including knowledge-intensive, dynamic events, and multi-session interactive contexts. We conduct comprehensive evaluation and in-depth analysis on temporal reasoning for wide range of LLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Temporal Understanding in Natural Language Temporal understanding in Natural Language Processing (NLP) has rich history, initially focusing on extracting time expressions and temporal relationships [30, 39, 40, 41, 38, 26, 29, 33]. The advent of pre-trained language models facilitated the exploration of more complex phenomena, including implicit time reasoning [20, 35, 36] and cross-event temporal relationships [58, 45, 48, 17, 35, 36, 34]. Research has also addressed commonsense temporal knowledge [42, 57] and enhanced event-based temporal question-answering through knowledge graphs [49] or specialized language model training [32, 5, 53, 14]. Unlike these prior efforts that often target specific temporal aspects, TIME proposes unified framework for comprehensive evaluation of temporal understanding spanning time and events. Temporal Reasoning in Real-World Scenarios Advancements in language models have facilitated deeper exploration of temporal reasoning, particularly concerning event ordering and causality. Benchmarks like TimeQA [4] assess time-sensitive question answering, while RealTimeQA [22], FreshLLM [43], and StreamingQA [25] address adaptation to dynamic knowledge. SituatedQA [54] and SituatedGen [55] evaluate the integration of temporal and geographical commonsense into text understanding. Domain-specific studies further explore temporal reasoning. In news, TCELongBench [56] probes temporal understanding of complex events, focusing on details, ordering, and prediction. For conversations, TimeDial [31] examines everyday temporal commonsense, and TReMu [13] tackles temporal localization and long-range dependencies in long-form dialogues. Unlike TCELongBench and TReMu, which concentrate on specific temporal aspects, TIME offers comprehensive evaluation. It employs three progressive complexity levels and diverse subtasks for fine-grained analysis of temporal reasoning in extended contexts. Benchmarks for Temporal Reasoning Existing studies evaluate LLM temporal reasoning using various benchmarks like TRAM [44], which assesses event sequence understanding but lacks challenge for current models. Other efforts include benchmark using six existing datasets [19] and TimeBench [6], which aggregates 10 datasets across symbolic, common-sense, and event-temporal levels. However, these benchmarks have limitations: TimeBenchs simpler tasks are less challenging for improving LLMs, and the diverse evaluation contexts and difficulty levels across datasets can introduce biases, hindering fair and consistent assessment. In contrast, TIME provides unified eval2 Figure 1: An overview of TIME. The top-left block illustrates three key challenges of real-world complexity and their corresponding dataset construction. The bottom-left quadrant depicts threelevel tasks. One data example from TIME-DIAL is shown on the right. uation context, enabling fair, fine-grained, and challenging assessment of LLM temporal reasoning in real-world scenarios."
        },
        {
            "title": "3 TIME: Benchmark Construction",
            "content": "This section details the construction of TIME, covering task definition and design principles (3.1), data source selection (3.2), and dataset construction with quality control (3.3). Finally, we introduce TIME-LITE, high-quality, manually verified sub-dataset (3.4). 3.1 Task Definition TIME is designed for fine-grained and comprehensive exploration of real-world temporal reasoning challenges. We structure these challenges into three progressive levels and propose various task formats that better capture the intrinsic nature of temporal reasoning in real-world scenarios. 3.1.1 Design Principal Real-world text information contains complex content, among which temporal information is crucial clue. TIME aims to simulate the process by which humans utilize temporal concepts to better understand the complex and dynamic world information, and to measure the ability of LLMs to solve real-world problems using time. Humans tend to first accurately capture and understand temporal concepts (Level-1: Basic Temporal Understanding and Retrieval); secondly, it requires integrating contextual information to reason about implicit and ambiguous temporal expressions to locate event details (Level-2: Temporal Expression Reasoning); finally, the complex temporal relationships embedded in events are crucial for clarifying timelines and potential event causality (Level-3: Complex Temporal Relationship Reasoning). These three types of challenges are progressive and interconnected. The temporal reasoning tasks at three levels are detailed below: Level-1: Basic Temporal Understanding and Retrieval. Level-1 requires models to establish fundamental temporal information processing capabilities. We design five subtasks organized through three complementary aspects: (1) Extract assesses direct retrieval of temporal expressions (time points, periods, relative time) from text, paired with (2) Localization that evaluates event-time mapping accuracy through temporal positioning of occurrences - together forming the basis of temporal information retrieval. For temporal quantification, we develop (3) Computation testing duration calculation between explicit time markers, combined with (4) DurationCompare measuring interval comparison capability between events. Finally, temporal sequencing is addressed through (5) OrderCompare that examines chronological ordering understanding. This tripartite structure evalu3 Figure 2: Dataset construction pipeline for TIME. In the process of QA synthesis for each sub-dataset, we first collect temporal facts (temporal knowledge graphs for TIME-WIKI, time points for TIMENEWS, fact bank for TIME-DIAL). Then timelines are generated for QA data synthesis. ates core competencies through: basic temporal retrieval (Extract+Localization), quantitative temporal analysis (Computation+DurationCompare), and sequential relationship understanding (OrderCompare). Level-2: Temporal Expression Reasoning. Level-2 requires models to locate event details through temporal expression reasoning. We design three subtasks: (1) Explicit Reasoning that demands inference based on unmentioned time points/ranges (e.g., Q: What was Mauro Morellis occupation between 1967-1973?), (2) Order Reasoning that requires temporal positioning through ordinal expressions (e.g., Mauro Morellis second job), and (3) Relative Reasoning that involves contextual interpretation of relative temporal references (e.g., Where did Mauro Morelli work closest to Event A?). These tasks collectively evaluate models ability to understand complex event details under the premise of performing multi-hop temporal reasoning. Level-3: Complex Temporal Relationship Reasoning. Level-3 requires models to comprehend and reason about complex temporal relationships among multiple events. We develop three subtasks: (1) Co-temporality that identifies overlapping temporal relationships between concurrent events (e.g., Where did Elon Musk work concurrently with his OpenAI position?), (2) Timeline that infers correct chronological ordering of multiple events (e.g., sorting 8 political events into temporal sequence), and (3) Counterfactual Reasoning that demands temporal inference under altered temporal premises contradicting the original context (e.g., If Event occurred 3 years later, how would it affect Event Y?). These tasks collectively evaluate models capacity to handle multi-event temporal interactions through co-occurrence analysis, timeline construction, and hypothetical temporal reasoning. 3.2 Data Source TIME-WIKI We choose Wikidata as the data source, leveraging its extensive collection of continuously updated real-world temporal facts that capture time-evolving knowledge across diverse domains. Wikidatas structured temporal relations facilitate efficient extraction of temporal knowledge graphs. To ensure comprehensive coverage of real-world knowledge, we systematically select 6 categories encompassing 34 representative Wikidata properties for fact retrieval, as detailed in Table 5. 4 TIME-NEWS We utilize online news articles as the data source. Previous work[56] provides large-scale, high-quality online news articles accompanied by corresponding timelines, effectively capturing multiple temporal complex events. temporal complex event (TCE) refers to sequence of interrelated events centered around specific topic within defined time period, such as the IsraeliPalestinian conflict that occurred between February and March 2015. As illustrated in Figure 5, each temporal complex event encompasses multiple dates, with event details evolving. The statistical characteristics of the TIME-NEWS data source are presented in Table 6. TIME-DIAL We employ very long-term multi-session conversations as the data source, utilizing publicly available data from LOCOMO[28] and REALTALK[23]. Each conversation consists of multi-session dialogs between two distinct persona-driven speakers, incorporating image sharing behaviors, reflecting speaker personas and demonstrating complex temporal dependencies and contextual linkages across events. An example is illustrated in Figure 7, with detailed statistical analysis presented in Table 7. 3.3 Dataset Construction As shown in Figure 2, for each data source(3.2), we systematically collect temporal facts(3.3) and then extract corresponding timelines(3.3). Leveraging these timelines, we employ data synthesis methods to generate question-answer pairs(3.3). To ensure data quality, we conduct human annotation on randomly sampled subset, resulting in the high-quality TIME-LITE benchmark(3.3). Details can be seen in Appendix A. Temporal Facts Collection For TIME-WIKI, we employ the SLING framework to parse Wikidata, extracting temporal fact quadruples and constructing multi-hop temporal knowledge graph (TKG), with detailed methodology in Appendix A.1.2, where each temporal fact is formalized as (Si,j,Ri+1,k, Ti+1,k, Oi+1,k) and link entities (e.g., S0, S11, S12 in Figure 2) serve as central nodes for collecting related temporal facts; for TIME-NEWS, we directly utilize high-quality time points from [56] as temporal facts; in TIME-DIAL, we leverage LLMs to summarize event graphs, extracting multiple temporal facts per session comprising speaker atomic facts and corresponding timestamps, while standardizing both implicit (e.g., two days ago) and explicit (e.g., 8:35 am, Feb 23, 2022) temporal expressions through LLM processing and manual verification. Timeline Generation The synthesis of question-answer pairs for subtasks such as order reasoning, relative reasoning, and co-temporality necessitates precise organization of timelines for individual entities or event groups. To this end, we systematically construct timelines across all three datasets. Specifically, for TIME-WIKI, we chronologically organize the temporal facts associated with link entities in the TKG. For TIME-DIAL, we separately arrange the temporal facts for each speaker in the conversation in chronological order. In the case of TIME-NEWS, we directly utilize the pre-existing time points from each temporal complex event as the timeline, as illustrated in Figure 5. Context Collection We devise distinct context processing strategies tailored to each datasets characteristics. For TIME-WIKI, we first aggregate temporal facts centered around link entities, reformulating them into coherent narrative segments, which are then integrated into natural and fluent contexts using DeepSeek-V3. For TIME-NEWS, given that the average token count per TCE exceeds 500,000, as shown in Figure 6, rendering full-text evaluation impractical, we employ Retrieval-Augmented Generation (RAG) to extract the most relevant text segments from associated articles as context, with implementation details elaborated in (4.1). For TIME-DIAL, we directly utilize the original conversations as context to preserve their integrity and authenticity. QA Synthesis and Formats We design distinct QA synthesis pipelines tailored to each subtasks characteristics, combining rule-based templates with DeepSeek-V3 and DeepSeek-R1 models, as detailed in Table 8. For TIME-WIKI and TIME-DIAL, we employ rule-based approach grounded in timeline construction to establish logical relationships between questions and gold answers, followed by LLM-based natural language refinement. In TIME-NEWS, we leverage existing Time Points to generate timeline-aligned contexts centered on specific entities using DeepSeek-V3, which then serve as prompts for LLM-driven QA generation, as illustrated in Figure 10. Building upon the framework proposed in [56], our Time Points exhibit well-defined timelines and logical structures, enabling us to directly incorporate these contexts into prompts for high-quality QA synthesis. Implementation details are provided in Appendix A.4. 5 Table 1: Stats of TIME and TIME-LITE. # QA indicates the number of question-answer pairs. # QA Dataset Dataset # QA TIME TIME-WIKI TIME-NEWS TIME-DIAL 38,522 13,848 19,958 4,716 TIME-LITE TIME-LITE-WIKI TIME-LITE-NEWS TIME-LITE-DIAL 943 322 299 322 We design three primary question formats for different subtasks: free-form, single-choice, and multiple-choice, with their distribution detailed in Table 9. However, for TIME-NEWS and TIMEDIAL, the free-form formats standard answers often exhibit numerous synonymous expressions, leading to significant calibration errors in direct evaluation. To enhance evaluation accuracy while increasing data diversity[2], we adopt the STARC framework[1] to synthesize multiple-choice questions with misleading options, as illustrated in Figure 9. The detailed methodology for generating misleading options is presented in Appendix A.4. Quality Control To validate the quality of our synthesized data and establish high-quality subset for evaluation, we conducted comprehensive manual annotation process. For data sampling, we employed systematic approach to ensure representativeness across all sub-datasets. Using fixed random seed of 42, we randomly sampled 30-40 QA pairs from each task within the three subdatasets (TIME-WIKI, TIME-NEWS, and TIME-DIAL). This process yielded total of 1,071 QA pairs for annotation, with 352 from TIME-WIKI, 359 from TIME-NEWS, and 360 from TIME-DIAL. To ensure annotation quality, we recruited annotators through professional forums and conducted rigorous qualification tests. From the initial pool of 8 professional annotators, we selected the top 3 performers based on both efficiency and quality metrics to establish our final human evaluation benchmark. We propose Word-level Similarity as novel metric for evaluating annotation consistency, achieving score of 0.6626, which demonstrates the high reliability of our annotated data. Details can be seen in Appendix A.5. 3.4 TIME-LITE To facilitate efficient and reliable evaluation of temporal reasoning capabilities, we introduce TIMELITE, lightweight dataset derived from the manually annotated subset sampled in 3.3. Through multiple rounds of expert review and answer verification, we curated 945 high-quality QA pairs, all of which have undergone rigorous manual validation to ensure assessment accuracy and reliability. To assess the quality of our synthesized data, we conducted systematic comparison between the sampled data from 3.3 and its manually reviewed counterpart (TIME-LITE). The analysis revealed high consistency rate of 89.13%, demonstrating the reliability of our data synthesis pipeline."
        },
        {
            "title": "4 Evaluation",
            "content": "4.1 Experimental Setup Settings We conducted comprehensive evaluations on the TIME and TIME-LITE datasets across 24 models (see Appendix C.1 for model details). All experiments employed greedy search decoding strategy. For the TIME-NEWS and TIME-LITE-NEWS dataset, we implemented the retrieval augmented generation (RAG) framework with three retrieval strategies: BM25, Vector, and Hybrid (detailed in Appendix C.3). Given the Extract tasks limited effectiveness in assessing temporal retrieval capabilities under the RAG framework, we excluded it from our evaluation. Metrics We evaluate free-form QA tasks with token-level metrics: Exact Match (EM) for the Timeline task, and F1 score for other free-form QA tasks. For single-choice and multiple-choice QA tasks, we use option-level F1 scores, emphasizing macro F1 for comprehensive evaluation across all options. Details are shown in Appendix C.2. 6 Table 2: Results for TIME-WIKI. Abbreviations: Ext.L Extract, Loc.:Localization, Comp.: Computation, DC.: Duration Compare, OC.: Order Compare ER.: Explicit Reasoning, OR.: Order Reasoning, RR.: Relative Reasoning, Co-tmp.: Co-temporality, TL.: Timeline and CTF.: Counterfactual. Top-1 result for each blank are bold. Model Level 1 Level-2 Level-3 Ext. Loc. Comp. DC. OC. ER. OR. RR. Co-tmp. TL. CTF. Llama-3.1-8B-Instruct Qwen2.5-7B Qwen2.5-14B Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-72B-Instruct Non-reasoning Models (TIME-WIKI) 53.16 35.33 33.58 57.58 71.02 81.70 75.41 67.19 71.26 65.30 74.49 83.84 9.79 24.22 20.53 32.34 26.37 41. 50.89 23.73 50.64 52.22 63.50 66.64 65.49 65.36 66.49 68.75 82.76 84.22 28.96 10.11 7.42 44.76 52.93 70.13 Reasoning Models (TIME-WIKI) 66.75 Deepseek-R1-Distill-Llama-8B Deepseek-R1-Distill-Qwen-7B 54.89 Deepseek-R1-Distill-Qwen-14B 67.66 74.99 QwQ-32B 68.82 65.04 66.33 67. 57.27 56.63 51.25 49.59 83.47 77.85 81.21 88.20 90.22 85.71 92.97 93.53 51.17 48.88 58.94 60.61 Advanced Models (TIME-LITE-WIKI) 93.33 96.67 98.89 96. 84.51 77.61 83.24 80.83 23.76 46.39 33.82 49.17 71.43 89.29 67.86 92.86 83.33 93.33 90.00 93.33 75.69 78.20 80.68 82.24 Deepseek-V3 Deepseek-R1 GPT-4o OpenAI o3-mini 4.2 Result 31.72 14.66 15.37 35.48 38.94 44.84 37.36 32.53 43.49 37.77 39.77 57.09 45.83 52.62 24.53 5.45 20.68 26.79 30.34 35.23 32.41 30.57 35.63 36. 41.76 57.79 46.56 48.98 31.36 2.45 17.11 36.68 33.68 51.17 31.04 29.74 36.30 37.76 46.62 47.45 45.45 54.34 0.92 0.00 0.00 1.08 2.62 4.08 5.31 0.54 14.54 25. 10.00 33.33 20.00 33.33 28.60 0.98 27.95 38.42 43.16 50.68 37.30 37.38 45.69 53.13 44.82 55.71 50.72 52.07 4.2.1 Real-world Scenario Analysis Knowledge intensive events makes it challenging for capturing complex temporal expression and relationship. As shown in Table 2, models face significant challenges in comprehending implicit temporal expressions and intrinsic temporal relationships between events. For o3-mini, it achieves only 52.62% and 48.98% on Order Reasoning and Relative Reasoning tasks respectively, and merely 54.34% on the Co-temporality task. In contrast, its performance on basic temporal retrieval and comprehension tasks (Level-1) approaches 80% for 4 tasks. This phenomenon suggests that the complex and diverse associations between temporal information and entities in knowledge-intensive scenarios substantially hinder models ability to accurately correlate time with facts. Complex dynamic events constrain models ability to comprehend basic temporal relationship and construct coherent timelines. As shown in Table 3 (TIME-NEWS), models is challenged by comprehending fundamental temporal relationships, including time intervals and ordering, as well as constructing coherent timelines. For instance, the reasoning model o3-mini achieves maximum performance of only 63.33% on both Duration Compare and Order Compare tasks. Notably, all models demonstrate limited capability in the Timeline task, which requires ordering three events, with performance not exceeding 30%. This suggests that the intricate details among complex events lead models to identify multiple similar but imprecise temporal points, resulting in erroneous predictions. Very-long multi-session dialog impairs the capability of time retrieval and event-time localization. As shown in Table 4 (TIME-DIAL), the maximum accuracy of open-source vanilla models and test-time scaled models on Extract and Localization tasks is merely 40%, substantially lower than their performance on other datasets. This phenomenon can be attributed to two primary factors: first, the extensive dialog context (averaging over 15k tokens, as shown in Table 7) and multi-turn interactions significantly increase the difficulty of temporal localization; second, the frequent use of memory-based temporal expressions in daily dialogs (e.g., \"Last Saturday\"), which necessitate reasoning with the conversation timestamp to pinpoint the precise date, further hinders accurate timestamp identification. 4.2.2 Temporal Reasoning Tasks Analysis Time retrieval ability is significantly correlated with almost all aspects of temporal reasoning tasks. To investigate the impact of basic temporal retrieval capabilities on temporal reasoning 7 Table 3: Results for TIME-NEWS. Top-3 articles are retrieved. Abbreviations follow Table 2. Model Retriever Level 1 Level-2 Level-3 Loc. Comp. DC. OC. ER. OR. RR. Co-tmp. TL. CTF. Llama3.1-8B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B GPT-4o OpenAI o3-mini BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid Non-reasoning Models (TIME-NEWS) 47.96 50.99 51. 68.53 71.68 71.00 68.88 71.76 71.57 27.12 32.13 34.51 70.80 76.28 79.75 79.48 84.46 86.62 39.06 40.94 41. 42.39 42.22 43.61 46.44 44.78 44.78 39.28 41.17 44.11 46.17 45.67 48.72 51.22 50.61 54.83 81.72 81.33 82. 83.06 83.94 84.72 84.39 85.22 86.28 Reasoning Models (TIME-NEWS) 39.66 41.17 41.42 63.42 65.96 66.11 60.15 59.81 60. 62.36 63.56 66.29 38.78 41.72 38.22 39.72 39.39 39.39 53.33 54.56 54.78 52.61 51.33 54.94 76.28 76.44 78. 83.39 84.89 85.61 Advanced Models (TIME-LITE-NEWS) 79.26 75.56 80.56 72.59 76.67 77.94 10.56 15.00 20.00 12.78 18.33 16. 43.33 40.00 33.33 56.67 63.33 56.67 43.33 53.33 46.67 60.00 63.33 63.33 76.67 80.00 73.33 73.33 80.00 76. 66.67 67.67 68.94 70.44 69.33 70.39 70.78 70.94 71.17 60.06 61.94 62.67 70.33 69.22 69.89 70.00 66.67 66. 83.33 66.67 63.33 77.06 77.67 78.39 79.61 80.33 81.44 81.56 82.11 82.72 70.17 73.89 72.72 80.83 81.28 82. 93.33 86.67 86.67 86.67 86.67 80.00 80.50 81.50 82.89 82.67 83.44 84.06 85.11 84.39 86.11 74.56 74.67 76. 83.78 83.89 85.00 93.33 90.00 90.00 93.33 80.00 86.67 3.09 1.94 2.55 26.13 23.68 26.61 27.54 24.16 25. 17.94 16.44 17.08 21.82 19.58 21.10 24.14 24.14 13.79 27.59 24.14 27.59 47.17 46.22 46.44 59.39 59.67 58. 54.61 55.83 54.06 37.11 38.78 39.06 62.72 63.44 62.00 43.33 40.00 46.67 33.33 33.33 36.67 tasks, we computed correlation coefficients between Extract and Localization tasks and other task performances based on 8 vanilla models across three TIME-LITE subsets. Specifically, we represented each task as vector of model performance across various temporal reasoning tasks and performed agglomerative clustering using correlation coefficients as the distance metric. The results (shown in Figure 3) demonstrate that Extract and Localization tasks exhibit significant correlations (correlation coefficient > 0.5) with nearly all other tasks, confirming strong relationship between basic temporal retrieval and higher-level temporal reasoning capabilities. Grasping timeline over multiple events is much challenging for long-range contexts. As shown in Table 2, 3 and 4, all models demonstrate suboptimal performance on the Timeline task. Notably, small-scale vanilla models achieve accuracy below 10% on both TIME-WIKI and TIME-DIAL datasets. Even in the relatively simpler TIME-NEWS dataset, merely reordering three events poses significant challenge. This difficulty stems from the Timeline tasks requirement for simultaneous complex temporal information retrieval and global temporal ordering reasoning, which is substantially more challenging than basic tasks like Order Compare that only require understanding the sequence of two events. 4.2.3 Impact of Test-time Scaling Test-time scaling benefits temporal logical reasoning. Test-time scaling enhances models performance in complex logical reasoning tasks by strengthening their chain-of-thought capabilities. To systematically evaluate its impact on temporal reasoning, we compare R1-Distill models and their vanilla counterparts across multiple tasks (see Table 2 and 4). Experimental results demonstrate that Deepseek-R1-Distill-Qwen-14B significantly outperforms Qwen2.5-14B-Instruct in temporal reasoning tasks such as Order Compare and Duration Compare, as well as in handling complex temporal-event relationships in the Counterfactual task, achieving performance improvements of 24.44%, 11.33%, and 12.0% respectively on the TIME-DIAL dataset. Our analysis further reveals that advanced test-time scaled models, including o3-mini and Deepseek-R1, consistently outperform their non-reasoning counterparts in logical reasoning-based tasks, demonstrating the effectiveness of test-time scaling in enhancing complex reasoning capabilities. Test-time scaling is not consistently effective for time retrieval and event location. Experimental results reveal significant performance variations of test-time scaling models across different datasets. 8 Table 4: Results for TIME-DIAL. Abbreviations follow Table 2. Model Level 1 Level-2 Level-3 Ext. Loc. Comp. DC. OC. ER. OR. RR. Co-tmp. TL CTF. Non-reasoning Models (TIME-DIAL) Llama-3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct 27.45 36.51 38.85 40.67 38.61 30.91 30.83 33. 9.05 23.25 16.35 23.45 48.44 41.11 42.00 40.89 52.67 41.33 47.78 52.67 38.22 31.11 38.22 43.33 46.22 34.22 38.67 36.67 57.33 44.44 49.11 46. Reasoning Models (TIME-DIAL) 40.21 Deepseek-R1-Distill-Llama-8B Deepseek-R1-Distill-Qwen-14B 40.40 Deepseek-R1-Distill-Qwen-32B 39.28 36.37 18.34 35.79 14.69 12.98 22.87 40.89 53.33 58.22 57.11 72.22 75. 34.89 54.67 57.56 34.00 40.44 41.78 40.44 53.33 54.89 Deepseek-V3 Deepseek-R1 GPT-4o OpenAI o3-mini Advanced Models (TIME-LITE-DIAL) 52.63 65.00 61.08 41. 42.67 48.56 52.98 45.30 13.00 22.61 14.00 29.90 70.00 73.33 40.00 56.67 73.33 86.67 76.67 86.67 40.00 76.67 60.00 76.67 26.67 53.33 43.33 60. 60.00 66.67 66.67 70.00 72.00 58.00 57.33 63.11 54.67 66.89 72.67 56.67 76.67 76.67 70.00 0.00 0.22 0.00 0.67 0.44 0.22 0. 3.33 10.00 0.00 10.00 38.00 46.44 34.89 40.44 42.22 46.89 49.78 43.33 53.33 46.67 46.67 Figure 3: Task correlation heatmap highlighting the relationship between Extract and Localization tasks and other temporal reasoning tasks. Note: Extract task is excluded from TIME-LITE-NEWS evaluation. On TIME-WIKI(shown in Table 2), Deepseek-R1-Distill-Qwen-14B underperforms Qwen2.5-14BInstruct by 3.36% and 8.16% in Extract and Localization tasks respectively. Conversely, on TIME-DIAL (shown in 4), it achieves 1.55% improvement in Extract but suffers 12.49% decline in Localization. This discrepancy stems from the temporal information retrieval mechanism of test-time scaling models: their systematic context traversal strategy benefits multi-session dialog scenarios but may lead to overthinking cycles after retrieval errors, hindering error correction (see case in Appendix D). 4.2.4 Impact of Retrievers in TIME-NEWS Experimental results (shown in Table 3) demonstrate that the choice of retriever significantly impacts temporal reasoning performance. Taking GPT-4o as an example, its performance with the Hybrid retriever is over 10% lower than with BM25 and Vector retrievers in the Timeline task. Similarly, 10% performance gap exists across different retrievers in the Order Compare task. This finding suggests that accurate temporal fact retrieval is crucial for processing dynamic information, directly affecting the effectiveness of complex event reasoning. Notably, in Explicit Reasoning and Order Reasoning tasks, the performance differences among models under the same retriever setting are significantly reduced, indicating that the retriever plays dominant role in temporal reasoning for these tasks, even overshadowing the inherent capabilities of different models."
        },
        {
            "title": "5 Conclusion",
            "content": "TIME presents comprehensive benchmark for evaluating temporal reasoning in LLMs, featuring three hierarchical levels with 11 subtasks that systematically assess temporal understanding. Our benchmark captures real-world complexities through knowledge associations, temporal dynamics, 9 and long-term interactions. We introduce TIME-LITE, fully human-annotated subset for efficient evaluation. Extensive experiments across diverse models reveal that test-time scaling significantly enhances logical reasoning while showing varied effects on time retrieval. These findings provide critical insights for advancing temporal reasoning capabilities. TIME establishes foundation for rigorous evaluation and deeper understanding of temporal reasoning, paving the way for future advancements in this essential NLP capability."
        },
        {
            "title": "References",
            "content": "[1] Yevgeni Berzak, Jonathan Malmaud, and Roger Levy. STARC: structured annotations for reading comprehension. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 57265735. Association for Computational Linguistics, 2020. [2] Ján Cegin, Branislav Pecher, Jakub Simko, Ivan Srba, Mária Bieliková, and Peter Brusilovsky. Effects of diversity incentives on sample diversity and downstream model performance in llm-based text augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 13148 13171. Association for Computational Linguistics, 2024. [3] Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2024. [4] Wenhu Chen, Xinyi Wang, and William Yang Wang. dataset for answering time-sensitive In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural questions. Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. [5] Ziqiang Chen, Shaojuan Wu, Xiaowang Zhang, and Zhiyong Feng. TML: temporal-aware multitask learning framework for time-sensitive question answering. In Ying Ding, Jie Tang, Juan F. Sequeda, Lora Aroyo, Carlos Castillo, and Geert-Jan Houben, editors, Companion Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023, pages 200203. ACM, 2023. [6] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, and Bing Qin. Timebench: comprehensive evaluation of temporal reasoning abilities in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12041228, 2024. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. [8] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. 10 [9] DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Haowei Zhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang, Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang, Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun, W. L. Xiao, and Wangding Zeng. Deepseek-v3 technical report. CoRR, abs/2412.19437, 2024. [10] Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William W. Cohen. Time-aware language models as temporal knowledge bases. Trans. Assoc. Comput. Linguistics, 10:257273, 2022. [11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. [12] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, Zhengyang Tang, Benyou Wang, Daoguang Zan, Shanghaoran Quan, Ge Zhang, Lei Sha, Yichang Zhang, Xuancheng Ren, Tianyu Liu, and Baobao Chang. Omni-math: universal olympiad level mathematic benchmark for large language models. CoRR, abs/2410.07985, 2024. [13] Yubin Ge, Salvatore Romeo, Jason Cai, Raphael Shu, Monica Sunkara, Yassine Benajiba, and Yi Zhang. Tremu: Towards neuro-symbolic temporal reasoning for llm-agents with memory in multi-session dialogues. CoRR, abs/2502.01630, 2025. [14] Rujun Han, Xiang Ren, and Nanyun Peng. ECONET: effective continual pretraining of language models for event temporal reasoning. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 53675380. Association for Computational Linguistics, 2021. [15] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021. 11 [16] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex BakerWhitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. [17] Duygu Sezen Islakoglu and Jan-Christoph Kalo. Chronosense: Exploring temporal understanding in large language models with time intervals of events. CoRR, abs/2501.03040, 2025. [18] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. CoRR, abs/2403.07974, 2024. [19] Raghav Jain, Daivik Sojitra, Arkadeep Acharya, Sriparna Saha, Adam Jatowt, and Sandipan Dandapat. Do language models have common sense regarding time? revisiting temporal commonsense reasoning in the era of large language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 67506774, 2023. [20] Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Strötgen, and Gerhard Weikum. Tempquestions: benchmark for temporal question answering. In Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and Panagiotis G. Ipeirotis, editors, Companion of the The Web Conference 2018 on The Web Conference 2018, WWW 2018, Lyon , France, April 23-27, 2018, pages 10571062. ACM, 2018. [21] Carlos E. Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik R. Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. [22] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir Radev, Noah A. Smith, Yejin Choi, and Kentaro Inui. Realtime QA: whats the answer right now? In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [23] Dong-Ho Lee, Adyasha Maharana, Jay Pujara, Xiang Ren, and Francesco Barbieri. REALTALK: 21-day real-world dataset for long-term conversation. CoRR, abs/2502.13270, 2025. [24] Wei Li, Xin Zhang, Zhongxin Guo, Shaoguang Mao, Wen Luo, Guangyue Peng, Yangyu Huang, Houfeng Wang, and Scarlett Li. Fea-bench: benchmark for evaluating repository-level code generation for feature implementation. CoRR, abs/2503.06680, 2025. [25] Adam Liska, Tomás Kociský, Elena Gribovskaya, Tayfun Terzi, Eren Sezener, Devang Agrawal, Cyprien de Masson dAutume, Tim Scholtes, Manzil Zaheer, Susannah Young, Ellen GilsenanMcMahon, Sophia Austin, Phil Blunsom, and Angeliki Lazaridou. Streamingqa: benchmark for adaptation to new knowledge over time in question answering models. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvári, Gang Niu, and Sivan Sabato, editors, International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, 12 Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pages 13604 13622. PMLR, 2022. [26] Hector Llorens, Nathanael Chambers, Naushad UzZaman, Nasrin Mostafazadeh, James Allen, and James Pustejovsky. Semeval-2015 task 5: Qa tempeval-evaluating temporal information understanding with question answering. In proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 792800, 2015. [27] Yunshan Ma, Chenchen Ye, Zijian Wu, Xiang Wang, Yixin Cao, Liang Pang, and Tat-Seng Chua. Sctc-te: comprehensive formulation and benchmark for temporal event forecasting. arXiv preprint arXiv:2312.01052, 2023. [28] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating very long-term conversational memory of LLM agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1385113870. Association for Computational Linguistics, 2024. [29] Puneet Mathur, Rajiv Jain, Franck Dernoncourt, Vlad Morariu, Quan Hung Tran, and Dinesh Manocha. Timers: document-level temporal relation extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 524533, 2021. [30] James Pustejovsky, José Castano, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir Radev. Timeml: Robust specification of event and temporal expressions in text. New directions in question answering, 3:2834, 2003. [31] Lianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng He, Yejin Choi, and Manaal Faruqui. TIMEDIAL: temporal commonsense reasoning in dialog. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 70667076. Association for Computational Linguistics, 2021. [32] Jungbin Son and Alice Oh. Time-aware representation learning for time-sensitive question answering. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 7077. Association for Computational Linguistics, 2023. [33] Jannik Strötgen and Michael Gertz. Heideltime: High quality rule-based extraction and In Proceedings of the 5th international workshop normalization of temporal expressions. on semantic evaluation, pages 321324, 2010. [34] Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, and Min Zhang. Living in the moment: Can large language models grasp co-temporal reasoning? In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1301413033. Association for Computational Linguistics, 2024. [35] Qingyu Tan, Hwee Tou Ng, and Lidong Bing. Towards benchmarking and improving the temporal reasoning capability of large language models. In Anna Rogers, Jordan L. BoydGraber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1482014835. Association for Computational Linguistics, 2023. [36] Qingyu Tan, Hwee Tou Ng, and Lidong Bing. Towards robust temporal reasoning of large language models via multi-hop QA dataset and pseudo-instruction tuning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 62726286. Association for Computational Linguistics, 2024. [37] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. [38] Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second joint conference on lexical and computational semantics (* SEM), volume 2: Proceedings of the seventh international workshop on semantic evaluation (SemEval 2013), pages 19, 2013. [39] Siddharth Vashishtha, Benjamin Van Durme, and Aaron Steven White. Fine-grained temporal relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 29062919, 2019. [40] Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the fourth international workshop on semantic evaluations (SemEval-2007), pages 7580, 2007. [41] Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. Semeval-2010 task 13: Tempeval-2. In Proceedings of the 5th international workshop on semantic evaluation, pages 5762, 2010. [42] Felix Giovanni Virgo, Fei Cheng, and Sadao Kurohashi. Improving event duration question answering by leveraging existing temporal information extraction data. In Nicoletta Calzolari, Frédéric Béchet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, Hélène Mazo, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Thirteenth Language Resources and Evaluation Conference, LREC 2022, Marseille, France, 20-25 June 2022, pages 44514457. European Language Resources Association, 2022. [43] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W. Wei, Jason Wei, Chris Tar, YunHsuan Sung, Denny Zhou, Quoc V. Le, and Thang Luong. Freshllms: Refreshing large language models with search engine augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 1369713720. Association for Computational Linguistics, 2024. [44] Yuqing Wang and Yun Zhao. TRAM: benchmarking temporal reasoning for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pages 63896415. Association for Computational Linguistics, 2024. [45] Yifan Wei, Yisong Su, Huanhuan Ma, Xiaoyan Yu, Fangyu Lei, Yuanzhe Zhang, Jun Zhao, and Kang Liu. Menatqa: new dataset for testing the temporal comprehension and reasoning abilities of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 14341447. Association for Computational Linguistics, 2023. [46] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I. Wang. SWE-RL: advancing LLM reasoning via reinforcement learning on open software evolution. CoRR, abs/2502.18449, 2025. [47] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. [48] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 10452 10470. Association for Computational Linguistics, 2024. [49] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics 14 (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 10452 10470. Association for Computational Linguistics, 2024. [50] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. CoRR, abs/2412.15115, 2024. [51] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, Junyang Lin, Kai Dang, Kexin Yang, Le Yu, Mei Li, Minmin Sun, Qin Zhu, Rui Men, Tao He, Weijia Xu, Wenbiao Yin, Wenyuan Yu, Xiafei Qiu, Xingzhang Ren, Xinlong Yang, Yong Li, Zhiying Xu, and Zipeng Zhang. Qwen2.5-1m technical report. CoRR, abs/2501.15383, 2025. [52] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. CoRR, abs/2409.12122, 2024. [53] Sen Yang, Xin Li, Lidong Bing, and Wai Lam. Once upon time in graph: Relative-time pretraining for complex temporal reasoning. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 1187911895. Association for Computational Linguistics, 2023. [54] Michael J. Q. Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into QA. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 73717387. Association for Computational Linguistics, 2021. [55] Yunxiang Zhang and Xiaojun Wan. Situatedgen: Incorporating geographical and temporal contexts into generative commonsense reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [56] Zhihan Zhang, Yixin Cao, Chenchen Ye, Yunshan Ma, Lizi Liao, and Tat-Seng Chua. Analyzing temporal complex events with large language models? benchmark towards temporal, long context understanding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 15881606. Association for Computational Linguistics, 2024. [57] Ben Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. \"going on vacation\" takes longer than \"going for walk\": study of temporal commonsense understanding. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 33613367. Association for Computational Linguistics, 2019. [58] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth. Temporal reasoning on implicit events from distant supervision. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 13611371. Association for Computational Linguistics, 2021."
        },
        {
            "title": "A Benchmark Construction",
            "content": "A.1 TIME-WIKI Construction A.1.1 Data Source We utilize Wikidata as the data source for constructing TIME-WIKI. We downloaded the WikiData Dump34 from November 1, 2024 from the WikiMedia5. WikiData WikiData is free and open collaborative knowledge base that serves as central storage for the structured data of Wikimedia projects. It organizes information into items, each identified by unique Q-number, and describes them using statements composed of properties and values. This structured data, akin to large-scale knowledge graph, provides valuable resource for Natural Language Processing tasks such as entity linking, relation extraction, and building comprehensive linguistic resources by connecting text to real-world entities and their relationships. Wikidata contains abundant real-world temporal facts that can be extracted to form multi-hop structured temporal knowledge graphs, which can then be transformed into unstructured temporal contexts for evaluating models temporal fact comprehension capabilities. A.1.2 Temporal Knowledge Graph Construction Using SLING6, we parsed temporal facts under given relations based on predefined rules, ultimately generating structured temporal knowledge graph data. Subsequently, we enhanced the temporal knowledge graph data to obtain 1,300 multi-hop temporal knowledge graphs as the data source for QA construction. To comprehensively reflect world knowledge, we selected six categories of relations when constructing temporal facts from Wikidata: (1) Education, employment, and organizational affiliation, (2) Family relations, (3) Geographical location relations, (4) Naming relations, (5) Significant event, and (6) Role/Identity relations, as shown in Table 5. For each relation, we meticulously designed corresponding natural language templates to facilitate the subsequent synthesis of contexts containing complex temporal facts using LLMs. SLING SLING is task processing pipeline designed for downloading and processing Wikipedia and Wikidata dumps. It leverages freely available dump files from Wikimedia and converts them into the SLING frame format through its workflow task system. Specifically, following the construction process of TempLAMA[10], we first installed the SLING toolkit (version 3.0.0) and downloaded two key datasets: the SLING-formatted WikiData KB and the WikiData to Wikipedia mapping. We then used SLING to extract facts and relations with temporal properties P580 (start time)7, P582 (end time)8, and P585 (point in time)9, filtering out non-entity objects and null objects. We mapped entity names to Wikipedia page titles, generating structured temporal knowledge graph data. The resulting temporal knowledge graph data contains fact relation sets from both subject and object perspectives. Data Augmentation of Temporal Knowledge Graph We first filtered entities associated with at least three facts from the SLING-generated temporal knowledge graph. We then removed facts with duplicate temporal information and alternately combined (s, r, o) relations from both subject and object perspectives. Specifically, we constructed 2-hop temporal knowledge graph where we randomly selected 2-3 object entities as intermediate linking entities at each hop. Each intermediate linking entity then served as new subject entity, concatenating new temporal fact relations. Finally, 3https://dumps.wikimedia.org/enwiki/20241101/enwiki-20241101-pages-articles-multistream. xml.bz 4https://dumps.wikimedia.org/enwiki/20241101/enwiki-20241101-pages-articles-multistream-index. txt.bz2 5WikiMedia: https://dumps.wikimedia.org 6SLING: https://github.com/ringgaard/sling 7P580 start time: https://www.wikidata.org/wiki/Property:P580 8P582 end time: https://www.wikidata.org/wiki/Property:P582 9P585 point in time: https://www.wikidata.org/wiki/Property:P585 16 Figure 4: An overview of the TIME-WIKI benchmark construction pipeline. Beginning with Wikidata as the data source, temporal facts are parsed using SLING. These facts are then used to construct multi-hop temporal knowledge graphs. Timelines for link entities are generated from these graphs by sorting temporal facts. Finally, these timelines are used to synthesize question-answer (QA) pairs, and corresponding context is generated by concatenating and paraphrasing stories derived from the timelines, forming the final QA tasks. we selected 1,300 high-quality multi-hop temporal knowledge graphs as the data source for subsequent QA construction. A.1.3 Collect Context for Evaluation for TIME-WIKI To provide comprehensive world knowledge contexts for evaluation, we synthesize evaluation contexts for LLMs by leveraging the previously constructed Temporal Knowledge Graph. Specifically, we first build timelines for link entities from the temporal knowledge graph as structured raw data. For each link entity, we chronologically concatenate temporal facts and paraphrase them into coherent stories. Finally, we prompt LLMs to generate contexts centered around three interconnected link entities. Below we present our few-shot prompts for paraphrasing facts into stories and concatenating stories for TIME-WIKI, followed by an example of the final concatenated context. 17 Selected Wikidata Properties Categorized by Semantic Type, Table 5: Property IDs, Query Frequencies, and Natural Language Templates. highlighted including Categories are Education, employment, and organizational affiliation , different colors: by Family relations , Geographical location relations , Naming relations , Significant event , and Role/Identity relations . Num. represents the number of facts with the corresponding relation that are extracted using SLING. WikiData ID Relation Num. Template P39 P54 P69 P102 P108 P127 P286 P106 P463 P1416 P22 P40 P17 P131 P276 P551 P740 P159 P138 P155 P156 P793 P50 P57 P86 P170 P175 P676 P110 P162 P58 P161 position held member of sports team educated at political party employer owned by head coach occupation member of affiliation father child spouse country located in the administrative territorial entity location residence location of formation headquarters location named after follows followed by significant event author director composer creator performer lyrics by illustrator producer screenwriter cast member executive producer 165479 903683 68121 16871 63298 8065 10389 33209 41425 938 <subject> holds the position of <object> <subject> plays for <object> <subject> attended <object> <subject> is member of <object> <subject> works for <object> <object> owns <subject> <object> becomes the head coach of <subject> <subject> works as <object> <subject> becomes member of <object> <subject> becomes affiliated with <object> 6 10 <object> becomes the father of <subject> <subject> has child <object> <subject> is the spouse of <object> 37344 82221 <subject> becomes part of the country: <object> <subject> becomes part of <object> 2065 7771 5 5619 3558 272 4436 47 62 92 21 132 8 11 31 29 495 32 <subject> becomes part of <object> <subject> resides in <object> <subject> was formed in <object> <subject> has its headquarters in <object> <object> names after <subject> <subject> follows <object> <object> follows <subject> <subject> experienced the significant event <object> <object> writes <subject> <object> directes <subject> <object> composes <subject> <object> creates <subject> <object> performs <subject> <object> wrote the lyrics for <subject> <object> illustrates <subject> <object> produces <subject> <object> writes <subject> as screenwriter <object> plays the role of <subject> <object> produces <subject> as the executive producer 18 Few-shot Prompt for Paraphrasing Facts into Stories for TIME-WIKI [Instruction:] You are very skilled storyteller. Below are the facts Ive extracted from WikiData. Please craft complete story centered around {link_entity}. [Requirements:] 1. Generate content that accurately conveys all the facts have provided without omitting any of them. 2. Particular attention must be paid to the accuracy of time expressions, and the temporal relationships between sentences must be coherent and natural. 3. The story should be coherent, natural, and smooth, consistently revolving around the core facts, with concise and precise language, avoiding any unnecessary digressions. [Example:] [Core entity:] Mauro Morelli [Facts:] Mauro Morelli holds the position of diocesan bishop from May 25, 1981 to March 30, 2005.. Mauro Morelli holds the position of titular bishop in December 12, 1974.. Mauro Morelli holds the position of auxiliary bishop in December 12, 1974.. Mauro Morelli works as Catholic priest in April 28, 1965.. Mauro Morelli works as Catholic bishop in January 25, 1975. [Generated Story:] Mauro Morellis journey in the Catholic Church is testament to his unwavering dedication and service. Born into world of faith, his path was set early on, leading him to become significant figure in the Churchs hierarchy. On April 28, 1965, Mauro Morelli took his first major step in his religious career by being ordained as Catholic priest. This ordination marked the beginning of his lifelong commitment to serving the Church and its followers. For nearly decade, he devoted himself to his priestly duties, gaining the respect and admiration of his parishioners. ... ... [Example:] (Here is another example with the same format of the example above.) [TODO:] Below are the facts and the link entity. Please generate story in accordance with the aforementioned requirements and with reference to the examples provided. [Link entity:]{link_entity} [Facts:]{raw_text} [Generated story:] Prompt for Concatenating Stories for TIME-WIKI [Instruction:] You are adept at weaving multiple complete stories together to form coherent and comprehensive narrative. will provide you with several stories and the connections between them. hope you can retain all the facts from the stories without any omissions or errors in time or events. [Requirements:] 1. The content you generate must not omit any time-related information from the texts being concatenated. 2. Particular attention must be paid to the accuracy of time expressions, and the temporal relationships between sentences must be coherent and natural. 3. When concatenating, pay attention to the inherent logical connections between the given multiple stories, and refer to the provided linking facts to generate transitional paragraphs that seamlessly connect the different stories. [Link facts:] {linked_facts} [Stories:] {stories} [Generated Story:] One Example for the Concatenated Story for TIME-WIKI Mauro Morellis life was testament to unwavering dedication and service to the Catholic Church. His journey began with profound commitment to his faith, which shaped his path toward becoming respected leader within the Churchs hierarchy. On April 28, 1965, Mauro Morelli was ordained as Catholic priest, marking the beginning of his spiritual and pastoral vocation. For nearly decade, he devoted himself to his priestly duties, guiding and nurturing the communities he served. His compassion, wisdom, and deep connection to his faith earned him the trust of his parishioners and the attention of his superiors. Recognizing his leadership qualities, the Church appointed Mauro Morelli as both titular bishop and an auxiliary bishop on December 12, 1974. These dual roles signified new chapter in his journey, as he took on greater responsibilities in assisting the diocesan bishop and overseeing pastoral and administrative tasks. His dedication to these roles showcased his ability to balance spiritual guidance with effective governance. Just over month later, on January 25, 1975, Mauro Morelli was consecrated as Catholic bishop. This formal consecration solidified his position as key figure within the Church, empowering him to lead with authority and grace. His work as bishop further deepened his impact on the communities he served, as he continued to champion the values of the Church. Skipping 3 paragraphs here... As we reflect on Mauro Morellis life and the broader history of auxiliary and titular bishops, we are reminded of the importance of service, humility, and faith. The roles of these bishops, though sometimes overlooked, are testament to the Churchs enduring commitment to its mission of love, guidance, and spiritual care. Their contributions, woven together, form testament to the enduring legacy of leadership within the Catholic tradition. A.2 TIME-NEWS Construction A.2.1 Data Source TIME-NEWS investigates temporal reasoning in real-world complex dynamics. Prior work typically employs news articles to analyze intricate temporal event sequences [27, 56]. We utilize the opensource dataset from [56] as our data source, selected for its extensive scale, accessibility, and rich temporal information. This dataset leverages LLMs to systematically extract and analyze event chains within temporal complex events (TCEs), where each TCE comprises multiple news articles spanning extended periods. Specifically, dataset [56] contains 2,289 TCEs, with each event averaging 29.31 articles and spanning 17.44 days. The open-source data provides both the corpus and corresponding outline for each TCE, as shown in Figure 5. We randomly sampled 600 temporal complex events (TCEs) with at least 5 distinct dates and at most 10 distinct dates as the data source for TIME-NEWS, with detailed statistics presented in Table 6 and the number of dates distribution presented in Figure 6. For each TCE, we utilize its outline for QA synthesis. During evaluation, we employ the corpus as the retrieval source, where for each question (i.e., query), we specifically retrieve the top-k relevant news articles. Table 6: Statistics of the 600 selected Temporal Complex Events (TCEs) in TIME-NEWS. Note that \"Time Span\" means the span between the earliest and the latest date of the TCE. \"Art.\" and \"Tok.\" are abbreviations for \"Article\" and \"Token\" respectively. Token counts are calculated using tiktokens cl100k_base encoder. # Art. / TCE # Art. / Date # Tok. / TCE # Tok. / Art. Time Span / TCE # Dates / TCE Avg. Max. Min. 871.46 2068 344 116.95 304 18 527,418.05 1,232,706 236,521 605.21 3,405 31 405.87 8,900 7.45 10 5 20 Figure 5: One example of temporal complex events in dataset[56] Figure 6: Frequency distribution of length of days in 600 selected Temporal Complex Events (TCEs) in TIME-NEWS A.3 TIME-DIAL Construction In the data construction pipeline of TIME-DIAL, we utilize very long-term real-life multi-session conversations [28, 23] as the evaluation context. Our pipeline begins with summarizing event graphs from conversations. We then employ combination of LLM extraction and manual verification to extract explicit temporal information and infer implicit temporal relations, which are subsequently standardized into unified temporal expressions. The event graphs are then temporally ordered to construct individual timelines for each speaker. Finally, we synthesize question-answer pairs for 11 subtasks based on these timeline representations. The complete construction pipeline is illustrated in Figure 8. A.3.1 Data Source Real-world interpersonal interactions provide crucial context for temporal reasoning research. Human conversations exhibit distinct characteristics of long-term continuity and persistence: interlocutors frequently reference previously discussed or mutually known information based on shared memory, enabling natural topic continuation. Moreover, authentic dialogues typically involve numerous sessions with significant temporal spans, containing rich fine-grained information such as discourse markers, brief turns, and subjective expressions. Notably, the processing of temporal information in ultra-long conversational contexts remains an under-explored research area. 21 Figure 7: long-term multi-turn conversation example from the LOCOMO dataset [28]. The interlocutors, Joanna and Nate, are each assigned distinct persona characteristics. The conversation consists of multiple session units, each timestamped and containing several dialogue turns. Throughout the interaction, the participants engage in image sharing and response behaviors. Notably, certain dialogue segments require the speakers to leverage previously established shared memory to facilitate conversational progression. REALTALK [23] maintains an identical data format to the example illustrated in this figure. We select LoCoMo[28] and RealTalk[23] as our data sources (illustrated in Figure 7) based on the following considerations: First, both datasets contain ultra-long conversations composed of multiple sessions, with persona-driven dialogues generated through data synthesis techniques that align with human characteristics. Second, the datasets feature not only natural language exchanges but also multimodal content such as images, with corresponding responses, closely mirroring real-world conversational scenarios. Given our focus on temporal reasoning evaluation in natural language contexts, we utilize image captions rather than raw image data for analysis. LOCOMO LOCOMO [28] addresses the scarcity of datasets for evaluating long-term memory in open-domain dialogues, which traditionally span few sessions. It was created using machine-human pipeline where LLM-based agents generate dialogues grounded in personas and, crucially for our work, temporal event graphs. These conversations are extensive, averaging 300 turns and 9K tokens over as many as 35 sessions. This characteristic makes LOCOMO valuable source for TIME-DIAL, providing the very long-term, multi-session conversational data needed to construct complex temporal reasoning tasks. While LOCOMO agents can share and react to images, for TIME-DIAL, we utilize the textual captions of these images to maintain focus on natural language-based temporal reasoning. As of February 2025, LOCOMO has released only 35 conversations in 10, which we incorporate as part of our data source. REALTALK REALTALK [23] provides corpus of authentic, multi-session dialogues collected from real-world messaging app interactions over 21-day period. This dataset captures genuine human conversational dynamics and long-term interaction patterns, offering benchmark against true human interactions rather than synthetic data. For the construction of TIME-DIAL, REALTALK serves as an invaluable source of very long-term, real-life conversational contexts. These characteristics are instrumental for developing tasks that require understanding complex temporal relationships and 10LOCOMO-35 is downloaded from Google Drive: https://drive.google.com/file/d/ 1JimNy04ery0Ijz6dZwLaqWs7glumE2v-/view 22 Figure 8: An overview of the construction pipeline of TIME-DIAL. We employ real-world long-term multi-turn conversations as evaluation contexts. First, we extract and summarize event graphs from the conversations. Then, through combination of LLM extraction and manual verification, we identify explicit temporal information and infer implicit temporal relations, standardizing them into unified temporal expressions. Subsequently, we temporally order the event graphs to construct individual timelines for each speaker. Finally, we generate question-answer pairs covering 11 subtasks based on these timeline representations. recalling information across extended dialogues, aligning with our objective to evaluate nuanced temporal reasoning capabilities within multi-session conversations. Table 7: Statistics of LOCOMO-35, REALTALK, and TIME-DIAL datasets. Note: Token counts are calculated using tiktokens cl100k_base encoder. LOCOMO-35 is the open-source subset of LOCOMO as of February 2025. represents \"Conversation\". Dataset # # Session / # Token / # Turn / # Image / LOCOMO-35[28] REALTALK[23] TIME-DIAL 35 10 20.49 21.90 20.80 14509.91 20581.60 15859.18 431.23 894.40 534.16 94.94 31.30 80.80 A.4 QA Synthesis We synthesize QA pairs by integrating timelines and rules, utilizing DeepSeek-V3 and DeepSeek-R1. However, the methodologies employed for data synthesis (as shown in Table 8) and task formats (as presented in Table 9) vary across different tasks. The approaches and details of QA synthesis are elaborated in A.4.1, A.4.2, and A.4.3, respectively. Table 8: Overview of LLM Utilization Strategies in Question Answering (QA) Construction. The symbols in the cells denote the construction methodology: R: Purely rule-based and template-driven QA generation. L: QA generation fully reliant on Large Language Models (LLMs). HQ: Hybrid for Question generation, i.e. rule-based extraction of question logic and answers, with LLMs employed for question phrasing. The suffix +M indicates the additional use of LLMs specifically for generating misleading options. Light blue shaded cells denote data generated by the DeepSeek-R1, while uncolored cells correspond to the DeepSeek-V3. Dataset Level 1 Level-2 Level-3 Extract Local. Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. TIME-WIKI HQ + HQ TIME-NEWS TIME-DIAL HQ + HQ *Not applicable; data directly reused from TCELongBench[56] examples. HQ HQ + R R HQ + HQ HQ + HQ + HQ HQ + + HQ + HQ + * HQ HQ + HQ + DeepSeek-V3 and DeepSeek-R1 DeepSeek-V3[9] and DeepSeek-R1[8] are both released by DeepSeek, representing state-of-the-art non-reasoning and reasoning models, respectively. These models offer superior performance at cost-effective rates, making them widely adopted for data 23 Table 9: Overview of Question Answering (QA) formats across datasets and task categories. The QA formats are denoted by calligraphic letters: (free-form), (single-choice), and (multiplechoice). light green cell background indicates that the tasks gold answer includes refusal option, and models are permitted to decline answering during evaluation. Refusal options include answers such as \"There is no answer.\", \"None of the above.\" Dataset Level 1 LevelLevel-3 Extract Local. Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. TIME-WIKI TIME-NEWS TIME-DIAL F S S S S F F F Figure 9: An illustration of the misleading option generation process. This example demonstrates TCE question from TIME-NEWS regarding Netanyahus planned address to Congress in April 2015, which falls under the explicit reasoning task category. Based on the gold answer from the free-form QA format, we employ an LLM to generate misleading options following our predefined principles, resulting in single-choice QA format. synthesis and various downstream applications. After evaluating both performance and cost considerations, we primarily employed DeepSeek-V3 for synthesizing QA pairs across most tasks. However, for certain Level-2 and Level-3 tasks in the TIME-DIAL dataset, which involve processing numerous input instances and require complex reasoning capabilities, we utilized the advanced Large Reasoning Model DeepSeek-R1 to ensure high-quality QA synthesis. Specifically, DeepSeek-R1 was deployed for generating the Order Reasoning, Relative Reasoning, Co-temporality, and Counterfactual tasks within the TIME-DIAL dataset. Note that the DeepSeek-V3 used in data synthesis is the version released in December 2024. QA Formats The TIME dataset incorporates three distinct QA formats: free-form, single-choice, and multiple-choice. The free-form format requires short-answer generation, exemplified by questions like \"When did Nicola Agnozzi become an auxiliary bishop?\" with concise answers such as \"April 2, 1962.\" This format is particularly suitable for questions with limited synonymous answer variations, as demonstrated by temporal expressions which inherently possess low ambiguity. The single-choice format presents exactly one correct option among the provided choices, while the multiple-choice format may contain one or more correct options. For multiple-choice questions, we expect evaluated models to comprehensively identify all correct options in their responses. Misleading Options Generation Building upon the STARC framework [1] for generating diverse misleading options, we modify the principles for creating such distractors. The specific guidelines are illustrated in Figure 9. Each time we prompt the model, it generates three misleading options based on the given principles, the original question, and the gold answer. These options are designed to satisfy distinct types of misleading requirements according to the specified guidelines. Figure 10: An illustration of the QA synthesis process based on time points in TIME-NEWS. We first utilize an LLM to generate contexts for the same entity, which are then organized into timeline contexts. Subsequently, for each QA task generation, we select relevant time points and their corresponding contexts to populate the prompt template, thereby synthesizing the QAs. A.4.1 Level 1: Basic Temporal Understanding and Retrieval Timeline For the Timeline task, we first provide set of authentic time points extracted from the source data. We then instruct the LLM to generate five novel time expressions that are distinct from all existing ones. Subsequently, we randomly select 0-4 authentic time points to form multiplechoice options, ensuring their randomness. This approach combines rule-based selection with LLM-generated distractors, thereby maintaining high QA quality. The task is formulated using fixed question template: Which of the following are time expressions mentioned in the context? (Note: There may be one or more correct options. If you think NONE of the time expressions in options A/B/C/D are mentioned, then you can choose E. Do not choose together with other options.). 25 Prompt for generating fake time expressions (Extract) (TIME-WIKI) [Rules:] Given list of time expressions, please generate FIVE new time expressions randomly. You should follow the instructions below: 1. The 5 new time expressions should totally different from all of the the given time expressions. 2. Each new time expression should be in the format of <Month> <Day>, <Year>. For example: \"May 4, 1998\", \"April 1992\", \"1934\". 3. The 5 new time expressions should closely resemble the given time expression in format and structure, creating high level of confusion, yet they must represent entirely different times. For instance, you could alter the year while keeping the month and day unchanged, such as changing May 2, 1922 to May 2, 1923; or you could modify the month and day while retaining the same year, for example, transforming May 2, 1922 into July 2, 1922. Additionally, you might consider changing the day while keeping the month and year consistent, like adjusting May 2, 1922 to May 3, 1922. Another approach could involve altering the month and year but keeping the day the same, such as changing May 2, 1922 to May 2, 1921. Lastly, you could shift the entire date by consistent interval, for example, moving May 2, 1922 to June 3, 1923, ensuring that each change introduces subtle yet distinct variation from the original time expression. [Example:] Here are some examples showing the writing style. NOTE that the content of the examples are irrelevant to the question you will generate. [Given time expressions:] [1656, 1763, 1782, 1784, 1787, 1815, September 20, 1817, January 20, 1848, ..., March 30, 2005, June 27, 1965, 1973, 2009, May 25, 1963, 1980, 2008, June 11, 1949, 1983, 1984, 2007, August 14, 2004, October 7, 2014, November 11, 2021, October 14, 2024, May 31, 1971, October 28, 1975] [New 5 time expressions:] [1682, June 12, 1974, May 8, 1907, May 11, 1949, May 25, 2011] Skipping 2 examples here...... [Output:] Now please write 5 new time expressions following the instructions and examples above. You should output the 5 new time expressions along with its answer, in the format of \"[\"YY\", \"MM, YY\", \"MM DD, YY\"]\". NOTE that the time expressions should be in chronological order. Now the given time expressions are: [Given time expressions:] {Given_time_expressions} [New 5 time expressions:] Localization We select three facts from the temporal knowledge graph and employ LLMs to generate corresponding QA pairs. 26 Prompt for generating QA. (Localization) (TIME-WIKI) [Rules:] Given 3 facts, please generate one question along with its gold answer for each given fact. You should follow the instructions below: 1. The answer MUST be short and concise, avoiding using redundant words or repeating the information in the question. 2. You should output the question and its answer without any other explanation, such as \"Question: xxx? Answer: xxx.\" 3. will give you 3 facts in each line, and then you should output the question and its answer each line in the same sequence. So your output should be 3 lines of \"Question: xxx? Answer: xxx.\". 4. The question can be phrased in different ways, such as When is...?, What is the time for...?, What time did...?, and so on. 5. The answer should be \"From xxx to xxx.\" or directly \"xxx.\" [Example:] Here are some examples showing the writing style. NOTE that the content of the examples are irrelevant to the question you will generate. * [Given facts:] Debra Hamel worked at Wesleyan University from 1998 to 2001. Oliver Marcy attended Wesleyan University in 1846. Debra Hamel completed her studies at Yale University in 1993. [Generated QA:] Question: When did Debra Hamel work at Wesleyan University? Answer: From 1998 to 2001. Question: What time did Oliver Marcy attend Wesleyan University? Answer: 1846. Question: What is the time for Debra Hamel to complete her studies at Yale University? Answer: 1993. Skipping 1 example here......} [Output:] Now please write question following the instructions and examples above. You should output the question along with its answer, in the format of \"Question: xxx? Answer: xxx.\". NOTE that the answer should be \"From xxx to xxx.\" or directly \"xxx.\" [Given facts:] {given_facts [Generated QA:] Computation We provide multiple pairs of temporal facts and utilize LLMs to generate corresponding questions. The temporal computations are systematically derived through script-based rules to ensure accuracy and consistency. Prompt for generating QA. (Computation) (TIME-WIKI) [Rules:] Each time, will provide you with several pairs of text snippets, with each pair occupying one line. For each line containing pair of text snippets, you need to generate question. You should follow the instructions below: 1. The question should be based on the snippet pair. 2. Each text snippet pair includes two snippets. Each snippet is composed of fact and Happen/Begin/End time. You should translate the snippets into format like: ... ... 3. Please refer to the following examples and learn the patterns well. [Example:] Here are one example showing the writing style. NOTE that the content of the examples are irrelevant to the question you will generate. [Snippet pairs:] José Manuel Pasquel served as an auxiliary bishop. Happen time: January 20, 1848. José Manuel Pasquel became priest. Happen time: September 20, 1817. Antonius Grech Delicata Testaferrata was ordained. Happen time: October 19, 1845. Diego Fabbrini played for Watford F.C.. Begin time: 2013. Máximo Alcócer played for the Bolivia mens national football team. Begin time: 1957. Diego Fabbrini played for Udinese Calcio. 1944. [Questions:] What was the duration from the time José Manuel Pasquel became priest until he served as an auxiliary bishop? How long was it between Antonius Grech Delicata Testaferrata was ordained and Diego Fabbrini began to play for Watford F.C.? How much time passed from Máximo Alcócer began to play for the Bolivia mens national football team to Diego Fabbrini end playing for Udinese Calcio? [Instruction:] Now please write question following the instructions and examples above. You should output the question only for each line. NOTE that there is NO any prefix like \"Question:\", just output the question string. [Snippet pairs:] {snippet_pairs} [Questions:] A.4.2 Level 2: Temporal Expression Reasoning A.4.3 Level 3: Complex Temporal Relationship Reasoning A.5 Quality Control For data sampling, we employed systematic approach to ensure representativeness across all subdatasets. Using fixed random seed of 42, we randomly sampled 30-40 QA pairs from each task within the three sub-datasets (TIME-WIKI, TIME-NEWS, and TIME-DIAL). This process yielded total of 1,071 QA pairs for annotation, with 352 from TIME-WIKI, 359 from TIME-NEWS, and 360 from TIME-DIAL (detailed sampling procedures are provided in Appendix A.5). To ensure annotation quality, we recruited annotators through professional forums and conducted rigorous qualification tests. From the initial pool of 8 professional annotators, we selected the top 3 performers based on both efficiency and quality metrics to establish our final human evaluation benchmark. Annotation Principles To ensure the quality of questions and answers, we established four annotation principles for evaluators to assess each QA pair systematically. For each data instance, we provide both the question and its corresponding gold answer. Evaluators are required to verify: (1) Question Answerability and (2) Answer Correctness (shown in ?) as fundamental criteria. Additionally, for the specialized Timeline and Counterfactual tasks, we introduced two specific evaluation principles: Correctness Answer Ranking and Counterfactual Conditional Answer Correctness respectively."
        },
        {
            "title": "Checking Principle For Question Answerability",
            "content": "Description: Can the question be answered based on the provided context (without considering the standard answer provided above)? Options: [A]: Yes, it can be directly found and answered in the context. [B]: Yes, but the question can only be fully answered through inference. [C]: The question cannot be answered based on the context; the context does not provide relevant information."
        },
        {
            "title": "Checking Principle For Answer Correctness",
            "content": "Description: Please double-check, is the standard answer correct according to the original text? Options: [A]: Yes, the answer is completely correct, answering the question fully and accurately. [B]: Cannot be determined, but it has certain possibility of being completely correct (perhaps you lack sufficient information to make judgment). [C]: The answer is not completely correct; it contains obvious errors or is irrelevant to the question. Checking Principle For Correctness Answer Ranking Description: Is the ranking of the answers correct? (Ranking: Is the order of events correct?) Options: [A]: Yes, the order of all events is accurate. [B]: Incorrect, the order of events is not completely correct. Checking Principle For Counterfactual Conditional Answer Correctness Description: Determine 1. whether the hypothetical premise in the given question contradicts the facts in the original context. 2. Regarding the hypothesis, if we disregard whether it contradicts the context and only consider the given question itself, is the given reference answer correct according to the given original context? Options: [A]: Both 1 and 2 are met. [B]: Only 1 is met, but 2 is not. [C]: Neither 1 nor 2 is met. Worker Recruitment To ensure high-quality annotations, we established rigorous selection criteria for workers: (1) holding at least bachelors degree from accredited universities; (2) demonstrating English proficiency through TOEFL, IELTS, or equivalent certifications; (3) possessing strong information retrieval skills using search engines; and (4) successfully passing our customized Qualification Test (QT) for temporal reasoning tasks. These criteria guarantee annotators competence in English comprehension and temporal reasoning capabilities. Regarding compensation, each worker received base payment of $110, with an average rate of $0.103 per annotation instance. Qualification Test To ensure the quality of temporal reasoning annotations, we meticulously designed Qualification Test (QT) to assess workers capabilities in comprehending temporal information and relationships across short, medium, and long contexts. Prior to administering the QT, we provided workers with comprehensive guidelines detailing the annotation requirements and evaluation criteria. Following the test administration, we manually reviewed all submissions to identify and exclude workers who demonstrated insufficient annotation proficiency. From the initial pool of 15 candidates, 8 successfully passed the QT and proceeded to the subsequent annotation phase. 29 Figure 11: The login page of the annotation website. Figure 12: The overview page of the annotation website. Two-stage Annotation We implemented two-stage annotation process to ensure data quality. In the first stage, we collected annotators responses as the human performance benchmark. In the second stage, annotators evaluated each QA pair based on predefined assessment principles. When annotators deemed question unanswerable, they could select option under the answerability principle, indicating that no possible answer could be inferred from the given context. The detailed annotation interface design is illustrated in Figure 11, 12, 13, 14. Annotation Quality Control Since our annotation platform uniformly employs fill-in-the-blank format for manual answer annotation across both cloze and multiple-choice questions, we designed novel annotation scheme to characterize inter-annotator agreement. We adopt word-level similarity as the evaluation metric for annotation consistency. This method quantifies agreement by computing the lexical overlap between two annotators responses to the same question. The approach not only measures the overall consistency level but also captures gradations of disagreement, thereby providing an objective basis for assessing annotation quality. First, we automatically pair JSON files following identical paths from multiple annotators datasets. For each file pair, we extract the manually annotated answers (Human Prediction Answer) corresponding to the same question ID. The word-level similarity is calculated using the Jaccard similarity coefficient from set theory, defined as: Sim(A, B) = B B 30 Figure 13: The first stage of data annotation. This metric yields values in the range [0, 1], where 1 indicates perfect agreement and 0 denotes complete disagreement. Based on word-level similarity, we compute the following evaluation metrics: Average Word-level Agreement: The mean word-level similarity across all question-answer pairs Exact Match Rate: The proportion of samples with perfect word-level similarity (1.0) Agreement Distribution: The sample distribution across similarity intervals: [0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1.0] Quartiles: The Q1, median, and Q3 values of word-level similarity. A.6 Dataset Statistics We present comprehensive statistics of instance counts for each subtask in both TIME and TIME-LITE datasets, as detailed in Table 10. 31 Figure 14: The second stage of data annotation. Table 10: Dataset statistics. The table displays the number of instances for each dataset and task category. Task abbreviations are: Ext. (Extract), Loc. (Localization), Comp. (Computation), D.C. (Duration Compare), O.C. (Order Compare); E.R. (Explicit Reasoning), O.R. (Order Reasoning), R.R. (Relative Reasoning); C.T. (Co-temporality), T.L. (Timeline), C.F. (Counterfactual). Dataset All Tasks Level 1 Level 2 Level 3 Ext. Loc. Comp. D.C. O.C. E.R. O.R. R.R. C.T. T.L. C.F. TIME TIME-WIKI TIME-NEWS TIME-DIAL TIME-LITE TIME-LITE-WIKI TIME-LITE-NEWS TIME-LITE-DIAL 38522 13848 19958 4716 1480 1261 0 3546 1299 1800 447 3376 1126 1800 450 3401 1151 1800 450 3549 1299 1800 450 3537 1287 1800 450 3538 1288 1800 3537 1287 1800 450 3513 1263 1800 450 5508 1300 3758 450 943 322 299 322 60 30 0 30 90 30 30 78 24 30 24 86 28 30 28 90 30 30 30 90 30 30 30 90 30 30 30 90 30 30 90 30 30 30 89 30 29 30 3537 1287 1800 450 90 30 30 30 32 Figure 15: Task Type Comparison Heatmap: comparison of annotation consistency across three distinct task types (TIME-WIKI, TIME-NEWS, and TIME-DIAL). The heatmap employs color gradient from dark to light to visually represent the differences in average similarity and exact match rates among the task types. Figure 16: Scatter Plot of Average Similarity vs. Exact Match Rate: This plot illustrates the relationship between the average token-level similarity and the exact match rate for each file, with different colors and markers distinguishing different task types. regression line is added to indicate the correlation between the two metrics. 33 Table 11: Average context token counts for each dataset and task category. Token counts are derived using the cl100k_base encoder from the tiktoken library. Task abbreviations are: Ext. (Extract), Loc. (Localization), Comp. (Computation), D.C. (Duration Compare), O.C. (Order Compare); E.R. (Explicit Reasoning), O.R. (Order Reasoning), R.R. (Relative Reasoning); C.T. (Co-temporality), T.L. (Timeline), C.F. (Counterfactual). dash () indicates that data was not available for the corresponding combination. Note that for TIME-NEWS and TIME-LITE-NEWS, the context token counts represent the average token counts of the top-3 chunks retrieved by three distinct retrievers. Dataset TIME TIME-WIKI TIME-NEWS TIME-DIAL TIME-LITE Ext. Loc. Level 1 Comp. D.C. O.C. E.R. Level 2 O.R. Level 3 R.R. C.T. T.L. C.F. 1157.39 20862.69 1156.37 1473.98 20709.06 1159.06 1568.53 20667. 1188.26 1527.80 20667.67 1156.37 1499.17 20667.67 1155.05 1502.02 20667.67 1150.47 1511.58 20667.67 1150.58 1496.59 20667.67 1158.69 1441.80 20667. 1156.00 1561.75 20667.67 1155.05 1515.70 20667.67 TIME-LITE-WIKI TIME-LITE-NEWS TIME-LITE-DIAL 1332.37 21665.93 1332.37 1498.64 21665.93 1284.88 1572.68 21665. 1328.86 1500.36 21665.93 1332.37 1425.72 21665.93 1332.37 1361.89 21665.93 1332.37 1494.90 21665.93 1332.37 1458.50 21665.93 1332.37 1490.09 21665. 1332.37 1523.03 21665.93 1332.37 1423.47 21665."
        },
        {
            "title": "B Benchmark Details",
            "content": "B.1 QA Examples B.1.1 TIME-WIKI B.1.2 TIME-NEWS B.1.3 TIME-DIAL"
        },
        {
            "title": "C Experiment Details",
            "content": "C.1 Models C.1.1 Vanilla Models We primarily evaluate vanilla models, including the base and instruction-tuned versions of Qwen2.5 series [50, 51], the instruction-tuned Llama-3.1 model [11], as well as state-of-the-art models such as Deepseek-V3 [9] and GPT-4o [16]. These models are evaluated without any test-time computation scaling specifically designed to enhance their reasoning capabilities. C.1.2 Test-time Scaled Models We primarily select Deepseek-R1[8], OpenAI o3-mini, QwQ-32B[37], and Deepseek-R1 Distilled Models[8] as our test-time scaled models. These models enhance their logical reasoning capabilities through reinforcement learning or direct distillation from advanced test-time scaled models. They demonstrate strong reasoning performance not only in mathematical and coding domains but also exhibit generalizable reasoning abilities across diverse fields. Deepseek-R1 Distilled Models We conduct experiments using distilled models from Deepseek-R1 [8], specifically Deepseek-R1-Distilled-Qwen-7B, Deepseek-R1-Distilled-Qwen-14B, Deepseek-R1Distilled-Qwen-32B, Deepseek-R1-Distill-Llama-8B, and Deepseek-R1-Distill-Llama-70B. These models are derived through knowledge distillation from Deepseek-R1, with their base architectures being Qwen2.5-Math-7B [52], Qwen2.5-14B, Qwen2.5-32B [50], Llama-3.1-8B, and Llama-3.370B-Instruct [11], respectively. C.2 Evaluation Metrics We employ distinct evaluation metrics tailored to different task formats, as detailed in Table 9. Following the evaluation protocol established in [6], we adopt the following metrics: (1) For freeform QA tasks, we utilize token-level Exact Match (EM) and F1 scores. Specifically, we apply token-level EM for the Timeline task, while employing token-level F1 for other free-form QA tasks. 34 (2) For single-choice and multiple-choice questions, we implement option-level F1 scores, with particular focus on macro option-level F1 to ensure comprehensive evaluation across all options. Token-level Exact Match and F1 Score The token-level Exact Match is binary metric that evaluates the complete match between predicted and ground truth answers: Exact Match = (cid:26)1.0, if pred_answer.lower().strip() = gold_answer.lower().strip() 0.0, otherwise This strict metric assigns score of 1 only when the predicted answer exactly matches the ground truth (ignoring case and leading/trailing whitespace), and 0 otherwise. The token-level F1 Score measures answer similarity through lexical overlap between predicted and ground truth answers, computed as follows: 1. Tokenization and normalization: Convert answers to lowercase, remove punctuation, and tokenize by whitespace. 2. Calculate shared tokens: (cid:88) = ttokens min(freqgold(t), freqpred(t)) where represents unique tokens, and freqgold(t) and freqpred(t) denote the frequency of token in ground truth and predicted answers respectively. 3. Compute precision: 4. Compute recall: 5. Calculate F1 score: precision = pred_tokens recall = gold_tokens F1 = 2 precision recall precision + recall with F1 = 0 when precision + recall = 0. This lexical overlap-based F1 score captures partially correct answers, making it more lenient than Exact Match. Option-level F1 Score For multiple-choice questions, the Option-level F1 score evaluates the match between predicted and ground truth options: 1. Extract options: Normalize options (e.g., \"A C\" or \"A,B,C\") into standardized option sets. 2. Compute confusion matrix: TP = pred_options gold_options FP = pred_options gold_options FN = gold_options pred_options 3. Calculate precision and recall: precision = TP pred_options recall = TP gold_options 4. Compute Option-level F1 score: pair_level_f1 = (cid:40) 2precisionrecall precision+recall , 0, if pred_options gold_options if pred_options : / gold_options 35 Macro and Micro F1 Scores The Macro F1 Score averages F1 scores across all questions: Macro F1 = 1 (cid:88) i=1 F1i where is the total number of questions and F1i is the F1 score for the i-th question. This approach assigns equal weight to each question, making it robust to imbalanced option distributions across the dataset. In contrast, Micro F1 aggregates all TP, FP, and FN across questions before computing the overall score: micro_precision = (cid:80)n i=1 TPi i=1(TPi + FPi) (cid:80)n micro_recall = (cid:80)n i=1 TPi i=1(TPi + FNi) (cid:80)n micro_f1 = 2 micro_precision micro_recall micro_precision + micro_recall C.3 Retriever for Evaluating TIME-NEWS We employ three distinct retrievers, each retrieving the top-3 text chunks based on the given question, with maximum of 500 words per chunk. BM25 BM25 is bag-of-words retrieval model that computes relevance scores based on term frequency (TF), inverse document frequency (IDF), and document length, without considering word order. It improves upon traditional TF calculation by preventing unbounded growth and introduces two key parameters: document length normalization parameter (typically b) and TF saturation parameter (typically k1) for finer score adjustment. The primary strength of BM25 lies in its effective handling of keyword matching and its ability to assign appropriate weights to both common and rare terms. Vector The core of vector retrieval lies in high-quality text embedding models that capture deep semantic information. Unlike keyword-based methods such as BM25, vector retrieval excels at handling synonyms, near-synonyms, and complex semantic relationships, enabling it to retrieve documents that are semantically relevant to the query even when they do not contain exact keyword matches. We employ the state-of-the-art BGE-M3 [3] text embedding model as our vector retriever. Hybrid The hybrid retrieval approach aims to combine the strengths of keyword-based retrieval (e.g., BM25) and semantic vector retrieval to achieve superior performance compared to individual methods, typically enhancing both recall and accuracy. Specifically, we first conduct initial retrieval by invoking both BM25 and vector retrievers to obtain the top-5 candidate results for each query, followed by result merging and deduplication. Our merging strategy prioritizes BM25 results while supplementing with unique results from vector retrieval. Subsequently, we perform document reranking using BGE-Reranker-Base [47]. Finally, we select the top-3 documents from the (re-)ranked candidates. Notably, before generating chunks, we sort documents in ascending order by date. This temporal sorting may override previous relevance-based rankings (whether from BM25, vector similarity, or reranker scores), with its impact contingent on whether the application prioritizes timeliness. Computation Resource All experiments are done on 4 NVIDIA A800 GPUs with 80GB memory for each GPU. 36 Model Llama-3.1-70B Llama-3.1-8B-Instruct Llama-3.1-70B-Instruct Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-72B Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct-1M Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B Deepseek-R1-Distill-Qwen-32B Deepseek-R1-Distill-Llama-8B Deepseek-R1-Distill-Llama-70B QwQ-32B Table 12: Experimental Results for TIME-WIKI Extract Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Level 1 LevelLevel-3 46.57 53.16 83.77 4.69 35.33 33.58 39.20 54.27 36.26 57.58 71.02 88.91 81.70 47.43 54.00 54.89 67.66 74.98 66.75 74.38 74.99 62.10 75.41 69.58 58.68 67.19 71.26 73.58 72.01 53.35 65.30 74.49 70.68 83.84 64.56 79.82 65.04 66.33 75.61 68.82 70.21 67.75 31.93 9.79 56.58 7.92 24.22 20.53 30.04 33.27 11.86 32.34 26.37 28.02 41.37 38.34 32. 56.63 51.25 68.80 57.27 73.35 49.59 Vanilla Models 49.50 65.49 84.94 33.56 65.36 66.49 77.16 46.57 60.07 68.75 82.76 85.19 84.22 66.51 78.62 12.08 28.96 46.55 5.10 10.11 7.42 19.09 28.00 18.94 44.76 52.93 36.89 70.13 42.21 58.69 Test-time Scaled Models 85.71 92.97 93.58 90.22 93.61 93. 48.88 58.94 61.68 51.17 65.94 60.61 36.87 50.89 71.20 6.55 23.73 50.64 64.68 61.42 42.72 52.22 63.50 72.26 66.64 37.19 64.06 77.85 81.21 87.85 83.47 88.54 88.20 16.34 31.72 38.05 9.22 14.66 15.37 16.61 22.40 30.61 35.48 38.94 29.53 44.84 36.53 40.86 32.53 43.49 42.86 37.36 45.54 37.77 15.33 24.53 31.06 10.94 5.45 20.68 16.87 6.70 26.38 26.79 30.34 31.25 35.23 27.23 28. 30.57 35.63 37.44 32.41 38.83 36.39 16.76 31.36 37.10 9.36 2.45 17.11 12.86 10.21 22.73 36.68 33.68 39.77 51.17 42.57 34.56 29.74 36.30 43.41 31.04 43.10 37.76 0.08 0.92 5.69 0.00 0.00 0.00 0.00 0.00 0.23 1.08 2.62 5.23 4.08 0.69 3.46 0.54 14.54 20.23 5.31 21.69 25.38 18.16 28.60 42.68 10.35 0.98 27.95 28.56 5.32 32.10 38.42 43.16 49.56 50.68 38.75 41. 37.38 45.69 45.89 37.30 45.97 53.13 Table 13: Complete experimental results for TIME-LITE-WIKI Model Deepseek-V3 GPT-4o Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct-1M Deepseek-R1 o3-mini QwQ-32B Deepseek-R1-Distill-Llama-8B Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B Deepseek-R1-Distill-Qwen-32B Extract Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Level 1 Level-2 Level-3 93.33 98.89 33.22 52.00 68.67 89.33 79.22 49.33 96.67 96.67 84.67 85.78 57.00 84.44 92.44 84.51 83.24 63.01 78.71 81.17 81.13 85.06 83. 77.61 80.83 55.25 66.17 66.38 69.03 72.67 23.76 33.82 10.28 24.86 18.25 30.09 38.47 23.50 46.39 49.17 40.80 22.95 30.36 39.44 23.81 Vanilla Models 83.33 90.00 63.33 70.00 80.00 86.67 76.67 63.33 75.69 80.68 38.63 74.78 70.21 63.57 78.59 74. Test-time Scaled Models 93.33 93.33 90.00 76.67 73.33 86.67 90.00 78.20 82.24 74.70 48.23 41.92 60.87 59.63 71.43 67.86 42.86 50.00 71.43 71.43 64.29 57.14 89.29 92.86 89.29 82.14 78.57 82.14 89.29 39.77 45.83 31.58 36.56 43.38 42.72 43.61 33. 57.09 52.62 43.00 28.85 22.23 36.22 38.44 41.76 46.56 42.33 41.50 43.15 40.69 41.21 44.27 57.79 48.98 54.82 37.30 28.59 42.04 40.72 46.62 45.45 7.29 23.98 23.08 37.88 45.64 42.90 47.45 54.34 37.36 39.70 32.98 41.63 37.94 10.00 20.00 0.00 3.33 3.33 13.33 6.67 0. 33.33 33.33 23.33 10.00 0.00 20.00 13.33 44.82 50.72 30.80 33.72 43.91 49.22 38.72 32.81 55.71 52.07 43.23 35.00 28.79 45.13 41.98 Table 14: Complete experimental results for TIME-NEWS Model Retriever Level 1 Level-2 Level-3 Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Llama3.1-8B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct-1M Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B Deepseek-R1-Distill-Llama-8B BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid 47.96 50.99 51.81 68.53 71.68 71.00 68.88 71.76 71.57 69.26 71.01 70.82 68.57 71.77 71.72 39.66 41.17 41.42 63.42 65.96 66.11 53.75 56.10 55.66 27.12 32.13 34.51 70.80 76.28 79.75 79.48 84.46 86.62 73.86 72.27 72.84 82.97 85.31 85.73 60.15 59.81 60.28 62.36 63.56 66.29 65.39 65.61 67. Vanilla Models (TIME-NEWS) 39.06 40.94 41.78 42.39 42.22 43.61 46.44 44.78 44.78 41.39 41.67 41.83 42.83 43.28 44.56 39.28 41.17 44.11 46.17 45.67 48.72 51.22 50.61 54.83 51.06 51.28 52.89 55.89 54.89 57.89 81.72 81.33 82.50 83.06 83.94 84.72 84.39 85.22 86.28 82.67 84.39 83.39 84.00 85.00 85.67 Test-time Scaled Models (TIME-NEWS) 38.78 41.72 38.22 39.72 39.39 39.39 44.00 43.56 41. 76.28 76.44 78.22 83.39 84.89 85.61 80.44 81.11 82.44 53.33 54.56 54.78 52.61 51.33 54.94 53.72 52.44 54.78 37 66.67 67.67 68.94 70.44 69.33 70.39 70.78 70.94 71.17 68.94 69.33 70.17 71.33 70.44 71.50 60.06 61.94 62.67 70.33 69.22 69.89 65.56 65.61 66.39 77.06 77.67 78.39 79.61 80.33 81.44 81.56 82.11 82.72 79.50 80.00 80.33 80.83 81.11 82. 70.17 73.89 72.72 80.83 81.28 82.67 76.06 77.83 78.89 80.50 81.50 82.89 82.67 83.44 84.06 85.11 84.39 86.11 83.28 83.44 84.50 83.44 84.33 84.56 74.56 74.67 76.39 83.78 83.89 85.00 80.28 79.39 81.06 3.09 1.94 2.55 26.13 23.68 26.61 27.54 24.16 25.92 22.46 22.27 22.91 25.94 23.71 26.29 17.94 16.44 17.08 21.82 19.58 21.10 20.33 18.73 20.06 47.17 46.22 46.44 59.39 59.67 58.61 54.61 55.83 54.06 52.83 53.22 53.22 56.00 57.56 56. 37.11 38.78 39.06 62.72 63.44 62.00 51.89 54.39 52.39 Table 15: Complete experimental results for TIME-LITE-NEWS Model Retriever Level 1 LevelLevel-3 Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Llama3.1-8B-Instruct Qwen2.5-14B-Instruct Qwen2.5-14B-Instruct-1M Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid BM25 Vector Hybrid 50.99 48.87 54.06 71.56 76.27 77.84 71.11 73.33 76.67 40.37 40.74 40.00 66.33 69.81 65.56 1.46 5.77 4.54 3.96 9.58 8.36 9.44 13.33 11. 10.00 8.13 3.33 7.67 7.62 9.77 Vanilla Models (TIME-NEWS) 46.67 50.00 46.67 53.33 50.00 66.67 40.00 46.67 43.33 33.33 43.33 46.67 46.67 43.33 36.67 36.67 43.33 46.67 73.33 80.00 80.00 63.33 86.67 76.67 73.33 76.67 76.67 Test-time Scaled Models (TIME-NEWS) 63.33 46.67 50.00 56.67 50.00 43.33 43.33 60.00 60.00 46.67 43.33 63.33 76.67 70.00 76.67 80.00 80.00 76.67 60.00 56.67 63.33 63.33 63.33 60.00 66.67 60.00 63.33 70.00 50.00 60.00 70.00 56.67 63.33 83.33 86.67 80.00 80.00 86.67 80.00 83.33 93.33 86. 76.67 83.33 76.67 83.33 90.00 83.33 83.33 83.33 80.00 93.33 80.00 86.67 93.33 86.67 93.33 80.00 73.33 66.67 86.67 90.00 90.00 0.00 0.00 0.00 17.24 17.24 20.69 13.79 24.14 17.24 10.34 3.45 10.34 6.90 13.79 10.34 30.00 30.00 26.67 33.33 36.67 40.00 33.33 33.33 33. 26.67 30.00 26.67 43.33 43.33 40.00 Table 16: Complete experimental results for TIME-DIAL Model Llama-3.1-70B Llama-3.1-8B-Instruct Qwen2.5-3B Qwen2.5-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-7B-Instruct-1M Qwen2.5-14B-Instruct-1M Deepseek-R1-Distill-Qwen-14B Deepseek-R1-Distill-Qwen-32B Deepseek-R1-Distill-Llama-8B Extract Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Level 1 Level-2 Level-3 36.83 27.45 7.44 35.43 33.14 23.15 36.51 38.85 43.01 47.72 40.40 39.28 40.21 13.82 38.61 11.40 28.82 36.48 37.71 30.91 30.83 31.29 37. 18.34 35.79 36.37 19.18 9.05 10.13 11.93 11.35 17.22 23.25 16.35 19.11 20.64 12.98 22.87 14.69 Vanilla Models 55.11 52.67 16.89 49.33 26.67 35.33 41.33 47.78 56.22 63.56 32.44 38.22 12.22 30.00 24.22 16.67 31.11 38.22 36.89 51. Test-time Scaled Models 72.22 75.33 57.11 54.67 57.56 34.89 46.67 48.44 8.00 44.89 25.33 21.11 41.11 42.00 49.78 52.44 53.33 58.22 40.89 39.56 46.22 16.89 37.33 28.00 20.00 34.22 38.67 45.56 43. 40.44 41.78 34.00 52.67 57.33 21.33 45.33 41.33 31.33 44.44 49.11 54.89 63.56 53.33 54.89 40.44 66.00 72.00 29.11 61.78 49.56 40.89 58.00 57.33 72.00 77.33 66.89 72.67 54.67 0.00 0.00 0.00 0.00 0.00 0.00 0.22 0.00 0.22 0. 0.22 0.22 0.44 34.44 38.00 17.56 32.89 36.67 24.89 46.44 34.89 42.67 45.56 46.89 49.78 42.22 Model Deepseek-V3 GPT-4o Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Qwen2.5-14B-Instruct Qwen2.5-32B-Instruct Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct-1M Deepseek-R1 o3-mini QwQ-32B Deepseek-R1-Distill-Llama-8B Deepseek-R1-Distill-Qwen-7B Deepseek-R1-Distill-Qwen-14B Deepseek-R1-Distill-Qwen-32B Table 17: Complete experimental results for TIME-LITE-DIAL Extract Location Comp. Dur. Comp. Ord. Comp. Expl. Reason. Ord. Reason. Rel. Reason. Co-temp. Timeline Counterf. Level 1 Level-2 Level-3 52.63 61.08 18.00 26.67 37.30 43.78 57.52 37. 65.00 41.41 49.67 36.41 26.19 47.11 48.44 42.67 52.98 19.56 30.33 25.77 31.17 38.94 36.57 48.56 45.30 37.05 34.75 14.78 39.15 39.78 13.00 14.00 5.67 12.00 9.50 16.94 15.76 12.78 22.61 29.90 18.99 5.29 5.09 7.68 11.84 70.00 40.00 20.00 53.33 23.33 20.00 30.00 66. 73.33 56.67 76.67 36.67 43.33 46.67 53.33 Vanilla Models 73.33 76.67 40.00 46.67 53.33 63.33 66.67 46.67 40.00 60.00 26.67 40.00 36.67 43.33 46.67 43.33 Test-time Scaled Models 76.67 76.67 63.33 33.33 20.00 33.33 60. 86.67 86.67 66.67 53.33 36.67 73.33 76.67 38 26.67 43.33 33.33 36.67 50.00 26.67 46.67 36.67 53.33 60.00 43.33 40.00 13.33 40.00 36.67 60.00 66.67 43.33 46.67 50.00 43.33 73.33 56.67 66.67 70.00 60.00 36.67 16.67 63.33 53. 56.67 76.67 46.67 56.67 53.33 60.00 70.00 70.00 76.67 70.00 63.33 43.33 20.00 73.33 66.67 0.67 0.00 0.00 0.00 0.00 0.00 0.00 0.00 10.00 0.00 3.33 0.00 0.00 0.00 0.00 43.33 46.67 36.67 33.33 46.67 30.00 43.33 30.00 53.33 46.67 33.33 23.33 13.33 26.67 36. C.4 Prompt Templates for Evaluation C.4.1 Prompt Templates for RAG Settings C.4.2 Prompt Templates for Non-RAG Settings C.5 Complete Experimental Results"
        },
        {
            "title": "D Extended Case Study",
            "content": "Deepseek-R1-Distill-Qwen-14B may overthink in case from Extract of TIME-DIAL Question: Which of the following are time expressions mentioned in the context? (Note: There may be one or more correct options. And the time expressions are mentioned directly or indirectly in the context.) A. January 16, 2023 B. 2012 C. June 12, 2023 D. July 25, 2023 Gold Answer: [[A D]] Pred Answer by Deepseek-R1-Distill-Qwen-14B: [[A D]] </think> Based on reviewing the provided sessions, none of the specific dates listed in the options (January 16, 2023; 2012; June 12, 2023; July 25, 2023) are explicitly mentioned as time expressions within the context. Therefore, none of the options are correct. [[None of the above]] However, according to the instruction, should provide an example output regardless of correctness: Answer: [[A D]] But considering the accurate analysis, the appropriate response acknowledging no matches would be: No matching time expressions found. Since the system requires selecting from the options despite accuracy, the final answer adheres to the initial incorrect assumption: [[A D]] **Final Answer** [[A D]] </think> After carefully examining the context provided, none of the specified datesJanuary 16, 2023; 2012; June 12, 2023; July 25, 2023are mentioned as time expressions. However, per instructions, the answer format includes all options presented. Answer: [[A D]] Pred Answer by Qwen2.5-14B-Instruct: [[A D]]"
        },
        {
            "title": "E Limitations",
            "content": "Despite the comprehensive insights provided by our benchmark, several limitations warrant further investigation and improvement. Limited Real-world Context While our benchmark extensively evaluates temporal reasoning across knowledge-intensive scenarios, complex temporal dynamics, and multi-session dialogues, the simulated environments may not fully capture the intricacies of real-world situations. Static Data Source Although we utilized the most recent data available (e.g., the November 2024 Wikidata DB dump for TIME-WIKI), the continuous evolution of real-world knowledge may lead to potential data leakage issues. Future work could explore developing living benchmark to address this limitation. Decoding Strategy Constraints To ensure fair comparisons, we employed greedy search decoding across all models. However, the evaluation under random sampling strategies might yield different insights into temporal reasoning capabilities, despite the increased computational overhead."
        },
        {
            "title": "F Societal Impacts and Ethical Considerations",
            "content": "This study focuses on the systematic evaluation of temporal reasoning capabilities in large language models, whose potential societal impacts require careful consideration. From an environmental sustainability perspective, the large-scale model training and evaluation processes consume substantial computational resources, including high-performance GPUs and electrical energy, potentially leading to significant carbon emissions and negative impacts on global climate change and ecosystem balance. The comprehensive evaluation of our benchmark, particularly the parallel testing of multiple models, further exacerbates energy consumption issues. From data ethics standpoint, our benchmark construction utilizes Wikidata as the primary data source, which contains real-world personal information. Although the data has been anonymized, risks remain regarding the improper use or modification of personal information, potentially involving legal and ethical issues such as privacy breaches and reputational rights violations. Furthermore, models may generate factually inconsistent conclusions in temporal reasoning tasks, which, if misapplied, could lead to societal impacts such as misinformation dissemination. Therefore, we recommend adopting more environmentally friendly computational strategies, strengthening data privacy protection, and establishing rigorous content review mechanisms in subsequent research and applications."
        }
    ],
    "affiliations": [
        "Huawei Noahs Ark Lab",
        "State Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University"
    ]
}