{
    "paper_title": "Grounded Misunderstandings in Asymmetric Dialogue: A Perspectivist Annotation Scheme for MapTask",
    "authors": [
        "Nan Li",
        "Albert Gatt",
        "Massimo Poesio"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce a perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using a scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both a resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs' capacity to model perspective-dependent grounding in collaborative dialogue."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 8 1 7 3 0 . 1 1 5 2 : r Grounded Misunderstandings in Asymmetric Dialogue: Perspectivist Annotation Scheme for MapTask Nan Li, Albert Gatt, Massimo Poesio Utrecht University, Utrecht, The Netherlands {n.li, a.gatt, m.poesio}@uu.nl Abstract Collaborative dialogue relies on participants incrementally establishing common ground, yet in asymmetric settings they may believe they agree while referring to different entities. We introduce perspectivist annotation scheme for the HCRC MapTask corpus (Anderson et al., 1991) that separately captures speaker and addressee grounded interpretations for each reference expression, enabling us to trace how understanding emerges, diverges, and repairs over time. Using scheme-constrained LLM annotation pipeline, we obtain 13k annotated reference expressions with reliability estimates and analyze the resulting understanding states. The results show that full misunderstandings are rare once lexical variants are unified, but multiplicity discrepancies systematically induce divergences, revealing how apparent grounding can mask referential misalignment. Our framework provides both resource and an analytic lens for studying grounded misunderstanding and for evaluating (V)LLMs capacity to model perspective-dependent grounding in collaborative dialogue. Keywords: reference expression, common ground, misunderstanding, collaborative task, annotation scheme 1."
        },
        {
            "title": "Introduction",
            "content": "In everyday collaborative dialogue, speakers and addressees continuously coordinate their understanding of what is being discussed (Clark and Brennan, 1991; Pickering and Garrod, 2004). central mechanism for this coordination involves reference expressions (REs)linguistic forms used to pick out entities in the shared context. Utterances become part of the common ground only after being recognized and acknowledged by the addressee (Clark and Wilkes-Gibbs, 1986; Clark and Schaefer, 1989; Clark and Brennan, 1991), process known as grounding. Traditional approaches to reference resolution assume that once grounding is achieved via explicit or tacit confirmation, both interlocutors successfully refer to the same entity. However, this assumption can fail in asymmetric settings where participants have access to different information. Such asymmetric settings were the focus of the HCRC MapTask ( Anderson et al., 1991; see Figure 1 for an example), where two participants navigate route using slightly different maps. While participants in such settings show systematic adaptation behaviors to address information asymmetry (Bard et al., 2000; Viethen et al., 2011a,b; Healey et al., 2018), making full misunderstandings rare, few studies have examined the personal interpretations of REs by the two interlocutors and the subtle divergences that may persist even after apparent grounding. These limitations also exist in the MapTask annotations (discussed in Section 4.1), making them difficult to be used to identify misunderstandings. To understand how interlocutors achieve and Figure 1: simplified example of misunderstandings from MapTask dialogues. Left: the instruction givers map contains two instances of parked van and the intended target is circled. Middle: the instruction followers map shows only one parked van at the bottom that is shared with the giver. Right: the giver says go to the parked van, the follower acknowledges but grounds another instance, leading to correction (No!). Red highlights the reference expression. sometimes fail to achieve mutual understanding, we must first capture each participants personal interpretation. Taking both perspectives allows us to quantify where apparent alignment hides subtle misalignments and to measure how frequent such misunderstandings actually are under different landmark discrepancy settings. Prior frameworks for modeling dialogue states offer rich tools to trace how shared understanding is established (Poesio and Traum, 1997; Cooper et al., 1999; Matheson et al., 2000; Lascarides and Asher, 2009; Schlangen and Skantze, 2011; Ginzburg, 2012). However, these symbolic, frame-based representations have not been widely integrated into reference-level annotation schemes, partly because they are difficult for large language models (LLMs) to reproduce in constrained, schemadriven manner. We introduce an annotation framework based on the MapTask annotations that explicitly captures the divergence between the speakers intended and the addressees interpreted referents during collaborative dialogue. Our scheme uses hierarchy of attributes to represent personal interpretations and incremental grounding, making it both linguistically principled and operational for LLM annotation. Applying this framework to MapTask dialogues with GPT-5 (OpenAI, 2025), we scale annotation across the entire MapTask corpus. While full misunderstandings remain uncommon, our analysis reveals systematic patterns of understanding state transitions. These findings not only shed light on how diverging interpretations emerge and resolve in collaborative tasks but also establish benchmark for evaluating language-only and vision-language models grounding abilities. Our research questions are: RQ1 Is it possible to develop personalinterpretation annotation scheme capture nuanced understanding states for REs in collaborative tasks like MapTask? RQ2 Given conflicting personal interpretations, can we trace how understanding evolves across turns until the successful grounding? RQ3 Can LLMs, under schema-constrained protocol, reliably annotate personal interpretations; and how does their output inform future evaluation of (V)LLMs on grounded dialogue? Our contributions include: We operationalize reference expression grounding perspectivedependent alignment process, and propose annotation scheme at the RE level. graded, as We present scheme-constrained LLM-in-theloop annotation pipeline of MapTask dialogues, producing 13k annotated REs with reliability estimates. We release analysis protocols that associate RE-level annotations with dual-perspective interpretations and incremental understanding states, enabling quantitative study of misalignment dynamics in MapTask dialogues and informing future evaluation of LLMs and VLMs on common grounding. The remainder of this paper is structured as follows. Section 2 reviews related work. Section 3 introduces the MapTask corpus and its challenges for reference grounding. Section 4 details our annotation scheme and LLM-in-the-loop annotation pipeline. Section 5 analyzes understanding state distributions and transitions. Finally, Section 6 concludes with implications and future directions. 2. Related work Collaborative Reference and Grounding Reference expressions are central to establishing and maintaining common ground in interactive dialogue (Clark and Wilkes-Gibbs, 1986; Clark and Schaefer, 1989). Successful mutual understanding is constrained by many factors, e.g., communication medium, interactive alignment at different linguistic levels (Clark and Brennan, 1991; Pickering and Garrod, 2004). Previous work reveals that speakers do not always fully accommodate addressee knowledge, exhibiting egocentric biases even when explicitly aware of information asymmetry (Horton and Keysar, 1996; Keysar et al., 2000; Lane et al., 2006) This theoretical foundation has motivated NLP tasks to study grounded reference in collaborative settings with asymmetric information. The HCRC MapTask (Anderson et al., 1991) and iMap corpus (Louwerse et al., 2007) both instantiate navigation tasks where participants have slightly different maps, eliciting rich adaptation behavior (Bard et al., 2000; Viethen et al., 2011a,b). More recent work includes the OneCommon Corpus (Udagawa and Aizawa, 2019), partially-observable environment for both participants. These approaches typically treat grounding as single end state rather than tracking divergent personal interpretations throughout the dialogue. Current VLMs have excelled at some object identification tasks (Yu et al., 2016), where referents (objects in images) are assumed to be deterministically grounded once specified. Some recent work shows that VLMs struggle to adopt different frames of reference and visual perspectives (Zhang et al., 2024; Lee et al., 2025). Moreover, existing benchmarks rarely evaluate whether models can track evolving, asymmetric interpretations as dialogue progresses. Our work addresses this gap by annotating personal interpretations at the RE level, enabling future evaluation of whether (V)LLMs understand incremental grounding processes in (asymmetric) dialogue. Understanding State Tracking Meaning coordination operates through continuous running repairs than discrete error correction episodes (Healey et al., 2018). This repair-driven coordination directs the participants evolving understanding states, and motivates the need to track rather divergent interpretations as they emerge and resolve through integration. Formal dialogue theories have modeled interlocutors intentions and repairs through structured information state models. Some proposals (Poesio and Traum, 1997; Matheson et al., 2000) extended Discourse Representation Theory to integrate semantic and pragmatic information, showing how turn-taking, discourse segmentation, and grounding are represented through conversation acts in common ground. Ginzburg (2012) proposed the KoS framework1, where unresolved Questions Under Discussion (QUDs) persist until explicitly addressed. Lascarides and Asher (2009) modeled agreement and correction dynamics but assumed highly idealized scenario which excludes misunderstandings. Schlangen and Skantze (2011) treated divergent understandings as sub-utterance level hypothesis-revision process. Some recent work leveraged annotations from other meaning represenatations e.g., Abstract Meaning Representation, to represent common ground structure and information states (Khebour et al., 2024; Lai et al., 2025). We are informed by this line of work, but focus on RE-level perspectivist interpretations and inferring understanding states through the alignment and misalignment of these interpretations. LLM as an Annotator LLMs have demonstrated strong performance on various semantic and pragmatic annotation tasks (Eichin et al., 2025; Chen et al., 2025; Qamar et al., 2025). They also achieve comparable results to human annotators in subjective tasks such as sentiment analysis (Bojić et al., 2025). However, challenges remain for schemeconstrained annotation tasks requiring structured outputs (Ettinger et al., 2023). Recent advances in grammar-constrained decoding and JSON schemabased generation offer promising solutions, enabling LLMs to generate outputs that adhere to predefined structures while maintaining semantic accuracy (Geng et al., 2023; Park et al., 2025). Our work builds on these developments by employing multi-layer attribute resolution pipeline to guide LLMs in capturing personal interpretations in collaborative dialogue. 3. MapTask: Annotation and Beyond 3.1. Development and Follow-up Studies HCRC MapTask is corpus documenting unscripted dialogues in collaborative route-following task with asymmetric maps. Participants must collaborate to reproduce route on one participants 1A term loosely connected to conversation-oriented semantics (Ginzburg, 2012). map (the instruction givers) on the others (the instruction followers). They cannot see each others maps and can only communicate verbally, with eye contact possible in half of the dialogues. The intentional differences between maps on routes, landmark names, and landmark placements often leads to ambiguous or misleading instructions. This makes the MapTask corpus valuable resource and testbed for studying grounding, reference, and repair theories in dialogue. Subsequent research analyzed reference expressions in MapTask from multiple perspectives. Bard et al. (2000) show that speakers reduce reduce articulatory clarity when mentioning landmarks for the second time, even when the addressee has not heard the first mention or cannot see the referent, suggesting that intelligibility control is driven primarily by the speakers own knowledge rather than careful modeling of the listeners information state. Varges (2005) treated spatial descriptions in MapTask as reference expressions that distinguish particular map points from distractors, observing that speakers frequently use spatial relations (e.g., above the west lake) and vague quantifiers (e.g., about two inches southwest) when referring to featureless route points around named landmarks. Marge and Rudnicky (2011) analyzed miscommunication patterns in MapTask, documenting how instruction followers detect ambiguity (e.g., multiple possible referents) and misunderstanding through clarification questions, repair strategies, and explicit feedback when requested actions appear impossible or unclear. Poesio et al. (2004) applied the MATE annotation scheme (Poesio et al., 1999) to an Italian adaptation of MapTask to make anaphoric relations explicit. 3.2. Task Design and Landmark"
        },
        {
            "title": "Discrepancies",
            "content": "The HCRC MapTask corpus consists of 16 pairs of maps and 128 dialogues, with 8 dialogues recorded for each map. In each dialogue, two participants are given visually similar but non-identical maps. One participant, the giver, has route marked on the givers map and must guide the other participant, the follower, to reproduce this route on the followers map through verbal communication alone. Both maps share the same starting point and most landmarks, but only the givers map displays the complete route and endpoint. Critically, participants cannot see each others maps, creating an asymmetric information setting. The corpus contains 267 unique landmarks (differentiated by names) across all maps. While both participants maps share many identical landmarks (132 landmarks, 49.4%), the remaining 135 landmarks (50.6%) exhibit systematic discrepancies. We classify these discrepancies into three categories 2 and present the distribution by discrepancy type in Table 1. Discrepancy Type Count 20 Lexical 99 Existence 16 Multiplicity Discrepant Identical Total 135 132 267 % 7.5 37.1 6.0 50.6 49.4 Ref. 914 2878 956 4748 % 7.0 22.0 7.3 36.3 63.7 100.0 13081 100.0 Table 1: Distribution of landmark types and corresponding reference expressions in the MapTask corpus. Lexical Discrepancy (20 landmarks forming 10 pairs, 7.5%): Landmarks appear at the same location on both maps with identical icons but slightly different name labels. For instance, one map may label landmark cliffs while the other reads sandstone cliffs; or carved wooden pole versus totem pole. landmarks, Discrepancy (100 Existence 37.5%): Landmarks appear on one participants map but are entirely absent from the others. The participant viewing the map without the landmark must either signal the absence or imagine its location based on the partners description. Multiplicity Discrepancy (16 landmarks, 6.0%): Landmarks appear twice on one map (always the givers) but only once on the other (the followers). Shared instances of this type occupy the same location on both maps but are positioned farther from the route than the unique, non-shared instances. 3.3. Annotations Leveraged in This Work The dataset of the MapTask corpus is publicly available online3 with rich multi-level annotations. For our experiment and analysis, we leverage the following annotation layers: (1) Timed Units, wordlevel transcriptions aligned with audio timestamps, providing the basic segmentation of speech and text. (2) Reference Expressions, annotations of reference expressions linked to landmark IDs, recorded based on timed units. (3) Move Tags and Transactions, move-level annotations following the 2In the original MapTask paper (Anderson et al., 1991), the authors claim that they also add some phonetic contrast features to landmark names to create discrepancies; we assign them to the following three categories according to our definition and observation. 3https://groups.inf.ed.ac.uk/maptask/ coding scheme of Carletta et al. (1997), capturing the communicative function of utterances (e.g., instruct, acknowledge, query), recorded based on timed units. Transactions are sequences of moves that move from one point to another on maps. We conducted preprocessing to integrate these annotations into unified format suitable for LLM annotation. First, we reassigned landmark identifiers (IDs) to distinguish between giver-side and follower-side interpretations (in Section 4.1), as the original landmark ID scheme fails to highlight the discrepancies in some cases. Second, we segmented the dialogue transcripts into transactions to control the context window when providing context to the model. Finally, we generated structured prompts combining reference expressions, new unified landmark ID candidates, and context dialogue to support the annotation workflow described in Section 4.2. 4. Modeling Personal Interpretations 4.1. Approach and General Principles We introduce perspectivist, RE-level annotation scheme that records the speaker-intended and addressee-interpreted landmark IDs, plus five binary attributes in fixed decision order. Landmark ID Design The original MapTask annotations assign landmark IDs by landmark names, which treat landmarks of multiplicity discrepancies (e.g., two parked van at the givers map, one at the followers map) as the same entity, but treat lexical variants (e.g., cliffs versus sandstone cliffs) as different entities. This conflation obscures whether participants successfully align their interpretations. To address this limitation, we introduce new unified landmark ID assigning strategy that distinguishes giver-side and follower-side landmark instances. In this paper, we use the acronyms mtlm and umlm to refer to the original MapTask landmark ID and our unified landmark ID, respectively. Our landmark IDs follow the format <mapid>_<landmark-name>#<ordinal>@<side>, where: map-id: the original Map ID (m0, m1, . . . , m15) landmark-name: the landmark name as it appears on the map (e.g., parked_van, cliff) ordinal: Positional index (0, 1, . . . ) for landmarks of the same type occurring multiple times on maps, ordered from bottom to top. Landmarks of this type have the same ordinal number if they are at the same location on both maps, which means it is possible that the only parked van on the followers map is m0_parked_van#1@f if the givers map has two parked van, while on the givers map m0_parked_van#0@g denotes the unique lower instance. location based on the speakers description. In such cases, we assign the speakers landmark ID to the addressees interpretation and mark is_imagined = true, signaling that alignment was achieved. side: Map side (g for giver, for follower) When reference expression refers to multiple we concatenate IDs with the operator +, e.g., m0_parked_van#0@g+m0_parked_van#1@g. simultaneously, landmarks Beyond landmark IDs, we annotate five binary attributes that jointly characterize the nature of the expression and the addressees understanding state. These attributes are designed in hierarchical decision cascade that models the incremental resolution of reference and embeds Chain-of-Thought (Wei et al., 2022) into LLMs reasoning process. is_quantificational Indicates whether the reference expression functions as an existence query from the speakers perspective rather than definite reference. For instance, parked van in Do you have parked van? is quantificational, whereas the parked van in Go past the parked van is not. Such quantificational expressions do not typically presuppose specific referent in MapTask contexts. is_specified Indicates whether the dialogue context provides sufficient evidence to infer the addressees interpretation. If the addressee fails to respond or produces an utterance unrelated to the reference expression, the interpretation remains unspecified. is_accommodated Indicates whether the addressee successfully address the reference expression without signaling comprehension failure. Accommodation fails when the addressee explicitly requests repetition, clarification, or indicates mishearing (e.g., What? or Pardon?). is_grounded Indicates whether the addressee links the reference expression to specific landmark on the maps. An accommodated expression may fail to ground if the addressee cannot identify corresponding landmark (e.g., dont have that or Where is it?). Grounding is prerequisite for assigning landmark ID to the addressees interpretation. is_imagined Indicates whether addressees interpretation refers to landmark absent from his/her own map. This occurs when the addressee, having recognized an existence discrepancy, mentally projects the landmarks the 4.2. Annotation Pipeline Informed by recent advances in Prompt Engineering (Brown et al., 2020; Wei et al., 2022; Sahoo et al., 2024), we develop heuristic prompting annotation pipeline leveraging LLMs to assist with the perspective-taking annotation task. To seek balance between information density and context length (Liu et al., 2023; Kuratov et al., 2024), we provide the dialogue text from the start of the conversation until the end of the current transaction as context. We use scheme-constrained prompt with GPT-5 via the OpenAI Batch API4 and enforce JSON-schema output to scale annotation with quality control. All parameters are set to default. For each reference expression, we construct structured prompt containing: Background Overview of the MapTask task design and landmark discrepancy types, and general introduction of the annotation task. Task description: Detailed description of the annotation task, emphasizing the approach of using landmark IDs and 5-layer attribute hierarchy to annotate. Landmark ID explanation: Explanation of the unified landmark ID format. Annotation rule: Step-by-step workflow instructions operationalizing the five-attribute decision and landmark ID resolution cascade. Output format: JSON schema with required fields. Target reference expressions: All reference expressions in the current transaction to annotate with their IDs. Dialogue context: Dialogue transcript from the start of the conversation until the end of the current transaction, with reference expressions highlighted by brackets. Dialogue acts: Move-level annotations indicating communicative function in time order, e.g. \"[g utt:1 move:instruct] starting off we are above caravan park [f utt:2 move:acknowledge] mmhmm\". 4https://platform.openai.com/docs/ guides/batch Prompt Template (Abbreviated) <background> Background on MapTask, landmark discrepancies, and reference resolution </background> <task_description> Annotate personal interpretations for reference expressions </task_description> <landmark_id_explanation> Unified landmark ID format and semantics </landmark_id_explanation> <annotation_rule> Follow the annotation workflow for each RE step by step: Step 0: Identify the speaker and the addressee, and match them with the giver and follower based on the actual situation; fill in the landmark ID for the speakers interpretation (intended landmark) first. Steps 1-5: Check and fill in attributes is_quantificational, is_specified, is_accommodated, is_grounded, and is_imagined in order. Step 6: If grounded, fill the landmark ID for the addressees interpretation. Step 7: Provide concise (<50 words) evidence-based reason summarizing the judgement in the reason field for each RE. </annotation_rule> <output_format> JSON schema with required fields </output_format> <target_ref_ids> IDs of reference expressions to annotate </target_ref_ids> <context_dialogue> Dialogue transcript with surrounding context </context_dialogue> <context_dialogue_acts> Move-level annotations for context </context_dialogue_acts> <landmark_candidates> All landmarks on both maps with IDs </landmark_candidates> Figure 2: Prompt template for annotation, abbreviated due to space limits. Landmark candidates: List of landmarks on maps with their umlm IDs, based on the scheme defined in the previous section. annotation guidelines, with ambiguous cases discussed and resolved with other authors. First, the model The prompt (see Figure 2) guides the model through sequential and heuristic annotation workflow. identifies the speaker and addressee roles (giver/follower) and assigns the speakers intended landmark ID. Second, it evaluates is_quantificational to determine whether the expression presupposes referent. If not quantificational, the model proceeds through the remaining four attributes in order. Finally, if is_grounded=true, the model assigns the addressees interpreted landmark ID. The model is asked to give concise textual reason citing dialogue evidence for our further analysis. 4.3. Quality Evaluation We apply the annotation pipeline to all 128 dialogues in the MapTask corpus, resulting in 13,077 annotated reference expression instances 5. To evaluate the reliability of the annotation pipeline, we selected 3 dialogues (dialogue IDs: q1ec2, q1nc3, q1nc7) for complete human verification, comprising 504 reference expressions. We compute micro-averaged accuracy and scores for each binary attribute by comparing LLM predictions against the human gold standard. The human gold standard was collected by one experienced annotator (one of the paper authors) following the same 54 reference expressions (from 3 dialogues) were not annotated by the LLM despite being present in the dialogue context and RE list, likely due to uncertainty in generation. We will verify these cases in follow-up runs. Attribute is_quantificational is_specified is_accommodated is_grounded is_imagined 504 466 455 447 422 Acc. 100.00% 1.00 97.85% 0.990 97.58% 0.988 96.20% 0.980 97.63% 0.891 F1 Errors 0 10 11 17 10 RE-level: 28/504 REs (5.6%) with 1 error 48 Table 2: Evaluation metrics by attribute across 3 dialogues (q1ec2, q1nc3, q1nc7; micro-averaged). indicates the number of REs evaluated at each attribute (determined by gold annotations). Bottom line reports the RE-level error count (unique REs with 1 attribute errors). The LLM annotations exhibit relatively high validity across multiple evaluation dimensions when For all compared to human gold standard. grounded reference expressions, the LLM achieves 95.5% accuracy and 99.5% micro-F1, demonstrating reliable recovery of each participants intended and interpreted referents. Table 2 then presents the micro-averaged evaluation results for the five binary attributes across all 3 dialogues. The LLM achieves high performance across all attributes, with accuracy ranging from around 97% to 100% and F1 scores from 0.89 to 1.00. At the reference expression level, 28 of 504 REs (5.6%) contained at least one attribute error, totaling 48 attributelevel mismatches. Attribute-level error breakdown shows that is_grounded has the highest error count (17), largely stemming from false negatives where the RE is grounded in gold by missed by the model, which typically occur when grounding is inferred from implicit confirmation or reuse rather than explicit echoing. 5. Discussion 5.1. Classification of Understanding"
        },
        {
            "title": "States",
            "content": "Our annotation framework derives understanding states from hierarchical attribute decisions at the RE level. We define three primary categories that capture the alignment or divergence between speaker and addressee interpretations: Aligned The addressee successfully accommodates and grounds the reference expression to the speakers intended landmark, and the landmark mtlm IDs match between speaker and addressee interpretations. This represents successful reference resolution. Misunderstood The addressee grounds the reference expression to different landmark than the speaker intended. This is the critical case: both participants believe successful reference has occurred, yet their interpretations diverge. Such silent misalignments may persist undetected unless subsequent dialogue reveals the discrepancy. Pending The interlocutors await further information or signal comprehension failure. This category includes several subtypes corresponding to communication breakdown scenarios associated with the attributes discussed in Section 4.1 (excluding is_imagined). Table 3 presents the distribution of understanding states across our annotated corpus with the misunderstood type reaching 7.07%. However, as discussed in Section 3.2, 7.5% of landmarks exhibit lexical discrepancies where the same landmarks are labeled with different names on maps. For instance, when the giver says the old mill (referring to m12_old_mill@g) and the follower only has m12_mill_wheel@f on his/her map, they may initially correct each other but usually quickly reach an agreement that they are referring to the same landmark at the same location. In such cases, despite the initial lexical mismatch, we assume they have reached an aligned understanding state. Verified by our observations, we unified the 10 pairs of lexical variants and convert the misunderstood cases caused by them into Aligned ones. Table 3 demonstrates the understanding state distribution after this unification as well. Now the misunderstanding rate drops to 1.82%, significant reduction from the original 7.07%. State Aligned Pending Misunderstood Before After Count 8,750 3,403 924 % Count 9,435 3,403 66.9 26.0 7.07 % 72.1 26.0 1.82 Total REs 13,077 100.0 13, 100.0 Table 3: Distribution of understanding states before and after lexical variant unification in the MapTask LLM-annotated dialogues. The relatively low misunderstanding rate aligns with Healey et al. (2018)s analysis of rarity of sustained misunderstandings in collaborative dialogue: participants actively monitor alignment through continuous running repairs and employ clarification strategies to prevent persistent divergence. Discrepancy Type Total REs Misunderstanding Rate (%) 1.4 Lexical 12.0 Multiplicity 3.2 Existence 0.2 Identical 914 956 2,878 8,333 13 115 93 Total 13,081 239 1.8 Table 4: Misunderstanding counts and rates by landmark discrepancy type. Table 4 demonstrates how these misunderstandings distribute across different types of landmark discrepancies. The distribution shows pattern: multiplicity discrepancies account for 50.9% of all misunderstandings (115/226 cases) despite representing only 7.3% of reference expressions in the corpus. The misunderstanding rate of this type is more then 6x higher than the corpus average (12.0% versus 1.86%). This reveals the success of the MapTask asymmetric design in eliciting misunderstandings, as multiplicity discrepancies inherently create ambiguity about which specific landmark instance is being referred to: when landmark type appears twice on the givers map but only once on the followers map, participants often assume referential uniqueness (Lane et al., 2006; Bard et al., 2000). For example, the giver refers to instance #0, the follower accommodates to their visible instance #1, and both believe successful reference has occurred. This silent divergence persists unless subsequent spatial descriptions reveal the mismatch. For misunderstood cases within the existence discrepancy and identical landmarks, we find that the most frequently occurring cause is still lexical similarity. For example, white mountain on Map m9 with the highest misunderstanding occurrences in the identical type is usually confused with slate mountain on the same map, which has the highest misunderstanding occurrences in the existence discrepancy type. Rock fall with the second highest misunderstanding occurrences in the existence discrepancy type is often misunderstood as stones on the same map. These landmarks all have different names, icons, and locations. 5.2. Understanding State Transition To analyze how understanding evolves across repeated references to the same landmark, we extracted reference chainssequences of reference expressions targeting the same landmark mtlm within dialogue. Across 128 dialogues, we identified 1,665 reference chains with mean length of 6.89 references per landmark (range: 170). Discrepancy Type Identical Lexical Existence Multiplicity # Chains Mean Length Max Length Median Length 6 9 3 8.16 10.70 4.09 7.0 939 79 539 108 70 61 31 26 Figure 3: Cumulative distribution of turns-to-ground by discrepancy type. at all, indicating that participants sometimes stop early and abandon negotiation on some landmarks. This aligns with task-oriented dialogue efficiency. Table 5: Chain length by discrepancy type. 5.3. Special Case The length of these chains partly reflects how persistently participants negotiate understanding of landmarks and use these landmarks to generate spatial descriptions. Table 5 summarizes chain lengths by discrepancy type. Lexical variants yield the longest chains on average (10.7 references), as participants often need multiple turns to resolve naming mismatches and confirm shared understanding. Landmarks with no discrepancy generate the second longest chains, while existence discrepancies yield the shortest. Combined with our observations, this suggests that existence discrepancies are usually quickly ruled out as reference anchors when participants realize the landmark is absent from one map, whereas references to nondiscrepant landmarks persist longer because they support iterative spatial description and navigation coordination, aligning with (Viethen et al., 2011a; Reitter and Moore, 2014)s findings. To further disentangle the efforts participants invest in coordinating understanding of landmarks from using grounded landmarks for navigation, we measure the number of turns (counted per utterance) between chains first reference and its first aligned state (turns-to-ground). Figure 3 shows the cumulative distribution of turns-to-ground by discrepancy type across 1,334 chains that reached alignment. The longer negotiation for multiplicity likely reflects the cognitive complexity of distinguishing between multiple same-named landmarks versus simply confirming/denying existence. Participants must coordinate not only on the landmark name but also on which specific instance is intended, requiring more iterative spatial descriptions and confirmations. There are 331 chains that hold no aligned state In dialogue q8ec4, the giver mistakenly takes the finish as the start point, leading to persistent spatial misalignment that lasts for nearly half the conversation. The speaker insists on using absolute directions and distances (e.g., thirty degrees up the way and two inches to your left) to give instructions rather than landmark-relative spatial descriptions, extending the misunderstanding further. In the annotation, the LLM fails to capture the misalignment since the beginning, partly because of an omniscient/perspective bias: the model can see both maps at once, so it underestimates asymmetric knowledge and treats contradictions as noise rather than evidence of wrong starting point. Prior Studies show that human addressees can achieve more efficient communication than overhearers, while VLMs perform poorly at overhearing (Schober and Clark, 1989; Wang et al., 2025). Future evaluations could explicitly test omniscience and perspective (analyzer versus participant versus overhearer conditions) to test models perspective-taking ability in grounded dialogue. 6. Conclusion Understanding when interlocutors truly ground reference expressions versus only appear to do so requires capturing both sides interpretations. This work establishes such perspectivist view by mapping reference expressions to understanding states in asymmetric collaborative dialogue. Our annotation scheme disentangles the speakers intended and the addressees interpreted referents through five-attribute hierarchy and unified landmark IDs, operationalized via scheme-constrained prompting pipeline. Our quantitative analyses reveal given the rarity of full misunderstandings, multiplicity discrepancies act as consistent hazard for eliciting misalignment. Our statistics also support prior studies on interlocutors preferences for reusing grounding-established REs (Healey et al., 2018; Viethen et al., 2011a; Reitter and Moore, 2014). We plan to establish evaluations based on these annotated dialogues for evaluating (V)LLM incremental grounding abilities under different information settings, moving beyond object identification toward perspective-taking in collaborative dialogue. 7. Limitations Annotation Reliability While the LLM annotations achieves high performance compared to human gold standard (5.6% RE-level error rate; see Section 4.3), comprehensive inter-annotator agreement studies are needed to validate annotation reliability across diverse annotators. Spatial Understanding Precision Our perspectivist scheme tracks landmark-level interpretations but does not capture finer-grained spatial reasoning. The MapTask involves route navigation between landmarks, yet our annotations focus solely on landmark reference resolution. Path choices, direction ambiguities, and spatial relations between landmarks remain unannotated. Multimodal information The original MapTask corpus was collected in conversational settings where participants could perceive prosody, and even have eye contact in half of the cases. Our annotation pipeline relies exclusively on dialogue transcripts and maps, potentially missing nonverbal signals that influence grounding. For instance, backchannel responses like mmhmm may carry different conversational functions (acknowledgment versus query) depending on intonation, which our text-only approach cannot distinguish. While we demonstrate that text-based LLM annotation achieves reasonable accuracy, incorporating multimodal information could further improve annotation quality and theoretical coverage. 8. References Anne Anderson, Miles Bader, Ellen Gurman Bard, Elizabeth Boyle, Gwyneth Doherty, Simon Garrod, Stephen Isard, Jacqueline Kowtko, Jan McAllister, Jim Miller, et al. 1991. The HCRC Map Task corpus. Language and speech, 34(4):351 366. Ellen Gurman Bard, Anne Anderson, Catherine Sotillo, Matthew Aylett, Gwyneth DohertySneddon, and Alison Newlands. 2000. Controlling the Intelligibility of Referring Expressions in Dialogue. Journal of Memory and Language, 42(1):122. Ljubiša Bojić, Olga Zagovora, Asta Zelenkauskaite, Vuk Vuković, Milan Čabarkapa, Selma Veseljević Jerković, and Ana Jovančević. 2025. Comparing large language models and human annotators in latent content analysis of sentiment, political leaning, emotional intensity and sarcasm. Scientific reports, 15(1):11477. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901. Jean Carletta, Amy Isard, Stephen Isard, Jacqueline Kowtko, Gwyneth Doherty-Sneddon, and Anne Anderson. 1997. The reliability of dialogue structure coding scheme. Computational linguistics, 23(1):1331. Huiyao Chen, Meishan Zhang, Jing Li, Min Zhang, Lilja Øvrelid, Jan Hajič, and Hao Fei. 2025. Semantic role labeling: systematical survey. arXiv preprint arXiv:2502.08660. Herbert H. Clark and Susan E. Brennan. 1991. Grounding in communication. In Lauren B. Resnick, John M. Levine, and Stephanie D. Teasley, editors, Perspectives on Socially Shared Cognition., pages 127149. American Psychological Association. Herbert H. Clark and Edward F. Schaefer. 1989. Contributing to discourse. Cognitive Science, 13(2):259294. Herbert H. Clark and Deanna Wilkes-Gibbs. 1986. Referring as collaborative process. Cognition, 22(1):139. Robin Cooper, Staffan Larsson, Colin Matheson, Massimo Poesio, and David Traum. 1999. Coding instructional dialogue for information states. Deliverable D1, 1. Florian Eichin, Yang Janet Liu, Barbara Plank, and Michael Hedderich. 2025. Probing llms for multilingual discourse generalization through unified label set. arXiv preprint arXiv:2503.10515. Allyson Ettinger, Jena Hwang, Valentina Pyatkin, Chandra Bhagavatula, and Yejin Choi. 2023. you are an expert linguistic annotator: Limits of LLMs as analyzers of Abstract Meaning RepIn Findings of the Association for resentation. Computational Linguistics: EMNLP 2023, pages 82508263, Singapore. Association for Computational Linguistics. Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-constrained decoding for structured NLP tasks without finetuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1093210952, Singapore. Association for Computational Linguistics. Jonathan Ginzburg. 2012. The Interactive Stance. Oxford University Press. Patrick G. T. Healey, Gregory J. Mills, Arash Eshghi, and Christine Howes. 2018. Running Repairs: Coordinating Meaning in Dialogue. Topics in Cognitive Science, 10(2):367388. William Horton and Boaz Keysar. 1996. When do speakers take into account common ground? Cognition, 59(1):91117. Boaz Keysar, Dale Barr, Jennifer Balin, and Jason Brauner. 2000. Taking perspective in conversation: The role of mutual knowledge in comprehension. Psychological science, 11(1):3238. Ibrahim Khalil Khebour, Kenneth Lai, Mariah Bradford, Yifan Zhu, Richard A. Brutti, Christopher Tam, Jingxuan Tu, Benjamin A. Ibarra, Nathaniel Blanchard, Nikhil Krishnaswamy, and James Pustejovsky. 2024. Common Ground Tracking in Multimodal Dialogue. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 35873602. ELRA and ICCL. Yury Kuratov, Aydar Bulatov, Petr Anokhin, Ivan Rodkin, Dmitry Sorokin, Artyom Sorokin, and Mikhail Burtsev. 2024. Babilong: Testing the limits of llms with long context reasoning-in-ahaystack. Advances in Neural Information Processing Systems, 37:106519106554. Kenneth Lai, Lucia Donatelli, Richard Brutti, and James Pustejovsky. 2025. model of information state in situated multimodal dialogue. In 16th International Conference on Computational Semantics, page 292. Liane Wardlow Lane, Michelle Groisman, and Victor S. Ferreira. 2006. Dont Talk about Pink Elephants! Speakers Control over Leaking Private Information during Language Production. Psychological Science, 17(4):273277. Alex Lascarides and Nicholas Asher. 2009. Agreement, disputes and commitments in dialogue. Journal of semantics, 26(2):109158. Phillip Lee, Jihyeon Je, Chanho Park, Mikaela Angelina Uy, Leonidas Guibas, and Minhyuk Sung. 2025. Perspective-aware reasoning in visionlanguage models via mental imagery simulation. arXiv preprint arXiv:2504.17207. Nelson Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172. Max Louwerse, Nick Benesh, Mohammed Hoque, Patrick Jeuniaux, Gwyneth Lewis, Jie Wu, and Megan Zirnstein. 2007. Multimodal communication in face-to-face computer-mediated conversations. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 29. Matthew Marge and Alexander Rudnicky. 2011. Towards overcoming miscommunication in situated dialogue by asking questions. In AAAI Fall Symposium: Building Representations of Common Ground with Intelligent Agents. Colin Matheson, Massimo Poesio, and David Traum. 2000. Modelling Grounding and Discourse Obligations Using Update Rules. In 1st Meeting of the North American Chapter of the Association for Computational Linguistics. OpenAI. 2025. Gpt-5. Large Language Model. Kanghee Park, Timothy Zhou, and Loris DAntoni. 2025. Flexible and efficient grammar-constrained decoding. arXiv preprint arXiv:2502.05111. Martin J. Pickering and Simon Garrod. 2004. Toward mechanistic psychology of dialogue. Behavioral and Brain Sciences, 27(02). Massimo Poesio, Florence Bruneseaux, and Laurent Romary. 1999. The mate meta-scheme for coreference in dialogues in multiple languages. In ACL99 Workshop Towards Standards and Tools for Discourse Tagging, pages 6574. Massimo Poesio, Rodolfo Delmonte, Antonella Bristot, Chiran Luminita, and Sara Tonelli. 2004. The venex corpus of anaphora and deixis in spoken and written italian. Massimo Poesio and David R. Traum. 1997. Conversational Actions and Discourse Situations. Computational Intelligence, 13(3):309347. Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. 2016. Modeling context in referring expressions. In European conference on computer vision, pages 6985. Springer. Zheyuan Zhang, Fengyuan Hu, Jayjun Lee, Freda Shi, Parisa Kordjamshidi, Joyce Chai, and Ziqiao Ma. 2024. Do vision-language models represent space and how? evaluating spatial frame of reference under ambiguities. arXiv preprint arXiv:2410.17385. Ayesha Qamar, Jonathan Tong, and Ruihong Huang. 2025. Do llms understand dialogues? case study on dialogue acts. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2621926237. David Reitter and Johanna Moore. 2014. Alignment and task success in spoken dialogue. Journal of Memory and Language, 76:2946. Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927. David Schlangen and Gabriel Skantze. 2011. general, abstract model of incremental dialogue processing. Dialogue & Discourse, 2(1):83111. Michael Schober and Herbert Clark. 1989. Understanding by addressees and overhearers. Cognitive Psychology, 21(2):211232. Takuma Udagawa and Akiko Aizawa. 2019. Natural Language Corpus of Common Grounding under Continuous and Partially-Observable Context. Sebastian Varges. 2005. Spatial descriptions as referring expressions in the maptask domain. In Proceedings of the Tenth European Workshop on Natural Language Generation (ENLG-05). HAE Viethen, Robert Dale, and Markus Guhe. 2011a. Generating subsequent reference in shared visual scenes: Computation vs. re-use. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLG) Edinburgh, Scotland, pages 1158 1167. Unknown Publisher. Henriette Viethen, Robert Dale, and Markus Guhe. 2011b. The Impact of Visual Context on the Content of Referring Expressions. In Proceedings of the 13th European Workshop on Natural Language Generation, pages 4452. Association for Computational Linguistics. Zhengxiang Wang, Weiling Li, Panagiotis Kaliosis, Owen Rambow, and Susan E. Brennan. 2025. LVLMs are Bad at Overhearing Human Referential Communication. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837."
        }
    ],
    "affiliations": [
        "Utrecht University"
    ]
}