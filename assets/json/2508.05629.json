{
    "paper_title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification",
    "authors": [
        "Yongliang Wu",
        "Yizhou Zhou",
        "Zhou Ziheng",
        "Yingzhe Peng",
        "Xinyu Ye",
        "Xinting Hu",
        "Wenbo Zhu",
        "Lu Qi",
        "Ming-Hsuan Yang",
        "Xu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT."
        },
        {
            "title": "Start",
            "content": "Preprint. Working in Progress. ON THE GENERALIZATION OF SFT: REINFORCEMENT LEARNING PERSPECTIVE WITH REWARD RECTIFICATION Yongliang Wu1 Yizhou Zhou2 Zhou Ziheng3 Yingzhe Peng1 Xinyu Ye4 Xinting Hu5 Wenbo Zhu6 Lu Qi7 Ming-Hsuan Yang8 Xu Yang1 1Southeast University 4Shanghai Jiao Tong University 7Wuhan University 6University of California, Berkeley yongliang0223@gmail.com, zyz0205@hotmail.com, xuyang palm@seu.edu.cn 3University of California, Los Angeles 5Nanyang Technological University 2Independent Researcher 8University of California, Merced 5 2 0 2 7 ] . [ 1 9 2 6 5 0 . 8 0 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "We present simple yet theoretically motivated improvement to Supervised FineTuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at https://github.com/yongliang-wu/DFT."
        },
        {
            "title": "INTRODUCTION",
            "content": "Supervised Fine-Tuning (SFT), which involves training model on expert demonstration datasets, has become standard Large Language Model (LLM) post-training approach for adapting models to new tasks or enhancing existing capabilities (Chung et al., 2024; Zhang et al., 2024; Sanh et al., 2022; Ouyang et al., 2022). Its widespread adoption is largely due to its ease of implementation and rapid acquisition of expert-like behaviors (Wei et al., 2022; Zhou et al., 2023). However, despite these advantages, SFT typically suffers from limited generalization compared to reinforcement learning (RL) methods (Chu et al.; Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022; Huan et al., 2025; Swamy et al., 2025). RL leverages explicit reward or verification signals, allowing the model to explore diverse strategies, thereby achieving stronger generalization. Nonetheless, RL approaches often require significant computational resources, are sensitive to hyperparameter tuning, and rely on the availability of reward signals, conditions that are not always feasible in practice (Schulman et al., 2017; Ouyang et al., 2022; Sheng et al., 2025; Strubell et al., 2019; Liu & Yin, 2024; Winsta, 2025). Even when RL is viable, SFT remains advantageous in rapidly acquiring expert behavioral patterns that RL may struggle to discover independently (Mandlekar et al., 2022; Chen et al., 2025b). To leverage the complementary strengths of both approaches, numerous hybrid methods have been developed by incorporating SFT and RL (Ouyang et al., 2022; Sheng et al., 2025; Rafailov et al., 2023; Liu et al., 2025; Qiu et al., 2025). However, can we fundamentally improve SFT itself? This is crucial since SFT is the only viable approach when there are no negative samples in the dataset and no reward or verification model available. In this work, we address this gap by providing mathematical analysis that elucidates the fundamental differences between SFT and RL. We demonstrate that the gradient update of SFT can Equal Contribution. Project Leader. Corresponding Author. 1 Preprint. Working in Progress. be interpreted as special case of policy gradient method with specific, implicitly defined reward structure. Our analysis reveals that this implicit reward is both extremely sparse and inversely proportional to the policys assigned probability of expert actions (see Equation 6 for precise math details). Consequently, this leads to an ill-posed reward structure, especially when the model assigns low probabilities to expert actions; the resulting gradient experiences unbounded variance, creating pathological optimization landscape. Building on this mathematical analysis, we propose Dynamic Fine-Tuning (DFT), principled solution that addresses the root cause of the implicitly ill-posed reward structure. For each token, our method simply rescales the standard SFT objectives with the token probability, effectively neutralizing the inverse probability weighting that leads to an unexpected reward structure and unbounded variance. This theoretically motivated modification fundamentally transforms the gradient estimator from an unstable, biased and probability-dependent mechanism into stable, uniformly-weighted update procedure. The empirical results of our method are highly significant. For example, by fine-tuning the Qwen2.5-Math models (Qwen Team et al., 2024b) on the NuminaMath dataset (LI et al., 2024), the improvement with our approach over the baseline is generally multiple times greater than that of improvement by SFT. Crucially, when standard SFT experiences performance degradation on challenging datasets including Olympiad Bench(International Mathematical Olympiad, 2024), AIME 2024 (American Institute of Mathematics, 2024), and AMC 2023 (Mathematical Association of America, 2023), likely due to overfitting, our method consistently deliver substantial improvements, highlighting its more robust generalization capabilities. The result is validated across model types, sizes, and data sizes (Table 1, Figure 1). In addition, we explore the applicability of our method within RL scenarios (see Table 3, where negative samples or dense reward signals are available (Levine et al., 2020). Our experiments demonstrate that our approach not only significantly outperforms other offline RL methods such as DPO (Rafailov et al., 2023), RFT/RAFT (Dong et al., 2023; Ahn et al., 2024), but also our method performs favorably against online RL methods such as GRPO and PPO for Qwen2.5-Math-1.5B model in math tasks. Note that our method is much simpler in computation resources or procedure than most of the RL methods that may require reference model or certain batch size, offering practical alternative. To understand how DFT affects the model differently, we analyze the change in the probability distribution after training (Figure 2. We observe that traditional SFT training uniformly increases token probabilities toward fitting the training data more tightly, whereas our approach, interestingly, also pushes some token distribution away from the training set. In particular, although most tokens still fit the training set closer, the percentage of less strongly fitted tokens increases markedly. We provide an in-depth discussion of this phenomenon in Section 4.3.1. The contributions of this work are theoretical and practical. On the theoretical side, we mathematically establish LLM SFT as special RL in policy gradient space, pinpoint the underlying reasons for SFTs limited generalization, and derive method to improve it. On the experimental side, we show that such simple solution, just one line of code, can substantially enhance LLM SFTs performance and generalization capabilities across various tasks and models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "The trade-off between SFT and RL is central theme in modern language model alignment. SFT is widely adopted for its simplicity and efficiency in mimicking expert demonstrations (Chung et al., 2024; Zhou et al., 2023; Wei et al., 2022), process analogous to behavioral cloning in robotics (Sammut, 2011; Mandlekar et al., 2022). However, the literature frequently notes that this approach can lead to overfitting and poor generalization compared to RL, which leverages reward signals to explore and discover more robust policies (Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022; Swamy et al., 2025). Recently, Chu et al. conduct systematic comparison of SFT and RL on both textual and visual tasks, confirming that SFT memorizes while RL generalizes. More importantly, they also find that SFT remains necessary as an initialization step to stabilize output formatting before RL training can be effective. Nevertheless, RL comes with significant practical hurdles, including high computational costs, hyperparameter sensitivity, and the need for an explicit 2 Preprint. Working in Progress. reward function, often limit its applicability (Schulman et al., 2017; Strubell et al., 2019; Sheng et al., 2025). To exploit the benefits of both paradigms, dominant line of research has focused on hybrid methods. The most established strategy involves an SFT pre-training phase followed by RL-based refinement, often using learned reward model, as popularized by InstructGPT (Ouyang et al., 2022). More recent approaches have explored alternative combinations, such as interleaving SFT and RL steps to improve stability and performance (Sheng et al., 2025; Liu et al., 2025; Qiu et al., 2025). Other prominent approaches, such as Direct Preference Optimization (DPO) (Rafailov et al., 2023), bypass explicit reward modeling by directly optimizing policy on preference data, effectively integrating imitation and reinforcement signals into single loss function. Chen et al. (2025a) Negativeaware Fine-Tuning (NFT) while enables LLMs to self-improve by modeling their own incorrect generations through an implicit negative policy. Although powerful, these methods are designed for settings where reward signal, preference pairs, or negative samples are available. They augment the training pipeline, but do not fundamentally improve the SFT process in its native context, where there are only positive expert demonstrations. Our work diverges by focusing on enhancing SFT itself, without requiring any external feedback. On the other side, theoretical line of inquiry has sought to unify SFT and RL. For example, Du et al. (2025) reframe RLHF as reward-weighted form of SFT, simplifying the pipeline while maintaining dependence on an explicit reward. Wang et al. (2025) demonstrate that SFT can be viewed as an RL method with an implicit reward, proposing solutions such as smaller learning rates to manage an otherwise vanishing KL constraint. Abdolmaleki et al. (2025) analyze learning from both positive and negative feedback, showing how their balance affects policy convergence. Qin & Springenberg (2025) reframe SFT as lower bound of RL and improve it by introducing importance weighting based on the data-generating policy. Although these works point out the general connection between SFT and RL through the lens of weighting, they stop short of providing precise mathematical equivalence between the SFT gradient and the off-line policy gradient. In contrast, our work is the first to rigorously establish this equivalence, explicitly identifying that the key difference lies in an inverse-probability weighting term present in SFT. This insight directly motivates our proposed solution: simply multiplying the SFT loss with the model probability to cancel out the weighting. Interestingly, our method yields design for the cross-entropy (CE) loss that is diametrically opposite to the well-known Focal Loss (Lin et al., 2017). Our modified CE is log(p), while the focal loss is (1 p)γ log(p). Focal Loss intentionally downweights well-classified samples to improve performance on underrepresented classes, whereas we intentionally downweight poorlyclassified samples to improve generalization. This contrast may reflect fundamental shift in the era of LLMs, where underfitting becomes less problematic than overfitting."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 PRELIMINARIES Supervised Fine-Tuning. Let = {(x, y)} denote corpus of expert demonstrations, where is the full reference response for query x. SFT minimizes the sentence-level cross-entropy: LSFT(θ) = E(x,y)D (cid:2) log πθ (cid:0)y x(cid:1)(cid:3). Its gradient is: θLSFT(θ) = E(x,y)D (cid:2)θ log πθ (cid:0)y x(cid:1)(cid:3). (1) (2) Reinforcement Learning. Let denote response sampled from the policy πθ( x) for query x. Given reward function r(x, y) R, the policy objective is J(θ) = ExDx, yπθ(x) (cid:2)r(x, y)(cid:3). Its policy gradient at the sentence level is θJ(θ) = ExDx, yπθ(x) (cid:2)θ log πθ(y x) r(x, y)(cid:3). (3) (4) 3 Preprint. Working in Progress."
        },
        {
            "title": "3.2 UNIFY SFT–RL GRADIENT EXPRESSION",
            "content": "Rewriting SFT Gradient as Policy Gradient via Importance Sampling. The SFT gradient in Equation 2 is taken under the fixed demonstration distribution. We convert it to an on-policy expectation by inserting an importance weight that compares the expert (Dirac Delta) distribution with the model distribution. (cid:2)θ log πθ (cid:0)y x(cid:1)(cid:3) = ExDx E(x,y)D (cid:0)y x(cid:1)(cid:3) (5) Eyπθ(x) (cid:124) 1[y = y] πθ(y x) (cid:2)θ log πθ (cid:123)(cid:122) resample + reweight (cid:125)"
        },
        {
            "title": "Define the auxiliary variables",
            "content": "w(y x) = 1 πθ(y x) , r(x, y) = 1[y = y], Reorganize the Equation 5 and rewrite it using the above auxiliary variables, we obtain the form θLSFT(θ) = ExDx, yπθ(x) (cid:2)w(y x) θ log πθ(y x) r(x, y)(cid:3). (6) This form of SFT gradient now closely aligns with policy gradient Equation 4. Thus we can see, conventional SFT is precisely an on-policy-gradient with the reward as an indicator function of matching the expert trajectory but biased by an importance weighting 1/πθ. Given the unavoidable sparsity of reward signals in the SFT setting, we identify the importance sampling weight 1/πθ as fundamental cause of SFTs poor generalization relative to RL. When the model assigns low probability to the expert response, the weight becomes large, resulting in unbounded and high-variance reward estimates from an RL perspective. This large variance issue is exacerbated by the extreme sparsity of the reward functionsince r(x, y) = 1[y = y] is nonzero only when the model exactly matches the expert output. As result, optimization tends to overfit to rare exact-match demonstrations, undermining the models ability to generalize beyond the training data. 3.3 PROPOSED METHOD Reward Rectification via Dynamic Reweighting. To neutralize the skewed reward issue identified when viewing SFT under an RL objective, we dynamically reweight the reward by multiplying by corrective inverse ratio given by the policy probability 1/w. The resulting dynamically finetuned gradient is then θLDFT(θ) = θLSFT(θ) sg( 1 ) = θLSFT(θ) sg(πθ(y x)). (7) where sg() denotes the stop-gradient operator, ensuring that gradients do not flow through the reward scaling term w. To facilitate transitioning to later equations, we directly write 1/w to be πθ(y x) instead of πθ(y x) because the indicator function in Equation 5 or Equation 6 would leave all cases where = as 0. Now since the gradient does not flow, the corrected SFT loss also becomes simple reweighted loss, called Dynamic Fine-tuning (DFT). x)(cid:1) log πθ(y LDFT(θ) = E(x,y)D sg (cid:0)πθ(y x) (cid:105) . (8) (cid:104) In practice, however, computing importance weights over the entire trajectory can induce numerical instability. common treatment of this issue is to simply apply importance sampling in token-level, as was adopted in PPO (Schulman et al., 2017). This leads to the final DFT loss version: LDFT(θ) = E(x,y)D (cid:104) (cid:88) t=1 sg (cid:0)πθ(y y <t, x)(cid:1) log πθ(y (cid:105) <t, x) . (9) Note that the reward of this corrected SFT (in RL form), i.e., DFT, now becomes 1 uniformly for all expert trajectory. This is akin to contemporary verification based reward approach RLVR (DeepSeek-AI et al., 2025) that assigns uniform reward to all correct samples. Consequently, it avoids over-concentration on specific low-probability reference tokens, leading to more stable updates and improved generalization without introducing any additional sampling or reward models. 4 Preprint. Working in Progress."
        },
        {
            "title": "4.1 MAIN EXPERIMENT - SFT SETTING",
            "content": "We focs on the standard SFT setting, characterized by having only expert demonstration data without negative samples, reward models, or verification signals. Our expert dataset typically originates from external policies, such as expert models or human annotations. The primary objective of this experiment is to rigorously evaluate whether DFT can robustly surpass standard SFT across diverse tasks, model architectures, model sizes, and dataset sizes."
        },
        {
            "title": "4.1.1 SETUP AND IMPLEMENTATION DETAILS",
            "content": "Dataset and Models. We train with the NuminaMath CoT dataset (LI et al., 2024), comprising around 860,000 mathematical problems paired with the corresponding solutions. The dataset spans various sources, including Chinese high school mathematics exercises and U.S. and international mathematical olympiads. To efficiently manage computational resources, we randomly sample 100,000 instances from the dataset for training, which is sufficient since the evaluation accruacy curve 1 indicates the convergence of all methods well before the dataset exhaustion. We conduct experiments using multiple state-of-the-art models, including Qwen2.5-Math-1.5B, Qwen2.5Math-7B (Qwen Team et al., 2024a), LLaMA-3.2-3B, LLaMA-3.1-8B (Dubey et al., 2024), and DeepSeekMath-7B-Base (Shao et al., 2024). Training Details. Our implementation builds upon the verl framework (Sheng et al., 2025), using recommended SFT hyperparameters. Specifically, we employ the AdamW optimizer with learning rates of 5 105 for all models except the LLaMA-3.1-8B, for which we adopt lower learning rate of 2 105. We set the mini-batch size to 256 and the maximum input length to 2048 tokens. The learning rate follows cosine decay schedule with warm-up ratio of 0.1. We also include concurrent method, Importance-Weighted SFT (iw-SFT) (Qin & Springenberg, 2025), for comparison. All training settings follow those reported in the original paper, except that we set the number of training epochs to 1. Evaluation Settings. For mathematical reasoning tasks, we evaluate on established benchmarks including Math500 (Hendrycks et al.), Minerva Math (Lewkowycz et al., 2022), Olympiad Bench (AI Mathematical Olympiad, 2024), AIME 2024(American Institute of Mathematics, 2024), and AMC 2023(Mathematical Association of America, 2023). Each model uses the default chat template and Chain-of-Thought (CoT) prompting to stimulate step-by-step reasoning. All reported results represent average accuracy across 16 decoding runs, evaluated with temperature of 1.0 and maximum generation length of 4096 tokens. 4.1.2 MAIN RESULTS DFT consistently yields significantly average performance improvements over base models compared to standard SFT across all evaluated LLMs. As shown in Table 1, for example, for Qwen2.5-Math-1.5B, DFT achieves an average gain of +15.66 points over the base model, which is over 5.9 larger than the +2.09 point improvement from SFT. This pattern generalizes across other model families and sizes: LLaMA-3.2-3B benefits from +3.46 point gain with DFT, exceeding the SFT gain (+2.05) by approximately 1.4; LLaMA-3.1-8B achieves +10.02 from DFT, surpassing SFTs +5.33 by 1.88; DeepSeekMath-7B sees +15.51 point improvement via DFT, which is 1.58 larger than SFTs +7.18; and Qwen2.5-Math-7B reaches +15.90 point gain, nearly 3.8 higher than the SFT improvement of +2.37. DFT demonstrates generalization and robustness, especially on challenging benchmarks where standard SFT yields minimal or even negative impact. For instance, on Olympiad Bench, SFT degrades performance for Qwen2.5-Math-1.5B, dropping accuracy from 15.88 to 12.63, while DFT boosts it to 27.08, +11.20 point improvement over base model. On AIME24, SFT reduces accuracy for Qwen2.5-Math-7B by 4.20 points (from 6.68 to 2.48), whereas DFT improves performance to 8.56, achieving +1.88 point gain over the base model despite the difficulty of the benchmark. similar trend is observed on AMC23. SFT reduces the performance of Qwen2.5-Math-1.5B from 19.38 to 18.75, while DFT raises it to 38.13, +18.75 point gain over base. For Qwen2.5-Math5 Preprint. Working in Progress. Table 1: Average@16 accuracy of five state-of-the-art large language models on five mathematical reasoning benchmarks: Math500, Minerva Math, Olympiad Bench, AIME 2024, and AMC 2023. The best performance of each model across benchmarks is bold. Math500 Minerva Math Olympiad Bench AIME24 AMC23 Avg. LLaMA-3.2-3B LLaMA-3.2-3B w/SFT LLaMA-3.2-3B w/DFT LLaMA-3.1-8B LLaMA-3.1-8B w/SFT LLaMA-3.1-8B w/DFT DeepSeekMath-7B DeepSeekMath-7B w/SFT DeepSeekMath-7B w/DFT Qwen2.5-Math-1.5B Qwen2.5-Math-1.5B w/SFT Qwen2.5-Math-1.5B w/DFT Qwen2.5-Math-7B Qwen2.5-Math-7B w/SFT Qwen2.5-Math-7B w/DFT 1.63 8.65 12.79 1.86 16. 27.44 6.15 26.83 41.46 31.66 43. 64.89 40.12 53.96 68.20 1.36 2. 2.84 0.98 5.78 8.26 2.15 7. 16.79 8.51 13.04 20.94 14.39 16. 30.16 1.01 2.06 2.90 0.94 3. 6.94 1.74 6.33 15.00 15.88 12. 27.08 17.12 18.93 33.83 0.41 0. 0.83 0.21 0.00 0.41 0.21 0. 1.24 4.16 1.87 6.87 6.68 2. 8.56 1.56 3.13 3.91 1.01 5. 1.19 3.24 4.65 1.00 6.33 12. 11.02 2.97 8.28 2.64 9.82 16. 18.15 19.38 18.75 38.13 27.96 26. 45.00 15.92 18.01 31.58 21.25 23. 37.15 7B, SFT yields only marginal improvement (+1.86), whereas DFT achieves +17.04 point gain. These results underscore that DFT not only scales more effectively across models of varying capacities, but also exhibits greater resilience on difficult reasoning tasks where traditional SFT struggles. This highlights its potential as more robust fine-tuning paradigm to enhance mathematical reasoning capabilities in LLMs. DFT exhibits better learning efficiency and faster convergence characteristics. Figure 1 reveals clear differences in learning dynamics between DFT and standard SFT on Qwen2.5-Math-1.5B across all math reasoning benchmarks. Compared to SFT, our method demonstrates three distinct advantages: (1) Faster convergence, achieving peak performance within the first 120 training steps on most benchmarks; (2) Better early-stage performance, with DFT already outperforming best final accuracy of SFT within the first 1020 steps; and (3) Higher sample efficiency, consistently requiring fewer updates to reach relatively optimal results. This accelerated convergence indicates that the dynamic reweighting mechanism in DFT leads to more informative gradient updates, guiding the model toward high-quality solutions early in training. It also suggests that DFT helps avoid the optimization plateaus or noise-prone regions often encountered in standard SFT, thereby enabling more efficient acquisition of complex mathematical reasoning patterns. DFT outperforms the concurrent Importance-Weighted SFT (iw-SFT) in most settings across model families and benchmarks. As shown in Table 2, DFT achieves higher average accuracy than iw-SFT on most model families: LLaMA-3.2-3B (+2.39), LLaMA-3.1-8B (+4.15), DeepSeekMath7B (+3.34), and Qwen2.5-Math-1.5B (+1.30). Although iw-SFT slightly outperforms our method on Qwen2.5-Math-7B (+2.45), this improvement is not consistent across datasets. In particular, on the LLaMA model family, iw-SFT exhibits signs of limited robustness. For LLaMA-3.2-3B, iw-SFT underperforms standard SFT on Math500 (5.13 vs. 8.65) and AMC23 (2.03 vs. 3.13). Similarly, for LLaMA-3.1-8B, iw-SFT results in worse performance than SFT on Minerva Math (4.31 vs. 5.78) and AMC23 (7.34 vs. 8.28). These cases demonstrate that iw-SFT might struggle to generalize beyond specific training signals, and may even degrade performance under distribution shifts or on harder benchmarks. In contrast, DFT consistently improves upon both the base model and SFT across nearly all datasets, including those where iw-SFT fails. These results underline better generalization ability of DFT in diverse mathematical reasoning scenarios. Moreover, iw6 Preprint. Working in Progress. Figure 1: Accuracy progression for Qwen2.5-MATH-1.5B across mathematical benchmarks, illustrating faster convergence and better performance achieved by DFT relative to SFT. Table 2: Comparison with concurrent work iw-SFT on math benchmarks. DFT outperforms the iw-SFT in most settings across model families and benchmarks. The best performance is bold. Math500 Minerva Math Olympiad Bench AIME24 AMC23 Avg. LLaMA-3.2-3B w/iw-SFT LLaMA-3.2-3B w/DFT LLaMA-3.1-8B w/iw-SFT LLaMA-3.1-8B w/DFT DeepSeekMath-7B w/iw-SFT DeepSeekMath-7B w/DFT Qwen2.5-Math-1.5B w/iw-SFT Qwen2.5-Math-1.5B w/DFT Qwen2.5-Math-7B w/iw-SFT Qwen2.5-Math-7B w/DFT 5.13 12.79 18.21 27.44 35. 41.46 59.38 64.89 70.28 68.20 2. 2.84 4.31 8.26 8.75 16.79 17. 20.94 25.70 30.16 1.51 2.90 4. 6.94 11.11 15.00 26.82 27.08 34. 33.83 0.00 0.83 0.20 0.41 0. 1.24 8.13 6.87 16.46 8.56 2. 3.91 7.34 2.26 4.65 6.87 12. 11.02 18.28 16.25 40.00 38.13 51. 45.00 14.81 18.15 30.28 31.58 39. 37.15 SFT incurs additional computational overhead by requiring separate reference model to compute importance weights, whereas DFTdynamically derives its own weighting directly from the models token probabilities, resulting in more efficient training procedure. 4.2 EXPLORATORY EXPERIMENT - OFFLINE RL SETTING 4.2.1 SETUP AND IMPLEMENTATION DETAILS Data Preparation. We conduct an exploratory investigation of applying DFT an offline RL setting, where the sparsity of reward issue could be alleviated comparing to SFT setting. Specifically, we adopt the commonly used rejection sampling fine-tuning (RFT) framework Dong et al. (2023); Ahn et al. (2024). Following the setup in Section 4.1, we sample responses for 10,000 math quesPreprint. Working in Progress. Table 3: Evaluation results on five mathematical reasoning benchmarks in an offline reinforcement learning setting using reward signals from rejection sampling. DFT achieves the best overall performance, surpassing both offline (RFT, DPO) and online (PPO, GRPO) baselines, demonstrating its efficiency and strength as simple yet effective fine-tuning strategy. Qwen2.5-Math-1.5B Setting Math500 Minerva Math Olympiad Bench AIME24 AMC23 15.88 19. 31.66 8.51 4.16 Qwen2.5-Math-1.5B w/SFT Qwen2.5-Math-1.5B w/iw-SFT Qwen2.5-Math-1.5B w/DFT SFT SFT SFT Qwen2.5-Math-1.5B w/DPO Qwen2.5-Math-1.5B w/RFT Qwen2.5-Math-1.5B w/PPO Qwen2.5-Math-1.5B w/GRPO Offline Offline Online Online Qwen2.5-Math-1.5B w/iw-SFT Offline Offline Qwen2.5-Math-1.5B w/DFT 43.14 59.38 62.50 46.89 48. 56.10 62.86 60.80 64.71 11.64 17.08 22.94 11.53 14.19 15.41 18.93 18.13 25. 13.41 26.82 26.87 22.86 22.29 26.33 28.62 27.83 30.93 1.03 8.13 7.31 4.58 4. 7.50 8.34 8.33 7.93 14.84 40.00 33.75 30.16 30.78 37.97 41.25 44.21 48. Avg. 15.92 16.81 30.28 30.67 23.20 23.97 28.66 32.00 31.86 35.43 tions using temperature of 1.0 and generate four responses per question from the base model itself. Correct responses are identified using math verify and retained as training data, resulting in approximately 140,000 examples. For DPO training, we construct 100,000 positivenegative preference pairs from the generated responses. Training Details. All experiments are conducted using the Qwen2.5-math-1.5B model. We compare DFT with representative offline RL methods, including DPO (Rafailov et al., 2023) and RFT (Dong et al., 2023; Ahn et al., 2024), as well as online RL methods PPO (Schulman et al., 2017) and GRPO (Shao et al., 2024). For RFT and DFT, the training setup follows the configuration in Section 4.1. For DPO, we use the ms-swift framework (Zhao et al., 2024) with learning rate of 1 106, batch size of 128, and warmup ratio of 0.05. For PPO and GRPO, training is performed using the verl framework (Sheng et al., 2025) with learning rate of 1 106, batch size of 256, and warmup ratio of 0.1. We set the number of response per input to = 4 for GRPO. 4.2.2 RESULTS DFT demonstrates best performance in the offline reinforcement learning setting, outperforming both offline and online RL baselines. As shown in Table 3, DFT achieves an average score of 35.43, exceeding the best offline method RFT by +11.46 points (from 23.97 to 35.43), and even outperforming the strongest online RL algorithm GRPO by +3.43 points (from 32.00 to 35.43). Our model performs well across all five benchmarks. On Math500, DFT scores 64.71, slightly ahead of GRPO (62.86) and better than PPO (56.10) and RFT (48.23). The gains are notable on more challenging benchmarks: on AMC23, DFT achieves 48.44, +7.19 point margin over GRPO and +17.66 point gain over RFT. Similarly, on Minerva Math, DFT reaches 25.16, outperforming GRPO (18.93) by +6.23 points, PPO (15.41) by +9.75, and all offline baselines by wider gap. We also compare against the concurrent iw-SFT (Qin & Springenberg, 2025) method under the offline setting. While iw-SFT performs competitively on certain datasets, achieving 60.80 on Math500 and 44.21 on AMC23, its overall average performance (31.86) still falls short of our method by +3.57 point margin. Moreover, iw-SFT yields only marginal improvements compared to its own performance under the standard SFT setting, achieving an average score of 31.86 in the offline RL setting versus 30.28 with SFT. This modest gain of +1.58 points stands in stark contrast to the improvement achieved by DFT (+4.76, from 30.67 to 35.43). These results suggest that iw-SFT struggles to effectively leverage reward supervision under offline constraints, whereas DFT is able to consistently convert such signals into more robust generalization and higher task performance. These results highlight the strength of DFT as simple yet effective fine-tuning strategy. Despite its lack of iterative reward modeling or environment interaction, it provides stronger learning signal than both offline methods like DPO/RFT and online policy optimization algorithms like PPO/GRPO in certain scale train set. This suggests that DFT can serve as more efficient and scalable alternative Preprint. Working in Progress. Figure 2: Token probability distributions on the training set before training and after fine-tuning with DFT, SFT, and various RL methods including DPO, PPO, and GRPO. logarithmic scale is used on the y-axis to improve visualization clarity. to traditional RL pipelines, particularly in domains where preference supervision is available but reward modeling or online response sampling is expensive or impractical. 4.3 ABLATION AND INVESTIGATION 4.3.1 TOKEN PROBABILITY DISTRIBUTION To understand how the model trained by DFT is different from standard SFT and other RL methods, we look into the token probability distribution of the models output over the training set in Figure 2. The results reveal how the methods alter the probability landscape. SFT tends to uniformly increase token probabilities, shifting the entire distribution towards higher confidence, but mainly targeting the lower and lowest probability tokens. The highest probability token portion barely increases. In stark contrast, DFT exhibits polarizing effect: it significantly boosts the probabilities of subset of tokens while actively suppressing the probabilities of others. This leads to bimodal distribution, with more tokens occupying both the highest and lowest probability bins. Other RL methods such as DPO, GPPO and PPO show the same trend as DFT, although the scale is much milder than it. We look into the words that belong to the lowest probability bin, and find that they are generally the conjuncative words or punctuations such as the, let, ,, . etc. These results suggest that for robust learning, models should not attempt to fit all tokens with uniform confidence. For large language models, it may be beneficial to deprioritize fitting tokens that serve grammatical functions rather than carrying primary semantic content. This concept is analogous to human pedagogy, where students are taught to focus on substantive concepts rather than perfecting the usage of common connective words. 4.3.2 TRAINING HYPER-PARAMETERS ABLATION To assess the robustness and sensitivity of our approach (DFT) with respect to key training hyperparameters, we conduct an ablation study focused on learning rate and batch size, using the Qwen2.5-Math-1.5B base model. This analysis aims to answer two central questions: (1) Is the 9 Preprint. Working in Progress. Figure 3: Ablation study of training hyper-parameters, learning rates and batch size, for DFT and SFT on Qwen2.5-Math-1.5B model. performance gap between DFT and SFT due to suboptimal hyperparameter configuration in SFT? (2) How sensitive are both methods to changes in learning rate and batch size? We evaluate both DFT and SFT across four learning rates: 2e-4, 1e-4, 5e-5, and 1e-5. As shown in Figure 3 (left), both methods exhibit certain degree of sensitivity to the learning rate. DFT consistently outperforms SFT under all configurations, suggesting that the performance gap cannot be attributed solely to suboptimal hyperparameter choices in SFT. For both methods, intermediate learning rates (1e-4 and 5e-5) yield the best results, while both lower (1e-5) and higher (2e-4) values lead to noticeable degradation. These findings highlight the importance of properly tuning the learning rate in gradient-based fine-tuning. We further assess the impact of batch size, sweeping values from 32 to 256. As shown in Figure 3 (right), both DFT and SFT exhibit relatively stable performance across the full range of batch sizes. While minor fluctuations are observed, there is no consistent trend indicating that larger or smaller batches significantly affect final accuracy. This suggests that batch size is not dominant factor for either method in this setup. This suggests that batch size is not dominant factor for either method in this setup, and default values may suffice in practice."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we address the well-documented generalization gap between Supervised Fine-Tuning and Reinforcement Learning. We provide novel theoretical analysis, demonstrating that the standard SFT gradient is equivalent to policy gradient update with an ill-posed implicit reward that is inversely proportional to the models confidence. This insight explains SFTs tendency to overfit and its unstable optimization dynamics. Building on this analysis, we introduce DFT, simple yet powerful method that rectifies this issue by dynamically reweighting the SFT loss with the token probability. This one-line modification stabilizes the learning process and promotes better generalization. Our extensive experiments show that DFT consistently and substantially outperforms standard SFT across various models and challenging mathematical reasoning benchmarks. Furthermore, when adapted to an offline RL setting, DFT surprisingly surpasses established online and offline RL algorithms, highlighting its effectiveness and efficiency. Our work provides both deeper understanding of SFT and practical, high-impact solution that significantly closes the performance gap with more complex RL methods. Limitations. While our experiments demonstrate substantial gains from DFT on mathematical reasoning benchmarks, this evaluation is confined to math-focused datasets and models up to 7 billion parameters. We have not yet assessed performance on other task domains (e.g., code generation, commonsense QA) or with larger LLMs (e.g., 13 B+). Moreover, our current study is limited to text-only scenarios. In future work, we plan not only to extend our study to broader range of text benchmarks and to scale DFT to state-of-the-art models, but also to validate its effectiveness on vision-language tasks to confirm its generality across modalities. 10 Preprint. Working in Progress."
        },
        {
            "title": "REFERENCES",
            "content": "Abbas Abdolmaleki, Bilal Piot, Bobak Shahriari, Jost Tobias Springenberg, Tim Hertweck, Michael Bloesch, Rishabh Joshi, Thomas Lampe, Junhyuk Oh, Nicolas Heess, et al. Learning from negative feedback, or positive feedback or both. In ICLR, 2025. 3 Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pp. 225237, 2024. 2, 7, 8 AI Mathematical Olympiad. Ai mathematical olympiad prize datasets, 2024. URL https:// www.kaggle.com/competitions/ai-mathematical-olympiad-prize. 5 American Institute of Mathematics. Aime 2024 competition mathematical problems, 2024. URL https://www.maa.org/math-competitions/aime. 2, Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova Dasgupta, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Hase, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. 1, 2 Huayu Chen, Kaiwen Zheng, Qinsheng Zhang, Ganqu Cui, Yin Cui, Haotian Ye, Tsung-Yi Lin, Ming-Yu Liu, Jun Zhu, and Haoxiang Wang. Bridging supervised learning and reinforcement learning in math reasoning. arXiv preprint arXiv:2505.18116, 2025a. 3 Zhipeng Chen, Yingqian Min, Beichen Zhang, Jie Chen, Jinhao Jiang, Daixuan Cheng, Wayne Xin Zhao, Zheng Liu, Xu Miao, Yang Lu, Lei Fang, Zhongyuan Wang, and Ji-Rong Wen. An empirical study on eliciting and improving r1-like reasoning models. arXiv preprint arXiv:2503.04548, 2025b. URL https://arxiv.org/abs/2503.04548. 1 Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems, volume 30, 2017. 1, 2 Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. In Forty-second International Conference on Machine Learning. 1, 2 Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 1, DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng 11 Preprint. Working in Progress. Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. 4 Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. Transactions on Machine Learning Research, 2023, 2023. 2, 7, 8 Yilun Du et al. Simplify rlhf as reward-weighted sft: variational method. arXiv preprint arXiv:2502.11026, 2025. URL https://arxiv.org/abs/2502.11026. 3 Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 5 Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). 5 Maggie Huan, Yuetai Li, Tuney Zheng, Xiaoyu Xu, Seungone Kim, Minxin Du, Radha Poovendran, Graham Neubig, and Xiang Yue. Does math reasoning improve general llm capabilities? understanding transferability of llm reasoning. arXiv preprint arXiv:2507.00432, 2025. 1 International Mathematical Olympiad. Mathematical olympiad problems 2024, 2024. URL https://www.imo-official.org. Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. 2 Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. Advances in neural information processing systems, 35:38433857, 2022. 5 Lewis Jia LI, Edward Beeching, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert han Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, Polu. numina-math-models-and-datasets-66f94e8de52a7bfd5af7e28e, 2, 5 Soletskyi, Jiang, Ziju Shen, Ziand Stanislas https://huggingface.co/collections/AI-MO/ 2024. Numinamath. Tunstall, Lipkin, Roman Ben Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pp. 29993007, 2017. 3 Mingyang Liu, Gabriele Farina, and Asuman Ozdaglar. Uft: Unifying supervised and reinforcement fine-tuning. arXiv preprint arXiv:2505.16984, 2025. 1, Vivian Liu and Yiqiao Yin. Green ai: exploring carbon footprints, mitigation strategies, and trade offs in large language model training. Discover Artificial Intelligence, 4(49), 2024. 1 Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li FeiFei, Silvio Savarese, Yuke Zhu, and Roberto Martın-Martın. What matters in learning from offline human demonstrations for robot manipulation. In Conference on Robot Learning, pp. 16781690. PMLR, 2022. 1, 2 12 Preprint. Working in Progress. Mathematical Association of America. Amc 2023 competition problems, 2023. URL https: //www.maa.org/math-competitions. 2, 5 Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35: 2773027744, 2022. 1, 2, 3 Chongli Qin and Jost Tobias Springenberg. Supervised fine tuning on curated data is reinforcement learning (and can be improved). arXiv preprint arXiv:2507.12856, 2025. 3, 5, 8 Haibo Qiu, Xiaohan Lan, Fanfan Liu, Xiaohu Sun, Delian Ruan, Peng Shi, and Lin Ma. Metisrise: Rl incentivizes and sft enhances multimodal reasoning model learning. arXiv preprint arXiv:2506.13056, 2025. URL https://www.arxiv.org/pdf/2506.13056. 1, 3 Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2.5: party of foundation models. arXiv preprint arXiv:2412.15115, 2024a. Qwen Team, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024b. 2 Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, volume 36, 2023. 1, 2, 3, 8 Claude Sammut. Behavioral Cloning. Springer, 2011. 2 Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutton, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. 1 John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 1, 3, 4, Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 5, 8 Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. In Proceedings of the Twentieth European Conference on Computer Systems, pp. 12791297, 2025. 1, 3, 5, 8 Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 36453650, 2019. 1, 3 Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and Andrew Bagnell. All roads lead to likelihood: The value of reinforcement learning in fine-tuning. arXiv preprint arXiv:2503.01067, 2025. 1, 2 Yifan Wang et al. Implicit reward as the bridge: unified view of sft and dpo connections. arXiv preprint arXiv:2507.00018, 2025. URL https://arxiv.org/abs/2507.00018. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. 2022. 1, 2 Jenis Winsta. The hidden costs of ai: review of energy, e-waste, and inequality in model development. arXiv preprint arXiv:2507.09611, 2025. 1 Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2024. 1 13 Preprint. Working in Progress. Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, and Yingda Chen. Swift:a scalable lightweight infrastructure for fine-tuning, 2024. URL https://arxiv.org/abs/2408.05517. 8 Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jianfeng Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. In Advances in Neural Information Processing Systems, volume 36, 2023. 1,"
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "Nanyang Technological University",
        "Shanghai Jiao Tong University",
        "Southeast University",
        "University of California, Berkeley",
        "University of California, Los Angeles",
        "University of California, Merced",
        "Wuhan University"
    ]
}