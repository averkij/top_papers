{
    "paper_title": "WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation",
    "authors": [
        "Zhongyu Yang",
        "Jun Chen",
        "Dannong Xu",
        "Junjie Fei",
        "Xiaoqian Shen",
        "Liangbing Zhao",
        "Chun-Mei Feng",
        "Mohamed Elhoseiny"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Knowledge discovery and collection are intelligence-intensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, a novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose a multi-perspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, a benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipedia-style articles. We show some of our generated examples in https://wikiautogen.github.io/ ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 5 6 0 9 1 . 3 0 5 2 : r WikiAutoGen: Towards Multi-Modal Wikipedia-Style Article Generation Zhongyu Yang1, 2* , Jun Chen1*, Dannong Xu1, 3, Junjie Fei1 Xiaoqian Shen1, Liangbing Zhao1, Chun-Mei Feng4, Mohamed Elhoseiny1 1King Abdullah University of Science and Technology, 2Lanzhou University 3The University of Sydney, 4IHPC, A*STAR {zhongyu.yang, jun.chen, junjie.fei, xiaoqian.shen liangbing.zhao, mohamed.elhoseiny}@kaust.edu.sa daxu8019@uni.sydney.edu.au, fengcm.ai@gmail.com Figure 1. Comparison of existing text-only article generation methods and our proposed WikiAutoGen. Existing approaches [17, 33] rely exclusively on textual sources, often producing inconsistent or inaccurate results. For example, in (a), the target topic is Benzoxonium Chloride, yet the baseline incorrectly generates information about Benzalkonium Chloride. In contrast, our WikiAutoGen framework integrates both visual and textual modalities to generate coherent multimodal content. Additionally, WikiAutoGen employs multiperspective self-reflection mechanism, significantly improving content accuracy and reliability, as illustrated in (b)."
        },
        {
            "title": "Abstract",
            "content": "Knowledge discovery and collection are intelligenceintensive tasks that traditionally require significant human effort to ensure high-quality outputs. Recent research has explored multi-agent frameworks for automating Wikipedia-style article generation by retrieving and synthesizing information from the internet. However, these methods primarily focus on text-only generation, overlooking the importance of multimodal content in enhancing informativeness and engagement. In this work, we introduce WikiAutoGen, novel system for automated multimodal Wikipedia-style article generation. Unlike prior approaches, WikiAutoGen retrieves and integrates relevant images alongside text, enriching both the depth and visual appeal of generated content. To further improve factual accuracy and comprehensiveness, we propose multiperspective self-reflection mechanism, which critically assesses retrieved content from diverse viewpoints to enhance reliability, breadth, and coherence, etc. Additionally, we introduce WikiSeek, benchmark comprising Wikipedia articles with topics paired with both textual and image-based representations, designed to evaluate multimodal knowledge generation on more challenging topics. Experimental results show that WikiAutoGen outperforms previous methods by 8%-29% on our WikiSeek benchmark, producing more accurate, coherent, and visually enriched Wikipediastyle articles. We show some of our generated examples in https://wikiautogen.github.io/ 1. Introduction 1 Equal contribution 2 Work done during an internship at KAUST Knowledge discovery and content generation are essential for organizing and disseminating information, but they remain time-consuming and intelligence-intensive, requiring significant human effort to collect, structure, and verify information. With the advent of large-scale AI models like large language model (LLM) [2, 5, 10, 27, 43], there is growing potential to automate knowledge collection, synthesis, and organization in more efficient and scalable manner [17, 33]. Such automation not only accelerates knowledge discovery but also enhances accessibility, making information more readily available and up to date. Recently, several methods, such as Storm [33] and CoStorm[17], have been proposed to automate Wikipedia-style article generation. While these approaches can produce Wikipedia-like content, they still face key limitations: (1) they are restricted to text-only generation, failing to incorporate multimodal content like relevant images; (2) the generated articles often lack breadth, depth, and reliability, reducing their overall informativeness and credibility. To address these challenges, we introduce WikiAutoGen, multi-agent framework designed to automatically generate high-quality, multimodal Wikipedia-style articles. Unlike prior works, WikiAutoGen can directly search both textual and visual content and generate multi-modal content (see Fig 1), enhancing the informativeness and engagement of generated articles. Additionally, we propose novel multi-perspective self-reflection module, which enables the system to self-regulate, refine, and critically evaluate its generated content. This mechanism enhances the reliability, depth, and breadth of the information by encouraging iterative improvement and multi-source validation. To advance the development and evaluation of multimodal knowledge generation, we introduce WikiSeek, new benchmark designed to tackle challenging topics represented through both images and text. Existing benchmarks primarily focus on text generation or cover only straightforward topics (see Table. 1). In contrast, WikiSeek is multimodal and specifically targets more complex subjects with limited coverage on Wikipedia, making it significantly harder for current methods to retrieve and synthesize comprehensive information. This increases the challenge of content generation, pushing models to explore deeper, enhance their retrieval capabilities, and improve their ability to handle underexplored subjects. Extensive experiments demonstrate that WikiAutoGen significantly outperforms existing methods in generating high-quality textual and visual content. We evaluated text quality across nine key dimensions and image quality across four essential criteria. Experimental results show that WikiAutoGen achieves improvements ranging from 8% to 29% in textual evaluations and from 11% to 14% in image evaluations compared to baseline methods for the input topics of text-only, image-only and image-text. Our contributions can be summarized as follows: We introduce WikiSeek, new multimodal benchmark DATASETS Surfer100 [21] FreshWiki [33] IRP [3] WildSeek [17] WikiSeek (Ours) Dataset Statistics Type Retrieval Modality Difficulty Levels easy easy n/a n/a high Table 1. Comparison of WikiSeek with existing benchmarks. Modalities are indicated by (images). Difficulty levels are categorized based on the average number of characters in corresponding Wikipedia pages: difficult (<500 characters), medium (5002000 characters), and easy (>2000 characters). (text) and designed for evaluating Wikipedia-style article generation, featuring challenging topics with limited existing coverage, represented through both text and images. We propose WikiAutoGen, multimodal article generation framework capable of synthesizing comprehensive content by effectively integrating textual and visual inputs. Evaluations on the WikiSeek benchmark demonstrate that WikiAutoGen achieves significant performance improvements of 8%29% over previous best model on textual generation. We develop novel multi-perspective self-reflection module, enhancing the readability, informativeness, and coherence of generated articles by providing iterative feedback from diverse viewpoints, such as reader, writer, and editor. 2. Realted Work Automatic Expository Writing. LLMs have demonstrated significant practical value in automatic expository writing, generating human-like articles, such as Wikipedia-style surveys [4, 20, 26, 38, 45, 49]. While LLMs excel in traditional Natural Language Generation (NLG) tasks, producing longform, well-structured, coherent, and logically organized text remains persistent challenge [12, 31, 34, 39, 44]. To address this, [37] introduced domain-specific content keywords and progressively refined them into complete passages through multiple stages. Shao et al. [17, 33] highlighted the crucial role of pre-writing strategies, identify them as key factor in writing systems that significantly enhance the quality of generated articles. Recently, some automatic writing systems expanded their knowledge boundaries through mindmaps and tree-based methods [3, 23, 34, 48]. While these iterative content planning methods effectively leverage widely accessible information for common topics, they prove largely ineffective in less-explored or niche domains, which have been overlooked [13, 50]. Meanwhile, existing methods are limited to generating text-based articles and fail to incorporate other modalities, such as visual elements. This inherent con2 straint in data input inevitably leads to information loss and incompleteness, reducing the article readability and making knowledge acquisition more challenging. Conversely, this work integrates visual content and retrieves knowledge across multiple modalities, allowing visual information to complement textual content by capturing details that may otherwise be overlooked. Self-reflection. Recent progress in optimizing LLMs through self-reflection mechanisms is significant. The core is to let models analyze and refine outputs with autogenerated feedback. Existing approaches construct feedback sources from three angles: (1) The LLM conducts iterative self-evaluation [35]; (2) separately trained critic module provides specialized feedback [14]; (3) External knowledge from sources like Wikipedia and browsers is integrated [1, 25]. Notably, REFINER [29] proved that trained critic module can enhance reasoning without finetuning the reasoning module, supporting feedback mechanism optimization. Meanwhile, some methods [9, 55] created feedback mechanism using error-type templates and context-hypothesis mappings. Recent studies [30, 36, 46, 47, 53] focus on LLMs in-context learning. They design prompt templates to help models generate good feedback from historical outputs or patterns. Notably, the multiperspective self-reflection method by [51], which relies on Navigator-Reasoner heuristic interaction, is limited to LLM-based discussion without new knowledge acquisition, leading to low information richness and verifiability. In contrast, our approach enhances multi-perspective selfreflection with multi-web search, addressing complex topic exploration and retrieval challenges while enabling multidimensional control over topic and article quality. Retrieval-Augmented Generation. Retrieval-Augmented Generation(RAG) has shown great potential for knowledgeintensive and information-seeking tasks, which has been widely used to improve the generation quality of language models by incorporating external knowledge [1, 7, 8, 15, 18, 52]. Although RAG has been extensively explored in language domains and its application in multi-modal contexts is advancing, despite the great potential of text-based RAG, it can only handle textual inputs and cannot utilize the rich information in multi-modal data such as images and videos as noted in [4042, 54, 56]. However, the applications of multi-modal RAG (MRAG) remain largely task-specific [22, 24]. Also, there are multimodal RAG approaches, and no comprehensive framework exists for integrating MRAG into automatic expository writing. Moreover, both singleand multi-stage approaches struggle with multi-modal ambiguity, such as uncertain text-image associations, leading to suboptimal responses. 3. WikiSeek benchmark 3.1. Task Definition The task is to automatically generate multi-modal Wikipedia-style article. Given topic represented as text , an image I, or combination of both (T, I), the goal is to generate an article that cites list of references R, incorporating relevant and supporting knowledge. This capability is particularly crucial in domains such as investigative journalism, scientific research, and market analysis, where reliable and well-sourced information is essential. 3.2. WikiSeek Construction key challenge in this task is the lack of an appropriate benchmark that includes multimodal topics. Existing benchmarks primarily focus on textual topics, failing to capture the complexities of multimodal content generation. To bridge this gap, we introduce WikiSeek, new benchmark designed to evaluate more challenging topics that incorporate both text and images. WikiSeek provides rigorous evaluation framework, enabling more comprehensive assessment of multimodal knowledge generation. In the following sections, we outline the construction process of this benchmark in detail. Benchmark construction pipeline. Our WikiSeek benchmark is designed with two key objectives: 1) To evaluate multi-modal article generation, where both the querying topics and output consist of text and images. 2) To focus on more challenging topics that are underexplored on Wikipedia, where limited relevant information is available, making retrieval and synthesis more difficult. We select topics from the WikiWeb2M dataset [6], which comprises approximately 2 million English Wikipedia articles containing both text and images. To identify challenging topics, we focus on articles where the main content has fewer than 500, 300, and 100 characters, categorizing them as hard, very hard, and extremely hard, these more challenging or rare respectively. Typically, topics have minimal coverage on Wikipedia, representing less-documented and lesser-known subjects. Topic filtering and quality control. To curate highincludes both challenging and quality benchmark that meaningful topics, we implement rigorous topic filtering process. First, we retain only Wikipedia articles with fewer than 500 characters, ensuring focus on underexplored and more difficult topics. We then sample hundreds of topics associated with images from the WikiWeb2M dataset [6]. However, some topics, such as 1997 in Japan and .bh, are overly broad or too abstract, making them unsuitable for evaluation. To address this, we manually verify all selected topics and remove those that lack meaningful content or pose evaluation challenges. After this filtering process, we obtain final set of 300 3 Figure 2. Overview of WikiAutoGen, our multimodal framework for Wikipedia-style article generation. The pipeline includes three main stages: (1) an Outline Proposal module that structures the article outline based on the multimodal topic input (image and text); (2) Textual Article Writing module involving persona generation, multi-agent collaborative exploration, and article drafting; and (3) Multimodal Article Writing module that incorporates relevant images through positioning proposals, retrieval, selection, and final polishing. The entire generation process is enhanced by Multi-Perspective Self-Reflection module, leveraging supervisory and agentspecific feedback (writer, reader, editor) to iteratively improve article quality in terms of coherence, readability and engagement, etc topics, evenly distributed across three difficulty levels (hard, very hard, and extremely hard), with 100 topics per level. These topics are represented in one of three formats: textonly, image-only, or combination of text and images. 4. Method Writing high-quality multimodal Wikipedia-style article usually requires multi-agent system that effectively breaks down the process into distinct stages: e.g., proposing an outline, retrieving relevant materials from the internet, and synthesizing the final article. Beyond these stages, maintaining quality necessitates collaboration across multiple roles, ensuring the article is well-structured, accurate, and engaging. To address these challenges, we introduce WikiAutoGen framework. This framework facilitates structured collaboration among specialized agents, enabling comprehensive topic exploration, multi-modal content generation, and coherent content generation. In the following sections, we provide detailed breakdown of WikiAutoGen and its core components. 4.1. WikiAutoGen Framework Our WikiAutoGen framework consists of several key components that work collaboratively to generate high-quality multimodal Wikipedia-style articles, as illustrated in Fig. 2. Each module plays distinct role in the article generation process: 1). Outline Proposal Module. This module converts the given text and image topics into structured outline proposals, laying the foundation for content organization. 2). Textual Article Writing Module. This stage involves multiple sub-components, including persona generator, multi-agent discussion system, and the article generation process, ensuring the content is well-structured and contextually rich. 3). Multi-Perspective Self-Reflection Module. This component evaluates the generated text from multiple viewpoints, including those of writer, reader, and editor, providing constructive feedback to refine and enhance the article. 4). Multimodal Article Writing Module. This final stage integrates visual content, consisting of image positioning proposals, image retrieval, image selection, and multimodal refinement, ensuring cohesive and well-balanced article presentation. These components collectively enable WikiAutoGen to produce high-quality and multi-modal Wikipedia-style articles. 4.2. Outline Proposal Given multimodal topic, the first step is to interpret the input and generate topic-related outlines to guide knowledge exploration and information retrieval. We achieve this by leveraging large language model (LLM) [16] and external search tools. For text-based topics, the LLM analyzes the input, identifies relevant subtopics, and generates structured outline to facilitate knowledge exploration. For image-based topics, we utilize Google Vision Search to retrieve metadata, including descriptions and contextual information. Named entity recognition (NER) [11] is then applied to extract the top 10 most frequent entities as query keywords. These key4 words, along with the original topic, are then fed into the LLM to generate structured outline. In the case of image-text topics, we combine insights from both modalities by extracting subtopics from the text and retrieving image metadata. The LLM then refines the topic and synthesizes cohesive outline, integrating textual and visual information to guide comprehensive knowledge retrieval. 4.3. Textual Article Writing The textual article writing process involves multiple components working together to generate well-structured and informative article. It consists of persona generator, multiagent knowledge exploration, and article generation. Persona generator. Given draft outline, the LLM generates distinct roles relevant to the topic (where is customizable parameter, 1), each acting as an independent agent. The LLM assigns specific objectives to each agent based on their role. These agents are equipped with the ability to connect to search engine, allowing them to retrieve relevant information and contribute to more comprehensive and well-supported article. Multi-agent knowledge exploration. The knowledge exploration stage involves fixed agent, the asker, and LLM-generated agents assigned specific roles. The asker iterates through the outline, posing targeted questions, while the other agents search the internet for relevant information. They then share their findings, discuss them, and refine their understanding. During the discussion, they also interact with the multi-perspective self-reflection module, which provides feedback and improvement advise from writers perspective on reliability, engagement, consistency and informativeness. This iterative process ensures wellrounded knowledge base before proceeding to article generation. Article generation. After gathering web knowledge, the next step is to summarize the collected content and generate the textual article using an LLM-based writing agent. Once the initial draft is produced, the agent iteratively send each generated section to the multi-perspective self-reflection module for feedback and then refine each paragraph. This self-reflection module evaluates the article from writers perspective, providing suggestions to enhance coherence and readability. The writing agent then incorporates these refinements, iteratively improving the text to produce the final article. 4.4. Multi-Perspective Self-Reflection. Writing high-quality Wikipedia-style article requires addressing multiple aspects, including topic consistency, readability, engagement, and informativeness, to provide an optimal reading experience. Therefore, we introduce multiperspective self-reflection module that systematically evaluates and refines content across these dimensions. This module take four distinct viewpoints and assess the article from seven perspectives. Perspectives. Our multi-perspective self-reflection focus on the seven key criteria to improve the paper writing quality. They include reliability, engagement, informativeness, coherence, readability, consistency and helpfulness. We provide detail explanation for them in the Appendix. Supervisor viewpoint. The supervisor assesses whether the generated content fully addresses the questions posed by the asker, evaluates the articles depth, breadth, and coherence, and reviews the effectiveness of the multi-agent discussion. Additionally, it evalutes whether the generated content aligns with the topic and proposed outlines. Based on these criteria, the supervisor provides an evaluation and passes the feedback to the next role. Depending on the specific needs, the next role can be the writer, reader, or editor. Writer viewpoint. From the writers viewpoint, the primary focus is on the multi-agent knowledge exploration and article generation stages. The writer evaluates whether the generated content maintains coherence, ensures engagement, verifies factual accuracy, and upholds logical consistency. Based on these assessments, the writer provides targeted improvement suggestions. For instance, to enhance coherence, it may recommend rearranging sentences or adding transitional words and phrases. To improve readability, it might suggest simplifying complex concepts. Finally, the writer responds with set of refined suggestions. Reader viewpoint. To create high-quality multi-modal article, it is essential to effectively integrate textual content with relevant images to enhance reader engagement and readability. To achieve this, our framework first employs an LLM to propose suitable image placements within the article and generate descriptive content specifying the types of images to include. This initial proposal is then reviewed by the multi-perspective self-reflection module, which evaluates the image positioning and the generated image descriptions from the perspective of readability, engagement, and helpfulness. Based on this assessment, the module provides constructive feedback, ensuring that visual content is effectively integrated to enrich the overall reader experience. Editor viewpoint. After inserting images into the generated article, there may still be discrepancies or inconsistencies between the visual content and the corresponding textual descriptions. To address this, our framework sends the images along with their related text segments to the multiperspective self-reflection module. This module evaluates the alignment and coherence between the images and their accompanying texts from an editorial viewpoint. It provides targeted suggestions, such as refining the image captions, adjusting image placement, or enhancing textual explanations to better reflect visual content. This step ensures improved relevance, coherence, and readability. 5 Methods Content Quality Informativeness Reliability Engagement Average Alignment Consistency Relevance Repetition Breadth Depth Verifiability Engagement Novelty oRAG [1] Storm [33] Co-Storm [17] OmniThink [50] WikiAutoGen (Ours) oRAG Storm Co-Storm OmniThink WikiAutoGen (Ours) oRAG Storm Co-Storm OmniThink WikiAutoGen (Ours) 61.35 72.49 78.05 70.53 81. 50.10 45.80 47.00 43.61 82.57 60.08 67.20 70.15 64.56 85.26 73.96 79.13 84.10 79.67 90.87 72.16 59.60 61.40 58.29 88.75 75.16 75.29 79.29 75.33 90.63 Text as Topic 71.11 69.47 75.40 69.26 83.62 63.30 65.62 68.42 63.55 79.64 Image as Topic 65.47 46.38 47.76 45.67 80.22 42.01 42.69 41.98 38.63 77.24 Image-Text as Topic 72.24 64.33 68.89 63.64 82.11 58.38 61.61 61.90 57.44 79.31 76.04 71.22 75.11 72.41 88.02 50.92 45.99 44.85 43.03 87.20 70.94 66.64 67.31 64.63 88.44 52.42 62.61 67.70 61.21 73. 43.26 39.92 41.83 42.26 74.99 50.57 58.26 61.22 56.38 75.20 45.47 52.41 58.20 48.57 70.69 33.90 34.23 35.03 29.18 68.41 42.47 49.21 52.44 43.04 68.59 57.51 58.80 61.02 57.39 71. 40.66 42.38 41.99 38.31 69.36 55.01 56.25 57.05 54.14 68.79 45.24 55.58 61.61 53.21 69.21 36.91 35.17 37.69 33.57 68.69 43.95 51.27 55.68 48.86 71.06 60.71 65.26 69.96 63.98 78. 48.38 43.57 44.39 41.39 77.49 58.76 61.12 63.77 58.67 78.82 Table 2. Comparison of article generation performance for textual content. We evaluate on textual generation for content quality, informativeness, reliability, and engagement, under three input modalities (Text-only, Image-only, and Image-Text) on our WikiSeek benchmark. 4.5. Multi-modal Article Writing After generating the textual content, the next step is to identify and incorporate relevant images to enhance the articles readability and expressiveness. This multimodal integration process involves several stages: image positioning proposal, image retrieval, image selection, and finally, an article refinement step that seamlessly integrates the images and text. Image positioning proposal. After obtaining the complete textual article, the next step is to utilize an LLM-based agent to suggest appropriate placements and corresponding descriptions for relevant images. These initial proposals are then refined through interaction with the multi-perspective self-reflection module, which evaluates their relevance, coherence, and engagement from the readers perspective. Image retrieval. We then retrieve relevant images by performing searches based on multiple sources, including general image search engines, Wikipedia, and also the websites mentioned in the references. After that, we can obtain list of relevant images. Image selection. We first use the CLIP model [32] to retrieve candidate images by computing their similarity to the query caption, selecting the top-3 most relevant images. Subsequently, we leverage multi-modal model [16] to further evaluate these candidates and select the most suitable image for inclusion in the article. Paper polish. After finalizing images selection, we integrate the chosen images into the article and proceed to polishing stage. Due to potential discrepancies between textual content and visual figures, we employ multi-modal model to revise the entire article, enhancing coherence and consistency across modalities. Additionally, this multimodal model interacts with the multi-perspective self-reflection module, obtaining editorial feedback to further refine the integrated content. 5. Experiment 5.1. Experiment setup"
        },
        {
            "title": "For",
            "content": "Implementation Details. the language model (LM) components of WikiAutoGen, we employ zero-shot prompting implemented using the DSPy framework [19] in conjunction with GPT-4o [16], GPT-4o-mini and GPT-o3mini [28]. Specifically, we use GPT-o3-mini for the multiperspective self-reflection module due to its strong reasoning capabilities, GPT-4o for the multimodal knowledge exploration tasks, and GPT-4o-mini for all other remaining tasks. WikiAutoGen retrieves real-time web information using Serpers API1, with each query returning up to 5 web pages. Throughout the experiments, we maintain consistent settings by fixing the LM temperature at 1.0 and the top value at 0.9. Evaluation metrics. We evaluate the multimodal articles by separately assessing their textual and visual content. - Text quality evaluation. Following prior evaluation frameworks [17, 33], we utilize GPT-4o as the evaluator to assess generated articles across nine criteria grouped into four main aspects: Content quality: alignment, relevance, repetition, and consistency; 1https://serper.dev/ 6 Modules Content Quality Informativeness Reliability Engagement Average Multi-agent Outline proposal Self-reflection Alignment Consistency Relevance Repetition Breadth Depth Verifiability Engagement Novelty 50.63 71.81 79.11 75.91 77.68 82. 62.87 76.38 86.20 84.08 85.57 88.75 53.10 72.67 80.93 82.20 84.73 87.20 51.19 68.25 77.21 75.56 78.49 80.22 48.58 64.36 72.65 73.24 75.08 77.24 44.35 69.16 62.13 66.66 68.85 74.99 43.64 64.20 69.19 65.41 65.59 68. 45.54 57.04 64.45 63.03 65.73 69.36 39.08 55.17 64.11 58.36 66.45 68.69 48.78 66.56 72.88 71.60 73.80 77.49 Table 3. Ablation study. We study the impact of individual modules (Multi-agent knowledge exploration, outline proposal, and selfreflection) on article generation performance for text content. The input modality is image-only. Method oRAG Storm Co-Storm OmniThink WikiAutoGen (Ours) oRAG Storm Co-Storm OmniThink WikiAutoGen (Ours) oRAG Storm Co-Storm OmniThink WikiAutoGen (Ours) 63.61 51.89 54.19 54.93 74.76 57.36 55.20 57.62 58.82 70.12 Coherence Engagement Helpfulness Text as Topic 56.26 45.97 48.64 49.36 66.31 Image as Topic 52.07 43.98 45.61 50.62 61.69 Image-Text as Topic 56.31 50.26 45.65 51.40 70.29 58.039 49.53 51.55 56.13 77. 61.21 52.59 54.32 59.88 71.90 66.38 59.67 54.28 61.76 72.24 62.94 55.78 51.29 57.88 72.11 Info. Sup. Average 51.90 43.97 45.07 47.55 64.78 45.96 41.84 41.56 47.23 63. 49.78 46.78 42.88 49.97 69.29 57.28 49.26 51.38 52.67 68.99 54.32 46.99 48.26 53.47 68.78 58.85 53.12 48.53 55.25 70.98 Table 4. Comparison of article generation performance for image content. We evaluate textual generation for image-text coherence, image engagement, image helpfulness, and information supplement on our WikiSeek benchmark. Informativeness: breadth and depth; Reliability: verifiability; Engagement: engagement and novelty. - Image quality evaluation. As there are no existing benchmarks specifically designed for evaluating multimodal Wikipedia-style article generation, we propose an evaluation method inspired by the textual evaluation frameworks [17, 33] to evluate image quality. Specifically, we assess image quality based on four criteria: image-text coherence, engagement, helpfulness, and information supplement (the images ability to provide additional useful information beyond the textual context). Baselines We compare our approach with four representative LLM-based baseline frameworks for automated expository writing on our WikiSeek benchmark: Outline-driven RAG (oRAG) generates articles guided by outlines produced by Self-RAG [1]. Storm [33] leverages LLM-driven conversations and outlines from diverse perspectives to generate Wikipediastyle content. Co-STORM [17] utilizes collaborative discourse among multiple LLM agents to explore and discover unknown information. OmniThink [50] enhances article quality through iterative expansion and reflection, emulating human slowthinking to increase knowledge density. 7 Since these baselines originally lack multimodal capabilities, we equip them with image retrieval functionalities to enable fair multimodal comparison. Specifically, each baseline can also retrieve image via Google image search, extract relevant metadata, and search for images based on generated textual descriptions. 5.2. Experimental results Textual content comparison on WikiSeek. We demonstrate our results in the Table 2. The results indicate that our approach consistently achieves the highest average scores across all evaluation dimensionscontent quality, informativeness, reliability, and engagementhighlighting its comprehensive effectiveness in multimodal article generation. For text-only inputs, our method achieves an average score of 78.73, significantly outperforming the best baseline (Co-Storm, 69.96) by approximately 8.8 points, demonstrating superior coherence, alignment, and informativeness. In image-only scenarios, the improvement is even more pronounced, with our model obtaining 77.49, surpassing the strongest baseline (oRAG, 48.38) by approximately 29.1 points, reflecting its exceptional capability to extract meaningful textual insights from visual content. For combined image-text topics, our model maintains its advantage with an average score of 78.82, showing clear improvement (15.05 points) over the next-best baseline (Co-Storm, 63.77). Overall, the substantial performance gains indicate that our multimodal approach excels at synthesizing cohesive and engaging content from diverse inputs. Image content comparison on WikiSeek. Table 4 compares the performance of image content across different article generation methods on our WikiSeek benchmark. Our WikiAutoGen consistently achieves the highest scores across all evaluation metrics for all three input modalities. Specifically, our method significantly improves upon baseline methods, outperforming the next-best method by approximately 11.34 points on image-text coherence (72.24 vs. Storms 59.67) under the image-text topics. Similarly, for image-only topics, our approach excels particularly in helpfulness (77.63) and coherence (71.90), demonstrating the models superior capability in selecting images that meaningfully complement textual content and provide additional useful information. Overall, these results underscore ods across all difficulty levels. Notably, as the difficulty increases (from hard to extremely hard), the performance gap between WikiAutoGen and Storm widens from 12.35 points to 15.35 points, and similarly increases compared to other baselines. This demonstrates the robustness and stability of WikiAutoGen in effectively handling highly challenging and underexplored topics. 5.4. Human evaluation To assess the quality of the generated Wikipedia-style articles, we conduct human evaluation study using Amazon Mechanical Turk (AMT)2. We randomly sample 100 textbased topics from the WikiSeek dataset and perform pairwise comparisons between articles generated by our method (WikiAutoGen), Storm, and OmniThink. For each topic, three independent participants evaluate the articles in randomized order. Participants first answer the question: Do you think adding images would improve comprehension of the topic? Results shown in Fig. 4 (left) indicate that 99% of participants agree images enhance comprehension. Additionally, participants answer multiple-choice questions including: (1) Which article is the easiest to understand? (2) Which article is the most engaging in terms of narrative, examples, or overall presentation? (3) Which article provides the most comprehensive background information and in-depth analysis? (4) Which article is your overall favorite? The order of these questions is randomized to reduce bias. As illustrated in Fig. 4 (right), participants consistently prefer articles generated by WikiAutoGen over Storm and OmniThink across all evaluation criteria. Figure 4. Human evaluation study. We randomly sample textonly topics and conduct comparative evaluations between WikiAutoGen, Storm, and OmniThink. Left: Participants respond to the question, Do you think adding images would improve comprehension of the topic?. Right: Participants answer multiplechoice questions evaluating readability, engagement, informativeness, and overall preference. 2https://www.mturk.com/ 8 Figure 3. Ablation study across different data difficulty levels. We compare our WikiAutoGen with Storm and OmniThink on three difficulty categories: hard (300500 characters), very hard (100300 characters), and extremely hard (<100 characters). our methods effectiveness in seamlessly integrating images to enhance multimodal coherence, engagement, and informativeness, substantially advancing the quality and reader experience of multimodal articles. 5.3. Ablation studies Ablation on different components of WikiAutoGen. We conduct an ablation study to analyze the individual contributions of three key components on textual article generation from image-only topics. The components include multi-agent knowledge exploration, outline proposal, and multi-perspective self-reflection. Results are shown in Table 3. Specifically, without the outline proposal, the system simply retrieves single image and extracts its description from metadata. Without multi-agent knowledge exploration, only single fixed agent responds to the asker. Introducing the multi-agent module significantly improves average performance from 48.78 (no modules enabled) to 66.56 (+17.78 points), highlighting its effectiveness in collaborative knowledge exploration. Using the outline proposal alone boosts performance further to 72.88 (+24.10 points), underscoring its importance for content structuring. The self-reflection module individually achieves 71.60 (+22.82 points), indicating its strength in refining coherence and consistency. Combining all three modules results in the highest performance (77.49), demonstrating their complementary roles in generating coherent, informative, and engaging articles from image-based inputs. Ablation on different difficulty levels. In our WikiSeek benchmark, topics are grouped into three difficulty levels based on article length (character count): hard (300500 characters), very hard (100300 characters), and extremely hard (less than 100 characters), with 100 examples per level. We evaluate text-only topics and report the average text evaluation performance in Fig. 3. The results indicate that our WikiAutoGen consistently outperforms baseline meth6. Conclusion In this paper, we introduced WikiAutoGen, comprehensive multi-agent framework designed for automated multimodal Wikipedia-style article generation. Our approach integrates both visual and textual content, significantly enhancing the depth, informativeness, and engagement of generated articles. To address limitations in existing methods, we proposed novel multi-perspective self-reflection mechanism, ensuring improved coherence, reliability, and overall article quality. Furthermore, we presented WikiSeek, challenging multimodal benchmark specifically crafted to evaluate the performance of models in generating content for less-explored topics. Experimental results demonstrated that WikiAutoGen substantially outperforms existing stateof-the-art methods across textual and visual evaluation metrics. Particularly notable were the improvements in content quality, informativeness, and reader engagement, validating the effectiveness of integrating multimodal inputs and iterative self-reflection."
        },
        {
            "title": "References",
            "content": "[1] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. ArXiv, abs/2310.11511, 2023. 3, 6, 7 [2] Jinze Bai et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Accessed: 2025-03-04. 2 [3] Nishant Balepur, Jie Huang, and Kevin Chen-Chuan Chang. Imitate, retrieve, paraphrase. Expository text generation: ArXiv, abs/2305.03276, 2023. [4] Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwasniewski, Jurgen Muller, Lukas Gianinazzi, Aleˇs Kubˇcek, Hubert Niewiadomski, Aidan OMahony, Onur Mutlu, and Torsten Hoefler. Demystifying chains, trees, and graphs of thoughts. In ArXiv, 2024. 2 [5] DeepSeek-AI: Xiao Bi et al. Deepseek llm: Scaling opensource language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Accessed: 2025-03-04. 2 [6] Andrea Burns, Krishna Srinivasan, Joshua Ainslie, Geoff Brown, Bryan A. Plummer, Kate Saenko, Jianmo Ni, and Mandy Guo. Wikiweb2m: page-level multimodal wikipedia dataset. ArXiv, abs/2305.05432, 2023. 3 [7] Jun Chen, Dannong Xu, Junjie Fei, Chun-Mei Feng, and Mohamed Elhoseiny. Document haystacks: Visionlanguage reasoning over piles of 1000+ documents. ArXiv, abs/2411.16740, 2024. 3 [8] Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. ArXiv, abs/2210.02928, 2022. 3 [9] Steven Coyne. Template-guided grammatical error feedback In Proceedings of the 17th Confercomment generation. ence of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 94104, Dubrovnik, Croatia, 2023. Association for Computational Linguistics. 3 [10] Google DeepMind. Gemini: Multimodal language model. https : / / deepmind . google / technologies / gemini/, 2023. Accessed: 2025-03-04. 2 [11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. 4 [12] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. BAMBOO: comprehensive benchmark for evaluating long text modeling capacities of large language models. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 20862099, Torino, Italia, 2024. ELRA and ICCL. 2 [13] Fan Gao, Hang Jiang, Rui Yang, Qingcheng Zeng, Jinghui Lu, Moritz Blum, Dairui Liu, Tianwei She, Yuang Jiang, and Irene Li. Evaluating large language models on wikipediastyle survey generation. In Annual Meeting of the Association for Computational Linguistics, 2023. 2 [14] Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv, abs/2305.11738, 2023. [15] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. 3 [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 4, 6 [17] Yucheng Jiang, Yijia Shao, Dekun Ma, Sina J. Semnani, and Monica S. Lam. Into the unknown unknowns: Engaged human learning through participation in language model agent conversations. In Conference on Empirical Methods in Natural Language Processing, 2024. 1, 2, 6, 7 [18] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, Online, 2020. Association for Computational Linguistics. 3 [19] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei Zaharia, and Christopher Potts. DSPy: Compiling declarative language model calls into selfIn Proceedings of the Twelfth Interimproving pipelines. national Conference on Learning Representations (ICLR), 2024. 6 [20] Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan Rossi, Franck Dernoncourt, Hanieh Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. Longlamp: benchmark for personalized long-form text generation. arXiv preprint arXiv:2407.11016, 2024. 2 [21] Irene Li, Alexander R. Fabbri, Rina Kawamura, Yixin Liu, Xiangru Tang, Jaesung Tae, Chang Shen, Sally Ma, Tomoe Mizutani, and Dragomir R. Radev. Surfer100: Generating In Internasurveys from web resources, wikipedia-style. tional Conference on Language Resources and Evaluation, 2021. 2 [22] Yangning Li, Yinghui Li, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinran Zheng, Hui Wang, Hai-Tao Zheng, Pengjun Xie, Philip S. Yu, Fei Huang, and Jingren Zhou. Benchmarking multimodal retrieval augmented generation with dynamic vqa dataset and self-adaptive planning agent. ArXiv, abs/2411.02937, 2024. 3 [23] Yi Liang, You Wu, Honglei Zhuang, Li Chen, Jiaming Shen, Yiling Jia, Zhen Qin, Sumit K. Sanghai, Xuanhui Integrating Wang, Carl Yang, and Michael Bendersky. planning into single-turn long-form text generation. ArXiv, abs/2410.06203, 2024. 2 [24] Cui Long, Yongbin Liu, Chunping Ouyang, and Ying Yu. Bailicai: domain-optimized retrieval-augmented ArXiv, generation framework for medical applications. abs/2407.21055, 2024. 3 [25] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback. ArXiv, abs/2303.17651, 2023. 3 [26] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wentau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore, 2023. Association for Computational Linguistics. [27] OpenAI. Chatgpt: Optimizing language models for dialogue. https://openai.com/blog/chatgpt, 2022. Accessed: 2025-03-04. 2 [28] OpenAI. Openai o3-mini, 2025. 6 [29] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. In Conference of the European Chapter of the Association for Computational Linguistics, 2023. 3 [30] Alexandre Piche, Aristides Milios, Dzmitry Bahdanau, and Chris Pal. Llms can learn self-restraint through iterative selfreflection. ArXiv, abs/2405.13022, 2024. 3 [31] Haoran Que, Feiyu Duan, Liqun He, Yutao Mou, Wangchunshu Zhou, Jiaheng Liu, Wenge Rong, Zekun Moore Wang, Jian Yang, Ge Zhang, et al. Hellobench: Evaluating long text generation capabilities of large language models. arXiv preprint arXiv:2409.16191, 2024. 2 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML), pages 87488763. PMLR, 2021. 6 [33] Yijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and Monica S. Lam. Assisting in writing wikipedia-like articles from scratch with large language models. In North American Chapter of the Association for Computational Linguistics, 2024. 1, 2, 6, [34] Zejiang Shen, Tal August, Pao Siangliulue, Kyle Lo, Jonathan Bragg, Jeff Hammerbacher, Doug Downey, and Joseph Chee Chang. Beyond summarization: Designing ai support for real-world expository writing tasks. ArXiv, abs/2304.02623, 2023. 2 [35] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning. In Neural Information Processing Systems, 2023. 3 [36] Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment of language models from scratch with minimal human supervision. ArXiv, abs/2305.03047, 2023. 3 [37] Bowen Tan, Zichao Yang, Maruan Al-Shedivat, Eric Xing, and Zhiting Hu. Progressive generation of long text with pretrained language models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 43134324, Online, 2021. Association for Computational Linguistics. 2 [38] Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, et al. Proxyqa: An alternative framework for evaluating long-form text generation with large language models. arXiv preprint arXiv:2401.15042, 2024. 2 [39] Chen Tang, Hongbo Zhang, Tyler Loakman, Chenghua Lin, and Frank Guerin. Enhancing dialogue generation via dynamic graph knowledge aggregation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46044616, Toronto, Canada, 2023. Association for Computational Linguistics. [40] Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, and Ji rong Wen. Learning to imagine: Visuallyaugmented natural language generation. In Annual Meeting of the Association for Computational Linguistics, 2023. 3 [41] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. ArXiv, abs/2405.09818, 2024. [42] Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, and Jifeng Dai. Mminterleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer. ArXiv, abs/2401.10208, 2024. 3 [43] Hugo Touvron et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [44] Qiyao Wang, Shiwen Ni, Huaren Liu, Shule Lu, Guhong Chen, Xi Feng, Chi Wei, Qiang Qu, Hamid AlinejadRokny, Yuan Lin, and Min Yang. Autopatent: multiagent framework for automatic patent generation. ArXiv, abs/2412.09796, 2024. 2 10 [45] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, and Yue Zhang. Autosurvey: Large language models can automatically write surveys. ArXiv, abs/2406.10252, 2024. 2 [46] Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang. theoretical understanding of self-correction through in-context alignment. ArXiv, abs/2405.18634, 2024. 3 [47] Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, and Min Zhang. Taste: Teaching large language models to translate through self-reflection. In Annual Meeting of the Association for Computational Linguistics, 2024. 3 [48] Yilin Wen, Zifeng Wang, and Jimeng Sun. Mindmap: Knowledge graph prompting sparks graph of thoughts in large language models. In Annual Meeting of the Association for Computational Linguistics, 2023. 2 [49] Yuhao Wu, Ming Shan Hee, Zhiqing Hu, and Roy KaSpinning the golden thread: Benchmarking Wei Lee. long-form generation in language models. arXiv preprint arXiv:2409.02076, 2024. [50] Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Ningyu Zhang, Jiang Yong, Pengjun Xie, Fei Huang, and Huajun Chen. Omnithink: Expanding knowledge boundaries in machine writing through thinking. ArXiv, abs/2501.09751, 2025. 2, 6, 7 [51] Hanqi Yan, Qinglin Zhu, Xinyu Wang, Lin Gui, and Yulan He. Mirror: multiple-perspective self-reflection method for knowledge-rich reasoning. ArXiv, abs/2402.14963, 2024. 3 [52] Yibin Yan and Weidi Xie. Echosight: Advancing visualIn Conference on language models with wiki knowledge. Empirical Methods in Natural Language Processing, 2024. 3 [53] Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, and Uri Alon. In-context principle learning from mistakes. ArXiv, abs/2402.05403, 2024. 3 [54] Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt5: Interleaved vision-and-language generation via generative vokens. ArXiv, abs/2310.02239, 2023. 3 [55] Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan. Hypothesis generation with large language models. ArXiv, abs/2404.04326, 2024. [56] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. ArXiv, abs/2304.10592, 2023."
        }
    ],
    "affiliations": [
        "IHPC, A*STAR",
        "King Abdullah University of Science and Technology",
        "Lanzhou University",
        "The University of Sydney"
    ]
}