{
    "paper_title": "TextAtlas5M: A Large-scale Dataset for Dense Text Image Generation",
    "authors": [
        "Alex Jinpeng Wang",
        "Dongxing Mao",
        "Jiawei Zhang",
        "Weiming Han",
        "Zhuobai Dong",
        "Linjie Li",
        "Yiqi Lin",
        "Zhengyuan Yang",
        "Libo Qin",
        "Fuwei Zhang",
        "Lijuan Wang",
        "Min Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains a persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, a novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million long-text generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on long-text image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as a valuable dataset for training and evaluating future-generation text-conditioned image generation models."
        },
        {
            "title": "Start",
            "content": "TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Alex Jinpeng Wang 1 Dongxing Mao 1 Jiawei Zhang 2 Weiming Han 2 Zhuobai Dong 1 Linjie Li 3 Yiqi Lin 4 Zhengyuan Yang 3 Libo Qin 1 Fuwei Zhang 2 Lijuan Wang 3 Min Li 1 https://textatlas5m.github.io 5 2 0 2 1 1 ] . [ 1 0 7 8 7 0 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Text-conditioned image generation has gained significant attention in recent years and are processing increasingly longer and comprehensive text prompt. In everyday life, dense and intricate text appears in contexts like advertisements, infographics, and signage, where the integration of both text and visuals is essential for conveying complex information. However, despite these advances, the generation of images containing long-form text remains persistent challenge, largely due to the limitations of existing datasets, which often focus on shorter and simpler text. To address this gap, we introduce TextAtlas5M, novel dataset specifically designed to evaluate long-text rendering in text-conditioned image generation. Our dataset consists of 5 million longtext generated and collected images across diverse data types, enabling comprehensive evaluation of large-scale generative models on longtext image generation. We further curate 3000 human-improved test set TextAtlasEval across 3 data domains, establishing one of the most extensive benchmarks for text-conditioned generation. Evaluations suggest that the TextAtlasEval benchmarks present significant challenges even for the most advanced proprietary models (e.g. GPT4o with DallE-3), while their open-source counterparts show an even larger performance gap. These evidences position TextAtlas5M as valuable dataset for training and evaluating future-generation text-conditioned image generation models. Datasets are released. 1. Introduction Text-conditioned image generation is processing inputs of longer texts, with growth in processing inputs of longer contexts from 77 text tokens in Dall-E (Ramesh et al., 2021a) to 300 text tokens in PixArt-Î± (Chen et al., 2023c), and 1Central South University 2North University of China 3Microsoft 4National University of Singapore. Figure 1: comparison of TextAtlas5M with previous text-rich image datasets (Schuhmann et al., 2022; Tuo et al., 2023; Chen et al., 2024b). Unlike prior datasets with short or unstructured alt-text, TextAtlas5M features more diverse, complex long-form text. recently achieving 2,000-token capacity in autoregressive architectures (Team, 2024). In this regard, the task of generating comprehensive and controllable images with longer text input, such as image with complex layout or dense text, is considered promising testbed. Text plays central role in image generation, serving as one of the most pervasive elements in visual communication through news, books, advertisements, and more. For instance, over 50% of images in large-scale image generation datasets like LAION-2B (Schuhmann et al., 2022) contain text detectable by OCR models (Lin et al., 2025). However, despite the increasing prevalence of dense text in real-world scenarios, state-of-the-art models such as SDXL3 (Esser et al., 2024), DallE-3 (Betker et al., 2023) and Chameleon (Team, 2024) struggle with tasks requiring the generation of dense text (see Appendix for examples). This limitation stems from the reliance on existing text-rich image datasets like Marion10M and AnyWords3M (Tuo et al., 2023; Chen et al., 2023a), which focus on short and simple text, failing to meet the demand for handling longer and more intricate inputs. Such limitations are particularly evident in practical scenariosranging from advertising TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 1: Dataset Comparison with Existing Text-Rich Image Generation Datasets. The last two columns detail the sources of automatically generated labels, while the final column presents the average text token length derived from OCR applied to the images. Dataset Name TextCaps (Sidorov et al., 2020) SynthText (Gupta et al., 2016) Marion10M (Chen et al., 2024b) AnyWords3M (Tuo et al., 2023) RenderedText (Wendler) TextAtlas5M Samples 28K 0.8M 10M 3M 12M 5M Annotations Caption OCR Caption+OCR Caption+OCR Text Caption/OCR"
        },
        {
            "title": "Labels\nHuman\nAuto\nAuto\nAuto\nAuto",
            "content": "Real&Synthetic Image Auto/Human Token Length 26.36 13.75 16.13 9.92 21.21 148.82 layouts that require seamless brand messaging integration to infographics that demand precise synchronization of text and visuals, as shown by the Notice Board in Figure 1. To address these challenges, we introduce TextAtlas5M, comprehensive dataset designed to advance and evaluate text-to-image generation models, with particular focus on generating dense-text images. As illustrated in Figure 1 and Table 1, TextAtlas5M stands out in several key ways compared to previous text-rich datasets. Unlike earlier datasets, which primarily focus on short and simple text, TextAtlas5M includes diverse and complex range of data. It spans from interleaved documents and synthetic data to realworld images containing dense text, offering more varied and challenging set of examples. Moreover, our dataset features longer text captions, which pose additional challenges for models, and includes human annotations for particularly difficult examples, ensuring more thorough evaluation of model capabilities. The synthetic subset progresses through three levels of complexity, starting with simple text on clean backgrounds. It then advances to interleaved data, blending text with visual elements, and culminates in synthetic natural images, where realistic scenes integrate seamlessly with text. The real image subset captures diverse, real-world densetext scenarios. It includes filtered samples from datasets like AnyText (Tuo et al., 2023) and TextDiffuser (Chen et al., 2024b), detailed descriptions from PowerPoint slides, book covers, and academic PDF papers. To enrich diversity, we also gather dense-text images guided by predefined topics from CommonCrawl1 and LAION-5B (Schuhmann et al., 2022). To assess the capability of model in dense text image generation, we introduce dedicated test set, TextAtlasEval, designed for comprehensive evaluation. This test set spans four distinct data types, ensuring diversity across domains and enhancing the relevance of TextAtlas5M for real-world applications. Our contributions are as follows: 1https://commoncrawl.org/ Introduction of TextAtlas5M: We present TextAtlas5M, large-scale dataset for text-conditioned image generation, focusing on text-rich images. The dataset includes three levels of synthetic data and diverse collection of real images, curated from widely relevant real-world topics. Long-Text Evaluation Benchmark: We design dedicated test set to address the longstanding gap in metrics for evaluating long-text information in image generation. By requiring models to effectively process and generate longer text, TextAtlas5M sets itself apart from existing text rendering benchmarks. Comprehensive Model Evaluation: We thoroughly evaluate proprietary and open-source models to assess their long-text generation capabilities. The results reveal the significant challenges posed by TextAtlas5M and provide valuable insights into the limitations of current models, offering key directions for advancing text-rich image generation in future research. 2. Related Works Text-conditioned Image Synthesis. Recent advances in generative modeling have prominently featured diffusionbased (Song & Ermon, 2019; Ho et al., 2020; Ho & Salimans, 2022) and autoregressive-based (Ramesh et al., 2021b; Esser et al., 2021) frameworks. Diffusion models, such as DALL-E (Ramesh et al., 2021a) and Parti (Yu et al., 2022), produce high-fidelity outputs through an iterative refinement process but are limited by slow inference speeds. While, autoregressive models (Lu et al., 2024; Sun et al., 2023; Zhan et al., 2024; Sun et al., 2024), model images as sequential token streams by leveraging vector quantization (Van Den Oord et al., 2017) to discretize raw pixel data into tokens, which balances efficiency and sample quality, making autoregressive modeling increasingly popular. Despite significant progress, current methods still face challenges in generating dense, stylized text within images while maintaining high precision and aesthetic coherence. Our 2 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 2: Examples of Subsets in TextAtlas5M. The dataset includes Synthetic Data and Real Data splits. Synthetic data consists of three progressively complex subsets, while real data is gathered from diverse sources. approach bridges this gap by building carefully curated, text-rich dataset to enhance the accuracy and stylistic variety, even when conditioned on complex, lengthy prompts. Text-Image Pair Datasets for Generation. MS-COCO (Lin et al., 2014) and TextCaps (Sidorov et al., 2020) are widely used image-text pair benchmarks. MSCOCO features descriptive annotations and TextCaps adds more contextually rich captions. Recently, CC3M (Changpinyo et al., 2021) and LAION (Schuhmann et al., 2022) further emphasize large-scale data sourced from the Web, which have been instrumental in training text-conditioned image generation models. However, both primarily cater to short or moderately long captions, limiting their suitability for tasks involving lengthy textual content. More recent efforts, Marion10M (Chen et al., 2024b) and AnyWords3M (Tuo et al., 2023) aim to diversify text inputs but often lack high-quality annotations or precise alignment, prioritizing visual scenes over accurate textual rendering. To bridge these gaps, we introduce long text rendering dataset TextAtlas5M explicitly designed for generating images from extensive and structured text. To the best of our knowledge, this is the first large-scale dataset of its kind, addressing the limitations of existing resources and enabling advancements in long text-to-image generation tasks. Visual Text Rendering. Rendering text accurately in images requires balancing textural correctness, visual quality, and contextual coherence. Prior work in text image synthesis is broadly categorized into two directions: structured methods (Tuo et al., 2023; Yang et al., 2024; Ma et al., 2023; Chen et al., 2024b; 2023b; Liu et al., 2022), which enforce layout guidelines to achieve precise text placement for design-oriented tasks (e.g., posters), and unconstrained approaches (Chen et al., 2024c) that prioritize flexibility for long-text generation (e.g., documents) without extra guidelines. Despite their strengths, both approaches struggle to render extended text sequences in diverse and precise way. These challenges are largely due to the absence of highquality, large-scale dense-text image datasets. To close this gap, we introduce diverse and comprehensive text-rich dataset to enable more precise and flexible text rendering. 3. Dataset Construction The primary goal of our TextAtlas5M is to collect diverse scenes in daily life containing dense text. However, acquiring high-quality real world text-rich data is both expensive and time-consuming. To balance quality and scalability, we first construct Synthetic Image Split with widely-used topics, providing easier cases for model training and evaluation. Further, we collect Real Image Split from diverse sources including PowerPoint presentations, documents, existing long sequence data, and visually appealing real-world images. By combining these tiers, TextAtlas5M provides comprehensive and scalable resource for dense text rendering. 3.1. Synthetic Images Split CleanTextSynth: We create simple dataset of text-only images, without incorporating additional visual elements, using the interleaved Obelics (LaurenÃ§on et al., 2024), in 3 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation relevant text, which is rendered into designated regions, producing fully annotated images aligned with each topic. See Appendix A.1 for details on prompting and rendering. 3.2. Real Images Split PPT2Details. We first consider PowerPoint presentations, widely used and text-rich format. SlideShare1M (Araujo & Girod, 2016) is dataset containing 1 million PowerPoint slides in an interleaved format, with most slides featuring dense text. To annotate this dataset, we utilize QwenVL (Wang et al., 2024). Each slide is first converted into an image, and the model is then used to generate detailed descriptions of the text, images, and other elements, such as diagrams, tables, and vectors from this image. To ensure high-quality annotations, we filter out slides without text and those of low quality. After this process, we retain total of 0.3 million high-quality samples, providing rich resource for further analysis and modeling. PPT2Structed. In addition to PPT2Details, we further access detailed slide elements with bounding box annotations for high-quality PowerPoint presentations. The AutoSlideGen dataset (Sefid et al., 2021) comprises 5,000 slides derived from scientific papers, where presentations are crafted to effectively convey research innovations. To build this dataset, we process each slide using PyMuPDF to extract element bounding boxes and their corresponding text content. For slides containing images, we leverage the Qwen2-VL (Wang et al., 2024) model to generate descriptive captions. Text elements are preserved in their raw form to maintain accuracy and context. This process produces structured dataset of 96,000 annotated samples, providing detailed elements along with their positional information. Paper2Text: Another prominent text-rich scene is PDF documents, such as those found in Arxiv papers. Using the Arxiv Paper dataset (arXiv.org submitters, 2024), we process each page by extracting its content with PyMuPDF (Inc., 2025). For this subset, we focus primarily on text information. Specifically, we retain attributes such as font color, size, and type. This approach enables the creation of detailed annotation containing comprehensive descriptions of the text elements on each page. CoverBook: We utilize the Cover Book dataset (Iwana et al., 2016), sourced from Amazon and Inc. marketplaces. This dataset comprises 207,572 books spanning 32 diverse categories, with each book providing cover image, title, author, and category information. To create rich captions, we concatenate the title, author, category, and year information for each book, resulting in detailed textual descriptions that enhance the datasets usability for various tasks. LongWordsSubset. straightforward approach to obtain Figure 3: StyledTextSynth Generation Pipeline. An LLM generates diverse text-rich topics, and text-to-image model creates images with refined empty regions, where the text is seamlessly integrated into the specified scene. which we randomly sample text sequences of length L. Using OpenCV 2 for text rendering, we place sequences on white canvases with variations in font (e.g., Helvetica, Times New Roman), size (1248pt), color (RGB hex), and rotation (45). This results in 2 million samples of clean, wellformatted text, ideal for foundational experiments. TextVisionBlend: Crafting Context-Rich Interleaved Data. Interleaved data seamlessly blends visual and textual elements in formats like blogs, wikis, and newspapers. Inspired by this, we created synthetic interleaved image-text dataset that enhances data organization and contextual richness. We source high-quality image-text pairs from Obelics (LaurenÃ§on et al., 2024) and WIT (Srinivasan et al., 2021), then design random layouts to automatically combine them. Using PyMuPDF (Inc., 2025), we generate white-background images and parseable PDFs, ensuring structured interleaved content. From these PDFs, we extract detailed annotations, including bounding boxes, font styles, and sizes, enriching each sample. To enhance contextual richness, we used vision-language models Qwen2-VL (Wang et al., 2024) and BLIP (Li et al., 2022) to generate image captions, consolidating all annotations and captions into comprehensive sample files. This dataset captures the complexity of interleaved data, serving as valuable resource for research and applications. See Appendix A.2 for details. StyledTextSynth: Building on pure-text images and interleaved text-image scenes, we address more complex embedded text scenarios, such as billboards, to enhance dataset diversity. The overall pipeline is shown in Figure 3. Using GPT4o (OpenAI, 2024) as world simulator, we identify 50 real-world text-integration scenes, refining them into 18 high-frequency topics (e.g., urban signage, product packaging). GPT4o generates scene descriptions, which serve as prompts for SD3.5 to create text-free images. We then identify suitable text placement areas, refining them with human annotations as needed. Next, LLMs like GPT4o and Llama3.1 (Dubey et al., 2024) generate contextually 2https://opencv.org 4 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 4: TextScensHQ Generation Pipeline. The data is filtered using OCR results and manual annotation with correcting inconsistencies from vision-language models. long-text samples is to filter existing text-rich image datasets. For this purpose, we use two widely adopted text rendering benchmarks: AnyWords3M (Tuo et al., 2023) and Marion10M (Chen et al., 2023a). Since most samples in these datasets contain short text with bounding boxes, we apply filtering process to select samples with at least seven words. The resulting subsets are named LongWordsSubsetA (from AnyWords3M) and LongWordsSubset-M (from Marion10M). To ensure data quality, we further refine the subsets by removing cases with duplicate words, excessive consecutive repetitions, or invalid text. Additionally, we retain only English-language samples. Detailed descriptions of the filtering process are provided in the Appendix B.1. TextScenesHQ. To create diverse and high-quality textrich image dataset, we developed TextScenesHQ. Similar to StyledTextSynth, we use GPT4o as world simulator to generate 26 predefined topics rich in text content. The overall pipeline is illustrated in Figure 4. The process begins with retrieval images aligned with the specified topics from Common Crawl 3. These images are then filtered using OCR-based filtering rules to select those containing long text. Images that do not meet this threshold undergo manual screening, during which we identify candidates for enhancement, such as adding text to advertisement backgrounds to enrich their visual complexity. After cleaning, we annotate the images using advanced multimodal large language models like Qwen2-VL (Wang et al., 2024) and Intern-VL2 (Chen et al., 2024d). These models generate detailed textual descriptions and bounding boxes for detected text regions. To ensure annotation quality, we validate them through semantic similarity checks using LLM, ensuring consistency and relevance. For contrastive data and complex layout images, we incorporate human annotations to re-label the corresponding text to improve the data quality. Finally, the curated images and their validated annotations are organized into comprehensive dataset, providing robust resource for training and evaluation. See Appendix B.5 for more details. 3https://commoncrawl.org/ Figure 5: Joint Distribution of Text Tokens and Image Count in TextVisionBlend. Each axis is visualized alongside the corresponding marginal distributions. 3.3. Evaluation Dataset Split To evaluate the dense-text image generation ability for existing model, we further propose TextAtlasEval. Adopting stratified random sampling weighted by subset complexity levels: 33% from advanced synthetic tiers (StyledTextSynth), 33% from real-world professional domains TextScenesHQ, and 33% from web-sourced interleaved TextVisionBlend coverage of both controlled and organic scenarios. For the StyledTextSynth and TextScenesHQ subsets, we sample data from each topic to ensure the evaluation set covers wide range of topics. For TextVisionBlend we perform random sampling to obtain the samples, and we finally get test set with 3000 samples. In this way, our dataset cover different domain of data which allowing us to assess the capabilities of model across multiple dimensions. 3.4. Data Combination Since each sub-dataset in our dataset contains different annotations, primary challenge is how to effectively organize these data. Overall, the annotations in our data are divided into two main categories: image scene descriptions and rendered text. Additionally, some sub-datasets include bounding box information for the rendered text and font size, which enables precise localization and ensures the generation of structured data. To integrate the scene description and rendered text into coherent and natural long descriptions, we utilize large language models (LLMs) to merge these two types of data. At the same time, to more efficiently leverage the existing 5 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 2: LDA topic analysis. Top 10 topics selected based on the highest proportions, trained on 200,000 random samples from TextAtlas5M. concept for each topic is derived from the related words. Topic Position Content Signs Colors Community Display Tickets Map Flights Safety Keywords Ratio 15.12% top, right, left, bottom, section, words, text, picture, image, icon 14.79% image, various, wall, text, several, background, includes, shows, poster, including 14.50% sign, words, picture, shows, signs, right, left, large, image, building 13.54% text, white, background, black, blue, image, red, letters, green, yellow 8.29% 4.88% 4.65% 3.32% 2.84% 2.67% success, school, words, community, conference, services, people, information, university, program screen, shows, displaying, image, words, digital, options, code, display, menu pm, ticket, train, day, date, card, weather, 12, seat, time map, notes, park, sticky, near, children, bus, chalkboard, road, stop flight, time, gate, information, pass, numbers, details, shows, times, number area, indicating, pointing, arrow, health, museum, line, parking, safety, art data, we adopt adaptive prompting strategies for different sub-datasets. For the LongWordsSubset, which contains both captions and text, we use LLMs to generate multiple distinct ways of integrating and . Another example is for the StyledTextSynth dataset, where we use Qwen2-VL to generate image scene captions while requiring the LLM to insert placeholders into the generated captions, allowing for natural combination of the caption and the text. For instance: \"A cozy classroom with blackboard displaying <>.\". For detailed processing strategies on different datasets, please refer to the appendix. 4. Analysis of TextAtlas5M In this section, we first analyze the high-level statistics of our TextAtlas5M. Then we analyze the topic modeling and do the qualitative assessment of the properties of TextAtlas5M. Figure 6: Kernel density estimations representing the distribution of perplexity scores for TextAtlas5M compared to reference datasets. The lower the perplexity for document, the more it resembles Wikipedia article. 4.1. General Statistics Joint Distribution of TextVisionBlend. Figure 5 shows the joint distribution of token numbers and image numbers in interleaved data split TextVisualBlend. We limit the number of images in document to 4 images for clarity. The documents of TextAtlas5M contain median number of images of 2 and median number of tokens of 33. Perplexity analysis. We utilized the pre-trained Llama-27B (Touvron et al., 2023) to calculate perplexity scores for 10,000 documents from each dataset. Lower perplexity scores suggest stronger resemblance to the types of text corpus used for Llama-2, including Wikipedia and other high-quality sources. Figure 6 presents the distributions of these scores. Our findings indicate that the average perplexity of CleanTextSynth is significantly lower compared to that of LongWordsSubset-A and LongWordsSubset-M. Additionally, the distribution of TextVisionBlend also aligns closely with that of the high-quality, diverse datasets used for Llama 2 training. We also observe that the text quality in synthetic datasets, such as CleanTextSynth, is significantly higher than that of real-image subsets like TextScenesHQ. 4.2. Topic Distribution Analysis Topic Analysis in StyledTextSynth and TextScenesHQ: As illustrated in Figure 7, we present an analysis of the topic distribution in the StyledTextSynth and TextScenesHQ datasets. To provide comprehensive comparison, we additionally highlight the broader topic coverage in real images. We make the following observations: i. Our dataset encompasses wide variety of text-rich topics, such as weather reports, banners, TV shopping ads, and monitor displays. This diversity is crucial for maintaining the richness and generalizability of the samples. ii. By leveraging Large Language Models (LLMs) as world simulators to generate topics, we ensure that most topics are consistent across real and synthetic images, effectively bridging the gap between these two data sources. iii. Some topics generated by the LLM are not suitable for rendering, either 6 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 7: Topic distribution in StyledTextSynth and TextScenesHQ subset, showcasing diverse range of text-rich topics such as weather reports, banners, and TV shopping ads. StyledTextSynth includes carefully selected 18 topics, while TextScenesHQ ultimately contains 26 distinct topics. These topics are generated using GPT-4 as world simulator and then filtered by humans to eliminate overlap while ensuring diversity. that our dataset contains extensive positional data, crucial element for understanding and processing interleaved content. To enhance the interpretability of the results, we employed GPT-4o to generate concise topic names based on the top 20 associated words for each topic. The full list of topics and their frequencies are in Appendix C.3. 4.3. Qualitative Assessment of Interleaved Samples StyledTextSynth: 154 images involve 15 topics. No watermarks or NSFW images were found. The OCR test results are as follows: In some topics, such as academic reports, the text in the images has clear contrast with the background and relatively large font size, resulting in high OCR recognition rate. However, when different but still distinct font colors overlap, the OCR results become inaccurate. Additionally, erroneous fonts generated by SD3.5 also affect OCR performance. Moreover, environmental lighting in the images can interfere with OCR accuracy. In topics with poor rendering quality, such as booklet pages, the OCR results tend to deteriorate accordingly. TextScenesHQ: We randomly sampled 200 images covering 23 topics, of which 4.0% found watermarks, but no NSFW images were detected. OCR recognition tests show that when the text is small or the contrast with the background is not obvious, the recognition accuracy decreases; Quantitative analysis reveals 22.3% OCR accuracy degradation (from 89.4% to 67.1%) when text-background contrast drops below 30% RGBa critical threshold for model robustness evaluation. Figure 8: Clip score distribution over all sub datasets in TextAtlas5M. We randomly sample 10k samples for each subset in TextAtlas5M. due to inherent complexities or limitations in the synthetic generation process. These topics are excluded to maintain the quality and feasibility of the dataset. Topic Modeling: Following the methodology of MMC4 (Zhu et al., 2024), we applied Latent Dirichlet Allocation (LDA) (Blei et al., 2003) to examine the diversity of topics in our TextAtlas5M. LDA provides insights into the relative proportions of each topic and the most frequently associated terms. From these 20 topics, we selected the 10 topics with the highest proportions for detailed analysis, as presented in Table 2. This table highlights the datasets content diversity, covering topics such as Signs, Tickets, and Community. Interestingly, positional information is the most prominently represented category, which highlights 7 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 9: Long text image generation remain challenge for existing model. Existing public methods, such as AnyText (Tuo et al., 2023) and Text Diffuser2 (Chen et al., 2023a), are capable of rendering short text but struggle with longer sequences. In contrast, models like GPT4o with DALLE-3 (Betker et al., 2023) and SD3.5 Large (Esser et al., 2024) show superior performance, though they still produce inaccuracies such as duplicated words or missing letters when handling extended text. For interleaved documents, all methods perform poorly due to their lack of layout planning capabilities. The yellow highlights text, while the blue indicates scene descriptions. In addition, some text is truncated due to blur or being located at the edge of the picture, affecting the recognition effect. For artistic words, the OCR recognition ability is poor, especially when the objects are designed as artistic words, they can hardly be correctly recognized, while artistic words similar to printed text have relatively good results. Case studies show calligraphic text achieves 58.2% recognition rate versus 92.7% for standard fonts, exposing current models typographic generalization limits. At the same time, among these 200 pictures, 14.0% of the pictures contain portraits (single portraits, group photos, advertising portraits and video covers), and 7.5% of the pictures contain pictures with logos. 4.4. Visual-Linguistic Similarity Assessment In this experiment, we analyze image-text similarity by using text as queries and images as candidates. We utilize the CLIP model to compute matching scores between text and images. Specifically, we use the open-CLIP 4 implementation of the ViT-B-32 model trained on the LAION2B dataset. The results are presented in Figure 8. We 4https://github.com/mlfoundations/open_clip observe that the CLIP scores for LongWordsSubset-A, LongWordsSubset-M, and Cover Book datasets are higher compared to other subsets. Notably, these splits include image captions, which likely contribute to the higher alignment. In contrast, our generated interleaved data exhibits lower matching scores. This suggests that the interleaved format is less optimized for the CLIP model, presenting challenge for off-the-shelf models to effectively align text and images in this format. Interestingly, for the Arxiv Paper subset, the alignment scores are higher than those of the interleaved data. This indicates that the CLIP model, trained on imagetext data, possesses some OCR-like capabilities, enabling it to recognize and align textual elements within documents. 5. Evaluate existing models We include in total 6 text-to-image generation models for evaluation in this section. Specifically, AnyText (Tuo et al., 2023), PixArt-Î£ (Chen et al., 2024a),TextDiffuser2 (Chen et al., 2023a), Infinity (Han et al., 2024) GPT4o (OpenAI, 2024) with Dall E3 (Betker et al., 2023) and SD 3.5 Large (Esser et al., 2024). In this experiment, we compare 8 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 3: Evaluation of Long-Text Image Generation on TextAtlasEval. Metrics include F1 score (F1.), CLIP Score (CS), and Character Error Rate (CER). The StyledTextSynth dataset covers 18 topics, while TextScensHQ consists of 26 common dense-text scenarios. Method Anytext TextDiffuser2 PixArt-Î£ Infinity-2B SD3.5 Large FID CS OCR TextVisionBlend OCR (F1.) (Acc.) StyledTextSynth TextScenesHQ OCR (Cer.) FID CS OCR (Acc.) OCR (F1.) OCR (Cer.) FID CS OCR (Acc.) OCR (F1.) OCR (Cer.) - - - - - - 81.29 0.1891 2.40 95.69 0.1979 2.98 118.85 0.1846 14.55 - - 1.57 3.44 16. - - 0.83 0.83 0.88 117.71 0.2501 0.35 114.31 0.2510 0.76 82.83 0.2764 0.42 84.95 0.2727 0.80 71.09 0.2849 27.21 0.66 1.46 0.62 1.42 33.86 0.98 0.99 0.90 0.93 0.73 101.32 0.2174 0.42 84.10 0.2252 0.66 72.62 0.2347 0.34 71.59 0.2346 1.06 64.44 0.2363 19.03 0.80 1.25 0.53 1.74 24. 0.95 0.96 0.91 0.88 0.73 image and its corresponding text description. In the evaluation of OCR-related results, we use PaddleOCR as the OCR detector to assess the relationship between the OCR results of generated images and the real text. We separately test the OCR accuracy, F1 score, and Character Error Rate (CER) of the generated images. Specifically, for accuracy and F1 score, we use word-level evaluation to check whether the generated text can correctly represent readable words, allowing for up to 80% mismatch between words. Higher accuracy and F1 scores indicate stronger consistency between the generated text and the real text. For CER, we compare the full OCR detection results with the ground truth text to calculate the character error rate. lower CER indicates higher match between the text in the generated image and the real text. The results are presented in Table 3. Since AnyText and TextDiffuser2 do not support interleaved format inputs, we exclude their results on TextVisionBlend. For the StyledTextSynth and TextScensHQ splits, we observe that SD3.5 (Esser et al., 2024) achieves significantly better performance, indicating its strong generalization ability and suitability across different scene types. It is worth noting that although SD-3.5 (Esser et al., 2024) Large significantly outperforms PixArt (Chen et al., 2023b) and Infinity (Han et al., 2024) in OCR-related scores on the TextVisionBlend subset, its FID and CLIP scores are lower than those of the other two models. To better understand this phenomenon, we present two representative cases in Figure 10, analyzing the differences in model performance on this subset. In the first row of Figure 10, SD-3.5 fails to capture the interleaved image layout and does not render text well, whereas both Infinity and PixArt follow the interleaved structure and white-background requirement, despite their poor text quality. This may explain SD-3.5s lower CLIP and FID scores. Meanwhile, in the second row, all three models exhibit interleaved characteristics, but only SD-3.5 generates relatively complete text in the image. This likely contributes to its strong OCR-related performance. Figure 10: Generation example on TextVisionBlend test set. SD-3.5 generates significantly more accurate text but occasionally struggles to maintain proper interleaved layout. these models on the long text image generation task. Text-to-Image Generation Visualization To provide clearer understanding, we present visual comparisons in Figure 9. Our observations highlight that previous text-toimage generation methods struggle with rendering dense text. For instance, AnyText (Tuo et al., 2023) renders only handful of words, while Text Diffuser 2 (Chen et al., 2023a) captures only part of the text. In contrast, GPT-4 with DALLE 3 demonstrates the best performance in text rendering. These results underscore that dense-text image generation remains challenging task for current models. Metric comparison. To further evaluate the model performance, we test the current state-of-the-art models on TextAtlasEval. We evaluate the commonly used FrÃ©chet Inception Distance (FID) to measure the similarity between generated images and ground truth images. lower FID score indicates that the generated images are visually more similar to the real images. For the evaluation of CLIP score, we use the CLIP-ViT-B/32 5 model to calculate the similarity between generated images and text. higher CLIP score indicates stronger alignment between the generated 5https://github.com/mlfoundations/open_clip TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Overall, when generating images with complex requirements, SD-3.5 performs poorly in terms of image layout and certain specifications. We speculate that this may be related to the models supported input text length. PixArt-Sigma can accommodate up to 300 text tokens, while Infinity, as an autoregressive generation model, supports even longer text inputs. greater text input capacity may provide an advantage in understanding complex instructions. 6. Conclusion In this paper, we introduce TextAtlas5M, novel large-scale dataset specifically designed for long-text rendering, addressing the existing gap in datasets capable of supporting such tasks. To demonstrate the utility of models in handling long-text generation, we create dedicated test set and evaluate current state-of-the-art text-to-image generation models. Additionally, the open availability of large-scale, diverse, and high-quality long-text rendering dataset like TextAtlas5M is crucial for advancing the training of textconditioned image generation models. There are several promising directions for further enhancing TextAtlas5M, which we have not explored in this paper due to the increased computational costs these approaches entail: i. Multiple rounds of dataset bootstrapping to iteratively ii. Generating multiple synthetic improve data quality. captions per image to further expand the dataset corpus."
        },
        {
            "title": "References",
            "content": "Araujo, A. and Girod, B. Slideshare dataset: Slides from topics related to engineering and science, June 2016. URL https://exhibits.stanford.edu/ data/catalog/mv327tb8364. arXiv.org submitters. arxiv dataset, 2024. URL https: //www.kaggle.com/dsv/7548853. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang, J., Lee, J., Guo, Y., et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3): 8, 2023. Blei, D. M., Ng, A. Y., and Jordan, M. I. Latent dirichlet allocation. Journal of machine Learning research, 3(Jan): 9931022, 2003. Changpinyo, S., Sharma, P., Ding, N., and Soricut, R. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 35583568, 2021. Chen, J., Huang, Y., Lv, T., Cui, L., Chen, Q., and Wei, F. Textdiffuser-2: Unleashing the power of language models for text rendering. arXiv preprint arXiv:2311.16465, 2023a. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixartbackslash alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023b. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., et al. Pixart-alpha: Fast training of diffusion transformer for photorealistic textto-image synthesis. arXiv preprint arXiv:2310.00426, 2023c. Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-Ï: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pp. 7491. Springer, 2024a. Chen, J., Huang, Y., Lv, T., Cui, L., Chen, Q., and Wei, F. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36, 2024b. Chen, L., Tan, S., Cai, Z., Xie, W., Zhao, H., Zhang, Y., Lin, J., Bai, J., Liu, T., and Chang, B. spark of vision-language intelligence: 2-dimensional autoregressive transformer for efficient finegrained image generation. arXiv preprint arXiv:2410.01912, 2024c. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for In Proceedings of the generic visual-linguistic tasks. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024d. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Esser, P., Kulal, S., Blattmann, A., Entezari, R., MÃ¼ller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Gupta, A., Vedaldi, A., and Zisserman, A. Synthetic data for text localisation in natural images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 23152324, 2016. 10 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Han, J., Liu, J., Jiang, Y., Yan, B., Zhang, Y., Yuan, Z., Peng, B., and Liu, X. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Inc., A. S. Pymupdf python binding for mupdf. https: //github.com/pymupdf/PyMuPDF, 2025. Accessed: 2025-01-15. Iwana, B. K., Raza Rizvi, S. T., Ahmed, S., Dengel, A., and Uchida, S. Judging book by its cover. arXiv preprint arXiv:1610.09204, 2016. LaurenÃ§on, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush, A., Kiela, D., et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740 755. Springer, 2014. Lin, Y., He, C., Wang, A. J., Wang, B., Li, W., and Shou, M. Z. Parrot captions teach clip to spot text. In European Conference on Computer Vision, pp. 368385. Springer, 2025. Liu, R., Garrette, D., Saharia, C., Chan, W., Roberts, A., Narang, S., Blok, I., Mical, R., Norouzi, M., and Constant, N. Character-aware models improve visual text rendering. arXiv preprint arXiv:2212.10562, 2022. Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2643926455, 2024. Ma, J., Zhao, M., Chen, C., Wang, R., Niu, D., Lu, H., and Lin, X. Glyphdraw: Seamlessly rendering text with intricate spatial structures in text-to-image generation. arXiv preprint arXiv:2303.17870, 2023. OpenAI. Hello gpt-4o, 2024. URL https://openai. com/index/hello-gpt-4o. Accessed: 2024-0909. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021a. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021b. Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35: 2527825294, 2022. Sefid, A., Mitra, P., Wu, J., and Giles, C. L. Extractive research slide generation using windowed labeling ranking. In Proceedings of the Second Workshop on Scholarly Document Processing. Association for Computational Linguistics, 2021. URL https://aclanthology. org/2021.sdp-1.10. Sidorov, O., Hu, R., Rohrbach, M., and Singh, A. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 742758. Springer, 2020. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Srinivasan, K., Raman, K., Chen, J., Bendersky, M., and Najork, M. Wit: Wikipedia-based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 21, 2021. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Sun, Q., Yu, Q., Cui, Y., Zhang, F., Zhang, X., Wang, Y., Gao, H., Liu, J., Huang, T., and Wang, X. Emu: Generative pretraining in multimodality. In The Twelfth International Conference on Learning Representations, 2023. 11 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Team, C. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Tuo, Y., Xiang, W., He, J.-Y., Geng, Y., and Xie, X. Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054, 2023. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Wendler, C. RenderedText dataset. https: //huggingface.co/datasets/wendlerc/ RenderedText. Accessed: 2024-11-02. Yang, Y., Gui, D., Yuan, Y., Liang, W., Ding, H., Hu, H., and Chen, K. Glyphcontrol: Glyph conditional control for visual text generation. Advances in Neural Information Processing Systems, 36, 2024. Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Zhan, J., Dai, J., Ye, J., Zhou, Y., Zhang, D., Liu, Z., Zhang, X., Yuan, R., Zhang, G., Li, L., et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. Zhu, W., Hessel, J., Awadalla, A., Gadre, S. Y., Dodge, J., Fang, A., Yu, Y., Schmidt, L., Wang, W. Y., and Choi, Y. Multimodal c4: An open, billion-scale corpus of images interleaved with text. Advances in Neural Information Processing Systems, 36, 2024. 12 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation A. Creation of the Synthetic Dataset. A.1. Creation of StyledTextSynth A.1.1. TEXT PROMPT FOR STYLEDTEXTSYNTH IMAGE GENERATION General Prompt: The core of the synthetic data involves utilizing text-conditioned image generation methods. For simple topics like billboards, we follow \"General Prompt\" approach with the following guidelines: Provide reasonable description of the billboard. Ensure the billboard faces the camera directly. The billboard should occupy at least one-third of the image. Incorporate complex background for added detail. Keep the billboards color consistent, with no additional context. Limit the total text on the billboard to fewer than 160 words. Ensure the billboard is visible, vehicle-related, and not overlapped by other objects. We show an example in Table 4, with detail instruction the LLM generate reasonable scene description for image generation model. Table 4: An example prompt and generated description for silver screen image. GPT4o: want to use model to generate some pictures of silver screen, so please give me some prompt follow these rules. Silver screen without any content and pure color but vertical , and looks like facing the camera but dont have any content, and the silver screen should take up 1/3 of the image, and have some specific complex background description, and the total word count should not exceed 160 words result is paragraph. GPT4o res: vertical silver screen, empty and pure in color, facing the camera directly. The screen occupies the lower third of the image, with rugged mountain range in the background, peaks dusted with snow, and the sky painted in warm hues of fading sunset. Topic with Human-Designed Seeds: Certain topics, such as studio scenes, can involve highly complex setups. To address these challenges, we simplify the generation of text regions by incorporating human-designed seeds. Specifically, the instructions include general prompt combined with predefined, human-curated scene to serve as the seed, ensuring better coverage and control over these intricate scenarios. Weather Report: host stands in studio, with display screen behind them showing weather conditions for various locations. In the foreground, white rectangular box occupies one-third of the screen. TV Shopping: host stands beside table, holding product in their hand, with an advertising board placed vertically next to them. Instruction Manual: An instruction manual is placed vertically, with objects associated with it positioned nearby. Template Topics with Fixed Text Region Position: Some topics can be used as template to generate more samples, what is different is that the text appeared position is fixed. For these generate samples, we can simply generate more samples by simple text replace. 13 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation News: Describe an image that could appear on television news. It can depict either positive or negative events, with diverse content. The total word count should be less than 160 words. Cinema Poster: Describe an image of movie poster. It can be of any genre. The total word count should be less than 160 words. Table 5: Rendered Text Generation for Specialized Scenes Using the GPT4o Model. GPT4o: Please generate 50 texts about noticeboard, 40-80 words per text, the requirements are as follows: Any notification is fine. Only output text, and pay attention to the diversity of output, and do not have text with repeated meanings. The output format should be 1.text 2.text... GPT4o res: 1.The monthly community cleanup event will take place this Saturday at 9 AM. Volunteers are encouraged to gather at the community center with gloves and trash bags. Lets work together to keep our neighborhood clean! 2.Attention students: The library will close an hour earlier than usual at 5 PM on Friday due to maintenance work. Please plan your study sessions accordingly. ... 50. The community board game night is happening this Friday at 7 PM in the activity room. Bring your favorite games! Table 6: Rendered Text Generation for Specialized Scenes Using the Llama3.1 Model. Llama3.1: Please generate 100 texts about noticeboard, 40-80 words per text, the requirements are as follows: Any notification is fine. Only output text, and pay attention to the diversity of output, and do not have text with repeated meanings. The output format should be 1.text 2.text ... Llama3.1 res: 1.The Student Union is hosting Cultural Festival on August 10th to celebrate the diversity of our campus community. Come and experience the vibrant traditions, music, and food from around the world. There will be performances, workshops, and marketplace featuring local vendors. All are welcome to attend. ... 100.The Neurodiversity Awareness Group invites you to seminar on neuroinclusive education on Wednesday, November 15th, at 4:00 PM in the lecture hall. Learn about the latest research and strategies for supporting students with neurological differences. A.1.2. UTILIZING LLMS FOR TEXT GENERATION IN SPECIFIC SCENES After generating scene images without text, it is necessary to call upon an LLM to create contextually relevant sentences for rendering. To ensure realism in the generated outputs, we employ Scene-Dependent Text Generation. For general topics, such as noticeboards, we prompt the LLM to produce sentences based on the given topic. Examples of such outputs are shown in Table 5 and Table 6. The LLMs accurately generate text appropriate for the given scene. Visual-Dependent Scenes: In some cases, the visual appearance of the scene is closely connected to the text and requires fine-grained visual understanding. For instance, generated poster with rich visual elements may need text that complements its design. In such scenarios, we use LVM models that process both text and image inputs to produce reasonable outputs. An example of this process is shown in Table 7. By incorporate specific instruction, the model produce reasonable output. 14 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 7: Rendered text generation for specialized scenes using the QWen2-VL model. QWen2-VL Synopsis: This is picture about movie poster. Please imagine story for the picture based on the content of the picture, and then output story outline. Just output the story outline, do not output other content. The story outline should be between 30-60 words. QWen2-VL Synopsis res: In world on the brink of destruction, lone, mysterious warrior with fiery sword must infiltrate an impenetrable fortress guarded by an army of elite, dark-magic-wielding soldiers to stop an ancient evil from unleashing catastrophic apocalypse. QWen2-VL Title: This is picture about movie poster. Please imagine story for the picture based on the content of the picture, and then give the story title. Only output the title, do not output other content, and the title should not exceed 6 words. Qwen2-VL Title Result: Ember of Forgotten Reckoning Final Transformation: A.1.3. HOW DO WE SELECT DATA TOPICS FOR RENDERING? Originally we generate 50 topics that include dense text, one more question is how to filter these topics. With this in mind, we design the following filter rules: 1. Avoid topics directly tied to font generation, such as store signs, wayfinding signs, or on-screen text. 2. Exclude topics where the renderable area is too small, such as mobile phone screenshots. 3. Avoid topics with unclear boundaries or artistic fonts that are hard to recognize, such as neon signs. 4. Prioritize topics with better rendering results in SD3.5 over similar ones, e.g., choose \"digital display\" over \"OLED display\" or \"banner\" over \"protest marches.\" A.1.4. TEXT DEDUPLICATION IN STYLEDTEXTSYNTH DATA The text in the synthetic data is generated by Llama 3.1, GPT4o, and Qwen2VL. In most topics, there is basically no obvious disharmony between the generated images and the scenes, so we mainly use the text generated by Llama 3.1 and GPT4-o. 15 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation For some scenes where the images and texts are highly correlated, VLM is needed to generate texts that match the image content. Under the same or semantically similar topics, there may be semantic duplication. To this end, we semantically deduplicate the generated text based on the sentence-transformers library. Deduplication Process with Semantic Hashing The deduplication process consists of the following steps: 1. High-Dimensional Semantic Representation: Obtain the high-dimensional semantic representation of the text. 2. Dimensionality Reduction: Map the high-dimensional semantic vector to fixed low-dimensional space using random projection. 3. Semantic Hash Generation: Generate semantic hashes based on the projection results. 4. Pairwise Similarity Comparison: Use Hamming similarity to perform pairwise comparisons of the semantic hashes. Hamming similarity threshold of 0.9 is applied to detect and remove semantically similar texts. This structured approach ensures effective and accurate text deduplication while maintaining semantic integrity. A.1.5. MIDDLE-QUALITY SAMPLE FILTERING To enhance the quality of the generated middle-quality samples, we apply set of filtering rules to reject unsuitable samples. The primary criteria for rejection are as follows: Insufficient areas available for rendered text. Excessive similarity to other topics, reducing diversity. Difficulty rendering text in curved areas. Unrealistic or artificial appearance of the image. Challenges in identifying or defining bounding boxes. Presence of incorrect or irrelevant text. Poor recognition quality or unclear visual details. Examples of rejected samples are illustrated in Figure 11. A.2. Gen Interleaved Data Benchmark The process of generating interleaved data is divided into three main parts: data selection, PDF generation, and annotation generation. A.2.1. DATA SELECTION We select data from WIT (Srinivasan et al., 2021) and OBELICS (LaurenÃ§on et al., 2024). The WIT dataset contains samples from Wikipedia, with each sample comprising an image and multiple associated text segments, such as titles, main text, subtitles, subtext, and image captions. From this dataset, we sample instances containing single image and interleaved text. The OBELICS dataset consists of interleaved image-text documents sourced from web pages in the Common Crawl. Each OBELICS sample includes multiple images with their corresponding text segments. For our purposes, we sample data containing two to four images to maintain manageable image sizes. Finally, we sampled 69.8% data from the WIT and 30.2% data from the OBELICS, respectively. 16 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 11: The rejected StyledTextSynth samples. A.2.2. PDF GENERATION After selecting the data, we use the PyMuPDF (Inc., 2025) library to generate parseable PDF files based on the sampled data. To accommodate the two types of data, we design different layout generation strategies according to their respective structures. For both datasets, the layout strategy involves first randomly assigning image positions on the page, followed by allocating text boxes in manner that optimally utilizes the remaining space. For the WIT dataset, since the text segments have predefined types (e.g., title, main text, subtitle), we impose additional constraints to ensure structural consistency. For instance, titles are placed at the top of the corresponding image to maintain semantic alignment. In contrast, for the OBELICS dataset, we adopt simpler approach where the text boxes are sequentially assigned in top-left to bottom-right order across the layout. To implement the layouts, we use the insert_htmlbox() from PyMuPDF to insert images and text into the PDFs. The font for each sample is randomly selected to introduce variation. To further standardize the generated PDFs, we limit the text in each text box to maximum of 50 words. Additionally, after generating the PDFs, we save rendered image version of each page to serve as the corresponding image data for our dataset. A.2.3. ANNOTATION GENERATION After generating parseable PDFs, we use the PyMuPDF to extract information such as the bounding boxes of text and images, as well as text font sizes and styles. Additionally, we utilize Qwen2-VL to generate captions for each image within the PDF. The prompt used for caption generation is: \"Generate the caption of the image, and the caption should be no more than 50 words.\" Finally, we obtain detailed information for the text, including its content and bounding boxes, along with the captions and 17 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 8: Description generation prompt. Generate data template Prompt \"I have scene description and OCR text O. Please generate 200 unique combinations that naturally merge the description and OCR text into cohesive, longer paragraphs. Save the output in plain text file.\" bounding boxes of the images. By combining these elements based on the generated template, we produce the interleaved data annotations. A.3. Template Generation Details To create templates for summarizing descriptions and text, we utilized an LLM to generate total of 600 templates. The prompt used for this process is detailed in Table 8. A.4. Bounding Box Annotation and Detector Training for StyledTextSynth Sample A.4.1. BOUNDING BOX GENERATION 1. YOLO Results: Based on the models output, select the bounding box with the largest rectangular area when multiple results are present. 2. Fine-tuned RT-DETR_R50VD Results (Packing Box): Use the models output to identify the bounding box. If multiple results are present, select the one with the smallest difference between width and height. 3. RT-DETR_R50VD Results (Booklet Page): Check if the output contains the label \"book\". If multiple results are present, choose the bounding box with the smallest width-to-height difference. For all three methods above, the results are passed through SAM2 (Segment Anything Model v2) to refine recognition. The SAM2 output is converted into center points to serve as prompts, improving predictions for slanted surfaces. A.4.2. DETECTOR TRAINING YOLO Training Process: 1. For each topic (excluding template topics like Alumni Profile and News, as well as rejected topics), start with the YOLOv11l initial weights and manually label about 1,000 failed detection samples. Use the following annotation methods: (a) For slanted images, use the labelme tool to annotate with quadrilateral bounding boxes (four points). (b) For upright images, annotate using the Code tool with two points (top-left and bottom-right). Annotate areas of the image where the topic is unobstructed, and convert all annotations to YOLO format with the class label set to 0. 2. Train the model on the mixed annotated dataset for approximately 400 epochs (or more) to obtain initial weights. 3. Test the initial weights on each topic. detection rate of at least 40% is considered usable for that topic. 4. For topics with low detection rates in Step 3, augment the manually labeled dataset for that topic with approximately 1,000 additional images. Apply transformations such as scaling, composition, flipping, and affine transformations. Combine the augmented data with the original manually labeled data (totaling approximately 2,000 images), and retrain the model for an additional 200 epochs. 5. Certain topics may share weights based on detection performance. For example: Billboard and TV Shopping can share the same weights. Blackboard Classroom and Advertisement Poster can share the same weights. 18 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Fine-Tuned RT-DETR_R50VD Training: 1. Fine-tune the RT-DETR_R50VD model specifically for the Packing Box topic. Use 1,000 manually annotated packing box samples from Step 1. 2. Train the model for approximately 100 epochs. A.5. Text Rendering Details A.5.1. 1. BBOX TEXT RENDERING: After obtaining images generated by Stable Diffusion and images from CommonCrawl, which contain large fillable text areas (such as billboards, electronic screens, etc.), we use YOLO v11 and RT-DETR_r50vd to identify and label the fillable areas in the images. However, these detectors can only recognize rectangular areas, and the labeled fillable regions are often slightly larger than the actual fillable areas. Therefore, we further use SAM2, starting from the center point of the bounding box, to search for color-matching areas. This ensures that the new bounding boxes generated by SAM2 more accurately cover the fillable areas, breaking the traditional rectangular limitation and supporting the detection and filling of irregular quadrilaterals. For text content generation, we use Llama-3.1-8B, GPT-4o, and Qwen2-VL-7B. Among them, Qwen2-VL-7B is mainly used for generating text related to cinema posters. Rectangular Bbox Text Rendering For detected rectangular bounding boxes, we directly render text within the area. The font is randomly chosen from 10 common fonts, and the font size is automatically adjusted based on the bounding box size to fill the area as much as possible, ensuring both aesthetics and readability. Irregular Quadrilateral Bbox Text Rendering For detected irregular quadrilateral bounding boxes, we first create transparent layer and render the text on that layer. Then, we use perspective transformation to adjust the transparent layer to match the irregular quadrilateral shape of the bounding box and finally composite it onto the original image, ensuring the text accurately fits the fillable area. A.5.2. 2. TEMPLATE RENDERING METHOD: For images related to News Shows, Weather Reports, and Cinema Posters, where text usually appears in relatively fixed areas, we use template rendering approach for text filling. We create background templates for these topics based on real-world images, label the fillable areas of the templates with bounding boxes, render the background templates onto the original image, and then fill the text according to the bounding box annotations of the templates. 19 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation B. Creation of the Real Dataset. B.1. Data Selection Details from Existing Datasets To ensure the quality of selected samples, we apply rigorous filtering pipeline consisting of the following steps: 1. English-Like Check: Text is evaluated for resemblance to English. At least 70% of the words must contain alphabetic characters to ensure alignment with natural English patterns. 2. Minimum Length Check: Samples with fewer than seven words are excluded. This criterion eliminates excessively short texts that may lack meaningful content. 3. Unique Word Ratio Check: To promote diversity, the ratio of unique words to total words must exceed 0.3. Samples with overly repetitive word usage are filtered out. 4. Consecutive Repetition Check: Text containing more than three consecutive repetitions of the same word is excluded to prevent redundancy and improve coherence. 5. Word Validity Check: Each word must include at least one alphabetic character and be longer than one character. This ensures all words are meaningful and eliminates noise or random symbols. 6. Text Cleaning: Non-alphanumeric characters, except spaces, are removed. Multiple spaces are normalized into single space to ensure the text is clean and consistently formatted. 7. Annotation Sorting: Annotations are ordered spatially, following top-to-bottom and left-to-right sequence based on the coordinates of bounding polygons. This ensures spatial coherence in the text layout. This pipeline is designed to refine the dataset and maintain high standards for text quality and diversity. B.2. Extracting Powerpoint Data We extract the powerpoint data with PyMuPDF (Inc., 2025). Specifically, we transform the each page of powerpoint into pdf format, then we rephrase the powerpoint data by blocking description. For example, it split the all page into different block. Each block include elements like text or image, for text element we extract the word and for image we use the QWen-VL to generate caption and the prompt is simple Describe this image. For example, we simply call image. B.3. Data Selection Details for TextScenesHQ Dataset After crawling images according to topics, we use the easyOCR6 library to recognize the text in the images. First, we save images containing more than 10 words, and then organize the text information from the upper left to the lower right to construct JSON file. The content of the JSON file includes the text and its corresponding bounding box. During this process, some difficult data may have spelling errors, including but not limited to confusion between numbers and letters, spelling errors, and capitalization errors. In this regard, we use Llama 3.1 (Dubey et al., 2024) to check and correct the recognized text to improve the accuracy and quality of the text. B.4. TextScenesHQ Image Filtering For real image data, we primarily discarded samples where the text was not clearly visible. Additionally, samples were rejected if the detected language was not English or if the text contained too few words (fewer than 10 in this study). We show the rejected samples in Figure 12. B.5. TextScenesHQ Image Annotation After using OCR for filtering and generating bounding boxes around the text in the images, we convert the detected Chinese text and its corresponding bounding boxes into text JSON format. Due to the diversity and complexity of the images, OCR results may contain spelling errors and misordered text. To address this, we perform three corrective steps using Llama 3. 6https://github.com/JaidedAI/EasyOCR 20 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 12: The rejected TextScenesHQ samples. and Qwen 2.5-Coder. First, Llama 3.1 is used to correct any spelling mistakes in the text. Next, we use Llama 3.1 to reorder the text slightly to align with the proper syntax, as OCR typically outputs text in left-to-right, top-to-bottom sequence without considering the multi-column layout in the images. After reordering, we generate the corrected text JSON. The third step involves addressing any potential formatting issues in the JSON. If the JSON generated in the second step is not parsable, we use Qwen 2.5-Coder to output the text JSON in markdown format to ensure proper structure. For the image background descriptions, we use Qwen 2.5-VL to generate contextual information while preventing it from outputting any descriptions of the text within the image. Additionally, we created 500 diverse and complex scenario templates using GPT-4o to generate wide range of image descriptions. These descriptions, combined with the corresponding text JSON, are used to generate comprehensive image information in JSON format. B.6. Quality classification In this work, we mainly split the data quality according to the visual appealing semantic, and if the image include dense text and have correct captions. 21 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation C. Annotation Details C.1. Examples from All Subsets Our TextAtlas5M comprises total of 10 subsets, which can be categorized into three types: i. Images without specific scene, ii. Images with specific scene, and iii. Images with specific scene and bounding box annotations. Images without Specific Scene. For simple synthetic datasets such as Paragen-2M, where the background is plain white, we generate descriptions for image creation using prompts like: \"Please generate an image of xxx based on the following text: .\" Images with Specific Scene. This category includes images accompanied by scene description and OCR text O. Using our template, we generate longer, natural descriptions by combining these elements. Images with Specific Scene and Bounding Box. For datasets like AutoSlideGen, ArxivPaper generation, and interleaved sample generation, bounding box annotations are provided for each element. In these cases, we utilize LLMs to summarize all elements into coherent paragraph. Specifically, we include details such as bounding box coordinates and the text within each box. All subset examples are visualized in Figure 13. C.2. Processing Methods For datasets that already have captions and OCR results, such as anyword3m and mario10m, we use templates generated by GPT for concatenation (as you did before). For paragen2m, which is pure text data, we use structured sentence descriptions, e.g., \"a text white background image...\". For autogen and interleave data, which are interleaved distributions, we list the text and image separately in bullet points, while placing the required elements (like bbox) and fonts in the corresponding context section. For midquality data, to ensure natural integration, we generate scene captions using Qwen2-VL and require it to generate render text placeholder <>, which is then replaced with the rendered text. High-quality data is processed by Llama3.1 to generate scene descriptions and optimize the OCR results (see section 3.2 for the concatenation method). C.3. All LDA Topics In this section, we list top 20 LDA topics of TextAtlas5M in Table 9. Based on the topic distribution in the table, several patterns emerge: 1. High Proportion of Common Topics: Topics such as \"Position\" (15.12%), \"Signs\" (14.50%), and \"Colors\" (13.54%) account for significant portion of the dataset. These themes likely reflect common real-world scenarios, such as signage, positioning of text and images, and the use of colors in visual communication. 2. Content-Related Themes: Content-centric topics like \"Content\" (14.79%), \"Community\" (8.29%), and \"Safety\" (2.67%) also show relatively high proportions, suggesting that the dataset includes considerable amount of text related to information dissemination and visual design, commonly seen in advertising and informational graphics. 3. Lower Proportion of Specialized Domains: Topics like \"Products\" (1.48%), \"Cloud\" (0.55%), and \"Shops\" (0.78%) have smaller representations, indicating that the dataset covers fewer instances of text-image combinations related to specific industries or niche topics. 4. Use of Numbers and Symbols: Topics related to numbers, such as \"Numbers\" (1.75%) and \"Symbols\" (0.61%), occupy lower proportions, possibly reflecting that numeric and symbolic content is less prevalent in the dataset, despite its importance in some contexts. Overall, the dataset is more focused on common visual and textual elements seen in everyday life, such as positioning, signage, and color usage, with relatively lower emphasis on specialized topics or numeric/symbolic content. 22 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 13: randomly sampled selection from all subsets, including both synthetic and real data. 23 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Table 9: Full set of topics for the = 20 LDA model in TextAtlas5M. Proportion Keywords 14.79% Topic Content"
        },
        {
            "title": "Products\nCloud",
            "content": "Food Market Display Travel Flights Map Books Symbols Tickets Lorem Community Signs Safety Position Numbers Shops 1.48% 0.55% 1.46% 2.39% 4.88% 2.71% 2.84% 3.32% 2.66% 0.61% 4.65% 1.02% 8.29% 14.50% 2.67% 15.12% 1.75% 0.78% Colors 13.54% image, various, wall, text, several, background, includes, shows, poster, including love, product, size, case, fitness, body, water, san, bottle, products service, cloud, customer, things, programs, create, security, close, brooklyn, ideas coffee, food, guide, best, real, cup, home, tour, game, fresh new, x, sale, york, b, 0, f, c, car, market screen, shows, displaying, image, words, digital, options, code, display, menu please, page, make, thank, world, one, travel, go, good, see flight, time, gate, information, pass, numbers, details, shows, times, number map, notes, park, sticky, near, children, bus, chalkboard, road, stop board, book, display, library, books, de, reading, read, step, titled mounted, symbol, platform, 100, keyboard, signage, function, premium, keys, shift pm, ticket, train, day, date, card, weather, 12, seat, time lorem, ipsum, dolor, sit, amet, consectetur, ut, elit, adipiscing, sed success, school, words, community, conference, services, people, information, university, program sign, words, picture, shows, signs, right, left, large, image, building area, indicating, pointing, arrow, health, museum, line, parking, safety, art top, right, left, bottom, section, words, text, picture, image, icon 1, 2, 3, 4, 5, 6, 10, 7, 9, destination depicts, shop, flights, counter, little, morning, synergy, scheduled, customers, eget text, white, background, black, blue, image, red, letters, green, yellow 24 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation D. Visualization of TextAtlas5M D.1. Example of StyledTextSynth Samples To better investigate all topics included in the StyledTextSynth sample, we show the examples in Figure 14. We mainly list Blackboard Classroom, News, Banner, Sliver Screen, Notice Board, Advertisement Board, TV Shopping, Billboard, Booklet Page, Academic Report, Alumni Profiles, Tablet Screen, Printed Paper, Cinema Poster and Packing Box. D.2. Examples of TextScenesHQ Samples We present examples of topics with the largest number of samples in Figure 15. These include: Product Labeling, Billboard, Packing Box, Monitor, Instruction Manual, Booklet Page, Mobile Phone Screenshot, Wall Decal, Floor Poster, Game Live, OLED Display, Protest Marches, Weather Report, Noticeboard, News Show, Blackboard Classroom, Digital Display, Cinema Caption, Wayfinding Sign, Academic Report, Alumni Profiles, Banner, Clothes with Text, and Store Sign. These topics represent text-rich scenes commonly encountered in daily life. By applying our carefully designed filtering rules, we have ensured that only TextScenesHQ data is preserved for rendering. Table 10: Data Level, Datasets, and Annotations Overview. Data Split Synthetic Images Real Images In Total Dataset Name CleanTextSynth TextVisionBlend StyledTextSynth PPT2Details PPT2Structured LongWordsSubset-A LongWordsSubset-M 1,299,992 Cover Book Paper2Text TextScenesHQ TextAtlas5M 207,566 356,658 36,576 0.5M #Samples Annotations 1,907,721 547,837 426,755 298,565 96,457 266,534 Real Text Parsed json+BLIP Caption Human+ QWEN+Intern-VL QWEN2-VL Caption Parsed json+QWEN2-VL Caption Caption + OCR Caption + OCR Name + Author + Category PyMuPdf phrased Text Human+Llama+Qwen+GPT4o - Type Pure Text Pure Text Synthetic Image Powerpoint Image Powerpoint Image Real Image Real Image Real Image Pure Text Real Image - Token Length 70.70 265.62 90.00 121.97 774.67 38.57 34.07 28.01 28.01 120.81 148.82 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation Figure 14: StyledTextSynth examples. 26 TextAtlas5M: Large-scale Dataset for Dense Text Image Generation 27 Figure 15: TextScenesHQ topics."
        }
    ],
    "affiliations": [
        "Central South University",
        "Microsoft",
        "National University of Singapore",
        "North University of China"
    ]
}