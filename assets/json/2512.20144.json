{
    "paper_title": "Multi-hop Reasoning via Early Knowledge Alignment",
    "authors": [
        "Yuxin Wang",
        "Shicheng Fang",
        "Bo Wang",
        "Qi Luo",
        "Xuanjing Huang",
        "Yining Zheng",
        "Xipeng Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 4 4 1 0 2 . 2 1 5 2 : r Multi-hop Reasoning via Early Knowledge Alignment Yuxin Wang*1,2, Shicheng Fang*1,3 , Bo Wang1, Qi Luo1, Xuanjing Huang1,2, Yining Zheng1, Xipeng Qiu1,3 1Computer Science, Fudan University 2Institute of Modern Languages and Linguistics, Fudan University 3Shanghai SII {wangyuxin21,25113050022,22110240036,qluo22}@m.fudan.edu.cn {ynzheng19,xjhuang,xpqiu}@fudan.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Retrieval-Augmented Generation (RAG) has emerged as powerful paradigm for Large Language Models (LLMs) to address knowledgeintensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at Github."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have demonlanstrated remarkable capabilities in natural *Equal contribution. 1 guage understanding and generation, yet they face fundamental limitations when dealing with knowledge-intensive tasks that require access to upto-date or domain-specific information. RetrievalAugmented Generation (RAG) has emerged as promising paradigm to address these limitations by dynamically incorporating external knowledge from retrieval corpora into the generation process (Karpukhin et al., 2020; Lewis et al., 2020). Standard RAG systems perform single retrieval step followed by generation, but the intrinsic difficulty of retrieving multi-hop information in one step causes lot of failure. Recent advances have shown that iterative approaches where models can perform multiple rounds of retrieval and reasoningsignificantly improve performance on complex multi-hop reasoning tasks(Jin et al., 2025a; Guan et al., 2025; Luo et al., 2025a; Song et al., 2025). However, although assumed well, these iterative systems can still suffer from retrieval failure, resulting from the plan failure which leads to the suboptimal reasoning chains, particularly when the initial reasoning step lacks sufficient contextual grounding. These scenarios are illustrated in Figure 1 with real example from the dataset. Iterative RAG systems(Jin et al., 2025a; Song et al., 2025) are often optimized by Reinforcement Learning (RL)(Schulman et al., 2017a; Shao et al., 2024b), offering principled approach to learn effective retrieval and reasoning strategies. RL-based RAG frameworks treat the retrieval and generation process as sequential decision-making problem, where agents learn to search for information and generate responses to maximize cumulative rewards based on answer accuracy and efficiency metrics. The success of RL training heavily depends on the quality of the exploitation and the exploration efficiency during the learning process. Recent studies on entropy(Wang et al., 2025; Cui et al., 2025) show that entropy measurement is good signal for this exploitation and exploration RL-optimized iterative retrieval and generation process. This framework significantly improves the quality of reasoning foundations and reduces the likelihood of cascading errors in subsequent iterations. Analysis from an Entropy Perspective. We analyze the training dynamics of Group Relative Policy Optimization (GRPO)(Shao et al., 2024b) in iterative RAG from an entropy perspective and show that with lower entropy in the training phase, instead of insufficient exploration, our approach leads to more efficient exploration strategies focusing on the retrieval set, faster roads to the answer during RL training compared to traditional approaches that start with uninformed, model initialized thinking. Comprehensive Experimental Validation. We conduct extensive experiments on standard RAG datasets, showing consistent improvements in both answer accuracy and retrieval recall. Besides, generalization experiments show no degrading of generalization with our method."
        },
        {
            "title": "2.1 Retrieval-Augmented Generation",
            "content": "The concept of augmenting language models with external knowledge retrieval has gained significant traction in recent years. Early work by (Karpukhin et al., 2020) introduced Dense Passage Retrieval (DPR), which demonstrated the effectiveness of dense vector representations for retrieval in open-domain question answering. (Lewis et al., 2020) proposed Retrieval-Augmented Generation and lot of works(Gao et al., 2023; Li et al., 2023) has emerged. To apply better retrieval, LightRAG(Guo et al., 2025) employs dual-level retrieval system for better generation. Structure-based retrieval methods like GraphRAG(Edge et al., 2025), PathRAG(Chen et al., 2025), HippoRAG2(Gutiérrez et al., 2025), HyperGraphRAG(Luo et al., 2025b) have been proposed to utilize fine-grained retrieval like entities or links and generate better responses. Traditional single-step RAG systems often fall short when dealing with complex reasoning tasks that require multiple pieces of evidence. This limitation has motivated research into iterative RAG systems. Figure 1: Standard RAG and Iterative RAG pipeline. While standard RAG suffers from the impossibility of multi-hop retrieval in one step, iterative RAG also suffers from plan failure in the initial think, which is caused by lack of information of the retrieval set. balance, which is important because the exploitation of retrieved information and exploration in the retrieval set control the whole reasoning process. Poor initial reasoning steps in exploration can lead to compounding errors throughout the iterative process. From both the perspective of an iterative RAG system and the RL training dynamics, the quality of initial planning plays crucial role in the effectiveness of generating right answers. When models begin their reasoning process without adequate contextual knowledge, they often generate misguided hypotheses or pursue irrelevant reasoning paths relying on themselves, which is far from the information the environment can give, leading to cascade of poor retrieval decisions and incorrect conclusions. This problem is particularly pronounced in the early stages of RL training, where random or poorly informed initial actions can significantly hinder the learning process. By enhancing the initial planning step with early knowledge, we hypothesize that models can establish more accurate reasoning foundations, leading to better exploration strategies with less entropy and more efficient learning dynamics. This Early Knowledge Alignment (EKA) not only improves the immediate reasoning quality but also provides clearer learning signals for the RL algorithm, enabling faster roads to the right answer. Our contribution is as follows: Early Knowledge Alignment (EKA). We propose novel approach that augments the initial thinking step in iterative RAG systems with early knowledge, providing models with better grounding before entering the 2 2.2 Iterative and Multi-Hop RAG Approaches Chain-of-Thought (CoT) prompting (Wei et al., 2022) encourages models to generate intermediate reasoning steps, effectively simulating an iterative thinking process. IRCoT (Trivedi et al., 2022a) demonstrated that interleaving retrieval and generation steps can significantly improve performance on multi-hop reasoning tasks. ITER-RETGEN (Shao et al., 2023) proposed framework where models can decide when to retrieve additional information based on their confidence levels. WebGPT (Nakano et al., 2021) showed that models can be trained to browse the web iteratively to gather information for answering questions. ReAct (Yao et al., 2023) combined reasoning and acting in language models, enabling them to perform dynamic retrieval based on their reasoning traces. More recent work by (Jiang et al., 2023a) introduced Self-RAG, which uses reflection tokens to control retrieval timing and assess the quality of retrieved passages,while Selfask, proposed by (Press et al., 2023), implements an autonomous question formulation mechanism during the reasoning process. FLARE (Jiang et al., 2023b) incorporates adaptive retrieval when LLMs generate low-confidence tokens. 2.3 Reinforcement Learning for RAG"
        },
        {
            "title": "Optimization",
            "content": "The application of reinforcement learning to optimize RAG systems has emerged as promising research direction. Several approaches, such as R1-Searcher(Song et al., 2025), R3-RAG(Li et al., 2025b), and DeepRAG(Guan et al., 2025), employ two-stage training process. They first use manually curated data to perform Supervised FineTuning (SFT) on the LLM, and subsequently apply reinforcement learning to further align the model with the available knowledge boundaries. Similarly, s3 (Jiang et al., 2025) proposes modular framework that employs RL to optimize search agent while keeping the generator frozen, focusing on input context optimization rather than joint reasoning. critical problem is that some multi-hop questions have more than one good reasoning paths, which requires high quality for sft data. Search-R1(Jin et al., 2025a), DeepResearcher(Zheng et al., 2025) and Graph-R1(Luo et al., 2025a) directly applies reinforcement learning on LLMs. Consequently, these approaches rely more heavily on the LLMs innate reasoning capabilities to solve the questions without preceding SFT stage. This may introduce redundant paths when LLM does not align with the 3 retrieval set. Our method applies Early Knowledge Alignment to alleviate this problem."
        },
        {
            "title": "3 Preliminaries\n3.1 PPO\nProximal Policy Optimization (PPO) (Schulman\net al., 2017b) is an actor-critic reinforcement learn-\ning algorithm that has become the predominant\nmethod for RL fine-tuning of large language mod-\nels (Ouyang et al., 2022). For language model\nfine-tuning, PPO maximizes the following objec-\ntive:",
            "content": "JP O(θ) = E[qP (Q),oπθold (Oq)] (1) (cid:88) 1 min (rt(θ)At, clip(rt(θ), 1 ϵ, 1 + ϵ)At) , t=1 (2) πθold where rt(θ) = πθ(otq,o<t) (otq,o<t) is the probability ratio between the current policy πθ and the old policy πθold. Here, and represent questions sampled from the dataset (Q) and corresponding outputs generated by the old policy, respectively. The clipping parameter ϵ constrains the policy ratio to the interval [1 ϵ, 1 + ϵ], preventing destabilizing updates. At denotes the advantage function, typically computed using Generalized Advantage Estimation (GAE)(Schulman et al., 2015) based on rewards and learned value function Vψ. 3.2 GRPO (Shao et al., 2024b) propose Group Relative Policy Optimization (GRPO), illustrated in Figure 2. GRPO eliminates the need for value function approximation by using the average reward of multiple sampled outputs as baseline. For each question q, GRPO samples group of outputs {o1, o2, . . . , oG} from the old policy πθold and optimizes the following objective: JGRPO(θ) = qP (Q), {oi}G i=1πθold (Oq) (cid:88) (cid:34)"
        },
        {
            "title": "1\nG",
            "content": "1 oi oi (cid:88) (cid:16) min (cid:0)rt(θ) ˆAi,t, t=1 i=1 clip(rt(θ), 1 ε, 1 + ε) ˆAi,t (cid:35) . β DKL(πθ πref) (cid:17) (cid:1) (3) where ri,t(θ) = πθ(oi,tq,oi,<t) (oi,tq,oi,<t) is the probability ratio, and ˆAi,t represents the advantage comπθold puted using relative rewards within each group: 4."
        },
        {
            "title": "Iterative Thinking and Searching",
            "content": "ˆAi,t = (cid:101)ri = ri mean(r) std(r) (4) where = {r1, r2, , rG} is the rewards tensor of samples in the group correspondingly. The group-relative advantage computation aligns naturally with how reward models are trainedon comparative datasets where outputs for the same question are ranked against each other."
        },
        {
            "title": "4 Method",
            "content": "We propose Early Knowledge Alignment (EKA), simple but effective module that enhances iterative RAG systems by incorporating early knowledge before the initial planning. Our method addresses the fundamental limitation of normal planning, in all existing iterative RAG systems where models begin reasoning without sufficient contextual grounding, often leading to suboptimal retrieval strategies and redundant exploration during reinforcement learning. Figure 2 illustrates the GRPO training pipeline of EKA. The policy LLM receives Early Knowledge P0 from the SearchEngine before its first thinking step. Subsequently, the model proceeds with the standard rollout and update phases as in conventional GRPO training. Algorithm is referred to Appendix A. Following the initial search, our method proceeds with iterative thinking and searching, now grounded by early knowledge, until final answer is generated. The action pipeline is set as [a0, a1, a2, ...at] where a0 is Search and at each subsequent step > 0, action ai is Search or Answer if ai1 = Think and Think if ai1! = Think. Each action is defined as: Think: Generate reasoning steps based on current knowledge. Search: Query the knowledge base for additional information. Answer: Provide the final answer when sufficient information is gathered. To guide the model in producing this sequence of actions, we employ the prompt detailed in Table 1, which instructs it to generate structured outputs."
        },
        {
            "title": "Template",
            "content": "Table 1: the updated prompt. for Note that early knowledge is provided within <knowledge>...</knowledge> at the beginning, and additional retrieved knowledge is placed within the same tags after </query>. Answer the given question. You can query from knowledge base provided to you to answer the question. You can query knowledge as many times as you want. The initial knowledge you need for the first think is between <knowledge>...</knowledge>. You must first conduct reasoning inside <think>...</think> relied on the initial knowledge. If you need to query knowledge, you can set query statement between <query>...</query> to query from knowledge base after <think>...</think>. When you have the final answer, you can output the answer inside <answer>...</answer>. Question: question. <knowledge>Knowledge</knowledge>. Assistant:"
        },
        {
            "title": "4.3 Theoretical Analysis",
            "content": "In this section we propose the following proposition: Proposition 1. Early Knowledge Alignment is better than traditional thinking in iterative RAG from an entropy perspective. Proof. The formal proof is provided in Appendix C, and the empirical results regarding entropy are presented in Section 6.1."
        },
        {
            "title": "5 Experiments\nWe choose two RAG methods based on reinforce-\nment learning as our backbone, Search-R1(Jin\net al., 2025a) and Graph-R1(Luo et al., 2025a),\naccompanied with two different dataset splitting,\nto show our method’s robustness across different\nmethods and retrieval set. In Search-R1 setting,\nmodels are trained in two IND (in-domain) datasets\n(HotpotQA and NQ) and other datasets are OOD\nIn Graph-R1\n(out-of-domain) datasets for test.",
            "content": "Figure 2: GRPO training with EKA. Given an input question q, our EKA approach first performs an initial retrieval step to gather relevant knowledge before generating the initial thinking step. Specifically, we retrieve the top-k most relevant passages from the knowledge corpus using retriever: P0 = Retrieve(q, D, k), (5) where P0 = {p1, p2, . . . , pk} represents the initially retrieved passages. 4 Table 2: Main results in Graph-R1 setting with best in bold. means prompt engineering, means training, means no knowledge interaction, means chunk-based knowledge, and means graph-based knowledge. Method 2Wiki. HotpotQA Musique NQ PopQA TriviaQA Avg. EM EM F1 EM F1 EM EM F1 EM F1 EM R-S 4.69 NaiveGeneration 7.03 StandardRAG 3.91 GraphRAG 3.13 LightRAG 3.91 PathRAG HippoRAG2 7.03 HyperGraphRAG 4.69 17.03 22.31 16.02 16.59 12.42 16.27 21.14 18.75 35.16 19.53 18.75 10.94 19.53 21.88 31.79 46.70 31.67 30.70 23.12 31.78 37.46 3.13 9.38 7.03 3.91 3.13 6.25 6. GPT-4o-mini 2.34 11.45 7.03 17.31 3.91 15.14 2.34 14.39 2.34 11.49 7.81 12.37 3.91 20.40 21.59 26.85 20.31 19.09 20.01 24.56 22.95 10.36 18.75 8.59 5.47 2.34 9.38 13.28 25.95 30.58 20.92 24.47 15.65 21.10 29.48 28.91 31.25 32.03 25.00 19.53 32.81 28.91 47.73 11.36 48.55 18.10 45.13 12.50 40.18 9.77 7.03 37.44 48.86 13.80 44.95 13. 25.92 32.05 24.87 24.24 20.02 25.82 29.40 - 52.68 32.48 47.42 46.71 36.41 61.82 Qwen2.5-7B-Instruct NaiveGeneration StandardRAG SFT R1 R1-Searcher Search-R1 Search-R1-PPO 18.58 21.10 27.59 37.05 46.36 51.26 60. 12.25 12.75 20.28 30.99 33.96 38.21 60.75 6.25 10.16 19.53 31.25 39.84 43.77 54.68 0.00 0.78 5.47 7.03 10.16 17.18 32.81 3.12 7.81 11.72 25.00 27.34 35.15 + EKA 56.25 4.06 4.53 10.02 14.53 16.63 21.45 41.54 +21.10 +22.54 +10.91 +9.18 +15.63 +20.09 32.91 39.84 + EKA 57.03 35.32 - 13.00 52.67 15.97 - 19.02 - 28.45 51.26 44.93 53.06 43.79 65.02 48.97 +5.18 +3.12 +4.14 +10.94 +8.76 +9.62 +11.64 +11.96 49.31 32.27 65.02 46.84 +17.19 +19.09 +4.68 +1.55 +9.38 +2.41 +14.84 +14.57 +10.14 +8.08 +1.56 +0.33 +9.63 +7.67 +15.71 60.40 49.87 55.47 + EKA 60.94 51.99 64.90 +5.47 +3.22 +2.35 +3.45 +3.91 +5.46 +4.69 +2.12 +3.90 +2.27 +0.78 +0.44 +3.51 +2.83 +4.50 3.12 24.51 24.90 5.34 39.21 15.57 57.33 25.91 64.76 34.51 61.03 38.29 69.79 47.91 1.56 1.56 5.12 16.41 32.03 38.34 34.37 -3.97 18.75 33.59 7.03 8.59 31.25 49.22 56.25 51.56 62.50 12.82 13.10 27.93 30.35 47.12 47.03 51.17 0.78 3.12 20.31 26.56 41.41 43.75 46.87 14.20 15.39 24.01 33.12 42.29 43.80 55. 69.29 37.76 69.62 47.39 71.93 48.57 72.37 52.08 44.26 52.34 60.15 61.71 46.23 53.90 33.59 38. 45.31 49.21 21.09 30.47 47.66 52.34 56.28 57.83 42.38 61.47 57.82 60. 46.17 51.63 63.28 64.06 51.22 53.49 39.08 49.22 62.69 66.14 36.72 40. 65.04 68.26 57.03 59.38 67.97 + EKA 70.31 60.65 53.81 65.13 56.02 +2.34 +1.66 +1.56 +1.95 +1.56 +0.34 +0.79 +2.21 +0.78 +0.73 +2.34 +1.41 +1.56 +1.38 +4.48 76.43 56.12 77.84 57.68 39.84 40. 57.54 57.88 49.22 50.00 53.33 54.06 68.75 71.09 64.85 66.23 43.75 45. 67.19 68.75 72.52 74.47 75.46 77.12 Qwen2.5-14B-Instruct Graph-R1 Graph-R Table 3: R-S comparison of EKA. 2Wiki HotpotQA Musique NQ PopQA TriviaQA Avg. Graph-R +EKA 55.24 60.69 56.27 60.36 52.95 61.54 69.25 72.86 61.55 64. 67.16 68.99 60.40 64.90 setting, models are trained within each dataset. Furthermore, comprehensive retrieval set with chunks using the full Wikipedia corpus (Fullwiki) is used in the Search-R1 setting, and smaller, dataset-specific structure-augmented retreival set is used in the Graph-R1 setting. We also run EKA on Search-R1 in the Graph-R1 setting with smaller, dataset-specific chunk-based retreival set. 5."
        },
        {
            "title": "Implementations",
            "content": "Baselines. In Graph-R1 setting, we follow the previous work, including training-free methods from Graph-R1: NaiveGeneration, StandardRAG(Lewis et al., 2020), GraphRAG(Edge et al., 2025), LightRAG(Guo et al., 2025), PathRAG(Chen et al., 2025), HippoRAG2(Gutiérrez et al., 2025), trainHyperGraphRAG(Luo et al., 2025b) ing:SFT(Zheng et al., 2024), R1(Shao et al., 2024a), R1-Searcher(Song et al., 2025) and Graph-R1(Luo et al., 2025a) itself, we cite their performances , for comparison if not specified. In the Search-R1 setting, additional baselines including CoT(Wei et al., 2022), IRCoT(Trivedi et al., 2022a), Searcho1(Li et al., 2025a), and Rejection Sampling(Ahn et al., 2024) is compared. Detailed description of these baselines are put in the Appendix D. We use Qwen2.5-7B-Instruct(Qwen et al., 2025) and Qwen2.5-14B-Instruct as LLM backbone for training. We also have done additional experiments on Qwen3(Yang et al., 2025) in Appendix B.1 and Section 5.4. Retriever. The retriever we used is highly dependent on the backbone. In Search-R1, the retriever is E5(Wang et al., 2022). In Graph-R1, the retriever is hypergraph-based retrieval with bge-large-env1.5(Chen et al., 2023). Datasets and Metrics. Due to the different dataset splitting protocols in Search-R1 and GraphR1, we conduct our experiments under both settings to ensure fair comparison. In Graph-R1 setting, we follow the original paper setting and use 6 common datasets(Jin et al., 2025b) for QA, including 2Wikihop(Ho et al., 2020), HotpotQA(Yang et al., 2018), Musique(Trivedi et al., 2022b), NQ(Kwiatkowski et al., 2019), PopQA(Mallen et al., 2023), TriviaQA(Joshi et al., 2017). Also in this setting we compare with Search-R1 baselines. We use EM, F1 and R-S to evaluate results. EM and F1 measures the answer and R-S measures the retrieval performances. In Search-R1 setting, we follow the original paper setting, appending one new dataset Bamboogle(Press et al., 2022), and using F1 score for comparison. Detailed information are referred to Appendix D."
        },
        {
            "title": "5.2 Comparison in Graph-R1 Setting",
            "content": "We show the results in Table 2. Note that Search-R1 uses PPO method in its paper but Graph-R1 runs GRPO in their experiments so we run the SearchR1-PPO by ourselves as the PPO variants in the table. We found that EKA improves the performance of Graph-R1 by an average of 3 F1 points, SearchR1 by an average of 11 F1 points and Search-R1PPO by an average of 7 F1 points, demonstrating substantial performance gain across different RL methods. Also, the improvement in R-S scores indicates that EKA can actually improve the exploitation in focusing retrieval necessary information. Then we analysis the R-S of EKA compared with Graph-R1 in Table 3. This suggests that EKAs performance gains are partially driven by improved retrieval quality."
        },
        {
            "title": "5.3 Comparison in Search-R1 Setting",
            "content": "In Search-R1 setting, we show the results of using Fullwiki as the retrieval set to show our methods robustness in retrieval set. As constructing full Wikipedia hypergraph in the manner of Graph-R1 is currently computationally prohibitive, we only use Search-R1 as our backbone. The results shows that EKA also can increase performances when the retrieval set is very large, and can show incremental performances in both IND and OOD datasets in Table 4. Notably, EKA improves the performance of Search-R1 by an average of 6.3 F1 points."
        },
        {
            "title": "5.4 Training-free EKA",
            "content": "To demonstrate versatility and scalability, we evaluate EKA as training-free inference module on larger models where RL fine-tuning is computationally prohibitive. By aligning with the retrieval set before reasoning, EKA consistently delivers substantial gains across benchmarks  (Table 5)  . These results confirm that \"plan failure\" from ungrounded thinking persists even in large-scale models, and EKA serves as robust, plug-and-play solution to mitigate hallucinations and enhance reasoning stability without parameter updates."
        },
        {
            "title": "6 Ablations",
            "content": "Experiments are done in the Graph-R1 setting in the ablation section, and we aim to answer the following three questions: Q1. Why Early Knowledge Alignment can make the performance better, from an entropy perspective. Q2. Can Early Knowledge shorten the number of thinking turns? And what is metrics dynamics in every step in the training? Q3. Will Early Knowledge Alignment in RL training downgrade the generalization of trained models?"
        },
        {
            "title": "6.1 Entropy Analysis",
            "content": "In RL training, the entropy demonstrates models exploration ability in training. However, in the context of multi-hop RAG, unconstrained exploration is not always beneficial, as the reasoning process must remain aligned with the information available in the retrieval set. EKA is designed precisely to provide this initial alignment. We show the comparison of Graph-R1s entropy of tokens between \"<answer>...</answer>\", \"<think>...</think>\", \"<query>...</query>\" with EKA or not in Figure 3. (a) (b) (c) Figure 3: Entropy comparison of backbone (GraphR1) and EKA. (a), (b), and (c) show average entropy of tokens between \"<answer>...</answer>\", \"<think>...</think>\", \"<query>...</query>\". We found that the entropy values for all action types are generally lower with EKA than without it. At zero step with the same LLM, the lower entropy of tokens between \"<answer>\" \"</answer>\" (which is actually the answer tokens) of EKA fits the intermediate conclusion in the proof in Appendix that Eπ (cid:2)I(A; HEKA Q)(cid:3) Eπ [I(A; HT Q)] , (6) 6 Table 4: Main results (F1 scores) compared in Search-R1 setting. The best performance is set in bold. / represents IND/OOD datasets. Icons have the same meaning as Table 2."
        },
        {
            "title": "General QA",
            "content": "Multi-Hop QA NQ TriviaQA PopQA HotpotQA 2Wiki. Musique Bamboogle Avg."
        },
        {
            "title": "Direct Inference\nCoT\nIRCoT\nStandard RAG",
            "content": "Search-o1 SFT R1-base R1-instruct Rejection Sampling Search-R1 +EKA 13.40 4.80 22.40 34.90 15.10 31.80 29.70 27.00 36.00 39.30 49.80 +10.50 Qwen2.5-7B-Instruct 18.30 9.20 13.30 29.90 14.00 5.40 30.10 39. 13.10 12.10 20.20 19.90 38.00 39.70 48.90 +9.20 18.70 21.70 24.20 23.70 33.10 37.00 45.70 +8.70 40.80 18.50 47.80 58.50 44.30 35.40 53.90 53.70 59.20 61.00 66.10 +5.10 25.00 11.10 14.90 23.50 17.60 25.90 27.30 29.20 29.60 41.40 42.70 +1. 3.10 2.20 7.20 5.80 5.80 6.60 8.30 7.20 12.30 14.60 20.90 +6.30 12.00 23.20 22.40 20.80 29.60 11.20 29.60 29.30 35.50 36.80 39.50 +2.70 18.10 10.60 23.90 30.40 20.60 20.70 27.60 27.10 34.80 38.50 44.80 +6. Table 5: Performance (F1 Score) of EKA as trainingfree inference strategy on large-scale models. EKA consistently improves performance across all datasets without any parameter updates. Model 2Wiki HotpotQA Musique NQ PopQA TriviaQA Qwen2.5-32B-Instruct + EKA 13.73 18. Qwen3-235-A30B-Instruct 30.56 38.39 + EKA 23.96 26.14 37.80 48.82 8.29 13.04 19.93 28. 11.62 15.63 21.49 24.68 15.19 17.08 28.73 33.61 23.65 27.84 38.55 44. Next, we show the F1 and R-S scores in the training step in Figure 4. We found that with EKA, our models RS is high from the beginning. Even when we exclude the early knowledge in computing the metrics, the R-S score of backbone with EKA can still increase to higher value than the model without EKA. which predicts the lower entropy of EKA answer tokens. Although there is single training step where the answer entropy for EKA is momentarily higher, the overarching trend shows that EKA consistently leads to lower answer token entropy. Besides, the lower entropy of think and search tokens show that LLM with EKA has more determined exploration direction in thinking and searching, which is exactly what we assume in the beginning."
        },
        {
            "title": "6.2 Shorter turns and Metrics Dynamics.",
            "content": "We show that with EKA, the exploration turns of LLMs shrinks about one turn on average in Table 6. Shorter turns means less noise in the retrieval that can make LLM more focus on the right information. Table 6: Average turns of Graph-R1 with or without EKA. 2Wiki HotpotQA Musique NQ PopQA TriviaQA Avg. Graph-R1 +EKA 3.12 2.72 3.12 2.80 3.88 2.68 3.06 1.52 3.53 1.91 2.82 1. 3.26 2.22 (a) (b) (c) Figure 4: F1 and R-S scores per training step on the 2Wiki dataset. (a) F1 score. (b) R-S score. (c) R-S score excluding the early knowledge."
        },
        {
            "title": "6.3.1 Generalization across datasets",
            "content": "While the generalization performance on OOD datasets using the Search-R1 backbone was presented in Table 4, this section evaluates the generalization of EKA with the Graph-R1 backbone. The results show that our method not only achieves better results in IID conditions but also show better generalization results on average than without EKA. 7 Table 7: Generalization test on backbone and EKA. The row datasets are training datasets and the column datasets are test datasets."
        },
        {
            "title": "Train Datasets",
            "content": "2Wiki. HotpotQA Musique NQ"
        },
        {
            "title": "PopQA TriviaQA",
            "content": "Avg. 2Wiki. +EKA + HotpotQA +EKA +"
        },
        {
            "title": "Musique",
            "content": "+EKA + NQ +EKA +"
        },
        {
            "title": "PopQA",
            "content": "+EKA +"
        },
        {
            "title": "TriviaQA",
            "content": "+EKA + 65.04 68.26 +3.22 58.27 60.86 +2.59 43.87 54.90 +11.03 52.13 54.77 +2.64 47.41 48.51 +1. 46.83 52.17 +5.34 59.92 63.90 +3.98 62.69 66.14 +3.45 52.32 59.99 +7.67 53.19 55.83 +2.64 58.45 57.52 -0. 53.82 55.18 +1.36 35.92 44.53 +8.61 33.27 38.87 +5.60 46.17 51.63 +5.46 34.57 37.75 +3.18 35.99 34.66 -1. 22.87 31.31 +8.44 45.24 46.89 +1.65 37.89 45.14 +7.25 43.66 47.63 +3.97 49.87 51.99 +2.12 43.40 43.88 +0. 41.66 44.87 +3.21 42.57 50.78 +8.21 44.30 47.60 +3.30 44.76 48.98 +4.22 43.10 48.72 +5.62 51.22 53.49 +2. 44.71 47.23 +2.52 63.38 65.53 +2.15 57.20 66.96 +9.76 64.45 69.82 +5.37 63.74 67.38 +3.64 68.91 69.98 +1. 71.93 72.37 +0.44 52.01 56.65 +4.64 48.94 54.26 +5.32 49.21 55.49 +6.28 49.43 52.74 +3.31 50.90 51.34 +0. 46.97 50.52 +3."
        },
        {
            "title": "6.3.2 Mismatched Early Knowledge\nWe further investigate the robustness of Early\nKnowledge Alignment (EKA) against variations\nin the quality and source of the early knowledge\nP0.",
            "content": "Noisy Early Knowledge. In real-world scenarios, the Early Knowledge P0 may contain irrelevant information or noise. To simulate this, we conduct experiments using the full Wikipedia corpus as the retrieval source for the initial step (denoted as EKA-wiki), which introduces significantly more noise compared to the dataset-specific retrieval sets. As shown in Table 8, although the introduction of noise in EKA-wiki leads to slight performance drop compared to the standard EKA, it still consistently outperforms the baseline without EKA in average. This demonstrates that the benefit of EKA comes from the grounding effect of the early knowledge, which remains effective even when it is imperfect. Table 8: Performance(F1 Score) comparison with noisy early knowledge. Method 2Wiki HotpotQA Musique NQ PopQA TriviaQA Qwen2.5-7B-Instruct + EKA (Standard) + EKA-wiki (Noisy) 65.04 68.26 66. 62.69 66.14 62.91 46.17 51.63 47.16 49.87 51.99 50.43 51.22 53.49 53.98 71.93 72.37 71.77 Mismatched Retriever. To verify that our improvements are not dependent on specific retrieval model, we evaluate EKA using different dense retrievers. We compare the default BGE retriever (EKA-bge) with the E5 retriever (EKA-e5). Table 9 presents the results across six datasets. We observe that EKA yields consistent performance gains regardless of the retriever used, confirming that the EKA framework is retriever-agnostic and generalizes well across different semantic embedding spaces. Table 9: Ablation study on retriever quality. Method 2Wiki HotpotQA Musique NQ PopQA TriviaQA Qwen2.5-7B-Instruct EKA-bge (Standard) EKA-e5 65.04 68.26 68. 62.69 66.14 64.74 46.17 51.63 54.27 49.87 51.99 50.74 51.22 53.49 53.46 71.93 72.37 72."
        },
        {
            "title": "7 Conclusion\nAll in all, we propose an easy but effective mod-\nule in iterative RAG pipeline called Early Knowl-\nedge Alignment (EKA) that can guide right di-\nrections of thinking, resulting in more efficient\nexploration in RL training and better end-to-end\nperformances. Our comprehensive experiments\nrigorously validate the efficacy and robustness of\nEKA. The approach delivers substantial perfor-\nmance gains to state-of-the-art RL-based frame-\nworks, including Search-R1 and Graph-R1, across\ndiverse RL algorithms (PPO and GRPO) and varied\nretrieval contexts—from small, structured corpora\nto large-scale, unstructured document sets. In addi-\ntion, EKA consistently maintains or even improves\nupon the generalization capabilities of the back-\nbone models, showcasing its reliability. Crucially,",
            "content": "8 we also demonstrate EKAs scalability as plugand-play, training-free module for large models. This motivates us the shift of designing advanced RAG systems: from plan-first model to the early knowledge alignment process."
        },
        {
            "title": "8 Limitations",
            "content": "While Early Knowledge Alignment achieves performances in multi-hop QA, whether it works in much more complex Deepresearch scenerios remains undiscovered."
        },
        {
            "title": "9 Reproducibility Statement",
            "content": "We present detailed training algorithm in Appendix A, technical proofs in Appendix C, and additional experimental/implementation details in Appendix D. Additionally, code for our model is uploaded as supplemental materials with the submission."
        },
        {
            "title": "References",
            "content": "Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157. Boyu Chen, Zirui Guo, Zidan Yang, Yuluo Chen, Junze Chen, Zhenghao Liu, Chuan Shi, and Cheng Yang. 2025. Pathrag: Pruning graph-based retrieval augmented generation with relational paths. Preprint, arXiv:2502.14902. Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. 2023. Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation. Preprint, arXiv:2309.07597. Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, and 1 others. 2025. The entropy mechanism of reinforcement learning arXiv preprint for reasoning language models. arXiv:2505.22617. Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, Dasha Metropolitansky, Robert Osazuwa Ness, and Jonathan Larson. 2025. From local to global: graph rag approach to query-focused summarization. Preprint, arXiv:2404.16130. Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations. In Empirical Methods in Natural Language Processing (EMNLP). Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. 2025. Deeprag: Thinking to retrieve step by step for large language models. arXiv preprint arXiv:2502.01142. Zirui Guo, Lianghao Xia, Yanhua Yu, Tu Ao, and Chao Huang. 2025. Lightrag: Simple and fast retrievalaugmented generation. Preprint, arXiv:2410.05779. Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, and Yu Su. 2025. From rag to memory: Non-parametric continual learning for large language models. Preprint, arXiv:2502.14802. Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing multihop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th International Conference on Computational Linguistics, pages 66096625, Barcelona, Spain (Online). International Committee on Computational Linguistics. Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. 2025. s3: You dont need that much data to train search agent via rl. arXiv preprint arXiv:2505.14146. Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023a. Active retrieval augmented generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 79697992. Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023b. Active retrieval augmented generation. Preprint, arXiv:2305.06983. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025a. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. Preprint, arXiv:2503.09516. Jiajie Jin, Yutao Zhu, Zhicheng Dou, Guanting Dong, Xinyu Yang, Chenghao Zhang, Tong Zhao, Zhao Yang, and Ji-Rong Wen. 2025b. Flashrag: modular toolkit for efficient retrieval-augmented generation research. In Companion Proceedings of the ACM on Web Conference 2025, WWW 25, page 737740, New York, NY, USA. Association for Computing Machinery. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 16011611, Vancouver, Canada. Association for Computational Linguistics. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452466. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for knowledgeIn Advances in Neural Inforintensive nlp tasks. mation Processing Systems, volume 33, pages 9459 9474. Curran Associates, Inc. Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, and Xipeng Qiu. 2023. Unified demonstration retriever for incontext learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 46444668, Toronto, Canada. Association for Computational Linguistics. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic searchenhanced large reasoning models. arXiv preprint arXiv:2501.05366. Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, and Xipeng Qiu. 2025b. R3-rag: Learning step-bystep reasoning and retrieval for llms via reinforcement learning. arXiv preprint arXiv:2505.23794. Haoran Luo, Guanting Chen, Qika Lin, Yikai Guo, Fangzhi Xu, Zemin Kuang, Meina Song, Xiaobao Wu, Yifan Zhu, Luu Anh Tuan, and 1 others. 2025a. Graph-r1: Towards agentic graphrag framework via end-to-end reinforcement learning. arXiv preprint arXiv:2507.21892. Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, Qika Lin, Yu Feng, Zemin Kuang, Meina Song, Yifan Zhu, and Luu Anh Tuan. 2025b. Hypergraphrag: Retrieval-augmented generation via hypergraph-structured knowledge representation. Preprint, arXiv:2503.21322. Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. When not to trust language models: Investigating effectiveness of parametric and non-parametric memories. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 98029822, Toronto, Canada. Association for Computational Linguistics. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and 1 others. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, and 1 others. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350. Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. Preprint, arXiv:2210.03350. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. 2015. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438. John Schulman, Filip Wolski, Prafulla Dhariwal, ProxPreprint, Alec Radford, and Oleg Klimov. 2017a. imal policy optimization algorithms. arXiv:1707.06347. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Proximal arXiv preprint Radford, and Oleg Klimov. 2017b. policy optimization algorithms. arXiv:1707.06347. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. Preprint, arXiv:2305.15294. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024a. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, and 1 others. 2024b. Deepseekmath: Pushing the limits of mathematical reasonarXiv preprint ing in open language models. arXiv:2402.03300. 10 System Demonstrations), pages 400410, Bangkok, Thailand. Association for Computational Linguistics. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025. DeepResearcher: Scaling deep research via reinforcement learning in real-world environments. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 414431, Suzhou, China. Association for Computational Linguistics. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and JiRong Wen. 2025. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. Preprint, arXiv:2503.05592. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022a. Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions. arXiv preprint arXiv:2212.10509. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022b. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 10:539554. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weaklysupervised contrastive pre-training. arXiv preprint arXiv:2212.03533. Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, and 1 others. 2025. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, and 1 others. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824 24837. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 23 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23692380, Brussels, Belgium. Association for Computational Linguistics. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR). Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. 2024. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3:"
        },
        {
            "title": "A Algorithm",
            "content": "Algorithm 1 Early Knowledge Alignment Require: Input x, LLM πθ, Retrieval set R, Max turns B. Ensure: Output y. 1: Initialize 2: Initialize 0 3: Initialize Searching Knowledge P0 = R(x) and update + P0 4: while < do 5: Rollout yb while True do Generating yt πθ( x, + yb) concatenate token yb yb + yt if yt in [</query>, </answer>, <eos>] then break end if end while + yb if extract <query> </query> from yb then Extract Parse(yb, <query>, </query>) Retrive knowledge = R(q) Continue rollout + </knowledge>d</knowledge> else if extract </answer> from yb then return 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: end if count turns + 1 20: 21: end while 22: return y"
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 Qwen3 Model Results We show the Qwen3-4B-Instruct-2507 models performances in the training step in Figure B.1. It is shown that even bad results, EKA can still improve Qwen3 performances. We check the output of Qwen3 and find that the reason is that Qwen3 instruction models have used \"think\" token in its pre-train so when they have removed think pattern in 2507 model, its hard for the model to generate the thinking process in the pipeline, resulting in low performances. B.2 Case Study In this section, we show classical example of why Early Knowledge Alignment is useful. In Graph-R1, when the model lacks planning ability to split the question into two parts, it will generate useless searching for both two things in turns and turns. As shown in Table 10, it fails to retrieve the directors. While as shown in Table 11 the model with EKA knows searching for two things is useless, then it will split the question and search for two directors separately and finally retrieve the right documents, resulting in the right answer."
        },
        {
            "title": "C Theoretical Proof",
            "content": "Proposition 1. Early Knowledge Alignment is better than traditional thinking in iterative RAG from an entropy perspective. Proof. Given the condition of iterative RAG for an LLM π divides the budget across rounds as = (cid:80)T t=1 Bt. At each round >= 1, we denote Pt as the retrieval results at this step, and the prior 12 Figure 5: Qwen3-4B-Instruct-2507 models F1 score in each step in 2Wiki dataset. Backbone is Graph-R1. evidence Ht1 = {P1, . . . , Pt1}. The LLM uses Ht1 to update its internal belief ht1 and selects new evidence Pt of size Bt by actively exploring the graph based on current uncertainty. The updated belief ht is obtained via Bayesian inference, and the entire process forms dynamic system: ht = (ht1, Pt, RG). (7) To evaluate retrieval progress, we define Lyapunov-style potential function Vt = H(A Q, Ht), which quantifies the remaining uncertainty after round t. Each retrieval step reduces entropy by: Vt1 Vt = I(A; Pt Q, Ht1), We focus on the first step of iterative RAG that = 1. The entropy reduction for the first step is V0 V1 = I(A; P1 Q, H0). (8) (9) In Model-Initialized thinking, H0 = {} while in our Early Knowledge Alignment, the H0 = {P0}. Thus Summing over all rounds, the total information gain of the adaptive strategy satisfies: Eπ (cid:2)I(A; HEKA Q)(cid:3) = Eπ Eπ I(A; EKA (cid:35) Q, HEKA t1 ) (cid:35) I(A; Pt Q, Ht1) (cid:34) (cid:88) t=1 (cid:34) (cid:88) t=1 = Eπ [I(A; HT Q)] , (10) (11) (12) while the unequality comes from the fact that with H0 = {P0}, which is highly related to Q, at each step >= 1, I(A; EKA Q, HEKA t1 ) I(A; Pt Q, Ht1), which means the EKA is no worse than the traditional thinking. Let ρT denote the information gain per token at the end of the iterative operation: ρT = I(A; HT Q) , 13 (13) (14) Table 10: case study of Graph-R1. Question: Which film has the director born later, ILl Tell The World or Saranggola? Ground Truth: Saranggola Graph-R1: <think> To answer this question, need to find information about the birth years of the directors of the movies ILl Tell The World and Saranggola, then compare them. </think> <query> SELECT directorBirthYear WHERE movieName = \"ILl Tell The World\" OR movieName = \"Saranggola\" </query> <knowledge> Invalid tool call format. Please use <query>{ \"query\": \"statement\" }</query> format. </knowledge> <think> The previous query did not work as expected, so Ill try different approach. First, need to get the birth years of the directors of both movies. </think> <query> SELECT directorBirthYear WHERE movieName = \"ILl Tell The World\" OR movieName = \"Saranggola\" </query> <knowledge> Invalid tool call format. Please use <query>{ \"query\": \"statement\" }</query> format. </knowledge> <think> Given the error in my previous attempts, Ill directly query for the birth years of the directors of ILl Tell The World and Saranggola, and then compare them. </think> <query> { \"query\": \"SELECT directorBirthYear WHERE movieName = ILl Tell The World OR movieName = Saranggola\" } </query> <knowledge> { \"results\": [ { \"<knowledge>\": \"Lena Dunham (born May 13, 1986) is an American actress, writer, director, and producer.\", \"<coherence>\": 1.0 }, { \"<knowledge>\": \"Priyadarshan (born January 30, 1957) is an Indian film director, producer, and screenwriter.\", \"<coherence>\": 0.508 }, { \"<knowledge>\": \"V. S. Raghavan (director)\", \"<coherence>\": 0.333 } ] } </knowledge> <think> found the birth years of the directors of both movies. Lena Dunham is the director of ILl Tell The World and was born in 1986. While V. S. Raghavan is the director of Saranggola and was born in 1935. Since Lena Dunhams birth year is later than V. S. Raghavan, the film ILl Tell The World has the director born later. </think> <answer> ILl Tell The World </answer> From Bayesian viewpoint, retrieval efficiency can be seen as how much uncertainty is reduced per token. EKA achieves greater entropy reduction under the same budget, or requires fewer tokens to reach the same posterior certainty, it is strictly more efficient. Moreover, by Fanos inequality, Pe H(A Q) I(A; HT Q) + 1 log , (15) we conclude that the lower the conditional entropy, the lower the expected error. Therefore, greater mutual information directly translates into improved answer accuracy. In conclusion, Early Knowledge Alignment enables the agent to get more information gain and lower entropy at the end of iterative RAG, leading to more efficient and accurate question answering."
        },
        {
            "title": "D Detailed Implementations and Hyperparameters",
            "content": "D.1 Baselines in Graph-R1 Setting Baselines in Graph-R1 setting first utilizes GPT-4o-mini as the inference-only generator. This includes NaiveGeneration, which performs zero-shot generation without retrieval to evaluate the base models capacity, and StandardRAG (Lewis et al., 2020), conventional chunk-based retrieval-augmented generation approach. We also include several graph-based retrieval methods: GraphRAG (Edge et al., 2025), which constructs entity graphs for one-shot retrieval; LightRAG (Guo et al., 2025), lightweight variant that builds compact graphs for more efficient retrieval; PathRAG (Chen et al., 2025), which performs retrieval via path-based pruning on entity graphs; HippoRAG2 (Gutiérrez et al., 2025), which employs hierarchical path planner over knowledge graphs to improve retrieval efficiency; and HyperGraphRAG (Luo et al., 2025b), which constructs n-ary relational hypergraphs to support single retrieval step. The second set of baselines is based on the Qwen2.5-Instruct (7B) model. We begin with foundational methods, including NaiveGeneration approach as lower-bound, the classic StandardRAG (Lewis et al., 2020) pipeline, and SFT (Zheng et al., 2024), which involves supervised fine-tuning on QA pairs. Furthermore, we evaluate several advanced methods trained with reinforcement learning: R1 (Shao et al., 2024a), GRPO-trained policy that generates answers directly without retrieval; Search-R1 (Jin et al., 14 Table 11: case study of Graph-R1+EKA. Question: Which film has the director born later, Ill Tell The World or Saranggola? Ground Truth: Saranggola Graph-R1+EKA: <think> From the knowledge provided, Ill Tell The World is 1945 American comedy film directed by Leslie Goodwins. Saranggola is 1999 Filipino drama film directed by Gil Portes. To determine which film has the director born later, will need to find the birth years of Leslie Goodwins and Gil Portes. </think> <query> { type: search, query: Leslie Goodwins birth year } </query> <knowledge> {results: [{<knowledge>: They Met in Argentina is 1941 American film directed by Leslie Goodwins and Jack Hively for RKO Pictures., <coherence>: 1.7}, {<knowledge>: Leslie Goodwins, <coherence>: 1.0}, {<knowledge>: Leslie Goodwins (17 September 1899 8 January 1969) was an English film director and screenwriter., <coherence>: 0.819}, {<knowledge>: With Love and Kisses is 1936 American musical comedy film directed by Leslie Goodwins and starring Pinky Tomlin, Toby Wing and Kane Richmond., <coherence>: 0.643}, {<knowledge>: Ill Tell the World is 1945 American comedy film directed by Leslie Goodwins and written by Henry Blankfort and Lester Pine., <coherence>: 0.333}, {<knowledge>: The Mexican Spitfires Baby is 1941 American comedy film directed by Leslie Goodwins., <coherence>: 0.25}, {<knowledge>: Leslie Goodwins is acknowledged but lacks detailed background in the provided text., <coherence>: 0.2}, {<knowledge>: Murder in the Blue Room is 1944 American film directed by Leslie Goodwins. It is the second remake of the 1933 American Pre-Code murder-mystery film Secret of the Blue Room., <coherence>: 0.167}]} </knowledge> <think> From the knowledge provided, Leslie Goodwins was born on September 17, 1899. Now need to find the birth year of Gil Portes to determine which film has the director born later. </think> <query> {type: search, query: Gil Portes birth year} </query> <knowledge> {results: [{<knowledge>: Gil M. Portes (September 13, 1945 May 24, 2017) was Filipino film director, film producer and screenwriter., <coherence>: 1.125}, {<knowledge>: Saturday Night (1950 film) is 1950 Spanish drama film directed by Rafael Gil., <coherence>: 1.0}, {<knowledge>: The Man Who Wanted to Kill Himself (1970 film), <coherence>: 0.5}, {<knowledge>: Carlos Gardel (born Charles Romuald Gardès; 11 December 1890 24 June 1935) was French Argentine singer, songwriter, composer and actor., <coherence>: 0.5}, {<knowledge>: Rafael Gil was Spanish film director and screenwriter., <coherence>: 0.4}, {<knowledge>: The Autonomines is Spanish comedy film directed by Rafael Gil and starring Alfredo Landa, María Casanova, and Manolo Codeso., <coherence>: 0.333}, {<knowledge>: Gil was prominent director of the Franco era., <coherence>: 0.333}, {<knowledge>: El canto del gallo is 1955 Spanish drama film directed by Rafael Gil., <coherence>: 0.25}, {<knowledge>: Gils film La noche del sábado was nominated for the Gold Lion at the 1950 Venice Film Festival., <coherence>: 0.25}, {<knowledge>: Saranggola (international title: The Kite) is 1999 Filipino drama film directed by Gil Portes, starring Ricky Davao, Lester Llansang and Jennifer Sevilla., <coherence>: 0.167}]} </knowledge> <think> From the knowledge provided, Leslie Goodwins was born on September 17, 1899. Gil Portes was born on September 13, 1945. Now can compare the birth years to determine which film has the director born later. </think> <answer> Saranggola </answer> 2025a), multi-turn chunk-based retrieval method trained with GRPO; R1-Searcher (Song et al., 2025), two-stage GRPO-based method for chunk-based retrieval; and Graph-R1(Luo et al., 2025a), an agentic GraphRAG framework enhanced by end-to-end reinforcement learning. D.2 Baselines In Search-R1 Setting In Search-R1 setting, despite the baselines in last section, we also compare against prominent reasoning and generation strategies: CoT (Wei et al., 2022): reasoning with chain of thought; IRCoT(Trivedi et al., 2022a): reasoning with chain of thought with retreival; Search-o1(Li et al., 2025a): integrating an agentic search workflow into the reasoning process; and Rejection Sampling(Ahn et al., 2024): SFT on trajectories that succeed. D.3 Metrics Exact Match (EM). This metric provides strict evaluation of answer accuracy. It determines if the generated answer yi is identical to the ground-truth reference after both have undergone normalization process. This process typically includes lowercasing, removing punctuation, and standardizing whitespace. The score is 1 if they match perfectly, and 0 otherwise. The final EM score is the average over all samples: EM ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 {norm(yi) = norm(y )} . (16) F1 Score. Unlike the all-or-nothing EM, the F1 score offers more nuanced measure of quality by assessing the word-level (token) overlap between the prediction and the ground truth. It calculates the harmonic mean of precision (the fraction of predicted tokens that are correct) and recall (the fraction of ground-truth tokens that are predicted), providing balanced assessment of token accuracy: F1 ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 2 tokens(yi) tokens(y ) tokens(yi) + tokens(y ) . (17) Retrieval Similarity (R-S). This metric evaluates the quality of the retrieval component of the RAG system, rather than the final generated answer. It measures the semantic relevance of the retrieved context k(i) gold. To do this, both texts are converted into vector representations using semantic embedding function Enc(), and their cosine similarity is computed: retr compared to the ideal \"gold\" context k(i) R-S ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:16) cos Enc(k(i) retr), Enc(k(i) gold) (cid:17) . (18) D.4 Hyperparameters We show in Table 12 the hyperparameters in Graph-R1 setting. In Search-R1 setting, the hyperparameters are the same as Search-R1. The models with EKA share the same hyperparameters with the backbone method. Method Backbone Batch Size Max Length Top-K Algo Epochs NaiveGeneration StandardRAG GraphRAG LightRAG PathRAG HippoRAG2 HyperGraphRAG SFT R1 Search-R1 Search-R1-PPO R1-Searcher Graph-R1 Qwen2.5 / GPT-4o-mini Qwen2.5 / GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini GPT-4o-mini Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) Qwen2.5 (7B) 16 128 128 128 128 128 4096 4096 4096 4096 4096 4096 N/A 5 Chunks 60 60 60 60 60 N/A N/A 5 Chunks / Turn 5 Chunks / Turn 5 Chunks / Turn 5 Chunks / Turn LoRA GRPO GRPO PPO GRPO GRPO 3 3 6 10 3 3 Table 12: Hyperparameter settings in Graph-R1 setting."
        }
    ],
    "affiliations": [
        "Computer Science, Fudan University",
        "Institute of Modern Languages and Linguistics, Fudan University",
        "Shanghai SII"
    ]
}