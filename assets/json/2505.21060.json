{
    "paper_title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles",
    "authors": [
        "Peng Wang",
        "Xiang Liu",
        "Peidong Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stylizing 3D scenes instantly while maintaining multi-view consistency and faithfully resembling a style image remains a significant challenge. Current state-of-the-art 3D stylization methods typically involve computationally intensive test-time optimization to transfer artistic features into a pretrained 3D representation, often requiring dense posed input images. In contrast, leveraging recent advances in feed-forward reconstruction models, we demonstrate a novel approach to achieve direct 3D stylization in less than a second using unposed sparse-view scene images and an arbitrary style image. To address the inherent decoupling between reconstruction and stylization, we introduce a branched architecture that separates structure modeling and appearance shading, effectively preventing stylistic transfer from distorting the underlying 3D scene structure. Furthermore, we adapt an identity loss to facilitate pre-training our stylization model through the novel view synthesis task. This strategy also allows our model to retain its original reconstruction capabilities while being fine-tuned for stylization. Comprehensive evaluations, using both in-domain and out-of-domain datasets, demonstrate that our approach produces high-quality stylized 3D content that achieve a superior blend of style and scene appearance, while also outperforming existing methods in terms of multi-view consistency and efficiency."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 0 6 0 1 2 . 5 0 5 2 : r Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles Peng Wang1,2 Xiang Liu2 Peidong Liu2 2 Westlake University 1 Zhejiang University {wangpeng,liupeidong}@westlake.edu.cn, liuxiangnick@gmail.com Project Page Figure 1: Styl3R. Given unposed sparse-view images and an arbitrary style image, our method predicts stylized 3D Gaussians in less than second using feed-forward network."
        },
        {
            "title": "Abstract",
            "content": "Stylizing 3D scenes instantly while maintaining multi-view consistency and faithfully resembling style image remains significant challenge. Current state-of-theart 3D stylization methods typically involve computationally intensive test-time optimization to transfer artistic features into pretrained 3D representation, often requiring dense posed input images. In contrast, leveraging recent advances in feedforward reconstruction models, we demonstrate novel approach to achieve direct 3D stylization in less than second using unposed sparse-view scene images and an arbitrary style image. To address the inherent decoupling between reconstruction and stylization, we introduce branched architecture that separates structure modeling and appearance shading, effectively preventing stylistic transfer from distorting the underlying 3D scene structure. Furthermore, we adapt an identity loss to facilitate pre-training our stylization model through the novel view synthesis task. This strategy also allows our model to retain its original reconstruction capabilities while being fine-tuned for stylization. Comprehensive evaluations, using both in-domain and out-of-domain datasets, demonstrate that our approach produces high-quality stylized 3D content that achieve superior blend of style and scene appearance, while also outperforming existing methods in terms of multi-view consistency and efficiency. Equal Contribution; Corresponding author. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in 3D reconstruction from 2D images [14, 26, 46] have made it increasingly practical to generate high-quality 3D scenes from casual captures. However, integrating artistic features such as the visual style of reference image into these reconstructions remains technically challenging. This difficulty arises from the fundamental mismatch between style transfer and 3D reconstruction: altering the visual appearance to reflect desired style often conflicts with the need to preserve multi-view consistency and structural coherence in the reconstructed scene. Creating stylized 3D scenes typically requires professional expertise, artistic creativity, and substantial manual effort. While 2D image style transfer [8, 11, 19] has achieved impressive efficiency and visual fidelity, naïvely extending these techniques to 3D often results in view-inconsistent stylization. In contrast, recent 3D stylization methods [7, 22, 23, 51] offer improved appearance consistency but rely on dense multi-view inputs, known camera poses, and time-consuming per-scene or per-style optimization. These requirements render them impractical for casual users or time constrained applications. This leads to central question: How can we achieve fast, multi-view consistent 3D stylization from few unposed content images and an arbitrary style image without requiring test-time optimization? To address this challenge, we propose Styl3R, feed-forward network that jointly reconstructs and stylizes 3D scenes from sparse, unposed content images and an arbitrary style image. Our method flexibly handles 2 to 8 input content images, requiring no test-time optimization, no camera supervision, and no sceneor style-specific fine-tuning, making it both practical and accessible for real-world usages. For clarity, we summarize the key differences between prior methods and ours in Table 1. In particular, Styl3R adopts dual-branch architecture consisting of structure branch and an appearance branch. The structure branch predicts the structural parameters of 3D Gaussians from unposed content images by leveraging dense geometry prior. The appearance branch, responsible for determining the color of the 3D Gaussians, consists of transformer decoder layers that blend style features with content features from multiple viewpoints. This design enables the generation of photorealistic and stylized appearances while maintaining multi-view consistency. To further preserve photometric shading capabilities during stylization fine-tuning, we introduce an identity loss by randomly feeding content image into the appearance branch, encouraging the model to retain its ability to reconstruct the original appearance. We evaluate our method on both in-domain and out-of-domain datasets. It outperforms prior approaches in terms of both multi-view consistency and efficiency, which producing high-quality 3D stylized content in only 0.15 second. Our main contributions are: We introduce feed-forward network for 3D stylization that operates on sparse, unposed content images and an arbitrary style image, does not require test-time optimization, and generalizes well to out-of-domain inputs making it practical for interactive applications. We design dual-branch network architecture that decouples appearance and structure modeling, effectively enhancing the joint learning of novel view synthesis and 3D stylization. Our method achieves state-of-the-art zero-shot 3D stylization performance, surpassing existing zero-shot methods and approximate the efficacy of style-specific optimization techniques, as demonstrated through both quantitative metrics and qualitative results."
        },
        {
            "title": "2 Related Work",
            "content": "2D Style Transfer. The task of transferring the visual style of reference image onto another content image has been extensively studied. The seminal work [8] introduced neural style transfer by iteratively optimizing the stylized image to match the Gram matrices of VGG-extracted features [35] from both content and style images. Subsequent approaches, such as AdaIN [11], WCT [20], LST [19], and SANet [30], reformulated the problem into feed-forward setting by aligning the feature statistics of content and style. With the emergence of transformer-based architectures [5, 41], more recent methods like AdaAttN [24], StyleFormer [44], StyTr2 [4], S2WAT [49], and Master [39] 2 Method 2D Methods [4, 11, 24] 3D StyleRF [22] StyleGaussian [23] ARF [23] Styl3R (Ours) Sparse View - Scene Style Zero-shot Zero-shot Consistency View Pose Free - Fast Inference Table 1: Comparison with existing style transfer methods. 2D methods can instantly stylize images after training without any additional tuning, but they fail to ensure multi-view consistency. Prior 3D methods ensure consistency but rely on dense posed inputs and per-scene or per-style tuning, slowing inference. Our method combines both strengths, achieving view-consistent stylization in under second without further tuning. leverage attention mechanisms to enhance feature representation, moving beyond the limitations of intermediate features extracted from pretrained VGG networks [35]. Despite progress in visual quality and efficiency, 2D style transfer methods lack geometric awareness and multi-view consistency, often causing artifacts when naïvely extended to 3D. In contrast, our method achieves geometry-consistent, view-coherent stylization with fast inference and rendering. 3D Style Transfer. 3D stylization has been explored using various scene representations. Early methods leveraged explicit forms like meshes [9] and point clouds [10, 16, 27], enabling style transfer via differentiable rendering and geometric warping, but they struggled with complex scenes due to limited expressiveness. Recent work adopts implicit representations like NeRF [26] and 3D Gaussian Splatting (3DGS)[14]. StylizedNeRF[12] incorporates 2D stylizer with view-consistent losses; ARF [51] improves structural fidelity with feature matching; StyleRF [22] enables zero-shot stylization via feature-space transformation; INS [6] disentangles style and content with geometric regularization. For 3DGS, StyleGaussian [23] and [33] embed style features into Gaussian parameters; SGSST [7] introduces multi-scale losses for high-res output; InstantStyleGaussian [48] stylizes via dataset updates; StylizedGS [50] achieves controllable editing by disentangling content, style, and structure. However, most methods require dense views, known poses, and per-scene optimization, limiting their applicability in casual settings. Other recent efforts [29, 37] explore object-centric stylization using attention and diffusion models (e.g. Zero123++ [34]) but suffer from high inference times (30s per object). In contrast, our method does not directly rely on the output of external pretrained models and achieves stylization in under one second. Feed Forward Generalizable 3D Reconstruction. Though NeRF [26] and 3DGS [14] achieve high-quality view synthesis, they rely on dense, calibrated inputs and costly per-scene optimization, hindering their practical usages for time-constrained applications. To address this, generalizable reconstruction methods [2, 13, 42, 45, 47] have emerged, primarily leveraging geometric priors like cost volumes or epipolar constraints to aggregate multi-view information from sparse inputs. In parallel, feed-forward 3DGS pipelines [1, 3, 38] infer pixel-aligned Gaussians from image features for efficient and high-quality rendering. More recent models [36, 46] advance this direction by directly reconstructing 3D scenes from sparse, unposed images without the need for camera poses, highlighting the potential of learning-based pipelines to generalize in purely feed-forward manner. Building upon this line of work, we propose novel 3D stylization pipeline that disentangles structure and appearance, unifying 3D reconstruction and style transfer in single network."
        },
        {
            "title": "3 Method",
            "content": "i }N RHW 3, where and are the height Given set of sparse unposed images = {Ic i=1 (Ic and width of image, superscript represents content) capturing scene along with an arbitrary style image Is RHW 3 (superscript represents style), our task is to instantly get the stylized 3D }N HW reconstruction of the scene represented by set of pixel-aligned Gaussians Gs = {gs , j=1 without compromising multi-view consistency and the underlying scene structure. These Gaussians j)}N HW Gs are parameterized by {(µj, αj, rj, sj, cs are the Gausj=1 sians position, opacity, orientation, scale and stylized color. Alternatively, it is also able to predict the non-stylized Gaussians Gc, which share all the other attributes but have different set of colors {cc , where µj, αj, rj, sj and cs compared to Gs. j}N HW j=1 Figure 2: Overview of Styl3R. Our model consists of structure branch and an appearance branch that output different attributes of Gaussians. For the structure branch, sparse unposed images are encoded by shared content encoder, then content tokens of each image are separately fed into their structure decoders with information sharing between other views. Attributes that govern the structure of the scene are then regressed from structure heads. For the color branch, style image is encoded by the style encoder, then the output style tokens perform cross attention with content tokens from all viewpoints in the stylization decoder. Finally the color of Gaussians are predicted from these blended tokens output by this decoder, which compose all Gaussian parameters along with the output from structure branch. Apart from style image, the appearance branch can also accept content image which gives the Gaussians their original colors. Inspired by [4, 46], we propose dual-branch architecture that separates the network into structure building branch and an appearance shading branch. In the appearance branch, we employ stylization decoder that first performs global self-attention across content tokens from all views to ensure multiview consistency, then injects style tokens to perform cross-attention with content tokens without interfering the structure branch. An overview of the pipeline is shown in Fig. 2. In this section, we first introduce the structure branch that leverage dense geometry prior from DUSt3R [43] (Sec. 3.1). Then we illustrate the appearance branch that governs the color of output Gaussians (Sec. 3.2). Finally, we design training curriculum that facilitate the learning of stylization while effectively preserving geometry prior (Sec. 3.3). 3.1 Structure Branch In order to leverage the dense geometry prior from DUSt3R [43], we employ ViT-based encoderdecoder architecture to estimate the structure of the scene. set of sparse unposed images capturing the scene are first patchified and then encoded by shared ViT encoder to set of content tokens = {tc , where is the patch size. The encoded tokens from one view is then fed into ViT decoder which perform cross attention with concatenated tokens from all other views to ensure channeling of multi-view information as in [46]. From these output tokens of the decoder for each view, one DPT head [32] is employed to predict the center positions µj of Gaussians, another DPT head regresses other structural attributes: orientations rj, scales sj and opacities αj. k= k} 3.2 Appearance Branch To separate the colorization of Gaussians from the estimation of scene structure, we propose an appearance branch that ensures the subsequent stylization would not degrade the learned geometry prior in the structure branch. For ease of alignment between encoded content and style tokens, we leverage the same architecture as content image encoder for style image encoder, but with different set of learnable weights. This ViT-based style encoder receives an arbitrary style image Is and then outputs set of style tokens = {ts m=1 , which are then sent to the stylization decoder. m} Stylization Decoder. Within the stylization decoder, content tokens output by the content encoder are stylized by style tokens s. Initially, from multiple views are concatenated, and then 4 perform global self-attention to get ˆT ensuring multi-view consistency. Then these self-attended content tokens ˆT are used to generate queries while style tokens are used to generate keys and values for the cross attention that blends these two streams of information as shown in Eq. 1. cs = CrossAttention( ˆT cW Q,T sW K,T sW ) (1) where Q, and are the projection matrices to generate queries, keys and values for cross attention. After this blending, stylization decoder finally outputs set of stylized content tokens cs. Color Head. From these stylized content tokens cs, DPT head is used to predict the stylized for each Gaussian, which is adapted according to the given style image Is. These color color cs components {cs along with the other parameters regressed from the structure branch j)}N HW compose the complete set of attributes {(µj, αj, rj, sj, cs j=1 for stylized Gaussians Gs. j}N HW j=1 Content as Style. Worth noticing, content image Ic can also be viewed as special style image that maps the content to its original appearance. This naturally leads the stylization of appearance branch to normal photorealistic shading in novel view synthesis. Thus, inputting content image Ic to style branch will give us the non-stylized Gaussians Gc. This insight is applied to facilitate subsequent training. 3.3 Training Curriculum Notably, 3D stylization and reconstruction are not inherently well aligned, as optimizing for style loss may degrade the underlying 3D structure of the scene [7]. To address this, we adopt twostage training curriculum. In the first stage, the model is trained to accurately estimate the scene structure and perform standard photorealistic shading. After this stage, we proceed to stylization fine-tuning stage, during which the structure branch is frozen to ensure faithful preservation of the scene geometry. Novel View Synthesis Pre-training. At this stage, we train the whole model end-to-end for the novel view synthesis (NVS) by solely using photometric loss calculated between novel view images rendered from Gc and ground truth target RGB images as in Eq. 2. During NVS training, we randomly input one content image Ic to the appearance branch. This encourages the appearance branch to preserve the original scene color at this stage. After this pre-training phase, given set of sparsely unposed images, the structure branch can predict intricate 3D structure, while the appearance branch is capable of performing photorealistic shading for Gaussians, which lays solid foundation for the next stylization fine-tuning stage. Stylization Fine-tuning. Building upon the preceding novel view synthesis (NVS) pre-training, the model can primarily focus on learning to stylize the appearance of the Gaussians. In each forward pass, we input content images and style image Is to the network, which outputs the stylized Gaussians Gs to render stylized images at novel viewpoints. These images are used to calculate the losses in Eq. 2 to update the appearance branch. As mentioned in [7], optimizing all Gaussians parameters towards style loss can drastically degrade the structure of reconstructed scene, thus we only fine-tune the appeareance branch during this stage, and freeze the structure branch. In terms of loss functions, we first employ weighted combination of style and content losses. For the style loss, we measure the differences in mean and variance between novel view images rendered from Gs and the reference style image Is, computed over the relu1_1, relu2_1, relu3_1, and relu4_1 feature maps of VGG19 [35]. For the content loss, we compare the relu3_1 and relu4_1 feature map responses between images rendered from Gs and the corresponding ground-truth target RGB images. Empirically, we find that incorporating both relu3_1 and relu4_1 in the content loss more effectively preserves the structural fidelity of the original scene, compared to using single layer as commonly done in previous style transfer approaches [7, 51], as shown in Fig. 7. Besides, to preserve the models NVS ability during stylization fine-tuning, we adapt the identity loss from [30]. Apart from the style image Is, we also feed randomly selected content image Ic to the appearance branch to obtain the non-stylized Gaussians Gc. Similar to the first stage, we also minimize the photometric loss between novel view images rendered from Gc and ground truth target RGB images while optimizing style and content losses. Figure 3: Novel View Transfer Comparision on RE10K. Despite limited image overlap, our method generates stylized novel views that more faithfully capture style details while preserving the original scene structure. In comparison, StyleRF [22] and StyleGaussian [23] tend to produce over-smoothed results that deviate from the true color tone of the reference style. ARF [51] suffers from style overflow, leading to significant loss of content appearance. As 2D baseline, StyTr2 [4] operates directly on ground-truth novel views, but fails to retain fine structural details of the scene. Training Losses. The losses used in two training stages are summarized as below. = (cid:26)Lphoto(Gc), λLstyle(Gs) + Lcontent(Gs) + Lphoto(Gc), NVS pre-training stylization fine-tuning (2) where Lphoto is the photometric loss which is linear combination of MSE and LPIPS [52] loss with weights of 1 and 0.05, respectively, and λ = 10 is the weight for style loss. Progressive Multi-view Training To stabilize multi-view training, we first pre-train the model on the 2-view setting for the NVS task, which is then used to initialize the 4-view NVS training and subsequent stylization fine-tuning. Though trained with 4 input views, our model can flexibly handle 2 to 8 views during inference as shown in Fig. 8."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets. We use combination of RealEstate10K (RE10K) [53] and DL3DV [21] as our scene dataset, covering both indoor and outdoor videos with diverse camera motion patterns. For style supervision, we use WikiArt [31], and assign unique style image to each scene in the training and evaluation sets. This setup ensures that neither the test scenes nor styles were seen during training. To evaluate zero-shot generalization, we test on the Tanks and Temples [15] dataset which is widely used by prior 3D style transfer methods [7, 22, 23, 51]. Baselines. Since no existing methods can instantly stylize 3D reconstructions from sparse, unposed content images and style reference image (as outlined in Table 1), we carefully select set of representative baselines for comparison. For 2D-based approaches, we adopt two-stage pipeline using AdaIN [11], AdaAttN [24], and StyTr2 [4]: we first extract ground-truth novel view images and then apply each 2D stylization model to these images. For 3D-based methods, we compare against ARF [51], StyleRF [22], and StyleGaussian [23], which perform 3D stylization but require dense input views and test-time optimization. To ensure proper functionality, we train these methods 6 Figure 4: Cross-dataset generalization on Tanks and Temples dataset. Our model achieves superior or comparable zero-shot style transfer on out-of-distribution data, outperforming style-free baselines such as StyleRF [22] and StyleGaussian [23] that require per-scene optimization, and matching the performance of ARF [51], which further demands per-scene and per-style optimization. Consistency Method Short-range Long-range LPIPS RMSE LPIPS RMSE Stylization Time 2 AdaIN [11] 0.163 0.063 0.323 0.111 AdaAttN [24] 0.224 0.071 0.331 0.098 0.167 0.059 0.315 0.098 StyTr2 [4] 0.004 0.024 0.029 3 0.062 0.021 0.172 0.042 90 mins StyleRF [22] 0.048 0.022 0.137 0.043 132 mins StyleGS [23] 12 mins ARF [51] 0.093 0.038 0.217 0.070 Styl3R(Ours) 0.044 0.022 0.107 0.038 0.147 Table 2: Quantitative Results. Performance comparison of Styl3R with 2D and 3D baselines on RE10K in terms of view consistency. Stylization time refers to processing time excluding IO time. Figure 5: Visual Comparison. Visualizations of different views produced by StyTr2 [4] and our method. The highlighted regions (lamp and bed sheet) show noticeable color discrepancies in StyTr2, while our approach maintains consistent color across views. with dense inputs, acknowledging that this gives them an advantage and makes the comparison less favorable to our approach, which requires only sparse input views. Specifically, ARF [51] requires both per-scene and per-style optimization; while StyleRF [22] and StyleGaussian [23] support zero-shot style transfer, they still depend on per-scene optimization. Evaluation Metrics. Because of the novel and under-explored nature of 3D stylization, there are few metrics for assessing the quality of the stylization. Therefore, we evaluate the multi-view consistency as in prior 3D stylization works [7, 22, 23]. We estimate optical flow between sequential images using RAFT [40], then warp the earlier frame with softmax splatting [28]. Consistency is measured by LPIPS [52] and RMSE between the warped and target images over valid pixels. Shortand long-range consistency are computed between adjacent views and those seven frames apart, respectively. To evaluate novel view synthesis quality, we report standard image similarity metrics: PSNR, SSIM, and LPIPS [52]. Implementation details. We use PyTorch. The content and style encoder adopts standard ViTLarge architecture with patch size 16, while the structure and stylization decoder is based on ViT-Base model. We initialize the encoder, decoder, and the Gaussian center prediction head with pretrained weights from MASt3R [18], whereas the remaining layers are initialized randomly. The 7 Method PSNR SSIM LPIPS pixelSplat [1] MVSplat [3] NoPoSplat [46] NoPoSplat* Ours Ours-stylization 23.848 23.977 25.033 24.836 24.871 24.055 0.806 0.811 0.838 0.832 0.837 0. 0.185 0.176 0.160 0.166 0.165 0.179 Table 3: NVS comparison on RE10K. * denotes 0-degree spherical harmonics, as used in our model, while [46] defaults to 4-degree. Figure 6: Ablations. NVS results w/o and w/ identity loss during stylization fine-tuning. The former fails to retain the true color tone of the scene. Figure 7: Ablations. Stylization results of model trained with content loss consist of different layers. Using relu3_1 and relu4_1 in content loss preserve the original scene appearance more faithfully. model is trained on images with resolution of 256 256 pixels. Besides, we use 0 degree spherical harmonics for Gaussians following [7]. Training takes 1.5 days on 8 NVIDIA A100 GPUs. 4.1 Experimental Results 3D Stylization Results. As shown in Fig. 3 and Table 2, our method significantly outperforms all baselines. Visually, our stylizations achieve more balanced trade-off between content preservation and faithful style transfer. Among test-time optimization-based 3D baselines, StyleRF [22] and StyleGaussian [23] often fail to reproduce the reference styles color tone accurately, resulting in overly whitened or darkened outputs. ARF [51], while better at capturing style colors, tends to overfit and apply excessive stylization that obscures scene details. For example, in the third row of Fig. 3, furniture in the living room becomes nearly indistinguishable due to heavy sketch-line artifacts. As 2D baseline, StyTr2 [4] generates visually pleasing results on individual ground-truth novel views, but it lacks multi-view consistency, as evidenced in Table 2 and Fig. 5. In contrast, our method consistently produces superior stylizations while maintaining the best shortand long-range consistency metrics, benefiting from our attention mechanism that operates jointly over multi-view content and style tokens. Although StyleRF achieves slightly lower RMSE in short-range evaluations, this is largely due to its over-smoothed outputs, as clearly illustrated in Fig. 3. Video results in supplementary. Cross-Data Generalization. To evaluate the generalization performance of our method, we directly apply it to the Tanks and Temples dataset [15], widely used benchmark in prior works. As shown in Fig. 4, our model demonstrates superior performance on out-of-distribution scenes such as Garden, Ignatius, and Horse which are object-centric scenes that differ significantly from the RE10K training data, outperforming existing state-of-the-art methods. Notably, even though StyleRF [22] and StyleGaussian [23] are trained per scene, they fail to generalize to arbitrary style inputs. While ARF [51] achieves better results in some scenes, it requires dense, calibrated views along with per-scene and per-style optimization, limiting its practicality for time-constrained applications. Novel View Synthesis. Our final model supports both stylized and standard 3D reconstruction, depending on whether the input to the appearance branch is style or content image. We report two sets of metrics: one for the stylized output (Ours-stylization) and one for standard reconstruction without stylization fine-tuning (Ours). As shown in Table 3, Ours achieves performance comparable to NoPoSplat [46], despite not initializing the stylization decoder with pretrained weights. While Oursstylization shows slight drop in performance, it enables simultaneous support for both photorealistic and stylized reconstruction. Our results are from 2-view RE10K models, consistent with NoPoSplat. Stylization Time. We define stylization time as the total time from receiving the input content and style images to producing the final stylized outputs. This metric more practically reflects how quick user can obtain stylized results. For 3D methods, this includes both the reconstruction time and any Figure 8: Ablations. Results for 2-view and 4-view models when inputting 2 and 8 content images. The 2-view model fails to align Gaussians across multiple views when provided with 8 input views. Figure 9: Application. Style Interpolation with 2 style images by interpolating their style tokens. It can be observed that the style of the scene smoothly transit from one to another. stylization-related training or optimization. As shown in Table 2, our method achieves significantly faster stylization time than all existing 3D approaches, while approaching state-of-the-art 2D methods in terms of speed. 4.2 Ablation Studies Identity Loss to Preserve NVS Capability. We explore the necessity of identity loss during stylization fine-tuning. It can be observed in Fig. 6 that the model fails to recover the original appearance of the scene while performing NVS if we disable this loss. Content Loss Layers. As discussed in Sec. 3.3, using both relu3_1 and relu4_1 for the content loss better preserves structural details without sacrificing artistic expression. As shown in Fig. 7, relying solely on relu3_1 tends to cause the style to overwhelm the underlying scene structure. Flexibility of Number of Input Views. As discussed in Sec. 3.3, our model trained with 4 content images demonstrates strong generalization, effectively handling between 2 to 8 input views. As in Fig. 8, both the 2-view and 4-view models produce satisfactory stylizations when given only 2 content images. However, when the input is increased to 8 content images, the 2-view model struggles to align Gaussians across views, resulting in duplicated artifacts such as multiple pillars and sofas. In contrast, the 4-view model performs remarkably well, despite never being trained with 8-view inputs. 4.3 Application Style Interpolation. We demonstrate an application of our model, style interpolation, in Fig. 9. Specifically, we interpolate the style tokens from two reference style images before passing them to the stylization decoder, producing blended stylization that smoothly transitions between the two styles. This approach can be easily extended to more than two styles by simply computing weighted sum of their respective style tokens."
        },
        {
            "title": "5 Conclusion",
            "content": "This paper introduces feed-forward network for instant 3D stylization from sparse, unposed input views and single reference style image, which generalizes to arbitrary scenes and styles without testtime optimization. The network is composed of structure branch and an appearance branch, jointly enabling consistent novel view synthesis and stylization. Extensive experiments demonstrate that our method outperforms existing baselines in zero-shot stylization quality, while achieving significantly faster inference speed, making it more practical for real-world and interactive applications. It is worth noting that our method currently supports only static scenes; extending it to handle dynamic scenarios is an important direction for future work."
        },
        {
            "title": "References",
            "content": "[1] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. [2] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1412414133, 2021. [3] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. In European Conference on Computer Vision, pages 370386. Springer, 2024. [4] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1132611336, 2022. [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [6] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural stylization. In European Conference on Computer Vision, 2022. [7] Bruno Galerne, Jianling Wang, Lara Raad, and Jean-Michel Morel. Sgsst: Scaling gaussian splatting styletransfer. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [8] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional In Proceedings of the IEEE conference on computer vision and pattern neural networks. recognition, pages 24142423, 2016. [9] Lukas Höllein, Justin Johnson, and Matthias Nießner. Stylemesh: Style transfer for indoor 3d scene reconstructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 61986208, 2022. [10] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and Ming-Hsuan Yang. Learning to stylize novel views. In ICCV, 2021. [11] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV, 2017. [12] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin Gao. Stylizednerf: Consistent 3d scene stylization as stylized nerf via 2d-3d mutual learning. In Computer Vision and Pattern Recognition (CVPR), 2022. [13] Mohammad Mahdi Johari, Yann Lepoittevin, and François Fleuret. Geonerf: Generalizing nerf with geometry priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1836518375, 2022. [14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. [15] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017. [16] Georgios Kopanas, Julien Philip, Thomas Leimkühler, and George Drettakis. Point-based neural rendering with per-view optimization. Computer Graphics Forum (Proceedings of the Eurographics Symposium on Rendering), 40(4), June 2021. 10 [17] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: modular and hackable transformer modelling library. https://github.com/facebookresearch/xformers, 2022. [18] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [19] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang. Learning linear transformations for fast arbitrary style transfer. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. [20] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in neural information processing systems, 30, 2017. [21] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14458 14467, 2021. [22] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Eric Xing. Stylerf: Zero-shot 3d style transfer of neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83388348, 2023. [23] Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, and Shijian Lu. Stylegaussian: Instant 3d style transfer with gaussian splatting. In SIGGRAPH Asia 2024 Technical Communications, pages 14, 2024. [24] Songhua Liu, Tianwei Lin, Dongliang He, Fu Li, Meiling Wang, Xin Li, Zhengxing Sun, Qian Li, and Errui Ding. Adaattn: Revisit attention mechanism in arbitrary neural style transfer. In Proceedings of the IEEE International Conference on Computer Vision, 2021. [25] Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Transactions on Graphics (TOG), 2019. [26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [27] Fangzhou Mu, Jian Wang, Yicheng Wu, and Yin Li. 3d photo stylization: Learning to generate stylized novel views from single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1627316282, 2022. [28] Simon Niklaus and Feng Liu. Softmax splatting for video frame interpolation, 2020. [29] Ipek Oztas, Duygu Ceylan, and Aysegul Dundar. 3d stylization via large reconstruction model, 2025. [30] Dae Young Park and Kwang Hee Lee. Arbitrary style transfer with style-attentional networks. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 58805888, 2019. [31] Fred Phillips and Brandy Mackintosh. Wiki art gallery, inc.: case for critical thinking. Issues in Accounting Education, 26(3):593608, 2011. [32] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179 12188, 2021. 11 [33] Abhishek Saroha, Mariia Gladkova, Cecilia Curreli, Dominik Muhle, Tarun Yenamandra, and Daniel Cremers. Gaussian splatting in style. In DAGM German Conference on Pattern Recognition, pages 234251. Springer, 2024. [34] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model, 2023. [35] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. [36] Brandon Smart, Chuanxia Zheng, Iro Laina, and Victor Adrian Prisacariu. Splatt3r: Zero-shot gaussian splatting from uncalibrated image pairs. arXiv preprint arXiv:2408.13912, 2024. [37] Bingjie Song, Xin Huang, Ruting Xie, Xue Wang, and Qing Wang. Style3d: Attention-guided multi-view style transfer for 3d object generation, 2024. [38] Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1020810217, 2024. [39] Hao Tang, Songhua Liu, Tianwei Lin, Shaoli Huang, Fu Li, Dongliang He, and Xinchao Wang. Master: Meta style transformer for controllable zero-shot and few-shot artistic style transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1832918338, 2023. [40] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020. [41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [42] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 46904699, 2021. [43] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [44] Xiaolei Wu, Zhihao Hu, Lu Sheng, and Dong Xu. Styleformer: Real-time arbitrary style transfer via parametric style composition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1461814627, 2021. [45] Han Xu, Jiteng Yuan, and Jiayi Ma. Murf: Mutually reinforcing multi-modal image registration and fusion. IEEE transactions on pattern analysis and machine intelligence, 45(10):12148 12166, 2023. [46] Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In The Thirteenth International Conference on Learning Representations, 2025. [47] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 45784587, 2021. [48] Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, and Lin-Lin Ou. Instantstylegaussian: Efficient art style transfer with 3d gaussian splatting. arXiv preprint arXiv:2408.04249, 2024. [49] Chiyu Zhang, Xiaogang Xu, Lei Wang, Zaiyan Dai, and Jun Yang. S2wat: Image style transfer via hierarchical vision transformer using strips window attention. In Proceedings of the AAAI conference on artificial intelligence, 2024. 12 [50] Dingxi Zhang, Yu-Jie Yuan, Zhuoxun Chen, Fang-Lue Zhang, Zhenliang He, Shiguang Shan, and Lin Gao. Stylizedgs: Controllable stylization for 3d gaussian splatting. arXiv preprint arXiv:2404.05220, 2024. [51] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic radiance fields. In European Conference on Computer Vision, pages 717733. Springer, 2022. [52] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The Unreasonable Effectiveness of Deep Features as Perceptual Metric. In Computer Vision and Pattern Recognition (CVPR), pages 586595, 2018. [53] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: learning view synthesis using multiplane images. ACM Transactions on Graphics (TOG), 37(4):112, 2018."
        },
        {
            "title": "Appendix",
            "content": "In the supplementary, we provide the following: more implementation details, including architecture and training hyperparameters for our model in Sec. A; more technical details on training 2D and 3D baselines in Sec. A; more visual results of our model and comparisons with baselines in Sec. B. We highly recommend visiting our project website for more comprehensive demonstration of our methods stylization quality and temporal consistency. The website features: qualitative comparisons with baselines on the RE10K [53] and Tanks and Temples [15] datasets; more out-of-domain stylization results on the Tanks and Temples and NeRF LLFF [25] datasets; style interpolation examples showcasing transitions between three different styles."
        },
        {
            "title": "A More Implementation Details",
            "content": "Training. In terms of optimization, we employ AdamW optimizer. For Novel View Synthesis (NVS) pretraining, we train the stylization decoder, color head and structure head with initial learning rate of 2 104, and fine-tune the other parameters with 2 105. Then for stylization fine-tuning, we continue optimizing the color head and stylization decoder with initial learning rate of 2 104 and fine-tune only the style encoder with 2 105, and keep all the other parameters in the structure branch fixed. Architecture. To expedite the inference of network, we use the flash attention implementation from xFormers [17] in all of our encoders and decoders. As in [46], we feed the tokens from the 1-st, 7-th, 10-th and 13-rd block into DPT [32] for upsampling. Baselines Training. For the 2D methods (StyTr2 [4], AdaIN [11], AdaAttN [24]), we directly utilize their publicly released pretrained checkpoints, available at StyTr2 code , AdaIN code, and AdaAttN code, respectively. As these methods do not support 3D reconstruction from 2D images, we apply them directly to stylize the ground-truth 2D novel views, bypassing any reconstruction process. In contrast, the 3D methods (ARF [51], StyleRF [22], StyleGaussian [23]) are unable to reconstruct geometry from sparse, unposed inputs. Therefore, we train them using all available scene images (on average, more than 100 per scene) along with their corresponding camera poses. This setup results in an unfair comparison with our method, which operates on sparse and unposed inputs."
        },
        {
            "title": "B More Visual Results",
            "content": "We present additional qualitative results in Fig. 10, Fig. 11, Fig. 12, Fig. 13, and Fig. 14, which highlight the superior performance of our method compared to prior state-of-the-art style transfer approaches. While existing 3D methods rely on densely posed images, our approach enables instant 3D stylization across arbitrary scenes and styles without such constraints. More Comparisons with Baselines. To better showcase the superiority of our method, we visualize more comparison results with 3D baseline methods on different scenes and styles, as shown in Fig. 10. More Visual Results of Our Method. To validate our method is compatible with arbitrary scenes and styles, we show stylization results with exhaustive combinations from randomly selected scenes and styles, as shown in Fig. 11, Fig. 12, Fig. 13 and Fig. 14. 14 Figure 10: Novel View Transfer Comparision on RE10K. Our method faithfully preserves style and scene structure, even with limited image overlap. In contrast, StyleRF [22] and StyleGaussian [23] produce over-smoothed results with inaccurate color tones, while ARF [51] suffers from style overflow. Figure 11: Additional Results on RE10K from Our Method. 16 Figure 12: Additional Results on RE10K from Our Method. 17 Figure 13: Additional Results on RE10K from Our Method. Figure 14: Additional Results on RE10K from Our Method."
        }
    ],
    "affiliations": [
        "Westlake University",
        "Zhejiang University"
    ]
}