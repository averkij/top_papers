{
    "paper_title": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression",
    "authors": [
        "Ammar Ali",
        "Baher Mohammad",
        "Denis Makhov",
        "Dmitriy Shopkhoev",
        "Magauiya Zhussip",
        "Stamatios Lefkimmiatis"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present ROCKET, a training-free model compression method that achieves state-of-the-art performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under a global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as a multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to a target model size. Second, it introduces a single-step sparse matrix factorization inspired by dictionary learning: using only a small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 20-50\\% compression rates. Notably, it retains over 90\\% of the original model's performance at 30\\% compression without any fine-tuning. Moreover, when applying a light fine-tuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8B-parameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code for ROCKET is at github.com/mts-ai/ROCKET/tree/main."
        },
        {
            "title": "Start",
            "content": "ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Ammar Ali * 1 2 Baher Mohammad * 1 2 Denis Makhov 2 Dmitriy Shopkhoev 2 Magauiya Zhussip 2 Stamatios Lefkimmiatis 2 6 2 0 2 1 1 ] . [ 1 8 0 0 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We present ROCKET, training-free model compression method that achieves state-of-theart performance in comparison with factorization, structured-sparsification and dynamic compression baselines. Operating under global compression budget, ROCKET comprises two key innovations: First, it formulates layer-wise compression allocation as multi-choice knapsack problem, selecting the optimal compression level for each layer to minimize total reconstruction error while adhering to target model size. Second, it introduces single-step sparse matrix factorization inspired by dictionary learning: using only small calibration set, it sparsifies weight coefficients based on activation-weights sensitivity and then updates the dictionary in closed form via least squares bypassing iterative optimization, sparse coding, or backpropagation entirely. ROCKET consistently outperforms existing compression approaches across different model architectures at 2050% compression rates. Notably, it retains over 90% of the original models performance at 30% compression without any finetuning. Moreover, when applying light finetuning phase, recovery is substantially enhanced: for instance, compressing Qwen3-14B to an 8Bparameter model and healing it with just 30 million tokens yields performance nearly on par with the original Qwen3-8B. The code implementing ROCKET. 1. Introduction In recent years, transformers have achieved unprecedented success across wide range of tasks in both computer vision *Equal contribution 1Department of Computer Science, ITMO University, Saint-Petersburg, Russia 2MWS AI, Moscow, Russia. Correspondence to: Ammar Ali <ammarali32@itmo.ru>, Baher Mohammad <b.mohammad@mts.ai>. Preprint. February 12, 2026. and natural language processing. Modern large language models (LLMs) typically scale up to billions of parameters, significantly increasing the computational and memory requirements for both training and inference stages. This substantial resource demand poses critical challenge for their wider practical deployment, especially on edge devices or in latency-sensitive applications. Due to the excessive size of modern LLMs, there has been significant research effort to make such models more efficient and accessible under constrained hardware budgets. Such efforts primarily focus on three key strategies: quantization (Hassibi & Stork, 1992), distillation(Hinton et al., 2015), and weight compression via matrix factorization (Wang et al., 2020). Among these, post-training weight factorization has emerged as particularly promising direction, enabling substantial parameter reduction without the need for costly retraining or fine-tuning. dominant paradigm in this area is low-rank approximation using truncated Singular Value Decomposition (SVD), which approximates each weight matrix as the product of two smaller dense matrices. However, this strategy imposes rigid structural constraint forcing all columns of the weight matrix to lie in single shared low-dimensional subspace. This often limits the representational capacity and leads to significant performance degradation under moderate to high compression. This limitation has spurred the development of methods that go beyond single shared subspace representation , adopting instead union-of-subspaces framework akin to dictionary learning. In such models, weight matrix is expressed as combination of subset of basis matrices (Zhussip et al., 2025), or alternatively, its individual columns are represented as sparse linear combinations of atoms from shared dictionary (Shopkhoev et al., 2025). These formulations provide greater flexibility by capturing the heterogeneous local structures present within the weight matrix. Despite their theoretical appeal, practical adoption of these methods faces severe computational challenges: conventional dictionary learning algorithms rely on iterative alternating minimization between sparse coding and dictionary update steps, which is prohibitively expensive for large-scale LLM weight matrices (Aharon et al., 2006). ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression In this work, we propose ROCKET, fast, training-free compression method that overcomes the representational rigidity of low-rank factorization while avoiding the computational burden of iterative dictionary learning. Our approach introduces two key innovations. First, ROCKET compresses weight matrices via single-step structured sparsification of low-rank basis. This yields factorization that inherits the expressive power of union-of-subspaces models yet operates orders of magnitude faster than alternating minimization schemes. Second, rather than applying uniform compression or relying on heuristic layer-wise sensitivity estimates, ROCKET formulates global compression allocation as multi-choice knapsack problem. For each layer, it selects the optimal compression configuration from set of precomputed candidates to minimize total weight reconstruction error under target model size constraint. Together, these components enable ROCKET to produce compact models that achieve substantially higher accuracy compared to existing post-training compression methods. The contributions of this work are summarized as follows: (1) We propose ROCKET, an efficient, training-free LLM compression method that factorizes weight matrices into sparse dictionary representation computable in single step, eliminating the need for iterative optimization; (2) We introduce calibration-guided criterion for sparsifying the coefficient matrix, operating effectively in both the original and whitened weight spaces to preserve salient directional information; (3) We formulate layer-wise compression allocation as multi-choice knapsack problem, enabling dynamic, performance-aware distribution of the global compression budget across layers; (4) Through extensive experiments, we demonstrate that ROCKET consistently outperforms state-of-the-art compression methods including structured sparsification, low-rank factorization, and adaptive budget allocation techniques across multiple modalities (text, vision, and audio). 2. Related Work This work intersects three primary research directions in model compression: (1) dynamic per-layer allocation of compression budgets, (2) structured matrix factorization for weight approximation, and (3) sparsification techniques. We review recent advances in each area, with emphasis on methods most relevant to our training-free, reconstructionaware compression framework. Structured Matrix Factorization for Weight Approximation Early approaches employed truncated SVD to obtain low-rank approximations of transformer weights. However, several studies (Yuan et al., 2023; Wang et al., 2025b; Chen et al., 2021) demonstrated that weight matrices themselves are not inherently low-rank; instead, activations exhibit lowrank structure. These works proposed data-aware low-rank approximations using whitening transform estimated from small calibration dataset, yielding significantly more effective compression. more general representation was recently introduced in (Shopkhoev et al., 2025), where weights are expressed as sparse linear combinations of dictionary atoms in whitened space, computed via K-SVD and Orthogonal Matching Pursuit (OMP) updates. This approach overcomes the limitation of fixed, layer-invariant bases in low-rank methods since it allows each column of the weight matrix to reside in different low-dimensional subspace, effectively promoting more flexible union-of-subspaces modeling strategy. Our approach extends this line of work by replacing the computationally intensive iterative K-SVD/OMP optimization with novel single-step greedy algorithm. This eliminates alternating minimization while achieving higher reconstruction accuracy and orders-of-magnitude faster compression critical for scaling to billion-parameter models. Budget Allocation and Layer Importance Many early compression methods apply uniform compression across all layers, implicitly assuming equal layer importance. Recent work challenges this assumption. LLM-Pruner (Ma et al., 2023b) estimates the importance of coupled layer groups using gradient and Hessian-based metrics, pruning less critical groups. ARS (Gao et al., 2024) proposes an adaptive rank selection mechanism using differentiable binary masks, regularized to respect the ordering of singular values from SVD, thereby allocating more capacity to important layers. DobiSVD (Wang et al., 2025a) introduces learnable truncation threshold per weight matrix, optimized during training via multi-objective loss balancing task performance and global compression ratio. Similarly, ARA (Xv et al., 2025) dynamically assigns ranks to linear modules by learning monotonic probabilistic mask over singular values, guided by loss that accounts for cases where full-rank retention is more efficient than decomposition. In contrast to these training-based approaches, our method performs budget allocation in purely post-training setting. We formulate the problem as multi-choice knapsack optimization, where each layer is associated with discrete set of feasible compression configurations. Using dynamic programming, we select the globally optimal combination that minimizes total weight reconstruction error while enforcing per-layer upper bound on relative reconstruction error, ensuring both global efficiency and local fidelity. Sparsification Methods Unstructured pruning has demonstrated strong efficacy in compressing large language models. While magnitude-based pruning (Han et al., 2015) remains simple baseline, Frantar et al. (Frantar & Alistarh, 2023) showed its inadequacy for LLMs and proposed SparseGPT, Hessian-aware, layer-wise pruning method that reconstructs output errors via an efficient approximate 2 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression solver. Alternative importance metrics have also been explored. WANDA (Sun et al., 2024) computes saliency score as the product of weight magnitude and the L2 norm of corresponding input activations (estimated from calibration data), pruning the lowest-scoring weights per output neuron. Bonsai (Kolawole et al., 2024) formulates module importance as an underdetermined regression problem, estimating importance using only forward passes to enable efficient structured pruning. Although sparsification achieves high compression ratios, it often yields irregular memory access patterns that hinder inference acceleration on modern hardware. Our method produces structured sparse-factorizations that are compatible with standard dense linear algebra operations, potentially offering practical balance between compression efficiency, reconstruction quality, and hardware compatibility. 3. Method Recent training-free compression methods for LLMs exhibit fundamental trade-off between computational efficiency and representational flexibility. On one end, truncated SVD (SVD-LLM) (Wang et al., 2025b) enforces rigid, globally shared low-rank subspace, enabling fast compression but severely limiting reconstruction fidelity under aggressive ratios. On the other, CoSpaDi (Shopkhoev et al., 2025) employs conventional sparse dictionary learning (K-SVD + OMP) to realize union-of-subspaces model, at the cost of increased runtime and poor scalability for multi-billion models. ROCKET bridges this gap by introducing calibrationaware, single-step structured sparsification strategy grounded in eigen decomposition. Given calibration dataset RN d1 and weight matrix Rd1d2, we seek an approximation (cid:102)W that minimizes the activationaware reconstruction error: (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)XW (cid:102)W (cid:13)F subject to (cid:102)W C, (1) arg min (cid:102)W where imposes structural constraints on (cid:102)W. Following established data-aware compression strategies, we operate in the whitened activation space. Let be the Cholesky factor of the Gram matrix = XX, and define the decorrelated input = XL1, which satisfies YY = Id1 . It can then be shown that the objective simplifies to: (cid:13) (cid:13) (cid:13)XW (cid:102)W (cid:13) (cid:13) (cid:13)F min (cid:102)W = min (cid:102)W = min (cid:102)W (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)Y(LW (cid:102)W) (cid:13)F (cid:13) (cid:13) (cid:13)LW (cid:102)W (cid:13) (cid:13) (cid:13)F , (2) We compute the top-r eigenvectors of WLW : WLW BΛrB, where Rd1r is column-orthogonal (BB = Ir) and Λr = diag(λ1, . . . , λr). The coefficient matrix is obtained via orthogonal projection: := BWL Rrd2 and low-rank weight factorization is computed as WL BC. This formulation enables structured sparsification: rather than pruning W, we note that XW (YB)C and operate on C. Because is semi-orthogonal, zeroing cij deactivates the i-th latent direction for the j-th output dimension. Since sparsity is applied independently per column, each output may activate distinct subset of basis vectors realizing union-of-subspaces model similar to dictionary learning. However, the ultimate goal is to reconstruct W, not WL. The inverse whitening transform L1 maps errors back to the original space, but it is generally non-orthogonal. Consequently, two coefficients with identical magnitudes in the whitened space may contribute very differently to the final reconstruction error, depending on how their corresponding basis vectors bi are scaled by L1. Specifically, an error along bi incurs cost proportional to L1bi2. To account for this directional sensitivity, we define two complementary importance measures. The whitened-space importance impwhite reflects local optimality under orthonormality: ij impwhite ij = cij. (3) The original-space importance imporig tual impact on the reconstructed weight: (cid:13) (cid:13)2 . ij = cij (cid:13) (cid:13)L1bi imporig ij quantifies the ac- (4) We fuse these importance scores via scale-invariant geometric interpolation. For balance parameter λ [0, 1], the combined importance is: (cid:17)λ (cid:1)1λ (cid:16) ij imporig ij (cid:13)L1bi = cij (cid:13) impij = (cid:0)impwhite (cid:13) λ 2 . (cid:13) (5) This can be interpreted as solving weighted sparse approximation problem in the whitened space, where weights encode the distortion induced by L1. We set λ = 0.5 (geometric mean), which empirically balances activation fidelity and weight-space stability. Let ν Rr with νi = (cid:13) matrix is: (cid:13) λ 2 . The full importance (cid:13) Imp = (cid:0)ν1 d2 applying row-wise scaling consistent with the per-direction nature of the metric distortion. (cid:13)L1bi (cid:1) , (6) which is attributed to having orthogonal columns. If we denote the whitened weight as WL := LW, the problem (cid:13) (cid:13) (cid:13)WL (cid:99)W reduces to minimizing , with (cid:99)W = (cid:102)W. (cid:13) (cid:13) (cid:13)F We then apply two-stage sparsification strategy. First, we perform column-wise hard thresholding on C, retaining the top-s entries per column according to impij. To 3 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Figure 1. Overview of the proposed method. Left: Budget allocation formulated as shortest-path problem on directed graph, where nodes represent compression options and edges encode cost (reconstruction error), solved via DP algorithm to find the optimal sequence of operations. Right: The selected optimal path determines per-layer compression parameters (rank Ki and sparsity Si), which are then applied to each layer via Eigen value decomposition (EVD) followed by structured hard thresholding sparsification (T (.)) of coefficients. allow global flexibility beyond fixed per-column budgets, we initially over-sparsify to ratio of cr + β where β is relatively small (we set β = 5e3 in all experiments), then reactivate the most important masked coefficients across the entire matrix until the exact target compression ratio cr is reached. This yields sparse coefficient matrix Csparse. Crucially, after sparsification, there is no longer need to enforce orthonormality on the basis. The initial eigenbasis was used only to derive an expressive, data-adaptive representation; once Csparse is fixed, we can optimize the left factor freely to minimize reconstruction error in the whitened space. We therefore compute the final dictionary Dfinal (we use different notation to highlight that orthonormality for the left matrix factor is not required) by solving ridge-regularized least-squares problem: WL DCsparse2 Dfinal = arg min + µD2 (7) which admits closed-form Cholesky-based solution. This step relaxes the semi-orthogonality constraint, yielding better fit without increasing inference cost. The final compressed weight is recovered as: (cid:102)W = L1DfinalCsparse, (8) stored as two factors = L1Dfinal and = Csparse. Thus, ROCKET unifies three perspectives: (i) closedform surrogate to iterative dictionary learning, replacing alternating updates with eigen decomposition and optimal thresholding; (ii) generalization of SVD: when no sparsity is applied (s = r), it recovers standard low-rank SVD; (iii) structured sparsification method, preserving the UV product for seamless merging during inference. Layer Profiling. To enable optimal global compression under fixed parameter budget, we first perform 4 lightweight layer profiling pass. For each compressible layer, we evaluate predefined set of (rank, sparsity) configurations. For each candidate, we: (i) compute the eigendecomposition of WLW , (ii) determine rank and sparsity level s, (iii) sparsify using fused importance scores, (iv) compute Dfinal via least squares, and (v) record the actual parameter count (cost), ks ratio, and relative reconstruction error (eℓ,i 1) in the original space for layer ℓ and option given as eℓ,i = discrete set of feasible options Oℓ = {(cℓ,i, ksℓ,i, eℓ,i)}. . This yields, per layer ℓ, Wℓ(cid:102)WℓiF WℓF Constrained Multi-Choice Knapsack Formulation. Let there be compressible layers. For layer ℓ {1, . . . , L}, let Oℓ = {(cℓ,i, ksℓ,i, eℓ,i)}Kℓ i=1 denote its feasible compression options obtained during profiling, where cℓ,i R0 is the parameter count, ksℓ,i is the sparsity to truncation ratio, and eℓ,i 0 the Frobenius reconstruction error. Let Ctotal be the global parameter budget (e.g., for target cr% compression). The optimal allocation is traditionally cast as multi-choice knapsack problem (MCKP): min xℓ,i0,1 s.t. (cid:88) Kℓ(cid:88) ℓ= i=1 (cid:88) Kℓ(cid:88) ℓ=1 i=1 eℓ,i, xℓ,i (9) cℓ,i, xℓ,i Ctotal, Kℓ(cid:88) i=1 xℓ,i = 1, ℓ. To prevent degradation below uniform-compression baseline, we introduce an additional hard constraint. Let eref be the average reconstruction error across all layers when each is compressed at fixed reference ratio (e.g., ρref). We then require: (cid:80)Kℓ i=1 eℓ,i xℓ,i α.eref ℓ {1, . . . , L}, where, α is tunable hyperIn all experiments, we set α = αmin := parameter. inf {α > 0 feasible solution exists for α} . Further perROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression model tuning of α may yield improved performance. This constraint eliminates pathological solutions that achieve low global error by severely damaging few layers while overpreserving others. The problem remains MCKP with layerwise error caps, ensuring both global optimality and local robustness. We reformulate the problem using graph theory in Appendix C, which clarifies the dynamic programming solution that follows. Dynamic Programming for Allocation. We solve the constrained MCKP via bottom-up dynamic programming. Let DPℓ[k] denote the minimal error after processing the first ℓ layers with discretized kept parameter count k. The recurrence is: (cid:16) (cid:17) , (10) DPℓ+1[k + βκℓ+1,i] = min DPℓ[k] + εℓ+1,i where β = param precision/Ptotal, κℓ,i is the kept count, and εℓ,i the error. After each layer, we prune dominated states (k1 < k2 and DP[k1] DP[k2]), keeping the state space small in practice. The algorithm runs in O(LM B) time and O( B) space, outperforming Dijkstrabased approaches in both speed and memory while yielding the same globally optimal solution. In summary, ROCKET is fully training-free pipeline that: (1) constructs data-adaptive basis via eigen decomposition in the whitened activation space; (2) performs importanceweighted structured sparsification with global refinement; (3) relaxes orthogonality post-sparsification via closedform least-squares update; and (4) allocates global parameter budget through constrained knapsack solver with perlayer robustness guarantees. This combination achieves the expressivity of sparse dictionary learning and the efficiency of spectral methods, enabling high-fidelity compression of billion-parameter LLMs with minimal overhead. 4. Experiments In this section, we describe our experimental setup and compare ROCKET against recent compression methods. We focus on low-rank and dictionary-based approaches, specifically SVD-LLM (Wang et al., 2025b) and CoSpaDi (Shopkhoev et al., 2025), as well as budget allocation based methods including ARS (Gao et al., 2024), Dobi-SVD (Wang et al., 2025a), and ARA (Xv et al., 2025). We also provide Comparisons with sparsification and width-pruning methods such as LLM-Pruner (Ma et al., 2023a), SliceGPT (Ashkboos et al., 2024), Bonsai (Kolawole et al., 2024), and Wanda (Sun et al., 2024). We also conduct ablations to isolate the contribution of each design choice. 4.1. Experimental Setup We evaluate our method in per-layer setting using LLaMA and Qwen models. All evaluations are performed in zeroshot setting on the following benchmarks: PIQA (Bisk 5 et al., 2019), HellaSwag (Zellers et al., 2019), OpenAI LAMBADA (Paperno et al., 2016), ARC-Easy and ARCChallenge (Clark et al., 2018), SciQ (Welbl et al., 2017), RACE (Lai et al., 2017), and MMLU (Hendrycks et al., 2021a). In addition, we report perplexity on WikiText (Merity et al., 2016) and LAMBADA-OpenAI. We apply compression at compression weight ratios ranging from 0.2 to 0.5, in steps of 0.1. For methods that need calibration data, we use 256 randomly sampled sequences from the RefinedWeb dataset (Penedo et al., 2023) (fixed across all experiments). We also test how the choice of calibration dataset affects results in the appendix. Unless otherwise noted, we compress all dense linear layers in the self-attention blocks (Q, K, V, and projections) and the feed-forward network (gate, up, and down projections). Embedding layers and the lm head are not compressed following other works. Comparison with SVD-LLM and CoSpaDi To contextualize ROCKETs performance, we directly compare it against SVD-LLM and CoSpaDi, the two most closely related training-free compression methods. All methods are evaluated in strictly training-free setting, no fine-tuning, healing, or data augmentation is applied post-compression. As shown in Table 1 and Fig 2, ROCKET consistently outperforms both baselines by significant margin across multiple architectures (Qwen3-8B, Llama3-8B and Llama3.2-1B) and compression ratios (20%50%), in terms of both zeroshot accuracy and perplexity (check Appendix E.1 for more detailed results). Notably, ROCKET exhibits superior scalability under aggressive compression and increasing model scales (detailed in Appendix E.2): while baseline methods suffer severe degradation beyond 30% compression, ROCKET retains more robust performance even at 50% compression (e.g., 51.3 average accuracy on Qwen3-8B vs. 38.1 for SVD-LLM and 42.0 for CoSpaDi). This demonstrates that ROCKETs combination of calibration-guided factorization, sparsification and optimal layer-wise budget allocation effectively preserves model fidelity under strict parameter constraints. Comparison against other budget allocation methods To evaluate the effectiveness of ROCKETs layer-wise budget allocation, we compare it against four established parameter allocation strategies: SVD-LLM (Wang et al., 2025b), which applies uniform compression across layers; Adaptive Rank Selection (ARS) (Gao et al., 2024); Dobi-SVD (Wang et al., 2025a); and Adaptive Rank Allocation (ARA) (Xv et al., 2025). Figure 3 shows normalized performance across three model-compressions configurations Qwen3-8B at 20% and 40% compression, and LLaMA2-7B at 40% compression with all scores scaled to their respective dense baselines. ROCKET consistently outperforms all baselines, demonstrating that globally optimizing the allocation of parameters ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Table 1. Performance comparison of ROCKET vs SOTA SVD-LLM and CoSpaDi methods on Qwen3-8B at different compression ratios (CR). Best results are highlighted with bold. Method CR Qwen3 8B SVD-LLM CoSpaDi 0. ROCKET SVD-LLM CoSpaDi 0.3 ROCKET SVD-LLM CoSpaDi 0.4 ROCKET SVD-LLM CoSpaDi 0. ROCKET PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. WikiText LAMBADA Accuracy Perplexity 77. 73.8 76.5 77.6 70.4 72.4 75. 66.3 68.9 71.0 61.5 63.8 68. 74.9 63.9 68.0 72.9 55.2 60. 67.2 44.6 49.0 58.7 34.9 39. 48.4 64.1 62.2 65.6 66.0 53. 62.6 68.0 37.9 49.9 63.7 25. 32.4 47.5 80.7 68.7 72.2 75. 59.3 63.9 72.1 45.0 49.4 65. 37.4 41.2 54.8 56.7 45.7 48. 53.9 37.1 41.2 47.7 28.1 29. 40.4 25.3 26.8 33.0 95.7 90. 93.2 94.5 87.2 88.4 92.9 77. 82.0 86.1 65.1 70.4 81.6 40. 40.5 40.7 41.4 38.4 39.5 41. 35.3 36.8 41.9 31.6 33.2 38. 73.0 54.7 60.8 67.2 44.8 51. 62.3 29.1 36.6 52.3 24.0 28. 37.9 70.5 62.5 65.7 68.7 55. 60.0 65.8 45.4 50.3 60.0 38. 42.0 51.3 1.2E+01 2.1E+01 1.8E+01 4.6E+ 6.4E+00 4.9E+00 1.5E+01 4.7E+00 2.7E+01 2.3E+ 1.1E+01 6.3E+00 1.8E+01 4.4E+00 4.3E+01 3.6E+ 3.6E+01 1.5E+01 2.4E+01 5.9E+00 7.6E+01 5.9E+ 8.8E+01 4.1E+01 3.5E+01 2.4E+01 Llama3-8B Llama3.2-1B 80 60 40 20 g 0.8 60 0.6 40 0.4 0. 20 1 0.8 0.6 0.4 0. ) ( / 1 Orig Acc CoSpaDi Acc Orig 1/ ln(P ) SVD-LLM Acc Ours Acc SVD-LLM 1/ ln(P ) CoSpaDi 1/ ln(P ) Ours 1/ ln(P ) 0.2 0. 0.4 0.5 0.2 0.3 0.4 0. Compression Ratio (CR) Compression Ratio (CR) Figure 2. Comparison of Accuracy and Inverse Log Perplexity for Llama3-8B and Llama3.2-1B. via constrained knapsack selection preserves significantly more of the original models capabilities than uniform or trainable strategies, especially under aggressive compression and across different architectures. Comparison with Depth and Sparsity Pruning Methods To demonstrate the effectiveness of our method, we compare it against several approaches that address model pruning from different perspectives. These include depth pruning (e.g., SliceGPT), combined depth/width pruning (e.g., LLM-Pruner), structured sparsification (e.g., Wanda, Bonsai), and adaptive low-rank decomposition with quantization (e.g., Dobi-SVD). As shown in Table 2, ROCKET achieves strong performance, outperforming all other baselines at 60% compression ratio (0.56 average accuracy). Since Dobi-SVD uses quantization, we additionally evaluate ROCKET with post-compression quantization to ensure fair comparison. Under this setting, ROCKET surpasses Dobi-SVD at 40% compression ratio (0.65 vs. 0.63). At 60% compression ratio, ROCKET again leads with an average accuracy of 0.60 compared to Dobi-SVDs 0.52. This demonstrates that ROCKETs training-free pipeline is not only simple and fast but also highly effective, matching or exceeding other pruning methods. 4.2. Post-Compression Healing To evaluate the potential for lightweight recovery, we apply simple healing step, fine-tuning on small amount of data to the ROCKET-compressed Qwen3-14B model, which was reduced to 8B parameters (40% compression). During healing, we fine-tune both the unmasked entries in the factorized weights and the associated dictionary, while keeping the sparsity pattern fixed. Training is performed on 30 million tokens of high-quality text sampled from the AllenAI C4 dataset. As shown in Table 3, the healed model, named ROCKETQwen3-8B (healed) achieves an average accuracy of 67.96, substantially improving over the training-free compressed version (63.56) and approaching the performance of the native Qwen3-8B (70.46), even surpassing it on several benchmarks (e.g., PIQA, Lambada). For per-benchmark results, please refer to appendix E.4. This demonstrates that ROCKET not only excels in the training-free regime but also provides high-quality initialization that enables effective recovery with minimal data and compute. Critically, this result represents practical step forward for model development: rather than training multiple models of different sizes from scratch, one can train single large 6 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Figure 3. Comparison of ROCKET with alternative budget allocation methods (Uniform, ARS(Gao et al., 2024), Dobi-SVD(Wang et al., 2025a), and ARA(Xv et al., 2025)) on three model configurations: Qwen3-8B at 20% and 40% pruning, and LLaMA2-7B at 40% pruning. Subplots show normalized performance on eight benchmarks (C4 perplexity inverted so higher is better), scaled to each models dense baseline (value=1.0). ROCKET consistently retains the most performance under the same parameter constraints. Table 2. Performance comparison of ROCKET against depthand sparsity-based pruning methods on Llama3.1-8B across compression levels and benchmarks. Avg denotes the average accuracy across all benchmarks, Drop indicates the relative accuracy drop percentage compared to the dense (uncompressed) model, and Quant. indicates whether post-compression quantization is applied. Models CR Method Quant. PIQA HellaSwag WinoGrande ARC ARC Avg. () Drop () Baseline 0.4 LLaMA-3.1-8b LLM-Pruner SliceGPT Bonsai Wanda-sp Dobi-SVD ROCKET ROCKET 0.6 Dobi-SVD ROCKET 0.80 0.66 0.62 0.59 0.57 0.76 0.72 0.78 0.68 0.75 0.59 0.32 0.40 0.29 0.28 0.52 0.43 0. 0.41 0.49 0.74 0.54 0.53 0.49 0.50 0.72 0.66 0.72 0.66 0.68 0.81 0.58 0.49 0.47 0.44 0.73 0.64 0. 0.58 0.72 0.51 0.23 0.25 0.18 0.17 0.39 0.33 0.42 0.27 0.38 0.69 0.46 0.46 0.41 0.39 0.63 0.56 0. 0.52 0.60 0% 33.3% 33.3% 40.6% 43.5% 8.70% 18.84% 5.79% 24.6% 13.0% Table 3. Healing results of Qwen3-14B model after compressing by 40% resultsing in an 8B version. Table 4. Average accuracy of Qwen3-4B-VL before and after ROCKET compression (20%)."
        },
        {
            "title": "Method",
            "content": "perplexity Avg. Acc."
        },
        {
            "title": "Method MMB MMMU MMS OCR RWQA",
            "content": "Qwen3-14B (dense) Qwen3-8B (dense) ROCKET-Qwen3-8B (training-free) ROCKET-Qwen3-8B (healed) 1.1E+01 1.2E+01 2.4E+01 1.3E+01 73.32 70.46 63.56 67.96 model and compress it to any desired size using ROCKET, leveraging the resulting sparsity for faster inference (see Appendix D), and optionally applying light healing to recover performance. With cleaner data and longer training, the healed model has the potential to match or even surpass dense counterpart of equal size, offering flexible, efficient, and scalable alternative to traditional multi-size training pipelines. 4.3. Generalization to Other Modalities To assess the generality of ROCKET beyond languageonly models, we apply it to two transformer-based archithe vision-language tectures from different modalities: model Qwen3-4B-VL and the speech generation model VibeVoice (Peng et al., 2025). For VibeVoice, we use 256 transcriptions-only from mls eng 10k dataset dense ROCKET 83.76 78.95 49.44 44.44 61.85 54.85 81.70 74.50 71.50 65. and validate speech results on different samples from mls eng 10k (Pratap et al., 2020). For Qwen3-4B-VL, we construct multimodal calibration set using 256 samples from the MathVista portion of the MathVerse dataset and evaluate on MMBench-en-dev (MMB) (Liu et al., 2023a), MMMU-val (MMMU) (Yue et al., 2024), MMStar (MMS) (Chen et al., 2024), OCRBench (Liu et al., 2023b), and RealWorldQA (RWQA). In both cases, we compress the models to 20% of their original size without any fine-tuning. As shown in Table 4, ROCKET preserves strong performance on Qwen3-4B-VL, achieving 65.75 average accuracy (over 90% of the original models performance). On VibeVoice, Table 5 shows near-identical speech quality: WER remains stable (0.149 vs. 0.148), and UTMOS drops only slightly (3.43 vs. 3.52), staying close to the groundtruth reference (3.73). These results demonstrate that ROCKET generalizes effectively across modalities. 7 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Table 5. ROCKET applied to VibeVoice (speech generation model) at 20% compression. Table 7. Ablation study on ROCKETs core components for Llama3-1B at 20% compression. ROCKET uses uniform budget allocation across layers. Budg. Alloc. means Budget Allocation."
        },
        {
            "title": "Method",
            "content": "WER UTMOS Ground Truth VibeVoice (dense) ROCKET 0.04 0.148 0.149 3.73 3.52 3.43 5. Ablations To evaluate the contribution of each key component in ROCKET, we conduct series of ablation studies using the Llama3-1B model. All experiments follow the evaluation protocol in Section 3.1, ensuring fair and controlled comparison. We report average accuracy across the benchmarks along with word-level WikiText perplexity. In this section, we focus on two central ablations: (1) the choice of reconstruction error metric used during layer profiling, (2) the individual contributions of ROCKETs two core components a) structured sparsification and b) dynamic per-layer budget allocation, In the appendix we also provide ablations on the effect of calibration data, and alternative sparsification strategies. All variants are evaluated at fixed global compression ratio of 20%, with all other design choices held constant, enabling precise attribution of performance differences to specific methodological choices within the ROCKET framework. 5.1. Ablation on Reconstruction Error Metric In ROCKET, the layer profiling stage enumerates set of candidate compression configurations per layer, where each candidate is defined by pair (cr, ks), namely, the compression ratio and the sparsity ratio applied to the coefficient matrix. Given the original layer weights W, each candidate (cr, ks) induces reconstructed weight matrix W. For each candidate, we compute reconstruction error, which serves as the estimated cost in the constrained multi-choice knapsack problem. The optimizer then selects one candidate per layer to minimize the sum of estimated error while satisfying the global parameter budget. We evaluate four variants for this per-candidate error estimate: (1) relative Frobenius error WF /WF (our default), (2) ℓ1 distance W1, (3) mean cosine distance across columns, and (4) spectral distance Table 6. Ablation on reconstruction error metric for layer profiling in ROCKET for Llama3-1B at 20% compression. Error Metric None (Baseline) L1 Distance Mean Cos Columns Spectral Distance Frobenius (ours) Avg. Acc. Perplexity 57.6 35.2 51.1 51.3 52.4 1.2E+01 1.8E+02 1.9E+01 1.8E+01 1.8E+01 Method None SVD-LLM CoSpaDi ROCKET ROCKET Sparse Budg. Alloc. Avg. Acc. Perplexity 57.6 37.6 42.7 45.4 52.4 1.2E+01 1.7E+02 6.4E+01 2.7E+01 1.8E+01 W2. All variants run under the same configurations. As shown in Table 6, the relative Frobenius error yields the best downstream performance, while ℓ1-based estimates lead to significant degradation, highlighting the effectiveness of this metric for effective budget allocation. 5.2. Ablation on core components To assess the contribution of ROCKETs key design choices, we compare three variants: (1) SVD-LLM which uses neither sparsification nor dynamic budget allocation ;(2) CoSpaDi, which uses K-SVD-based sparse dictionary learning; (3) ROCKET with uniform compression across layers; and (4) full ROCKET, which further incorporates optimal knapsack-based budget allocation. As shown in Table 7, we first notice that using sparsification improves both average accuracy and perplexity. Moreover, replacing CoSpaDis iterative sparsification with our closed-form, activation-aware approach is not only much faster, but also improves average accuracy from 42.7 to 45.4 and reduces perplexity from 64 to 27. Adding optimal budget allocation yields further significant gain, reaching 52.4 average accuracy and 18 perplexity, demonstrating that both our sparsification strategy and global parameter allocation are critical to ROCKETs performance. 6. Conclusion and Limitations ROCKET introduces fast, training-free LLM compression method that combines calibration-guided structured weight factorization with optimal layer-wise budget allocation via knapsack formulation. It achieves state-of-the-art performance retaining over 90% of original accuracy at 30% compression without any fine-tuning. Neverrtheless, it is important to mention that the dynamic programming solution, while efficient for standard dense models, is hard to scale to architectures with very large number of compressible components such as modern Mixture-of-Experts (MoE) models with 128 or more experts per block. This is due to the combinatorial growth in compression options and scalable alternatives are left for future work. Moreover, our healing experiments assume fixed sparsity pattern determined during the training-free compression phase, which is sub-optimal. Jointly learning adaptive sparsity patterns during fine-tuning may yield further improvements and is direction we intend to explore. 8 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression 7. Ethical Statement and Broader Impact ROCKET is training-free compression method designed to improve the efficiency and accessibility of large language models without requiring additional data or extensive computational resources for fine-tuning. By enabling highfidelity model compression with minimal environmental and economic cost, it supports more sustainable deployment of AI systems, particularly in resource constrained settings. The method does not introduce new data collection, human annotation, or model behaviors beyond those already present in the original pretrained model; thus, it neither amplifies nor mitigates existing biases in the base model. Users should remain vigilant about the ethical implications of the underlying models outputs, as ROCKET preserves its functional characteristics including potential biases or safety limitations. We encourage responsible deployment, including thorough evaluation and alignment measures when compressed models are used in real-world applications."
        },
        {
            "title": "References",
            "content": "Aharon, M., Elad, M., and Bruckstein, A. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on Signal Processing, 54(11):43114322, 2006. doi: 10.1109/TSP.2006. 881199. Ashkboos, S., Croci, M. L., Nascimento, M. G. D., Hoefler, T., and Hensman, J. Slicegpt: Compress large language models by deleting rows and columns. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=vXxardq6db. Bisk, Y., Zellers, R., Bras, R. L., Gao, J., and Choi, Y. Piqa: Reasoning about physical commonsense in natural language, 2019. Chen, L., Li, J., Dong, X., Zhang, P., Zang, Y., Chen, Z., Duan, H., Wang, J., Qiao, Y., Lin, D., et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. Chen, P. H., Yu, H.-f., Dhillon, I. S., and Hsieh, C.-j. Drone: data-aware low-rank compression for large nlp models. In Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS 21, Red Hook, NY, USA, 2021. Curran Associates Inc. ISBN 9781713845393. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge, 2018. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1032310337. PMLR, 2023. Gao, S., Hua, T., Hsu, Y.-C., Shen, Y., and Jin, H. Adaptive rank selections for low-rank approximation of language models. In Duh, K., Gomez, H., and Bethard, S. (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 227241, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.13. URL https:// aclanthology.org/2024.naacl-long.13/. Han, S., Pool, J., Tran, J., and Dally, W. J. Learning both weights and connections for efficient neural network. In Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp. 11351143, 2015. Hassibi, B. and Stork, D. Second order derivatives for In Hanson, network pruning: Optimal brain surgeon. S., Cowan, J., and Giles, C. (eds.), Advances in Neural Information Processing Systems, volume 5. MorganKaufmann, 1992. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding, 2021a. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the MATH dataset. In Vanschoren, J. and Yeung, S. (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowledge in neural network. CoRR, abs/1503.02531, 2015. URL http://arxiv.org/abs/1503.02531. Kolawole, S., Dery, L., Kagy, J.-F., Smith, V., Neubig, G., and Talwalkar, A. Everybody prune now: Structured pruning of llms with only forward passes. arXiv preprint arXiv:2402.05406, 2024. Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. Race: Large-scale reading comprehension dataset from examinations, 2017. 9 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Liu, Y., Duan, H., Zhang, Y., Li, B., Zhnag, S., Zhao, W., Yuan, Y., Wang, J., Liu, C. H. Z., Chen, K., and Lin, D. Mmbench: Is your multi-modal model an all-around player? arXiv:2307.06281, 2023a. Liu, Y., Li, Z., Huang, M., Yang, B., Yu, W., Li, C., Yin, X., lin Liu, C., Jin, L., and Bai, X. Ocrbench: On the hidden mystery of ocr in large multimodal models. 2023b. doi: 10.1007/s11432-024-4235-6. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023a. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. In Advances in Neural Information Processing Systems, 2023b. Macko, V. and Boˇza, V. Macko: Sparse matrix-vector multiplication for low sparsity. arXiv preprint arXiv: 2511.13061, 2025. Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. Building large annotated corpus of English: The Computational Linguistics, 19(2): Penn Treebank. 313330, 1993. URL https://www.aclweb.org/ anthology/J93-2004. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer sentinel mixture models, 2016. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction requiring broad discourse context, 2016. 2311.12022. URL https://doi.org/10.48550/ arXiv.2311.12022. Shopkhoev, D., Zhussip, M., Makhov, D., Ali, A., and Lefkimmiatis, S. Compressing llms via calibration-guided sparse dictionary learning. 2025. URL https://openreview.net/forum? id=oLBIcEHhxs. Cospadi: Sprague, Z., Ye, X., Bostrom, K., Chaudhuri, S., and Durrett, G. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/forum? id=jenyYQzue1. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language In The Twelfth International Conference on models. Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https: //openreview.net/forum?id=PxoFut3dWW. Suzgun, M., Scales, N., Scharli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., and Wei, J. Challenging big-bench tasks and whether chain-of-thought can solve them. In Rogers, A., Boyd-Graber, J. L., and Okazaki, N. (eds.), Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1300313051. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-ACL. 824. URL https://doi.org/10.18653/v1/ 2023.findings-acl.824. Penedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and Launay, J. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. Stanford alpaca: An instruction-following llama https://github.com/tatsu-lab/ model. stanford_alpaca, 2023. Peng, Z., Yu, J., Wang, W., Chang, Y., Sun, Y., Dong, L., Zhu, Y., Xu, W., Bao, H., Wang, Z., Huang, S., Xia, Y., and Wei, F. Vibevoice technical report. CoRR, abs/2508.19205, 2025. doi: 10.48550/ARXIV. 2508.19205. URL https://doi.org/10.48550/ arXiv.2508.19205. Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert, R. Mls: large-scale multilingual dataset for speech research. ArXiv, abs/2012.03411, 2020. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV. Wang, Q., Ke, J., Tomizuka, M., Keutzer, K., and Xu, C. Dobi-svd: Differentiable SVD for LLM compression and some new perspectives. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025a. URL https://openreview.net/forum? id=kws76i5XB8. Wang, X., Zheng, Y., Wan, Z., and Zhang, M. Svd-llm: Truncation-aware singular value decomposition for large language model compression. In International Conference on Learning Representations, 2025b. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., Li, T., Ku, ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression M., Wang, K., Zhuang, A., Fan, R., Yue, X., and Chen, W. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. Wang, Z., Wohlwend, J., and Lei, T. Structured pruning of large language models. In Webber, B., Cohn, T., He, Y., and Liu, Y. (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 61516162. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020. EMNLP-MAIN.496. URL https://doi.org/10. 18653/v1/2020.emnlp-main.496. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions, 2017. Xv, L., Gao, J., Gao, X., Liu, T., and Fu, Y. ARA: adaptive rank allocation for efficient large language model SVD compression. CoRR, abs/2510.19389, 2025. doi: 10.48550/ARXIV.2510.19389. URL https://doi. org/10.48550/arXiv.2510.19389. Yuan, Z., Shang, Y., Song, Y., Wu, Q., Yan, Y., and Sun, G. ASVD: activation-aware singular value decomposition for compressing large language models. CoRR, abs/2312.05821, 2023. doi: 10.48550/ARXIV. 2312.05821. URL https://doi.org/10.48550/ arXiv.2312.05821. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B., Yuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y., Huang, W., Sun, H., Su, Y., and Chen, W. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR, 2024. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence?, 2019. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023. doi: 10.48550/ARXIV.2311.07911. URL https:// doi.org/10.48550/arXiv.2311.07911. Zhussip, M., Shopkhoev, D., Ali, A., and Lefkimmiatis, S. Share your attention: Transformer weight sharing via matrix-based dictionary learning. arXiv preprint arXiv:2508.04581, 2025. ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression A. Proposed Algorithm In Algo. 1, we present the proposed method in the form of formal algorithm to facilitate clear and systematic understanding of the methods complete pipeline construction and implementation. Algorithm 1 ROCKET: Training-Free Heterogeneous transformer Compression Require: Pre-trained transformer with linear layers {W(ℓ)}L ℓ=1, calibration data RN d1, global parameter budget {λ = 0.5}"
        },
        {
            "title": "Ctotal",
            "content": "9: ℓ=1 end for 1DCsparse ΛrB, BCsparse2 for each candidate rank and sparsity ratio do Ensure: Compressed model with factorization {B(ℓ), C(ℓ)}L 1: Compute whitening transform: Lt chol(XX) 2: for each layer ℓ = 1 to do 3: W(ℓ) LtW(ℓ) 4: 5: 6: 7: 8: Apply Eigen Value Decomposition to find top-r eigenvectors B: WL Form coefficient matrix: BTWL Compute importance scores: impij cij L1biλ 2 Over-sparsify to ratio + β, then reactivate top entries globally to reach exact sparsity Solve for optimal left factor: arg minB W(ℓ) Reconstruct weight: (cid:102)W(ℓ) Lt Record option: cost cℓ,i nnz(D) + nnz(Csparse), error eℓ,i Store (cℓ,i, eℓ,i) in Oℓ for each state in DPℓ1 do for each option Oℓ do if eℓ,i α eref then + cℓ,i DPℓ[k] min (cid:0)DPℓ[k], DPℓ1[k] + eℓ,i 10: 11: 12: 13: 14: end for 15: Compute reference error eref from uniform compression baseline and min α that admits solution 16: Initialize DP table: DP0[0] 0, others 17: for ℓ = 1 to do 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: end for 28: Find = arg minkCtotal DPL[k] 29: Backtrack to recover optimal per-layer choices {i 30: for ℓ = 1 to do 31: 32: end for end for Prune dominated states in DPℓ: remove (k1, err1) if (k2, err2) with k2 k1 and err2 err final from option ℓ end if end for Assign D(ℓ) final, C(ℓ) ℓ }L ℓ= (cid:1) B. Norm-Preserving Properties of Low-Rank Approximation and Sparsification Let Rd1d2 with singular value decomposition (SVD) = UΣV, where Rd1r, Rd2r have orthonormal columns, Σ = diag(σ1, . . . , σr), σ1 σr > 0, and = rank(W). ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression B.1. Relative Error of Rank-k Truncated SVD The optimal rank-k approximation (1 r) is with error given by the EckartYoungMirsky theorem: Wk = UkΣkV , Since F = (cid:80)r i=1 σ2 , it follows that Wk2 = (cid:88) σ2 . i=k+1 WkF WF = (cid:115) (cid:80)r i=k+1 σ2 (cid:80)r i=1 σ2 1, with equality iff = 0. B.2. Sparsification of the Coefficients Let : Rkd2 Rkd2 be an entrywise sparsification operator satisfying (ΣkV )F ΣkV F"
        },
        {
            "title": "Define the sparsified reconstruction as",
            "content": "Since is norm-non-increasing, (cid:102)W = BkT (ΣkV ). (ΣkV )F ΣkV = WkF WF . Thus, (cid:102)WF = (ΣkV )F WF . The worst-case relative error occurs when (cid:102)W = 0, yielding (cid:102)WF WF = 1. For any non-zero sparsified reconstruction derived from the SVD basis of W, the error is strictly less than or equal to this maximum. Therefore, (cid:102)WF WF 1. B.3. Equivalence of Eigenvalue-Based Basis Construction and SVD in ROCKET In ROCKET, the weight matrix Rd1d2 is generally rectangular and non-symmetric, so eigenvalue decomposition (EVD) cannot be applied directly to W. Instead, compression operates on the whitened weight WL = LW, where = chol(XX) whitens the input activations using small calibration set RN d1. The method then computes the top-k eigenvectors of the symmetric positive semi-definite matrix WLW . This EVD yields where Bk Rd1k has orthonormal columns (B Bk = Ik) and Λk = diag(λ1, . . . , λk) with λi 0."
        },
        {
            "title": "Let the compact singular value decomposition of WL be",
            "content": "WLW = BkΛkB , WL = UΣV. 13 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression"
        },
        {
            "title": "Then",
            "content": "WLW = UΣ2U, which is precisely the eigenvalue decomposition of WLW singular vectors: Bk = Uk and λi = σ2 . Therefore, the eigenbasis coincides exactly with the left . The coefficient matrix is obtained via orthogonal projection: WL = ΣkV , WL = Ck = which matches the right factor in the SVD. Hence, the reconstruction WL = BkCk is identical to the rank-k truncated SVD of WL. Sparsification is applied to Ck using an operator : Rkd2 Rkd2 satisfying"
        },
        {
            "title": "The sparsified approximation in the whitened space is defined as",
            "content": "T (Ck)F CkF . (cid:102)WL = Bk (Ck). Since Bk has orthonormal columns, the Frobenius norm is preserved under multiplication: Consequently, the relative reconstruction error in the whitened space satisfies (cid:102)WLF = (Ck)F CkF = WLF . WL (cid:102)WLF WLF 1, with equality only in the degenerate case (cid:102)WL = 0. Because the EVD-derived basis Bk is mathematically identical to the left singular vectors of WL, all norm-preserving properties and error bounds established for truncated SVD carry over verbatim. Finally, the compressed weight in the original space is recovered as (cid:102)W = L1 (cid:102)WL. Although L1 is not orthogonal, the theoretical guarantees in the whitened space where the core approximation occurs remain intact. Thus, despite using EVD of WLW for computational efficiency, ROCKET achieves the same norm constraints and relative error bound ( 1) as classical SVD-based low-rank approximation followed by norm-non-increasing sparsification. C. Mapping MCKP to Graph Theory. We reformulate the constrained multi-choice knapsack problem (MCKP) as shortest-path problem on directed acyclic graph (DAG), where the key to enforcing the global compression ratio lies in clever target-aware sink connectivity rule. Each node is labeled (ℓ, p), with ℓ {0, . . . , L} denoting the layer index and Z0 representing the scaled (cid:5) using scale = cumulative number of parameters pruned up to layer ℓ, discretized via = (cid:4)scale (cid:80)ℓ param precision/Ctotal. For each feasible compression option in layer ℓ, we add an edge from (ℓ1, pin) to (ℓ, pout) with pout = pin + scale cℓ,i and edge cost error scale factor eℓ,i, while discarding any option violating the per-layer error cap eℓ,i αeref. Crucially, we connect terminal node (L, p) to the sink if and only if pmin = scale (1 ρtarget) Ctotal, where ρtarget is the desired compression ratio. This structural embedding of the global budget constraint via node naming and selective sink connectivity ensures that any path reaching the sink automatically satisfies the target compression, transforming the constrained combinatorial optimization into an unconstrained shortest-path search solvable exactly (up to discretization) by Dijkstras algorithm. The graph thus encodes both layer-wise flexibility and global feasibility, with complexity rendered tractable by limiting the candidate space per layer to modest grid of compression ratios and ks sparsity levels. ℓ=1 cℓ,i This graph formulation underlies the dynamic programming procedure in Algorithm 1, where states correspond to nodes and transitions to edges. 14 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Start (0 layers) (0, 0) (1, p(3) 1 ) (1, p(2) 1 ) (1, p(1) 1 ) After layer 0 (2, p(4) 2 ) (2, p(3) 2 ) (2, p(2) 2 ) = pmin (L, p(3) ) Feasible sink (L, p(2) ) (L, pmin) (L, p(1) ) After layer L1 (2, p(1) 2 ) After layer 1 State (i, p): = number of layers processed = scaled total parameters kept Transition: pick option for layer Sink: only if pmin Figure 4. Exact state-space graph matching. Each state (i, p) represents having processed the first layers with scaled parameters retained. From (i, p), the algorithm branches to all options for layer i, producing states (i + 1, + p). The sink is reachable only from states with pmin, enforcing the global compression ratio by construction. D. Inference Optimization To accelerate inference, we leverage MACKO (Macko & Boˇza, 2025), specialized sparse matrixvector multiplication kernel that outperforms PyTorchs built-in implementation for structured, column-wise sparse matrices. Because ROCKET employs calibration-guided, adaptive compression strategy, the sparsity level quantified by the ratio k/s of dictionary atoms to nonzeros per coefficient column varies across layers. This heterogeneity arises from the solution to constrained multi-choice knapsack problem, which allocates parameters to layers based on their marginal contribution to reconstruction fidelity under global budget. Empirically, we observe that MLP layers (gate and up projections) are assigned significantly higher sparsity and lower rank than attention projections (query, key, value, out). This behavior is theoretically justified: MLP weight matrices are substantially larger (dmodel 4dmodel) than attention matrices (dmodel dmodel), and their calibration-aware reconstruction error increases more slowly with sparsification. Consequently, the optimizer preferentially compresses MLP layers to maximize parameter savings while preserving overall model accuracy. Figure 5 compares the runtime of MLP projections in Qwen3-8B using either PyTorchs default sparse kernels or MACKO. For large, moderately sparse coefficient matrices (e.g., gate and up projections), MACKO provides consistent speedups; for smaller or denser matrices (e.g., down projections), both implementations perform similarly. In attention layers, where weight matrices are small and less aggressively compressed, we retain PyTorchs native implementation, as it proves faster in practice. Additionally, we fuse dictionaries and coefficients for layers sharing the same input namely, {query, key, value} in attention and {gate, up} in the MLP before applying MACKO. In terms of theoretical floating-point operations (FLOPs), ROCKET is structurally analogous to CoSpaDi: both represent weight matrix Rd1d2 as BC, where Rd1k is dense dictionary and Rkd2 is column-wise sparse. Assuming optimal reuse of the intermediate product XD, the FLOP count is d1Kactive + sd2, where Kactive is the number of distinct active atoms. Under fixed global parameter budget, the total FLOP count may be similar between the two methods. However, key distinction lies in budget allocation: CoSpaDi typically enforces uniform sparsity across layers, whereas ROCKET dynamically assigns heterogeneous (kℓ, sℓ) pairs per layer based on reconstruction sensitivity. This results in lower FLOPs in large, robust layers (e.g., MLP) and higher FLOPs in small, sensitive layers (e.g., attention) even though the latter contribute minimally to total computation due to their size. The net effect is more balanced per-layer runtime profile and better utilization of sparse kernels, which explains the consistent throughput advantage of ROCKET over CoSpaDi  (Table 8)  despite comparable theoretical operation counts. Table 8. Throughput (tokens/s) for Qwen3-8B with batch size = 1 and context length = 256."
        },
        {
            "title": "Method",
            "content": "20% 30% 40% SVD-LLM 24.36 25.45 CoSpaDi 26.74 ROCKET 24.31 25.76 26.60 24.72 25.62 26.36 15 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Figure 5. Comparison between PyTorchs built-in sparse-matrix vector multiplication and MACKO across MLP layers in Qwen3-8B. MACKO shows consistently better running time for large coefficients (gate and up projections), while being on par with PyTorch for smaller sparse matrices (down projection). D.1. Environmental Impact As shown in Table 9, the ROCKET method not only achieves superior compression performance but also offers dramatic environmental benefits. Compared to COSPADI, ROCKET consumes over 100 times less energy, completes compression 96 times faster, and produces 23 times lower CO2 emissions. These results highlight ROCKET as both high-performance and environmentally sustainable solution. Table 9. Comparison of energy consumption, runtime, and CO2 emissions for COSPADI and ROCKET using the Llama3-1b model. Method Model GPU CPU Energy Consumed (kWh) Duration (s) CO2 Emissions (kg eq) CoSpaDi Llama3-1b ROCKET Llama3-1b 1 NVIDIA A100-SXM4-40GB AMD EPYC 7742 64-Core Processor 1 NVIDIA A100-SXM4-40GB AMD EPYC 7742 64-Core Processor 7.88 0. 90080.97 930 0.782 0.0337 E. Further results E.1. Detailed Comparison with CoSpaDi and SVD-LLM We begin this section by providing the detailed per-benchmark results from Figure 2 in Tables 10, 11. As shown, ROCKET outperforms CosPaDi and SVD-LLM by large margin across different benchmarks and compression ratios. Table 10. ROCKET comparison vs low-rank and SDL counterparts in data-aware scenarios on Llama3.2-1B at different compression ratios (CR). Best results are provided in bold All experiments are in training-free setup. Method CR Llama3.2 1B SVD-LLM CoSpaDi ROCKET SVD-LLM CoSpaDi ROCKET SVD-LLM CoSpaDi ROCKET SVD-LLM CoSpaDi ROCKET 0.2 0. 0.4 0.5 PIQA Hella Swag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Wiki Text LAMBADA Accuracy Perplexity 74.5 62.1 66.1 71.9 55.7 56. 66.3 51.8 53.5 63.9 51.1 51. 57.3 63.7 36.4 42.9 56.7 30. 32.4 46.8 27.3 28.2 39.4 26. 27.0 31.6 63.0 24.4 38.4 9.1 18.2 38.0 1.3 3.8 23. 0.0 0.3 9.4 60.5 36.0 39. 56.7 30.5 31.9 47.9 26.9 27. 41.0 26.1 26.3 34.9 36.2 25. 26.0 32.4 21.5 22.1 27.4 22. 23.0 25.7 25.9 24.0 22.6 88. 64.9 71.6 88.6 45.9 56.7 79. 32.3 36.9 72.1 26.1 29.5 37.8 29.0 31.7 36.4 25.8 28. 34.1 24.4 24.0 30.9 23.9 24. 26.1 37.0 23.0 24.8 30.8 23. 23.1 27.4 23.0 23.1 23.0 23. 23.3 22.9 57.6 37.6 42.7 52. 30.2 33.7 45.9 26.2 27.5 39. 25.4 25.8 31.9 1.2E+01 1.7E+02 6.4E+ 1.8E+01 5.9E+02 2.9E+02 3.5E+01 1.6E+03 8.0E+ 8.8E+01 3.1E+03 1.8E+03 3.3E+02 5.7E+00 1.7E+ 3.5E+01 1.3E+01 2.5E+03 6.6E+02 2.6E+01 3.3E+ 9.2E+03 1.3E+02 1.0E+05 7.3E+04 2.2E+03 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression Table 11. Performance comparison of ROCKET vs SOTA SVD-LLM methods on Llama3-8B at different compression ratios (CR). Best results are highlighted with bold."
        },
        {
            "title": "Method",
            "content": "CR Llama3 8B SVD-LLM"
        },
        {
            "title": "CoSpaDi",
            "content": "0."
        },
        {
            "title": "ROCKET",
            "content": "SVD-LLM"
        },
        {
            "title": "CoSpaDi",
            "content": "0."
        },
        {
            "title": "ROCKET",
            "content": "SVD-LLM"
        },
        {
            "title": "CoSpaDi",
            "content": "0."
        },
        {
            "title": "ROCKET",
            "content": "SVD-LLM"
        },
        {
            "title": "CoSpaDi",
            "content": "0."
        },
        {
            "title": "ROCKET",
            "content": "PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. WikiText LAMBADA Accuracy Perplexity 80.7 71. 75.2 76.9 65.8 70.5 76.8 60. 63.7 71.9 55.1 58.4 79.1 58. 66.5 74.8 46.4 56.2 69.5 34. 41.4 60.4 24.7 31.8 75.6 59. 73.8 73.6 38.1 61.3 70.4 11. 30.3 59.3 1.8 13.6 77.7 55. 66.5 73.7 41.9 54.2 70.5 32. 39.1 64.4 26.0 30.7 53.5 34. 41.6 47.1 27.7 33.5 42.3 24. 26.6 36.4 23.1 24.8 93.9 86. 89.5 92.7 70.0 85.7 90.9 44. 68.5 88.1 30.5 46.2 40.3 35. 38.2 40.7 31.8 36.2 39.7 25. 30.5 36.8 22.4 25.8 62.2 32. 42.8 54.9 27.2 32.2 47.8 23. 25.4 39.7 21.0 23.4 70.4 54. 61.8 66.8 43.6 53.7 63.5 32. 40.7 57.1 25.6 31.8 7.3E+00 4.1E+ 2.0E+01 3.1E+00 1.1E+01 4.3E+00 1.1E+01 3.8E+ 1.5E+02 4.5E+01 6.1E+01 9.2E+00 1.5E+01 4.4E+ 5.5E+02 1.8E+02 1.3E+03 1.2E+02 2.9E+01 8.1E+ 1.8E+03 7.4E+02 6.5E+03 5.2E+02 E.2. Scaling Behavior Across Model Sizes ) o s p n f % ( g 100 90 80 60 50 0 Average Accuracy vs. Model Size 5 15 20 25 30 35 Model Size (Billion Parameters) CR = 0.2 CR = 0.4 Figure 6. Average accuracy (relative to uncompressed model) as function of model size for two compression ratios. In Figure 6, we present the results of compressing Qwen models of varying sizes from 0.6B to 32B parameters at two compression ratios (20% and 40%). The results show that larger models retain higher fraction of their original (uncompressed) performance after compression. This suggests that larger models may still be significantly underfitted relative to their capacity. 17 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression E.3. Evaluation on Advanced Benchmarks Table 12. Performance comparison of ROCKET against CoSpaDi and SVD-LLM on new set of benchamrks."
        },
        {
            "title": "Method",
            "content": "CR IfeVal BBH MATH GPQA MUSR MMLU-Pro Accuracy Qwen3 8B 39.21 60.86 52. 1.06 1.96 SVD-LLM 0.2 25.54 41."
        },
        {
            "title": "ROCKET",
            "content": "0.2 0.2 28.90 45.25 31.89 50. 11.10 SVD-LLM 0.3 22.90 34."
        },
        {
            "title": "ROCKET",
            "content": "0.3 0.3 25.18 38.22 25.54 47. SVD-LLM 0.4 22.66 30."
        },
        {
            "title": "ROCKET",
            "content": "0.4 0.4 26.14 23.99 32.72 40. 0.98 0.98 2.64 0.83 0.76 1. 36.16 28.36 28.61 31.45 25.59 24. 29.19 23.07 26.01 28.85 43.12 39. 42.06 39.68 41.40 38.36 39.94 37. 38.10 41.00 47.72 26.30 31.46 36. 18.82 22.81 32.27 11.55 16.74 25. In Table 12 we compare against other methods on new set of benchmarks. These modern benchmarks IFEVal (Zhou et al., 2023), BBH (Suzgun et al., 2023), MATH(Hendrycks et al., 2021b), GPQA(Rein et al., 2023), MuSR(Sprague et al., 2024), and MMLU-Pro(Wang et al., 2024) represent an evolution in the evaluation of large language models (LLMs), targeting more rigorous, diverse, and realistic capabilities. IfeVal (Instruction-Following Evaluation) assesses models ability to follow precise, verifiable natural language instructions. BBH (Big-Bench Hard) isolates 23 of the most challenging tasks from the original BIG-Bench suite to probe complex reasoning. MATH evaluates advanced mathematical problem-solving across algebra, geometry, and other domains. GPQA tests graduate-level scientific knowledge with high difficulty and minimal data contamination risk. MuSR (Multi-step Reasoning) focuses on multi-hop and long-context reasoning, while MMLU-Pro enhances the original MMLU by increasing answer choices (from 4 to 10) and reducing ambiguity, thereby offering cleaner, more demanding assessment of expert knowledge. Unfortunately other papers are still following the old benchmarks therefore for fair comparison we stick with them for comparison in the main paper. E.4. Post-Compression Healing In Table 13, we provide the per-benchmark results corresponding to the summary in Table 3. As noted, we are approaching the performance of models trained from scratch and in some cases nearly matching or surpassing them despite using compressed model. Specifically, we compressed Qwen-14B to an 8B model and applied very limited fine-tuning phase (using only approximately 30 million tokens) while keeping the sparsity pattern fixed an approach that is known to be suboptimal. Nevertheless, the resulting model achieves performance comparable to the original Qwen3-8B trained from scratch. We expect that fine-tuning with higher quality, carefully curated data would further improve results. Moreover, as previously mentioned, enabling trainable sparsity patterns remains direction for future work. Table 13. Post-compression healing results on Qwen3-8b models. Method Qwen3-14B (dense) Qwen3-8B (dense) ROCKET-Qwen3-8B (training-free) ROCKET-Qwen3-8B (healed) PIQA HellaSwag Lambada ARC-e ARC-c SciQ Race MMLU WikiText Preplexity Avg. Acc. 79.86 77.70 72.68 78.51 78. 74.90 62.63 74.67 67.88 64.10 70.26 65.55 82.82 80.70 67.76 75.29 60. 96.50 43.25 56.70 44.19 53.07 95.70 91.20 93.50 40.90 39.80 37.89 77. 73.00 59.99 65.23 1.1E+01 3.7E+00 1.2E+01 2.5E+01 1.3E+01 4.6E+00 3.8E+00 4.7E+00 73. 70.46 63.56 67.96 18 ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression F. Further Ablations F.1. Ablation on calibration data Table 14. Ablation on calibration data for Llama3-1B at 20% compression (CR = 0.2). The first row shows the uncompressed baseline. Higher accuracy and lower perplexity are better. Calibration Data CR PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Acc. WikiTextword LAMBADAPPL Accuracy Perplexity None (Baseline) RefinedWeb PTB WikiText Alpaca 0 0. 0.2 0.2 0.2 74.5 71.9 70. 70.7 73.7 63.7 56.7 54.9 56. 57.2 63.0 46.0 47.3 50.9 53. 60.5 56.7 55.2 57.9 57.9 36. 32.4 31.9 33.6 34.9 88.3 88. 87.8 88.1 87.5 37.8 36.4 34. 35.5 35.4 37.0 30.8 26.7 28. 30.5 57.6 52.4 51.1 52.7 53. 1.2E+01 1.8E+01 2.1E+01 1.7E+01 2.0E+01 5.7E+ 1.3E+01 1.3E+01 1.1E+01 9.7E+00 We evaluate the sensitivity of ROCKET to the choice of calibration dataset by comparing four sources: RefinedWeb (Penedo et al., 2023), PTB (Marcus et al., 1993), WikiText (Merity et al., 2016), and Alpaca (Taori et al., 2023). As shown in Table 14, while instruction-tuned data such as Alpaca yield slightly higher average accuracy (53.8 vs. 52.4), the differences across datasets are relatively small, confirming that ROCKET is robust to the calibration data choice. Nevertheless, to ensure fair comparison with CoSpaDi, which uses RefinedWeb as its default calibration data, we adopt RefinedWeb for all primary experiments reported in this paper. F.2. Ablation on the Sparsification Strategy Table 15. Ablation on sparsification strategies for Llama3-1B at 20% compression (CR = 0.2). All methods use the same calibration data and global parameter budget. Sparsification Strategy None (Baseline) ROCKET Per-Row Sparsification Global Importance Sparsification Whitened-Space Importance Only CR 0 0.2 0.2 0. 0.2 Accuracy Perplexity PIQA HellaSwag LAMBADA ARC-e ARC-c SciQ Race MMLU Avg. Acc. WikiTextword LAMBADAPPL 74. 71.9 67.8 69.5 71.5 63.7 56. 48.1 54.7 55.7 63.0 46.0 32. 42.7 47.3 60.5 56.7 48.6 53. 56.3 36.2 32.4 29.4 30.7 32. 88.3 88.6 79.9 85.7 87.9 37. 36.4 31.6 35.5 35.4 37.0 30. 26.7 28.1 28.9 57.6 52.4 45. 50.1 52.0 1.2E+01 1.8E+01 3.2E+01 1.9E+ 1.9E+01 5.7E+00 1.3E+01 3.8E+01 1.7E+01 1.3E+ To evaluate the impact of our sparsification strategy, we compare ROCKET against three alternative approaches for pruning the coefficient matrix = ΣV: (1) Per-Row Sparsification, where importance scores are computed identically but sparsity is enforced independently per row (breaking column-wise structure); (2) Global Importance Sparsification, which ignores any structural constraints and simply zeros out the globally least-important entries based on the full importance matrix; and (3) Whitened-Space Importance Only, which disables the original-space fidelity term by setting λ = 0 in Eq. (8) (Theoretically optimal with respect to the whitened space). Table 15 show that ROCKETs column-aware, activation-and-weight-balanced sparsification outperforms alternatives, demonstrating the efficiency of both structural awareness and dual-space importance scoring."
        }
    ],
    "affiliations": [
        "Department of Computer Science, ITMO University, Saint-Petersburg, Russia",
        "MWS AI, Moscow, Russia"
    ]
}