{
    "paper_title": "Reconstructing 4D Spatial Intelligence: A Survey",
    "authors": [
        "Yukang Cao",
        "Jiahao Lu",
        "Zhisheng Huang",
        "Zhuowei Shen",
        "Chengfeng Zhao",
        "Fangzhou Hong",
        "Zhaoxi Chen",
        "Xin Li",
        "Wenping Wang",
        "Yuan Liu",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reconstructing 4D spatial intelligence from visual observations has long been a central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer a comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present a new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 -- reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 -- reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 -- reconstruction of 4D dynamic scenes; (4) Level 4 -- modeling of interactions among scene components; and (5) Level 5 -- incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence."
        },
        {
            "title": "Start",
            "content": "Reconstructing 4D Spatial Intelligence: Survey Yukang Cao, Jiahao Lu, Zhisheng Huang, Zhuowen Shen, Chengfeng Zhao, Fangzhou Hong, Zhaoxi Chen, Xin Li, Wenping Wang, Yuan Liu, Ziwei Liu 1 AbstractReconstructing 4D spatial intelligence from visual observations has long been central yet challenging task in computer vision, with broad real-world applications. These range from entertainment domains like movies, where the focus is often on reconstructing fundamental visual elements, to embodied AI, which emphasizes interaction modeling and physical realism. Fueled by rapid advances in 3D representations and deep learning architectures, the field has evolved quickly, outpacing the scope of previous surveys. Additionally, existing surveys rarely offer comprehensive analysis of the hierarchical structure of 4D scene reconstruction. To address this gap, we present new perspective that organizes existing methods into five progressive levels of 4D spatial intelligence: (1) Level 1 reconstruction of low-level 3D attributes (e.g., depth, pose, and point maps); (2) Level 2 reconstruction of 3D scene components (e.g., objects, humans, structures); (3) Level 3 reconstruction of 4D dynamic scenes; (4) Level 4 modeling of interactions among scene components; and (5) Level 5 incorporation of physical laws and constraints. We conclude the survey by discussing the key challenges at each level and highlighting promising directions for advancing toward even richer levels of 4D spatial intelligence. To track ongoing developments, we maintain an up-to-date project page: https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence. Index Terms4D spatial intelligence, low-level cues, scene reconstruction, dynamics modeling, interactions, physics, video"
        },
        {
            "title": "1 INTRODUCTION",
            "content": "T HE automatic reconstruction of 4D spatial intelligence using machine learning or deep learning techniques has long been crucial and challenging problem in computer vision. By capturing both the static configurations and dynamic changes over time, 4D spatial intelligence shall provide comprehensive representation and understanding of the spatial environments that integrate the threedimensional geometric structures with their temporal evolution. This field has attracted significant attention due to its wide range of applications in video games [1], movies [2], and immersive experiences (AR/VR) [3], [4], where highfidelity 4D scenes serve as the foundation for delivering realistic user experiences. Beyond these applications that primarily focus on the fundamental components of 4D spatial intelligence namely low-level cues such as depth, camera pose, point map, and 3D tracking, as well as scene composing elements and dynamics spatial intelligence also plays pivotal role in advancing embodied AI [5], [6], [7] and world models [8]. These latter domains place strong emphasis on the interactions among scene components and the physical plausibility of the reconstructed This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2 (MOE-T2EP20221-0012, MOE-T2EP20223-0002), and under the RIE2020 Industry Alignment Fund Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s). (Corresponding author: Ziwei Liu.) Y. Cao, F. Hong, Z. Chen, and Z. Liu are with S-Lab, College of Computing and Data Science, Nanyang Technological University, Singapore 639798. E-mail: yukang.cao@ntu.edu.sg, fangzhou.hong@ntu.edu.sg, zhaoxi.chen@e.ntu.edu.sg, ziwei.liu@ntu.edu.sg J. Lu, C. Zhao, Y. Liu are with Intelligent Graphics Lab, The Hong Kong University of Science and Technology. E-mail: lujiahao@mail.ustc.edu.cn, zhaochf.afterjourney@gmail.com, yuanly@ust.hk Z. Shen, Z. Huang, X. Li, and W. Wang are with Texas A&M University. E-mail: mickshen@tamu.edu, hzs@tamu.edu, xinli@tamu.edu, wenping@tamu.edu environments. In recent years, techniques for reconstructing 4D spatial intelligence have seen rapid advancements. Several surveys [9], [10] have provided valuable perspectives from various angles and have highlighted persistent challenges in the field. For example, [11], [12], [13] reviewed the recent process in deep stereo matching to obtain the low-level scene information; [14], [15], [16] offered comprehensive overview of advances in 3D scene reconstruction, covering range of input modalities and diverse 3D representations; [9], [10] classified dynamic 4D scene reconstruction methods into categories based on their core architectural principles. However, the field has advanced considerably, driven by the emergence of novel 3D representations [17], [18], [19], high-quality video generation techniques [20], [21], [22] that provide richer input data, and more efficient models capable of delivering superior reconstruction quality. Despite these strides, additionally, none of the existing surveys thoroughly examines the different compositional levels of the dynamic 4D scenes, nor do they offer detailed analysis of their respective developments and open challenges. This would potentially lead to fragmented understanding that overlooks critical components. These gaps highlight the need for comprehensive, up-to-date survey that systematically categorizes 4D spatial intelligence into distinct levels, consolidates recent advancements, and maps the evolving landscape of 4D scene reconstruction. Driven by this urgent situation, we categorize the existing methods for reconstructing 4D spatial intelligence into five levels and provide structured overview of their respective advances: Level 1 reconstruction of low-level 3D cues. At Level 1, the system targets the reconstruction of fundamental 3D cues namely, depth, camera pose, point maps, and 3D tracking. These low-level cues define the 5 2 0 2 8 ] . [ 1 5 4 0 1 2 . 7 0 5 2 : r 2 Fig. 1. Classification of 4D spatial intelligence by level. Specifically, in this survey, we categorize the methods of reconstructing 3D spatial intelligence from video into five levels: (1) low-level 3D cues, (2) 3D scene components, (3) 4D dynamic scenes, (4) modeling of interactions among scene components, and (5) incorporation of physical laws and constraints. core structure of 3D scene. Traditionally, this task has been broken down into separate subfields such as keypoint detection [23], [24], [25] and matching [26], [27], [28], [29], robust estimation [28], [30], Structure-from-Motion (SfM) [31], [32], [33], [34], Bundle Adjustment (BA) [35], [36], [37], [38], and dense Multi-View Stereo (MVS) [39], [40], [41], [42], [43]. Recent approaches like DUSt3R [44] and its followups [45], [46], [47], [48] aim to jointly solve these subproblems, enabling more integrated and collaborative reasoning. Building on transformer-based advances [49], [50], [51], [52], [53], VGGT [54] further introduces an end-to-end framework that rapidly estimates these low-level 3D cues within seconds. Level 2 reconstruction of 3D scene components. On top of level 1, level 2 methods move beyond basic 3D cues to reconstruct individual scene elements such as humans, objects, and buildings. While some methods may involve the composition or spatial arrangement of these elements within scene, they generally do not model or enforce the correctness of their interactions. Recent methods for this level leverage the innovations in 3D representations like NeRF [55], 3D Gaussians [56], and meshes (DMTET [18], FlexiCube [57]) to improve the reconstructed fine-scale details, rendering efficiency, and global structural coherence, making the results ideal for photorealistic scene reconstruction and immersive virtual experiences. Level 3 reconstruction of 4D dynamic scenes. Level 3 incorporates dynamics into reconstructed 4D scenes, marking key step toward enabling the bullet time experience of 4D spatial intelligence and delivering more immersive visual content. Existing approaches can generally be categorized into two main directions. The first line of work [58], [59], [60], [61], [62] reconstructs static canonical radiance field and models temporal changes through learned deformations over time. In contrast, the other type of techniques [63], [64], [65], [66], [67], [68], [69] encode time directly as an additional parameter within the 3D representation, allowing for continuous modeling of scene dynamics. Level 4 modeling of interactions among scene components. Advancing beyond the reconstruction of low-level cues, scene elements, and dynamics, Level 4 of spatial intelligence enters more mature phase focusing on modeling interactions between different components within scene. Given that humans are often the central agents of interaction, early works [70], [71], [72], [73], [74] primarily concentrated on capturing the motion of humans and manipulated objects observable in video inputs. With recent progress in 3D representations, recent methods [75], [76], [77], [78], [79], [80] have achieved more accurate reconstructions of both human and object appearances. Furthermore, the study of humanscene interactions [81], [82], [83], [84], [85] has gained traction, serving as foundational step toward constructing comprehensive world models. Level 5 Incorporation of physical laws and constraints. While Level 4 systems are capable of modeling interactions between different scene components, they typically overlook underlying physical principles such as gravity, friction, and pressure. As result, these methods may fall short in applications like embodied AI [5], [6], [7], where the goal is often to enable real-world robots to imitate actions and interactions observed in videos. Level 5 systems address this limitation by focusing on enforcing physical plausibility within reconstructed 4D scenes. Recent approaches [86], [87], [88], leveraging platforms such as IsaacGym [89] and reinforcement learning techniques [90], [91], [92], have demonstrated the ability to learn and replicate human-like skills directly from video data, marking significant step toward physically grounded spatial intelligence. Beyond human-related applications, the physical modeling of general 3D objects, such as simulating object deformation, collisions, and dynamics, as well as physical scenes, has also become an active area of research [93], [94], [95], expanding the scope and applicability of Level 5 reconstruction systems. Scope. This survey primarily focuses on approaches for reconstructing 4D scenes from video inputs. Specifically, 3 we examine key developments and representative works across our defined Levels 1 through 5 of the 4D sptial intelligence. The papers reviewed are predominantly drawn from leading conferences and journals in computer vision and computer graphics, along with select preprints released on arXiv in 2025. Our selection criteria emphasize relevance to the scope of this survey, with the goal of providing comprehensive overview of recent rapid progress in the field. We do not include the 3D generation methods [96], [97], [98] and 4D generation approaches [99], [100], [101], [102], [103], [104] based on generative video diffusion models [20], [21], [22], as these methods typically yield single type of input and have limited direct relevance to 4D reconstruction techniques. Additionally, this survey does not delve into detailed analysis of various 3D representations. Readers interested in these complementary areas are encouraged to read existing surveys on 4D generation [105], [106], [107], [108] and the evolution of 3D representation methods [10], [15], [109], [110]. Organization. An overview of the different levels of 4D spatial intelligence is illustrated in Fig. 1. In the following sections, we introduce taxonomy that organizes recent research efforts according to the evolving process of reconstructing five key levels from video inputs: low-level 3D cues (Sec. 2), basic 3D scene components (Sec. 3), dynamic 4D scenes (Sec. 4), interaction between scene components (Sec. 5), and physics modeling (Sec. 6). The overall structure of the survey is summarized in Fig. 1. Finally, in Sec. 7, we critically reflect on current methodologies, identify open challenges at each level of spatial intelligence, and discuss future directions for advancing 4D spatial intelligence beyond these five defined levels."
        },
        {
            "title": "2 LEVEL 1 – LOW-LEVEL 3D CUES",
            "content": "Depth, camera pose, and 3D tracking are commonly regarded as low-level cues in 3D scene modeling. These parameters capture the fundamental geometric and positional structure of the environment, forming the basis for higherlevel tasks such as object reconstruction, scene composition, and physical interaction modeling. In this sense, they function similarly to pixels and edges in 2D vision. As such, we define the reconstruction of these elements as level 1 of 4D spatial intelligence. The paradigms of the methods for obtaining these low-level cues from videos are illustrated in Fig. 3. They can be further categorized according to their respective objectives and the type of input videos."
        },
        {
            "title": "2.1 Depth estimation",
            "content": "Video-based depth estimation aims to generate accurate and temporally consistent depth maps from RGB video sequences. Early approaches typically relied on inferencetime optimization to align depth across frames [111], or employed self-supervised warping using estimated egomotion and optical flow [112], [113], [114], often further enhanced by test-time refinement [115], [116]. While effective, these methods are computationally expensive and heavily dependent on the accuracy of pose and flow estimations. To address these challenges, feed-forward architectures have been introduced. Cost-volumebased methods construct 3D 4 ods [150], [151], [152], [153], [154], [155], [156] have been proposed to combine learning-based techniques with geometric insights, leveraging the strengths of both paradigms. More recently, to reduce reliance on manual hyperparameter tuning required by these hybrid methods, further studies [157], [158] have explored reinforcement learning for adaptive decision-making in VO systems. It is also worth noting that VO is closely related to Visual Simultaneous Localization and Mapping (VSLAM), which extends VO by concurrently constructing map of the environment. Methods that jointly estimate camera pose and dense depth for mapping purposes will be discussed in later section on unified camera pose and depth estimation from video. 2.3 3D tracking 3D tracking estimation aims to recover the motion of scene elements in dynamic videos, providing temporally coherent correspondences in 3D space. notable approach in this area is OmniMotion [159], which represents an input video using quasi-3D canonical volume and performs dense, pixel-wise tracking by establishing bijective mappings between the local input space and the canonical space. Through per-video optimization, it jointly estimates the motion trajectories across the entire sequence, enabling consistent tracking over time. Building upon this framework, OmniTrackFast [160] enhances both computational efficiency and robustness by factorizing the underlying function representation into local spatiotemporal feature grid, and further improves the models expressiveness by introducing non-linear functions into the coupling blocks. In contrast to these optimization-heavy methods, SpatialTracker [161] proposes feed-forward architecture that supports longrange 3D tracking across videos without the need for testtime optimization, offering more scalable and efficient alternative. SceneTracker [162] employs an iterative strategy to approximate the optimal 3D trajectory, dynamically indexing and constructing both appearance correlation and depth residual features in parallel. DELTA [163] introduces coarse-to-fine trajectory estimation strategy, allowing for efficient dense tracking across the entire frame rather than being limited to sparse set of locations. Seurat [164] derives depth directly from 2D tracking inputs to recover 3D trajectories. TAPIP3D [165] constructs spatio-temporal feature clouds from videos by utilizing depth and camera motion information to project 2D video features into 3D world space, where the effects of camera movement are effectively neutralized. Together, these methods illustrate the evolving landscape of 3D tracking, spanning from optimizationbased pipelines to fully end-to-end learning systems."
        },
        {
            "title": "2.4 Unified 3D cues modeling",
            "content": "Accurate 4D scene reconstruction requires not only highquality geometry but also coherent spatial-temporal understanding across frames. To achieve this, recent research has moved toward unified frameworks that jointly estimate depth, camera pose, and even 3D tracking from video. These joint approaches reduce the inconsistencies and ambiguities that often arise when these components are estimated independently, leading to more robust and temporally consistent Fig. 2. The paradigms of methods for reconstructing low-level cues from video input. (I) Video-based depth reconstruction methods recently leverage the diffusion model to obtain the depth maps; (II) Methods for reconstructing camera pose from video input typically employ the neural network to infer the camera pose based on the encoded image features; (III) 3D tracking methods uses point tracker and transformers to achieve 3D tracking from video input; (IV) Recent methods, such as VGGT, apply DINO to extract the features and then train transformerbased DPT heads to infer the unified 3D attributes. Enc., Dec., Spt. Grid, Qry. Points, and Cam. denote Encoder, Decoder, Supporting Grid, Query Points, and Camera Head correspondingly. matching volumes to enforce temporal coherence [117], [118], [119], [120], while flow-guided approaches integrate optical flow cues directly [121], [122]. Recurrent models leverage temporal recurrence to iteratively refine predictions across frames [123], [124], and attention-based mechanisms dynamically reweight temporal features [125], [126], [127]. Other notable feed-forward systems include [126], [128], [129], [130], [131]. More recently, large-scale pretraining and diffusion-based frameworks have pushed the frontier further. DepthCrafter [132], ChronoDepth [133], and DepthAnyVideo [134] leverage video diffusion models to generate depth sequences directly, while [135] extends the ViT-based Depth Anything V2 [136] for video depth estimation. These models exhibit strong temporal consistency and robust generalization across diverse scenes. Overall, the field has progressed from optimization-heavy, posedependent pipelines to efficient feed-forward networks, and most recently, to pretrained, diffusion-driven models that achieve both high accuracy and temporal coherence."
        },
        {
            "title": "2.2 Camera pose estimation",
            "content": "Camera pose estimation from RGB videos can be generally solved by Visual Odometry (VO) algorithms, which are widely applied in robotics applications. Classical geometrybased VO methods are typically categorized into two groups: feature-based and direct approaches. Feature-based VO [137], [138], [139], [140], [141] estimates camera motion by detecting and tracking visual features across frames, while direct VO [142], [143], [144], [145] infers motion by minimizing photometric error or applying feature warping [146]. With the advent of deep learning, learning-based VO methods [147], [148], [149] have gained prominence, often outperforming traditional approaches in controlled settings but facing challenges in generalizing to unseen environments. To overcome these limitations, hybrid methreconstructions. The field has seen wide range of solutionsfrom optimization-based pipelines that refine predictions per video, to end-to-end feed-forward architectures designed for efficiency and scalability. In this section, we review representative methods that integrate one or more of these key components, highlighting their methodological designs, trade-offs, and contributions to the broader goal of learning-based 4D spatial intelligence. Unifying depth and camera pose estimation Jointly predicting depth and camera pose from monocular video is foundational step toward full 4D scene reconstruction. Many recent methods tackle this challenge by leveraging monocular depth priors and applying per-video optimization to enforce temporal and geometric consistency. For example, Robust-CVD [166] applies flexible deformation splines for large-scale geometric alignment and introduces geometry-aware filtering to refine high-frequency depth details; CasualSAM [167] fine-tunes monocular depth model on individual video sequences to jointly optimize both depth and camera pose. MegaSaM [168] adapts traditional visual SLAM paradigms to handle dynamic scenes, maintaining dense map construction and pose estimation. More recently, DUSt3R [44] proposes unified framework that simultaneously predicts depth, camera pose, and dense point map, allowing mutual refinement across these outputs. Building on this idea, MonST3R [46] and Align3R [47] extend the approach to dynamic scenes by fine-tuning on motion-rich datasets and producing temporally consistent point trajectories. Align3R further integrates monocular depth priors to enhance reconstruction quality, although both methods still rely on global alignment during postprocessing. Easi3R [169] is simple yet efficient trainingfree method for 4D reconstruction. It adapts attention during inference, eliminating the need for pre-training from scratch or network fine-tuning. GeometryCrafter [170] introduces pointmap Variational Autoencoder (VAE) that learns latent space independent of video-specific distributions, enabling effective pointmap encoding and decoding for accurate 3D/4D reconstruction and camera parameter estimation. In contrast to optimization-heavy pipelines, Spann3R [171] adopts feed-forward approach that enables continuous 4D reconstruction via spatial memory mechanism. CUT3R [172], on the other hand, leverages compressed state representation that not only encodes observed information but also supports the inference of unobserved structures. Point3R [173] and StreamVGGT [174] further enhance the capabilities of streaming 3D/4D reconstruction. Pi3 [175] introduces an innovative feed-forward neural network that fundamentally changes visual geometry reconstruction by removing the dependency on fixed reference view. Several diffusion-based methods [176], [177], [178] have also demonstrated strong performance on this task. Aether [176], Geo4D [177], and UniGeo [178] can simultaneously predict high-quality depth and accurate camera poses, benefiting from their denoising-based designs. Unifying depth, camera pose, and 3D tracking Recent methods have made significant progress toward jointly estimating video depth, camera pose, and 3D tracking. Uni4D [179] adopts multi-stage optimization framework that integrates multiple pretrained models to handle both static and dynamic 3D reconstruction. BA-Track [180] dis5 entangles camera-induced motion from object motion using 3D point tracker, enabling robust bundle adjustment across the entire scene, and further enforces temporal depth consistency through scale-map-based post-processing. Built upon DUSt3R [44], several recent works extend its capability toward dynamic 3D reconstruction and tracking. Stereo4D [181] leverages large-scale training on temporally consistent 3D point clouds to recover long-term pseudomotion trajectories along with depth and camera pose. DPM [182] introduces time into the representation, leading to multiple possible spatial-temporal reference frames for defining point maps. The authors identify minimal and sufficient subset of these reference combinations that can be regressed by network to address the aforementioned sub-tasks. St4RTrack [183] jointly predicts point maps at single timestamp within unified world coordinate system and chains these predictions across time to reconstruct longrange trajectories, effectively integrating reconstruction and tracking. POMATO [184] combines pointmap matching with temporal motion modeling, establishing cross-view pixelto-pointmap correspondences and introducing temporal module to enforce scale consistency and improve 3D point tracking. D2USt3R [185] directly regresses 4D pointmaps that represent both static and dynamic scene geometry, explicitly modeling spatial and temporal aspects to provide dense spatio-temporal correspondences beneficial for downstream tasks. Zero-MSF [186] presents joint geometry-andmotion estimation framework supported by large-scale data generation pipeline, which produces 1M annotated samples from diverse synthetic scenes. It also identifies and adopts an effective parameterization for scene flow through systematic evaluation. In contrast to the optimization-heavy methods above, TracksTo4D [187] operates in an unsupervised and feed-forward manner. It takes 2D point tracks as input and predicts full 4D structures from casually captured videos without requiring ground truth or supervision. Leveraging recent advancements in transformers [188], VGGT [54] introduces an end-to-end architecture capable of efficiently predicting low-level 3D cues within seconds. SpatialTrackerV2 [189] proposes unified, feedforward 3D point tracking, monocular integrates point depth, and camera pose estimation. It decomposes worldspace motion into scene geometry, camera ego-motion, and pixel-wise object motion, with fully differentiable, endto-end architecture that scales across synthetic, RGB-D, and in-the-wild data. tracker that On the other hand, jointly estimating depth and camera pose from dynamic RGB video remains an open challenge due to occlusions, object motion, and scene complexity. Several methods address this by enforcing temporal and geometric consistency within short frame windows [150], [190], [191], [192], [193], [194], yielding locally consistent results. However, these methods often suffer from accumulated errors over longer sequences. In response, dense visual SLAM methods [195], [196], [197], [198] extend traditional SLAM pipelines to produce globally consistent and dense depth maps instead of sparse point clouds. Notably, recent SLAM systems [199], [200], [201], [202] adopt 3D Gaussian Splatting [19] as scene representation, benefiting from its real-time rendering and high-fidelity geometry. Please refer to the survey in [203] for comprehensive overview. Recent studies have also revisited classical Structurefrom-Motion (SfM) techniques within differentiable framework. FlowMap [204], for example, presents an end-toend model that jointly estimates camera intrinsics, extrinsics, and depth maps from monocular videos. Similarly, DUSt3R [44] and MASt3R [205] reformulate pairwise reconstruction as point map regression, simultaneously inferring depth, pose, intrinsics, and pixel correspondences. Extensions such as MASt3R++ [45], [206] further improve dense multi-view stereo pipelines. Building on these ideas, recent works [48], [171], [207], [208], [209], [210] have demonstrated fast and accurate joint estimation of camera pose and depth across entire video sequences, signaling significant progress toward scalable 4D reconstruction systems."
        },
        {
            "title": "3 LEVEL 2 – 3D SCENE COMPONENTS",
            "content": "While low-level cues provide the geometric and positional foundations necessary for understanding the scenes layout, they are typically insufficient for capturing higherlevel semantics and object-level structures. Moving forward from this, level 2 methods focus on recovering the detailed representations of individual elements, such as objects, humans, and architectural structures, as well as their spatial arrangement within scene. An overview of representative approaches in this category is shown in Fig. 2. We specifically categorize the approaches into two types: (1) smallscale 3D object/scene reconstruction and (2) large-scale 3D scene reconstruction. In the following subsection, we begin by reviewing the key 3D representations that underpin these methods."
        },
        {
            "title": "3.1 Scene representations",
            "content": "In recent years, variety of 3D scene representations have been developed and adopted for static surface reconstruction. In this subsection, we first highlight the most commonly used representations and explain how they are typically integrated into modern neural network architectures. Point cloud. Point clouds, composed of discrete 3D points often enriched with attributes like color and normals, are fundamental representation for surface geometry. Beyond basic points, surfels, points with orientation and radius, offer richer representation and support differentiable rendering [19], [211], [212]. This enables optimization of point properties such as position, color, and size. Recent methods like Neural Point-based Rendering [1], [213], SynSin [214], Pulsar [215], [216], and ADOP [217] incorporate learnable features to better capture appearance and shape. Others, including FVS [218], SVS [219], and FWD-Transformer [220], further improve rendering by warping point-based features to novel views for color prediction, enhancing reconstruction quality. Meshes. Meshes, formed by connecting vertices with edges and polygons (typically triangles or quads), are widely used for representing complex 3D shapes due to their flexibility and computational efficiency [221], [222]. To utilize this kind of 3D representation, neural networks are generally designed to predict vertex positions [223], [224], while textured appearances are commonly achieved using per-vertex colors or UV-mapped textures. To integrate meshes into 3D 6 reconstruction pipelines, differentiable mesh rendering is essential. Techniques such as OpenDR [225], Neural Mesh Renderer [226], Soft Rasterizer [227], and Paparazzi [228] support gradient-based learning, while general-purpose renderers like Mitsuba [229] and Taichi [230] enable meshbased differentiable rendering via automatic differentiation. Neural radiance field (NeRF). Neural Radiance Fields (NeRF) [17] represent 3D scenes as continuous volumetric fields instead of discrete geometry like point clouds or meshes. By using neural network (typically an MLP), NeRF maps 3D point and viewing direction to color and density values. Rendering is achieved through volumetric integration along camera rays, where sampled densities and colors are accumulated to produce the final pixel color using differentiable volume rendering [231]. This implicit representation has enabled high-quality novel view synthesis and has seen widespread applications in editing [232], inverse rendering [233], camera pose estimation [234], and avatar reconstruction [235]. Extensions like NeuS [236] and VolSDF [237] integrate signed distance functions (SDFs) into NeRF to provide sharper surface definitions. These methods convert signed distances into opacities using differentiable mappings and optimize the scene by minimizing photometric losses. NeRFs [17], [238], [239], [240], [241], [242] have become versatile tool across wide range of tasks in computer vision and graphics. They have been successfully applied to scene editing [232], [243], camera pose optimization [234], [244], inverse rendering [233], [245], generalization to unseen scenes [246], [247], acceleration [248], and free-viewpoint video generation [249], [250]. NeRFs also support avatar modeling tasks such as face and body reenactment [235], [251]. Beyond graphics, their adaptability has also extended to fields like robotics [252], medical imaging [253], and even astronomy [254], highlighting the broad applicability of neural volumetric representations. 3D Gaussian splatting (3DGS). 3D Gaussian Splatting (3DGS) [19] offers an efficient alternative to NeRFs by directly optimizing set of 3D Gaussians, each defined by position µ, opacity α, anisotropic covariance Σ R33, and spherical harmonic (SH) coefficients SH to model viewdependent color c: = {(µi, Σi, ci, αi)}N i=1, (1) where is the number of 3D Gaussian primitives. Note that Spherical Harmonic (SH) is used for controlling the color of each Gaussian to accurately capture the view-dependent appearance of the scene. Unlike NeRFs that rely on MLPs, 3DGS represents scenes with explicit primitives, enabling high-resolution rendering with significantly faster training."
        },
        {
            "title": "3.2 Small-scale 3D object/scene reconstruction",
            "content": "Given the significant advantages of 3D reconstruction from video sequences, early research efforts focused substantially on regressing mesh as unified representation for geometric surface reconstruction. primary task within this domain involves reconstructing the surface given fixed, bounded video input. Pioneering methods typically relied on Structure-from-Motion (SfM) [31], [32], [33], [34] and Multi-View Stereo (MVS) [39], [40], [41], [42], [43] pipelines 7 tion strategies (QuickSplat [274]); and modifying Gaussian primitive representations [275], [276]. In contrast to these optimization-based methods, feedforward approaches have emerged to directly predict feature volumes via an end-to-end network. From these predicted feature volumes, representations such as signed distance functions (SDF) coupled with radiance fields (e.g., SparseNeuS [277], GenS [278], C2F2NeuS [279], UFORecon [280], SuRF [281], ReTR [282]) or 2D Gaussian representations (LaRa [283]) can be efficiently extracted. These methods demonstrate considerable potential for achieving generalized and real-time surface reconstruction by directly regressing both geometry and appearance in single forward pass. However, the substantial memory requirements inherent in constructing and processing large feature volumes can impose practical limitations on reconstruction fidelity and scalability. Reconstructing the environment from first-person view is also an important application of spatial intelligence from video. Recently, with the advancements of world models and embodied AI, there have been few attempts in static scene reconstruction from egocentric videos. Specifically, SceneScript [284] utilizes large language model to predict scene description codes from point clouds, which are generated from visual SLAM. EgoLifter [285] and Photoreal Reconstruction [286] leverage 3DGS to optimize photo-realistic static scenes directly from egocentric videos."
        },
        {
            "title": "3.3 Large-scale 3D scene reconstruction",
            "content": "Modeling large-scale 3D scenes presents inherent challenges due to the complex, multi-scale nature of the required parameterization and the absence of predefined spatial boundaries. NeRF++ [287] pioneered solutions for this domain by decomposing the radiance field into distinct bounded foreground and inverse sphere-based background components. This foundational approach enabled photorealistic rendering, extending far beyond the immediate camera frustum. Subsequently, Mip-NeRF360 [255] addressed critical issues of aliasing and scale imbalance through integrated cone sampling, non-linear distortion field, and online distillation, achieving high-fidelity 360 reconstructions. Building upon this, Zip-NeRF [288] effectively integrated MipNeRF360s anti-aliasing capabilities with accelerated hashgrid representations, achieving comparable quality while training an order of magnitude faster; CityGS [289], [290] and OctreeGS [291] proposed novel divide-and-conquer training and Level-of-Detail (LoD) strategy to achieve efficiency large-scale training and rendering; CityGS-X [292] further adopted batch-level multi-task rendering process to achieve more efficient modeling; LODGE [293] recently introduced hierarchical LoD representation, which iteratively selects optimal subsets of Gaussians based on the camera distance, to make real-time rendering feasible even on memory-constrained devices. These foundational advances enabled further scaling to vast environments. Partitioning strategies emerged as key solution, with Block-NeRF [294] and MegaNeRF [295] decomposing scenes into independent local networks to support neighborhood and city-block navigation. City-NeRF [296] extended this concept through progresFig. 3. The paradigms of methods for reconstructing 3D scene components from video input. 3D reconstruction methods for smallscale and large-scale scenes often share similar architectures, differing primarily in the spatial extent they handle. As shown in the left panel (Image source: MipNeRF360 [255]), small-scale scenes correspond to the unaffected domain. large-scale scenes additionally incorporate contracted domain. Examples illustrating both scene types are provided in the right panel. to predict dense depth maps, which were subsequently fused into surfaces using techniques like Poisson reconstruction [256] or Delaunay triangulation [257]. For instance, COLMAP [39], [258] employs pixel-level SIFT features for matching and depth prediction. MVSNet [259] leverages deep neural networks to extract latent features and aggregate depth predictions across multiple frames. However, the depth maps generated by these approaches are frequently noisy, and both Poisson reconstruction [256] and Delaunaybased methods exhibit high sensitivity to noise within the underlying point clouds. This sensitivity often results in reconstructed surfaces of compromised quality. Building upon these foundations and driven by the development of more efficient 3D representations, recent reconstruction methods derive surfaces directly from implicit volumetric representations, notably Neural Radiance Fields (NeRF) [55] and 3D Gaussian Splatting (3DGS) [56]. These approaches are capable of yielding high-quality geometry and superior view synthesis. Specifically, NeRFbased surface reconstruction techniques, such as NeuS [236], VolSDF [237], NeAT [260], and Neuralangelo [261], jointly optimize Signed Distance Function (SDF) field alongside the radiance field, subsequently extracting mesh via the Marching Cubes algorithm [262]. Later on, with the development of 3DGS, 2DGS [263], GOF [264], and PGSR [265], propose to obtain mesh by applying Truncated Signed Distance Function (TSDF) fusion to multi-view depth renderings. SuGaR [266] integrates Gaussians within an SDF field to provide high-quality reconstructions. The field continues to advance rapidly, with several recent techniques further enhancing Gaussian Splatting for surface reconstruction by introducing diverse innovations: recovering high-frequency details at scale [267]; enforcing geometric consistency across multiple viewpoints [268]; utilizing triangulation constraints specifically for indoor scenes (Tri2plane [269]); applying elongation splitting and assimilation strategies for improved accuracy (ESA-GS [270]); inferring Unsigned Distance Functions (UDFs) (GaussianUDF [271]); introducing sorted opacity fields (SOF [272]); leveraging photometric and reflectance priors [273]; employing learned initializasive network and dataset expansion, seamlessly integrating satellite-to-street-level perspectives. Concurrently, F2NeRF [297] accommodated arbitrary camera trajectories via perspective warping, while Gaussian splatting approaches like Scaffold-GS [298] achieved efficient view-adaptive rendering by anchoring sparse Gaussians on learned scaffolds. Complementing these representation and partitioning frameworks, regularization techniques are also proposed to enhance the reconstruction robustness. MonoSDF [299] strengthened outdoor geometry by integrating monocular depth and normal cues within signed distance function (SDF) formulation. Mixture-of-experts systems like SCALAR-NeRF [300] and Switch-NeRF [301] employ shared decoders or learned gating mechanisms to fuse predictions from multiple local models, establishing critical pathway toward real-time, appearance-consistent reconstruction at city scales. Another major advancement in surface reconstruction from static videos lies in the development of online, endto-end systems that support real-time, interactive, and scalable applications. notable early contribution is NeuralRecon [302], which introduced real-time reconstruction based on TSDF volumes using 3D convolutional GRUs [303]. Building on this foundation, later works incorporated more advanced techniques: TransformerFusion [304] leveraged transformer architectures; Flora [305] improved feature aggregation; and VisFusion [306] introduced visibility-aware fusion and ray-based sparsification. To further reduce reliance on strict photometric consistency, several methods (SimpleRecon [120], DG-Recon [307], CVRecon [308], and FineRecon [309]) begin integrating geometric priors from foundation models, enabling better use of both global and local contextual information. Self-supervised techniques, like MonoSelfRecon [310], have also contributed to this trend. More recently, GeoRecon [311] and DetailRecon [312] have advanced the field further by improving global structural consistency and preserving fine-grained surface details."
        },
        {
            "title": "4 LEVEL 3 – 4D DYNAMIC SCENES",
            "content": "The static nature of reconstructions from Level 2 methods limits their applicability in real-world, dynamic environments. To address this, Level 3 methods focus on introducing temporal dynamics into the scene, enabling the reconstruction of 4D representations that capture motion and changes over time. There are two popular directions (as illustrated in Fig. 4), which are: (1) reconstruct canonical space while learning its deformation over time, and (2) extend the original 3D representation by explicitly incorporating time as an additional input. Typically, these approaches can be broadly categorized into two groups based on their primary subjects: general 4D scene reconstruction and human-centric dynamic modeling."
        },
        {
            "title": "4.1 General 4D scene reconstruction",
            "content": "Surface reconstruction from dynamic videos. Dynamic surface reconstruction from video is vital area of research with wide-ranging applications in fields such as robotics, virtual reality, and autonomous systems. Early methods [313], [314], 8 Fig. 4. The paradigms of methods for reconstructing dynamic scenes from video input. Methods in this domain typically adopt one of two strategies for temporal modeling: (I) explicitly incorporating time as an additional input to extend static 3D representation, or (II) reconstructing canonical 3D space and learning its deformation over time. Def. denotes Deformation module. [315], [316] typically relied on deforming predefined object templates, which limited their ability to handle complex motions or occlusions. The advent of differentiable rendering [227] has enabled more flexible reconstruction pipelines, allowing systems like LASR [317] and ViSER [318] to model articulated shapes directly from video. Building on this, several approaches have extended NeRF frameworks to dynamic and articulated objects, including BANMo [319], PPR [320], and REACTO [321]. The use of neural implicit 3D representations [55], [322] has further removed the need for template constraints, enabling fully unconstrained reconstruction of dynamic scenes [323], [324], [325], [326]. More recently, with the development of 3DGS, methods such as [327], [328], [329], [330], [331], [332], [333] have incorporated 3DGS, significantly improving reconstruction speed, temporal coherence, and robustness to challenging motion. Novel view synthesis from dynamic videos. Beyond reconstructing dynamic 4D scenes, both NeRF and 3D Gaussian Splatting (3DGS) have been widely adopted for generating novel viewpoints. This free-viewpoint video delivers immersive visual experiences while enabling the creation of freeze-frame (bullet time) effects [334]. One popular direction is to reconstruct static canonical radiance field and learn its deformation with time, as introduced by DNeRF [335]. Building upon this idea, several NeRF-based approaches [58], [59], [60], [61], [62] employ scene-specific optimization to model non-rigid motions, dynamic appearances, and complex specular effects. In parallel, recent methods [336], [337], [338], [339], [340], [341], [342], [343], [344] based on 3D Gaussian splatting leverage explicit pointbased representations to directly encode dynamic geometry and appearance, offering improved computational efficiency and easier editing. Rather than relying solely on deformation fields, some approaches [345], [346], [347] model motion using vector fields derived from optical flow [348], providing an alternative and interpretable way to describe temporal dynamics. Another line of work extends radiance field representations by explicitly incorporating time as an additional input, enabling true 4D reconstruction. NeRF-based approaches [63], [64], [65], [66], [67], [68], [69] treat time as learnable parameter, allowing the model to capture temporal changes in geometry and appearance. Some approaches [63], [249] use temporal flow to regularize training, while others [349] apply depth-based warping to synthesize novel views, even under inconsistent depth estimates. Additional efforts [350], [351], [352], [353], [354] enhance temporal modeling by embedding explicit timeaware features, improving both coherence and efficiency in capturing scene dynamics. On the other hand, 3DGSbased frameworks [355], [356], [357] incorporate timestamps as additional Gaussian attributes, enabling real-time and highfidelity dynamic view synthesis through explicit pointbased rendering. However, these methods can struggle with maintaining geometric consistency across time. To overcome this, more recent GS-based approaches [343], [358], [359], [360], [361], [362], [363] represent 4D scenes as temporally evolving trajectories of 3D Gaussians. This formulation improves temporal coherence, enhances tracking robustness, and provides more accurate reconstruction of dynamic geometry. Feed-forward approaches to dynamic scene reconstruction have opened new possibilities for real-time 4D modeling. As the first approach, MonoNeRF [364] pioneered this direction by introducing generalizable dynamic radiance field that jointly encodes spatial and temporal features. Similarly, [365] tackles occlusion in dynamic settings without per-scene optimization. FlowIBR [366] reduces optimization time by leveraging pre-training on large static datasets, while [367] shows that strong temporal and geometric consistency can be achieved without tuning appearance for each scene. Most recently, [368] advances the paradigm by aggregating information from all context frames to reconstruct target frames directly, further improving efficiency and generalization. DAS3R [369] combines 3D Gaussian Splatting with DUSt3R to reduce the optimization difficulty typically associated with 3D Gaussian Splatting, and achieves more accurate background reconstruction results. LINO-UniPS [370], built upon VGGT, leverages learnable light register tokens to decouple illumination and normal features. It further introduces global cross-image attention mechanism to enhance multi-view lighting representation and normal consistency."
        },
        {
            "title": "4.2 Human-centric dynamic modeling",
            "content": "As illustrated in Fig. 5, human-centric dynamic modeling methods can be grouped into two main categories based on their underlying 3D representation: SMPL-based human mesh recovery and appearance-rich dynamic human modeling. In the following, we will first illustrate SMPL [374], parametric 3D human template, which forms the basis for human modeling. SMPL [374]. The parametric human model SMPL [374] represents the 3D shape by incorporating body vertices, joints, face and hands landmarks, and expression parameters. Formally, given the pose parameter θ and shape parameter β, SMPL can map the canonical model with nS vertices to observation space: (β, θ) = lbs(T (β, θ), J(β), θ, W), (β, θ) = + Bs(β) + Bp(θ), (2) where is the function representing the SMPL model in the observation space, and gives the transformed vertices. is the blend weight, Bs and Bp are the shape blend shape 9 Fig. 5. The illustrations of methods for reconstructing 4D dynamic humans from video input. Human-centric dynamic modeling approaches are generally categorized based on their representations: (I) methods that apply SMPL parametric model as their representation to derive the human pose and shape parameters (image source: Neural Body Fitting [371]), (II) methods that similarly apply SMPL but focus more on the prediction based on egocentric videos (image source: EgoAllo [372]), and (III) appearance-rich non-parametric methods that are capable of reconstructing the textured topologies, such as garments and accessories, from video data (image source: Neural Body [373]). function and pose blend shape function, respectively. lbs() denotes the linear blend skinning function, corresponding to articulated deformation. It poses () based on the pose parameters θ and joint locations J(β), using the blend weights W, individually for each body vertex: vo = vc, = (cid:88) k=1 wkGk(θ, jk), (3) where vc and vo respectively are SMPL vertices under the canonical pose and observation space, wk is the skinning weight, Gk(θ, jk) is the affine deformation that transforms the k-th joint jk from the canonical space to observation space, and is the number of neighboring joints. SMPL-X [375] evolves from SMPL to include more face vertices, expression parameters ϕ, and the expression blend shape function Be into the model: (β, θ, ϕ) = lbs(T (β, θ, ϕ), J(β), θ, W), (β, θ, ϕ) = + Bs(β) + Be(ϕ) + Bp(θ). (4) SMPL-based human mesh recovery and tracking Human mesh recovery (HMR) from dynamic videos has garnered significant research attention in recent years, facilitated by the use of parametric models like SMPL [374]. Early work approached the problem frame-by-frame using optimization [376], [377] or deep learning [378], [379] to estimate human pose and shape. Progress in this area includes enhancements to camera modeling [380], graphbased or location-aware estimators [381], [382], [383], hybrid optimization and regression [384], [385], kinematic parts and dense correspondences [371], [386], [387], [388], [389], [390], [391], image-aligned features [392], [393], [394], and physical constraints [395], [396]. Building on the success of transformer architectures [397], [398], recent innovations emphasize tokenized representation [399], [400], [401], [402] and scaling up models for human pose and shape estimation to millions of training instances [403]. To capture motion over time, video-based HMR methods integrate temporal information using recurrent networks [404], [405], [406], VAEs [407], and optical flow [408]. Moreover, to tackle the challenges in estimating globally consistent human motion from unconditioned and dynamic cameras, researchers have adopted BERT-like [409] masked pretraining for motion sequence modeling [410], structure-from-motion (SfM) [411], optical flow [412], and simultaneous localization and mapping (SLAM) [413], [414] techniques to holistically model human motion and camera movements in shared world coordinate. Egocentric motion tracking. Different from exocentric videos, egocentric videos are captured with head-mounted devices, e.g., smart glasses [415] or VR/AR devices, from the first-person view, carrying rich information of the dynamic world, including wearers themselves. The camera motion in egocentric videos reflects the head movement and can be used as conditional signal for full-body motion generation [416], [417], typically referred to as one-point body tracking. Additional environmental cues, such as explicit hand detection [372] or implicit video features [418], [419], further enhance the performance of one-point tracking. In VR applications, handheld controllers offer additional hand trajectories, providing extra constraints for three-point body tracking. This would make the full-body motion tracking model grounded [419], [420], [421], [422]. However, both one-point and three-point tracking remain ill-posed problems due to the lack of lower-body observations. To mitigate this, wide field-of-view (FoV) cameras mounted on the head and angled downward are often employed to capture more of the body, improving the accuracy in fullbody motion reconstruction [423], [424]. Appearance-rich dynamic human modeling Beyond HMR and interactive behavior capture, reconstructing animatable human avatars from RGB videos has emerged as prominent research direction [425], encapsulating novel pose animation and novel view synthesis. Early methods relied on explicit geometric representations such as meshes, skeletons and silhouettes to model articulated motions and non-rigid surface deformations [426]. notable milestone was VideoAvatar [427], which introduced canonical space mapping to decouple pose estimation from geometry and texture learning. Built upon this network, LiveCap [428] further improves the efficiency to achieve real-time performance. However, explicit representations faced limitations due to their dependency on pre-defined avatar templates, prompting shift toward implicit neural representations. Pioneering approaches in this domain include regression models for avatar template prediction [429], latent code aggregation across sequential observations [373], and Fourier occupancy fields (FOF) for rapid reconstruction [430]. Further developments extended NeRF to articulated human structures via generative models [431], motion field-based 10 Fig. 6. Examples of methods for modeling SMPL-based humancentric interaction. Image source: InterDreamer [439], CIRCLE [440], and BUDDI [441]. canonical warping [235], and hybrid frameworks integrating volumetric rendering with background reconstruction [85], [432]. Additional strategies explored disentangled representations via graph neural networks [433] and hybrid implicit-explicit representations for spatio-temporally coherent avatars [434]. More recently, 3D Gaussian Splatting (3DGS) [56] has enabled high-fidelity, animatable avatars with fast and flexible rendering [359], [435], [436], [437], [438], setting new standards in both reconstruction quality and view synthesis."
        },
        {
            "title": "5 LEVEL 4 – INTERACTIONS AMONG SCENE COM-\nPONENTS",
            "content": "Advancing from previous levels, level 4 of spatial intelligence enters more mature phase. At this level, the methods focus on modeling interactions between different components within scene. Considering human is often the central subject of interaction, our following discussion emphasizes human-centric interaction modeling. These approaches are also categorized into three groups based on their input types and the underlying representations: SMPL-based humancentric interaction, appearance-rich human-centric interaction, and egocentric human-centric interaction."
        },
        {
            "title": "5.1 SMPL-based human-centric interaction",
            "content": "Building on human mesh recovery (HMR), recent research has progressed toward capturing 3D interactive behaviors from videos, which can be broadly classified into three categories: human-object interaction (HOI), human-scene interaction (HSI), and human-human interaction (HHI). Examples of different categories is provided in Fig. 6 Human-object-interaction (HOI). Considering the lack of high-quality 3D HOI data with videos [442], early methods tackled 3D HOI by utilizing traditional optimization frameworks to reconstruct human-object spatial arrangements through heuristic contact priors [443], [444]. The emergence of scalable methods for collecting 3D HOI data involving videos [70], [71], [72], [73], [74], [417] prompted learning-based approaches, which present significant advantages in generalization and inference efficiency. Pioneering works proposed to model object proximity relative to the human by learning signed distance fields (SDFs) from data [445], [446], and then operate post-optimization based on the learned field. To enhance robustness and accuracy, particularly under occlusions, methods based on generative models such as normalizing flows [75], [76] learn the distribution of human-object spatial arrangements conditioned on input video, thereby mitigating outlier predictions. separate strategy augments auxiliary input modality, such as IMU, to facilitate object tracking [77]. However, persistent limitation of these methods is their reliance on well-reconstructed object template geometries, restricting their feasibility and applicability across diverse scenarios. To overcome this challenge, recent approaches like HDM [447] and InterTrack [79] proposed to learn geometric correspondences within families of similar object categories using diffusion models [448], [449]. This enabled geometry-agnostic reconstruction of 3D HOI point clouds directly from image frames and facilitated the construction of large-scale synthetic 3D HOI datasets. Human-scene-interaction (HSI). Extending to full scenes including movable objects and fixed contextual layout, early methods focused on estimating contacts between human and static scenes from image frames [450], [451], which are trained on data with sparse labels and inaccurate 3D scene geometries. To address the limitations of scale and quality of comprehensive 3D scene modeling in HSI data, GTA-IM [452] constructed synthetic data comprising videos alongside pseudo 3D HSI labels obtained from 3D assets within the game engine. Similarly, CIRCLES [440] integrated real-world motion capture with digital environments via VR applications, while TRUMANS [453] replicated 3D scene assets in reality. These methods provided richer and more accurate 3D labels, enabling reconstruction of HSI from videos to advance into diverse indoor and outdoor contexts involving dynamic objects. Nevertheless, significant gap persists between 3D assets and real-world environments. Jointly reconstructing both humans and dynamic scenes from casual, realworld videos like web footage remains highly desirable. SitComs3D [81] targeted on television show with multiple shots of the same scene. By disentangling human and scene using different representations, SitComs3D [81] expressed the scene as NeRF [17] and estimated human motion within this context. More recently, leveraging advanced low-level 3D attributes prediction models (introduced in Level 1), JOSH [82] jointly recovered human motion, 3D scene structure, camera poses, contacts, and optimized them contextually with physics-based constraints. While ODHSR [83] achieved the same target by holistically representing the human and scene as 3DGS for each frame. Human-human-interaction (HHI). In the case of multiperson interactions, earlier monocular and sparse multiview systems utilized 3D keypoint heatmaps for multihuman pose estimation [454], [455]. However, these methods ignore geometric constraints and physical contacts, leading to unrealistic results. To address this issue, datasets [456], [457] and methods [458] introduced instance-level prior and geometric collision loss to obtain physically plausible multi-human interactions from multi-view videos. While BUDDI [441] and HumanInteraction [459] leveraged generative models such as diffusion models [448], [449] and VQ-VAE [460] to model interaction prior, which effectively provides desirable initial estimation for following optimization iterations. MultiPhys [461] take another strategy to incorporate physical simulator to search for the optimal policy in physically correct motion space via an imitation learning framework."
        },
        {
            "title": "5.2 Appearance-rich human-centric interaction",
            "content": "Earlier attempts at reconstructing textured humancentric interactions from videos were limited by the lack of high-quality 3D datasets, making it difficult to model complex interactions between humans and objects. Fortunately, recent progress in differentiable 3D representations, particularly NeRF and 3D Gaussian Splatting (3DGS), has opened up new possibilities for capturing these interactions without relying on expensive 4D datasets. representative example is HOSNeRF [84], which enables joint reconstruction of humans and their interacted objects (e.g., backpacks) from RGB videos. Specifically, it extends the human skeleton with object bones, allowing the model to account for deformations introduced by contact. The process to obtain color and density value σ for each point xc can be then written as: (γ(xc), Oi c) (cid:55) (c, σ), (5) where () is the NeRF module, γ() denotes the positional embedding, and Oi is the learnable state embedding representing object states in the canonical space at frame i. These embeddings allow the model to conditionally represent different interaction configurations. Following HOSNeRF, other recent methods further extend this direction: NeuMan [85] decouples the human and scene by training separate NeRFs for each, improving flexibility and scene composition; PPR [320] combines differentiable physics simulation with differentiable rendering, optimizing the reconstruction via coordinate descent to improve realism; RAC [462] generalizes the approach to 12 Fig. 7. The paradigms of methods for reconstructing appearancerich human-centric interaction. These methods generally build on SMPL-based linear blend skinning (LBS) deformation, extending the human body skeleton to include interacted objects. An example result is shown in the figure below. (image source: HOSNeRF [84]). H. Def., O. Def., and Ext. LBS denote Human Deformation, Object Deformation, and Extended SMPL-based LBS correspondingly. animals and humans by learning consistent skeletons with fixed bone lengths. 5.3 Egocentric human-centric interaction Egocentric videos, captured from first-person perspective, uniquely record the wearers interactions with objects, environments, and other people, offering rich context for reconstructing and understanding human-centric interactions. Most existing benchmarks and models primarily focus on hand-object interactions. An early effort in this domain, H2O [463], captured egocentric hand-object interactions using head-mounted RGB-D camera along with multiple third-person cameras. HOI4D [464] further scales up the egocentric hand-object interaction capture with more objects. HOT3D [465] leverages Project Aria glasses [415] and Quest 3 headsets in multi-camera rig to enable more precise annotation of hand and object poses. Beyond household scenarios, egocentric hand-object interaction also plays crucial role in other domains. For instance, POVSurgery [466] introduces synthetic dataset tailored for estimating hand and instrument poses in surgical settings; Ego-Exo4D [467] and Nymeria [417] collect the first largescale 4D datasets for human scene interactions."
        },
        {
            "title": "6 LEVEL 5 – INCORPORATION OF PHYSICAL LAWS\nAND CONSTRAINTS",
            "content": "Recent advancements in 4D scene reconstruction and embodied AI have paved the way for shift in research focus, i.e., modeling the underlying physics of the environment to achieve more comprehensive representation of 4D spatial intelligence that imitates real-world human experiences. This evolution in research aims not only to capture dynamic human-scene interactions over time but also to embed physical plausibility and reasoning into these reconstructions. Such developments are crucial for enabling intelligent robotic systems to operate effectively in complex, unstructured environments. As illustrated in Fig. 8, current efforts in this area primarily focus on 4D reconstruction of dynamic humans and 3D scenes. While some recent methods [468] have incorporated physics to improve 3D object generation, we do not cover this line of work in our illustration. Fig. 8. The paradigms of methods for inferring physically grounded 3D spatial understanding from videos. (I) Physical dynamic human modeling methods learn motion policies from real-world captures of human-object interactions, enabling deployment in simulators and transfer to humanoid robotics (image source: SkillMimic [469]). (II) Physically plausible 3D scene reconstruction mitigates missing geometry artifacts prevalent in traditional approaches, producing simulation-ready environments (image source: PhyRecon [470])."
        },
        {
            "title": "6.1 Dynamic 4D human simulation with physics",
            "content": "Recent advancements in physical human modeling have focused on generating realistic, physics-based character animation using motion capture data, imitation learning, and reinforcement learning. This body of work spans two key areas: physics-driven character animation and human-object interaction (HOI). Physics-based character animation Generating physically accurate motion for human and animal characters has long been central challenge in animation and control research [471], [472], [473], [474], [475], [476], [477], [478]. Recent advances in physics-based character animation emphasize learning from large MoCap datasets using reinforcement learning (RL) [158] and imitation learning [87], [479], [480], [481], [482], [483]. Among them, foundational approach is DeepMimic [479], which learns to reproduce dynamic motions through direct trajectory tracking. AMP [484], based on GAIL [485], improves realism by using generative adversarial framework, where discriminator judges the realism of the motion, guiding the controller during training. However, AMP demands training separate policy for each task. Extensions like ASE [486], CALM [487], ControlVAE [488], PULSE [88], OmniGrasp [489], HOVER [490], ASAP [491], and UniPhys [492] aim to extract more general motion priors that can be reused across tasks. MaskedMimic [493] introduces masked conditional VAE for multi-task learning, but still struggles with generalizing to unseen control signals. In parallel, there has been rise in text-driven control approaches [487], [494], [495], [496], where high-level natural language is used to direct character behavior. For instance, SuperPADL [495] employs multi-stage training pipeline combining reinforcement learning and behavior cloning. PDP [496] uses diffusion models to create multimodal controllers that interpret text commands, improving robustness by injecting noise during training. Despite these innovations, text-driven physical controllers still lag behind kinematic methods in expressiveness and diversity due to difficulties in distilling controllable and reliable multimodal behaviors. To bridge this gap, researchers have turned to hierarchical control frameworks [497], [498], [499], [500]. These methods split the problem into high-level planning stage and low-level controller. The planner might generate trajectories [497], waypoints [499], or partial-body targets [498], which are tracked by an RL policy. For example, CLoSD [497] combines kinematic diffusion-based planner with physics-based tracker. However, mismatches between high-level kinematic plans and low-level physical feasibility often lead to artifacts like foot sliding or jitter, which require task-specific fine-tuning [497], [498]. Learning human-object interaction (HOI) Human-object interaction presents additional complexity due to the need for fine contact control, multi-body coordination, and realistic physical responses. Early systems approached HOI using handcrafted state machines or models like inverted pendulums to simulate behaviors such as running or jumping [501], [502], [503]. More recent works use deep including RL [504] to model more diverse interactions, sports and tool use [505], [506], [507]. Imitation learning is natural fit for HOI, and methods like [508], [509], [510] attempt to model whole-body interactions using motion priors or grasping models. However, directly adapting locomotion-based imitation frameworks (e.g., DeepMimic [479], AMP [484]) to HOI has proven unstable. These methods often fail to address unbalanced reward structures, crucial contact dynamics, or the importance of relative positioning, leading to poor performance. Some approaches use interaction graphs to model spatial dependencies between body and object [511], [512], but these primarily focus on kinematics and often lack physical realism. In contrast, more recent frameworks introduce contact-aware rewards that explicitly encourage correct and stable contact, significantly improving performance on complex HOI tasks. This contact-driven perspective, introduced in early work like [86], [469], allows for unified training across wide range of interaction scenarios without the need for handcrafted rewards or separate pipelines. 6.2 3D scene reconstruction with physical plausibility As we move toward building the 3D virtual worlds that can mimic real-world actions, physically plausible 3D scene reconstruction is gradually becoming central focus in [513]. Addressing this chal3D scene modeling [470], lenge, PhysicsNeRF [93] injects explicit physics guidance specifically, depth-ranking, sparsity, and cross-view alignment lossesto achieve stable and physically consistent geometry even from extremely sparse multi-view inputs. Building on this foundation, inverse-rendering pipelines such as PBR-NeRF [94] integrate neural radiance fields with physics-based rendering priors. This coupling enables the joint optimization of geometry, illumination, and spatially varying materials, effectively mitigating the physically impossible albedolighting entanglement inherent in vanilla NeRFs. Progressing to the scene level, CAST [95] first retrieves CAD proxies from single RGB image and subsequently applies physics-aware correction step. This step rigorously enforces support, non-penetration, and objectrelation constraints, resulting in contact-consistent layouts. PhyRecon [470] proposes to leverage the differentiable gradients from the simulator [230], [514] to improve the physical plausibility of the reconstruction scene components. Orthogonal to explicit simulators, Aug-NeRF [515] employs triple-level, physically grounded augmentations as regularization strategy during training. This technique dramatically reduces view-inconsistent floaters and enhances generalization capabilities. Finally, specialized methodologies are emerging for targeted phenomena; for instance, the Planar Reflection-Aware NeRF [516] explicitly models secondary reflected rays. This advancement eliminates the floaters frequently hallucinated behind reflective surfaces like glass and mirrors, thereby further improving the physical plausibility of reconstructions in everyday indoor scenes."
        },
        {
            "title": "7.1 Level 1 – low-level 3D attributes",
            "content": "Challenges Despite remarkable advances, reconstructing depth, camera, and 3D tracking from video remains challenging, due to its inherent ill-posed nature, especially for unconstrained, dynamic inputs. (1) core challenge lies in handling occlusions, dynamic object motion, and nonLambertian surfaces, which violate many of the assumptions underlying existing methods. (2) Many methods still require post-processing steps, global alignment, or manual hyperparameter tuning, limiting their automation and applicability. (3) Ensuring robustness to sensor noise, and generalization to diverse viewpoints and motion patterns (e.g., handheld, drone, egocentric) still remain open concerns. Future directions Given the key challenges outlined above, several promising directions can be explored. (1) Developing world models that jointly represent geometry, motion, semantics, and uncertainty could offer principled approach to reducing ambiguity in 4D reconstruction. These models can also take cues from vision-language foundation models, using large-scale pretraining on both synthetic and real-world video to learn strong inductive biases. (2) Additionally, further progress can also be made to achieve interactive annotation tools and multi-agent data collection, to provide richer supervision for training more robust systems."
        },
        {
            "title": "7.2 Level 2 – 3D scene components",
            "content": "Challenges While 3D scene reconstruction from video has seen remarkable progress, multiple technical challenges remain. (1) There is no universally optimal scene representation. Point clouds, meshes, NeRFs, and 3D Gaussian Splatting each present trade-offs between fidelity, efficiency, and expressiveness. (2) Recovering fine-scale geometry in unbounded or textureless regions, particularly under challenging conditions such as motion blur, lighting changes, or sparse viewpoints, remains difficult. (3) Reconstructions from egocentric videos are especially prone to degradation due to rapid motion and limited field of view (FoV), often resulting in incomplete or distorted outputs. Future directions In light of these challenges, future directions could potentially focus on: (1) Developing hierarchical and scalable architectures, such as mixture-of-experts models, sparse voxel grids, and scaffolded splatting, might be key to enabling efficient reconstruction and rendering across large-scale environments. (2) Advancing egocentric and dynamic scene understanding through cross-modal learning, by integrating signals such as IMU data, audio, and textual descriptions, may improve robustness in complex and motion-heavy scenarios."
        },
        {
            "title": "7.3 Level 3 – 4D dynamic scenes",
            "content": "Challenges Despite rapid progress in 4D scene reconstruction and human-centric dynamic modeling, several critical challenges remain unresolved: (1) Feed-forward methods accelerate reconstruction but suffer speed-generalizationquality trade-offs, often relying on per-scene optimization or massive training data that hinder scalability. (2) Complex dynamic phenomena, including fluids, smoke, semi-rigid objects, and topological changes (e.g., object splitting/merging), remain largely unsolved. (3) Similarly, egocentric reconstruction is severely challenged by selfocclusions and limited FoV in head-mounted fisheye captures, complicating dynamic scene recovery. Future directions Based on these issues, several directions deserve to be considered. (1) Hybrid implicit-explicit representations could balance reconstruction speed and fidelity, while physics-informed priors (biomechanics, fluid dynamics) may enforce physically plausible motion. (2) Benchmarks and collaborative exo-ego capture frameworks are critical to evaluate temporal coherence, overcome occlusion limitations, and enable scalable AR/VR/robotics applications."
        },
        {
            "title": "7.4 Level 4 – interactions among scene components",
            "content": "Challenges Reconstructing human-centric interactions from video remains highly challenging task due to several technical limitations. Human-object interaction (HOI) methods often require accurate object templates, which restricts their ability to generalize across diverse object categories and deformable instances. Human-scene interaction (HSI) models struggle to consistently align humans with dynamic environments over time, particularly in real-world videos where spatial and temporal cues are sparse. For humanhuman interaction (HHI), monocular setups frequently face issues such as occlusion, depth ambiguity, and unrealistic contact modeling. Across all domains, maintaining physical plausibility, preventing interpenetration, and ensuring temporal coherence remain persistent challenges. Moreover, the lack of large-scale, high-quality datasets that capture textured interactions or egocentric viewpoints continues to impede the methods generalization and real-world deployment. Future directions To address these challenges, future research could focus on category-agnostic modeling through geometry-aware or generative approaches such as diffusion 14 models, to enable broader generalization across interaction types. Integrating differentiable physics, learned contact priors, or imitation learning frameworks may improve physical realism in interactions. For egocentric interactions, future progress may be able to come from multimodal fusion (e.g., video, IMU, gaze) and learning from large-scale benchmarks. Last but not least, achieving real-time and interactive simulation of human-centric behaviors might require combining video-based reconstruction with embodied reasoning and action modeling."
        },
        {
            "title": "7.5 Level 5 – incorporation of physical laws and con-\nstraints",
            "content": "Challenges Despite significant progress, physics-based reconstruction and simulation from video still face some challenges. Reinforcement learning based character animation often suffers from sample inefficiency, high computational cost, and unstable optimization, particularly for complex or contact-rich motions. Generalization across diverse motion types remains limited; most policies are task-specific, and hierarchical controllers frequently introduce artifacts due to mismatches between high-level kinematic planning and low-level physical feasibility. For HOI situations, achieving stable contact behaviors and precise coordination between human and object dynamics remains difficult, especially under limited supervision or with diverse object geometries. In 3D scene reconstruction, enforcing physical plausibility, such as support, friction, and non-penetration, is difficult when scene geometry is incomplete or inferred from sparse views. As result, many methods continue to produce floating artifacts, interpenetrations, or physically implausible contacts due to insufficient physical priors. Future directions To overcome these limitations, future research could first explore multimodal representations that integrate video cues, physical constraints, and motion priors from different tasks and domains. Secondly, differentiable physics engines may offer promising foundation for jointly optimizing geometry, dynamics, and interaction constraints within fully learnable pipeline. Thirdly, developing hierarchical and diffusion-based motion planning, particularly when conditioned on text or scene context, may also lead to more expressive, controllable, and physically plausible character behaviors. Finally, better integration of perception and control, allowing characters to adapt their behavior based on inferred scene dynamics, may unlock more interactive and physically realistic virtual humans."
        },
        {
            "title": "8 CONCLUSION",
            "content": "In this report, we begin by categorizing 4D spatial intelligence into five distinct levels: Level 1 basic 3D cues; Level 2 components of 3D scenes; Level 3 dynamic 4D scenes; Level 4 interactions among scene components; and Level 5 integration of physical laws and constraints. We then provide thorough review of methods corresponding to each level. Additionally, we discuss the remaining challenges faced by current techniques and explore promising future directions to overcome these issues. As this is rapidly evolving field with new papers published weekly or even daily, we hope this survey offers an accessible entry point for interested readers and inspires progress toward potential Level 6 in 4D spatial intelligence. [21] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi, D. J. Fleet et al., Imagen video: High definition video generation with diffusion models, arXiv preprint arXiv:2210.02303, 2022."
        },
        {
            "title": "REFERENCES",
            "content": "[1] K.-A. Aliev, A. Sevastopolsky, M. Kolos, D. Ulyanov, and V. Lempitsky, Neural point-based graphics, in European conference on computer vision. Springer, 2020, pp. 696712. [2] H. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Niessner, P. Perez, C. Richardt, M. Zollh ofer, and C. Theobalt, Deep video portraits, ACM transactions on graphics (TOG), vol. 37, no. 4, pp. 114, 2018. [4] [3] N. Deng, Z. He, J. Ye, B. Duinkharjav, P. Chakravarthula, X. Yang, and Q. Sun, Fov-nerf: Foveated neural radiance fields for virtual reality, IEEE Transactions on Visualization and Computer Graphics, vol. 28, no. 11, pp. 38543864, 2022. S. Li, C. Li, W. Zhu, B. Yu, Y. Zhao, C. Wan, H. You, H. Shi, and Y. Lin, Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction, in Proceedings of the 50th Annual International Symposium on Computer Architecture, 2023, pp. 113. Y. Liu, W. Chen, Y. Bai, X. Liang, G. Li, W. Gao, and L. Lin, Aligning cyber space with physical world: comprehensive survey on embodied ai, arXiv preprint arXiv:2407.06886, 2024. T. Gupta, W. Gong, C. Ma, N. Pawlowski, A. Hilmkil, M. Scetbon, M. Rigter, A. Famoti, A. J. Llorens, J. Gao et al., The essential role of causality in foundation world models for embodied ai, arXiv preprint arXiv:2402.06665, 2024. J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu, B. Jia, and S. Huang, An embodied generalist agent in 3d world, arXiv preprint arXiv:2311.12871, 2023. [7] [5] [6] [8] H. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan, 3d-vla: 3d vision-language-action generative world model, arXiv preprint arXiv:2403.09631, 2024. T. Wu, Y.-J. Yuan, L.-X. Zhang, J. Yang, Y.-P. Cao, L.-Q. Yan, and L. Gao, Recent advances in 3d gaussian splatting, Computational Visual Media, vol. 10, no. 4, pp. 613642, 2024. [9] [10] B. Fei, J. Xu, R. Zhang, Q. Zhou, W. Yang, and Y. He, 3d gaussian splatting as new era: survey, IEEE Transactions on Visualization and Computer Graphics, 2024. [11] M. S. Hamid, N. Abd Manap, R. A. Hamzah, and A. F. Kadmin, Stereo matching algorithm based on deep learning: survey, Journal of King Saud University-Computer and Information Sciences, vol. 34, no. 5, pp. 16631673, 2022. [12] K. Zhou, X. Meng, and B. Cheng, Review of stereo matching algorithms based on deep learning, Computational intelligence and neuroscience, vol. 2020, no. 1, p. 8562323, 2020. [13] H. Laga, L. V. Jospin, F. Boussaid, and M. Bennamoun, survey on deep learning techniques for stereo-based depth estimation, IEEE transactions on pattern analysis and machine intelligence, vol. 44, no. 4, pp. 17381764, 2020. [14] K. Gao, Y. Gao, H. He, D. Lu, L. Xu, and J. Li, Nerf: Neural radiance field in 3d vision, comprehensive review, arXiv preprint arXiv:2210.00379, 2022. [15] Y. Bao, T. Ding, J. Huo, Y. Liu, Y. Li, W. Li, Y. Gao, and J. Luo, 3d gaussian splatting: Survey, technologies, challenges, and opportunities, IEEE Transactions on Circuits and Systems for Video Technology, 2025. [16] G. Wang, L. Pan, S. Peng, S. Liu, C. Xu, Y. Miao, W. Zhan, M. Tomizuka, M. Pollefeys, and H. Wang, Nerf in robotics: survey, arXiv preprint arXiv:2405.01333, 2024. [17] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, in European conference on computer vision. Springer, 2020, pp. 405421. [18] T. Shen, J. Gao, K. Yin, M.-Y. Liu, and S. Fidler, Deep marching tetrahedra: hybrid representation for high-resolution 3d shape synthesis, 2021. [19] B. Kerbl, G. Kopanas, T. Leimk uhler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering, ACM Transactions on Graphics (ToG), vol. 42, no. 4, pp. 114, 2023. J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, Video diffusion models, Advances in Neural Information Processing Systems, vol. 35, pp. 86338646, 2022. [20] [22] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts et al., Stable video diffusion: Scaling latent video diffusion models to large datasets, arXiv preprint arXiv:2311.15127, 2023. [23] P. C. Ng and S. Henikoff, Sift: Predicting amino acid changes that affect protein function, Nucleic acids research, vol. 31, no. 13, pp. 38123814, 2003. J. Revaud, C. De Souza, M. Humenberger, and P. Weinzaepfel, R2d2: Reliable and repeatable detector and descriptor, Advances in neural information processing systems, vol. 32, 2019. [24] [25] D. DeTone, T. Malisiewicz, and A. Rabinovich, Superpoint: Selfsupervised interest point detection and description, in Proceedings of the IEEE conference on computer vision and pattern recognition workshops, 2018, pp. 224236. [26] P.-E. Sarlin, D. DeTone, T. Malisiewicz, and A. Rabinovich, Superglue: Learning feature matching with graph neural networks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 49384947. J. Sun, Z. Shen, Y. Wang, H. Bao, and X. Zhou, Loftr: Detectorfree local feature matching with transformers, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 89228931. [27] [28] E. Brachmann and C. Rother, Neural-guided ransac: Learning where to sample model hypotheses, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 43224331. [29] P. Lindenberger, P.-E. Sarlin, and M. Pollefeys, Lightglue: Local feature matching at light speed, in Proceedings of the IEEE/CVF international conference on computer vision, 2023, pp. 17 62717 638. [30] D. Barath, D. Mishkin, L. Cavalli, P.-E. Sarlin, P. Hruby, and M. Pollefeys, Affineglue: Joint matching and robust estimation, arXiv preprint arXiv:2307.15381, 2023. J. L. Schonberger and J.-M. Frahm, Structure-from-motion revisited, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 41044113. [31] [32] O. [33] Ozyesil, V. Voroninski, R. Basri, and A. Singer, survey of structure from motion*. Acta Numerica, vol. 26, pp. 305364, 2017. J. Iglhaut, C. Cabo, S. Puliti, L. Piermattei, J. OConnor, and J. Rosette, Structure from motion photogrammetry in forestry: review, Current Forestry Reports, vol. 5, no. 3, pp. 155168, 2019. [34] P. Lindenberger, P.-E. Sarlin, V. Larsson, and M. Pollefeys, Pixelperfect structure-from-motion with featuremetric refinement, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 59875997. S. Agarwal, N. Snavely, S. M. Seitz, and R. Szeliski, Bundle adjustment in the large, in European conference on computer vision. Springer, 2010, pp. 2942. [35] [36] C. Engels, H. Stewenius, and D. Nister, Bundle adjustment rules, Photogrammetric computer vision, vol. 2, no. 32, 2006. [37] C. Zach, Robust bundle adjustment revisited, in European Conference on Computer Vision. Springer, 2014, pp. 772787. [39] [38] B. Triggs, P. F. McLauchlan, R. I. Hartley, and A. W. Fitzgibbon, Bundle adjustmenta modern synthesis, in International workshop on vision algorithms. Springer, 1999, pp. 298372. J. L. Sch onberger, E. Zheng, M. Pollefeys, and J.-M. Frahm, Pixelwise view selection for unstructured multi-view stereo, in European Conference on Computer Vision (ECCV), 2016. J. Zhang, Y. Yao, S. Li, Z. Luo, and T. Fang, Visibility-aware multi-view stereo network, British Machine Vision Conference (BMVC), 2020. J. Yang, W. Mao, J. M. Alvarez, and M. Liu, Cost volume pyramid based depth inference for multi-view stereo, in The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. [40] [41] [42] F. Wang, S. Galliani, C. Vogel, P. Speciale, and M. Pollefeys, Patchmatchnet: Learned multi-view patchmatch stereo, 2021. [43] X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, and P. Tan, Cascade cost volume for high-resolution multi-view stereo and stereo matching, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 24952504. [44] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, Dust3r: Geometric 3d vision made easy, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 69720 709. [46] [45] R. Murai, E. Dexheimer, and A. J. Davison, Mast3r-slam: Realtime dense slam with 3d reconstruction priors, arXiv preprint arXiv:2412.12392, 2024. J. Zhang, C. Herrmann, J. Hur, V. Jampani, T. Darrell, F. Cole, D. Sun, and M.-H. Yang, Monst3r: simple approach for estimating geometry in the presence of motion, arXiv preprint arXiv:2410.03825, 2024. J. Lu, T. Huang, P. Li, Z. Dou, C. Lin, Z. Cui, Z. Dong, S.-K. Yeung, W. Wang, and Y. Liu, Align3r: Aligned monocular depth estimation for dynamic videos, arXiv preprint arXiv:2412.03079, 2024. J. Yang, A. Sax, K. J. Liang, M. Henaff, H. Tang, A. Cao, J. Chai, F. Meier, and M. Feiszli, Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass, arXiv preprint arXiv:2501.13928, 2025. [47] [48] [49] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, Transformer in transformer, Advances in neural information processing systems, vol. 34, pp. 15 90815 919, 2021. [50] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao, C. Xu, Y. Xu et al., survey on vision transformer, IEEE transactions on pattern analysis and machine intelligence, vol. 45, no. 1, pp. 87110, 2022. [51] N. Kitaev, Ł. Kaiser, and A. Levskaya, Reformer: The efficient transformer, arXiv preprint arXiv:2001.04451, 2020. [52] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, Point transformer, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 16 25916 268. [53] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran, Image transformer, in International conference on machine learning. PMLR, 2018, pp. 40554064. J. Wang, M. Chen, N. Karaev, A. Vedaldi, C. Rupprecht, and D. Novotny, Vggt: Visual geometry grounded transformer, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 52945306. [54] [55] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, Nerf: Representing scenes as neural radiance fields for view synthesis, Communications of the ACM, vol. 65, no. 1, pp. 99106, 2021. [56] B. Kerbl, G. Kopanas, T. Leimk uhler, and G. Drettakis, 3d gaussian splatting for real-time radiance field rendering, ACM Transactions on Graphics, vol. 42, no. 4, July 2023. [Online]. Available: https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ [57] T. Shen, J. Munkberg, J. Hasselgren, K. Yin, Z. Wang, W. Chen, Z. Gojcic, S. Fidler, N. Sharp, and J. Gao, Flexible isosurface extraction for gradient-based mesh optimization, ACM Trans. Graph., vol. 42, no. 4, jul 2023. [Online]. Available: https://doi.org/10.1145/3592430 [58] K. Park, U. Sinha, J. T. Barron, S. Bouaziz, D. B. Goldman, S. M. Seitz, and R. Martin-Brualla, Nerfies: Deformable neural radiance fields, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 58655874. [59] E. Tretschk, A. Tewari, V. Golyanik, M. Zollh ofer, C. Lassner, and C. Theobalt, Non-rigid neural radiance fields: Reconstruction and novel view synthesis of dynamic scene from monocular video, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 95912 970. [60] K. Park, U. Sinha, P. Hedman, J. T. Barron, S. Bouaziz, D. B. Goldman, R. Martin-Brualla, and S. M. Seitz, Hypernerf: higher-dimensional representation for topologically varying neural radiance fields, arXiv preprint arXiv:2106.13228, 2021. [61] H. Yu, J. Julin, Z. A. Milacski, K. Niinuma, and L. A. Jeni, Dylin: Making light field networks dynamic, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 39712 406. [62] C.-D. Fan, C.-W. Chang, Y.-R. Liu, J.-Y. Lee, J.-L. Huang, Y.-C. Tseng, and Y.-L. Liu, Spectromotion: Dynamic 3d reconstruction of specular scenes, arXiv preprint arXiv:2410.17249, 2024. [63] Z. Li, S. Niklaus, N. Snavely, and O. Wang, Neural scene flow fields for space-time view synthesis of dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 64986508. [64] C. Gao, A. Saraf, J. Kopf, and J.-B. Huang, Dynamic view synthesis from dynamic monocular video, in Proceedings of the 16 IEEE/CVF International Conference on Computer Vision, 2021, pp. 57125721. [65] M. You and J. Hou, Decoupling dynamic monocular videos for dynamic view synthesis, IEEE Transactions on Visualization and Computer Graphics, 2024. [66] C. Wang, B. Eckart, S. Lucey, and O. Gallo, Neural trajectory fields for dynamic novel view synthesis, arXiv preprint arXiv:2105.05994, 2021. [67] H. Lin, Q. Wang, R. Cai, S. Peng, H. Averbuch-Elor, X. Zhou, and N. Snavely, Neural scene chronology, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 20 75220 761. [68] A. Lou, B. Planche, Z. Gao, Y. Li, T. Luan, H. Ding, T. Chen, J. Noble, and Z. Wu, Darenerf: Direction-aware representation for dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 50315042. J. Yang, B. Ivanovic, O. Litany, X. Weng, S. W. Kim, B. Li, T. Che, D. Xu, S. Fidler, M. Pavone et al., Emernerf: Emergent spatial-temporal scene decomposition via self-supervision, arXiv preprint arXiv:2311.02077, 2023. [69] [70] R. Dabral, S. Shimada, A. Jain, C. Theobalt, and V. Golyanik, Gravity-aware monocular 3d human-object reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 36512 374. [71] X. Xu, H. Joo, G. Mori, and M. Savva, D3d-hoi: Dynamic interactions from videos, arXiv preprint 3d human-object arXiv:2108.08420, 2021. [72] B. L. Bhatnagar, X. Xie, I. A. Petrov, C. Sminchisescu, C. Theobalt, and G. Pons-Moll, Behave: Dataset and method for tracking human object interactions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15 935 15 946. [73] Y. Huang, O. Taheri, M. J. Black, and D. Tzionas, Intercap: Joint markerless 3d tracking of humans and objects in interaction, in DAGM German Conference on Pattern Recognition. Springer, 2022, pp. 281299. [74] N. Jiang, T. Liu, Z. Cao, J. Cui, Z. Zhang, Y. Chen, H. Wang, Y. Zhu, and S. Huang, Full-body articulated human-object interaction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 93659376. [75] C. Huo, Y. Shi, Y. Ma, L. Xu, J. Yu, and J. Wang, Stackflow: Monocular human-object reconstruction by stacked normalizing flow with offset, arXiv preprint arXiv:2407.20545, 2024. [76] C. Huo, Y. Shi, and J. Wang, Monocular human-object reconstruction in the wild, in Proceedings of the 32nd ACM International Conference on Multimedia, 2024, pp. 55475555. [77] C. Zhao, J. Zhang, J. Du, Z. Shan, J. Wang, J. Yu, J. Wang, and L. Xu, Im hoi: Inertia-aware monocular capture of 3d humanobject interactions, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 729741. [78] Y. Xie, C.-H. Yao, V. Voleti, H. Jiang, and V. Jampani, Sv4d: Dynamic 3d content generation with multi-frame and multi-view consistency, arXiv preprint arXiv:2407.17470, 2024. [79] X. Xie, J. E. Lenssen, and G. Pons-Moll, Intertrack: Tracking human object interaction without object templates, arXiv preprint arXiv:2408.13953, 2024. [80] Y. Cao, L. Pan, K. Han, K.-Y. K. Wong, and Z. Liu, Avatargo: Zero-shot 4d human-object interaction generation and animation, arXiv preprint arXiv:2410.07164, 2024. [81] G. Pavlakos, E. Weber, M. Tancik, and A. Kanazawa, The one where they reconstructed 3d humans and environments in tv shows, in European Conference on Computer Vision. Springer, 2022, pp. 732749. [82] Z. Liu, J. Lin, W. Wu, and B. Zhou, Joint optimization for 4d human-scene reconstruction in the wild, arXiv preprint arXiv:2501.02158, 2025. [83] Z. Zhang, M. Kaufmann, L. Xue, J. Song, and M. R. Oswald, Odhsr: Online dense 3d reconstruction of humans and scenes from monocular videos, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 21 82421 835. J.-W. Liu, Y.-P. Cao, T. Yang, Z. Xu, J. Keppo, Y. Shan, X. Qie, and M. Z. Shou, Hosnerf: Dynamic human-object-scene neural radiance fields from single video, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18 48318 494. [84] [85] W. Jiang, K. M. Yi, G. Samei, O. Tuzel, and A. Ranjan, Neuman: Neural human radiance field from single video, in European Conference on Computer Vision. Springer, 2022, pp. 402418. [86] Y. Wang, J. Lin, A. Zeng, Z. Luo, J. Zhang, and L. Zhang, Physhoi: Physics-based imitation of dynamic human-object interaction, arXiv preprint arXiv:2312.04393, 2023. [87] Z. Luo, J. Cao, A. W. Winkler, K. Kitani, and W. Xu, Perpetual humanoid control for real-time simulated avatars, in International Conference on Computer Vision (ICCV), 2023. [88] Z. Luo, J. Cao, J. Merel, A. Winkler, J. Huang, K. M. Kitani, and W. Xu, Universal humanoid motion representations for physics-based control, in The Twelfth International Conference on Learning Representations, 2024. [Online]. Available: https: //openreview.net/forum?id=OrOd8PxOO2 [89] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Macklin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., Isaac gym: High performance gpu-based physics simulation for robot learning, arXiv preprint arXiv:2108.10470, 2021. [90] M. A. Wiering and M. Van Otterlo, Reinforcement learning, Adaptation, learning, and optimization, vol. 12, no. 3, p. 729, 2012. [91] L. P. Kaelbling, M. L. Littman, and A. W. Moore, Reinforcement learning: survey, Journal of artificial intelligence research, vol. 4, pp. 237285, 1996. [92] R. S. Sutton, A. G. Barto et al., Reinforcement learning, Journal of Cognitive Neuroscience, vol. 11, no. 1, pp. 126134, 1999. [93] M. R. Barhdadi, H. Kurban, and H. Alnuweiri, Physicsnerf: Physics-guided 3d reconstruction from sparse views, 2025. S. Wu, S. Basu, T. Broedermann, L. V. Gool, and C. Sakaridis, Pbr-nerf: Inverse rendering with physics-based neural fields, 2025. [94] [95] K. Yao, L. Zhang, X. Yan, Y. Zeng, Q. Zhang, W. Yang, L. Xu, J. Gu, and J. Yu, Cast: Component-aligned 3d scene reconstruction from an rgb image, 2025. [96] V. Voleti, C.-H. Yao, M. Boss, A. Letts, D. Pankratz, D. Tochilkin, C. Laforte, R. Rombach, and V. Jampani, Sv3d: Novel multiview synthesis and 3d generation from single image using latent video diffusion, in European Conference on Computer Vision. Springer, 2025, pp. 439457. [97] Z. Chen, Y. Wang, F. Wang, Z. Wang, and H. Liu, V3d: Video diffusion models are effective 3d generators, arXiv preprint arXiv:2403.06738, 2024. [98] Y. Cao, Y.-P. Cao, K. Han, Y. Shan, and K.-Y. K. Wong, Dreamavatar: Text-and-shape guided 3d human avatar generation via diffusion models, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 958968. S. Bahmani, I. Skorokhodov, V. Rong, G. Wetzstein, L. Guibas, P. Wonka, S. Tulyakov, J. J. Park, A. Tagliasacchi, and D. B. Lindell, 4d-fy: Text-to-4d generation using hybrid score distillation sampling, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 79968006. [99] [100] S. Bahmani, X. Liu, W. Yifan, I. Skorokhodov, V. Rong, Z. Liu, X. Liu, J. J. Park, S. Tulyakov, G. Wetzstein et al., Tc4d: Trajectoryconditioned text-to-4d generation, in European Conference on Computer Vision. Springer, 2024, pp. 5372. [101] H. Zhang, X. Chen, Y. Wang, X. Liu, Y. Wang, and Y. Qiao, 4diffusion: Multi-view video diffusion model for 4d generation, Advances in Neural Information Processing Systems, vol. 37, pp. 15 27215 295, 2024. [102] H. Liang, Y. Yin, D. Xu, H. Liang, Z. Wang, K. N. Plataniotis, Y. Zhao, and Y. Wei, Diffusion4d: Fast spatial-temporal consistent 4d generation via video diffusion models, arXiv preprint arXiv:2405.16645, 2024. [103] Y. Cao, X. Guo, M. Zhang, H. Xie, C. Gu, and Z. Liu, Crowdmogen: Zero-shot text-driven collective motion generation, arXiv preprint arXiv:2407.06188, 2024. [104] Y. Cao, Y.-P. Cao, K. Han, Y. Shan, and K.-Y. K. Wong, Guide3d: Create 3d avatars from text and image guidance, arXiv preprint arXiv:2308.09705, 2023. [105] Q. Miao, K. Li, J. Quan, Z. Min, S. Ma, Y. Xu, Y. Yang, and Y. Luo, Advances in 4d generation: survey, arXiv preprint arXiv:2503.14501, 2025. [106] C. Li, C. Zhang, A. Waghwase, L.-H. Lee, F. Rameau, Y. Yang, S.-H. Bae, and C. S. Hong, Generative ai meets 3d: survey on text-to-3d in aigc era, arXiv preprint arXiv:2305.06131, 2023. [107] J. Liu, X. Huang, T. Huang, L. Chen, Y. Hou, S. Tang, Z. Liu, W. Ouyang, W. Zuo, J. Jiang et al., comprehensive survey on 3d content generation, arXiv preprint arXiv:2402.01166, 2024. 17 [108] X. Li, Q. Zhang, D. Kang, W. Cheng, Y. Gao, J. Zhang, Z. Liang, J. Liao, Y.-P. Cao, and Y. Shan, Advances in 3d generation: survey, arXiv preprint arXiv:2401.17807, 2024. [109] H. Kato, D. Beker, M. Morariu, T. Ando, T. Matsuoka, W. Kehl, and A. Gaidon, Differentiable rendering: survey, arXiv preprint arXiv:2006.12057, 2020. [110] B. Deng, Y. Yao, R. M. Dyke, and J. Zhang, survey of nonrigid 3d registration, in Computer Graphics Forum, vol. 41, no. 2. Wiley Online Library, 2022, pp. 559589. [111] R. G. Pacheco and R. S. Couto, Inference time optimization using branchynet partitioning, in 2020 IEEE Symposium on Computers and Communications (ISCC). IEEE, 2020, pp. 16. [112] Z. Yin and J. Shi, Geonet: Unsupervised learning of dense depth, optical flow and camera pose, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 19831992. [113] A. Gordon, H. Li, R. Jonschkowski, and A. Angelova, Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 89778986. [114] J. Bian, Z. Li, N. Wang, H. Zhan, C. Shen, M.-M. Cheng, and I. Reid, Unsupervised scale-consistent depth and ego-motion learning from monocular video, Advances in neural information processing systems, vol. 32, 2019. [115] X. Luo, J.-B. Huang, R. Szeliski, K. Matzen, and J. Kopf, Consistent video depth estimation, ACM Transactions on Graphics (ToG), vol. 39, no. 4, pp. 711, 2020. [116] Y. Chen, C. Schmid, and C. Sminchisescu, Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 70637072. [117] X. Long, L. Liu, W. Li, C. Theobalt, and W. Wang, Multi-view depth estimation using epipolar spatio-temporal networks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 82588267. [118] V. Guizilini, R. Ambrus, , D. Chen, S. Zakharov, and A. Gaidon, Multi-frame self-supervised depth with transformers, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 160170. [119] J. Watson, O. Mac Aodha, V. Prisacariu, G. Brostow, and M. Firman, The temporal opportunist: Self-supervised multi-frame monocular depth, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 11641174. [120] M. Sayed, J. Gibson, J. Watson, V. Prisacariu, M. Firman, and C. Godard, Simplerecon: 3d reconstruction without 3d convolutions, in European Conference on Computer Vision. Springer, 2022, pp. 119. [121] J. Xie, C. Lei, Z. Li, L. E. Li, and Q. Chen, Video depth estimation by fusing flow-to-depth proposals, in 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 10 10010 107. [122] C. Eom, H. Park, and B. Ham, Temporally consistent depth prediction with flow-guided memory units, IEEE Transactions on Intelligent Transportation Systems, vol. 21, no. 11, pp. 46264636, 2019. [123] V. Patil, W. Van Gansbeke, D. Dai, and L. Van Gool, Dont forget the past: Recurrent depth estimation from monocular video, IEEE Robotics and Automation Letters, vol. 5, no. 4, pp. 68136820, 2020. [124] H. Zhang, C. Shen, Y. Li, Y. Cao, Y. Liu, and Y. Yan, Exploiting temporal consistency for real-time video depth estimation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 17251734. [125] Y. Wang, Z. Pan, X. Li, Z. Cao, K. Xian, and J. Zhang, Less is more: Consistent video depth estimation with masked frames modeling, in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 63476358. [126] R. Yasarla, H. Cai, J. Jeong, Y. Shi, R. Garrepalli, and F. Porikli, Mamo: Leveraging memory and attention for monocular video depth estimation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 87548764. [127] C. Zhao, Y. Zhang, M. Poggi, F. Tosi, X. Guo, Z. Zhu, G. Huang, Y. Tang, and S. Mattoccia, Monovit: Self-supervised monocular depth estimation with vision transformer, in 2022 international conference on 3D vision (3DV). IEEE, 2022, pp. 668678. [128] Z. Li, W. Ye, D. Wang, F. X. Creighton, R. H. Taylor, G. Venkatesh, and M. Unberath, Temporally consistent online depth estima18 tion in dynamic scenes, in Proceedings of the IEEE/CVF winter conference on applications of computer vision, 2023, pp. 30183027. environments, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 40484055. [129] Z. Teed and J. Deng, Deepv2d: Video to depth with differentiable structure from motion, in International Conference on Learning Representations, 2020. [130] Y. Wang, M. Shi, J. Li, Z. Huang, Z. Cao, J. Zhang, K. Xian, and G. Lin, Neural video depth stabilizer, in Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023, pp. 94669476. [131] Y. Wang, M. Shi, J. Li, C. Hong, Z. Huang, J. Peng, Z. Cao, J. Zhang, K. Xian, and G. Lin, Nvds+: Towards efficient and versatile neural stabilizer for video depth estimation, IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 118, 2024. [132] W. Hu, X. Gao, X. Li, S. Zhao, X. Cun, Y. Zhang, L. Quan, and Y. Shan, Depthcrafter: Generating consistent long depth sequences for open-world videos, arXiv preprint arXiv:2409.02095, 2024. [133] J. Shao, Y. Yang, H. Zhou, Y. Zhang, Y. Shen, M. Poggi, and Y. Liao, Learning temporally consistent video depth from video diffusion priors, arXiv preprint arXiv:2406.01493, 2024. [134] H. Yang, D. Huang, W. Yin, C. Shen, H. Liu, X. He, B. Lin, W. Ouyang, and T. He, Depth any video with scalable synthetic data, arXiv preprint arXiv:2410.10815, 2024. [135] S. Chen, H. Guo, S. Zhu, F. Zhang, Z. Huang, J. Feng, and B. Kang, Video depth anything: Consistent depth estimation for superlong videos, arXiv preprint arXiv:2501.12375, 2025. [136] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, and H. Zhao, Depth anything v2, arXiv preprint arXiv:2406.09414, 2024. [137] R. Mur-Artal and J. D. Tard os, Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras, IEEE transactions on robotics, vol. 33, no. 5, pp. 12551262, 2017. [138] Q. Wang, Z. Yan, J. Wang, F. Xue, W. Ma, and H. Zha, Line flow based simultaneous localization and mapping, IEEE Transactions on Robotics, vol. 37, no. 5, pp. 14161432, 2021. [139] S. Kannapiran, N. Bendapudi, M.-Y. Yu, D. Parikh, S. Berman, A. Vora, and G. Pandey, Stereo visual odometry with deep learning-based point and line feature matching using an attention graph neural network, in 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2023, pp. 3491 3498. [140] F. Shu, J. Wang, A. Pagani, and D. Stricker, Structure plp-slam: Efficient sparse mapping and localization using point, line and plane for monocular, rgb-d and stereo cameras, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 21052112. [141] H. Jiang, R. Qian, L. Du, J. Pu, and J. Feng, Ul-slam: universal monocular line-based slam via unifying structural and nonstructural constraints, IEEE Transactions on Automation Science and Engineering, 2024. [142] J. Engel, T. Sch ops, and D. Cremers, Lsd-slam: Large-scale direct monocular slam, in European conference on computer vision. Springer, 2014, pp. 834849. [143] J. Engel, V. Koltun, and D. Cremers, Direct sparse odometry, IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 3, pp. 611625, 2017. [144] L. Zhou, G. Huang, Y. Mao, S. Wang, and M. Kaess, Edplvo: Efficient direct point-line visual odometry, in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 75597565. [145] N. Yang, L. v. Stumberg, R. Wang, and D. Cremers, D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 12811292. [146] J. Pelecanos and S. Sridharan, Feature warping for robust speaker verification, in Proceedings of 2001 speaker Odyssey: the speaker recognition workshop. European Speech Communication Association, 2001, pp. 213218. [147] W. Wang, Y. Hu, and S. Scherer, Tartanvo: generalizable learning-based vo, in Conference on Robot Learning. PMLR, 2021, pp. 17611772. [148] S. Wang, R. Clark, H. Wen, and N. Trigoni, Deepvo: Towards end-to-end visual odometry with deep recurrent convolutional neural networks, in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 20432050. [149] S. Shen, Y. Cai, W. Wang, and S. Scherer, Dytanvo: Joint refinement of visual odometry and motion segmentation in dynamic [150] C. Zhao, Y. Tang, Q. Sun, and A. V. Vasilakos, Deep direct visual odometry, IEEE transactions on intelligent transportation systems, vol. 23, no. 7, pp. 77337742, 2021. [151] Z. Teed, L. Lipson, and J. Deng, Deep patch visual odometry, Advances in Neural Information Processing Systems, vol. 36, pp. 39 03339 051, 2023. [152] K. Xu, Y. Hao, S. Yuan, C. Wang, and L. Xie, Airslam: An efficient and illumination-robust point-line visual slam system, IEEE Transactions on Robotics, 2025. [153] L. Lipson, Z. Teed, and J. Deng, Deep patch visual slam, in European Conference on Computer Vision. Springer, 2024, pp. 424 440. [154] L. Lai, Z. Shangguan, J. Zhang, and E. Ohn-Bar, Xvo: Generalized visual odometry via cross-modal self-training, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 10 09410 105. [155] F. Wimbauer, W. Chen, D. Muhle, C. Rupprecht, and D. Cremers, Anycam: Learning to recover camera poses and intrinsics from casual videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [156] C. Rockwell, J. Tung, T.-Y. Lin, M.-Y. Liu, D. F. Fouhey, and C.-H. Lin, Dynamic camera poses and where to find them, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12 44412 455. [157] S. Zhang, J. He, Y. Zhu, J. Wu, and J. Yuan, Efficient camera exposure control for visual odometry via deep reinforcement learning, IEEE Robotics and Automation Letters, 2024. [158] N. Messikommer, G. Cioffi, M. Gehrig, and D. Scaramuzza, Reinforcement learning meets visual odometry, in European Conference on Computer Vision. Springer, 2024, pp. 7692. [159] Q. Wang, Y.-Y. Chang, R. Cai, Z. Li, B. Hariharan, A. Holynski, and N. Snavely, Tracking everything everywhere all at once, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 19 79519 806. [160] Y. Song, J. Lei, Z. Wang, L. Liu, and K. Daniilidis, Track everything everywhere fast and robustly, in European Conference on Computer Vision. Springer, 2024, pp. 343359. [161] Y. Xiao, Q. Wang, S. Zhang, N. Xue, S. Peng, Y. Shen, and X. Zhou, Spatialtracker: Tracking any 2d pixels in 3d space, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 40620 417. [162] B. Wang, J. Li, Y. Yu, L. Liu, Z. Sun, and D. Hu, Scenetracker: Long-term scene flow estimation network, arXiv preprint arXiv:2403.19924, 2024. [163] T. D. Ngo, P. Zhuang, C. Gan, E. Kalogerakis, S. Tulyakov, H.-Y. Lee, and C. Wang, Delta: Dense efficient long-range 3d tracking for any video, arXiv preprint arXiv:2410.24211, 2024. [164] S. Cho, J. Huang, S. Kim, and J.-Y. Lee, Seurat: From moving points to depth, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 72117221. [165] B. Zhang, L. Ke, A. W. Harley, and K. Fragkiadaki, Tapip3d: Tracking any point in persistent 3d geometry, arXiv preprint arXiv:2504.14717, 2025. [166] J. Kopf, X. Rong, and J.-B. Huang, Robust consistent video depth estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 16111621. [167] Z. Zhang, F. Cole, Z. Li, M. Rubinstein, N. Snavely, and W. T. Freeman, Structure and motion from casual videos, in European Conference on Computer Vision. Springer, 2022, pp. 2037. [168] Z. Li, R. Tucker, F. Cole, Q. Wang, L. Jin, V. Ye, A. Kanazawa, A. Holynski, and N. Snavely, Megasam: Accurate, fast, and robust structure and motion from casual dynamic videos, arXiv preprint arXiv:2412.04463, 2024. [169] X. Chen, Y. Chen, Y. Xiu, A. Geiger, and A. Chen, Easi3r: Estimating disentangled motion from dust3r without training, arXiv preprint arXiv:2503.24391, 2025. [170] T.-X. Xu, X. Gao, W. Hu, X. Li, S.-H. Zhang, and Y. Shan, Geometrycrafter: Consistent geometry estimation for open-world videos with diffusion priors, arXiv preprint arXiv:2504.01016, 2025. [171] H. Wang and L. Agapito, 3d reconstruction with spatial memory, arXiv preprint arXiv:2408.16061, 2024. [172] Q. Wang, Y. Zhang, A. Holynski, A. A. Efros, and A. Kanazawa, Continuous 3d perception model with persistent state, arXiv preprint arXiv:2501.12387, 2025. 19 [173] Y. Wu, W. Zheng, J. Zhou, and J. Lu, Point3r: Streaming 3d reconstruction with explicit spatial pointer memory, arXiv preprint arXiv:2507.02863, 2025. [195] G. Lu, Deep unsupervised visual odometry via bundle adjusted pose graph optimization, in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 61316137. [174] D. Zhuo, W. Zheng, J. Guo, Y. Wu, J. Zhou, and J. Lu, Streaming 4d visual geometry transformer, arXiv preprint arXiv:2507.11539, 2025. [196] Z. Teed and J. Deng, Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras, Advances in neural information processing systems, vol. 34, pp. 16 55816 569, 2021. [175] Y. Wang, J. Zhou, H. Zhu, W. Chang, Y. Zhou, Z. Li, J. Chen, J. Pang, C. Shen, and T. He, π3: Scalable permutationequivariant visual geometry learning, 2025. [Online]. Available: https://arxiv.org/abs/2507.13347 [197] Z. Zhu, S. Peng, V. Larsson, Z. Cui, M. R. Oswald, A. Geiger, and M. Pollefeys, Nicer-slam: Neural implicit scene encoding for rgb slam, in 2024 International Conference on 3D Vision (3DV). IEEE, 2024, pp. 4252. [176] A. Team, H. Zhu, Y. Wang, J. Zhou, W. Chang, Y. Zhou, Z. Li, J. Chen, C. Shen, J. Pang et al., Aether: Geometric-aware unified world modeling, arXiv preprint arXiv:2503.18945, 2025. [198] H. Li, X. Gu, W. Yuan, L. Yang, Z. Dong, and P. Tan, Dense rgb slam with neural implicit maps, arXiv preprint arXiv:2301.08930, 2023. [177] Z. Jiang, C. Zheng, I. Laina, D. Larlus, and A. Vedaldi, Geo4d: Leveraging video generators for geometric 4d scene reconstruction, arXiv preprint arXiv:2504.07961, 2025. [178] Y.-T. Sun, X. Yu, Z. Huang, Y.-H. Huang, Y.-C. Guo, Z. Yang, Y.- P. Cao, and X. Qi, Unigeo: Taming video diffusion for unified consistent geometry estimation, arXiv preprint arXiv:2505.24521, 2025. [179] D. Y. Yao, A. J. Zhai, and S. Wang, Uni4d: Unifying visual foundation models for 4d modeling from single video, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 11161126. [180] W. Chen, G. Zhang, F. Wimbauer, R. Wang, N. Araslanov, A. Vedaldi, and D. Cremers, Back on track: Bundle adfor dynamic scene reconstruction, arXiv preprint justment arXiv:2504.14516, 2025. [181] L. Jin, R. Tucker, Z. Li, D. Fouhey, N. Snavely, and A. Holynski, Stereo4d: Learning how things move in 3d from internet stereo videos, arXiv preprint arXiv:2412.09621, 2024. [182] E. Sucar, Z. Lai, E. Insafutdinov, and A. Vedaldi, Dynamic point maps: versatile representation for dynamic 3d reconstruction, arXiv preprint arXiv:2503.16318, 2025. [183] H. Feng, J. Zhang, Q. Wang, Y. Ye, P. Yu, M. J. Black, T. Darrell, and A. Kanazawa, St4rtrack: Simultaneous 4d reconstruction and tracking in the world, arXiv preprint arXiv:2504.13152, 2025. [184] S. Zhang, Y. Ge, J. Tian, G. Xu, H. Chen, C. Lv, and C. Shen, Pomato: Marrying pointmap matching with temporal motion for dynamic 3d reconstruction, arXiv preprint arXiv:2504.05692, 2025. [185] J. Han, H. An, J. Jung, T. Narihira, J. Seo, K. Fukuda, C. Kim, S. Hong, Y. Mitsufuji, and S. Kim, Dˆ 2ust3r: Enhancing 3d reconstruction with 4d pointmaps for dynamic scenes, arXiv preprint arXiv:2504.06264, 2025. [186] Y. Liang, A. Badki, H. Su, J. Tompkin, and O. Gallo, Zeroshot monocular scene flow estimation in the wild, arXiv preprint arXiv:2501.10357, 2025. [187] Y. Kasten, W. Lu, and H. Maron, Fast encoder-based 3d from casual videos via point track processing, in The Thirty-eighth Annual Conference on Neural Information Processing Systems. [188] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah, Transformers in vision: survey, ACM computing surveys (CSUR), vol. 54, no. 10s, pp. 141, 2022. [189] Y. Xiao, J. Wang, N. Xue, N. Karaev, Y. Makarov, B. Kang, X. Zhu, H. Bao, Y. Shen, and X. Zhou, Spatialtrackerv2: 3d point tracking made easy, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2025. [Online]. Available: https://arxiv.org/abs/2507.12462 [190] R. Li, S. Wang, Z. Long, and D. Gu, Undeepvo: Monocular visual odometry through unsupervised deep learning, in 2018 IEEE international conference on robotics and automation (ICRA). IEEE, 2018, pp. 72867291. [191] Z. Teed and J. Deng, Deepv2d: Video to depth with differentiable structure from motion, arXiv preprint arXiv:1812.04605, 2018. [192] S. Li, X. Wu, Y. Cao, and H. Zha, Generalizing to the open world: Deep visual odometry with online adaptation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 13 18413 193. [193] L. Sun, W. Yin, E. Xie, Z. Li, C. Sun, and C. Shen, Improving monocular visual odometry using learned depth, IEEE Transactions on Robotics, vol. 38, no. 5, pp. 31733186, 2022. [194] W. Zhao, S. Liu, Y. Shu, and Y.-J. Liu, Towards better generalization: Joint depth-pose learning without posenet, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 91519161. [199] Y. Zhang, F. Tosi, S. Mattoccia, and M. Poggi, Go-slam: Global optimization for consistent 3d instant reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 37273737. [200] G. Zhang, E. Sandstr om, Y. Zhang, M. Patel, L. Van Gool, and M. R. Oswald, Glorie-slam: Globally optimized rgb-only implicit encoding point cloud slam, arXiv preprint arXiv:2403.19549, 2024. [201] E. Sandstr om, K. Tateno, M. Oechsle, M. Niemeyer, L. Van Gool, M. R. Oswald, and F. Tombari, Splat-slam: Globally optimized rgb-only slam with 3d gaussians, arXiv preprint arXiv:2405.16544, 2024. [202] H. Matsuki, R. Murai, P. H. Kelly, and A. J. Davison, Gaussian splatting slam, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 18 03918 048. [203] F. Tosi, Y. Zhang, Z. Gong, E. Sandstr om, S. Mattoccia, M. Oswald, and M. Poggi, How nerfs and 3d gaussian splatting are reshaping slam: survey. arxiv 2024, arXiv preprint arXiv:2402.13255, 2024. [204] C. Smith, D. Charatan, A. Tewari, and V. Sitzmann, Flowmap: High-quality camera poses, intrinsics, and depth via gradient descent, arXiv preprint arXiv:2404.15259, 2024. [205] V. Leroy, Y. Cabon, and J. Revaud, Grounding image matching in 3d with mast3r, in European Conference on Computer Vision. Springer, 2024, pp. 7191. [206] B. Duisterhof, L. Zust, P. Weinzaepfel, V. Leroy, Y. Cabon, and J. Revaud, Mast3r-sfm: fully-integrated solution for unconstrained structure-from-motion, arXiv preprint arXiv:2409.19152, 2024. [207] S. Elflein, Q. Zhou, S. Agostinho, and L. Leal-Taixe, Light3r-sfm: Towards feed-forward structure-from-motion, arXiv preprint arXiv:2501.14914, 2025. [208] Y. Cabon, L. Stoffl, L. Antsfeld, G. Csurka, B. Chidlovskii, J. Revaud, and V. Leroy, Must3r: Multi-view network for stereo 3d reconstruction, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 10501060. [209] W. Jang, P. Weinzaepfel, V. Leroy, L. Agapito, and J. Revaud, Pow3r: Empowering unconstrained 3d reconstruction with camera and scene priors, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 10711081. [210] S. Liu, W. Li, P. Qiao, and Y. Dou, Regist3r: Incremental registration with stereo foundation model, arXiv preprint arXiv:2504.12356, 2025. [211] H. Pfister, M. Zwicker, J. Van Baar, and M. Gross, Surfels: Surface elements as rendering primitives, in Proceedings of the 27th annual conference on Computer graphics and interactive techniques, 2000, pp. 335342. [212] W. Yifan, F. Serena, S. Wu, C. Oztireli, and O. Sorkine-Hornung, Differentiable surface splatting for point-based geometry processing, ACM Transactions on Graphics (TOG), vol. 38, no. 6, pp. 114, 2019. [213] P. Dai, Y. Zhang, Z. Li, S. Liu, and B. Zeng, Neural point cloud rendering via multi-plane projection, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 78307839. [214] O. Wiles, G. Gkioxari, R. Szeliski, and J. Johnson, Synsin: Endto-end view synthesis from single image, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 74677477. [215] C. Lassner and M. Zollhofer, Pulsar: Efficient sphere-based neural rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14401449. [216] G. Kopanas, J. Philip, T. Leimk uhler, and G. Drettakis, Pointbased neural rendering with per-view optimization, in Computer Graphics Forum, vol. 40, no. 4. Wiley Online Library, 2021, pp. 2943. [217] D. uckert, L. Franke, and M. Stamminger, Adop: Approximate differentiable one-pixel point rendering, ACM Transactions on Graphics (ToG), vol. 41, no. 4, pp. 114, 2022. [218] G. Riegler and V. Koltun, Free view synthesis, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIX 16. Springer, 2020, pp. 623640. [219] , Stable view synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 12 21612 225. [220] A. Cao, C. Rockwell, and J. Johnson, Fwd: Real-time novel view synthesis with forward warping and depth, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15 71315 724. [221] M. Botsch, L. Kobbelt, M. Pauly, P. Alliez, and B. Levy, Polygon mesh processing. CRC press, 2010. [222] L. A. Shirman and C. H. Sequin, Local surface interpolation with bezier patches, Computer Aided Geometric Design, vol. 4, no. 4, pp. 279295, 1987. [223] A. Burov, M. Nießner, and J. Thies, Dynamic surface function networks for clothed human bodies, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 10 75410 764. [224] J. Thies, M. Zollh ofer, and M. Nießner, Deferred neural rendering: Image synthesis using neural textures, Acm Transactions on Graphics (TOG), vol. 38, no. 4, pp. 112, 2019. [225] M. M. Loper and M. J. Black, Opendr: An approximate differentiable renderer, in Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13. Springer, 2014, pp. 154169. [226] H. Kato, Y. Ushiku, and T. Harada, Neural 3d mesh renderer, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 39073916. [227] S. Liu, T. Li, W. Chen, and H. Li, Soft rasterizer: differentiable renderer for image-based 3d reasoning, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 77087717. [228] H.-T. D. Liu, M. Tao, and A. Jacobson, Paparazzi: surface editing by way of multi-view image processing. ACM Trans. Graph., vol. 37, no. 6, pp. 2211, 2018. [229] M. Nimier-David, D. Vicini, T. Zeltner, and W. Jakob, Mitsuba 2: retargetable forward and inverse renderer, ACM Transactions on Graphics (TOG), vol. 38, no. 6, pp. 117, 2019. [230] Y. Hu, T.-M. Li, L. Anderson, J. Ragan-Kelley, and F. Durand, Taichi: language for high-performance computation on spatially sparse data structures, ACM Transactions on Graphics (TOG), vol. 38, no. 6, p. 201, 2019. [231] N. Max, Optical models for direct volume rendering, IEEE Transactions on Visualization and Computer Graphics, vol. 1, no. 2, pp. 99108, 1995. [232] R. Martin-Brualla, N. Radwan, M. S. Sajjadi, J. T. Barron, A. Dosovitskiy, and D. Duckworth, Nerf in the wild: Neural radiance fields for unconstrained photo collections, in CVPR, 2021, pp. 72107219. [233] K. Zhang, F. Luan, Q. Wang, K. Bala, and N. Snavely, Physg: Inverse rendering with spherical gaussians for physics-based material editing and relighting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 54535462. [234] C.-H. Lin, W.-C. Ma, A. Torralba, and S. Lucey, Barf: Bundleadjusting neural radiance fields, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 57415751. [235] C.-Y. Weng, B. Curless, P. P. Srinivasan, J. T. Barron, and I. Kemelmacher-Shlizerman, Humannerf: Free-viewpoint rendering of moving people from monocular video, in Proceedings of the IEEE/CVF conference on computer vision and pattern Recognition, 2022, pp. 16 21016 220. [236] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction, Advances in Neural Information Processing Systems, vol. 34, pp. 27 17127 183, 2021. [237] L. Yariv, J. Gu, Y. Kasten, and Y. Lipman, Volume rendering of neural implicit surfaces, Advances in Neural Information Processing Systems, vol. 34, pp. 48054815, 2021. [238] M. Niemeyer and A. Geiger, GIRAFFE: Representing scenes as compositional generative neural feature fields, 2021. 20 [239] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. MartinBrualla, and P. P. Srinivasan, Mip-nerf: multiscale representation for anti-aliasing neural radiance fields, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 58555864. [240] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-nerf 360: Unbounded anti-aliased neural radiance fields, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 54705479. [241] D. Verbin, P. Hedman, B. Mildenhall, T. Zickler, J. T. Barron, and P. P. Srinivasan, Ref-nerf: Structured view-dependent appearance for neural radiance fields, in 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2022, pp. 54815490. [242] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely, Dynibar: Neural dynamic image-based rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 42734284. [243] S. Zhi, T. Laidlow, S. Leutenegger, and A. J. Davison, In-place scene labelling and understanding with implicit scene representation, in ICCV, 2021, pp. 15 83815 847. [244] P. Truong, M.-J. Rakotosaona, F. Manhardt, and F. Tombari, Sparf: Neural radiance fields from sparse and noisy poses, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 41904200. [245] M. Boss, R. Braun, V. Jampani, J. T. Barron, C. Liu, and H. Lensch, Nerd: Neural reflectance decomposition from image collections, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 12 68412 694. [246] A. Yu, V. Ye, M. Tancik, and A. Kanazawa, pixelnerf: Neural radiance fields from one or few images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 45784587. [247] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla, N. Snavely, and T. Funkhouser, IBRNet: Learning multi-view image-based rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 46904699. [248] C. Reiser, S. Peng, Y. Liao, and A. Geiger, Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 33514 345. [249] Y. Du, Y. Zhang, H.-X. Yu, J. B. Tenenbaum, and J. Wu, Neural radiance flow for 4d view synthesis and video processing, in 2021 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE Computer Society, 2021, pp. 14 30414 314. [250] T. Li, M. Slavcheva, M. Zollhoefer, S. Green, C. Lassner, C. Kim, T. Schmidt, S. Lovegrove, M. Goesele, R. Newcombe et al., Neural 3d video synthesis from multi-view video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 55215531. [251] S. Peng, J. Dong, Q. Wang, S. Zhang, Q. Shuai, X. Zhou, and H. Bao, Animatable neural radiance fields for modeling dynamic human bodies, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 14 31414 323. [252] A. Zhou, M. J. Kim, L. Wang, P. Florence, and C. Finn, Nerf in the palm of your hand: Corrective augmentation for robotics via novel-view synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 907 17 917. [253] D. uckert, Y. Wang, R. Li, R. Idoughi, and W. Heidrich, Neat: Neural adaptive tomography, ACM Transactions on Graphics (TOG), vol. 41, no. 4, pp. 113, 2022. [254] A. Levis, P. P. Srinivasan, A. A. Chael, R. Ng, and K. L. Bouman, Gravitationally lensed black hole emission tomography, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 19 84119 850. [255] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Mip-NeRF 360: Unbounded anti-aliased neural radiance fields, CoRR, vol. abs/2111.12077, 2022. [256] M. Kazhdan and H. Hoppe, Screened poisson surface reconstruction, ACM Transactions on Graphics (ToG), vol. 32, no. 3, pp. 113, 2013. [257] D.-T. Lee and B. J. Schachter, Two algorithms for constructing delaunay triangulation, International Journal of Computer & Information Sciences, vol. 9, no. 3, pp. 219242, 1980. [258] J. L. Sch onberger and J.-M. Frahm, Structure-from-motion revisited, in Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [259] Y. Yao, Z. Luo, S. Li, T. Fang, and L. Quan, Mvsnet: Depth inference for unstructured multi-view stereo, in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 767 783. [260] X. Meng, W. Chen, and B. Yang, Neat: Learning neural implicit surfaces with arbitrary topologies from multi-view images, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 248258. [261] Z. Li, T. uller, A. Evans, R. H. Taylor, M. Unberath, M.-Y. Liu, and C.-H. Lin, Neuralangelo: High-fidelity neural surface reconstruction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 84568465. [262] W. E. Lorensen and H. E. Cline, Marching cubes: high resolution 3d surface construction algorithm, in Seminal graphics: pioneering efforts that shaped the field, 1998, pp. 347353. [263] B. Huang, Z. Yu, A. Chen, A. Geiger, and S. Gao, 2d gaussian splatting for geometrically accurate radiance fields, in ACM SIGGRAPH 2024 conference papers, 2024, pp. 111. [264] Z. Yu, T. Sattler, and A. Geiger, Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes, ACM Transactions on Graphics (TOG), vol. 43, no. 6, pp. 113, 2024. [265] D. Chen, H. Li, W. Ye, Y. Wang, W. Xie, S. Zhai, N. Wang, H. Liu, H. Bao, and G. Zhang, Pgsr: Planar-based gaussian splatting for efficient and high-fidelity surface reconstruction, IEEE Transactions on Visualization and Computer Graphics, 2024. [266] A. Guedon and V. Lepetit, Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 53545363. [267] S. Chen, Z. Li, Z. Chen, Q. Yan, G. Shen, and R. Duan, 3d gaussian splatting for fine-detailed surface reconstruction in largescale scene, arXiv preprint arXiv:2506.17636, 2025. [268] J. Kim, G. Park, and S. Lee, Multiview geometric regularization of gaussian splatting for accurate radiance fields, arXiv preprint arXiv:2506.13508, 2025. [269] Y. Xie, H. Xiao, and W. Kang, Tri 2 plane: Advancing neural implicit surface reconstruction for indoor scenes, IEEE Transactions on Multimedia, 2025. [270] Y. Chen, W. Wu, Y. Peng, Y. Fei, and L. Zheng, Esa-gs: Elongation splitting and assimilation in gaussian splatting for accurate surface reconstruction, Computer Aided Geometric Design, p. 102434, 2025. [271] S. Li, Y.-S. Liu, and Z. Han, Gaussianudf: Inferring unsigned distance functions through 3d gaussian splatting, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 27 11327 123. [272] L. Radl, F. Windisch, T. Deixelberger, J. Hladky, M. Steiner, D. Schmalstieg, and M. Steinberger, Sof: Sorted opacity fields fast unbounded surface reconstruction, arXiv preprint for arXiv:2506.19139, 2025. [273] R. Bruneau, B. Brument, Y. Queau, J. Melou, F. B. Lauze, J.-D. Durou, and L. Calvet, Multi-view surface reconstruction using normal and reflectance cues, arXiv preprint arXiv:2506.04115, 2025. [274] Y.-C. Liu, L. ollein, M. Nießner, and A. Dai, Quicksplat: Fast 3d surface reconstruction via learned gaussian initialization, arXiv preprint arXiv:2505.05591, 2025. [275] K. Jiang, V. Sivaram, C. Peng, and R. Ramamoorthi, Geometry field splatting with gaussian surfels, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 5752 5762. [276] Z. Shen, Y. Liu, Z. Chen, Z. Li, J. Wang, Y. Liang, Z. Yu, J. Zhang, Y. Xu, S. Schaefer et al., Solidgs: Consolidating gaussian surfel splatting for sparse-view surface reconstruction, arXiv preprint arXiv:2412.15400, 2024. [277] X. Long, C. Lin, P. Wang, T. Komura, and W. Wang, Sparseneus: Fast generalizable neural surface reconstruction from sparse views, in European Conference on Computer Vision. Springer, 2022, pp. 210227. [278] R. Peng, X. Gu, L. Tang, S. Shen, F. Yu, and R. Wang, Gens: Generalizable neural surface reconstruction from multi-view images, Advances in Neural Information Processing Systems, vol. 36, pp. 56 93256 945, 2023. 21 [279] L. Xu, T. Guan, Y. Wang, W. Liu, Z. Zeng, J. Wang, and W. Yang, C2f2neus: Cascade cost frustum fusion for high fidelity and generalizable neural surface reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18 29118 301. [280] Y. Na, W. J. Kim, K. B. Han, S. Ha, and S.-E. Yoon, Uforecon: generalizable sparse-view surface reconstruction from arbitrary and unfavorable sets, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 50945104. [281] R. Peng, S. Shen, K. Xiong, H. Gao, J. Jiao, X. Gu, and R. Wang, Surface-centric modeling for high-fidelity generalizable neural surface reconstruction, in European Conference on Computer Vision. Springer, 2024, pp. 183200. [282] Y. Liang, H. He, and Y. Chen, Retr: Modeling rendering via transformer for generalizable neural surface reconstruction, Advances in neural information processing systems, vol. 36, pp. 62 332 62 351, 2023. [283] A. Chen, H. Xu, S. Esposito, S. Tang, and A. Geiger, Lara: Efficient large-baseline radiance fields, in European Conference on Computer Vision. Springer, 2024, pp. 338355. [284] A. Avetisyan, C. Xie, H. Howard-Jenkins, T.-Y. Yang, S. Aroudj, S. Patra, F. Zhang, D. Frost, L. Holland, C. Orme et al., Scenescript: Reconstructing scenes with an autoregressive structured language model, in European Conference on Computer Vision. Springer, 2024, pp. 247263. [285] Q. Gu, Z. Lv, D. Frost, S. Green, J. Straub, and C. Sweeney, Egolifter: Open-world 3d segmentation for egocentric perception, in European Conference on Computer Vision. Springer, 2024, pp. 382400. [286] Z. Lv, M. Monge, K. Chen, Y. Zhu, M. Goesele, J. Engel, Z. Dong, and R. Newcombe, Photoreal scene reconstruction from an egocentric device, in ACM SIGGRAPH, 2025. [287] K. Zhang, G. Riegler, N. Snavely, and V. Koltun, Nerf++: Analyzing and improving neural radiance fields, CoRR, vol. abs/2010.07492, 2020. [288] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman, Zip-nerf: Anti-aliased grid-based neural radiance fields, CoRR, vol. abs/2304.06706, 2023. [289] Y. Liu, C. Luo, L. Fan, N. Wang, J. Peng, and Z. Zhang, Citygaussian: Real-time high-quality large-scale scene rendering with gaussians, in European Conference on Computer Vision. Springer, 2024, pp. 265282. [290] Y. Liu, C. Luo, Z. Mao, J. Peng, and Z. Zhang, Citygaussianv2: Efficient and geometrically accurate reconstruction for large-scale scenes, arXiv preprint arXiv:2411.00771, 2024. [291] K. Ren, L. Jiang, T. Lu, M. Yu, L. Xu, Z. Ni, and B. Dai, Octreegs: Towards consistent real-time rendering with lod-structured 3d gaussians, arXiv preprint arXiv:2403.17898, 2024. [292] Y. Gao, H. Li, J. Chen, Z. Zou, Z. Zhong, D. Zhang, X. Sun, and J. Han, Citygs-x: scalable architecture for efficient and geometrically accurate large-scale scene reconstruction, arXiv preprint arXiv:2503.23044, 2025. [293] J. Kulhanek, M.-J. Rakotosaona, F. Manhardt, C. Tsalicoglou, M. Niemeyer, T. Sattler, S. Peng, and F. Tombari, Lodge: Levelof-detail large-scale gaussian splatting with efficient rendering, arXiv preprint arXiv:2505.23158, 2025. [294] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar, Block-nerf: Scalable large-scene neural view synthesis, in Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR), 2022. [295] H. Turki, D. Ramanan, and M. Satyanarayanan, Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs, CoRR, vol. abs/2112.10703, 2022. [296] Y. Xiangli, L. Xu, X. Pan, N. Zhao, A. Rao, C. Theobalt, B. Dai, and D. Lin, Bungeenerf (city-nerf): Progressive neural radiance field for extreme multi-scale scene rendering, in European Conf. Computer Vision (ECCV), 2022, pp. 106122. [297] P. Wang, Y. Liu, Z. Chen, L. Liu, Z. Liu, T. Komura, C. Theobalt, and W. Wang, F2-nerf: Fast neural radiance field training with free camera trajectories, Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition (CVPR), 2023. [298] T. Lu, M. Yu, L. Xu, Y. Xiangli, L. Wang, D. Lin, and B. Dai, Scaffold-gs: Structured 3d gaussians for view-adaptive rendering, CoRR, vol. abs/2312.00109, 2023. [299] Z. Yu, S. Peng, M. Niemeyer, T. Sattler, and A. Geiger, Monosdf: Exploring monocular geometric cues for neural implicit surface reconstruction, CoRR, vol. abs/2206.00665, 2022. [300] Y. Chen and G. H. Lee, Scalar-nerf: Scalable large-scale neural radiance fields for scene reconstruction, CoRR, vol. abs/2311.16657, 2023. [301] Z. Mi and D. Xu, Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields, in International Conference on Learning Representations (ICLR), 2023. [302] J. Sun, Y. Xie, L. Chen, X. Zhou, and H. Bao, Neuralrecon: Real-time coherent 3d reconstruction from monocular video, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 15 59815 607. [303] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, Learning phrase representations using rnn encoder-decoder for statistical machine translation, arXiv preprint arXiv:1406.1078, 2014. [304] A. Bozic, P. Palafox, J. Thies, A. Dai, and M. Nießner, Transformerfusion: Monocular rgb scene reconstruction using transformers, Advances in Neural Information Processing Systems, vol. 34, pp. 14031414, 2021. [305] L. Wang, Y. Gong, Q. Wang, K. Zhou, and L. Chen, Flora: dual-frequency loss-compensated real-time monocular 3d video reconstruction, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 2, 2023, pp. 25992607. [306] H. Gao, W. Mao, and M. Liu, Visfusion: Visibility-aware online 3d scene reconstruction from videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 31717 326. [307] J. Ju, C. W. Tseng, O. Bailo, G. Dikov, and M. Ghafoorian, Dgrecon: Depth-guided neural 3d scene reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18 18418 194. [308] Z. Feng, L. Yang, P. Guo, and B. Li, Cvrecon: Rethinking 3d geometric feature learning for neural reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 75017 760. [309] N. Stier, A. Ranjan, A. Colburn, Y. Yan, L. Yang, F. Ma, and B. Angles, Finerecon: Depth-aware feed-forward network for detailed 3d reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18 42318 432. [310] R. Li, U. Mahbub, V. Bhaskaran, and T. Nguyen, Monoselfrecon: Purely self-supervised explicit generalizable 3d reconstruction of indoor scenes from monocular rgb views, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 656666. [311] W. Bei, X. Fan, H. Jian, X. Du, D. Yan, J. Xu, and Q. Ge, Georecon: coarse-to-fine visual 3d reconstruction approach for highresolution images with neural matching priors, International Journal of Digital Earth, vol. 17, no. 1, p. 2421956, 2024. [312] F. Chu, Y. Cong, Y. Wang, and R. Chen, Detailrecon: Focusing on detailed regions for online monocular 3d reconstruction, IEEE Transactions on Multimedia, 2025. [313] D. Casillas-Perez, D. Pizarro, D. Fuentes-Jimenez, M. Mazo, and A. Bartoli, The isowarp: the template-based visual geometry of isometric surfaces, International Journal of Computer Vision, vol. 129, no. 7, pp. 21942222, 2021. [314] N. Kairanda, E. Tretschk, M. Elgharib, C. Theobalt, and V. Golyanik, f-sft: Shape-from-template with physics-based deformation model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 39483958. [315] S. Zuffi, A. Kanazawa, D. W. Jacobs, and M. J. Black, 3d menagerie: Modeling the 3d shape and pose of animals, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 63656373. [316] X. Li, S. Liu, S. De Mello, K. Kim, X. Wang, M.-H. Yang, and J. Kautz, Online adaptation for consistent mesh reconstruction in the wild, Advances in Neural Information Processing Systems, vol. 33, pp. 15 00915 019, 2020. [317] G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, H. Chang, D. Ramanan, W. T. Freeman, and C. Liu, Lasr: Learning articulated shape reconstruction from monocular video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 15 98015 989. [318] G. Yang, D. Sun, V. Jampani, D. Vlasic, F. Cole, C. Liu, and D. Ramanan, Viser: Video-specific surface embeddings for articulated 3d shape reconstruction, Advances in Neural Information Processing Systems, vol. 34, pp. 19 32619 338, 2021. [319] G. Yang, M. Vo, N. Neverova, D. Ramanan, A. Vedaldi, and H. Joo, Banmo: Building animatable 3d neural models from 22 many casual videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 28632873. [320] G. Yang, S. Yang, J. Z. Zhang, Z. Manchester, and D. Ramanan, Ppr: Physically plausible reconstruction from monocular videos, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 39143924. [321] C. Song, J. Wei, C. S. Foo, G. Lin, and F. Liu, Reacto: Reconstructing articulated objects from single video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 53845395. [322] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove, Deepsdf: Learning continuous signed distance functions for shape representation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 165174. [323] W. Mao, R. Hartley, M. Salzmann et al., Neural sdf flow for 3d reconstruction of dynamic scenes, in The Twelfth International Conference on Learning Representations. [324] R. Shao, Z. Zheng, H. Tu, B. Liu, H. Zhang, and Y. Liu, Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 16 63216 642. [325] E. Johnson, M. Habermann, S. Shimada, V. Golyanik, and C. Theobalt, Unbiased 4d: Monocular 4d reconstruction with neural deformation model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 65986607. [326] J. Choe, C. Choy, J. Park, I. S. Kweon, and A. Anandkumar, Spacetime surface regularization for neural dynamic scene reconstruction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 87117 881. [327] I. Liu, H. Su, and X. Wang, Dynamic gaussians mesh: Consistent mesh reconstruction from monocular videos, arXiv preprint arXiv:2404.12379, 2024. [328] S. Ma, Y. Luo, and Y. Yang, Reconstructing and simulating dynamic 3d objects with mesh-adsorbed gaussian splatting, arXiv preprint arXiv:2406.01593, 2024. [329] W. Cai, W. Ye, P. Ye, T. He, and T. Chen, Dynasurfgs: Dynamic surface reconstruction with planar-based gaussian splatting, arXiv preprint arXiv:2408.13972, 2024. [330] X. Li, J. Tong, J. Hong, V. Rolland, and L. Petersson, Dgns: Deformable gaussian splatting and dynamic neural surface for monocular dynamic 3d reconstruction, arXiv preprint arXiv:2412.03910, 2024. [331] S. Wang, B. Huang, R. Wang, and S. Gao, Space-time 2d gaussian splatting for accurate surface reconstruction under complex dynamic scenes, arXiv preprint arXiv:2409.18852, 2024. [332] C. Zheng, L. Xue, J. Zarate, and J. Song, Gstar: Gaussian surface tracking and reconstruction, arXiv preprint arXiv:2501.10283, 2025. [333] D. Chen, B. Oberson, I. Feldmann, O. Schreer, A. Hilsmann, and P. Eisert, Adaptive and temporally consistent gaussian surfels for multi-view dynamic reconstruction, arXiv preprint arXiv:2411.06602, 2024. [334] M. Magnor, M. Pollefeys, G. Cheung, W. Matusik, and C. Theobalt, Video-based rendering, in ACM SIGGRAPH 2005 Courses, 2005, pp. 1es. [335] A. Pumarola, E. Corona, G. Pons-Moll, and F. Moreno-Noguer, D-nerf: Neural radiance fields for dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 10 31810 327. [336] G. Wu, T. Yi, J. Fang, L. Xie, X. Zhang, W. Wei, W. Liu, Q. Tian, and X. Wang, 4d gaussian splatting for real-time dynamic scene rendering, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 20 31020 320. [337] Z. Yang, X. Gao, W. Zhou, S. Jiao, Y. Zhang, and X. Jin, Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 20 33120 341. [338] J. Bae, S. Kim, Y. Yun, H. Lee, G. Bang, and Y. Uh, Per-gaussian embedding-based deformation for deformable 3d gaussian splatting, in European Conference on Computer Vision. Springer, 2024, pp. 321335. [339] Y.-H. Huang, Y.-T. Sun, Z. Yang, X. Lyu, Y.-P. Cao, and X. Qi, Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 42204230. [340] J. Lu, J. Deng, R. Zhu, Y. Liang, W. Yang, X. Zhou, and T. Zhang, Dn-4dgs: Denoised deformable network with temporal-spatial aggregation for dynamic scene rendering, Advances in Neural Information Processing Systems, vol. 37, pp. 84 11484 138, 2024. [341] Z. Lu, X. Guo, L. Hui, T. Chen, M. Yang, X. Tang, F. Zhu, and Y. Dai, 3d geometry-aware deformable gaussian splatting for dynamic view synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 89008910. [342] Q. Liu, Y. Liu, J. Wang, X. Lv, P. Wang, W. Wang, and J. Hou, Modgs: Dynamic gaussian splatting from causually-captured monocular videos, arXiv preprint arXiv:2406.00434, 2024. [343] D. Das, C. Wewer, R. Yunus, E. Ilg, and J. E. Lenssen, Neural parametric gaussians for monocular non-rigid object reconstruction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 10 71510 725. [344] J. Lei, Y. Weng, A. Harley, L. Guibas, and K. Daniilidis, Mosca: Dynamic gaussian fusion from casual videos via 4d motion scaffolds, arXiv preprint arXiv:2405.17421, 2024. [345] C. Wang, L. E. MacDonald, L. A. Jeni, and S. Lucey, Flow supervision for deformable nerf, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 21 12821 137. [346] Q. Gao, Q. Xu, Z. Cao, B. Mildenhall, W. Ma, L. Chen, D. Tang, and U. Neumann, Gaussianflow: Splatting gaussian dynamics for 4d content creation, arXiv preprint arXiv:2403.12365, 2024. [347] R. Zhu, Y. Liang, H. Chang, J. Deng, J. Lu, W. Yang, T. Zhang, and Y. Zhang, Motiongs: Exploring explicit motion guidance for deformable 3d gaussian splatting, Advances in Neural Information Processing Systems, vol. 37, pp. 101 790101 817, 2024. [348] S. S. Beauchemin and J. L. Barron, The computation of optical flow, ACM computing surveys (CSUR), vol. 27, no. 3, pp. 433466, 1995. [349] J. S. Yoon, K. Kim, O. Gallo, H. S. Park, and J. Kautz, Novel view synthesis of dynamic scenes with globally coherent depths from monocular camera, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 53365345. [350] W. Xian, J.-B. Huang, J. Kopf, and C. Kim, Space-time neural irradiance fields for free-viewpoint video, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 94219431. [351] Q. Wang, V. Ye, H. Gao, J. Austin, Z. Li, and A. Kanazawa, Shape of motion: 4d reconstruction from single video, arXiv preprint arXiv:2407.13764, 2024. [352] Y.-L. Liu, C. Gao, A. Meuleman, H.-Y. Tseng, A. Saraf, C. Kim, Y.- Y. Chuang, J. Kopf, and J.-B. Huang, Robust dynamic radiance fields, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 1323. [353] A. Cao and J. Johnson, Hexplane: fast representation for dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 130141. [354] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and A. Kanazawa, K-planes: Explicit radiance fields in space, time, and appearance, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 47912 488. [355] Z. Li, Z. Chen, Z. Li, and Y. Xu, Spacetime gaussian feature splatting for real-time dynamic view synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 85088520. [356] Z. Yang, H. Yang, Z. Pan, and L. Zhang, Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting, arXiv preprint arXiv:2310.10642, 2023. [357] Y. Duan, F. Wei, Q. Dai, Y. He, W. Chen, and B. Chen, 4drotor gaussian splatting: towards efficient novel view synthesis for dynamic scenes, in ACM SIGGRAPH 2024 Conference Papers, 2024, pp. 111. [358] C. Stearns, A. Harley, M. Uy, F. Dubost, F. Tombari, G. Wetzstein, and L. Guibas, Dynamic gaussian marbles for novel view synthesis of casual monocular videos, in SIGGRAPH Asia 2024 Conference Papers, 2024, pp. 111. [359] M. Kocabas, J.-H. R. Chang, J. Gabriel, O. Tuzel, and A. Ranjan, Hugs: Human gaussian splats, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 505 515. 23 from 4d gaussians in highly deformable scenes, arXiv preprint arXiv:2312.00583, 2023. [361] J. Luiten, G. Kopanas, B. Leibe, and D. Ramanan, Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, arXiv preprint arXiv:2308.09713, 2023. [362] J. Sun, H. Jiao, G. Li, Z. Zhang, L. Zhao, and W. Xing, 3dgstream: On-the-fly training of 3d gaussians for efficient streaming of photo-realistic free-viewpoint videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20 67520 685. [363] K. Katsumata, D. M. Vo, and H. Nakayama, An efficient 3d gaussian representation for monocular/multi-view dynamic scenes, arXiv preprint arXiv:2311.12897, 2023. [364] F. Tian, S. Du, and Y. Duan, Mononerf: Learning generalizable dynamic radiance field from monocular videos, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 17 90317 913. [365] B. Van Hoorick, P. Tendulkar, D. Surıs, D. Park, S. Stent, and C. Vondrick, Revealing occlusions with 4d neural fields, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 30113021. [366] M. usching, J. Bengtson, D. Nilsson, and M. Bj orkman, Flowibr: Leveraging pre-training for efficient neural image-based rendering of dynamic scenes, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 80168026. [367] X. Zhao, R. A. Colburn, F. Ma, M. A. Bautista, J. M. Susskind, and A. Schwing, Pseudo-generalized dynamic view synthesis from video, in The Twelfth International Conference on Learning Representations, 2024. [368] H. Liang, J. Ren, A. Mirzaei, A. Torralba, Z. Liu, I. Gilitschenski, S. Fidler, C. Oztireli, H. Ling, Z. Gojcic et al., Feed-forward bullet-time reconstruction of dynamic scenes from monocular videos, arXiv preprint arXiv:2412.03526, 2024. [369] K. Xu, T. H. E. Tse, J. Peng, and A. Yao, Das3r: Dynamics-aware gaussian splatting for static scene reconstruction, arXiv preprint arXiv:2412.19584, 2024. [370] H. Li, H. Chen, C. Ye, Z. Chen, B. Li, S. Xu, X. Guo, X. Liu, Y. Wang, B. Zhang et al., Light of normals: Unified feature representation for universal photometric stereo, arXiv preprint arXiv:2506.18882, 2025. [371] M. Omran, C. Lassner, G. Pons-Moll, P. Gehler, and B. Schiele, Neural body fitting: Unifying deep learning and model based human pose and shape estimation, in 2018 international conference on 3D vision (3DV). IEEE, 2018, pp. 484494. [372] B. Yi, V. Ye, M. Zheng, Y. Li, L. uller, G. Pavlakos, Y. Ma, J. Malik, and A. Kanazawa, Estimating body and hand motion in an ego-sensed world, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 70727084. [373] S. Peng, Y. Zhang, Y. Xu, Q. Wang, Q. Shuai, H. Bao, and X. Zhou, Neural body: Implicit neural representations with structured latent codes for novel view synthesis of dynamic humans, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 90549063. [374] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and M. J. Black, Smpl: skinned multi-person linear model, ACM Trans. Graph., vol. 34, no. 6, pp. 248:1248:16, Oct. 2015. [375] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. A. Osman, D. Tzionas, and M. J. Black, Expressive body capture: 3D hands, face, and body from single image, in Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 10 975 10 985. [376] F. Bogo, A. Kanazawa, C. Lassner, P. Gehler, J. Romero, and M. J. Black, Keep it smpl: Automatic estimation of 3d human pose and shape from single image, in Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14. Springer, 2016, pp. 561578. [377] G. Pavlakos, V. Choutas, N. Ghorbani, T. Bolkart, A. A. Osman, D. Tzionas, and M. J. Black, Expressive body capture: 3d hands, face, and body from single image, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 10 97510 985. [378] A. Kanazawa, M. J. Black, D. W. Jacobs, and J. Malik, End-toend recovery of human shape and pose, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 71227131. [360] B. P. Duisterhof, Z. Mandi, Y. Yao, J.-W. Liu, M. Z. Shou, S. Song, and J. Ichnowski, Md-splatting: Learning metric deformation [379] G. Pavlakos, L. Zhu, X. Zhou, and K. Daniilidis, Learning to estimate 3d human pose and shape from single color image, 24 in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 459468. conference on computer vision and pattern recognition, 2021, pp. 19541963. [380] M. Kocabas, C.-H. P. Huang, J. Tesch, L. uller, O. Hilliges, and M. J. Black, SPEC: Seeing people in the wild with an estimated camera, in Proc. International Conference on Computer Vision (ICCV), Oct. 2021, pp. 11 03511 045. [381] N. Kolotouros, G. Pavlakos, and K. Daniilidis, Convolutional mesh regression for single-image human shape reconstruction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 45014510. [382] Z. Li, J. Liu, Z. Zhang, S. Xu, and Y. Yan, Cliff: Carrying location information in full frames into human pose and shape estimation, in European Conference on Computer Vision. Springer, 2022, pp. 590606. [383] I. Sarandi and G. Pons-Moll, Neural localizer fields for continuous 3d human pose and shape estimation, 2024. [384] N. Kolotouros, G. Pavlakos, M. J. Black, and K. Daniilidis, Learning to reconstruct 3d human pose and shape via model-fitting in the loop, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 22522261. [385] J. Song, X. Chen, and O. Hilliges, Human body model fitting by learned gradient descent, in European Conference on Computer Vision. Springer, 2020, pp. 744760. [386] R. A. Guler and I. Kokkinos, Holopose: Holistic 3d human reconstruction in-the-wild, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 10 884 10 894. [387] Y. Xu, S.-C. Zhu, and T. Tung, Denserac: Joint 3d pose and shape estimation by dense render-and-compare, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 77607770. [388] H. Zhang, J. Cao, G. Lu, W. Ouyang, and Z. Sun, Danet: Decompose-and-aggregate network for 3d human shape and pose estimation, in Proceedings of the 27th ACM International Conference on Multimedia, 2019, pp. 935944. [389] W. Zeng, W. Ouyang, P. Luo, W. Liu, and X. Wang, 3d human mesh regression with dense correspondence, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 70547063. [390] G. Georgakis, R. Li, S. Karanam, T. Chen, J. Koˇsecka, and Z. Wu, Hierarchical kinematic human mesh recovery, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XVII 16. Springer, 2020, pp. 768 784. [391] M. Kocabas, C.-H. P. Huang, O. Hilliges, and M. J. Black, Pare: Part attention regressor for 3d human body estimation, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 11 12711 137. [392] G. Moon and K. M. Lee, I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from single rgb image, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part VII 16. Springer, 2020, pp. 752768. [393] H. Zhang, Y. Tian, X. Zhou, W. Ouyang, Y. Liu, L. Wang, and Z. Sun, Pymaf: 3d human pose and shape regression with pyramidal mesh alignment feedback loop, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 11 44611 456. [394] H. Zhang, Y. Tian, Y. Zhang, M. Li, L. An, Z. Sun, and Y. Liu, Pymaf-x: Towards well-aligned full-body model regression from monocular images, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 10, pp. 12 28712 303, 2023. [395] S. Shimada, V. Golyanik, W. Xu, and C. Theobalt, Physcap: Physically plausible monocular 3d motion capture in real time, ACM Transactions on Graphics (ToG), vol. 39, no. 6, pp. 116, 2020. [396] S. Tripathi, L. uller, C.-H. P. Huang, O. Taheri, M. J. Black, and D. Tzionas, 3d human pose estimation via intuitive physics, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 47134725. [397] A. Vaswani, Attention is all you need, Advances in Neural Information Processing Systems, 2017. [398] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., An image is worth 16x16 words: Transformers for image recognition at scale, 2020. [400] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa, and J. Malik, Humans in 4d: Reconstructing and tracking humans with transformers, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 14 78314 794. [401] S. K. Dwivedi, Y. Sun, P. Patel, Y. Feng, and M. J. Black, Tokenhmr: Advancing human mesh recovery with tokenized pose representation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 13231333. [402] M. U. Saleem, E. Pinyoanuntapong, P. Wang, H. Xue, S. Das, and C. Chen, Genhmr: Generative human mesh recovery, arXiv preprint arXiv:2412.14444, 2024. [403] Z. Cai, W. Yin, A. Zeng, C. Wei, Q. Sun, W. Yanjun, H. E. Pang, H. Mei, M. Zhang, L. Zhang et al., Smpler-x: Scaling up expressive human pose and shape estimation, Advances in Neural Information Processing Systems, vol. 36, 2024. [404] A. Kanazawa, J. Y. Zhang, P. Felsen, and J. Malik, Learning 3d human dynamics from video, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, pp. 56145623. [405] M. Kocabas, N. Athanasiou, and M. J. Black, Vibe: Video inference for human body pose and shape estimation, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 52535263. [406] Z. Wan, Z. Li, M. Tian, J. Liu, S. Yi, and H. Li, Encoderdecoder with multi-level attention for 3d human shape and pose estimation, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 13 03313 042. [407] D. Rempe, T. Birdal, A. Hertzmann, J. Yang, S. Sridhar, and L. J. Guibas, Humor: 3d human motion model for robust pose estimation, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 11 48811 499. [408] C. Doersch and A. Zisserman, Sim2real transfer learning for 3d human pose estimation: motion to the rescue, Advances in Neural Information Processing Systems, vol. 32, 2019. [409] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, BERT: Pretraining of deep bidirectional transformers for language understanding, in NAACL-HLT, 2019. [410] Y. Yuan, U. Iqbal, P. Molchanov, K. Kitani, and J. Kautz, Glamr: Global occlusion-aware human mesh recovery with dynamic cameras, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022. [411] V. Ye, G. Pavlakos, J. Malik, and A. Kanazawa, Decoupling human and camera motion from videos in the wild, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 21 22221 232. [412] Y. Sun, Q. Bao, W. Liu, T. Mei, and M. J. Black, Trace: 5d temporal regression of avatars with dynamic cameras in 3d environments, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 88568866. [413] S. Shin, J. Kim, E. Halilaj, and M. J. Black, Wham: Reconstructing world-grounded humans with accurate 3d motion, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 20702080. [414] Y. Wang, Z. Wang, L. Liu, and K. Daniilidis, Tram: Global trajectory and motion of 3d humans from in-the-wild videos, in European Conference on Computer Vision. Springer, 2024, pp. 467487. [415] J. Engel, K. Somasundaram, M. Goesele, A. Sun, A. Gamino, A. Turner, A. Talattof, A. Yuan, B. Souti, B. Meredith et al., Project aria: new tool for egocentric multi-modal ai research, arXiv preprint arXiv:2308.13561, 2023. [416] J. Li, K. Liu, and J. Wu, Ego-body pose estimation via ego-head pose estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 14217 151. [417] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim et al., Nymeria: massive collection of multimodal egocentric daily motion in the wild, in European Conference on Computer Vision. Springer, 2024, pp. 445465. [418] V. Guzov, Y. Jiang, F. Hong, G. Pons-Moll, R. Newcombe, C. K. Liu, Y. Ye, and L. Ma, Hmdˆ 2: Environment-aware motion generation from single egocentric head-mounted device, arXiv preprint arXiv:2409.13426, 2024. [399] K. Lin, L. Wang, and Z. Liu, End-to-end human pose and mesh reconstruction with transformers, in Proceedings of the IEEE/CVF [419] F. Hong, V. Guzov, H. J. Kim, Y. Ye, R. Newcombe, Z. Liu, and L. Ma, Egolm: Multi-modal language model of egocentric mo25 tions, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 53445354. ting, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 50205030. [420] A. Castillo, M. Escobar, G. Jeanneret, A. Pumarola, P. Arbelaez, A. Thabet, and A. Sanakoyeu, Bodiffusion: Diffusing sparse observations for full-body human motion synthesis, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 42214231. [421] Y. Du, R. Kips, A. Pumarola, S. Starke, A. Thabet, and A. Sanakoyeu, Avatars grow legs: Generating smooth human motion from sparse tracking inputs with diffusion model, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 481490. [422] J. Jiang, P. Streli, H. Qiu, A. Fender, L. Laich, P. Snape, and C. Holz, Avatarposer: Articulated full-body pose tracking from sparse motion sensing, in European conference on computer vision. Springer, 2022, pp. 443460. [423] J. Wang, Z. Cao, D. Luvizon, L. Liu, K. Sarkar, D. Tang, T. Beeler, and C. Theobalt, Egocentric whole-body motion capture with fisheyevit and diffusion-based motion refinement, 2023. [424] J. Wang, D. Luvizon, W. Xu, L. Liu, K. Sarkar, and C. Theobalt, Scene-aware egocentric 3d human pose estimation, CVPR, 2023. [425] R. Wang, Y. Cao, K. Han, and K.-Y. K. Wong, survey on 3d human avatar modelingfrom reconstruction to generation, arXiv preprint arXiv:2406.04253, 2024. [426] W. Xu, A. Chatterjee, M. Zollh ofer, H. Rhodin, D. Mehta, H.- P. Seidel, and C. Theobalt, Monoperfcap: Human performance capture from monocular video, ACM Transactions on Graphics (ToG), vol. 37, no. 2, pp. 115, 2018. [427] T. Alldieck, M. Magnor, W. Xu, C. Theobalt, and G. Pons-Moll, Video based reconstruction of 3d people models, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 83878397. [428] M. Habermann, W. Xu, M. Zollhoefer, G. Pons-Moll, and C. Theobalt, Livecap: Real-time human performance capture from monocular video, ACM Transactions On Graphics (TOG), vol. 38, no. 2, pp. 117, 2019. [429] C. Guo, X. Chen, J. Song, and O. Hilliges, Human performance capture from monocular video in the wild, in 2021 International Conference on 3D Vision (3DV). IEEE, 2021, pp. 889898. [430] Q. Feng, Y. Liu, Y.-K. Lai, J. Yang, and K. Li, Fof: Learning fourier occupancy field for monocular real-time human reconstruction, Advances in Neural Information Processing Systems, vol. 35, pp. 73977409, 2022. [431] S.-Y. Su, F. Yu, M. Zollh ofer, and H. Rhodin, A-nerf: Articulated neural radiance fields for learning human shape, appearance, and pose, Advances in neural information processing systems, vol. 34, pp. 12 27812 291, 2021. [432] C. Guo, T. Jiang, X. Chen, J. Song, and O. Hilliges, Vid2avatar: 3d avatar reconstruction from videos in the wild via self-supervised scene decomposition, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 12 858 12 868. [433] S.-Y. Su, T. Bagautdinov, and H. Rhodin, Danbo: Disentangled articulated neural body representations via graph neural networks, in European Conference on Computer Vision. Springer, 2022, pp. 107124. [434] B. Jiang, Y. Hong, H. Bao, and J. Zhang, Selfrecon: Self reconstruction your digital avatar from monocular video, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 56055615. [435] L. Hu, H. Zhang, Y. Zhang, B. Zhou, B. Liu, S. Zhang, and L. Nie, Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 634644. [436] Z. Li, Z. Zheng, L. Wang, and Y. Liu, Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 71119 722. [437] S. Zheng, B. Zhou, R. Shao, B. Liu, S. Zhang, L. Nie, and Y. Liu, Gps-gaussian: Generalizable pixel-wise 3d gaussian splatting for real-time human novel view synthesis, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 68019 690. [438] Z. Qian, S. Wang, M. Mihajlovic, A. Geiger, and S. Tang, 3dgsavatar: Animatable avatars via deformable 3d gaussian splat- [439] S. Xu, Y.-X. Wang, L. Gui et al., Interdreamer: Zero-shot text to 3d dynamic human-object interaction, Advances in Neural Information Processing Systems, vol. 37, pp. 52 85852 890, 2024. [440] J. P. Ara ujo, J. Li, K. Vetrivel, R. Agarwal, J. Wu, D. Gopinath, A. W. Clegg, and K. Liu, Circle: Capture in rich contextual environments, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 21 21121 221. [441] L. uller, V. Ye, G. Pavlakos, M. Black, and A. Kanazawa, Generative proxemics: prior for 3d social interaction from images, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 96879697. [442] M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and M. Nießner, Pigraphs: learning interaction snapshots from observations, ACM Transactions On Graphics (TOG), vol. 35, no. 4, pp. 112, 2016. [443] J. Y. Zhang, S. Pepose, H. Joo, D. Ramanan, J. Malik, and A. Kanazawa, Perceiving 3d human-object spatial arrangements from single image in the wild, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XII 16. Springer, 2020, pp. 3451. [444] S. Goel, G. Pavlakos, J. Rajasegaran, A. Kanazawa*, and J. Malik*, Humans in 4D: Reconstructing and tracking humans with transformers, in International Conference on Computer Vision (ICCV), 2023. [445] X. Xie, B. L. Bhatnagar, and G. Pons-Moll, Chore: Contact, human and object reconstruction from single rgb image, in European Conference on Computer Vision. Springer, 2022, pp. 125 145. [446] , Visibility aware human-object interaction tracking from single rgb camera, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 47574768. [447] X. Xie, B. L. Bhatnagar, J. E. Lenssen, and G. Pons-Moll, Template free reconstruction of human-object interaction with procedural interaction generation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 10 003 10 015. [448] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. [449] J. Song, C. Meng, and S. Ermon, Denoising diffusion implicit models, arXiv preprint arXiv:2010.02502, 2020. [450] M. Hassan, V. Choutas, D. Tzionas, and M. J. Black, Resolving 3d human pose ambiguities with 3d scene constraints, in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 22822292. [451] C.-H. P. Huang, H. Yi, M. oschle, M. Safroshkin, T. Alexiadis, S. Polikovsky, D. Scharstein, and M. J. Black, Capturing and inferring dense full-body human-scene contact, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13 27413 285. [452] Z. Cao, H. Gao, K. Mangalam, Q.-Z. Cai, M. Vo, and J. Malik, Long-term human motion prediction with scene context, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16. Springer, 2020, pp. 387404. [453] N. Jiang, Z. Zhang, H. Li, X. Ma, Z. Wang, Y. Chen, T. Liu, Y. Zhu, and S. Huang, Scaling up dynamic human-scene interaction modeling, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 17371747. [454] Y. Sun, Q. Bao, W. Liu, Y. Fu, M. J. Black, and T. Mei, Monocular, one-stage, regression of multiple 3d people, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 11 17911 188. [455] Q. Shuai, Z. Yu, Z. Zhou, L. Fan, H. Yang, C. Yang, and X. Zhou, Reconstructing close human interactions from multiple views, ACM Transactions on Graphics (TOG), vol. 42, no. 6, pp. 114, 2023. [456] Y. Yin, C. Guo, M. Kaufmann, J. J. Zarate, J. Song, and O. Hilliges, Hi4d: 4d instance segmentation of close human interaction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 17 01617 027. [457] R. Khirodkar, J.-T. Song, J. Cao, Z. Luo, and K. Kitani, Harmony4d: video dataset for in-the-wild close human interactions, Advances in Neural Information Processing Systems, vol. 37, pp. 107 270107 285, 2024. [458] F. Lu, Z. Dong, J. Song, and O. Hilliges, Avatarpose: Avatarguided 3d pose estimation of close human interaction from sparse multi-view videos, in European Conference on Computer Vision. Springer, 2024, pp. 215233. [459] B. Huang, C. Li, C. Xu, L. Pan, Y. Wang, and G. H. Lee, Closely interactive human reconstruction with proxemics and physicsguided adaption, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 10111021. [460] A. Van Den Oord, O. Vinyals et al., Neural discrete representation learning, 2017. [461] N. Ugrinovic, B. Pan, G. Pavlakos, D. Paschalidou, B. Shen, J. Sanchez-Riera, F. Moreno-Noguer, and L. Guibas, Multiphys: Multi-person physics-aware 3d motion estimation, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 23312340. [462] G. Yang, C. Wang, N. D. Reddy, and D. Ramanan, Reconstructing animatable categories from videos, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 16 99517 005. [463] T. Kwon, B. Tekin, J. St uhmer, F. Bogo, and M. Pollefeys, H2o: Two hands manipulating objects for first person interaction recognition, in Proceedings of the IEEE/CVF international conference on computer vision, 2021, pp. 10 13810 148. [464] Y. Liu, Y. Liu, C. Jiang, K. Lyu, W. Wan, H. Shen, B. Liang, Z. Fu, H. Wang, and L. Yi, Hoi4d: 4d egocentric dataset for categorylevel human-object interaction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 21 01321 022. [465] P. Banerjee, S. Shkodrani, P. Moulon, S. Hampali, F. Zhang, J. Fountain, E. Miller, S. Basol, R. Newcombe, R. Wang et al., Introducing hot3d: An egocentric dataset for 3d hand and object tracking, arXiv preprint arXiv:2406.09598, 2024. [466] R. Wang, S. Ktistakis, S. Zhang, M. Meboldt, and Q. Lohmeyer, Pov-surgery: dataset for egocentric hand and tool pose estimation during surgical activities, in International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, 2023, pp. 440450. [467] K. Grauman, A. Westbury, L. Torresani, K. Kitani, J. Malik, T. Afouras, K. Ashutosh, V. Baiyya, S. Bansal, B. Boote et al., Ego-exo4d: Understanding skilled human activity from firstand third-person perspectives, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 19 38319 400. [468] R. Li, C. Zheng, C. Rupprecht, and A. Vedaldi, Dso: Aligning 3d generators with simulation feedback for physical soundness, arXiv preprint arXiv:2503.22677, 2025. [469] Y. Wang, Q. Zhao, R. Yu, A. Zeng, J. Lin, Z. Luo, H. W. Tsui, J. Yu, X. Li, Q. Chen et al., Skillmimic: Learning reusable basketball skills from demonstrations, arXiv e-prints, pp. arXiv2408, 2024. [470] J. Ni, Y. Chen, B. Jing, N. Jiang, B. Wang, B. Dai, P. Li, Y. Zhu, S.-C. Zhu, and S. Huang, Phyrecon: Physically plausible neural scene reconstruction, Advances in Neural Information Processing Systems, vol. 37, pp. 25 74725 780, 2024. [471] J. Lee, J. Chai, P. S. Reitsma, J. K. Hodgins, and N. S. Pollard, Interactive control of avatars animated with human motion data, in Proceedings of the 29th annual conference on Computer graphics and interactive techniques, 2002, pp. 491500. [472] Y. Lee, K. Wampler, G. Bernstein, J. Popovic, and Z. Popovic, Motion fields for interactive character locomotion, in ACM SIGGRAPH Asia 2010 papers, 2010, pp. 18. [473] A. Safonova and J. K. Hodgins, Construction and optimal search of interpolated motion graphs, in ACM SIGGRAPH 2007 papers, 2007, pp. 106es. [474] A. Treuille, Y. Lee, and Z. Popovic, Near-optimal character animation with continuous control, in ACM SIGGRAPH 2007 papers, 2007, pp. 7es. [475] S. Levine, J. M. Wang, A. Haraux, Z. Popovic, and V. Koltun, Continuous character control with low-dimensional embeddings, ACM Transactions on Graphics (TOG), vol. 31, no. 4, pp. 110, 2012. [476] H. Zhang, S. Starke, T. Komura, and J. Saito, Mode-adaptive neural networks for quadruped motion control, ACM Transactions on Graphics (ToG), vol. 37, no. 4, pp. 111, 2018. [477] D. Holden, T. Komura, and J. Saito, Phase-functioned neural networks for character control, ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 113, 2017. 26 [478] H. Y. Ling, F. Zinno, G. Cheng, and M. Van De Panne, Character controllers using motion vaes, ACM Transactions on Graphics (TOG), vol. 39, no. 4, pp. 401, 2020. [479] X. B. Peng, P. Abbeel, S. Levine, and M. van de Panne, Deepmimic: Example-guided deep reinforcement learning of physics-based character skills, ACM Trans. Graph., vol. 37, no. 4, pp. 143:1143:14, Jul. 2018. [Online]. Available: http: //doi.acm.org/10.1145/3197517.3201311 [480] T. Wang, Y. Guo, M. Shugrina, and S. Fidler, Unicon: Universal neural controller for physics-based character motion, arXiv preprint arXiv:2011.15119, 2020. [481] J. Won, D. Gopinath, and J. Hodgins, scalable approach to control diverse behaviors for physically simulated characters, ACM Transactions on Graphics (TOG), vol. 39, no. 4, pp. 331, 2020. [482] , scalable approach to control diverse behaviors for physically simulated characters, ACM Trans. Graph., vol. 39, no. 4, 2020. [Online]. Available: https://doi.org/10.1145/ 3386569. [483] N. Wagener, A. Kolobov, F. V. Frujeri, R. Loynd, C.-A. Cheng, and M. Hausknecht, MoCapAct: multi-task dataset for simulated humanoid control, in Advances in Neural Information Processing Systems, vol. 35, 2022, pp. 35 41835 431. [484] X. B. Peng, Z. Ma, P. Abbeel, S. Levine, and A. Kanazawa, Amp: Adversarial motion priors for stylized physics-based character control, ACM Transactions on Graphics (ToG), vol. 40, no. 4, pp. 120, 2021. [485] J. Ho and S. Ermon, Generative adversarial imitation learning, Advances in neural information processing systems, vol. 29, 2016. [486] X. B. Peng, Y. Guo, L. Halper, S. Levine, and S. Fidler, Ase: Largescale reusable adversarial skill embeddings for physically simulated characters, ACM Transactions On Graphics (TOG), vol. 41, no. 4, pp. 117, 2022. [487] C. Tessler, Y. Kasten, Y. Guo, S. Mannor, G. Chechik, and X. B. Peng, Calm: Conditional adversarial latent models for directable virtual characters, in ACM SIGGRAPH 2023 Conference Proceedings, 2023, pp. 19. [488] H. Yao, Z. Song, B. Chen, and L. Liu, Controlvae: Model-based learning of generative controllers for physics-based characters, ACM Trans. Graph., vol. 41, no. 6, 2022. [Online]. Available: https://doi.org/10.1145/3550454.3555434 [489] Z. Luo, J. Cao, S. Christen, A. Winkler, K. Kitani, and W. Xu, Omnigrasp: Grasping diverse objects with simulated humanoids, Advances in Neural Information Processing Systems, vol. 37, pp. 21612184, 2024. [490] T. He, W. Xiao, T. Lin, Z. Luo, Z. Xu, Z. Jiang, J. Kautz, C. Liu, G. Shi, X. Wang et al., Hover: Versatile neural whole-body controller for humanoid robots, arXiv preprint arXiv:2410.21229, 2024. [491] T. He, J. Gao, W. Xiao, Y. Zhang, Z. Wang, J. Wang, Z. Luo, G. He, N. Sobanbab, C. Pan et al., Asap: Aligning simulation and realworld physics for learning agile humanoid whole-body skills, arXiv preprint arXiv:2502.01143, 2025. [492] Y. Wu, K. Karunratanakul, Z. Luo, and S. Tang, Uniphys: Unified planner and controller with diffusion for flexible physics-based character control, arXiv preprint arXiv:2504.12540, 2025. [493] C. Tessler, Y. Guo, O. Nabati, G. Chechik, and X. B. Peng, Maskedmimic: Unified physics-based character control through masked motion, in ACM Transactions On Graphics (TOG). ACM New York, NY, USA, 2024. [494] J. Juravsky, Y. Guo, S. Fidler, and X. B. Peng, Padl: Languagedirected physics-based character control, in SIGGRAPH Asia 2022 Conference Papers, 2022, pp. 19. [495] , Superpadl: Scaling language-directed physics-based control with progressive supervised distillation, in ACM SIGGRAPH 2024 Conference Papers, 2024, pp. 111. [496] T. E. Truong, M. Piseno, Z. Xie, and K. Liu, Pdp: Physics-based character animation via diffusion policy, in SIGGRAPH Asia 2024 Conference Papers, 2024, pp. 110. [497] G. Tevet, S. Raab, S. Cohan, D. Reda, Z. Luo, X. B. Peng, A. H. Bermano, and M. van de Panne, Closd: Closing the loop between simulation and diffusion for multi-task character control, arXiv preprint arXiv:2410.03441, 2024. [498] N. Hansen, J. SV, V. Sobal, Y. LeCun, X. Wang, and H. Su, Hierarchical world models as visual whole-body humanoid controllers, 2025. [499] D. Rempe, Z. Luo, X. B. Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler, and O. Litany, Trace and pace: Controllable pedestrian animation via guided trajectory diffusion, in Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [500] J. Wang, Z. Luo, Y. Yuan, Y. Li, and B. Dai, Pacer+: On-demand pedestrian animation controller in driving scenarios, in Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [501] J. K. Hodgins, W. L. Wooten, D. C. Brogan, and J. F. OBrien, Animating human athletics, in Proceedings of the 22nd annual conference on Computer graphics and interactive techniques, 1995, pp. 7178. [502] K. Yin, K. Loken, and M. Van de Panne, Simbicon: Simple biped locomotion control, ACM Transactions on Graphics (TOG), vol. 26, no. 3, pp. 105es, 2007. [503] S. Coros, P. Beaudoin, and M. Van de Panne, Generalized biped walking control, ACM Transactions On Graphics (TOG), vol. 29, no. 4, pp. 19, 2010. [504] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, Deep reinforcement learning: brief survey, IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 2638, 2017. [505] L. Liu and J. Hodgins, Learning basketball dribbling skills using trajectory optimization and deep reinforcement learning, ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 114, 2018. [506] , Learning to schedule control fragments for physics-based characters using deep q-learning, ACM Transactions on Graphics (TOG), vol. 36, no. 3, pp. 114, 2017. [507] J. Tan, Y. Gu, C. K. Liu, and G. Turk, Learning bicycle stunts, ACM Transactions on Graphics (TOG), vol. 33, no. 4, pp. 112, 2014. [508] H. Zhang, Y. Yuan, V. Makoviychuk, Y. Guo, S. Fidler, X. B. Peng, and K. Fatahalian, Learning physically simulated tennis skills from broadcast videos, ACM Trans. Graph., 2023. [509] J. Bae, J. Won, D. Lim, C.-H. Min, and Y. M. Kim, Pmp: Learning to physically interact with environments using part-wise motion priors, in ACM SIGGRAPH 2023 Conference Proceedings, 2023, pp. 110. [510] J. Braun, S. Christen, M. Kocabas, E. Aksan, and O. Hilliges, Physically plausible full-body hand-object interaction synthesis, arXiv preprint arXiv:2309.07907, 2023. [511] E. S. Ho, T. Komura, and C.-L. Tai, Spatial relationship preserving character motion adaptation, in ACM SIGGRAPH 2010 papers, 2010, pp. 18. [512] Y. Zhang, D. Gopinath, Y. Ye, J. Hodgins, G. Turk, and J. Won, Simulation and retargeting of complex multi-character interactions, in ACM SIGGRAPH 2023 Conference Proceedings, 2023, pp. 111. [513] T. Zhang, H.-X. Yu, R. Wu, B. Y. Feng, C. Zheng, N. Snavely, J. Wu, and W. T. Freeman, Physdreamer: Physics-based interaction with 3d objects via video generation, 2024. [514] Y. Hu, L. Anderson, T.-M. Li, Q. Sun, N. Carr, J. Ragan-Kelley, and F. Durand, Difftaichi: Differentiable programming for physical simulation, ICLR, 2020. [515] T. Chen, P. Wang, Z. Fan, and Z. Wang, Aug-nerf: Training stronger neural radiance fields with triple-level physicallygrounded augmentations, 2022. [516] C. Gao, Y. Wang, C. Kim, J.-B. Huang, and J. Kopf, Planar reflection-aware neural radiance fields, 2024. Yukang Cao is currently Research Fellow at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received Ph.D degree from the Department of Computer Science, The University of Hong Kong (HKU) advised by Prof. Kwan-Yee K. Wong in 2024. He was the recipient of HKU-PS scholarship during Ph.D. He received my B.Eng from Zhejiang University in 2020. His research interests include computer vision and deep learning. Particularly, he is interested in 3D representation learning. 27 Jiahao Lu is currently pursuing Ph.D. degree at The Hong Kong University of Science and Technology. He received his bachelors degree in Artificial Intelligence and Automation from Huazhong University of Science and Technology, Wuhan, Hubei, P. R. China, in 2022. His research interests include computer vision and machine learning, with focus on 3D reconstruction, 3D perception, and 3D generation. Zhisheng Huang is currently PhD student under the co-supervision of Professor Wenping Wang and Professor Xin Li in the CSE Department at Texas A&M University (TAMU). He completed both his Bachelors and Masters degrees at Wuhan University. His research interests lie in 3D computer vision and graphics. Zhuowen Shen is currently pursuing Ph.D. degree in Computer Science and Engineering at Texas A&M University, coadvised by Prof. Wenping Wang and Prof. Xin Li. He received his Masters degree in Computer Science and Engineering from the University of Michigan, Ann Arbor, in 2023. His research interests lie in computer vision and machine learning, with focus on 3D reconstruction and 3D representation learning. Chengfeng Zhao is currently first-year Ph.D student at Intelligent Graphics Lab in HKUST, supervised by Prof. Yuan Liu. Prior to this, He obtained his master and bachelors degree from ShanghaiTech University, advised by Prof. Lan Xu. He was also fortunate to work closely with Prof. Jingyi Yu and Prof. Yuexin Ma. His research interests are in Computer Graphics and 3D Computer Vision, specifically video generation, human motion synthesis, learning-based garment simulation, large models, etc. Fangzhou Hong is currently research fellow at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received Ph.D. degree from MMLab at Nanyang Technological University, supervised by Prof. Ziwei Liu in 2025. He received B.Eng. degree in software engineering from Tsinghua University, China, in 2020. His research interests include computer vision and deep learning. Particularly, he is interested in 3D representation learning. Zhaoxi Chen is currently Ph.D. student at MMLab@NTU, Nanyang Technological University, supervised by Prof. Ziwei Liu. He received the bachelors degree from Tsinghua University, in 2021. He received the AISG PhD Fellowship in 2021. His research interests include inverse rendering and 3D generative models. He has published several papers in CVPR, ICCV, ECCV, ICLR, NeurIPS, TOG, and TPAMI. He ICCV, also served as reviewer for CVPR, NeurIPS, TOG, and IJCV. Ziwei Liu is currently an associate professor at Nanyang Technological University, Singapore. His research revolves around computer vision, machine learning, and computer graphics. He has published extensively on top-tier conferences and journals in relevant fields, including CVPR, ICCV, ECCV, NeurlPS, ICLR, ICML, TPAMI, TOG, and Nature Machine Intelligence. He is the recipient of the Microsoft Young Fellowship, Hong Kong PhD Fellowship, ICCV Young Researcher Award, HKSTP Best Paper Award and WAIC Yunfan Award. He serves as an Area Chair of CVPR, ICCV, NeurlPS, and ICLR, as well as an Associate Editor of IJCV. Xin Li is currently Professor and Chair of the Section of Visual Computing and Computational Media, within the College of Performance, Visualization, and Fine Arts. He is an affiliated faculty member (courtesy appointment) of the Department of Computer Science and Engineering, College of Engineering. He is also affiliated with Aggie Computer Graphics Group . He got my B.S. degree in Computer Science in 2003 at University of Science and Technology of China (USTC) with major in Computer Science, and obtained his M.S. and Ph.D. degrees in Computer Science from State University of New York at Stony Brook in 2005 and 2008. Before He joined Texas A&M University, he was faculty member at School of Electrical Engineering and Computer Science, Louisiana State University (from 2008 to 2022). Wenping Wang is currently Professor of Computer Science & Engineering at Texas A&M University. His research interests include computer graphics, computer visualization, computer vision, robotics, medical image processing, and geometric computing. He has published over 300 technical papers in these fields. He is journal associate editor of Computer Aided Geometric Design (CAGD) and IEEE Transactions on Visualization and Computer Graphics, and has chaired number of international conferences, including Pacific Graphics 2012, ACM Symposium on Physical and Solid Modeling (SPM) 2013, SIGGRAPH Asia 2013, and Geometry Summit 2019. He received the John Gregory Memorial Award for his contributions in geometric modeling. He is an ACM Fellow and IEEE Fellow. Yuan Liu is an assistant professor at the Hong Kong University of Science and Technology (HKUST). Prior to that, Yuan worked in Nanyang Technological University (NTU) as PostDoc researcher and obtained his PhD degree at the University of Hong Kong (HKU). His research mainly concentrates on 3D vision and graphics. He currently works on topics about 3D AIGC, including 3D neural representations, 3D generative models, and 3D-aware video generation."
        }
    ],
    "affiliations": [
        "Intelligent Graphics Lab, The Hong Kong University of Science and Technology",
        "S-Lab, College of Computing and Data Science, Nanyang Technological University, Singapore 639798",
        "Texas A&M University"
    ]
}