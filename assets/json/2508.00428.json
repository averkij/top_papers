{
    "paper_title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation",
    "authors": [
        "Nan Xiang",
        "Tianyi Liang",
        "Haiwen Huang",
        "Shiqi Jiang",
        "Hao Huang",
        "Yifei Huang",
        "Liangyu Chen",
        "Changbo Wang",
        "Chenhui Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, a visual prompt engineering system for T23D that transforms unstructured exploration into a guided visual process. Our approach introduces three key innovations: a dual-branch structure combining retrieval and generation for diverse candidate exploration; a multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and a prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers."
        },
        {
            "title": "Start",
            "content": "To appear in IEEE Transactions on Visualization and Computer Graphics. Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D Generation Nan Xiang , Tianyi Liang , Haiwen Huang , Shiqi Jiang , Hao Huang, Yifei Huang , Liangyu Chen ,"
        },
        {
            "title": "Changbo Wang",
            "content": ", and Chenhui Li , 5 2 0 2 1 ] . [ 1 8 2 4 0 0 . 8 0 5 2 : r Fig. 1: The user interface of Sel3DCraft includes the Model Input View (A); the Image Browser View (B), which covers the generated and retrieved images (B1), the multi-view hybrid-level scoring function (B2), and the scoring rationale (B3); the 3D Viewer View (C); the Text Exploration View (D); and the Keyword Contribution View (E). Abstract Text-to-3D (T23D) generation has transformed digital content creation, yet remains bottlenecked by blind trial-and-error prompting processes that yield unpredictable results. While visual prompt engineering has advanced in text-to-image domains, its application to 3D generation presents unique challenges requiring multi-view consistency evaluation and spatial understanding. We present Sel3DCraft, visual prompt engineering system for T23D that transforms unstructured exploration into guided visual process. Our approach introduces three key innovations: dual-branch structure combining retrieval and generation for diverse candidate exploration; multi-view hybrid scoring approach that leverages MLLMs with innovative high-level metrics to assess 3D models with human-expert consistency; and prompt-driven visual analytics suite that enables intuitive defect identification and refinement. Extensive testing and user studies demonstrate that Sel3DCraft surpasses other T23D systems in supporting creativity for designers. Index TermsPrompt engineering, text-to-3D generation, shape exploration, visualization design, visual perception. 1 INTRODUCTION Text-to-3D (T23D) generation, which transforms textual prompts directly into 3D models, has revolutionized digital content creation across industries such as virtual reality, gaming, film, and animation. Despite reducing model creation time, critical gap remains between user expectations and generated output quality. Current T23D tools operate in an end-to-end black-box manner N. Xiang, T. Liang, H. Huang, S. Jiang, H. Huang, Y. Huang, L. Chen, C. Wang, and C. Li are with East China Normal University. These authors contributed equally. Chenhui Li and Liangyu Chen are corresponding authors. Email: {51275902056, tyliang, hwhuang, 52265901032, 10215102453}@stu.ecnu.edu.cn, yifeihuang17@gmail.com, lychen@sei.ecnu.edu.cn, {cbwang,chli}@cs.ecnu.edu.cn. (see Fig. 2). They offer limited control over the generation process and frequently produce models that fail to meet expectations [18, 28, 52]. The absence of effective prompt recommendation mechanisms forces users into costly trial-and-error cycles. This disrupts creative workflows and limits the technologys practical utility for professional designers. Recent advances in prompt engineering have shown promise in textto-image domains [5, 13], enabling efficient candidate exploration and prompt refinement. However, translating these approaches to T23D generation introduces unique challenges. Unlike 2D images, 3D models require physical structural integrity. Moreover, effective evaluation must consider geometry, texture, and coherence across multiple viewsa multi-dimensional quality assessment problem beyond current prompt engineering frameworks. To bridge this gap, can we develop visual approach that transforms costly trial-and-error into structured exploration for text-to-3D generation? Our formative study (Sec. 3) with professional designers However, current prompt engineering is largely confined to 2D. To overcome this limitation, we propose novel approach leveraging multiview images as an intermediate representation. The key innovation is our semantic-based treemap wordle, which introduces T23D imagetext matching [22] to link prompts to multi-view images. This allows users to intuitively perceive how different parts of prompts contribute to specific 3D regions and iteratively refine inputs by selecting relevant keywords. 2.2 Creativity Support Systems Creativity support systems (CSS) have been extensively studied in HCI by leveraging generative models and interactive tools to enhance human creativity in design workflows. In text-to-text and text-to-image generation, systems such as TaleBrush [11] combined sketches with generative language models to support storytelling. PromptPaint [10], on the other hand, enabled users to guide image generation through paint-like interactions, highlighting CSSs potential to inspire both visual and textual creativity. In interface design, Scout [50] supported rapid exploration of layout alternatives through high-level constraints, underscoring CSSs role in enhancing design efficiency and fostering innovation. For visual search, GenQuery [48] used generative models to support expressive query formulation, helping users retrieve content aligned with their creative intent. In 3D design, 3DALL-E [30] incorporated text-to-image AI into 3D modeling workflows, providing designers with inspiration and reference images while preventing design fixation. In contrast, our system directly generates diverse and high-quality 3D models, offering designers immediate creative inspiration rather than relying on indirect image references. 2.3 Shape Exploration The field of shape exploration has evolved significantly, driven by the need to navigate and understand large collections of images and 3D models. The pioneering work of Design Galleries by Marks et al. [32] laid the foundation by introducing novel user interface concept for browsing collections through parametric representation space. Building on this, Averkiou et al. [2] extended the concept into 3D space, advancing shape exploration methodologies. Subsequent research introduced techniques like tree-based visualizations [4], augmented scatter plots [20,57,61,68], and node-link graphs [25,63,64] for large-scale exploration. These approaches have enriched our ability to visualize and delve into complex datasets, revealing intricate structures and relationships. In interactive summarization and conceptual design, Juxtaform by Pandey et al. [39] enables designers to efficiently navigate large collections and derive new shape ideas, addressing key challenges in conceptual design. Existing shape exploration techniques often neglect the dynamic exploration of 3D models generated from textual descriptions and the semantic impact of prompts. The advent of large-scale 3D datasets [12] has enabled new exploration methods, with the Contrastive LanguageImage Pre-training (CLIP) model [26] bridging modalities in these datasets. Our system utilizes OpenShape, text-image-3D pre-training model, as the retrieval branch. By leveraging CLIP-based prior, the system integrates multi-modal data from text, images, and 3D models, efficiently retrieving 3D models from vast dataset. 2.4 Text-to-3D Generation Text-to-3D generation combines linguistic descriptions with advanced rendering techniques to create 3D content directly from textual prompts. Early approaches like DreamFusion [41] leveraged CLIP [42] and NeRF [34] for text-aligned 3D content, while techniques such as Score Distillation Sampling (SDS) [56] optimized 3D representations through 2D diffusion priors. Recent advancements have addressed three key challenges in T23D generation: representation efficiency, generation speed, and multi-view consistency. For representation, diffusion models have been applied to various 3D formats including point clouds [35], voxel grids [53], 3D Gaussians [16, 66], and compact latent spaces [45] with methods like Michelangelo [67] and LION [65]. For generation speed, triplane-based methods like LRM [18] and Instant3D [23] offer Fig. 2: The workflow of text-to-3D generation tools involves four steps: (1) users input text or images and receive four rotating previews of coarse models; (2) single model is selected for time-intensive refinement; (3) the final model is exported; and (4) if unsatisfied, users can iterate on prompts during generation or refinement. Our system enhances the first and fourth steps, improving efficiency in prompt engineering and model selection. reveals key inefficiencies: limited model options, view inconsistency issues, and lack of effective evaluation and iteration mechanisms. We present Sel3DCraft, an interactive visual prompt engineering system for T23D generation. The system features dual-branch structure for initial candidate synthesis (Sec. 4.1). It leverages multi-view images as 3D visual medium and integrates Multi-modal Large Language Models (MLLMs) for multi-dimensional assessment (Sec. 4.2). Notably, the prompt recommendation mechanism (Sec. 4.3) follows the what-you-see-is-what-you-get paradigm. Furthermore, we propose cross-modal visual analytics representation (Sec. 5.2) that integrates three coordinated components: (1) multi-view satellite charts for clustering multi-view images via quality metric projection, (2) multi-view hybrid-level scoring heatmaps for model evaluation and defect localization, and (3) prompt-driven treemap wordle for diverse semantic recommendation. Validation of the semantic scoring approach (Sec. 6) and user study (Sec. 7) demonstrate Sel3DCrafts potential and superior creativity support. Compared to baselines, Sel3DCraft reduces model creation time by 70.5% (118.83s vs 402.17s) and prompt iterations by 66.2%, with significantly higher model quality ratings (4.58 vs 2.46, Fig. 8). Our contributions are threefold: We introduce T23D visual prompt engineering as novel task. For the first time, we leverage MLLMs to evaluate multi-view images of 3D models in Visual Analytics Science and Technology. This integration enables iterative refinement and significantly improves both the process and the outcome of 3D generation. We propose cross-modal multi-view visual representation approach for T23D generation. user study validated the usefulness of these visualizations in boosting user engagement. We present Sel3DCraft, an interactive visual system unifying retrieval and generation. Through user evaluations, we demonstrate that the system achieves faster model creation while maintaining comparable quality. 2 RELATED WORK 2.1 Visual Prompt Engineering Prompt engineering has evolved significantly with advances in large language models [6, 38] and text-to-image generation systems [36, 44]. Early work focused on automated text-to-text prompt optimization through methods like AutoPrompt [47] and template generation [14]. As tasks increased in complexity, interactive systems like PromptIDE [49] and PromptChainer [59] enabled collaborative human-AI refinement of prompts. Visual prompt engineering introduces new challenges for text-toimage models. Studies by Liu [29] and Oppenlaender [37] established foundational taxonomies for prompt modifiers. Recent advancements such as Li et al. [24] developed visual analytics system to guide image captioning and uncover biases, aligning with AI-assisted content creation goals. Wang [55] demonstrated how visual prompt engineering democratizes content creation, while PromptMagician [13] refines prompts through multi-level visualizations. 2 To appear in IEEE Transactions on Visualization and Computer Graphics. rapid one-step generation, while Trellis [3] introduced hierarchical latent model for high-quality one-shot generation. For multi-view consistency, MVDream [46], Wonder3D [31], and One-2-3-45 [27] have significantly improved coherence across viewpoints, while LGM [51] established new efficiency benchmarks. Unlike these methods that focus primarily on generation algorithms, our system emphasizes the human-in-the-loop exploration experience. We adopt TripoSR [18] for its efficiency and integrate multi-view MLLM-as-a-Judge evaluation [7, 60] to filter low-quality content with interactive visual prompt engineering. 3 FORMATIVE STUDY To understand how 3D designers use current text-to-3D tools in 3D content creation, we conducted formative study involving group interviews (Sec. 3.1). Through the interview results, we captured the users insights in the creative workflow (Sec. 3.2) and derived the design requirements for our system (Sec. 3.3). 3.1 Interview Process The target users of this study are 3D designers who utilize T23D tools for creativity support and inspiration. We recruited three 3D design domain experts: digital media art professor (U1), 3D modeler (U2), and 3D product designer (U3). They all have over ten years of experience in 3D model design and are familiar with T23D platforms for digital content production. During the study, participants were encouraged to freely create using existing text-to-3D websites, such as Meshy1 and Luma AI2, until they were satisfied with their results. Subsequently, we conducted 40-minute interview with each participant, focusing on their workflows and the challenges they encountered. 3.2 Interview Results Based on user feedback from the interviews, we identified four key insights, corresponding to the stages of model design: model selection, macro-level and micro-level model evaluation, and model iteration. Insight 1. Limited model selecting options. In the formative study, users noted the number of models generated in one iteration is limited. U3 mentioned, Since can only get four models at time, sometimes am not satisfied with any of them and have to waste an iteration. U2 added, Having so few choices in one iteration makes the experience frustrating. It doesnt allow for much exploration and feels like lot of work repeating. Insight 2. Inconsistency issues between model views. Unlike 2D images, generated 3D models often appear correct from the front but deform on the sides and appear dark on the back. This is common issue in generative models known as view inconsistency. In current T23D tools, users need to manually rotate them to observe inconsistencies. U1 stated, Manually rotating each model is bit annoying and cant immediately identify flaws for complex models. Insight 3. Requirements for intuitive model evaluations. When selecting satisfactory model, users need to individually evaluate each model. Intuitive evaluation assistance can save time and improve efficiency, even for experienced users. U3 emphasized, Clear, objective metrics for geometry cleanliness or texture fidelity would significantly streamline the decision-making process. Insight 4. Difficulty in crafting appropriate text prompts. Users often fail to achieve desired results from text input in single iteration. However, they also find it challenging to effectively refine prompts in subsequent iterations. U3 noted, Im not that sure about what level of detail is required in the prompt to guide model generation properly, especially when controlling specific factors. U1 shared, If the system could suggest better ways to phrase my inputs, it would save me lot of trial and error. 1https://www.meshy.ai 2https://lumalabs.ai 3.3 Design Requirements Based on the insights from the interview results, we derive the following design requirements: R1. Generate enough images for exploration and selection (Insight 1). As stated in Insight 1, most websites typically provide only three to five model candidates per iteration. This severely restricts the users exploration space, forcing them to undergo multiple rounds of iteration. Therefore, the system should provide sufficient number and wide variety of image candidates for users to browse, helping them efficiently find preferred images. R2. Support 3D-aware multi-view results (Insight 2). Although rotating single model is not time-consuming, individually rotating each model for observation is waste of time. To reduce observation time, the system should support multi-view exploration mode that highlights inappropriate views, enabling users to quickly filter models. R3. Provide human-aligned multi-level evaluation (Insight 3). When selecting models, subjective aesthetic standards are determined by the 3D designers themselves. However, objective evaluation criteria, such as lighting and color consistency or physical structure, can be automated to save time. We find it essential to provide automatic evaluation aligned with human semantics, helping users efficiently identify and discard low-quality models. R4. Recommend iterative refinement of prompts (Insight 4). When users modify text prompts, numerous manual attempts may not yield the desired results, resulting in waste of time. If the system provides prompt suggestions after each iteration, guiding users towards discovering phrases that can further boost model output, it would not only help users find satisfactory models but also improve model generation efficiency. 4 TECHNICAL METHOD Our approach directly addresses the design requirements (R1-R4) through tightly integrated pipeline. This pipeline combines multimodal exploration, perceptual evaluation, and interactive refinement. Sel3DCraft transforms the conventional T23D workflow from an unguided trial-and-error process into structured visual exploration. This transformation is guided by human perceptual metrics. The framework  (Fig. 3)  implements three-stage pipeline. The exploration stage expands the candidate space through parallel retrieval and generation pathways (R1). large language model generates semantically diverse prompt candidates TP. Simultaneously, textto-3D pretrained model retrieves candidate shapes S3D and the T2I model generates image sets IT 2I. This creates rich design space for exploration. The evaluation stage addresses multi-view consistency challenges (R2) through comprehensive perspective rendering and automated assessment. Both low-level Fl() and high-level Fh() scoring functions serve as subcomponents of our multi-view scoring function Fmv. These functions provide human-aligned quality metrics (R3) that identify structural and semantic inconsistencies without manual inspection. The refinement stage translates evaluation metrics into visual guidance for prompt engineering (R4). We project quality scores onto visual attributes to create perceptually intuitive representations. These representations include prompt treemap wordle and keyword contribution map. This enables users to make informed adjustments based on quality feedback rather than blind iteration. 4.1 Multi-modal Unified Visual Representation At the core of our T23D visual prompt engineering framework is multi-modal unified retrieval mechanism. This mechanism bridges the semantic gap between users textual descriptions and the desired 3D output. Our approach integrates text, 2D images, and 3D shape representations into coherent exploration space. This allows users to understand the relationships between separate spaces (R1). Pre-trained Model-based Retrieval. Since there is no specific dataset like diffusionDB [58] for T2I, efficient retrieval from large 3D model repositories is crucial for T23D Visual Prompt Engineering. 3 Fig. 3: The Sel3DCraft pipeline begins by expanding user text input into multiple candidates via LLM. dual-branch pipeline then generates/retrieves corresponding multi-view images, displayed in an interactive satellite view for 3D perception. Each candidate is automatically evaluated using eight-level semantic scores with heatmap visualization. For refinement, score-guided recommendation mechanism suggests optimal keywords, enabling iterative optimization until user satisfaction is achieved. Mainstream T23D models often use the Objaverse dataset, containing over 800,000 annotated 3D models. To facilitate retrieval, we use OpenShape [26], large-scale Pretrained Model built on Objaverse. OpenShape integrates text, images, and 3D shapes into joint embeddings. Using OpenShape, our pipeline retrieves prompt candidates TPC, image candidates, and 3D shape candidates S3D for each set of prompt inputs. Multi-modal Synthesis. The second branch first gets prompt candidates TT 2I by augmenting the retrieval prompt TPC using an association module. It then generates image candidates IT 2I applying text-toimage (T2I) model [40]. The generated images are assessed by 3D-friendly scoring function F3D to select the most suitable image candidates for 3D modeling. TripoSR [18] generates 3D models from the selected images and converts them to multi-view renderings Imview. The renderings are evaluated by multi-view scoring function Fmv to compute keyword scores. Integration and User Interaction. The fusion step combines the outputs from two distinct branches within joint embedding space: Ff s(T, I, S) = (TPC, 2I, 3D), (1) where Ff denotes the fusion function, the input text, the input image, the input 3D model, the joint embedding space for image browser, TPC the text from retrieval, TT 2I the augmented text for T2I, and S3D the optimized 3D model. The key technical challenge lies in establishing effective crossmodal alignment between textual descriptions, 2D images, and 3D models while preserving modality-specific information. Our joint embedding construction process addresses this through two-stage approach: first aligning text-image pairs through contrastive learning, then extending to 3D space via multi-view rendering representation. Further implementation details are provided in Appendix A.3. 3D-Friendly Score. Since not all 2D images are suitable for generating 3D models, we introduce metric to evaluate 3D-friendliness in multi-view image mode. Through empirical analysis of 200 images and their corresponding 3D models, we found that images containing multiple objects within complex backgrounds consistently produced incomplete or distorted 3D models, with 78% failure rate observed across our test set. We discovered strong correlation between foreground segmentation quality and 3D model completeness (Pearsons r=0.82), 4 with centered, single objects against simple backgrounds achieving 3.4 times higher quality scores than complex scenes. The metric integrates three key components: Center Offset (CO), Bounding Box Intersection Over Union (IoUBB), and binary map Intersection Over Union (IoU), leveraging saliency maps [19] and SAM [21] for foreground segmentation. The formula for our composite metric f3D is given by: f3D (I) = wO (1 Omax ) + wIoU IoU + wBB IoUBB. (2) We tested various weight combinations on validation set of 50 images and evaluated the correlation between our score and human expert ratings of 3D model quality. Through this process, we found that the final weights wO = 0.3, wIoU = 0.4, and wBB = 0.3 achieved the highest correlation coefficient (r = 0.76) with expert assessments. This weighting reflects our finding that segmentation quality (IoU) most strongly impacts generation outcomes, while center positioning and bounding box coverage are also significant but less influential. 4.2 Multi-view Hybrid-level Scoring Function While our dual-branch architecture expands the exploration space, this advantage becomes meaningful only when coupled with efficient filtering mechanisms. Addressing R2 and R3, we developed hybrid scoring framework. This framework combines low-level computational metrics with high-level perceptual judgments through MLLM-based semantic evaluations. Through systematic literature analysis and iterative refinement with 3D artists, we identified eight quality dimensions. These dimensions are structured as complementary semantic hierarchies. Low-level semantics quantify measurable properties including light-color consistency and cross-view coherence via CLIP-based metrics. High-level semantics leverage MLLM perception to evaluate abstract qualities. These include 3D plausibility, text-image alignment, texture-geometry coherency, and low-level texture. Table 1 summarizes our eight semantic scoring dimensions. 4.2.1 Low-level Scoring Function To address inconsistency issues, we propose low-level scoring function aimed at assessing image suitability for reconstruction. To appear in IEEE Transactions on Visualization and Computer Graphics. Table 1: Eight-Dimensional Semantic Scoring Framework for Multi-view 3D Model Evaluation Metric Inputs Role & Computational Method Low-Level Metrics Color Consistency Imview Light Consistency CLIP Score CLIP-I Score High-Level Metrics Text-Image Alignment 3D Plausibility TextureGeometry Coherency Low-Level Texture Consistency assessment via Bhattacharyya coefficient on Lab histograms [54] Lighting evaluation via L-channel histogram comparison [34] Semantic alignment via textimage cosine similarity [17] Imview (T, I) (Ire , j) Cross-view coherence via feature alignment using CLIP [43] (T, Imview) Semantic correspondence via Imview Imview Imview MLLM-based evaluation [60] Spatial reasoning via MLLMbased assessment [7] Surface analysis via MLLMbased evaluation [60] Detail assessment via MLLMbased evaluation [7] Color and Light Consistency. Multi-view consistency is core challenge in 3D generation tasks [34], and the most intuitive visual manifestation of multi-view consistency is the consistency of color and light. Ensuring color and light consistency fcolor, flight Fl across multi-view images Imview generated from 3D candidate models S3D is crucial. We analyze Lab color histograms and L-channel histograms across multi-view images to evaluate consistency. The Lab color space aligns closely with human visual perception, making it ideal for assessing color consistency. We use the Bhattacharyya coefficient to compare individual Lab histograms against the average, with values closer to 1 indicating higher consistency. Specifically, for color consistency we compute fcolor = 1 Lab, HLab), where is the number of views, H(i) Lab is the Lab histogram of view i, and BC denotes the Bhattacharyya coefficient. Light consistency follows the same approach using L-channel histograms. i=1 BC(H(i) CLIP Score vs CLIP-I. Beyond visual attributes, multi-view consistency also encompasses semantic and feature-level coherence. To satisfy designers requirements for 3D models to accurately reflect user intent and maintain consistency across different views, we have selected two metrics: CLIP Score [17] and CLIP-I [43]. CLIP Score measures text-image semantic alignment using cosine similarity between prompt embeddings and image embeddings in CLIP space, formulated as fclip(T, I) = cos(Etext (T ), Eimage(I)). In contrast, CLIP-I assesses internal visual consistency by comparing the front view (0 rotation) against other viewpoints of the same 3D model, calculated as fclipI(Ire , j) = cos(Eimage(Ire ), Eimage(I j)) where Ire is the reference front view. 4.2.2 High-level Scoring Function While low-level scores measure visual consistency through direct attribute computation, high-level semantic evaluation employs MLLMs to assess results from human perspectives. Compared to traditional evaluation methods, MLLM-as-a-Judge [7, 60] provides explanatory evaluations with comprehensive performance feedback, serving as scalable and reproducible alternative to costly manual assessment. According to discussions with designers, we involve four humanaligned scores based on R3: Text-Image Alignment, 3D Plausibility, Texture-Geometry Coherency, and Low-Level Texture. These metrics are precisely defined as follows: Text-Image Alignment evaluates semantic correspondence between input prompts and generated models using MLLM reasoning beyond simple CLIP simi5 larity. 3D Plausibility assesses spatial coherence and physical realism across viewpoints, determining whether the model maintains structural integrity in 3D space. Texture-Geometry Coherency examines how surface details align with object contours and shape, ensuring realistic texture mapping. Low-Level Texture focuses on fine detail preservation including edges, patterns, and color variations that contribute to visual fidelity. Our MLLM prompt design includes several key components: clear task definitions specifying evaluation criteria and scoring scales; contrastive examples presenting paired highand low-quality outputs for calibration; and step-by-step evaluation framework guiding the model through structured assessment. We use paired images for contrastive scoring, where the MLLM compares consecutive views of the same 3D model to generate both semantic scores and explanatory reasoning. This approach provides transparent, step-by-step evaluations that enhance user understanding of model quality assessments. The complete prompt template can be found in Appendix C.2. 4.3 Score-guided Prompt Recommendation The semantic gap between text and 3D outputs hinders text-to-3D creation, as users struggle to convert quality feedback into actionable prompts. We address this through visual recommendation system that translates abstract quality assessments into concrete semantic suggestions (R4). Traditional prompt engineering lacks structured guidance. Our treemap wordle bridges this gap by mapping quality metrics to keywords through spatial layout, size, and color. Prompt Conceptual Augmentation. To enhance the text-to-3D synthesis, we introduce streamlined prompting mechanism designed to refine and extend the semantic scope of user inputs using LLMs. This is formally expressed by: TP = faug(t0), (3) where faug() denotes the mapping function that combines the original prompt t0 with predefined corpus of 3D-specific prompts to generate set of enriched prompt candidates TP = {t1, . . . ,tN }. Treemap Wordle Prompt Recommendation. An essential aspect of our visual prompt recommendation system is to capture and adapt to users preferences for different criteria of 3D model generation. The multi-view scoring function Fmv evaluates each 3D model S3D from eight perspectives. This yields score vector Fmv(s) R8. To provide recommendations that align with users interests, we propose treemap wordle visualization for interactive prompt exploration. The key idea is to project the high-level score vectors into visually interpretable low-dimensional space. In this space, similar prompts are grouped together and important keywords are highlighted. Prompt Clustering and Keyword Extraction. We apply HDBSCAN clustering on 2D embeddings to group similar prompts. For each cluster, we compute word frequencies and select the most frequent words as representative keywords. The score of each keyword is calculated as the average of all scores from models containing that keyword, enabling quality-based keyword recommendations. Visual Encoding. We visualize clusters using treemap layout where rectangular areas reflect cluster sizes. Keywords appear as word clouds with font sizes indicating frequencies and colors representing eight-level semantics (see Fig. 4). Keyword Contribution Map. We compute CLIP cross-attention maps to connect keywords with multi-view images using IoU between attention maps and foreground regions. The treemap wordle visualization highlights dominant themes in generated 3D models by prevalence and quality. By clicking on keyword wki, the user can filter 3D models to include only those models whose prompts ps contain wki. This allows targeted exploration of specific preferences. The user can also merge multiple keywords to compose new prompts on the fly. Throughout the prompt update process, the visual prompt recommendation system updates the treemap wordle dynamically. This reflects the users current focus. Fig. 4: Interactive design of visualization charts and mapping relationships between data. (A) illustrates the visual design of the treemap wordle. (B) depicts the design of the multi-view satellite chart. (C) presents the systems multi-view hybrid-level scoring heatmaps. (D) shows the keyword contribution map representing the semantic contribution. 5 SEL3DCRAFT SYSTEM This section introduces Sel3DCraft, an interactive visual analytics system that supports dual-branch 3D model generation from textual descriptions, and provides interactive features such as model evaluation and prompt recommendations. In Sec. 5.1, we present the systems interface design. We then explain how users interact with the system (Sec. 5.2) and demonstrate the interaction through scenario design (Sec. 5.3). 5.1 User Interface Our systems design philosophy focuses on maximizing users ability to perceive and understand relevant information with minimal interaction. As shown in Fig. 1, the systems interface comprises five modules: (A) Model Input View, enabling users to input initial prompts and perform prompt associations (R1); (B) Image Browser View, which displays images retrieved and generated from prompts as satellite charts (R2), and presents multi-view heatmaps along with eight-level semantic scores (R3); (C) 3D Viewer View, showcasing the final generated model for observation; (D) Text Exploration View, clustering prompts by semantics into treemap wordle to recommend keywords for iterative modifications (R4); and (E) Keyword Contribution View, displaying the semantic contribution of keywords to multi-view images (R4). All modules are closely interconnected through multi-modal conversions of text, images, and 3D models. The following sections detail the innovative visualization designs within the interface. 5.2 Interacting with Sel3DCraft As shown in Fig. 4, the interactive modules of this system consist of four main components: multi-view satellite charts, hybrid-level scoring heatmaps, treemap wordle, and keyword contribution map. Next, we will provide detailed introduction to each of these components (For more intuitive view of the interactions, please refer to our supplementary video). 5.2.1 Multi-view Images Surrounding Satellite Chart The satellite charts multi-view display enables intuitive browsing of prompt-generated images, allowing users to quickly identify and discard those with perspective defects without needing to rotate and inspect each one in the 3D viewer. As shown in Fig. 4 (B), the central image shows the frontal view, while eight surrounding images (at 45 intervals) represent rotation angles (0-360) of multi-view images relative to the main view. The distance of each view from the center reflects its semantic score for 3D plausibilitylower-scoring views are positioned farther outward, with circle colors radiating from deep blue to light blue. Therefore, tighter cluster indicates higher overall quality. Users can hover to view full prompts and individual scores (R2). Fig. 5: The first scenario demonstrates the systems ability to generate targeted models. 5.2.2 Multi-view Hybrid-level Scoring Heatmaps The Image Browser Views bottom panel presents hybrid-level scoring through two visual components (Fig. 4 (C)). First, heatmaps highlight defect areas in red, with heavily flawed images outlined in red framesusers can click for detailed inspection. Second, multi-line chart tracks eight semantic dimensions (x-axis) versus their scores (yaxis), with each colored line representing view. By long-pressing the left mouse button on the vertical axis, users can create selection to isolate and view semantic polylines within the selected area, providing more intuitive and focused view (R3). 5.2.3 Treemap Wordle Text Exploration The treemap wordle (Fig. 4 (A)) clusters text from all 3D candidates to recommend supplementary keywords. Its eight horizontally divided color-coded sections represent different semantics, with high-level semantics (deemed more important by designers) occupying 20% more area than low-level ones. Higher-frequency keywords appear larger within each section for quick selection. When user hovers over keyword, related images in the Image Browser are highlighted. Adding keywords to the input triggers new iteration, regenerating both image and 3D candidates. The keyword contribution map (Fig. 4 (D)) employs Sankey diagram to visualize each keywords impact to the multi-view images. Contribution values are encoded using color gradient from yellow to purple, representing high to low contributions, respectively. threshold slider allows users to filter out lower-contributing links, retaining only the most significant connections (R4). 5.3 System Scenario Design The system supports three main usage scenarios. In Scenario 1, users create the character Cattiva from \"PalWorld,\"3 stopping when the model sufficiently resembles the target. This demonstrates the systems accuracy and quality in matching specific styles. In Scenario 2, users design logo for Sel3DCraft with creative freedom in visual style, which showcases the systems ability to foster creativity while contrasting with Scenario 1s replication task. In Scenario 3, users generate teacup models with watertight geometry and sufficient wall thickness to ensure 3D printability, highlighting the systems practical applicability and fabrication potential. Take Scenario 1 as an example. The interaction flow of the system is illustrated in Fig. 5. The user started with the prompt, pink cat Pokemon with blue eyes, to describe Cattiva. Then she selected preferred multi-view satellite image to view the expanded prompt and added Japanese as new keyword. After the system produced more accurate images, the user used the hybrid-level scoring function to select the most satisfactory model from large pool of candidates. However, by preliminary reviewing in the 3D viewer, the user decided to explore the treemap wordle for further refinement. The user identified thinner as significant contributor aligned with her improvement goals while analyzing the keyword contribution map. She then incorporated this term into the prompt and proceeded to another iteration. The final model matched Cattivas original appearance, demonstrating the systems effectiveness in accurate character modeling. 3https://palworld.gaming.tools/en 6 To appear in IEEE Transactions on Visualization and Computer Graphics. Fig. 6: Human evaluation results of high-level semantics on generation. 6 VALIDATION OF THE SEMANTIC SCORING APPROACH 6.1 Human Evaluation on High-level Semantics To further validate the utility of high-order semantics, we measured their alignment with human preferences, as human judgment is often considered the gold standard for quality and reliability. For each order of semantics, evaluators were presented with two sets of multi-view images, one with higher semantic score based on our systems evaluation mechanism. Evaluators were then asked to select the set with better semantic representation. response was considered correct if the evaluator selected the set with the higher semantic score. Additionally, we excluded image pairs with semantic score differences of less than 0.3, as humans struggle to distinguish such fine-grained differences. We invited twenty university students as evaluators to investigate whether non-experts perceive the systems model evaluation as sufficiently accurate. Five-student groups performed single-level semantic evaluation tasks, with each participant distinguishing thirty multi-view image pairs. The results (see Fig. 6) show that evaluators correctly identified the model with higher semantic scores with an accuracy rate of over 79%. However, consistency in texture-geometry coherency was lower, likely due to the limited familiarity of amateur users with industry standards for 3D models, making such judgments more challenging. 6.2 Expert Annotation on Semantic Consistency We also invited the same three experts (U1-U3) who participated in our formative study to annotate fifty 3D models based on high-level semantics. Subsequently, we employed the Bland-Altman analysis to evaluate the consistency between the average scores from expert annotations and the semantic scores assessed by the MLLMs. The results show that most data points fall within the consistency limits, and the mean difference is close to zero, indicating minimal deviation between the expert annotations and the system-generated scores, thus demonstrating strong consistency (see Appendix D). To validate the generalizability of our evaluation system across different AI models, we employed Gemini 2.0 Flash and Qwen2.5VL for automated quality assessment of 3D models, in addition to GPT4o, which we used for our systems scoring functions. These assessments were compared against expert manual annotations. While minor discrepancies exist in individual samples, all AI evaluation models demonstrate strong alignment with expert ratings in overall trends (see Appendix D). This finding indicates that our cross-model evaluation framework exhibits considerable robustness, where different AI assessors can effectively capture quality metrics analogous to human experts criteria. 7 USER STUDY Sel3DCraft, embodying human-in-the-loop methodology, necessitates user study to ascertain its efficacy and usability. We adopted multi-stage evaluation strategy. The first phase focused on evaluating (1) the usefulness of the three proposed visualization components, (2) the overall effectiveness of the system, and (3) its creative advantages over two baseline systems. Based on user feedback, we then made minor refinements to the visual styling of the multi-view satellite chart and the keyword contribution mapwithout altering their core functionalityand carried out focused follow-up evaluation to assess the impact of these design adjustments. 7.1 Participants We recruited twelve participants (eight females, four males, average age = 23.7) from various fields, including computer science, digital 7 media, and industrial design. All participants had over three years of professional experience in 3D design and frequently used text-to-3D tools for artistic exploration. On average, they rated their familiarity with text-to-3D tools as 4.75 out of 5. However, their understanding of prompt engineering was relatively limited, with an average rating of 3.76 out of 5. 7.2 Baseline Systems In the formulation of our study, two baseline systems were designed to serve as comparative benchmarks, enabling nuanced evaluation of our systems performance and user experience. 7.2.1 Vanilla T23D Tool Baseline utilizes the Meshy website to generate text-to-3D models. Users can input text, and four preliminary models are generated with each iteration. Users then select the most suitable model for further refinement. However, Baseline lacks both prompt recommendation and model scoring functionalities. We selected Baseline as the control group representing the traditional T23D workflow. 7.2.2 T2I Visual Prompt Engineering Baseline employs PromptMagician [13] to retrieve relevant images based on user input and provides real-time keyword recommendations. Users can then convert the selected images into 3D models using the TripoSR [18] web interface provided in our study. Although Baseline employs prompt engineering similar to our system, it is limited to T2I generation and lacks corresponding scoring mechanism to validate the feasibility of image-to-model generation. 7.3 Procedure and Tasks The evaluation process consists of three main phases: introduction, comparative experiment, and survey feedback collection. Introduction (10 min). At the beginning of the evaluation, we introduced the study background and evaluation plan to the users. Next, we described the interface features of the systems and demonstrated their functionalities through clear examples [62]. Comparative Experiment (30 min). The comparative study evaluated three usage scenarios: targeted generation of predefined models, theme-based creative design based on personal preferences, and preparation of 3D printing-ready models. Participants selected one scenario and used all three systems within ten minutes per system. The order of system usage (Baseline A, Baseline B, and Sel3DCraft) was randomized across participants to mitigate order effects. Feedback Survey (20 min). To conclude the study, we administered two 5-point Likert-scale questionnaires. The first assessed the utility of our system across its three core modules and its overall effectiveness (see Fig. 7). The second evaluated creativity support compared to Baselines and (see Fig. 8). Follow-up interviews provided qualitative insights for deeper analysis. 7.4 Results Analysis After the comparative experiment, two questionnaires were collected from participants. Based on participant feedback, we analyzed the systems usefulness, effectiveness, and differences compared to the two baseline systems. We ultimately divided our results analysis into two parts: qualitative feedback and quantitative comparison to baselines. 7.4.1 Qualitative Feedback Usefulness of Satellite Chart Most users agreed that the Image Browser provided diverse range of images (Q1). P1 commented, The view contains wide variety of image types, allowing for free exploration based on user input. Each single-view image is surrounded by multi-view images in satellite-like arrangement, helping users identify incorrect physical structures (Q2). P8 noted, When creating cattiva, found misaligned tail in one of the views, so immediately discarded the image. Usefulness of the Multi-view Hybrid-level Scoring Function. The semantic scores were considered reasonable and effective (Q3). P6 easily identified low text-image alignment through the eight-level line Fig. 7: Questionnaire results of the usefulness and effectiveness of our system. A1, A2, and A3 denote usefulness evaluations for the Satellite Chart, Multi-view Hybrid-level Scoring Function, and Treemap Wordle Text Exploration visualizations respectively, while A4 represents the overall system effectiveness assessment. The average scores and 0.95 confidence intervals are listed above the user option counts on the right side. chart. Participants agreed that heatmaps effectively highlighted defects (Q4) by marking anomalous areas in red (P12). Usefulness of the Treemap Wordle Text Exploration. The treemap wordle was widely praised. Users found keyword contributions clearly visualized (Q5) and keywords simple and easy to understand (Q6). P2 noted keywords were primarily clear adjectives and nouns. The Text Exploration View helped users find suitable supplementary keywords (Q7). P4 noted it effectively bridged knowledge gaps during generation. Effectiveness of the system Through iteration, all participants found satisfactory final model (Q8). Additionally, they found the system easy to use (Q9) and did not require technical support (Q10). The vast majority of participants agreed that the systems various functions were well integrated (Q11). P1 noted that hovering over word in the Text Exploration View highlighted the corresponding image, effectively conveying the connection between the prompt, image, and 3D candidate set. P6 appreciated the scoring module, as it objectively reflected image quality, eliminating time-consuming subjective judgments. Finally, the majority of participants indicated that they would like to use this system frequently (Q12). Comparison of System to Baselines The comparative experiments employed two baselines evaluated based on the Creativity Support Index [9]. As shown in Fig. 8, our system outperformed both baselines across all criteria. Baseline restricts users to four fixed candidates in one iteration without effective guidance for refinement options (P7), while our system enables free exploration through retrieved/generated models. P3 remarked, can see notable increase in available options, which grants me more freedom. For specialized tasks like Sel3DCraft logo design, Baseline As 5-character text limit hindered performance (P6). Users reported Baseline Bs Image Browser contained many 3Dunsuitable images, whereas our system visually grays them out for distinction. While Baseline provided semantic guidance, it lacked scoring mechanism - addressed by our Text Exploration Views sizecoded keywords. Additionally, some users particularly appreciated the eight-level semantic evaluation, which provided comprehensive model quality evaluation and helped them clarify their needs. Fig. 8: Questionnaire results comparing our system with two baseline systems. 7.4.2 Quantitative Comparison to Baselines We also conducted quantitative comparison to baselines based on the Creativity Support Index and iteration time (see Tab. 2). The detailed paired-sample t-tests confirming statistical significance can be found in Appendix E. Our system vs. Baseline A. Our system enabled greater exploration freedom, with higher enjoyment and exploration scores than Baseline (p < .001). It also outperformed Baseline across all categories, offering more immersive and rewarding experience (Immersion = .003, Results worth effort < .001, see Appendix E.2). Our system vs. Baseline B. Quantitative results demonstrated that our system achieved higher levels of expressiveness (p < .001) and perceived value in terms of results worth the effort (p < .001) compared to Baseline B, highlighting its effectiveness in supporting satisfying user interactions (see Appendix E.2). Table 2: Time analysis of the system in the user study. The proposed method shows improved efficiency in terms of total time, number of iterations, and average time per iteration compared to the baseline methods. The values represent the 95% confidence intervals. Category Time (s) Iterations Time/Iteration (s) Baseline Baseline Our Method 391.75 412.58 118.83 2.75 3.17 1.00 148.38 40.73 142.40 35.53 118.83 41.73 7.5 Follow-up Evaluation of Revised Visualizations Based on the results of our earlier evaluation (see Fig. 7) and user feedback, we identified two specific concerns: five participants indicated that the keyword contribution map did not effectively communicate the influence of keywords on multi-view images; additionally, P3 noted that although the satellite chart was useful for assessing image quality when viewed in isolation, it caused some confusion when integrated into the full system interface. To address these issues, we refined the visual styling of both componentsusing distinct layouts and separable visual channelswithout altering their core functionality or underlying logic (see Figs. 9 and 10). Fig. 9: Satellite chart before and after revision. The left shows the original version, and the right shows the revised version. 8 To appear in IEEE Transactions on Visualization and Computer Graphics. modify models. For instance, in our system, some users suggested that for multi-view images with obvious defects, fine-tuning and reintroducing them into the model generation process could improve the interactive improvement of model creation. Therefore, future work should develop designer-friendly fine-tuning tools for seamless pipeline integration, or explore text-guided model adjustments -further enhancing 3D creation flexibility and human-centered interaction. 8.2 System Generalizability In addition to facilitating text-to-3D prompt engineering, our system can be extended to various applications, such as game development and 3D asset management. For game production, it enables rapid filtering of style-consistent 3D assets across different scenarios. This contributes to improving the overall visual consistency and player experience in the game. In 3D model library management, the system facilitates batch quality assessment for accuracy, style adherence, and diversity. This helps maintain and optimize the quality of the 3D model library while improving the efficiency of retrieval and usage. 8.3 Limitations One key limitation of our system is its reliance on fixed viewpoints, which can obscure critical aspects of 3D model. For example, in the teacup case, the lack of top view prevents inspection of the interior, often resulting in the selection of solid, unrealistic models. This limitation can hinder users from fully evaluating geometric details and making optimal choices. Additionally, the system does not currently support 3D animation previews, which are widely used in platforms like Sketchfab4. As result, dynamic behaviorssuch as joint articulation or material deformationcannot be assessed, limiting the systems applicability in scenarios where animation is essential. To mitigate the fixed-viewpoint limitation, we plan to augment the multi-view image evaluation stage with dedicated top-down and birdseye views. This will allow users to inspect occluded regions (e.g., the interior of teacup) and thus more comprehensively assess geometric details, improving evaluation quality. Moreover, to address the lack of animation support, we will integrate short 3D animation snippets into the final 3D viewer. Users will be able to play and navigate through animations directly within the interface, enabling assessment of dynamic behaviors (e.g., object articulation, material deformation) that static views cannot convey. 9 CONCLUSION AND FUTURE WORK Sel3DCraft advances text-to-3D generation through dual-branch architecture combining model generation and retrieval. To overcome the limitations of traditional T23D tools, the system utilizes multi-view images as an intermediary, developing multi-view hybrid scoring feature to ensure human semantics alignment. The system integrates novel visualizations like treemap wordle and satellite charts for prompt recommendations and candidates visualization. We conducted user studies to validate the usefulness and effectiveness of the system. By comparing with two other baselines, we demonstrate the advantages of Sel3DCraft in enhancing creativity in design process. Recent advancements in T23D have enabled the generation of 3D objects with components [8, 15]. In many design scenarios, such as blind box design, designers are particularly interested in the spatial relationships of these components. Future work will focus on leveraging MLLMs for geometric analysis [1], thereby creating user-friendly exploration spaces and corresponding structured prompt engineering. ACKNOWLEDGMENTS The authors wish to acknowledge the support from NSSFC under Grant 22ZD05, NNSFC under Grant 62472178, and the Natural Science Foundation of Shanghai Municipality, China under Grant 24ZR1418300. 4https://sketchfab.com/ 9 Fig. 10: Keyword contribution map before and after revision. The left shows the original version, and the right shows the revised version. To further evaluate the usability of the revised visual components, we conducted follow-up study focusing specifically on the updated multi-view satellite chart and keyword contribution map. This supplementary evaluation complements the original user study by focusing solely on the usability and clarity of the revised visual encodings. The systems core functionality, tasks, and experimental flow remained unchanged. The supplementary evaluation is guided by the Principles of Effective Data Visualization [33], focusing on four aspects: color usage, clarity of information, user feedback, and perceived contribution to overall system usefulness. All twelve participants from the original user studyeach with over three years of experience in 3D designwere invited back for this follow-up. As they were already familiar with the system and task structure, no additional tutorial was required. Each participant selected one of the three previously defined usage scenarios and completed design task using two versions of the system: the original and the revised. To mitigate learning effects, the order of system usage was counterbalanced: six participants used the original system first, while the remaining six began with the revised version. Each participant was given total of 15 minutes to use both versions. After completing the tasks, they responded to five-point Likert-scale questionnaire. The results are summarized in Fig. 11. Fig. 11: The supplementary questionnaire results on revised visualization design evaluation. SC represents the multi-view satellite chart, and KC represents the keyword contribution map. 8 DISCUSSION 8.1 Design Implications Optimizing Keyword Distillation for Effective Prompt Refinement. We score keywords and prioritize high-quality terms in the treemap wordle to balance conciseness with semantic relevance. Future work could improve keyword extraction precision. Aligning Human Semantics with Machine Evaluation. Our eightlevel semantic evaluation framework combines computational metrics with MLLM-based assessments. User studies confirmed high satisfaction with the systems effectiveness. Future work could explore aesthetic appeal and artistic value metrics. Enhancing User-controlled Fine-tuning in Model Creation. When dissatisfied with results, users can either regenerate or directly REFERENCES [1] A. Abdelreheem, A. Eldesokey, M. Ovsjanikov, and P. Wonka. ZeroIn J. Kim, M. C. Lin, and B. Bickel, shot 3d shape correspondence. eds., SIGGRAPH Asia 2023 Conference Papers, SA 2023, Sydney, NSW, Australia, December 12-15, 2023, pp. 59:159:11. ACM, 2023. doi: 10. 1145/3610548.3618228 9 [2] M. Averkiou, V. G. Kim, Y. Zheng, and N. J. Mitra. Shapesynth: Parameterizing model collections for coupled shape exploration and synthesis. vol. 33, pp. 125134, 2014. doi: 10.1111/CGF.12310 2 [3] M. Averkiou, V. G. Kim, Y. Zheng, and N. J. Mitra. Shapesynth: Parameterizing model collections for coupled shape exploration and synthesis. Comput. Graph. Forum, 33(2):125134, 2014. doi: 10.1111/CGF.12310 3 [4] D. Bertucci, M. M. Hamid, Y. Anand, A. Ruangrotsakun, D. Tabatabai, M. Perez, and M. Kahng. Dendromap: Visual exploration of large-scale image datasets for machine learning with treemaps. IEEE Trans. Vis. Comput. Graph., 29(1):320330, 2023. doi: 10.1109/TVCG.2022.3209425 2 [5] S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman. Promptify: Text-to-image generation through interactive prompt exploration with large language models. In S. Follmer, J. Han, J. Steimle, and N. H. Riche, eds., Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 20231 November 2023, pp. 96:196:14. ACM, 2023. doi: 10. 1145/3586183.3606725 1 [6] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. HerbertVoss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds., Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. [7] D. Chen, R. Chen, S. Zhang, Y. Wang, Y. Liu, H. Zhou, Q. Zhang, Y. Wan, P. Zhou, and L. Sun. Mllm-as-a-judge: Assessing multimodal llm-asIn Forty-first International a-judge with vision-language benchmark. Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net, 2024. 3, 5 [8] Y. Chen, T. Wang, T. Wu, X. Pan, K. Jia, and Z. Liu. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, eds., Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part XXIV, vol. 15082 of Lecture Notes in Computer Science, pp. 128146. Springer, 2024. doi: 10.1007/978-3-031-72691-0_8 9 [9] E. Cherry and C. Latulipe. Quantifying the creativity support of digital tools through the creativity support index. ACM Trans. Comput. Hum. Interact., 21(4):21:121:25, 2014. doi: 10.1145/2617588 8 [10] J. J. Y. Chung and E. Adar. Promptpaint: Steering text-to-image generation through paint medium-like interactions. pp. 6:16:17, 2023. doi: 10.1145/ 3586183.3606777 2 [11] J. J. Y. Chung, W. Kim, K. M. Yoo, H. Lee, E. Adar, and M. Chang. Talebrush: Sketching stories with generative pretrained language models. In S. D. J. Barbosa, C. Lampe, C. Appert, D. A. Shamma, S. M. Drucker, J. R. Williamson, and K. Yatani, eds., CHI 22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, pp. 209:1209:19. ACM, 2022. doi: 10.1145/3491102. 3501819 2 [12] M. Deitke, R. Liu, M. Wallingford, H. Ngo, O. Michel, A. Kusupati, A. Fan, C. Laforte, V. Voleti, S. Y. Gadre, E. VanderBilt, A. Kembhavi, C. Vondrick, G. Gkioxari, K. Ehsani, L. Schmidt, and A. Farhadi. Objaverse-xl: universe of 10m+ 3d objects. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds., Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. [13] Y. Feng, X. Wang, K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang, and W. Chen. Promptmagician: Interactive prompt engineering for text-toimage creation. IEEE Trans. Vis. Comput. Graph., 30(1):295305, 2024. doi: 10.1109/TVCG.2023.3327168 1, 2, 7 [14] T. Gao, A. Fisch, and D. Chen. Making pre-trained language models better few-shot learners. In C. Zong, F. Xia, W. Li, and R. Navigli, eds., Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 38163830. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.ACL-LONG.295 2 [15] H. Han, R. Yang, H. Liao, J. Xing, Z. Xu, X. Yu, J. Zha, X. Li, and W. Li. REPARO: compositional 3d assets generation with differentiable 3d layout alignment. CoRR, abs/2405.18525, 2024. doi: 10.48550/ARXIV.2405. 18525 9 [16] X. He, J. Chen, S. Peng, D. Huang, Y. Li, X. Huang, C. Yuan, W. Ouyang, and T. He. GVGEN: text-to-3d generation with volumetric representation. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, eds., Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part VIII, vol. 15066 of Lecture Notes in Computer Science, pp. 463479. Springer, 2024. doi: 10.1007/978-3-031-73242-3_26 2 [17] J. Hessel, A. Holtzman, M. Forbes, R. L. Bras, and Y. Choi. Clipscore: reference-free evaluation metric for image captioning. In M. Moens, X. Huang, L. Specia, and S. W. Yih, eds., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 75147528. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.EMNLP-MAIN.595 5 [18] Y. Hong, K. Zhang, J. Gu, S. Bi, Y. Zhou, D. Liu, F. Liu, K. Sunkavalli, T. Bui, and H. Tan. LRM: large reconstruction model for single image to 3d. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1, 2, 3, 4, [19] X. Hou and L. Zhang. Saliency detection: spectral residual approach. In 2007 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2007), 18-23 June 2007, Minneapolis, Minnesota, USA. IEEE Computer Society, 2007. doi: 10.1109/CVPR.2007.383267 4 [20] W. Kam-Kwai, X. Wang, Y. Wang, J. He, R. Zhang, and H. Qu. Anchorage: Visual analysis of satisfaction in customer service videos via anchor events. IEEE Trans. Vis. Comput. Graph., 30(7):40084022, 2024. doi: 10.1109/ TVCG.2023.3245609 2 [21] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W. Lo, P. Dollr, and R. B. Girshick. Segment anything. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 39924003. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00371 4 [22] J. Li, D. Li, C. Xiong, and S. C. H. Hoi. BLIP: bootstrapping languageimage pre-training for unified vision-language understanding and generation. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvri, G. Niu, and S. Sabato, eds., International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, vol. 162 of Proceedings of Machine Learning Research, pp. 1288812900. PMLR, 2022. 2 [23] J. Li, H. Tan, K. Zhang, Z. Xu, F. Luan, Y. Xu, Y. Hong, K. Sunkavalli, G. Shakhnarovich, and S. Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 2 [24] Y. Li, J. Wang, P. Aboagye, C.-C. M. Yeh, Y. Zheng, L. Wang, W. Zhang, and K.-L. Ma. Visual analytics for efficient image exploration and userguided image captioning. IEEE Transactions on Visualization and Computer Graphics, 30(6):28752887, 13 pages, apr 2024. doi: 10.1109/ TVCG.2024.3388514 2 [25] P. P. Liang, Y. Lyu, G. Chhablani, N. Jain, Z. Deng, X. Wang, L. Morency, and R. Salakhutdinov. Multiviz: Towards visualizing and understanding multimodal models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [26] M. Liu, R. Shi, K. Kuang, Y. Zhu, X. Li, S. Han, H. Cai, F. Porikli, and H. Su. Openshape: Scaling up 3d shape representation towards open-world understanding. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds., Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2, 4 [27] M. Liu, C. Xu, H. Jin, L. Chen, M. V. T., Z. Xu, and H. Su. One-2-3-45: 10 To appear in IEEE Transactions on Visualization and Computer Graphics. Any single image to 3d mesh in 45 seconds without per-shape optimization. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds., Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 3 [28] R. Liu, R. Wu, B. V. Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 92649275. IEEE, 2023. doi: 10.1109/ICCV51070.2023.00853 [29] V. Liu and L. B. Chilton. Design guidelines for prompt engineering text-toimage generative models. In S. D. J. Barbosa, C. Lampe, C. Appert, D. A. Shamma, S. M. Drucker, J. R. Williamson, and K. Yatani, eds., CHI 22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, pp. 384:1384:23. ACM, 2022. doi: 10.1145/3491102.3501825 2 [30] V. Liu, J. Vermeulen, G. W. Fitzmaurice, and J. Matejka. 3dall-e: Integrating text-to-image AI in 3d design workflows. In D. Byrne, N. Martelaro, A. Boucher, D. J. Chatting, S. F. Alaoui, S. E. Fox, I. Nicenboim, and C. MacArthur, eds., Proceedings of the 2023 ACM Designing Interactive Systems Conference, DIS 2023, Pittsburgh, PA, USA, July 10-14, 2023, pp. 19551977. ACM, 2023. doi: 10.1145/3563657.3596098 2 [31] X. Long, Y. Guo, C. Lin, Y. Liu, Z. Dou, L. Liu, Y. Ma, S. Zhang, M. Habermann, C. Theobalt, and W. Wang. Wonder3d: Single image to 3d using cross-domain diffusion. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 1622, 2024, pp. 99709980. IEEE, 2024. doi: 10.1109/CVPR52733.2024. 00951 3 [32] J. Marks, B. Andalman, P. A. Beardsley, W. T. Freeman, S. F. Gibson, J. K. Hodgins, T. Kang, B. Mirtich, H. Pfister, W. Ruml, K. Ryall, J. E. Seims, and S. M. Shieber. Design galleries: general approach to setting parameters for computer graphics and animation. In G. S. Owen, T. Whitted, and B. Mones-Hattal, eds., Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH 1997, Los Angeles, CA, USA, August 3-8, 1997, pp. 389400. ACM, 1997. doi: 10. 1145/258734.258887 2 [33] S. R. Midway. Principles of effective data visualization. Patterns, 1(9):100141, 2020. doi: 10.1016/j.patter.2020.100141 [34] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In A. Vedaldi, H. Bischof, T. Brox, and J. Frahm, eds., Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, vol. 12346 of Lecture Notes in Computer Science, pp. 405421. Springer, 2020. doi: 10.1007/978-3-030-58452 -8_24 2, 5 [35] A. Nichol, H. Jun, P. Dhariwal, P. Mishkin, and M. Chen. Point-e: system for generating 3d point clouds from complex prompts. CoRR, abs/2212.08751, 2022. doi: 10.48550/ARXIV.2212.08751 2 [36] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew, I. Sutskever, and M. Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvri, G. Niu, and S. Sabato, eds., International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, vol. 162 of Proceedings of Machine Learning Research, pp. 1678416804. PMLR, 2022. 2 [37] J. Oppenlaender. Prompt engineering for text-based generative art. CoRR, abs/2204.13988, 2022. doi: 10.48550/ARXIV.2204.13988 2 [38] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. 2022. [39] K. Pandey, F. Chevalier, and K. Singh. Juxtaform: interactive visual summarization for exploratory shape design. ACM Trans. Graph., 42(4):52:1 52:14, 2023. doi: 10.1145/3592436 2 [40] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Mller, J. Penna, and R. Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. 2024. 4 [41] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Textto-3d using 2d diffusion. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. 2 G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In M. Meila and T. Zhang, eds., Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol. 139 of Proceedings of Machine Learning Research, pp. 8748 8763. PMLR, 2021. 2 [43] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision. In M. Meila and T. Zhang, eds., Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol. 139 of Proceedings of Machine Learning Research, pp. 8748 8763. PMLR, 2021. 5 [44] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen. Hierarchical text-conditional image generation with CLIP latents. CoRR, abs/2204.06125, 2022. doi: 10.48550/ARXIV.2204.06125 [45] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1067410685. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01042 2 [46] Y. Shi, P. Wang, J. Ye, L. Mai, K. Li, and X. Yang. Mvdream: Multi-view diffusion for 3d generation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 3 [47] T. Shin, Y. Razeghi, R. L. L. IV, E. Wallace, and S. Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In B. Webber, T. Cohn, Y. He, and Y. Liu, eds., Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pp. 42224235. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.EMNLP -MAIN.346 2 [48] K. Son, D. Choi, T. S. Kim, Y. Kim, and J. Kim. Genquery: Supporting expressive visual search with generative models. In F. F. Mueller, P. Kyburz, J. R. Williamson, C. Sas, M. L. Wilson, P. O. T. Dugas, and I. Shklovski, eds., Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI 2024, Honolulu, HI, USA, May 11-16, 2024, pp. 180:1180:19. ACM, 2024. doi: 10.1145/3613904.3642847 2 [49] H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and A. M. Rush. Interactive and visual prompt engineering for ad-hoc task adaptation with large language models. IEEE Trans. Vis. Comput. Graph., 29(1):11461156, 2023. doi: 10.1109/TVCG.2022.3209479 2 [50] A. Swearngin, C. Wang, A. Oleson, J. Fogarty, and A. J. Ko. Scout: Rapid exploration of interface layout alternatives through high-level design constraints. In R. Bernhaupt, F. F. Mueller, D. Verweij, J. Andres, J. McGrenere, A. Cockburn, I. Avellino, A. Goguey, P. Bjn, S. Zhao, B. P. Samson, and R. Kocielnik, eds., CHI 20: CHI Conference on Human Factors in Computing Systems, Honolulu, HI, USA, April 25-30, 2020, pp. 113. ACM, 2020. doi: 10.1145/3313831.3376593 2 [51] J. Tang, Z. Chen, X. Chen, T. Wang, G. Zeng, and Z. Liu. LGM: large multi-view gaussian model for high-resolution 3d content creation. In A. Leonardis, E. Ricci, S. Roth, O. Russakovsky, T. Sattler, and G. Varol, eds., Computer Vision - ECCV 2024 - 18th European Conference, Milan, Italy, September 29-October 4, 2024, Proceedings, Part IV, vol. 15062 of Lecture Notes in Computer Science, pp. 118. Springer, 2024. doi: 10. 1007/978-3-031-73235-5_1 [52] J. Tang, J. Ren, H. Zhou, Z. Liu, and G. Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 1 [53] Z. Tang, S. Gu, C. Wang, T. Zhang, J. Bao, D. Chen, and B. Guo. Volumediffusion: Flexible text-to-3d generation with efficient volumetric encoder. CoRR, abs/2312.11459, 2023. doi: 10.48550/ARXIV.2312.11459 2 [54] G. T. Toussaint. Comments on \"the divergence and bhattacharyya distance measures in signal selection\". IEEE Trans. Commun., 20(3):485, 1972. doi: 10.1109/TCOM.1972.1091157 5 [55] B. Wang. Democratizing content creation and consumption through human-ai copilot systems. In S. Follmer, J. Han, J. Steimle, and N. H. Riche, eds., Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, UIST 2023, San Francisco, CA, USA, 29 October 20231 November 2023, pp. 105:1105:4. ACM, 2023. doi: 10.1145/3586182.3616707 2 [42] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, [56] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 1261912629. IEEE, 2023. doi: 10.1109/CVPR52729.2023.01214 2 [57] X. Wang, J. He, Z. Jin, M. Yang, Y. Wang, and H. Qu. M2lens: Visualizing and explaining multimodal models for sentiment analysis. IEEE Trans. Vis. Comput. Graph., 28(1):802812, 2022. doi: 10.1109/TVCG.2021. 3114794 2 [58] Z. J. Wang, E. Montoya, D. Munechika, H. Yang, B. Hoover, and D. H. Chau. Diffusiondb: large-scale prompt gallery dataset for text-to-image generative models. In A. Rogers, J. L. Boyd-Graber, and N. Okazaki, eds., Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 893911. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.51 3 [59] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai. Promptchainer: Chaining large language model prompts through visual programming. In S. D. J. Barbosa, C. Lampe, C. Appert, and D. A. Shamma, eds., CHI 22: CHI Conference on Human Factors in Computing Systems, New Orleans, LA, USA, 29 April 2022 - 5 May 2022, Extended Abstracts, pp. 359:1359:10. ACM, 2022. doi: 10.1145/3491101.3519729 2 [60] T. Wu, G. Yang, Z. Li, K. Zhang, Z. Liu, L. J. Guibas, D. Lin, and G. Wetzstein. Gpt-4v(ision) is human-aligned evaluator for text-to-3d generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2024, Seattle, WA, USA, June 16-22, 2024, pp. 22227 22238. IEEE, 2024. doi: 10.1109/CVPR52733.2024.02098 3, 5 [61] J. Xia, L. Huang, W. Lin, X. Zhao, J. Wu, Y. Chen, Y. Zhao, and W. Chen. Interactive visual cluster analysis by contrastive dimensionality reduction. IEEE Trans. Vis. Comput. Graph., 29(1):734744, 2023. doi: 10.1109/ TVCG.2022.3209423 [62] L. Yang, C. Xiong, J. K. Wong, A. Wu, and H. Qu. Explaining with examples: Lessons learned from crowdsourced introductory description of information visualizations. IEEE Trans. Vis. Comput. Graph., 29(3):1638 1650, 2023. doi: 10.1109/TVCG.2021.3128157 7 [63] H. Zeng, X. Wang, Y. Wang, A. Wu, T.-C. Pong, and H. Qu. Gesturelens: Visual analysis of gestures in presentation videos. IEEE Transactions on Visualization and Computer Graphics, 2022. doi: 10.1109/TVCG.2022. 3169175 2 [64] H. Zeng, X. Wang, A. Wu, Y. Wang, Q. Li, A. Endert, and H. Qu. Emoco: Visual analysis of emotion coherence in presentation videos. IEEE Transactions on Visualization and Computer Graphics, 26(1):927937, 2019. doi: 10.1109/TVCG.2019.2934656 2 [65] X. Zeng, A. Vahdat, F. Williams, Z. Gojcic, O. Litany, S. Fidler, and K. Kreis. LION: latent point diffusion models for 3d shape generation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds., Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. 2 [66] B. Zhang, Y. Cheng, J. Yang, C. Wang, F. Zhao, Y. Tang, D. Chen, and B. Guo. Gaussiancube: Structuring gaussian splatting using optimal transport for 3d generative modeling. CoRR, abs/2403.19655, 2024. doi: 10.48550/ARXIV.2403.19655 2 [67] Z. Zhao, W. Liu, X. Chen, X. Zeng, R. Wang, P. Cheng, B. Fu, T. Chen, G. Yu, and S. Gao. Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds., Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. 2 [68] H. Zhu, M. Zhu, Y. Feng, D. Cai, Y. Hu, S. Wu, X. Wu, and W. Chen. Visualizing large-scale high-dimensional data via hierarchical embedding of knn graphs. Visual Informatics, 5(2):5159, 2021. doi: 10.1016/j.visinf .2021.06."
        }
    ],
    "affiliations": [
        "East China Normal University"
    ]
}