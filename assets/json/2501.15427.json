{
    "paper_title": "OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas",
    "authors": [
        "Xiaoyang Wang",
        "Hongming Zhang",
        "Tao Ge",
        "Wenhao Yu",
        "Dian Yu",
        "Dong Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and cost-efficiency in developing and deploying role-playing dialogue agents. This study explores a large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release our synthetic characters and instruction-tuning dialogues to support public research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 7 2 4 5 1 . 1 0 5 2 : r OpenCharacter: Training Customizable Role-Playing LLMs with Large-Scale Synthetic Personas Xiaoyang Wang, Hongming Zhang, Tao Ge, Wenhao Yu, Dian Yu, Dong Yu Tencent AI Lab Seattle {shawnxywang,hongmzhang,getao,wenhaowyu,yudian,dyu}@global.tencent.com"
        },
        {
            "title": "Abstract",
            "content": "Customizable role-playing in large language models (LLMs), also known as character generalization, is gaining increasing attention for its versatility and costefficiency in developing and deploying role-playing dialogue agents. This study explores large-scale data synthesis approach to equip LLMs with character generalization capabilities. We begin by synthesizing large-scale character profiles using personas from Persona Hub and then explore two strategies: response rewriting and response generation, to create character-aligned instructional responses. To validate the effectiveness of our synthetic instruction tuning data for character generalization, we perform supervised fine-tuning (SFT) using the LLaMA-3 8B model. Our best-performing model strengthens the original LLaMA-3 8B Instruct model and achieves performance comparable to GPT-4o models on role-playing dialogue. We release1 our synthetic characters and instruction-tuning dialogues to support public research."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) Achiam et al. [2023], Touvron et al. [2023], due to their superior capabilities and versatility across wide range of applications in domains like math and reasoning Hendrycks et al. [2021], Srivastava et al. [2022], Chen et al. [2024b], Rein et al. [2023], programming Chen et al. [2021], natural language virtual assistant and dialogue Hendrycks et al. [2020], Zheng et al. [2023], multi-modal understanding Yue et al. [2024], Liu et al. [2024], agent systems Jimenez et al. [2023], Jing et al. [2024], etc, have become ubiquitous in the recent research. Among these various applications, the role-playing dialogue agent Shao et al. [2023], Wang et al. [2021] has shown certain maturity and great commercial value in diverse tasks such as online customer support, content creation and entertainment, and non-player character (NPC) dialogue in video games, etc. Under different system design philosophies, the role-playing dialogue agent can either be designed as in-domain role-playing, where the LLM is trained to act as specified character (or multiple specified characters), or as out-of-domain role-playing, where the LLM is trained to act as arbitrary usercustomized characters that do not appear in model training. Recent commercial implementations such as Character.ai [2022] and Doubao [2023] have enabled functions of dialogue with user-customized characters. LLM-related research such as Zhou et al. [2023] also studies the topic of dialogue with customizable characters. However, there is limited public data corpus or instruction following model with customizable character capabilities. Furthermore, much of the existing work related to role-playing agents relies on human-annotated or crowd-sourced data. It would struggle to achieve out-of-domain role-playing, which typically requires modeling contrastive data distributions from distinct characters. 1https://huggingface.co/datasets/xywang1/OpenCharacter Inspired by the existing work related to role-playing dialogue agents, our study explores enabling LLMs with out-of-domain role-playing capabilities. We refer to this ability as Character Generalization, which allows LLMs to generalize (adapt) to arbitrary user-customized characters. As discussed above, such capabilities require LLMs to model massive contrastive data from distinct characters, which is impractical to achieve if the model is trained with purely human annotated data. On the other hand, data synthesis is playing more and more important roles in enhancing LLMs capabilities by creating massive high-quality synthetic training data. The recent data synthesis work Persona Hub Ge et al. [2024] provides large-scale diverse synthetic personas that can benefit the modeling of distinct characters. Based upon Persona Hub, we explore to achieve character generalization through large-scale data synthesis utilizing large-scale synthetic characters. From our point of view, character generalization can be achieved when the LLMs are trained with well-curated post-training data containing enough diverse characters with enriched character profiles and high-quality dialogues. We would then need to explore the optimal data synthesis strategy towards character generalization. Specifically, as shown in the overall framework in Figure 1, we start with character profile synthesis using the personas released in Persona Hub to obtain synthetic characters with enriched character profiles. Next, we study two strategies to obtain instruction responses that align with the given character: response rewritten (OpenCharacter-R) and response generation (OpenCharacter-G). OpenCharacter-R rewrites the instruction response from the existing corpus with the given character, whereas OpenCharacter-G directly generates new response aligning with the given character. To demonstrate the effectiveness of our data synthesis approach for character generalization in out-ofdomain role-playing, we conduct supervised fine-tuning (SFT) with LLaMA-3 8B model Dubey et al. [2024] using the synthetic dialogue data. Our best-performing fine-tuned model significantly improves the original LLaMA-3 8B model with the same model size, and is in general comparable (in most cases outperforming) the popular GPT-4o Hurst et al. [2024] models. To further facilitate related research in role-playing dialogue agent, we release 20k synthetic characters and their corresponding 306k role-playing instruction-response dialogue pairs."
        },
        {
            "title": "2 Related Work",
            "content": "There are few recent studies using LLMs for role-playing dialogue. Li et al. [2023] collect in total of 32 characters from TV shows and animations and use extracted dialogues from the corresponding novels or scripts combined with GPT-3 and GPT-4 simulated dialogues for role-playing model training. Tu et al. [2023] create 64 characters for each of the 16 types of Myers-Briggs Type Indicator (MBTI) personalities, leading to 1,024 characters in total. They use two ChatGPT agents acting as the seeker and supporter, respectively, to simulate the role-playing conversation. Wang et al. [2023b] construct 100 roles and use GPT-synthesized role-based QA pairs for role-playing fine-tuning. Lu et al. [2024] collect 4k characters from Wikipedia and generate self-simulated role-playing dataset. Different from these studies, our work is built on large-scale synthetic characters originating from synthetic personas in Persona Hub and can easily scale up to significantly larger numbers. Character generalization is an important aspect of role-playing dialogue. Zhou et al. [2023] study the problem of customizable characters for dialogue with LLMs and the study is related to our work. However, their approach builds the corpus in Chinese through crowdsourcing, and releases portion of the corpus comprising 1,034 dialogue sessions from 250 characters. Comparatively, our approach embraces the LLM-synthetic characters and dialogue sessions. We experiment with over 100k dialogue sessions from approximately 20k characters, and the character number can increase by 50k times if necessary. In our practice, we find large-scale diverse characters are helpful towards character generalization. In addition, different from the generalization of persona or character-driven role-playing dialogue, Li et al. [2024b] study LLM-based style generalization with evaluation styles including humor, poems, romance, sci-fi, etc. The evaluation of persona or character-driven role-playing dialogue also draws attention in multiple related studies Wang et al. [2024], Tu et al. [2024], Chen et al. [2024a], Qin et al. [2024], Xu et al. [2024], Samuel et al. [2024]. Among these studies, we choose PersonaGym for our evaluation benchmark considering it evaluates the out-of-domain persona-driven role-playing dialogue capability with multiple evaluation metrics including expected action, toxicity control, linguistic habits, persona consistency, and action justification. 2 Data synthesis usually refers to generating data by models or algorithms instead of directly by humans. LLMs are widely used to produce synthetic data by specifying data synthesis prompt. These LLM-based data synthesis methods can generally be categorized into three types: instancedriven Wang et al. [2023a], Yu et al. [2023], key-point-driven Li et al. [2024a], Huang et al. [2024], and persona-driven Ge et al. [2024]. Among them, the persona-driven data synthesis in Persona Hub Ge et al. [2024] can easily scale-up with diverse instructions generated through prompting with large-scale (e.g., billions of) synthetic personas."
        },
        {
            "title": "3 Approach",
            "content": "3.1 Problem Definition In this work, we study the problem of character-based role-playing dialogue. Specifically, we focus on the dialogue between user and the LLM back-boned agent with character profile. The characterbased dialogue can then be defined as: given user specified agent character profile and the dialogue input x, model θ (e.g., LLMs) predicts the response by acting as the character C: = arg max (yx, C; θ) (1) In-domain role-playing. Intuitively, the model θ can be learned through post-training (e.g., supervised fine-tuning (SFT)) with character-based role-playing dialogue data consisting of manuallylabeled or synthetic dialogue samples between the user and the requested specific character Cs. The characters dialogue responses in well-curated SFT dataset would reflect the characters language style, experience, and personality, which LLMs can learn through SFT. The SFT learning objective for in-domain role-playing can be written as: θs = arg max θ (cid:88) (xi,yi) log (yixi, Cs; θ) (2) During inference, the learned model θs directly responds to the users new requests by acting as the character Cs. Out-of-domain role-playing. Different from in-domain role-playing, out-of-domain role-playing requires the model θ to act as new characters Cx that do not appear in model training. It fits the practical scenarios where users create or customize fictional characters for LLMs to respond online without re-training. To achieve out-of-domain role-playing, the learned model θg needs to generalize to new characters (e.g., Cx) on the fly according to the detailed character profile in Cx. We speculate that the LLMs nowadays can achieve character generalization when trained with well-curated post-training data containing enough diverse characters with enriched character profiles and high quality dialogues. Given set of training samples each with the character profile Ci, dialogue input xi, and character response yi, our SFT training objective for out-of-domain role-playing with character generalization capability is: θg = arg max θ (cid:88) (xi,yi,Ci) log (yixi, Ci; θ) (3) The quality and quantity of training set (xi, yi, Ci)N i=1 plays important roles on deciding how the learned model θg can generalize on the fly to different unseen characters. Intuitively, model trained with larger number of diverse characters would be stronger for out-of-domain role-playing problems. Thus, our approach focuses on curating large-scale training set in the next. 3.2 Character Generalization with Data Synthesis Recently, the work Persona Hub Ge et al. [2024] studies to scale synthetic data creation with largescale (e.g., 1 billion) personas in Persona Hub. It is motivated by the observation that including persona in the data synthesis prompt can steer the LLM to generate more distinctive synthetic data. 3 Figure 1: Our overall data synthesis approach. As an example, we start with character profile synthesis using persona from Persona Hub, and then explore character-driven response rewriting and generation. For our character generalization purpose in this work, we wish to create out-of-domain role-playing SFT training data that contains diverse characters and high-quality dialogue sessions between users and characters on large scale. Persona Hub currently releases 200,000 distinct synthetic personas, upon which we can create large-scale character library and synthesize dialogues between users and these characters. Our data synthesis framework is shown in Figure 1. We start with character profile synthesis using synthetic personas as input. Compared to the original personas in Persona Hub, these synthetic character profiles contain fine-grained synthetic knowledge of the character. With the synthetic character profiles, we explore two different dialogue synthesis strategies: character-driven response rewriting and character-driven response generation. 3.2.1 Character Profile Synthesis The personas in Persona Hub are typically one-sentence short descriptions about person briefing his or her professional expertise and interests, as shown in Figure 1. It lacks fine-grained knowledge about the persons identity, experience, personality, etc. Based on these personas, it is necessary to further create synthetic characters with character profile detailing the characters fine-grained knowledge considering: 1. Such knowledge better describes character and provides more information for the model to ground on for role-playing. 2. The model should be able to utilize fine-grained character information on the fly if the user provides more details. Based on the above interpolation, our approach starts with prompting LLMs to create synthetic character profile with persona input by imagining the following details of the provided persona: name, age, gender, race, birth place, appearance, general experience, and personality. Our prompt for character synthesis with Persona Hub is shown in Figure 3. Our approach differs from the existing approaches Tu et al. [2023], Lu et al. [2024] that collect characters by extracting information from knowledge bases like Wikipedia. By building largescale library consisting of synthetic characters, our system is not bound by the upper limit of possible number of characters currently existing either in the real or fictional human world. Moreover, characters in knowledge bases are typically famous or with importance, and building character library on knowledge bases would bring distributional bias to the character library. We explore enabling LLMs out-of-domain role-playing capability utilizing the synthetic character library. With such capability, the model can theoretically perform well on arbitrarily specified characters no matter they exist in the actual world or not. 3.2.2 Character-driven Response Rewriting (OpenCharacter-R) Since the introduction of ChatGPT OpenAI [2022], LLMs dialogue capability largely coincides with its instruction following capability. An LLM-based assistant that can follow users single-turn Figure 2: An example of our response synthesis. For users question from LIMA, we show the abbreviated version of responses that are rewritten and generated respectively through our approach illustrated in Figure 1. Both responses align with the given character, though the rewritten response largely keeps the knowledge details of the original response while the generated response does not typically contain the exact details. Prompt for Character Profile Synthesis You are helpful assistant. will provide you with short persona description. Your task is to create character based on the given persona. You can output brief character description containing the following information: character name, age, gender, race, birth place, appearance, general experience, and personality. Note: 1. Your response should start with Name:. 2. Your character description should be specific and consistent with the persona. {persona} Figure 3: Our data synthesis prompt for character profile synthesis to generate the enriched character profile with persona from Persona Hub as the input. or multi-turn instruction and respond being both helpful and harmless to the users request would naturally serve as the dialogue agent of virtual assistant. Consequently, for character-based dialogue, our focus becomes enabling LLMs instruction following capability as the dialogue agent of specified character. Nowadays, plenty of high-quality instruction tuning corpora such as LIMA Zhou et al. [2024] and Alpaca Taori et al. [2023] are widely used. These corpora typically provide carefully curated user instruction and manually labeled or LLM-generated assistant response y. To enable role-playing while maintaining LLMs instruction following capability with skills for different domains such as question answering, math and reasoning, etc., we propose the characterdriven response rewriting method, in which we keep the instructions from the public instruction tuning datasets, but rewrite the original response into yC that addresses the users request in (x, yC). In such compliance with the style and background of the character as (x, y) rewrite response manner, our method can theoretically utilize most of the existing instruction tuning, i.e., SFT data in Prompt for Character-driven Response Rewriting (OpenCharacter-R) You are helpful assistant. will provide you with short Persona Description of new character, more detailed Character Specification of the new character, and session of Dialogue between user and an assistant. You are asked to rewrite the assistants response to the user by imagining how the new character would respond to the same user. Note: 1. Do not change the users sentences. 2. The rewritten response should align with the new characters language style, experience, and personality. Please return the rewritten dialogue session in the following JSON format: ```json [{\"role\": \"user\", \"content\": \"user's sentence\"}, {\"role\": \"assistant\", \"content\": \"assistance's sentence\"}] ``` # Persona Description {persona} # Character Specification {character profile} # Dialog ## user {users sentence} ## assistant {assistants sentence} ... Prompt for Character-driven Response Generation (OpenCharacter-G) You are helpful assistant. will provide you with the Persona Description and the Character Specification of character, together with Users Query. You need to imagine how the provided character would address the Users Query according to the characters language style, experience, and personality. Please directly return the characters response to the Users Query. # Persona Description {persona} # Character Specification {character profile} # Users Query {instruction} Figure 4: Our data synthesis prompts for character-driven response rewriting (OpenCharacter-R) and characterdriven response generation (OpenCharacter-G), respectively. other words, for various types of LLM tasks to enable the out-of-domain role-playing capability but keep the LLMs skills in different domains. Figure 4 shows our prompt for rewriting the instruction-response dialogue in the perspective of given character C. We specify the model to keep the users utterances, and rewrite only the assistances utterances by aligning with the new characters language style, experience, and personality. The prompt will guide LLMs to output the rewritten dialogue in multi-turn JSON format. 3.2.3 Character-driven Response Generation (OpenCharacter-G) Besides rewriting responses of existing instruction tuning corpus, we further look into the direct generation of character-compliance dialogue response from users instruction. Persona Hub releases set of 50,000 high-quality synthetic complex instructions without responses. We call this set PH-Instruct for simplicity. Besides Ph-Instruct, we further explore the direct generation of characterdrive responses for existing instruction-tuning corpora such as LIMA and Alpaca. Compared to the OpenCharacter-R strategy discussed in Section 3.2.2, OpenCharacter-G could potentially synthesize new responses better than the original ones with the help of more recent and advanced LLM. Moreover, as compared in Figure 1, OpenCharacter-G could bring in different knowledge or perspectives to address the instruction, while OpenCharacter-R would largely keep the original contents but with different personality. We prompt LLMs to generate the response yC to the instruction according to the given character yC. Figure 4 shows our prompt for generating character-compliance profile as generate response dialogue response given the users instruction. Different from OpenCharacter-R that rewrites the whole multi-turn dialogue session through one-time prompting, OpenCharacter-G uses the turn-byturn manner. 3.3 Supervised Fine-tuning We conduct supervised fine-tuning (SFT) as in Equation 3 with the synthetic instruction-response pairs from either character-driven response rewriting or character-driven response generation. For each dialogue session, we randomly pick synthetic characters C1, C2, ..., Cn from the synthetic character pool containing synthetic character profiles, and synthesize instruction-response pairs as (x, yC1), (x, yC2 ), ..., (x, yCn ) by rewriting or generation. Our approach thus mixes all these synthetic instruction-response pairs from different characters for supervised fine-tuning. Specifically, we use = 20, 000 and = 3 in our implementation."
        },
        {
            "title": "4 Evaluation",
            "content": "Evaluating LLMs capabilities on role-playing is non-trivial task. It requires to assess many different aspects, including but not limited to the following: language style consistency with the persona or character, knowledge/character background consistency with the persona or character, dialogue semantic coherency with the dialogue context, as well as the response helpfulness and harmlessness related measurements, etc. 4.1 Evaluation with PersonaGym In this work, to synchronize with the open-source community for role playing evaluation, we choose to use PersonaGym Samuel et al. [2024], which is recently released evaluation framework and benchmark for assessing persona agents, consisting of 200 personas and 10k questions for testing. It measures the persona dialogue response quality through five different metrics, including expected action, toxicity control, linguistic habits, persona consistency, and action justification, each with score from 1 to 5. 4.2 Evaluation with PersonaGym-Light To further accelerate the development process and decrease the prompting cost, we shrink the original PersonaGym testing data by 1/10 by keeping only the first of the ten questions in each of the five metrics for all of the 200 personas. We call this benchmark PersonaGym-Light, which consists of 200 personas and 1k questions for testing. PersonaGym-Light adopts the original personas and test scenarios from PersonaGym, and uses part of the testing data for faster and cheaper model development iteration. By keeping all its 200 testing personas and five metrics, it largely keeps the capability of PersonaGym in evaluating LLMs with diverse persona agents. 4.3 Evaluation Model and Overall Score In the original PersonaGym implementation Samuel et al. [2024], both GPT-4o Hurst et al. [2024] and LLaMA-3-70B-Instruct Dubey et al. [2024] models are used as evaluators. However, in our practice, we find that LLaMA-3-70B-Instruct suffers significantly from its maximum 8k context length. The combined length of the evaluation prompt and output from PersonaGym constantly exceeds LLaMA3s context length limit. Comparatively, GPT-4o allows 128k context window that is sufficient for PersonaGym evaluation. As result, in this work, we use GPT-4o (i.e., gpt-4o-2024-08-06 in this paper) as the only evaluator for performance evaluation. For simplicity, we denote our overall evaluation score on PersonaGym-Light as PScore-L, and the overall evaluation score on PersonaGym 7 as PScore. Both scores take the numeric average of the 1 to 5 scores of the five metrics defined in PersonaGym."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Data Synthesis As discussed in Figure 1, our data synthesis framework first conducts character profile synthesis using synthetic personas from Persona Hub and then conducts character-driven response synthesis. 5.1.1 Character Synthesis We generate approximately 20k synthetic character profiles in English using GPT-4o (gpt-4o-202405-13). Each character profile includes synthetic information about the characters name, age, gender, race, birth place, appearance, general experience, and personality. An abbreviated example of the synthetic character profile is shown in Figure 1. These synthetic character profiles are publicly released. 5.1.2 Response Synthesis With the synthetic character profiles, we conduct response synthesis with two types of strategies: character-driven response rewriting (OpenCharacter-R) and character-driven response generation (OpenCharacter-G), as discussed in Section 3.2.2 and Section 3.2.3, respectively. Both methods require the users instruction input x, which we obtain directly from popular instruction tuning corpus including LIMA Zhou et al. [2024], Alpaca Taori et al. [2023], and Persona Hub Instruction (PHInstruct) Ge et al. [2024]. LIMA and Alpaca contain responses obtained through human annotation and ChatGPT generation, respectively, and we explore both OpenCharacter-R and OpenCharacter-G strategies for these two corpora. Figure 2 shows an example of question from LIMA with our rewritten and generated responses. On the other hand, we use only OpenCharacter-G strategy for PH-Instruct since no original responses are provided. Brief statistics on these instruction datasets and data synthesis details are provided in Table 1. Table 1: Statistics on our character-driven response synthesis. is the number of characters randomly assigned to each dialogue session. We apply only OpenCharacter-G to PH-Instruct since its responses are not released. Corpus No. of questions No. of characters OpenCharacter-R OpenCharacter-G LIMA Alpaca PH-Instruct Total 1,074 51,010 50,000 102,084 3 3 3 3 2,986 19,991 19,990 19, - As discussed in Section 3.3, we denote as the number of characters randomly assigned to each dialogue session and set = 3. Both GPT-4o (gpt-4o-2024-05-13) and LLaMA-3-70B-Instruct models are used for response synthesis of both strategies. 5.2 Supervised Fine-tuning (SFT) To achieve character generalization, we conduct SFT with the instruction and synthetic character response pairs from large-scale diverse characters. Implementation details are discussed below. 5.2.1 Data Recipes To analyze the effectiveness of different prompting strategies and the performances of prompting models (i.e., GPT-4o and LLaMA-3-70B-Instruct) for character-driven response synthesis, we experiment with six different data recipes as indicated in Table 2. Note that we always combine LIMA and Alpaca instructions considering they are standard nowadays in instruction tuning research. 8 Table 2: The SFT data recipes for OpenCharacter ablation study. For our final OpenCharacter model, we combine all instructions from PH-Instruct, LIMA and Alpaca, and use OpenCharacter-G strategy. Ablation Settings Corpus Strategy Prompting Model Ablation-1 Ablation-2 Ablation-3 Ablation-4 Ablation-5 OpenCharacter LIMA & Alpaca LIMA & Alpaca PH-Instruct PH-Instruct LIMA & Alpaca OpenCharacter-R OpenCharacter-R OpenCharacter-G OpenCharacter-G LLaMA-3-70B-Instruct OpenCharacter-G LLaMA-3-70B-Instruct PH-Instruct, LIMA & Alpaca OpenCharacter-G LLaMA-3-70B-Instruct gpt-4o-2024-05-13 LLaMA-3-70B-Instruct gpt-4o-2024-05-"
        },
        {
            "title": "Model System Prompt",
            "content": "You are an AI character with the following Persona and Character Profile. # Persona {persona} # Character Profile {character profile} Please stay in your character and keep in compliance with the above Persona and Character Profile. Be helpful and harmless to the users requests. Model System Prompt w/o Character Profile You are an AI character with the following Persona. # Persona {persona} Please stay in your character and keep in compliance with the above Persona and Character Profile. Be helpful and harmless to the users requests. Figure 5: Our models system prompts to incorporate the user-specified persona and character profile. We further remove the character profile-related content for test scenarios without character profiles (e.g., PersonaGym). With the recipes indicated in Table 2, we can conduct various ablation studies, including comparing the strategies of OpenCharacter-R and OpenCharacter-G, comparing the prompting models, and comparing different instruction corpus. Moreover, we aim to find recipe that works best for the task. 5.2.2 Model System Prompt By design, our model incorporates the user-specified persona and character profile in its system prompt for role-playing and character generalization. To enable the trained model with such capabilities, we formulate the system prompt in our fine-tuning data to include the corresponding personas and character profiles. The system prompt for our SFT model is shown in Figure 5. 5.2.3 Training Setting For all data recipes, we conduct SFT by using either LLaMA-3-8B-Base or LLaMA-3-8B-Instruct as our backbone model. The model is trained through Megatron-LM, where the tensor parallel size equals 8. We add loss masks to system prompts and instruction tokens to ensure only response tokens are considered in training loss computation. We use Adam Optimizer with beta values of (0.9, 0.95). Linear learning rate decay is chosen with the maximum learning rate to be 1e5, and the minimum learning rate to be 1e6. 5.3 Existing Models Besides our trained models with different data recipes, we further compare with the most popular instruction-tuned LLMs including LLaMA-3-8B-Instruct, LLaMA-3-70B-Instruct, GPT-3.5 (gpt3.5-turbo-1106), GPT-4o-mini (gpt-4o-mini-2024-07-18), and GPT-4o (gpt-4o-2024-05-13 9 and gpt-4o-2024-08-06) for the generalizable character role-playing task. We use the PeronsaGym prompting setting to evaluate these models, and compare their performances with our trained models. 5.4 Performances We compare the performances of our models with those of existing publicly available models on PersonaGym-Light and PersonaGym benchmarks, respectively. We use PersonaGym-Light besides PersonaGym due to its lower cost and shorter time for large-scale evaluation and ablation study. 5.4.1 Results on PersonaGym-Light We first evaluate our OpenCharacter model that is trained based on LLaMA-3-8B-Instruct using instructions from PH-Instruct, LIMA, and Alpaca and responses synthesized through OpenCharacterG. We compare it with different existing models listed in Section 5.3. Their performances on PersonaGym-Light are shown in Table 3. From this comparison, we observe OpenCharacter model improves over LLaMA-3 8B Instruct model, and outperforms different GPT-3.5 and GPT-4o models. Table 3: Model performances on PersonaGym-Light. EA, TC, LH, PC, and AJ stands for the evaluation metrics expected action, toxicity control, linguistic habits, persona consistency, and action justification, respectively. Their standard deviations over 200 personas are included in parentheses. The tested gpt-4o-mini is in version gpt-4o-mini-2024-07-18. We test the LLaMA-3 8B and 70B models with versions LLaMA-3-8B-Instruct and LLaMA-3-70B-Instruct, respectively. Our OpenCharacter model is trained based on LLaMA-3-8B-Instruct with its training data recipe indicated in Table 2. Model gpt-3.5-turbo-1106 gpt-4o-2024-05-13 gpt-4o-mini gpt-4o-2024-08-06 LLaMA-3 Instruct LLaMA-3 Instruct OpenCharacter Size - - - - 8B 70B 8B EA TC LH PC AJ 4.67 (.50) 4.74 (.44) 4.74 (.44) 4.81 (.40) 4.80 (.40) 4.73 (.45) 4.70 (.53) 4.99 (.21) 4.96 (.33) 4.99 (.21) 4.95 (.46) 4.76 (.82) 4.75 (.85) 4.92 (.50) 3.12 (.60) 3.69 (.82) 3.58 (.80) 3.75 (.81) 4.05 (.71) 4.38 (.59) 4.32 (.60) 4.42 (.58) 4.75 (.54) 4.72 (.45) 4.68 (.47) 4.64 (.52) 4.79 (.41) 4.54 (.56) 4.37 (.57) 4.87 (.34) 4.89 (.40) 4.85 (.45) 4.85 (.38) 4.97 (.18) 4.85 (.38) PScore-L 4.31 (.24) 4.60 (.24) 4.58 (.23) 4.60 (.24) 4.62 (.25) 4.72 (.24) 4.66 (.27) We further conduct comprehensive ablation study on different data recipes discussed in Table 2. The ablation study is on PersonaGym-Light due to our budget limit on OpenAI API calls. Table 4 gives the model performances under difference ablation settings. Through Table 4, we can conduct multiple ablation analysis. First and foremost, we can observe that for each data recipe, the models trained on LLaMA-3-8B-Instruct constantly outperform the corresponding models trained on LLaMA-3-8BBase in the PScore-L metric, indicating comprehensive first-stage instruction tuning in the general domain enhances the models capabilities on role-playing tasks. Besides backbone models, we further analysis factors including prompting models, prompting strategies, and instruction corpus. Prompting Models. If we analyze PScore-L of ablation settings Ablation-1 vs. Ablation-2 or Ablation3 vs. Abation-4 in Table 4, we can find that data synthesis by prompting with LLaMA-3-70B-Instruct achieves better model performance than prompting with gpt-4o-2024-05-13 in three of the four comparison pairs. This conclusion coincides with the observation from Table 3 that LLaMA-3-70BInstruct outperforms gpt-4o-2024-05-13 in PScore-L, as well as the similar observation from Table 5 in PScore. Prompting Strategies. We can further compare the performances of ablation settings Ablation-1 vs. Ablation-3 or Ablation-2 vs. Ablation-5 in Table 4 to analyze OpenCharacter-R vs. OpenCharacter-G. To our surprise, OpenCharacter-G significantly outperforms OpenCharacter-R in all scenarios of our ablation study with PScore-L. We interpret that both LIMA and Alpaca are early instruction tuning corpus with less advanced ground-truth response quality, and rewriting their original responses could result in further degraded training responses that lead to worse model performance. However, we regard the OpenCharacter-R strategy as an optional method for applications (e.g., character role-playing in novels or games) where the knowledge from the original responses should be strictly obeyed. In such applications, direct response generation with LLMs from the realistic world could bring in knowledge or facts that are no longer realistic in the virtual or domain-specific world, potentially causing knowledge hallucination issues for the role-playing model. 10 Table 4: Ablation study on PersonaGym-Light, with the training data recipe for these models indicated in Table 2. EA, TC, LH, PC, and AJ stands for the evaluation metrics expected action, toxicity control, linguistic habits, persona consistency, and action justification, respectively. Their standard deviations over 200 personas are included in parentheses. Ablation Settings Size EA TC LH PC AJ PScore-L 1. Models trained based on LLaMA-3-8B-Base 8B Ablation-1 8B Ablation-2 Ablation-3 8B 8B Ablation-4 8B Ablation-5 OpenCharacter 8B 4.45 (.70) 4.26 (.79) 4.71 (.52) 4.69 (.53) 4.74 (.46) 4.70 (.49) 4.94 (.54) 4.88 (.52) 4.98 (.22) 4.86 (.61) 4.86 (.66) 4.90 (.58) 2. Models trained based on LLaMA-3-8B-Instruct Ablation-1 Ablation-2 Ablation-3 Ablation-4 Ablation-5 OpenCharacter 4.53 (.60) 4.46 (.70) 4.74 (.45) 4.71 (.48) 4.72 (.49) 4.70 (.53) 4.96 (.30) 4.92 (.46) 4.97 (.30) 4.88 (.59) 4.93 (.47) 4.92 (.50) 8B 8B 8B 8B 8B 8B 3.58 (.79) 3.81 (.84) 3.68 (.74) 4.18 (.60) 4.17 (.65) 4.16 (.65) 3.77 (.80) 4.05 (.80) 3.89 (.71) 4.31 (.62) 4.28 (.64) 4.32 (.60) 4.16 (.65) 4.12 (.59) 4.52 (.63) 4.52 (.52) 4.52 (.51) 4.50 (.57) 4.22 (.57) 4.21 (.55) 4.59 (.49) 4.51 (.58) 4.54 (.51) 4.54 (.56) 4.04 (.64) 3.98 (.57) 4.79 (.49) 4.84 (.37) 4.89 (.33) 4.80 (.40) 4.26 (.64) 4.22 (.52) 4.80 (.51) 4.86 (.35) 4.86 (.36) 4.85 (.38) 4.23 (.31) 4.21 (.32) 4.53 (.26) 4.62 (.24) 4.64 (.27) 4.61 (.25) 4.35 (.30) 4.37 (.30) 4.60 (.26) 4.65 (.23) 4.66 (.25) 4.66 (.27) Instruction Corpus. By comparing PScore-L performances of Ablation-4 vs. Ablation-5 settings in Table 4, we can find that instructions from the combination of LIMA and Alpaca are slightly more effective than PH-Instruct. On the other hand, both instruction sets are very effective in enabling LLMs with role-playing capabilities, with both Abaltion-4 and Ablation-5 models trained on LLaMA-3-8B-Base outperforming the GPT-4o models. Furthermore, our Ablation-5 model trained on LLaMA-3-8B-Base outperforms the LLaMA-3 8B Instruct model in PScore-L. 5.4.2 Results on PersonaGym We further pick our best-performing model settings including Ablation-5 and OpenCharacter, and compare their performances with those of the existing models on the full-scale PersonaGym benchmark. The results are listed in Table 5. Table 5: Model performances on PersonaGym. EA, TC, LH, PC, and AJ stands for the evaluation metrics expected action, toxicity control, linguistic habits, persona consistency, and action justification, respectively. Their standard deviations over 200 personas are included in parentheses. The tested gpt-4o-mini is in version gpt-4o-mini-2024-07-18. We test the LLaMA-3 8B and 70B models with versions LLaMA3-8B-Instruct and LLaMA-3-70B-Instruct, respectively. We include the performances of our Ablation-5 and OpenCharacter models with their training data recipe indicated in Table 2. Model gpt-4o-2024-05-13 gpt-4o-mini gpt-4o-2024-08-06 LLaMA-3 Instruct LLaMA-3 Instruct Size - - - 8B 70B EA TC LH PC AJ PScore 4.59 (.24) 4.56 (.19) 4.55 (.20) 4.52 (.21) 4.59 (.16) 4.97 (.17) 4.97 (.21) 4.97 (.19) 4.58 (.60) 4.59 (.62) 3.48 (.53) 3.70 (.49) 3.80 (.47) 4.05 (.36) 4.33 (.27) 4.75 (.17) 4.67 (.25) 4.72 (.23) 4.54 (.20) 4.72 (.18) 4.61 (.17) 4.64 (.15) 4.64 (.15) 4.57 (.15) 4.64 (.13) 4.48 (.13) 4.51 (.14) 4.53 (.12) 4.45 (.14) 4.58 (.14) 1. OpenCharacter models trained based on LLaMA-3-8B-Base Ablation-5 OpenCharacter 4.78 (.45) 4.69 (.53) 4.45 (.24) 4.42 (.27) 4.19 (.28) 4.20 (.29) 8B 8B 2. OpenCharacter models trained based on LLaMA-3-8B-Instruct 4.27 (.29) Ablation-5 OpenCharacter 4.27 (.25) 4.45 (.24) 4.47 (.24) 4.81 (.43) 4.78 (.47) 8B 8B 4.44 (.27) 4.45 (.21) 4.55 (.14) 4.53 (.14) 4.48 (.14) 4.46 (.15) 4.44 (.28) 4.51 (.20) 4.56 (.13) 4.58 (.12) 4.50 (.15) 4.52 (.13) From the results in Table 5, we can see both settings Ablation-5 and OpenCharacter, when fine-tuned with LLaMA-3 8B Base model, outperform the LLaMA-3 8B Instruct model on the full PersonaGym benchmark. Moreover, our OpenCharacter fine-tuned with LLaMA-3 8B Instruct model outperforms both gpt-4o-2024-05-13 and gpt-4o-mini models, and performs only slightly worse than the gpt-4o2024-08-06 model. These observations largely coincides with our observations on PersonaGym-Light 11 in Table 3, indicating using lighter version of PersonaGym benchmark would still be feasible for model development with faster evaluation at lower cost. 5.5 Discussions Our performance analysis in Section 5.4 indicates the following aspects we can further discuss: Character Generalization. With large enough diverse synthetic characters and their corresponding instruction responses, we can confidently enable LLMs with character generalization capability. On the other hand, it could still be tricky to combine more instructions for better benchmark performance. Ideally, the more user instructions we add to post-training, the better the model should generalize to different LLM characters and user requests. However, our results in Table 4 and Table 5 indicate our OpenCharacter data that includes the largest number of instructions, though performing the best when using the LLaMA-3-8B-Instruct model as the backbone, does not perform as well as the Ablation-5 setting when both using the LLaMA-3-8B-Base model as the backbone. We speculate this is due to the complexity of the instructions in the PH-Instruct corpus compared to those in the LIMA and Alpaca corpus. It requires stronger instruction-following backbone model to learn better on more complex instruction set. Size of Backbone Model. As we can see, the LLaMA-3-70B-Instruct model, which is significantly larger than the 8B models fine-tuned by us, performs the best in our evaluations. It indicates that model size matters in character generalization. We speculate that larger models, with their higher model capacity and stronger reasoning capability, would be more powerful in their nature for character generalization. On the other hand, with our proposed method, we can still empower the relatively smaller models with excellent character generalization performance. Knowledge of the World. Current studies on LLM role-playing and character generalization focus largely on the domain of our actual world. However, it would be more and more interesting to study the topic in the settings of the virtual world or the domain-specific world. As discussed previously in Section 5.4, such settings require the role-playing responses to obey strict rules defined by the knowledge scope of the character, the facts of the world, as well as the domain requirements from the detailed applications. We expect our proposed OpenCharacter-R strategy to be specifically helpful in such scenarios. Moreover, we believe new benchmark for persona or character-based role-playing in the virtual world would be necessary for future studies in this direction."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we study the problem of data synthesis for role-playing dialogue agents with generalizable characters. Based on synthetic personas from Persona Hub, we first synthesize character profiles with enriched information such as the characters experience and personality. Then, these synthetic characters are used to either rewrite or generate responses to the users instructions from the existing instruction tuning corpus. We utilize these synthetic role-playing instruction tuning samples to fine-tune LLaMA-3 8B models and demonstrate the effectiveness of our synthetic data for out-of-domain persona-based role-playing dialogue. Our observations in this research indicate few possible future directions. As we prove in this work, character generalization could be achieved with LLMs trained in large-scale character-aligned dialogue. practical future work could be further increasing the size of character profiles with their corresponding dialogues and conducting post-training with larger and stronger LLM backbone model. Also, further studies are needed to improve the handling of knowledge from the virtual world and the virtual characters considering most of the current LLMs are trained with knowledge from the actual world."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Character.ai. https://character.ai/, 2022. 12 Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Gao Xing, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, and Fei Huang. SocialBench: Sociality evaluation of role-playing conversational agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 21082126, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.125. URL https://aclanthology.org/2024.findings-acl.125/. Jiaao Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context: Unlocking compositionality in large language models. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1383813890, Miami, Florida, USA, November 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-emnlp.812. URL https://aclanthology.org/2024.findings-emnlp.812/. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Doubao. https://www.doubao.com/chat/, 2023. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tao Ge, Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. Scaling synthetic data creation with 1,000,000,000 personas. arXiv preprint arXiv:2406.20094, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning. arXiv preprint arXiv:2403.02333, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. Liqiang Jing, Zhehui Huang, Xiaoyang Wang, Wenlin Yao, Wenhao Yu, Kaixin Ma, Hongming Zhang, Xinya Du, and Dong Yu. Dsbench: How far are data science agents to becoming data science experts? arXiv preprint arXiv:2409.07703, 2024. Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597, 2023. Haoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun Wang, Xingxing Zhang, Haoyang Huang, Shaohan Huang, Xiaolong Huang, Zeqiang Huang, Dongdong Zhang, et al. Synthetic data (almost) from scratch: Generalized instruction tuning for language models. arXiv preprint arXiv:2402.13064, 2024a. Jinpeng Li, Zekai Zhang, Quan Tu, Xin Cheng, Dongyan Zhao, and Rui Yan. Stylechat: Learning recitation-augmented memory in llms for stylized dialogue generation. arXiv preprint arXiv:2403.11439, 2024b. 13 Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: Advancing multimodal chart understanding with large-scale instruction tuning. In Kevin Duh, Helena Gomez, and Steven Bethard, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 12871310, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.naacl-long.70. URL https://aclanthology.org/2024.naacl-long.70/. Keming Lu, Bowen Yu, Chang Zhou, and Jingren Zhou. Large language models are superpositions of all characters: Attaining arbitrary role-play via self-alignment. arXiv preprint arXiv:2401.12474, 2024. OpenAI. Chatgpt. https://openai.com/index/chatgpt/, 2022. Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. InFoBench: Evaluating instruction following ability in large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1302513048, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.772. URL https://aclanthology.org/2024.findings-acl.772/. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024. Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. Character-LLM: trainable agent In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of for role-playing. the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13153 13187, Singapore, December 2023. Association for Computational Linguistics. URL https: //aclanthology.org/2023.emnlp-main.814/. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Quan Tu, Chuanqi Chen, Jinpeng Li, Yanran Li, Shuo Shang, Dongyan Zhao, Ran Wang, and Rui Yan. Characterchat: Learning towards conversational ai with personalized social support. arXiv preprint arXiv:2308.10278, 2023. Quan Tu, Shilong Fan, Zihang Tian, Tianhao Shen, Shuo Shang, Xin Gao, and Rui Yan. CharacterEval: Chinese benchmark for role-playing conversational agent evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1183611850, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.638. URL https://aclanthology.org/2024.acl-long.638/. Xiaoyang Wang, Chen Li, Jianqiao Zhao, and Dong Yu. Naturalconv: chinese dialogue dataset towards multi-turn topic-driven conversation. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021. 14 Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. InCharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18401873, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.102. URL https://aclanthology.org/2024.acl-long.102/. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.754. URL https://aclanthology.org/2023.acl-long.754/. Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et al. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746, 2023b. Rui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, and Yanghua Xiao. Character is destiny: Can large language models simulate persona-driven decisions in role-playing? arXiv preprint arXiv:2404.12138, 2024. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36, 2024. Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832, 2023."
        }
    ],
    "affiliations": [
        "Tencent AI Lab"
    ]
}