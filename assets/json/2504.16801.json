{
    "paper_title": "Decoupled Global-Local Alignment for Improving Compositional Understanding",
    "authors": [
        "Xiaoxing Hu",
        "Kaicheng Yang",
        "Jun Wang",
        "Haoran Xu",
        "Ziyong Feng",
        "Yupei Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIP's ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the model's inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce a Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the model's inherent capabilities, we incorporate a self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with a frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous state-of-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 1 0 8 6 1 . 4 0 5 2 : r Decoupled Global-Local Alignment for Improving Compositional Understanding Xiaoxing Hu xiaoxinghhh@gmail.com Beijing Institute of Technology Beijing, China Kaicheng Yang kaichengyang@deepglint.com DeepGlint Beijing, China Jun Wang junwang@deepglint.com DeepGlint Beijing, China Haoran Xu xhr964691257@163.com Zhejiang University Zhejiang Province, China Ziyong Feng ziyongfeng@deepglint.com DeepGlint Beijing, China Yupei Wang wangyupei2019@outlook.com Beijing Institute of Technology Beijing, China Abstract Contrastive Language-Image Pre-training (CLIP) has achieved success on multiple downstream tasks by aligning image and text modalities. However, the nature of global contrastive learning limits CLIPs ability to comprehend compositional concepts, such as relations and attributes. Although recent studies employ global hard negative samples to improve compositional understanding, these methods significantly compromise the models inherent general capabilities by forcibly distancing textual negative samples from images in the embedding space. To overcome this limitation, we introduce Decoupled Global-Local Alignment (DeGLA) framework that improves compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the models inherent capabilities, we incorporate self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with frozen teacher model derived from an exponential moving average. Under the constraint of self-distillation, it effectively mitigates the catastrophic forgetting of pretrained knowledge during fine-tuning. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (IGC) loss and TextGrounded Contrast (TGC) loss to enhance vision-language compositionally. Extensive experimental results demonstrate the effectiveness of the DeGLA framework. Compared to previous stateof-the-art methods, DeGLA achieves an average enhancement of 3.5% across the VALSE, SugarCrepe, and ARO benchmarks. Concurrently, it obtains an average performance improvement of 13.0% (a) General performance. (b) Compositional performance (c) An illustration for the compositional understanding. Figure 1: (a) General performance comparison across 11 classification datasets. (b) Compositional performance comparison on VALSE, ARO, and SugarCrepe benchmarks. (c) CLIP scores for images with aligned and unaligned captions. DeGLA demonstrates significantly enhanced capabilities in compositional understanding. Equal contribution. Corresponding author. on zero-shot classification tasks across eleven datasets. Our code will be released at https://github.com/xiaoxing2001/DeGLA. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Preprint, 2025 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/YY/MM https://doi.org/XXXXXXX.XXXXXXX CCS Concepts Computing methodologies Computer vision representations. Keywords Vision-Language Model, Multi-Modal, Compositional Understanding Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al."
        },
        {
            "title": "1 Introduction\nThe rapid expansion of mobile networks and social platforms signif-\nicantly boosts the large-scale generation of image-text pairs, laying\na crucial foundation for vision-language pre-training. In particular,\nContrastive Language-Image Pre-training [49] (CLIP) leverages two\ndistinct unimodal encoders for images and texts, and employs con-\ntrastive loss [43] for representation learning. After pretraining on\nextensive image-text pairs sourced from the internet, CLIP exhibits\nstrong transferability and is extensively applied across various tasks,\nincluding image captioning [1, 30, 38], object detection [25, 36, 54],\nand segmentation [4, 15, 17, 29, 39, 55, 60, 62].",
            "content": "As CLIP attracts increasing attention from researchers, several enhanced methods based on CLIP have been proposed [14, 32, 67]. SLIP [41] introduces multitask learning framework that integrates self-supervised learning with CLIP pretraining. FILIP [59] enhances the fine-grained alignment between image patches and textual words by refining the contrastive loss, while maintaining the capability for offline pre-computation of image and text representations during inference, ensuring efficiency in both large-scale training and inference phases. ALIP [57] proposes the use of synthetic captions and adaptive contrastive loss to mitigate the influence of noisy data and improve the efficacy of pre-training data utilization. SigLIP [65] utilizes sigmoid loss, which supports scaling up batch sizes and also performs effectively at smaller batch sizes. However, global contrastive learning inadequately leverages compositional structures within image-text pairs, thereby limiting CLIPs capability to capture nuanced compositional information in multimodal data [61]. As shown in Figure 1c, CLIP struggles to discern the relationship between athlete and basketball. Recent studies [7, 8, 20, 61, 66] have sought to improve the compositional understanding of CLIP. NegCLIP [61] introduces the Attribution, Relation, and Order (ARO) benchmark and, for the first time, proposes fine-tuning framework that integrates hard negative samples to enhance CLIPs compositional understanding. StructureCLIP [20] integrates Scene Graph Knowledge (SGK) to augment multimodal structured representations. Hard-positives [23] incorporates hard positive samples during fine-tuning to strengthen the models capacity to capture subtle but semantically related variations among similar instances. CE-CLIP [66] introduces simple yet effective fine-tuning framework with two fine-grained alignment losses to enhance compositional understanding, achieving stateof-the-art performance on multiple vision-language compositional reasoning benchmarks. However, our analysis reveals that existing methods improve CLIPs compositional understanding at the cost of general capabilities, exhibiting significant catastrophic forgetting of pre-trained knowledge. To address the limitation of previous works, we innovatively introduce Decoupled Global-Local Alignment (DeGLA) framework, which aims to improve compositional understanding while substantially mitigating losses in general capabilities. To optimize the retention of the inherent capabilities of the model, we incorporate self-distillation mechanism within the global alignment process, aligning the learnable image-text encoder with teacher model derived from an exponential moving average. To improve compositional understanding, we first leverage the in-context learning capability of Large Language Models (LLMs) to construct about 2M high-quality negative captions across five types. Subsequently, we propose the Image-Grounded Contrast (ICC) and Text-Grounded Contrast (TCC) to improve fine-grained understanding. We conduct extensive experiments and demonstrate that our method achieves new state-of-the-art performance in both compositional and general tasks. The main contributions are summarized as follows: We observe that previous methods, while enhancing CLIPs compositional understanding, often compromise its general understanding capabilities. We propose simple yet effective negative caption generation pipeline that leverages the context learning capability of Large Language Models (LLMs) to generate high-quality negative captions. We introduce the DeGLA framework, which employs selfdistillation mechanism within the global alignment to maintain the models inherent general comprehension capabilities. Additionally, it combines Image-Grounded Contrast (IGC) loss and Text-Grounded Contrast (TGC) loss to improve vision-language compositional understanding. We conduct extensive experiments and demonstrate that DeGLA achieves new state-of-the-art performance in both compositional and general tasks."
        },
        {
            "title": "2.2 Vision-Language Compositionality\nRecently, there have been some works [18, 22, 45, 47, 51, 61] aim to\nenhance the compositional understanding of CLIP. NegCLIP [61]\nintroduces the Attribution, Relation, and Order (ARO) benchmark\nand utilizes hard negatives, consisting of nearest neighboring im-\nages within each batch, to force models to discern fine-grained\ndifferences in highly similar scenes. Structure-CLIP [20] incor-\nporates Scene Graph Knowledge (SGK) to enhance multimodal\nstructured representations. CE-CLIP [66] proposes a simple yet\neffective strategy to optimize the utilization of existing image-text\ndatasets, achieving state-of-the-art performance across multiple\nvision-language compositional reasoning benchmarks. However, al-\nthough these methods significantly enhance CLIP’s compositional",
            "content": "Decoupled Global-Local Alignment for Improving Compositional Understanding Figure 2: The overview of our proposed LLM-driven negative caption generation pipeline. We leverage the robust in-context learning capabilities of LLM to generate five types of hard negative captions. understanding, they frequently compromise its original generalization capabilities. Developing an approach that simultaneously improves both general and compositional understanding continues to be substantial challenge."
        },
        {
            "title": "3 Method\nIn subsequent sections, we first present the preliminary knowl-\nedge of CLIP in Section 3.1, followed by a detailed exposition of\nour proposed LLM-driven negative caption generation pipeline in",
            "content": "Preprint, April, 2025 Table 1: Detailed descriptions of the generated high-quality negative samples. Negative types Rewrite rule Intra-sentence reshuffle Subtype1 Reshuffle the overall order of the sentence Swap the nouns in the sentence Subtype2 Swap the adjectives in the sentence Subtype Minimal semantic substitution Subtype4 Replace the adjectives in the sentence Subtype5 Replace the nouns in the sentence Section 3.2. comprehensive description of the DeGLA training framework is provided in Section 3.3."
        },
        {
            "title": "3.1 Preliminaries of CLIP\nCLIP consists of a text encoder E𝑇 , and an image encoder E𝐼 . It\nencodes a batch of image-text pairs {(𝐼𝑖,𝑇𝑖 )} B\n𝑖=1 into the feature\nspace {(𝑣𝑖, 𝑡𝑖 )} B\n𝑖=1. The model employs the InfoNCE loss [44] to\noptimize the embeddings by minimizing the distance between cor-\nresponding image and text features while maximizing the distance\nbetween non-corresponding pairs. The image-to-text contrastive\nloss function is defined as follows:",
            "content": "L𝐼 𝑇 = log exp(𝑣𝑖 𝑡 𝑖 /𝜏) 𝑗=1 exp(𝑣𝑖 𝑡 𝑗 /𝜏) (cid:205)B , (1) where 𝑣𝑖 serves as the anchor. Given 𝑡𝑖 as the anchor, The symmetric text-to-image contrastive loss is formulated as follows: L𝑇 𝐼 = log exp(𝑡𝑖 𝑣 𝑖 /𝜏) 𝑗=1 exp(𝑡𝑖 𝑣 𝑗 /𝜏) (cid:205)B , (2) where denotes the dot product used to calculate the similarity between normalized image and text embeddings, and 𝜏 is learnable temperature parameter. The overall CLIP loss is: L𝐶𝐿𝐼 𝑃 = 1 2 (L𝐼 𝑇 + L𝑇 𝐼 ). (3) CLIP encodes all semantic content from images and texts into global representation during its training process, which constrains its capability for compositional understanding. While recent works [20, 61, 66] have sought to enhance this ability by introducing hard negative samples to promote the learning of local semantics, our analysis reveals experimental results demonstrate that such enhancements often degrade the models inherent general comprehension capabilities. Consequently, this paper focuses on improving compositional understanding without compromising the original general comprehension abilities of the model."
        },
        {
            "title": "3.2 LLM-Driven Negative Caption Generation\nTo improve compositional understanding of CLIP, we first leverage\nthe in-context learning capability of Large Language Models (LLMs)\nto construct about 2M high- quality negative captions across multi-\nple types. Previous methods for generating compositional negative\ncaptions can be classified into two categories: rule-based genera-\ntion [20, 61] and unmasking-based generation [7, 8, 66]. However,\nrule-based approaches are limited in their ability to produce com-\nplex compositional negatives, such as those involving minimal\nsemantic substitutions. In contrast, unmasking-based methods can\ngenerate diverse negatives but often fail to avoid producing hard",
            "content": "Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. positive samples such as the cat is eating the food the cat is eating the hot food. To address these limitations, we propose an LLM-driven generation method to enhance and standardize the generation of compositional negative captions. Specifically, we categorize negative captions into two primary types: (1) intra-sentence reshuffling, which improves the VLMs sensitivity to sentence order, and (2) minimal semantic substitution, which enhances the VLMs ability to discern subtle semantic variations. For intra-sentence reshuffling, we examine three subtypes: (a) full sentence reordering, (b) noun swapping, and (c) adjective swapping. For minimal semantic substitution, we investigate two subtypes: (a) adjective replacement and (b) noun replacement. Detailed descriptions of each negative sample type are presented in Table 1. As shown in Figure 2, we initially generate high-quality negative samples to fully leverage the in-context learning capabilities of LLMs. For each type, we first use ChatGPT4-Turbo to generate 200 rewritten examples according to rewritten rules, then manually select the 50 highest-quality examples through rigorous evaluation. Building upon these examples, we employ the Llama-3.1-8B-Instruct model1 to generate largescale compositional negatives. To ensure semantic divergence from original sentences and avoid hard positives, we incorporate explicit filtering constraints in the prompts (e.g., generated sentences must exhibit distinct semantics). Complete prompt templates and representative samples are provided in the appendix."
        },
        {
            "title": "3.3 DeGLA Framework\nGlobal Alignment. Building upon the base setting [61], we inte-\ngrate negative captions into the Equation 1 for global alignment,\nyielding the augmented image-to-text contrastive:",
            "content": "L𝐼 𝑇 = log (cid:16) (cid:205)B 𝑗=1 exp(𝑣𝑖 𝑡 𝑗 /𝜏) + (cid:205)K 𝑖 /𝜏) 𝑘=1 exp(𝑣𝑖 𝑡 𝑖,𝑘 /𝜏) exp(𝑣𝑖 𝑡 , (cid:17) (4) where 𝑡 𝑗,𝑘 denotes the 𝑘-th negative text related to the 𝑗-th text. In this work, we set = 4 to denote the number of negative samples paired with each positive sample during training, due to the merging of two subtypes, as detailed in the appendix. Then we utilize the hard negative-aware image-text contrastive loss as our base loss which can be formulated as: L𝑏𝑎𝑠𝑒 = 1 2 (L𝐼 𝑇 + L𝑇 𝐼 ), (5) However, directly incorporating hard negative text into global contrastive learning significantly compromises the models general capabilities, as indicated in Section 4. In this paper, we introduce self-distillation mechanism within the global alignment framework to mitigate the catastrophic forgetting of pre-trained knowledge. To enhance model robustness and prevent rapid forgetting of pre-trained knowledge during training, we employ an Exponential Moving Average (EMA) strategy to update the weights of the frozen image encoder 𝐼 and text encoder 𝑇 : 𝐼 = 𝛼 𝐼 + (1 𝛼)E𝐼 , 𝑇 = 𝛼 E 𝑇 + (1 𝛼)E𝑇 , (6) 1https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct Figure 3: The proposed training framework of DeGLA. where the hyperparameter 𝛼 controls the update speed of the frozen models parameters. Given batch of image-text pairs {𝐼𝑖,𝑇𝑖, 𝑇𝑖 }, we obtain the image and text embeddings {𝑣𝑖, 𝑡𝑖, 𝑡𝑖 } from the learnable encoders (E𝐼 , E𝑇 ) and {𝑣 𝑖 } from the frozen EMA encoders (E 𝑇 ), respectively. Following L2 normalization, we compute the squared distance between the image-text embeddings from the learnable and frozen EMA encoders to quantify the knowledge discrepancy between the fine-tuned and pre-trained models: 𝐼 , 𝑖 , 𝑡 𝑖 , 𝑡 L𝐷𝑖𝑠𝑡𝑖𝑙𝑙 = (cid:32) 𝑖=1 (cid:13) (cid:13)𝑣𝑖 𝑣 𝑖 (cid:13) 2 (cid:13) 2 + (cid:13) (cid:13)𝑡𝑖 𝑡 𝑖 (cid:13) 2 (cid:13) 2 + (cid:13) 𝑡𝑖,𝑘 𝑡 (cid:13) 𝑖,𝑘 (cid:13) (cid:13) 2 (cid:13) (cid:13) 2 (cid:33) . 𝑘=1 (7) Under the constraint of Equation 7, the image-text embeddings undergo subtle refinements within the pre-trained representation space, thereby preserving the models general capabilities from excessive degradation. Local Alignment. In addition to the global alignment of Lbase, to further enhance compositional understanding, we propose Local Alignment, which includes Image-Grounded Contrast Loss(IGC) and Text-Grounded Contrast Loss(TGC). We initially introduce the Image-Grounded Contrast loss to attract image embeddings towards positive text embeddings and repel them from negative text embeddings in the feature space (as shown in Figure 3). Specifically, given an image-text pair (𝐼𝑖,𝑇𝑖 ) and corresponding hard negative texts 𝑇𝑖 , the IGC loss L𝐼𝐺𝐶 is defined as follows: L𝐼𝐺𝐶 = log exp(𝑣𝑖 𝑡 𝑖 /𝜏) + (cid:205)K exp(𝑣𝑖 𝑡 𝑖 /𝜏) 𝑘=1 exp(𝑣𝑖 𝑡 𝑖,𝑘 /𝜏) ), (8) where 𝜏 is temperature parameter that controls the sharpness of the distribution. Switching to the textual perspective, hard negative texts, which are minor modifications of the originals, remain proximate to the positive texts within the feature space of the CLIP text encoder. This closeness may lead to mismatches during image embedding alignment with positive text embeddings, potentially impairing compositional understanding. Different from the ImageGrounded Contrast loss, the Text-Grounded Contrast (TGC) loss L𝑇𝐺𝐶 operates solely within the text modality. Its function is to enhance the text encoders ability to more effectively discriminate Decoupled Global-Local Alignment for Improving Compositional Understanding Preprint, April, Table 2: Results (%) on VALSE. The best results are marked in bold, and the second-best results are underlined. The number represents the improvement of our method compared to the CE-CLIP. Model #Params Existence Plurality Counting number quantifiers Sp.rel. relations Actions Coreference repl. actant swap standard clean Foil-it! Avg. BLIP[31] BEIT3[52] BLIP2[30] MiniGPT-4[68] 583M 1.9B 3.4B >9B Hard Negative based method XVLM-coco[63] CE-XVLM[66] CLIP[50] CyCLIP[12] NegCLIP[61] Structure-CLIP [20] CE-CLIP[66] DeGLA (ours) 216M 216M 151M 151M 151M 151M 151M 151M 86.3 77.4 55.5 65. 83.0 83.5 68.7 69.3 76.8 75.6 78.6 82.4 73.2 74.6 71.5 72.5 75.6 72.8 57.1 58.3 72.0 67.1 77.6 73.8 68.1 68.8 66.0 67.4 67.5 72.1 61.0 61.0 65.2 62.0 64.3 68.3 71.5 74.0 62.4 68. 70.2 68.7 65.4 66.4 72.7 68.2 74.0 75.3 77.2 86.7 83.6 83.2 73.8 71.8 77.8 78.1 81.6 80.4 81.2 82.6 61.1 65.2 51.6 58.8 68.6 69.1 71.8 72.0 84.8 88.3 88.5 88.8 53.8 50.0 48.6 52. 46.4 51.0 54.1 53.2 58.9 44.5 54.9 58.5 48.2 44.2 51.9 51.0 49.6 46.8 51.0 51.6 54.8 58.7 52.9 54.8 93.8 96.0 95.9 95.8 94.8 93.8 89.8 88.8 91.8 91.2 93.7 93.8 70.0 70.4 65.4 68. 69.5 70.8 65.3 65.5 71.7 69.1 72.2 74.1(+1.9) Table 3: Results(%) on SugarCrepe. Vera and Grammar are text-only models. The best results are marked in bold, and the second-best results are underlined. The number represents the improvement of our method compared to the CE-CLIP. Model Human Vera [35] Grammar [40] BLIP2 [30] 100. 49.4 50.0 - Hard Negative based method 90.9 CLIP [50] 92.7 NegCLIP [61] 91.4 Structure-CLIP [20] 93.1 CE-CLIP [66] 94.5 DeGLA (ours) REPLACE SWAP ADD Object Attribute Relation Avg. Object Attribute Avg. Object Attribute Avg. 99. 49.6 50.0 - 80.0 85.9 85.0 88.8 92.6 97.0 49.1 50.0 - 69.2 76.5 74.4 79.0 84.2 98. 49.4 50.0 86.7 80.2 85.0 83.6 87.0 90.5(+3.5) 99.0 49.4 50.0 - 62.7 75.3 72.7 72.8 81.6 100. 49.2 50.0 - 61.4 75.2 80.5 77.0 82.1 99.5 49.3 50.0 69.8 64.0 75.4 76.6 74.9 81.9(+6.9) 99. 49.4 50.0 - 77.2 88.8 85.5 92.4 93.8 99.0 49.6 50.0 - 68.2 82.8 81.1 93.4 95.7 99. 49.5 50.0 86.5 72.7 85.8 83.3 92.9 94.8(+1.9) Table 4: Results (%) on ARO. The best results are marked in bold, and the second-best results are underlined. The number represents the improvement of our method compared to the CE-CLIP. Model Relation Attribute COCO-order Flickr-order Avg. BLIP[31] BEIT3[52] BLIP2[30] MiniGPT-4[68] 59.0 60.6 41.2 46.9 Hard Negative based method CLIP [50] CyCLIP[12] NegCLIP[61] Structure-CLIP [20] CE-CLIP[66] DeGLA (ours) 59.2 59.1 80.4 81.8 83.9 81.6 88.0 74.6 71.3 55. 62.9 65.4 70.5 80.5 76.4 74.3 - - - - 48.4 - 86.9 81.7 80.9 93.8 - - - - 59.1 - 90.5 83.9 83.7 94.7 - - - - 57.4 - 82.1 82.0 81.2 86.1(+4.9) between positive and negative texts, thus improving the compositional understanding of vision language models. The L𝑇𝐺𝐶 is formulated as: L𝑇𝐺𝐶 = log exp(𝑡𝑖 𝑡 𝑖 /𝜏) + (cid:205)K 𝑖 represents the progressively frozen text embedding ob𝑇 . This embedding is where 𝑡 tained from the frozen EMA text encoder 𝑖 /𝜏) 𝑘=1 exp(𝑡𝑖 𝑡 exp(𝑡𝑖 𝑡 𝑖,𝑘 /𝜏) (9) , used as an anchor in local alignment for two main reasons: (1) It acts as the positive sample in contrastive learning, enabling the model to discriminate between positive and compositional negative samples; and (2) It mitigates potential overfitting of the text encoder to the feature space of the fine-tuning dataset. Finally, the overall loss function is defined as: L𝑎𝑙𝑙 = L𝐵𝑎𝑠𝑒 + 𝜆1L𝐼𝐺𝐶 + 𝜆2L𝑇𝐺𝐶 + 𝜆3L𝐷𝑖𝑠𝑡𝑖𝑙𝑙 . (10) where 𝜆1, 𝜆2, 𝜆3 are loss weights to balance the influence of different loss functions."
        },
        {
            "title": "4 Experiments\n4.1 Implement details\nTraining Setup. To ensure a fair comparison with previous stud-\nies [61, 66], we employ LLaMA3.1-instruct-8B [13] to generate\ncompositional negative samples from the MSCOCO dataset, facili-\ntating direct comparisons with NegCLIP [61] and CE-CLIP [66]. We\nutilize the CLIP-ViT/B-32 model as the foundation vision-language\nmodel, initializing it with pretrained weights from CLIP [50]. The\nmodel is fine-tuned on 8 NVIDIA V100 (32G) GPUs for 5 epochs\nusing a batch size of 256, consistent with the protocols of previous",
            "content": "Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. Table 5: Zero-shot classification performance on 11 datasets. The best results are marked in bold, and the second-best results are underlined. The number represents the improvement of our method compared to the CE-CLIP. Model CIFAR10 CIFAR100 Food101 Pets Flowers SUN397 Cars DTD Caltech101 Aircraft ImageNet Avg. Pretrained model CLIP [50] 86. Hard negative based method NegCLIP [61] Structure-CLIP [20] CE-CLIP [66] DeGLA (ours) 86.1 76.8 80.5 86.5 61.0 59.9 47.4 54.1 59.5 78.5 79. 58.4 59.9 48.8 38.7 86.3 15. 57.9 61.0 72.1 55.1 57.6 75.6 78.7 61.4 59.0 76.0 53.9 31.3 30.1 52.8 56.8 48.3 49.2 59. 43.5 16.4 22.8 45.7 37.7 29.4 27.6 38.1 84.3 71.0 74.4 84.0 11.6 7.6 9.1 14.1 54.0 37.3 38.1 54.5 58.1 43.8 45.7 58.7(+13.0) Table 6: Liner probe performance on 11 datasets. The best results are marked in bold, and the second-best results are underlined. The number represents the improvement of our method compared to the CE-CLIP. Model CIFAR10 CIFAR100 Food101 Pets Flowers SUN397 Cars DTD Caltech101 Aircraft ImageNet Avg. Pretrained model CLIP [50] 95.0 Hard negative based method NegCLIP [61] Structure-CLIP [20] CE-CLIP [66] DeGLA (ours) 94.6 91.9 94.3 95. 80.1 80.0 75.5 78.5 80.5 88.5 89.3 94.6 74. 80.8 73.6 90.5 44.8 74.3 80. 86.1 81.2 84.3 86.7 89.6 86.2 88.1 89.5 93.9 89.6 92.6 94.6 72.9 69.0 71.0 74.0 78.8 67.4 74.1 78.8 72.9 67.7 71.8 73. 90.0 65.2 88.3 89.6 43.2 37.7 39.6 43.5 72.9 67.7 70.7 73.4 79.5 72.7 77.6 79.9(+2.3) Figure 4: Zero-shot image-text retrieval performance comparison on MSCOCO and Flickr30k. works [8, 61, 66]. We employ AdamW as the optimizer, initialized with learning rate of 1 106 and weight decay of 0.1. The parameters 𝛼, 𝛽1, and 𝛽2 are set to 0.9996, 0.9, and 0.98, respectively. We perform hyperparameter search for 𝜆1, 𝜆2, 𝜆3, with optimal values of 𝜆1 = 0.1, 𝜆2 = 0.1, and 𝜆3 = 0.005. Evaluation Setup. To comprehensively evaluate DeGLA, we conduct comparative experiments on three compositional reasoning benchmarks: ARO [61], VALSE [45], and SugarCrepe [18]. To verify that DeGLA retains general capabilities, we further assess its performance on zero-shot classification, liner probe, and retrieval tasks. For zero-shot classification, we evaluate on eleven datasets: CIFAR10, CIFAR-100 [27], Food101 [2], Oxford Pets [46], Flowers102 [42], SUN397 [56], Stanford Cars [26], DTD [5], Caltech101 [10], FGVCAircraft [37], and ImageNet [6]. The datasets used for evaluating the linear probe is the same as that used for zero-shot classification. For zero-shot image-text retrieval, we report results on MSCOCO [3] and Flickr30k [48]. We compare DeGLA with three kinds of models: (1)Text-only models, including vera [35] and Grammar [40]. (2) State-of-the-art generative vision-language models, including BLIP [31], BLIP-2 [30], MiniGPT-4 [68]); (3) High-performance vision-language understanding models, such as BEIT-3 [52], XVLM [64]; (4) Specialized Figure 5: Performance trade off between compositional reasoning (average performance on VALSE, SugarCrepe, and ARO benchmarks) and zero-shot classification. compositional improvement methods NegCLIP [61], CyCLIP [12], Structure-CLIP [20], and CE-CLIP [66]."
        },
        {
            "title": "4.2 Compositional Reasoning\nPerformance on VALSE. We first evaluate DeGLA on VALSE,\na benchmark specifically designed to assess pre-trained vision-\nlanguage models’ sensitivity to foiled instances. This benchmark\nsystematically tests fundamental linguistic phenomena across vi-\nsual and linguistic modalities, including existence, plurality, count-\ning, spatial relations, actions, and entity coreference. As indicated\nin Table 2, DeGLA outperforms other models in existence, counting,\nand spatial relations, and ranks second in plurality, actions replace-\nment, coreference standard, and foil-it. Compared to CLIP, DeGLA\nexhibits an 8.8% average performance improvement, demonstrating\nthat our approach more effectively leverages hard negatives through\nlocal and global alignment strategies. Thanks to the diverse nega-\ntive samples generated in this study and the employment of both",
            "content": "Decoupled Global-Local Alignment for Improving Compositional Understanding Preprint, April, 2025 (a) Ablation of 𝜆1, 𝜆2 (b) Ablation of 𝜆3 (c) Ablation of hard negative types. Figure 6: Ablation of the different loss weights and hard negative types. image-grounded and text-grounded contrastive learning, DeGLA significantly surpasses CE-CLIP, achieving an overall performance increase of 1.9%. Performance on SugarCrepe. SugarCrepe is benchmark designed to reduce language bias in existing benchmarks and provide more accurate assessment of models compositional understanding. As indicated in Table 3, DeGLA achieves new state-of-the-art results across all metrics. DeGLA shows significant improvements over the CLIP model, with increases of 10.3% on Replace, 17.9% on Swap, and 22.1% on Add. These results suggest that DeGLA effectively distinguishes between positive and negative samples through local alignment. Compared to CE-CLIP, our method achieves performance enhancements of 3.5%, 6.9%, and 1.9% on Replace, Swap, and Add, respectively. This substantial improvement is attributed to two primary factors: First, the negative sample generation method we introduced leverages Large Language Models (LLMs) to produce high-quality negative text descriptions, reducing the generation of noisy and challenging positive samples. Second, we implement image-grounded and text-grounded contrast mechanisms that enhance the models discriminative capabilities by drawing positive sample pairs closer and pushing negative sample pairs further apart in the feature space, thus improving the models compositional understanding. Performance on ARO. We present the performance of our proposed DeGLA on the ARO benchmark in Table 4. DeGLA achieves substantial improvements, registering an average performance increase of 28.7% over CLIP and 4.9% over CE-CLIP. It is noteworthy that while DeGLA exhibits significant overall performance gains, it still trails CE-CLIP and Structure-CLIP in the domains of relations and attributes. The primary reason for this discrepancy is that our negative sample generation method prioritizes the production of diverse array of negative samples, rather than focusing specifically on enhancing the understanding of relations and attributes."
        },
        {
            "title": "4.3 General Understanding\nZero-shot classification. In Table 5, we present the zero-shot clas-\nsification performance across 11 datasets. Notably, while Structure-\nCLIP and CE-CLIP exhibit robust compositional understanding,\nthey significantly reduce the model’s original general capabilities,\nwith average accuracies decreasing by 17.2% and 15.3%, respectively,\ncompared to CLIP. In contrast, our proposed DeGLA employs a\nself-distillation constraint module for global alignment, effectively",
            "content": "Table 7: Ablation of different components. CN: compositional negatives. SD: self-distillation. Model CN IGC TGC SD ARO ZS-Avg.(11) CLIP [50] CE-CLIP [66] DeGLA(ours) 57.4 81.2 80.8 84.8 82.2 86.2 86. 61.0 45.7 55.4 55.1 57.2 56.9 58.7 minimizing the loss of general capability. Our method shows 13.0% improvement in average accuracy over CE-CLIP, while substantially preserving the models inherent general capabilities. As illustrated in Figure 5, DeGLA achieves more effective balance between compositional reasoning and general comprehension capabilities compared to other methods. Linear probe. In Table 5, we detail the linear probe performance across 11 datasets. Consistent with the zero-shot classification results, Structure-CLIP and CE-CLIP significantly reduce the models original general capabilities, with average accuracies decreasing by 7.8% and 2.9%, respectively, compared to CLIP. Conversely, our proposed DeGLA model not only demonstrates superior compositional understanding relative to CE-CLIP but also achieves 2.3% average performance improvement in linear probe tasks across these datasets. Zero-shot image text retrieval. We compare the zero-shot retrieval performance of DeGLA, Structure CLIP, and CE-CLIP on the MSCOCO and Flickr30k datasets. As illustrated in Figure 4, DeGLA surpasses previous models on both datasets, demonstrating superior performance in standard retrieval tasks. This finding is consistent with our earlier experimental results, further substantiating DeGLAs excellence in both general capabilities and compositional reasoning."
        },
        {
            "title": "4.4 Analysis\nTrade-off Analysis. As illustrated in Figure 5, we analyze the\ntrade-off between general capabilities and compositional reason-\ning. For the evaluation of compositional reasoning, we utilize the\naverage scores from three benchmarks. Typically, enhancements in\ncompositional reasoning are accompanied by declines in general",
            "content": "Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. Figure 7: The case study between CLIP and DeGLA. Green indicates true caption, while red indicates false caption. The bar chart represents the models prediction results. capabilities, as demonstrated by CE-CLIP, which, despite significant gains in compositional reasoning compared to CLIP, suffers substantial reduction in general capabilities. In contrast, through its meticulously designed data generation pipeline and training framework, DeGLA achieves an optimal balance. Compared to the baseline CE-CLIP, DeGLA records 13% improvement in zero-shot classification and an average enhancement of 3.5% in compositional reasoning tasks, further underscoring its superiority. Ablation of different components. To validate the efficacy of the proposed method, we perform comprehensive ablation studies on key components using the ARO benchmark in Table 7. By combining all modules, DeGLA achieves the most balanced performance, attaining an 86.1% average score on ARO with 4.9% higher than the previous SOTA (CE-CLIP). For zero-shot classification, it outperforms CE-CLIP by 13.0%, demonstrating superior generalization. Analysis of component contributions reveals that integrating highquality negative samples increases ARO performance by 23.4% but decreases zero-shot classification performance by 5.6%. The local alignment losses, IGC and TGC, improve ARO performance by 4.0% and 1.4%, respectively. After Combining further boosting ARO performance by 5.4%, while zero-shot classification still decreases by 4.1%. Notably, TGC mitigates declines in general capability by using frozen text embeddings from the EMA text encoder, ensuring minimal deviation from the pretrained CLIP model during training. The introduction of self-distillation mechanism significantly enhances general capability, improving zero-shot classification by 1.8% with negligible 0.1% loss in ARO performance. Ablation of Losses. We analyze the impact of the weights 𝜆1, 𝜆2, and 𝜆3 of the three losses on the performance of ARO, as illustrated in Figure 6a and Figure 6b. Initially, we identify the optimal combination of (𝜆1, 𝜆2), finding that the model achieves its best performance, 86.1%, with 𝜆1 = 0.1 and 𝜆2 = 0.1. Consequently, we adopt these parameters. Subsequent analysis of 𝜆3 variations reveals that increasing distillation enhances zero-shot classification performance but reduces combinatorial understanding, reflecting the conflicting objectives of self-distillation and compositional reasoning. Ultimately, we select 𝜆3 = 0.005, as this setting provides well-balanced model, as indicated by the red dashed circle in the figure. Ablation of compositional negative texts. To verify the effectiveness of our proposed instruction-based negative generation pipeline, we conduct an ablation analysis on five subtypes of negative texts, as depicted in Figure 6c. The results show that combining these subtypes achieves optimal performance, thereby validating the pipelines effectiveness. This success is attributed to the representativeness of our samples, which effectively enhance the compositional understanding of CLIP."
        },
        {
            "title": "4.5 Case Study\nWe provide a case study comparison between CLIP and DeGLA\non ARO and SugarCrepe in Figure 7. The results show that the\nCLIP model exhibits a pronounced “bag-of-words” phenomenon, as\nindicated in previous work NegCLIP [61], meaning it struggles with\nrecognizing structural changes in texts, such as swapping the order\nof targets in a sentence, adding targets, or replacing targets. For\ninstance, CLIP fails to select the correct text from options like “Two\nducks swim in a pond with green water.” and “Two swans swim in a\npond with green ducks.”. In contrast, DeGLA is capable of correctly\ndistinguishing between them based on the given image. Notably,\nafter training on our diverse compositionality-related hard nega-\ntive data generated by our proposed pipeline in Section 3.2, DeGLA\ndemonstrates enhanced compositional reasoning understanding\ncompared to pretrained CLIP, such as intra-sentential reshuffle and\nminimal semantic substitution, thereby underscoring the effective-\nness of our data generation pipeline.",
            "content": "Decoupled Global-Local Alignment for Improving Compositional Understanding Preprint, April,"
        },
        {
            "title": "5 Conclusion\nIn this paper, we observe that while previous methods enhance\nCLIP’s compositional understanding, they often compromise its\ngeneral capabilities. To overcome this limitation, we introduce a\nDecoupled Global-Local Alignment (DeGLA) framework, which\nnot only improves compositional understanding but also signifi-\ncantly reduces losses in general capabilities. To optimize the re-\ntention of the model’s inherent capabilities, we incorporate a self-\ndistillation mechanism within the global alignment process, align-\ning the learnable image-text encoder with a frozen teacher model\nderived from an exponential moving average. For enhancing compo-\nsitional understanding, we leverage the in-context learning capabil-\nities of Large Language Models (LLMs) to generate approximately\n2M high-quality negative captions across five types. We further pro-\npose the Image-Grounded Contrast (IGC) loss and Text-Grounded\nContrast (TGC) loss to improve vision-language compositionality.\nExtensive experimental results demonstrate the effectiveness of the\nDeGLA framework. We hope that our work provides insights into\nvision-language compositional understanding.",
            "content": "References [1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: visual language model for few-shot learning. NIPS 35 (2022), 2371623736. [2] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101mining discriminative components with random forests. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part VI 13. Springer, 446461. [3] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. 2015. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 (2015). [4] Seokju Cho, Heeseong Shin, Sunghwan Hong, Anurag Arnab, Paul Hongsuck Seo, and Seungryong Kim. 2024. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation. In CVPR. 41134123. [5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. 2014. Describing textures in the wild. In CVPR. 36063613. [6] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: large-scale hierarchical image database. In CVPR. Ieee, 248255. [7] Sivan Doveh, Assaf Arbelle, Sivan Harary, Roei Herzig, Donghyun Kim, Paola Cascante-Bonilla, Amit Alfassy, Rameswar Panda, Raja Giryes, Rogerio Feris, et al. 2023. Dense and aligned captions (dac) promote compositional reasoning in vl models. NIPS 36 (2023), 7613776150. [8] Sivan Doveh, Assaf Arbelle, Sivan Harary, Eli Schwartz, Roei Herzig, Raja Giryes, Rogerio Feris, Rameswar Panda, Shimon Ullman, and Leonid Karlinsky. 2023. Teaching structured vision & language concepts to vision & language models. In CVPR. 26572668. [9] Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. 2023. Improving clip training with language rewrites. NIPS 36 (2023), 3554435575. [10] Li Fei-Fei, Rob Fergus, and Pietro Perona. 2004. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In CVPR. IEEE, 178178. [11] Shijie Geng, Jianbo Yuan, Yu Tian, Yuxiao Chen, and Yongfeng Zhang. 2023. HiCLIP: Contrastive language-image pretraining with hierarchy-aware attention. ICLR (2023). [12] Shashank Goel, Hritik Bansal, Sumit Bhatia, Ryan Rossi, Vishwa Vinay, and Aditya Grover. 2022. Cyclip: Cyclic contrastive language-image pretraining. NIPS 35 (2022), 67046719. [13] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [14] Tiancheng Gu, Kaicheng Yang, Xiang An, Ziyong Feng, Dongnan Liu, Weidong Cai, and Jiankang Deng. 2024. RWKV-CLIP: robust vision-language representation learner. arXiv preprint arXiv:2406.06973 (2024). [15] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. 2021. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921 (2021). [16] Geoffrey Hinton. 2015. Distilling the Knowledge in Neural Network. arXiv preprint arXiv:1503.02531 (2015). [17] Lukas Hoyer, David Joseph Tan, Muhammad Ferjad Naeem, Luc Van Gool, and Federico Tombari. 2025. Semivl: Semi-supervised semantic segmentation with vision-language guidance. In ECCV. Springer, 257275. [18] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2024. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. NIPS 36 (2024). [19] Libo Huang, Yan Zeng, Chuanguang Yang, Zhulin An, Boyu Diao, and Yongjun Xu. 2024. eTag: Class-Incremental Learning via Embedding Distillation and Task-Oriented Generation. In AAAI, Vol. 38. 1259112599. [20] Yufeng Huang, Jiji Tang, Zhuo Chen, Rongsheng Zhang, Xinfeng Zhang, Weijie Chen, Zeng Zhao, Zhou Zhao, Tangjie Lv, Zhipeng Hu, et al. 2024. StructureCLIP: Towards Scene Graph Knowledge to Enhance Multi-Modal Structured Representations. In AAAI, Vol. 38. 24172425. [21] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural language understanding. arXiv preprint arXiv:1909.10351 (2019). [22] Amita Kamath, Jack Hessel, and Kai-Wei Chang. 2023. Whats\" up\" with visionlanguage models? Investigating their struggle with spatial reasoning. arXiv preprint arXiv:2310.19785 (2023). [23] Amita Kamath, Cheng-Yu Hsieh, Kai-Wei Chang, and Ranjay Krishna. 2024. The hard positive truth about vision-language compositionality. In ECCV. Springer, 3754. [24] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generating image descriptions. In CVPR. 31283137. [25] Prannay Kaul, Weidi Xie, and Andrew Zisserman. 2023. Multi-modal classifiers for open-vocabulary object detection. In ICML. PMLR, 1594615969. [26] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 2013. 3d object representations for fine-grained categorization. In ICCV. 554561. [27] Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning multiple layers of features from tiny images. (2009). [28] Janghyeon Lee, Jongsuk Kim, Hyounguk Shon, Bumsoo Kim, Seung Hwan Kim, Honglak Lee, and Junmo Kim. 2022. Uniclip: Unified framework for contrastive language-image pre-training. NIPS 35 (2022), 10081019. [29] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and René Ranftl. 2022. Language-driven semantic segmentation. arXiv preprint arXiv:2201.03546 (2022). [30] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML. PMLR, 1973019742. [31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML. PMLR, 1288812900. [32] Yanghao Li, Haoqi Fan, Ronghang Hu, Christoph Feichtenhofer, and Kaiming He. 2023. Scaling language-image pre-training via masking. In CVPR. 2339023400. [33] Zheng Li, Xiang Li, Lingfeng Yang, Borui Zhao, Renjie Song, Lei Luo, Jun Li, and Jian Yang. 2023. Curriculum temperature for knowledge distillation. In AAAI, Vol. 37. 15041512. [34] Zheng Li, Jingwen Ye, Mingli Song, Ying Huang, and Zhigeng Pan. 2021. Online knowledge distillation for efficient pose estimation. In ICCV. 1174011750. [35] Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah Smith, Yejin Choi, and Hannaneh Hajishirzi. 2023. Vera: general-purpose plausibility estimation model for commonsense statements. arXiv preprint arXiv:2305.03695 (2023). [36] Chuofan Ma, Yi Jiang, Xin Wen, Zehuan Yuan, and Xiaojuan Qi. 2023. Codet: Cooccurrence guided region-word alignment for open-vocabulary object detection. NIPS 36 (2023), 7107871094. [37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. 2013. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151 (2013). [38] Oscar Mañas, Pau Rodriguez, Saba Ahmadi, Aida Nematzadeh, Yash Goyal, and Aishwarya Agrawal. 2022. Mapl: Parameter-efficient adaptation of unimodal pre-trained models for vision-language few-shot prompting. arXiv preprint arXiv:2210.07179 (2022). [39] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, et al. 2022. Simple open-vocabulary object detection. In ECCV. Springer, 728755. [40] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: framework for adversarial attacks, data augmentation, and adversarial training in nlp. arXiv preprint arXiv:2005.05909 (2020). [41] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. 2022. Slip: Selfsupervision meets language-image pre-training. In ECCV. Springer, 529544. [42] Maria-Elena Nilsback and Andrew Zisserman. 2008. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing. IEEE, 722729. [43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. [44] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 (2018). [45] Letitia Parcalabescu, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2021. VALSE: task-independent benchmark for vision and language models centered on linguistic phenomena. arXiv preprint arXiv:2112.07566 (2021). [46] Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. 2012. Cats and dogs. In CVPR. IEEE, 34983505. [47] Wujian Peng, Sicheng Xie, Zuyao You, Shiyi Lan, and Zuxuan Wu. 2024. Synthesize Diagnose and Optimize: Towards Fine-Grained Vision-Language Understanding. In CVPR. 1327913288. [48] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV. 26412649. [49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. PmLR, 87488763. [50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. PMLR, 87488763. [51] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022. Winoground: Probing vision and language models for visio-linguistic compositionality. In CVPR. 52385248. [52] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. 2022. Image as foreign language: Beit pretraining for all vision and visionlanguage tasks. arXiv preprint arXiv:2208.10442 (2022). [53] Kan Wu, Houwen Peng, Zhenghong Zhou, Bin Xiao, Mengchen Liu, Lu Yuan, Hong Xuan, Michael Valenzuela, Xi Stephen Chen, Xinggang Wang, et al. 2023. Tinyclip: Clip distillation via affinity mimicking and weight inheritance. In ICCV. 2197021980. [54] Size Wu, Wenwei Zhang, Lumin Xu, Sheng Jin, Xiangtai Li, Wentao Liu, and Chen Change Loy. 2024. CLIPSelf: Vision Transformer Distills Itself for OpenVocabulary Dense Prediction. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=DjzvJCRsVf [55] Yao Wu, Mingwei Xing, Yachao Zhang, Yuan Xie, and Yanyun Qu. 2024. Clip2uda: Making frozen clip reward unsupervised domain adaptation in 3d semantic segmentation. In ACM MM. 86628671. [56] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and Antonio Torralba. 2010. Sun database: Large-scale scene recognition from abbey to zoo. In CVPR. IEEE, 34853492. [57] Kaicheng Yang, Jiankang Deng, Xiang An, Jiawei Li, Ziyong Feng, Jia Guo, Jing Yang, and Tongliang Liu. 2023. Alip: Adaptive language-image pre-training with synthetic caption. In ICCV. 29222931. [58] Kaicheng Yang, Tiancheng Gu, Xiang An, Haiqiang Jiang, Xiangzi Dai, Ziyong Feng, Weidong Cai, and Jiankang Deng. 2024. Clip-cid: Efficient clip distillation via cluster-instance discrimination. AAAI (2024). [59] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. 2021. Filip: Fine-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783 (2021). [60] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. 2023. Convolutions die hard: Open-vocabulary segmentation with single frozen convolutional clip. NIPS 36 (2023), 3221532234. [61] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. 2022. When and why vision-language models behave like bags-of-words, and what to do about it? arXiv preprint arXiv:2210.01936 (2022). [62] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. 2022. Open-vocabulary detr with conditional matching. In ECCV. Springer, 106122. [63] Yan Zeng, Xinsong Zhang, and Hang Li. 2021. Multi-Grained Vision Language PreTraining: Aligning Texts with Visual Concepts. arXiv preprint arXiv:2111.08276 (2021). [64] Yan Zeng, Xinsong Zhang, and Hang Li. 2021. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276 (2021). [65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid loss for language image pre-training. In ICCV. 1197511986. [66] Le Zhang, Rabiul Awal, and Aishwarya Agrawal. 2024. Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding. In CVPR. 1377413784. [67] Lu Zhang, Ke Yan, and Shouhong Ding. 2024. AlignCLIP: Align Multi Domains of Texts Input for CLIP models with Object-IoU Loss. In ACM MM. 10921100. [68] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 (2023). Decoupled Global-Local Alignment for Improving Compositional Understanding Preprint, April, 2025 Supplementary Material A.1 Training Details Details of the hyperparameter configurations are presented in Table 8. An asterisk (*) indicates that the batch size for our positive image-text pairs is 256, and each positive text is paired with four negative texts during training. Hyperparameters Batch size Optimizer Weight decay Adam 𝛽 Adam 𝜖 Learning rate Learning rate schedule Ema 𝛼 (𝜆1, 𝜆2, 𝜆3) Epochs Training GPUs 256* AdamW 0.1 (0.9,0.98) 1e-6 1e-6 cosine decay 0.9996 (0.1,0.1,0.005) 5 8V100 Table 8: Detailed hyper-parameters for training DeGLA. A.2 Detatils of General Understanding"
        },
        {
            "title": "Downstream Datasets",
            "content": "Zero-shot Classification. Following the previous work RWKVCLIP [14], we evaluate the zero-shot classification performance of the models on 11 datasets. The prompt used in zero-shot classification is presented in Table 13. Linear Probe. The datasets used for the linear probe evaluation are the same as those used for zero-shot classification. Details on each dataset and the corresponding evaluation metrics are provided in Table 9. Zero-shot Image-Text Retrieval. The detail of zero-shot imagetext retrieval benchmark is presented in Table 10. We evaluate retrieval performance on MS-COCO and Flickr30k following standard protocols. For MS-COCO, we use the 1K test set from [24], while Flickr30k employs 1,000 images with 5 captions each. In both datasets, we assess bidirectional retrieval (image-to-text and text-to-image) using pre-trained model embeddings without finetuning. Performance is measured by Recall@K (R@1, R@5, R@10), representing the percentage of correct matches in the top-K results. A.3 Details of Compositional Reasoning"
        },
        {
            "title": "Benchmarks",
            "content": "In Table 11, we summarize an overview of the three compositional reasoning benchmarks employed in this work. ARO. ARO [61] benchmark is used to probe the VLMs understanding of relations, attributes, and order in image-text data. Specifically, the Relation task involves swapping objects in the text, the Attribute task involves switching attributes in the text, and the Order task involves disrupting the sequence of the entire sentence. VALSE. The VASLE [45] benchmark assesses the compositional understanding of Visual Language Models (VLMs) across six dimensions: existence, plurality, counting, spatial relations, actions, and entity coreference. The existence subset employs single cue Dataset Classes Train size Test size Evaluation metric Food101 CIFAR10 CIFAR100 SUN397 Cars Aircraft DTD Pets Caltech101 Flowers ImageNet 102 10 100 397 196 100 47 37 101 102 1000 75,750 50,000 50,000 19,850 8,144 6,667 3,760 3,680 3,000 2,040 1,281, 25,250 10,000 10,000 19,850 8,041 3,333 1,880 3,669 5,677 6,149 50,000 accuracy accuracy accuracy accuracy accuracy mean per class accuracy mean per class mean-per-class mean per class accuracy Table 9: List of linear probe datasets with the data distribution and evaluation metrics. Dataset Test Images Captions per Image Evaluation Protocol MSCOCO Flickr30k 5,000 1,000 5 5 Image-to-Text & Text-to-Image Image-to-Text & Text-to-Image Table 10: Zero-shot image-text retrieval evaluation settings. to evaluate whether models can discern the presence or absence of specific entities in images. The plurality subset similarly uses single cue to determine if models can differentiate between singular and plural noun phrases depicted in images, such as exactly one flower vs. some flowers. The counting subset, which includes balanced, adversarial, and small-number scenarios, tests the models ability to confirm the accuracy of stated number of entities against those shown in the image. The spatial relations subset uses single cue to assess model performance in identifying different spatial relationships, creating foils by modifying the spatial preposition in the original caption. The actions subset, encompassing action replacement and actant swap settings, evaluates the models capability to (i) match the described action with the depicted action, and (ii) accurately identify the agents performing and receiving the action. Finally, the coreference subset examines the models proficiency in resolving pronominal references within multimodal contexts, covering both pronouns linked to noun phrases visually grounded in the image and deictic or image-level references. SugarCrepe. The SugarCrepe [45] benchmark encompasses three modalities: Replace, Swap, and Add, enhancing the assessment of visual-linguistic compositional understanding. In the Replace modality, hard negative is generated by substituting single atomic concept in positive caption, which then mismatches with the corresponding image. We classify the replacements according to their conceptual types: REPLACE-OBJ, REPLACE-ATT, and REPLACE-REL. The Swap modality produces hard negatives by interchanging two atomic concepts of the same type within positive caption, maintaining the original content. This includes SWAP-OBJ and SWAP-ATT categories, excluding relation swaps to avoid generating incoherent text. The Add modality creates hard negatives by introducing new atomic concept into positive caption, leading to discrepancy with the image. It includes ADD-OBJ and ADD-ATT, while relation insertions are excluded due to their implausibility. A.4 Details of Negative Caption Generation Following CE-CLIP [66], we adopt 2014 train split of COCO [3] as our base dataset and employ the LLM-driven negative caption Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. Figure 8: Details of the prompt utilized for generating high-quality examples and negative captions Benchmark #Examples Composionablity ARO [61] VALSE [45] SugarCrepe [18] 82695 8309 5948 Relation,Attribution,Order Existence, Plurality, Counting, Spatial Relations, Actions,Entity Coreference Relation,Attribution,Order,Semantic Substitution Table 11: Details of compositional reasoning benchmarks. Method Pretrain Finetune Hard negative data Finegrained loss Hard negative type Self distillation Rule-based Un-masking based LLM-based CLIP [49] NegCLIP [61] Structure-CLIP [20] CE-CLIP [66] DeGLA (ours) Table 12: Comparison of different methods. generation pipeline to generate compositionally-enhanced data for fine-tuning. The base dataset has 83k images and 414K captions. Through our augmentation process, we generate five hard negative captions per original caption, resulting in total of 2.07 million hard negative captions. Prompt Details. In Figure 8, we detail the prompts employed in this study. Notably, the prompts we designed are concise and effective, especially in ensuring that the generated sentences are hard negative rather than hard positive. To achieve this, we append the instruction ensuring that the rewritten sentences have significantly different or opposite meanings from the original sentences at the end of the prompt, which greatly enhanced the quality of both the examples and the final rewritten captions. Examples Generated by ChatGPT. In Figure 9, we present several representative examples generated by ChatGPT. These examples closely adhere to our rules and serve as high-quality exemplars to guide the large-scale generation of negative samples. Rewritten Captions. In Figure 10, we present examples of negative samples generated by LLaMA3.1-instruct-8B. These examples Decoupled Global-Local Alignment for Improving Compositional Understanding Preprint, April, 2025 Figure 9: Rewritten examples generated by ChatGPT. Figure 10: Hard negative examples generated by LLaMA3.1-8B-instruct. demonstrate the high quality of captions produced by the LLMdriven negative caption generation pipeline. The captions adhere accurately to our rewriting rules, ensuring that the generated sentences possess meanings that are distinct from or opposite to the original ones. This efficacy can be attributed to the high-quality examples showcased in Figure 9 and the meticulously designed prompts outlined in Figure 8. Preprint, April, 2025 Xiaoxing Hu, Kaicheng Yang et al. CIFAR 10 & CIFAR 100 photo of {label}. high contrast photo of {label}. photo of big {label}. low contrast photo of the {label}. photo of the small {label}. Food101 photo of {label}, type of food. Caltech101 photo of {label}. sketch of {label}. embroidered {label}. an origami {label}. doodle of {label}. sculpture of the {label}. rendition of the {label}. the plushie {label}. drawing of the {label}. Stanford Cars photo of {label}. photo of my dirty {label}. DTD photo of {label} texture. photo of the {label} texture. FGVC Aircraft photo of {label}, type of aircraft. Flowers102 photo of {label}, type of flower. Pets photo of {label}, type of pet. SUN39 photo of {label}. ImageNet bad photo of {label}. low resolution photo of the {label}. cropped photo of the {label}. bright photo of {label}. drawing of {label}. close-up photo of {label}. pixelated photo of the {label}. plastic {label}. photo of the {label}. photo of one {label}. the origami {label}. an origami {label}. photo of the clean {label}. photo of weird {label}. sketch of the {label}. jpeg corrupted photo of the {label}. photo of the small {label}. drawing of the {label}. dark photo of {label}. itap of my {label}. blurry photo of {label}. bad photo of {label}. photo of the {label}. high contrast photo of the {label}. photo of the big {label}. black and white photo of {label}. good photo of {label}. blurry photo of the {label}. bad photo of the {label}. low contrast photo of {label}. photo of small {label}. black and white photo of the {label}. good photo of the {label}. painting of {label}. tattoo of {label}. cartoon {label}. art of {label}. photo of the {label}. sketch of the {label}. the embroidered {label}. the origami {label}. doodle of the {label}. plastic {label}. toy {label}. {label} in video game. graffiti of {label}. painting of the {label}. tattoo of the {label}. the cartoon {label}. art of the {label}. sculpture of {label}. rendition of {label}. plushie {label}. drawing of {label}. the plastic {label}. the toy {label}. the {label} in video game. graffiti of the {label}. photo of the {label}. photo of my clean {label}. photo of my {label}. photo of my new {label}. love my {label}! photo of my old {label}. photo of {label} pattern. photo of the {label} pattern. photo of {label} thing. photo of the {label} thing. photo of {label} object. photo of the {label} object. photo of the {label}, type of aircraft. photo of the {label}. photo of many {label}. rendering of {label}. tattoo of {label}. photo of clean {label}. photo of my {label}. black and white photo of the {label}. sculpture of the {label}. photo of the dirty {label}. good photo of the {label}. doodle of {label}. the {label} in video game. low resolution photo of {label}. photo of large {label}. blurry photo of {label}. embroidered {label}. good photo of {label}. photo of the weird {label}. photo of the large {label}. itap of {label}. photo of cool {label}. sculpture of {label}. graffiti of {label}. the embroidered {label}. photo of dirty {label}. the plastic {label}. painting of the {label}. bright photo of the {label}. jpeg corrupted photo of {label}. rendering of the {label}. close-up photo of the {label}. sketch of {label}. the toy {label}. rendition of {label}. cartoon {label}. pixelated photo of {label}. plushie {label}. the cartoon {label}. black and white photo of {label}. graffiti of the {label}. photo of small {label}. photo of the hard to see {label}. bad photo of the {label}. photo of hard to see {label}. dark photo of the {label}. photo of the cool {label}. painting of {label}. cropped photo of {label}. blurry photo of the {label}. {label} in video game. photo of {label}. doodle of the {label}. rendition of the {label}. photo of nice {label}. art of {label}. itap of the {label}. photo of the nice {label}. art of the {label}. the plushie {label}. toy {label}. tattoo of the {label}. Table 13: Full list of prompts to evaluate the performance of zero-shot classification on 11 visual recognition datasets."
        }
    ],
    "affiliations": [
        "Beijing Institute of Technology, Beijing, China",
        "DeepGlint, Beijing, China",
        "Zhejiang University, Zhejiang Province, China"
    ]
}