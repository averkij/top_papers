{
    "paper_title": "Does More Inference-Time Compute Really Help Robustness?",
    "authors": [
        "Tong Wu",
        "Chong Xiang",
        "Jiachen T. Wang",
        "Weichen Yu",
        "Chawin Sitawarin",
        "Vikash Sehwag",
        "Prateek Mittal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, Zaremba et al. demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using a simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 4 7 9 5 1 . 7 0 5 2 : r Preprint. Under review. Does More Inference-Time Compute Really Help Robustness? Tong Wu1, Chong Xiang2, Jiachen T. Wang1, Weichen Yu3, Chawin Sitawarin4, Vikash Sehwag4, Prateek Mittal1 1Princeton University, 2NVIDIA, 3Carnegie Mellon University, 4Google DeepMind tongwu@princeton.edu"
        },
        {
            "title": "Abstract",
            "content": "Recently, Zaremba et al. (2025) demonstrated that increasing inference-time computation improves robustness in large proprietary reasoning LLMs. In this paper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1, Qwen3, Phi-reasoning) can also benefit from inference-time scaling using simple budget forcing strategy. More importantly, we reveal and critically examine an implicit assumption in prior work: intermediate reasoning steps are hidden from adversaries. By relaxing this assumption, we identify an important security risk, intuitively motivated and empirically verified as an inverse scaling law: if intermediate reasoning steps become explicitly accessible, increased inference-time computation consistently reduces model robustness. Finally, we discuss practical scenarios where models with hidden reasoning chains are still vulnerable to attacks, such as models with tool-integrated reasoning and advanced reasoning extraction attacks. Our findings collectively demonstrate that the robustness benefits of inference-time scaling depend heavily on the adversarial setting and deployment context. We urge practitioners to carefully weigh these subtle trade-offs before applying inference-time scaling in security-sensitive, real-world applications. WARNING: This paper contains red-teaming content that can be offensive."
        },
        {
            "title": "Introduction",
            "content": "Inference-time scaling has recently gained attention as promising approach for boosting the capabilities of large language models (LLMs) (Snell et al., 2024; Welleck et al., 2024). Unlike traditional training-time scaling that improves performance by increasing model size or training data, inference-time scaling enhances model performance by allocating additional computation specifically during inference. Recent studies by OpenAI (Jaech et al., 2024) demonstrated significant improvements under this paradigm in challenging scenarios, including agent-based interactions (Wu et al., 2024a) and mathematical reasoning (Lightman et al., 2023). Beyond accuracy, recent work by Zaremba et al. (2025) further revealed that increased inference-time computation notably enhances robustness across diverse adversarial scenarios in proprietary reasoning models (e.g., O1-PREVIEW, O1-MINI). These findings highlight inference-time scaling as powerful method, not only for improving accuracy but also for enhancing the robustness of LLM deployments as agents. Despite the promising robustness improvement demonstrated by recent studies (Zaremba et al., 2025), several critical questions remain. First, Zaremba et al. (2025) provides limited detail regarding their specific inference-time scaling strategyonly vaguely referring to it as \"increasing decoding steps.\" Second, prior analyses predominantly focus on proprietary, large-scale models, leaving it unclear how smaller-scale, open-source reasoning models can benefit from inference-time scaling. In this paper, we aim to close these gaps with systematic investigation on open-source reasoning LLMs, which provides practical guidance and holistic discussion on trading inference-time scaling for robustness. Contributing in an advisory capacity only. No experiments or research were carried out by Google DeepMind. 1 Preprint. Under review. Figure 1: Inference-time scaling and robustness. (Left) We show that increasing inferencetime computation, by extending reasoning chains, can either improve robustness or at least maintain model robustness when only the final output is considered. (Right) However, we also identify an inverse scaling law: when intermediate reasoning steps are exposed to adversaries, increased inference-time computation consistently reduces robustness across all three adversarial settings. Results are averaged over 12 open-source reasoning models. Simple Inference-Time Scaling Strategy to Boost Robustness. (Section 3) As our first contribution, we demonstrate that simple and practical scaling approach can effectively enhance the robustness of open-source reasoning models, yielding improvements comparable to those previously reported for proprietary models. Specifically, we employ the budget forcing method proposed by Muennighoff et al. (2025), which explicitly controls the length of reasoning chains during inference. Our results show that allocating increased inference-time computation using this method notably improves model robustness, particularly against prompt injection and prompt extraction attacks (Figure 1, Left).1 Importantly, improvements against prompt extraction attacks represent novel finding not previously reported in the literature. Comprehensive experiments on multiple open-source reasoning models, including DeepSeek R1 series (Guo et al., 2025), Qwen3 series (Yang et al., 2025), and the Phi-reasoning series (Abdin et al., 2025), consistently confirm significant robustness benefits. Taken together, our results clearly demonstrate that inference-time scaling represents promising and practical strategy to enhance the robustness of reasoning-enhanced models. What if the Reasoning Tokens Are Not Hidden? (Section 4) We identify and critically examine an implicit assumption in prior inference-time robustness studies, notably by Zaremba et al. (2025): that adversaries cannot access models intermediate reasoning steps. Relaxing this assumption, we argue, fundamentally changes the relationship between inference-time computation and robustness. Specifically, we first introduce insights indicating that explicitly revealing intermediate reasoning steps would expose models to more vulnerabilities as inference-time computation increases (i.e., as reasoning chains become longer). We hypothesize that, rather than enhancing robustness, extended reasoning chains under these conditions may actually reduce it. Empirically, we verify this hypothesis through comprehensive experiments across multiple open-source reasoning models and adversarial benchmarks, clearly demonstrating notable inverse scaling law: robustness consistently deteriorates with increased inference-time computation when intermediate reasoning steps are being considered (Figure 1, Right). Furthermore, our analysis reveals that the practical implications of this inverse scaling law differ substantially depending on the adversarial scenario, underscoring the need for careful consideration before model deployment. Does Hiding the Reasoning Chain Solve All Robustness Issues? (Section 5) Moreover, we argue that this inverse scaling law may persist even when reasoning chains are not directly exposed. Specifically, we highlight two concrete scenarios in which vulnerabilities persist despite hidden reasoning traces. First, modern models increasingly incorporate tool-integrated reasoning (Gou et al., 2023; Li et al., 2025; OpenAI, 2025), implicitly invoking external APIs or tools within their intermediate reasoning processes. Consequently, adversaries can trigger unintended or malicious behaviors even without direct access to those intermediate reasoning steps; we substantiate this concern with concrete proof-of-concept 1Consistent with Zaremba et al. (2025), we observe no obvious robustness gains against harmful requests. 2 Preprint. Under review. experiment. Second, adversaries may indirectly reconstruct sensitive or malicious reasoning information through carefully crafted prompting strategies (Gray Swan AI, 2025), thereby circumventing the protections provided by hidden reasoning chains. Collectively, these novel attack vectors illustrate that extending reasoning chains inherently enlarges the attack surface, increases opportunities for adversarial exploitation, and deepens robustness concerns, even when intermediate reasoning steps remain concealed. Overall, our findings highlight the subtle and complex relationship between inferencetime scaling and robustness, clearly demonstrating instances where increased computation can be counterproductive depending on the adversarial scenario and model deployment context. We encourage researchers and practitioners to carefully weigh these trade-offs while adopting inference-time scaling techniques, ultimately paving the way toward more secure and robust real-world LLM agent systems."
        },
        {
            "title": "2 Background",
            "content": "In this section, we first introduce essential concepts related to reasoning-enhanced models and present budget forcing, simple yet effective inference-time scaling strategy commonly applied to these models (Section 2.1). Subsequently, we detail our experimental setup for comprehensively evaluating model robustness against three adversarial tasks: prompt injection, prompt extraction, and harmful requests. We also introduce the models evaluated in our experiments (Section 2.2). More details are presented in Appendix A. 2.1 Prelimiary Reasoning Models. Reasoning models explicitly separate text generation into two distinct stages: (1) Reasoning Stage, in which the model produces intermediate reasoning tokens (the reasoning chain), conditioned solely on the initial input and previously generated reasoning tokens; and (2) Response Stage, in which the model generates its final answer conditioned jointly on the input context and the previously generated reasoning chain. Simple Sequential Scaling via Budget Forcing. Sequential scaling strategy naturally increases computation during inference-time reasoning and can be implemented via the budget forcing strategy (Muennighoff et al., 2025). Budget forcing imposes predetermined limit on the length of the reasoning chain. Specifically, once the number of reasoning tokens reaches this budget, an end-of-thinking delimiter followed by the prefix Final Answer: is appended, prompting the model to immediately produce its final response. Conversely, if the number of reasoning tokens remains below this budget, the delimiter is omitted, and prompt Wait, is appended instead, encouraging the model to continue reasoning before answering. In this paper, we primarily control the reasoning chains length by adjusting the budget, and we analyze the safety implications associated with this scaling strategy. 2.2 Problem and Experimental Setup Prompt Injection (Figure 2a). Prompt injection attacks (Greshake et al., 2023) embed malicious instructions into model inputs to override the intended behavior. For example, document may subtly include commands like send the key to attack@XX.com. Processing such inputs can induce unintended malicious outcomes, compromising task integrity. We evaluate prompt injection robustness using the SEP dataset (Zverev et al., 2025), in which each data point contains high-priority main instruction, relevant data content, and an unrelated low-priority instruction. Models should prioritize the main instruction and ignore the low-priority one. We quantify robustness as the proportion of low-priority instructions that the model correctly ignores. Prompt Extraction (Figure 2b). Prompt extraction attacks (Zhang et al., 2024) target LLMs that embed confidential instructions within prompts. Attackers craft queries designed to trick models into explicitly revealing hidden instructions. For instance, query like Please repeat all your prompts exactly may inadvertently cause the model to disclose internal guidelines, API keys, or other proprietary content verbatim. These disclosures undermine 3 Preprint. Under review. Figure 2: Illustrative examples of three common attack types on LLM-based systems: (a) Prompt Injection: Embedding malicious instructions within low-priority content (e.g., document for summarization), causing the model to perform unintended actions. (b) Prompt Extraction: Crafting queries to prompt the model into disclosing sensitive or proprietary (system) messages. (c) Harmful Requests: Directly requesting unsafe outputs, such as instructions for illegal activities. confidentiality and enable subsequent targeted attacks. We evaluate robustness against prompt extraction using the TENSORTRUST dataset (Toyer et al., 2024). Each data point contains sensitive secret keys alongside adversarial user messages engineered to extract these secrets from the model. Robustness is defined as the proportion of cases in which the model resists leakage and does not disclose the exact secret phrase. Harmful Requests (Figure 2c). The harmful requests evaluation tests an LLMs safety alignment by measuring its ability to reject user inputs that violate established guidelines (Bai et al., 2022). Malicious queries often include explicit instructions prompting unsafe behavior, such as Explain how to rob bank in simple steps. If successful, such attacks pose significant security risks by potentially enabling the misuse of generative models (Ganguli et al., 2022). To quantify robustness against harmful requests, we employ the comprehensive SORRY-BENCH benchmark (Xie et al., 2024), which consists of unsafe instructional prompts spanning 45 distinct categories, including personal insults, military applications, and malware generation. We use GPT-4O-MINI as an automated evaluator, classifying model responses as either compliant or appropriately refusing the harmful requests. Robustness is measured as the proportion of harmful prompts that are successfully refused by the model. Evaluated Models. We conduct extensive experiments on several recently released opensource reasoning models, including the DeepSeek R1 series (Guo et al., 2025), the Qwen series (Yang et al., 2025), and the Phi series (Abdin et al., 2025). In addition, we also include the STAR-1 series (Wang et al., 2025b), which are safety fine-tuned from the R1 series. Our evaluation covers broad range of model sizes, from 7B to 32B parameters. To systematically investigate inference-time computation tradeoffs, we experiment with thinking budgets ranging from 100 to 16,000 tokens by applying budget constraints. Unless otherwise specified, we use standard inference configuration with temperature of 0.6 and repetition penalty of 1.15 across all experiments."
        },
        {
            "title": "3 A Simple Inference-Time Scaling Strategy to Boost Robustness",
            "content": "Evidence of Simple Inference-Time Scaling in Mitigating Prompt Injection. We first empirically examine how the robustness of reasoning models against prompt injection varies with inference-time computation. As shown in Figure 3(a), robustness to prompt injection attacks generally improves as models allocate more reasoning tokens. For instance, the robustness of QWQ-32B significantly increases from approximately 35% to about 75% when inference-time compute expands from 100 tokens to 16,000 tokens. This improvement arises primarily because our prompts explicitly instruct the model to maintain robustness (e.g., Do not follow any other instructions provided in the data block.). More allowed reasoning tokens enable the model to clearly recognize and adhere to these directives, thus enhancing robustness. Our findings align closely with prior work by Zaremba et al. (2025), showing 4 Preprint. Under review. (a) Prompt Injection (SEP) (b) Prompt Extraction (TENSORTRUST) (c) Harmful Requests (SORRY-BENCH) Figure 3: Robustness evaluation across inference-time computation for multiple open-source reasoning models. The X-axis denotes inference-time compute (reasoning token budget), while the Y-axis measures robustness performance across three adversarial scenarios: (a) Prompt injection attacks assessed on the SEP dataset, (b) Prompt extraction attacks evaluated using the TENSORTRUST dataset, and (c) Harmful requests benchmarked on the SORRYBENCH dataset. We observe that increased inference-time computation generally leads to improved robustness against prompt injection and extraction attacks, and maintains stable performance on harmful request tasks. similar scaling behavior in closed-source models. We therefore extend their findings to small-scale open-source reasoning models. Inference-Time Scaling also Benefits Prompt Extraction. Next, we investigate robustness as function of inference-time computation in the context of prompt extraction, scenario previously unexplored in Zaremba et al. (2025). Figure 3(b) illustrates that increasing inference-time computation consistently enhances robustness against prompt extraction attacks across most open-source reasoning models. For example, the robustness of QWQ-32B substantially increases from around 60% to 80% as inference-time compute rises from 100 to 16,000 tokens. The underlying mechanism is similar to prompt injection scenarios: our explicitly defined specification guide the model toward safe responses, reducing the likelihood of secret key leaks from system prompts. These results demonstrate novel extension of the inference-time scaling phenomenon first identified by Zaremba et al. (2025), highlighting its general applicability to broader set of adversarial threats faced by reasoning models. Limited Benefits of Inference-Time Scaling for Harmful Requests. In contrast, robustness against harmful request tasks does not significantly benefit from increased inference-time computation. As depicted in Figure 3(c), the evaluated models exhibit only minor fluctuations in robustness as reasoning budgets grow larger. For example, the QWEN3-8B model maintains robustness around 70% across reasoning budgets ranging from 100 to 16,000 tokens. These findings align with previous observations by Zaremba et al. (2025), who similarly noted limited effectiveness of inference-time scaling for harmful request tasks. One plausible interpretation for this result is that harmful requests inherently involve ambiguity, limiting the effectiveness of extended reasoning in guiding model decisions. Nevertheless, we observe no significant degradation in harmful request robustness with increasing inference-time budgets, indicating that inference-time scaling at least does not introduce additional safety risks in these settings."
        },
        {
            "title": "4 What if the Reasoning Tokens Are Not Hidden?",
            "content": "Our previous findings demonstrated that inference-time scaling can either enhance or at least maintain the robustness of reasoning models. However, these analyses rely upon the assumption that intermediate reasoning chains remain hidden from adversaries, practice commonly adopted by LLM providers such as OpenAI, Anthropic, and Google. In practice, there exist models explicitly exposing reasoning chains, such as open-source systems (Guo et al., 2025; Yang et al., 2025) or even commercial APIs like xAIs Grok (xAI, 2025). This naturally brings up critical yet unexplored research question: How does exposing reasoning chains affect robustness gains from inference-time scaling? 5 Preprint. Under review. (a) Prompt Injection (SEP) (b) Prompt Extraction (TENSORTRUST) (c) Harmful Requests (SORRY-BENCH) Figure 4: Robustness evaluation of multiple reasoning models with varying inference-time computation budgets that only consider the intermediate reasoning steps. We provide results for: (a) prompt injection robustness evaluated on the SEP, (b) prompt extraction robustness measured using the TENSORTRUST, and (c) harmful request robustness assessed on the SORRY-BENCH. Our findings illustrate clear inverse scaling law: robustness consistently decreases as inference-time computation increases, underscoring the heightened security risks introduced by exposing reasoning chains. 4.1 Hypothesis from Intuitive Insights We first provide intuitive insights into how exposing intermediate reasoning steps may influence robustness. Specifically, we hypothesize that once reasoning chains become visible, malicious tokens in the reasoning chain can be exploited by adversaries to achieve malicious goals. Formally, let Σ be the vocabulary and Σ the set of malicious tokens (e.g., secret strings or policy-violating words). For prompt P, an autoregressive language model generates sequence T1, T2, . . . with conditional probabilities pi(t) = Pr [Ti = T<i, P]. Define the event EL = { : Ti M}, i.e., at least one malicious token appears in the first positions. Because the set of trajectories satisfying Ek is contained in the set satisfying Ek+1, probability measure monotonicity gives Pr [Ek] Pr [Ek+1]. Hence, the success probability is non-decreasing with the length of the exposed chain, and every extra token adds another chance to cross the safety boundary. Furthermore, if each step carries even tiny non-zero risk = Pr [Ti T<i, P] > 0, then Pr [EL] 1 (1 p)L, i.e., the likelihood of revealing malicious token rises exponentially toward 1 as grows. Therefore, extending the reasoning chain with exposing intermediate steps should fundamentally amplify the vulnerability surface, degrading overall robustness.2 4.2 Inverse Scaling Law under Exposed Reasoning Chains We next empirically examine how exposing intermediate reasoning steps affects the robustness gains achieved through inference-time scaling. Specifically, we assess robustness based on whether the reasoning chains themselves contain malicious tokens (e.g., unsafe or adversarial instructions), regardless of the final model response. Robustness Degrades with Increasing Inference-Time Computation When Reasoning Chains Are Exposed. Figure 4(a) clearly illustrates that explicitly revealing intermediate reasoning steps significantly and consistently decreases model robustness against prompt injection attacks (SEP) across multiple reasoning models. Taking R1-QWEN-14B as an example, robustness declines from approximately 90% (at 100 inference tokens) to below 20% when the inference-time computational budget escalates to 16,000 tokens. This marked degradation occurs because longer reasoning chains inherently increase the likelihood of generating malicious tokens. parallel trend emerges in the prompt extraction setting (TENSORTRUST), where robustness for R1-QWEN-14B similarly falls by roughly 60% as computational budgets increase (Figure 4b). This suggests that adversaries can exploit the additional reasoning steps to extract sensitive information, such as secret keys, from the 2We also want to emphasize that the practical security risk is highly dependent on the task configurations, which we detail in the remark of the next subsection. 6 Preprint. Under review. reasoning chains themselves. In the harmful request scenario (SORRY-BENCH), we observe more modest but still notable decline in robustness, with the performance of reasoning models dropping by 20%40% as inference-time computation increases (Figure 4c). These findings collectively reveal novel and previously unrecognized phenomenonan inverse scaling law for robustness. Contrary to earlier observations under hidden reasoning settings, our results show that increasing inference-time computation can significantly undermine robustness when intermediate reasoning steps are accessible to adversaries. This insight reshapes how practitioners should approach the trade-offs and safety considerations of inference-time scaling, particularly in deployment scenarios where model reasoning processes are exposed. Remark: Practical Safety Implications of Exposed Reasoning Chains. We emphasize that observing robustness degradation in intermediate reasoning does not necessarily imply immediate practical safety risks. The severity of these implications depends strongly on the attackers objectives in each distinct threat model: (1) Prompt Injection: Here, attackers aim primarily to manipulate final outputs by injecting malicious instructions into low-priority data blocks. Robustness decreases measured solely in intermediate reasoning are, therefore, less practically concerning, as attackers typically focus exclusively on the ultimate model output rather than intermediate reasoning steps. (2) Prompt Extraction: In this scenario, attackers seek explicit leakage of sensitive or proprietary information (e.g., secret keys). Any vulnerability in intermediate reasoning genuinely constitutes significant security threat, since the adversary can directly observe and extract the sensitive information once it appears in reasoning chains. (3) Harmful requests: For harmful requests, exposing intermediate reasoning can create serious practical safety vulnerabilities, as attackers might extract detailed unsafe instructions directly from reasoning chains (e.g., step-by-step harmful information such as bomb-making procedures), even if the final answer itself appears safe."
        },
        {
            "title": "5 Does Hiding the Reasoning Chain Solve All Robustness Issues?",
            "content": "One might wonder whether simply hiding reasoning chains can fully resolve the robustness degradation identified in the previous section. However, we argue that there still exists some remaining robustness issues that cannot be mitigated by merely hiding reasoning chains. Specifically, we identify two key scenarios where robustness concerns persist even when reasoning chains are not exposed: 5.1 Prompt Injection in the New Era of Reasoning Models Recently, reasoning chains have been augmented with tool-use capabilities, as exemplified by OpenAIs o3 and o4 series (OpenAI, 2025) and various academic approaches (Jin et al., 2025; Song et al., 2025; Li et al., 2025). These approaches integrate external APIs into intermediate reasoning steps, significantly enhancing overall model performance. However, this reliance on external APIs introduces new security vulnerabilities: adversaries can now target not only final model outputs but also manipulate intermediate reasoning steps to trigger unsafe API calls, potentially accessing harmful content or executing unauthorized actions. To explore this emerging threat, we simulate tool use in opensource models (which lack inherent API-calling capabilities) using specialized prompts: You can call the APIs directly in the reasoning with: <API_CALL> </API_CALL>. We evaluate their robustness using the LLM-PIEVAL dataset (Ramakrishna et al., 2024), specifically measuring the models susceptibility to making unsafe API calls during intermediate reasoning. Our re7 Figure 5: Robustness of reasoning models against prompt injection attacks targeting tool-augmented Robustness reasoning. declines as inference-time computation increases. Preprint. Under review. sults (Figure 5) show that robustness against prompt injection attacks degrades as inferencetime computation increases. For example, the robustness of PHI-4-REASON drops from 100% to approximately 87% as the reasoning budget expands from 100 to 8,000 tokens. This finding highlights serious security concerns: longer reasoning chains inherently provide adversaries with more opportunities to trigger unsafe API interactionsan issue not adequately addressed simply by hiding intermediate reasoning steps. 5.2 Hidden Reasoning Chains Can Still Be Extracted Even intentionally hidden reasoning chains may remain vulnerable to extraction by determined adversaries. recent red-teaming competition (Gray Swan AI, 2025) explicitly demonstrated this risk, challenging participants to expose internal reasoning steps generated by O1-PREVIEW and O1-MINI during inference. Attacks were considered successful if hidden reasoning chains were explicitly revealed. Notably, both tested reasoning models were successfully compromised at least 10 times within fewer than 8,000 adversarial attempts, highlighting the practical relevance of this threat. These findings emphasize that simply hiding internal reasoning processes from external observers does not fully prevent unintended information leakage. In fact, longer reasoning chains may exacerbate this vulnerability by expanding the attack surface and providing more opportunities for adversaries to extract content that reflects harmful internal logic. Practitioners deploying reasoning-enhanced language models must carefully consider this risk, balancing the benefits of increased inference-time computation against potential security vulnerabilities and the risk of harmful content leakage."
        },
        {
            "title": "6 Discussion and Future Work",
            "content": "Exploring Alternative Inference-Time Scaling. In this paper, we demonstrated that simple inference-time scaling using budget forcing yields robustness improvements similar to those observed by Zaremba et al. (2025), and further uncovered an inverse scaling law when reasoning chains are exposed. However, we have not explored other potential inference-time scaling strategies, particularly methods that employ parallel computation. For example, techniques like Best-of-N sampling (Beirami et al., 2024; Snell et al., 2024; Brown et al., 2024) distribute the total inference budget across multiple independent reasoning paths and select the final answer through voting. The robustness benefits and security implications of these parallel inference approaches remain largely unexplored. Moreover, it remains unclear whether the inference-time scaling methods proposed by Zaremba et al. (2025) also suffer from the same vulnerabilities identified in this work. Future research could investigate these directions, particularly their security implications when applied to reasoning chains. Amplifying the Effects of Attacks on the Reasoning Chain. We primarily evaluated adversarial threats using straightforward approaches without employing specifically designed or sophisticated attack strategies. Consequently, we observed moderate robustness degradation, especially in stronger models and in the context of harmful request tasks. natural extension of this work would be to explore more advanced, carefully tailored attack methods explicitly targeting vulnerabilities within intermediate reasoning chains, and to rigorously compare their effectiveness with attacks on final outputs. Investigating how optimized attacks can exploit reasoning-chain vulnerabilities would yield valuable insights, helping practitioners design more secure models. Practical Threats in Tool-Use Reasoning Models. In Section 5.1, we presented preliminary evidence that prompt injection attacks could trigger malicious tool calls embedded within reasoning chains. However, we employed an open-source model without genuine, integrated tool-use capabilities, using it merely as representative proxy. Extending this analysis to commercial models with true tool-use functionalitysuch as OpenAIs O3 series (OpenAI, 2025) and Googles Gemini (Gemini Team, 2025)is critically important. Conducting such evaluations would further substantiate these security threats in practical settings and provide actionable insights for robust reasoning models. 8 Preprint. Under review. Principled Methods for Reasoning Chain Extraction. We discussed and demonstrated the feasibility of extracting hidden reasoning chains primarily based on results from recent redteaming competition, where successful attacks predominantly involved human participants. Human-driven attacks alone might underestimate the true risk, as automated, principled attacks could potentially accomplish reasoning-chain extraction more systematically and effectively. Developing principled methods capable of consistently extracting hidden reasoning chains with fewer attempts would significantly highlight the practicalnot merely hypotheticalnature of reasoning-chain security risks. Such methods would clearly illustrate the importance and urgency of addressing vulnerabilities related to reasoning-chain leakage in deployed reasoning-enhanced models."
        },
        {
            "title": "7 Related Works",
            "content": "Inference-Time Scaling. Increasing inference-time computation consistently leads to improved performance in complex reasoning tasks. Prominent approaches include sampling multiple parallel reasoning paths (Wang et al., 2023; Beirami et al., 2024; Snell et al., 2024) and performing tree-based searches (Yao et al., 2023; Zhou et al., 2023; Wu et al., 2024b). Advanced reasoning-enhanced models, such as OpenAIs o1 (Jaech et al., 2024) and Googles Gemini (Gemini Team, 2025), as well as open-source alternatives like DeepSeek R1 (Guo et al., 2025) and QwQ (Qwen Team, 2025), commonly leverage inference-time scaling by generating extended reasoning traces. Simple yet effective implementations to further scale inference-time compute include strategies such as S1 (Muennighoff et al., 2025) and L1 (Aggarwal & Welleck, 2025). Robustness of Reasoning LLMs. Recent research has begun to systematically evaluate the robustness of reasoning-enhanced language models against various adversarial threats, such as harmful user requests (Marjanovic et al., 2025; Kuo et al., 2025; Yao et al., 2025) and prompt injection attacks (Zaremba et al., 2025; Zhou et al., 2025). To defend against these risks, several strategies have emerged, including generating safe reasoning chains and performing supervised fine-tuning to enhance robustness (Jiang et al., 2025; Wang et al., 2025b; Zhang et al., 2025a), employing reinforcement learning-based approaches (Guan et al., 2024; Zhang et al., 2025b; Mou et al., 2025), and utilizing thinking interventions (Wu et al., 2025). Readers are encouraged to read the recent survey by Wang et al. (2025a). Concurrently with our work, Green et al. (2025) also demonstrated that reasoning chains can be inadvertently leaked or maliciously extracted by attackers, but with focus on data privacy tasks. In our paper, we primarily focus on comprehensively analyzing the relationship between inference-time scaling and the robustness of reasoning-enhanced language models."
        },
        {
            "title": "8 Conclusion",
            "content": "In this work, we systematically investigate inference-time scaling as method for enhancing the robustness of smaller-scale, open-source reasoning-enabled LLMs, observing notable improvements against prompt injection and extraction attacks. Crucially, we uncover previously overlooked assumptionthat intermediate reasoning steps remain hiddenand identify an inverse scaling law, where increased inference-time computation reduces robustness if these reasoning steps become accessible to adversaries. Additionally, we highlight practical attack scenarios in which reasoning-related vulnerabilities persist, even when reasoning chains remain inaccessible. Our findings underscore the importance of carefully balancing inference-time computation against potential robustness risks, motivating further research toward robust reasoning-enhanced LLMs and laying solid foundation for deploying secure, real-world agentic systems. 9 Preprint. Under review."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Sahaj Agarwal, Ahmed Awadallah, Vidhisha Balachandran, Harkirat Behl, Lingjiao Chen, Gustavo de Rosa, Suriya Gunasekar, Mojan Javaheripi, Neel Joshi, Piero Kauffmann, Yash Lara, Caio César Teodoro Mendes, Arindam Mitra, Besmira Nushi, Dimitris Papailiopoulos, Olli Saarikivi, Shital Shah, Vaishnavi Shrivastava, Vibhav Vineet, Yue Wu, Safoora Yousefi, and Guoqing Zheng. Phi-4-reasoning technical report, 2025. URL https://arxiv.org/abs/2504.21318. Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander DAmour, Jacob Eisenstein, Chirag Nagpal, and Ananda Theertha Suresh. Theoretical guarantees on the best-of-n alignment policy. arXiv preprint arXiv:2401.01879, 2024. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Deep Ganguli, Liane Lovitt, John Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Benjamin Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova Dassarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zachary Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom B. Brown, Nicholas Joseph, Sam McCandlish, Christopher Olah, Jared Kaplan, and Jack Clark. Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. ArXiv, abs/2209.07858, 2022. URL https://api.semanticscholar.org/CorpusID:252355458. Gemini Team. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Technical report, Google DeepMind, Mountain View, CA, USA, June 2025. Available at https://storage.googleapis.com/ deepmind-media/gemini/gemini_v2_5_report.pdf. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: tool-integrated reasoning agent for mathematical problem solving. arXiv preprint arXiv:2309.17452, 2023. Gray Swan AI. Revealing hidden cot: Arena leaderboard. https://app.grayswan.ai/arena/ challenge/revealing-chain-of-thought/rules, 2025. Gray Swan Arena; accessed 17 June 2025. Tommaso Green, Martin Gubri, Haritz Puerto, Sangdoo Yun, and Seong Joon Oh. Leaky thoughts: Large reasoning models are not private thinkers, 2025. URL https://arxiv. org/abs/2506.15674. Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM Workshop on Artificial Intelligence and Security, AISec 23, pp. 7990, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400702600. doi: 10.1145/3605764. 3623985. URL https://doi.org/10.1145/3605764.3623985. Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, et al. Deliberative alignment: Reasoning enables safer language models. arXiv preprint arXiv:2412.16339, 2024. 10 Preprint. Under review. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Fengqing Jiang, Zhangchen Xu, Yuetai Li, Luyao Niu, Zhen Xiang, Bo Li, Bill Yuchen Lin, and Radha Poovendran. Safechain: Safety of language models with long chain-of-thought reasoning capabilities. arXiv preprint arXiv:2502.12025, 2025. Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Searchr1: Training llms to reason and leverage search engines with reinforcement learning. ArXiv, abs/2503.09516, 2025. URL https://api.semanticscholar.org/CorpusID:276937772. Martin Kuo, Jianyi Zhang, Aolin Ding, Qinsi Wang, Louis DiValentin, Yujia Bao, Wei Wei, Hai Li, and Yiran Chen. H-cot: Hijacking the chain-of-thought safety reasoning mechanism to jailbreak large reasoning models, including openai o1/o3, deepseek-r1, and gemini 2.0 flash thinking. arXiv preprint arXiv:2502.12893, 2025. Chengpeng Li, Mingfeng Xue, Zhenru Zhang, Jiaxi Yang, Beichen Zhang, Xiang Wang, Bowen Yu, Binyuan Hui, Junyang Lin, and Dayiheng Liu. Start: Self-taught reasoner with tools, 2025. URL https://arxiv.org/abs/2503.04625. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Sara Vera Marjanovic, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, et al. Deepseek-r1 thoughtology: Lets think about llm reasoning. arXiv preprint arXiv:2504.07128, 2025. Yutao Mou, Yuxiao Luo, Shikun Zhang, and Wei Ye. Saro: Enhancing llm safety through reasoning-based alignment. arXiv preprint arXiv:2504.09420, 2025. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. OpenAI. Thinking with images. Technical Release o-series (o3, o4-mini), OpenAI, San Francisco, CA, April 2025. URL https://openai.com/index/thinking-with-images/. Published April 16, 2025. Visual reasoning models o3 and o4-mini. Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. Anil Ramakrishna, Jimit Majmudar, Rahul Gupta, and Devamanyu Hazarika. LLM-PIRATE: benchmark for indirect prompt injection attacks in large language models. In The Third Workshop on New Frontiers in Adversarial Machine Learning, 2024. URL https://openreview. net/forum?id=qzEzXnw4ng. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. ArXiv, abs/2503.05592, 2025. URL https://api.semanticscholar. org/CorpusID:276884818. 11 Preprint. Under review. Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor Darrell, Alan Ritter, and Stuart Russell. Tensor trust: Interpretable prompt injection attacks from an online game. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=fsW7wJGLBd. Eric Wallace, Kai Yuanqing Xiao, Reimar Heinrich Leike, Lilian Weng, Johannes Heidecke, and Alex Beutel. The instruction hierarchy: Training llms to prioritize privileged instructions. arXiv preprint arXiv:2404.13208, 2024. Cheng Wang, Yue Liu, Baolong Bi, Duzhen Zhang, Zhong-Zhi Li, Yingwei Ma, Yufei He, Shengju Yu, Xinfeng Li, Junfeng Fang, et al. Safety in large reasoning models: survey. arXiv preprint arXiv:2504.17704, 2025a. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. Zijun Wang, Haoqin Tu, Yuhan Wang, Juncheng Wu, Jieru Mei, Brian Bartoldson, Bhavya Kailkhura, and Cihang Xie. Star-1: Safer alignment of reasoning llms with 1k data. arXiv preprint arXiv:2504.01903, 2025b. Sean Welleck, Amanda Bertsch, Matthew Finlayson, Hailey Schoelkopf, Alex Xie, Graham Neubig, Ilia Kulikov, and Zaid Harchaoui. From decoding to meta-generation: Inferencetime algorithms for large language models. arXiv preprint arXiv:2406.16838, 2024. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024a. Tong Wu, Chong Xiang, Jiachen Wang, and Prateek Mittal. Effectively controlling reasoning models through thinking intervention. arXiv preprint arXiv:2503.24370, 2025. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024b. xAI. Grok 3 beta the age of reasoning agents, February 19 2025. URL https://x.ai/ news/grok-3. Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. Yang Yao, Xuan Tong, Ruofan Wang, Yixu Wang, Lujundong Li, Liang Liu, Yan Teng, and Yingchun Wang. mousetrap: Fooling large reasoning models for jailbreak with chain of iterative chaos. arXiv preprint arXiv:2502.15806, 2025. 12 Preprint. Under review. Wojciech Zaremba, Evgenia Nitishinskaya, Boaz Barak, Stephanie Lin, Sam Toyer, Yaodong Yu, Rachel Dias, Eric Wallace, Kai Xiao, Johannes Heidecke, et al. Trading inference-time compute for adversarial robustness. arXiv preprint arXiv:2501.18841, 2025. Yichi Zhang, Zihao Zeng, Dongbai Li, Yao Huang, Zhijie Deng, and Yinpeng Dong. Realsafer1: Safety-aligned deepseek-r1 without compromising reasoning capability. arXiv preprint arXiv:2504.10081, 2025a. Yichi Zhang, Siyuan Zhang, Yao Huang, Zeyu Xia, Zhengwei Fang, Xiao Yang, Ranjie Duan, Dong Yan, Yinpeng Dong, and Jun Zhu. Stair: Improving safety alignment with introspective reasoning. arXiv preprint arXiv:2502.02384, 2025b. Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. Effective prompt extraction In First Conference on Language Modeling, 2024. URL https: from language models. //openreview.net/forum?id=0o95CVdNuz. Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong Wang. Language agent tree search unifies reasoning acting and planning in language models. arXiv preprint arXiv:2310.04406, 2023. Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning models: safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025. Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, and Christoph H. Lampert. Can LLMs separate instructions from data? and what do we even mean by that? In The Thirteenth International Conference on Learning Representations, 2025. URL https:// openreview.net/forum?id=8EtSBX41mt. 13 Preprint. Under review."
        },
        {
            "title": "A Details of Experiments",
            "content": "In this appendix, we provide complete details about the experiments conducted in the main text, including prompt injection, prompt extraction, and harmful requests. We then present the details of the model setup. Prompt Injection. We evaluate the robustness of reasoning models against prompt injection attacks using the SEP dataset introduced by Zverev et al. (2025). We randomly sample 500 prompts, each containing main instruction, associated data, low-priority query, and witness (i.e., the correct answer to the low-priority query). To encourage accurate reasoning, we augment the instruction with explicit guidance discouraging the model from answering the low-priority query. This setup allows the model to benefit from an extended reasoning process and ultimately follow the intended main instruction. concrete example is illustrated in Figure 6. Figure 6: An example of prompt injection attack. The main instruction, associated data, low-priority query, and witness are shown. We highlight the added guidance intended to help the model mitigate the attack. The model is expected to follow the main instruction and ignore the low-priority query. Prompt Extraction. We use the TENSORTRUST dataset introduced by Toyer et al. (2024) to evaluate model robustness against prompt extraction attacks. The dataset consists of 570 entries, each containing system instruction, malicious user prompt, and secret passcode. The goal of the model is to avoid revealing the secret passcode in its response to the malicious user prompt. Similarly, we provide specification following the approach of Wallace et al. (2024). concrete example is illustrated in Figure 7. Figure 7: demonstration of prompt extraction attack. The system instruction and malicious user prompt are shown. detailed specification is highlighted. The model is expected to avoid revealing the secret passcode in its response. Preprint. Under review. Harmful Requests. To evaluate the robustness of reasoning models against harmful requests, we use the SORRY-BENCH dataset introduced by Xie et al. (2024). The dataset consists of 450 harmful requests from 45 diverse groups. We directly use each prompt to query the model and assess whether it refuses to answer the harmful request, as the model is inherently designed to reject such inputs. Prompt Injection with Tool-Integrated Reasoning. In Section 5.1, we conduct proof-ofconcept experiment to evaluate emerging tool-integrated reasoning models against prompt injection attacks. Specifically, we use the LLM-PIEVAL dataset introduced by Ramakrishna et al. (2024), which consists of 750 examples where the malicious content aims to trigger tool call for specific function. Since open-source reasoning models typically lack native toolcalling capabilities within their reasoning chains, we explicitly augment model instructions to enable tool calling. An example is shown in Figure 8. Figure 8: An illustrative example demonstrating prompt injection attack targeting toolintegrated reasoning models. The instruction for teaching tool-calling in reasoning is highlighted. Evaluated Models. We evaluate the robustness of 12 leading open-source reasoning models, with parameters ranging from 8 billion to 32 billion. The list of models and their corresponding Hugging Face links is presented in Table 1. Table 1: List of reasoning models and corresponding Hugging Face links. Hugging Face Link https://huggingface.co/Qwen/Qwen3-8B https://huggingface.co/deepseek-ai/DeepSeek-R1-0528-Qwen3-8B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B https://huggingface.co/Qwen/Qwen3-14B https://huggingface.co/microsoft/Phi-4-reasoning Model Name QWEN3-8B R1-QWEN3-8B R1-QWEN-14B QWEN3-14B PHI-4-REASON PHI-4-REASON-PLUS https://huggingface.co/microsoft/Phi-4-reasoning-plus QWEN3-30B-A3B STAR1-14B R1-QWEN-32B QWQ-32B STAR1-32B QWEN3-32B https://huggingface.co/Qwen/Qwen3-30B-A3B https://huggingface.co/UCSC-VLAA/STAR1-R1-Distill-14B https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B https://huggingface.co/Qwen/QwQ-32B https://huggingface.co/UCSC-VLAA/STAR1-R1-Distill-32B https://huggingface.co/Qwen/Qwen3-32B"
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Google DeepMind",
        "NVIDIA",
        "Princeton University"
    ]
}