{
    "paper_title": "ZeroGUI: Automating Online GUI Learning at Zero Human Cost",
    "authors": [
        "Chenyu Yang",
        "Shiqian Su",
        "Shi Liu",
        "Xuan Dong",
        "Yue Yu",
        "Weijie Su",
        "Xuehui Wang",
        "Zhaoyang Liu",
        "Jinguo Zhu",
        "Hao Li",
        "Wenhai Wang",
        "Yu Qiao",
        "Xizhou Zhu",
        "Jifeng Dai"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, a scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 6 7 3 2 . 5 0 5 2 : r ZeroGUI: Automating Online GUI Learning at Zero Human Cost Chenyu Yang2,1, Shiqian Su2,1*, Shi Liu1*, Xuan Dong2,1*, Yue Yu2,1*, Weijie Su1*,(cid:0), Xuehui Wang3,1, Zhaoyang Liu4, Jinguo Zhu1, Hao Li1, Wenhai Wang5,1, Yu Qiao1, Xizhou Zhu2,1 (cid:0), Jifeng Dai2,1 1Shanghai Artificial Intelligence Laboratory 2Tsinghua University 3Shanghai Jiao Tong University 4Hong Kong University of Science and Technology 5The Chinese University of Hong Kong {yangcy23,ssq24,x-dong21,yuyue21}@mails.tsinghua.edu.cn, {liushi,suweijie,zhujinguo,qiaoyu}@pjlab.org.cn, {zhuxizhou,daijifeng}@tsinghua.edu.cn, wangxuehui@sjtu.edu.cn, {zyliumy,lihaothu}@gmail.com,whwang@ie.cuhk.edu.hk,"
        },
        {
            "title": "Abstract",
            "content": "The rapid advancement of large Vision-Language Models (VLMs) has propelled the development of pure-vision-based GUI Agents, capable of perceiving and operating Graphical User Interfaces (GUI) to autonomously fulfill user instructions. However, existing approaches usually adopt an offline learning framework, which faces two core limitations: (1) heavy reliance on high-quality manual annotations for element grounding and action supervision, and (2) limited adaptability to dynamic and interactive environments. To address these limitations, we propose ZeroGUI, scalable, online learning framework for automating GUI Agent training at Zero human cost. Specifically, ZeroGUI integrates (i) VLM-based automatic task generation to produce diverse training goals from the current environment state, (ii) VLM-based automatic reward estimation to assess task success without hand-crafted evaluation functions, and (iii) two-stage online reinforcement learning to continuously interact with and learn from GUI environments. Experiments on two advanced GUI Agents (UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance across OSWorld and AndroidLab environments. The code is available at https://github.com/OpenGVLab/ZeroGUI."
        },
        {
            "title": "Introduction",
            "content": "GUI agents are designed to perceive and interact with Graphical User Interfaces (GUIs). Early methods achieved this by building pipelines or relying on structured inputs such as HTML or DOM trees. Recently, the emergence of large Vision-Language Models (VLMs) [64, 63, 4] has enabled the development of end-to-end, pure-vison-based agents[20, 11, 71, 48], capable of perceiving GUI screenshots and performing actions such as clicking, scrolling, or typing to complete user-provided task instructions. These agents have demonstrated strong potential across various applications, including digital task automation, intelligent copilots, human-computer interaction, etc. Despite these progress, as shown in Fig. 1, existing approaches usually adopt an offline learning framework, which presents two fundamental limitations: (1) they heavily rely on high-quality human annotations for both GUI grounding [11, 66, 26] (i.e., identifying screen elements) and action Equal contribution. Project lead. This work is done when Chenyu Yang, Shiqian Su, Shi Liu, Xuan Dong, Yue Yu, Xuehui Wang and Hao Li are interns at Shanghai Artificial Intelligence Laboratory. (cid:0) Corresponding to Weijie Su <suweijie@pjlab.org.cn> and Xizhou Zhu <zhuxizhou@tsinghua.edu.cn>. Figure 1: Left: Existing Offline Training Framework for GUI Agents incurs high human costs, relying on manually collected and annotated interaction trajectories, typically under supervised fine-tuning (SFT) paradigm. Right: Our ZeroGUI is scalable online learning framework with automated task generation and reward estimation at zero human cost. VLM proposes diverse tasks, which are executed by the agent; the agent then receives VLM-based rewards and updates its policy via reinforcement learning (RL). trajectories [10, 39, 71, 48] (i.e., sequence of actions to complete task). These human-annotated labels are expensive, time-consuming, and difficult to scale across diverse platforms and tasks. (2) they fall short in adapting to dynamic and interactive environments. GUIs in the real world are non-stationary and uncertain: elements may shift, disappear, or behave differently depending on the systems state. Existing agents often overfit to static or narrowly defined tasks and struggle to generalize in open-ended scenarios. To overcome these limitations, online learning, where GUI agents are continuously updated through interaction with GUI environments, is desirable approach but remains challenging. Most existing environments, such as OSWorld [68] and AndroidLab [70], only provide test set consisting of manually crafted tasks and verification functions. Constructing training set with diverse tasks and associated success verifiers by hand is expensive and not scalable. Furthermore, in real-world environments, novel scenarios or tasks often lack ground-truth annotations, making it difficult to provide direct supervisory signals for agent learning. To develop scalable online learning framework, we focus on automating the construction of tasks and their corresponding success verifiers in GUI environments. This is feasible with the help of advanced VLMs, because they have been trained with large-scale GUI-related data and excel at understanding GUI elements, actions, and state transitions. They can assess task completion and propose relevant tasks based on observed information. In addition, when training GUI agents, it is sufficient to evaluate the encountered states rather than exhaustively covering all possible scenarios, which significantly reduces the complexity of automated task verification. Specifically, we propose ZeroGUI, fully automated online training framework in which GUI agents continuously interact with GUI environments to complete automatically generated tasks and update their policies using annotation-free rewards. As illustrated in Fig. 1, ZeroGUI consists of the following components: (1) VLM-Based Automatic Task Generation, which proposes large and diverse set of training tasks based on random initial states. (2) VLM-Based Automatic Reward Estimation, which predicts binary rewards that indicate task success, serving as supervision signals. The estimator leverages the trajectories of GUI agents as input, eliminating the need for hand-crafted task verifiers. (3) Two-stage Online Reinforcement Learning, involves training stage on generated tasks followed by test-time adaptation stage. We adapt the RL framework to support multi-step interactions between the GUI agent and the environment. We apply our ZeroGUI to two advanced VLM-based GUI agents (UI-TARS [48] and Aguvis [71]) and leverage both desktop (OSWorld [68]) and mobile (AndroidLab [70]) environments for evaluation. Experiments show that our ZeroGUI yields significant improvements in task success rates. Training on generated tasks extends the agents capability coverage, while test-time training helps the agent adapt to test tasks. In particular, on OSWorld, ZeroGUI-UI-TARS-7B achieves 14% relative improvements and ZeroGUI-Aguvis-7B achieves 63% relative improvements. 2 In summary, our contributions are as follows: We propose ZeroGUI, fully automated online learning framework that enables GUI agents to improve through interaction with GUI environments, eliminating the need for collecting and labeling offline training data. We design automatic VLM-based task generation and reward estimation, which generate training tasks and provide supervisory rewards in GUI environments without human annotations. We introduce two-stage reinforcement learning strategy. In the first stage, training on generated tasks builds the agents general capabilities. In the second stage, test-time training enables the agent to adapt to target test tasks. The proposed ZeroGUI significantly improves task success rates across multiple GUI environments and generalizes well to different base models."
        },
        {
            "title": "2 Related Work",
            "content": "GUI Agents are AI systems aimed to perceive, understand, and act upon graphical user interfaces. Early systems [75, 65] heavily relied on structured representations such as HTML, DOM trees, but recent progress in Vison Language Models (VLMs) has enabled shift toward purely vision-based approaches. However, due to the small size and visual variability of UI elements, general-purpose VLMs still struggle with accurate grounding. To mitigate this, several works [80, 17, 61, 77] incorporate specialized UI parsers to assist proprietary VLMs [1, 23, 2, 16]. Other efforts [20, 11, 66, 29, 71, 72, 26, 48] focus on building large-scale grounding datasets via manual labeling or automated pipelines for supervised fine-tuning. Beyond grounding, the long-horizon and high-variability nature of GUI tasks makes planning another key challenge. To this end, works such as [10, 39] collect expert trajectories across platforms for imitation learning. Aguvis[71] enhances these datasets with VLM generated chain-of-thought annotations, while UI-TARS[48] introduces both positive and negative samples to facilitate selfreflection and error correction via direct preference optimization. Given the high cost of collecting high-quality demonstrations, recent methods [41, 67, 35] explore reinforcement fine-tuning with rule-based rewards and limited expert data. Nevertheless, several studies [3, 47] report that models trained solely on static trajectories often struggle to generalize in dynamic, real-world environments. To improve adaptability, works like [47, 32] train outcome reward models (ORMs) in dynamic and interactive environments [82, 33] to support more robust RL-based adaptation. However, many interactive GUI environments [68, 51] lack curated training task sets, making ORM training difficult. To address this, we propose VLM-based Automatic Task Generation and VLM-based Automatic Reward Estimation methods, enabling scalable RL training in dynamic and interactive environments and laying the foundation for more generalizable GUI agent training framework. Interactive Environments for GUI Agents. Environments for GUI agents can be broadly categorized into non-interactive and interactive settings. Non-interactive environments (e.g., [13, 80, 31] for the web domain; [28, 58, 21, 50, 78, 27, 39, 7] for Android; and cross-platform settings like [24, 10]) are typically static and predefined. Interactive environments (e.g., [15, 6, 79] for Linux or Windows; [57, 30, 74, 82, 25, 19, 40, 14, 45] for web; [62, 51, 70] for Android; and cross-platform frameworks such as [68, 69, 33]) facilitate autonomous perception and action, enabling agents to operate in more realistic and dynamic settings. Interactive environments for GUI agents typically consist of an operating system platform (e.g., desktop OS or Android emulator), an action execution interface, and an observation module. Agents interact with the environment through predefined action space, or using PyAutoGUI on desktops and Android Debug Bridge (ADB) on mobile platforms. The environment provides feedback in the form of screenshots and UI structure data, enabling agents to perceive both visual and structured information for decision-making. Evaluation metrics in this dynamic environments are primarily based on manually predefined rules or hardcoded scripts, which are complex and lack scalability. RL for Post-Training. Reinforcement learning methods such as PPO[54] and DPO[49] have been widely used in the post-training of Large Language Models (LLMs), particularly within the RLHF framework[44]. Recently, RL methods such as GRPO[55], DAPO[76], and Dr.GRPO[36] have demonstrated strong effectiveness in improving reasoning abilities of LLMs, as exemplified by works like [18, 60]. Building on these developments, series of studies [73, 81, 22, 12, 46, 37, 56, 8] extend RL approaches to the post-training of VLMs, mainly focusing on mathematical reasoning and other general vision tasks where annotated labels are easily accessible. In parallel, emerging paradigms such as test-time reinforcement learning[83] attract increasing attention, aiming to enable effective RL training on unlabeled data. Our work further advances this line of research by applying test-time RL to VLM post-training in dynamic, interactive GUI environments. Specifically, we adapt the GRPO algorithm to multi-step settings and enhance training stability through tuning the KL regularization term."
        },
        {
            "title": "3 ZeroGUI",
            "content": "Existing offline methods rely on carefully collected trajectories and designed tasks, limiting their scalability and adaptability. To enable online adaptation with vision-language models, we introduce ZeroGUI, an automatic online training framework at zero human cost. Formulation. The completion of GUI task can be formulated as Markov Decision Process (MDP) [5], denoted by (S, A, R, ). Given task instruction I, the GUI agent interacts with the environment. At each step t, the agent predicts an action at according to its policy: at πθ (atI, st) , st = (ot, ht) (1) where the agents state st combines the current observation ot and history information ht containing previous observations and actions. The process terminates when encountering terminate action or reaching maximum number of steps, resulting in trajectory: τ = {I, (o1, a1), (o2, a2), . . . , (oT , aT )} (2) Framework Overview. As illustrated in Fig. 2, ZeroGUI consists of three key components: automatic task generation, automatic reward estimation, and two-stage online reinforcement learning process. In the first stage, VLM automatically generates set of training tasks, and the agent is trained using rewards estimated by VLM-based evaluator. In the second stage, the agent performs testtime training on the test set without ground-truth labels, receiving rewards solely from the same VLM-based evaluator. The following sections describe each component of our ZeroGUI in detail. 3.1 Automatic Task Generation Current offline training methods heavily rely on high-quality human annotations for GUI grounding and trajectory data with manually designed task instructions. To enable online training without human supervision, it is crucial to develop scalable and automated task generation pipeline. key challenge in task generation is to ensure generalization, particularly given the limited number of evaluation samples in existing GUI agent benchmarks. Our generated tasks must not only align with the operational constraints of the target environments but also exhibit sufficient diversity to cover broad behavioral space. To address this, we propose the following prompting designs: (1) Example-Guided Prompting. Prompt with combination of instruction exemplars and randomly sampled initial state screenshots, which guide the model toward environment-specific and realistic task proposals. (2) Multi-Candidate Generation. In each generation step, we request multiple task candidates simultaneously, encouraging the model to produce diverse set rather than overfitting to narrow task style. Fig. 2 illustrates this generation process and showcases representative generated tasks with test tasks from OSWorld as reference. To further train the agents to recognize unachievable goals and provide appropriate feedback, we prompt the VLM to generate subset of infeasible tasks. Such tasks are intentionally unsolvable within the environment and require the agent to explicitly output FAIL response. 3.2 Automatic Reward Estimation Existing interactive environments typically use script-based verifiers running within them to determine task success (e.g.checking file contents or system states). These verifiers often involve complex Figure 2: Top: Overview of ZeroGUI. It adopts Two-stage Online Reinforcement Learning paradigm. In the first stage, tasks are automatically generated by VLM, while in the second stage, tasks are drawn from the test set. These tasks are executed by the GUI agent. After each interaction, reward is assigned automatically by the VLM based on the agents trajectory, and the policy network is updated via reinforcement learning. Bottom left: Automatic Task Generation. The VLM receives random initial screenshot and set of task exemplars to generate diverse novel tasks. Bottom right: Automatic Reward Estimation. The final reward is obtained via majority voting of multiple VLM evaluations based on all screenshots of the trajectory. commands and logic to cover all possible cases, which heavily relies on manual implementation and debugging. Thus, setting up such verifiers for large-scale generated training tasks is both costly and unnecessary. To support general task verification or reward estimation in online training, we employ visionlanguage model (VLM) to assign binary rewards to trajectories. However, VLM-based assessment is often imperfect, e.g., it may overlook details or suffer from hallucinations, leading to incorrect labeling. Among the two error types, i.e., false positives and false negatives, our experiments show that false positives have greater impact (Sec. 4.3). This is likely because positive rewards offer more informative signals for policy improvement, and too many false positives can distort the improvement. In contrast, negative rewards often provide less targeted guidance, making the effect of false negatives relatively minor. Therefore, as illustrated in Fig. 2, the reward estimator focuses on reducing false positives and improving precision with the following designs: (1) All screenshots in the trajectory are included. The success of some tasks can only be determined by changes in the environment before and after an action, so all screenshots are needed. (2) The agents responses are excluded. They may contain hallucinations of success, even when the task actually fails. Such content can mislead the VLM to give false positive rewards. (3) voting mechanism is adopted. The VLM is queried multiple times, and the reward is assigned based on either majority agreement or stricter unanimous agreement (i.e., = 1 only if all outputs indicate success). This voting strategy further reduces the risk of false positives. 3.3 Two-stage Online Reinforcement Learning With both automatic task generation and reward estimation mechanisms in place, the GUI agent can perform online learning by continuously interacting with the GUI environment and updating 5 its policy guided by the rewards. Furthermore, since our reward estimator does not rely on internal environment states or ground-truth labels, it can also provide rewards for test tasks, enabling test-time adaptation. To this end, we introduce two-stage training strategy: (1) Training on generated tasks. The agent learns fundamental capabilities from generated tasks. (2) Test-time training. The agent adapts to target test tasks with rewards from the reward estimator. Reinforcement learning (RL) is adopted for this two-stage online training. We start from the Group Relative Policy Optimization (GRPO) [55], which eliminates the need for an additional value function and is effective for the post-training of LLMs and VLMs in other scenarios [18, 73, 8]. To adapt the original GRPO algorithm to the online RL of GUI agents, we propose the following modifications: (1) Extend the optimization objective to multi-step trajectories. For given task I, we follow GRPO i=1. The advantage ˆA(i)of to sample group of trajectories {τ (i)}G the trajectory τ (i) is computed by normalizing the rewards within the group (Eq. 4). The difference comes that in original GRPO, each sample is single generated sequence, while in our setting, each trajectory consists of multiple action prediction sequences from model-environment interaction (Eq. 2). For trajectory τ (i), we assign each prediction sequence a(i) with advantage ˆA(i) and compute its objective as follow: i=1 and obtain their rewards {R(i)}G (i) (θ) = 1 a(i) a(i) (cid:88) k=1 (cid:16) min (cid:16) r(i) t,k(θ) ˆA(i), clip (cid:16) r(i) t,k(θ), 1 ϵ, 1 + ϵ (cid:17) ˆA(i)(cid:17) βDKL(πθπref) (cid:17) where (cid:16) (cid:17) πθ r(i) t,k(θ) = a(i) t,k (cid:16) a(i) t,k where denotes the t-th step in the trajectory. The action a(i) indicates the k-th output token. The final objective is the average over all sequences: R(i) mean (cid:0){R(i)}G std (cid:0){R(i)}G t,<k , a(i) (cid:12) (cid:12)I, s(i) (cid:12) (cid:12)I, s(i) ˆA(i) = , a(i) πθold (cid:17) , t,<k i=1 (cid:1) i=1 is sampled according to Eq. 1, and (cid:1) (3) (4) (θ) = 1 i=1 (i) (cid:80)G (cid:88) (i) (cid:88) i=1 t= (i) (θ) (5) (2) Modify the KL loss term for better training stability. GRPO uses the k3-estimator [53] for the KL loss, i.e., DGRPO KL = πref/πθ log πref/πθ 1. However, we find that it can cause large gradients and is prone to overflow or underflow. We replace it with the k2-estimator [53], i.e., per-token MSE loss, which provides more stable gradients and avoids numerical overflow: DKL(πθπref) = (cid:16) 1 2 log πθ (cid:16) a(i) t,k (cid:12) (cid:12)I, s(i) , a(i) t,<k (cid:17) log πref (cid:16) a(i) t,k (cid:12) (cid:12)I, s(i) , a(i) t,<k (cid:17)(cid:17)2 (6) Experiments in Sec. 4.3 confirm that this modification improves training stability. Besides, removing the KL constraint may cause policy distribution drift, so simply dropping this loss term is not desirable option. Further derivations and analyses are provided in the appendix."
        },
        {
            "title": "4 Experiment",
            "content": "4.1 Experiment Settings 4.1.1 Evaluation Environments and Metrics OSWorld. OSWorld [68] is benchmark built upon computer environment designed for evaluating multi-modal agents on complex real-world tasks. It comprises 369 tasks that span web applications, desktop software, and OS-level operations. Among them, 30 tasks (8.1% of the test set) are infeasible by design to assess the ability to detect deprecated or hallucinated features. We report evaluation results on both the full test set and the subset of feasible tasks (i.e., excluding the infeasible ones). Our evaluation is conducted on the Ubuntu platform from OSWorld with screenshot-only mode. The screen size is 1920 1080 and the maximum number of steps is limited to 15. To reduce the influence of network instability and environmental variability, we report the mean and standard deviation 6 of scores over 4 runs. Additionally, we incorporate the following metrics for further analysis: (1) pass@k: the expected proportion of tasks the model can solve within trials, reflecting its potential capacity coverage. (2) all-pass@k: the expected proportion of tasks the model completes in all trials, indicating the consistency in performance. The unbiased estimators [9] are given by: (cid:34) pass@k := ExiD 1 (cid:35) (cid:1) (cid:0)nci (cid:1) (cid:0)n , all-pass@k := ExiD (cid:35) (cid:34) (cid:0)ci (cid:1) (cid:1) (cid:0)n (7) where each task xi is tested times and ci is the number of correct samples. We set = 8, = 4, and the sampling temperature as 0.5 to estimate these two metrics. AndroidLab. AndroidLab [70] is an interactive Android environment that includes the Android system and 9 offline-deployable apps (e.g., Clock, Calendar). It comprises 138 test tasks, which are categorized into two types: operational tasks and query-detecting tasks. Operational tasks involve completing goals through operations and are evaluated by predefined rules. Query-detecting tasks require the model to extract information and return text answer, scored by GPT. We observe that the GPT-based evaluation for certain tasks is not fully reliable, so we report evaluation results on both the full test set and the subset of operation tasks. While existing methods have been evaluated in XML or SoM modes, we implement screenshot-only setting to support our model and test the corresponding baseline. Success rate (SR) and sub-goal success rate (Sub-SR) are caculated as the metrics. 4.1.2 Implementation Details For task generation, we use GPT-4o [23] to generate 10 tasks for OSWorld and 5 tasks for AndroidLab at the same time. In total, more than 4,000 Ubuntu-based tasks and 225 Android-based tasks are generated. For training, 725 Ubuntu tasks and 175 Android tasks are randomly sampled from the generated pool, which is approximately twice the size of their respective test sets. For reward estimation, Qwen2.5-VL-32B [4] is deployed locally for efficiency. We query the VLM 4 times with temperature of 1.0 and use unanimous agreement voting to determine the reward. For training, we choose UI-TARS-7B-DPO [48] and Aguvis-7B [71] as our base models. We use the AdamW [38] optimizer with constant learning rate of 2e-6. For GRPO [55], we set the group size = 64 and the KL coefficient β = 0.1. We adopt DAPO [76] dynamic sampling, which filters out tasks with accuracy equal to 1 or 0. For each rollout step, sampling continues until 16k sequences are collected, followed by single gradient update. We train 1 epoch for both generated tasks and test-time tasks. Ablation studies are conducted on the Daily domain of OSWorld (including three apps: Chrome, Thunderbird, and VLC Player) to reduce experimental burden. More details are provided in the appendix. 4.2 Main Results 4.2.1 OSWorld We evaluate our proposed ZeroGUI on the OSWorld benchmark, and the results are shown in Tab. 1. Other existing approaches are also listed as reference. (1) Compared to the base models, our proposed ZeroGUI leads to significant improvements in task success rate, especially on the feasible subset. Specifically, for UI-TARS-7B-DPO, we achieve +2.5, 14% improvement on all tasks and +4.5, 40% on the feasible subset. For Aguvis-7B, although the base model performs poorly, our method still yields gains of +1.9, 63% and +2.1, 88%, respectively, with even greater relative improvements. This demonstrates the effectiveness and generalization of our self-improving online training framework. (2) Both of our training stages: generated task training and test-time training, contribute to performance gains. The pass@4 and all-pass@4 metrics further reveal their complementary roles. generated task training improves pass@4 significantly, indicating that large-scale and diverse generated tasks help to extent the models capability coverage. Test-time training mainly boosts all-pass@4, suggesting that the behavior consistency of the model is enhanced after being adapted to the target tasks. Notably, using only test-time training underperforms the two-stage setup, highlighting that generated training provides beneficial ability foundation that allows RL in the next stage to unlock more tasks and gain more informative rewards. 7 Table 1: Test results on OSWorld benchmark. Test settings and metrics are described in Sec. 4.1. The success rates are reported in meanstd. Absolute and relative improvements with respect to the base model are highlighted in green. * reported results taken from previous papers. Test set Feasible subset pass@4 all-p@4 SR pass@4 all-p@4 Model GPT-4o [23] Gemini-Pro-1.5 [16] Claude Computer-Use [2] OpenAI Operator [43] CogAgent-9B-20241220 [20] Aguvis-72B [71] UI-TARS-72B-DPO [48] SR 5.0* 5.4* 14.9* 19.7* 8.1* 10.3* 22.7* Aguvis-7B [71] + ZeroGUI (Gen. task only) + ZeroGUI 3.00.4 4.10.3 (+1.1, 37%) 4.90.4 (+1.9, 63%) UI-TARS-7B-DPO [48] + ZeroGUI (Test-time only) + ZeroGUI (Gen. task only) + ZeroGUI 17.71.1 (18.7*) 18.20.9 (+0.5, 3%) 18.21.3 (+0.5, 3%) 20.21.0 (+2.5, 14%) - - - - - - - 7.2 8.0 8. 25.5 26.4 27.8 28.0 - - - - - - - 1.4 1.3 1.8 9.4 8.6 7.5 9.6 - - - - - - - - - - - - - - 2.40.5 3.60.4 (+1.2, 50%) 4.50.4 (+2.1, 88%) 11.30.6 14.40.8 (+3.1, 27%) 14.71.0 (+3.4, 30%) 15.80.5 (+4.5, 40%) 6.5 7.6 7. 18.5 21.7 22.1 22.2 - - - - - - - 0.6 0.7 1.1 4.9 6.9 5.6 7.3 Table 2: Test results on AndroidLab benchmark. Test settings and metrics are described in Sec. 4.1. The success rates are reported in meanstd. Absolute improvements with respect to the base model are highlighted in green. * reported results taken from previous papers. Note: we fix some code errors in the original task verifiers, and some baseline methods get higher scores after the correction. Mode Model Test set SR Sub-SR Operation subset SR Sub-SR SoM Gemini-1.5-Pro [16] Claude-3.5-Sonnet [2] GPT-4o [23] AutoGLM [32] 16.7* 34.8 (29.0*) 38.0 (31.2*) 36.2* 18.4* 38.5 (32.6*) 44.3 (35.0*) - - 34.5 38.4 - Screenshot UI-TARS-7B-DPO [48] + ZeroGUI (Gen. task only) 46.42.05 (+0.7) 47.52.12 (+1.8) + ZeroGUI 45.71.52 50.5 52.0 (+1.5) 52.6 (+2.1) 54.61.72 55.62.06 (+1.0) 63.5 (+2.0) 57.42.30 (+2.8) 64.4 (+2.9) - 38.8 47.3 - 61.5 (3) The improvement on the full test set is smaller than on the feasible subset (e.g., +2.5 vs. +4.5 in the average success rate for UI-TARS-7B-DPO), indicating decrease in the detection of infeasibility. This may be due to two reasons: (a) the VLM lacks detailed knowledge of specific software, making it hard to judge infeasibility; (b) noisy rewards with false positives may cause the model to become overconfident. To mitigate this, we included portion of generated infeasible tasks in the training set, which has greatly alleviated this problem (see Sec. 4.3). 4.2.2 AndroidLab We also evaluate ZeroGUI on the AndroidLab benchmark, with the results shown in Tab. 2. (1) From the SR perspective, ZeroGUI achieves +2.8 improvement on the operation subset and +1.8 improvement on the full test set. This demonstrates that the proposed ZeroGUI generalizes well across different interactive GUI environments. (2) From the Sub-SR perspective, ZeroGUI achieves +2.9 improvement on the operation subset. Despite leveraging only the overall task rewards, it still yields performance gains in sub-task metrics. 4.3 Ablation Study Task Generation. Tab. 3a ablates our designs of task generation. (1) Removing examples during task generation or generating only one task at time leads to drop in test performance. We attribute this to two factors: providing task examples helps align the distribution of generated tasks with the target domain, while generating multiple tasks increases diversity, which are crucial for training data. 8 Table 3: Ablations of key components in task generation and reward estimation. The models are trained and tested on the Daily domain of OSworld.SR, Feas., and Infeas. denotes the success rates on all test tasks, feasible tasks, and infeasible tasks, respectively. w/o multi. cand.: without multiple candidates. All Screen.: all screenshots. With Res.: with agents response. (a) Task generation (b) Reward estimation Method SR Feas. Infeas. All Screen. With Res. Voting Precision Recall SR w/o examples 22.3 w/o multi. cand. 24.1 w/o infeasible 24.0 27.2 Ours 20.8 22.0 24.2 25.4 33.8 40.7 22.1 41.3 - - - - - - - 47.5 53.7 44.3 61. 40.4 61.7 74.5 51.1 23.9 25.6 24.0 27.2 (2) Excluding infeasible tasks results in sharp decline on the infeasible subset, indicating that such tasks help the model identify unachievable goals and reduces overconfidence. Reward Estimation. We ablate the key designs in the reward estimator. First, set of trajectories (UI-TARS-7B-DPO on generated tasks) are randomly selected and manually labeled the ground-truth rewards. We then apply different reward estimation methods to this set and evaluate their precision and recall. In addition, we train separate models using rewards estimated by each method and compare their success rates on the test tasks. The results are shown in Tab. 3b. (1) Using only the final screenshot instead of all screenshots results in low precision, recall, and test success rate. (2) Including the agents response during reward estimation yields the highest recall but significantly lowers both precision and test success rate, indicating that the VLM is misled by the response and produces many false positives. (3) Excluding the agents response and applying voting mechanism increase precision while decreasing recall, and also lead to notable improvement in test success rate. This suggests that false positive errors have more detrimental effect on model training. RL Training. To evaluate the effectiveness of our online RL training, we compare it against two baselines: offline rejection sampling fine-tuning (RFT) and online RFT. Results are reported in Tab. 4. Offline RFT first collects trajectories for all tasks using the base model and fine-tunes only on positive samples. Its performance is limited due to distribution mismatch between the collected trajectories and the updated policy, and it fails to leverage rewards from new tasks discovered after policy updates. Online RFT performs better but still lags behind online RL. This is mainly because RFT discards all negative samples, while RL enables the model to learn from them and avoid repeating past mistakes. We evaluate the effect of replacing the k3-KL loss in original GRPO with k2-KL loss. As shown in Fig. 3, k2-KL yields higher and more stable training accuracy. Test success rates in Tab. 4 further validate the superiority of k2-KL in our setting. Table 4: Ablations of online RL training and the modified KL loss. The models are trained and tested on the Daily domain of OSworld. Training Method SR Offline RFT Online RFT Online RL (Ours) k3-KL (GRPO) k2-KL (ours) 22.4 24.5 27.2 26.1 27.2 Figure 3: Comparison of training accuracies with k3-KL (GRPO) and k2-KL (ours)."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we present ZeroGUI, fully automated online learning framework for GUI agents that eliminates the need for manually collected and labeled offline training data. By leveraging vision-language models (VLMs), ZeroGUI enables automatic generation of training tasks and reward signals within GUI environments, removing human cost from both task design and evaluation. 9 We further introduce two-stage reinforcement learning strategy: training on generated tasks to acquire general capabilities, followed by test-time adaptation using VLM-based rewards. We conduct extensive experiments across different base models (Aguvis-7B and UI-TARS-7B-DPO) and different GUI environments. Notably, ZeroGUI-Aguvis-7B achieves 63% relative improvement, while ZeroGUI-UI-TARS-7B shows 14% relative improvement. Acknowledgements. The work is supported by the National Key R&D Program of China (NO. 2022ZD0161300), by the National Natural Science Foundation of China (U24A20325, 62321005, 62376134)."
        },
        {
            "title": "A Implementation Details",
            "content": "A.1 Task Generation We use GPT-4o [23] for task generation. Each prompt consists of task exemplars and the screenshot of randomly sampled initial state. To encourage diversity, each prompt generates multiple task candidates: 10 per prompt in OSWorld [68] and 5 per prompt in AndroidLab [70]. In total, we generate over 4,000 Ubuntu-based tasks and 225 Android-based tasks, from which we randomly sample 725 and 175 tasks, respectively, as the final training set. The prompt template used to generate OSWorld tasks is shown below: Task Generation Prompt Template The proposed task should be similar to the example tasks but not exactly the The {Number of Instructions} instructions should cover different aspects of You will be given screenshot of the current state of the computer. Based on the screenshot, generate {Number of Instructions} human-like instructions for an Ubuntu-based task related to using {App Name} app. 1. The task should be something that **can be achieved** using common interactions on standard Ubuntu system from the current state. 2. same. 3. using {App Name}. differ from the examples you have generated before. 4. instructions, and can be evaluated objectively by observing the action trajectory. 5. ensure that the URL or resource exists. 6. difficulty and the complexity should be evenly distributed. The tasks should be clear and unambiguous, with specific and detailed The task you propose must require at least 3 steps to complete, and the During the generation, think about any other kind of tasks If the task you propose requires access to any internet resources, you must The following are examples of good task instructions (their initial screenshots/states are different): {List of Exemplars} Please follow the **content, type, and distribution** of the examples provided to you, and generate task instructions based on the initial screenshot. **Do not repeat the existing examples.** Now, you will be given initial computer state screenshot. {Number of Instructions} task instructions based on the screenshot. Return only the task instruction text. line. {Number of Instructions} lines of instructions should be returned. Each instruction should occupy single Please generate A.2 Reward Estimation For efficient reward estimation, we use locally deployed QwenVL-2.5-32B-Instruct [4]. The VLM assigns binary rewards (success/failure) to the agents trajectories. The VLM is prompted with all screenshots of the trajectory and the task instruction. To improve reliability, we use voting mechanism: the VLM is queried multiple times with temperature 1.0, and the final reward is based on unanimous agreement. The reward estimation prompt for OSWorld tasks is shown below:"
        },
        {
            "title": "Reward Estimation Prompt Template",
            "content": "First, analyze and understand the task instruction. Describe what should the Describe what you observe in each screenshot, analysis what actions were You will be given task instruction and series of screenshots of the task execution. Please analyze the screenshots and provide detailed analysis of the task completion by following the steps below: 1. screenshots look like if the task is completed successfully. 2. taken and what changes were made to the UI to achieve the task (or mistakes made). 3. When you analyze the screenshots, please pay attention to the very detailed elements and changes in the UI. Every small detail may affect the final result. After all screenshots are analyzed, provide overall reasoning about how 4. the task was completed or failed at **the final state**. Make sure you have considered all demands of the task instruction. 5. screenshot) successfully (score 1 for success, 0 for failure). completed during the process but not at the final state, it should be considered as failure (0 score). Determine if the task was completed at **the final state** (the last If the task is Provide your response strictly in the following format: TASK REQUIREMENT: [Your understanding of the task instruction] SCREENSHOT ANALYSIS: Screenshot 1: [Analysis of first screenshot] Screenshot 2: [Analysis of second screenshot] ... REASONING: [Your reasoning] FINAL ANSWER: [Your final answer] SCORE: [0/1] Here is an example: (Task Instruction: Thunderbird and store the .eml files with only subject names to my Google Drive folder called \"emails\".) Please help me backup my emails in \"Bills\" folder in TASK REQUIREMENT: - Backup the emails in \"Bills\" folder in Thunderbird. - Store the backup .eml files with only subject names, and the emails should be saved in the Google Drive folder called \"emails\". - Once succeed, the emails should be visible in the Google Drive folder \"emails\". Or at least there should be saving action performed. SCREENSHOT ANALYSIS: Screenshot 1: - Thunderbird email client is open. - The \"Bills\" folder is visible under \"Local Folders.\" - There is no observable action performed yet in this screenshot. Screenshot 2: - The \"Bills\" folder has been selected, and the folder content is displayed. \"Amazon Web Services Invoice Available\" and \"Your - Two emails are visible: receipt from (formerly Twitter).\" 11 - No further actions are taken on the emails. Screenshot 3: - Both emails in the \"Bills\" folder are selected. - Content previews of both emails are displayed on the right-hand side. - No observable attempt to export or save the emails is visible. Screenshot 4: - The right-click context menu is accessed for the selected emails. - The \"Save As...\" option is hovered over, indicating intent to save the selected emails. Screenshot 5: - The file navigation window opens, allowing the user to choose save destination. - No specific Google Drive folder (e.g., \"emails\") is accessed or visible in this screenshot. Screenshot 6: - The \"Desktop\" option in the file picker is hovered over. - Still no indication of Google Drive folder (\"emails\") selection. Screenshot 7: - The \"Show other locations\" option is hovered over in the file picker. - No confirmation that the user is navigating to Google Drive or saving the files with subject names only. Screenshot 8: - The \"Software Updates Available\" notification appears. The file picker is still open without any observable confirmation of file saving or destination selection. - It remains unclear where or if the emails have been saved. REASONING: Based on the screenshots provided: While there was some intent to save the emails (as shown by the selection 1. and access of the \"Save As...\" function), there is no confirmation that the .eml files were saved with subject names only and placed in the required Google Drive folder (\"emails\"). 2. instructions. The screenshots lack evidence of the completion of the task as per the FINAL ANSWER: The task was not completed successfully due to the lack of observable saving action. SCORE: 0 Now, please **strictly follow the format** and analyze the following screenshots (The last line should only be SCORE: [0/1], no other text): Task Instruction: Screenshots (by order): {instruction} {screenshots} A.3 RL Training Hyper-parameters of our RL training are listed in Tab. 5."
        },
        {
            "title": "B Analysis of KL Loss",
            "content": "B.1 Theoretical Derivation and Analysis Here, we derive the gradient from the KL loss and show why our modification benefits stable training. 12 Table 5: Training hyper-parameters of ZeroGUI. GRPO group size KL loss coefficient rollout temperature rollout top-p rollout frequency penalty train batch size optimizer learning rate lr schedule weight decay optimizer momentum 64 0.1 0.5 0.9 1.0 16384 AdamW 2e-6 constant 0.0 β1, β2 = 0.9, 0. In our implementation, the model has only single update after each exploration stage, ensuring that πθ = πθold. Therefore, Eq. 3 can be simplified by removing the min and clip operation: (θ) = 1 a (cid:88) k=1 (cid:18) πθ(ak) πθold(ak) ˆA βDKL(πθπref) (cid:19) (8) where denotes predicted action sequence with the corresponding advantage ˆA, πθ(ak) stands for πθ (akI, s, a<k), is the task instruction and is the agents state. GRPO. GRPO [55] uses per-token k3-estimator [53] for the KL loss: DGRPO KL (πθπref) = πref(ak) πθ(ak) log πref(ak) πθ(ak) 1 (9) The gradient with respect to the parameter θ is: θJ GRPO(θ) = 1 a (cid:88) k=1 (cid:18) ˆA + β (cid:18) πref(ak) πθ(ak) (cid:19)(cid:19) 1 θ log πθ(ak) (10) Referring to GRPO, the Gradient Coefficient of the KL loss can be written as: GC GRPO KL (ak) = πref(ak) πθ(ak) 1 (11) Ours. We replace the original KL loss in GRPO with the k2-estimator [53], i.e., per-token MSE loss: DKL(πθπref) = 1 2 (log πθ(ak) log πref(ak))2 The gradient becomes: θJ (θ) = 1 a (cid:88) k=1 (cid:16) ˆA + β (log πref(ak) log πθ(ak)) (cid:17) θ log πθ(ak) GCKL(ak) = log πref(ak) log πθ(ak) (12) (13) (14) Analysis. We plot the curve of the gradient coefficients GCKL and GC GRPO in Fig. 4, with log πθ log πref as the x-axis. We also record the KL loss and the token-wise maximum and minimum of log πθ log πref during training, as shown in Fig. 5. As the training model θ gradually deviates from KL 13 Figure 4: Gradient coefficient of KL loss. (a) KL loss. (b) max(log πθ log πref) (c) min(log πθ log πref) Figure 5: KL loss curve and token-wise maximum and minimum of log πθ log πref during training. the reference model, there exist some tokens such that log πθ log πref (Fig. 5c). For GRPO, this leads to huge gradient coefficient, which may cause training instability. In contrast, ours has stable gradient coefficient and avoids large gradients. Some concurrent works [34] have also reached derivations and conclusions similar to ours. Furthermore, in practical implementations, language models typically output log probabilities, i.e., log π, computed from logits. When computing the KL loss of GRPO (Eq. 9), exponentiation is required, which is prone to overflow or underflow. In contrast, our KL loss (Eq. 12) avoids this issue. B.2 Supplementary Ablation Study Figure 6: Test success rates with different KL loss coefficients β. We compare different KL loss coefficients β, and the results shown in Fig. 6. Although some existing work [76, 36] suggests removing the KL penalty for general reasoning tasks, our findings differ in the context of training GUI agents. We observe that setting β yields the best test performance. Removing the KL loss entirely (β = 0) or using small β (e.g., 0.01) leads to performance degradation, likely due to the policy distribution drift that the model overfits to current tasks. In contrast, large β (e.g., 1) imposes excessive constraints on optimization, also resulting in worse results."
        },
        {
            "title": "C Analysis of Task Generation",
            "content": "C.1 Task Example Table 6: Examples of generated tasks and test tasks on OSWorld Domain Generated Task Instructions Test Task Instructions Chrome Look up MITs Deep Learning State of the Art lecture mentioned in the article and play the video if its available. Find mens T-Shirt that is in large size with stripe pattern, short sleeve and under the Sales&Discount. GIMP Could you add new layer to the image, call it Overlay, and set its mode to Multiply? Could you make the background of this image transparent for me? Could you create new column called Average Sales for each product in Zones 1, 2, and 3 by calculating the average of Q1, Q2, Q3, and Q4? Could you add the hyperlink https://its.example.com to the text Information Technology Services (ITS) in the main content? would like to pad all the numbers in the Old ID column with zeros in front, to fill them up to seven digits in the New 7 Digit ID column. Export the current document into PDF, keep the file name Could you create folder called WorkProjects in your home directory and put copy of installed_packages.txt inside it? Use terminal command to count all the lines of all php files in current directory recursively, show the result on the terminal Could you adjust the playback speed in VLC to 1.5x for this video? want to finish it faster, even if the audio isnt perfect. Help me modify the folder used to store my recordings to Desktop Office OS VLC VS Code Id like to stop the Welcome Page from showing up every time open VS Code. Can you help me with that? Please help me use VS Code to open the project in the user folder under home. As shown in Tab. 6 and Tab. 7, we list some of the generated tasks and the test tasks on OSWorld and AndroidLab for comparison. The generated tasks leverage the world knowledge embedded in VLMs to extend common operations, thereby enhancing data diversity. For example, the model generates Android tasks such as enabling auto-rotate and setting screen timeout, and Ubuntu tasks like speeding up video playbackfeatures not present in the original test tasks. Additionally, the generated tasks are both feasible and well-grounded. For instance, tasks like copying package list file to new project folder and averaging sales data for Ubuntu, or setting reasonable alarms and editing calendars as humans would for Android, ensure practical applicability. C.2 Visualization We visualize the task instructions in the original test set and our generated set. Following OSGenesis [59], we use Sentence-BERT [52] to encode instructions into high-dimensional semantic space, which is then reduced to two dimensions using UMAP [42]. As shown in Fig. 7, orange points represent instructions from the original OSWorld test set, while blue points correspond to instructions generated by our method. Each cluster in the visualization corresponds to distinct application domain (e.g., specific applications or task types), reflecting strong semantic coherence within domains. Notably, the generated instructions, conditioned on exemplars, not only align well with their target domains, but also extend 15 Table 7: Examples of generated tasks and test tasks on AndroidLab Apps Generated Task Instructions Test Task Instructions Clock Set an alarm for 5:30AM every Tuesday and Thursday, label it as Workout, and disable vibrate. need to set 10:30PM clock every weekend, and label it as Watch Football Games to remind me. Does my alarm at 4PM turn on vibrate? Calendar Arrange an event titled project deadline on March 15th, and attach note saying Submit by noon. want to add an event at 5:00PM today, whose Title is work. Bluecoins Modify the transaction on May 3, 2024, from expense to income, set the amount to 780 CNY, and update the note to Refund. Change the type of the transaction on May 2, 2024, from income toexpense, adjust the amount to 520 CNY, and change the note to Wrong Operation. Contacts Set Xus birthday to be 1989/06/15 and add his website as xuportfolio.com. Add birthday to AAA as 1996/10/24 Set contacts ABCs website to be abc.github.com Setting Set screen timeout duration to 5 minutes. Enable the auto-rotate screen feature. Turn on airplane mode of my phone Turn my phone to Dark theme Figure 7: Visualization of the test and generated task instructions in OSWorld. into previously unexplored regions of the instruction space. This illustrates the capacity of our method to produce semantically valid and diverse tasks beyond the original dataset."
        },
        {
            "title": "D Qualitative Comparisons",
            "content": "We conduct case studies to further demonstrate the effectiveness of our proposed ZeroGUI. We observe that the base model UI-TARS-7B-DPO [48] shows limited task comprehension and inadequate attention to detail, frequently falling into repetitive action loops during task execution. In contrast, after our ZeroGUI training, the model exhibits significantly more stable behavioral strategies and stronger task execution capabilities. For example, as shown in Fig. 8 and 9, task in the VS Code domain of OSWorld involves the instruction: want to make the tabs wrapped over multiple lines when exceeding available space, please help modify the setting of VS Code. During execution, the base model attempts to modify the Tab Size parameter but fails to delete the default value before entering new one. Instead, it 16 prepends the new number to the existing value, resulting in an incorrect setting. This faulty operation is then repeated multiple times, indicating that the model lacks the ability to detect invalid actions. In contrast, our model adopts more robust action strategy: it first uses keyboard shortcut to select all existing content, then inputs the correct value, and successfully completes the task. Another example comes from task in the LibreOffice Impress domain, where the instruction is: Add an image none.png on the Desktop to slide 2 with 1cm*1cm size. As shown in Fig. 10 and 11, after clicking the Insert menu, the base model attempts to select the Image option but misclicks blank area due to inaccurate grounding, causing the menu to close prematurely. However, the model fails to detect this change and continues to attempt to click the Image option under the now-closed Insert menu, resulting in ineffective repetition. In contrast, our model completes the full insertion process more reliably. It successfully opens the image insertion interface, selects the correct image file, adjusts the width and height step by step, and ultimately finishes the long-horizon task accurately. This comparison further demonstrates the improved stability and general performance of our method in handling complex tasks. When executing tasks in AndroidLab, the model also shows significant performance improvements. Take Calendar task as an example, with the instruction: You should use calendar to complete the following task: Arrange an event titled homework for me at May 21st, and set the notification time to be 10 minutes before., as illustrated in Fig. 12 and 13. The base model shows insufficient understanding of the instruction, overlooking details such as the event title and notification time. It neither adds the event title nor sets the notification, and closes the interface ultimately. In contrast, our trained model accurately captures and executes these detailed requirements, successfully adding the event on the specified date and setting the notification, which demonstrates the effectiveness of our training framework in mitigating issues arising from overlooked details and improving task accuracy. It is worth noting that the base model fails to complete any of the above tasks, indicating the lack of supervision from successful trajectories. Through training on generated tasks, our model learns more generalizable interaction strategies. This capability not only enhances its performance on specific tasks, but also demonstrates the significant potential and practical effectiveness of the proposed training framework in enabling the model to adapt to complex GUI environments. 17 Figure 8: The trajectory of the base model UI-TARS-7B-DPO when executing VS Code task. The instruction is want the tabs to wrap onto multiple lines when they exceed the available space. Please help modify the VS Code settings accordingly. The original thoughts are in Chinese and have been translated into English for presentation purposes. Subsequent repetitive steps have been omitted. 18 Figure 9: The trajectory of our trained model when executing VS Code task. The instruction is want the tabs to wrap onto multiple lines when they exceed the available space. Please help modify the VS Code settings accordingly. The original thoughts are in Chinese and have been translated into English for presentation purposes. Figure 10: The trajectory of the base model UI-TARS-7B-DPO when executing LibreOffice Impress task. The instruction is Add an image none.png on the Desktop to slide 2 with 1cm*1cm size. The original thoughts are in Chinese and have been translated into English for presentation purposes. Subsequent repetitive steps have been omitted. 20 Figure 11: The trajectory of our trained model when executing LibreOffice Impress task. The instruction is Add an image none.png on the Desktop to slide 2 with 1cm*1cm size. The original thoughts are in Chinese and have been translated into English for presentation purposes. 21 Figure 12: The trajectory of the base model UI-TARS-7B-DPO when executing Calendar task. The instruction is You should use calendar to complete the following task: Arrange an event titled homework for me at May 21st, and set the notification time to be 10 minutes before. Figure 13: The trajectory of our trained model when executing Calendar task. The instruction is You should use calendar to complete the following task: Arrange an event titled homework for me at May 21st, and set the notification time to be 10 minutes before."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. Introducing claude 3.5 sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, 2024. Accessed: 2025-05-11. [3] H. Bai, Y. Zhou, J. Pan, M. Cemri, A. Suhr, S. Levine, and A. Kumar. Digirl: Training in-the-wild devicecontrol agents with autonomous reinforcement learning. Advances in Neural Information Processing Systems, 37:1246112495, 2024. [4] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] R. Bellman. markovian decision process. Journal of mathematics and mechanics, pages 679684, 1957. [6] R. Bonatti, D. Zhao, F. Bonacci, D. Dupont, S. Abdali, Y. Li, Y. Lu, J. Wagle, K. Koishida, A. Bucker, et al. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264, 2024. [7] Y. Chai, S. Huang, Y. Niu, H. Xiao, L. Liu, D. Zhang, P. Gao, S. Ren, and H. Li. Amex: Android multi-annotation expo dataset for mobile gui agents. arXiv preprint arXiv:2407.17490, 2024. [8] L. Chen, L. Li, H. Zhao, Y. Song, and Vinci. R1-v: Reinforcing super generalization ability in visionlanguage models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 202502-02. [9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [10] W. Chen, J. Cui, J. Hu, Y. Qin, J. Fang, Y. Zhao, C. Wang, J. Liu, G. Chen, Y. Huo, et al. Guicourse: From general vision language models to versatile gui agents. arXiv preprint arXiv:2406.11317, 2024. [11] K. Cheng, Q. Sun, Y. Chu, F. Xu, Y. Li, J. Zhang, and Z. Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. [12] H. Deng, D. Zou, R. Ma, H. Luo, Y. Cao, and Y. Kang. Boosting the generalization and reasoning of vision language models with curriculum reinforcement learning. arXiv preprint arXiv:2503.07065, 2025. [13] X. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2web: Towards generalist agent for the web. Advances in Neural Information Processing Systems, 36:2809128114, 2023. [14] A. Drouin, M. Gasse, M. Caccia, I. H. Laradji, M. Del Verme, T. Marty, L. Boisvert, M. Thakkar, Q. Cappart, D. Vazquez, et al. Workarena: How capable are web agents at solving common knowledge work tasks? arXiv preprint arXiv:2403.07718, 2024. [15] D. Gao, L. Ji, Z. Bai, M. Ouyang, P. Li, D. Mao, Q. Wu, W. Zhang, P. Wang, X. Guo, et al. Assistgui: Task-oriented pc graphical user interface automation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1328913298, 2024. [16] Google. Gemini 1.5 pro generative ai on vertex ai. https://cloud.google.com/vertex-ai/ generative-ai/docs/models/gemini/1-5-pro, 2024. Accessed: 2025-05-11. [17] B. Gou, R. Wang, B. Zheng, Y. Xie, C. Chang, Y. Shu, H. Sun, and Y. Su. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243, 2024. [18] D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [19] H. He, W. Yao, K. Ma, W. Yu, Y. Dai, H. Zhang, Z. Lan, and D. Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. arXiv preprint arXiv:2401.13919, 2024. [20] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1428114290, 2024. 24 [21] Y.-C. Hsiao, F. Zubach, G. Baechler, V. Carbune, J. Lin, M. Wang, S. Sunkara, Y. Zhu, and J. Chen. Screenqa: Large-scale question-answer pairs over mobile app screenshots. arXiv preprint arXiv:2209.08199, 2022. [22] W. Huang, B. Jia, Z. Zhai, S. Cao, Z. Ye, F. Zhao, Z. Xu, Y. Hu, and S. Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. [23] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [24] R. Kapoor, Y. P. Butala, M. Russak, J. Y. Koh, K. Kamble, W. AlShikh, and R. Salakhutdinov. Omniact: dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. In European Conference on Computer Vision, pages 161178. Springer, 2024. [25] J. Y. Koh, R. Lo, L. Jang, V. Duvvur, M. C. Lim, P.-Y. Huang, G. Neubig, S. Zhou, R. Salakhutdinov, and D. Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. [26] H. Li, J. Chen, J. Su, Y. Chen, Q. Li, and Z. Zhang. Autogui: Scaling gui grounding with automatic functionality annotations from llms. arXiv preprint arXiv:2502.01977, 2025. [27] W. Li, W. E. Bishop, A. Li, C. Rawles, F. Campbell-Ajala, D. Tyamagundlu, and O. Riva. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154, 2024. [28] Y. Li, J. He, X. Zhou, Y. Zhang, and J. Baldridge. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776, 2020. [29] K. Q. Lin, L. Li, D. Gao, Z. Yang, S. Wu, Z. Bai, W. Lei, L. Wang, and M. Z. Shou. Showui: One vision-language-action model for gui visual agent. arXiv preprint arXiv:2411.17465, 2024. [30] E. Z. Liu, K. Guu, P. Pasupat, T. Shi, and P. Liang. Reinforcement learning on web interfaces using workflow-guided exploration. arXiv preprint arXiv:1802.08802, 2018. [31] J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024. [32] X. Liu, B. Qin, D. Liang, G. Dong, H. Lai, H. Zhang, H. Zhao, I. L. Iong, J. Sun, J. Wang, et al. Autoglm: Autonomous foundation agents for guis. arXiv preprint arXiv:2411.00820, 2024. [33] X. Liu, T. Zhang, Y. Gu, I. L. Iong, Y. Xu, X. Song, S. Zhang, H. Lai, X. Liu, H. Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024. [34] Y. Liu. Rethinking kl divergence in rlhf: From single sample to mini-batch to expectation. https://www.notion.so/Rethinking-KL-Divergence-in-RLHF-From-Single-Sample-to-Mini-Batch-toExpectation-1c18637cdeb3800ab47cd01d3fa33ea5, 2025. Notion Blog. [35] Y. Liu, P. Li, C. Xie, X. Hu, X. Han, S. Zhang, H. Yang, and F. Wu. Infigui-r1: Advancing multimodal gui agents from reactive actors to deliberative reasoners. arXiv preprint arXiv:2504.14239, 2025. [36] Z. Liu, C. Chen, W. Li, P. Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [37] Z. Liu, Z. Sun, Y. Zang, X. Dong, Y. Cao, H. Duan, D. Lin, and J. Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025. [38] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [39] Q. Lu, W. Shao, Z. Liu, F. Meng, B. Li, B. Chen, S. Huang, K. Zhang, Y. Qiao, and P. Luo. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451, 2024. [40] X. H. Lù, Z. Kasner, and S. Reddy. Weblinx: Real-world website navigation with multi-turn dialogue. arXiv preprint arXiv:2402.05930, 2024. [41] Z. Lu, Y. Chai, Y. Guo, X. Yin, L. Liu, H. Wang, G. Xiong, and H. Li. Ui-r1: Enhancing action prediction of gui agents by reinforcement learning. arXiv preprint arXiv:2503.21620, 2025. 25 [42] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approximation and projection for dimension reduction. arXiv preprint arXiv:1802.03426, 2018. [43] OpenAI. Introducing operator. https://openai.com/index/introducing-operator/, 2025. Accessed: 2025-05-16. [44] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [45] Y. Pan, D. Kong, S. Zhou, C. Cui, Y. Leng, B. Jiang, H. Liu, Y. Shang, S. Zhou, T. Wu, et al. Webcanvas: Benchmarking web agents in online environments. arXiv preprint arXiv:2406.12373, 2024. [46] Y. Peng, G. Zhang, M. Zhang, Z. You, J. Liu, Q. Zhu, K. Yang, X. Xu, X. Geng, and X. Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [47] Z. Qi, X. Liu, I. L. Iong, H. Lai, X. Sun, W. Zhao, Y. Yang, X. Yang, J. Sun, S. Yao, et al. Webrl: Training llm web agents via self-evolving online curriculum reinforcement learning. arXiv preprint arXiv:2411.02337, 2024. [48] Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025. [49] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36: 5372853741, 2023. [50] C. Rawles, A. Li, D. Rodriguez, O. Riva, and T. Lillicrap. Androidinthewild: large-scale dataset for android device control. Advances in Neural Information Processing Systems, 36:5970859728, 2023. [51] C. Rawles, S. Clinckemaillie, Y. Chang, J. Waltz, G. Lau, M. Fair, A. Li, W. Bishop, W. Li, F. CampbellAjala, et al. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573, 2024. [52] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084, 2019. [53] J. Schulman. Approximating kl divergence. http://joschu.net/blog/kl-approx.html, 2020. Accessed: 2025-05-16. [54] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [55] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [56] H. Shen, P. Liu, J. Li, C. Fang, Y. Ma, J. Liao, Q. Shen, Z. Zhang, K. Zhao, Q. Zhang, et al. Vlm-r1: stable and generalizable r1-style large vision-language model. arXiv preprint arXiv:2504.07615, 2025. [57] T. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 31353144. PMLR, 2017. [58] L. Sun, X. Chen, L. Chen, T. Dai, Z. Zhu, and K. Yu. Meta-gui: Towards multi-modal conversational agents on mobile gui. arXiv preprint arXiv:2205.11029, 2022. [59] Q. Sun, K. Cheng, Z. Ding, C. Jin, Y. Wang, F. Xu, Z. Wu, C. Jia, L. Chen, Z. Liu, et al. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. arXiv preprint arXiv:2412.19723, 2024. [60] K. Team, A. Du, B. Gao, B. Xing, C. Jiang, C. Chen, C. Li, C. Xiao, C. Du, C. Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [61] J. Wan, S. Song, W. Yu, Y. Liu, W. Cheng, F. Huang, X. Bai, C. Yao, and Z. Yang. Omniparser: unified framework for text spotting key information extraction and table recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1564115653, 2024. [62] J. Wang, H. Xu, J. Ye, M. Yan, W. Shen, J. Zhang, F. Huang, and J. Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception. arXiv preprint arXiv:2401.16158, 2024. 26 [63] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, Y. Fan, K. Dang, M. Du, X. Ren, R. Men, D. Liu, C. Zhou, J. Zhou, and J. Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [64] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, S. XiXuan, et al. Cogvlm: Visual expert for pretrained language models. Advances in Neural Information Processing Systems, 37: 121475121499, 2024. [65] H. Wen, Y. Li, G. Liu, S. Zhao, T. Yu, T. J.-J. Li, S. Jiang, Y. Liu, Y. Zhang, and Y. Liu. Autodroid: Llm-powered task automation in android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking, pages 543557, 2024. [66] Z. Wu, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, K. Cheng, Z. Ding, L. Chen, P. P. Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. [67] X. Xia and R. Luo. Gui-r1: generalist r1-style vision-language action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [68] T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [69] T. Xu, L. Chen, D.-J. Wu, Y. Chen, Z. Zhang, X. Yao, Z. Xie, Y. Chen, S. Liu, B. Qian, et al. Crab: Crossenvironment agent benchmark for multimodal language model agents. arXiv preprint arXiv:2407.01511, 2024. [70] Y. Xu, X. Liu, X. Sun, S. Cheng, H. Yu, H. Lai, S. Zhang, D. Zhang, J. Tang, and Y. Dong. Androidlab: Training and systematic benchmarking of android autonomous agents. arXiv preprint arXiv:2410.24024, 2024. [71] Y. Xu, Z. Wang, J. Wang, D. Lu, T. Xie, A. Saha, D. Sahoo, T. Yu, and C. Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. [72] Y. Yang, Y. Wang, D. Li, Z. Luo, B. Chen, C. Huang, and J. Li. Aria-ui: Visual grounding for gui instructions. arXiv preprint arXiv:2412.16256, 2024. [73] Y. Yang, X. He, H. Pan, X. Jiang, Y. Deng, X. Yang, H. Lu, D. Yin, F. Rao, M. Zhu, et al. R1onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. [74] S. Yao, H. Chen, J. Yang, and K. Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:2074420757, 2022. [75] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. [76] Q. Yu, Z. Zhang, R. Zhu, Y. Yuan, X. Zuo, Y. Yue, T. Fan, G. Liu, L. Liu, X. Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [77] W. Yu, Z. Yang, J. Wan, S. Song, J. Tang, W. Cheng, Y. Liu, and X. Bai. Omniparser v2: Structured-pointsof-thought for unified visual text parsing and its generality to multimodal large language models. arXiv preprint arXiv:2502.16161, 2025. [78] J. Zhang, J. Wu, Y. Teng, M. Liao, N. Xu, X. Xiao, Z. Wei, and D. Tang. Android in the zoo: Chain-ofaction-thought for gui agents. arXiv preprint arXiv:2403.02713, 2024. [79] H. H. Zhao, D. Gao, and M. Z. Shou. Worldgui: Dynamic testing for comprehensive desktop gui automation. arXiv preprint arXiv:2502.08047, 2025. [80] B. Zheng, B. Gou, J. Kil, H. Sun, and Y. Su. Gpt-4v (ision) is generalist web agent, if grounded. arXiv preprint arXiv:2401.01614, 2024. [81] H. Zhou, X. Li, R. Wang, M. Cheng, T. Zhou, and C.-J. Hsieh. R1-zeros\" aha moment\" in visual reasoning on 2b non-sft model. arXiv preprint arXiv:2503.05132, 2025. [82] S. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, T. Ou, Y. Bisk, D. Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [83] Y. Zuo, K. Zhang, S. Qu, L. Sheng, X. Zhu, B. Qi, Y. Sun, G. Cui, N. Ding, and B. Zhou. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}