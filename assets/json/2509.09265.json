{
    "paper_title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents",
    "authors": [
        "Jiawei Wang",
        "Jiacai Liu",
        "Yuqian Fu",
        "Yingru Li",
        "Xintao Wang",
        "Yuan Lin",
        "Yu Yue",
        "Lin Zhang",
        "Yang Wang",
        "Ke Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face a significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-by-step feedback. In this paper, we identify a fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce a bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Project page is at https://empgseed-seed.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 5 6 2 9 0 . 9 0 5 2 : r Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents Jiawei Wang, Jiacai Liu, Yuqian Fu, Yingru Li, Xintao Wang, Yuan Lin, Yu Yue, Lin Zhang, Yang Wang,, Ke Wang,"
        },
        {
            "title": "ByteDance",
            "content": "Work done at ByteDance Seed, Corresponding authors"
        },
        {
            "title": "Abstract",
            "content": "In long-horizon tasks, recent agents based on Large Language Models (LLMs) face significant challenge that sparse, outcome-based rewards make it difficult to assign credit to intermediate steps. Previous methods mainly focus on creating dense reward signals to guide learning, either through traditional reinforcement learning techniques like inverse reinforcement learning or by using Process Reward Models for step-bystep feedback. In this paper, we identify fundamental problem in the learning dynamics of LLMs: the magnitude of policy gradients is inherently coupled with the entropy, which leads to inefficient small updates for confident correct actions and potentially destabilizes large updates for uncertain ones. To resolve this, we propose Entropy-Modulated Policy Gradients (EMPG), framework that re-calibrates the learning signal based on step-wise uncertainty and the final task outcome. EMPG amplifies updates for confident correct actions, penalizes confident errors, and attenuates updates from uncertain steps to stabilize exploration. We further introduce bonus term for future clarity that encourages agents to find more predictable solution paths. Through comprehensive experiments on three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we demonstrate that EMPG achieves substantial performance gains and significantly outperforms strong policy gradient baselines. Date: September 12, 2025 Correspondence: Yang Wang at wangyang.127@bytedance.com, Ke Wang at wangke@bytedance.com Project Page: https://empgseed-seed.github.io/"
        },
        {
            "title": "1 Introduction",
            "content": "The advent of Large Language Models (LLMs) has catalyzed the development of autonomous agents that are capable of tackling complex, multi-step tasks [32, 39]. However, fundamental challenge persists in training these agents for long-horizon tasks: the sparsity of outcome-based rewards. In many realistic scenarios, such as web navigation [38], software engineering [41], and deep search [2], feedback is only available at the end of the complete generation. This makes it difficult to assign appropriate credit for standard reinforcement learning (RL) algorithms to discern the crucial intermediate steps. To solve the problem of the sparse reward challenge, prior work has explored two primary directions: implicit reward guidance and explicit step-wise supervision. The first involves traditional reinforcement learning techniques aimed at creating densified reward signals. Methods like reward shaping [22], intrinsic motivation based on state novelty or curiosity [3, 23], and inverse reinforcement learning [7, 48] attempt to estimate the value of intermediate actions. However, these approaches often struggle to scale. They are either computationally prohibitive, ill-suited for the vast, 1 Figure 1 Overview of the EMPG mechanism and its algorithm performance. Left: Conceptual diagram contrasting the uniform credit assignment of baseline methods with EMPGs confidence-modulated signal. Right: Final performance comparison on key long-horizon benchmarks showing EMPGs superiority, along with the training dynamics on Musique that highlight its ability to achieve sustained improvement and avoid the baselines performance plateau. combinatorially complex state and action spaces inherent to LLM-driven agent tasks, or heavily reliant on human prior knowledge. The second line of research, particularly successful in structured reasoning domains, employs Process Reward Models (PRMs) [20] to provide step-by-step feedback. Yet, PRMs suffer from significant drawbacks: they demand prohibitive human annotation costs to build, are susceptible to noise when trained on synthetic data, and often exhibit poor generalization to out-of-distribution problems. These limitations are exacerbated in complex, interactive agent tasks where defining single \"correct\" step is itself non-trivial, context-dependent challenge, making the application of PRMs impractical. Policy entropy is cornerstone concept in RL, traditionally used to balance the exploration-exploitation trade-off. Recently, it has been repurposed as direct learning signal in LLM reasoning tasks, where minimizing entropy is used as an unsupervised objective to increase the models certainty [1, 9]. While effective in some contexts, this approach is vulnerable to the critical issue of \"hallucinated confidence,\" where the model becomes confidently incorrect [45]. More recent efforts use entropy not as reward, but as modulator. For instance, Seed-GRPO [4] leverages semantic uncertainty to down-weight the advantage of high-entropy responses in mathematical reasoning, while Cheng et al. [5] proposes shaping the advantage function with token-level entropy to improve long-form generation. However, these efforts are restricted to single-turn, generative reasoning tasks, operating at either the token-level or response-level. It remains underexplored how to leverage agents intrinsic uncertainty for credit assignment in long-horizon, multi-step decision-making. Our work begins by analyzing the fundamental dynamics of the policy gradient itself. We formally show that for standard softmax policy, the expected norm of the score function is monotonic function of the policys entropy (Proposition 1). In simple terms, high-entropy (uncertain) actions naturally produce large gradients, while low-entropy (confident) actions produce small ones. This inherent behavior presents dual challenge for learning: 1) confident and correct steps, which should be strongly reinforced, receive small updates, limiting learning speed, and 2) uncertain exploratory steps can introduce large, noisy gradients that destabilize training. This reveals critical need to explicitly re-calibrate the learning signal based on an actions uncertainty. To address this, we propose Entropy-Modulated Policy Gradients (EMPG), framework that reshapes the learning 2 landscape by directly adapting to this dynamic, as illustrated in Figure 1. Instead of naively rewarding low entropy, EMPG introduces Self-Calibrating Gradient Scaling mechanism, which dynamically modulates the policy gradient based on step-wise uncertainty: 1) for confident and correct actions, it amplifies the updates, while 2) for uncertain steps, it attenuates updates to ensure stable exploration. Furthermore, to encourage agents to find predictable solution paths, EMPG introduces future clarity, an additional bonus term in the advantage function that provides an intrinsic signal for actions that lead to less uncertain subsequent states. This guides agents to perform purposeful exploration, steering them away from chaotic or unpromising high-entropy trajectories toward states with greater clarity about the next steps. This dual approach enables EMPG to forge dense, informative, and well-calibrated learning signal from sparse external feedback. To validate our framework, we conduct extensive experiments on challenging long-horizon agent benchmarks such as WebShop [38], ALFWorld [29], and Deep Search [2], demonstrating the effectiveness and scalability of our approach across models of various sizes. Our key contributions are as follows: We first identify and formalize fundamental challenge in policy gradient methods: the inherent coupling of gradient magnitude and policy entropy. This dynamic leads to inefficient learning for confident actions and instability from uncertain ones, motivating the need for explicit signal re-calibration. We introduce Entropy-Modulated Policy Gradients, framework designed to solve this problem. EMPG combines Self-Calibrating Gradient Scaling to correct the flawed gradient dynamics with Future Clarity Bonus to promote exploration towards more predictable states. Extensive experiments on demanding agent tasks (WebShop, ALFWorld, Deep Search) show that EMPG substantially outperforms strong baselines like GRPO and DAPO."
        },
        {
            "title": "2.1 LLM-based Autonomous Agents",
            "content": "The advent of LLMs has catalyzed the development of sophisticated autonomous agents capable of performing complex, multi-step tasks that were previously unattainable. Specialized agents have been designed for diverse applications, including software development (e.g., coding agents [12, 41]), information retrieval (search agents [10, 18]), and complex web interactions (browser-use agents [6, 36, 38]). For training these agentic models, reinforcement learning has proven to be powerful and essential paradigm. Recent research on RL-based agents, such as Search-R1 [13], SWE-RL [33], and WebAgent-R1 [34], has demonstrated that RL can effectively enhance agent performance and enable learning in highly interactive and dynamic environments. Despite these successes, fundamental problem remains to be fully addressed: the difficulty of credit assignment in long-horizon tasks. The multi-step nature of these problems, where reward signal is often only available upon completion, hinders the efficiency and stability of the training process."
        },
        {
            "title": "2.2 Reinforcement Learning from Internal Feedback",
            "content": "To overcome the challenges of sparse external rewards, recent studies have explored using internal feedback, generated by the model itself, to create denser training signals. This approach often leverages unsupervised signals derived from model uncertainty [1, 43, 46] or self-consistency [42, 49], frequently quantified by policy entropy. However, the role of entropy has been interpreted in conflicting ways. Some studies argue that correct responses typically exhibit lower entropy, thus proposing unsupervised entropy minimization as method to improve performance [1, 9]. Conversely, other works suggest that high entropy encourages exploratory reasoning. For instance, SEED-GRPO [4] uses semantic entropy to modulate policy updates for diversity, while others explicitly incorporate policy entropy into the advantage term to promote exploration [5, 30]. Recently, EDGE-GRPO [44] proposes entropy modulation in single-turn mathematical reasoning. Similar to our method, they modulate policy gradients by amplifying updates for confident correct responses and attenuating updates for incorrect or uncertain ones. However, EMPG fundamentally differs from EDGE-GRPO in both motivation and scope: First, while EDGE-GRPO focuses on correcting confidence misalignment within single-turn mathematical reasoning, EMPG is specifically designed for the multi-step credit assignment problem in long-horizon tasks. Second, towards the challenges in multi-turn long-horizon tasks, EMPG dynamically assigns credit across the entire trajectory to amplify the crucial steps."
        },
        {
            "title": "3.1 Policy Optimization in Reinforcement Learning",
            "content": "Our work is grounded in policy gradient methods, which seek to optimize policy πθ parameterized by θ to maximize the expected reward objective: (πθ) := Eτ πθ [R(τ )] where τ is trajectory sampled under policy πθ and R(τ ) is its total return. The policy gradient theorem allows for direct optimization of this objective via gradient ascent. The gradient is estimated as an expectation over trajectories: (1) θJ (πθ) = Eτ πθ (cid:34) (cid:88) t=0 A(st, at)θ log πθ(atst) (2) (cid:35) where st and at are the state and action at time step t, respectively. key challenge in estimating this gradient is its inherently high variance. To mitigate this, an advantage function, A(st, at), is used to measure the relative quality of an action. This advantage is typically estimated using learned value model, which predicts the expected return from given state [25]. However, this approach has significant drawbacks. The value model is often comparable in size to the policy model, introducing substantial memory and computational overhead. Furthermore, the effectiveness of the algorithm hinges on the reliability of its value estimates, which are inherently difficult to learn accurately [15, 21], especially for complex tasks with long response horizons. Due to these challenges, value-free methods, which estimate the advantage directly from sampled trajectories without learned value function, have become increasingly popular [27, 40]. Our work is also grounded in this value-free paradigm, foregoing value model to improve training efficiency and stability."
        },
        {
            "title": "3.2 RL Framework for Long-Horizon Agent Tasks",
            "content": "We formalize the long-horizon task as standard reinforcement learning problem. An LLM agent interacts with an environment over trajectory τ = (s0, a0, r0, ..., sT , aT , rT ). The reward signal is sparse, with rt = 0 for all non-terminal steps. Assuming an undiscounted setting (γ = 1), the trajectory return R(τ ) is thus determined solely by the final outcome: (cid:88) R(τ ) = γtrt = rT {0, 1} (3) In our work, single step corresponds to complete \"reason-then-act\" cycle (e.g., as in ReAct [39]), forming multi-step decision-making process. This sparse-reward, long-horizon setting epitomizes two fundamental RL challenges: the credit assignment problem and the exploration problem. t="
        },
        {
            "title": "3.3 Strategies for Learning from Sparse Outcome-Based Rewards",
            "content": "To enable effective learning from sparse, outcome-based rewards in long-horizon tasks, several powerful strategies have emerged that form the foundation of modern LLM RL. Trust Region Learning. Proximal Policy Optimization (PPO) [25] serves as the bedrock algorithm. Its primary innovation is not credit assignment, but ensuring training stability. It achieves this by constraining policy updates within trust region, using clipped objective on the probability ratio ρt(θ) = πθ(atst) πθold (atst) . When applied to sparse reward tasks, PPOs effectiveness fundamentally depends on the quality of its advantage estimates, which implicitly perform the task of credit assignment [15]. Group-Based Advantage Estimation. Group Relative Policy Optimization (GRPO) [27] builds upon this foundation with direct solution for credit assignment. It addresses the high variance of the policy gradient inherent in sparse rewards by sampling multiple responses (M ) and computing Z-score-like advantage: Aij = r(xi, yij) meanM k=1(r(xi, yik)) stdM k=1(r(xi, yik)) + ϵ (4) 4 Here, r(xi, yij) is the final outcome-based reward for the j-th response, and ϵ is small constant added for numerical stability. This comparative evaluation effectively identifies the best-in-batch responses, providing robust signal. Adaptive Data Curation. Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) [40] further refines the learning process by curating the data itself. It addresses failure modes in GRPO by filtering and resampling trajectories to form more informative training batches. By focusing updates on buffer of high-quality samples, it improves the efficiency of learning from the sparse reward signal. While powerful, these strategies share common reliance on processing external, outcome-based reward signals. As they are primarily designed for single-turn generation, they treat entire action sequences as monolithic blocks. When applied to interactive agent tasks, this leads to coarse, trajectory-level credit assignment that fails to pinpoint which specific actions in long sequence were critical for success. This approach ignores the rich, intrinsic signals available at each step of the generative process. Our work diverges by proposing new paradigm that peers inside the model, leveraging its intrinsic, step-wise uncertainty."
        },
        {
            "title": "3.4 Theoretical Motivation: A Two-Part Re-Calibration of Policy Gradients",
            "content": "Our approach is motivated by fundamental analysis of the relationship between policys gradient and its predictive uncertainty. Standard policy gradients, while effective, possess an inherent dynamic that can hinder stable and efficient learning. Specifically, the magnitude of the gradient is inherently coupled with the policys entropy, often leading to inefficiently small updates for confident actions and potentially destabilizing large updates for uncertain ones. This dynamic, which we aim to re-calibrate, is formally characterized by the following proposition. Proposition 1. For policy πθ parameterized by softmax over logits zθ(s), the expected squared L2-norm of the score function zθ log πθ(as) with respect to the logits is direct function of the policys Rényi-2 entropy [24], H2(π): Eaπθ(s) (cid:2)zθ(s) log πθ(as)2(cid:3) = 1 exp(H2(π)) (5) detailed proof is provided in Appendix A. Equation (5), which builds upon established relationships between different measures of policy entropy (e.g., in Li [19]), proves that the expected gradient norm is monotonically coupled with policy entropy. This presents dual challenge: 1) confident and correct step should be reinforced strongly, but its naturally small gradient limits its impact; and 2) the large gradients from highly uncertain exploratory steps can introduce noise and destabilize training. Our first component, Self-Calibrating Gradient Scaling, directly addresses this by re-calibrating the magnitude of the update based on current-step uncertainty. However, re-calibrating the update magnitude is only half the solution. truly effective learning signal must also guide the agent in useful direction. This motivates our second component, the Future Clarity Bonus, which can be conceptually justified through the lens of information theory. By providing an intrinsic motivation for the agent to seek low-entropy next states, the bonus encourages actions that yield high Information Gain about the optimal future path. This corresponds to local, step-wise objective of minimizing the policys entropy at the next state: H(cid:0)πθ(st+1)(cid:1). min at (6) This objective, which aligns with established principles like the Empowerment framework [16], imbues the agent with generalizable meta-skill: to actively seek clarity in the face of ambiguity. In summary, EMPG provides complete, two-part re-calibration of the learning signal. The gradient scaling module ensures each update has an appropriate magnitude, while the future clarity bonus provides principled intrinsic motivation that shapes the policys direction towards robust and predictable solution paths."
        },
        {
            "title": "4 Entropy-Modulated Policy Gradients",
            "content": "Building on the theoretical motivation established in our preliminaries, we introduce Entropy-Modulated Policy Gradients (EMPG), framework designed to re-calibrate the learning dynamics of policy gradients for long-horizon agent tasks. As shown in Section 3.4, standard policy gradients are inherently biased towards applying smaller updates to confident 5 (low-entropy) steps and larger updates to uncertain (high-entropy) ones. EMPG is engineered to counteract this behavior, enabling more efficient and stable learning from sparse, outcome-based rewards."
        },
        {
            "title": "4.1 Quantifying Step-Level Uncertainty",
            "content": "The core of our method is to quantify the agents confidence at each decision-making step. While various uncertainty measures exist, we opt for practical and computationally efficient proxy: the average token-level entropy over single \"reason-then-act\" step. For step stept composed of tokens {w1, ..., wm}, the step-level entropy Ht is: Ht = 1 (cid:88) (cid:88) j= vV p(vw<j) log p(vw<j) (7) where p(vw<j) is the probability of token from the vocabulary , as provided by the LLMs policy πθ. lower Ht indicates higher confidence in the generated step, corresponding to lower-entropy state in the sense of Proposition 1. While we use policy entropy for its computational efficiency, future work could explore alternative uncertainty estimators, such as those derived from Monte Carlo dropout or the variance in logits from an ensemble of model heads. However, we believe entropy provides the most direct link to the gradient dynamics analyzed in Proposition 1, making it the most theoretically grounded choice for our framework."
        },
        {
            "title": "4.2 The Modulated Advantage for Gradient Re-Calibrating",
            "content": "In the sparse reward setting, standard RL advantage function provides uniform learning signal for all steps within single trajectory. While simple, this approach overlooks the varying contributions of different steps and their impact on learning stability. To address this, we introduce novel, modulated advantage estimate, Amod, for each step in trajectory τi: Amod(i, t) = A(i) g(H (i) ) (cid:124) (cid:123)(cid:122) (cid:125) self-calibrating gradient scaling + ζ (H (i) t+1) (cid:124) (cid:125) (cid:123)(cid:122) future clarity bonus (8) This formulation fundamentally re-calibrates the learning signal through two complementary forms of advantage shaping. The first term utilizes step-level entropy-based function g(H (i) ) to dynamically reweight the trajectorys shared advantage A(i), thereby achieving more granular and confidence-aware gradient update. The second term, future clarity bonus, is an additive shaping signal that encourages the agent to select actions that lead to more predictable and less ambiguous future state. Together, these two mechanisms transform coarse, trajectory-level signal into rich and precise learning signal for each step, which we analyze further in the following sections. Self-Calibrating Gradient Scaling g(H). To counteract the natural gradient dynamics, the scaling function g(H) is designed to be self-calibrating and adaptive. It achieves this by enforcing the constraint that the mean of g(H (i) ) over any given mini-batch is normalized to one. Mathematically, for mini-batch of size NB, this constraint is given by: 1 (cid:80)NB i=1 Ti NB(cid:88) Ti(cid:88) i= t=1 g(H (i) ) = 1 (9) This principled design ensures the modulation redistributes the learning signal rather than simply inflating or deflating it, offering stability, adaptivity, and reduction in hyperparameters. We implement this by normalizing base exponential function by its mean over the mini-batch: g(H (i) ) = 1 (cid:80)NB j=1 Tj exp(k (i) (cid:80)Tj norm,t) t=1 exp(k (i) (cid:80)NB j=1 norm,t) (10) is lower than the batch average), g(H (i) For confident step (H (i) ) > 1, which amplifies its gradient. This accelerates convergence for confident and correct decisions (A(i) > 0) and provides strong corrective penalty for confident errors (A(i) < 0), combating \"hallucinated confidence\". Conversely, for an uncertain step (H (i) is higher than average), g(H (i) ) < 1, which attenuates its gradient, preventing noisy updates from high-entropy exploration from destabilizing the policy. Algorithm 1 Entropy-Modulated Policy Gradients (EMPG) 1: Initialize: Policy πθ. 2: for each training iteration do 3: 4: 5: 6: Collect batch of trajectories = {τi} by running policy πθ. Calculate outcome-based advantages A(i) for each trajectory τi B. Compute all step-level entropies {Ht} for all steps in the batch. Normalize all entropies {Ht} to {Hnorm,t} using batch min-max scaling. Compute the self-calibrating scaling factors {g(Ht)} for all steps using Eq. 10. for each step in each trajectory τi do Calculate future clarity bonus (H (i) t+1) using Eq. 11. Compute modulated advantage Amod(i, t) using Eq. 8. end for Normalize the batch of all modulated advantages to get {Afinal(i, t)}. Update policy parameters θ using policy gradients with {Afinal(i, t)}. 7: 8: 9: 10: 11: 12: 13: 14: end for Future Clarity Bonus (H). Beyond re-calibrating individual step updates, EMPG also encourages the agent to find globally stable and predictable solution paths. The second term in Eq. 8 serves as an intrinsic motivation for this goal: (H (i) t+1) = exp(k (i) norm,t+1) (11) This term adds positive bonus proportional to the confidence (low entropy) of the next step. Weighted by the hyperparameter ζ > 0, this \"future clarity\" bonus actively guides the agent away from states of high confusion and towards sequences of high-quality, unambiguous decisions."
        },
        {
            "title": "4.3 Normalization Procedures",
            "content": "Batch-Level Entropy Normalization. To ensure the modulation function g(H) operates on consistent scale, we normalize step-level entropies within each training batch using min-max scaling. This stateless approach allows the normalization to adapt dynamically to the policys evolving confidence level. For each entropy value Ht in the batch: (i) norm,t = minbatch(H) maxbatch(H) minbatch(H) + ϵ (12) Final Advantage Normalization. After computing the modulated advantage Amod for all steps in batch, we perform final batch-level normalization (zero mean). This standard variance reduction technique, which is crucial for stable policy updates, is achieved by subtracting the mean of Amod over the mini-batch of size NB: Afinal(i, t) = Amod(i, t) 1 NB NB(cid:88) Tj (cid:88) j= tj =1 Amod(j, tj) (13) The overall EMPG algorithm is summarized in Algorithm 1, with an implementation provided in the appendix E. Furthermore, we provide rigorous theoretical derivation for the EMPG update rule in Appendix B."
        },
        {
            "title": "5.1 Experimental Setup",
            "content": "Tasks and Benchmarks. We evaluate our method on three challenging long-horizon agent benchmarks featuring sparse, binary success rewards: WebShop [38], web navigation task requiring complex instruction following; ALFWorld [29], text-based environment combining instruction following with common-sense reasoning; and Deep Search [13], multi-step information retrieval and synthesis task. For Deep Search, we further categorize the evaluation sets into in-domain (ID) and out-of-domain (OOD) to assess generalization. 7 Models and Agent Framework. Our agent employs the ReAct paradigm [39], where the LLM first generates thought before producing an action. For WebShop and ALFWorld, we use Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct to compare our results with existing work. For the more complex Deep Search task, we use the powerful Qwen2.5-32BInstruct model to conduct in-depth analysis. Baselines and Implementation. We compare EMPG against strong policy gradient baselines: GRPO [27] and DAPO [40]. Our method, EMPG, is implemented as an advantage modulation module that is applied directly on top of these baselines. This allows us to fairly measure the benefits of leveraging intrinsic uncertainty signals. For the WebShop and ALFWorld benchmarks, we based our implementation on the public codebase of GiGPO [8] for fair comparison. For the DeepSearch benchmark, we curated training dataset of 17k instances by filtering from several sources, including WebWalker [35], HotpotQA [37], 2WikiMultiHopQA [11], NaturalQuestions [17], and TriviaQA [14]."
        },
        {
            "title": "5.2 Main Results",
            "content": "Our comprehensive experiments demonstrate that EMPG yields significant and consistent performance improvements across diverse range of tasks, baselines, and model scales. Performance on ALFWorld and WebShop. As shown in Table 1, EMPG serves as robust enhancement to existing policy optimization algorithms. On the Qwen2.5-1.5B model, applying EMPG boosts the average success rate of GRPO on ALFWorld by +8.1 points and DAPO by +7.3 points. This effectiveness scales to the larger Qwen2.5-7B model, where EMPG again improves both baselines on ALFWorld and elevates the DAPO success rate on WebShop to an impressive 82.7%. These results confirm that EMPG is highly compatible and provides reliable gains for different RL backbones. Performance and Scalability on Deep Search. To investigate the scalability of our approach on more powerful models and complex retrieval tasks, we evaluated EMPG on the Deep Search benchmark using the Qwen2.5-32B-Instruct model. The results, presented in Table 2, further validate our method. Applying EMPG to the strong DAPO baseline boosts the overall average score from 62.0 to 65.3, substantial improvement of +3.3 points. This performance gain is notably robust, with EMPG improving the in-domain average by +3.1 points and demonstrating even stronger generalization with +3.9 point gain on out-of-domain tasks. Summary. Taken together, the results across all three benchmarks confirm that EMPG is versatile and scalable enhancement for training LLM agents. It consistently improves performance regardless of the underlying RL algorithm, the nature of the task, or the size of the base model, validating our core hypothesis that leveraging intrinsic uncertainty is powerful tool for learning from sparse rewards."
        },
        {
            "title": "5.3 Analysis",
            "content": "To understand the mechanisms behind EMPGs effectiveness, we conduct series of in-depth analyses focusing on three key questions: (1) What are the individual contributions of EMPGs core components? (2) How does EMPG affect the learning process over time? (3) Why is step-level analysis of entropy crucial? Ablation Study and Generalization Analysis. To dissect the contributions of our methods two main components, we perform detailed ablation study using the results from the Deep Search benchmark, as presented in Table 2. The study reveals distinct and complementary duality in their roles, which stems from how they shape the policy during training. The Future Clarity Bonus acts as powerful exploitation signal during training. By reinforcing known, high-quality decision sequences within the training data, it helps the model master the in-domain distribution, leading to strong performance gain of +2.6 points on ID tasks. Conversely, the Self-Calibrating Gradient Scaling serves as powerful regularization mechanism during training, teaching the model how to behave when it is uncertain. By attenuating updates for high-entropy steps, it produces final policy that is inherently more robust and less brittle. This learned robustness is then observed during testing on out-of-domain tasks, where the model faces novel inputs that induce high uncertainty. Because the policy has learned not to overreact in such situations, it exhibits superior generalization, providing robust gain of +3.9 points on OOD tasks. This demonstrates that EMPG is not merely overfitting; instead, by learning fundamental skill of how to handle uncertainty, it acquires more resilient problem-solving approach that 8 Table 1 Performance on ALFWorld and WebShop. Results are averaged over 3 random seeds. For ALFWorld, we report the average success rate (%) for each subtask as well as the overall result. For WebShop, we report both the average score and the average success rate (%). Methods marked with * are our reproduced results. The remaining results are adopted from GiGPO [8]. Method Base: Closed-Source Model Prompting GPT-4o Prompting Gemini-2.5-Pro Base: Qwen2.5-1.5B-Instruct Prompting Qwen2.5 Prompting ReAct Prompting Reflexion RL Training PPO (with critic) RL Training RLOO RL Training GRPO* with EMPG* RL Training DAPO* with EMPG* Base: Qwen2.5-7B-Instruct Prompting Qwen2.5 Prompting ReAct Prompting Reflexion RL Training PPO (with critic) RL Training RLOO RL Training GRPO* with EMPG* RL Training DAPO* with EMPG* ALFWorld WebShop Pick Look Clean Heat Cool Pick All Score Succ. 75.3 92.8 60.8 63.3 5.9 17.4 35.3 64.8 88.3 87.9 85.5 88.1 97. 33.4 48.5 62.0 92.3 87.6 88.8 92.9 98.9 99.0 5.5 20.5 22.2 40.5 52.8 40.0 33.5 61.4 80.7 21.6 35.4 41.6 64.0 78.2 43.7 75.2 86.1 86.8 31.2 62.1 3.3 15.7 21.7 57.1 71.0 78.1 78.9 82.5 87.5 19.3 34.3 44.9 92.5 87.3 88.1 74.8 94.9 97. 56.7 69.0 21.6 26.6 9.7 6.2 13.6 60.6 62.8 35.7 76.2 90.1 87.0 6.9 13.2 30.9 89.5 81.3 70.3 86.3 83.2 94.9 4.2 7.7 19.4 46.4 66.4 65.2 74.7 83.9 88.3 2.8 18.2 36.3 80.3 71.9 77.7 73.7 81.4 75. 49.8 58.7 0.0 2.0 3.7 47.4 56.9 44.4 69.1 69.5 80.0 3.2 17.6 23.8 68.8 48.9 56.8 65.3 90.1 90.3 48.0 60.3 4.1 12.8 21.8 54.4 69.7 65.6 73.7 (+8.1) 80.8 88.1 (+7.3) 14.8 31.2 42.7 80.4 75.5 74.8 78.5 (+3.7) 90.0 91.6 (+1.6) 31.8 42.5 23.1 40.1 55.8 73.8 73.9 78.0 80.4 85.9 86.8 26.4 46.2 58.1 81.4 80.3 77.8 81.0 90.6 92.0 23.7 35.9 5.2 11.3 21.9 51.5 52.1 58.2 60.8 (+2.6) 73.2 73.8 (+0.6) 7.8 19.5 28.8 68.7 65.7 65.6 69.3 (+3.7) 79.6 82.7 (+3.1) generalizes effectively. Crucially, the full EMPG model, which integrates both mechanisms, demonstrates powerful synergy: the model learns to efficiently exploit known patterns while being robust to novel ones. Enhancing Training Stability. Beyond improving sample efficiency, EMPG also significantly enhances the stability and robustness of the training process. common failure mode in online RL fine-tuning is \"policy collapse,\" where the agents policy diverges late in training, leading to catastrophic drop in performance. We visualize this phenomenon by tracking the KL Loss during training, as shown in Figure 2. The DAPO baseline agent initially learns effectively, but its KL Loss becomes highly erratic after approximately 240 training steps, indicating severe instability. In contrast, the EMPG-enhanced agent maintains low and stable KL Loss throughout the entire training run. This demonstrates that EMPGs mechanisms, particularly the self-calibrating gradient scaling, effectively regularize the policy updates, preventing the overly aggressive changes that can lead to divergence and ensuring more reliable convergence to high-performance policy. To ensure fair comparison, we select the checkpoint at 220 steps for both the baseline and EMPG for final evaluation. Despite this, our method could continue to improve its performance with further training. Step-Level vs. Token-Level Entropy Dynamics. Our work diverges from prior analyses [31] by focusing on entropy at the \"reason-act\" step level rather than the token level. To validate this choice, we investigate whether the token-level observationthat RL updates primarily affect high-entropy tokensholds at the step level. We analyze over 9,000 steps on ALFWorld and plot the average entropy change for steps, binned by their initial entropy percentile (Figure 3). Our findings are significant: unlike at the token level, even steps with very low initial entropy (e.g., the 15%-20% percentile) still undergo substantial average entropy changes. This shows the dynamics do not transfer; confident step can still require significant policy updates. This key finding underscores the importance of our step-centric approach and motivates the design of EMPG to modulate updates across the entire confidence spectrum. Analysis of Learning Dynamics. An analysis of the learning dynamics, presented in Figure D.1, clearly reveals EMPGs critical role in overcoming the performance limitations of baseline methods. Across all experiments on both the ALFWorld and WebShop benchmarks, the baseline agents consistently reach distinct performance plateau, where their 9 Table 2 Main results on Deep Search tasks, categorized by domain. EMPG demonstrates strong performance on both in-domain (ID) and out-of-domain (OOD) datasets, with particularly notable gain in generalization to OOD tasks. Method WebWalker HotpotQA 2wiki Avg. Musique Bamboogle Avg. Avg. In-domain (ID) Out-of-domain (OOD) Overall Qwen2.5-32B-Instruct DAPO (Baseline) Ablation Studies + Gradient Scaling + Future Bonus + EMPG (Ours) 55.1 54.9 60.6 57.5 66.4 68. 63.5 38.8 80.8 59.8 62.0 68.8 69. 71.2 67.4 67.9 71.0 63.7 66.1 66.6 41.0 40. 41.8 86.4 82.4 84.8 63.7 61.4 63.7 63.7 64. 65.3 Gain vs. Baseline (+2.4) (+4.8) (+2.1) (+3.1) (+3.0) (+4.0) (+3.9) (+3.3) Figure 2 KL Loss dynamics during training for the Qwen2.532B-Instruct model. The DAPO baseline (orange) suffers from late-stage instability, evidenced by the sharp, erratic spike in KL Loss. The EMPG-enhanced model (blue) remains stable throughout, showcasing its robustness. Figure 3 Average entropy change after RL fine-tuning within each 5% entropy percentile range. Unlike token-level findings, even low-entropy steps undergo significant changes, validating our step-level analysis. learning stagnates and the success rate ceases to improve. In stark contrast, the EMPG-enhanced agents decisively break through this performance ceiling. By providing richer and more effective learning signal, EMPG enables the agents to sustain their learning momentum, pushing beyond the baselines peak and ultimately converging to significantly higher final success rate. This demonstrates that EMPG is not just accelerating learning, but is fundamentally guiding the agent to discover superior policies that are otherwise inaccessible, effectively escaping the local optima where the baseline methods become trapped."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced Entropy-Modulated Policy Gradients (EMPG), novel and principled framework to alleviate the long-standing credit assignment problem in long-horizon LLM agent training. By leveraging the intrinsic uncertainty of the agents \"reasoning-action\" steps, EMPG dynamically re-calibrates the policy gradient, moving beyond the limitations of sparse, end-of-task rewards. Our method directly addresses the dual challenges of standard policy gradients: it amplifies updates for confident and correct actions, strongly penalizes confident but incorrect steps, and attenuates updates for uncertain steps to promote stability. Through comprehensive experiments on challenging benchmarks, including WebShop, ALFWorld, and Deep Search, we demonstrated substantial performance gains over strong baselines like GRPO and DAPO. More fundamentally, our work addresses general challenge inherent in policy gradient methods operating in high-dimensional action spaces: the \"entropy-gradient coupling\" problem. We frame EMPG not as domain-specific technique but as general-purpose method for variance reduction and credit assignment, using the policys own uncertainty as an adaptive, step-level baseline. 10 Our findings suggest that an agents intrinsic uncertainty is powerful, yet underexplored, signal for self-supervision in complex decision-making processes. EMPG provides scalable alternative to costly process-based reward models, forging dense, informative learning signal from minimal external feedback. For future work, we plan to explore the application of EMPG to other long-horizon tasks, such as embodied AI and multi-agent collaboration. We believe that this work lays foundational stone for developing more efficient, robust, and self-correcting autonomous agents."
        },
        {
            "title": "References",
            "content": "[1] Shivam Agarwal, Zimin Zhang, Lifan Yuan, Jiawei Han, and Hao Peng. The unreasonable effectiveness of entropy minimization in llm reasoning. arXiv preprint arXiv:2505.15134, 2025. [2] Salaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang, Arda Kaz, Windsor Nguyen, Sewoong Oh, et al. Open deep search: Democratizing search with open-source reasoning agents. arXiv preprint arXiv:2503.20201, 2025. [3] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation. Advances in neural information processing systems, 29, 2016. [4] Minghan Chen, Guikun Chen, Wenguan Wang, and Yi Yang. Seed-grpo: Semantic entropy enhanced grpo for uncertainty-aware policy optimization. arXiv preprint arXiv:2505.12346, 2025. [5] Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. [6] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems, 2023. [7] Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang Wang, and Weipeng Chen. From novice to expert: Llm agent policy optimization via step-wise reinforcement learning. arXiv preprint arXiv:2411.03817, 2024. [8] Lang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training. arXiv preprint arXiv:2505.10978, 2025. [9] Zitian Gao, Lynx Chen, Joey Zhou, and Bryan Dai. One-shot entropy minimization. arXiv preprint arXiv:2505.20282, 2025. [10] Yichen He, Guanhua Huang, Peiyuan Feng, Yuan Lin, Yuchen Zhang, Hang Li, et al. Pasa: An llm agent for comprehensive academic paper search. arXiv preprint arXiv:2501.10120, 2025. [11] Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing multi-hop qa dataset for comprehensive evaluation of reasoning steps. arXiv preprint arXiv:2011.01060, 2020. [12] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [13] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [14] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. Triviaqa: large scale distantly supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551, 2017. [15] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Accurate credit assignment in rl for llm mathematical reasoning. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. [16] Alexander Klyubin, Daniel Polani, and Chrystopher Nehaniv. Empowerment: universal agent-centric measure of control. In IEEE Congress on Evolutionary Computation, 2005. [17] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: benchmark for question answering research. Transactions of the Association for Computational Linguistics, 2019. [18] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366, 2025. [19] Yingru Li. Logit dynamics in softmax policy gradient methods. arXiv preprint arXiv:2506.12912, 2025. [20] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In International Conference on Learning Representations, 2023. [21] Jiacai Liu, Chaojie Wang, Chris Yuhao Liu, Liang Zeng, Rui Yan, Yiwen Sun, Yang Liu, and Yahui Zhou. Improving multi-step reasoning abilities of large language models with direct advantage policy optimization. arXiv preprint arXiv:2412.18279, 2024. [22] Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In International Conference on Machine Learning, 1999. 12 [23] Deepak Pathak, Pulkit Agrawal, Alexei Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, 2017. [24] Alfréd Rényi. On measures of entropy and information. In Proceedings of the fourth Berkeley symposium on mathematical statistics and probability, volume 1: contributions to the theory of statistics, 1961. [25] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [26] ByteDance Seed, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, et al. Seed1. 5-thinking: Advancing superb reasoning models with reinforcement learning. arXiv preprint arXiv:2504.13914, 2025. [27] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [28] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [29] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In International Conference on Learning Representations, 2021. [30] Abdullah Vanlioglu. Entropy-guided sequence weighting for efficient exploration in rl-based llm fine-tuning. arXiv preprint arXiv:2503.22456, 2025. [31] Shenzhi Wang, Le Yu, Chang Gao, Chujie Zheng, Shixuan Liu, Rui Lu, Kai Dang, Xionghui Chen, Jianxin Yang, Zhenru Zhang, Yuqiong Liu, An Yang, Andrew Zhao, Yang Yue, Shiji Song, Bowen Yu, Gao Huang, and Junyang Lin. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025. [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In Advances in neural information processing systems, 2022. [33] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. [34] Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, et al. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning. arXiv preprint arXiv:2505.16421, 2025. [35] Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025. [36] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, et al. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562, 2023. [37] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher Manning. Hotpotqa: dataset for diverse, explainable multi-hop question answering. arXiv preprint arXiv:1809.09600, 2018. [38] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Advances in Neural Information Processing Systems, 2022. [39] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations, 2023. [40] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Honglin Yu, Weinan Dai, Yuxuan Song, Xiang Wei, Haodong Zhou, Jingjing Liu, Wei Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yong-Xu Wu, and Mingxuan Wang. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [41] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. arXiv preprint arXiv:2401.07339, 2024. 13 [42] Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, and Dacheng Tao. Consistent paths lead to truth: Self-rewarding reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.08745, 2025. [43] Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question is already half the answer: Fully unsupervised llm reasoning incentivization. arXiv preprint arXiv:2504.05812, 2025. [44] Xingjian Zhang, Siwei Wen, Wenjun Wu, and Lei Huang. Edge-grpo: Entropy-driven grpo with guided error correction for advantage diversity. arXiv preprint arXiv:2507.21848, 2025. [45] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, et al. Sirens song in the ai ocean: survey on hallucination in large language models. Computational Linguistics, pages 145, 2025. [46] Xuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason without external rewards. arXiv preprint arXiv:2505.19590, 2025. [47] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [48] Brian Ziebart, Andrew Maas, Andrew Bagnell, Anind Dey, et al. Maximum entropy inverse reinforcement learning. In The Association for the Advancement of Artificial Intelligence, 2008. [49] Yuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen Zhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning. arXiv preprint arXiv:2504.16084, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Proof of Proposition 1 We aim to prove that Eakπ single action ak, which we state as lemma. (cid:2)z log πk2(cid:3) = 1 (cid:80)V j=1 π2 . The proof requires the result for the gradient norm of Lemma. The squared L2-norm of the score function with respect to the logits, for chosen action ak, is given by: log πk2 = 1 2πk + (cid:80)V j=1 π2 . Proof of Lemma. Let the logits be = (z1, . . . , zV ). The policy is πk = exp(zk)/ (cid:80) of the log-probability log πk with respect to an arbitrary logit zi is log πk The squared L2-norm of the gradient vector log πk is therefore: zi exp(zj). The partial derivative = δik πi, where δik is the Kronecker delta. log πk2 = (cid:88) (δik πi)2 = (1 πk)2 + i=1 (cid:88) (πi)2 i=k = (1 2πk + π k) + π2 = 1 2πk + (cid:88) i=k (cid:88) j= π2 Proof of Proposition 1. The expectation is taken over all possible choices of action ak according to the policy distribution π. Using the result from the lemma: Ekπ (cid:2)z log πk2(cid:3) = = = (cid:88) k=1 (cid:88) k=1 (cid:88) k=1 πk (cid:0)z log πk2(cid:1) πk 1 2πk + π2 (cid:88) j=1 πk 2 (cid:88) k=1 π2 + (cid:88) k=1 πk (cid:88) j=1 π2 (cid:88) π2 (cid:88) j=1 k= (cid:88) π2 j=1 = 1 2 = 1 2 (cid:88) k=1 (cid:88) k=1 π2 + π2 + = 1 (cid:88) k= π2 πk (Factor out constant term) ), we can identify the term (cid:80) π2 Recalling the definition of Rényi entropy of order 2, H2(π) = log((cid:80)V as the collision probability, which is equivalent to exp(H2(π)). Substituting this into our result yields the final information-theoretic form: j=1 π2 Ekπ (cid:2)z log πk2(cid:3) = 1 exp(H2(π)) This completes the proof of the proposition."
        },
        {
            "title": "B Theoretical Foundation of the EMPG Update Rule",
            "content": "In this section, we provide rigorous theoretical justification for the Entropy-Modulated Policy Gradients (EMPG) algorithm. We demonstrate that the EMPG update rule can be formally derived as the gradient of composite objective function, JEMPG(θ). This interpretation substantiates that EMPG is principled optimization method that reshapes the standard reinforcement learning objective to favor policies that are both effective and robust. B.1 The Standard Policy Gradient Objective We begin with the standard objective in policy-based reinforcement learning, which is to maximize the expected total return. In the context of sparse, outcome-based rewards, this objective simplifies to maximizing the expected advantage (return) of trajectory τ : J(θ) = Eτ πθ [A(τ )] where A(τ ) is the scalar return for trajectory τ sampled from the policy πθ. The gradient of this objective is given by the Policy Gradient Theorem: (14) θJ(θ) = Eτ πθ (cid:34)(cid:32)T 1 (cid:88) t=0 θ log πθ(atst) (cid:33) (cid:35) A(τ ) (15) For any single trajectory τ , the gradient estimator is G(τ )(θ) = A(τ ) (cid:80)T 1 t=0 θ log πθ(atst). This formulation reveals the core issue identified in Proposition 1: the contribution of each steps score function, θ log πθ(atst), is weighted uniformly by the trajectorys outcome A(τ ), while its norm is intrinsically coupled with the policy entropy Ht. B.2 The EMPG Composite Objective Function We posit that EMPG performs gradient ascent on composite objective function JEMPG(θ). This objective augments the standard RL objective with term that explicitly accounts for policy uncertainty, thereby decoupling the learning signals magnitude and direction from the policys raw confidence. We define this objective as: JEMPG(θ) = Jextrinsic(θ) + Jintrinsic(θ) (16) Here, Jextrinsic(θ) is re-weighted extrinsic objective that addresses the gradient magnitude problem, and Jintrinsic(θ) is an intrinsic objective that guides the policys direction towards states of higher certainty. B.2.1 The Re-weighted Extrinsic Objective The self-calibrating gradient scaling component of EMPG, A(τ ) g(H (τ ) on modified extrinsic objective. Formally, we define state-dependent weighting function ω(st, θ) = g(H (τ ) is function of the policys entropy at state st. The gradient update for this component is: ), can be interpreted as performing an update ), which G(τ ) extrinsic(θ) = 1 (cid:88) t=0 A(τ ) ω(s(τ ) , θ) θ log πθ(atst) (17) This formulation is equivalent to optimizing the standard objective J(θ) under state-dependent measure, where the contribution of each state is re-weighted. While deriving closed-form objective Jextrinsic(θ) is non-trivial because ω depends on θ in complex manner (via batch statistics), this interpretation is sufficient to justify the update rule. The weighting function ω(st, θ) serves as an adaptive, information-theoretic learning rate that directly counteracts the dynamics described in Proposition 1. It amplifies the learning signal for confident (low-entropy) steps and dampens it for uncertain (high-entropy) steps, thus achieving direct re-calibration of the gradients magnitude. B.2.2 The Intrinsic Clarity Objective The Future Clarity Bonus can be modeled as the gradient of well-defined intrinsic objective function. We define an intrinsic reward, rint , awarded at step for transitioning to state st+1 with high policy clarity: Definition (Clarity Reward). The intrinsic clarity reward at step is function of the policy entropy at the subsequent state st+1: (st+1; θ) = ζ (H(πθ(st+1))) = ζ exp(k Hnorm,t+1) rint (18) This reward incentivizes actions that lead to predictable future states. The corresponding intrinsic objective, Jintrinsic(θ), is the expected cumulative intrinsic reward: Jintrinsic(θ) = Eτ πθ (cid:35) rint (st+1; θ) (cid:34)T 1 (cid:88) t=0 (19) Applying the policy gradient theorem to this objective, and using the immediate intrinsic reward as one-step advantage estimate (a common form of advantage shaping), yields the gradient: θJintrinsic(θ) = Eτ πθ (cid:34)T 1 (cid:88) (θ log πθ(atst)) rint (st+1; θ) = Eτiπθ t=0 (cid:34)Ti1 (cid:88) t= (θ log πθ(atst)) ζ (H (τ ) t+1) (cid:35) (cid:35) This gradient precisely matches the Future Clarity Bonus component of the EMPG update. B.3 Synthesis: The Full EMPG Gradient By combining the gradients of the extrinsic and intrinsic objectives, we recover the full EMPG gradient estimator for single trajectory τ : EMPG(θ) = G(τ ) G(τ ) extrinsic(θ) + θJintrinsic(θ)τ 1 (cid:88) A(τ ) g(H (τ ) ) θ log πθ(atst) + 1 (cid:88) t=0 ζ (H (τ ) t+1) θ log πθ(atst) A(τ ) g(H (τ ) (cid:17) ) + ζ (H (τ ) t+1) θ log πθ(atst) = = t=0 1 (cid:88) (cid:16) t=0 This derivation confirms that the EMPG algorithm performs principled gradient ascent on the composite objective JEMPG(θ). This objective function holistically reshapes the optimization landscape by (1) adaptively scaling the extrinsic reward signal to ensure its magnitude is motivationally salient rather than merely function of policy entropy, and (2) introducing an intrinsic drive towards robust, predictable solution paths. This dual-pronged approach provides theoretical foundation for why EMPG successfully mitigates the challenges posed by the inherent dynamics of standard policy gradients."
        },
        {
            "title": "C Experimental Settings",
            "content": "This appendix provides detailed description of the experimental settings, hardware configurations, and hyperparameter choices for our experiments across the three main benchmarks. Due to the differences in training frameworks and task environments, the settings for WebShop/ALFWorld and Deep Search are described in separate subsections. C.1 WebShop and ALFWorld Experiments Our experiments on WebShop and ALFWorld are conducted within the Verl-Agent framework, an extension of the veRL [28] training codebase specifically designed for training large language model (LLM) agents via reinforcement learning. Verl-Agent provides powerful and scalable platform for long-horizon, multi-turn RL training by enabling fully customizable per-step input structures, history management, and memory modules. It supports diverse set of RL algorithms and rich suite of agent environments, making it highly suitable for our work. 17 (20) (21) (22) (23) (24) For fair comparison, all experiments were re-executed on our hardware platform. While the original experiments were performed using H200 GPUs, our work utilized A100 GPUs due to resource constraints. We observed that the original training scripts for the Qwen2.5-1.5B-Instruct model, designed for 2 H100, would result in out-of-memory errors on A100s. Therefore, we used 4 A100 GPUs for the 1.5B models and 8 A100 GPUs for the 7B models. All baselines were re-trained under the same hardware, seeds, and settings to ensure strict comparability. The key hyperparameters for these experiments are summarized in Table 3. Table 3 Key Hyperparameters for WebShop and ALFWorld Experiments. Parameter Value Actor Learning Rate KL Loss Coefficient KL Penalty Entropy Coefficient Clip High (DAPO) Clip Low (DAPO) Clip Low/High (GRPO) Batch Size Training Step Rollout Group Size Rollout Temperature ζ k, Max Actions (ALFWorld) Max Actions (WebShop) History Observation GPUs 1e-6 0.01 low var kl 0.001 0.28 0.2 0.2 16 150 8 1.0 0.05 1.0 50 15 2 4 A100 (1.5B), 8 A100 (7B) C.2 Deep Search Experiments Our experiments on the Deep Search task were conducted using proprietary RL training framework from ByteDance. The agent was equipped with two primary tools: Bing Search as the search engine and web viewer tool capable of reading web page content and summarizing long articles. key part of the Deep Search training was the data curation process. We constructed unique training dataset of 17,000 instances by filtering from variety of public benchmarks, including WebWalker [35], HotpotQA [37], 2WikiMultiHopQA [11], NaturalQuestions [17], and TriviaQA [14]. We gratefully acknowledge the initial data collection and preliminary filtering by the DeepResearcher team [47]. We performed two deeper filtering steps: 1. Direct Answer Filtering: We sampled 5 results per question using Doubao-Seed-1.6 (Thinking) [26]. We then filtered out all questions that could be answered directly (where at least one of the 5 results was correct) to ensure the agent learns to use its search tools rather than relying on memorized answers. 2. Agent Workflow Filtering: We further filtered the dataset by sampling 8 results using search workflow built on Doubao-Seed-1.6 (Thinking). We removed data points that were \"stably all-correct\" to focus the RL training on more challenging instances and improve training efficiency. The key hyperparameters for the RL training on the Deep Search task are detailed in Table 4."
        },
        {
            "title": "D Analysis of Learning Dynamics",
            "content": "This section provides detailed visualization of the learning dynamics, complementing the analysis in the main body of the paper. Figure D.1 illustrates the training progress of EMPG-enhanced agents compared to their baseline counterparts (GRPO and DAPO) on both the WebShop and ALFWorld benchmarks. As shown in the learning curves, the baseline agents consistently hit performance ceiling, with their success rates stagnating early in the training process. In contrast, 18 Table 4 Key Hyperparameters for Deep Search Experiments. Parameter Value Actor Learning Rate KL Loss Coefficient KL Penalty Entropy Coefficient Clip High Clip Low Batch Size Training Step Rollout Group Size Rollout Temperature ζ k, Max Actions GPUs 1e-6 0.001 low var kl 0.0 0.28 0.2 64 220 16 1.0 0.1 1.0 15 32 A100 our EMPG-enhanced agents overcome this plateau, sustaining their learning momentum to achieve significantly higher final success rates across all settings. This evidence supports our central claim that EMPG provides more effective learning signal, enabling agents to escape the local optima that trap standard policy gradient methods."
        },
        {
            "title": "E Algorithm Implementation Details",
            "content": "We provide PyTorch-style pseudocode implementation for the core logic of our method in Algorithm 2. This function calculates the final modulated advantage, Afinal, used for the policy update, as detailed in Section 4. The process consists of four main stages: 1. Step-Level Entropy Collection: The function first iterates through the batch of trajectories to identify agent action steps (i.e., the assistant responses). For each step t, it computes the corresponding step-level entropy Ht by averaging the policys token-level entropies for that action. 2. Modulation Component Calculation: All collected step entropies {Ht} are normalized across the batch using min-max scaling to produce {Hnorm,t} (as per Eq. 12). These normalized values are then used to compute the two key components of our method: the self-calibrating scaling factor g(Ht) (Eq. 10) and the future clarity bonus term g(Ht+1) (Eq. 11). 3. Advantage Modulation: The function then applies these components to the original outcome-based advantage. For each step, the advantage is scaled by g(Ht) and augmented by the future clarity bonus ζ g(Ht+1), yielding the modulated advantage Amod as defined in our main formula (Eq. 8). 4. Final Normalization: Finally, to reduce variance and ensure stable training, the entire batch of resulting modulated advantages is normalized to have mean of zero. This produces the final advantage Afinal (Eq. 13) that is used to compute the policy gradient. (a) WebShop: GRPO (b) WebShop: DAPO (c) ALFWorld: GRPO (d) ALFWorld: DAPO Figure D.1 Learning dynamics comparison for the Qwen2.5-7B-Instruct model on the WebShop and ALFWorld benchmarks (evaluated on the validation set). In all four scenarios, the EMPG-enhanced agents (orange, dashed) demonstrate superior success rate compared to their respective baselines (blue, solid). Algorithm 2 PyTorch-Style Pseudocode for EMPG Advantage Calculation 1 import numpy as np 2 import torch 3 4 def compute_empg_advantage(tokenizer, batch, k=1.0, k_f=1.0, zeta=0.1): 5 \"\"\" Args: 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 61 63 64 65 66 67 69 70 71 tokenizer: The tokenizer for identifying response segments. batch: data batch with responses, old_entropy, advantages. (float): Hyperparameter for self-calibrating gradient scaling. k_f (float): Hyperparameter for the future clarity bonus. zeta (float): Hyperparameter for the future clarity bonus. \"\"\" # --- 1. First Pass: Collect Step-Level Entropies --- all_step_entropies = [] # segments_to_modify stores {sample_idx, start, end} for each step segments_to_modify = [] for in range(batch.batch.batch_size[0]): # Find \"assistant\" segments, which correspond to agent steps. token_segments = process_token_sequences( batch.batch[responses][i], tokenizer.encode(\"<im_start>assistantn\"), tokenizer.encode(<im_end>) ) for start, end in token_segments: if start >= end: continue # Calculate the average token-level entropy for the step step_entropy = batch.batch[old_entropy][i][start:end].mean().item() all_step_ entropies.append(step_entropy) segments_to_modify.append({sample_idx: i, start: start, end: end}) if not all_step_entropies: return # --- 2. Calculate Modulated Advantage Components --- = np.array(all_step_entropies) # Batch-level entropy normalization (Eq. 12) with epsilon = 1e-8 min_H, max_H = np.min(H), np.max(H) H_norm = (H - min_H) / (max_H - min_H + 1e-8) # Self-calibrating gradient scaling g(H) (Eq. 10) g_H_unnormalized = np.exp(-k * H_norm) mean_g_H = np.mean(g_H_unnormalized) g_H = g_H_unnormalized / (mean_g_H + 1e-8) # Future clarity bonus f(H) (Eq. 11) f_H = np.exp(-k_f * H_norm) # Convert to tensors for PyTorch operations g_H = torch.tensor(g_H, device=batch.batch[advantages].device, dtype=torch.float32) f_H = torch.tensor(f_H, device=batch.batch[advantages].device, dtype=torch.float32) # --- 3. Second Pass: Apply Advantage Modulation (Eq. 8) --- step_advantages = [] for i, segment in enumerate(segments_to_modify): idx, start, end = segment[sample_idx], segment[start], segment[end] # Apply self-calibrating gradient scaling batch.batch[advantages][idx][start:end] *= g_H[i] # Add future clarity bonus if there is next step next_seg = segments_to_modify[i+1] if i+1 < len(segments_to_modify) else None if next_seg and next_seg[sample_idx] == idx: batch.batch[advantages][idx][start:end] += zeta * f_H[i+1] step_advantages.append(batch.batch[advantages][idx][start]) # --- 4. Final Advantage Normalization (Eq. 7) --- if step_advantages: final_adv_mean = torch.mean(torch.stack(step_advantages)) batch.batch[advantages] -= final_adv_mean"
        }
    ],
    "affiliations": [
        "ByteDance Seed"
    ]
}