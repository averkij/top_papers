{
    "paper_title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
    "authors": [
        "Junjie Ye",
        "Yuming Yang",
        "Yang Nan",
        "Shuo Li",
        "Qi Zhang",
        "Tao Gui",
        "Xuanjing Huang",
        "Peng Wang",
        "Zhongchao Shi",
        "Jianping Fan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge."
        },
        {
            "title": "Start",
            "content": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels Junjie Ye1*, Yuming Yang1, Yang Nan1, Shuo Li1, Qi Zhang1,3, Tao Gui1,3,4, Xuanjing Huang1,3, Peng Wang2, Zhongchao Shi2, Jianping Fan2 1Fudan University 2Lenovo Research, Beijing, China 3Shanghai Key Lab of Intelligent Information Processing 4Shanghai Innovation Institute jjye23@m.fudan.edu.cn, tgui@fudan.edu.cn 5 2 0 2 0 2 ] . [ 1 6 9 5 6 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on models knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) (Bai et al., 2022b; OpenAI, 2023; Team, 2024; Yang et al., 2024a) acquire extensive world knowledge through pretraining on massive text corpora (Chen et al., 2023; Ye et al., 2023). This knowledge is subsequently shaped through post-training techniques such as supervised fine-tuning (SFT) (Yang et al., 2024b) and reinforcement learning (Bai et al., 2022a), enabling LLMs to perform diverse downstream tasks, including reading comprehension (Samuel *Equal Contribution. Corresponding Author. Figure 1: Illustration of parameter restoration. We find that SFT introduces many unnecessary parameter updates, and model performance can be significantly improved by restoring some of the most updated parameters in the fine-tuned model to their original values in the pre-trained model. et al., 2024), code generation (Rozière et al., 2023), and tool use (Ye et al., 2024a,b). Recent research has explored how model knowledge evolves during training. For instance, pretraining has been shown to encode knowledge modularly (Wang et al., 2024), with each parameter storing up to 2 bits of information (Allen-Zhu and Li, 2025). instruction fineConversely, tuning may increase hallucinations (Gekhman et al., 2024; Ghosh et al., 2024). Empirical evidence suggests that preserving the distribution of internal representations is crucial to maintaining performance (Ren et al., 2024), and models with richer knowledge can be easier to fine-tune for enhanced reasoning ability (Ye et al., 2025). Despite these insights, the specific impact of SFT on model knowledge remains insufficiently understood. Key open questions include how model knowledge changes with different categories and scales of fine-tuning data, the mechanisms behind these changes, and strategies to mitigate undesirable effects. This gap limits our ability to predict and control knowledge change behavior in fine-tuned models. To address this, we evaluate five LLMs from the LLaMA-2 and LLaMA-3 families on the closed-book question answering (CBQA) task. We categorize fine-tuning data into five groups based on the knowledge mastery level and systematically examine how performance varies across these categories and different data scales. Surprisingly, models fine-tuned with 1,920 samples perform up to 14% worse than those fine-tuned with only 240 samples. Moreover, performance fluctuates by over 12% depending on the data category used. To investigate these discrepancies, we conduct token-level analysis by computing the KullbackLeibler (KL) divergence (Kullback and Leibler, 1951) between token logits of fine-tuned and pre-trained models (Section 4). Our results show that as fine-tuning data size increases, KL divergence initially decreases, reflecting reduced deviation from the pre-trained model. However, beyond threshold, KL divergence sharply rises, especially when fine-tuning on poorly mastered data, correlating with performance degradation. Building on these findings, we perform parameter-level analysis (Section 5) by selectively restoring parameters that changed most during SFT back to their pre-trained values (Figure 1). We observe that restoring up to 90% of parameter updates does not harm and can even improve performance on training and test sets, with improvements exceeding 10% in some cases. This indicates that many SFT-induced updates are unnecessary for knowledge enhancement, suggesting new directions for optimizing fine-tuning. In summary, our contributions are as follows: We conduct extensive experiments on the CBQA task and reveal surprising effects of fine-tuning data category and scale on model knowledge. Through token-level and parameter-level analyses, we find that 90% of the parameter updates from fine-tuning do not contribute to knowledge enhancement. We demonstrate that restoring these parameters improves model performance, offering practical guidance for more effective finetuning strategies."
        },
        {
            "title": "2 Related Work",
            "content": "CBQA and Model Knowledge The CBQA task evaluates an LLMs ability to answer factual questions using its internal knowledge, without relying on external reference materials (Zhang et al., 2024; Wen et al., 2024; Sticha et al., 2024). This makes CBQA rigorous test of the models knowledge accuracy and completeness. One significant challenge in CBQA is addressing hallucinations-instances where the model generates incorrect or fabricated answers (Huang et al., 2023; Kandpal et al., 2023; Kang and Choi, 2023). To mitigate hallucinations and enhance performance, several strategies have been proposed. For instance, Ren et al. (2024) investigate the impact of fine-tuning on the consistency of models pre-existing knowledge, emphasizing the need for stable knowledge retention during finetuning. Similarly, Gekhman et al. (2024) identify overfitting to fine-tuning data as major source of hallucinations, noting that fine-tuning with data unfamiliar to the model exacerbates this issue. Additionally, Ye et al. (2024c) examine how variations in dataset size and quality influence CBQA outcomes, highlighting the trade-offs between data volume and model performance. Despite these advances, prior studies primarily focus on dataset characteristics and overlook the fine-tuning processs internal dynamics. In contrast, our work provides detailed analysis at both the token and parameter levels, identifying unnecessary parameter updates during fine-tuning as key factor contributing to performance degradation on CBQA. Data Quality and Scale of SFT SFT plays pivotal role in adapting LLMs to labeled data, enabling strong performance on downstream tasks. Consequently, constructing high-quality finetuning datasets is critical for maximizing SFTs effectiveness (Muennighoff et al., 2023; Lin et al., 2024; Ma et al., 2024). Recent research highlights the effectiveness of SFT with small, high-quality datasets, achieving performance on par with larger datasets (Zhou et al., 2023; Yang et al., 2025b). High-quality data is typically characterized as accurate, diverse, and complex (Huang et al., 2024; Liu et al., 2024; Ye et al., 2024d; Yang et al., 2025a), prompting efforts to synthesize such datasets automatically (Xu et al., 2023, 2024; Zhu et al., 2024). Concurrently, studies show that scaling the quantity of fine-tuning data, while maintaining quality, can yield further performance improvements (Kaplan et al., 2020; Chung et al., 2022; Wei et al., 2022; Dong et al., 2024). While prior work has explored dataset quality and size, few studies have examined how models prior knowledge of fine-tuning data influences performance or how different data quantities affect the models knowledge. Our study differs by investigating SFT performance on the CBQA task, focusing on how mastery levels and data scale impact model knowledge."
        },
        {
            "title": "3 Experiments",
            "content": "To explore how SFT affects the factual knowledge of LLMs in the CBQA setting, we conduct series In this section, we of controlled experiments. outline the datasets used (Section 3.1), the models tested (Section 3.2), and the experimental setup (Section 3.3), followed by presentation of the results and summary of our findings (Section 3.4)."
        },
        {
            "title": "3.1 Dataset",
            "content": "Following Gekhman et al. (2024) and Ye et al. (2024c), we use the ENTITYQUESTIONS (Sciavolino et al., 2021) to construct the training and testing datasets for our experiments, which is CBQA-specific dataset containing knowledge across 24 topics extracted from Wikipedia. Training Data Our training dataset, denoted as Dtrain, consists of data on 10 location-related topics extracted from the original training corpus. Following the method of Ye et al. (2024c), we classify the training samples based on the pretrained model Ms mastery level on the knowledge associated with each data point k. Specifically, we enhance the multi-template completion mechanism of Ye et al. (2024c) to allow to complete each data point using multiple templates. The training data is then divided into five categories according to the proportion RM of knowledge points correctly completed.1 Formally: DM traini = {k Dtrain RM {k Dtrain RM = 0}, = 0, 4 , ( i1 4 ]}, {1, 2, 3, 4}. Dtrain DM train0 DM train DM train2 DM train3 DM train Number 18456 Dtest DM test0 DM test1 11558 DM test2 DM test3 7436 DM test4 Number Dtestood DM 2383 testood0 DM 4127 3664 testood1 DM 4539 1484 testood2 DM 1271 1109 testood3 DM 1120 915 testood4 Number Table 1: An example of data distribution, where refers to LLaMA-3-8B. remaining 14 topics are used as the out-of-domain testing dataset Dtestood. Similar to the training data, both Dtest and Dtestood are categorized as: Dtest = 4 (cid:91) i=0 DM testi, Dtestood = 4 (cid:91) i=0 DM testoodi An example of data distribution is listed in Table 1."
        },
        {
            "title": "3.2 Models",
            "content": "Given the dominance of decoder-only architectures in current LLMs, our analysis focuses exclusively on models of this type. We examine five LLMs from two model families: LLaMA-2-7B, LLaMA2-13B, and LLaMA-2-70B from the LLaMA2 family (Touvron et al., 2023), and LLaMA3-8B and LLaMA-3-70B from the LLaMA-3 family (Dubey et al., 2024)."
        },
        {
            "title": "3.3 Experimental Setup",
            "content": "Our experiment involves data categorization, training, and testing, aimed at evaluating model performance under diverse settings. Data Categorization To balance the stability and diversity of the generated output, we design 21 mapping templates tailored to each topics data. The sampling temperature is set to 0.7 to introduce controlled randomness, and each prompt is sampled 10 times to enhance robustness. The outputs maximum token length is limited to 32. using conducted Training Training is batch size of 8 over 1 epoch, employing the AdamW (Loshchilov and Hutter, 2019) optimizer with cosine learning rate scheduling for stable and efficient convergence. The learning rate is set to 1 105.4 Testing Data For the in-domain testing dataset Dtest, we select data from the same 10 locationrelated topics in the original test set. Data from the 1For additional details on data processing, see Appendix F. 2Data distribution of other LLMs can be found in Appendix D. 3Details of models can be found in Appendix B.1. 4To ensure fair comparison, we use uniform prompt templates during training, as detailed in Appendix A. (a) LLaMA-3-8B (In-Domain) (b) LLaMA-3-8B (Out-of-Domain) (c) LLaMA-3-70B (In-Domain) (d) LLaMA-3-70B (Out-of-Domain) Figure 2: In-domain (AccM tuned with varying data scales, where All indicates the use of the entire dataset listed in Appendix D. test) and out-of-domain (AccM testood) performance of the LLaMA-3 family models fineTesting For testing, we utilize greedy decoding strategy with maximum output length of 16, maintaining consistency with the prompt templates used during training. To mitigate bias from the training data selection, we generate five distinct training datasets by random sampling. Each experiment is repeated using these datasets, and the final results are reported as the mean and variance across the five runs. Evaluation metrics include accuracy, categorized by different knowledge mastery levels, with the mean accuracy across all test sets serving as the final metric:"
        },
        {
            "title": "AccM",
            "content": "test ="
        },
        {
            "title": "AccM",
            "content": "testood = 4 (cid:88) i=0 4 (cid:88) i="
        },
        {
            "title": "AccM",
            "content": "testi/"
        },
        {
            "title": "AccM",
            "content": "testoodi/"
        },
        {
            "title": "3.4 Main Results",
            "content": "We fine-tune each of the five selected LLMs using datasets with five different mastery levels. To conduct more detailed analysis, we compare changes in model performance across varying data scales. To enhance robustness, we ensure balanced data distribution across topics and repeat each experiment three times. Figure 2 presents the in-domain and out-of-domain test results for the LLaMA-3 family of models.5 From the results, we observe two unexpected phenomena. Phenomenon 1 Regardless of the type of training data used, LLMs achieve their optimal performance with just 240 data points. Adding more training data beyond this point risks degrading model performance. Our analysis reveals that model performance improves as the amount of fine-tuned data increases from 60 to 240 entries, aligning with the general expectation that more data enhances performance. However, performance peaks at only 240 entries, and adding additional fine-tuned data not only fails to yield further improvements but often leads to significant decline. For instance, when fine-tuned with barely mastered data (i.e., DM train0), LLaMA3-8B achieves an AccM test score that is 8.86% lower 5Test results for the LLaMA-2 family of models can be found in Appendix C.1. Source DM DM DM DM DM train0 train1 train2 train train4 DM DM DM DM DM train0 train1 train2 train train4 In-Domain test2 AccM Out-of-Domain testood2 AccM AccM test0 AccM test1 AccM test3 AccM test4 AccM test AccM testood0 AccM testood1 AccM testood3 AccM testood4 AccM testood 1.750.17 0.980.14 0.780.03 0.640.15 0.640.06 3.720.33 1.940.11 1.230.07 1.000.11 0.900.05 16.070.67 40.120.74 36.560.53 27.203.69 24.263. 55.031.39 63.930.55 75.611.18 70.331.73 68.282.00 71.061.09 74.190.73 83.981.37 85.901.47 83.291.23 83.461.23 84.223.96 90.711.31 91.661.57 93.191.91 22.681.53 43.850.29 38.171.78 31.520.61 26.161.45 47.281.26 63.451.47 71.680.82 68.320.30 64.270.75 57.972.25 66.221.66 77.581.27 81.110.73 78.000. 72.083.20 79.540.65 85.891.44 88.491.60 89.830.77 = LLaMA-3-8B 1.910.33 1.660.09 1.450.35 1.390.34 0.930.11 45.470.40 52.690.88 57.530.86 55.151.64 53.931.56 = LLaMA-3-70B 3.080.39 2.610.45 2.060.50 1.910.79 0.810.35 40.751.51 51.000.53 54.910.89 54.090.45 51.830.05 15.891.20 23.880.45 25.020.30 21.663.13 17.721. 25.901.59 31.010.79 31.262.10 26.701.71 21.803.65 59.010.51 65.030.77 70.521.59 63.912.70 63.644.39 67.041.63 72.630.16 74.511.27 69.602.77 66.525.65 74.080.63 79.630.63 83.660.67 81.340.93 80.552.05 82.610.95 84.690.30 88.630.97 89.611.44 84.852.57 80.330.98 83.840.55 87.890.45 86.871.85 88.431. 85.741.30 86.220.69 92.011.19 91.221.39 92.292.63 46.240.29 50.800.45 53.710.49 51.041.73 50.251.83 52.870.79 55.430.26 57.691.16 55.811.47 53.252.97 Table 2: Performance of the fine-tuned LLaMA-3 family models on in-domain and out-of-domain test sets, using 1920 data points with varying levels of mastery. rate of decline differs depending on the knowledge mastery level of the training data. Notably, models fine-tuned with data from DM train0 exhibit steeper performance drop compared to those trained on other data types. For instance, when finetuned with 1,920 entries, the AccM test difference between LLaMA-3-8B models trained on DM train0 and DM train2 reaches 12.06%, which is 1.50 times the difference observed with only 240 training entries. Table 2 illustrates the performance of LLaMA-3 family models across various test sets when fine-tuned with 1,920 entries from different categories. The results show that models trained on DM train0 experience substantial performance degradation on test sets other than DM test0. More generally, training on low-mastery data significantly impairs performance on high-mastery test data. Conversely, training on high-mastery data (e.g., DM train4) leads to suboptimal performance on low-mastery test data. Training with mid-level mastery data, such as DM train2, strikes better balance, yielding superior overall performance."
        },
        {
            "title": "4 Token-Level Analysis",
            "content": "To explain the performance variation observed across fine-tuned LLMs, we analyze how finetuning alters token-level output distributions compared to the pre-trained model. Specifically, we compute the divergence in predicted token distributions between fine-tuned and pre-trained models using KL divergence (Section 4.1). This tokenlevel analysis reveals some interesting findings (Section 4.2)."
        },
        {
            "title": "4.1 KL Divergence Computation",
            "content": "Figure 3: Illustration of logits re-normalization. Since the pre-trained LLM tends to assign high probabilities to common dummy words, we identify the ten highest logits in the fine-tuned LLM and extract the corresponding values from the pre-trained LLM. After re-normalization, we compute the KL divergence to quantify the distributional difference. when trained with 1,920 entries compared to 240 entries. decline of 13.69% is even observed when comparing 240 entries from DM train2. Notably, when LLMs are trained with the full dataset for each data category, their performance on the CBQA task is nearly at its lowest across all data categories. This striking finding suggests that increasing the volume of fine-tuned data does not necessarily enhance model knowledge and may impair it. Phenomenon 2 When the amount of fine-tuned data reaches certain threshold (e.g., 1,920 entries), model performance varies significantly based on the knowledge mastery level of the training data. While model performance generally declines when the fine-tuned data exceeds 240 entries, the Given the performance degradation observed in Section 3.4, we investigate the underlying token Figure 4: Performance on DM fine-tuned on LLaMA-3-8B. test4 (AccM test4) of LLMs distribution shifts caused by SFT. Specifically, we use KL divergence to quantify the differences in token probabilities between fine-tuned and pretrained models. higher KL divergence suggests more significant shift in the models token probability distribution. Data Selection Given that the pre-trained model is used to complement the prior text, the quality of its completions depends on both the input prompt and the structure of the mapping template, as outlined in Section 3.3. The selection of appropriate data is critical to ensuring the robustness of the results. For DM test4, we observe that the pre-trained models completion success rate exceeds 75% across multiple samples and templates, suggesting that this dataset is relatively insensitive to variations in the mapping template. In contrast, other datasets are more sensitive to such variations, so our comparison of different LLMs in this section is limited to DM test4. For each topic, we select the mapping template yielding the highest success rate across samples and focus our analysis on tokens in completions where the answers appear near the beginning of the generated text. Logits Re-normalization Our goal is to compute the KL divergence between the logits distributions for the first token predicted by both the fine-tuned and pre-trained LLMs. However, as shown in Figure 3, the pre-trained model tends to assign higher probabilities to common dummy words (e.g., the, a, etc.), whereas fine-tuned models typically reduce the likelihood of these words in If we directly favor of more relevant tokens. compute the KL divergence on the raw logits, these dummy words could distort the results and obscure meaningful differences between the Figure 5: KL divergence of logits distribution between LLaMA-3-8B fine-tuned with different datasets and the pre-trained one. models. To mitigate this issue, we introduce logits re-normalization procedure. Specifically, we sort the logits predicted by the fine-tuned model and extract the top 10 values, denoted as l0, l1, . . . , l9. We then identify the corresponding logits, 9, from the pre-trained models completions. Moreover, we apply the softmax function to these logits to derive their normalized probabilities, respectively: 1, . . . , 0, pi = Softmax(li), = Softmax(l i). After completing the logits re-normalization, we compute the KL divergence between the probability distributions and for the fine-tuned and pretrained models as follows: sKL(p p) = (cid:88) pi log pi ."
        },
        {
            "title": "4.2 Results Analysis",
            "content": "We analyze the performance of individual LLMs fine-tuned based on LLaMA-3-8B, presenting their results on DM test4 in Figure 4 and their KL divergence relative to the pre-trained models distribution in Figure 5. From these results, we derive two key findings. Finding 1 Regardless of the category of finetuning data, the difference in predicted logits distributions between the fine-tuned and pre-trained models initially decreases and then increases as the amount of data grows. Figure 5 illustrates how the predicted logits distributions of fine-tuned model diverge from the pre-trained model as training data increases. When fine-tuning with small dataset (e.g., 60 samples), the logits distribution shifts significantly leading to unstable due to insufficient data, training. As the dataset grows (e.g., 240 samples), this discrepancy decreases, indicating improved stability. However, with further increases, the difference in logits distributions grows again, particularly for models trained on DM train0 and DM train1. This suggests that as training data increases, the model deviates further from its pretrained knowledge. The effect is more pronounced when fine-tuning on low-mastery data, making the model more susceptible to knowledge shifts. Finding 2 As the difference in the predicted logits distribution between the fine-tuned model and the pre-trained model increases, model performance declines, indicating negative impact of excessive knowledge shifts. Figure 4 and Figure 5 reveal strong correlation between performance degradation on DM test4 and increasing divergence in logits distributions. Since DM test4 contains samples well mastered by the pre-trained model, substantial shifts in learned knowledge during fine-tuning can lead to catastrophic forgetting, where previously acquired knowledge is lost, thereby degrading performance. This effect is particularly evident when training with large datasets. For instance, the model finetuned on DM train0 experiences the most significant knowledge shift and performs the worst among all fine-tuned models. Since changes in logits distribution reflect underlying modifications to model parameters, we hypothesize that excessive parameter updates during fine-tuning, especially when using large or low-mastery datasets, lead to overall performance decline."
        },
        {
            "title": "5 Parameter-Level Analysis",
            "content": "The observations and analyses in Section 4 indicate that excessive parameter updates can degrade model performance. To further investigate this, we analyze the impact at the parameter level by progressively restoring the updated parameters and examining the resulting performance changes (Section 5.1). Our findings indicate that significant proportion of parameter updates during SFT do not contribute to performance improvement and may even be detrimental (Section 5.2)."
        },
        {
            "title": "5.1 Parameter Restoration",
            "content": "To examine the impact of excessive parameter updates on model performance, we design an experimental framework for parameter restoration. Proportion 1% 3% 5% 10% 20% 40% 60% train0 DM DM DM DM DM train train2 train3 train4 train0 DM DM DM DM DM train train2 train3 train4 Number of Training Data: 240 70.59% 78.82% 82.35% 87.06% 91.76% 96.47% 99.12% 71.01% 79.29% 82.84% 87.57% 92.31% 97.04% 99.11% 71.13% 79.17% 82.74% 87.50% 92.26% 96.43% 99.12% 70.72% 78.97% 82.51% 87.22% 91.93% 96.65% 99.09% 70.98% 78.74% 82.18% 87.36% 91.95% 96.55% 99.04% Number of Training Data: 70.56% 78.50% 82.24% 86.92% 92.06% 96.26% 98.69% 70.89% 78.87% 82.63% 87.32% 92.02% 96.71% 98.69% 70.75% 78.77% 82.08% 87.26% 91.98% 96.70% 98.70% 70.74% 78.70% 81.98% 87.13% 91.82% 96.50% 98.70% 70.83% 78.70% 82.41% 87.04% 92.13% 96.30% 98.70% Percentage of Table 3: total parameter updates concentrated in different proportions of the most highly updated parameters in various LLMs fine-tuned on LLaMA-3-8B. Specifically, we compare the fine-tuned model with the pre-trained model, sorted by the rate of parameter change.6 Table 3 reports the percentage of total parameter updates attributed to different proportions of the most highly updated parameters in LLMs fine-tuned on LLaMA-38B. The results indicate that parameter updates are heavily concentrated in small subset of parameters. For instance, more than 70% of the total updates occur in fewer than 1% of the parameters. Following this, we progressively restore the most significantly updated parameters to their original values in the pre-trained model, starting with the largest updates and gradually including smaller ones, while monitoring the corresponding changes in model performance. This process is illustrated in Figure 1."
        },
        {
            "title": "5.2 Results Analysis",
            "content": "We evaluate the performance of LLaMA-3-8B after restoring different proportions of parameters across various fine-tuning datasets. The results are summarized in Table 4. Our analysis of these results reveals several noteworthy findings. Finding 1 The majority of parameter updates introduced by SFT are unnecessary and can significantly degrade model knowledge.7 Table 4 shows that restoring portion of the models parameters to their pre-trained values consistently improves performance, regardless of the fine-tuning dataset. For instance, when finetuning with 1,920 samples, restoring 20% of the parameters enhances the performance of all models. Specifically, the model fine-tuned with DM train0 achieves 9.85% performance gain. Table 3 6Specific calculation details can be found in Appendix B.2. 7More discussion can be found in Appendix E. Restore DM train0 DM train1 DM train2 DM train3 DM train4 Restore DM train0 DM train1 DM train2 DM train3 DM train4 Number of Training Data: 240 Number of Training Data: 0 1% 3% 5% 10% 20% 40% 60% 0 1% 3% 5% 10% 20% 40% 60% 55.33 55.76 56.64 57.22 58.32 59.07 59.77 1.68 44.96 46.73 48.53 49.85 52.10 54.81 55.44 1.48 57.96 58.17 58.52 58.68 59.45 59.81 33.40 2.20 59.32 59.62 59.77 59.89 60.40 59.88 42.44 3. 59.12 59.24 59.40 59.63 59.83 59.91 11.20 2.56 Number of Training Data: 1920 52.43 53.72 55.01 55.96 57.14 58.33 22.06 1.12 58.80 59.85 60.56 61.10 61.67 62.21 59.97 1.62 57.70 58.68 59.23 59.65 60.02 58.93 6.92 0.51 53.97 54.30 54.31 54.44 54.69 46.45 23.83 1. 55.22 55.88 56.76 57.34 58.24 58.66 56.50 0.60 0 1% 3% 5% 10% 20% 40% 60% 0 1% 3% 5% 10% 20% 40% 60% 52.37 52.62 53.03 53.27 53.44 54.18 53.79 0.20 49.40 50.78 52.03 52.54 53.42 54.50 53.64 0.30 51.70 52.39 52.82 53.09 53.87 54.36 20.77 0. 55.35 56.45 56.47 56.80 56.46 55.95 45.49 0.32 55.23 56.17 56.41 56.56 56.72 55.52 17.56 0.20 Number of Training Data: 1920 52.38 54.20 55.12 55.12 55.08 53.91 20.51 0.10 54.04 55.17 56.00 56.34 56.68 57.10 53.84 0.27 53.79 54.75 55.52 55.84 55.54 52.23 9.67 0. 50.69 50.82 50.74 50.59 49.71 43.13 31.19 0.23 51.70 52.62 53.35 53.77 54.32 53.82 50.17 0.18 (a) In-Domain (AccM test) (b) Out-of-Domain (AccM testood) Table 4: Performance of LLaMA-3-8B after restoring different scales of parameters across various fine-tuning datasets. Improvements over the non-restored model are highlighted in green , while performance declines are shown in red , with darker shades indicating larger differences. Restore 0 1% 3% 5% 10% 20% 40% 60% XSum GSM8K ROUGE-1 ROUGE-2 ROUGE-L ACC 42.57 42.50 42.63 42.36 42.57 41.31 15.59 0 19.50 19.71 19.78 19.47 19.40 18.59 4.15 0 34.55 34.67 34.75 34.44 34.60 33.51 12.09 0 57.69 57.69 57.75 58.49 59.60 58.72 0 Table 5: Performance of LLaMA-3-8B after restoring different scales of parameters on XSum (Narayan et al., 2018) (Summarization) and GSM8K (Cobbe et al., 2021) (Math). further reveals that over 90% of the total parameter variation is restored at this point. Importantly, the benefits of parameter restoration generalize across tasks, as shown in Table 5. However, the degree of improvement depends on the relevance of the task to the models knowledge. Notably, performance on the training set also improves, suggesting that many of the parameter updates introduced by SFT neither help fit the training data nor support generalization, and may impair previously learned knowledge. Compared to other strategies, restoring redundant parameter updates is an effective and simple method for enhancing model performance, offering useful insights for designing more efficient fine-tuning approaches.8 8A comparison of different strategies is presented in Appendix C.3. Finding 2 Models fine-tuned with larger datasets or lower-mastery data are more adversely affected by unnecessary parameter changes during SFT. While SFT consistently introduces unnecessary parameter updates that degrade model performance, the extent of this effect depends on the scale and category of fine-tuning data. On one hand, models fine-tuned with larger datasets experience greater impact. Specifically, models trained with 240 samples generally show performance degradation when more than 20% of the parameters are restored. In contrast, models fine-tuned with 1,920 samples continue to gain performance improvements even after restoring 40% of the parameters. This suggests that fine-tuning with 1,920 samples introduces higher proportion of unnecessary updates. Additionally, the maximum performance gain achieved through parameter restoration is greater for models fine-tuned with 1,920 samples than for those fine-tuned with 240 samples. On the other hand, models fine-tuned with low-mastery data are also more affected. Regardless of dataset size, models fine-tuned with DM train0 consistently allow more parameter restoration while achieving greater performance gains compared to other models. For instance, when using 1,920 samples, the model fine-tuned with DM train0 can restore 40% of the parameters and achieve 10.48% performance gain, whereas the model fine-tuned with DM train4 achieves maximum gain of only 3.44% after restoring 20% of the parameters."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we conduct an in-depth analysis of five LLMs across two families on the CBQA task, revealing that both the category and scale of finetuning data significantly influence performance in unexpected ways. Through token-level analysis, we find that large changes in token logits correlate with degraded model performance, suggesting that excessive parameter updates can harm model knowledge. At the parameter level, we show that up to 90% of the updates made during SFT are unnecessary or even detrimental for knowledge enhancement. By selectively restoring these updates, we improve model performance while preserving prior knowledge. Our findings challenge conventional fine-tuning practices and offer practical guidance for developing more efficient methods for LLMs."
        },
        {
            "title": "Limitations",
            "content": "Although we conduct an in-depth analysis of anomalies arising from SFT, our work has certain limitations. On one hand, the study does not propose more efficient fine-tuning strategy based on the findings. This is because the focus is on phenomenological analysis to uncover the underlying mechanisms of SFT on model knowledge. Future work should focus on designing adaptive fine-tuning strategies that minimize unnecessary updates while maximizing performance gains. On the other hand, due to resource constraints, the analysis is limited to the LLaMA-2 and LLaMA-3 model series. However, preliminary validation on other model families shows that the conclusions generalize, suggesting broader applicability."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors wish to thank the anonymous reviewers for their helpful comments. This work was partially funded by the Science and Technology Commission of Shanghai Municipality (No.24511103100), National Natural Science Foundation of China (No.62476061,62206057), Shanghai Rising-Star Program (23QA1400200), Natural Science Foundation of Shanghai (23ZR1403500)."
        },
        {
            "title": "References",
            "content": "Zeyuan Allen-Zhu and Yuanzhi Li. 2025. Physics of language models: Part 3.3, knowledge capacity In The Thirteenth International scaling laws. Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom B. Brown, Jack Clark, Sam McCandlish, Chris Olah, Benjamin Mann, and Jared Kaplan. 2022a. Training helpful and harmless assistant with reinforcement learning from human feedback. CoRR, abs/2204.05862. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. 2022b. Constitutional CoRR, AI: harmlessness from AI abs/2212.08073. feedback. Xuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. How robust is GPT-3.5 to predecessors? comprehensive study on language understanding tasks. CoRR, abs/2303.00293. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Scaling instructionLe, and Jason Wei. 2022. finetuned language models. CoRR, abs/2210.11416. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue, Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, and Jingren Zhou. 2024. How abilities in large language models are affected by supervised fine-tuning data composition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 177198. Association for Computational Linguistics. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurélien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Rozière, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Graeme Nail, Grégoire Mialon, Guan Pang, Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Imanol Arrieta Ibarra, Touvron, Isabel M. Kloumann, Ishan Misra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, and et al. 2024. The llama 3 herd of models. CoRR, abs/2407.21783. Iliyan Zarov, Jonathan Frankle and Michael Carbin. 2019. The lottery ticket hypothesis: Finding sparse, trainable neural networks. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Zorik Gekhman, Gal Yona, Roee Aharoni, Matan Eyal, Amir Feder, Roi Reichart, and Jonathan Herzig. 2024. Does fine-tuning llms on new knowledge encourage hallucinations? CoRR, abs/2405.05904. Sreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Kumar, Ramaneswaran S., Deepali Aneja, Zeyu Jin, Ramani Duraiswami, and Dinesh Manocha. 2024. closer look at the limitations of instruction tuning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024. OpenReview.net. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Hui Huang, Bing Xu, Xinnian Liang, Kehai Chen, Muyun Yang, Tiejun Zhao, and Conghui Zhu. 2024. Multi-view fusion for instruction mining of large language model. Inf. Fusion, 110:102480. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2023. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. CoRR, abs/2311.05232. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pages 1569615707. PMLR. Cheongwoong Kang and Jaesik Choi. 2023. Impact of co-occurrence on factual knowledge of large language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 77217735. Association for Computational Linguistics. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. CoRR, abs/2001.08361. Solomon Kullback and Richard Leibler. 1951. On information and sufficiency. The annals of mathematical statistics, 22(1):7986. Jianzhe Lin, Maurice Diesendruck, Liang Du, and Robin Abraham. 2024. Batchprompt: Accomplish In The Twelfth International more with less. Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2024. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Ilya Loshchilov and Frank Hutter. 2019. Decoupled In 7th International weight decay regularization. Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At which training stage does code data help llms reasoning? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 61386148. Association for Computational Linguistics. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M. Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual generalization In Proceedings of through multitask finetuning. the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pages 1599116111. Association for Computational Linguistics. Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. the Dont give me the details, summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 17971807. Association for Computational Linguistics. just OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Mengjie Ren, Boxi Cao, Hongyu Lin, Cao Liu, Xianpei Han, Ke Zeng, Guanglu Wan, Xunliang Cai, and Le Sun. 2024. Learning or self-aligning? rethinking instruction fine-tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 6090 6105. Association for Computational Linguistics. Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023. Code llama: Open foundation models for code. CoRR, abs/2308.12950. Vinay Samuel, Houda Aynaou, Arijit Ghosh Chowdhury, Karthik Venkat Ramanan, and Aman Chadha. 2024. Can llms augment low-resource reading comprehension datasets? opportunities and challenges. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics, ACL 2024 - Student Research Workshop, Bangkok, Thailand, August 11-16, 2024, pages 411421. Association for Computational Linguistics. Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric questions challenge dense retrievers. In Proceedings of the 2021 Conference on Empirical Methods in Abigail Sticha, Norbert Braunschweiler, Rama Sanand Doddipatla, and Kate M. Knill. 2024. Advancing faithfulness of large language models in goalIn ACM oriented dialogue question answering. Conversational User Interfaces 2024, CUI 2024, Luxembourg, July 8-10, 2024, page 32. ACM. Meta Team. 2024. Introducing llama 3.1: Our most capable models to date. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288. Mengru Wang, Yunzhi Yao, Ziwen Xu, Shuofei Qiao, Shumin Deng, Peng Wang, Xiang Chen, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen, and Ningyu Zhang. 2024. Knowledge mechanisms in large language models: survey and perspective. In Findings of the Association for Computational Linguistics: EMNLP 2024, Miami, Florida, USA, November 12-16, 2024, pages 7097 7135. Association for Computational Linguistics. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models. Trans. Mach. Learn. Res., 2022. Zhihua Wen, Zhiliang Tian, Zexin Jian, Zhen Huang, Pei Ke, Yifu Gao, Minlie Huang, and Dongsheng Li. 2024. Perception of knowledge boundary for large language models through semi-open-ended question answering. CoRR, abs/2405.14383. Canwen Xu, Daya Guo, Nan Duan, and Julian J. McAuley. 2023. Baize: An open-source chat model Tao Gui, and Xuanjing Huang. 2024a. Rotbench: multi-level benchmark for evaluating the robustness of large language models in tool learning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, EMNLP 2024, Miami, FL, USA, November 12-16, 2024, pages 313333. Association for Computational Linguistics. Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, and Zhengyin Du. 2024b. Tl-training: task-feature-based framework for training large language models in tool use. CoRR, abs/2412.15495. Junjie Ye, Yuming Yang, Qi Zhang, Tao Gui, Xuanjing Huang, Peng Wang, Zhongchao Shi, and Jianping Fan. 2024c. 60 data points are sufficient to fine-tune llms for question-answering. CoRR, abs/2409.15825. Tian Ye, Zicheng Xu, Yuanzhi Li, and Zeyuan AllenZhu. 2024d. Physics of language models: Part 2.2, how to learn from mistakes on grade-school math problems. CoRR, abs/2408.16293. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. LIMO: less is more for reasoning. CoRR, abs/2502.03387. Liang Zhang, Katherine Jijo, Spurthi Setty, Eden Chung, Fatima Javid, Natan Vidra, and Tommy Clifford. 2024. Enhancing large language model performance to answer questions and extract information more accurately. CoRR, abs/2402.01722. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: less is more for alignment. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2024. Minigpt-4: Enhancing vision-language understanding with advanced large In The Twelfth International language models. Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. with parameter-efficient tuning on self-chat data. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 6268 6278. Association for Computational Linguistics. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. CoRR, abs/2406.08464. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024a. Qwen2.5 technical report. CoRR, abs/2412.15115. Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Tao Gui, Qi Zhang, and Xuanjing Huang. 2025a. Measuring data diversity for instruction tuning: systematic In Proceedings analysis and reliable metric. of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025, Vienna, Austria, July 27 - August 1, 2025, pages 1853018549. Association for Computational Linguistics. Yuming Yang, Wantong Zhao, Caishuang Huang, Junjie Ye, Xiao Wang, Huiyuan Zheng, Yang Nan, Yuran Wang, Xueying Xu, Kaixin Huang, Yunke Zhang, Tao Gui, Qi Zhang, and Xuanjing Huang. 2025b. Beyond boundaries: Learning universal entity taxonomy across datasets and In languages for open named entity recognition. Proceedings of the 31st International Conference on Computational Linguistics, COLING 2025, Abu Dhabi, UAE, January 19-24, 2025, pages 10902 10923. Association for Computational Linguistics. Zhaorui Yang, Tianyu Pang, Haozhe Feng, Han Wang, Wei Chen, Minfeng Zhu, and Qian Liu. 2024b. Self-distillation bridges distribution gap in language In Proceedings of the 62nd model fine-tuning. Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1028 1043. Association for Computational Linguistics. Junjie Ye, Xuanting Chen, Nuo Xu, Can Zu, Zekai Shao, Shichun Liu, Yuhan Cui, Zeyang Zhou, Chao Gong, Yang Shen, Jie Zhou, Siming Chen, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. comprehensive capability analysis of GPT-3 and GPT3.5 series models. CoRR, abs/2303.10420. Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang,"
        },
        {
            "title": "A Prompt for SFT",
            "content": "To ensure fair comparison, we use uniform prompt templates during training. {% if messages[0]['from'] == 'system' %} {% set system_message = '<<SYS>>n' + messages[0]['value'] trim + 'n<</SYS>>nn' %} {% set messages = messages[1:] %} {% else %} {% set system_message = '' %} {% endif %} {% for message in messages %} {% if (message['from'] == 'user') != (loop.index0 % 2 == 0) %} {{ raise_exception('Conversation roles must alternate user/assistant...') }} {% endif %} {% if loop.index0 == 0 %} {% set content = system_message + message['value'] %} {% else %} {% set content = message['value'] %} {% endif %} {% if message['from'] == 'user' %} {{ bos_token + '<question> ' + content trim + ' <answer>' }} {% elif message['from'] == 'assistant' %} {{ ' ' + content trim + ' ' + eos_token }} {% endif %} {% endfor %}"
        },
        {
            "title": "B More Details of Experiments",
            "content": "B.1 Details of Models To ensure generalizable results, we analyze five LLMs from two different families. LLaMA-2 Family The LLaMA-2 family includes three open-source LLMs developed by Meta. These models are pre-trained on over 2 trillion tokens, equipping them with extensive world knowledge and strong semantic representations. For this study, we select LLaMA-2-7B, LLaMA-2-13B, and LLaMA-270B. LLaMA-3 Family The LLaMA-3 family builds upon the LLaMA-2 architecture with significant advancements, such as improved parameter efficiency and task generalization. We analyze LLaMA-3-8B and LLaMA-3-70B. B.2 Details of Parameter Restoration To assess how excessive parameter updates affect model performance, we compare the fine-tuned model with the pre-trained model by ranking parameters according to their relative change. For each parameter i, let pi denote its value before fine-tuning and si its value afterward. The relative change is defined as: ri = si pi pi We sort all parameters in descending order of ri to obtain the set I. To measure the concentration of parameter updates, we compute the cumulative sum of ri for the top percentage of parameters in I, divided by the total sum of all ri. For instance, Table 3 shows that the top 1% of parameters contribute 70.59% of the total relative change."
        },
        {
            "title": "C More Results",
            "content": "In this section, we present additional experimental results that are not included in the main body of the paper due to the limitation of space. C.1 Test Results for the LLaMA-2 Family Models We fine-tune five LLMs using datasets with five different levels of mastery. The results for the LLaMA-3 family models are presented in Section 3.4, while the results for the LLaMA-2 family are shown in Figure 6. Notably, although the peak performance occurs at different data sizes depending on the base model and hyperparameters, the trend of performance degradation beyond certain point (size) remains consistent. (a) LLaMA-2-7B (In-Domain) (b) LLaMA-2-7B (Out-of-Domain) (c) LLaMA-2-13B (In-Domain) (d) LLaMA-2-13B (Out-of-Domain) (e) LLaMA-2-70B (In-Domain) (f) LLaMA-2-70B (Out-of-Domain) Figure 6: In-domain (AccM tuned with varying data scales, where All indicates the use of the entire dataset listed in Appendix D. test) and out-of-domain (AccM testood) performance of the LLaMA-3 family models fineC.2 Performance on the Training Set We compare the performance of different LLMs fin-tuned from LLaMA-3-8B on their respective training sets when restoring different proportions of parameters. The results in Table 6 show that parameter reduction improves model performance on the training set, further supporting the idea that SFT introduces significant number of unnecessary or even detrimental parameter updates. Restore DM train0 DM train1 DM train2 DM train3 DM train4 0 5% 20% 0 5% 20% Number of Training Data: 240 61.25 62.92 62. 84.58 85.00 83.75 90.00 90.83 92.5 Number of Training Data: 1920 62.81 64.74 65.00 83.44 85.52 89.06 89.48 90.47 90. 12.08 12.50 11.25 16.56 15.68 15.16 92.92 93.75 82.92 93.39 94.22 94.90 Table 6: Performance of LLaMA-3-8B on the training set after restoring different scales of parameters across various fine-tuning datasets. Improvements over the non-restored model are highlighted in green , while performance declines are shown in red , with darker shades indicating larger differences. C.3 Comparison of Results Across Different Strategies We compare the performance of LLaMA-3-8B trained using four different strategies: LLaMA-3-8B-Instruct: chat-optimized version fine-tuned by Meta, demonstrating strong performance across various benchmarks. SFT (Mixed): Fine-tuning LLaMA-3-8B using randomly mixed dataset. Results are tested across different data volumes, with the best outcomes reported. SFT (Divided): Fine-tuning LLaMA-3-8B with data divided based on the models mastery level. The best results are reported when fine-tuning with 1,920 samples. LoRA: Fine-tuning LLaMA-3-8B using randomly mixed dataset with LoRA (Hu et al., 2022). Parameter Restore: Fine-tuning LLaMA-3-8B using the divided dataset, followed by parameter restoration process. The best results are reported when fine-tuning with 1,920 samples. The results in Table 7 indicate that data division and parameter restoration strategies significantly enhance model performance, offering valuable insights for optimizing data selection and fine-tuning approaches. Strategies LLaMA-3-8B-Instruct SFT (Mixed) SFT (Divided) LoRA Parameter Restoration AccM test AccM 62.21 57.10 58.67 53.88 58.80 54.04 53.83 54.14 57.82 51. testood Table 7: Performance of different LLMs fine-tuned using various strategies. The best results are highlighted in bold."
        },
        {
            "title": "D Data Distribution of Different LLMs",
            "content": "Since data division is based on the models mastery of the data, we analyze the data distributions corresponding to different pre-trained LLMs. The results for LLaMA-3-8B are presented in Section 3.1, while the distributions for other models are shown in Table 8. Dtrain DM train0 DM train DM train2 DM train3 DM train Dtrain DM train0 DM train1 DM train2 DM train3 DM train4 Number 12530 Dtest DM test0 26805 DM test1 14961 DM test2 11542 DM test3 10106 DM test4 Number Dtest DM test0 30566 DM test 9336 DM test2 7508 DM test 5809 DM test4 Number Dtestood DM 1595 testood0 DM 2795 3374 testood1 DM 1876 testood2 DM 1704 1491 testood3 DM 1542 testood4 1055 1219 Number Number Dtestood DM 2941 testood0 DM 5201 3805 testood1 DM 4181 1162 testood2 DM 1030 958 testood3 DM 786 415 testood4 Number (a) LLaMA-3-70B (b) LLaMA-2-7B Dtrain DM train0 DM train1 DM train DM train3 DM train4 Dtrain DM train0 DM train1 DM train2 DM train3 DM train4 Number 20899 Dtest DM test0 30562 DM test1 DM test2 7996 DM test3 DM test4 Number 15378 Dtest DM test0 29468 DM test1 13385 DM test2 9344 DM test3 8369 DM test4 Number Dtestood DM 2675 testood0 DM 4671 3791 testood1 DM 4242 1275 testood2 DM 1233 1006 testood3 DM 808 486 testood4 Number Number Dtestood DM 1956 testood0 DM 3669 testood1 DM 4537 1719 testood2 DM 1511 1199 testood3 DM 1338 888 1012 testood Number (c) LLaMA-2-13B (d) LLaMA-2-70B Table 8: Data distribution for different LLMs."
        },
        {
            "title": "E Discussion of Redundant Parameter Updates",
            "content": "E.1 Distribution of Redundant Parameter Updates To investigate why SFT leads to redundant parameter updates, we analyze the distribution of redundant parameters in LLaMA-3-8B. As shown in Table 9 and Table 10, these parameters are spread across all layers of the model, with higher concentration in the initial layers (i.e, 02), fewer in the final layers (i.e., 3031), and more uniform distribution in the middle layers (i.e., 329). This pattern may be due to the initial layers primarily encoding semantic information that is already well-learned during pretraining, resulting in greater parameter redundancy. In contrast, the final layers, which focus on output formatting, exhibit less redundancy. Furthermore, we observe that most redundant parameters are concentrated in the FFN layers, suggesting that their high parameter count presents potential target for optimization. We also acknowledge that the emergence of redundant parameters may be linked to the lottery ticket hypothesis (Frankle and Carbin, 2019). Layer 0 1 3 4 5 6 7 9 10 11 12 13 15 Percentage 3.77 3.26 3.25 3. 3.17 3.20 3.19 3.20 3.23 3. 3.22 3.18 3.10 3.13 3.15 3. Layer 16 17 18 19 21 22 23 24 25 27 28 29 30 31 Percentage 3.16 3.11 3.08 3.07 3.10 3. 3.06 3.03 3.01 2.99 3.00 3. 2.98 2.95 2.99 2.84 Table 9: Distribution of redundant parameter updates across layers in LLaMA-3-8B."
        },
        {
            "title": "Module",
            "content": "mlp.down mlp.up mlp.gate attn.o attn.q attn.v attn.k"
        },
        {
            "title": "Percentage",
            "content": "28.91 28.26 23.37 9.40 6.41 2. 1.09 Table 10: Distribution of redundant parameter updates across modules in LLaMA-3-8B. E.2 Layers for Preserving Model Behavior To identify which layers are most critical for preserving model behavior, we perform experiments that selectively restore parameters in different layers and evaluate the resulting performance. As shown in Table 11, our results reveal that the lower layers (03) are most crucial for maintaining model behavior, while the middle layers (427) have the least impact. This implies that SFT affects not only the predictive distributions in the upper layers but also substantially modifies the lower-layer parameters, thereby influencing overall model performance."
        },
        {
            "title": "Restored Layers",
            "content": "0-3 4-7 8-11 12-15 16-19 2024-27 28-31 Dtrain-0 Dtrain-1 Dtrain-2 Dtrain-3 Dtrain-4 9.83 21.59 53.75 2.80 4.00 57.05 58.22 56.50 59.43 53.72 55.59 57.83 59.54 59.12 54. 55.56 58.02 59.38 59.17 53.94 55.22 58.03 59.26 59.15 53.74 55.50 57.99 59.36 59.18 53.91 55.14 57.96 58.95 59.15 53.14 57.60 57.99 34.68 50.76 47.32 Table 11: Model performance when restoring different layers under different datasets."
        },
        {
            "title": "F Details of Data Processing",
            "content": "In this section, we provide additional details on data processing. F.1 Robust Multi-Template Complementation Mechanism (2024c), consider knowledge fact represented as triple As described in Ye et al. (subject, relation, object), such as (P ainblanc, Given sentence = map(subject, relation) that maps the subject and relation (e.g., Painblanc is located in), an LLM is considered to have memorized if it can predict = map(object) by mapping the object (e.g., France) such that M(x). locatedin, rance). Since is probabilistic model influenced by different mapping templates and sampling probabilities, we design Nmap = 21 different mappings for each knowledge fact k. With the temperature set to 0.7, the model generates Nsample = 10 outputs for each mapping. The degree to which the LLM memorizes is then calculated as: RM = (cid:80)Nmap i=1 (cid:80)Nsample j=1 I(yi Mj(xi)) Nmap Nsample where xi and yi represent the results from the ith mapping, Mj denotes the jth sampled output, and I() is the indicator function. This approach effectively utilizes the characteristics of LLMs to evaluate their mastery of different data. However, as entities often have multiple aliases (e.g., USA and United States), the singular entity labeling in the original dataset may introduce biases. To enhance robustness, synonym mapping table  (Table 12)  is constructed to expand the set of equivalent entity names, significantly improving result accuracy. This table is also used in judging the accuracy of LLMs answers after SFT. Object Synonyms USA, United States, United States of America New York, New York City UMich, University of Michigan South Korea, Republic of Korea, Korea Saint Petersburg, St. Petersburg Baires, Buenos Aires PRC, Peoples Republic of China, China Ohio State University, Ohio State Bosnia, Bosnia and Herzegovina, Bosna Hercegovina University of Texas at Austin, University of Texas, UT Austin Cambridge University, Cambridge, University of Cambridge United States of America New York City University of Michigan South Korea Saint Petersburg Buenos Aires Peoples Republic of China Ohio State University Bosnia and Herzegovina University of Texas at Austin University of Cambridge United States Military Academy United States Military Academy, West Point Rio de Janeiro University of Edinburgh Museo del Prado Salt Lake City North Carolina State University NC State, North Carolina State University University of Durham, Durham University University of Durham Harvard Law School Harvard University, Harvard Law School University of Paris (1896-1968) Université de Paris, University of Paris, Paris University Newcastle upon Tyne University of Oslo Hebrew University of Jerusalem University of Jerusalem, Hebrew University, Hebrew University of Jerusalem Carnegie Mellon University University of Oxford Autodromo Nazionale Monza Indiana State House Imperial College London United Arab Emirates Carnegie Mellon University, Carnegie Mellon Oxford University, University of Oxford Monza, Autodromo Nazionale Monza Indiana State House, Indiana State Imperial College, Imperial College London UAE, United Arab Emirates, The Emirates Rio de, Rio de Janeiro Edinburgh University, University of Edinburgh Prado Museum, Museo Nacional del Prado, Museo del Prado Salt Lake, Salt Lake City Newcastle upon Tyne, Newcastle University of Oslo, Oslo University Table 12: Synonym mapping table for objects in the dataset. F.2 Topics and Mapping Templates of Data We categorize 10 location-related topics as in-domain data and another 14 unrelated topics as out-ofdomain data, designing 21 mapping templates for each topic. The corresponding data details of in-domain data are listed from Table 13 to Table 22, while the corresponding data details of out-of-domain data are listed from Table 23 to Table 36. Topic: P17 Question Template: Which country is {subject} located in? Mapping Templates: {subject} is located in The location of {subject} is in {subject} finds its place within the borders of The {subject} is situated in the country, If youre seeking the {subject}, look no further than the nation of The land encompassing the {subject} is known as {subject} can be found in {subject} has its roots in The place {subject} calls home is {subject} is situated in The geographical location of {subject} is in {subject} can be discovered in the nation of The country where {subject} is found is {subject}s location is in {subject} resides in The country of {subject} is {subject} belongs to {subject} exists in You can find {subject} in {subject} is part of {subject} lies within the borders of Table 13: Information and mapping templates for topic P17 (in-domain). Topic: P19 Question Template: Where was {subject} born? Mapping Templates: {subject} was born in The birthplace of {subject} was It is known that {subject} came into the world in {subject} entered the world in {subject} was born, and that location is {subject}s life began in The location of {subject}s birth is {subject}s birth occurred in The place where {subject} was born is {subject} hailed from The answer to where {subject} was born lies in {subject} originated from {subject} came into this world in {subject} entered life in {subject} first drew breath in The origin of {subject} is in {subject} hails from The place of birth for {subject} is {subject}s birth took place in When it comes to birth, {subject} was born in If you were to ask where {subject} was born, it would be Table 14: Information and mapping templates for topic P19 (in-domain). Topic: P20 Question Template: Where did {subject} die? Mapping Templates: {subject} met their end at {subject} breathed their last at {subject}s life came to close at The place of {subject}s death is The location of {subject}s demise is The site of {subject}s final rest is The place where {subject} passed away is {subject}s mortal remains are in {subject} succumbed to death in The destination of {subject}s last days was The story of {subject}s life concluded in {subject} bid farewell to the world from within the confines of The final resting place of {subject} is {subject} took his final breath in {subject}s life journey ended in {subject} died in The place where {subject} died is {subject}s death occurred in {subject} took their last breath in When it comes to death, {subject} died in Looking at the end of {subject}s life, they died in Table 15: Information and mapping templates for topic P20 (in-domain). Topic: P36 Question Template: What is the capital of {subject}? Mapping Templates: The capital of {subject} is When considering the capital of {subject}, it is In {subject}, the city designated as the capital is {subject}s capital city is The capital city of {subject} is located in {subject} is governed from The seat of government in {subject} is {subject}s governmental hub is The administrative center of {subject} is The political heart of {subject} beats in One can find {subject}s seat of power in the city of One would find {subject}s governing institutions nestled within the boundaries of {subject}s capital is The capital of the region {subject} is {subject}s capital designation goes to The main city of {subject} is {subject} has its capital in The chief city of {subject} is Looking at {subject}, its capital is In terms of capital cities, {subject} has As the capital of {subject}, youll find Table 16: Information and mapping templates for topic P36 (in-domain). Topic: P69 Question Template: Where was {subject} educated? Mapping Templates: {subject} received education at {subject} completed the studies at {subject} was schooled at {subject} was educated in {subject} graduated from {subject} spent the formative years at {subject}s alma mater is {subject} pursued the studies at {subject} gained the knowledge at The academic journey of {subject} took place in The institution where {subject} studied is Education for {subject} was pursued within the walls of The educational institution attended by {subject} is {subject} is an alumnus/alumna of The academic background of {subject} includes The place where {subject} was educated is {subject} attended school in The education of {subject} took place in The place of {subject}s education is {subject} received their education from In terms of education, {subject} was schooled in Table 17: Information and mapping templates for topic P69 (in-domain). Topic: P131 Question Template: Where is {subject} located? Mapping Templates: The location of {subject} is where youll find If you look where {subject} is, youll see Where {subject} resides, there also is {subject} is located at {subject} can be found in {subject} is positioned at {subject} is stationed at {subject} is based at {subject} is headquartered at The current location of {subject} is One would locate {subject} in the vicinity of Currently, {subject} resides or occupies {subject} is in The geographical position of {subject} is {subject} is placed in You can find {subject} in {subject} exists in {subject} lies in The location of {subject} is {subject} is situated in {subject} resides in Table 18: Information and mapping templates for topic P131 (in-domain). Topic: P159 Question Template: Where is the headquarter of {subject}? Mapping Templates: The headquarter of {subject} is located in {subject} has its headquarter in You can find the headquarter of {subject} in {subject}s central office is situated in The main hub of {subject} is {subject} is headquartered in The location of {subject}s headquarter is {subject}s headquarter can be found at The address of {subject}s headquarter is {subject}s headquarters are located at The central hub of operations for {subject} can be found in The administrative heart of {subject} resides at {subject}s head office is located in {subject} has its main base in {subject}s headquarters can be found in The headquarters of {subject} is located in {subject}s headquarters is in The main office of {subject} is in {subject}s headquarter is located in The headquarter of {subject} is situated in When it comes to headquarters, {subject}s is in Table 19: Information and mapping templates for topic P159 (in-domain). Topic: P276 Question Template: Where is {subject} located? Mapping Templates: {subject} can be found at The location of {subject} is {subject} is situated at {subject} has its base in {subject} is headquartered in {subject} operates out of The place where {subject} is located is {subject} is positioned at The site of {subject} is {subject} can be found in the location The whereabouts of {subject} are at {subject} is situated in the place called {subject} is established in The coordinates of {subject} point to The address of {subject} leads to {subject} is located in {subject} resides in You can find {subject} in When it comes to location, {subject} is in Looking at where {subject} is, it is in In terms of location, {subject} is situated in Table 20: Information and mapping templates for topic P276 (in-domain). Topic: P495 Question Template: Which country was {subject} created in? Mapping Templates: {subject} was created in The creation of {subject} took place in The origin of {subject} is traced back to {subject} was born in {subject} originated from {subject} was founded in {subject} was created in the country of The country of origin for {subject} is {subject} originated in the country of The birthplace of {subject} is none other than {subject}s formation occurred in the borders of Historically, {subject} emerged in the country known as {subject} was conceptualized in The country credit for the creation of {subject} goes to The country that witnessed the creation of {subject} is The country where {subject} was created is {subject} was made in {subject} came into being in If you were to ask where {subject} was created, it would be Looking at the origin of {subject}, it was created in In terms of country of origin, {subject} was created in Table 21: Information and mapping templates for topic P495 (in-domain). Topic: P740 Question Template: Where was {subject} founded? Mapping Templates: The founding of {subject} took place in {subject} was originally established in {subject}s origin is traced back to {subject} was founded in {subject} originated in {subject} has its roots in The founding location of {subject} is {subject} has its origins in The birthplace of {subject} is One can trace {subject}s beginnings to {subject} came into existence in The roots of {subject} dig deep into the soil of {subject} traces its beginnings back to The inception of {subject} took place in {subject} was brought into existence in The founding place of {subject} is The origin of {subject} is in The establishment of {subject} occurred in If you were to ask where {subject} was founded, it would be Looking at the origin of {subject}, it was founded in In terms of its founding location, {subject} was established in Table 22: Information and mapping templates for topic P740 (in-domain). Topic: P112 Question Template: Who founded {subject}? Mapping Templates: The founder of {subject} is {subject} was founded by The establishment of {subject} was initiated by {subject} owes its existence to {subject} was brought into being by {subject} is brainchild of {subject} was established by {subject} has its roots in The person who founded {subject} is The visionary behind {subject}s establishment is The inception of {subject} can be traced back to The idea and realization of {subject} were the brainchild of {subject} was brought into existence by {subject}s founder is known to be {subject} owes its inception to {subject} was created by The creation of {subject} is attributed to {subject} was started by {subject} originated with {subject}s origins lie with {subject} can trace its roots back to Table 23: Information and mapping templates for topic P112 (out-of-domain). Topic: P127 Question Template: Who owns {subject}? Mapping Templates: The owner of {subject} is {subject} is owned by Ownership of {subject} belongs to {subject} belongs to {subject} is in the possession of {subject} is property of {subject} is possessed by {subject} is under the ownership of {subject} is held by The proprietor of {subject} is none other than Responsibility for {subject} falls under the jurisdiction of The property known as {subject} is under the stewardship of The rights to {subject} are held by The individual who owns {subject} is The rightful owner of {subject} is identified as Ownership of {subject} is held by The possession of {subject} is with The entity owning {subject} is {subject}s owner is {subject} is in the hands of If youre looking for the owner of {subject}, its Table 24: Information and mapping templates for topic P127 (out-of-domain). Topic: P170 Question Template: Who was {subject} created by? Mapping Templates: {subject} was created by The creator of {subject} was The person who created {subject} is known as {subject} was founded by {subject} owes its creation to {subject} was developed by {subject}s creator is {subject} was the creation of The person behind {subject} is {subject} was brought into existence by The originator of {subject} is The creative force behind {subject} is attributed to {subject} came into existence thanks to {subject} was brought to life by {subject} was conceptualized by The creation of {subject} is attributed to The entity responsible for creating {subject} is {subject} was made by {subject}s creation is attributed to When it comes to creation, {subject} was created by Looking at the creation of {subject}, it was done by Table 25: Information and mapping templates for topic P170 (out-of-domain). Topic: P175 Question Template: Who performed {subject}? Mapping Templates: The performer of {subject} was {subject} was performed by The one responsible for performing {subject} was {subject} was brought to life by {subject} was presented by {subject} was executed by The artist behind {subject} is The talent behind {subject} is The one who performed {subject} was The one who executed {subject} skillfully was The artist responsible for {subject}s interpretation was The responsibility of performing {subject} fell upon {subject} was enacted by The act of {subject} was performed by {subject} was executed on stage by The execution of {subject} was done by {subject} was carried out by The realization of {subject} was by {subject} had its performance by The performance of {subject} was done by Looking at the performance of {subject}, it was done by Table 26: Information and mapping templates for topic P175 (out-of-domain). Topic: P176 Question Template: Which company is {subject} produced by? Mapping Templates: {subject} is produced by The producer of {subject} is The production company behind {subject} is {subject} is created by {subject} is assembled by {subject} comes from {subject} is manufactured by The company responsible for {subject} is {subject} is product of The production of {subject} falls under the umbrella of {subject} comes from the production house of The production of {subject} is handled by none other than The company behind the production of {subject} is The company that crafts {subject} is Every unit of {subject} bears the production mark of {subject} comes from the company The production of {subject} is handled by The company responsible for producing {subject} is The company that produces {subject} is When it comes to production, {subject} is produced by Looking at the production of {subject}, it is done by Table 27: Information and mapping templates for topic P176 (out-of-domain). Topic: P26 Question Template: Who is {subject} married to? Mapping Templates: {subject}s spouse is {subject} has been married to {subject} is in marital union with The person {subject} is married to is {subject}s partner in marriage is {subject}s better half is {subject} is wed to {subject} exchanged vows with {subject} shares life with {subject} shares marital bond with Their love story culminated in wedding, uniting {subject} and The answer to {subject}s nuptials lies in the presence of {subject} is married to {subject} has tied the knot with {subject} shares matrimonial life with The spouse of {subject} is {subject} is wedded to In marriage, {subject} is united with The one {subject} is married to is {subject}s husband/wife is When it comes to marriage, {subject} is married to Table 28: Information and mapping templates for topic P26 (out-of-domain). Topic: P40 Question Template: Who is {subject}s child? Mapping Templates: The child of {subject} is known to be Belonging to {subject} as child is As child to {subject}, there is {subject}s child is {subject} is the parent of {subject}s offspring is {subject}s youngster is {subject}s family includes {subject}s lineage includes {subject} has child named The offspring of {subject} is identified as The child of {subject} is recognized as The offspring of {subject} includes {subject} is the biological parent of {subject} is the father/mother to The child of {subject} is The progeny of {subject} is The next generation of {subject} includes If you were to ask who {subject}s child is, its Looking at {subject}s offspring, its In terms of children, {subject} has Table 29: Information and mapping templates for topic P40 (out-of-domain). Topic: P413 Question Template: What position does {subject} play? Mapping Templates: {subject} plays The position of {subject} is In the team, {subject} holds the position of {subject} plays the position of {subject}s role is {subject} is The position played by {subject} is {subject} holds the position of {subject} is player in the position of In the game, {subject} assumes the role of {subject} is known for the position as When playing, {subject} takes up the position of The role {subject} takes on is {subject} is assigned to the position The position that {subject} occupies is {subject} occupies the position of {subject} fulfills the role of {subject} is positioned as The position that {subject} plays is {subject}s position is If you were to ask what position {subject} plays, its Table 30: Information and mapping templates for topic P413 (out-of-domain). Topic: P50 Question Template: Who is the author of {subject}? Mapping Templates: {subject} was authored by The writer of {subject} is The person who authored {subject} is The author of {subject} is {subject} was written by {subject} is work by The creator of {subject} is The person responsible for {subject} is {subject} owes its existence to The creative mind behind {subject} is none other than {subject} was penned by the talented writer, The work known as {subject} was brought to life by the author, {subject} is work authored by The penname associated with {subject} is The words of {subject} were put together by The person who wrote {subject} is {subject} was created by {subject} was drafted by If you were to ask who authored {subject}, it was Looking at the authorship of {subject}, it was written by {subject} is creation of Table 31: Information and mapping templates for topic P50 (out-of-domain). Topic: P136 Question Template: What type of music does {subject} play? Mapping Templates: The music played by {subject} is When {subject} plays music, it is The musical style of {subject} can be categorized as {subject}s sound is characterized as {subject}s musical talent lies in {subject} has knack for {subject}s genre of music is {subject} is known for playing {subject}s music style is The genre that {subject} excels in is When it comes to music, {subject} is known for their proficiency in The tunes produced by {subject} belong to the category of {subject}s music falls under the category of {subject} has musical style that is categorized as The music played by {subject} can be described as The type of music {subject} plays is The genre of music {subject} plays is The style of music {subject} plays is {subject} plays the music type of Musically, {subject} is known to play In terms of musical style, {subject} plays Table 32: Information and mapping templates for topic P136 (out-of-domain). Topic: P106 Question Template: What kind of work does {subject} do? Mapping Templates: {subject} is employed in {subject} earns living by working as {subject}s occupation is {subject} is engaged in {subject}s profession is {subject} works as {subject} makes living as {subject} has career in {subject} is involved in {subject} engages in the occupation of The work that {subject} undertakes is classified as The focus of {subject}s employment lies in The type of work {subject} engages in is The work performed by {subject} falls under The work done by {subject} falls under the category of The kind of work {subject} does is {subject} operates in the field of The work {subject} performs is When it comes to work, {subject} does {subject} works in the field of The work done by {subject} is Table 33: Information and mapping templates for topic P106 (out-of-domain). Topic: P264 Question Template: What music label is {subject} represented by? Mapping Templates: {subject} is represented by The music label representing {subject} is Regarding representation, {subject} is under {subject} has record deal with {subject} has musical partnership with {subject}s music is released by {subject} is signed to {subject} is affiliated with {subject} has contract with {subject} is represented by the music label The talented {subject} is associated with the music label {subject}s discography is managed by the renowned label {subject} is under contract with the music label {subject} is affiliated with the music label The music label backing {subject} is {subject} is signed with the music label {subject} works with the music label {subject} is under the music label The music label that represents {subject} is {subject} has representation from If you were to ask what music label represents {subject}, it is Table 34: Information and mapping templates for topic P264 (out-of-domain). Topic: P407 Question Template: Which language was {subject} written in? Mapping Templates: {subject} was originally written in The language used for writing {subject} was The original text of {subject} appeared in {subject} was penned in The language of {subject} is {subject} was composed in {subject} was created in {subject} is written in the language of The writing language of {subject} is {subject} was composed in the language known as The linguistic medium of {subject} is The choice of language for {subject} is {subject} was written in the language of The language used to write {subject} is The original language of {subject} is The writing of {subject} is in {subject} is composed in The text of {subject} is in {subject} was written in If you were to ask what language {subject} was written in, its Looking at the language of {subject}, its Table 35: Information and mapping templates for topic P407 (out-of-domain). Topic: P800 Question Template: What is {subject} famous for? Mapping Templates: {subject} is famous for The fame of {subject} is due to People recognize {subject} for {subject} is renowned for {subject}s claim to fame is {subject} is celebrated for {subject} is known for {subject} is distinguished by {subject} is admired for Fame comes to {subject} due to Among its achievements, {subject} is celebrated for {subject}s popularity largely stems from {subject}s notable recognition comes from {subject} is celebrated widely due to The fame of {subject} is attributed to The reason {subject} is famous is {subject} is well-known for {subject} gained fame for If you were to ask what {subject} is famous for, its Looking at what made {subject} famous, its In terms of fame, {subject} is associated with Table 36: Information and mapping templates for topic P800 (out-of-domain)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Lenovo Research, Beijing, China",
        "Shanghai Innovation Institute",
        "Shanghai Key Lab of Intelligent Information Processing"
    ]
}