{
    "paper_title": "Global-Local Tree Search in VLMs for 3D Indoor Scene Generation",
    "authors": [
        "Wei Deng",
        "Mengshi Qi",
        "Huadong Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable success across various fields. However, there are few studies on 3D indoor scene generation with VLMs. This paper considers this task as a planning problem subject to spatial and layout common sense constraints. To solve the problem with a VLM, we propose a new global-local tree search algorithm. Globally, the method places each object sequentially and explores multiple placements during each placement process, where the problem space is represented as a tree. To reduce the depth of the tree, we decompose the scene structure hierarchically, i.e. room level, region level, floor object level, and supported object level. The algorithm independently generates the floor objects in different regions and supported objects placed on different floor objects. Locally, we also decompose the sub-task, the placement of each object, into multiple steps. The algorithm searches the tree of problem space. To leverage the VLM model to produce positions of objects, we discretize the top-down view space as a dense grid and fill each cell with diverse emojis to make to cells distinct. We prompt the VLM with the emoji grid and the VLM produces a reasonable location for the object by describing the position with the name of emojis. The quantitative and qualitative experimental results illustrate our approach generates more plausible 3D scenes than state-of-the-art approaches. Our source code is available at https://github.com/dw-dengwei/TreeSearchGen ."
        },
        {
            "title": "Start",
            "content": "Global-Local Tree Search in VLMs for 3D Indoor Scene Generation"
        },
        {
            "title": "Wei Deng",
            "content": "Mengshi Qi* State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China {dw-dengwei, qms, mhd}@bupt.edu.cn"
        },
        {
            "title": "Huadong Ma",
            "content": "5 2 0 2 5 2 ] . [ 2 6 7 4 8 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Vision-Language Models (VLMs), such as GPT-4, have achieved remarkable success across various fields. However, there are few studies on 3D indoor scene generation with VLMs. This paper considers this task as planning problem subject to spatial and layout common sense constraints. To solve the problem with VLM, we propose new global-local tree search algorithm. Globally, the method places each object sequentially and explores multiple placements during each placement process, where the problem space is represented as tree. To reduce the depth of the tree, we decompose the scene structure hierarchically, i.e. room level, region level, floor object level, and supported object level. The algorithm independently generates the floor objects in different regions and supported objects placed on different floor objects. Locally, we also decompose the sub-task, the placement of each object, into multiple steps. The algorithm searches the tree of problem space. To leverage the VLM model to produce positions of objects, we discretize the top-down view space as dense grid and fill each cell with diverse emojis to make to cells distinct. We prompt the VLM with the emoji grid and the VLM produces reasonable location for the object by describing the position with the name of emojis. The quantitative and qualitative experimental results illustrate our approach generates more plausible 3D scenes than stateof-the-art approaches. Our source code is available at https://github.com/dw-dengwei/TreeSearchGen . 1. Introduction 3D indoor scene generation [7, 8, 11, 12, 14, 21, 30, 34, 36, 37, 39] refers to the process of automatically producing realistic 3D indoor scenes with computer program. In recent years, some studies [8, 11, 14, 30, 37] have explored on using natural language for scene generation, as the textual prompts offer user-friendly interface. Realistic 3D indoor *Corresponding author: qms@bupt.edu.cn Figure 1. Illustration of chain-like (left) and tree-like (right) reasoning in VLMs on 3D scene generation. Each node represents token or language sequence. The red dashed nodes indicate the VLM produces an inappropriate output. The chain-like method cannot correct the prior errors and the subsequent process reasons based on the errors, leading to non-realistic layout, such as exceeding the room. In contrast, the tree-like method can modify the output if mistake occurs, resulting in more realistic layout. scene generation is crucial in fields such as interior design, 3D gaming, virtual/augmented reality, and embodied AI. The primary challenge in generating high-quality scenes is modeling plausible and realistic spatial relationships (i.e., 3D layouts) between objects. To achieve this, early studies [14, 30] adopt data-driven fashion. They train generative models on 3D scene datasets to learn the joint distribution of real indoor scenes and their textual descriptions. However, collecting 3D scene datasets is challenging and costly, resulting in limited dataset scales and less robust models. Recently, researchers have focused on using large Vision-Language Models (VLMs) [8, 11, 37, 40] to achieve this task. VLMs are trained on large-scale datasets, enabling them to understand complex user instructions and in1 fer 3D layouts from language based on common knowledge of indoor furnishing. For example, Feng et al. [8] propose LayoutGPT to generate the dimension, location, and orientation parameters for each object within scene from textual prompt. However, as shown in Figure 1, the current VLMs perform token-level, left-to-right decision-making process during inference time [38], which is not suited for 3D layout reasoning. Specifically, such an auto-regressive reasoning method cannot modify previous outputs. If previous object is placed at an inappropriate location, this error will accumulate. Consequently, it remains challenge to improve VLMs reasoning ability for 3D scene generation. 3D scene generation needs to search with tree-like approach rather than chain-like. When furnishing room, humans usually place objects sequentially. In addition, each object has multiple candidate positions. We pick one of them for the current object and continue to put the next objects. This process is iteratively applied until we place all the objects. If an object is put in an inappropriate location, it stops the next object from being put in the room and we will adjust the previous decision. Consequently, in Figure 1, the problem space can be represented as tree. Each layer in the tree denotes an individual object and each node in the same layer represents the candidate placement for the object. The chain from the root node to the last layer is layout solution. Our objective is to search on the tree to find such chain, subjected to space range, the placement common sense, non-overlapping, and non-floating constraints. Based on the above analysis, we propose novel globallocal tree search method to enhance VLM reasoning for 3D indoor scene generation. Complex scenes with numerous objects make searching deep tree challenging. Our key insight is that the scene structures can be represented hierarchically. We first construct hierarchical scene representation from user input. scene is decomposed as room level, region level, floor object level, and supported object level. This representation serves as proxy between the textual input and the scene output and we can place the objects region by region, which reduces the computational cost. Subsequently, we propose global-local tree search method to generate the layout for the object within the same region. The global tree search method places objects sequentially, mimicking human behavior. Specifically, it starts from the root node (the region) and leverages the local tree search method to generate node in the next layer. Then, the global tree search method turns to the next layer and iteratively applies such process until it generates all objects. The local tree search method generates the position of an individual object. At each layer, it determines one position parameter by textually and visually prompting the VLM. In our approach, each layer has multiple alternatives. Thus, our method performs tree search in the problem space. To summarize, our contributions are three-fold: (1) We propose novel global-local tree search method to enhance VLM reasoning for generating realistic 3D indoor scenes; (2) We design hierarchical scene representation as commonsense bridge between textual input and 3D indoor scenes, further reducing computational costs. (3) Our quantitative and qualitative studies demonstrate that our method generates more realistic 3D indoor scenes than state-of-the-art approaches. User studies suggest that our approach ranks the best among the three approaches. 2. Related Work 3D Scene Generation is not as trivial as directly using 3D object generation model to generate multiple objects. The primary challenge lies in layout generation, which requires modeling spatial and semantic relationships among In the early stage, some works train generaobjects. tive models (e.g. GANs [18, 19], VAEs [7, 12, 39], diffusion models [14, 30, 36, 39], and auto-regressive models [21, 34]) with 3D scene datasets [9, 10, 32]. However, the 3D scene datasets are considerably small compared with 3D object datasets (3D-FRONT [9] 18k+ vs. Objaverse [6] 800k+ and Objaverse-XL [5] 10M+). Thus, models trained on small-scale datasets are less robust and limited in achieving satisfactory performance. Recently, some works [8, 11, 27, 37] utilize the commonsense of VLMs to understand user-provided instructions and generate layouts. For example, they use CLIP [27] to retrieve the most relevant 3D objects by measuring the similarity scores between the textual object descriptions and the image renderings of the objects from 3D object database. LayoutGPT [8] directly outputs the scene layout formatted as Cascading Style Sheets (CSS), including size, location, and orientation for each object. However, the current pre-trained language models cannot comprehensively perceive the space, and thus usually yield unsatisfactory results, such as objects intersecting with each other. To this end, HoloDeck [37] and AnyHome [11] propose to generate scene graphs, which represent the objects as nodes and spatial relationships as edges, and propose rule-based algorithms to convert the scene graph into the room layout. However, the rules are imperfect, leading to low diversity and impractical results. In our work, we strive to utilize the VLM to perceive the space of indoor scenes and achieve reasonable object placements rather than rule-based algorithm. Reasoning with Vision-Language Models is very important in problem-solving, decision-making, and critical thinking [13]. VLMs can be prompted with standard input-output paradigm [1, 2, 4, 15, 20, 31]. However, language models are trained to generate coherent language sequences. Such simple prompting method falls short when the task is complex and requires multi-step reasoning. To enable step-by-step reasoning Wei et al. [35] pro2 pose Chain-of-Thought (CoT), which enforces the language models output intermediate thoughts. CoT significantly improves the performance on reasoning tasks. Nevertheless, some tasks require exploring multiple alternatives at each intermediate step rather than just picking one. To this end, Yao et al. [38] propose Tree-of-Thoughts (ToT), which maintains tree of thoughts and leverages classical tree search algorithms to find solutions for complex task. Inspired by ToT, we treat generating layouts for 3D scenes as tree search problem and propose the global-local tree search method to generate layouts. 3. Problem Formulation Given textual prompt x, we aim to generate 3D scene with objects = {o1, o2, , oN } with VLM, where each object oi = (ci, si, pi, ri), ci is the category of the object, si, pi, and ri denote size, position and orientation, respectively. The standard input-output (IO) reasoning method directly produces 3D scene: pIO(Sx). During the reasoning of VLMs, the chainof-thought (CoT) method yields chain of intermediate thoughts t1, t2, , tn to bridge and S. Each intermediate thought ti pCoT(tix, t1, t2, , ti1) is sampled auto-regressively and produces the final output pCoT(Sx, t1, t2, , tn). However, there is large semantic gap between the input and output, thus it is hard for the current method to directly map from to S. Besides, both IO and CoT methods follow left-to-right manner, which cannot modify previous decisions. In our work, we first generate proxy for linkpxS = ing the user input and the 3D scene: pP S(SP )pxP (P x). It is hierarchical representation, which decomposes scene into multiple regions (see more details in Section 4). We independently generate all the regions and then combine the regions into whole room. Each region contains an object set = {o } and an edge set = {eijoi, oj S}. The object set is different from = {o1, o2, , oN }. In S, the category ci and size si are determined while the placement attributes, i.e. pi and ri, have not been determined in pxP (P x). There is an anchor object in that represents the primary function of this region. Each edge is the spatial relationship between an object and the anchor, denoted as ei,a. 2, , 1, In pP S(SP ), we propose an improved reasoning method as tree search for 3D scene generation (see more details in Section 5). We aim to determine the orientation ri and location pi for all S. We propose global-local tree search method to achieve this. It searches on the problem space, tree, aiming at finding solution for the placements of each object, and will trace back if it fails to go deeper into the tree. The global tree search method is an object-level solver, which manages the genIt puts the objects via the local erating process globally. tree search method sequentially as the human does: oi+1 p(oi+1eia, o1, o2, , oi, i+1). It suggests that the determination of the orientation and location of the (i + 1)th object is conditioned on the objects whose orientation and location are determined, relationship with the anchor, and the category ci+1 and size si+1 of the (i + 1)th object itself. In contrast, the local tree search method is parameter-level solver. It decomposes the generation of the orientation and location into smaller sub-tasks. Each sub-task also has multiple alternatives and the problem space spans as tree. In the following sections of this paper, we first introduce the hierarchical scene representation in Section 4. Then, we propose our global-local tree search method in Section 5.1 and Section 5.2, respectively. 4. Hierarchical Scene Representation Generating 3D indoor scene from natural language is complex task because there is large semantic gap between the text and the scene. To this end, we first leverage the VLM to understand the user requirements and produce hierarchical scene structure representation, serving as proxy between the text and the scene. In this section, we introduce the hierarchical representation and how to generate such representation with the VLM. We represent the scene structure hierarchically including room, region, floor object, and supported object levels, as shown in Figure 2. We start from the user input and prompt the VLM to generate the hierarchical scene representation from left to right. During the generation process, we explicitly prompt the VLM to follow the users input and consider the common sense of indoor furnishing. Room level. In Figure 2 (1), the first level is the root node, denoting the entire room . We prompt the VLM to generate reasonable dimension for the room, parameterized by the length and width dim = (l, w). Region level. In Figure 2 (2), the second level is the region level, we divide the room into multiple functional regions. Each region is characterized by its dimensions dimi = (li, wi). We enforce them to share the width with the room to ease the calculation of region positions and absolute object positions in the region. Floor object level. In Figure 2 (3), the third level is the floor object level, which represents the objects that should be placed on the floor, called floor objects. Each floor object should belong to exactly one region. As described in Section 3, we utilize the VLM to generate and for each region. We first prompt the VLM to generate some objects with its category ci and size si, where the object should be semantically consistent with the region function. We request the VLM to choose an anchor object from the object set. Other objects are spatially related to the anchor object, such as coffee table placed in front of the sofa (anchor), and we also utilize the VLM to determine the spatial rela3 Figure 2. We prompt VLM to generate the hierarchical scene representation level by level. From left to right, we decompose the scene into room, region, floor object, and supported object levels. The final representation is shown on the right-most side in this figure. tionships, i.e. edge between objects and the anchor ei,a. The VLM can choose from the following options: place front, place beside, and place around. To get the 3D object models in the object set, we retrieve them from the Objaverse-1.0 database [6]. We adopt the process outlined in HoloDeck [37]. We use the CLIP model [27] to measure visual similarity, Sentence-BERT for textual similarity [28], and compute dimension discrepancies for object retrieving. In particular, we use the VLM to generate the placei.e. place along wall, ment rule for the anchor object, place in center, and place at corner. The anchor always faces the free space of the region. The anchor position and orientation are important because we need to use ei,a to reason the placement of other objects. Supported object level. In Figure 2 (4), the fourth level is the supported object level, which presents the objects that should be placed on the floor objects, named supported objects. Some floor objects, like desks and nightstands, can support other objects, while others, such as wardrobes and floor lamps, cannot. Similarly, we also build an object set and an edge set and retrieve 3D models for the supported objects akin to the process at the floor object level. 5. Global-Local Tree Search in VLM Based on the analysis in Section 3 and the proxy in Section 4, we propose global-local tree search method to reason 3D scene with VLM via pP S(SP ). To make the VLM reason spatially, which requires perceiving the scene, we feed textual-visual prompts to the VLM. Specifically, we discretize the top-down view of scene layout as grid, as shown in Figure 4. In the grid, the existing objects are represented as rectangles and the anchor object is highlighted in red. We prompt the VLM with the grid and textual instructions, guiding it to reason spatially on the grid For example, we wish to put nightstand, which occupies two columns, to the right side of the anchor. The cells on the right side of the anchor are filled with distinct emojis to make the VLM distinguish the cells. The VLM is asked to answer the name of the emojis of the columns where the nightstand should be placed. 5.1. Global Tree Search Given the object set and the edge set defined in the hierarchical scene representation, we aim to reason the orientation and location of each object. For orientations, we prompt the VLM to select rule from the following options: face to anchor, back to anchor, face the same direction of the anchor, and face the opposite direction of the anchor. Based on the rules and the anchors location and orientation, we can determine the orientation of the non-anchor objects. Based on the problem formulation in Section 3, we treat it as tree search problem. To solve this, we propose our global tree search method to globally maintain this process. The global tree search method can be described as: (1) decompose the thoughts, (2) generate the thoughts, and (3) search the tree of thoughts to find solution with the depthfirst-search (DFS) algorithm, where thoughts stand for the nodes in the problem tree. Thought decomposition. As shown in Figure 3 (2), the tree is started from the region node and each layer represents an individual object. We decompose the task into the placement for each object. We initially place the anchor object in the empty region. The remaining objects are then placed in descending order of their dimensions. Thought generation. In Figure 3 (2), given the relationship between the (i + 1)th object and the anchor ei+1,a and tree state si = {o1, o2, , oi}, we propose at most alternative thoughts for the (i + 1)th object. That is, o(j) i+1 pθ(oi+1si, ei+1,a)(j = 1, , k), where pθ is thought generator. The thought generator pθ is the local tree search method, which will be introduced in Section 5.2. Search algorithm. We aim to explore the most promising solution first, thus we use the DFS algorithm to search for the solution. As shown in Figure 3 (2), the algorithm starts from the region node. The algorithm iteratively generates thought in each layer with the local tree search method. If it successfully generates thought in layer + 1, i.e. enough space to satisfy the spatial relation, the algorithm will walk 4 Figure 3. To generate layout for scene with quantities of objects, we independently generate the layout for each region. The global and local tree search method starts from the root node and goes deep by generating thought. If the thought generator fails to produce thought, it will trace back to the parent node and move to another thought. sually prompt the VLM, as shown in Figure 4. The VLM is prompted to consider the layout common sense to generate reasonable thoughts. For side determination, the VLM should choose one of the side options. For row and column determinations, the VLM needs to choose the emojis where the number of them is equal to the object dimension. Thought Evaluation. The VLM needs to evaluate intermediate thoughts. For example, in the side determination step, we prompt the VLM to evaluate if the chosen side has an appropriate position to put the new object. In particular, for the last step, we check whether the bounding box of the new object will intersect with others. The output of the evaluator means whether the current step succeeds. Search algorithm. The local tree search shares the same search algorithm with the global counterpart. If the local tree search fails to produce all the thoughts in the three layers although it fully searches the tree with maximum attempts, it suggests the global tree search method unable to place the current object. 6. Experiments Type Size Description Bathroom Small cozy bathroom with compact shower and sleek vanity Bathroom Medium modern bathroom with marble countertops and spacious shower Bedroom Medium modern bedroom with comfortable queen-sized bed Kitchen Medium modern kitchen with kitchen island and stainless-steel finishes Living room Small snug living room with rustic coffee table and warm throw blankets Living room Medium mid-century living room with retro furniture Living room Large living room featuring oversized sofas and projector setup Table 1. Some example prompts generated by ChatGPT. In this section, we present our experimental results on Figure 4. We discretize the top-down view as grid and fill the cells with emojis. The brick and white go emojis stand for the wall and region boundary respectively. to the next layer of the tree. Otherwise, the algorithm will propose thoughts via the thought generator at most times o(1k) until succeeds. If all the attempts fail, it suggests i+1 that the previous placements, o1, o2, , oi, tend to be inappropriate. The algorithm then traces back to layer and regards it fails in the layer. Then, it will retry in layer i. The algorithm performs iteratively and ends after successfully proposing thought in the last layer of the tree. 5.2. Local Tree Search The local tree search method plays the thought generator for the global tree search method. It takes as input the relation between the (i + 1)th object and the anchor ei+1,a and the intermediate tree state si = [o1, o2, , oi], and outputs the position pi+1 of the (i+1)th object. We also regard location determination as tree search problem, named local tree search. The algorithm is described as follows: Thought decomposition. First, we determine the object should be placed on which side of the anchor in the topdown view (i.e. left, right, top, and bottom). Second, if the side determination step chooses top or bottom, we determine the rows in the grid as shown in Figure 4 where the new object should be placed. On the contrary, if the side is left or right, we determine columns in this step. Third, we determine the axis which is different from Step 2. At each step, there are also multiple alternatives. Thus the problem space can be presented as tree. Thought generation. At each step, we textually and vi5 3D scene generation and provide comparisons with stateof-the-art approaches. 6.1. Setup Input Prompts. We leverage ChatGPT to produce textual prompts for four types of scenes: bathroom, bedroom, kitchen, and living room. Each prompt consists of room size (one of small, medium, or large) and description of how to furnish the room, such as room with queen-sized bed for sleeping and an office table for working. We generate 120 prompts, with 30 prompts for each scene type. All the generated samples in our experiments originate from these prompts. Table 1 illustrates some example inputs in our experiment generated by ChatGPT. Parameter Settings. We apply OpenAI GPT-4o API to conduct our experiments. The breadth (the maximum attempts k) of the tree is important for the global-local tree If is too small, it may not successsearch algorithm. fully find an optimal solution. If is too large, the search space will be large and the API cost will be extremely high. Therefore, we make trade-off between effect and cost. In the global tree search module, we set = 3 for the anchor objects. For other objects, we set = 1. In the local tree search module, we set = 2 for the side determination step and set = 1 for others. Metrics. To quantitatively measure the methods, we follow HoloDeck [37] to compute the CLIP score [27]. Specifically, we use the OpenCLIP library with the ViT-L/14 model pre-trained on the LAION-2B dataset [29]. Subsequently, we calculate the cosine similarity between the topdown rendering of scenes and prompt template topdown view of [scene type]. We multiply the similarities by 100 as the CLIP score. To qualitatively measure the methods, we calculate the reciprocal rank. We first render the scenes from the topdown view for each scene with Blender. Subsequently, we pair the results from different methods feeding the same input and shuffle them. We invite 15 annotators to rank the generated scenes. They are asked Which scene is more realistic and makes common sense, jointly considering the position and orientation? If the reciprocal rank is close to 1 of method, it indicates the generation result by the method ranked first among the annotators. 6.2. Compared Approaches We compare our method with two state-of-the-art approaches: HoloDeck and AnyHome: AnyHome [11] leverages an LLM to generate the spatial relationships between objects and then uses rule-based algorithm to generate coarse layout. 1 Subsequently, AnyHome utilizes score sampling distillation process [3, 22] 1Since the official code of AnyHome does not include the refinement stage, we only got coarse results for AnyHome. to refine the coarse result. HoloDeck [37] also produces spatial relationships between objects with an LLM. It iteratively adds objects to the scene. When putting each object, it first gets all valid placements without object collisions and without exceeding the rooms boundaries. Then, HoloDeck chooses the optimal place that satisfies most spatial relationships. The spatial relationships are checked by pre-defined rules. 6.3. Quantitative Results As shown in Figure 5, our proposed method outperforms HoloDeck and AnyHome in most of the scene types. HoloDeck and AnyHome rely on pre-defined rules for obIn contrast, our ject placement, limiting their flexibility. method puts an individual object with the local tree search module. This module decomposes the task into several steps and utilizes VLM to reason in the top-down view space. The pre-trained VLM incorporates rich common sense and we only use it to solve such small sub-tasks that the VLM can produce reliable responses. Nevertheless, our method has less 0.24 CLIP score than HoloDeck in the kitchen. This may be because the layout of kitchens is commonly place objects along walls and they do not have obvious spatial relationships between the objects. Besides, by horizontally comparing the results of our method in different scene types, our method achieves 29.93 and 30.18 in the bedroom and living room, respectively, outperforming others. This may also be because there are obvious spatial relationships in the bedroom and living room. For instance, TV stand in front of sofa, nightstand beside bed, table in front of sofa, etc. 6.4. Qualitative Results Method Bathroom Bedroom Kitchen Living room Average AnyHome HoloDeck Ours 0.421 0.16 0.660 0.27 0.751 0. 0.422 0.528 0.19 0.26 0.577 0. 0.24 0.27 0.834 0.719 0.25 0. 0.401 0.15 0.563 0.23 0.868 0. 0.443 0.20 0.596 0.25 0.793 0. Table 2. Mean () and STD reciprocal rank of different methods. Comparison. We show some samples generated by different methods of four scene types in Figure 6. From the visualizations, our method generates more realistic scenes. For example, objects in the living room generated by our method are well placed. In Figure 6 (A), chair is placed in front of desk for users to sit on the chair and work on the table. In Figure 6 (B), the sofa, coffee table, and TV stand are center-aligned to make users sit on the sofa and watch TV or reach objects on the coffee table. In contrast, 6 Figure 5. Performance comparison in terms of CLIP score by our proposed model with state-of-the-art methods. Figure 6. The generation results for the bathroom, living room, kitchen, and bedroom. For visualization, we manually set floor textures for the rooms. The results are rendered with the Blenders Cycles engine. HoloDeck [37] tends to generate objects along walls, leading to large free space in the middle of the rooms in Figure 6 (C). Besides, HoloDeck produces semantically inconsistent result in the bathroom, as shown in Figure 6 (D). The armchair should not be present in living room or bedroom rather than bathroom. AnyHomes [11] results are the worst and produce semantically inconsistent results. In Figure 6 (E), the dining table and dining chairs are not commonly placed in kitchen. These findings validate the effectiveness of our hierarchical scene representation. We do not allow rest region present in bathroom or dining region present in kitchen. Consequently, our hierarchical scene representation can reduce the possibility of providing semantic inconsistency objects. Intermediate results. We show visualization results of intermediate steps in Figure 7. We can clearly see our method first divides the room into two regions and generate graphs for each region. In Figure 7 (4), object 2 fails to be put into the rest region because of not enough space. In Figure 7 (5), we trace back to object 1 and rotate it, enabling to have enough space to put object 2. In Figure 7 (6-7), we generate the floor objects for each region and supported objects. Scene control. We show visualized example of the complex prompt input and the corresponding scene in Figure 8. The generated scene reflects details in the prompt because of the powerful VLM to understand users instruction. User study. We also conduct user studies to evaluate the generation quality of our method, HoloDeck, and AnyHome. We report the mean reciprocal rank of each method in Table 2. It shows our method achieved the most promising results compared with state-of-the-art approaches. The mean reciprocal rank of our method is 0.793 on average, 7 Figure 7. Visualization results of intermediate steps of generating scene. Figure 8. Visualization example of the scene control for complex prompt. which stands for the samples generated by our approach ranking 1/0.793 = 1.26 among the annotators. In addition, our method gets +0.360 and +0.197 reciprocal rank improvements on average compared with AnyHome and HoloDeck respectively. Notably, we perform an impressive reciprocal rank score in bedroom (0.846) and living room (0.868), which denotes our method has the advantage in such scenes containing obvious spatial relationships. 6.5. Ablation Study Method Bathroom Bedroom Kitchen Living room Average IO CoT Ours 0.437 0.23 0.681 0.26 0. 0.27 0.350 0.441 0.09 0.22 0. 0.676 0.25 0.27 0.780 0.714 0. 0.27 0.356 0.11 0.685 0.25 0. 0.25 0.396 0.18 0.686 0.26 0. 0.26 Table 3. Mean () and STD reciprocal rank of different methods. To validate the effectiveness of our global-local tree search approach in reasoning 3D layout, we conduct studies on the reasoning paradigms on this task. (1) First, we use standard Input-Output (IO) method akin to LayoutGPT [8]. The IO method uses the rooms type, size, and description as input to directly produce the name, location, dimension, and orientation of objects in scene with an LLM. (2) Second, we use Chain-of-Thought (CoT) method [35]. The CoT method places objects step by step. If an object fails to be placed in the scene, the algorithm will not adjust previous thoughts. The IO and CoT approaches are the special cases of our method. The IO method does not contain multiple intermediate thoughts and the CoT method reasons step by step but does not explore multiple candidates on intermediate thoughts. We change the maximum attempts = 1 8 for both global and local tree search modules to make our approach degrade to CoT. We illustrate the results of user studies in Table 3. It shows our reasoning method achieves 0.75 reciprocal rank on average and outperforms IO and CoT settings with +0.381 and +0.064 respectively. In addition, we also report the CLIP scores on different ablation settings in Figure 5. From the figure, our full method outperforms the ablation variants overall. The standard IO setting performs the worst. This may be because the training set of the LLM does not contain much 3D layout samples. Our method first leverages the LLM to produce the hierarchical scene representation. It serves as proxy between the input and layout. Based on this representation, we decompose the layout generation task as small as possible and leverage the VLM to reason in the space. Consequently, our method gets significant improvement compared with the standard IO setting. The CoT setting inherits our hierarchical scene representation and task decomposition and also achieves good results. However, we find marginal CLIP score improvement in our method compared with the CoT setting (only +0.21 improvement on average) and it performs slightly worse than CoT for the bathroom (0.46) and bedroom (0.12) scenes. The reason may be that we set small ks for our full method for effect and cost trade-off. It leads to our global-local tree search method not fully exploring the problem space. Besides, the algorithm aims to find global optimal. It will prune the sub-tree if it fails to put an object and discards local optimal solutions. In contrast, the CoT setting will ignore the failure objects, which can lead the algorithm to local optimal result. 7. Conclusion In this paper, we proposed global-local tree search method to boost the reasoning process of VLMs to generate layouts for 3D scenes. To bridge the semantic gap between natural language instructions and 3D scenes, and reduce the search cost, we represented an indoor scene structure hierarchically and incorporated it into the tree of thoughts in VLMs. The extensive experimental results demonstrated that our approach can generate realistic 3D indoor scenes compared with state-of-the-art methods. In the future, we will extend our proposed method into outdoor scenes and AR/VR applications. 8. Acknowledgment This work is partly supported by the Funds for the NSFC Project under Grant 62202063 and U24B20176, Beijing Natural Science Foundation (L243027)."
        },
        {
            "title": "References",
            "content": "[1] Jinze Bai, Shuai Bai, Shusheng Yang et al. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv:2308.12966, 2023. 2 [2] Tom Brown, Benjamin Mann, Nick Ryder et al. Language models are few-shot learners. In NeurIPS, 2020. 2 [3] Rui Chen, Yongwei Chen, Ningxin Jiao et al. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In ICCV, 2023. 6 [4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin et al. Palm: Scaling language modeling with pathways. JMLR, 2023. 2 [5] Matt Deitke, Ruoshi Liu, Matthew Wallingford et al. Objaverse-xl: universe of 10m+ 3d objects. In NeurIPS, 2023. [6] Matt Deitke, Dustin Schwenk, Jordi Salvador et al. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 2, 4 [7] Helisa Dhamo, Fabian Manhardt, Nassir Navab et al. Graphto-3d: End-to-end generation and manipulation of 3d scenes using scene graphs. In ICCV, 2021. 1, 2 [8] Weixi Feng, Wanrong Zhu, Tsu-Jui Fu et al. Layoutgpt: Compositional visual planning and generation with large language models. In NeurIPS, 2023. 1, 2, 8 [9] Huan Fu, Bowen Cai, Lin Gao et al. 3d-front: 3d furnished rooms with layouts and semantics. In ICCV, 2021. 2 [10] Huan Fu, Rongfei Jia, Lin Gao et al. 3d-future: 3d furniture shape with texture. IJCV, 129, 2021. [11] Rao Fu, Zehao Wen, Zichen Liu et al. Anyhome: Openvocabulary generation of structured and textured 3d homes. In ECCV, 2025. 1, 2, 6, 7 [12] Lin Gao, Jia-Mu Sun, Kaichun Mo et al. Scenehgn: Hierarchical graph networks for 3d indoor scene generation with fine-grained geometry. IEEE TPAMI, 45(7), 2023. 1, 2 [13] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. In ACL, 2023. 2 [14] Chenguo Lin and Yadong Mu. Instructscene: Instructiondriven 3d indoor scene synthesis with semantic graph prior. In ICLR, 2024. 1, 2 [15] Haotian Liu, Chunyuan Li, Qingyang Wu et al. Visual instruction tuning. In NeurIPS. Curran Associates, Inc., 2023. 2 [16] Changsheng Lv, Shuai Zhang, Yapeng Tian et al. Disentangled counterfactual learning for physical audiovisual commonsense reasoning. In NeurIPS, 2023. [17] Changsheng Lv, Mengshi Qi, Xia Li et al. Sgformer: Semantic graph transformer for point cloud-based 3d scene graph generation. In AAAI, 2024. [18] Thu Nguyen-Phuoc, Christian Richardt, Long Mai et al. Blockgan: Learning 3d object-aware scene representations from unlabelled images. In NeurIPS, 2020. 2 [19] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural feature fields. In CVPR, 2021. 2 [20] OpenAI. Gpt-4 technical report, 2024. 2 [21] Despoina Paschalidou, Amlan Kar, Maria Shugrina et al. Atiss: Autoregressive transformers for indoor scene synthesis. In NeurIPS, 2021. 1, 2 [22] Ben Poole, Ajay Jain, Jonathan T. Barron et al. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. [23] Mengshi Qi, Weijian Li, Zhengyuan Yang et al. Attentive relational networks for mapping images to scene graphs. In CVPR, 2019. [24] Mengshi Qi, Yunhong Wang, Annan Li et al. Stc-gan: Spatio-temporally coupled generative adversarial networks IEEE TIP, 29:54205430, for predictive scene parsing. 2020. [25] Mengshi Qi, Yunhong Wang, Jie Qin et al. stagnet: An attentive semantic rnn for group activity and individual action recognition. IEEE TCSVT, 30(2):549565, 2020. [26] Mengshi Qi, Jie Qin, Yi Yang et al. Semantics-aware spatialtemporal binaries for cross-modal video retrieval. IEEE TIP, 30:29893004, 2021. [27] Alec Radford, Jong Wook Kim, Chris Hallacy et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 2, 4, [28] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In EMNLP, 2019. 4 [29] Christoph Schuhmann, Romain Beaumont, Richard Vencu et al. Laion-5b: An open large-scale dataset for training next generation image-text models. In NeurIPS, 2022. 6 [30] Jiapeng Tang, Yinyu Nie, Lev Markhasin et al. Diffuscene: Denoising diffusion models for generative indoor scene synthesis. In CVPR, 2024. 1, 2 [31] Hugo Touvron, Thibaut Lavril, Gautier Izacard et al. Llama: Open and efficient foundation language models, 2023. 2 [32] Johanna Wald, Helisa Dhamo, Nassir Navab et al. Learning 3d semantic scene graphs from 3d indoor reconstructions. In CVPR, 2020. 2 [33] Haowen Wang, Zhengping Che, Yufan Yang et al. Rdfcgan: Rgb-depth fusion cyclegan for indoor depth completion. IEEE TPAMI, 46(11):70887101, 2024. [34] Xinpeng Wang, Chandan Yeshwanth and Matthias Nießner. Sceneformer: Indoor scene generation with transformers. In 3DV, 2021. 1, [35] Jason Wei, Xuezhi Wang, Dale Schuurmans et al. Chain-ofthought prompting elicits reasoning in large language models. In NeurIPS, 2022. 2, 8 [36] Yandan Yang, Baoxiong Jia, Peiyuan Zhi et al. Physcene: Physically interactable 3d scene synthesis for embodied ai. In CVPR, 2024. 1, 2 [37] Yue Yang, Fan-Yun Sun, Luca Weihs et al. Holodeck: Language guided generation of 3d embodied ai environments. In CVPR, 2024. 1, 2, 4, 6, 7 [38] Shunyu Yao, Dian Yu, Jeffrey Zhao et al. Tree of thoughts: Deliberate problem solving with large language models. In NeurIPS, 2023. 2, 3 9 [39] Guangyao Zhai, Evin Pı nar Ornek, Shun-Cheng Wu et al. Commonscenes: Generating commonsense 3d indoor scenes with scene graph diffusion. In NeurIPS, 2023. 1, [40] Xiaoyu Zhou, Xingjian Ran, Yajiao Xiong et al. GALA3d: Towards text-to-3d complex scene generation via layoutguided generative gaussian splatting. In ICML, 2024."
        }
    ],
    "affiliations": [
        "State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, China"
    ]
}