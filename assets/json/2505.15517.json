{
    "paper_title": "Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets",
    "authors": [
        "Kaiyuan Chen",
        "Shuangyu Xie",
        "Zehan Ma",
        "Ken Goldberg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm - using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, a Visual Question Answering (VQA) dataset generation framework for VLMs. Given a human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into a sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries - images with textural multiple-choice questions - based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, a large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 7 1 5 5 1 . 5 0 5 2 : r Robo2VLM: Visual Question Answering from Large-Scale In-the-Wild Robot Manipulation Datasets Kaiyuan Chen1, Shuangyu Xie1, Zehan Ma1 Ken Goldberg1 1University of California, Berkeley Equal contribution {kych, syxie, zehanma, goldberg}@berkeley.edu https://huggingface.co/datasets/keplerccc/Robo2VLM-"
        },
        {
            "title": "Abstract",
            "content": "Vision-Language Models (VLMs) acquire real-world knowledge and general reasoning ability through Internet-scale image-text corpora. They can augment robotic systems with scene understanding and task planning, and assist visuomotor policies that are trained on robot trajectory data. We explore the reverse paradigm using rich, real, multi-modal robot trajectory data to enhance and evaluate VLMs. In this paper, we present Robo2VLM, Visual Question Answering (VQA) dataset generation framework for VLMs. Given human tele-operated robot trajectory, Robo2VLM derives ground-truth from non-visual and non-descriptive sensory modalities, such as end-effector pose, gripper aperture, and force sensing. Based on these modalities, it segments the robot trajectory into sequence of manipulation phases. At each phase, Robo2VLM uses scene and interaction understanding to identify 3D properties of the robot, task goal, and the target object. The properties are used to generate representative VQA queries images with textural multiplechoice questions based on spatial, goal-conditioned, and interaction reasoning question templates. We curate Robo2VLM-1, large-scale in-the-wild dataset with 684,710 questions covering 463 distinct scenes and 3,396 robotic manipulation tasks from 176k real robot trajectories. Results suggest that Robo2VLM-1 can benchmark and improve VLM capabilities in spatial and interaction reasoning."
        },
        {
            "title": "Introduction",
            "content": "Emerging Vision-Language Models (VLMs) [1, 2, 3, 4, 5, 6, 7] can perform high-level reasoning and scene interpretation [8, 9]. Recent robotic manipulation systems that integrate VLMs demonstrate enhanced capabilities in semantic and long horizon task reasoning [10, 11, 12]. Yet, the key challenge persists: the image-text corpora used for VLM pre-training high-quality lack fine-grained spatial information, which are prerequisites for robots to identify long-tail objects, complex scenes, reason about spatial relationships, and plan physical interactions. To address this challenge, some research [13, 14, 15] relies on data generation through simulation [16, 17, 18]. However, such data has inherent limitations due to the sim-to-real gap, because simulator cannot accurately model visual properties such as noise, clutter, and lighting variations and physical properties such as contact dynamics, and interactions. Therefore, strong performance in simulation often fails to translate reliably to the physical world. Meanwhile, deriving spatial knowledge from real-world (in-the-wild) data typically requires extensive and costly human labeling [19, 20]. In contrast, teleoperated robot trajectories that are used to train visuomotor policies [21], such as VisionLanguage-Action(VLA) [10, 22] or diffusion policies [23], typically include precise, structured proprioceptive and kinematic informationjoint angles, end-effector poses, gripper states, and Preprint. Figure 1: Robo2VLM-1 dataset overview. The middle colorbar traces typical manipulation episodefrom pre-grasp through immobilization, contact, detach, and into post-grasp. Surrounding panels give example questions for each VQA category. Dashed arrows connect every category to the phase(s) in which its questions are sampled. Icons beneath each panel list the key sensing modalities (RGB, stereo depth, wrist/side cameras, gripper state, end-effector pose, language instructions) needed to derive ground-truth answers. forcetorque readingsthat implicitly encode 3D spatial information. We hypothesize that visual and textual data extracted from robot trajectories can improve VLMs spatial reasoning capabilities. We present Robo2VLM, multiple-choice Visual Question Answering (VQA) dataset generation framework for VLMs from real-world robot data. Given human-teleoperated robot trajectory, Robo2VLM segments the trajectory into distinct manipulation phases, selects representative frames from each phase, and generates questions whose answers are supported by synchronized proprioceptive and kinematic ground truth. We apply Robo2VLM to 176k diverse, real-world trajectories from the Open X-Embodiment (OXE) dataset [24], producing over 3 million VQA samples. Inspired by data optimization paradigms such as domain reweighting in natural language processing [25] and robot policy learning [26], we curate Robo2VLM-1, large-scale, in-the-wild VQA dataset with 684,710 questions covering 463 distinct scenes, 3,396 robotic manipulation tasks, and 149 manipulation skills. We evaluate 14 model configurations with state-of-the-art open source models (LLaVA, Llama and Qwen) and with different parameter sizes and prompting techniques. The results indicate that some VLMs can achieve near human performance in questions related to object reachability and interaction understanding. Evaluation also suggests significant gap to human performance, especially in complex reasoning of fine-grained spatial relationship and interactions. Finetuning LLaVA [4] with Robo2VLM-1 improves most of the spatial and interaction capabilities with increasing training dataset size, with maximum 50% accuracy gain in state reasoning and task understanding. This paper makes the following contributions: (1) Robo2VLM, VQA data generation framework from real robot trajectories. (2) Robo2VLM-1, an open VQA dataset with 684,710 questions covering diverse and realistic evaluation scenarios for manipulation. (3) Extensive evaluation data on state-ofthe-art and fine-tuned VLMs."
        },
        {
            "title": "2 Related Work",
            "content": "Large-Scale Robotics Datasets Recent as Open-X-Embodiment [24] and DROID [27], provide extensive teleoperated demonstrations of complex manipulation skills. These datasets are foundational for training modern genlarge-scale datasets, robotics such eralist robot policiesincluding Octo [22], RT-1 [28], RT-2 [29], OpenVLA [10], Gemini Robotics [11], Ï€0 [30], and Hi Robot [12]enabling them to learn diverse skills and understand nuanced physical interactions from broad data. Crucially for grounding VLMs, robotics datasets from Open-X-Embodiment contains rich sensory-modal including RGB video, proprioceptive [31, 28, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], depth data [31, 28, 32, 33, 35], and force-torque [37, 39, 40, 41], that reflect the dynamics of interaction. These information presents an opportunity to bridge robotics data with VLMs. VQA Benchmarks for Robotics and Embodied AI VQA offers powerful paradigm for evaluating the visual reasoning capabilities of VLMs [45, 46, 47]. Recently, VQA benchmarks have been developed for robotic tasks such as visual navigation in long-horizon planning [48, 49]. Simulationbased approaches [13, 14, 15] (often utilizing environments like [16, 17, 18]) generate large-scale VQA dataset, but face the persistent sim-to-real domain gap, where the result may not hold in reality due to factors like noise, clutter, and lighting variations. Real-world data benchmark, such as RoboVQA [19] (human-verified Q/A), improve generalization to real world setting but often involve significant manual annotation effort. These methods typically do not fully automate VQA generation by exploiting the rich spectrum of non-visual modalities (e.g., force, torque, proprioception), limiting their ability to support questions grounded in concepts such as grasp stability or multi-view spatial alignment. In contrast, Robo2VLM reduces the need for manual annotation and enables interaction and physical properties reasoning that are underexplored in previous VQA benchmarks, such as gripper states, grasping stability, task goal, and spatial information focus on the robot and target objects."
        },
        {
            "title": "3 Robo2VLM",
            "content": "Robo2VLM generates five-way multiple-choice question answering (MCQ) from real robot teleoperated trajectories. Robo2VLM offers the following key features: (1) High-quality and representative keyframe selection from long-horizon, in-the-wild, multi-modal robot trajectories, ensuring semantic diversity and relevance; (2) Manipulation-centric question generation encompassing spatial, goalconditioned, and interaction reasoning, each aligned with specific manipulation phases and grounded in corresponding sensor modalities. We begin by defining robot trajectory as time-synchronized sequence of data frames from multiple sensor modalities including exteroceptive and proprioceptive [50]. Let denote the length of trajectory, and let {1, 2, . . . , } index the discrete time steps. Definition 3.1 (Robot Observation Data Frame) At each time step t, the robot data frame is represented as tuple: (cid:16) Dt ="
        },
        {
            "title": "I RGB\nt",
            "content": ", Stereo , pEE , sGripper , ft (cid:17) = {I RGB RHW 3} is set of multi-view RGB images captured from monocular where RGB R2HW 3} denotes set of multi-view stereo image pair (left and cameras, Stereo = {I Stereo denotes the scalar SE(3) is the 6-DoF end-effector pose and sGripper right) if available, pEE gripper state such as gripper aperture, ft R6 is the force-torque vector from the end-effector sensor. The camera images are referred as exteroceptive sensing and the end-effector-related states belong to proprioceptive sensing. Definition 3.2 (Robot Trajectory) trajectory is defined as the temporally ordered sequence of observations D1:T with trajectory task language description l: = {D1:T , l} Given robot trajectory, Robo2VLM  (Fig. 2)  begin with scene-interaction understanding, applying semantic segmentation and manipulation phase classification to identify key segments (e.g., pregrasp/approaching, contact, grasp, release). From these, we extract keyframes based on phase transitions, scene coverage, and visibility of objects or the robot across multiple camera views. We use manipulation domain knowledge to design question prototype to target core manipulation skills such as spatial relationship, goal conditions, and interaction understanding. Robo2VLM instantiates 3 Figure 2: Robo2VLM framework. Robo2VLM generates multi-modal real-world robot trajectories through (1) manipulation phase classification, (2) keyframe selection guided by scene and interaction cues, and (3) structured VQA question prototype. these prototypes on selected keyframes and transforms them into natural language multiple-choice questions via visual-language grounding module that performs question conversion and spatial query projection."
        },
        {
            "title": "3.1 Scene-Interaction Understanding",
            "content": "Embodied Scene Understanding Given task description in nature language and all images from different camera views, we first parse the language instruction using an off-the-shelf LLM such as Qwen 2.5 [2] to obtain {target object}, scene, task, and skill description. For the spatial understanding in manipulation, we need to know the relative direction and displacement between target object and gripper. From the proprioceptive data, we obtain the target object interaction point ground-truth from the robot trajectory data frames. Manipulation Phase Segmentation To segment robotic manipulation trajectories into semantically meaningful phases, we define temporal phase classification function based on the sequence of endeffector poses, gripper aperture signals, and force-torque measurements: pEE , f1:T . To align different types of gripper aperture, sGripper is normalized to [0, 1], where 0 indicates fully open and 1 indicates fully closed. Let st [0, 1] denote the normalized aperture at time t, and st = st st1 its temporal derivative. st 0 denotes small change within tolerance margin Ïµ, typically set to filter out noise. Let ft be the force magnitude (if available). We introduce three threshold parameters: Ï„g (grasp threshold), Ï„c (closure threshold), and Ï„f (force threshold for contact detection). Manipulation processes can be represented as sequence of discrete phases, including approaching, stabilizing, contacting, releasing, and resetting or transitioning to subsequent actions. We denote the phase varible as Î¦ = {Î¦app, Î¦stab, Î¦cont, Î¦rel, Î¦reset, Î¦trans}. Each timestep is assigned label Ï•t Î¦ according to the following temporal logic rules: 1:T , sGripper 1:T Ï•t = Î¦app Î¦stab Î¦cont Î¦rel Î¦reset Î¦trans if st < Ï„g st < Ïµ if Ï•t1 = Î¦app st < Ï„g st Ïµ if Ï•t1 = Î¦stab st Ï„c st Ïµ (ft > Ï„f force unavailable) if Ï•t1 = Î¦cont st Ï„c st > Ïµ if Ï•t1 = Î¦rel st < Ï„g st > Ïµ otherwise The inclusion of force magnitude ensures that passive closure without external contact is not misclassified as active interaction. This multimodal phase labeling strategy captures both kinematic intent and physical contact, enabling robust segmentation of diverse manipulation behaviors. To enforce temporally coherent yet flexible phase progression, we define partial order over the manipulation phases: Î¦app Î¦stab Î¦cont Î¦rel Î¦reset Î¦app This structure enforces unidirectional transitions along the phase chain, while allowing both phase skipping (e.g., directly from Î¦app to Î¦cont) and looping from the terminal phase Î¦reset back to the 4 Table 1: Categorization of visual reasoning questions for robotic manipulation, with manipulation phase (colorcoded) and data modality context. Approach, Stabilize, Contact, Release, Rest. Capabilities Question Prototype Manip. Phase Sensor Modality Object State Spatial Relationship Scene Understanding Multiple View Task State-success Task State-Goal Action Understanding Spatial Reasoning Is the {target object} reachable by the robot? Whats the relative direction in 3-D between end effector and {target object}? Which point is closer to the camera viewing the scene? Which point in the right-side image corresponds to the point in the left-side image? Goal-conditioned Reasoning Has the robot successfully completed the task? What is the goal configuration for {interaction}? The robot is {interaction}. What is the robots current action phase? What will the robot do next? Interaction Phase Trajectory Understanding What task does this trajectory likely accomplish? Task State-grasp Robot State Is this stable grasp? Is the robot gripper currently open? Interaction Reasoning RGB RGB , Dt , pEE , Stereo RGB Stereo RGB RGB RGB , pEE , T1:t RGB RGB , pEE , pEE RGB RGB , ft , sGripper initial phase Î¦app, which is common in sequential manipulation routines. At each time step t, the phase label must satisfy Ï•t Ï•t1, or Ï•t = Î¦app if Ï•t1 = Î¦reset, ensuring temporal monotonicity or task repetition without reversal. The auxiliary state Î¦trans is used for ambiguous, missing, or conflicting observations where no confident assignment is possible. This symbolic, temporallyconstrained model supports robust segmentation of complex manipulation behaviors under noisy or partially missing sensory input."
        },
        {
            "title": "3.2 Visual Question Prototype",
            "content": "We design set of visual question prototypes, each of which aligns with specific manipulation task completion required robot capabilities and anchors to distinct manipulation phases as illustrated in Table 1. These prototypes are organized into three reasoning categories. Spatial Reasoning focuses on the robots understanding of object geometry, reachability, and spatial layout across viewpoints. Questions such as Is the object reachable? or Whats the relative direction between the gripper and the object? are grounded in the early approach stages. These rely on RGB, depth, stereo, and 3D gripper pose data, which together enable accurate localization and spatial inference across frames or views. and stabilize Goal-conditioned Reasoning probes the agents high-level understanding of tasks, including goal inference, future action prediction, and overall task success. Questions such as Is the task failed?, What will the robot do next?, and What is the robots current action phase? span multiple manipulation phases from approach . These require temporal context, pose estimation, and sometimes motion history, leveraging the multi-step evolution of the scene. through reset Interaction Reasoning focuses on physical interaction dynamics, such as grasp stability or the robots current actuator state. These occur during stabilize phases, and depend on RGB, tactile, or gripper aperture signals. For instance, the question Is this stable grasp? may depend on contact force readings or inferred object displacement. , and release , contact The ground truth of the questions are grounded by multiple sensor modality observations. We design the incorrect answers as part of the visual question prototypes. For example, in the scene understanding, we require the sampled points to be significantly different in depth from other points and from the depth sensor to account for sensor inaccuracy. In action understanding, the correct action arrow differs significantly from the distractor arrows by having large angular separation in the projected 2D image. To detect guessing by hallucination, we randomly replace some correct answers with \"None of Above\" option. 5 Figure 3: Distribution and key statistics of Robo2VLM-1 dataset. (Left) Robo2VLM-1 covers diverse scenes with the most frequent scenes in office (33.6%), lab (25.3%), and kitchen (16.9%). (Middle) Robo2VLM-1 covers tasks including common manipulation actions include pick (21.5%), put (20.6%), and move (9.9%). (Right) The table summarizes key dataset statistics including question characteristics, answer choices, and image resolutions."
        },
        {
            "title": "3.3 Keyframe Selection",
            "content": "Given that raw robotic trajectories often contain hundreds of frames sampled at high frequency, using all frames is computationally expensive and can introduce redundancy due to minimal temporal variation. Moreover, many intermediate frames are visually or semantically uninformative for downstream reasoning tasks. To address this, we select compact set of keyframes that retain essential semantic and visual cues while reducing redundancy and data volume. These keyframes are extracted from the multi-modal robot trajectory = {Ot}T t=1 based on manipulation phase transition, scene coverage diversity and context visibility."
        },
        {
            "title": "4 Robo2VLM-1 Dataset",
            "content": "Open X-Embodiment and its datasets Open X-Embodiment [24] is major collaborative research initiative that aggregates robotic demonstration data collected from 22 different robot embodiments across 35 research labs worldwide, encompassing over 1 million trajectories covering more than 500 skills. Applying domain reweighting [25], we select subset focusing on manipulation with real robot embodiments. In total, we use 13 datasets [31, 29, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 38, 44] with total of 176,139 trajectories. While most modalities are included in Open X-Embodiments release, we manually include modalities introduced by the original paper. For example, DROID dataset [31] includes camera calibration information and stereo depth. The detailed modality inclusion can be found in Table. 2. Table 2: Trajectories and sensing modalities across datasets with total of 176k trajectories. # Traj: number of trajectories; Prop: joint-state proprioception; Dpth: depth images; GripAp: gripper-aperture signal; # VQA: number of questions. denotes modality is available, denotes absent. Dataset # Traj Prop Dpth GripAp # VQA DROID [31] Fractal [28] Kuka MM [34] Autolab [35] Sirius [36] MVP [37] VINN [38] Fanuc [39] TableTop [41] VIOLA [42] BUDS [43] ROT [44] 92k 73k 3k 896 600 480 435 415 110 135 50 14 299k 267k 25k 22k 21k 8k 34 11k 5k 8k 6k 245 Robo2VLM for Open X-Embodiment We use Robo2VLM to process each robot trajectory from the Open X-Embodiment dataset by selecting and interpreting the scenes. The entire process takes 2935.7 GPU hours on Nvidia A100 GPUs. For each selected keyframe, Robo2VLM instantiates questions from embodied question templates resulting in the generation of pool of over 3 million VQA items. Table 3: Performance Comparison of Multimodal Foundation Models on OpenX-VQA Benchmark Categories (%). Upper part: zero-shot. Lower part: with CoT prompting. Spatial Reasoning Goal Reasoning Interaction Reasoning Model Zero-Shot Overall RS (%) (%) OS (%) SR (%) SU MV TS-G TS-S TS-GL AU (%) (%) (%) (%) (%) (%) IP (%) TU (%) 21.58 35.32 23.87 16.08 17.78 17.50 31.82 23.79 19.03 20.30 21.74 LLaVA 1.5-7B LLaVA 1.6 Mistral-7B 24.09 30.31 35.13 19.42 20.24 19.29 34.20 30.77 19.52 18.67 20.70 24.94 26.66 29.75 21.47 23.18 17.86 29.19 29.40 17.90 19.49 36.98 LLaVA 1.6-34B 18.13 51.56 28.60 31.94 55.87 18.51 26.61 16.43 28.23 35.27 Llama 3.2-90B 30.63 41.68 55.63 21.55 24.38 17.32 33.01 42.57 25.71 46.61 Qwen 2.5 VL-7B 49.39 71.37 21.85 28.53 17.50 34.21 55.08 12.90 30.45 63.80 37.68 Qwen 2.5 VL-32B 38.84 85.00 22.31 28.23 15.71 28.47 51.89 10.08 33.96 71.09 Qwen 2.5 VL-72B 8.06 7.82 37.76 CoT Reasoning 21.61 28.28 21.00 17.37 20.90 18.93 25.36 24.19 21.53 21.24 20.31 LLaVA 1.5-7B LLaVA 1.6 Mistral-7B 24.05 27.60 38.87 17.15 20.18 22.32 25.84 28.03 18.47 18.40 30.60 23.49 20.43 31.00 21.24 22.88 20.36 18.18 26.14 16.77 21.79 35.16 LLaVA 1.6-34B 30.45 32.34 79.87 13.35 26.37 18.57 29.90 29.14 14.27 19.76 59.24 Llama 3.2-90B 34.82 38.02 90.00 21.78 23.30 16.79 36.84 46.48 18.39 28.15 42.71 Qwen 2.5 VL-7B 48.85 90.50 18.82 29.19 19.82 35.17 60.43 18.71 32.21 71.35 Qwen 2.5 VL-32B 41.30 39.52 44.79 92.37 18.36 29.73 13.39 29.19 55.28 13.15 36.13 74.09 Qwen 2.5 VL-72B 22.37 22.83 30.59 49.77 39.73 49.32 54. 20.09 29.68 26.94 44.75 36.99 49.32 46.12 Category Abbreviations: Spatial Reasoning: RS: Robot State (gripper/arm position estimation), OS: Object State (object reachability/manipulability), SR: Spatial Relationship (relative positioning between robot and objects), SU: Scene Understanding (spatial layout comprehension), MV: Multiple View (cross-view correspondence). Goal-Conditioned Reasoning: TS-G: Task State-grasp (grasp stability assessment), TS-S: Task State-success (task completion status), TS-GL: Task State-goal (goal configuration understanding), Interaction Reasoning: AU: Action Understanding (robots current action phase), IP: Interaction Phase (prediction of next robot action), TU: Trajectory Understanding (overall task interpretation). Robo2VLM-1 Curation Inspired by data optimization paradigms such as domain reweighting in natural language processing [25] and robot policy learning [26], our curation process aims to balance the distribution of questions across diverse scene and task types. It selects representative and highquality subset of questions that effectively balances diversity across scenes, tasks, skills, and reasoning types, while ensuring clarity and unambiguous ground truth. In total, Robo2VLM-1 contains 684,710 questions, spanning 463 distinct real-world scenes, 3,396 unique robotic manipulation tasks, and 149 different manipulation skills."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we sample 60k VQA from Robo2VLM-1 with 50k training set (Robo2VLM-1-Train) and 10k testing set (Robo2VLM-1-Test). We mainly study two research questions: (1) How does Robo2VLM-1-Train dataset improve the spatial and interaction reasoning capabilities of VLMs? and (2) How effectively does Robo2VLM-1-Test evaluate VLMs in these reasoning tasks? Evaluation Setup We benchmark state-of-the-art open-source models in different configurations, including LLaVA, Llama 3.2 Vision, and Qwen2-VL/Qwen2.5-VL. Each model is evaluated under both zero-shot and Chain-of-Thought (CoT) prompting settings. For CoT, we follow the prompting strategy from [11] by appending the following instruction to the end of each question: Reason step by step about the answer, and show your work, for each step. Only after that, proceed to the final answer.\" We run simultaneous Llama-3.2-3B-Instruct to extract model outputs for final letter answer. We focus fine-tuning on language layers (both attention and MLP modules) while keeping vision layers frozen. For each configuration, we use random 2000 questions from the testing set. For consistency, all models are evaluated with temperature of 0.7, maximum completion token length of 4096, and overall context length of 10240. All models use their vision or vision instruct version with float16 quantization. All models are evaluated with 8 Nvidia A100 GPUs with 80GB memory. We use LoRA to fine-tune LLaVA 1.6 with rank 128 and alpha 256. 7 Figure 4: Fine-tuning LLaVA 1.6 with increasing training data of Robo2VLM-1 from 10k to 50k VQA items. Accuracy improvements almost all categories compared to no fine-tuning."
        },
        {
            "title": "5.1 Benchmark with Robo2VLM-1",
            "content": "Table 3 presents detailed comparison of visionlanguage foundation models on the Robo2VLM-1 benchmark, evaluated under both zero-shot and Chain-of-Thought (CoT) prompting conditions. The results reveal nuanced interactions across model architecture, scale, and reasoning strategy. Cross-Model Performance: Evaluation data on Robo2VLM-1-test suggests that Qwen models has higher overall accuracy compared to other VLMs of the same configuration, which align with the observation from other VQA benchmarks such as [51, 52]. Qwen 2.5 VL-72B achieves the highest zero-shot accuracy at 37.76%, while Qwen 2.5 VL-32B achieves 41.30% overall accuracy in the CoT setting. Qwen models particularly excel in object-centric categories such as Object State, where Qwen 2.5 VL-72B reaches 85.00% (zero-shot) and 92.37% (CoT), and Interaction Phase (IP) (71.09% zero-shot, 74.09% CoT for 72B). Impact of Model Scale. Zero-shot accuracy generally improves with model size rising from 30.63% (Qwen 7B) to 37.76% (Qwen 72B). However, this trend does not hold in the CoT setting, where the 32B model surpasses the 72B model (41.30% vs. 39.52%). The observation aligns the official technical report of Qwen2.5[2] that the mathematical and problem-solving capabilities of Qwen2.5-VL-32B are further enhanced through reinforcement learning. LLaMA models display different trend while the 11B model outperforms the 90B version in zero-shot setting, the larger model benefits more under CoT prompting, suggesting that scaling may unlock latent capabilities only when paired with explicit reasoning support. Effectiveness of CoT Prompting: CoT prompting generally enhances performance for both Qwen and LLaMA models. For example, Qwen 2.5 VL-7B improves from 30.63% to 34.82%, and LLaMA 3.2-90B increases from 28.60% to 30.45%. The most substantial gains are observed in Qwen 2.5 VL-32B, which improves from 37.68% to 41.30%. Results suggest that CoT benefits Task StateSuccess(from 55.08% to 60.43%), and Interaction Phase (from 63.80% to 71.35%). However, in the Spatial Relationship category, for example, Qwen 32Bs accuracy drops from 21.85% to 18.82%, indicating that verbose reasoning chains may introduce noise in tasks requiring precise spatial localization."
        },
        {
            "title": "5.2 Finetuning with Robo2VLM-1",
            "content": "We perform model finetuning experiment using Robo2VLM-1-train and evaluate on Robo2VLM-1test. We increase the training data samples from 10k to 50k in finetuning. As depicted in Figure 4, increasing the fine-tuning data generally leads to notable performance enhancements across most VQA categories. Significant gains are observed in Object State understanding, where accuracy improved from 29.34% to 80.24%. Task State-success also sees substantial rise from 47.65% to 68.03%. Other categories demonstrating clear positive trends with more data. However, in some categories such as Spatial Relationship and Task StateGoal, fine-tuning with limited data (e.g., 10k) underperforms the no-finetuning baseline. This may be because the model has not yet seen enough task-specific examples to begin generalizing, or because the question formats in Robo2VLM-1 differ from those seen during pretraining, requiring adaptation time. In some categories, finetuning with Robo2VLM-1 does not improve the performance due to the reasoning capability limitation of the base model. This is also reflected in the fact that LLaVA shows performance degradation in CoT prompting in Table 3. The interaction phase question requires the model to predict the next frame, 8 demanding complex reasoning and making it particularly challenging problem. This suggests that for complex tasks, the base model language performance is important for further improvement with Robo2VLM-1."
        },
        {
            "title": "5.3 Comparison with Human Performance",
            "content": "We conducted human evaluation covering all 11 categories defined in Table 3. For each category, human evaluator was asked to randomly answer questions from Robo2VLM-1-test. We use the average success rate as reference for comparison with three modelsLLaVA 1.6-7B, LLaVA 1.6-7B-Finetuned, and Qwen 2.5 VL32BCoT on the same set of categories as shown in Figure 5. Qwen 2.5 VL-32BCoT achieves near human accuracy, with 90.5% in Object State compared to 96.7% for humans, and 71.35% in Interaction Phase versus the human score of 80.0%. In more complex spatial reasoning tasks such as Spatial Relationship, where human achieves 60.0% accuracy, the best model (LLaVa 1.6-7B, finetuned) reaches only 19.42%. This may suggest that even if observing from multiple views, monocular image may lack the full depth information needed to accurately determine the spatial relationship. Furthermore, finetuning enhances model performance. LLaVA 1.6-7B finetuned on the Robo2VLM-1 training dataset shows consistent improvements across multiple categories, particularly in Task State, Object State, and Trajectory Understanding, compared to its non-finetuned LLaVA 1.6-7B. These findings demonstrate the potential Robo2VLM1 in studying and narrowing the gap between model and human performance in spatial and task reasoning. Figure 5: Comparison of human performance to different multimodal foundation models."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this paper, we introduce Robo2VLM, framework that generates VQA grounded in robot sensory modalities. We apply Robo2VLM to 176k real robot trajectories from Open X-Embodiment, and curate Robo2VLM-1, comprehensive dataset of 684,710 questions covering 463 distinct scenes, 3,396 robotic manipulation tasks, and 149 manipulation skills. Evaluation of state-of-the-art opensource VLMs suggests that some VLMs, such as Qwen2.5 VL 32B with CoT prompting, can achieve near human performance in questions related to object reachability and interaction understanding, while there is significant gap to human in reasoning fine-grained spatial relationship and interactions. Evaluation also suggests that finetuning Robo2VLM-1 dataset improves in spatial and interaction reasoning. Future work will focus on generalizing Robo2VLM to wider range of robot embodiments and generating an even more diverse dataset. We also plan to explore the deployment of models trained on Robo2VLM-1 to real-world robotic tasks. Limitation We acknowledge that Robo2VLM is data generation framework that relies on the quality of input tele-operated trajectories. If the original trajectory is wrongly calibrated, it compromises the quality of generated VQA data. Or if the original trajectory misses embodiment sensory modalities, such as NYU VINN [38] (0.2% of the 176k trajectories), it limits the amount of questions that Robo2VLM can generate."
        },
        {
            "title": "Acknowledgement",
            "content": "This research was performed at the AUTOLAB at UC Berkeley in affiliation with the Berkeley AI Research (BAIR) Lab. This work is supported in part by donations from Google."
        },
        {
            "title": "References",
            "content": "[1] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 87488763. PMLR, 1824 Jul 2021. [2] Qwen Team. Qwen2.5: party of foundation models, September 2024. [3] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [4] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [5] Anthropic. Claude 3.5 Sonnet. https://www.anthropic.com/news/claude-3-5-sonnet, June 2024. [6] OpenAI. GPT-4o System Card. https://openai.com/index/gpt-4o-system-card/, August 2024. [7] Koray Kavukcuoglu. Gemini 2.5: Our most intelligent AI model. https://blog.google/technology/ google-deepmind/gemini-model-thinking-updates-march-2025/, March 2025. [8] Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, and Fei Xia. Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1445514465, June 2024. [9] Siddharth Karamcheti, Suraj Nair, Ashwin Balakrishna, Percy Liang, Thomas Kollar, and Dorsa Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models, 2024. [10] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [11] Gemini Robotics Team, Saminda Abeyruwan, et al. Gemini robotics: Bringing ai into the physical world, 2025. [12] Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with hierarchical vision-language-action models, 2025. [13] Md Mofijul Islam, Alexi Gladstone, Riashat Islam, and Tariq Iqbal. EQA-MX: Embodied question answering using multimodal expression. In Proc. International Conference on Learning Representations (ICLR), 2024. [14] Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, and Tong Zhang. EMBODIEDBENCH: Comprehensive benchmarking multi-modal large language models for vision-driven embodied agents. arXiv preprint arXiv:2502.09560, 2025. [15] Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, and Jiajun Wu. Embodied agent interface: Benchmarking LLMs for embodied decision making. In NeurIPS 2024 Track on Datasets and Benchmarks, 2024. [16] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. ALFRED: benchmark for interpreting grounded instructions for household robots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [17] Andrew Szot, Edward Coumans, Alex Collett, and et al. Habitat 2.0: Training home assistants to rearrange their habitat. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2021. 10 [18] Eric Kolve, Roozbeh Mottaghi, Daniel Gordon, and et al. AI2-THOR: An interactive 3d environment for visual AI. arXiv preprint arXiv:1712.05474, 2017. [19] Pierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan, Christine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J. Joshi, Pete Florence, Wei Han, Robert Baruch, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi, Karol Hausman, Izhak Shafran, Brian Ichter, and Yuan Cao. Robovqa: Multimodal long-horizon reasoning for robotics. arXiv preprint arXiv:2311.00899, 2023. [20] Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, et al. Robobrain: unified brain model for robotic manipulation from abstract to concrete. CVPR, 2025. [21] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuomotor policies. Journal of Machine Learning Research, 17(39):140, 2016. [22] Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Charles Xu, Jianlan Luo, Tobias Kreiman, You Liang Tan, Dorsa Sadigh, Chelsea Finn, and Sergey Levine. Octo: An open-source generalist robot policy. https://octo-models.github.io, 2023. [23] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. In Proceedings of Robotics: Science and Systems (RSS), 23. [24] Open X-Embodiment Collaboration, Abby ONeill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, et al. Open X-Embodiment: Robotic learning datasets and RT-X models. https://arxiv.org/abs/2310.08864, 2023. [25] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. Advances in Neural Information Processing Systems, 36:6979869818, 2023. [26] Joey Hejna, Chethan Anand Bhateja, Yichen Jiang, Karl Pertsch, and Dorsa Sadigh. Remix: Optimizing data mixtures for large scale imitation learning. In Pulkit Agrawal, Oliver Kroemer, and Wolfram Burgard, editors, Proceedings of The 8th Conference on Robot Learning, volume 270 of Proceedings of Machine Learning Research, pages 145164. PMLR, 0609 Nov 2025. [27] Abhishek Sharma, Vishal Sundaresan, Yizhou Zhu, Parth Shah, Kuan Liu, Michael Laskin, Jonathan Tompson, Ayzaan Wahid, Yevgen Chebotar, and Karol Hausman. Droid: large-scale in-the-wild robot manipulation dataset. arXiv preprint arXiv:2310.01894, 2023. [28] Anthony Brohan et al. Rt-1: Robotics transformer for real-world control at scale. 2023. [29] Anthony Brohan et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control, 2023. [30] Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Lucy Xiaoyang Shi, James Tanner, Quan Vuong, Anna Walling, Haohuan Wang, and Ury Zhilinsky. Ï€0: vision-language-action flow model for general robot control. https://physicalintelligence.company/blog/pi0, 2024. [31] Alexander Khazatsky, Karl Pertsch, et al. Droid: large-scale in-the-wild robot manipulation dataset. 2024. [32] Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent plans for task agnostic offline reinforcement learning. 2022. [33] Oier Mees, Jessica Borja-Diaz, and Wolfram Burgard. Grounding language with visual affordances over unstructured data. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), London, UK, 2023. [34] Michelle Lee, Yuke Zhu, Krishnan Srinivasan, Parth Shah, Silvio Savarese, Li Fei-Fei, Animesh Garg, and Jeannette Bohg. Making sense of vision and touch: Self-supervised learning of multimodal representations for contact-rich tasks. In 2019 IEEE International Conference on Robotics and Automation (ICRA), 2019. 11 [35] Lawrence Yunliang Chen, Simeon Adebola, and Ken Goldberg. Berkeley UR5 demonstration dataset. https://sites.google.com/view/berkeley-ur5/home. [36] Huihan Liu, Soroush Nasiriany, Lance Zhang, Zhiyao Bao, and Yuke Zhu. Robot learning on the job: Human-in-the-loop autonomy and learning during deployment. In Robotics: Science and Systems (RSS), 2023. [37] Ilija Radosavovic, Tete Xiao, Stephen James, Pieter Abbeel, Jitendra Malik, and Trevor Darrell. Real-world robot learning with masked visual pre-training. In CoRL, 2022. [38] Jyothish Pari, Nur Muhammad Shafiullah, Sridhar Pandian Arunachalam, and Lerrel Pinto. The surprising effectiveness of representation learning for visual imitation, 2021. [39] Xinghao Zhu, Ran Tian, Chenfeng Xu, Mingyu Ding, Wei Zhan, and Masayoshi Tomizuka. Fanuc manipulation: dataset for learning-based manipulation with fanuc mate 200id robot. 2023. [40] Yifan Zhou, Shubham Sonawani, Mariano Phielipp, Simon Stepputtis, and Heni Amor. Modularity through attention: Efficient training and transfer of language-conditioned policies for robot manipulation. In Conference on Robot Learning, pages 16841695. PMLR, 2023. [41] Yifan Zhou, Shubham Sonawani, Mariano Phielipp, Heni Ben Amor, and Simon Stepputtis. Learning modular language-conditioned robot policies through attention. Autonomous Robots, pages 121, 2023. [42] Yifeng Zhu, Abhishek Joshi, Peter Stone, and Yuke Zhu. Viola: Imitation learning for vision-based manipulation with object proposal priors. 6th Annual Conference on Robot Learning (CoRL), 2022. [43] Yifeng Zhu, Peter Stone, and Yuke Zhu. Bottom-up skill discovery from unsegmented demonstrations for long-horizon robot manipulation. IEEE Robotics and Automation Letters, 7(2):41264133, 2022. [44] Siddhant Haldar, Vaibhav Mathur, Denis Yarats, and Lerrel Pinto. Watch and match: Supercharging imitation with regularized optimal transport. In Conference on Robot Learning, pages 3243. PMLR, 2023. [45] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. [46] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 69046913, 2017. [47] Jae Hee Lee, Matthias Kerzel, Kyra Ahrens, Cornelius Weber, and Stefan Wermter. What is right for me is not yet right for you: dataset for grounding relative directions via multi-task learning. arXiv preprint arXiv:2205.02671, 2022. [48] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [49] Peter Anderson, Qi Wu, Damien Teney, Joel Bruce, Mark Johnson, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [50] Henrik I. Christensen and Gregory D. Hager. Sensing and estimation. In Bruno Siciliano and Oussama Khatib, editors, Springer Handbook of Robotics, Springer Handbooks, pages 91112. Springer, 2016. [51] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts, 2024. [52] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021."
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}