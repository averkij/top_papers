{
    "paper_title": "Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers",
    "authors": [
        "Lukman Jibril Aliyu",
        "Umar Sani Muhammad",
        "Bilqisu Ismail",
        "Nasiru Muhammad",
        "Almustapha A Wakili",
        "Seid Muhie Yimam",
        "Shamsuddeen Hassan Muhammad",
        "Mustapha Abdullahi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Wildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as a promising tool for biodiversity monitoring and conservation. This paper presents a comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using a public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet-201, ResNet-152, EfficientNet-B4, and Vision Transformer ViT-H/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into a Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to African-grounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation."
        },
        {
            "title": "Start",
            "content": "Evaluating Deep Learning Models for African Wildlife Image Classification: From DenseNet to Vision Transformers Lukman Jibril Aliyu Arewa Data Science Academy Kano, Nigeria lukman.j.aliyu@gmail.com Umar Sani Muhammad Azman University Kano, Nigeria Bilqisu Ismail Arewa Data Science Academy Kano, Nigeria Nasiru Muhammad Arewa Data Science Academy Kano, Nigeria Almustapha A. Wakili Towson University Maryland, USA Seid Muhie Yimam Universitat Hamburg Hamburg, Germany Shamsuddeen Hassan Muhammad Imperial College London London, United Kingdom Mustapha Abdullahi Arewa Data Science Academy Kano, Nigeria AbstractWildlife populations in Africa face severe threats, with vertebrate numbers declining by over 65% in the past five decades. In response, image classification using deep learning has emerged as promising tool for biodiversity monitoring and conservation. This paper presents comparative study of deep learning models for automatically classifying African wildlife images, focusing on transfer learning with frozen feature extractors. Using public dataset of four species: buffalo, elephant, rhinoceros, and zebra; we evaluate the performance of DenseNet201, ResNet-152, EfficientNet-B4, and Vision Transformer ViTH/14. DenseNet-201 achieved the best performance among convolutional networks (67% accuracy), while ViT-H/14 achieved the highest overall accuracy (99%), but with significantly higher computational cost, raising deployment concerns. Our experiments highlight the trade-offs between accuracy, resource requirements, and deployability. The best-performing CNN (DenseNet-201) was integrated into Hugging Face Gradio Space for real-time field use, demonstrating the feasibility of deploying lightweight models in conservation settings. This work contributes to Africangrounded AI research by offering practical insights into model selection, dataset preparation, and responsible deployment of deep learning tools for wildlife conservation. Index TermsImage Classification, DenseNet, African wildlife, Computer Vision, Deep Learning. I. INTRODUCTION Africas rich wildlife heritage faces severe challenges from habitat loss, poaching, and climate change. It is estimated that an elephant is poached every 15 minutes in South Africa [1]. Monitoring animal populations traditionally relies on labor-intensive field surveys and camera trap image reviews. Advances in artificial intelligence offer promising tools to automate and ease these efforts. Image classification using deep learning has emerged as powerful approach to biodiversity monitoring [2]. By automatically identifying species in photographs taken on camera traps, drones, or smartphones, deep learning systems can greatly accelerate data collection and analysis for conservation biologists. In particular, [3] demonstrated that CNN could accurately identify wildlife species in millions of camera trap images, sometimes exceeding human accuracy. Similarly, projects such as Snapshot Serengeti and the iWildCam challenge have illustrated the potential of AI to handle largescale wildlife image datasets while highlighting the issue of domain shift between different environments [4]. Despite these successes, the implementation of such models in African conservation contexts presents unique challenges. Data on African fauna may be limited or imbalanced towards wellphotographed species, and models trained in one context (e.g., certain parks or image conditions) often struggle to generalize to new environments [5]. This paper explores the application of deep CNN architectures for classifying African wildlife images, with an emphasis on conditions and species relevant to African conservation. Our work is grounded in the African Wildlife [6] dataset, which consists of images of four key species (buffalo, elephant, rhinoceros, and zebra) that are important indicators of the health of the savannah ecosystem. We build on our preliminary findings that DenseNet-based classifier can achieve promising accuracy on this dataset. DenseNet [7] is known for its densely connected layers that promote feature reuse and mitigate the vanishing gradient problem. We hypothesize that it will be advantageous for learning from relatively small wildlife datasets. Using ImageNet transfer learning, we finetune DenseNet to recognize the target species. In this expanded study, we introduce comparative analysis with other modern architectures (namely ResNet [8], EfficientNet [9] and Vision Transformer [10]) to validate our choice of model. Furthermore, we significantly extend our review and discussion of the literature to contextualize our contributions within existing efforts in AI for Social Good and African sustainability goals. Our contributions are threefold: Improved Image Classification Pipeline: We present an improved image classification pipeline for African wildlife conservation, incorporating tailored dataset pre5 2 0 2 8 2 ] . [ 1 4 6 3 1 2 . 7 0 5 2 : r processing and model fine-tuning strategies specifically adapted to African ecological contexts. Comparative Evaluation of Architectures: We conduct comprehensive evaluation of DenseNets performance relative to other deep learning architectures, providing insights into the trade-offs between accuracy and computational efficiency. Ethical and Responsible AI Deployment: We explicitly address the ethical considerations, limitations, and broader societal impacts of deploying AI systems for conservation in Africa. II. RELATED WORK a) Deep Learning for Wildlife Conservation.: The intersection of computer vision and wildlife conservation has attracted considerable interest in recent years. Early pioneering work by [3] applied deep CNNs (e.g., ResNet-152) to the Snapshot Serengeti camera trap dataset [11], achieving remarkable accuracy in identifying over 40 species and demonstrating that deep learning can greatly improve ecological data processing. Following this, [12] emphasized the challenge of generalizing models to new locations (unseen domains) in the wildlife context. Their iWildCam 2018 study on the iwildCam dataset [13] showed that models trained on one set of camera traps suffered performance drops when applied to data from different regions, underlining the importance of diverse training data and domain adaptation techniques. More recently, the WILDS benchmark [14] formalized such distribution shift challenges, including an animal camera trap classification task that tests robustness of models to changes in geographical location and imaging conditions. Beyond camera trap imagery, deep learning has been used in various conservation scenarios. For instance, [15] developed an active learning framework to improve species classification, reducing the annotation burden by iteratively selecting the most informative wildlife images for labeling. Their approach, evaluated on large collection of European and African wildlife images, highlights how human-in-the-loop strategies can address dataset bias and scarcity. In aerial and drone-based wildlife monitoring, CNNs and object detectors have been used to detect animals in overhead imagery [5], expanding the toolkit for conservationists to include surveillance from the skies. These efforts collectively illustrate the growing role of AI in biodiversity assessment. b) CNN Architectures in Image Classification.: Convolutional neural networks have seen rapid evolution, with numerous architectures pushing the state of the art on image classification benchmarks. ResNet [8], introduced in 2015, demonstrated that very deep networks (with 50+ layers) could be effectively trained using residual skip connections, and it remains popular backbone for many vision tasks. DenseNet [7] further innovated by connecting each layer to all subsequent layers, maximizing feature reuse and alleviating vanishing gradients; this compact architecture often achieves comparable accuracy to deeper ResNets with fewer parameters. Another notable family is EfficientNet [9], which introduced principled compound scaling method to balance network depth, width, and resolution, leading to series of models ( from B0 to B7) that achieved excellent accuracy with high parameter efficiency. In parallel, the vision transformer (ViT) architecture [10] showed that transformer models (prevalent in NLP) can also excel at image recognition when trained on large datasets, by dividing images into patch embeddings and relying on selfattention mechanisms instead of convolutions. In wildlife image classification tasks, most studies have employed CNN architectures (often ResNet-based) via transfer learning. [16] proposed modified attention-based CNN (incorporating feature pyramid networks) and tested it on the same African Wildlife dataset used in this work, as well as an Animal-80 dataset; their method improved detection and classification performance by leveraging multi-scale features. Another recent study by [2] compared several deep models (DenseNet, ResNet, VGG, and the YOLOv8 detector) on custom wildlife dataset of 23 endangered species. They reported that the YOLOv8 model, though originally designed for object detection, achieved the highest classification accuracy (over 96% F1-score), outperforming the CNN classifiers. However, simpler CNNs like ResNet or DenseNet still remain competitive baselines, especially in scenarios with limited computational resources or where interpretability of the classification is needed (since one can visualize CNN feature maps more straightforwardly than transformer attention, for example). survey by [17] provides comprehensive overview of advancements in CNN-based image classification up to 2019, noting trends such as the move towards deeper but more efficient networks and the adoption of transfer learning as standard practice for limited data scenarios. While many CNN architectures are applicable to wildlife image classification, the choice often depends on the specific context: the size of the dataset, the need for speed (e.g., edge deployment), and the importance of generalization across domains. Our work contributes to this body of knowledge by evaluating DenseNet in an African wildlife context, using dataset gotten from Africa, and comparing it with ResNet and EfficientNet baseline, and vision transformer. We also discuss the ethical deployment of such models in conservation settings, an aspect less frequently addressed in technical studies. III. METHODOLOGY This study followed structured pipeline comprising data acquisition, preprocessing, model selection, training, and deployment as illustrated in Figure 1. Our goal was to evaluate the performance of different deep learning architectures on balanced wildlife image dataset, with an emphasis on efficient, ethical, and deployable AI for conservation efforts. A. Dataset and Preprocessing and Models In this study, we utilize the publicly available African Wildlife dataset [6], which consists of color images spanning four animal species: buffalo, elephant, rhinoceros, and zebra. Each class contains 376 images, resulting in balanced dataset Performance metrics, parameter counts, and training times are summarized in Table II. C. Experiment Tracking We used Weights & Biases (W&B) [22] for experiment tracking, logging, and visualization throughout this study. Key metrics such as training and validation accuracy, loss, F1score, precision, and recall were monitored in real time using W&B dashboards. Additionally, GPU power consumption during training was recorded and visualized to compare the computational efficiency of different architectures. All training runs were tagged, versioned, and documented via W&B to ensure reproducibility and systematic model comparison, and can be viewed at https://wandb.ai/lukmanaj/africa-wildlife-dli? nw=nwuserlukmanaj D. Deployment To demonstrate real-world applicability, we exported the best-performing convolutional modelDenseNet-201and deployed it as an interactive web application using Hugging Face Gradio. This application, available at https:// huggingface.co/spaces/lukmanaj/afri-wildlife-classify, enables conservationists and researchers to upload wildlife images and receive species predictions in real time, showcasing the practical potential of AI for biodiversity monitoring and wildlife protection. IV. EXPERIMENTAL RESULTS We evaluated four pretrained models: DenseNet-201, EfficientNet-B4, ResNet-152, and ViT-H/14 on the African Wildlife dataset, which consists of four species classes. Table reports the overall classification accuracy, macro-averaged F1score, and per-class F1-scores. Among the CNNs trained, DenseNet-201 achieved 67% accuracy and the highest F1-scores for the buffalo (0.72) and zebra (0.76) classes. EfficientNet-B4 performed the worst overall, with 48% accuracy and macro F1-score of 0.47. ResNet-152 yielded moderate results across all metrics. ViT-H/14 significantly outperformed all the other models, achieving 99% accuracy and the highest F1-scores for all classes. TABLE PERFORMANCE OF MODELS ON THE AFRICAN WILDLIFE TEST SET. METRICS INCLUDE ACCURACY, MACRO F1-SCORE, AND PER-CLASS F1-SCORES. Acc. Macro F1 Buffalo Elephant Rhino Zebra Model 0.76 DenseNet-201 67.0% 0.48 EfficientNet-B4 48.0% 0.67 57.0% ResNet-152 0.99 99.0% ViT-H/ 0.61 0.47 0.56 0.99 0.72 0.54 0.56 0.99 0.67 0.47 0.58 0.99 0.60 0.40 0.52 0.99 To provide additional context for model selection, Table II summarizes each models parameter count, estimated GFLOPs, Giga Floating Point Operations Per Second, which is unit of measurement that describes computers processing power, specifically its ability to perform floating-point Fig. 1. Deep Learning Pipeline for African Wildlife Species Classification. The workflow encompasses all stages from initial data acquisition through final deployment for real-time species identification in conservation applications. with total of 1,504 images. To enable robust model evaluation, the dataset is partitioned into two subsets: training set comprising 1,203 images (80%) and test set comprising 301 images (20%). This split ensures consistent framework for assessing classification performance across models. Each image was resized to 6464 pixels and normalized to the [0, 1] range. These preprocessing steps were adapted from [18], and they were essential in enhancing model generalization and robustness, especially with the relatively small dataset size. However, for the vision transformer training, the images were resized to 518 518 pixels as that is the minimum size that the model can take [19]. We evaluated four pretrained deep learning models for the classification task: DenseNet-201 [20] ,EfficientNet-B4 [9] ,ResNet-152 [8] and Vision Transformer (ViT-H/14) [10]. All models were initialized with ImageNet-pretrained weights using torchvision.models [21]. To reduce training time and avoid overfitting, we froze all feature extraction layers and fine-tuned only the final classification layers. B. Training Setup All experiments were conducted in the Kaggle cloud environment using an NVIDIA Tesla P100 GPU (16GB VRAM, CUDA 12.6, driver version 560.35.03). The models were implemented in PyTorch, with experiment tracking and visualization performed via Weights & Biases (W&B) [22]. Each model was trained using the following configuration: Optimizer: Adam Learning Rate: 0.001 Loss Function: CrossEntropyLoss Batch Size: 32 Epochs: 10 Input Resolution: 64 64 (518 518 for vision transformer as is required by the model) Training durations varied by architecture. DenseNet, EfficientNet and ResNet completed training in under two minutes, while the ViT-H/14 model required over an hour due to its large number of parameters and transformer-based design. operations (based on standard input resolution), training time, and notes on deployment feasibility. While ViT-H/14 achieved the highest accuracy, its large compute and memory footprint makes DenseNet-201 more practical option for lightweight deployments. TABLE II MODEL CHARACTERISTICS AND TRAINING TIMES. GFLOPS BASED ON 224224 INPUT. ONLY CLASSIFICATION LAYERS WERE FINE-TUNED."
        },
        {
            "title": "Model",
            "content": "Params (M) DenseNet-201 20.0 EfficientNet-B4 19.3 GFLOPs Acc. Time (s)"
        },
        {
            "title": "Notes",
            "content": "4.29 4.39 67% 92.5 48% 87.8 ResNet-152 ViT-H/14 60.2 632. 11.51 1016.7 57% 83.2 99% 6574.2 Resource intensive Gradio deployment Underperformed Good baseline V. DISCUSSION Our experiments underscore the potential of deep learning for wildlife image classification in African contexts. Using DenseNet-201, we achieved test accuracy of 67% and macro F1-score of 0.67, establishing strong CNN-based baseline across four key species: buffalo, elephant, rhino, and zebra. DenseNet consistently outperformed EfficientNetB4 and ResNet-152 in our setting, particularly in terms of per-class F1-scores for buffalo and zebra. This supports the hypothesis that DenseNets densely connected layers facilitate better feature propagation and reuse, making it well-suited for small and low-resolution datasets often encountered in conservation tasks. In contrast, EfficientNet-B4 underperformed (48% accuracy), despite its strong performance on ImageNet benchmarks. This may be due to its compound scaling design, which can be sensitive to input resolution and small datasets. ResNet152 achieved moderate results (57% accuracy), validating its robustness but still falling short of DenseNet in this task. As shown in Appendix 2, the Vision Transformer consumed significantly more GPU power throughout training compared to the CNN models. Detailed performance metrics, including macro-averaged F1 (3), precision (4), and recall (5) scores, are provided in the appendix. a) CNNs vs. Vision Transformers.: The most striking result came from the Vision Transformer ViT-H/14, which achieved 99% accuracy and near-perfect precision and recall across all classes. This highlights the potential of transformerbased models in wildlife classification tasksparticularly when leveraging large-scale pretraining. However, ViT-H/14 has over 600M parameters and high computational footprint, making it unsuitable for deployment on low-resource or edge devices without further model compression or distillation [23]. This is quite important if it will be integrated into system that does retraining using human-in-the-loop, as it will be computationally expensive to constantly retrain the model. Furthermore, in cases of use in places without good internet connection, performing inference offline will be faster in lightweight model. By comparison, CNNs like DenseNet offer more favorable balance between accuracy and deployability. While they may not match ViT-level accuracy, they can still provide robust performance at fraction of the compute cost, especially if they can be improved. b) Deployment and Domain Shift.: Our deployed prototype is developed using fine-tuned DenseNet-201 model and implemented as Hugging Face Gradio Space. This app enables field users to upload images and receive species predictions in real-time. However, when tested on smartphonecaptured field images, performance declined sharply. This domain shift between curated training data and real-world imagery is well documented by [12] and [14]. It emphasizes the need for more diverse, representative training datasets, incorporating variations in lighting, background, camera angle, and image quality. c) Comparison to Prior Work.: Our approach is relatively lightweight compared to detection-based or attentionenhanced models. Prior studies such as [16] and [2] have shown that YOLOv8 and similar architectures can yield higher accuracy by combining classification with object localization. While ViTs partially address this through attention, dedicated detection frameworks remain an attractive next step for improving real-world performance. d) Concluding Insights.: Despite limitations, our work illustrates that end-to-end AI tools for conservation are feasible using accessible tools and public data. Unlike many studies that stop at offline accuracy, we demonstrate functioning pipeline from training to deployment, making our research an impactful, Africa-grounded machine learning and providing foundation for future work on data diversity, model robustness, and real-world usability. VI. CONCLUSION AND FUTURE WORK In this paper, we presented deep learning approach for classifying African wildlife images, using DenseNet-201 model as the primary architecture. Our model achieved test accuracy of 67% on four-species dataset, demonstrating the feasibility of CNNs for species recognition with limited data. We also deployed the model in functional Hugging Face Gradio Space for user-friendly interaction. Future work will focus on expanding the datasetboth in size and species diversitythrough collaborations and possibly integrating camera trap images like those from Snapshot Serengeti [3]. Advanced data augmentation or synthetic data generation (e.g., with GANs) may further improve model robustness. We also plan to enhance the deployed app by incorporating user feedback for active learning and exploring deployment on edge devices (e.g., NVIDIA Jetson Nano) for offline use in remote areas [24]. Ethical AI practices by [25] will guide our work, and we aim to release our dataset and code publicly to support open, Africa-centric AI research. Finally, we plan to test the system under diverse African field conditions and on edge devices, to ensure the models effectiveness in real conservation deployments. VII. ETHICAL CONSIDERATIONS AND LIMITATIONS Bias in the dataset favoring well-photographed conditions could lead to uneven model performance across species or environments. Ethical use requires transparency about such limitations, especially when models may underperform on rarer or nocturnal species. Furthermore, data provenance must be considered; although the Kaggle dataset was public, deployment may require further permissions. Privacy concerns also arise if human subjects are unintentionally captured in future datasets. Any scale-up involving camera traps should include privacy safeguards. Deployment risks such as over-reliance and potential misuse (e.g., by poachers) necessitate human-in-the-loop approach and access controls. Overall, the current model is proof-of-concept and not yet robust enough for critical decision-making; however, it demonstrates value as prototype for ranger-assisted monitoring and citizen science. VIII. BROADER IMPACT This work contributes to the AI for Social Good agenda by applying machine learning to biodiversity monitoring, supporting SDG 15 (Life on Land). Automatic classification can accelerate wildlife surveys, improve anti-poaching efforts, and unlock underused camera trap datasets. Our African-centered approach demonstrates that impactful AI research can emerge from local challenges. By building and sharing open tools, we promote capacity-building among African researchers and ecologists. The methodology can also inspire adjacent applications such as crop monitoring or disease surveillance. Potential negative impacts, such as overreliance on AI or misuse, are acknowledged but can be mitigated through careful design and stakeholder participation. This project lays the groundwork for responsible, locally grounded AI systems that improve ecological conservation."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "We thank Arewa Data Science Academy for supporting this project through the Arewa Data Science Deep Learning with PyTorch fellowship. We are also grateful to the developers of the African Wildlife Dataset on Kaggle (Bianca Ferreira) and the open-source deep learning community."
        },
        {
            "title": "REFERENCES",
            "content": "[1] P. Henthorne, Elephant poaching in south africa, May 2020, university of San Francisco Office of Sustainability Student Blog. [Online]. Available: https://usfblogs.usfca.edu/sustainability/2020/05/15/ elephant-poaching-in-south-africa/ [2] S. Sharma, S. Dhakal, and M. Bhavsar, Transfer learning for wildlife classification: Evaluating YOLOv8 against densenet, resnet, and vggnet on custom dataset, Journal of Artificial Intelligence and Capsule Networks, vol. 6, no. 4, pp. 415435, 2024. [3] M. S. Norouzzadeh, A. Nguyen, M. Kosmala, A. Swanson, C. Packer, and J. Clune, Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning, Proceedings of the National Academy of Sciences (PNAS), vol. 115, no. 25, pp. E5716 E5725, 2018. [4] A. Tøn, A. Ahmed, A. S. Imran, M. Ullah, and R. M. A. Azad, Metadata augmented deep neural networks for wild animal classification, Ecological Informatics, vol. 83, p. 102805, Nov. 2024. [Online]. Available: http://dx.doi.org/10.1016/j.ecoinf.2024.102805 [5] Z. Xu, T. Wang, A. K. Skidmore, S. D. Phinn, and L. Wang, review of deep learning techniques for detecting animals in aerial and satellite images, International Journal of Applied Earth Observation and Geoinformation, vol. 128, p. 103732, 2024. [6] B. Ferreira, African wildlife dataset, https://www.kaggle.com/datasets/ biancaferreira/african-wildlife/data, 2020, accessed: 2024-02-13. [7] G. Huang, Z. Liu, L. V. D. Maaten, and K. Q. Weinberger, Densely connected convolutional networks, in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 47004708. [8] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770778. [9] M. Tan and Q. V. Le, Efficientnet: Rethinking model scaling for convolutional neural networks, in Proc. International Conference on Machine Learning (ICML), ser. PMLR, vol. 97, 2019, pp. 61056114. [10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, and et al., An image is worth 16x16 words: Transformers for image recognition at scale, in Proc. International Conference on Learning Representations (ICLR), 2021. [11] A. Swanson, M. Kosmala, C. Lintott, R. Simpson, A. Smith, and C. Packer, Data from: Snapshot serengeti, high-frequency annotated camera trap images of 40 mammalian species in an african savanna, 2015. [Online]. Available: https://doi.org/10.5061/dryad.5pt92 [12] S. Beery, G. V. Horn, and P. Perona, Recognition in terra incognita: Wildlife object classification in unseen domains, in Proc. European Conference on Computer Vision (ECCV) Workshops, 2018, pp. 5268. [13] S. Beery, G. van Horn, O. M. Aodha, and P. Perona, The iwildcam 2018 challenge dataset, 2019. [Online]. Available: https: //arxiv.org/abs/1904.05986 [14] P. W. Koh, S. Sagawa, H. Marklund, and et al., WILDS: benchmark of in-the-wild distribution shifts, in Proc. International Conference on Machine Learning (ICML), ser. PMLR, vol. 139, 2021, pp. 56375664. [15] L. Bothmann, L. Wimmer, O. Charrakh, T. Weber, H. Edelhoff, and W. Peters, Automated wildlife image classification: An active learning tool for ecological applications, Ecological Informatics, vol. 77, p. 102231, 2023. [16] C. C. Ukwuoma, Z. guang Qin, G. U. Nneji, and G. C. Urama, Animal species detection and classification framework based on modified multiscale attention mechanism and feature pyramid network, Scientific African, vol. 16, p. e01151, 2022. [17] F. Sultana, A. Sufian, and P. Dutta, Advancements in image classification using convolutional neural network, arXiv preprint arXiv:1905.03288, 2019. [18] C. Shorten and T. M. Khoshgoftaar, survey on data augmentation for deep learning, Journal of Big Data, vol. 6, p. 60, 2019. [19] P. C. Team, torchvision.models.vit 14, 2025, accessed: 202505-13. [Online]. Available: https://docs.pytorch.org/vision/main/models/ generated/torchvision.models.vit 14.html [20] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, Densely connected convolutional networks, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 47004708, 2017. [21] Torchvision Contributors, Models and pre-trained weights, https:// pytorch.org/vision/main/models.html, accessed: 2025-05-18. [22] L. Biewald, Experiment tracking with weights and biases, 2020, software available from wandb.ai. [Online]. Available: https://www. wandb.ai/ [23] S. Saha and L. Xu, Vision transformers on the edge: comprehensive survey of model compression and acceleration strategies, 2025. [Online]. Available: https://arxiv.org/abs/2503.02891 [24] S. Ingaleshwar, F. Tasharofi, M. A. Pava, H. Vaishya, and et al., Wildlife species classification on the edge: deep learning perspective, in Proc. 16th Int. Conf. on Agents and Artificial Intelligence (ICAART), 2024, pp. 600608. [25] World Wide Fund for Nature (WWF), Living planet report 2022 regional fact sheet: Africa, https://africa.panda.org/factsheets/, 2022, accessed 2025-05-09."
        },
        {
            "title": "ADDITIONAL TRAINING STATISTICS",
            "content": "Fig. 2. GPU power usage during model training. Fig. 5. Macro-averaged recall scores. Fig. 6. Overall accuracy comparison. Fig. 7. Test accuracy over steps. Fig. 3. Macro-averaged F1-score for each model. Fig. 4. Macro-averaged precision scores. Fig. 8. Training loss progression."
        }
    ],
    "affiliations": [
        "Arewa Data Science Academy Kano, Nigeria",
        "Azman University Kano, Nigeria",
        "Imperial College London London, United Kingdom",
        "Towson University Maryland, USA",
        "Universitat Hamburg Hamburg, Germany"
    ]
}