{
    "paper_title": "Uni$\\textbf{F}^2$ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models",
    "authors": [
        "Junzhe Li",
        "Xuerui Qiu",
        "Linrui Xu",
        "Liya Guo",
        "Delin Qu",
        "Tingting Long",
        "Chun Fan",
        "Ming Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) have emerged as a powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on $\\textbf{coarse}$ facial attribute understanding, with limited capacity to handle $\\textbf{fine-grained}$ facial attributes and without addressing generation capabilities. To overcome these limitations, we propose Uni$\\textbf{F}^2$ace, the first UMM tailored specifically for fine-grained face understanding and generation. In general, we train Uni$\\textbf{F}^2$ace on a self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and a two-level mixture-of-experts architecture. Specifically, we first build a large-scale facial dataset, Uni$\\textbf{F}^2$ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span a wide range of facial attributes. Second, we establish a theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the model's ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on Uni$\\textbf{F}^2$ace-130K demonstrate that Uni$\\textbf{F}^2$ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 0 2 1 8 0 . 3 0 5 2 : r UniF2ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models Junzhe Li1,2, Xuerui Qiu3, Linrui Xu4, Liya Guo5, Delin Qu6 Tingting Long2, Chun Fan2, Ming Li7 1School of Computer Science, Peking University 2Computer Center, Peking University 3Institute of Automation, Chinese Academy of Sciences 4Central South University 5Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University 6Fudan University 7Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ) lijunzhe1028@stu.pku.edu.cn, {l.tingting, fanchun}@pku.edu.cn, qiuxuerui2024@ia.ac.cn xulinrui@csu.edu.cn, gly22@mails.tsinghua.edu.cn, dlqu22@m.fudan.edu.cn, ming.li@u.nus.edu"
        },
        {
            "title": "Abstract",
            "content": "Unified multimodal models (UMMs) have emerged as powerful paradigm in foundational computer vision research, demonstrating significant potential in both image understanding and generation. However, existing research in the face domain primarily focuses on coarse facial attribute understanding, with limited capacity to handle finegrained facial attributes and without addressing generation capabilities. To overcome these limitations, we propose UniF2ace, the first UMM tailored specifically for finegrained face understanding and generation. In general, we train UniF2ace on self-constructed, specialized dataset utilizing two mutually beneficial diffusion techniques and two-level mixture-of-experts architecture. Specifically, we first build large-scale facial dataset, UniF2ace-130K, which contains 130K image-text pairs with one million question-answering pairs that span wide range of facial attributes. Second, we establish theoretical connection between discrete diffusion score matching and masked generative models, optimizing both evidence lower bounds simultaneously, which significantly improves the models ability to synthesize facial details. Finally, we introduce both token-level and sequence-level mixture-of-experts, enabling efficient fine-grained representation learning for both understanding and generation tasks. Extensive experiments on UniF2ace-130K demonstrate that UniF2ace outperforms existing UMMs and generative models, achieving superior performance across both understanding and generation tasks. 1. Introduction Recently, unified multimodal models (UMMs) have emerged as thriving and vibrant research field in mulFigure 1. UniF2ace is the first unified multimodal model specifically designed for face understanding and generation, encompassing tasks such as visual question answering, face image captioning and text-to-face image generation. The generated responses and images demonstrate UniF2aces significant potential in capturing fine-grained face attributes. timodal learning community, enabling both image understanding and generation within single network. This anyto-any generation approach marks significant step toward artificial general intelligence (AGI) [23, 49, 63]. UMMs enhance the flexibility and scalability of multimodal systems, streamlining the handling of diverse tasks and laying the foundation for more generalized systems that contribute to the development of world models [2, 26, 36, 54, 67, 69, 77]. Fine-grained face understanding and generation are essential for advancing computer vision and AGI, given the central role of faces in daily life. For example, accurate face understanding enables applications such as identity verification [51], emotion recognition [15, 46], and human-computer interaction [9, 33]. High-fidelity face generation drives progress in creative industries [37], virtual avatars [71], and data augmentation for model robustness [38]. These tasks also push the boundaries of multimodal reasoning and generative modeling, advancing AIs ability to capture human-like details, thereby bridging the gap between AI and human perception. Recent works in the face domain have addressed understanding and generation as separate tasks, each with inherent limitations. For face understanding, the investigators typically fine-tune pretrained multimodal large language models (MLLMs) on facial image-text datasets [8, 52, 68, 70, 72]. For instance, Face-MLLM [52] re-annotates the LAION-Face dataset [76] using Gemini-1.5-pro [55] and fine-tunes LLaVA-v1.5 [31] for face understanding. However, studies [24, 52] reveal that general-purpose MLLMs, such as the LLaVA family, struggle with fine-grained facial attribute understanding, leading to low-quality captions and inaccurate visual question-answering (VQA) pairs. Meanwhile, face generation methods [16, 20, 21, 41] primarily rely on diffusion models conditioned on multimodal inputs, such as semantic masks [61] and sketches [12, 60]. For example, Face-Makeup [11] conditions face generation on an input face image and text prompt to synthesize customized images while preserving structural integrity. However, these methods heavily depend on predefined visual prompts, limiting the models ability to extract fine-grained facial details from textual descriptions. Additional related works of unified multimodal models and face multimodal models can be found in Appendix A. In this work, we propose UniF2ace (see Fig. 1), the first UMM for the face domain, designed to simultaneously perform face understanding and generation tasks while capturing fine-grained facial attributes from both image and text modalities. The key challenges include aligning coarse captions with detailed facial attributes, achieving uniform embedding of images and text for seamless cross-modal alignment, and learning fine-grained facial representations for both understanding (image-to-text) and generation (textto-image). To support our research, we first introduce UniF2ace-130K, dataset containing 130K facial imagetext pairs and one million visual question-answering (VQA) pairs, spanning 46 attributes related to appearance, actions, and emotions. We annotate facial attributes for large set of images using our trained classifiers on the high-quality CelebV-HQ dataset [78]. These attributes correct and enhance captions generated by general MLLMs like GPT-4o [17], yielding precise fine-grained facial descriptions. Using carefully crafted prompts, we employ GPT-4 [1] to generate diverse VQAs, i.e., detailed description, conversation about face attributes, reasoning about action, based on these enhanced captions. For cross-modal alignment within single network, we combine autoregressive models for understanding and diffusion models for generation, inspired by Show-o [67]. We argue that synthesizing facial attributes consistent with text descriptions is more challenging than face understanding. To address this, we bridge two typical discrete diffusion paradigms, i.e., masked generative models [5] and scorebased diffusion models [39], through theoretical proof, enabling simultaneous optimization of evidence lower bounds (ELBOs) and significantly improving generation quality. Finally, to learn fine-grained patterns for both image-to-text understanding and text-to-image generation, we introduce powerful and efficient network architecture with token-level and sequence-level mixture-of-experts (MoE) layers, enabling adaptive handling of diverse attributes. We evaluate UniF2ace on the UniF2ace-130K test dataset, comparing its performance with state-of-the-art (SOTA) UMMs, e.g., JanusFlow [36] and TokenFlow [44]. Additionally, we compare it against advanced generative models, e.g., LlamaGen [53], Stable Diffusion 3 [14] and understanding-only multimodal models e.g., Qwen2-VL [59], InternVL2.5 [7] using popular metrics. The results demonstrate that UniF2ace significantly outperforms models of similar parameter scales and achieves performance comparable to, or even surpassing, larger-scale models. Our main contributions in this work are as follows: We introduce UniF2ace, the first unified multimodal model for fine-grained face understanding and generation, establishing solid baseline for future research. We present UniF2ace-130K, dataset containing 130K fine-grained image-caption pairs and one million VQAs. We develop an automated pipeline for generating fine-grained multimodal datasets, leveraging face attribute classifiers to enhance and correct captions generated by MLLMs. We establish theoretical connection by integrating score matching into the masked generative model, enabling the simultaneous optimization of two maximum likelihood ELBOs and benefiting the face generation. Additionally, we explore hybrid MoE architecture at both token and sequence levels to improve finegrained representation learning for both understanding and generation tasks. 2. Fine-grained Facial Dataset To overcome the limitations of existing datasets in the realm of multimodal facial modeling, we introduce high-quality dataset called UniF2ace-130K, which boasts remarkable alignment between facial images and textual descriptions (see Fig. 2). This dataset encompasses nearly 130K facial images, each paired with richly detailed captions. Additionally, it contains approximately 1M visual question answers, significantly enhancing its value for training and evaluating multimodal models. By offering such comprehensive resource, we aim to propel advancements in facial image understanding and generation, establishing solid foundation 2 Figure 2. Pipeline and examples of UniF2ace-130K construction. Left: three-stage pipeline for building UniF2ace-130K. Step-1: Highquality face images are collected. Step-2: Detailed captions are generated by GPT-4o with face attribute model trained to classify finegrained appearance, action, and emotion. Step-3: Question-answering pairs are created. These stages collectively refine GPT-4o-generated captions and produce fine-grained descriptions for VQAs generation. Right: representative example showcasing UniF2ace-130Ks ability to correct (e.g., gender), enhance (e.g., bags under eyes), and reason (e.g., talking, slight tiredness) in GPT-4o-generated captions. for wide range of multimodal learning tasks. The creation of UniF2ace-130K encompassed three key stages. (1) Step1: Collect high-quality facial images. (2) Step-2: Generate (3) Step-3: Create question-answering detailed captions. pairs. Each stage is outlined in detail below. (1) Step-1: Collect High-quality Facial Images. In this step, we curated more than 130,000 high-quality facial images from the following distinguished datasets. CelebVHQ [78] is large-scale video dataset featuring 35,666 clips representing 15,653 identities, each clip meticulously annotated with 83 facial attributes. We extracted one key frames from each video to utilize detailed annotations for finegrained face-text alignment. Flickr-Faces-HQ (FFHQ) [19] provided 70,000 high-quality PNG images at resolution of 1024 by 1024, offering substantial diversity in attributes such as age and ethnicity. Multi-Modal-CelebA-HQ (MMCelebA-HQ) [66] contributed 30,000 high-resolution images paired with descriptive captions that have proven invaluable for facial generation and analysis. (2) Step-2: Generate Detailed Captions. Existing face image datasets often lack detailed descriptions of finegrained attributes like bags under eyes or jewelry. To handle this, we develop two-stage caption generation process. In Stage I, we employed an advanced MLLM such as GPT-4o [17] to produce initial captions. We designed specialized prompt that incorporated brief face descriptions from the MM-CelebA-HQ dataset [66] to help GPT-4o accurately describe key facial attributes including appearance, emotion, and actions. The detailed descriptions of all prompts are presented later (see the Figure 4 of the supplementary material i.e., Fig.S4). In Stage II, we refined these captions by training face attribute classification models using the CelebV-HQ dataset [78]. Focusing on single-person images, we used the pretrained face model AntelopeV21 to extract face embeddings. By combining these with image embeddings from CLIP [45], we trained classification heads for appearance, action, and emotion attributes. We selected 29 appearances with accuracies over 93%, 10 actions with accuracies over 87%, and 7 emotions with accuracies over 80% as final preditions for inference. These highly accurate attributes were then predicted for all remaining images in FFHQ and MM-CelebA-HQ datasets [19, 66]. Finally, prompt integrating these classification results with the Stage captions was fed into GPT-4o to generate final captions that are both highly accurate and diverse. (3) Step-3: Create Question-answering Pairs. In this step, we proposed 1M VQAs covering diverse facial appearances, emotions, and character action reasoning for our UniF2ace-130K dataset. These VQAs are designed to enhance MLLMs ability to understand fine-grained faInspired by cial attributes through instruction tuning. LLaVA [32], we carefully designed prompts to enable GPT1https://github.com/deepinsight/insightface 3 Figure 3. Our UniF2ace architecture integrates Text-to-Image (T2I) and Multimodal Understanding (MMU) tasks. Text inputs are encoded via tokenizer, while input images are processed through VQGAN encoder, merging into unified token sequence. noise scheduler masks subset of image tokens, which are then processed by Transformer with Mixture-of-Experts (MoE) layers. These MoE layers are grouped for generation and understanding tasks, with the first operating at the token level using shared and routed experts, and the second incorporating domain-specific features at the sequence level. This hierarchical design enables fine-grained facial feature processing. The noise scheduler outputs pt(xtx0) for D3Diff loss computation, combined with text autoregressive loss to form the training objective. 4 [1] to generate series of VQAs based on image captions, facilitating fine-grained understanding and reasoning. Most current face-text datasets lack VQAs, while VQAs in general image-text datasets often focus on peoples clothing, location, and behavior, neglecting detailed facial descriptions. In contrast, our proposed VQAs encompass diverse facial details, including hair, nose, eyes, mouth, ears, skin, eyebrows, and adornments. Additionally, since facial attributes can reflect characters ongoing actions, our VQAs incorporate detailed reasoning processes to infer and describe these actions. By organizing the VQAs into the same format as the LLaVA dataset [32], we streamlined the process of adapting multimodal face models for post-training. This alignment minimizes alteration costs, ensuring efficient integration and enabling the models to leverage both datasets seamlessly for improved performance. 3. UniF2ace We introduce unified multimodal model, UniF2ace, designed to seamlessly model both the understanding and generation of fine-grained facial attributes. Our approach is realized from two perspectives: generation strategy (Section 3.1) and network architecture (Section 3.2). Regarding the generation strategy, we recognize that the generation of fine-grained facial attributes is significantly more challenging than understanding tasks, as highlighted in prior studies [13, 67, 77]. To address this, we harness the theory of score matching in discrete diffusion [35] and propose the dual discrete diffusion (D3Diff) training strategy, ensuring the meticulous synthesis of facial details. On the network architecture front, existing UMMs [67, 77] typically employ dense architectures and demand extensive training data. To overcome these limitations, we introduce token-level and sequence-level Mixture-of-Experts (MoE) layers. Distinct MoE modules are designed for generation and comprehension tasks, selectively integrating information such as facial embeddings to enhance the models ability to capture subtle facial attributes. 3.1. Dual Discrete Diffusion In this section, we first introduce the discrete diffusion model, then explain the masked generative model and its variants, and finally combine the masked generative model with the score matching method to achieve stable optimization of the generative model. In the discrete diffusion process, each token is confined to finite set, = {1, . . . , }, so its probability at time is represented as vector pt RN . The forward process is modeled as continuous-time Markov chain (CTMC) governed by the linear ordinary differential equation (ODE): dt pts(y x) = pts(y x) Qt, (1) 4 with the initial condition p0 pdata and the distribution converging to pstationary as . Here, Qt represents time-dependent sequence of transition matrices. The solution to this ODE is expressed as: of maximum likelihood. In Appendix D, we prove that our score loss provides tighter upper bound on the negative log-likelihood of the original data compared to the conventional masked generative loss. ts = exp (cid:0)(σ(t) σ(s)) Q(cid:1), (2) 3.2. Multi-level Grouped Mixture-of-Expert where σ(t) = (cid:82) 0 σ(s) ds denotes the cumulative noise level and exp denotes the matrix exponential. Following [34], the reverse process is formalized as: dpT dt = QT pT t, = pt(y) pt(x) Qt(x, y), (3) The score-based discrete diffusion model [34] introduces training-stable loss Lscore(sθ) that models the denoising process by estimating the score. This is defined as follows: Exp (cid:88) y=x (cid:18) wxy sθ(x)y where sθ(xt, t) (cid:104) pt(yt) pt(xt) p(y) p(x) (cid:105) log sθ(x)y +K (cid:19)(cid:19) , (cid:18) p(y) p(x) (4) is the predicted score from ytX the neural network, and K(a) = a(log 1) is normalizing constant ensuring Lscore 0. In our work, we focus on the absorbing state, namely the masked state, which is commonly used in masked generative models [5, 67]. We assume independence between tokens, as supported by [4, 47, 48]; the exact formulation is provided in Appendix C. key insight is that within masked generative models the posterior probability pθ(x0 xt) can be linked to the score in discrete diffusion model using Bayes theorem: pθ(x0 xt) pt(xt x0) (cid:20) pt(x0) (cid:21) pt(xt) = pt(xt x0)sθ(xt), (5) θ Leveraging this connection, we propose novel loss, the dual discrete diffusion (D3Diff) loss, for the posterior probability network, which is based on explicitly defined stochastic differential equations (SDEs) and is formulated as follows: LD3Diff = (cid:88) t=1 Eq(x0)q(xtx0) [log pθ(x0xt)] + (6) α Lscore (pt(xtx0) pθ(x0xt)) , Here, q(x0) represents the data distribution, q(xtx0) and pt(xtx0) are forward diffusion probabilities, and pθ(x0xt) is the network-predicted posterior with parameters θ. The score loss Lscore is balanced by hyperparameter α. Equation 6 links masked generative models and scorebased models in the discrete domain via Bayes theorem, enabling score loss application on pre-trained unified multimodal models [67] without additional cost. Unlike traditional masked generative loss, which relies solely on likelihood, our D3Diff loss optimizes two distinct upper bounds 5 To capture fine-grained facial attributes while maintaining hardware-friendly facial embeddings, we design distinct MoE layers, termed Multi-level Grouped MoE, tailored for both generation and understanding subtasks. This ensures optimal performance for each task, as illustrated in Fig. 3. We incorporate sequence-level MoE layer after the tokenlevel MoE layer to effectively process instance-level inputs, such as images and facial embeddings. Token-Level MoE. Following DeepSeekMoE [10], we partition feedforward neural network (FFN) into multiple experts with reduced hidden dimensions and use Top-K activation strategy  (Fig. 3)  . We also employ shared experts to integrate generalized knowledge across contexts. Unlike prior methods, we introduce grouped MoE, dividing experts into two groups based on the different tasks of Text-toImage (T2I) and Multimodal Understanding (MMU). Each group combines shared and routed MoE, with expert-level balance loss computed independently per group: LBalance = λt2i Nt2i(cid:88) i=1 fiPi + λmmu Nmmu(cid:88) j=1 fjPj, (7) where λt2i and λmmu are balance factors; Nt2i and Nmmu means routed experts for T2I and MMU tasks, respectively; and denote expert selection frequency and probability. Sequence-Level MoE. We propose sequence-level MoE, where distinct experts process the entire image feature. We design three experts for the T2I group: copy expert (skip operation), zero expert (discard operation), and noise expert. Following MOE++ [18], the copy and zero experts require no additional parameters. Ecopy(x) = and Ezero(x) = 0, (8) where Ecopy() is the copy expert and Ezero() is the zero expert. For the noise expert Enoise(), we first integrate the time-step embedding, which operates on the noise level σ(t) to obtain the noise embedding vector vnoise, following score-based discrete diffusion models [34]. Then, resampler : Rh RLD maps vnoise into the sequence feature space (see Appendix for resampler details). The resampled noise embedding is added as matrix to the sequence feature. Formally, the noise experts output is: Enoise(x) = λ1x + λ2S(vnoise), (9) and [λ1, λ2] is calculated by: [λ1, λ2] = Softmax(Wnoise Flatten(x)), (10) Figure 4. Comparative analysis of face images generation quality across SDXL [43], TokenFlow [44], OmniFlow [23], Show-o [67], and UniF2ace. Our proposed UniF2ace effectively captures more detailed information from prompts. We highlight fine-grained attributes. where Wnoise R2(LD) is trainable weight matrix. In the MMU task, we include copy experts and introduce CLIP experts and face experts, which are similar to noise experts. Next we extract image embeddings by CLIP [45] and face embeddings using AntelopeV2 as supplementary features to enhance fine-grained facial attribute capture. Formally, the outputs of the CLIP and face experts are: ECLIP(x) = α1x + α2S(G(X)), (11) Eface(x) = α1x + α2S(F(X)), where and are the image encoder and face encoder, respectively. is the input face image. (12) 3.3. Overall Training Objectives To perform both auto-regressive and discrete score-based diffusion modeling, we employ two learning objectives: 1) Next Token Prediction (NTP) and 2) Dual Discrete Given sequence with image tokens Diffusion. = {X1, X2, . . . , XN } and text tokens = {Y1, Y2, . . . , YM }. Then we maximize the likelihood of text tokens by employing the standard language modeling objective (NTP loss): LMMU = (cid:88) i=1 log (Yi Y<i, ), (13) Next, the overall training objectives of UniF2ace are formulated as: Ltotal = LMMU + λLD3Diff, (14) where λ denotes balancing coefficient, which is setted to 1 in our experiments. 4. Experiment 4.1. Implementation We train our model on the UniF2ace-130K training dataset part, comprising 120K 256 256 face images, each annotated with detailed captions and seven to eight VQAs, about 900K. And more details about implementations can be found in Appendix B. We evaluate the generation and understanding tasks separately on the UniF2ace-130K test dataset. For generation, we use VQAscore to measure the relevance of generated images to captions, reporting results based on CLIP-FlanT5-11B (VQAscore-CF5) [? ]and LLaVA-v1.5-13B (VQAscore-LV) [32] for robust assessment. We also employ Frechet Inception Distance (FID) to measure similarity to ground truth and VLM-score to evaluate facial realism. For understanding, we follow LLaVA [30] and use GPT-4o [17] and DeepSeek-v3 [29] to score responses on 1-10 scale across two dimensions: detailed captioning (Desc-GPT, Desc-DS), assessing accuracy in capturing face attributes, and visual question answering (Conv-GPT, Conv-DS), measuring precision in responding to fine-grained queries. 4.2. Face Generation To verify the effectiveness of UniF2ace, we compare it with SOTA generative models including autoregressive models like LlamaGen [73] and diffusion-based models like Stable 6 Type Model Method # Params VQAscore-CF5 VQAscore-LV FID VLM-score Gen. Only Und. and Gen. LlamaGen [53] DALL-E 3 [3] SD3 [14] SDXL [43] AR AR Diff Diff AR TokenFlow [44] Diff OmniFlow [23] AR + Diff JanusFlow [36] Show-o [67] AR + Diff UniF2ace(Ours) AR + Diff 0.8B - 2B 2.6B 7B 3.4B 1.3B 1.3B 1.8B 0.746 0.845 0.903 0.876 0.871 0.798 0.881 0.855 0.894 0.551 0.644 0.671 0.660 0.664 0.585 0.653 0.650 0. 183.466 106.477 93.471 123.095 98.194 180.933 72.825 142.557 66.005 49.773 50.122 75.944 72.764 73.177 24.96 61.593 75.618 88.049 Table 1. Comparing the generative capability of UniF2ace with other generative and unified multimodal models, UniF2ace achieves stateof-the-art performance for models of the same parameter size and delivers comparable or superior results against larger models. Bold indicates the best performance overall, while underlined denotes the best among Und. and Gen. types. We use red to highlight the larger model size than ours. Figure 5. Activation frequency of Token-Level and Sequence-Level MoE in different layers. The left column corresponds to understanding tasks, while the right column corresponds to generation tasks. Larger circles indicate experts that are activated more frequently. Diffusion (SD3) [14]. Additionally, we evaluate it against unified multimodal models (UMMs) using various methods including TokenFlow [44], OmniFlow [23] and others. The complete results are presented in Tab. 1. The results show that UniF2ace achieves SOTA on VQA-score-LV, FID, and VLM-score, and outperforms UMMs while nearing SD3 on VQA-score-CF5. This indicates that UniF2ace can generate higher-quality, more realistic face images while better capturing fine-grained facial attributes from text. We conduct qualitative assessments on challenging UniF2ace-130K test scenarios involving complex facial details (see Fig. 4). The results show that UniF2ace effectively captures detailed prompts like rosy cheeks and monsterlike hat including white eye in case1, and hoop earrings in case2. Additionally, UniF2ace generates notably more realistic face images compared to other models. More examples can be found in Fig.S1 and Fig.S2. We analyze MoE activation frequencies across layers, as shown in the right column of Fig. 5. For token-level MoEs, high activation frequencies are concentrated between experts 5 and 8, indicating limited token feature variability in the generation task. For sequence-level MoEs, noise and zero expert activations are evenly distributed, indicating effective training with selective noise embedding and truncation. 4.3. Face Understanding We compare UniF2ace with advanced autoregressive multimodal models like Qwen2-VL [59] and similar UMMs [65, 67]. As shown in Tab. 2, UniF2ace outperforms existing models in facial image captioning and VQA tasks with significantly fewer parameters. We present representative cases for VQAs. As shown in Fig. 6, in case 1, compared to VILA1.5 and JanusFlow, UniF2ace focuses on more details, like sideburns achieving the highest score. In case 2, UniF2ace identifies subtle earrings and associates them with stylish appearance, demonstrating its fine-grained face understanding. We also provide examples for captioning later (see Fig.S3). For MoEs in the understanding group, we analyze expert activation frequencies, as shown in the left column of Fig. 5. For token-level MoEs, tokens often select the same expert 7 Type Model Method # Params Desc-GPT Conv-GPT Desc-DS Conv-DS Und. Only Und. and Gen. VILA1.5 [28] Qwen2-VL [59] LLaVA-v1.5 [31] InternVL2.5 [7] TokenFlow [44] OmniFlow [23] JanusFlow [36] Show-o [67] UniF2ace(Ours) AR AR AR AR AR Diff AR + Diff AR + Diff AR + Diff 3B 7B 7B 8B 7B 3.4B 1.3B 1.3B 1.8B 4.76 5.16 4.28 5. 5.02 1.62 4.88 3.88 6.02 5.20 6.27 5.48 5.89 5.80 - 6.06 4.17 6.53 6.56 5.50 4.84 6.30 5.82 1.90 5.42 5.24 7.38 6.54 6.86 6.20 6. 6.39 - 6.77 4.90 7.29 Table 2. Evaluation on face understanding tasks compared with advanced multimodal models and unified multimodal models. Our UniF2ace achieves the highest scores across all metrics, demonstrating superior ability to extract and analyze features from face images. Loss Type Weight α VQAscore-CF5 VQAscore-LV FID VLM-score D3Diff Only Mask Only Score 0.1 0.01 0.001 0 0. 0.887 0.894 0.884 0.879 0.886 0.673 0.679 0.668 0.661 0.670 68.903 66.005 72.736 77.463 69.694 86.378 88.049 89.220 85.993 87.951 Table 3. Performance comparison with different loss. Considering all metrics, the optimal result is achieved with α = 0.01 in D3Diff. Bold: Best performance. Underlined: Second best performance. Token MoE Sequence MoE Generation Understanding VQAscore FID VLM-score Desc Conv 6.031 6.495 6.247 6.532 84.432 87.917 86.790 88.049 72.877 67.415 69.312 66.005 4.988 5.678 5.864 6. 0.878 0.887 0.889 0.894 Figure 6. Comparison of visual question-answering results and GPT-4o-based scores. in the top (the closest to the prediction head) and bottom layers. For sequence-level MoEs, face and CLIP experts are more frequently activated in layers closer to the top, indicating that deeper layers benefit from face and image embeddings to better analyze face images. Notably, activation patterns in the understanding group differ entirely from the generation group, highlighting the effectiveness of our group-based strategy. 4.4. Ablation Studies Coefficient in Dual Discrete Diffusion. The scorematching loss in dual discrete diffusion is approximately 200 greater than the masked generative loss, making the selection of weight coefficients α critical. We experiment with different coefficients for comparison, as shown in Tab. 3. To demonstrate the effectiveness of the D3Diff loss, we compare it with using only the masked generative loss or score loss individually. The results show that D3Diff loss achieves the best performance on the generation task. NoTable 4. Performance impact of token-level and sequence-level MoE in UniF2ace through ablation study. Both MoEs contribute significant performance improvements. tably, using only score loss outperforms only masked generative loss, supporting our theory proof in Appendix D. Token and Sequence Level MoEs. To verify the effectiveness of the token-level and sequence-level MoE individually, we conducted series of ablation experiments, as shown in Tab. 4. We separately evaluate metrics for generation and understanding tasks under different experimental settings. The VQAscore is assessed using CLIP-FlanT511B, while the understanding tasks are evaluated with GPT4o. The results show that token-level and sequence-level MoE outperform the baseline on different tasks, while their combination achieves the best performance. This underscores the efficacy of our design in enhancing the models ability to extract and analyze facial features. 5. Conclusion This paper introduces UniF2ace, the first unified multimodal model (UMM) designed for fine-grained face understanding and generation. The model bridges the gap between score-based models and masked generative models in discrete diffusion, while leveraging token-level and sequence-level mixture-of-experts (MoE) to sparsify the model. Extensive experiments show that UniF2ace outperforms existing UMMs and even surpasses larger generation-only or understanding-only models. This underscores the potential of our improvements to guide future research in specialized applications of UMM. Additionally, we constructed face-text aligned dataset, UniF2ace-130K, to further advance multimodal research in the community."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 4 [2] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pages 16921717. PMLR, 2023. 1 [3] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. 7 [4] Andrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and Arnaud Doucet. continuous time framework for discrete denoising models. Advances in Neural Information Processing Systems, 35: 2826628279, 2022. 5 [5] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 2, 5 [6] Xiaokang Chen, Zhiyu Wu, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie, Xingkai Yu, and Chong Ruan. Januspro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811, 2025. [7] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2, 8 [8] Tahar Chettaoui, Naser Damer, and Fadi Boutros. Froundation: Are foundation models ready for face recognition? Image and Vision Computing, page 105453, 2025. 2, 1 [9] Kalpana Chowdary, Tu Nguyen, and Jude Hemanth. Deep learning-based facial emotion recognition for human computer interaction applications. Neural Computing and Applications, 35(32):2331123328, 2023. 1 [10] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Yu Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. 5, 1 [11] Dawei Dai, Mingming Jia, Yinxiu Zhou, Hang Xing, and Chenghang Li. Face-makeup: Multimodal facial prompts for text-to-image generation. arXiv preprint arXiv:2501.02523, 2025. 2, 1 [12] Kangle Deng, Gengshan Yang, Deva Ramanan, and Jun-Yan Zhu. 3d-aware conditional image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44344445, 2023. 2 [13] Xinya Du, Junru Shao, and Claire Cardie. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2017. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. 2, 7 [15] Samira Hazmoune and Fateh Bougamouza. Using transformers for multimodal emotion recognition: Taxonomies and state of the art review. Engineering Applications of Artificial Intelligence, 133:108339, 2024. 1 [16] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-modal face generation and In Proceedings of the IEEE/CVF conference on editing. computer vision and pattern recognition, pages 60806090, 2023. 2, 1 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 3, 6 [18] Peng Jin, Bo Zhu, Li Yuan, and Shuicheng Yan. Moe++: Accelerating mixture-of-experts methods with zeroarXiv preprint arXiv:2410.07348, computation experts. 2024. [19] Tero Karras. style-based generator architecture for generative adversarial networks. arXiv preprint arXiv:1812.04948, 2019. 3 [20] Jihyun Kim, Changjae Oh, Hoseok Do, Soohyun Kim, and Kwanghoon Sohn. Diffusion-driven gan inversion for In Proceedings of multi-modal face image generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1040310412, 2024. 2, 1 [21] Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu. Dcface: Synthetic face generation with dual condition diffusion In Proceedings of the ieee/cvf conference on commodel. puter vision and pattern recognition, pages 1271512725, 2023. 2 [22] Bokyeung Lee, Hyunuk Shin, Bonhwa Ku, and Hanseok Ko. Frame level emotion guided dynamic facial expression recognition with emotion grouping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 56815691, 2023. 1 9 [23] Shufan Li, Konstantinos Kallidromitis, Akash Gokul, Zichun Liao, Yusuke Kato, Kazuki Kozuka, and Aditya Grover. Omniflow: Any-to-any generation with multi-modal rectified flows. arXiv preprint arXiv:2412.01169, 2024. 1, 6, 7, 8 [24] Yifan Li, Anh Dao, Wentao Bao, Zhen Tan, Tianlong Chen, Huan Liu, and Yu Kong. Facial affective behavior analysis with instruction tuning. In European Conference on Computer Vision, pages 165186. Springer, 2024. [25] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. 1 [26] Zijie Li, Henry Li, Yichun Shi, Amir Barati Farimani, Yuval Kluger, Linjie Yang, and Peng Wang. Dual diffusion for unified image generation and understanding. arXiv preprint arXiv:2501.00289, 2024. 1 [27] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Jinfa Huang, Junwu Zhang, Yatian Pang, Munan Ning, et al. Moe-llava: Mixture of experts for large visionlanguage models. arXiv preprint arXiv:2401.15947, 2024. 1 [28] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. Vila: On pre-training for visual language models, 2023. 8 [29] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. 6 [30] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2, 8 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 3, 4, 6 [33] Jiaxi Liu. Chatgpt: Perspectives from humancomputer interaction and psychology. Frontiers in Artificial Intelligence, 7:1418869, 2024. 1 [34] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 5, 2 [35] Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. In Forty-first International Conference on Machine Learning, 2024. 4 [36] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified multimodal understanding and generation. arXiv preprint arXiv:2411.07975, 2024. 1, 2, 7, 8 [37] Andrew Melnik, Maksim Miasayedzenkau, Dzianis Makaravets, Dzianis Pirshtuk, Eren Akbulut, Dennis Holzmann, Tarek Renusch, Gustav Reichert, and Helge Ritter. Face generation and editing with stylegan: survey. IEEE Transactions on pattern analysis and machine intelligence, 46(5): 35573576, 2024. [38] Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Dominik Lawatsch, Florian Domin, and Maxim Schaubert. Gandiffface: Controllable generation of synthetic datasets for face recognition with realistic variations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 30863095, 2023. 2 [39] Chenlin Meng, Kristy Choi, Jiaming Song, and Stefano Ermon. Concrete score matching: Generalized score matching for discrete data. Advances in Neural Information Processing Systems, 35:3453234545, 2022. 2 [40] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):19791993, 2018. 1 [41] Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, and Vishal Patel. Unite and conquer: Plug & play multi-modal synthesis using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60706079, 2023. 2 [42] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 1 [43] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 6, 7 [44] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. 2, 6, 7, [45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 6 [46] Ahmed Roshdy, Abdullah Karar, Samer Al Kork, Taha Beyrouthy, and Amine Nait-ali. Advancements in eeg emotion recognition: Leveraging multi-modal database integration. Applied Sciences, 14(6):2487, 2024. 1 [47] Subham Sekhar Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. arXiv preprint arXiv:2406.07524, 2024. 5 [48] Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in Neural Information Processing Systems, 37:103131103167, 2025. 5 10 [49] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili Yu. Llamafusion: Adapting pretrained language models for multimodal generation. arXiv preprint arXiv:2412.15188, 2024. 1 [50] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using In International confernonequilibrium thermodynamics. ence on machine learning, pages 22562265. pmlr, 2015. 2 [51] Srinivasan, Raja, Jehan, Murugan, Srinivasan, and Iot-enabled facial recognition for smart Muthulekshmi. hospitality for contactless guest services and identity verification. In 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO), pages 16. IEEE, 2024. [52] Haomiao Sun, Mingjie He, Tianheng Lian, Hu Han, and Shiguang Shan. Face-mllm: large face perception model. arXiv preprint arXiv:2410.20717, 2024. 2, 1 [53] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 2, 7 [54] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 1 [55] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2 [56] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. 1 [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [58] Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu, Shouhong Ding, and Aimin Zhou. Rethinking the learning paradigm for dynamic facial expression recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1795817968, 2023. 1 [59] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2, 7, 8 [60] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 2, 1 [61] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models. arXiv preprint arXiv:2207.00050, 2022. 2 Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 1 [63] Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. 1 [64] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. In Forty-first International Conference on Machine Learning, 2024. 1 [65] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 7, 1 [66] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image generation and manipulation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. [67] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. 1, 2, 4, 5, 6, 7, 8, 3 [68] Bohao Xing, Zitong Yu, Xin Liu, Kaishen Yuan, Qilang Ye, Weicheng Xie, Huanjing Yue, Jingyu Yang, and Heikki Kalviainen. Emo-llama: Enhancing facial emotion understanding with instruction tuning. arXiv preprint arXiv:2408.11424, 2024. 2, 1 [69] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 77547765, 2023. 1 [70] Zhipei Xu, Xuanyu Zhang, Runyi Li, Zecheng Tang, Qing Huang, and Jian Zhang. Fakeshield: Explainable image forgery detection and localization via multi-modal large language models. arXiv preprint arXiv:2410.02761, 2024. 2 [71] Yichao Yan, Zanwei Zhou, Zi Wang, Jingnan Gao, and Xiaokang Yang. Dialoguenerf: Towards realistic avatar faceto-face conversation video generation. Visual Intelligence, 2 (1):24, 2024. 2 [72] Qu Yang, Mang Ye, and Bo Du. Emollm: Multimodal emotional understanding meets large language models. arXiv preprint arXiv:2406.16442, 2024. [73] Lijun Yu, Jose Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu, et al. Language model beats diffusiontokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. 6, 1 [74] Fengda Zhang, Qianpei He, Kun Kuang, Jiashuo Liu, Long Chen, Chao Wu, Jun Xiao, and Hanwang Zhang. Distributionally generative augmentation for fair facial attribute classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22797 22808, 2024. 1 [62] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, [75] Zengqun Zhao, Yu Cao, Shaogang Gong, and Ioannis Patras. Enhancing zero-shot facial expression recognition by 11 llm knowledge transfer. arXiv preprint arXiv:2405.19100, 2024. [76] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. General facial representation In Proceedings of learning in visual-linguistic manner. the IEEE/CVF conference on computer vision and pattern recognition, pages 1869718709, 2022. 2 [77] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. 1, 4 [78] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebVHQ: large-scale video facial attributes dataset. In ECCV, 2022. 2, 3 12 UniF2ace: Fine-grained Face Understanding and Generation with Unified Multimodal Models"
        },
        {
            "title": "Supplementary Material",
            "content": "1. Appendix A: Related Works Unified Multimodal Models. Recent works [6, 26, 36, 62, 63] in image understanding and generation have primarily focused on unified multimodal models (UMMs). Early approaches [25, 64] often integrated external decoders of diffusion models (DMs) with text autoregressive models (ARMs). Inspired by next-token prediction tasks, they proposed using single Transformer [57] model to unify understanding and generation [65]. For instance, Janus-Pro [6] decouples the visual encoder into specialized tokenizers for separate handling of understanding and generation tasks. Chameleon [54] and Emu3 [62] employ an ARM to simultaneously manage both tasks, highlighting the advantages of autoregressive models in multitask settings. Additionally, Transfusion [77] and Show-o [67] combine text ARM with visual DM, enabling seamless integration of image understanding and generation. These studies have advanced the fusion of visual and text generation models, enhancing performance on multimodal tasks. However, despite the proliferation of UMMs, their application has largely been limited to generic domain tasks, with limited exploration in fine-grained visual analysis, particularly in the face domain. Unlike previous UMMs that simply combine ARMs and DMs, we pioneer sparse UMMs by introducing both token-level and sequence-level Mixture of Experts (MoEs), significantly improving model performance. Face Multimodal Models. Face multimodal models are primarily categorized into two types: face understanding models and face generation models. For understanding, early models were task-specific and lacked multimodality [22, 40, 58, 74]. Recent works [8, 52, 68, 75] leverage the reasoning capabilities of LLMs or MLLMs, often using MLLM-generated face Q&A data to fine-tune or post-train foundation models, incorporating face domain knowledge. For example, EMO-LLaMA [68] introduces facial experts to extract facial features, which are aggregated with handcrafted prompts and fed into LLaMA [56], enabling it to answer facial-related queries. For generation, recent works [11, 16, 20, 60] focus on using diffusion models to personalize face images by conditioning on textual and visual information, such as semantic masks, but avoid directly capturing fine-grained face attributes from text prompts. Despite these advances in understanding and generation separately, developing unified multimodal models (UMMs) remains significant research challenge. Addressing this gap can enhance cross-modal capabilities and advance progress toward Artificial General Intelligence (AGI). 2. Appendix B: Implementations Details Our UMM backbone is based on Show-o [67]. UniF2ace utilizes discrete image tokens as input, represented by the pre-trained MAGVIT-v2 [73]. For token-level MoE, each group (generation and understanding tasks) includes one shared expert and eight routed experts, selected via top-2 strategy. The expert structure is single-layer MLP with the gating mechanism [10]. In sequence-level MoE, the generation group employs two copy experts, one zero expert, and one noise expert. Noise embedding is implemented using sinusoidal embedding, following [42]. The noise resampler uses 4-layer Multi-Head Attention mechanism to map noise embeddings to the UniF2ace hidden space. For the understanding group, there are two copy experts, one CLIP expert, and one face expert. We use CLIP-ViT for image embedding and AntelopeV2 for face embedding, with the resampler configuration matching that of the noise expert. Moreover, training is divided into two stages: Stage uses only captions for generation and understanding tasks, while Stage II incorporates VQAs into the understanding task. This pipeline transitions the model from general image feature understanding to fine-grained feature capture. Both stages are trained on 8 NVIDIA A100 (80GB) GPUs, optimized using AdamW with weight decay of 0.01, 5K warm-up steps, and an initial learning rate of 5e-5 with cosine scheduling. The total batch size is 600 for Stage and 480 for Stage II, with 20K steps for Stage and 40K steps for Stage II. In the inference process of UniF2ace, following the computation method in [27], we compute the maximum and minimum activation parameters for UniF2ace under the Top-2 strategy due to the different number of parameters included between different experts in the sequence-level MoE. The total number of parameters for UniF2ace is 1.84B, the maximum activation parameter is about 1.63B, and the minimum activation parameter is about 1.42B. The average number of activation parameters tested in the UniF2ace-130K test dataset is 1.47B. 3. Appendix C: Absorbing-state Case with Independence between Tokens. The absorbing-state case means that for any single token with possible values in = {1, . . . , }, the transition 1 matrix is Qabsorb = 1 0 0 1 ... ... 0 0 0 0 1 0 1 0 ... ... . . . 1 1 0 0 . The reverse transition rate matrix of the reverse process from state xt to state ˆxt is (cid:40) pt(ˆxt) pt(xt) Qt (ˆxt, xt) , (cid:80) Qt (xt, ˆxt) = ˆxt = xt ˆxt = xt Qt (xt, k) , . k=xt As Qt (ˆxt, xt) is known, it is sufficient to estimate the concrete score pt(ˆxt) pt(xt) by score network sθ (xt, t) (cid:104) pt(ˆxt) . Score based discrete diffusion model is pt(xt) (cid:105) an effective objective to train the score network [34, 39]. Specifically, the score function in multidimensional discrete space is ˆxtX sθ (xt, t)ˆxt = sθ pt pt (cid:0)x1 (cid:0)x1 (cid:0)x1 . . . xi . . . (cid:98)xi . . . xi . . . xd . . . xd . . . xd t , t(cid:1) (cid:2)i, (cid:98)xi (cid:1) (cid:1) , and accordingly, (cid:0)x1 Qt . . . xi (cid:0) t, xi (cid:98)xi Qt . . . (cid:98)xi . . . xi 4. Appendix D: Relationship Between Score . . . xd . . . xd . . . xd (cid:1) sθ (cid:1) , t(cid:1) (cid:2)i, (cid:98)xi , x1 (cid:0)x1 (cid:3) . Loss and Masked Generative Loss. To prove that our score loss (loss (1)) provides tighter upper bound, we first introduce these two losses. (1) L1 = Lscore (x0) + DKL (cid:1), where Lscore (x0) is the diffusion weighted denoising score entropy for data point x0, and sθ = pθ(x0xt) p(xtx0) (cid:0)pT 0 ( x0) pbase Lscore (x0) = (cid:90) Extpt0(x0) (cid:88) y=xt Qt (xt, y) (cid:16) sθ (xt, t)y C2 = Eq(x0:T ) (cid:34) (cid:88) (cid:35) log (xt1 xt) t=1 (cid:34) (cid:88) Eq(x0:T ) (cid:88) (x0 xt1) log (x0 xt) (cid:35) t= x0 = Eq(x0:T ) (cid:34) (cid:88) t=1 log (xt, xt1) (cid:35) log (xt) (cid:88) t=1 (cid:88) Eq(x0:T )q(x0xt1) [log (x0 xt)] . t=1 (cid:104) log (x0) (cid:80)T C1 + C2 = Eq(x0:T ) From the derivation of [34, 67], both losses are expanded in the classic way of likelihood function (as equation (a)) by Jensen inequality. t=1 log (x0 xt) (cid:105) . (cid:3) (cid:26) = Eq(x0) log Eq(x1:T x0) log pθ 0 (x0) (cid:20) = Eq(x0) log (cid:90) (a) Eq(x0)q(x1:T x0) pθ (x0, x1 xT ) dx1 xT (cid:20) pθ (x0:T 1 xT ) (x1:T x0) pθ (x0:T 1 xT ) (x1:T x0) log (cid:20) (cid:21) (cid:21)(cid:27) (xT ) (cid:21) + log (xT ) (16) 0 (x0), and (cid:105) , Let be the posterior probability = log pθ = Eq(x0)q(x1:T x0) then K. log pθ(x0:T 1xT ) (cid:104) q(x1:T x0) + log (xT ) Analysis of L1. Inspired by [50], we then illustrate that = L1. Assume that is the forward transition, and is the transition of reverse process, (xT ) (xT ). As [50] states, the term is equal to the following formula strictly, (cid:88) (cid:90) = dx0dxtq (x0, xt) t=2 KL (q (xt1 xt, x0) (xt1 xt)) + Hq (xT x0) Hq (x1 x0) Hp (xT ) Hq (xT x0) Hp (xT ) = = (cid:90) (cid:90) xT x0 (cid:90) (cid:90) xT (cid:90) x0 (cid:90) xT x0 (xT x0) (x0) log (xT x0) dx0dxT (xT x0) (x0) dx0 log (xT ) dxT (xT x0) (x0) log (xT x0) (xT ) dx0dxT = Epdata(x0) [KL (q (xT x0) (xT )] (17) pt0 (y x0) pt0 (xt x0) (2) L2 = (cid:80)T log sθ (xt, t)y + (cid:18) pt0 (y x0) pt0 (xt x0) (cid:19)(cid:19) dt . Since Eq(x0)q(xtx0) [log pθ (x0 xt)] , where where = C1 + C2. The constants C1 and C2 are [67]: t=1 C1 = Eq(x0:T )[ (cid:88) t=1 log (xt xt1) + (cid:34) = Eq(x0:T ) (cid:88) t=1 log (xt, xt1) + log (xT ) (cid:125) (cid:123)(cid:122) (cid:124) Note that p(xT )=q(xT ) ] (cid:88) t=0 (cid:35) log (xt) , (15) 2 In addition, Hq (x1 x0) = Epdata Eq(x1x0) (cid:2)log p01 (x0 x1)(cid:3) (cid:88) t=1 Eq(x0)q(xtx0) [log pθ (x0 xt)] (C1 + C2) . is"
        },
        {
            "title": "Then",
            "content": "equivalent (cid:2)DKL (cid:0)pT 0 ( x0) π(cid:1), ExT pT 0(x0) DKL (cid:0)Lscore + DKL This is to say that = L1 strictly. to = (cid:0)pT 0 ( x0) π(cid:1)(cid:1), which is our loss. (cid:0)Px0 ( xT ) Pθ ( xT )(cid:1)(cid:3) say, or to Analysis of L2. The proof of L2 performs second scaling based on (a) (still using Jensens inequality), see (b). = Eq(x0)q(x1:T x0) (cid:20) log pθ (x0:T 1 xT ) (x1:T x0) (cid:0)pT 0 ( x0)(cid:1). Analysis of C1 + C2 + DKL (cid:0)pT 0 ( x0)(cid:1) 0, then we accomplish If C1 + C2 + DKL the proof that our bound is smaller upper bound. Since (cid:104) (cid:105) log q(x0) (cid:80)T t=1 log q(x0xt) C1 + C2 = Eq(x0:T ) , (cid:0)pT 0 ( x0)(cid:1) = Hq(xT x0) Hq(xT ), we then we DKL aim to simplify the following expression: (cid:21) Eq(x0:T ) log q(x0) (cid:34) (cid:88) (cid:35) log q(x0xt) +(Hq(xT x0) Hq(xT )) . + log (xT ) t=1 (19) First, recall the definition of entropy and the conditional log pθ (xt1 xt) (xt xt1) + log (xT ) entropy: (xt1 xt, x0) pθ (x0 xt) Hq(x) = Eq(x)[log q(x)] = (cid:88) q(x) log q(x). (20) = Eq(x0:T ) = Eq(x0:T ) (cid:88) t1 (cid:88) t1 (cid:88) log x0 + Eq(x0:T ) log (xT ) (cid:88) t1 log (xt xt1) (cid:124) (cid:123)(cid:122) C1 = Eq(x0:T ) (cid:34) (cid:88) (cid:88) log t1 q (x0 xt1) (x0 xt) q(xtxt1)q(xt1)/q(xt) (cid:123) (cid:125)(cid:124) (cid:122) (xt1 xt) (cid:35) pθ (x0 xt) + C1 (b) Eq(x0:T ) (cid:20) (cid:88) (cid:88) (x0 xt1) t1 x0 (cid:18) (xt1 xt) (x0 xt) log (cid:19) (cid:21) pθ (x0 xt) + C1 (cid:125) Hq(xy) = Eq(x,y)[log q(xy)] = q(x, y) log q(xy). (cid:88) x,y (21) Expanding the expectation in the given expression: (cid:34) Eq(x0:T ) log q(x0) (cid:88) t= (cid:35) log q(x0xt) (22) = (cid:88) x0:T (cid:34) q(x0:T ) log q(x0) (cid:88) t=1 (cid:35) log q(x0xt) . (23) Rewriting the terms: (cid:88) x0:T q(x0:T ) log q(x0) = (cid:88) x0 q(x0) log q(x0) = Hq(x0), (24) = (cid:88) t1 Eq(xt,x0) [log pθ (x0 xt)] + C1 + C2 = L2 . The intermediate derivation process is from [67]. Therefore, (a) = L1 (b) L2 i.e. (Lscore (x0) + DKL (cid:0)pT 0 ( x0) pbase (cid:1)) (18) Eq(xt,x0) [log pθ (x0 xt)] + C1 + C2 . (cid:88) t1 Then Lscore (x0) + DKL (cid:0)pT 0 ( x0) pbase (cid:1) (cid:88) x0:T (cid:88) = = q(x0:T ) (cid:88) t=1 log q(x0xt) (cid:88) q(xt) (cid:88) x0 q(x0xt) log q(x0xt) (25) t=1 xt (cid:88) t=1 Eq(xt)[Hq(x0xt)]. Thus, the first term simplifies to: Hq(x0) (cid:88) t=1 Eq(xt)[Hq(x0xt)]. (26) 3 Combining these terms, 3. Project the final latent to the output space: Hq(x0) (cid:88) t=1 Eq(xt)[Hq(x0xt)] + (Hq(xT x0) Hq(xT )) = LayerNorm(MT Wout) RLD This enables adaptive fusion of input vector into sequence features through learned latent queries. =Hq(x0) Hq(xT ) (cid:88) Eq(xt)[Hq(x0xt)] + Hq(xT x0). t=1 (27) Using the inequality that the expectation of conditional entropy satisfies: (cid:88) t=1 Eq(xt)[Hq(x0xt)] Hq(x0), (28) Since we only consider absorbing process, xT is fully masked at time , thus H(xT ) = 0. We then conclude that the entire expression is non-negative: Hq(x0) (cid:88) t=1 Eq(xt)[Hq(x0xt)] 0 , (29) and the above formula equal to 0 holds true when for any t, xt and x0 are independent. Therefore, we have: Lscore (x0) (cid:88) t=1 Eq(x0)q(xtx0) [log pθ (x0 xt)] . 5. Appendix E: Implementation of the Resampler We define resampler : Rh RLD , where is the length of the input vector, is the length of the sequence and is the hidden dimension of UniF2ace. Specifically, we define learnable hidden latent matrix: M0 RLd, M0 = LearnableParameter where is the hidden dimension of the resampler. process involves: Its 1. Project the noise embedding Rh via = xWin R1d 2. Iteratively refine the latent matrix through layers, sucn as the l-th layer: = Ml1 + MHA (Ml1, Concat(H, Ml1)) Ml = + FFN(M l) where MHA denotes the Multi-Head Attention mechanism, FFN denotoes the Feed-Forward Network. In MHA, the query, key, and value are denoted as: Ql = Ml1W (l) Kl = [H; Ml1]W (l) Vl = [H; Ml1]W (l) 4 Figure 1. More comparison of generated face images with other models. 5 Figure 2. More face images generated by UniF2ace 6 Figure 3. Comparison of captioning results and DeepSeeek-v3-based scores. We highlight fine-grained attributes with blue and errors in answers with red. 7 Figure 4. Prompts for building dataset. The first and second prompts are to GPT-4o, while the last prompt is to GPT-4. In the first prompt, the content in [] is used only when the image data includes built-in captions, such as in MM-CelebA-HQ dataset."
        }
    ],
    "affiliations": [
        "Central South University",
        "Computer Center, Peking University",
        "Fudan University",
        "Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)",
        "Institute of Automation, Chinese Academy of Sciences",
        "School of Computer Science, Peking University",
        "Yau Mathematical Sciences Center and Department of Mathematical Sciences, Tsinghua University"
    ]
}