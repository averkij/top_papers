{
    "paper_title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation",
    "authors": [
        "Armel Zebaze",
        "Benoît Sagot",
        "Rachel Bawden"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLMs have been shown to perform well in machine translation (MT) with the use of in-context learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. A common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present \\textsc{TopXGen}, an LLM-based approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into a high-resource source language. We show that \\textsc{TopXGen} boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ArmelRandy/topxgen."
        },
        {
            "title": "Start",
            "content": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation Armel Zebaze Benoît Sagot Rachel Bawden Inria, Paris, France firstname.lastname@inria.fr 5 2 0 2 2 1 ] . [ 1 0 8 6 8 0 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "LLMs have been shown to perform well in machine translation (MT) with the use of incontext learning (ICL), rivaling supervised models when translating into high-resource languages (HRLs). However, they lag behind when translating into low-resource language (LRLs). Example selection via similarity search and supervised fine-tuning help. However the improvements they give are limited by the size, quality and diversity of existing parallel datasets. common technique in low-resource MT is synthetic parallel data creation, the most frequent of which is backtranslation, whereby existing target-side texts are automatically translated into the source language. However, this assumes the existence of good quality and relevant target-side texts, which are not readily available for many LRLs. In this paper, we present TOPXGEN, an LLMbased approach for the generation of high quality and topic-diverse data in multiple LRLs, which can then be backtranslated to produce useful and diverse parallel texts for ICL and fine-tuning. Our intuition is that while LLMs struggle to translate into LRLs, their ability to translate well into HRLs and their multilinguality enable them to generate good quality, natural-sounding target-side texts, which can be translated well into high-resource source language. We show that TOPXGEN boosts LLM translation performance during fine-tuning and in-context learning. Code and outputs are available at https://github.com/ ArmelRandy/topxgen."
        },
        {
            "title": "Introduction",
            "content": "The performance of Machine Translation (MT) models has considerably evolved through the years, with new models increasingly supporting more languages (Bapna et al., 2022; Costa-jussà et al., 2022; Yang et al., 2023). However, performance remains unequal across languages, which themselves vary Figure 1: Overview of TOPXGEN. An LLM is used to write diverse set of paragraphs in an LRL guided by topics, example sentences in the LRL and example paragraphs in HRLs. The generated paragraphs are later cleaned and divided into sentences that are backtranslated into the source language to create sentencelevel parallel dataset. greatly in terms of available resources and representation in NLP research (Joshi et al., 2020). MT Models perform much better in high-resource languages (HRLs), such as English, French and German, compared to low-resource languages (LRLs) such as Hausa, which lack large quantities of highquality parallel data. Decoder-based LLMs can perform MT without relying on parallel data (in zero-shot fashion). However they lag behind supervised MT baselines when translating into LRLs (Hendy et al., 2023). In-Context Learning (ICL; Brown et al., 2020), which involves using few high-quality demonstrations has been shown to improve performance, especially when they are similar to the sentence to be translated (Moslem et al., 2023; Zebaze et al., 2025b), highlighting the importance of parallel datasets even in this setting. common approach is to synthesize parallel data using forward translation or back-translation (Schwenk, 2008; Bojar and Tamchyna, 2011; Sennrich et al., 2016). Forward translation comes with issues such as low translation fidelity and the neglect of cultural nuances in the target language. Back-Translation (BT; Sennrich et al., 2016) typically relies on good quality monolingual corpus in the LRL, which can be difficult to obtain. In this work, we explore the construction of synthetic datasets for MT into LRLs by automatically generating text in the LRL target language and then backtranslating into HRL source language. Marie and Fujita (2021) first proposed an approach with similar aim, using an LLM to generate in-domain monolingual data, which they then used to perform BT alongside parallel data. They generate their monolingual data using an LLM fine-tuned on each of their domain-specific datasets. However, their approach focuses on English and requires domainspecific fine-tuning (and datasets) in addition to relying on parallel data to perform BT, making it impractical for translation into LRLs. To address this, we introduce TOPXGEN (Figure 1), pipeline that exploits LLMs multilinguality and instruction-following capabilities. Unlike prior work, we generate monolingual data (sentences) beyond English to cover numerous LRLs. Instead of domain-specific fine-tuning, we directly prompt an LLM and guide its generations towards predefined list of topics in order to encourage diversity in the outputs. The quality of sentences stems from the ability of state-of-the-art multilingual LLMs to produce coherent text in LRLs (Enis and Hopkins, 2024)even if they struggle to translate accurately into them. We then backtranslate the generated sentence into HRL (English in this work) using reverse translation model (a supervised MT system or the same LLM). This target-aware generation helps mitigate the cultural loss often observed in LRLs with standard forward translation techniques. Moreover, translating into HRL offers practical advantage, as HRLs are generally easier to translate into with high fidelity. To evaluate TOPXGEN, we generate synthetic parallel data between English and ten low-resource languagesBasque, Hausa, Igbo, Kinyarwanda, Nepali, Somali, Sundanese, Swahili, Urdu, and Xhosausing Gemma-3-27B-It as the generator and NLLB-200-3.3B as the back-translator. We then assess both in-context learning and finetuning setups across small translation models. Our experiments show that TOPXGEN consistently outperforms other data generation methods and achieves performance comparable to humantranslated datasets."
        },
        {
            "title": "2 Related Work",
            "content": "Low-resource Machine Translation with LLMs. LLMs encounter many languages during their training but in various proportions (Abadji et al., 2022; Penedo et al., 2024). Through ICL (Brown et al., 2020), they can perform wide variety of tasks including MT. Decoder-based LMs are on par with supervised MT models such as M2M100 (Fan et al., 2021) and NLLB (Goyal et al., 2022)when translating between high-resource languages but still lag behind when translating into low-resource languages (Hendy et al., 2023; Zhu et al., 2024). Many works have emerged to bridge the gap at inference time by either using similarity-based incontext example selection (Moslem et al., 2023; Tanzer et al., 2024; Zebaze et al., 2025b) or more advanced prompting strategies (He et al., 2024; Briakou et al., 2024; Zebaze et al., 2025a). Another line of work involves fine-tuning LLMs. While most works focus on midto high-resource languages (Xu et al., 2024b,c, 2025), few of them explore fine-tuning on LRLs. They generally use bitexts mined from the internet (Schwenk et al., 2021a; El-Kishky et al., 2020; Schwenk et al., 2021b), out-of-domain benchmarks written by natives in low-resource languages (Muennighoff et al., 2023; Üstün et al., 2024; Uemura et al., 2024) or continual pretraining on monolingual data covering multiple LRLs to improve few-shot MT performance (Buzaaba et al., 2025). The scarcity of high-quality parallel data is the major bottleneck for low-resource MT but some studies have explored methods to generate such data using LLMs. Parallel Data Generation. Generating data using LLMs has emerged as popular alternative to costly human annotation, primarily for instruction datasets. One of the first approaches to demonstrate the effectiveness of this paradigm is SELFINSTRUCT (Wang et al., 2023; Taori et al., 2023). It consists in bootstrapping small set of seed instructions into bigger collection with the help of ICL. Building on this, Kou et al. (2024) proposed KNN-INSTRUCT which replaces the random selection of ICL demonstrations with nearest neighbor retrieval in an embedding space. Subsequent advances introduced multilingual instruction generation (Cui et al., 2024; Wei et al., 2023), generating increasingly complex and diverse instructions across domains (Xu et al., 2024a; Zeng et al., 2024; Chaudhary, 2023; Luo et al., 2025, 2024) and step-by-step explanations within responses (Mukherjee et al., 2023; Gunasekar et al., 2023; Li et al., 2023b). Other approaches leverage unlabeled human-written corpora as sources for generating instruction responses (Wei et al., 2024b,a; Li et al., 2024; Ben Allal et al., 2024). For MT and LRLs, many works generate datasets by simply machine translating existing ones in the languages of interest (The Aya Collection; Singh et al., 2024, the XLLMs-100 collection Lai et al., 2024 and Bactrian-X; Li et al., 2023a). One of the most common and early strategies was to backtranslate monolingual target side data into the source language to create synthetic parallel data (Schwenk, 2008; Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011; Sennrich et al., 2016; Caswell et al., 2019; Burlot and Yvon, 2018; Bogoychev and Sennrich, 2020; Marie et al., 2020). However, BTs reliance on high-quality monolingual data on the target side presents challenge when such data is hard to obtain. To address this, Marie and Fujita (2021) proposed using LLMs to synthesize monolingual data by fine-tuning GPT-2 (Radford et al., 2019) on multiple domains and generating multidomain English data. Combined with substantial amounts of parallel data, they apply BT to train model to translate from target language into English. We propose to also generate synthetic parallel datasets, but for low-resource MT instead of HRL translation for multi-domain adaptation, the challenge being to be able to generate high quality data in LRL. Instead of fine-tuning an LLM, our approach uses more recent strategies involving LLM prompting to generate high quality and diverse data, which we then use to perform ICL and fine-tuning for translation into LRLs."
        },
        {
            "title": "3 Methodology",
            "content": "We propose TOPXGEN (Figure 1), topic-guided and target-language centric method for automatically generating parallel sentence-level datasets between English and LRLs, with the end goal of improving MT into LRLs via ICL or supervised finetuning using the synthetic examples. It consists of two steps: data generation and back-translation. Data generation. We generate data in the LRL of interest by prompting multilingual LLM. We aim to produce data that is structurally and lexically diverse by generating it at the paragraph level data (which we then split into sentences before backtranslation) and by guiding the generation with predefined topics. To generate new paragraph in given target LRL, we prompt the LLM with: Topics: To foster diversity in the output texts. We rely on the generators instructionfollowing abilities and prompt it to generate content on randomly selected topic drawn from predefined list of Wikipedia topics (Ziadé, 2023) following (Li et al., 2023b).1 Seed paragraphs: To have the generator understand what we expect in terms of length and format. We use the 240 paragraphs from XQuAD (Artetxe et al., 2020), which are written in 11 HRLs and perform cross-lingual ICL (X-ICL; Cahyawijaya et al., 2024). Seed sentences. These serve to illustrate how sentence is structured in the LRL and to help ensure the outputs are generated in the correct script. We use the FLORES-200 dev set (Goyal et al., 2022; Costa-jussà et al., 2022). Note that during generation, we automatically remove generations that overlap too much with previous ones with respect to the ROUGE2 score (Lin, 2004) following (Wang et al., 2023). Given the collection of paragraphs produced by the generator, we build collection of sentences by applying sentence-splitter.3 We then perform language identification with fastText (Bojanowski et al., 2017; Costa-jussà et al., 2022) on each sentence and remove incorrectly labeled ones. Back-Translation. We then use multilingual back-translator (e.g., NLLB-200-3.3B; Costa-jussà et al., 2022) to translate the generated sentences into the HRL we want to learn to translate from. Translating into the HRL is likely to be of good 1In contrast to Marie and Fujita (2021), who focus on broad domains like IT or Health, we use fine-grained topics (e.g., specific personalities or events). 2Version: 0.1.2Pure python implementation of ROUGE-1.5.5 3https://github.com/mediacloud/sentence-splitter"
        },
        {
            "title": "Xhosa",
            "content": "Paragraphs Sentences Sentences (After decon.) 16,829 120,031 120,031 14,981 101,488 101,466 18,518 133,071 133,063 8,900 57,884 57,884 14,490 143,014 142, 14,623 96,315 96,315 10,483 78,264 78,257 11,489 86,981 86,981 13,923 131,133 131,118 15,781 104,992 104,979 Table 1: Statistics of the TOPXGEN dataset in terms of paragraphs and sentences. quality given that MT models perform better in this direction than the reverse one."
        },
        {
            "title": "3.1 The TOPXGEN dataset",
            "content": "We generate multiple paragraphs in 10 languages: Basque, Hausa, Igbo, Kinyarwanda, Nepali, Somali, Sundanese, Swahili, Urdu and Xhosa. Following Gunasekar et al. (2023) and Ben Allal et al. (2024), to avoid data contamination, we filtered the generated paragraphs to remove those containing significant overlap (at least one 10-gram overlap) with FLORES (also NTREX 128 and TICO-19, whose results are given in Appendix B.4). We apply the same strategy to the English translations of the sentences to make sure that they do not resemble the seed paragraphs that we used (i.e. XQuAD; Artetxe et al., 2020). In Table 1, we report the number of paragraphs generated per language and the number of sentences before and after decontamination. Each language has between 50k and 150k sentences for total of 1.05M sentences. We conduct further analysis in Appendices B.6, B.7 and B.8."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We work on translation from English to 10 LRLs: Basque, Hausa, Igbo, Kinyarwanda, Nepali, Somali, Sundanese, Swahili, Urdu and Xhosa. Datasets. FLORES-200 (Goyal et al., 2022; Costa-jussà et al., 2022) is dataset consisting of translations from web articles into 204 languages. These sentences are divided into two splits: devtest and dev. We use the devtest set (1012 examples) as the evaluation set and the dev set (997 examples) as the selection pool for few-shot MT. Models. We use Gemma-3-27b-It (Team et al., 2025) as the generator, and NLLB-200-3.3B (Costa-jussà et al., 2022) as the back-translator in order to reduce the computational cost, as translating in few-shot setting with Gemma-3-27b-It would be significantly more expensive. We use LLaMA-2-7B (Touvron et al., 2023) and the LLaMA-3-8B (Dubey et al., 2024) during fineresulting models tuning and compare against including strong multilingual LLMs LLaMA-3.1-8B-It & LLaMA-3.1-70B It (Dubey et al., 2024), Gemma-2-9B-It & Gemma-2-27B-It (Gemma Team et al., 2024), Aya-expanse-8B & Aya-expanse-32B (Dang et al., 2024), Qwen-2.5-7B-It & Qwen-2.5-32B-It (Yang et al., 2024; Team, 2024) and Command-R7B (Cohere et al., 2025). Evaluation Metrics. We mainly evaluate using MetricX-24 (Juraska et al., 2024). We use the reference-based version MetricX-24-XXL (which supports the same 101 languages as mT5 (Xue et al., 2021)). MetricX assigns score ranging from 0 to 25, with higher scores indicating more errors in the translation. We also use n-gram matching metrics via sacreBLEU (Post, 2018), namely BLEU4 (Papineni et al., 2002) and chrF++5 (Popovic, 2015; Popovic, 2017) for transparency reasons in Appendix B.3. Implementation Details. The generators temperature of generation is set to 1.0. We backtranslate with beam search (beam size = 5). For the topics, we use list of 67,573 Wikipedia topics curated by Ziadé (2023). We fine-tune unidirectional models for 5k steps (about 3 hours on 1 H100 80G) with learning rate of 1e-5, batch size of 4 with 4 gradient accumulation steps and maximum sequence length equal to 512. The multidirectional model requires 100k steps (about 30 hours on 1 H100 80G) and in both cases we choose the last checkpoint as the final model. For the statistical significant comparison we follow (Koehn, 2004) and use paired bootstrap resampling with 300 samples of 500 sentences and p-value threshold of 0.05. All models are evaluated in zero-shot fashion with greedy decoding unless stated otherwise. See Appendix A.1 and A.2 for additional details. 4nrefs:1case:mixedeff:notok:flores200smooth:expversion:2.4.2 5nrefs:1case:mixedeff:yesnc:6nw:2space:noversion:2.4.2 Models NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It Command-R7B Aya-expanse-32B Qwen-2.5-32B-It LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Models Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 24.96 27.17 23.33 26.06 16.69 3.16 8.35 7.27 12.14 3.42 18.15 13.00 14.93 23.70 25.64 21.77 24. 6.11 4.81 6.21 5.15 9.33 13.34 17.06 18.99 10.57 23.02 8.48 14.76 12.55 6.25 5.27 6.95 5.68 28.82 19. 17.54 19.01 13.76 1.88 4.73 4.26 17.50 1.71 12.28 13.11 13.77 19.65 20.52 18.22 19.36 Toplines 2.46 4. 19.92 15.37 4.89 7.56 23.35 12.61 4.58 7.92 Baselines 11.71 15.59 9.06 2.12 4.51 5. 13.57 2.03 8.32 10.89 8.37 14.65 21.46 21.35 20.03 9.34 23.08 16.15 Our Models 12.30 12.90 16.28 17.02 15.54 16.09 10.97 10.04 7.31 6.54 7.96 7.13 5.54 5.90 7.13 20.37 17.41 16.68 6.33 22.16 10.24 8.57 8.42 5.20 5.07 6.05 5. 7.00 8.25 4.33 2.19 3.51 2.99 4.06 2.58 4.47 7.30 7.61 11.76 13.60 9.76 11.62 14.83 13.45 19.99 22.60 21.18 22.84 19. 13.92 21.11 15.87 14.73 9.91 8.51 12.96 11.40 26.64 25.74 22.63 25.33 18.91 5.37 9.96 9.86 21.20 3.22 17. 15.11 16.08 21.88 23.24 20.84 22.37 6.60 3.47 4.00 4.31 4.80 9.12 7.91 9.26 5.31 15.40 6.71 7.98 6.31 4.21 3.77 4.52 3. Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It Command-R7B Aya-expanse-32B Qwen-2.5-32B-It LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 17.40 13.58 8.93 9.74 6.32 1.85 5.88 4.11 11. 2.05 4.74 8.89 8.20 13.20 13.70 12.03 12.71 4.82 5.50 10.77 10.52 14.67 20.20 15.13 19.35 7.41 21.97 18. 11.73 10.63 7.00 6.17 7.94 7.05 Toplines 4.97 4.77 36.20 35.16 4.59 3.80 Baselines 7.18 5.13 8.48 5.53 10.84 15.01 8.84 16.65 8.90 7.19 5.75 4.88 4.26 5.66 4.83 35.99 34.26 29.63 6.51 9.37 9.46 26. 2.85 22.61 3.78 4.46 5.23 18.85 17.25 17.44 6.63 22.86 8.63 Our Models 19.19 20.96 30.99 33.49 28.53 31. 11.42 9.52 5.23 4.51 6.03 5.07 21.44 17.01 14.60 17.34 12.54 8.41 9.81 8.12 11.63 6.43 14.17 14.70 15.42 16.84 18.16 16.24 17. 28.41 26.24 23.28 27.37 19.26 3.75 11.14 11.72 19.94 2.65 17.17 14.89 16.33 22.43 23.51 21.65 22.92 4.44 3. 4.01 3.64 5.25 13.14 7.72 9.31 5.72 19.10 6.47 8.43 6.63 4.16 3.78 4.54 3.96 23.24 12.82 9.27 6.67 6.99 2.33 4.84 4. 11.01 2.34 3.22 9.25 7.67 12.39 13.93 11.74 13.15 3.80 7.62 12.25 16.01 18.32 22.77 22.00 22.32 10. 23.42 22.53 15.24 13.09 9.33 7.77 10.56 8.61 Table 2: BLEU and MetricX scores for 10 English directions from FLORES 200. Best results after fine-tuning are highlighted in bold."
        },
        {
            "title": "5.1 Fine-tuning",
            "content": "We fine-tune two small models (LLaMA-2-7B and LLaMA-3-8B) on our constructed dataset and compare them to state-of-the-art models of various sizes in zero-shot.6 We evaluate two setups: unidirectional setting with one model per translation direction (EnglishX), and multidirectional setting with single model trained on all 10 directions. Results are shown in Table 27. Fine-tuning LLaMA-2-7B, whose performance is close to ran6We use 5-shot with BM25 selection in the FLORES dev set for base models as they cannot follows instructions. 7See Appendix B.2 for MT into English. dom in all directions turns it into model that outperforms Aya-expanse-32B, Qwen-2.5-32B and Command-R7B. Unidirectional fine-tuning of LLaMA-3-8B outperforms Gemma-2-27B-It and LLaMA-3.1-70B-It. With beam search (Freitag and Al-Onaizan, 2017), we get even better results with unidirectional models, closing the gap with the generator. Fine-tuning model to support the ten languages together leads to drop of performance of about 1 BLEU in all languages, i.e. there is no positive cross-lingual transfer. We provide more baseline comparisons in Appendix B.1. Additional results from fine-tuning NLLB-200-3.3B and Gemma-3-27B-PT on the TOPXGEN dataset are provided in Appendix B.5. 5.2 In-Context Learning We compare the performance obtained when doing 5-shot MT with example selection via similarity search in the FLORES dev set and in the TOPXGEN dataset with LLaMA-3.1-8B-It. As shown in Table 3, retrieval in the TOPXGEN dataset yields superior results compared to zero-shot showing that its content is qualitative enough to help the model during the translation. Moreover, it also works better than retrieval in FLORES, particularly in terms of MetricX. While TOPXGEN has its size and diversity as advantage, the FLORES dev set is in-domain with respect to the evaluation set as they come from the same research effort on top of being written by professional translators."
        },
        {
            "title": "5.3 Comparison to existing approaches",
            "content": "We investigate the impact of changing the data generation pipeline from our TOPXGEN to SELF-INSTRUCT (Wang et al., 2023) and KNNINSTRUCT (Kou et al., 2024). We keep the same parameters (Generator, Seed sentences, Backtranslator etc.) and compare the result of finetuning LLaMA-2-7B on 20K sentence pairs generated by each strategy in Sundanese and Somali. For KNN-INSTRUCT, we use the multilingual SONAR embeddings (Duquenne et al., 2023) to compute similarity between sentences and we perform 3 rounds with = 8. In Figure 2, we report the zero-shot (beam size = 5) BLEU and MetricX scores every 200 steps and compare the values across methods. TOPXGEN consistently outperforms SELF-INSTRUCT and KNN-INSTRUCT in terms of BLEU and MetricX at each checkpoint. We also compare fine-tuning on TOPXGEN to fine-tuning on high-quality professionally translated data such as SMOLSENT (Jones et al., 2023; Caswell et al., 2025) (863 parallel sentences) and the FLORES dev set (997 samples). We select the first 863 samples from each dataset and choose LLaMA-3-8B as the base model due to the limited data. We fine-tune one model for English Hausa and another for English Igbo and report the scores every 100 steps on Figure 3. As expected, TOPXGEN does not consistently work better than professional translations. However it outperforms SMOLSENT when translating in Hausa and competes with FLORES in Igbo. Moreover, TOPXGEN has the advantage of the scale, as proven in Table 2, fine-tuning on full TOPXGEN dataset (which is 100 times bigger) outperforms the best SMOLSENT Figure 2: TOPXGEN vs SELF-INSTRUCT & KNNINSTRUCT. and FLORES checkpoint by at least 3 BLEU. Figure 3: TOPXGEN vs FLORES & SMOLSENT."
        },
        {
            "title": "6 Ablation Studies",
            "content": "Impact of the generator and the number of topics. In this section, we focus on Sundanese and study 2 setups. First, we use gpt-4o-mini-2024-07-18 (OpenAI, 2024) as the generator LLM and analyze the performance of small models (LLaMA-2-7B and LLaMA-3-8B) finetuned on 55k sentences. Second, we reduce the number of topics from 67,573 to curated list of 509 elements and analyze how the fine-tuned modModels Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX LLaMA-3.1-8B-It ZERO-SHOT LLaMA-3.1-8B-It 5-SHOT FLORES LLaMA-3.1-8B-It 5-SHOT TOPXGEN 15.47 17.53 18.05 11.15 9.08 8.61 9.61 11.85 12.29 11.75 9.98 9.02 7.75 9.88 9. 17.39 14.51 13.66 3.73 5.97 5.69 20.71 19.57 19.16 12.21 17.18 17.16 7.66 6.37 6.02 Methods Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX LLaMA-3.1-8B-It ZERO-SHOT LLaMA-3.1-8B-It 5-SHOT FLORES LLaMA-3.1-8B-It 5-SHOT TOPXGEN 4.20 6.04 6. 19.63 17.12 15.39 10.30 14.06 13.28 10.71 8.56 8.14 20.44 23.47 24.14 9.21 8.69 8.22 19.24 20.75 21. 5.93 5.54 5.26 3.41 4.67 4.63 23.41 22.00 21.08 Table 3: BLEU and MetricX scores for 10 English directions from FLORES 200 with ICL. Best results are highlighted in bold. Scores that are statistically equivalent to the best are underlined. els perform for both generator(s). As shown in Table 4, stronger generator (here gpt-4o-mini) results in stronger students. Moreover, we observe that fine-tuning with more topics achieves stronger results, suggesting that diversity matters for building strong student models. The number of topics is also an important aspect when you generate data on larger scale, beyond the numbers that we explore in this paper. Methods Hausa Sundanese BLEU MetricX BLEU MetricX LLaMA-2-7B uni. NLLB LLaMA-2-7B uni. GEMMA LLaMA-3-8B uni. NLLB LLaMA-3-8B uni. GEMMA 13.77 13.14 20.52 19.58 8.42 9.31 5.07 5.64 15.42 15.19 18.16 17. 5.75 5.83 4.26 4.31 Table 5: Full quantitative results for English Hausa and English Sundanese on FLORES 200 (BLEU and MetricX scores). Best results are highlighted in bold. Scores that are statistically equivalent to the best are underlined. Models Less Topics Usual Topics BLEU MetricX BLEU MetricX Gemma 3 27B It LLaMA-2-7B uni. Gemma LLaMA-3-8B uni. Gemma gpt-4o-mini-2024-07-18 LLaMA-2-7B uni. GPT LLaMA-3-8B uni. GPT 17.01 14.03 16.97 24.17 17.65 19.79 4.77 6.04 4. 3.46 5.79 4.21 17.01 15.31 18.15 24.17 18.25 20. 4.77 5.72 4.35 3.46 5.65 4.09 Table 4: Results for English Sundanese direction on FLORES 200 (BLEU and MetricX scores) for different generators and topics. Impact of the Back-translator. Throughout the paper, we used NLLB-200-3.3B as the backtranslator. In this section, we study two setups. First, we analyze how the results change when the generator is also used as the back-translator. Back-translation is performed with greedy 5-shot with BM25 retrieval (Robertson et al., 1995; Lù, 2024) from the FLORES-200 dev set following (Zebaze et al., 2025b). We fine-tune separate models on Hausa and Sundanese, and compare their results with using NLLB-200-3.3B as the backtranslator in Table 5. Using the generator as the back-translator works almost as well as using NLLB-200-3.3B. This is direct by-product of the fluency and translation literacy of state-of-the-art LLMs in English. Second, we use the [fine-tuned] student as the back-translator (also in 5-shot) with the idea of performing iterative self-improvement (Zelikman et al., 2022; Chen et al., 2024) via iterative back-translation (He et al., 2016; Lample et al., 2018; Artetxe et al., 2018). For language l, we start by using M0 = LLaMA-2-7B to backtranslate (generated via TOPXGEN) and create X0. We create M1 by fine-tuning M0 on (Y X0) (X0 l). Similarly, we create X1 by back-translating with M1, build M2 by fine-tuning M0 on (Y X1) (X1 l) and so on. The same model performs the translation into both directions. We apply that pipeline with LLaMA-2-7B and LLaMA-3-8B on data generated by gpt-4o-mini in Sundanese  (Table 6)  . At round 0when the base student model also serves as the back-translatorwe already observe improvements in both translation directions, though they are smaller than when using NLLB as the backtranslator. After one iteration, LLaMA-2-7B shows further gains, with +3 BLEU increase and - 1 MetricX drop for English-to-Sundanese, and modest improvement in the reverse direction. However, the self-improvement process soon plateaus, largely because performance on the English side stagnates. In contrast, LLaMA-3-8B fails to significantly improve Sundanese-to-English performance at round 0, preventing the iterative process from taking off. We hypothesize that self-improvement stalls when the models English-side performance nears that of the generator on the Sundanese side. Thus, initializing the pipeline with higher-quality Sundanese data than that provided by TOPXGEN could potentially support more improvement and additional self-iteration rounds."
        },
        {
            "title": "Models",
            "content": "gpt-4o-mini LLaMA-2-7B Round 0 Round 1 Round 2 Round 3 LLaMA-3-8B Round 0 Round 1 Round 2 Round"
        },
        {
            "title": "MetricX",
            "content": "24.17 6.43 10.78 14.08 14.30 14.10 14.17 19.71 19.70 19.75 19.84 3.46 16.65 6.86 5.82 5.79 5.69 8.90 3.92 4.00 3.94 3. 39.66 16.30 21.84 22.36 22.25 21.85 33.77 35.55 35.37 35.40 35.44 2.67 8.76 6.41 6.37 6.53 6.52 3.83 3.48 3.48 3.46 3. Table 6: Results for English Sundanese direction on FLORES-200 (BLEU and MetricX scores) in 5-shot during Self-Improvement. Best results are highlighted in bold. Scores that are statistically equivalent to the best are underlined. Impact of the temperature of the generation of the generator. high temperature generally correlates with more diversity in the generations, but working with LRLs, there is risk of going off the track in terms of quality. Here we consider four temperatures and analyze the zero-shot (beam size = 5) performance on FLORES-200 every 200 steps after respectively fine-tuning LLaMA-3-8B on 50K and 40K sentence pairs generated by TOPXGEN with Gemma-3-27B-It respectively in Sundanese and Hausa. As illustrated in Figure 4, generating with temperature of = 1.0 leads to stronger model after fine-tuning, yielding up to 3 BLEU points more than = 0.0 and = 0.5, while maintaining comparable MetricX scores. Using temperature of = 1.2 does not yield improvements over = 1.0 in terms of BLEU, and results in lower MetricX scores compared to all other temperature values. Our attempts to generate content with temperature greater than 1.2 (typically 1.5, 2.0 and 5.0) resulted into garbage generations: nonsensical or invented sentences that mix morphemes, word fragments, and orthographic features from multiple scripts and languages."
        },
        {
            "title": "7 Conclusion",
            "content": "We presented TOPXGEN, scalable pipeline for constructing synthetic parallel datasets for MT into Figure 4: Impact of the temperature of the Generator. LRLs. Our method generates diverse monolingual data directly in wide range of LRLs using topicguided prompting (He et al., 2024; Aycock and Bawden, 2024), which we then back-translate into HRL, building synthetic parallel datasets. Our experiments demonstrate that models trained on data generated via TOPXGEN achieve strong performance across both unidirectional and multidirectional scenarios, approaching the generators performance while requiring only small data volumes and limited GPU resources. Furthermore, existing generation strategies such as SELF-INSTRUCT and KNN-INSTRUCT are compatible with our pipeline but fall short in terms of performance. TOPXGEN enables the creation of synthetic datasets which are comparable to professionally written text for ICL and fine-tuning, offering practical and effective solution for low-resource multilingual MT."
        },
        {
            "title": "Limitations",
            "content": "In this paper, we demonstrate that it is possible to use synthetic data to get smaller language models to improve their translation capabilities into lowresource languages. However, one limitation is the requirement of language model (generator) which understands pretty well the languages of interest. As suggested in the paper, using monolingual data scrapped from the internet can alleviate such limitation but it can be hard to find or manually build high-quality monolingual data covering wide variety of topics in some languages."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was partly funded by Rachel Bawden and Benoît Sagots chairs in the PRAIRIE institute, now PRAIRIE-PSAI, funded by the French national agency ANR, respectively as part of the Investissements davenir programme under the reference ANR-19-P3IA-0001 and as part of the France 2030 strategy under the reference ANR23-IACL-0008. This work was granted access to the HPC resources of IDRIS under the allocation 2025-AD011015933 made by GENCI."
        },
        {
            "title": "References",
            "content": "Julien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Benoît Sagot. 2022. Towards Cleaner DocumentIn ProOriented Multilingual Crawled Corpus. ceedings of the Thirteenth Language Resources and Evaluation Conference, pages 43444355, Marseille, France. European Language Resources Association. Antonios Anastasopoulos, Alessandro Cattelan, ZiYi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco Guzmán, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp Öktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020. TICO-19: the Translation Initiative for COvid-19. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Association for Computational Linguistics. Mikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018. In Unsupervised Statistical Machine Translation. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36323642, Brussels, Belgium. Association for Computational Linguistics. Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020. On the Cross-lingual Transferability of Monolingual Representations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 46234637, Online. Association for Computational Linguistics. AI at Meta. 2025. The Llama 4 herd: The beginning of new era of natively multimodal https://ai.meta.com/blog/ AI llama-4-multimodal-intelligence/. innovation. Seth Aycock and Rachel Bawden. 2024. Topic-guided Example Selection for Domain Adaptation in LLMbased Machine Translation. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 175195, St. Julians, Malta. Association for Computational Linguistics. Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, and 5 others. 2022. Building Machine Translation Systems for the Next Thousand Languages. Preprint, arXiv:2205.03983. Loïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias Müller, Santanu Pal, Matt Post, and Marcos Zampieri. 2019. Findings of the 2019 Conference on Machine Translation (WMT19). In Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1), pages 161, Florence, Italy. Association for Computational Linguistics. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. Cosmopedia. Nicola Bertoldi and Marcello Federico. 2009. Domain Adaptation for Statistical Machine Translation In Proceedings of with Monolingual Resources. the Fourth Workshop on Statistical Machine Translation, pages 182189, Athens, Greece. Association for Computational Linguistics. Nikolay Bogoychev and Rico Sennrich. 2020. Domain, Translationese and Noise in Synthetic Data for Neural Machine Translation. Preprint, arXiv:1911.03362. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching Word Vectors with Subword Information. Transactions of the Association for Computational Linguistics, 5:135146. Ondˇrej Bojar and Aleš Tamchyna. 2011. Improving Translation Model by Monolingual Data. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 330336, Edinburgh, Scotland. Association for Computational Linguistics. Eleftheria Briakou, Jiaming Luo, Colin Cherry, and Markus Freitag. 2024. Translating Step-by-Step: Decomposing the Translation Process for Improved Translation Quality of Long-Form Texts. In Proceedings of the Ninth Conference on Machine Translation, pages 13011317, Miami, Florida, USA. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language Models are In Advances in Neural InforFew-Shot Learners. mation Processing Systems, volume 33, pages 1877 1901. Curran Associates, Inc. Franck Burlot and François Yvon. 2018. Using Monolingual Data in Neural Machine Translation: SysIn Proceedings of the Third Contematic Study. ference on Machine Translation: Research Papers, pages 144155, Brussels, Belgium. Association for Computational Linguistics. Happy Buzaaba, Alexander Wettig, David Ifeoluwa Adelani, and Christiane Fellbaum. 2025. LughaLlama: Adapting Large Language Models for African Languages. Preprint, arXiv:2504.06536. Samuel Cahyawijaya, Holy Lovenia, and Pascale Fung. 2024. LLMs Are Few-Shot In-Context LowResource Language Learners. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 405433, Mexico City, Mexico. Association for Computational Linguistics. Isaac Caswell, Ciprian Chelba, and David Grangier. In Proceedings 2019. Tagged Back-Translation. of the Fourth Conference on Machine Translation (Volume 1: Research Papers), pages 5363, Florence, Italy. Association for Computational Linguistics. Isaac Caswell, Elizabeth Nielsen, Jiaming Luo, Colin Cherry, Geza Kovacs, Hadar Shemtov, Partha Talukdar, Dinesh Tewari, Baba Mamadi Diane, Koulako Moussa Doumbouya, Djibrila Diane, and Solo Farabado Cissé. 2025. SMOL: Professionally translated parallel data for 115 under-represented languages. Preprint, arXiv:2502.12301. Sahil Chaudhary. 2023. Code Alpaca: An Instructionfollowing LLaMA model for code generation. https://github.com/sahil280114/codealpaca. Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. 2024. Self-Play Fine-Tuning Converts Weak Language Models to Strong Language In Proceedings of the 41st International Models. Conference on Machine Learning, volume 235 of Proceedings of Machine Learning Research, pages 66216642. PMLR. Team Cohere, Aakanksha, Arash Ahmadian, Marwan Ahmed, Jay Alammar, Yazeed Alnumay, Sophia Althammer, Arkady Arkhangorodsky, Viraat Aryabumi, Dennis Aumiller, Raphaël Avalos, Zahara Aviv, Sammie Bae, Saurabh Baji, Alexandre Barbet, Max Bartolo, Björn Bebensee, Neeral Beladia, Walter BellerMorales, and 207 others. 2025. Command A: An Enterprise-Ready Large Language Model. Preprint, arXiv:2504.00698. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440 8451, Online. Association for Computational Linguistics. Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Y. Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, and 19 others. 2022. No Language Left Behind: Scaling Human-Centered Machine Translation. CoRR, abs/2207.04672. Yiming Cui, Ziqing Yang, and Xin Yao. 2024. Efficient and Effective Text Encoding for Chinese LLaMA and Alpaca. Preprint, arXiv:2304.08177. Dan Dan Friedman and Adji Bousso Dieng. 2023. The vendi score: diversity evaluation metric for machine learning. Transactions on machine learning research. John Dang, Shivalika Singh, Daniel Dsouza, Arash Ahmadian, Alejandro Salamanca, Madeline Smith, Aidan Peppin, Sungjin Hong, Manoj Govindassamy, Terrence Zhao, Sandra Kublik, Meor Amer, Viraat Aryabumi, Jon Ander Campos, Yi-Chern Tan, Tom Kocmi, Florian Strub, Nathan Grinsztajn, Yannis FletBerliac, and 26 others. 2024. Aya Expanse: Combining Research Breakthroughs for New Multilingual Frontier. Preprint, arXiv:2412.04261. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, and 514 others. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. Paul-Ambroise Duquenne, Holger Schwenk, and Benoît Sagot. 2023. SONAR: Sentence-Level Multimodal and Language-Agnostic Representations. Preprint, arXiv:2308.11466. Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. 2020. CCAligned: Massive Collection of Cross-Lingual Web-Document In Proceedings of the 2020 Conference on Pairs. Empirical Methods in Natural Language Processing (EMNLP), pages 59605969, Online. Association for Computational Linguistics. Maxim Enis and Mark Hopkins. 2024. From LLM to NMT: Advancing Low-Resource Machine Translation with Claude. Preprint, arXiv:2404.13813. Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, and 1 others. 2021. Beyond englishcentric multilingual machine translation. Journal of Machine Learning Research, 22(107):148. Christian Federmann, Tom Kocmi, and Ying Xin. 2022. NTREX-128 News Test References for MT Evaluation of 128 Languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pages 2124, Online. Association for Computational Linguistics. Markus Freitag and Yaser Al-Onaizan. 2017. Beam Search Strategies for Neural Machine Translation. In Proceedings of the First Workshop on Neural Machine Translation, pages 5660, Vancouver. Association for Computational Linguistics. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, and 178 others. 2024. Gemma 2: Improving Open Language Models at Practical Size. Preprint, arXiv:2408.00118. Naman Goyal, Cynthia Gao, Vishrav Chaudhary, PengJen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, MarcAurelio Ranzato, Francisco Guzmán, and Angela Fan. 2022. The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation. Transactions of the Association for Computational Linguistics, 10:522538. Maarten Grootendorst. 2022. BERTopic: Neural topic modeling with class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794. Nuno M. Guerreiro, Ricardo Rei, Daan van Stigt, Luisa Coheur, Pierre Colombo, and André F. T. Martins. 2024. xcomet: Transparent Machine Translation Evaluation through Fine-grained Error Detection. Transactions of the Association for Computational Linguistics, 12:979995. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. Preprint, arXiv:2306.11644. Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual Learning In Advances in Neural for Machine Translation. Information Processing Systems, volume 29. Curran Associates, Inc. Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, and Xing Wang. 2024. Exploring HumanLike Translation Strategy with Large Language Models. Transactions of the Association for Computational Linguistics, 11:229246. Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How Good Are GPT Models at Machine Translation? Comprehensive Evaluation. Preprint, arXiv:2302.09210. Edward Hu, yelong shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations. Alex Jones, Isaac Caswell, Ishank Saxena, and Orhan Firat. 2023. Bilex Rx: Lexical Data Augmentation for Massively Multilingual Machine Translation. Preprint, arXiv:2303.15265. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The State and Fate of Linguistic Diversity and Inclusion in the NLP World. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 62826293, Online. Association for Computational Linguistics. Juraj Juraska, Daniel Deutsch, Mara Finkelstein, and Markus Freitag. 2024. MetricX-24: The Google Submission to the WMT 2024 Metrics Shared Task. In Proceedings of the Ninth Conference on Machine Translation, pages 492504, Miami, Florida, USA. Association for Computational Linguistics. Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388395, Barcelona, Spain. Association for Computational Linguistics. Jianshang Kou, Benfeng Xu, Chiwei Zhu, and Zhendong Mao. 2024. KNN-Instruct: Automatic Instruction Construction with Nearest Neighbor Deduction. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1033710350, Miami, Florida, USA. Association for Computational Linguistics. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient Memory Management for Large Language Model Serving with PagedAttention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles. Wen Lai, Mohsen Mesgar, and Alexander Fraser. 2024. LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback. In Findings of the Association for Computational Linguistics: ACL 2024, pages 81868213, Bangkok, Thailand. Association for Computational Linguistics. Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and MarcAurelio Ranzato. 2018. PhraseBased & Neural Unsupervised Machine Translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 50395049, Brussels, Belgium. Association for Computational Linguistics. Haonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji, and Timothy Baldwin. 2023a. Bactrian-X: Multilingual Replicable Instruction-Following Models with Low-Rank Adaptation. Preprint, arXiv:2305.15011. Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Omer Levy, Luke Zettlemoyer, Jason Weston, and Mike Lewis. 2024. Self-Alignment with Instruction Backtranslation. In The Twelfth International Conference on Learning Representations. Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive Learning from Complex Explanation Traces of GPT-4. Preprint, arXiv:2306.02707. Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023b. Textbooks Are All You Need II: phi-1.5 technical report. Preprint, arXiv:2309.05463. Chin-Yew Lin. 2004. ROUGE: Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, JianGuang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, and Dongmei Zhang. 2025. WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct. In The Thirteenth International Conference on Learning Representations. Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2024. WizardCoder: Empowering Code Large Language Models with EvolInstruct. In The Twelfth International Conference on Learning Representations. Xing Han Lù. 2024. BM25S: Orders of magnitude faster lexical search via eager sparse scoring. Preprint, arXiv:2407.03618. Benjamin Marie and Atsushi Fujita. 2021. Synthesizing Monolingual Data for Neural Machine Translation. Preprint, arXiv:2101.12462. Benjamin Marie, Raphael Rubino, and Atsushi Fujita. 2020. Tagged Back-translation Revisited: Why Does It Really Work? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 59905997, Online. Association for Computational Linguistics. Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy Way. 2023. Adaptive Machine Translation with Large Language Models. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 227237, Tampere, Finland. European Association for Machine Translation. Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, Saiful Bari, Sheng Shen, Zheng Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2023. Crosslingual Generalization through Multitask Finetuning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1599116111, Toronto, Canada. Association for Computational Linguistics. OpenAI. 2024. Hello GPT-4o. https://openai.com/ index/hello-gpt-4o/. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL 02, page 311318, USA. Association for Computational Linguistics. Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. 2024. FineWeb2: sparkling update with 1000s of languages. Maja Popovic. 2015. chrF: character n-gram F-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 392395, Lisbon, Portugal. Association for Computational Linguistics. Maja Popovic. 2017. chrF++: words helping character n-grams. In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pages 612618, Copenhagen, Denmark. Association for Computational Linguistics. Matt Post. 2018. Call for Clarity in Reporting BLEU Scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186 191, Brussels, Belgium. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. Stephen Robertson, S. Walker, S. Jones, M. M. HancockBeaulieu, and M. Gatford. 1995. Okapi at TREC-3. In Overview of the Third Text REtrieval Conference (TREC-3), pages 109126. Gaithersburg, MD: NIST. Holger Schwenk. 2008. Investigations on large-scale lightly-supervised training for statistical machine translation. In Proceedings of the 5th International Workshop on Spoken Language Translation: Papers, pages 182189, Waikiki, Hawaii. Holger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzmán. 2021a. WikiMatrix: Mining 135M Parallel Sentences in 1620 In Proceedings Language Pairs from Wikipedia. of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 13511361, Online. Association for Computational Linguistics. Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021b. CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 64906500, Online. Association for Computational Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8696, Berlin, Germany. Association for Computational Linguistics. Shivalika Singh, Freddie Vargus, Daniel Dsouza, Börje F. Karlsson, Abinaya Mahendiran, Wei-Yin Ko, Herumb Shandilya, Jay Patel, Deividas Mataciunas, Laura OMahony, Mike Zhang, Ramith Hettiarachchi, Joseph Wilson, Marina Machado, Luisa Moura, Dominik Krzeminski, Hakimeh Fadaei, Irem Ergun, Ifeoma Okoh, and 14 others. 2024. Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11521 11567, Bangkok, Thailand. Association for Computational Linguistics. Garrett Tanzer, Mirac Suzgun, Eline Visser, Dan Jurafsky, and Luke Melas-Kyriazi. 2024. Benchmark for Learning to Translate New Language from One Grammar Book. In The Twelfth International Conference on Learning Representations. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https: //github.com/tatsu-lab/stanford_alpaca. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025. Gemma 3 Technical Report. Preprint, arXiv:2503.19786. Qwen Team. 2024. Qwen2.5: Party of Foundation Models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open Foundation and FineTuned Chat Models. Preprint, arXiv:2307.09288. Kosei Uemura, Mahe Chen, Alex Pejovic, Chika Maduabuchi, Yifei Sun, and En-Shiun Annie Lee. 2024. Instruction Tuning of African LanAfriInstruct: In Findings of the Asguages for Diverse Tasks. sociation for Computational Linguistics: EMNLP 2024, pages 1357113585, Miami, Florida, USA. Association for Computational Linguistics. Ahmet Üstün, Viraat Aryabumi, Zheng Yong, Wei-Yin Ko, Daniel Dsouza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1589415939, Bangkok, Thailand. Association for Computational Linguistics. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-Instruct: Aligning Language In ProModels with Self-Generated Instructions. ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, and Jun Xie. 2023. PolyLM: An Open Source Polyglot Large Language Model. Preprint, arXiv:2307.06018. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Zachary Mueller, Harm de Vries, Leandro Von Werra, Arjun Guha, and LINGMING ZHANG. 2024a. SelfCodeAlign: Self-Alignment for Code Generation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and LINGMING ZHANG. 2024b. Magicoder: Empowering Code Generation with OSS-Instruct. In Fortyfirst International Conference on Machine Learning. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, and 3 others. 2020. HuggingFaces Transformers: State-of-the-art Natural Language Processing. Preprint, arXiv:1910.03771. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. 2024a. WizardLM: Empowering Large Pre-Trained Language Models to Follow Complex Instructions. In The Twelfth International Conference on Learning Representations. Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla. 2024b. Paradigm Shift in Machine Translation: Boosting Translation Performance of In The Twelfth InternaLarge Language Models. tional Conference on Learning Representations. Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, and Huda Khayrallah. 2025. X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale. In The Thirteenth International Conference on Learning Representations. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024c. Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. In Forty-first International Conference on Machine Learning. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483 498. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, and 40 others. 2024. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671. Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong. 2023. BigTranslate: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages. Preprint, arXiv:2305.18098. Armel Zebaze, Benoît Sagot, and Rachel Bawden. 2025a. Compositional Translation: Novel LLMbased Approach for Low-resource Machine Translation. Preprint, arXiv:2503.04554. Armel Randy Zebaze, Benoît Sagot, and Rachel Bawden. 2025b. In-Context Example Selection via Similarity Search Improves Low-Resource Machine Translation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 1222 1252, Albuquerque, New Mexico. Association for Computational Linguistics. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrapping Reasoning With Reasoning. In Advances in Neural Information Processing Systems, volume 35, pages 1547615488. Curran Associates, Inc. Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024. Automatic Instruction Evolving for Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 69987018, Miami, Florida, USA. Association for Computational Linguistics. Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, and Lei Li. 2024. Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 27652781, Mexico City, Mexico. Association for Computational Linguistics. Tarek Ziadé. 2023. MediaWiki Categories https://huggingface.co/datasets/ Model. tarekziade/wikipedia-topics."
        },
        {
            "title": "A Reproducibility Details",
            "content": "A.1 Models, Datasets and Tools In Table 7, we list the links to the relevant resources used for the experiments. A."
        },
        {
            "title": "Implementation Details",
            "content": "We use HuggingFaces Transformers library (Wolf et al., 2020). For training the unidirectional models, we use per-device batch size of 4 and apply gradient accumulation over 4 steps. We use stacking with maximum length of 512 tokens. Training is conducted over 5,000 steps using learning rate of 1e-5, with 500 warmup steps. The learning rate decays to zero following cosine schedule. We also apply weight decay of 0.01. For the multidirectional models, which are trained across the 10 target languages, we use the same hyperparameters but extend training to 100,000 steps. We adopt the prompt template introduced by Xu et al. (2024b), and compute the loss only on the target sentence. In all experiments, we fine-tune in both directions (i.e., source-to-target and target-to-source), so each parallel sentence pair contributes two samples to the training set. On smaller datasets such as FLORES and SMOLSENT, we reduce the training steps to 1,000. In this setting, the batch size per device is 2, we use 100 warmup steps and we do not perform gradient accumulation. All other hyperparameters remain unchanged. Translate this from English to Hausa : English : \" We now have 4month - old mice that are non - diabetic that used to be diabetic ,\" he added . Hausa :"
        },
        {
            "title": "B Additional Experiments",
            "content": "B.1 More baselines Table 8 presents comparison of our fine-tuned student models on FLORES-200 against addi-"
        },
        {
            "title": "Datasets",
            "content": "FLORES-200 NTREX HF TICO-19 https://huggingface.co/datasets/facebook/flores hhttps://huggingface.co/datasets/mteb/NTREX https://huggingface.co/datasets/gmnlp/tico"
        },
        {
            "title": "Models evaluated",
            "content": "Aya-expanse-8B Aya-expanse-32B Command-R7B Qwen2.5-7B-Instruct Qwen2.5-32B-Instruct LLaMAX2-7B-Alpaca LLaMAX3-8B-Alpaca LLaMA-2-7B LLaMA-3-8B Gemma-2-9B-It Gemma-2-27B-It Gemma-3-4B-It Gemma-3-27B-It Gemma-3-4B-Pt Gemma-3-27B-Pt LLaMA-3.1-8B-It LLaMA-3.1-70B-It NLLB-200-3.3B https://huggingface.co/CohereLabs/aya-expanse-8b https://huggingface.co/CohereLabs/aya-expanse-32b https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024 https://huggingface.co/Qwen/Qwen2.5-7B-Instruct https://huggingface.co/Qwen/Qwen2.5-32B-Instruct https://huggingface.co/LLaMAX/LLaMAX2-7B-Alpaca https://huggingface.co/LLaMAX/LLaMAX3-8B-Alpaca https://huggingface.co/meta-llama/Llama-2-7b-hf https://huggingface.co/meta-llama/Meta-Llama-3-8B https://huggingface.co/google/gemma-2-9b-it https://huggingface.co/google/gemma-2-27b-it https://huggingface.co/google/gemma-3-4b-it https://huggingface.co/google/gemma-3-27b-it https://huggingface.co/google/gemma-3-4b-pt https://huggingface.co/google/gemma-3-27b-pt https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct https://huggingface.co/facebook/nllb-200-3.3B"
        },
        {
            "title": "Other resources",
            "content": "MetricX24-XXL XCOMET-XL BM25s FastText Wikipedia topics vLLM (Kwon et al., 2023) https://huggingface.co/google/metricx-24-hybrid-xxl-v2p6 https://huggingface.co/Unbabel/XCOMET-XL https://github.com/xhluca/bm25s https://huggingface.co/facebook/fasttext-language-identification https://huggingface.co/datasets/tarekziade/wikipedia-topics https://github.com/vllm-project/vllm Table 7: Links to datasets, benchmarks and models. tional baselines. They surpass all evaluated 7B models, and notably, unidirectional models based on LLaMA-3-8B with beam search outperform LLaMA-3.3-70B It. B.2 Translation into English In Table 9, we present the performance of the finetuned student models on FLORES-200 for translation from low-resource languages into English. The results mirror our observations in the reverse direction: unidirectional fine-tuning consistently outperforms multidirectional fine-tuning, and using strong base model (LLaMA-3-8B) leads to significantly better performancecomparable to models that are up to three times larger. B.3 ChrF++ and XCOMET scores We consider the XCOMET metric (Guerreiro et al., 2024), specifically its reference-based version: XCOMET-XL (which supports the same 100 languages as XLM RoBERTa (Conneau et al., 2020)). XCOMET scores range from 0 and 1, which we rescale to 0 to 100 (higher scores are better). We evaluate the translations produced on FLORES-200 by models fine-tuned on the TOPXGEN dataset and report the results in Table 10. The results are consistent with BLEU and MetricX. B.4 Results on NTREX-128 and TICO-19 In this section, we evaluate the models on 2 additional benchmarks: NTREX 128 (Barrault et al., 2019; Federmann et al., 2022) is an MT benchmark derived from WMT19 news data translated by professional human translators. It contains 1997 parallel sentences and is recommended for the evaluation of from-English translation directions. We use the first 1000 sentence pairs for evaluation, and the last 997 sentence pairs as the selection pool. TICO-19 (Anastasopoulos et al., 2020) is an MT benchmark comprising texts on the COVID-19 pandemic covering 35 languages. Its validation Models LLaMA-3.3-70B It LLaMA-3.1-8B-It Aya-expanse-8B Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Models Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 25.02 15.47 4.53 4.31 10.56 13.00 14.93 23.70 25.64 21.77 24.07 5.56 11.15 19.98 22.04 15.38 14.76 12.55 6.25 5.27 6.95 5.68 18.15 9.61 3.33 4.39 17. 13.11 13.77 19.65 20.52 18.22 19.36 6.09 11.75 16.53 19.37 6.75 8.57 8.42 5.20 5.07 6.05 5.76 Other Baselines 14.36 7.75 3.66 4.13 9.34 9.05 17.39 22.41 22.72 14. Our Models 12.30 12.90 16.28 17.02 15.54 16.09 10.97 10.04 7.31 6.54 7.96 7.13 8.11 3.73 2.25 1.94 4.57 7.30 7.61 11.76 13.60 9.76 11.62 14.05 20.71 23.11 24.28 18. 15.87 14.73 9.91 8.51 12.96 11.40 23.28 12.21 5.28 5.22 12.07 15.11 16.08 21.88 23.24 20.84 22.37 4.60 7.66 6.64 12.50 14.97 7.98 6.31 4.21 3.77 4.52 3.94 Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX LLaMA-3.3-70B It LLaMA-3.1-8B-It Aya-expanse-8B Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 9.35 4.20 4.46 3.50 9. 8.89 8.20 13.20 13.70 12.03 12.71 11.14 19.63 18.08 21.19 10.72 11.73 10.63 7.00 6.17 7.94 7.05 17.27 10.30 9.66 5.90 8.60 14.70 15.42 16.84 18.16 16.24 17.32 5.46 10.71 3.42 16.69 12. 7.19 5.75 4.88 4.26 5.66 4.83 Other Baselines 33.89 20.44 4.82 4.94 21.62 4.75 9.21 21.81 21.92 7.34 Our Models 19.19 20.96 30.99 33.49 28.53 31. 11.42 9.52 5.23 4.51 6.03 5.07 26.31 19.24 6.60 5.10 9.17 14.89 16.33 22.43 23.51 21.65 22.92 3.88 5.93 9.03 13.56 17.59 8.43 6.63 4.16 3.78 4.54 3.96 6.21 3.41 3.60 2.55 12. 9.25 7.67 12.39 13.93 11.74 13.15 16.14 23.41 23.99 24.28 10.31 15.24 13.09 9.33 7.77 10.56 8.61 Table 8: BLEU and MetricX scores for 10 English directions from FLORES 200. Best results after fine-tuning are highlighted in bold. and test sets consist of 971 (used as selection pool) and 2100 samples respectively. We focus on translating from English. We report the results obtained on NTREX-19 in Table 11 and those obtained on TICO-19 in Table 12. On both benchmarks, the best fine-tuned model usually ranks third, behind NLLB-200-3.3B and Gemma-3-27B-It. B.5 Fine-tuning the generator and the back-translator We conduct additional experiments to assess the impact of fine-tuning both the back-translator, NLLB-200-3.3B, and the generators base model, Gemma-3-27B-PT, on the TOPXGEN dataset. For NLLB, we retain the same training hyperparameters as before, modifying only the maximum sequence length to 128. In contrast, for Gemma, we apply LoRA (Hu et al., 2022), fine-tuning the q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, and down_proj modules with rank = 32, scaling factor α = 64, and dropout rate 0.01. We use the same training hyperparameters as in previous runs and fine-tune on 4H100 80GB GPUs. We compute BLEU and MetricX scores every 200 training steps and present the results in Figure 5. For languages like Basque, Nepali, and Urduwhere Gemma-3-27B-IT outperforms NLLB (see Table 2)we observe that fine-tuning NLLB on the TOPXGEN dataset yields substantial improvements. Specifically, NLLB gains up to 3 BLEU points in Basque and Nepali, and 1 point in Urdu, with corresponding MetricX gains of approximately 1 point, and up to 3 in Nepali. In cases where NLLB and Gemma-3-27B-IT perform comparably (e.g., Sundanese and Swahili), BLEU remains largely unchanged, while MetricX shows modest improvements. Conversely, when Gemma-3-27B-IT underperforms NLLB, further fine-tuning NLLB on TOPXGEN leads to decline in performancemore pronounced in BLEU, but also noticeable in MetricX. Finetuning Gemma-3-27B-PT on generations from Gemma-3-27B-IT yields significant gains in translation quality. However, we observe diminishing returns as training progresses across all directions. We hypothesize that the model does not learn translation per se through this process; rather, early training steps help it infer the structure of the translation task. Once this template is internalized, the model relies on its pretrained knowledge for transMethods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size= Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 36.21 35.59 35.07 36. 32.99 26.78 12.21 24.41 12.13 23.55 13.27 15.36 29.27 8.22 31.82 21.00 21.78 34.71 35.43 34.51 35.07 2.46 1.98 2.18 2. 2.82 4.12 11.63 5.16 10.34 6.48 11.06 8.73 3.09 13.54 2.74 8.41 7.75 3.21 3.07 3.19 3.05 36.98 31.20 28.76 31. 27.04 19.91 5.72 9.56 6.68 12.10 8.09 20.16 27.84 6.68 26.93 20.82 20.81 32.42 33.07 31.75 32.67 4.27 4.28 4.83 4. 5.65 8.69 16.84 15.39 16.88 14.63 17.37 6.77 4.56 17.13 5.61 9.79 9.26 6.07 5.81 6.33 6.04 32.88 25.30 21.23 26. 18.59 15.27 5.94 8.49 6.37 10.15 7.31 12.15 21.72 5.83 22.69 16.27 15.38 26.22 27.70 26.27 27.56 4.92 5.74 7.14 6. 8.50 10.89 18.60 15.20 16.23 14.38 17.73 12.39 6.60 17.39 7.43 12.21 11.59 7.84 7.35 8.04 7.60 35.17 28.59 21.86 23. 19.45 12.54 6.46 10.79 6.84 10.09 7.53 10.77 17.58 5.98 16.71 16.80 17.09 27.42 28.35 25.38 26.34 3.96 4.45 6.67 7. 8.28 12.17 17.84 13.99 17.29 15.74 18.99 11.19 7.78 17.53 9.06 11.20 10.77 6.70 6.33 8.17 7.74 44.98 40.73 39.14 40. 37.09 28.03 23.25 28.33 18.22 33.27 21.85 15.28 34.02 10.32 33.43 24.29 26.22 38.69 40.00 38.81 39.41 2.93 2.19 2.63 2. 2.92 4.97 6.49 4.49 6.75 4.37 7.06 12.19 3.21 11.24 3.76 7.43 6.75 3.79 3.62 3.98 3.76 Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 31.02 28.76 22.13 21.32 20.44 11.38 5.52 12.01 8.44 10.53 8.05 12.35 24. 6.02 14.09 15.94 16.33 24.69 25.75 23.29 24.53 5.50 4.57 6.85 8.06 7.95 14.42 17.66 13.60 16.54 15.31 18.09 11.21 5. 18.73 11.42 11.93 11.51 8.32 7.87 9.05 8.63 42.04 37.78 32.60 36.14 31.41 25.36 17.75 28.25 18.89 28.98 21.69 21.17 31. 16.30 33.77 30.78 31.30 38.46 39.15 37.55 38.53 3.33 2.92 3.74 3.64 4.08 5.88 8.86 4.64 7.01 5.40 7.74 8.13 4. 8.76 3.83 6.44 6.07 4.19 3.98 4.49 4.34 45.70 46.09 46.23 46.89 43.47 34.37 16.51 28.22 12.75 28.98 14.79 27.67 38. 10.78 39.46 27.55 28.77 42.77 44.04 42.51 43.86 2.88 2.19 2.26 2.41 2.66 4.40 10.98 5.89 11.65 6.67 12.09 5.96 3. 12.84 3.41 7.99 7.44 3.78 3.63 3.95 3.71 39.73 36.90 37.04 40.01 34.82 29.05 15.92 28.70 17.97 31.00 22.43 12.71 32. 10.93 33.30 21.30 23.33 35.61 36.62 35.44 36.33 3.08 2.14 2.54 2.37 2.98 3.96 8.86 4.00 7.55 3.93 6.37 13.89 3. 11.21 3.41 8.17 7.29 3.74 3.51 3.91 3.69 40.06 31.96 28.17 22.71 26.05 12.94 8.72 14.90 8.66 15.93 9.54 18.75 28. 8.17 17.08 20.73 19.68 31.38 32.14 29.34 30.62 3.71 5.14 5.76 8.68 6.66 13.18 16.50 11.56 15.76 12.47 16.22 7.29 5. 16.63 10.34 11.28 10.47 6.98 6.71 7.83 7.39 Table 9: BLEU and MetricX scores for 10 English directions from FLORES 200. Best results after fine-tuning are highlighted in bold. lation. Further fine-tuning on its own generations appears counterproductive. As shown in Table 13, the fine-tuned models do not surpass the performance of Gemma-3-27B-PT in the 5-SHOT BM25 setting, suggesting that self-generated data does not enhance the models understanding of the target languages. B.6 Analysis of the TOPXGEN dataset For each language, we compute the average MetricX-24 quality estimation (QE) scores over the first 20K sentences. We also report the average number of words and tokens per sentence for both the source (English) and target sides. As shown in Table 14, source sentences have relatively consistent average word count across languages. However, in terms of tokens, sentences in low-resource languages (LRLs) typically require twice as many tokens as their English counterparts. For language pairs involving Hausa, Nepali, Somali, and Urdu, TOPXGEN achieves higher QE scores than both FLORES and SMOL, suggesting that its topic-guided generation produces natural and coherent text in LRLs, accurately translated by the back-translation model. The Vendi Score (Dan Friedman and Dieng, 2023), calculated using SONAR embeddings, quantifies the diversity of texthigher values indicate Figure 5: BLEU and MetricX results for 10 EnglishX directions from FLORES 200. We fine-tune NLLB-200-3.3B and Gemma-3-27B-PT. We consider 1 model per direction and report the scores (greedy decoding) every 200 steps. Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Basque Hausa Igbo Kinyarwanda Nepali chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET 46.31 49.71 46.69 49.28 41.00 39.97 18.76 32.58 25.53 30.99 26.31 31.16 34. 16.90 41.36 36.44 38.59 46.83 48.28 45.36 47.02 76.88 78.97 73.43 77.61 60.10 51.19 54.79 30.74 26.96 27.15 21.13 34.21 56. 22.75 62.92 39.97 48.68 74.21 80.10 71.05 77.89 51.58 44.94 42.23 43.40 38.41 31.30 11.24 23.35 19.29 20.99 20.02 40.75 41. 11.64 34.05 37.46 37.62 44.73 45.21 42.80 43.76 65.65 58.57 58.72 55.46 50.13 31.23 27.13 21.84 22.00 22.32 20.42 36.32 55. 25.21 39.59 41.80 46.40 61.11 64.56 57.24 61.57 40.56 36.41 30.65 36.13 25.82 22.76 9.60 19.43 17.22 20.45 15.90 30.27 33. 11.25 23.20 31.02 32.41 36.65 37.73 35.65 36.61 19.04 18.40 18.95 18.74 19.76 19.77 23.63 19.58 19.02 20.23 19.98 18.74 19. 21.95 20.60 18.47 17.56 18.53 18.60 18.35 18.24 46.90 37.18 28.31 29.38 21.99 19.25 12.54 22.10 19.13 20.03 15.56 20.11 19. 13.92 18.37 26.53 27.47 34.51 36.32 30.30 32.23 26.93 23.12 24.61 24.69 24.25 23.33 22.83 24.56 23.35 24.86 21.85 24.34 25. 23.18 23.94 22.55 22.87 22.25 23.22 21.93 23.08 44.59 45.76 43.07 45.74 39.87 32.68 23.29 30.64 22.01 30.76 23.11 36.55 41. 15.40 37.47 35.84 38.38 42.99 44.72 42.39 44.42 75.82 83.12 79.71 78.20 74.51 55.28 50.46 51.95 62.55 45.73 31.12 39.17 69. 26.13 61.10 51.37 61.84 78.17 82.32 75.78 81.35 Somali Sundanese Swahili Urdu Xhosa chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET chrF++ XCOMET 41.58 39.02 32.03 33.33 27.81 22.56 12.23 28.18 24.87 23.56 21.24 33.31 35.14 13.76 20.97 32.56 33.33 38.34 39.08 36.82 37. 63.09 57.19 35.44 36.81 27.45 21.65 27.42 26.07 22.92 21.52 20.82 30.17 50.75 22.46 24.82 32.35 35.75 52.08 56.78 47.22 52. 45.28 42.95 39.54 42.48 37.63 33.22 30.79 33.84 33.46 31.17 26.60 31.55 34.94 24.55 38.27 39.99 41.90 42.90 44.18 42.00 43. 66.86 70.17 61.69 67.08 58.40 48.63 73.38 52.53 82.88 35.60 34.35 41.34 56.56 36.89 52.82 60.48 67.52 70.50 74.00 68.25 72. 58.60 58.37 58.13 57.52 53.64 45.53 26.97 33.71 25.24 34.26 25.25 49.18 50.83 15.59 46.86 44.42 46.66 55.21 57.01 53.11 54. 78.97 81.34 82.22 78.73 75.32 56.04 25.13 27.80 21.24 26.47 21.48 51.16 70.09 23.45 60.37 46.61 55.64 75.45 80.62 72.29 78. 47.17 44.61 32.32 46.05 38.20 38.60 19.81 31.86 27.02 30.87 24.03 32.08 38.90 15.02 35.91 34.98 37.23 42.37 43.36 41.38 42. 70.44 77.74 33.64 73.17 62.87 55.91 31.53 44.64 37.71 39.42 26.01 27.12 57.27 24.66 54.45 40.43 51.21 69.20 73.92 66.97 72. 46.78 38.44 41.68 28.32 27.10 18.93 14.80 23.70 21.26 23.01 18.34 34.42 33.91 12.44 16.69 31.05 32.80 36.84 38.52 35.77 37. 51.58 39.86 71.66 29.61 28.04 23.06 24.35 24.36 22.46 24.18 21.66 28.34 36.14 23.61 24.76 29.19 33.36 38.35 44.37 36.21 42. Table 10: ChrF++ and XCOMET-XL scores for 10 English directions from FLORES 200. Best fine-tuning results are highlighted in bold. greater diversity. The results are summarized in Table 14. On the target side, TOPXGEN generally achieves higher Vendi scores than FLORES (e.g., 1.123 vs. 1.096 in Somali), suggesting more diverse generations. Source-side sentences are also more diverse in TOPXGEN, though the difference is less pronounced. Notably, SMOLSENT, despite its smaller size, exhibits high diversity and occasionally surpasses TOPXGENparticularly in languages like Hausa on the target side. However, as shown in Figure 3, this increased diversity does not consistently lead to better translation quality than that achieved by TOPXGEN."
        },
        {
            "title": "We also evaluate how well",
            "content": "the paragraphs generated by TOPXGEN align with To do this, we gentheir intended topics. language using erate 1,000 paragraphs per Gemma-3-27B-It, and ask both Gemma-3-27B-It and Llama-4-Scout-17B-16E-Instruct to assess whether each paragraph accurately addresses its assigned topic. We consider two settings: the paragraph written in the lowresource language, and its English translation obtained via sentence-by-sentence translation using NLLB-3.3B. Results are presented in Table 15. According to Gemma-3-27B-It, 97% of the paragraphs it generates are on-topic, though this rate decreases to 90% when the same paragraphs are translated into English. Similarly, Llama-4-Scout-17B-16E-Instruct finds that 93% of the original paragraphs align with their topics, dropping to 8385% after translation. In summary, the generated paragraphs are generally well-aligned with the provided topics. Even in Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 18.29 21.62 18.31 21.70 13.40 13.23 4.02 8.79 5.04 6.91 4.69 10.29 11. 4.22 14.61 10.95 12.01 19.10 20.77 18.11 18.79 8.29 5.79 7.36 6.61 10.14 12.47 13.21 16.72 19.93 19.06 21.65 15.88 11. 22.20 10.07 16.01 14.17 7.86 6.78 8.44 7.13 25.43 19.54 17.37 18.58 14.69 10.46 2.56 6.42 4.86 6.25 5.12 18.45 17. 1.96 12.56 13.27 13.45 19.44 20.02 17.91 18.54 4.51 4.97 5.69 6.31 6.94 11.72 18.52 16.62 16.32 16.65 18.91 6.98 6. 21.23 10.41 9.32 9.30 5.84 5.84 6.76 6.45 22.43 16.64 14.07 18.44 11.76 10.07 2.98 5.65 4.27 6.68 5.40 10.10 15. 3.57 14.03 13.01 14.35 17.88 19.00 16.60 18.18 5.15 7.76 10.44 8.76 14.20 17.09 20.01 20.08 21.52 19.15 21.84 13.79 9. 21.84 15.19 11.64 10.70 8.19 7.38 8.55 8.04 21.05 14.98 8.06 8.95 5.03 4.55 2.92 5.00 3.29 4.23 2.97 5.52 5. 3.51 5.45 7.42 8.06 11.89 13.16 9.38 10.64 4.76 7.99 14.85 14.04 19.88 20.45 21.52 20.84 22.82 22.44 23.83 18.36 19. 22.34 20.42 16.93 15.56 11.18 9.66 13.98 11.97 22.12 18.38 17.39 18.42 14.64 9.60 4.35 8.02 4.33 7.97 4.00 9.82 16. 2.79 12.07 10.51 11.62 15.89 17.22 15.35 16.10 5.32 4.23 4.81 5.15 5.62 9.09 9.79 9.08 7.39 10.57 13.34 14.73 6. 19.29 8.17 9.51 7.96 5.36 4.81 5.79 5.00 Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 19.82 14.82 11.04 12.03 8.78 5.79 2.88 8.53 7.08 5.84 5.20 11.38 13.66 2.78 6.03 10.92 11.18 15.30 16.15 14.04 14. 4.74 5.48 10.83 10.82 14.21 19.23 19.62 14.47 17.18 18.67 20.75 9.57 7.30 21.07 17.96 12.75 11.31 7.77 6.83 8.74 7. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 38.63 36.97 35.99 34.83 32.52 22.25 6.90 12.15 6.81 12.29 6.86 24.80 29.45 4.02 26.37 19.89 21.88 32.06 34.31 30.25 33. 5.38 4.23 4.53 4.96 5.93 9.92 19.57 16.73 21.40 17.44 21.38 7.91 7.19 21.96 8.55 13.11 11.21 6.42 5.53 7.10 5. 32.16 28.62 26.89 30.50 21.08 21.19 3.55 11.18 7.11 11.70 5.56 11.74 22.58 3.62 19.42 14.05 15.42 22.34 24.26 21.80 23. 5.11 3.71 4.49 4.24 5.90 6.70 13.77 8.44 10.08 10.30 14.21 15.73 6.60 19.31 7.57 9.78 8.20 5.25 4.62 5.69 4. 18.73 12.41 9.38 7.50 7.61 3.49 2.44 5.24 4.03 4.98 3.00 11.10 10.51 2.68 3.25 9.00 7.75 11.53 12.18 10.65 11. 4.61 8.58 12.98 16.12 18.14 23.06 21.81 2.39 23.73 22.21 24.10 11.21 11.33 23.02 21.78 16.50 14.29 11.41 9.66 12.16 10. Table 11: BLEU and MetricX scores for 9 English directions from NTREX 128 (Federmann et al., 2022). cases where strict topical alignment is not achieved, the content remains relevant for machine translation training, where the primary requirement is having semantically equivalent sentences across languages. B.7 Topic Modeling We use BERTopic8 (Grootendorst, 2022) to assess whether the most relevant words identified through clustering align with the intended topics. As shown in Table 16, we present results on six Basque paragraphs translated into English from the TOPXGEN dataset. We experiment with two setups: (1) using gpt-4o-mini to generate topic label for each 8https://maartengr.github.io/BERTopic/index.html paragraph, and (2) extracting the top 10 words most relevant to each paragraphs context. We find that the GPT-generated topics often encompass the ground truth topics, which are typically more finegrained. For instance, Theodore Shackley is identified as CIA officer, Frank Shields as tennis player, and Deng Xi as Chinese philosopher-the relevant words reflect these identities accurately. B.8 Qualitative Analysis In Table 17, we observe that Gemma-3-27B-Its generations in share lexical overlapboth at the character and word levelwith Google Translates translations of the corresponding English sentences, as identified by the back-translator. Furthermore, Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B 5-SHOT BM25 LLaMA-3-8B 5-SHOT BM25 LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Methods NLLB-200-3.3B Gemma-3-27B-It Gemma-2-27B-It LLaMA-3.1-70B It Gemma-2-9B-It LLaMA-3.1-8B-It Command-R7B Aya-expanse-32B Aya-expanse-8B Qwen-2.5-32B-It Qwen-2.5-7B-It LLaMAX2-7B Alpaca LLaMAX3-8B Alpaca LLaMA-2-7B LLaMA-3-8B LLaMA-2-7B uni. LLaMA-2-7B uni. beam size=5 LLaMA-3-8B uni. LLaMA-3-8B uni. beam size=5 LLaMA-3-8B multi. LLaMA-3-8B multi. beam size=5 Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 30.96 19.59 18.60 20.98 16.06 11.83 3.22 6.80 5.38 7.52 5.89 17.64 19.49 2.62 14. 14.24 14.95 20.37 21.31 19.33 20.04 3.97 4.53 5.29 5.62 6.60 11.38 18.86 16.78 15.80 16.45 18.88 6.58 5.81 19.59 8. 8.68 8.27 5.50 5.17 6.17 5.88 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 22.52 11.29 6.93 8.07 4.33 3.53 2.14 3.73 2.79 3.58 2.29 5.01 4.29 2.98 5. 5.32 6.16 9.10 10.09 7.69 8.66 7.44 10.84 17.01 15.57 21.08 21.47 22.57 22.19 23.29 23.14 24.38 19.74 20.06 21.64 20. 18.62 17.27 13.05 11.64 15.61 14.04 32.72 27.37 26.11 26.17 22.37 14.75 7.24 11.79 7.47 12.52 6.89 13.83 24.10 6.63 24. 13.46 14.80 22.79 24.10 22.17 23.35 4.02 3.44 3.96 4.20 4.55 7.29 8.56 7.53 6.37 9.22 12.67 14.89 5.20 17.19 6. 9.07 7.67 4.68 4.40 4.90 4.33 Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 12.43 9. 6.61 6.74 5.21 3.58 1.65 4.76 4.04 3.91 3.27 7.13 8.05 1.35 1.89 6.41 6.85 9.16 9.59 8.26 8.80 12.73 13. 16.67 16.98 18.66 21.88 22.02 18.98 20.73 21.30 22.34 16.01 14.42 22.57 21.75 17.53 16.72 14.42 13.90 15.25 14.47 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - 36.95 34. 34.48 33.59 30.23 20.39 6.55 12.02 7.83 13.00 7.82 22.46 28.03 5.29 25.87 20.33 22.51 31.28 32.61 29.25 30.84 3.95 3. 3.87 4.14 4.79 9.34 19.48 16.93 21.26 16.95 21.32 7.27 6.00 20.11 7.04 11.35 9.36 5.42 4.76 6.01 5.33 32.87 28. 25.84 27.66 22.33 20.41 5.39 12.00 8.98 12.66 7.02 10.95 22.61 5.02 21.58 14.35 16.35 23.50 24.39 22.82 24.07 3.59 3. 4.08 3.65 4.96 5.75 12.83 7.59 8.87 9.64 13.49 16.62 5.15 17.49 6.15 8.90 7.13 4.31 3.94 4.64 4.20 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Table 12: BLEU and MetricX scores for 6 English directions from TICO-19 (Anastasopoulos et al., 2020). Best results after fine-tuning are highlighted in bold. Google Translate consistently produces translations that are similar to those obtained from backtranslating Gemma-3-27B-Its generations. This suggests that the model is not hallucinating content, but instead generating semantically faithful and plausible sentences in the target language, reinforcing the relevance of our data generation pipeline. Methods Gemma-3-27B-PT 5-SHOT BM25 Gemma-3-4B-PT 5-SHOT BM25 Gemma-3-4B-It Gemma-3-4B-PT uni. Gemma-3-4B-PT multi. Methods Basque Hausa Igbo Kinyarwanda Nepali BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX 31.04 19.78 9.39 22.57 21.94 3.73 8.29 15. 6.44 6.93 25.98 15.41 6.95 19.22 18.12 4.24 8.33 13.04 5.29 6.05 19.39 10.15 5. 15.88 14.96 6.10 13.73 18.72 7.64 8.45 21.02 6.98 4.15 11.35 9.57 5.77 17.32 21. 10.70 13.42 34.06 25.48 16.99 21.25 22.31 3.71 5.18 6.56 4.02 4.13 Somali Sundanese Swahili Urdu Xhosa BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX BLEU MetricX Gemma-3-27B-PT 5-SHOT BM25 Gemma-3-4B-PT 5-SHOT BM25 Gemma-3-4B-It Gemma-3-4B-PT uni. Gemma-3-4B-PT multi. 16.99 9.77 4. 13.04 12.22 4.96 10.42 15.94 6.41 7.31 24.54 16.91 9.32 17.29 16.49 3.88 7.41 8. 4.62 5.71 42.17 30.83 18.31 32.00 30.39 3.91 6.65 10.75 5.10 5.51 29.72 21.32 16. 22.39 21.92 3.33 4.97 6.33 3.88 4.09 19.10 8.63 4.08 12.60 11.64 5.41 15.04 20. 9.24 10.50 Table 13: BLEU and MetricX scores for 10 English directions from FLORES. Best results after fine-tuning are highlighted in bold."
        },
        {
            "title": "Urdu Xhosa",
            "content": "Source mean number of words Target mean number of words Source mean Gemma-3-27B-It tokens Target mean Gemma-3-27B-It tokens MetricX24-XXL QE scores TOPXGEN MetricX24-XXL QE scores FLORES MetricX24-XXL QE scores SMOL 22.18 17.49 27.91 46.11 3.19 2.65 - 23.47 26.72 28.84 50.44 3.58 3.63 4. 21.10 23.05 26.46 54.32 5.68 5.02 4.62 22.80 19.12 28.33 57.20 5.38 3.43 3.97 18.21 16.16 22.84 34.32 2.86 4.13 - 22.80 25.50 28.20 57.44 4.34 4.89 6.50 22.15 19.69 27.87 43.06 3.88 3.66 - 23.02 22.96 28.38 48.89 2.91 3.58 3. 19.04 24.30 23.54 35.08 2.62 3.69 - 23.03 15.41 28.62 56.02 5.89 3.82 3."
        },
        {
            "title": "FLORES Source\nFLORES Target",
            "content": "Vendi scores (Dan Friedman and Dieng, 2023) based on SONAR embeddings (Duquenne et al., 2023) 1.075 1.103 - - 1.051 1.066 1.070 1.098 1.076 1. 1.051 1.074 1.069 1.103 1.076 1.116 1.051 1.074 1.068 1.113 1.076 1. 1.051 1.081 1.089 1.115 - - 1.051 1.077 1.072 1.123 1.076 1. 1.051 1.096 1.072 1.107 - - 1.051 1.086 1.078 1.106 1.076 1. 1.051 1.081 1.086 1.106 - - 1.051 1.067 1.066 1.107 1.076 1. 1.051 1.077 Table 14: Statistics of the TOPXGEN dataset in comparison to FLORES and SMOL."
        },
        {
            "title": "Yes\nNo",
            "content": "972 28 946 54 904 59 896 100 973 27 982 924 76 903 97 935 53 903 77 854 143 837 Gemma-3-27B-It"
        },
        {
            "title": "In English",
            "content": ""
        },
        {
            "title": "In LRL",
            "content": "938 62 973 27 926 74 965 35 878 122 Llama-4-Scout-17B-16E-Instruct"
        },
        {
            "title": "In English",
            "content": ""
        },
        {
            "title": "In LRL",
            "content": "903 96 934 63 881 112 919 77 832 163 965 930 70 918 82 881 114 977 23 943 57 902 894 103 977 23 913 87 959 38 871 127 969 891 109 919 70 805 192 Table 15: Repartition of 1000 paragraphs based on whether they discuss the topic they correspond to. We query Gemma-3-27B-It and Llama-4-Scout-17B-16E-Instruct (at Meta, 2025) in two settings: before and after translation by the back-translator (NLLB-2003.3B)."
        },
        {
            "title": "Ground Truth Topics",
            "content": "gpt-4o-mini-2024-07-18 Labels Most Representative Words Theodore Shackley Prasophyllum atratum Frank Shields Guardians of the Galaxy Vol. 3 (soundtrack) Llanfawr Quarries Deng Xi"
        },
        {
            "title": "CIA interventions in Peru\nBiodiversity and conservation\nDevelopment of Alternative Sports\nInfluence of Music in Film\nLocal heritage and infrastructure\nInfluential Chinese Scholars",
            "content": "cia the united of states in training , war was species the australia is , flowers of . plant it tennis sport , . and the to in is players film music the nosov , soundtrack of . the monuments quarries , of castle and . these to the , of . and dynasty chinese qing to Table 16: Alignement between the ground truth topics and topics derived by BERTopic on six samples."
        },
        {
            "title": "Basque",
            "content": "Hauetako bakoitzak bere ezaugarriak ditu, batzuk produktu freskoen saldaritzan espezializatuz (okindegiak, frutariak) eta beste batzuk otoitz-zerbitzu ezarritik haratago doazen produktu eta zerbitzuak eskainiz (oinarrizko janari-elementuen salmenta nagusitzen duten tabako-dendetan, adibidez). Hauetako bakoitzak bere ezaugarriak ditu, batzuk produktu freskoen salmentan espezializatuta daude (baserriak, fruta-dendak) eta beste batzuk otoitz-zerbitzu finkatutik haratago doazen produktuak eta zerbitzuak eskaintzen dituzte (adibidez, oinarrizko elikagaien salmentan nagusi diren tabako-dendetan). Each of these has its own characteristics, some specializing in the sale of fresh products (farms, fruit shops) and others offering products and services that go beyond the established prayer service (for example in tobacco shops that dominate the sale of basic food items). Each of these has its own characteristics, with some specializing in the sale of fresh produce (bakeries, greengrocers) and others offering products and services that go beyond the established prayer service (for example, tobacconists who mainly sell basic food items)."
        },
        {
            "title": "Hausa",
            "content": "Wadannan matakan sun hada da sabbin kayayyakin safarar dukiya, da kuma inganta tsaron filin yayin da ake tafiya da komo. Wadannan matakan sun hada da sabbin hanyoyin jigilar kayayyaki, da kuma inganta tsaro fagen sufuri. These measures included new means of transporting goods, as well as improved field security during transportation. These measures include new transportation equipment, and improved field security during travel and return."
        },
        {
            "title": "Kinyarwanda",
            "content": "Impamvu Korea Times yifashishwa cyane, ni uko idashyira agahato ku makuru, kandi ngo ikunda kugaragaza ibintu bitandukanye na byinshi bisanzwe bimenyeshwa nizindi nzego za Leta. Impamvu yo kwamamara muri Korea Times nuko idakurikirana amakuru, kandi ikunda kwerekana amakuru atandukanye nizindi nzego za leta. The reason for the popularity of the Korea Times is that it does not censor information, and tends to present information that differs from most other government agencies. The reason the Korea Times is so widely used is that it does not impose restrictions on news, and it tends to present things that are different from what is usually reported by other government agencies."
        },
        {
            "title": "Somali",
            "content": "Qoyskiisu waxay ahaayeen kuwo qani ah oo leh xiriirro badan, taasoo ka caawisay inuu helaa fursado badan oo uu kaga shaqeeyo adeegga milatari. Qoyskiisu waxay ahaayeen kuwo hodan ah oo lahaa xidhiidho badan, taas oo ka caawisay inuu helo fursado badan oo uu kaga shaqeeyo adeegga milatariga. His family was wealthy and had many connections, which helped him get many opportunities to work in the military service. His family was wealthy and well-connected, which helped him find many opportunities to serve in the military."
        },
        {
            "title": "Sundanese",
            "content": "Inskripsi ieu, ditulis dina basa Latin jeung basa Yunani, mangrupa conto anu saé kana kabijakan administrasi Romawi anu ngamimitahan panggunaan basa lokal pikeun mastikeun komunikasi anu efektif jeung populasi setempat. Prasasti, ditulis dina basa Latin sarta Yunani, mangrupakeun conto alus ngeunaan kawijakan administrasi Romawi nu wanti pamakéan basa lokal pikeun mastikeun komunikasi éféktif jeung populasi lokal. The inscription, written in Latin and Greek, is good example of Roman administrative policy that encouraged the use of local languages to ensure effective communication with the local population. This inscription, written in Latin and Greek, is good example of the Roman administrative policy of encouraging the use of local languages to ensure effective communication with the local population."
        },
        {
            "title": "Swahili",
            "content": "Pamoja na uzuri wake wa pekee, mlima huu pia umekuwa chanzo cha hadithi na misemo ya kitaifa kwa watu wa Wales kwa muda mrefu. Pamoja na uzuri wake wa kipekee, mlima huu pia kwa muda mrefu umekuwa chanzo cha hadithi na misemo ya kitaifa kwa watu wa Wales Along with its unique beauty, this mountain has also long been the source of stories and national sayings for the people of Wales. Along with its unique beauty, this mountain has also long been the source of national legends and sayings for the Welsh people."
        },
        {
            "title": "Xhosa",
            "content": "Kukho imilinganiselo eyahlukeneyo esetyenziswa ukunje, ngokuphumela kwindlela yokuxabisa iimpahla, kodwa kwakungekho zibakala ezivela kunyaka. Imilinganiselo eyahlukeneyo iyasetyenziswa, ekhokelela kwinkqubo yokuxabisa, kodwa akuzange kubekho zibakala zonyaka. Various measurements are used, resulting in valuation system, but there were no facts from the year. There are different measures used today, resulting in way of valuing goods, but there were no facts from the year. Table 17: Examples of generations with their back-translators translations. We provide in red, Google Translates translations (in the source language) of NLLB-200-3.3Bs translations in English. We provide in blue, Google Translates translations (in English) of the generators (Gemma-3-27B-It) generations."
        }
    ],
    "affiliations": [
        "Inria, Paris, France"
    ]
}