{
    "paper_title": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion",
    "authors": [
        "Oscar Ovanger",
        "Levi Harris",
        "Timothy H. Keitt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \\textbf{F}usion under \\textbf{IN}dependent \\textbf{C}onditional \\textbf{H}ypotheses (\\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \\emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \\texttt{\\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}"
        },
        {
            "title": "Start",
            "content": "Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Oscar Ovanger 1 Levi Harris 2 Timothy H. Keitt"
        },
        {
            "title": "Abstract",
            "content": "Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce Fusion under INdependent Conditional Hypotheses (FINCH), an adaptive log-linear evidence fusion framework that integrates pretrained audio classifier with structured spatiotemporal predictor. FINCH learns per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family contains the audio-only classifier as special case and explicitly bounds the influence of contextual evidence, yielding risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using lightweight, interpretable, evidence-based approach. Code is available: anonymous-repository 6 2 0 2 3 ] . [ 1 7 1 8 3 0 . 2 0 6 2 : r 1. Introduction Ensemble learning studies how multiple predictive models for the same target variable can be combined to improve performance, robustness, or calibration relative to any single model (Kuncheva, 2004; Kittler et al., 1998). Classical ensemble methods such as bagging, boosting, and stacking typically assume that all predictors are trained on the same underlying data distribution and that ensemble weights or 1 Figure 1. Haemorhous mexicanus (North American House Finch), the namesake of our model. combination rules can be learned jointly with model parameters. In many modern applications, however, predictive models are pre-trained, heterogeneous, and fixed at inference time. These models may rely on different sources of evidence, be trained on distinct datasets, and operate under incompatible modeling assumptions. As result, joint retraining or finetuning is often infeasible. Instead, the problem becomes one of combining the outputs of fixed predictors in principled manner. We consider the common setting in which multiple predictive models provide complementary evidence about shared latent target. Let denote class label, and let and denote two sources of evidence. Conditional independence, y, (1) provides useful idealization that motivates multiplicative evidence fusion. If the full generative distributions were available, Bayesian inference would yield p(y x, s) p(x y) p(s y) p(y). (2) FINCH adopts the corresponding log-linear fusion form while remaining applicable in regimes where this independence assumption holds only approximately, and deviations can be mitigated through adaptive, bounded weighting. In practice, however, generative models are often unavailable. Instead, one typically has access only to discriminative predictors pθ(y x) and pψ(y s), trained independently. In this case, the posterior can be expressed as p(y x, s) pθ(y x) p(x) pψ(y s) p(s) p(y) , (3) Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion where the marginal distributions p(x), p(s), and the implied prior p(y) are generally unknown. As result, the true posterior is not directly computable from the discriminative models alone. common approximation in this setting is to combine discriminative predictors using log-linear models or productof-experts formulations (Hinton, 2002; Genest & Zidek, 1986). These models operate directly on posterior outputs and define fused distribution of the form log p(y x, s) = log pθ(y x)+log pψ(y s)log Z(x, s), (4) where Z(x, s) is normalization constant ensuring (cid:80) p(y x, s) = 1. Throughout the paper, we write fusion rules in unnormalized log-space; the normalization constant is implicitly handled by the final softmax and is therefore omitted when it does not affect comparisons across classes. Log-linear fusion is decision-theoretically justified as logarithmic opinion pool under mild axioms (Heskes, 1998) and provides practical surrogate when only discriminative models are available. Existing approaches typically employ fixed or globally learned weights when combining predictors. This implicitly assumes that the relative reliability of each evidence source is constant across the input space. In many applications, this assumption does not hold. The informativeness of given evidence source may vary substantially across samples, leading to degraded performance or pathological dominance when fixed weights are used. In this work, we introduce an adaptive log-linear fusion framework that preserves the structure and interpretability of classical product-of-experts models while allowing persample modulation of evidence strength. We assume that the constituent predictors are fixed and discriminative, and we do not retrain or recalibrate their parameters. Instead, we learn gating function that estimates the reliability of contextual evidence on per-sample basis. The resulting fused posterior is defined (up to normalization) as log pω(y x, s) = log pθ(y x) + ω(x, s) log pψ(y s), (5) where ω(x, s) 0 is learned weighting function. The normalized posterior pω(y x, s) is obtained by applying softmax over y. This formulation recovers the first model classifier when ω(x, s) = 0, bounds the influence of contextual evidence, and enables adaptive fusion without retraining the base models. We evaluate this framework in the context of bioacoustic species classification by combining state-of-the-art acoustic classifier with structured spatiotemporal prior derived from large-scale observational data. Experiments show that adaptive weighting consistently outperforms fixed-weight fusion and audio-only baselines, particularly in regimes where contextual information is heterogeneous or weak in isolation. 2. Related Work We organize related work into two themes: (i) theory and mechanisms for combining probabilistic predictors (loglinear pooling, products of experts, and gating), and (ii) bioacoustic applications, distinguishing audio-only foundation encoders from systems that incorporate spatiotemporal context. 2.1. Log-Linear Fusion, Opinion Pools, and Products of Experts The combination of multiple classifiers has been studied extensively in machine learning and statistics (Kuncheva, 2004; Kittler et al., 1998). Classical ensemble methods (e.g., averaging, voting, Bayesian combination) depend on classifier diversity and error correlation, and often learn weights jointly with model parameters or via validation tuning. Maximum entropy models and logarithmic opinion pools formalize classifier fusion as log-linear aggregation of predictive distributions (Berger et al., 1996; Genest & Zidek, 1986). Product-of-experts models combine distributions by multiplication, yielding sharp posteriors when experts agree and diffuse posteriors when they disagree (Hinton, 2002). Logarithmic opinion pools provide decision-theoretic justification for weighted log-linear combinations under mild axioms (Heskes, 1998), and log-linear fusion can be viewed as practical surrogate to Bayesian inference when only discriminative predictors are available (Bishop, 2006). Most prior approaches employ fixed or globally learned weights, implicitly assuming that expert reliability is stationary across the input space. Our work focuses instead on per-sample weighting of contextual evidence while preserving the interpretability of log-linear fusion. 2.2. Gating Networks, Mixtures of Experts, and Reliability Estimation Mixture-of-experts models introduce gating networks that assign input-dependent weights to multiple predictors (Jacobs et al., 1991; Jordan & Jacobs, 1994). These models are typically trained end-to-end, with both experts and gates optimized jointly. Our setting differs in two key respects. First, we treat the constituent predictors as fixed and trained independently on different evidence sources. Second, we use multiplicative fusion in probability space (log-additive in log-space), motivated by conditional independence of evidence sources given the label. The gating function modulates the strength 2 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion of contextual evidence rather than selecting among experts. related line of work studies uncertainty estimation and selective prediction. Bayesian approximations and calibrated uncertainty signals (Gal & Ghahramani, 2016; Kendall & Gal, 2017) as well as confidence-based methods (Hendrycks & Gimpel, 2017) are commonly used to identify unreliable predictions or out-of-distribution inputs. Selective classification allows abstention when confidence is low (Geifman & El-Yaniv, 2017). In contrast, our method uses reliability signals to continuously modulate the contribution of contextual evidence within probabilistic fusion model, rather than abstaining or discarding an expert. 2.3. Audio Foundation Models for Bioacoustics (Audio-Only Encoders) Modern bioacoustic classifiers increasingly rely on selfsupervised or large-scale pretrained audio encoders, followed by lightweight adaptation (e.g., linear probes or prototypical probes) on downstream datasets (Lauha et al., 2022; Wang et al., 2022; Hagiwara, 2022; Choudhary et al., 2022; Nolasco et al., 2023; Rauch et al., 2025). Recent foundationmodel style approaches provide strong transferable representations for animal sounds, including BEATs (Chen et al., 2022) and NatureLM-Audio (Robinson et al., 2025), as well as large-scale benchmark/analysis efforts such as BEANS and studies of what drives bioacoustic representation quality (Hagiwara et al., 2022; Miron et al., 2025). Perch-style models emphasize large-scale pretraining for bird and wildlife acoustics and provide strong embeddings for transfer learning (Ghani et al., 2023; van Merrienboer et al., 2025). These audio-only encoders are powerful because they can be pretrained on audio corpora without requiring spatiotemporal metadata, and then reused across tasks and regions. Our method explicitly preserves this modularity by freezing the audio encoder/classifier and incorporating context only at the fusion layer. 2.4. Spatiotemporal Context in Bioacoustics: Priors vs. Joint Models Contextual information such as location and season is often incorporated via ecological priors derived from species distribution models (Fink et al., 2013; Madhusudhana et al., 2021), including large-scale citizen-science abundance models (Fink et al., 2010; 2014; Johnston et al., 2015). Many practical bioacoustic systems combine acoustic predictions with such priors either heuristically (e.g., post-hoc filtering) or by concatenating metadata with learned audio features. BirdNET is widely used system for avian monitoring and is commonly deployed with metadata-informed constraints (e.g., location/date based filtering or contextual feasibility) alongside acoustic classification (Kahl et al., 2021). More generally, joint approaches that ingest audio and metadata into single predictor p(y x, s) are more expressive but must learn audio-context interactions directly and require paired supervision. As formalized in Appendix A2, such joint modeling becomes necessary when conditional dependence between evidence sources is strong, whereas under (approximate) conditional independence log-linear fusion is sufficient and avoids the complexity of retraining large multimodal architectures. Our contribution targets the common regime where context is informative but heterogeneous: we combine strong pretrained audio classifier with structured spatiotemporal prior through bounded, per-sample gating function, improving robustness while preserving audio-only fallback and modular pretraining benefits. 3. Method 3.1. Notation and Evidence Sources We consider multi-class classification problem with the following notation: : audio observation, represented as 3 log-mel spectrogram, S: spatiotemporal context (latitude, longitude, and time), Y: species label, pθ(y x): audio-based classifier, pψ(y s): spatiotemporal (ecological) classifier, pω(y x, s): fused classifier. While our application is bioacoustic classification, the proposed fusion framework applies generally to any setting with multiple conditionally independent evidence sources given the target label. 3.2. Log-Linear Fusion Family We adopt per-sample log-linear fusion model of the form log pω(y x, s) = log pθ(y x) + ω(x, s) log pψ(y s), (6) where pω denotes an unnormalized log-score. The normalized posterior pω(y x, s) is obtained by applying softmax over y. The scalar ω(x, s) 0 controls the influence of the spatiotemporal model on per-sample basis. We parameterize ω(x, s) using bounded transformation with learnable scale: ω(x, s) = ωmax σ(cid:0)gϕ(u(x, s))(cid:1) + ϵ, 3 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion where σ() denotes the sigmoid function, ϵ > 0 ensures numerical stability, and ωmax is trainable parameter constrained to lie in [ϵ, 10]. This design allows the model to adaptively learn the appropriate range of contextual influence from data, while preventing degenerate or unbounded fusion weights. When ω(x, s) = 0, the fused classifier exactly recovers the audio-only model pθ(y x). The fusion is intentionally asymmetric: the audio classifier contributes additively in log-space for all samples, whereas the contextual model modulates the posterior only when deemed informative. This reflects the complementary inductive biases of the two sources. Audio observations often provide high discriminative power but are susceptible to confusions between acoustically similar species, while spatiotemporal models provide coarse but robust constraints by encoding ecological feasibility. 3.3. Adaptive Gating: Reliability and Informativeness To adaptively weight contextual evidence, we learn gating function ω(x, s) parameterized by two-layer MLP. The gating network operates on feature vector u(x, s) that summarizes uncertainty, confidence, and contextual structure: Audio features: faudio(x) = (cid:20) max (cid:21) pθ(y x), H(pθ(y x)) , pθ(y1 x) pθ(y2 x) , where y1, y2 denote the top two predicted classes. Spatiotemporal features: fprior(s) = (cid:20) max (cid:21) pψ(y s), H(pψ(y s)) , pψ(y1 s) pψ(y2 s) . Metadata features: fmeta = (cid:2)sin(cid:0) 2πd 365 (cid:1) , cos(cid:0) 2πd 365 (cid:1) , sin(cid:0) 2πh 24 (cid:1) , cos(cid:0) 2πh 24 (cid:1) , lat 90 , lon 180 (cid:3) . The concatenated feature vector u(x, s) is passed through two-layer MLP gϕ(u) with ReLU activations and dropout. Its output is squashed through sigmoid and scaled by ωmax, yielding ω(x, s) [0, ωmax]. In addition to the gating parameters, we jointly learn temperature parameter applied to the audio logits, and introduce small constant ϵ to ensure numerical stability. The resulting fused model is log p(y x, s) = log pθ(y x) + ω(x, s) log(cid:0)pψ(y s) + ϵ(cid:1), and we obtain p(y x, s) by applying softmax over y. (7) 4 Avoiding Gate Collapse. To prevent the adaptive gate from collapsing to trivial solution (e.g., ω(x, s) 0), we introduce variance regularization term during training: Lvar = λvar Var(x,s)B[ω(x, s)], where the variance is computed over each minibatch B. This term encourages ω(x, s) to take non-constant values across inputs, promoting genuine adaptivity rather than global suppression of contextual evidence. We set λvar sufficiently small and rely on validation-based model selection so that the regularizer promotes non-trivial adaptivity without overriding the empirical preference for ω(x, s) 0 when context is helpful. 3.4. Decision-Theoretic Safety and Robustness The proposed fusion framework is designed to be robust to misspecified or weak contextual models. key structural property is recoverability: for any input (x, s), setting ω(x, s) = 0 exactly recovers the audio-only classifier. Thus, the fusion family contains an explicit audio-only fallback and does not require reliance on contextual information. The influence of the spatiotemporal model is further controlled by bounding the fusion weight ω(x, s) ωmax, which prevents pathological domination of the posterior by contextual evidence. Even highly confident but incorrect contextual predictions therefore cannot overwhelm strong acoustic evidence. Adaptive gating enables selective incorporation of context. When the spatiotemporal model is uninformative (e.g., nearuniform or high entropy), the learned gate is driven toward zero. When contextual evidence is informative and consistent with the audio prediction, increasing ω(x, s) sharpens the posterior by suppressing ecologically implausible alternatives. From decision-theoretic perspective, the fused classifier corresponds to minimizing expected log-loss under bounded, sample-dependent combination of log-likelihood terms. Recoverability guarantees that the audio-only hypothesis remains available within the model class, while bounded gating mitigates harmful over-reliance on contextual evidence in practice. Formal conditions are provided in the appendix. Finally, freezing the audio classifier throughout fusion training preserves the underlying acoustic decision function, ensuring that any changes in prediction arise solely from the fusion mechanism rather than modification of the audio model itself. Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Figure 2. FINCH: adaptive log-linear evidence fusion. (A) An audio encoder and linear head produce an acoustic posterior pθ(y x) from the input spectrogram. (B) spatiotemporal model maps context (location and time) to prior pψ(y s). (C) gating network computes nonnegative per-sample weight ω(x, s) from uncertainty and informativeness features, yielding unnormalized log-scores log pω(y x, s) = log pθ(y x) + ω(x, s) log pψ(y s), which are normalized by softmax over to obtain pω(y x, s). When ω(x, s) = 0, the audio-only model is recovered. (D) Directed acyclic graph illustrating the assumption that and are conditionally independent given the label y, motivating multiplicative fusion. 4. Experimental Setup 4.1. Datasets and Spatiotemporal Prior We evaluate our method on two large-scale bioacoustic benchmarks: the Cornell Birdcall Identification (CBI) dataset and BirdSet. CBI is derived from the eBird citizenscience corpus and was released as part of Kaggle competition (Howard et al., 2020). The dataset contains short audio recordings annotated with species labels and associated metadata, including latitude, longitude, and date. In total, CBI comprises 264 bird species. BirdSet is large-scale benchmark for avian audio classification with standardized splits and evaluation protocols (Rauch et al., 2025). We incorporate weekly species-level spatiotemporal abundance priors from the eBird Status and Trends project (AdaSTEM) (Fink et al., 2010; 2014; Johnston et al., 2015), queried by location and date. Priors are precomputed for training efficiency and used directly at inference. Full details are provided in Appendix A5. pendix A5). On BirdSet, AdaSTEM priors are not available; instead, we train lightweight metadata-only MLP on the BirdSet training split to obtain learned contextual predictor pψ(y s) from the same time/location encodings used by the gating network (Appendix A1). 4.2. Audio Encoder and Classification Head For audio representation learning, we use the BEATs encoder extracted from the NatureLM-Audio model (Robinson et al., 2025). Although NatureLM-Audio is trained end-toend for audio-language tasks, prior work has shown that the resulting BEATs encoder achieves state-of-the-art performance on bioacoustic benchmarks when used as frozen feature extractor (Miron et al., 2025). All parameters of the audio encoder are frozen throughout training. The encoder produces embeddings of dimension 4096, which are mean-pooled over time and passed to linear classification head Context model per benchmark. We use different spatiotemporal models depending on the dataset. On CBI, pψ(y s) is given by the external eBird Status & Trends (AdaSTEM) prior queried by location and date (Apy = Ah(x) + b, (8) where RC4096 and RC, with denoting the number of species. For CBI, = 264. 5 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion 4.3. Fusion Models and Training Procedure To disentangle representation learning from evidence fusion, we adopt three-stage training procedure in which the audio encoder remains frozen throughout. Stage 1: Audio-Only Training. We first train linear classifier on frozen audio embeddings to obtain an audioonly predictor pθ(y x), establishing strong acoustic baseline. Stage 2: Fixed-Weight Fusion. We then introduce fixed scalar fusion weight ω 0, shared across all samples, to combine audio and spatiotemporal evidence. This stage learns globally calibrated fusion baseline. Stage 3: Adaptive Gating Fusion. Finally, we replace the global scalar weight with an adaptive gating network (Section 3.3) that predicts nonnegative, sample-dependent fusion weight ω(x, s). The gating network is initialized to match the fixed-weight solution and trained to enable per-sample adaptivity. In Table 4 is summary of the model components and trainable parameters for each training stage. 5. Results 5.1. Main Benchmark Results (Aggregate) Table 2 summarizes performance on CBI and BirdSet subsets, comparing FINCH against strong audio-only baselines and existing bioacoustic systems. For CBI, FINCH uses an external spatiotemporal prior derived from eBird Status & Trends (AdaSTEM), whereas for BirdSet it uses learned metadata-only MLP prior trained on the BirdSet training split. CBI. On CBI, FINCH achieves the highest test accuracy among all evaluated methods under the linear-probe protocol, improving accuracy from 0.806 for the audio-only baseline to 0.826 with adaptive fusion. Fixed-weight fusion yields only marginal improvement (0.808), indicating that globally calibrated fusion is insufficient when contextual reliability varies across inputs. The spatiotemporal prior performs poorly in isolation (accuracy = 0.030), confirming that the observed gains arise from selective integration rather than reliance on context alone. Despite introducing only lightweight gating network on top of frozen predictors, FINCH outperforms both audio-only models and joint audiocontext systems evaluated under the same protocol. BirdSet. On BirdSet subsets, FINCH matches or improves upon strong audio-only baselines across multiple evaluation metrics, including retrieval (AUROC), detection (cmAP), and classification (Top-1 accuracy). These results are notable given that the contextual predictor is simple metadataonly MLP trained from limited supervision. While improvements vary across subsets and metrics, FINCH consistently demonstrates that adaptive evidence weighting can leverage weak and heterogeneous contextual signals without degrading performance relative to audio-only models. 5.2. Qualitative Analysis: Adaptive Fusion in Practice To complement the aggregate results, we present two representative test-time examples in Fig. 3 that illustrate how FINCH adaptively balances acoustic and spatiotemporal evidence. In the first example, the audio-only classifier assigns low probability to the true species, ranking several acoustically similar alternatives higher, while the spatiotemporal model is highly confident and geographically specific. FINCH assigns large gating weight and recovers the correct class by amplifying reliable contextual evidence. In the second example, neither the acoustic model nor the spatiotemporal prior is correct in isolation: the prior is diffuse across multiple species, and the audio model is confidently wrong. Here, adaptive fusion suppresses the acoustically dominant but ecologically implausible class, yielding correct posterior prediction despite the failure of both marginal predictors. Importantly, the learned gating weights differ substantially across the two cases, demonstrating that fixed-weight fusion would be unable to recover both outcomes. These examples highlight the role of adaptive reliability estimation in mediating error trade-offs and align with the conditional-independence analysis in Appendix A2. 6. Discussion This work demonstrates that adaptive log-linear fusion of approximately conditionally independent evidence sources can yield consistent improvements over both standalone predictors and fixed-weight fusion schemes. Across largescale bioacoustic benchmarks, FINCH outperforms strong audio-only baselines on evaluated subsets, demonstrating the effectiveness of adaptive evidence fusion under weak and heterogeneous contextual signals. These gains arise from selectively integrating contextual evidence on per-sample basis rather than enforcing global calibration between modalities. key property of the proposed framework is its decisiontheoretic safety. Under this formulation, the audio-only classifier serves as an explicit fallback within the fusion family. When contextual evidence is uninformative or misleading, the learned gating function is encouraged to suppress its influence, reducing the risk of performance degradation relative to the likelihood model. This behavior is reflected empirically in both aggregate metrics and qualitative exam6 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Table 1. Main results on CBI and BirdSet subsets. For BirdSet, we report retrieval AUROC (R), detection cmAP (m), and Top-1 accuracy (A) on four subsets (PER, NES, UHH, SSW), formatted as / / A. For CBI, we report Top-1 accuracy and the corresponding training regime (Method). FINCH uses an external AdaSTEM prior on CBI and learned metadata-only MLP prior on BirdSet. Model Audio ProtoPNet-5 BirdMAE-L BirdMAE-L (PP) Perch 2.0 Peak-select Perch 2.0 Random Audio-only (pθ(y x)) Prior-only (pψ(y s)) FINCH (ours) CBI Method LP LP LP LP LP BirdSet (subset metrics: / / A) Acc 0.785 0.792 0.806 0.030 0.826 PER (R/m/A) NES (R/m/A) UHH (R/m/A) SSW (R/m/A) 0.790 / 0.300 / 0.590 0.820 / 0.350 / 0.600 0.820 / 0.310 / 0.590 0.802 / 0.255 / 0.528 0.786 / 0.232 / 0.535 0.824 / 0.232 / 0.429 0.930 / 0.380 / 0.520 0.910 / 0.410 / 0.520 0.930 / 0.380 / 0.470 0.948 / 0.382 / 0.504 0.953 / 0.403 / 0.562 0.936 / 0.245 / 0. 0.870 / 0.310 / 0.490 0.820 / 0.300 / 0.420 0.830 / 0.300 / 0.360 0.901 / 0.375 / 0.462 0.912 / 0.380 / 0.595 0.927 / 0.536 / 0.747 0.970 / 0.420 / 0.660 0.930 / 0.410 / 0.700 0.940 / 0.380 / 0.620 0.974 / 0.455 / 0.782 0.973 / 0.469 / 0.789 0.642 / 0.025 / 0.688 stricter feasibility constraints may trade off robustness under dataset shift. Finally, the conditional independence assumption underlying log-linear fusion is inherently strong and does not hold exactly in real-world data. We emphasize that conditional independence is used here as modeling motivation rather than strict assumption. Our empirical analysis indicates that residual dependence between audio features and spatiotemporal context exists but is weak and heterogeneous across classes. This regime aligns with the intended operating conditions of FINCH: independence holds sufficiently well to motivate multiplicative fusion, while adaptive gating mitigates deviations without requiring explicit modeling of audiocontext interactions. When conditional dependence is strong, fully joint models may be preferable, but they incur substantially higher training and data requirements. More broadly, FINCH is best viewed as general fusion framework rather than bioacoustic-specific solution. The formulation applies to any setting in which multiple discriminative predictors provide complementary, approximately independent evidence about shared target, and extending this approach to other domains remains promising direction for future work. ples, where adaptive fusion corrects confident audio errors when context is reliable and avoids pathological domination when it is not. The design of the gating mechanism involves several modeling choices that we treat pragmatically rather than exhaustively. In this work, the gating network operates on compact set of uncertaintyand confidence-based summary statistics derived from the predictive distributions. Alternative feature constructions or more expressive gating architectures could be explored, but our results indicate that even lightweight model suffices to capture substantial heterogeneity in contextual reliability. We also note that the spatiotemporal prior used in this study is relatively weak in isolation. With 264 classes, random guessing yields an accuracy of approximately 0.38%, whereas the prior achieves roughly 3% accuracy. This gap confirms that the observed performance improvements stem from selective integration rather than reliance on context alone. Stronger ecological models could further amplify the benefits of adaptive fusion, but the current results suggest that even coarse contextual signals can meaningfully improve predictions when integrated safely. Both the CBI dataset and the spatiotemporal prior are derived from the broader eBird ecosystem, which raises natural questions about potential data entanglement. Importantly, the AdaSTEM priors are trained on independent citizenscience checklists aggregated over large spatial and temporal windows and do not incorporate the audio recordings or competition splits used in CBI. Moreover, the prior achieves very low standalone accuracy, suggesting that it functions as weak ecological feasibility signal rather than proxy for species labels. An important limitation concerns false positives. While contextual information can suppress ecologically implausible predictions, the logistic regressionbased prior employed here interpolates across space and time, assigning nonzero probability to regions with sparse or no observations. As result, false positives are reduced but not eliminated, and 7 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Figure 3. Motivating examples of adaptive fusion at test time. Top: The acoustic model assigns low probability to the true class (Grey-crowned Rosy Finch), ranking several acoustically similar species higher. The spatiotemporal model is highly confident and geographically specific, and the fused posterior correctly recovers the priors top prediction with high gating weight. Bottom: Neither the acoustic model (top prediction: Yellow-bellied Flycatcher) nor the spatiotemporal prior (diffuse over multiple warblers) is correct in isolation. However, adaptive fusion suppresses the acoustically dominant but ecologically implausible class, yielding the correct posterior prediction (Black-throated Green Warbler). In both cases, the learned gating weight differs substantially across inputs, illustrating why fixed-weight fusion would fail to recover these outcomes. 8 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion"
        },
        {
            "title": "Impact Statement",
            "content": "This work contributes general framework for adaptively fusing multiple sources of probabilistic evidence under approximate conditional independence. While our experiments focus on bioacoustic species classification, the proposed method is applicable to wide range of domains in which heterogeneous predictors provide complementary information. In ecological monitoring and biodiversity assessment, improved integration of acoustic and contextual information may support more accurate large-scale analysis of species distributions and temporal trends. At the same time, predictions produced by automated systems should not be treated as definitive evidence of species presence without appropriate human validation, particularly in conservation or policyrelevant settings. More broadly, the proposed fusion mechanism is designed to be robust to unreliable or misleading contextual signals by bounding their influence and preserving safe fallback to primary evidence. This design choice helps mitigate risks associated with overconfident or biased auxiliary models when deploying decision-support systems in real-world environments."
        },
        {
            "title": "References",
            "content": "Fink, D., Damoulas, T., Bruns, N. E., La Sorte, F. A., Hochachka, W. M., and Kelling, S. Adaptive spatiotemporal exploratory models reveal complex patterns of bird occurrence. Ecological Applications, 23(5):10671086, 2013. Fink, D., Damoulas, T., Bruns, N. E., La Sorte, F. A., Hochachka, W. M., Gomes, C. P., and Kelling, S. Crowdsourcing meets ecology: Hemispherewide spatiotemporal species distribution models. AI Magazine, 35(2): 1930, 2014. doi: https://doi.org/10.1609/aimag.v35i2. 2533. URL https://onlinelibrary.wiley. com/doi/abs/10.1609/aimag.v35i2.2533. Gal, Y. and Ghahramani, Z. Dropout as bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, 2016. Geifman, Y. and El-Yaniv, R. Selective classification for deep neural networks. In Advances in Neural Information Processing Systems, 2017. Genest, C. and Zidek, J. V. Combining probability distributions: critique and an annotated bibliography. Statistical Science, 1(1):114135, 1986. Ghani, B., Denton, T., Kahl, S., and Klinck, H. Global birdsong embeddings enable superior transfer learning for bioacoustic classification. Scientific Reports, 13(1): 22876, 2023. Berger, A. L., Della Pietra, S. A., and Della Pietra, V. J. Maximum Entropy Approach to Natural Language Processing. MIT Press, 1996. Hagiwara, M. Aves: Animal vocalization encoder based on self-supervision, 2022. URL https://arxiv.org/ abs/2210.14493. Bishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006. Chen, S., Wu, Y., Wang, C., Liu, S., Tompkins, D., Chen, Z., and Wei, F. Beats: Audio pre-training with acoustic tokenizers, 2022. URL https://arxiv.org/abs/ 2212.09058. Choudhary, S., Karthik, C. R., Lakshmi, P. S., and Kumar, S. Lean: Light and efficient audio classificaIn 2022 IEEE 19th India Council Intion network. ternational Conference (INDICON), pp. 16. IEEE, November 2022. doi: 10.1109/indicon56171.2022. 10039921. URL http://dx.doi.org/10.1109/ INDICON56171.2022.10039921. Fink, D., Hochachka, W. M., Zuckerberg, B., Winkler, D. W., Shaby, B., Munson, M. A., Hooker, G., Riedewald, M., Sheldon, D., and Kelling, S. Spatiotemporal exploratory models for broad-scale survey data. Ecological Applications, 20(8):21312147, 2010. 9 Hagiwara, M., Hoffman, B., Liu, J.-Y., Cusimano, M., Effenberger, F., and Zacarian, K. Beans: The benchmark of animal sounds, 2022. URL https://arxiv.org/ abs/2210.12300. Hendrycks, D. and Gimpel, K. baseline for detecting misclassified and out-of-distribution examples in neural networks. In International Conference on Learning Representations, 2017. Heskes, T. Selecting weighting factors in logarithmic opinion pools. Advances in Neural Information Processing Systems, 10, 1998. Hinton, G. E. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771 1800, 2002. Howard, A., Klinck, H., Dane, S., Kahl, S., tom denton, and Denton, T. identification. https://kaggle.com/competitions/ birdsong-recognition, 2020. Kaggle. Cornell birdcall Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Nolasco, I., Singh, S., Morfi, V., Lostanlen, V., StrandburgPeshkin, A., Vidana-Vila, E., Gill, L., Pamuła, H., Whitehead, H., Kiskin, I., Jensen, F. H., Morford, J., Emmerson, M. G., Versace, E., Grout, E., Liu, H., Ghani, B., and Stowell, D. Learning to detect an animal sound from five examples. Ecological Informatics, 77:102258, November 2023. ISSN 1574-9541. doi: 10.1016/j.ecoinf.2023.102258. URL http://dx.doi. org/10.1016/j.ecoinf.2023.102258. Rauch, L., Schwinger, R., Wirth, M., Heinrich, R., Huseljic, D., Herde, M., Lange, J., Kahl, S., Sick, B., Tomforde, S., and Scholz, C. Birdset: large-scale dataset for audio classification in avian bioacoustics, 2025. URL https://arxiv.org/abs/2403.10380. Robinson, D., Miron, M., Hagiwara, M., Weck, B., Keen, S., Alizadeh, M., Narula, G., Geist, M., and Pietquin, O. Naturelm-audio: an audio-language foundation model for bioacoustics, 2025. URL https://arxiv.org/ abs/2411.07186. van Merrienboer, B., Dumoulin, V., Hamer, J., Harrell, L., Burns, A., and Denton, T. Perch 2.0: The bittern lesson for bioacoustics, 2025. URL https://arxiv.org/ abs/2508.04665. Wang, H., Xu, Y., Yu, Y., Lin, Y., and Ran, J. An efficient model for vast number of bird species identification based on acoustic features. Animals, 12(18): 2434, September 2022. ISSN 2076-2615. doi: 10. 3390/ani12182434. URL http://dx.doi.org/10. 3390/ani12182434. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural Computation, 3(1):7987, 1991. Johnston, A., Fink, D., Reynolds, M. D., Hochachka, W. M., Sullivan, B. L., Bruns, N. E., Hallstein, E., Merrifield, M. S., Matsumoto, S., and Kelling, S. Abundance models improve spatial and temporal prioritization of conservation resources. Ecological Applications, 25(7): 17491756, 2015. doi: https://doi.org/10.1890/14-1826.1. URL https://esajournals.onlinelibrary. wiley.com/doi/abs/10.1890/14-1826.1. Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural Computation, 6(2): 181214, 1994. Kahl, S., Wood, C. M., Eibl, M., and Klinck, H. Birdnet: deep learning solution for avian diversity monitoring. Ecological Informatics, 61:101236, 2021. Kendall, A. and Gal, Y. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, 2017. Kittler, J., Hatef, M., Duin, R. P. W., and Matas, J. On IEEE Transactions on Pattern combining classifiers. Analysis and Machine Intelligence, 20(3):226239, 1998. Kuncheva, L. I. Combining Pattern Classifiers: Methods and Algorithms. Wiley, 2004. Lauha, P., Somervuo, P., Lehikoinen, P., Geres, L., Richter, T., Seibold, S., and Ovaskainen, O. Domainspecific neural networks improve automated bird sound recognition already with small amount of local data. Methods in Ecology and Evolution, 13(12):27992810, October 2022. doi: 10.1111/ ISSN 2041-210X. 2041-210x.14003. URL http://dx.doi.org/10. 1111/2041-210X.14003. Madhusudhana, S., Shiu, Y., Klinck, H., Fleishman, E., Liu, X., Nosal, E.-M., Helble, T., Cholewiak, D., Gillespie, D., ˇSirovic, A., and Roch, M. A. Improve automatic detection of animal call sequences with tempoJournal of The Royal Society Interface, ral context. 18(180):20210297, July 2021. ISSN 1742-5662. doi: 10.1098/rsif.2021.0297. URL http://dx.doi.org/ 10.1098/rsif.2021.0297. Miron, M., Robinson, D., Alizadeh, M., GilsenanMcMahon, E., Narula, G., Chemla, E., Cusimano, M., Effenberger, F., Hagiwara, M., Hoffman, B., Keen, S., Kim, D., Lawton, J., Liu, J.-Y., Raskin, A., Pietquin, O., and Geist, M. What matters for bioacoustic encoding, 2025. URL https://arxiv.org/abs/2508.11845. 10 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion Table 2. Model results on large-scale bioacoustic classification datasets: BirdSet (Rauch et al., 2025), CBI (Howard et al., 2020). (Method) Pre: pre-trained classification head, FT: full fine-tuning, LP: linear probe, PP: prototypical probe, 0: zero-shot. (BirdSet) Following along with Perch2.0 (van Merrienboer et al., 2025), we report area under ROC (AU-ROC) and class-mean average precision (cmAP). (BEANS) Similarly, here we record model accuracy (Acc) and mean AP (mAP). For CBI, FINCH uses the external AdaSTEM prior; for BirdSet, FINCH uses learned metadata-only MLP prior trained on the BirdSet training split. Best results highlighted in bold. Second best results underlined. BirdSet CBI"
        },
        {
            "title": "Acc",
            "content": "Audio ProtoPNet-5 BirdMAE-L BirdMAE-L AVES-Bio BirdNet BioLingual NatureLM-Audio Perch 2.0 - Peak-select Perch 2.0 - Random Audio-only (pθ(y x)) Prior-only (pψ(y s)) FINCH (ours) Pre FT PP"
        },
        {
            "title": "Pre",
            "content": "Pre FT LP FT 0.896 0.886 0.886 0.907 0.908 0.833 0.466 0. 0.423 0.440 0.409 0.623 0.601 0.521 0.430 0.619 0.431 0. 0.261 0.027 0.260 0.636 0.461 0.636 FT FT FT"
        },
        {
            "title": "LP\nPP\nLP",
            "content": "LP LP LP 0.598 0.702 0.744 0.778 0.785 0.789 0.792 0.806 0.030 0.826 Appendix A1: Additional Results We report additional experiments that complement the main results and further characterize the behavior of FINCH under alternative contextual models and fusion strategies. BirdSet learned contextual prior. BirdSet does not provide access to an external ecological prior such as eBird Status & Trends. For BirdSet experiments, we therefore learn contextual predictor pψ(y s) directly from spatiotemporal metadata. Specifically, we encode metadata using the same feature representation as in Section 3.3, (cid:104) fmeta(s) = sin(2πd/365), cos(2πd/365), sin(2πh/24), cos(2πh/24), lat 90 , lon 180 (cid:105) , and train small multilayer perceptron (MLP) on the BirdSet training split to predict species labels from fmeta(s) alone. After training, this model is treated as the contextual predictor pψ(y s) and frozen during FINCH fusion training, matching the setting of independently-trained experts. Table 2 reports results on BirdSet subsets across multiple evaluation metrics. Although the metadata-only prior is weak in isolation, adaptive fusion enables FINCH to match or exceed strong audio-only baselines on several subsets, particularly in Top-1 accuracy. These results indicate that FINCH remains effective even when contextual information is learned from limited supervision and exhibits substantial heterogeneity across datasets. Fixed-weight versus adaptive fusion on CBI. We further analyze the effect of fixed fusion weights when combining audio predictions with the eBird spatiotemporal prior on subset of the CBI dataset  (Table 3)  . As the fusion weight ω increases, performance degrades monotonically, reflecting the low informativeness and high entropy of the prior when applied indiscriminately. In contrast, adaptive fusion substantially outperforms all fixed-weight settings by selectively incorporating contextual evidence only when it is informative. This experiment highlights key failure mode of global fusion weights and motivates the use of input-dependent reliability estimation. 11 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion ω Acc mAP 0.0 0.2 0.4 0.8 1.6 2.0 Adaptive 64.28 63.91 64.12 60.22 59.76 57.72 66.10 0.333 0.328 0.332 0.325 0.316 0.311 0.352 Table 3. Ablation study for different values of ω. We report accuracy and mAP for models trained on 1000 samples from CBI. Fixed ω replaces the gating network in Eq. 6 with scalar. (Adaptive) learns to dynamically weight feature evidence. Appendix A2: Decision-Theoretical Safety of Gated Fusion We show that the proposed gated fusion mechanism is decision-theoretically safe in the sense of risk containment: it admits the audio-only classifier as recoverable special case and therefore cannot be forced to rely on misleading contextual information. Let ℓ(y, ˆp) = log ˆp(y) denote the log-loss. Consider the unnormalized fused score log pω(y x, s) = log pθ(y x) + ω(x, s) log pψ(y s), ω(x, s) 0, and define the normalized predictor pω(y x, s) by applying softmax over y. The key structural property of this formulation is recoverability: for any input (x, s), ω(x, s) = 0 = pω(y x, s) = pθ(y x). Thus, the audio-only classifier is always contained within the hypothesis class induced by gated fusion. As consequence, for any data-generating distribution and any loss function evaluated pointwise in (x, s), the optimal choice of ω(x, s) satisfies inf ω(x,s)0 E(cid:2)ℓ(cid:0)y, pω( x, s)(cid:1)(cid:3) E(cid:2)ℓ(cid:0)y, pθ( x)(cid:1)(cid:3) . In particular, whenever the spatiotemporal predictor pψ(y s) is uninformative or misleading for given input, the choice ω(x, s) = 0 achieves the same expected risk as the audio-only model. Conversely, if contextual information provides non-negative information gain for subset of inputs, then there exist inputs (x, s) for which ω(x, s) > 0 strictly improves expected log-loss relative to the audio-only baseline. The adaptive gating network learns data-driven approximation to this selective incorporation rule. Thus, adaptive gated fusion strictly generalizes both audio-only and fixed-weight fusion while preserving safe fallback. Appendix A3: When Log-Linear Fusion is Exact (Conditional Independence Case) We justify the log-linear fusion rule and clarify its relationship to joint audiocontext models. This analysis is intended to clarify the conditions under which log-linear fusion is theoretically exact; it does not assume that these conditions hold universally in practice. Assume that audio features and spatiotemporal context are conditionally independent given the class label y, i.e., By Bayes rule, the posterior satisfies p(x, y) = p(x y) p(s y). p(y x, s) = p(x, y) p(y) p(x, s) p(x y) p(s y) p(y). Rewriting this expression in terms of the available predictors, p(y x) = p(x y)p(y) p(x) , p(y s) = p(s y)p(y) p(s) , 12 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion we obtain p(y x, s) p(y x) p(y s) p(y) . Equivalently, up to an additive constant independent of y, log p(y x, s) = log p(y x) + log p(y s) log p(y) + const. Effect of Conditional Dependence. If conditional independence is violated, the joint distribution can be written as p(x, y) = p(x y) p(s y) κy(x, s), κy(x, s) := p(x, y) p(x y)p(s y) ."
        },
        {
            "title": "The posterior becomes",
            "content": "p(y x, s) p(y x) p(y s) p(y) κy(x, s), or, in log form (up to class-independent constant), log p(y x, s) = log p(y x) + log p(y s) log p(y) + log κy(x, s) + const. Crucially, log κy(x, s) depends explicitly on and therefore cannot, in general, be represented by class-agnostic scalar reweighting of log p(y s). Implications for Model Design. When conditional dependence is strong, accurately capturing κy(x, s) requires joint predictor that ingests (x, s) together. Such joint models are strictly more expressive but require paired supervision and explicit learning of audiocontext interactions. In contrast, when conditional independence approximately holds, log-linear fusion of independently trained predictors is sufficient and (up to normalization) optimal. FINCH is motivated by this regime: it preserves the posterior form implied by independence while introducing adaptive, reliability-aware weighting to account for model mismatch and heterogeneous uncertainty. Appendix A4: Empirical Test of Conditional Dependence The theoretical justification above relies on conditional independence assumption. We now empirically assess the extent to which this assumption holds in practice. Fixing class label y, we test whether audio embeddings e(x) contain information about spatiotemporal context statistics by training linear predictor ˆs = (e(x)) to predict spatiotemporal priors from audio embeddings alone. As baseline, we compare against constant predictor that always outputs the mean prior. Across 43 species with sufficient samples, the linear predictor does not consistently outperform the baseline on held-out data. While permutation test indicates that the observed differences are unlikely to arise purely by chance (p < 103), effect sizes are small (Cohens = 0.261), and explained variance is negligible or negative (mean R2 = 0.34). Only 16.3% of classes exhibit positive improvement over the baseline, and fewer than 12% achieve R2 > 0.05. Taken together, these results suggest that audio and spatiotemporal information are not strictly conditionally independent, but their dependence is weak and heterogeneous across classes. These findings support the use of adaptive gating: while conditional independence is not exact, deviations are sufficiently weak and variable that bounded, sample-dependent fusion mechanism is preferable to fixed or fully joint models. Appendix A5: Spatiotemporal Prior Details The spatiotemporal prior is based on weekly relative abundance estimates produced by the eBird Status and Trends project. These estimates are generated using Adaptive Spatio-Temporal Exploratory Models (AdaSTEM), an ensemble of localized regression models designed to account for heterogeneous sampling effort and detection bias in large-scale citizen-science data (Fink et al., 2010; 2014; Johnston et al., 2015). The abundance maps are provided on global 27 km 27 km grid at weekly resolution. Each abundance value represents the expected number of 13 Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion individuals detected by an expert observer at specific location and time. Raw values are stored as unsigned integers in the range [0, 255] and linearly rescaled to [0, 1]. To avoid introducing training-time bottleneck, we precompute spatiotemporal prior lookup table for all training samples. For each audio clip with metadata (lati, loni, datei), we query the corresponding weekly abundance prior p(yj lati, loni, datei) for all species yj. The resulting matrix of shape (Nsamples, Nspecies) is stored in an HDF5 file and indexed by sample ID. During training, batches retrieve prior values via indexed lookup, yielding an average access time of 0.22 µs, compared to approximately 0.01s for on-the-fly querying. At inference time, priors are queried directly from the abundance maps, as latency is not performance-critical. Appendix A6: Training Configuration and Optimization Details All training stages share the same optimization and training configuration unless otherwise noted. Optimization is performed using the AdamW optimizer with learning rate 103 and weight decay 102. Models are trained for 30 epochs with batch size of 96. We construct stratified validation split comprising 10% of the training data, stratified by species label. Model selection is performed based on validation accuracy. cosine learning rate schedule with linear warmup over the first 10% of training steps is used. Training is performed on GPU using mixed-precision arithmetic (bfloat16). Checkpoints are saved after every epoch. For each training stage, the checkpoint achieving the highest validation accuracy is retained for evaluation. Stage 2 is initialized from the best-performing Stage 1 checkpoint, and Stage 3 is initialized from the corresponding Stage 2 checkpoint. The adaptive gating network is initialized such that its output matches the fixed scalar fusion weight learned in Stage 2. This staged initialization stabilizes training. Throughout all stages, the audio encoder remains frozen. In Stages 2 and 3, the audio classification head is also frozen, and only fusion parameters are updated. All experiments are implemented in PyTorch. Random seeds are fixed across runs to ensure reproducibility. Unless explicitly stated, no additional data augmentation is applied beyond that inherent to the underlying audio encoder. Table 4. Summary of fusion model architectures and training configuration. Parameter counts assume = 264 species. Component Stage Stage Audio Encoder Encoder Architecture Encoder Output Dim. Audio Classifier Temperature Parameter Epsilon Parameter Scalar Weight Gating Network Frozen Frozen BEATs (NatureLM-Audio) 4096 4096 + 4096 + 1 1 1 1 1 2, Total Trainable Params 1.08M 1.09M Optimizer Learning Rate Batch Size Epochs LR Schedule Precision AdamW 1 103 96 30 Cosine w/ warmup bfloat"
        }
    ],
    "affiliations": [
        "1",
        "2"
    ]
}