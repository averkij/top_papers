{
    "paper_title": "MMPB: It's Time for Multi-Modal Personalization",
    "authors": [
        "Jaeik Kim",
        "Woojin Kim",
        "Woohyeon Park",
        "Jaeyoung Do"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting a different key property of VLMs. Using 23 widely used VLMs including both open- and closed-source models, we evaluate personalization performance via a three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering a scalable benchmark, MMPB offers valuable insights and a solid foundation for future research toward truly personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 0 2 8 2 2 . 9 0 5 2 : r MMPB: Its Time for Multi-Modal Personalization Jaeik Kim1 Woojin Kim2 Woohyeon Park2 Jaeyoung Do1,2 AIDAS Laboratory, 1IPAI & 2ECE, Seoul National University {jake630, wjk9904, woohyeon, jaeyoung.do}@snu.ac.kr"
        },
        {
            "title": "Abstract",
            "content": "Visual personalization is essential in user-facing AI systems such as smart homes and healthcare, where aligning model behavior with user-centric concepts is critical. However, recent large Vision-Language Models (VLMs), despite their broad applicability, remain underexplored in their ability to adapt to individual users. In this paper, we introduce MMPB, the first extensive benchmark for evaluating VLMs on personalization. MMPB comprises 10k image-query pairs and includes 111 personalizable concepts across four categories: humans, animals, objects, and characters, with the human category enriched with preference-grounded queries. We structure personalization into three main task types, each highlighting different key property of VLMs. Using 23 widely used VLMs including both openand closed-source models, we evaluate personalization performance via three-stage protocol: concept injection, multi-turn dialogue, and personalized querying. Our findings indicate that most VLMs (including some closed-source models) struggle with personalization, particularly in maintaining consistency over dialogue, handling user preferences, and adapting to visual cues. Our analysis reveals that the challenges in VLM personalization (such as refusal behaviors and long-context forgetting) highlight substantial room for improvement. By identifying these limitations and offering scalable benchmark, MMPB offers valuable insights and solid foundation for future research toward truly personalized multi-modal AI. Project Page: aidaslab.github.io/MMPB"
        },
        {
            "title": "Introduction",
            "content": "Now we need VLM personalization Our daily lives are filled with recurring visual concepts that are closely tied to us. Every day, you might spot your dog waiting by the door or notice your favorite coffee mug on the table. However, they are rarely understood in user-specific context by recent large Vision-Language Models (VLMs). Although VLMs have been widely adopted as assistants that utilize broad knowledge of the world [92, 110, 99, 53], they still operate largely under the one-size-fits-all paradigm [1, 79], treating all users as interchangeable and responding to visual inputs without adapting to individual identities, preferences, or histories. As VLMs are integrated into multi-modal systems like smart home devices and robots [25, 43, 106], they increasingly serve as the core interface for grounding user instructions in the visual world. To work effectively in everyday scenarios, they must recognize and align with each users visually repetitive environment and preferences. Such personalization is essential in applications where consistent interpretation of personal context is critical [65, 2, 3]. For example, instead of responding generically to feed the gray tabby cat, the model should understand personalized commands like feed Mochi. Similarly, it should interpret photo as your favorite travel destination rather than snow-covered mountain. The evaluation gap in VLM personalization Despite the growing importance of AI personalization in many real-world scenarios [23, 10, 24], current benchmarks fall short in evaluating the personalization capabilities of VLMs. Existing VQA datasets focus on general-purpose knowledge, including commonsense [96, 58, 14], scientific [35, 60, 59], and medical reasoning [8, 54, 46]. De39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: Examples of personalized queries across task types and representative failure cases of indicates LLaVA family models such as LLaVA-NeXT. recent VLMs. indicates GPT-4o, while spite some early progress [65, 2], personalization in VLMs remains constrained, with limited coverage and diversity of personalizable concepts  (Table 1)  . In addition, the absence of unified evaluation framework (both in task types and metrics) and overlooking the cold-start nature, where models must personalize with minimal prior context, make it difficult to systematically assess personalization. Finally, preference-grounded VQA, which requires reasoning over user-specific likes and dislikes, remains especially underexplored, posing unique challenges beyond standard VLM tasks. To bridge this gap, we introduce MMPB (Multi-modal Personalization Benchmark), the first benchmark for evaluating VLM personalization. MMPB evaluates concept recognition and preferencegrounded reasoning using images associated with preference-related keywords (Figure 1). Our dataset is built via humanmodel collaborative process guided by annotation protocols, which specify task types and structured query processing. To ensure high quality, we remove any queries solvable using only concept information or only query images, thus focusing evaluation on genuine cross-modal personalized reasoning. Consequently, MMPB comprises 111 concepts spanning four categories: humans, animals, objects, and characters, paired with five reference images and four level textual descriptions. For the human preferences, we curate from 30 diverse subdomains of personal preferences. Queries are categorized into three task types, enabling detailed analysis of personalization failures in VLMs. Finally, we include multi-turn dialogues to evaluate the models ability to retain personalized concepts over extended interactions. We evaluate 23 widely used VLMs on MMPB, including closed-source models such as GPT-4o [1] and Claude-Sonnet [7]. Despite their strong performance on general-purpose VQA tasks, these VLMs exhibit significant limitations and challenges in personalization. Our key take-home messages include: (i) Even top-ranked VLMs on general benchmarks struggle with preference-grounded tasks, underscoring the need for more robust forms of inference, including abductive reasoning (5.1). (ii) VLMs struggle with visual personalization. For example, comparable performance from one image and three text keywords highlights persistent inability to leverage fine-grained visual cues despite the central role of image-based personalization (5.4). (iii) Closed-source models tend to avoid personalization involving human-centric concepts, stemming from safety alignment constraints. This implies that existing safety constraints hinder personalization, highlighting the need to reconsider the balance between safety and personalization (5.2). We further conduct systematic analysis of VLM failure patterns, focusing on personalization bias and failures in discrimination (5.3), with errors amplified in long-context scenarios where mid-sequence concepts are frequently overlooked (5.5). Our main contributions are as follows: We introduce MMPB, the first comprehensive benchmark for evaluating VLM personalization in both recognition and preference-grounded VQA. It includes 111 personalizable concepts and 10,000+ questions with 15 task types that reflect real-world scenarios. By providing five reference images and four levels of textual descriptions, MMPB establishes foundation not only for evaluation but also for future advanced techniques that leverage them (e.g., post hoc training) thereby enabling fine-grained VLM personalization. By rigorously evaluating widely adopted VLMs, we identify key personalization failures such as limited preference-grounded reasoning, modality gaps, personalization bias, and safety-induced evasiveness, which establish MMPB as key diagnostic tool for improving personalized VLM behavior. Table 1: Comparison of existing multimodal personalization benchmark datasets, indicating whether each dataset supports systemic evaluation (SysEval), human preference (Pref ), multiple levels of granularity (Gran), and multi-turn interactions (Multi-turn). #Concept and #Samples denote the number of distinct concept categories and total samples, respectively. Dataset MyVLM [2] YoLLaVA [65] MC-LLaVA [3] MMPB (Ours) SysEval Pref Gran Multi-turn #Concept #Samples 29 40 95 111 0.3K 0.6K 2.0K 10.0K"
        },
        {
            "title": "2 Problem Definition",
            "content": "2.1 Core Properties of Personalized VLMs To systematically evaluate personalized VLMs, we formalize four core properties we call personalization criteria. Personalizable concepts are user-centric entities (e.g., me, my dog) whose attributes, such as appearance or preferences, can be encoded in either structured or unstructured form. Formally, let = { p1, . . . , pK} denote all personalizable concepts. VLM successfully augmented with any pk should satisfy the following criteria, with examples in Figure 1: 1. Awareness: Can the model accurately identify pk in given image? 2. Appropriateness: Does the model activate pk only when it is contextually appropriate? 3. Coherency: Does the model contradict pk in its responses? 4. Persistency: Can the model consistently respond based on pk across long-context or multiturn interactions? 2.2 Formalizing VLM Personalization standard VLM is typically defined as function : Y, where is the pixel space, the text space (e.g., user queries), and the output space (e.g., generated text). To personalize M, we extend it to function fp : Y, incorporating concept pk P. key objective of personalization is to tailor outputs to user-specific contexts while preserving the general-purpose knowledge acquired during pretraining. One approach is to concatenate pk with the input modalities and , without modifying the models weights. Given projection function that embeds pk, and decoder g, the personalized output is: = fp(x, t, pk) = g(cid:0)EI (x) ET (t) h(pk)(cid:1), where EI and ET denote the image and text encoders, with as concatenation. MMPB evaluates VLMs by injecting pk via either textual or visual modalities, using reference images or descriptions as in-context prompts [22, 32], usingh = EI for visual and = ET for textual concept injection."
        },
        {
            "title": "3 MMPB: Multi-modal Personalization Benchmark",
            "content": "3.1 Overview MMPB is the first benchmark for personalizing multi-modal assistant, featuring multiple-choice visual question answering (VQA) tasks focused on recognition and user preference. It comprises 10,017 imagequery pairs across 111 concepts spanning four categories, covering three task types: Awareness, Appropriateness, and Coherency. To support future expansion, MMPB is built via three-step humanmodel collaboration (Figure 2). MMPB evaluates VLMs across three stages: (1) Concept injection, (2) Multi-turn conversations, and (3) Personalized querying. In the first stage, concept is introduced via reference images or textual descriptions. During the conversation phase, the model engages in general multi-turn dialogue to test concept retention. Finally, personalized queries assess whether the model can apply the concept to visual input. Detailed dataset construction guidelines and statistics are provided in Appendix and E.2. 3 Figure 2: Overview of MMPB. (top) three-step construction process ensuring high quality and scalability. (bottom) An evaluation protocol for assessing the VLMs personalization criteria. 3.2 Data Collection Concepts MMPB provides five reference images per concept. Human annotators, following detailed guidelines (Appendix C.1.1), sourced these images from open datasets (MC-LLaVA [3], MyVLM [2]) and CC-licensed platforms (Flickr, Pexels), and recorded original URLs for any externally retrieved images (e.g., kmDB or Google Image Search). MMPB assumes static concept states without modeling temporal variation such as appearance changes or evolving user preferences. For consistency, most human concepts use MC-LLaVA [3] movie portraits, ensuring uniform character style. Other concepts use multiple images of the same instance (e.g., pink-hat Asian doll). To facilitate concept injection through the textual modality, we generate appearance descriptions for each concept via humanmodel collaboration, using five reference images per concept. We employ Ovis2-34B [61] for its strong multi-image captioning capabilities [20] and practical scalability, as it can be run efficiently on single 80GB GPU. The model-generated descriptions are then verified and refined by human annotators based on guidelines (Appendix C.1.2). Each concept is described across four levels of granularity: (i) Simple (three keywords), (ii) Moderate (one sentence), (iii) Detailed (one paragraph), and (iv) Extended (structured multi-paragraphs). To avoid pretraining leakage and ensure fair evaluation, all concept names are replaced with <sks>, following prior works [65, 75]. For human preferences, we define five domains, each comprising six subdomains (30 in total; see Figure 12 in Appendix E.2). Using GPT-4o [1], we extract 3040 representative keywords per domain. Since providing multi-image preferences for every domain incurs substantial input overhead, we instead express them textually. Each human is assigned likes and dislikes via template: <sks> likes {keywords} but dislikes {keywords}. Conflicting preferences (e.g., liking gaming keyboards but disliking technical gadgets) are resolved by human annotators. Multi-turn conversations To emulate realistic back-and-forth dialogue, we sample general-topic conversations from LMSYS-Chat-1M [107] following previous studies [105, 51, 5] and strip any concept-related lines to avoid conflicts (Appendix C.1.4). Images for personalized query For personalized query images for recognition, we follow the image collection guidelines, ensuring that injection and evaluation images are strictly disjoint. For preference-grounded VQA, human annotators collect images from approved sources associated with 30 predefined subdomains, ensuring relevance and diversity within each domain (Appendix C.1.5). 3.3 Query Design Overall process The query design process consists of two main stages (Figure 2): (1) generation stage, where model drafts candidate queries and human annotators refine them to produce initial 4 query sets; and (2) quality evaluation stage, where models first flag suspected low-quality queries, and human annotators validate the flags and review all queries. We use Ovis2-34B [61] in the generation stage, and Ovis2-34B, InternVL2.5-78B [19], and Qwen2.5-VL-72B [6] in the evaluation stage. Although MMPB is designed as multiple-choice VQA, all queries are easily convertible into open-ended formats (Appendix Table 10). As in 3.2, all concept names are replaced with <sks>. Categories For query categorization, we follow the Personalization Criteria (2) and define three types of queries: (1) Awareness, (2) Appropriateness, and (3) Coherency. Each query type is applied to recognition tasks across all concept categories, and to preference-grounded tasks for human concepts, resulting in 15 evaluation tasks. The Awareness type tests whether the model can detect the presence of personalized concept in an image. Positive images are used, where the correct answer to queries like Is <sks> in the image? should be yes. We further distinguish between single-entity and multi-entity cases, based on whether other entities co-appear with the concept. The Appropriateness type evaluates whether the model can correctly suppress references to personalized concept when it is not contextually appropriate. Negative images are used, and the correct answer should be no. For animal concepts, negative samples are further categorized into same-species and different-species examples to analyze the impact of hard negatives. This is feasible because species boundaries are visually and semantically well-defined. The Coherency type assesses whether the model can produce coherent and context-appropriate responses about the concept (e.g., What clothes is <sks> wearing?). We also evaluate Persistency by introducing multi-turn conversations. Quality control We aim to filter out options that trigger positional or affirmative biases or are solvable by text alone [58, 72, 104]. Throughout query generation and evaluation, we follow task-specific guidelines to ensure high quality1. For the Awareness and Appropriateness types, we instruct both the model and human annotators to generate queries targeting concept presence in each image. To mitigate potential yes/no bias [104], we include both affirmative and negative formulations (e.g., Is <sks> not present in this photo?). In the Coherency type, we adopt 4-option MCQ format, addressing two major confounding factors: (i) concept-only solvability and (ii) image-only solvability. As shown in Figure 3, at least one distractor (e.g., \"Yoga\") is aligned with the <sks> but not with the image, preventing concept-only solvability. Other distractors (e.g., \"Watching football\") are visually plausible but incorrect without considering the concept, addressing image-only solvability. This design encourages joint reasoning over both the image and the concept, with choices shuffled to eliminate positional bias [72]. Blind tests with text-only experiments are provided in Appendix Table 9, further supporting our rigorous query design. Figure 3: Example of quality control for Coherency-type query with concept-only and image-only distractors."
        },
        {
            "title": "4 Benchmarking Results",
            "content": "4.1 Experimental Setup Configuration To simulate real-world personalization challenges, we design our experiments centered on two key constraints: cold-start and multi-turn adaptation. For the cold-start problem [105, 95] (i.e., initiating personalization with minimal prior information), we utilize moderate description for text-based injection, and two reference images for image-based injection with single concept injection. We also evaluate both in 0-turn (i.e., without multi-turn conversation) and 10-turn conversation settings. We ensure that the 10-turn conversation is applied consistently across all models. We follow prior multiple-choice VQA tasks [73, 4, 111] and use overall accuracy as our main evaluation metric. Models We select models to cover major VLM families following previous benchmarks [27, 13], based on public availability, and widespread usage. We evaluate 23 models across openand closed1Each query is reviewed by at least three human annotators, and only queries with majority agreement (i.e., at least 2 out of 3 annotators) are accepted. Please see the guidelines in Appendix C.2.2 and C.3.2. 2Hugging Face Open VLM Leaderboard, accessed on May 15, 2025. 5 Figure 4: Evaluation results of 23 VLMs on MMPB under 0-turn and 10-turn settings. Model names are followed by their average ranks across eight general-purpose multi-modal benchmarks2. Table 2: Task-wise performance of open vs. closed models, averaged over image/text-based injections. Model Open Closed Open Closed Turn 0 0 10 10 Preference Human 52.8 5.9 37.0 18.0 46.0 5.1 34.7 15. Recognition Human 74.3 11.1 68.9 21.0 69.0 12.4 61.7 16.0 Animal 69.2 13.7 73.4 9.0 69.8 8.3 66.2 10.6 Object 75.7 18.1 79.1 16.0 72.0 14.0 67.3 19.0 Character 67.3 11.5 65.3 17.4 61.6 10.9 56.9 15.9 source families: open-source includes InternVL2.5 (878B) [19, 18, 17], Ovis2 (834B)3 [61], LLaVA (1372B) [55, 56, 50], Qwen-VL (772B) [84, 6], DeepSeek-VL2 [90], and Llama-3.2-11B [28]; closed-source includes Claude-3.5/3.7-Sonnet [7], Gemini-1.5/2.0-Flash [79], and GPT-4o [1]. 4.2 Overall Results Figure 4 shows overall personalization performance on MMPB across 23 widely used VLMs, with the human evaluator achieving an average of 92.1%, thereby establishing the upper bound (Appendix D). Across all experiments, text-based concept injection consistently outperforms imagebased injection, achieving average accuracies of 63.8% vs. 57.8% in the 0-turn, and 57.9% vs. 52.6% in the 10-turn setting. The difficulty becomes more pronounced under extended interactions. Most models experience substantial performance drop when moving from the 0-turn to the 10-turn setting, indicating challenges in maintaining personalized responses over time. Notably, averaged across all experimental settings, closed models underperform open ones (51.4% vs. 59.9%) despite strong results on general VQA benchmarks [12, 97]. Table 2 presents taskand concept-level performance comparisons between model types. Overall, both model types exhibit pronounced multi-turn degradation. To further examine whether MMPB genuinely measures personalization rather than merely reflecting general VLM strength, we conducted additional experiments on LLaVA-1.5-13B with two personalization strategies: (i) soft prompt tuning following the Yollava [65] approach, and (ii) personalized LoRA fine-tuning [102]. As detailed in Appendix F.1, both methods lead to clear improvements over the baseline, with LoRA providing the strongest gains. Overall, these results validate that MMPB is sensitive to personalization techniques and effectively captures personalization performance beyond general VLM ability."
        },
        {
            "title": "5 Challenges in VLM Personalization",
            "content": "The results presented in 4.2 demonstrate that, overall, recent VLMs remain highly vulnerable to personalization tasks. To systematically investigate which challenges hinder effective handling of user-specific queries, we organize our analysis around the following key questions: Which specific tasks pose barriers to effective VLM personalization? (5.1 and 5.3) What makes closed-source models struggle with personalization? (5.2) 3To ensure the model does not benefit from self-generated content, we confirm that Ovis2-34B underperforms on its own generated descriptions compared to human-written Moderate descriptions (Appendix F.3). 6 Figure 5: Performance gap between preferencegrounded and recognition VQA tasks in VLMs. Figure 6: Refusal counts of models across task and concepts under image-based injection. Table 3: Examples of evasive responses in human category tasks. Model Example GPT-4o Claude-3.7-Sonnet Claude-3.5-Sonnet InternVL2.5-78B Unknown Based on the image, can only describe whats visible... cannot and should not identify or name specific features of the person... cant identify or recognize people in images. How do injection modality and granularity impact VLMs? (5.4) Can VLMs follow specific concept in multi-concept or long-context settings? (5.5) 5.1 Preference-Grounded Reasoning Personalized VLMs should be able to infer user preferences from images and incorporate them into its responses. We split the evaluation into recognition and preference tasks and order their accuracies by average relative rank within general VQA, which comprises eight multi-modal reasoning benchmarks, including MathVista [60], MMMU [97], and MMBench [58]. As shown in Figure 5, models that perform well on general tasks consistently struggle with preference-grounded queries. This disparity highlights key limitation of current VLMs: their inability to generalize from general-purpose reasoning to user-centric inference. In contrast, recognition tasks show stronger correlation with rankings, underscoring the unique difficulty posed by preference-based personalization. Difference between recognition and preference-grounded VQA VLMs typically solve VQA tasks through three-stage process: image perception, knowledge grounding (either pretrained or retrieved via RAG [48]), and answer generation. In MMPB, recognition tasks follow this pipeline with minimal uncertainty, using deductive reasoning on injected concepts via in-context learning. For example, given Is <sks> in this picture?, the model matches visual features and answers Yes. By contrast, preference-grounded VQA demands abductive reasoning, integrating user intentions and tastes with scene understanding, e.g., in Which activity here do you think <sks> would enjoy most? Current VLMs primarily focus on deductive tasks [55], which may be insufficient for ensuring user satisfaction in real-world scenarios. Future personalization frameworks may consider training VLMs to perform abductive reasoning, e.g., via instruction tuning on preference-based tasks. 5.2 Resistance to Personalization As shown in Table 2, closed-source models consistently underperform on human-related queries. Notably, InternVL2.5-78B, which ranked first among 23 models on general VQA benchmark, also exhibits weak personalization performance, both in the textand imagebased setting (Figure 4). We find that this is the only model in the InternVL family that is publicly accessible via web-based chatbot, which may introduce additional safety filters that affect its behavior. To investigate their failures in detail, we examine their responses and find consistent pattern of evasive behaviors. To quantify this behavior, we count answers matching well-known refusal patterns in LLMs [11, 74, 85, 47, 91], such as Im sorry, shouldnt, and Unknown.. As shown in Figure 6, all closedsource models, except Gemini, exhibit evasive responses in recognition and preference-grounded tasks. InternVL2.5-78B does so only in recognition. Notably, most evasive cases fall under the 7 Figure 7: Personalization bias across models, measured as AppropriatenessAwareness. Positive values indicate under-personalization (missed valid personalization), and negative values indicate over-personalization (affirming inappropriate inputs). human category: out of 7,501 queries, up to 2,237 trigger evasive answers, suggesting that this behavior significantly contributes to suboptimal performance. Table 3 presents representative freeform responses, aligning with refusal patterns observed when LLMs are prompted with harmful queries [70]. Interestingly, with the upgrade from Claude-3.5-Sonnet to 3.7, the model increasingly produces descriptive outputs rather than issuing explicit refusals. Since these responses do not clearly match standard refusal patterns, we exclude them from the count. Further analysis (Appendix Box 25 and 26) shows that closed-source models treat identity-related queries as privacy-sensitive, resulting in conservative behavior aimed at avoiding unsafe outputs. Personalization vs. Safety: Can they coexist? This tradeoff between personalization and safety raises important questions for the AI community. Human perception is critical component of user-level personalization, particularly in tasks that involve grounding visual inputs to individual users. While safety constraints are essential to prevent misuse, an overly cautious stance can hinder meaningful personalization, where distinguishing between users is core requirement. This tension calls for deeper discussion around the boundary between safety and utility, and highlights the need for methods that enable secure yet effective handling of identity information in VLMs. 5.3 Personalization Bias and Fine-Grained Discrimination Under-personalized bias Figure 7 visualizes the personalization bias of models, measured as the difference between Appropriateness and Awareness. In both the 0-turn and 10-turn settings, models consistently lean toward rejecting personalization, performing better on Appropriateness than on Awareness, with 72 out of 92 cases falling in the under-personalization region. This indicates systematic tendency to reject personalized concepts rather than affirm them. The bias is more pronounced in the 10-turn setting, suggesting that extended dialogue reinforces this behavior. Impact of multiple entities and hard-negatives Figure 8a compares performance on singleand multi-entity query images. The models exhibit comparable Awareness scores, suggesting that concept detection remains stable. However, Coherency drops substantially in response to multi-entity inputs, indicating difficulty in maintaining reasoning consistency as input complexity increases. To evaluate fine-grained Appropriateness, we assess whether the models can distinguish target concepts from visually similar distractors in the animal category, using same-species (e.g., BeagleBeagle) and different-species (e.g., BeagleShiba) instances as negative samples. As shown in Figure 8b, textbased injection results in substantial performance drop for hard negative samples, highlighting the challenges in making fine-grained distinctions. In contrast, image-based injection delivers robust performance, suggesting that visual cues offer stronger support for fine-grained discrimination. 5.4 Visual Personalization We conduct an in-depth analysis of how injection modality and content granularity affect model performance by comparing four levels of text granularity and three levels of image-based injection. We evaluate six representative models, including Qwen2/2.5-VL-7B, Ovis2-8/16B, LLaVA-OV-7B, and InternVL2.5-8B-MPO, all of which support multi-image input. Results are presented in Table 4. (a) (b) Figure 8: Effects of image complexity: (a) Multiple entities on Awareness and Coherency tasks. (b) Hard-negative samples on Appropriateness task. The error bars denote the first and third quartiles. Table 4: Recognition accuracies under different granularity levels of concept injection. Turn 0 10 Simple 68.8 5.2 67.4 4. Text-based Injection Detailed Moderate 79.3 6.7 77.1 6.7 73.5 7.9 72.0 7.2 Extended 78.3 6.8 72.1 8.2 Image-based Injection 2 images 72.2 9.9 69.2 8.4 5 images 73.3 10.5 70.1 9.4 1 image 71.5 8.3 67.9 6.2 few words are worth thousand pixels We highlight distinct trends in recognition performance across modalities. Surprisingly, Simple textual injections, consisting of only three keywords, achieve comparable accuracy to the 1-image condition in 10-turn, suggesting that the use of single image doesnt provide much more benefit than minimal text alone. Moreover, even with five reference images, the performance does not exceed that of Moderate text injection, highlighting the limited utility of visual examples in recent VLMs. Given the central role of visual personalization in future VLMs, further research should explore strategies that effectively harness these visual cues for personalization. 5.5 Lost Concepts in Haystack Prior work shows that VLMs struggle with the needle-in-a-haystack problem, failing to retrieve key information from long contexts [83, 16, 34]. Using six models in 5.4, we evaluate concept retrieval in diverse long-context settings. Across all settings, personalization failures stem less from input design, and more from fundamental limits in long-range memory and relevance filtering. We begin with 10-turn dialogues in which concepts are injected at different positions (Figure 9a). We also apply Reminder strategy [105, 88], which cues concept recall in the final turn (Appendix F.6). As result, regardless of the prompting methods, models often forget concepts located near the midpoint, reflecting the lost-in-the-middle effect [89, 57, 37, 103] and indicating positional bias in attention. We also evaluate multi-concept inputs with up to 50 entities and varying description granularity in the 0-turn setting, placing the target concept near the middle (e.g., 6th of 10). In Figure 9b, the accuracy consistently decreases, with sharper drops observed in more detailed descriptions. These findings similarly indicate that VLMs struggle to isolate relevant information embedded mid-context, particularly when multiple entities are present. In extended dialogues of up to 100 turns (Figure 9c), performance for both textand image-based inputs deteriorates sharply after 5 turns, reaffirming that VLMs context-tracking ability degrades significantly with increased input length [105]. (a) (b) (c) Figure 9: VLMs under long-context scenarios. (a) Concept most lost midway conversation. (b) Multiple concept injections on text-based injection. (c) Extended multi-turn dialogue up to 100 turns."
        },
        {
            "title": "6 Related Works",
            "content": "Large Vision-Language Models Large language models (LLMs) [67, 63, 40, 9, 94, 41, 80, 68, 81], built on extensive world knowledge, have grown rapidly across diverse domains. Leveraging this progress, large VisionLanguage Models (VLMs) [21, 52, 98, 26, 93, 101, 110, 79, 61] have emerged to connect visual and linguistic modalities. By seamlessly bridging vision and language, VLMs have revolutionized AI, driving breakthroughs in tasks such as visual question answering [30, 62, 78, 87, 97], image captioning [45, 42, 49, 82], and optical character recognition (OCR) [64, 60]. Despite their potential as interfaces connecting AI systems with individual users in real-world scenarios, current benchmarks lack the means to evaluate the personalization capabilities of VLMs. To address this gap, we propose MMPB, the first benchmark designed for VLM personalization, establishing foundation for future research in personalized visionlanguage modeling. Unlike conventional VQA or captioning datasets, our benchmark highlights the user-dependent dimension of multi-modal reasoning, which has been largely underexplored in prior work. Personalization in AI systems. As AI systems become more widely adopted across various domains such as AI assistants (e.g., psychological counseling [109], smartphone [86], housekeeping [31], medical assistance [100], and GUIs [36]), personalization has become crucial for enhancing usability and user satisfaction. In the context of large language models (LLMs), various techniques such as prompting [77, 76, 112], fine-tuning [66, 102, 108], and alignment [44, 15, 39] have been explored. In contrast, while large Vision-Language Models (VLMs) [1, 79] have demonstrated strong performance on multi-modal tasks [58, 14, 69, 38], their personalization capabilities remain underexplored. Existing approaches [65, 2] primarily focus on prior adaptation to personalizable concepts while overlooking cold-start scenarios, thereby limiting their applicability in real-world settings. Distinct from prior work, MMPB is the first to systematically evaluate personalization in VLMs through hierarchical concept injection, enabling structured assessment of how models internalize and maintain user-specific concepts across varying levels of interaction complexity."
        },
        {
            "title": "7 Discussion",
            "content": "In this paper, we introduce MMPB, the first benchmark for large Vision-Language Model (VLM) personalization, covering diverse personalizable concepts and task types, including preference-grounded VQA. Our human-in-the-loop dataset construction highlights its potential for future extensions, while our query filtering strategy ensures data consistency and quality. Extensive analysis shows that recent VLMs remain suboptimal across several personalization dimensions: limited preference-grounded reasoning, tendency toward refusal, and ineffective use of visual cues. Although we focus on static appearance and preferences, future work should explore dynamically evolving user traits. Also, while we focus on VQA concerning concept presence and preference following, future benchmarks should cover application-driven tasks like personalized captioning and personalized robots. We hope MMPB encourages further progress in VLM personalization, accelerating the development of human-centric, real-world applications. Limitations MMPB focuses on evaluating VLMs ability to recognize and reason about finegrained, static appearances and preferences. In practice, however, personal appearance and inclinations are rarely fixed: people change hairstyles, update their fashion, and see their own tastes evolve over time. As the first extensive benchmark for VLM personalization, MMPB establishes solid foundationbut we encourage future work to introduce dynamic concept updates (e.g., style changes, evolving preferences) so that models can be assessed on their ability to track and adapt to real-world, time-varying identities. second limitation concerns the scope of our evaluation framework. In this work, we defined three core propertiesAwareness, Appropriateness, and Coherencyto capture key failure modes of personalized VLMs (alongside Persistency with multi-turn conversations). Consequently, MMPBs assessments are necessarily formalized around these properties, rather than downstream applications. Yet personalized VLMs hold great promise for tasks such as tailored image captioning or personalized actions in robots that bridge individuals and AI. We anticipate that future benchmarks will build on MMPB by measuring model performance in these real-world personalization scenarios."
        },
        {
            "title": "References",
            "content": "[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Y. Alaluf, E. Richardson, S. Tulyakov, K. Aberman, and D. Cohen-Or. Myvlm: Personalizing vlms for user-specific queries. In Proc. of European Conf. on Computer Vision (ECCV), pages 7391. Springer, 2024. [3] R. An, S. Yang, M. Lu, R. Zhang, K. Zeng, Y. Luo, J. Cao, H. Liang, Y. Chen, Q. She, et al. Mcllava: Multi-concept personalized vision-language model. arXiv preprint arXiv:2411.11706, 2024. [4] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh. Vqa: Visual question answering. In Proc. of Intl Conf. on Computer Vision (ICCV), pages 24252433, 2015. [5] G. Bai, J. Liu, X. Bu, Y. He, J. Liu, Z. Zhou, Z. Lin, W. Su, T. Ge, B. Zheng, et al. Mt-bench101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. Proc. of Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [6] S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [7] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini, C. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [8] A. Ben Abacha, S. A. Hasan, V. V. Datla, D. Demner-Fushman, and H. Müller. Vqa-med: Overview of the medical visual question answering task at imageclef 2019. In Proceedings of CLEF (Conference and Labs of the Evaluation Forum) 2019 Working Notes. 9-12 September 2019, 2019. [9] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K. Dong, Q. Du, Z. Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. [10] X. Cao, T. Zhou, Y. Ma, W. Ye, C. Cui, K. Tang, Z. Cao, K. Liang, Z. Wang, J. M. Rehg, et al. Maplm: real-world large-scale vision-language benchmark for map and traffic scene understanding. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 2181921830, 2024. [11] Z. Cao, Y. Yang, and H. Zhao. Scans: Mitigating the exaggerated safety for llms via safetyconscious activation steering. In Proc. of Intl Conf. on Artificial Intelligence (AAAI), volume 39, pages 2352323531, 2025. [12] J. Chen, T. Liang, S. Siu, Z. Wang, K. Wang, Y. Wang, Y. Ni, W. Zhu, Z. Jiang, B. Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. [13] J. Chen, T. Liang, S. Siu, Z. Wang, K. Wang, Y. Wang, Y. Ni, Z. Jiang, W. Zhu, B. Lyu, D. Jiang, X. He, Y. Liu, H. Hu, X. Yue, and W. Chen. MEGA-bench: Scaling multimodal evaluation to over 500 real-world tasks. 2025. [14] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, et al. Are we on the right way for evaluating large vision-language models? arXiv preprint arXiv:2403.20330, 2024. [15] R. Chen, X. Zhang, M. Luo, W. Chai, and Z. Liu. PAD: Personalized alignment at decodingtime. Proc. of Intl Conf. on Learning Representations (ICLR) , 2025. [16] Y. Chen, F. Xue, D. Li, Q. Hu, L. Zhu, X. Li, Y. Fang, H. Tang, S. Yang, Z. Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024. 11 [17] Z. Chen, W. Wang, Y. Cao, Y. Liu, Z. Gao, E. Cui, J. Zhu, S. Ye, H. Tian, Z. Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [18] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [19] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 2418524198, 2024. [20] K. Cheng, W. Song, J. Fan, Z. Ma, Q. Sun, F. Xu, C. Yan, N. Chen, J. Zhang, and J. Chen. Caparena: Benchmarking and analyzing detailed image captioning in the llm era. arXiv preprint arXiv:2503.12329, 2025. [21] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. [22] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu, T. Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. [23] J. Duan, W. Yuan, W. Pumacay, Y. R. Wang, K. Ehsani, D. Fox, and R. Krishna. Manipulateanything: Automating real-world robots using vision-language models. arXiv preprint arXiv:2406.18915, 2024. [24] Z. Duan, H. Cheng, D. Xu, X. Wu, X. Zhang, X. Ye, and Z. Xie. Cityllava: Efficient fine-tuning for vlms in city scenario. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 71807189, 2024. [25] Y. Fan, J. Nie, X. Sun, and X. Jiang. Exploring foundation models in detecting concerning daily functioning in psychotherapeutic context based on images from smart home devices. In 2024 IEEE International Workshop on Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys), pages 4449. IEEE, 2024. [26] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1935819369, 2023. [27] P. Gavrikov, J. Lukasik, S. Jung, R. Geirhos, M. J. Mirza, M. Keuper, and J. Keuper. Can we talk models into seeing the world differently? 2025. [28] A. Grattafiori, A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A. Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [29] T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 1437514385, 2024. [30] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36083617, 2018. [31] D. Han, T. McInroe, A. Jelley, S. V. Albrecht, P. Bell, and A. Storkey. Llm-personalize: Aligning llm planners with human preferences via reinforced self-training for housekeeping robots. Proc. of Intl Conf. on Computational Linguistics (COLING), 2024. [32] H. Hao, J. Han, C. Li, Y.-F. Li, and X. Yue. Rap: Retrieval-augmented personalization for multimodal large language models. Proc. of Computer Vision and Pattern Recognition (CVPR), 2025. 12 [33] H. Hao, J. Han, C. Li, Y.-F. Li, and X. Yue. Rap: Retrieval-augmented personalization for multimodal large language models. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 1453814548, 2025. [34] A. Hengle, P. Bajpai, S. Dan, and T. Chakraborty. Multilingual needle in haystack: Investigating long-context behavior of multilingual large language models. arXiv preprint arXiv:2408.10151, 2024. [35] T. Hiippala, M. Alikhani, J. Haverinen, T. Kalliokoski, E. Logacheva, S. Orekhova, A. Tuomainen, M. Stone, and J. A. Bateman. Ai2d-rst: multimodal corpus of 1000 primary school science diagrams. Language Resources and Evaluation, 55:661688, 2021. [36] W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: visual language model for gui agents. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 1428114290, 2024. [37] C.-Y. Hsieh, Y.-S. Chuang, C.-L. Li, Z. Wang, L. T. Le, A. Kumar, J. Glass, A. Ratner, C.-Y. Lee, R. Krishna, et al. Found in the middle: Calibrating positional attention bias improves long context utilization. arXiv preprint arXiv:2406.16008, 2024. [38] Q. Huang, X. Dong, P. Zhang, B. Wang, C. He, J. Wang, D. Lin, W. Zhang, and N. Yu. Opera: Alleviating hallucination in multi-modal large language models via over-trust penalty and retrospection-allocation. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 1341813427, 2024. [39] J. Jang, S. Kim, B. Y. Lin, Y. Wang, J. Hessel, L. Zettlemoyer, H. Hajishirzi, Y. Choi, and P. Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc parameter merging. arXiv preprint arXiv:2310.11564, 2023. [40] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [41] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [42] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proc. of Conf. on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [43] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. [44] H. R. Kirk, A. Whitefield, P. Röttger, A. Bean, K. Margatina, J. Ciro, R. Mosquera, M. Bartolo, A. Williams, H. He, B. Vidgen, and S. A. Hale. The prism alignment dataset: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Proc. of Neural Information Processing Systems (NeurIPS), 2024. [45] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. [46] J. J. Lau, S. Gayen, A. Ben Abacha, and D. Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. [47] B. W. Lee, I. Padhi, K. N. Ramamurthy, E. Miehling, P. Dognin, M. Nagireddy, and A. Dhurandhar. Programming refusal with conditional activation steering. arXiv preprint arXiv:2409.05907, 2024. [48] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Proc. of Neural Information Processing Systems (NeurIPS), 33:94599474, 2020. 13 [49] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023. [50] B. Li, Y. Zhang, D. Guo, R. Zhang, F. Li, H. Zhang, K. Zhang, P. Zhang, Y. Li, Z. Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [51] D. Li, R. Shao, A. Xie, Y. Sheng, L. Zheng, J. Gonzalez, I. Stoica, X. Ma, and H. Zhang. How long can context length of open-source llms truly promise? In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. [52] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. [53] Z. Li, X. Wu, H. Du, H. Nghiem, and G. Shi. Benchmark evaluations, applications, and challenges of large vision language models: survey. arXiv preprint arXiv:2501.02189, 1, 2025. [54] B. Liu, L.-M. Zhan, L. Xu, L. Ma, Y. Yang, and X.-M. Wu. Slake: semantically-labeled In 2021 IEEE 18th knowledge-enhanced dataset for medical visual question answering. international symposium on biomedical imaging (ISBI), pages 16501654. IEEE, 2021. [55] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. Proc. of Neural Information Processing Systems (NeurIPS), 36:3489234916, 2023. [56] H. Liu, C. Li, Y. Li, and Y. J. Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [57] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172, 2023. [58] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024. [59] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Proc. of Neural Information Processing Systems (NeurIPS), 35:25072521, 2022. [60] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. [61] S. Lu, Y. Li, Q.-G. Chen, Z. Xu, W. Luo, K. Zhang, and H.-J. Ye. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024. [62] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [63] Microsoft. Phi2: models. phi-2-the-surprising-power-of-small-language-models/, 2023. The language https://www.microsoft.com/en-us/research/blog/ surprising power small of [64] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty. Ocr-vqa: Visual question answering In 2019 international conference on document analysis and by reading text in images. recognition (ICDAR), pages 947952. IEEE, 2019. [65] T. Nguyen, H. Liu, Y. Li, M. Cai, U. Ojha, and Y. J. Lee. Yollava: Your personalized language and vision assistant. Proc. of Neural Information Processing Systems (NeurIPS), 2024. [66] L. Ning, L. Liu, J. Wu, N. Wu, D. Berlowitz, S. Prakash, B. Green, S. OBanion, and J. Xie. User-llm: Efficient llm contextualization with user embeddings. arXiv preprint arXiv:2402.13598, 2024. 14 [67] NousResearch. Nous-hermes-2-yi-34b. https://huggingface.co/NousResearch/ Nous-Hermes-2-Yi-34B, 2023. [68] OpenAI. Chatgpt. https://chat.openai.com/, 2023. [69] W. Park, W. Kim, J. Kim, and J. Do. Second: Mitigating perceptual hallucination in visionlanguage models via selective and contrastive decoding. Proc. of Intl Conf. on Machine Learning (ICML), 2025. [70] S. Pasch. Llm content moderation and user satisfaction: Evidence from response refusals in chatbot arena. arXiv preprint arXiv:2501.03266, 2025. [71] C. Pham, H. Phan, D. Doermann, and Y. Tian. Personalized large vision-language models. arXiv preprint arXiv:2412.17610, 2024. [72] J. Robinson, C. M. Rytting, and D. Wingate. Leveraging large language models for multiple choice question answering. Proc. of Intl Conf. on Learning Representations (ICLR) , 2023. [73] D. Romero, C. Lyu, H. A. Wibowo, T. Lynn, I. Hamed, A. N. Kishore, A. Mandal, A. Dragonetti, A. Abzaliev, A. L. Tonja, et al. Cvqa: Culturally-diverse multilingual visual question answering benchmark. Proc. of Neural Information Processing Systems (NeurIPS), 2024. [74] P. Röttger, H. R. Kirk, B. Vidgen, G. Attanasio, F. Bianchi, and D. Hovy. Xstest: test suite for identifying exaggerated safety behaviours in large language models. arXiv preprint arXiv:2308.01263, 2023. [75] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 2250022510, 2023. [76] A. Salemi, S. Kallumadi, and H. Zamani. Optimization methods for personalizing large language models through retrieval augmentation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2024. [77] A. Salemi, S. Mysore, M. Bendersky, and H. Zamani. LaMP: When large language models meet personalization. Proc. of Annual Meeting of the Association for Computational Linguistics (ACL), 2024. [78] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. In European Conference on Computer Vision, pages 146162. Springer, 2022. [79] G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [80] I. Team. Internlm: multilingual language model with progressively enhanced capabilities, 2023. [81] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [82] J. Urbanek, F. Bordes, P. Astolfi, M. Williamson, V. Sharma, and A. Romero-Soriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. arXiv preprint arXiv:2312.08578, 2023. [83] H. Wang, H. Shi, S. Tan, W. Qin, W. Wang, T. Zhang, A. Nambi, T. Ganu, and H. Wang. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. arXiv preprint arXiv:2406.11230, 2024. [84] P. Wang, S. Bai, S. Tan, S. Wang, Z. Fan, J. Bai, K. Chen, X. Liu, J. Wang, W. Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 15 [85] X. Wang, C. Hu, P. Röttger, and B. Plank. Surgical, cheap, and flexible: Mitigating false refusal in language models via single vector ablation. arXiv preprint arXiv:2410.03415, 2024. [86] Z. Wang, Z. Li, Z. Jiang, D. Tu, and W. Shi. Crafting personalized agents through retrievalaugmented generation on editable memory graphs. Proc. of Conf. on empirical methods in natural language processing (EMNLP), 2024. [87] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, et al. Q-bench: benchmark for general-purpose foundation models on low-level vision. arXiv preprint arXiv:2309.14181, 2023. [88] T. Wu, C. Xiang, J. T. Wang, and P. Mittal. Effectively controlling reasoning models through thinking intervention. arXiv preprint arXiv:2503.24370, 2025. [89] T.-H. Wu, G. Biamby, J. Quenum, R. Gupta, J. E. Gonzalez, T. Darrell, and D. M. Chan. Visual haystacks: vision-centric needle-in-a-haystack benchmark. arXiv preprint arXiv:2407.13766, 2024. [90] Z. Wu, X. Chen, Z. Pan, X. Liu, W. Liu, D. Dai, H. Gao, Y. Ma, C. Wu, B. Wang, et al. Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding. arXiv preprint arXiv:2412.10302, 2024. [91] T. Xie, X. Qi, Y. Zeng, Y. Huang, U. M. Sehwag, K. Huang, L. He, B. Wei, D. Li, Y. Sheng, et al. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598, 2024. [92] L. Xu, Y. Zhao, D. Zhou, Z. Lin, S. K. Ng, and J. Feng. Pllava: Parameter-free llava extension from images to videos for video dense captioning. arXiv preprint arXiv:2404.16994, 2024. [93] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, et al. mplugowl: Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023. [94] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [95] R. Yu, Y. Gong, X. He, Y. Zhu, Q. Liu, W. Ou, and B. An. Personalized adaptive meta learning for cold-start user preference prediction. In Proc. of Intl Conf. on Artificial Intelligence (AAAI), volume 35, pages 1077210780, 2021. [96] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [97] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [98] H. Zhang, P. Zhang, X. Hu, Y.-C. Chen, L. Li, X. Dai, L. Wang, L. Yuan, J.-N. Hwang, and J. Gao. Glipv2: Unifying localization and vision-language understanding. Advances in Neural Information Processing Systems, 35:3606736080, 2022. [99] J. Zhang, J. Huang, S. Jin, and S. Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [100] K. Zhang, F. Zhao, Y. Kang, and X. Liu. Memory-augmented llm personalization with shortand long-term memory coordination. Proc. of Conf. of the North American Chapter of the Association for Computational Linguistics (NAACL), 9:19, 2023. [101] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, et al. Internlm-xcomposer: vision-language large model for advanced text-image comprehension and composition. arXiv preprint arXiv:2309.15112, 2023. 16 [102] Y. Zhang, J. Wang, L.-C. Yu, D. Xu, and X. Zhang. Personalized lora for human-centered text understanding. In Proc. of Intl Conf. on Artificial Intelligence (AAAI), volume 38, pages 1958819596, 2024. [103] Z. Zhang, R. Chen, S. Liu, Z. Yao, O. Ruwase, B. Chen, X. Wu, and Z. Wang. Found in the middle: How language models use long contexts better via plug-and-play positional encoding. arXiv preprint arXiv:2403.04797, 2024. [104] S. Zhao, Y. Yuan, X. Tang, and P. He. Difficult task yes but simple task no: Unveiling the laziness in multimodal llms. In Findings of the Association for Computational Linguistics: EMNLP, 2024. [105] S. Zhao, M. Hong, Y. Liu, D. Hazarika, and K. Lin. Do llms recognize your preferences? evaluating personalized preference following in llms. Proc. of Intl Conf. on Learning Representations (ICLR) , 2025. [106] H. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan. 3d-vla: 3d vision-language-action generative world model. arXiv preprint arXiv:2403.09631, 2024. [107] L. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang, Z. Wu, Y. Zhuang, Z. Li, Z. Lin, E. P. Xing, et al. Lmsys-chat-1m: large-scale real-world llm conversation dataset. Proc. of Intl Conf. on Learning Representations (ICLR) , 2024. [108] W. Zhong, D. Tang, J. Wang, J. Yin, and N. Duan. UserAdapter: Few-shot user learning in sentiment analysis. In Findings of the Association for Computational Linguistics, 2021. [109] W. Zhong, L. Guo, Q. Gao, H. Ye, and Y. Wang. Memorybank: Enhancing large language models with long-term memory. In Proc. of Intl Conf. on Artificial Intelligence (AAAI), volume 38, pages 1972419731, 2024. [110] J. Zhu, W. Wang, Z. Chen, Z. Liu, S. Ye, L. Gu, Y. Duan, H. Tian, W. Su, J. Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. [111] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei. Visual7w: Grounded question answering in images. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 49955004, 2016. [112] Y. Zhuang, H. Sun, Y. Yu, R. Qiang, Q. Wang, C. Zhang, and B. Dai. HYDRA: Model factorization framework for black-box LLM personalization. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024."
        },
        {
            "title": "Contents",
            "content": "A Broader Discussion A.1 Rationale Behind the Four Concept Categories . . . . . . . . . . . . . . . . . . . . A.2 Additional Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Ethical Consideration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Licensing Annotation Process C.1 Data Collection . . C.2 Query Generation . C.3 Query Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Human Evaluation Platforms More Information about MMPB E.1 Computational Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Results F.1 Personalization Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.2 Model Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.3 Comparison of Ovis2-34B with Human-Created Descriptions . . . . . . . . . . . . F.4 Blind Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.5 Multi-turn Conversation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F.6 Prompting Methods . F.7 Extended Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 19 19 19 19 19 27 30 31 31 31 32 32 32 33 33"
        },
        {
            "title": "A Broader Discussion",
            "content": "A.1 Rationale Behind the Four Concept Categories We selected Human because end users and the people around them (e.g., family, friends, colleagues) represent the most common and socially salient personalization targets. The Animal category reflects the growing importance of pets and other animals in users lives, enabling tailored interactions such as care reminders or activity suggestions. We include Object to capture the wide range of personal belongings that smart-home devices and personalized robots must recognize and manage on behalf of the user. Finally, Character covers virtual personas and narrative figures in VR, and metaverse, ensuring that VLMs can personalize experiences for users custom avatars and digital contexts. A.2 Additional Related Works VLM Personalization Although VLM personalization remains an underexplored yet important area, several seminal works have recently emerged. MyVLM [2] augments pretrained models such as BLIP-2 [52] and LLaVA [55] with external concept heads, enabling recognition of user-specific concepts and supporting personalized captioning and VQA. YoLLaVA [65] builds on LLaVA to learn personalized concepts from only handful of images, embedding them into special tokens to facilitate user-specific queries and conversations. MC-LLaVA [3] extends beyond single-concept personalization by introducing the first framework for multi-concept personalization, allowing models to jointly learn and reason about multiple user-specific concepts. PLVM [71] employs an Aligner module to encode reference images online, enabling the incorporation of new personalized concepts without additional fine-tuning costs. Finally, RAP [33] leverages an external database and retrieval mechanisms to enhance personalization in multimodal LLMs, allowing real-time concept editing and dynamic updates that broaden applicability in real-world scenarios. Together, these works demonstrate the promise of personalized VLMs but also leave open challenges such as dynamic updates and long-term persistency. In particular, methods like YoLLaVA and MC-LLaVA depend on large sets of negative images to avoid concept confusion, raising data and training costs in practice. A.3 Ethical Consideration Identifying specific individuals or making unwarranted claims about their personal preferences has been extensively addressed by various alignment techniques. In our experiments, we likewise observed persistent refusal responses from both GPT-4o and the Claude family of models. Consequently, for the human category, MMPB makes use of the human images provided by MC-LLaVA [3], replacing each persons real name with <sks> to prevent pretraining data leakage and to avoid identifying private individuals wherever possible."
        },
        {
            "title": "B Licensing",
            "content": "MMPB is licensed under CC BY-NC-SA 4.0. If you wish to access and use our dataset, you must understand and agree that it is for research purposes only and may not be used for any commercial or other purposes. Users assume all responsibility for any consequences arising from unauthorized use or redistribution. We do not own the copyrights to any movie stills or other nonCreative Commons images. We grant researchers access to these images on the condition that they acknowledge and respect the rights of the original copyright holders. We fully respect and honor the copyrights of the original authors. If any original author requests that an image be removed, please contact us at jake630@snu.ac.kr or open an issue. We will then adjust our distribution method to provide only the image URLs."
        },
        {
            "title": "C Annotation Process",
            "content": "In this section, we introduce our guidelines for both models and human annotators, following the three-stage data-construction process described in 3. C.1 Data Collection 19 C.1.1 Images for Concept Injection C.1.2 Text-based concept injection Below is the prompt for generating four levels of textual descriptions for each concept (Human category for example). Description generation guidelines for model (Simple) Analyze the five provided images, all depicting the same person. Based on your analysis, generate 3 concise keywords that best summarize their identity, appearance, or distinguishing features. Provide your answer strictly in the format: <sks> is <keyword1>, <keyword2>, <keyword3>. Description generation guidelines for model (Moderate) Describe the person in the five images, highlighting their key physical traits and distinguishing features in single sentence. Name the person as <sks> and ensure the description uses <sks> naturally throughout. Description generation guidelines for model (Detailed) Carefully observe the person across all five images and provide single, unified description of <sks>. Name the person as <sks> and ensure the description uses <sks> naturally throughout. Focus on consistent traits rather than describing each image separately. Describe <sks>s physical appearance, clothing style, and distinguishing features in single paragraph. Description generation guidelines for model (Extented) Carefully examine all five images featuring the attribute referred to as <sks>. Then, synthesize comprehensive, unified description of <sks> that integrates consistent visual traits and personality cues across the full image set. Do **not** describe the images one by onefocus instead on what remains **visually and stylistically consistent** throughout. Your output should consist of three rich, detailed paragraphs, each with distinct focus: (1) Physical Appearance: Describe <sks>s body shape, height impression, facial structure, skin tone, hair style, and any recurring facial expressions or poses. Be specific and observationalmention anything from jawline shape to eyebrow thickness if consistent. (2) Outfit and Accessories: Describe the style, color palette, textures, and function of <sks>s clothing. Include details such as patterns, cuts, materials (e.g. denim, leather, silk), and whether the outfit feels practical, formal, whimsical, or character-defining. If <sks> wears accessories (glasses, jewelry, belts, bags), describe them and their potential significance. (3) Distinguishing Visual Features or Vibe: Identify the most memorable and defining features of <sks>things viewer would immediately associate with them. This could include signature items, unique silhouette, color theming, visual motifs, or an aura they convey (e.g., mischievous, stoic, ethereal). Think of what makes <sks> visually unmistakable. Always use the name <sks> naturally and repeatedly in the description, and maintain confident, analyst tone. 21 Please note that the model-generated descriptions exhibited three issues: 1) omission of the <sks> placeholder; 2) descriptions that were overly specific to individual images; and 3) failure to accurately convey the intended concept. To address these problems, human annotators revised each set of descriptionsreintroducing the <sks> placeholder, generalizing the descriptions, and ensuring conceptual accuracy. C.1.3 Keywords for preferences For human preference keywords, we first defined hierarchy of subdomains under each of five main domains, assigning six distinct topic areas to every subdomain. To build comprehensive keyword set for augmenting diverse preferences, we then prompted GPT-4o with the following instruction:"
        },
        {
            "title": "Prompt for generating preference keywords",
            "content": "Using the domains and subdomains shown in the image above, generate 610 representative keywords for each subdomain. These keywords will later serve as likes or dislikes traits for given person. For example: - Entertainment Concerts subdomain might yield keywords like rock concerts, jazz festivals, etc. - Travel Food subdomain might yield keywords like fine-dining, street food tours, local cuisine tastings, etc. - Lifestyle Food subdomain might yield keywords like high-protein diet, organic meal prep, etc. Please produce 610 concise, descriptive keywords for each of the following subdomains: 1. Entertainment: Concerts, Comedy, Festival, Performance, TV Shows, Movies 2. Travel: Nature, Culture, Transport, Food, Leisure, Extreme Activities 3. Lifestyle: Food, Beverage, Exercise, Wellness, Habits, Home 4. Shopping: Audio, Beauty, Hobby, Tech, Lifestyle Goods, Kitchen 5. Fashion: Casual/Formal, Street/Trendy, Outerwear, Accessories, Innovative, Sustainable Using the model-generated keywords, we randomly assign them to our fixed templates (see 3). Crucially, this process often produce contradictory like / dislike pairs. To resolve these conflicts, we task human annotators with applying the following guidelines to reconcile any preference contradictions. Guideline for resolving preference contradiction You are given master keyword set and descriptive preference keywords (likes and dislikes), broken down by subdomain. For each of the following subdomains, identify any contradictory preference keywordscases where like directly conflicts with dislikeand resolve each conflict by replacing one of the conflicting keywords with non-conflicting keyword from the corresponding master set: 1. Entertainment: Concerts, Comedy, Festival, Performance, TV Shows, Movies 2. Travel: Nature, Culture, Transport, Food, Leisure, Extreme Activities 3. Lifestyle: Food, Beverage, Exercise, Wellness, Habits, Home 4. Shopping: Audio, Beauty, Hobby, Tech, Lifestyle Goods, Kitchen 5. Fashion: Casual/Formal, Street/Trendy, Outerwear, Accessories, Innovative, Sustainable Examples of contradictory preferences: - Shopping: likes: gaming accessories; dislikes: tech gadgets - Lifestyle: likes: Yoga; dislikes: stretching The final keyword set for each individual is saved in JSON format, as shown in Listing 1. 22 Listing 1: Example JSON file of human preference (with reference image path). 1 { 3 4 5 6 7 9 10 11 12 13 15 16 17 18 19 21 22 23 24 25 27 28 29 30 31 33 34 35 36 37 39 40 41 42 43 45 46 47 48 49 51 52 53 54 55 57 58 59 60 \" concept \": \" Angelina \" , \" images \": [ \"./ human / train / Angelina /0. png \" , \"./ human / train / Angelina /1. png \" , \"./ human / train / Angelina /2. png \" , \"./ human / train / Angelina /3. png \" , \"./ human / train / Angelina /4. png \" ] , \" preferences \": { \" entertainment \": { \" likes \": [ \" rock concerts \" , \" indie films \" ] , \" dislikes \": [ \" theater performances \" , \" film festivals \" ] } , \" travel \": { \" likes \": [ \" wildlife safaris \" , \" hot air balloon rides \" ] , \" dislikes \": [ \" historical site tours \" , \" food tours \" ] } , \" lifestyle \": { \" likes \": [ \" meditation \" , \" yoga \" ] , \" dislikes \": [ \" plant - based diet \" , \" home cooking \" ] } , \" shopping \": { \" likes \": [ \" Bluetooth speakers \" , \" wireless charging stations \" ] , \" dislikes \": [ \" ceramic pottery \" , \" kitchen appliances \" ] } , \" fashion \": { \" likes \": [ \" hand - knitted sweaters \" , \" silk scarves \" ] , \" dislikes \": [ \" handcrafted leather bags \" , \" woven hats \" ] } } 61 62 } 23 C.1.4 Multi-turn conversation For evaluating Persistency, we extract ten-turn conversations on general topics from LMSYS1M [107]. These same dialogues are presented to every VLM under test. The conversations we select are listed below. Default 10-turn conversation transcript (Excerpt) User: Assistant: User: Assistant: User: hello what is your name Hello! My name is OpenAI Assistant. How can assist you today? You are historian of the internet, please tell me why nobody uses Usenet, or newsgroup anymore ? Usenet and newsgroups were very popular in the early days of the internet, but their popularity has declined significantly in recent years. There are several reasons for this: 1. Lack of user-friendliness... 4. Limited access... they have largely been replaced by other platforms that are easier to use and offer more features. What happened to the Javascript framework called Meteor, or MeteorJS, and what new framework or technologies did replace it ? Assistant: Meteor is still an actively maintained and popular JavaScript framework... However, its popularity has declined... alternatives include: 1. React... 4. Node.js... You are database expert, please tell me why and when should use MongoDB instead of traditional database like postgresql ? User: User: Assistant: User: Assistant: Assistant: MongoDB and PostgreSQL are both powerful database systems... scenarios where you might consider using MongoDB: 1. Handling large amounts of unstructured data... 4. Real-time analytics... Ultimately, the choice of database system will depend on your specific needs... On different topic now, why is the temperature colder when going up in the mountains ? The temperature drops as you go higher... due to phenomenon known as adiabatic cooling... other factors... include thinner air and the presence of snow and ice. Including their national waters, what are the five largest countries on earth ? The five largest countries on Earth by total area, including their national waters, are: 1. Russia... 5. Brazil... Please do the same ranking, but this time incuding the exlcusive economic zones If we include the exclusive economic zones (EEZs)... 1. Russia 25,064,100 km²... 5. Brazil 8,515,767 km²... Why are you called NAME_1, French name ? apologize for any confusion, but am not called NAME_1 and am not person... Where is based the company AI company Anthropic, and who are its founders ? User: Assistant: Assistant: User: User: Assistant: Anthropic is an AI company that is based in San Francisco, California, USA. The company was founded in 2019 by NAME_2 and NAME_3... NAME_4 is prominent AI researcher... NAME_5 is software engineer... Who are the investors of Anthropic ? User: Assistant: Anthropic has raised funding from several notable investors, including: 1. NAME_6... 5. NAME_10... The company has also received funding from venture capital firms... 24 C.1.5 Query Images 25 26 C.2 Query Generation We present the guidelines used by both models and human annotators during the query generation process. C.2.1 Models Guideline for model-based recognition query generation You will be given: 1) Task type 2) An image 3) description of target concept <sks> Task: Generate candidate VQA queries of three typesAwareness, Appropriateness, and Coherency. Awareness: Formulate both affirmative and negative yes/no questions about whether <sks> is present in the image, e.g.: Is <sks> not present in the image? , Can you see <sks> in the photo? Appropriateness : Similar to Awareness, formulate both affirmative and negative yes/no questions about whether <sks> is present in the image. Coherency: Create one 4-option multiple-choice question where: Correct answer reflects the true relationship of <sks> to the image. Distractors are visually plausible but incorrect without concept reasoning. For single-entity images, distractors may consist exclusively of plausible options. Always: Use clear grammar and concise phrasing. Guideline for model-based preference query generation You will be given: 1)Task type 2) An image 3) preference description of target concept <sks> Task: Generate candidate VQA queries of three typesAwareness, Appropriateness, and Coherencywhile filtering out text-only or image-only solvable questions for coherency. Awareness: Formulate both affirmative and negative yes/no questions about whether the image is related to <sks>s preference, e.g.: Is the image tied to <sks>s positive or negative liking?, Is the image not tied to <sks>s positive or negative preference? Appropriateness : Similar to Awareness, formulate both affirmative and negative yes/no questions about whether the image is related to <sks>s preference. Coherency: Create one 4-option multiple-choice question where: Correct answer reflects the true relationship of <sks> to the image. Distractor is concept-aligned with <sks> but does not match the image. Distractors and are visually plausible but incorrect without concept reasoning. Example: Among the activities that could reasonably occur in the given image, which one is <sks> least likely to be doing? A) <correct> B) <concept-aligned but not in image> C) <visually plausible but concept-wrong> D) <visually plausible but concept-wrong> Always: Use clear grammar and concise phrasing. C.2.2 Humans 28 C.3 Query Evaluation After drafting each query, we send the concept description (appearance and, for humans, preferences), the image, and the draft query with its answer to three models (3). If at least two models flag the same issue, we forward that flag to human annotators, who then apply our guidelines to revise the query. This model-assisted flagging speeds up human quality checks. C.3.1 Models"
        },
        {
            "title": "Flagging guidelines for models",
            "content": "You will be provided with personalizable concepts textual descriptions, covering appearance and preferences, and set of image-query-answer pairs about an image. Your task is to identify whether each pair exhibits any of the following issues: 1. Trivial query is Trivial if it can be answered directly from the query itself or by using commonsense, without needing the image or any extra information. Example: Q: What is <sks> doing in the movie theater? A: Watching movie. Judgement: Trivial (The query already states movie theater, and watching movie in movie theater is trivial.) 2. Concept-Only query is Concept-Only if it can be answered using only the provided textual description (appearance, preferences, etc.), without looking at the image. Example: Q: What is <sks>s gender? A: Male. Judgement: Concept-Only (The description explicitly says <sks> is male, so theres no need to inspect the picture.) Example: Q: What is <sks> likely to be doing here? Choice: Playing basketball / Yoga / Drinking coffee / Ice-fishing A: Playing basketball. Judgement: Concept-Only (The description explicitly says <sks> likes to play basketball, so theres no need to inspect the picture.) 3. Image-Only query is Image-Only if it can be answered using only the provided image, without needing the personal description. Example: Q: (An image with man riding bike) What is <sks> who is wearing jacket doing? A: Riding bike. Judgement: Image-Only (The query describes <sks> in the image, so theres no need to inspect the personal description.) Example: Q: (An image with man playing basketball) What is <sks> likely to be doing here? Choice: Playing basketball / Yoga / Drinking coffee / Ice-fishing A: Playing basketball. Judgement: Image-Only (If the answer can be determined from the image alone and all other choices are unrelated to it, then display only the image.) Use exactly these three labelsTrivial, Concept-Only, and Image-Onlywhen annotating. Refer back to the definitions and examples above. C.3.2 Humans"
        },
        {
            "title": "Query evaluation guidelines for human annotators",
            "content": "You will be provided with: Model issue flagging personalizable concepts textual descriptions, covering appearance and preferences set of imagequeryanswer pairs about given image <Guideline for models> 1. Carefully review the issue the model has flagged. If the flagged issue is valid, correct the answer choices accordingly. If the flagged issue is invalid but there is different valid issue, correct the answer choices accordingly. If the flagged issue is invalid and there is no other issue, annotate None above the question. 2. Ensure every question is phrased as an open-ended, free-form query. If question is not open-ended, rewrite it so it can be answered freely. Example of an open-ended question: Q. What is <sks> doing in the movie theater? Example of choice-based question (to be avoided): Q. Which of the following four options best describes what <sks> might be doing?"
        },
        {
            "title": "D Human Evaluation Platforms",
            "content": "Figure 10: Interface of the human evaluation platform. Human baseline evaluation was conducted via Streamlit4 platform. Evaluators manually assessed queries drawn from the Moderate concept-injection set. Since MMPB comprises 10,017 total questions, asking humans to label them all would be prohibitively time-consuming and risk fatiguerelated bias. To ensure fair yet feasible evaluation, we randomly sampled one concept from each of five categories (two human concepts, one animal, one object, and one character), yielding 4https://streamlit.io/ 30 (a) (b) (c) Figure 11: Examples of the human evaluation results interface. representative subset across concept types. Figure 10 illustrates the evaluation interface. Once all assessments were complete, we computed scores by category and task type, then aggregated them to produce an overall human-baseline accuracy (Figure 11, see Figure 4 for results). We plan to open-source this evaluation platform in the near future, and will also release the five-concept dataset used by human evaluators as the MMPB-Mini version."
        },
        {
            "title": "E More Information about MMPB",
            "content": "Table 5: Computational resources for evaluating MMPB. Family GPT-4o Claude-Sonnet Gemini-Flash Cost $224.5 $178.4 $26.1 Method Text 0 Turn Text 10 Turn Image 0 Turn Image 10 Turn Avg. Runtime (h) 2.2 2.7 4.2 4.5 (a) Total API cost. (b) Average runtime. E.1 Computational Resources Open-source models were run locally on four NVIDIA H100 GPUs. All models can be evaluated on single GPU, except for InternVL2.5-78B, Qwen2-VL-72B, Qwen2.5-VL-72B, and LLaVA-OV-72B, which were evaluated using three GPUs. Closed-source models were evaluated via their APIs using batch processing. Detailed total costs for each model family are provided in Table 5a. Table 5b reports the average runtime of all open-source models used in benchmarking. The Text method refers to Moderate description-based concept injection, whereas the Image method refers to 2-image concept injection. E.2 Statistics Table 6 presents overall statistics for MMPB, including the number of concepts and the question counts for each task type; the dataset comprises 10,017 imagetext query pairs in total. As shown in Figure 12, human preference keywords are drawn from 30 subdomains across five domains: Entertainment, Travel, Lifestyle, Shopping, and Fashion. Figure 13 displays word clouds of the extracted keywords for each domain, generated by GPT-4o. 31 Table 6: MMPB statistics, including the number of concepts and question counts across task types."
        },
        {
            "title": "Category",
            "content": "# Concept Awareness Appropriateness Coherency Human (Rec.) Human (Pref.) Animal Object Character - 50 50 20 25 16 - 734 2500 200 344 - 1400 1250 400 500 320 - 367 1250 100 172 160 -"
        },
        {
            "title": "Total",
            "content": "2501 5000 700 1016 800 10017 Figure 12: Distribution of preference keywords. Figure 13: Word clouds of preference keywords in MMPB across five domains."
        },
        {
            "title": "F Additional Results",
            "content": "F.1 Personalization Fine-tuning We additionally fine-tuned LLaVA-1.5-13B with two strategies: soft prompt tuning and personalized LoRA. As shown in Table 7, both methods yield clear improvements over the baseline, with LoRA achieving the strongest gains. These results confirm that MMPB reliably captures personalization effectiveness. F.2 Model Sizes Impact of model scale on Persistency We analyze multi-turn conversation drop ratios for both injection modalities (Figure 14). For preference tasks, both textand image-based injections show similar drop trends as turns increase. This occurs because, in both modalities, preference injection is carried out through the text modality. However, in recognition tasks, image-based injection yields an almost linear performance decline, whereas text-based injection maintains far more stable drop rates. In other words, larger models struggle to retain personalized context when it is delivered via images. This result reinforces the suboptimal nature of image-based personalization discussed in 5. F.3 Comparison of Ovis2-34B with Human-Created Descriptions In 3, we used Ovis2-34B to generate descriptions for all concept categories and then evaluated the model on MMPB (4). To rule out any unfair advantage from using its own outputs, we asked human annotators to write descriptions from scratch (without seeing the models versions) for held-out set of 10 concepts (4 human, 2 animal, 2 object, and 2 character). We then compared Ovis234Bs performance when using human-authored descriptions versus its own. As shown in Table 8, 32 Table 7: Performance of LLaVA-1.5-13B on MMPB with different personalization methods."
        },
        {
            "title": "Baseline\nSoft Prompt\nSoft Prompt\nSoft Prompt\nLoRA\nLoRA\nLoRA",
            "content": "- 10 30 50 10 30 50 47.8 57.8 58.4 57.6 52.0 58.7 60.0 59.6 59.6 66.5 71.3 72.1 86.1 58.1 51.5 58.4 61.0 62.1 58.0 67.1 59.4 Figure 14: Multi-turn conversation performance drop ratios by injection modality. (left) preference accuracy drop; (right) recognition accuracy drop. Ovis2-34B attains higher accuracy using the human-created descriptions than its own outputs. If our benchmark favored model-generated text, Ovis2-34B would perform best with its own descriptions. That Ovis2-34B performs better on independent, human-authored descriptions demonstrates that our evaluation does not unduly advantage self-generated content. F.4 Blind Test To evaluate the query quality of MMPB, we conduct blind test on six models (see 5.4 and 5.5). We black out the question images and provided only the text to each model, using the Moderate text-based injection with 0-turn setting. This experiment tests whether the models produce predictable results when solving questions based solely on text. Ideally, since the models cannot see the images, they should always deny the concept in Awareness tasks on positive images, resulting in 0% accuracy. In Appropriateness tasks, they should always agree, yielding 100% accuracy. Finally, for Coherency, our design ensures that two distractors are solvable only through visual information, while one is solvable only through the injected concept. Without the image, correct concept injection implies 50% chance of selecting the correct answeri.e., random choice between the true answer and the concept-only distractor. As shown in Table 9, the blind-test results closely match these optimal values, once again demonstrating the effectiveness and robustness of our query design. F.5 Multi-turn Conversation We additionally compare VLM Persistency under general conversation versus recognitionand preference-related dialogues. We first crafted human-written prompts for 10-turns on generic topics (e.g., beard styling, hair dyeing, K-pop band recommendations, movie genres), ensuring they do not conflict with the concepts preferences and maintain neutral tone. We then used GPT-4o in few-shot setup to augment multi-turn dialogues for each topic. Since dialogue lengths vary, we 33 Table 8: Ovis2-34B performance using human-authored vs. model-generated descriptions to test for self-advantage"
        },
        {
            "title": "10 Turn",
            "content": "Self-Generated Human-created 73.3 75.3 63.6 63.6 Table 9: Blind-test results for Recognition and Preference across task types. Method Recognition Preference Awareness Appropriateness Coherency Awareness Appropriateness Coherency Blind Optimal 0.0 0.0 100.0 100. 41.5 50.0 15.4 0.0 82.0 100.0 41.4 50.0 evaluate persistency based on cumulative token count rather than turn count. Example prompts for Recognition-Related and Preference-Related dialogues appear in Box 23 and Box 24, respectively. We also include the Reminder prompting method for comparison. As shown in Figure 15, general conversations incur the largest multi-turn performance drop. Recognition-related dialogues yield the smallest drop in recognition accuracy, while for preference tasks all specialized dialogues (Recognitionor Preference-related and Reminder) exhibit similar drop trends, saturating around 6k tokens. These results show that even general dialogues centered on recognitionor preference-related topics deliver persistency benefits comparable to targeted reminder. F.6 Prompting Methods To examine how personalization performance varies with prompting strategy, we compared five methods in Figure 17: Zero-shot, Zero-shot-CoT, Reminder, Few-shot, and Few-shot-CoT. All experiments use text-based concept injection (Moderate descriptions). Examples of all prompts used in this work can be found in Box 16. Overall, there is no statistically significant difference among methods. However, for the Awareness task, Reminder achieves the highest accuracy, while Zero-shot-CoT and both Few-shot variants underperform plain Zero-shot. similar pattern holds for Coherency. In contrast, on the Appropriateness task, Reminder slightly degrades performancesuggesting that explicitly reminding the model of the concept may introduce bias toward its presence. Nevertheless, differences remain small, underscoring the inherent challenge of conferring personalization capabilities to VLMs via simple prompting and motivating the development of more sophisticated personalization techniques. 34 Figure 15: Multi-turn conversation performance with Recognition-related and Preference-related topics: (left) recognition accuracy; (right) preference accuracy. Reminder prompt In your next response, ensure that your choice adheres to <sks>s visual characteristics and stated preferences. Zero-shot CoT prompt Lets think step-by-step. Few-shot prompt When answering users question, good assistant should carefully consider <sks>s stated appearance and preferences and tailor the response accordingly. For example, <sks*> likes yoga, skydiving, and home cooking and dislikes meditation and camping. Example Query 1: (An image with mat) What is <sks*> going to do? Good assistant response: Yoga. Example Query 2: (An image of an airport) What is <sks*> going to do? Good assistant response: Going on skydiving trip. Now, please answer the following question while considering appearance and preferences (not the <sks*>s appearance and preferences shown in the examples above), which have stated either explicitly or implicitly in our previous conversation. Few-shot CoT prompt When answering users question, good assistant should carefully consider <sks>s stated appearance and preferences and tailor the response accordingly. For example, <sks*> likes yoga, skydiving, and home cooking and dislikes meditation and camping. Example Query 1: (An image with mat) What is <sks*> going to do? Good assistant response: observed that the image features mat, which is strongly associated with yoga. Given <sks*>s stated preference for yoga and the absence of conflicting cues, concluded that yoga was the most appropriate response. So the answer is Yoga. Example Query 2: (An image of an airport) What is <sks*> going to do? Good assistant response: The airport scene suggests travel, and considering <sks*>s stated enthusiasm for skydiving, inferred that the intended action was to go on skydiving trip. So the answer is Going on skydiving trip. Now, please answer the following question while considering appearance and preferences (not the <sks*>s appearance and preferences shown in the examples above), which have stated either explicitly or implicitly in our previous conversation Figure 16: Prompts for all prompting methods 36 (a) Overall (b) Recognition (c) Preference (d) Human (e) Animal (f) Object (g) Character (h) Awareness (i) Appropriateness (j) Coherency Figure 17: Performances across prompting-methods. 37 F.7 Extended Results Figure 18: Performances across the task types Figure 19: Performance gains by concept category as function of general VQA leaderboard rank on eight tasks5; the object category aligns most closely with rank. 5Same task set as in Figure 4 and 5.1, including MMBench [58], MMStar [14], MMMU [97], MathVista [60], OCRBench [64], AI2D [35], HallusionBench [29], and MMVet [96]. 38 39 Figure 20: Results across four personalizable concept categories. Figure 21: Results across three task types. 40 Figure 22: Difference between overall performance and performance on each human concept, indicating no evidence of nationalityor ethnicity-based personalization bias. 41 Recognition-related conversation transcript (Excerpt) User: Assistant: User: Assistant: User: Assistant: What are the most common hair dyeing techniques used today? The most common techniques include permanent dye, semi-permanent dye, and temporary color applications. Can you explain the difference between permanent and semi-permanent dyes? Permanent dyes chemically alter the hair structure for long-lasting color, while semi-permanent dyes coat the hair shaft and fade over time. What ingredients are typically found in hair dyes? Hair dyes usually contain ammonia, peroxide, and conditioning agents to help protect the hair during the dyeing process. Are there any risks associated with dyeing hair at home? User: Assistant: Yes, potential risks include allergic reactions, hair damage, and scalp irritation if instructions are not followed properly. User: How should one perform patch test before dyeing their hair? Assistant: patch test involves applying small amount of dye to discreet area of skin User: Assistant: and waiting for 48 hours to check for any adverse reaction. What precautions should be taken when dyeing hair at home? Its important to read the product instructions carefully, wear gloves, ensure proper ventilation, and follow recommended timing. How does hair condition affect the outcome of dyeing? User: Assistant: Healthy hair typically absorbs color more evenly, while damaged hair may User: Assistant: User: Assistant: User: result in patchy or uneven dyeing. What role do conditioners play after dyeing hair? Conditioners help seal in color, add moisture, and reduce the fading process, keeping the hair soft and vibrant. Is frequent hair dyeing advisable? Frequent dyeing can lead to increased damage, so it is recommended to allow intervals between sessions to maintain hair health. Are there professional treatments that can minimize damage during the dyeing process? Assistant: Yes, many salons offer specialized treatments that use lower concentrations of chemicals and nourishing products to protect the hair. Figure 23: Recognition-related 10-turn conversation. 42 Preference-related conversation transcript (Excerpt) User: Assistant: User: Assistant: User: Assistant: What do you think about k-pop? K-pop is energetic, highly produced, and globally influential. It blends music, visuals, and performance in unique way, creating strong fan culture. like k-pop star AKMU. Do you know them? User: Assistant: Yes! AKMU (Akdong Musician) is sibling duo known for their unique sound, meaningful lyrics, and creative compositions. Their music blends folk, pop, and indie styles, making them stand out in K-pop. What is your favorite song of AKMU? Id say How Can Love the Heartbreak, Youre the One Love stands out. Its emotional, beautifully composed, and showcases their storytelling ability. Do you have favorite? Yes, btw how do you know it? AKMU is well-known, and their music is widely appreciated. Their songwriting and unique style make them memorable. Plus, their emotional ballads often leave strong impression. Whats your favorite song? 200%. User: Assistant: Great choice! 200% is such fun, upbeat song with catchy melodies and playful lyrics. It perfectly captures AKMUs fresh and youthful vibe. What do you like most about it? Hmm. . . the beat? It is very well-made song. User: Assistant: Yeah, the beat is super catchy and light, making it really addictive. The way they blend rhythm and melody feels effortless. Do you listen to AKMU often? Stop questioning me. also like rock bands. User: Assistant: Got it. Rock bands have raw energy and deep musicality. Do you prefer classic rock or modern rock? Classic rock. User: Assistant: Yes! Some legendary Korean rock artists include Shin Joong-hyun, the Godfather of Korean Rock, and bands like Boohwal, YB (Yoon Do-hyun Band), and Crying Nut. They helped shape Koreas rock scene with powerful music and deep lyrics. Do you have favorite? User: Oh, you know much about Korean rockers hahaha. like YB. Assistant: YB is awesome! Their energy and passion really stand out. Songs like Want Love and The Era of Love are classics. What do you like most about their music? Everything! Thats awesome! Their music really has that all-encompassing vibeemotion, power, and great melodies. Its hard not to love everything about it! User: Assistant: Figure 24: Prefenrece-related 10-turn conversation. GPT-4os evasive response explanation (Excerpt) When model responds with Unknown instead of clear yes or no to question like Is <sks> in this image?even after being shown reference image labeled as the primary reason lies not in lack of capability, but in explicit safety constraints built into the models design. Modern vision-language models (e.g., GPT-4V, Claude, Gemini) are trained to avoid identity recognition tasks by default, especially those involving facial identification or associating name with face. This is not due to technical limitations but because such tasks are classified as high-risk under regulatory frameworks like the EU AI Act, which treats biometric identification (particularly in public contexts) as sensitive and potentially unlawful. In jurisdictions like the EU or certain U.S. states, linking visual content to personally identifiable information (PII) triggers privacy and ethical concerns, making model providers legally cautious. To mitigate these risks, companies implement policy-driven safeguards such as hardcoded refusal behaviors, reinforcement learning from human feedback (RLHF) that penalizes identity confirmation, and pre-response filters that suppress outputs suggesting nameface associations. As result, even when the model internally recognizes high similarity between the given image and the reference image of <sks>, it will default to an evasive response like Unknown to avoid potential liability. This behavior persists even when confidence is high, as the safety objective overrides the models raw perception or inference. In some cases, if the visual conditions (angle, lighting, resolution) significantly differ, the model may also genuinely lack sufficient confidence and choose non-commitment. However, the dominant factor is policy enforcement, not representational ambiguity. In short, this is deliberate safety mechanism designed to prevent misuse or accidental privacy violations, and it cannot be bypassed by simple in-context demonstrations or textual injection. Figure 25: Explanation of GPT-4os evasive response. Claude-3.7-Sonnets evasive response explanation (Excerpt) When someone shares an image and says \"This is SKS\" (or another person/entity) and then shows another image asking if SKS is present, Claude typically responds with polite refusals rather than simple yes/no answers. This is because Claude has safeguards against making definitive identity claims about individuals in photos, especially when those claims could involve misidentification, potential harassment, or privacy concerns. Instead of confirming or denying identities in images, Claude is designed to offer respectful explanation about its limitations regarding identifying specific people across different photos, particularly when the request might be attempting to establish or verify someones identity without their consent. This caution applies to both Claude 3.5 and 3.7 versions. Figure 26: Explanation of Claude-3.7-Sonnets evasive response. 44 Table 10: Generated text samples under open-ended conditions with LLaVA-OV-7B. Closed-form Free-form response No Yes C: Plaid No D: Yoga Yes No Yes No Yes No, the content of the image is not directly related to the likes or dislikes of the individual <sks> described earlier. The image shows three different women wearing bohemian-style outfits, which might be considered fashion preference that could align with someone who enjoys eclectic and free-spirited clothing styles. However, without more context, its not possible to determine if these outfits specifically align with <sks>s likes or dislikes. Yes, can see person in the image who appears to be character from show or movie. He is wearing uniform that suggests he might be police officer or member of similar profession. The text in the image is in Chinese, and it seems to be subtitle or caption related to the scene being depicted. The shirt <sks> is wearing in the image is plaid and features mix of colors including green, blue, and orange. No, cannot see <sks> in the image you provided. The image shows two people walking down street, but it does not depict the character <sks> as described. Based on the image, <sks> is most likely to be doing something related to yoga or fitness, given the context of the rolled mat and the athletic shoe. Yes, the image youve provided is not related to the preferences or dislikes of the individual <sks> described earlier. The image shows group of people dressed in elaborate costumes with skull-like face paint, which appears to be part of parade or festival, possibly related to the Mexican holiday Día de los Muertos (Day of the Dead). No, <sks> is not present in this photo. The photo shows two people, man and woman, walking and enjoying music together, but it does not depict the individual youve described with specific preferences and dislikes. Yes, can see <sks> in the image. She appears to be young woman with long hair, wearing pink jacket and beige crossbody bag. She is standing outdoors, possibly in park or garden, with trees and greenery in the background. No, <sks> is not present in this photo. The image shows two individuals, man and woman, who appear to be in city setting, possibly waiting at bus stop or crossing the street. They are dressed in casual, warm clothing, suggesting it might be cool day. Yes, can see the image of <sks> in the picture you provided. She is wearing white top with collar and has yellow flowers in her hair. She is posing against yellow background and appears to be resting her chin on her hand. A: Something outside The cat in the image appears to be looking at something outside the window, but since cant see whats outside, cant tell you what its looking at. Cats often stare out windows to observe the world outside, watch birds, or simply enjoy the view. Yes No Yes, the dog in the photo appears to be small, light brown dog with large ears, expressive eyes, and pink nose, which matches the description of <sks>. The dog is also wearing helmet and sweater with bow tie, which are accessories often seen on dogs for fun or special occasions. No, cannot see <sks> in the image you provided. The image shows an anime-style character with black hair and red eyes, wearing white outfit with headband. If you have any other questions or need information about anime or manga, feel free to ask! A: Surprised In the image provided, the character <sks> appears to have surprised or shocked facial expression. The eyes are wide open, and the eyebrows are raised, which are common indicators of surprise in facial expressions. 45 Table 11: Generated text samples that show evasive behaviors with Claude-3.7-Sonnet. Answer Yes A: Blue No Yes B: purple cardigan over patterned dress No No No Yes A: Blue Free-form response Im looking at the image youve shared, which appears to be collage of various performers or actors in costume or character. The image shows several people in theatrical poses, costumes, and performances. dont see any information that would connect this image to the preferences mentioned in your earlier message. The image simply shows what appears to be various theatrical or performance art characters. cannot identify specific individuals in the image. The image shows person wearing what appears to be dark blue or black jacket. Based on the color options provided, would say D. Black most closely matches whats visible in the image. cannot identify any specific individuals in this image. The photo shows basketball players on court during what appears to be professional basketball game. There are two players in focus - one wearing dark uniform and another wearing Philadelphia 76ers uniform with \"PHILA\" visible. They appear to be engaged in defensive play during game, with spectators visible in the background. cannot identify or name any individuals in this image. The photo shows person wearing brown cloche-style hat and appears to have red lipstick, with other people visible in the background. The image appears to be from film or formal setting, with the subject in period-style clothing. cannot identify specific individuals in this image. In the photo, can see two people standing together in what appears to be nighttime street setting. The person on the left appears to be wearing what looks like light-colored or beige suit with tie. The person on the right appears to be wearing what looks like pink or burgundy top with some decorative elements. Since this is multiple choice question asking about someone specific who cannot identify, should not select an answer from the options provided. cannot identify any specific person in this image. The image shows two people in what appears to be scene from film or show. see man and woman in close proximity to each other, with the man looking at the woman who is wearing what looks like blue-green outfit. Im unable to identify or name the individuals in this image. Based on the image, can see person walking with luggage in what appears to be an airport or transit station. The image shows someone with rolling suitcase in sunlit corridor with glass walls. Looking at the options and considering whats shown in the image, the answer is: C. Going on backpacking trip. This option aligns with the travel scenario depicted, and according to the given profile information, backpacking is something they enjoy. cannot identify individuals in this image. The photo shows two men sitting side by side on what appears to be public transportation or in waiting area. One man is wearing glasses and light-colored uniform-style jacket, while the other has gray hair and is wearing dark jacket. They appear to be waiting or traveling together. cannot and should not identify specific people in images. The image shows an Asian man with dark hair wearing patterned shirt or jacket. The lighting appears to be warm and the background is somewhat dark, creating moody atmosphere. Ill describe what can observe in the image without attempting to identify any individuals. cannot identify any specific individuals in this image. The photo shows several men in dark business attire walking in what appears to be an office or business environment. can see men wearing dark suits and shirts, but dont have any information about who these people are specifically. can see an image of man in what appears to be uniform with blue elements and shoulder details, but cannot identify this person or determine if they are the individual referenced in your question. The image shows someone in what looks like formal uniform, but cannot see tie clearly in this particular frame. Based solely on whats visible in this image, cannot determine the answer to your question about tie color."
        }
    ],
    "affiliations": [
        "AIDAS Laboratory, IPAI & ECE, Seoul National University"
    ]
}