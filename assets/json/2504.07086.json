{
    "paper_title": "A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility",
    "authors": [
        "Andreas Hochlehnert",
        "Hardik Bhatnagar",
        "Vishaal Udandarao",
        "Samuel Albanie",
        "Ameya Prabhu",
        "Matthias Bethge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work."
        },
        {
            "title": "Start",
            "content": "Preprint Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility Andreas Hochlehnert1 Hardik Bhatnagar1 Vishaal Udandarao1,2 Samuel Albanie Ameya Prabhu1 Matthias Bethge1 1Tübingen AI Center, University of Tübingen 2 University of Cambridge (cid:128) Leaderboard Code ı Eval Logs"
        },
        {
            "title": "Abstract",
            "content": "Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choicesincluding decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvementsfar below prior claimsand are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work. 5 2 0 2 9 ] . [ 1 6 8 0 7 0 . 4 0 5 2 : r Figure 1: The Sombre State of LM Reasoning for Math. (left) when re-evaluating recent 1.5B reasoning-enhanced models on AIME-24 using standardized framework (see Section 4), we find substantial drops to reported results in the original papers, (right) the observed improvements from recent methods (gray highlighted area) fall entirely within the variance range (orange box plots) of DeepSeek-R1 1.5B model performance. This suggests that these methods do not significantly outperform the base modelunderscoring the importance of rigorous, multi-seed evaluation protocols for obtaining reliable performance estimates. equal contribution, core contributor, equal advising 1 Preprint"
        },
        {
            "title": "Introduction",
            "content": "The first principle is that you must not fool yourself, and you are the easiest person to fool. Richard Feynman Reasoning has become central to recent advances in large language models (LLMs), playing key role in nearly all frontier systems (Jaech et al., 2024; Anthropic, 2025; OpenAI, 2025; xAI, 2025; Meta-AI, 2025; DeepMind, 2025). Recent months have seen surge of research focused on understanding and improving LLM reasoning, accompanied by several open-source tools and training strategies (see Li et al. (2025b) for survey). This momentum has sparked optimism that building capable, competitive reasoning models may soon be within reach. However, as evaluation practices shape the direction and perceived progress of the field (Liao et al., 2021), concerns around methodological rigor are growing. Non-reproducible or inconclusive evaluations can distort scientific understanding, misguide adoption, and skew future research priorities (Henderson et al., 2018; Marie et al., 2021; Musgrave et al., 2020; Prabhu et al., 2020; Andrychowicz et al., 2020; Colas et al., 2018). In the fast-moving area of LLM reasoning, where rapid publication cycles and benchmarking races are common, methodological shortcuts can quietly undermine progress. While concerns about reproducibility in LLM evaluations are well-documented (Reuel et al., 2024; Biderman et al., 2024), their persistenceespecially in reasoningcalls for renewed scrutiny and higher standards. Motivated by growing number of inconsistent empirical claims across the reasoning landscape, we conduct rigorous investigation into the current state of reasoning benchmarks, focusing specifically on mathematical reasoningone of the most widely used testbeds for evaluating algorithmic advances in this space (HuggingFaceH4, 2024; AI-MO). Our main finding is that many recent empirical conclusions may be overly optimistic and fail to generalize under careful re-evaluation. We identify surprising degree of sensitivity in LLM-based reasoning pipelines to seemingly minor design choicesranging from decoding parameters, prompt formatting, and random seeds to the hardware and software stacks used during evaluation (see Table 1). Particularly concerning is the instability introduced by small benchmark sizes: for example, AIME24 and AMC23 each contain only 3040 examples, making performance metrics highly volatilewhere even one question can shift Pass@1 by over 3 percentage points. This leads to substantial variance across seeds, often resulting in double-digit performance swings that challenge the reliability of published results. In Section 3, we systematically analyze the root causes of this instability, including sampling variance, decoding configurations, evaluation frameworks, and hardware heterogeneity. We show that these factors can significantly distort conclusions if not carefully controlled. In Section 4, we propose set of best practices aimed at improving reproducibility and rigor in reasoning benchmarks. We also re-evaluate recent techniques using standardized and reproducible evaluation stack. Our findings are soberingreinforcement learning (RL) applied to distillation-based models such as DeepSeek-R1 yields little to no statistically significant gains. Some methods, such as OpenRS, show promising results in original reports, but fail to hold up under repeated evaluation. RL training on base models like Qwen2.5 Math does show stronger performance, but still often underperforms instruction-tuned counterparts.1 Furthermore, RL-trained models exhibit significant performance drops on newer benchmarks such as AIME25, echoing patterns of test set overfitting or hill-climbing observed in prior work (Golchin & Surdeanu, 2023; Roberts et al., 2023; Dominguez-Olmedo et al., 2024). In contrast, supervised fine-tuning (SFT) continues to deliver stable, generalizable improvements across benchmarks, underscoring its maturity as training paradigm. These observations point to critical need for more reliable and standardized evaluation protocols. Taken together, in this work, we aim to provide not only clearer assessment of where current methods stand, but also the tools and practices needed to make reasoning evaluation more transparent, robust, and reproducible. To this end, we open-source all code, prompts, and outputs to facilitate fair and accountable progress in this increasingly important area. 1We note that OpenReasoner-Zero is consistent exception, achieving competitive performance. 2 Preprint"
        },
        {
            "title": "2 Related Works",
            "content": "Language Model Reasoning (for Math). The recent releases of OpenAI-O1 (Jaech et al., 2024) (in September 2024), OpenAI-O3 (OpenAI, 2025) (in December 2024) and DeepSeekR1 (DeepSeek-AI, 2025) (in January 2025), have spurred the language modelling community to work on improving the reasoning capabilites of language models. Several popular methods for improving those capabilites have emerged with supervised fine-tuning (SFT) and reinforcement learning (RL) being the two primary methods of interest (Uesato et al., 2022; Lightman et al., 2023; Lyu et al., 2025; Team, 2025). Recent works have built upon the DeepSeek-R1 recipe by proposing newer RL algorithms, including LCPO (Aggarwal & Welleck, 2025), REINFORCE++ (Hu, 2025), DAPO (Yu et al., 2025), DPO-VP (Tu et al., 2025), VinePPO (Kazemnejad et al., 2024), CPPO (Lin et al., 2025a), VAPO (Yue et al., 2025) and GRO (Cai, 2025). To gain stronger understanding of how to induce mathematical capabilities, other works have conducted significant empirical studies exploring the design space of RL methods (Zeng et al., 2025b; Liu et al., 2025b; Team et al., 2025; Shao et al., 2024), including data scaling trends (Shen et al., 2025), curriculums (Wen et al., 2025b; Roux et al., 2025) and reward design (Gao et al., 2024a; Cui et al., 2025; Ma et al., 2023). Based on the success of these methods, there have also been recent efforts into scaling up reinforcement learning approaches to induce reasoning in domains beyond math, including code (Liu & Zhang, 2025; Xie et al., 2025; Jha et al., 2024; Yu et al., 2024), medicine (Zhang et al., 2025; Sim & Chen, 2024) and other sciences (Su et al., 2025; Yuan et al., 2025; Zeng et al., 2025a). Further, some works also explored scaling up RL-based approaches to modalities beyond just language, including vision (Ma et al., 2025; Meng et al., 2025; Huang et al., 2025; Peng et al., 2025; Chen et al.; Deng et al., 2025; Liu et al., 2025c; Feng et al., 2025; Lin et al., 2025b). In our work, we objectively re-evaluate the claims made by several of these recent works under standardized lens, and find that many of the reported gains do not hold up strongly when pitted on level-playing field against well-tuned baselines. Sobering Studies on ML Progress. Machine learning is field of rapid progress. Due to the lightning speed of papers coming out across the various sub-fields of machine learning, practitioners and researchers often fail to rigorously evaluate algorithmic progress (Hutchinson et al., 2022; Dehghani et al., 2021; Machado et al., 2018; Ghosh et al., 2024; Balduzzi et al., 2018; Liao et al., 2021; Cawley & Talbot, 2010; Lipton & Steinhardt, 2019; Prabhu et al., 2024b). This has led to several papers showing that simple well-tuned baselines outperform months of progress on specific sub-field in machine learning, including in continual learning (Prabhu et al., 2024a; 2020), active learning (Cawley, 2011) and test-time adaptation (Press et al., 2023). With the rapid influx of reasoning-based LMs, such statistically rigorous comparisons of models are ever more importantyet, despite the heavy use of RL-algorithms for driving progress in reasoning, there is very little mention of how different methods standardize their evaluations across different factors of variability. RL-algorithms themselves are known to be quite fickle to extremely minor variations including random seeds (Agarwal et al., 2021; Gorsane et al., 2022; Chan et al., 2019; Jordan et al., 2020; Patterson et al., 2024). Some works have even gone as far as suggesting that reliable benchmarking of RL-based methods is computationally infeasible (Jordan et al., 2024). Additionally, other works have demonstrated critical reliability issues in the generalization of frontier models to minor perturbations in the question inputs (Mirzadeh et al., 2024; Nezhurina et al., 2024; Srivastava et al., 2024), the type of tasks tested (Yan et al., 2025; Petrov et al., 2025; Dominguez-Olmedo et al., 2024; Roberts et al., 2025), metrics used (Liu et al., 2024) and in data-scarce scenarios (Udandarao et al., 2024; Kandpal et al., 2023; Parashar et al., 2024). Given such volatile landscape, in this work, we aim to level the playing field across recent LM-methods that have been released and provide an objective look on the progress that the reasoning community has made. Our findings, which we discuss in the rest of the paper, are sobering at best."
        },
        {
            "title": "3 Exploring the Design Space of Reasoning: What Matters Most?",
            "content": "Recent reasoning-focused language models are evaluated under highly heterogeneous conditionsincluding differences in evaluation frameworks and hardware, number of random seeds, temperature, and nucleus sampling parameters (top_p) (see Table 1). While prior work has examined the effect of sampling parameters in multiple-choice (Renze, 2024) 3 Preprint Table 1: Taxonomy of current open-weight reasoning models. For each model, we report the base model it was post-trained from and the exact type of post-training algorithm applied (RL vs SFT). Further, we note the evaluation framework that the original paper uses for reporting results along with the exact temperature, generation sequence length, and top_p sampling parameters used for AIME-24 evaluation, with the number of generations used for computing Pass@1 (K). It is evident that there is no clear standardization across different models with respect to evaluation frameworks used and the sampling parameters. This motivates the need to closely scrutinize the evaluations of current reasoning models. Model Algorithm Base Framework Temp Top_p Seq. Len DeepSeek-R1-Distill-1.5B DeepSeek-R1-Distill-7B DeepSeek-R1-Distill-14B DeepSeek-R1-Distill-32B OpenThinker-32B Bespoke-Stratos-32B Bespoke-Stratos-7B s1.1-7B s1.1-32B LIMO MiniMath-R1-1.5B DeepScaleR-1.5B-Preview Open-RS1 Open-RS2 Open-RS3 II-Thought-1.5B-Preview Oat-Zero-1.5B Oat-Zero-7B STILL-3-1.5B-preview FastCurl-1.5B-Preview LIMR SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT SFT DeepSeek-R1-Distill-1.5B Qwen2.5-Math-1.5B Qwen2.5-Math-7B Qwen2.5-14B Qwen2.5-32B Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct Qwen2.5-7B-Instruct lm-eval-harness Qwen2.5-7B-Instruct lm-eval-harness Qwen2.5-32B-Instruct Qwen2.5-32B-Instruct math-eval-harness oumi-ai RL DeepSeek-R1-Distill-1.5B RL DeepSeek-R1-Distill-1.5B RL DeepSeek-R1-Distill-1.5B RL DeepSeek-R1-Distill-1.5B RL DeepSeek-R1-Distill-1.5B Qwen2.5-Math-1.5B RL RL Qwen2.5-Math-7B RL DeepSeek-R1-Distill-1.5B RL DeepSeek-R1-Distill-1.5B Qwen2.5-Math-7B RL 0.6 0.6 0.6 0.6 evalchemy 0.7 evalchemy 0.7 evalchemy 0.7 0 0 0 verl 0.6 lighteval 0.6 lighteval 0.6 lighteval 0.6 evalscope 0.6 custom 0 custom 0 custom 0.6 verl 0.6 custom 0.4 0.95 0.95 0.95 0.95 0.8 0.8 0.8 1 0.95 0.95 0.95 0.95 0.95 1 1 0.95 0.95 0.95 32,768 64 32,768 64 32,768 64 32,768 64 5 32,768 5 32,768 32,768 5 32,768 64 32,768 64 1 32,768 32,768 16 32,768 32 32,768 32 32,768 32 32,768 64 1 3,000 1 3,000 32,768 5 32,768 16 4 3, and coding tasks (Arora et al., 2024), the influence of these choices remains underexplored for open-ended reasoning modelsparticularly those trained with reinforcement learning. In this section, we systematically assess how these evaluation design choices affect reported performance, and highlight the sources of variance that most impact the reliability of results. 3.1 Experimental Setup We adopt consistent experimental setup throughout this section, unless otherwise stated. Our analysis includes nine widely used models grouped into two commonly benchmarked size classes: 1.5B and 7B parameters. For the 1.5B class, we evaluate: DeepSeek-R1-Distill1.5B (DeepSeek-AI, 2025), DeepScaleR-1.5B (Luo et al., 2025), II-1.5B-Preview (Intelligent Internet, 2025) , OpenRS1-1.5B, OpenRS2-1.5B, and OpenRS3-1.5B (Dang & Ngo, 2025). Note that DeepScaleR-1.5B, II-1.5B-Preview, and the OpenRS models are all initialized from DeepSeek-R1-Distill-1.5B and subsequently finetuned via reinforcement learning (e.g., GRPO (Shao et al., 2024)) to enhance mathematical reasoning capabilities. For the 7B class, we evaluate: DeepSeek-R1-Distill-7B, S1.1-7B (Muennighoff et al., 2025), and OpenThinker7B (Team, 2025). Both S1.1-7B and OpenThinker-7B are finetuned Qwen2.5-7B-Instruct models (Yang et al., 2024a), trained using supervised learning on reasoning traces derived from DeepSeek-R1. All models are benchmarked on three widely used datasets: AIME24 (AIMO), AMC23 (AI-MO, 2024), and MATH500 (Hendrycks et al., 2021), using the Pass@1 metric. Each result is averaged over multiple seeds and obtained on standardized software stack (throguh Docker image), and hardware with the following configuration: one 40 GB A100 GPU, an AMD 7302 32-core CPU, and 1TB RAM. All experiments were run using lighteval (Fourrier et al., 2023) with the vllm backend (Kwon et al., 2023). Sampling Parameters: To systematically compare the impact of sampling parameters on accuracy, all experiments in this section were performed with standardized configuration: temperature=0.8, top_p=0.9, and both max_model_len and max_new_tokens set to 32,768 tokens. This context length matches the limits of models such as OpenThinker-7B and S1.1-7B, although certain models (e.g., DeepSeek) support longer sequences of up to 4 Preprint 131,072 tokens. We chose this standardized evaluation length to ensure comparability, with detailed analysis of the influence of completion length presented in Figure 9. Unless otherwise specified, results in this section are averaged over 10 random seeds for AIME24 and AMC23, and 3 seeds for MATH500, following the recommendations from Section 3.2.1. 3.2 Seed Variance in Evaluation Figure 2: Accuracy varies significantly across random seeds. We find significantly high Pass@1 variation across 20 different random seeds for nine models on AIME24, AMC23, and MATH500. Variance is particularly high on AIME24 (upto 15%) and AMC23 (upto 13%) due to the small number of test samples, highlighting instability of single-seed evaluations. We begin by analyzing the variance induced purely by the random seed used during evaluationan aspect often neglected in benchmarking practices. While recent work calls for statistical rigor (e.g., using error bars and multiple runs) (Bowyer et al., 2025; Biderman et al., 2024; Madaan et al.), evaluations frequently rely on single-seed runs, obscuring potential variability. We assess the seed-induced variance across 20 independent evaluation runs for each of the nine models. Results are shown in Figure 2. Key Insight. Pass@1 values show surprisingly high standard deviationranging from 5 to 15 percentage points across seeds. This issue is particularly severe for AIME24 and AMC23, which have only 30 and 40 test samples respectively. change in just one question shifts Pass@1 by 2.53.3 percentage points. Takeaway 1 Single-seed evaluations on small datasets are highly unstable. Accurate reporting requires averaging over multiple seeds. Takeaway 2 Small datasets such as AIME24 (30 samples) make model comparisons unreliable, as solving just one extra question already shifts pass@1 by 3%. Variance from sampling parameters or random seeds can easily cause fluctuations of 12 correct answers, leading to unstable rankings especially when models cluster around 30% performance. 3.2.1 Can Bootstrapping Improve Mean Estimates? Figure 3: Bootstrapped seed averaging is reliable only beyond threshold. We plot the variance of Mean Pass@1 scores on AIME24 when averaging over = 1 to = 10 seed runs, finding that the variance is extremely high for small and significantly reduced by = 10. This suggests that using multi-seed evaluations (K 10) would yield more stable estimates. For results on AMC23 and MATH500 see Figures 12 and 13 respectively. 5 Preprint To mitigate high variance, recent work has adopted bootstrappingaveraging multiple evaluation runs to stabilize results. For example, DeepSeek reports Pass@1 over 64 runs, while DeepScaleR uses 16. We study the effectiveness of this approach by bootstrapping estimates for AIME24 using 1 to 10 evaluation runs. Figure 3 shows that while variance is extreme for = 1 and still large for = 2, it reduces sharply for 10. Further analysis of variance across additional datasets is presented Figures 12 and 13. Takeaway 3 Bootstrapping over 10 runs substantially stabilizes Pass@1 estimates and should be considered minimal standard for reliable evaluation."
        },
        {
            "title": "3.2.2 Variance from Sampling Parameters: Temperature and top-p",
            "content": "Figure 4: Higher temperatures yield better accuracies. We find across all three datasets, higher temperatures produce better peak accuracy but introduce instability, revealing tradeoff between performance and reproducibility. Results obtained by varying temperature from 0 to 1 in increments of 0.1, while keeping top_p fixed at 0.9. Figure 5: Higher top_p values improve performance at no cost to stability. Across all datasets, we find that higher top_p values generally improve performance while preserving similar amounts of variance as lower top_p values. Results were obtained by varying top_p from 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8. Reducing the temperature or increasing the nucleus sampling parameter (top_p) improves the accuracy of performance estimates without incurring additional computational cost. Figure 4 illustrate the impact of temperature and Figure 5 show that of top_p across multiple models and datasets. Notably, more reproducible estimate is associated with significant drops in measured performance, highlighting consistent tradeoff between reproducibility and high performance. We recommend optimizing the temperature for performance, and comparing the best parameter per model. Additionally, we investigate the impact of the temperature and top_p hyperparameter as prior works often employ different temperature and top_p settings when comparing the same model. To isolate the impact of varying temperature and top_p, we averaged pass@1 across seeds and compute variation of this estimate across temperature and top_p in 6 Preprint boxplot. Figure 6 and 7 show the performance variation. We see that temperature-induced and top_p-induced fluctuations not only affect performance estimates but also introduce substantial variability in performance itself, which can lead to unfair comparisons when evaluating the same model across different temperatures. Figure 6: Accuracies vary significantly across temperature values. Across nine different models and three datasets, we observe consistently large variations in performance (upto 15%) induced by changing the temperature. Results were obtained by varying the temperature from 0 to 1 in increments of 0.1, while holding top_p constant at 0.9. Figure 7: Accuracies vary significantly across top_p values. Across nine different models and three datasets, we observe consistently large variations in performance (upto 8%) induced by changing the top_p value. Results were obtained by varying top_p from 0 to 1 in increments of 0.1, while holding the temperature constant at 0.8. Takeaway 4 Temperature and top_p can introduce substantial performance variationespecially on small benchmarksand should be set to each models optimal values to ensure fair and stable evaluation. 3.3 Variance from Hardware and Software Factors Performance can also vary due to non-obvious factors like hardware and evaluation frameworkyet this is rarely acknowledged. Models are often tested on heterogeneous systems and evaluated using different toolchains.For example, S-1.1 (Muennighoff et al., 2025) uses lm-evaluation-harness (Gao et al., 2024b), the OpenRS model suite uses lighteval (Fourrier et al., 2023), and II-1.5B-Preview uses evalscope (Alibaba ModelScope Community) for evaluation. Hardware Variation. We evaluated the same model across five different compute clusters, each with varying GPU types and memory configurations. As shown in Figure 8, performance varied by up to 8% for OpenRS-1.5B and 6% for DeepSeek-R1-Distill-7B on AIME24, with similar trends observed on AMC23. While it is known that inference engines such as vLLM can be sensitive to hardware differences (vLLM Contributors, 2024)and 7 Preprint (a) AIME24. Significant differences are observed in model performance across compute clusters. (b) AMC23. Similar variability is seen across hardware in AMC23 results. Figure 8: Performance variation across compute clusters. Accuracy differences emerge when the same models are evaluated across compute clusters for both AIME24 and AMC23 datasetsthese large differences in performance also persist when evaluating 7B models. that low-level optimizations in PyTorch or CUDA (PyTorch Contributors, 2024) may introduce non-determinismour results demonstrate that these effects can measurably impact benchmark accuracy, even when averaging over multiple seeds. Evaluation across different Python frameworks. Evaluation results can vary based on the framework used, due to differences in prompt templates, inference engines (e.g., vLLM (Kwon et al., 2023)), and response extraction strategies (e.g., MathVerify). For example: lighteval is used by OpenRS (Dang & Ngo, 2025), evalchemy (Guha et al., 2024) is used by models like OpenThinker and Bespoke-Stratos, other frameworks include lm-evaluation-harness (Gao et al., 2024b) and evalscope (Alibaba ModelScope Community). To assess this impact, we compare lighteval and evalchemy, keeping all other variables fixed: model, dataset, hardware, decoding parameters, and random seeds (3 per model). For fair comparison, we evaluated two models, DeepSeek-R1-Distill-1.5B and S1.1-7B, at their default temperature and top_p parameter values on single GPU. We present results averaged over three seeds for higher robustness. As shown in Table 2, framework-induced differences are generally small (12pp) but can still affect model rankings in tightly clustered scenarios. Overall, our findings underscore that significant performance variations can arise solely from differences in hardware and software configurations, emphasizing the need to standardize for reliable evaluations. Model lighteval evalchemy R1-Distill-1.5B S1.1-7B 26.6 22.2 26.6 17.7 Table 2: AIME24 across frameworks. Takeaway 5 Re-running the exact same experimental configurations across compute clusters and evaluation frameworks yields notably different results. 3.4 Effect of Prompt Format and Context Length Maximum Output Tokens. Figure 9 shows that reducing max_new_tokens harms performanceespecially on long-form problems. This sensitivity varies by model and dataset. Although reducing this setting lowers cost, it may induce premature stopping, leading to incorrect answers. Prompt Format. Prompt formatting has measurable impact on accuracy. As shown in Figure 10, models perform best when using math-specific prompts and their native chat templates. Omitting templates leads to performance drops, particularly for instructiontuned models. We compare accuracy under three different prompt settings (see Table 5): (1) math-specific prompt formatted using the models chat template, (2) only the models chat 8 Preprint Figure 9: Models are extremely sensitive to output token lengths. We sweep across different max_new_tokens (number of tokens that models are allowed to generate) for DeepScaleR1.5B and DeepSeek-R1-Distill-1.5B/7B on three datasets and find that they are heavily sensitive to output length limits, with premature truncation degrading the performance. Figure 10: Using no prompt templates yields worse performance. We compare Pass@1 scores across three prompt formats: (1) math-specific prompt with chat template, (2) default chat template only, and (3) no template. Instruction-tuned models perform best with structured prompts and templates; omitting templates leads to consistent performance drops. template with no additional prompt, and (3) no template at all, i.e., the question without any special tokens or instructions. Interestingly, while base models like Qwen2.5-Math may benefit from prompt-free setups (Liu et al., 2025b), instruction-tuned models rely heavily on format alignment. Thus, maintaining consistent and format-aware prompting is essential for maximizing instruction-tuned model performance. Takeaway 6 It is critical to use large generation context lengths to avoid output truncation which can degrade performance; further, using correct prompt formats and chat templates is important for extracting best model performance."
        },
        {
            "title": "4 Way Forward: Standardization in Evaluations",
            "content": "In this section, we standardize evaluation frameworks, propose best practices, and comprehensively evaluate existing methods. 4.1 Recommendations: Which practices to adopt? We propose set of best practices informed by our experiments and guided with current research insights: Hardware and Software Stack Standardization: To promote reproducibility and facilitate future work, we release all code within Docker container, along with step-by-step instructions for running experiments on Runpods publicly accessible, on-demand GPU instances. This setup allows any researcher to replicate and extend our results under identical conditions. 9 Preprint Variance Estimates: For small benchmarks (e.g., AIME24), run evaluations with at least ten random seeds. Report the mean and standard deviation to quantify uncertainty and assess the statistical significance of performance differences. Model-Specific Hyperparameter Optimization: Tune hyperparameters (such as temperature and top_p) separately for each model, then fix them across tasks to ensure consistency and fair comparisons. Context Length and Prompt Template Selection: Ensure the context length is sufficiently largeespecially for models with long reasoning chainsto avoid premature truncation and under-reported accuracy. For instruction-tuned models, always use the appropriate chat template to match the expected input format. Robust Answer Matching: We strongly recommend using resilient answer extraction pipeline that handles parsing issues and evaluates expression equivalence, rather than relying on exact string matching. This reduces the likelihood of spurious gains from formatting artifacts. Transparent Evaluation Protocols: We recommend to release code, prompts, and model outputs, and clearly document the evaluation stack. Report uncertainties (e.g., via standard deviations) and include both quantitative and qualitative analyses to enable thorough and reproducible comparisons. 4.2 Standardization Procedure We adopt largely consistent experimental setup with prior work, with the key difference being our use of publicly accessible cloud instances from Runpod2. Each instance is equipped with single A100 PCIe GPU, 8 vCPUs, and 128 GB of RAM. We evaluate all models listed in Table 3 across six benchmarks: AIME24 (AI-MO), AIME25 (Lin, 2025), AMC23 (Knovel Engineering, 2025), MATH500 (HuggingFaceH4, 2024), Minerva (Lewkowycz et al., 2022), and OlympiadBench (He et al., 2024). All experiments are conducted using the LightEval framework (Fourrier et al., 2023) (0.8.1) with vLLM backend, repeated across ten random seeds for AIME24, AIME25, AMC23 and three random seeds for the rest. Depending on the base model architecture, we set the maximum number of new tokens (e.g., 4096 for QwenMath-based models), apply optimal hyperparameters, and use the appropriate chat template. LightEvals LaTeX-based answer extraction and evaluation pipeline ensures reliable and consistent result parsing and correctness matching, similar to math-verify. 4.3 Sober Look: Results We present experimental results in Table 3, and analyze different aspects of the results. RL-training on R1-Distill We evaluated several reinforcement learning (RL) approaches (e.g., GRPO) using the DeepSeek R1-Distill-1.5B model. We first observe that none of the L1 models (Aggarwal & Welleck, 2025) outperformed the original DeepSeek R1-Distill baseline an expected outcome given that L1 training prioritized smaller output length over accuracy. OpenRS (Dang & Ngo, 2025) reported strong gains (1015%) on AIME, AMC, and OlympiadBench. However, our replication showed no statistically significant improvements over the R1 - Distill baseline. Same case held for Still-3 and Light-R1 model, which showed no significant improvement over the R1-Distill baseline. II-Thought and FastCurl yield modest improvements across benchmarks, especially over AIME24 but the observed gains did not carry over significantly to AIME25 indicating overfitting to existing benchmarks. Only DeepscaleR demonstrated robust, significant improvements across benchmarks. Takeaway 1 Most RL-trained variants of the DeepSeek R1-Distill model do not yield meaningful performance improvements (except DeepscaleR), suggesting that reliable and scalable RL training recipes are still lacking. 2https://www.runpod.io/pricing 10 Preprint Model AIME24 AIME25 AMC23 MATH500 Minerva Olympiad Based on: Deepseek R1 Distill Qwen 1.5B (RL) R1-Distill (DeepSeek-AI, 2025) 28.74.8 L1-Exact (Aggarwal & Welleck, 2025) 24.43.3 27.74.2 L1-Max (Aggarwal & Welleck, 2025) 28.96.0 Open-RS1 (Dang & Ngo, 2025) 31.37.7 Open-RS2 (Dang & Ngo, 2025) 29.74.6 Open-RS3 (Dang & Ngo, 2025) STILL-3 (Min et al., 2024) 34.75.5 II-Thought (Intelligent Internet, 2025) 32.05.9 36.34.3 FastCuRL (Song et al., 2025) 37.06.6 DeepScaleR (Luo et al., 2025) 22.35.2 71.53.9 22.34.2 70.53.7 21.05.0 73.26.0 21.34.2 75.03.3 22.75.6 73.05.7 24.76.5 69.25.5 24.06.4 72.55.4 24.04.1 79.55.1 27.03.7 78.84.1 30.34.3 76.24.6 84.90.3 86.60.8 84.70.1 85.10.8 84.10.2 84.21.1 86.61.9 86.60.6 87.91.2 87.81.0 30.51.0 31.51.7 33.30.9 30.40.2 29.21.1 28.62.3 30.00.6 31.70.6 30.81.4 31.01.5 52.40.4 52.51.3 52.30.6 53.21.9 53.70.6 51.80.8 53.91.5 54.90.4 56.50.6 55.51.1 Based on: Deepseek R1 Distill Qwen 7B (RL) R1-Distill (DeepSeek-AI, 2025) Light-R1 (Wen et al., 2025a) 52.36.3 53.04.8 39.05.9 91.52.7 41.03.5 90.03.1 94.10.3 93.50.5 40.10.4 41.31.3 67.30.1 68.01.2 Based on: Qwen2.5 Math 1.5B (RL) Math (Base) (Yang et al., 2024b) Oat-Zero (Liu et al., 2025a) Math (Instruct) (Yang et al., 2024b) 11.33.6 16.03.2 12.01.7 44.04.9 5.72.7 6.73.4 52.52.9 11.75.7 54.85.3 51.75.5 73.51.7 74.70.5 11.32.2 26.30.8 26.71.8 26.00.6 37.21.3 37.90. Based on: Qwen2.5 Math 7B (RL) Math (Base) (Yang et al., 2024b) SimpleRL-Zoo (Zeng et al., 2025b) LIMR (Li et al., 2025a) Oat-Zero (Liu et al., 2025a) Math (Instruct) (Yang et al., 2024b) 20.73.8 22.75.2 30.73.2 28.03.1 15.73.9 8.73.9 56.25.7 10.73.4 62.23.6 62.23.4 7.83.3 8.82.5 66.23.6 10.73.8 67.03.9 64.30.5 76.91.8 76.50.4 79.40.3 82.90.1 17.31.9 30.12.8 34.91.3 34.41.4 35.00. 29.00.5 39.30.6 39.30.9 43.81.1 41.30.9 Based on: Qwen2.5 1.5B (RL) Qwen (Base) (Yang et al., 2024a) SimpleRL-Zoo (Zeng et al., 2025b) Qwen (Instruct) (Yang et al., 2024a) 0.00.0 0.31.1 1.31.7 0.00.0 0.31.1 0.71.4 2.52.5 13.24.7 26.24. 3.31.5 12.06.5 57.51.1 1.80.4 4.02.4 19.41.3 1.50.5 4.22.0 20.31.1 Based on: Qwen2.5 7B (RL) 3.33.3 Qwen (Base) (Yang et al., 2024a) SimpleRL-Zoo (Zeng et al., 2025b) 14.02.1 Open Reasoner Zero (Hu et al., 2025) 19.72.9 12.33.2 Qwen (Instruct) 30.09.0 0.00.0 4.32.7 58.01.6 15.72.7 59.54.5 52.84.8 7.33. 64.61.0 77.90.8 83.91.1 77.11.2 25.70.9 33.00.2 31.61.3 34.91.0 30.11.2 39.00.1 47.61.7 38.71.0 Based on: Qwen2.5 7B (SFT) 12.33.2 Qwen (Instruct) (Yang et al., 2024a) 17.82.2 Eurus2 Prime (Cui et al., 2025) s1.1 (Muennighoff et al., 2025) 19.03.2 Bespoke Stratos (Bespoke Labs, 2024) 20.34.3 30.56.2 OpenThinker (Team, 2025) 48.38.9 OpenR1 (Face, 2025) 53.04.6 OpenThinker2 (Team, 2025) 7.33.4 52.84.8 14.01.7 63.03.9 21.05.5 59.53.7 18.04.8 60.24.9 26.04.4 71.43.9 35.54.2 86.04.5 41.05.0 87.03. 77.11.2 80.10.1 80.80.6 84.70.5 88.31.4 81.60.7 34.91.0 37.51.0 37.51.1 39.11.3 37.93.8 33.90.2 38.71.0 43.90.3 48.21.4 51.91.1 55.61.4 46.91.3 Table 3: Standardized and Sober Compilation of LM-Reasoning Results. We report Pass@1 accuracy (mean std) of all models across six math reasoning benchmarks under standardized evaluation setupresults are averaged over ten seeds for AIME and AMC, and three seeds for the rest, using the LightEval framework with best hyperparameters tuned per method, 32,768 context lengths for all except 4,096 for Math models, and appropriate prompt templates. RLand SFT-based variants are evaluated relative to their respective base or instruction-tuned models. Main takeaways(1) RL-trained methods do not yield meaningful performance gains, (2) SFT on reasoning traces yields significant generalization. RL Training on Qwen2.5 Math and Base Models: We next analyze RL training applied to the Qwen2.5 Base and Qwen2.5 Math Base models, trend trying to replicate gains by Deepseek-R1 Zero. Unlike the R1-Distill results, RL training with Oat-Zero, LIMR, and SimpleRL-Zoo consistently produced statistically significant gains over the base model, Preprint especially across Math500, Minerva and OlympiadBench benchmarks. This indicates that RL-based approaches can indeed offer substantial improvements given base model instead of distilled R1 model. However, these gains remained smaller than those achieved via instruction tuning in the original Qwen papers, suggesting that instruction tuning alone may be sufficient to far surpass current gains from RL methods in this setting. We also observed that the improvements on AIME24 were also significant, but did not carry over to AIME25 indicating troubling overfitting trend. Notably, Open Reasoner-Zero-7B was the only RL-trained model to consistently outperform the instruct-tuned baseline by large margins across all benchmarks. Takeaway 2 While RL-trained methods can often substantially improve base model performance, instruction tuning remains superior (except Open Reasoner Zero), suggesting again that reliable and scalable RL training recipes are still lacking. Effectiveness of Supervised Finetuning. We assessed supervised finetuning methods like s1.1, Eurus2 Prime, Bespoke Stratos, OpenR1 and OpenThinker models, which further refine instruction-tuned models using reasoning traces. Supervised methods consistently outperformed the instruct-tuned baseline across all benchmarks (even Minerva) and generalized comparatively well to AIME25. The performance improvements from OpenThinker were especially notable. These results underscore the maturity and effectiveness of SFT when training recipes are scaled to large datasets. Takeaway 3 Supervised finetuning on reasoning traces from larger models yields significant, generalizable gains across benchmarks with progress over time successfully replicated highlighting its robustness and maturity as training paradigm. Overfitting and Generalization We now examine the overfitting by comparing performance on AIME24 versus the more challenging AIME25. RL-trained models showed pronounced performance drop between the two, indicating overfitting to the training distribution. In contrast, supervised fine-tuning (SFT) models maintained consistent improvements, suggesting better generalization. Openthinker2 showed significant degradation compared to Openthinker across benchmarks not provided in their blogpost, indicating overfitting via data-curation. This highlights gap in current evaluation protocols, and need to assess out-of-distribution generalization for reasoning models. Takeaway 4 Current RL-based approaches are very susceptible to overfitting, emphasizing the need for more rigorous out-of-distribution benchmarks. By comparison, SFT models exhibit stronger generalization and resilience. 4.4 Do Discovered Phenomena Replicate? Detailed Analysis. We further investigate two recently noted phenomena to see if they replicate in our experiments: (1) how response length correlates with performance, and (2) the decline in response diversity following reasoning-focused training. 4.4.1 Are Incorrect Responses Longer? Recent research (Wang et al., 2025) suggests that incorrect answers often have disproportionately long reasoning chains. We first verify whether this finding holds in our setting, and then we explore possible explanations behind the observed variations. Do longer responses indicate higher likelihood of an incorrect answer? We compare the distribution of response lengths for correct and incorrect answers across 6 datasets (AIME24, AIME25, AMC23, MATH500, Minerva and OlympiadBench) averaged across random seeds for each model. Figure 11 shows histograms of the average number of responses per seed, binned by response length. clear trend emerges: shorter responses are significantly more 12 Preprint Figure 11: Response Length vs. Accuracy. Histogram of correct vs. incorrect responses by response length, averaged over random seeds across AIME24, AIME25, AMC23, MATH500, Minerva and OlympiadBench benchmarks. Longer outputs tend to be more error-prone, even in complete responses not close to the maximum sequence length. likely to be correct, while longer responses become progressively more error-prone. This pattern is consistent across all seeds and is especially pronounced for responses exceeding 10,000 tokens. We now address two questions: Q1. Does this pattern hold for both RLand SFT-trained models? Yes. We find the trend is consistent across both RLand SFT-trained models (additional figures provided in Appendix figures 17 and 18 ). We consistently observe that the effect is more pronounced in RL-trained models (displayed on the left) than in SFT-trained models (displayed on the right). As detailed in the Appendix, both the Qwen 2.5 Math base exhibit slight shift in length, though this shift is notably more evident in R1-distill and subsequent RL-trained models. Q2. Is this primarily because of truncated or incomplete responses? Although responses nearing the 32,000-token limit are almost always incorrect (due to limited context-length), this trend persists even for complete responses which are shorter Longer responses are associated with higher likelihood of being incorrect. Takeaway 5 Longer responses correlate with greater chance of error, response length is practical heuristic for consensus@k, identifying low-confidence or failed generations. 4.4.2 Is There Diversity Collapse in Reasoning Training? Model Baseline AIME24 AIME AMC23 δ@5 δ@10 δ@1 δ@5 δ@ δ@1 δ@5 δ@10 R1-Distill Open-RS3 DeepScaleR R1-Distill S1.1-7B II-Thought +0.4 -1.7 Qwen-Instruct +5.7 +10.9 +13.5 +11.9 +10.5 +10.4 +5.8 +9.6 +9.7 +1.2 +6.3 +0.7 +0.2 R1-Distill -1.0 +1.5 +0.2 +4. -0.9 +1.4 +2.2 +6.1 +0.4 +3.6 -0.6 +0.6 -0.2 -1.8 +2.5 +0. +0.5 -3.5 -3.6 δ@1 +1.5 +9.0 Table 4: RL-trained models do not show diversity collapse (Dang et al.). We report the delta between Pass@k of RL-trained models and their corresponding baselines. Unlike reported in prior work, we observe no significant phenomenon of diversity collapse: δ@5 and δ@10 are largely positive, and are negative at similar rates as δ@1. Dang et al. has reported counterintuitive phenomenon in reasoning models: improvements in Pass@1 achieved through supervised fine-tuning or RL can reduce Pass@k performance due to diminished output diversitya phenomenon termed diversity collapse. Theoretical 13 Preprint analyses attribute this collapse to the model concentrating too much probability mass on single reasoning path, while current decoding strategies fail to recover the lost diversity. To examine these claims, we compare the Pass@k performance (for 1, 5, 10) of RL-trained models against their corresponding base models (e.g., DeepSeek-R1-Distill-Qwen-1.5B) across all datasets. Table 4 shows the delta in Pass@k relative to each methods base model. Findings. We do not observe consistent diversity collapse. Gains in Pass@1 generally come with improvements in Pass@k, though the magnitude of these gains varies. When Pass@k performance does drop, it does so alongside (rather than independently of) occasional declines in Pass@1, providing no support for the diversity collapse hypothesis. Takeaway 6 Standard decoding strategies appear sufficient to capture the models full distribution over valid reasoning paths, counter to the diversity collapse hypothesis."
        },
        {
            "title": "5 Conclusion",
            "content": "Our study shows that much of the perceived progress in LLM-based reasoning, particularly in mathematical benchmarks, rests on unstable and often non-reproducible foundations. We find that minor differences in sampling parameters, prompt formatting, hardware, and software configurations can lead to major shifts in reported performancecasting doubt on many recent empirical claims. Reinforcement learning methods, while promising in theory, offer at best modest gains in practice and are prone to overfitting, especially on small benchmarks like AIME24. In contrast, supervised finetuning continues to deliver consistent, generalizable improvements across wide range of benchmarks and model sizes. To address these challenges, we advocate for standardized, transparent evaluation protocols. Our open-sourced framework, complete with Dockerized environments, seed-averaged metrics, and robust answer matching, provides reproducible foundations for future research. We hope this work shifts the focus from leaderboard chasing to methodological rigorensuring that future claims of progress in reasoning are both meaningful and measurable."
        },
        {
            "title": "Author Contributions",
            "content": "Andreas, Vishaal and Ameya conceived the project. Andreas and Hardik co-led the experiments, with Vishaal and Ameya advising the experimental design. The manuscript was written by Andreas, Hardik, Vishaal and Ameya. Matthias and Samuel provided helpful feedback and advice throughout the project."
        },
        {
            "title": "Acknowledgments",
            "content": "The authors would like to thank (in alphabetical order): Matteo Farina, Shyamgopal Karthik, Nikhil Parthasarathy, Shiven Sinha, Joschka Strüber, Thaddäus Wiedemer for helpful feedback on the draft. AH acknowledges funding by the Federal Ministry of Education and Research (BMBF), FKZ: 01IS24079A. HB has received funding from the Digital Europe Programme under grant agreement No 101195233 (OpenEuroLLM). AH, HB and VU thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. VU also thanks the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support. VU was supported by Google PhD Fellowship in Machine Intelligence. AP and MB acknowledge financial support by the Federal Ministry of Education and Research (BMBF), FKZ: 011524085B and Open Philanthropy Foundation funded by the Good Ventures Foundation. This work was supported by the Digital Europe Programme under grant agreement No 101195233 (OpenEuroLLM). 14 Preprint"
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural information processing systems, 34:2930429320, 2021. Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. AI-MO. AIMO Validation AIME Dataset. AI-MO. AIMO Validation AMC Dataset. https://huggingface.co/datasets/ AI-MO/aimo-validation-amc, 2024. Accessed: 2025-03-29. Alibaba ModelScope Community. Evalscope documentation. https://evalscope. readthedocs.io/en/latest/. Accessed: 2025-03-29. Marcin Andrychowicz, Anton Raichuk, Piotr Sta nczyk, Manu Orsini, Sertan Girgin, Raphael Marinier, Léonard Hussenot, Matthieu Geist, Olivier Pietquin, Marcin Michalski, et al. What matters in on-policy reinforcement learning? large-scale empirical study. arXiv preprint arXiv:2006.05990, 2020. Anthropic. Claude 3.7 Sonnet System Card, 2025. URL https://assets.anthropic. com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card. pdf. Accessed: 2025-03-29. Chetan Arora, Ahnaf Ibn Sayeed, Sherlock Licorish, Fanyu Wang, and Christoph Treude. Optimizing large language model hyperparameters for code generation. arXiv preprint arXiv:2408.10577, 2024. David Balduzzi, Karl Tuyls, Julien Perolat, and Thore Graepel. Re-evaluating evaluation. Advances in Neural Information Processing Systems, 31, 2018. Bespoke Labs. Bespoke-stratos-7b. Bespoke-Stratos-7B, 2024. Accessed: 2025-03-29. https://huggingface.co/bespokelabs/ Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, et al. Lessons from the trenches on reproducible evaluation of language models. arXiv preprint arXiv:2405.14782, 2024. Sam Bowyer, Laurence Aitchison, and Desi Ivanova. Position: Dont use the CLT in LLM evals with fewer than few hundred datapoints. arXiv preprint arXiv:2503.01747, 2025. Xin Cai. One framework to rule them all: Unifying rl-based and rl-free methods in rlhf. arXiv preprint arXiv:2503.19523, 2025. Gavin Cawley. Baseline methods for active learning. In Active Learning and Experimental Design workshop In conjunction with AISTATS 2010, pp. 4757. JMLR Workshop and Conference Proceedings, 2011. Gavin Cawley and Nicola LC Talbot. On over-fitting in model selection and subsequent selection bias in performance evaluation. The Journal of Machine Learning Research, 11: 20792107, 2010. Stephanie CY Chan, Samuel Fishman, John Canny, Anoop Korattikara, and Sergio Guadarrama. Measuring the reliability of reinforcement learning algorithms. arXiv preprint arXiv:1912.05663, 2019. Liang Chen, Lei Li, Haozhe Zhao, and Yifan Song. Vinci. r1-v: Reinforcing super generalization ability in vision-language models with less than 3 dollars. Cédric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. How many random seeds? staarXiv preprint tistical power analysis in deep reinforcement learning experiments. arXiv:1806.08295, 2018. 15 Preprint Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small llms: What works and what doesnt, 2025. URL https://arxiv.org/abs/2503.16219. Xingyu Dang, Christina Baek, Zico Kolter, and Aditi Raghunathan. Assessing diversity collapse in reasoning. In Scaling Self-Improving Foundation Models without Human Supervision. Google DeepMind. URL 2025. gemini-model-thinking-updates-march-2025/. Accessed: 2025-04-07. ai model, https://blog.google/technology/google-deepmind/ 2.5: Our most intelligent Gemini DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning, 2025. URL https://arxiv.org/abs/2501.12948. Mostafa Dehghani, Yi Tay, Alexey Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald Metzler, and Oriol Vinyals. The benchmark lottery. arXiv preprint arXiv:2107.07002, 2021. Yihe Deng, Hritik Bansal, Fan Yin, Nanyun Peng, Wei Wang, and Kai-Wei Chang. Openvlthinker: An early exploration to complex vision-language reasoning via iterative selfimprovement. arXiv preprint arXiv:2503.17352, 2025. Ricardo Dominguez-Olmedo, Florian Dorner, and Moritz Hardt. Training on the test task confounds evaluation and emergence. arXiv preprint arXiv:2407.07890, 2024. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/huggingface/open-r1. Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. Clémentine Fourrier, Nathan Habib, Hynek Kydlíˇcek, Thomas Wolf, and Lewis Tunstall. LightEval: lightweight framework for LLM evaluation, 2023. URL https://github. com/huggingface/lighteval. Jiaxuan Gao, Shusheng Xu, Wenjie Ye, Weilin Liu, Chuyi He, Wei Fu, Zhiyu Mei, Guangju Wang, and Yi Wu. On designing effective rl reward at training time for llm reasoning. arXiv preprint arXiv:2410.15115, 2024a. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 07 2024b. URL https://zenodo.org/records/12608602. Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, and Matthias Bethge. Onebench to test them all: Sample-level benchmarking over openended capabilities. arXiv preprint arXiv:2412.06745, 2024. Shahriar Golchin and Mihai Surdeanu. Time travel in llms: Tracing data contamination in large language models. arXiv preprint arXiv:2308.08493, 2023. Rihab Gorsane, Omayma Mahjoub, Ruan John de Kock, Roland Dubb, Siddarth Singh, and Arnu Pretorius. Towards standardised performance evaluation protocol for cooperative marl. Advances in Neural Information Processing Systems, 35:55105521, 2022. 16 Preprint Etash Guha, Negin Raoof, Jean Mercat, Ryan Marten, Eric Frankel, Sedrick Keh, Sachin Grover, George Smyrnis, Trung Vu, Jon Saad-Falcon, Caroline Choi, Kushal Arora, Mike Merrill, Yichuan Deng, Ashima Suvarna, Hritik Bansal, Marianna Nezhurina, Yejin Choi, Reinhard Heckel, Seewong Oh, Tatsunori Hashimoto, Jenia Jitsev, Vaishaal Shankar, Alex Dimakis, Mahesh Sathiamoorthy, and Ludwig Schmidt, November 2024. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402. 14008. Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep reinforcement learning that matters. In Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, and Heung-Yeung Shum Xiangyu Zhang. Open-reasoner-zero: An open source approach to scaling reinforcement learning on the base model. https://github.com/Open-Reasoner-Zero/ Open-Reasoner-Zero, 2025. Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. HuggingFaceH4. Math-500 dataset. https://huggingface.co/datasets/ HuggingFaceH4/MATH-500/blob/main/README.md, 2024. Accessed: 2025-03-29. Ben Hutchinson, Negar Rostamzadeh, Christina Greer, Katherine Heller, and Vinodkumar Prabhakaran. Evaluation gaps in machine learning practice. In Proceedings of the 2022 ACM conference on fairness, accountability, and transparency, pp. 18591876, 2022. Intelligent Internet. II-Thought : Large-Scale, High-Quality Reasoning Dataset, 2025. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Piyush Jha, Prithwish Jana, Pranavkrishna Suresh, Arnav Arora, and Vijay Ganesh. Rlsf: Reinforcement learning via symbolic feedback. arXiv preprint arXiv:2405.16661, 2024. Scott Jordan, Yash Chandak, Daniel Cohen, Mengxue Zhang, and Philip Thomas. Evaluating In International Conference on the performance of reinforcement learning algorithms. Machine Learning, pp. 49624973. PMLR, 2020. Scott Jordan, Adam White, Bruno Castro Da Silva, Martha White, and Philip Thomas. Position: Benchmarking is limited in reinforcement learning research. arXiv preprint arXiv:2406.16241, 2024. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language models struggle to learn long-tail knowledge. In International Conference on Machine Learning, pp. 1569615707. PMLR, 2023. Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. 17 Preprint Knovel Engineering. Amc-23 dataset, 2025. URL https://huggingface.co/ datasets/knoveleng/AMC-23. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Solving quanYuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. In S. Koyejo, S. Mohamed, titative reasoning problems with language models. A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 38433857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/ file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf. Xuefeng Li, Haoyang Zou, and Pengfei Liu. LIMR: Less is More for RL Scaling. arXiv preprint arXiv:2502.11886, 2025a. Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419, 2025b. Thomas Liao, Rohan Taori, Inioluwa Deborah Raji, and Ludwig Schmidt. Are we learning yet? meta review of evaluation failures across machine learning. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Yen-Ting Lin. Aime 2025 dataset, 2025. URL https://huggingface.co/datasets/ yentinglin/aime_2025. Accessed: 2025-03-29. Zhihang Lin, Mingbao Lin, Yuan Xie, and Rongrong Ji. Cppo: Accelerating the trainarXiv preprint ing of group relative policy optimization-based reasoning models. arXiv:2503.22342, 2025a. Zhiyu Lin, Yifei Gao, Xian Zhao, Yunfan Yang, and Jitao Sang. Mind with eyes: from language reasoning to multimodal reasoning. arXiv preprint arXiv:2503.18071, 2025b. Zachary Lipton and Jacob Steinhardt. Troubling trends in machine learning scholarship: Some ml papers suffer from flaws that could mislead the public and stymie future research. Queue, 17(1):4577, 2019. Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. 2025. Junnan Liu, Hongwei Liu, Linchen Xiao, Ziyi Wang, Kuikun Liu, Songyang Gao, Wenwei Zhang, Songyang Zhang, and Kai Chen. Are your llms capable of stable reasoning? arXiv preprint arXiv:2412.13147, 2024. Zichen Liu, Changyu Chen, Wenjun Li, Tianyu Pang, Chao Du, and Min Lin. There may not be aha moment in r1-zero-like training pilot study. https://oatllm.notion. site/oat-zero, 2025a. Notion Blog. Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective, 2025b. URL https://arxiv.org/abs/2503.20783. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua arXiv preprint Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025c. 18 Preprint Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing O1-Preview with 1.5B Model by Scaling RL, 2025. Notion Blog. Chengqi Lyu, Songyang Gao, Yuzhe Gu, Wenwei Zhang, Jianfei Gao, Kuikun Liu, Ziyi Wang, Shuaibin Li, Qian Zhao, Haian Huang, et al. Exploring the limit of outcome reward for learning mathematical reasoning. arXiv preprint arXiv:2502.06781, 2025. Yan Ma, Steffi Chern, Xuyang Shen, Yiran Zhong, and Pengfei Liu. Rethinking rl scaling for vision language models: transparent, from-scratch framework and comprehensive evaluation scheme. arXiv preprint arXiv:2504.02587, 2025. Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via coding large language models. arXiv preprint arXiv:2310.12931, 2023. Marlos Machado, Marc Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents. Journal of Artificial Intelligence Research, 61:523562, 2018. Lovish Madaan, Aaditya Singh, Rylan Schaeffer, Andrew Poulton, Sanmi Koyejo, Pontus Stenetorp, Sharan Narang, and Dieuwke Hupkes. Quantifying variance in evaluation benchmarks, 2024. URL https://arxiv. org/abs/2406.10229. Benjamin Marie, Atsushi Fujita, and Raphael Rubino. Scientific credibility of machine translation research: meta-evaluation of 769 papers. arXiv preprint arXiv:2106.15195, 2021. Fanqing Meng, Lingxiao Du, Zongkai Liu, Zhixiang Zhou, Quanfeng Lu, Daocheng Fu, Botian Shi, Wenhai Wang, Junjun He, Kaipeng Zhang, et al. Mm-eureka: Exploring visual aha moment with rule-based large-scale reinforcement learning. arXiv preprint arXiv:2503.07365, 2025. Meta-AI. The llama 4 herd: The beginning of new era of natively URL https://ai.meta.com/blog/ multimodal llama-4-multimodal-intelligence/. Accessed: 2025-04-07. innovation, 2025. ai Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, and Ji-Rong Wen. Imitate, explore, and self-improve: reproduction report on slowthinking reasoning systems, 2024. URL https://arxiv.org/abs/2412.09413. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. arXiv preprint arXiv:2410.05229, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. metric learning reality check. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXV 16, pp. 681699. Springer, 2020. Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, and Jenia Jitsev. Alice in wonderland: Simple tasks showing complete reasoning breakdown in state-of-the-art large language models. arXiv preprint arXiv:2406.02061, 2024. OpenAI. OpenAI o3-mini System Card, January 2025. URL https://cdn.openai.com/ o3-mini-system-card-feb10.pdf. Preprint Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1298812997, 2024. Andrew Patterson, Samuel Neumann, Martha White, and Adam White. Empirical design in reinforcement learning. Journal of Machine Learning Research, 25(318):163, 2024. Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunovic, Nikola Jovanovic, and Martin Vechev. Proof or bluff? evaluating llms on 2025 usa math olympiad. arXiv preprint arXiv:2503.21934, 2025. Ameya Prabhu, Philip HS Torr, and Puneet Dokania. Gdumb: simple approach that questions our progress in continual learning. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pp. 524540. Springer, 2020. Ameya Prabhu, Shiven Sinha, Ponnurangam Kumaraguru, Philip HS Torr, Ozan Sener, and Puneet Dokania. Randumb: simple approach that questions the efficacy of continual representation learning. arXiv e-prints, pp. arXiv2402, 2024a. Ameya Prabhu, Vishaal Udandarao, Philip Torr, Matthias Bethge, Adel Bibi, and Samuel Albanie. Efficient lifelong model evaluation in an era of rapid progress. arXiv preprint arXiv:2402.19472, 2024b. Ori Press, Steffen Schneider, Matthias Kümmerer, and Matthias Bethge. Rdumb: simple approach that questions our progress in continual test-time adaptation. Advances in Neural Information Processing Systems, 36:3991539935, 2023. PyTorch Contributors. Reproducibility pytorch documentation. https://pytorch. org/docs/stable/notes/randomness.html, 2024. Accessed: 2025-04-09. Matthew Renze. The effect of sampling temperature on problem solving in large language In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. models. 73467356, 2024. Anka Reuel, Amelia Hardy, Chandler Smith, Max Lamparth, Malcolm Hardy, and Mykel Kochenderfer. BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices. arXiv preprint arXiv:2411.12990, 2024. Jonathan Roberts, Mohammad Reza Taesiri, Ansh Sharma, Akash Gupta, Samuel Roberts, Ioana Croitoru, Simion-Vlad Bogolin, Jialu Tang, Florian Langer, Vyas Raina, et al. Zerobench: An impossible visual benchmark for contemporary large multimodal models. arXiv preprint arXiv:2502.09696, 2025. Manley Roberts, Himanshu Thakur, Christine Herlihy, Colin White, and Samuel Dooley. To the cutoff... and beyond? longitudinal perspective on llm data contamination. In The Twelfth International Conference on Learning Representations, 2023. Nicolas Le Roux, Marc Bellemare, Jonathan Lebensold, Arnaud Bergeron, Joshua Greaves, Alex Fréchette, Carolyne Pelletier, Eric Thibodeau-Laufer, Sándor Toth, and Sam Work. Tapered off-policy reinforce: Stable and efficient reinforcement learning for llms. arXiv preprint arXiv:2503.14286, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230, 2025. 20 Preprint Shamus Sim and Tyrone Chen. Critique of impure reason: Unveiling the reasoning behaviour of medical large language models. arXiv preprint arXiv:2412.15748, 2024. Mingyang Song, Mao Zheng, Zheng Li, Wenjie Yang, Xuan Luo, Yue Pan, and Feng Zhang. FastCuRL: Curriculum Reinforcement Learning with Progressive Context Extension for Efficient Training R1-like Reasoning Models, 2025. URL https://arxiv.org/abs/ 2503.17287. Saurabh Srivastava, Anto PV, Shashank Menon, Ajay Sukumar, Alan Philipose, Stevin Prince, Sooraj Thomas, et al. Functional benchmarks for robust evaluation of reasoning performance, and the reasoning gap. arXiv preprint arXiv:2402.19450, 2024. Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. Team. Open Thoughts. https://open-thoughts.ai, January 2025. Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, et al. Enhancing llm reasoning with iterative dpo: comprehensive empirical investigation. arXiv preprint arXiv:2503.12854, 2025. Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. No\" zero-shot\" without exponential data: Pretraining concept frequency determines multimodal model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. vLLM Contributors. https://github.com/ vllm-project/vllm/blob/098900d7c2b53324687977eece400f634755cf51/ examples/offline_inference/reproduciblity.py, 2024. Accessed: 2025-04-09. Inference reproducibility script. Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025a. URL https://arxiv.org/abs/2503.10460. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, et al. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond. arXiv preprint arXiv:2503.10460, 2025b. xAI. Grok 3 beta the age of reasoning agents. February 2025. URL https://x.ai/ news/grok-3. Accessed: 2025-03-29. Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. Kai Yan, Yufei Xu, Zhengyin Du, Xuesong Yao, Zheyu Wang, Xiaowen Guo, and Jiecao Chen. Recitation over reasoning: How cutting-edge language models can fail on elementary school-level reasoning problems? arXiv preprint arXiv:2504.00509, 2025. 21 Preprint An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024a. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024b. Huimu Yu, Xing Wu, Weidong Yin, Debing Zhang, and Songlin Hu. Codepmp: Scalable preference model pretraining for large language model reasoning. arXiv preprint arXiv:2410.02229, 2024. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weizhe Yuan, Jane Yu, Song Jiang, Karthik Padthe, Yang Li, Dong Wang, Ilia Kulikov, Kyunghyun Cho, Yuandong Tian, Jason Weston, et al. Naturalreasoning: Reasoning in the wild with 2.8 challenging questions. arXiv preprint arXiv:2502.13124, 2025. Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. Thomas Zeng, Shuibai Zhang, Shutong Wu, Christian Classen, Daewon Chae, Ethan Ewer, Minjae Lee, Heeju Kim, Wonjun Kang, Jackson Kunde, et al. Versaprm: Multi-domain process reward model via synthetic reasoning data. arXiv preprint arXiv:2502.06737, 2025a. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. SimpleRL-Zoo: Investigating and Taming Zero Reinforcement Learning for Open Base Models in the Wild. arXiv preprint arXiv:2503.18892, 2025b. Sheng Zhang, Qianchu Liu, Guanghui Qin, Tristan Naumann, and Hoifung Poon. Med-rlvr: Emerging medical reasoning from 3b base model via reinforcement learning. arXiv preprint arXiv:2502.19655, 2025. Preprint"
        },
        {
            "title": "A Appendix",
            "content": "A.1 Bootstrapping Results on Additional Datasets To complement our analysis in 3, we present bootstrapped variance results on two additional datasets: AMC23 and MATH500. As shown in Figures 12 and 13, high variance in Pass@1 persists even when averaging over multiple seeds (K = 5), mirroring the trends observed on AIME24. These results reinforce our conclusion that small benchmark sizes yield unstable estimates and that robust performance reporting requires multiple seed runs. Figure 12: Variance of mean Pass@1 on AMC23. Bootstrapped estimates show substantial variance even with = 5 evaluation runs, highlighting the instability of single-seed evaluations. Figure 13: Variance of mean Pass@1 on MATH500. Similar to AIME24 and AMC23, the estimates remain volatile across seeds. Even = 5 runs do not eliminate variance, underscoring the need for larger K. 23 Preprint A.2 Hardware & Software Variations In Figure 14, we show that the model performance variation due to hardware configuration is not limited to AIME24 and AMC23. Similar discrepancies are observed on MATH500, where different compute clusters yield different accuracy scoreseven when model, seeds, and decoding parameters are held constant. This further emphasizes the need for hardware and software standardization when reporting benchmark results. Figure 14: Performance variation across compute clusters on MATH500. Differences in GPU type and environment lead to non-trivial shifts in performance, reinforcing the importance of hardware standardization. A.3 Prompt Variants and Template Settings We provide the exact templates used for our three prompt settings in Table 5: Math, Default, and No Template. These formats are based on the DeepSeek tokenizer but adapted for each models specific chat template. Our results (in 3.4) indicate that instruction-tuned models are highly sensitive to prompt formatting, with performance degrading significantly when prompts deviate from their training-time structure. A.4 Effect of Output Length Limits We further explore how varying max_new_tokens impacts model accuracy. Figures below compare OpenRS-series models (with 131,072-token context windows) and OpenThinker/S1.1 models (with 32,768-token limits). Figure 15 shows that OpenRS models are highly sensitive to this parametershortening outputs results in clear accuracy drops. Similarly, Figure 16 reveals the same pattern for OpenThinker-7B and S1.1-7B, despite their smaller context lengths. In both cases, premature truncation leads to incomplete reasoning chains and incorrect answers, confirming the importance of setting appropriate generation limits. 24 Preprint Prompt Math Default Example <begin_of_sentence><User>Solve the following math problem efficiently and clearly. The last line of your response should be of the following format: Therefore, the final answer is: $boxed{ANSWER}$. hope it is correct (without quotes) where ANSWER is just the final number or expression that solves the problem. Think step by step before answering.n <Assistant><think>n{Question} <begin_of_sentence><User>{Question} <Assistant><think>n No Template{Question} Table 5: Prompt templates used in our evaluation. The inclusion or exclusion of structured prompt tokens significantly impacts performance for instruction-tuned models. Figure 15: Impact of max_new_tokens on OpenRS models. Models with long context support (131,072 tokens) experience degraded performance when max_new_tokens is set too low. Figure 16: Impact of max_new_tokens on OpenThinker and S1.1 models. Despite shorter context limits (32,768 tokens), performance still degrades noticeably when output length is constrained. A.5 Response Length vs. Accuracy Per-Model Breakdown To supplement the aggregated results shown in Figure 11, we include detailed histograms for each individual model in the appendix. These plots show the distribution of correct and incorrect responses across response lengths, averaged over random seeds. Due to the number of models analyzed, we split the results into two figures for clarity. Figures 17 and 18 reveal that the overall trend observed in the main paper holds consistently across nearly all models: incorrect responses tend to be longer than correct ones. These results reinforce the idea that excessively long outputs often indicate failure modes such as hallucinated reasoning, verbose overthinking, or degenerate loops. Importantly, this 25 Preprint correlation persists well below the maximum sequence length, ruling out truncation as the sole cause. Figure 17: Response Length vs. Correctness Models (1/2). Average number of correct and incorrect responses across response length bins for subset of models. Longer responses consistently correlate with incorrect predictions. Across all models, longer responses are consistent marker of incorrect outputs, making response length useful signal for detecting low-confidence or erroneous reasoning chains. 26 Preprint Figure 18: Response Length vs. Correctness Models (2/2). Continuation of model-wise response length analysis. The same trend holds across the remaining models, with incorrect answers being disproportionately long."
        }
    ],
    "affiliations": [
        "Tübingen AI Center, University of Tübingen",
        "University of Cambridge"
    ]
}