{
    "paper_title": "Predicting Emergent Capabilities by Finetuning",
    "authors": [
        "Charlie Snell",
        "Eric Wallace",
        "Dan Klein",
        "Sergey Levine"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as a function of compute. However, downstream capabilities are far less predictable -- sometimes even exhibiting emergent jumps -- which makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random few-shot accuracy on a task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover a simple insight for this problem: finetuning LLMs on a given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit a parametric function that predicts when emergence will occur (i.e., \"emergence laws\"). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present a case study of two realistic uses for emergence prediction."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 2 ] . [ 1 5 3 0 6 1 . 1 1 4 2 : r Published as conference paper at COLM"
        },
        {
            "title": "Predicting Emergent Capabilities by Finetuning",
            "content": "Charlie Snell University of California, Berkeley Eric Wallace Dan Klein Sergey Levine"
        },
        {
            "title": "Abstract",
            "content": "A fundamental open challenge in modern LLM scaling is the lack of understanding around emergent capabilities. In particular, language model pretraining loss is known to be highly predictable as function of compute. However, downstream capabilities are far less predictablesometimes even exhibiting emergent jumpswhich makes it challenging to anticipate the capabilities of future models. In this work, we first pose the task of emergence prediction: given access to current LLMs that have random fewshot accuracy on task, can we predict whether future models (GPT-N+1) will have non-trivial accuracy on that task? We then discover simple insight for this problem: finetuning LLMs on given task can shift the point in scaling at which emergence occurs towards less capable models. To operationalize this insight, we can finetune LLMs with varying amounts of data and fit parametric function that predicts when emergence will occur (i.e., emergence laws). We validate this approach using four standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence (MMLU, GSM8K, CommonsenseQA, and CoLA). Using only small-scale LLMs, we find that, in some cases, we can accurately predict whether models trained with up to 4x more compute have emerged. Finally, we present case study of two realistic uses for emergence prediction."
        },
        {
            "title": "Introduction",
            "content": "The pretraining loss for language models has been shown to follow simple predictable power law as function of compute, model parameters, and data (Kaplan et al., 2020; Hoffmann et al., 2022). This finding has enabled precise empirical science to develop around the scaling behavior of LLMs (Muennighoff et al., 2023; Aghajanyan et al., 2023; Krajewski et al., 2024), which has in turn led to much of the rapid improvement of language model capabilities in recent years (OpenAI et al., 2024; Anil et al., 2023). On the other hand, the specific downstream capabilities corresponding to given pretraining loss are generally much less predictable, posing significant challenges for 1) model developers who may want to make specific architectural or data decisions on the basis of future LLM capabilities; 2) policymakers who will need time to assess, plan for, and prepare for potentially dangerous future LLM capabilities like deception, bio-risk, or malicious software agents (Shevlane et al., 2023; Bengio et al., 2023); and 3) stakeholders who need to make reliable business, financial, and investment decisions on the basis of future LLM capabilities. Of particular concern and interest is the phenomenon of emergent capabilities in large language models Wei et al. (2022a). On certain downstream tasks, models may exhibit emergence wherein only beyond certain, seemingly arbitrary, threshold in LLM scaling (i.e., the point of emergence) do models spontaneously improve beyond random-chance. In cases where existing models have already crossed the point of emergence on given task, demonstrating smooth performance improvements (the post-emergence regime), it is possible to make highly accurate predictions about the performance of future models (Caballero et al., 2023; Gadre et al., 2024; OpenAI et al., 2024; Owen, 2024). However, on tasks in which all existing models demonstrate random-chance performance (the pre-emergence regime), making any kind of prediction at all about future model capabilities remains an important Corresponding author: csnell22@berkeley.edu. An early version of this work appeared in COLM 2024. 1 Published as conference paper at COLM 2024 Figure 1: We find that task-specific finetuning systematically shifts the point of emergence towards less capable models. Motivated by this finding, we develop an emergence law, which models how the point of emergence shifts as function of the amount of finetuning data. Using this emergence law, we can then extrapolate prediction for the point of emergence in the few-shot setting. unsolved challenge in LLM scaling. In this case, there is no known method for predicting at what point, if any, models will demonstrate emergence, let alone how performance will scale thereafter. To this end, we pose the problem of emergence prediction. Concretely, can we accurately predict the point in scaling at which emergence will occur on given task, while only having access to pre-emergence model checkpoints? We demonstrate an extremely simple yet highly effective solution to this problem; namely that it is possible to predict few-shot emergent capabilities in future LLMs (e.g., GPT-N+1) by finetuning todays weaker LLMs (e.g., GPT-N). For example, in Figure 2 (left), we see that finetuning models for GSM8K, rather than prompting them, systematically shifts the point of emergence from stronger to weaker LLMs. Moreover, by varying the amount of finetuning data, the emergence point is shifted accordingly. Motivated by this finding, we develop an emergence law; parametric function that models how the point of emergence shifts as function of the amount of finetuning data. Using our emergence law, we can extrapolate prediction for the point of emergence in the few-shot setting (see Figure 1). To validate our approach, we use four standard NLP benchmarksMMLU, GSM8K, CommonsenseQA, and CoLAwhere large-scale open-source LLMs have already demonstrated emergence. By fitting an emergence law using only small-scale, pre-emergence LLMs, we find that we are not only able to accurately predict the point in scaling at which emergence will occur with more capable LLMs, but also, in some cases, we can do so using models trained with only 1/4th the FLOPS needed to achieve emergence (see Figure 2 right). Finally, we present case study of two real world uses for emergence prediction. We first demonstrate that emergence prediction can be used to cheaply assess pretraining data quality. Using the difficult APPS coding benchmark, we then demonstrate that our approach can be used to predict more complex capabilities, closer to those of future frontier models."
        },
        {
            "title": "2 Background & Related Work",
            "content": "Emergence in Large Language Models. Emergence Wei et al. (2022a) refers to the phenomenon in which beyond certain point in LLM scaling, models suddenly improve beyond chance performance on given task. As result, despite the fact that pretraining loss can be reliably predicted as function of model scale (Kaplan et al., 2020; Hoffmann et al., 2022), the downstream capabilities corresponding to particular model scale cannot always be. Practical challenges with emergence. The phenomenon of emergent capabilities introduces significant challenges for safety preparedness, frontier model development, and business decision making with LLMs. In particular, it is possible that future language models may demonstrate dangerous emergent capabilities, such as planning, deception, bio-risk, or the ability to generate malicious software (Anwar et al., 2024; Hendrycks et al., 2 Published as conference paper at COLM 2024 Figure 2: Left: we predict emergence in the few-shot setting by leveraging information about how pre-emergence models behave after finetuning. Our key finding is that finetuning effectively shifts the point of emergence from stronger to weaker models. Moreover, by varying the amount of finetuning data, the emergence point is shifted accordingly. We can use this finding to predict when few-shot emergence will occur by fitting parametric function to the results (i.e., emergence law) and then taking limit. Right: using this approach, we can predict emergence up to 4x the FLOPs in advance. 2023). Without any ability to predict when and if these capabilities will emerge, we are only left to speculate, which is highly sub-optimal given the potentially high stakes. Furthermore, model developers may need to make architecture and pretraining data decisions on the basis of downstream capabilities which may only emerge with scale, making it challenging and costly to do so. As result, the inability to reason about and predict emergent capabilities in advance presents significant unsolved challenge in language model scaling. Emergence is Mirage? recent work by Schaeffer et al. (2023) claimed that the phenomenon of emergence is not due to sudden fundamental changes in the model, but rather due to the researchers choice of metric. They argue that emergence is typically observed when using discontinuous metrics of accuracy, such as exact-match, and that using continuous metric, such as the models correct answer probability, will instead show smooth scaling. However, in many practical and policy-relevant settings, the metric of primary interest may be fundamentally discontinuous. Additionally, in some cases finding metrics that yield smooth scaling may be challenging (Barak, 2023). In particular, in Figure 13 we show that when using continuous probability-based evaluation metric on two canonical language model benchmarks namely MMLU (Hendrycks et al., 2021) and CommonsenseQA (Talmor et al., 2019) we still observe emergence. Similar findings have also been observed by Du et al. (2024); Lieberum et al. (2023). Additional tools are therefore needed to resolve the practical challenges induced by the phenomenon of emergence. Predicting emergence. The literature suggests several promising directions for predicting and understanding emergent capabilities. One direction uses interpretability or training dynamics based techniques to elucidate underlying phase changes inside the model (Lieberum et al., 2023; Olsson et al., 2022). Another approach uses general high-resolution metric which is more amenable to standard scaling analysis (OpenAI et al., 2024; Hu et al., 2024). Finally, recent work from Ruan et al. (2024) proposes observational scaling laws, which leverage correlations between many different model families to enable more effective predictions. Each of these works can be used in complement to our approach: scaling law inspired emergence law, which predicts emergence by using information about how pre-emergence models behave after being finetuned for task of interest."
        },
        {
            "title": "3 Emergence Prediction",
            "content": "We now introduce the problem of emergence prediction. We define emergence prediction as the problem of identifying the point in scaling at which emergence will occur using only pre-emergence model checkpoints (i.e., have near-trivial performance on given task). 3 Published as conference paper at COLM 2024 Figure 3: Left: the finetuned and few-shot performance of intermediate LLM checkpoints. We plot downstream accuracy against pretraining loss for all 3B, 7B, and 13B intermediate OpenLLaMA V1 checkpoints on MMLU and GSM8K. We see that the point of emergence is systematically shifted towards weaker LLMs after finetuning. Additionally, the magnitude of the shift is consistent across all model sizes at the same pretraining loss. Right: varying the amount of finetuning data. We finetune the 3B intermediate checkpoints on subsets of the full finetuning data. We see that as we increase the amount of finetuning data, the point of emergence shifts further towards less capable LLMs. Emergence with loss. We define model scale to be models pretraining loss rather its FLOPs or parameter count. Prior work has found pretraining loss to be highly predictive of downstream capabilities (Du et al., 2024; Gadre et al., 2024; Huang et al., 2024; Xia et al., 2023). Thus, it makes for more precise independent variable when studying emergence. Modeling emergence with ReLU. To formally define emergence prediction, we model the phenomenon of emergence using ReLU function. While prior work Gadre et al. (2024) has found downstream performance to be well modeled by an exponential function of pretraining loss, we use ReLU because our primary focus is on modeling the point at which emergence occurs, rather than accurately extrapolating performance post-emergence. The ReLU elbow, in this case, provides clear denotation for the precise point at which emergence begins. Specifically, we model emergence with: PerfA,B,E(L(M)) = max(E L(M), 0) + (1) The output is downstream performance on our task of interest Perf(L(M)), and the input L(M) corresponds to the pretraining loss of given model checkpoint M. and are parameters defining the slope and floor of the ReLU respectively; and denotes the point in 4 Published as conference paper at COLM 2024 pretraining loss at which emergence occurs (e.g., the ReLU elbow). The goal of emergence prediction is therefore to accurately predict for given task and series of models. Estimating uncertainty for emergence prediction. Making an exact pointwise prediction for may not always be feasible: due to noise in both the model checkpoints and the prediction method, we may have considerable uncertainty about the exact point of emergence. We therefore not only aim to predict single MLE point for when emergence will occur, but also calibrated probability distribution over all possible points at which it may occur."
        },
        {
            "title": "4 How does Finetuneing Interact with Emergence?",
            "content": "In this work, we explore one particular approach to the problem of emergence prediction. Namely, we aim to use information about how pre-emergence models behave under the influence of task-specific finetuning to obtain predictive power about the point of emergence in the few-shot setting. To develop such an approach, we first need to understand how finetuning effects the emergence curve. Here, we empirically characterize this relationship. 4.1 Experimental Setup We conduct experiments on two canonical LLM tasks MMLU (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) in which we observe emergence in the few-shot setting with our most capable open LLMs. We also include results on CommonsenseQA (Talmor et al., 2019) (CSQA) and CoLA (Wang et al., 2018) in Appendix A.4 and Appendix A.11. To obtain granular set of measurements along the emergence curve, we not only use different sized models but also different intermediate checkpoints for each model size. Specifically, we use series of intermediate 3B, 7B, and 13B checkpoints from the OpenLLaMA V1 series (Geng & Liu, 2023). For all tasks except MMLU, we finetune on the provided training split, and we evaluate on the test split. For MMLU, there is no formal training set, so we finetune on the test set and evaluate on the validation set. See Appendix A.10 for more details. To understand how the emergence curve is effected by finetuning, we finetune each 3B, 7B, and 13B checkpoint on all tasks. We also finetune all 3B model checkpoints on sampled subsets of the full finetuning data to understand the effect of the amount of data. Finally, to help elucidate the underlying mechanisms behind our observations, we conduct additional experiments with PEFT in Appendix A.2. In this section, we use full parameter finetuning. 4.2 Empirical findings Finetuning shifts the point of emergence towards weaker LLMs. In Figure 3 (left), we plot the few-shot and finetuned performance of each model against pretraining loss on GSM8K and MMLU. We see that the finetuned models follow similar emergence ReLU shape as in the few-shot setting. However the finetuned ReLU elbow is systematically shifted towards less capable LLMs. Moreover, the shift is consistent across all model sizes at the same pretraining loss, demonstrating that pretraining loss can act as an effective independent variable for representing capabilities in both the few-shot Du et al. (2024); Gadre et al. (2024); Huang et al. (2024); Xia et al. (2023) and finetuned settings, further motivating our use of pretraining loss as the independent variable for emergence prediction. The emergence shift is effected by the amount of finetuning data. We also plot the performance of our 3B model checkpoints after being finetuned on subsets of the full data in Figure 3 (right). On both MMLU and GSM8K, we see that as we increase the amount of finetuning data, the point of point of emergence is shifted further towards less capable LLMs. The amount of finetuning data can therefore modulate the emergence shift. These observations demonstrate clear pattern of structure in how task-specific finetuning interacts with emergence: fine-tuning shifts the emergence elbow from strong to weak models and the amount of finetuning data controls the magnitude of this shift. Next, we will use this insight to develop scaling-law-style functional form which can effectively predict the point of emergence in the few-shot setting. 5 Published as conference paper at COLM"
        },
        {
            "title": "5 Scaling Laws for Emergence Prediction",
            "content": "We now use our observations about how finetuning interacts with emergence to develop novel emergence prediction method. Specifically, we use the empirical results of many finetuning runs to fit scaling law inspired parametric function (i.e., an emergence law), which effectively models how the point of emergence shifts as function of the amount of finetuning data for task. By then extrapolating the resulting emergence law into the low data limit, we can make prediction about the point of emergence in the few-shot setting. In the following section, we detail our emergence law functional form and how its parameters are fit to empirical data. 5.1 Modeling the Emergence Shift Symbol Description Amount of finetuning data L(M) Pretraining loss of model Perf Downstream performance A, B, ReLU parameters Eθ (D) k, α, D0 Emergence law; models emergence shift Emergence law parameters Low data extrapolation limit Optional parameter; shifts ReLU base Table 1: Symbols used in Section 5. Predicting emergence with an emergence law. To predict the point of emergence, we use empirical data to fit the parameters of an emergence law: function Eθ(D) with parameters θ, which models how the emergence ReLU elbow shifts as function of the amount of finetuning data D. Using our emergence law fit, we can then extrapolate prediction for the point of emergence in the few-shot setting. In particular, we take limit to D0 under the model, where D0 is low data extrapolation limit, representing the few-shot setting. Finally, we can obtain an uncertainty interval for the point of emergence by applying statistical uncertainty estimation techniques to θ. Emergence law functional form and implementation. In practice, we find that the point of emergence is well modeled by power law in the log of the amount of finetuning data: Ek,α,C(D) = log(D)α + where k, α, and constitute the learnable parameters θ. To obtain our few-shot prediction under this model, we set the low data extrapolation limit D0 to the number of examples in our few-shot prompt. We then run MCMC posterior sampling, with uniform prior over the parameters, to obtain calibrated uncertainty interval for the point of emergence. 5.2 Fitting the Emergence Law We cannot directly fit Eθ(D) to set of (D, E) input-output observation tuples, since in practice, we do not observe the point of emergence directly. Instead, we observe Perf(L(M), D): downstream performance as function of given model checkpoint (represented by its pretraining loss L(M), as sufficient statistic) and the amount of data used to finetune it. In Figure 3 (right) we plot examples of these raw data tuples on MMLU and GSM8K. In this setting, the point of emergence corresponding to particular is statistic of the raw data, which can be computed by modeling the data using the ReLU in Equation 1. Therefore, in order to obtain fit for Eθ(D) using the raw downstream data, we extend Equation 1, by parameterizing the ReLU elbow with Eθ(D). This gives the function: PerfA,B,θ(L(M), D) = max(Eθ(D) L(M), 0) + We fit all parameters A, B, and θ jointly to the raw empirical data using MSE loss. We then extract the resulting Eθ(D) and take limit to D0, as described in Section 5.1. 6 Published as conference paper at COLM 2024 Figure 4: Our MLE emergence law predictions on each task. The grey line is our extrapolated prediction and the multi-color lines are the fit. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. We see that across all tasks, we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that. For visual clarity, we plot subset of the data used for fitting (see Appendix A.11 for all). 5.3 Collecting Empirical Datapoints To obtain our empirical data, we need: 1) set of model checkpoints of varying capabilities (e.g., varying L(M)); and 2) range of finetuning data amounts (e.g., varying D). Selecting the model checkpoints. To satisfy 1), we can avoid the need to fully pretrain multiple models from scratch by instead utilizing number of evenly spaced intermediate checkpoints from single pretraining run. As we observed in Figure 3 (left) and as noted in recent works (Du et al., 2024; Gadre et al., 2024) different-sized models at the same pretraining loss demonstrate similar downstream performance, even after finetuning. Thus, using multiple intermediate checkpoints from the same pretraining run is just as effective as using models from separate pretraining runs of varying scale. Selecting the finetuning data amounts. To obtain wide range of finetuning data amounts, we randomly sample subsets of the full finetuning data. Since we ultimately want to predict emergence by extrapolating performance into the low data limit, it is important that we focus our data collection on the smallest subsets for which we can still observe emergence with the checkpoints we have. These datapoints will provide some of the most useful signal for accurately extrapolating into the low data limit. In our case, these subsets are often on the order of few hundred finetuning examples. To further emphasize these datapoints when fitting the emergence law parameters, we weight the MSE loss for each datapoint in an inverse proportion to the amount of finetuning data used. 5.4 Including Few-shot Data in the Emergence Law In addition to the datapoints obtained from finetuning, we may also optionally want to include the few-shot results from our pre-emergence models when fitting the emergence 7 Published as conference paper at COLM Figure 5: The cumulative distribution function (CDF) of our emergence posterior on GSM8K and MMLU (see Appendix A.11 for all tasks). The CDFs slope peaks at the mode of the distribution. We see that the distribution spikes near the true emergence point, followed by moderately long tail. law. While these few-shot results will all give near-random performance, they will still be informative for telling the model that emergence has not happened yet. To effectively include these few-shot datapoints in our model, we add an optional parameter , which effectively models the upwards shift in the base of the emergence ReLU that we see when finetuning in Figure 3 (right). We believe that this shift is largely due to the model learning trivial features from the fine-tuning data, such as the base-rate of the correct answer1. This additional term gives the following model: PerfA,B,,θ(L(M), D, 1 is finetuned) = max(Eθ(D) L(M), 0) + + 1 is finetuned Here, 1 prompting. For the few-shot datapoints, we set to D0. is finetuned indicates if given datapoint is the result of finetuning, rather than few-shot"
        },
        {
            "title": "6 Evaluating the Emergence Law",
            "content": "We now evaluate the efficacy of our emergence prediction methodology on standard NLP benchmarks where large-scale open-source LLMs already demonstrate emergence. In this setting, we fit an emergence law using smaller-scale LLMs, which have random-chance performance on the task. We then check the accuracy of our predictions against the true point of emergence observed from larger models. In the following section, we first validate that the emergence law presented in Section 5 can accurately predict the point of emergence. We then conduct ablations to validate each of our main methodological decisions. Finally, we calculate precisely how far in advance we can predict emergence. We first detail our experimental setup and then discuss each experiment. 6.1 Experimental Setup Models and tasks. Since fitting our emergence law requires access to number of intermediate model checkpoints, we use the same 3B, 7B and 13B intermediate OpenLLaMA V1 checkpoints used in Section 4. We conduct experiments using 4 standard NLP benchmarks MMLU, GSM8K, CommonsenseQA (CSQA), and CoLAfor which we observe emergence in the few-shot setting with the most capable checkpoints. On each of these tasks, none of the 3B checkpoints have emerged, whereas the 7B and 13B models undergo emergence by the end of training. We therefore only use the 3B model checkpoints for fitting our emergence law, and then make use of the 7B and 13B checkpoints for validating the accuracy of our predictions. We include additional details in Appendix A.10. 1On GSM8K always guessing the mode answer in the training set gets 2.6% test accuracy. Published as conference paper at COLM 2024 Figure 6: Ablations. The bar height represents the MLE prediction error (lower is better). The error bar represents the 5th and 95th percentile errors obtained from MCMC posterior sampling, and the circle is the median. Left: comparing emergence law functional forms. Log Power Law and Power Law refer to different functional forms for Eθ(D). No Few-shot is the Log Power Law without the parameter. We see that removing the log worsens predictions, and the has minimal effect on accuracy. Right: varying the low data extrapolation limit D0. is the number of few-shot examples. We see that within reasonable range (e.g., < 10N) the value of D0 has minimal impact on accuracy. Fitting the emergence law. Following the procedure in Hoffmann et al. (2022), we obtain an MLE fit for the emergence law, by applying the L-BFGS optimizer. We include additional details in Appendix A.5. We estimate probability distribution for the point of emergence, by taking 100k samples from the No-U-Turn Sampler (Hoffman et al., 2014). Evaluation. We determine the ground-truth point of emergence on each of our tasks, by fitting ReLU (Equation 1) to the full set of few-shot results from all model checkpoints (3B, 7B, and 13B), using the procedure described in Appendix A.5. We report the absolute difference between our MLE prediction and the ground-truth. Using our MCMC probability distribution, we also report 90% confidence interval for the prediction error. We consider prediction successful if our MLE prediction falls within 0.1 nats of the true emergence point. 6.2 Can our Emergence Law Successfully Predict the Point of Emergence? In Figure 4, we plot the MLE fit for our emergence law on all four tasks. We see that on all tasks, our predictions are very accurate, falling well within 0.1 nats of the true emergence point2. In Figure 5, we additionally plot the cumulative distribution function (CDF) of the MCMC emergence posterior on MMLU and GSM8K (see Appendix A.11 for all tasks). We see that the distribution generally spikes near the true point of emergence, followed by moderate sized tail: our emergence law is generally confident about when emergence will occur but has some uncertainty about the possibility of emergence occurring later. Overall, these results serve to validate the predictive accuracy of our emergence law. 6.3 Comparing Emergence Law Design Decisions We would now like to understand how the main decisions in our method effect prediction accuracy. On all four tasks, we ablate: 1) the functional form of the emergence law; 2) the value of the low data extrapolation limit D0; 3) the specific set of model checkpoints and finetuning data amounts selected for fitting the emergence law; and 4) the statistical approach used for estimating uncertainty. We detail each ablation below. 2On GSM8K three of our 3B checkpoints are technically post-emergence according to the groundtruth. However, in absolute terms, these checkpoints have near-trivial performance (< 3%). Therefore without access to larger models these checkpoints would be considered pre-emergence. Additionally, in Figure 7, we see that even in the absence of these checkpoints, we can still make accurate predictions. 9 Published as conference paper at COLM 2024 Figure 7: How far in advance can we predict emergence? We hold out checkpoints to see how far in advance, in pretraining FLOPS, we can successfully predict emergence. The position of each blue bar corresponds to the FLOPS needed to train the most capable model used for fitting. The blue circle represents the median of the MCMC posterior, and the error bar represents the 5th to 95th percentiles. If the MLE prediction error is > 0.1 nats, we consider that prediction unsuccessful and denote it with red-cross3. On MMLU we can predict emergence using models trained with 1022 FLOPS, but no fewer. The earliest post-emergence model on MMLU was trained with 5 1022 FLOPS, hence we can predict 4-5x the FLOPS in advance on MMLU. On GSM8K we also predict 4x the FLOPS in advance4. However, on CoLA and CommonsenseQA we only predict 2x the FLOPS in advance. Comparing functional forms. Recall, we model the emergence shift Eθ(D) using power law in the log of the amount of finetuning data (Section 5.1). This function is then used to parametrize ReLU with Eθ(D) as the elbow (Section 5.2). We finally add an optional parameter to account for the pre-emergence few-shot data (Section 5.4). In this section, we ablate both the functional form for Eθ(D) and the choice to include . Specifically, for Eθ(D) we consider power law in D, rather than the log of (otherwise unchanged). We see in Figure 6 (left) that across all tasks, the log power law generally makes the best predictions. Furthermore, the inclusion of marginally improves predictions but has an overall small effect. These findings validate the efficacy of our emergence law functional form. Varying the low data limit. As described in Section 5.1, we predict the point of emergence by taking limit under Eθ(D) into the low data extrapolation limit D0. By default we set D0 to the number of examples in the few-shot prompt N. Here, we ablate the effect of choosing different values for D0. We set D0 to smaller value of 1 (e.g., the smallest integer for which log(D) is well defined), and also to larger values of 2N and 10N. In Figure 6 3In some cases the failed predictions would be well off the plot and we want to keep the axis bounds constrained for presentation clarity. We include the full results in Appendix A.6. 4We count the earliest successful prediction for this calculation. However, GSM8K has failed prediction between two successes, likely due to noise. In Appendix A.6, we see that this failed prediction is just outside the success threshold, with much of the error bar falling well within 0.1 nats. Published as conference paper at COLM 2024 Figure 8: Comparing OpenLLaMA V1 and V2 emergence. On both MMLU and CommonsenseQA, the V2 models emerge first, suggesting that the V2 pretraining data is likely higher quality. (right), we see that setting D0 to generally results in the best predictions, but varying D0 within reasonable range (e.g., < 10N) has minimal impact on accuracy, demonstrating the robustness of our emergence law to the choice of D0. Additional ablations. In Appendix A.6 we also ablate our procedure for selecting the empirical examples used for fitting the emergence law (Section 5.3). We find that, indeed, the smaller finetuning data amounts are often important for making accurate predictions. However, in some cases we find that it is possible to make accurate predictions using far fewer finetuning data amounts and model checkpoints than we used in our main experiments, suggesting that there may be room for future work to greatly improve both the efficiency and effectiveness of our data collection procedure. Finally, in Appendix A.7, we compare our MCMC uncertainty intervals with those obtained via bootstrap sampling. We find that in general the two approaches to uncertainty estimation yield similar intervals. 6.4 How Far in Advance can we Predict the Point of Emergence? We would like to understand how far in advance, in terms of pretraining FLOPS, we can successfully make predictions. To do this, we hold out additional 3B checkpoints when fitting the emergence law. In Figure 7, we plot each emergence prediction against the FLOPS needed to train the most capable model used for fitting. We find that the degree to which we can predict emergence in advance is somewhat task dependent. In particular, on GSM8K and MMLU we are able to reliably make predictions well in advance. On CommonsenseQA and CoLA, on the other hand, we find that our ability to make advance predictions is more limited. Comparing the FLOPS of the earliest point at which we can predict emergence against the FLOPS required for training the earliest post-emergence checkpoint, gives an estimate of how far in advance we can predict. We find that on MMLU and GSM8K we can predict emergence up to 4.3x and 3.9x FLOPS in advance respectively. However, on CommonsenseQA and CoLA we are only able predict 1.9x and 2.3x in advance respectively."
        },
        {
            "title": "7 A Case Study of Real World Uses for Emergence Prediction",
            "content": "Now that we have validated the efficacy of our emergence law approach, we demonstrate two proof-of-concept applications of it: 1) cheaply assessing pretraining data quality; and 2) predicting the emergence of more complex capabilities, which may only appear in future frontier models. We detail our experiments for both of these settings below. 11 Published as conference paper at COLM 2024 Figure 9: Comparing emergence predictions for OpenLLaMA V1 and V2 on MMLU. We plot the MLE predictions (left) and the CDFs (right) for both series. While our focus is on predicting the specific point of emergence (e.g., the ReLU elbow), we plot the full ReLU for visual clarity. The V2 models are correctly predicted to emerge before V1, providing initial evidence that our approach can be used to evaluate data quality. See Appendix A.11 for plots with all the data used for fitting. 7.1 Cheaply Evaluating Pretraining Data Quality Emergence prediction can enable model developers to more cheaply make modeling decisions on the basis of downstream scaling trends. Without emergence prediction, making such decisions can be costly, requiring the training of large models before seeing any signal. Our emergence law can predict emergence up to 4x the FLOPs in advance, and as such, it presents potentially useful way to make these decisions more cheaply. Pretraining data quality is one such decision which can be costly to evaluate (Thrush et al., 2024). The downstream capabilities associated with particular pretraining dataset (e.g., coding) may only emerge at very large compute budgets (Blakeney et al., 2024), making it expensive to iterate on data. We can use emergence prediction to enable cheaper data iteration cycle. Background. To test the use of emergence prediction for evaluating data quality, we extend our experiments with OpeLLaMA V1 in Section 6, by also experimenting with OpenLLaMA V2 (Geng & Liu, 2023) on MMLU. The V1 and V2 models only differ in that they were trained on different corpa (Appendix A.8). As result, we should expect their emergence points to differ. The series that emerges earlier should be preferable5. Since the V1 and V2 models were pretrained on different corpa, their pretraining losses are not comparable. To compare them, we use held-out loss on the C4 validation set as our independent variable L(M). In Figure 8, we plot few-shot performance as function of C4 validation loss for both model series on MMLU and CommonsenseQA. We see that the V2 series emerges earlier than V1, suggesting that the V2 data is higher quality. Predicting data quality. We now predict the emergence for both series. Similar, to the V1 series, the 3B V2 checkpoints are all pre-emergence. Therefore, just as we did in Section 6, we hold out the larger models and only use the 3B checkpoints for fitting (see Appendix A.10 for details). We see in Figure 9, that our emergence law predicts OpenLLaMA V2 to emerge before V1. In both cases our prediction error is also well below 0.1 nats, providing initial evidence that emergence laws can be used to cheaply evaluate data quality. 7.2 Predicting the Capabilities of Future LLMs: Case Study with LLaMA 2 Emergence prediction can potentially be used to predict frontier LLM capabilities, including those which are safety relevant. However, these capabilities will be much more complex than the tasks studied in Section 6, all of which emerge well before todays frontier. To 5While the earlier emergence could show more gradual improvement post-emergence, we do not observe this on either task in Figure 8. We leave further investigation of this possibility to future work. 12 Published as conference paper at COLM 2024 Figure 10: Predicting emergence on APPS with LLaMA 2. On the left, we plot our MLE prediction. On the right, we convert this loss-based prediction into parameter count under the LLaMA 2 scaling law. The green point represents the MLE prediction, and the error bar represents the 5th to 95th percentiles under the MCMC posterior. We predict that emergence would most likely occur at 325B parameters with wide error bar from 250B to 500B parameters. For visual clarity, the left plot includes subset of the full data used for fitting (see Appendix A.11 for all). therefore validate the efficacy of our emergence law on more complex tasks, we conduct proof-of-concept experiment using LLaMA 2 on the difficult APPS coding benchmark. Background. APPS is much more challenging than the tasks studied in Section 6. Namely, all LLaMA 2 models demonstrate approximately random greedy pass@1 accuracy (0.04% for 7B and 13B and 0.16% for 70B), making all of these models pre-emergence. We note that LLaMA 2 potentially under-performs on coding tasks because the pretraining data was reportedly not optimized for code (Rozi`ere et al., 2024; Patel, 2024). Nonetheless, on easier code benchmarks like human-eval (Chen et al., 2021) and MBPP (Austin et al., 2021), LLaMA 2 achieves non-trivial performance (Touvron et al., 2023b; Rozi`ere et al., 2024), suggesting that some code was still present in pretraining. We should therefore expect that if Meta were to scale beyond 70B on the same data, we would eventually see emergence on APPS. We aim to predict this point of emergence. While we cannot verify the accuracy of this prediction, this experiment will show that the general trend in which finetuning shifts the point of emergence (Section 4) still holds for more complex tasks and with larger LLMs, acting as proof-of-concept for forecasting emergence closer to the frontier. Predicting emergence on APPS. To fit an emergence law on APPS, we finetune all three open LLaMA 2 models 7B, 13B, and 70B on subsets of the APPS training split (Appendix A.10 for details). We evaluate greedy decoding on the test set. In Figure 10 (left), we see that our observations from Section 4 finetuning shifts the point of emergence also hold on APPS, suggesting that our method can transfer to more complex tasks. We predict the point of emergence to be 0.15 nats beyond 70B LLaMA 2. We convert this prediction into parameter count, by mapping it onto the LLaMA 2 scaling law (see Appendix A.9). After this transformation, we find that emergence would most likely occur at 325B parameters with wide error bar between 250B and 500B parameters (Figure 10 right). While we cannot validate our prediction accuracy, this experiment serves as proof-of-concept that our approach can be used to predict capabilities closer to the frontier."
        },
        {
            "title": "8 Limitations and Future Directions",
            "content": "In Section 6.4, we found that our specific emergence prediction approach (e.g., emergence law) can accurately predict the point of emergence up to 4x the FLOPS in advance, representing meaningful progress on the challenging unsolved problem of emergence prediction. However, these results fall far short of the 1000x demonstrated in OpenAI et al. (2024) for predicting post-emergence downstream capabilities. Such advance predictions may be 13 Published as conference paper at COLM 2024 necessary in particularly high-stakes settings; therefore further progress is needed. Below we outline the limitations of our approach and discuss possible future directions. Better data selection may improve predictions. Our data collection procedure was not specifically designed to maximize the degree to which we can predict emergence in advance and thus can likely be improved. For example, some datapoints are more important than others for making accurate predictions (see Appendix A.6). It may therefore be possible to improve predictions by borrowing ideas from the active learning literature (Settles, 2009) to select the maximally informative datapoints for emergence law fitting. Why does finetuning result in an emergence shift? We have limited understanding of why finetuning shifts the point of emergence. In Appendix A.2, we experimented with alternative methods for inducing an emergence shift, finding model parameter updates to be an important factor. However, these results do not present complete explanation: we do not understand how finetuning interacts with emergence at mechanistic level. Is the role of finetuning to accelerate an underlying phase change inside the model, or is it just surfacing existent latent capabilities? This is an exciting question for future work to explore. Predictions may not transfer in all settings. All of our experiments are conducted using transformer checkpoints, which only meaningfully difer in the amount of pretraining data used and the parameter count. It is not well understood whether LLMs with substantially different architectures (e.g., state-space models) (Tay et al., 2022; Arora et al., 2023; Gu & Dao, 2023) or trained differently (e.g., using distillation) will demonstrate the same downstream capabilities at given pretraining loss. It is therefore possible that our emergence laws will not transfer in these settings. Future work work should investigate this possibility. Task-specific finetuning may be limited. Finetuning models broadly on many tasks at once with limited amount of data, can often fail to substantially improve models general capabilities (Gudibande et al., 2023). While we instead focused on finetuning for specific tasks in this work, it is possible that in certain settings (e.g., LM agents), models may be required to compose many different skills together. In this case, finetuning may be less effective. While we observed encouraging results on the challenging APPS benchmark in Section 7.2, further evaluation of the limits of our approach is needed."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Ruiqi Zhong, Nick Lourie, Nicholas Tomlin, Jason Wei, Kevin Liu, Jonathan Uesato, Young Geng, Daniel Bauman, and Jiayi Pan for discussion and feedback on earlier drafts of our paper. Charlie Snell is supported by the OpenAI Superalignment Fellowship. This research was supported with Cloud TPUs from Googles TPU Research Cloud (TRC)."
        },
        {
            "title": "References",
            "content": "Rishabh Agarwal, Avi Singh, Lei M. Zhang, Bernd Bohnet, Luis Rosias, Stephanie Chan, Biao Zhang, Ankesh Anand, Zaheer Abbas, Azade Nova, John D. Co-Reyes, Eric Chu, Feryal Behbahani, Aleksandra Faust, and Hugo Larochelle. Many-shot in-context learning, 2024. URL https://arxiv.org/abs/2404.11018. Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for generative mixed-modal language models, 2023. Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, et al. Gemini: family of highly capable multimodal models, 2023. 14 Published as conference paper at COLM 2024 Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario unther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Sean Eigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Yoshua Bengio, Danqi Chen, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges in assuring alignment and safety of large language models, 2024. URL https://arxiv.org/abs/2404.09932. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher Re. Zoology: Measuring and improving recall in efficient language models. arXiv preprint arXiv:2312.04927, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021. URL https://arxiv.org/abs/2108.07732. Emergent Boaz Barak. both?, abilities or URL emergent-abilities-and-grokking-fundamental-mirage-or-both/. 08-07-2024. Fundamental, mirage, and grokking: https://windowsontheory.org/2023/12/22/ Accessed: 2023. Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing ai risks in an era of rapid progress. arXiv preprint arXiv:2310.17688, 2023. Cody Blakeney, Mansheej Paul, Brett W. Larsen, Sean Owen, and Jonathan Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training, 2024. URL https://arxiv.org/abs/2406.03476. James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http://github.com/google/jax. Ethan Caballero, Kshitij Gupta, Irina Rish, and David Krueger. Broken neural scaling laws, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Together Computer. Redpajama-data: An open source recipe to reproduce llama training dataset, 2023. URL https://github.com/togethercomputer/RedPajama-Data. Zhengxiao Du, Aohan Zeng, Yuxiao Dong, and Jie Tang. Understanding emergent abilities of language models from the loss perspective, 2024. URL https://arxiv.org/abs/2403. 15796. Published as conference paper at COLM 2024 Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks, 2024. Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. URL https://github.com/openlm-research/open_llama. Young Geng. Scalax: scaling utilities for jax, 2024. URL https://github.com/young-geng/ scalax. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. arXiv preprint arXiv:2305.15717, 2023. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding, 2021. Dan Hendrycks, Mantas Mazeika, and Thomas Woodside. An overview of catastrophic ai risks, 2023. URL https://arxiv.org/abs/2306.12001. Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020. Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. arXiv preprint arXiv:2102.01293, 2021. Matthew Hoffman, Andrew Gelman, et al. The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):15931623, 2014. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. Shengding Hu, Xin Liu, Xu Han, Xinrong Zhang, Chaoqun He, Weilin Zhao, Yankai Lin, Ning Ding, Zebin Ou, Guoyang Zeng, Zhiyuan Liu, and Maosong Sun. Predicting emergent abilities with infinite resolution evaluation, 2024. URL https://arxiv.org/ abs/2310.03262. Yuzhen Huang, Jinghan Zhang, Zifei Shan, and Junxian He. Compression represents intelligence linearly, 2024. URL https://arxiv.org/abs/2404.09937. Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. Scaling laws for downstream task performance of large language models. arXiv preprint arXiv:2402.04177, 2024. Maor Ivgi, Yair Carmon, and Jonathan Berant. Scaling laws under the microscope: Predicting transformer performance from small scale experiments. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Findings of the Association for Computational Linguistics: EMNLP 2022, pp. 73547371, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-emnlp.544. URL https://aclanthology.org/2022.findings-emnlp.544. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. 16 Published as conference paper at COLM 2024 Jakub Krajewski, Jan Ludziejewski, Kamil Adamczewski, Maciej Pi oro, Michał Krutul, Szymon Antoniak, Kamil Ciebiera, Krystian Kr ol, Tomasz Odrzyg ozdz, Piotr Sankowski, Marek Cygan, and Sebastian Jaszczur. Scaling laws for fine-grained mixture of experts, 2024. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu noz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! 2023. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. Tom Lieberum, Matthew Rahtz, Janos Kramar, Neel Nanda, Geoffrey Irving, Rohin Shah, and Vladimir Mikulik. Does circuit analysis interpretability scale? evidence from multiple choice capabilities in chinchilla, 2023. URL https://arxiv.org/abs/2307.09458. Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models, 2023. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/incontext-learning-and-induction-heads/index.html. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, et al. GPT-4 technical report, 2024. David Owen. How predictable is language model benchmark performance?, 2024. URL https://arxiv.org/abs/2401.04757. Dwarkesh Patel. Mark zuckerberg - llama 3, open sourcing $10b models, & caesar augustus. Podcast, Apr 2024. URL https://www.dwarkeshpatel.com/p/mark-zuckerberg. Accessed: 9/9/2024. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, 17 Published as conference paper at COLM and web data only. arXiv preprint arXiv:2306.01116, 2023. URL https://arxiv.org/abs/ 2306.01116. Du Phan, Neeraj Pradhan, and Martin Jankowiak. Composable effects for flexible and accelerated probabilistic programming in numpyro. arXiv preprint arXiv:1912.11554, 2019. Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code llama: Open foundation models for code, 2024. URL https://arxiv.org/abs/2308.12950. Yangjun Ruan, Chris J. Maddison, and Tatsunori Hashimoto. Observational scaling laws and the predictability of language model performance, 2024. URL https://arxiv.org/ abs/2405.10938. Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models mirage?, 2023. Burr Settles. Active learning literature survey. Computer Sciences Technical Report 1648, University of Wisconsin-Madison Department of Computer Sciences, 2009. Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo, Nahema Marchal, Markus Anderljung, Noam Kolt, et al. Model evaluation for extreme risks. arXiv preprint arXiv:2305.15324, 2023. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: question answering challenge targeting commonsense knowledge, 2019. Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022. Tristan Thrush, Christopher Potts, and Tatsunori Hashimoto. Improving pretraining data using perplexity correlations, 2024. URL https://arxiv.org/abs/2409.05816. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023b. URL https://arxiv.org/abs/2307.09288. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multi-task benchmark and analysis platform for natural language In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing understanding. and Interpreting Neural Networks for NLP, pp. 353355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446. Published as conference paper at COLM 2024 Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022b. Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales, 2023. URL https://arxiv.org/abs/2212.09803. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Additional Related Work Neural scaling laws. number of works have studied the scaling behavior of language model pretraining loss Kaplan et al. (2020); Hoffmann et al. (2022); Aghajanyan et al. (2023); Muennighoff et al. (2024); Henighan et al. (2020); Krajewski et al. (2024). Most related to our work are Hernandez et al. (2021) and Isik et al. (2024), which study how finetuned language models scale as function of model size and finetuning data amount. However, these works critically differ from ours, in that they study performance on settings which are already smoothly improving with scale. Scaling laws for downstream metrics. In addition to understanding the scaling behavior of upstream metrics, number of works have proposed methods for modeling the scaling behavior of downstream performance (Ivgi et al., 2022; Caballero et al., 2023; Gadre et al., 2024; Isik et al., 2024; Owen, 2024; Ruan et al., 2024; Hu et al., 2024). All of these works differ from our setting in that they assume the downstream metric of interest is already showing signs of smooth improvement as function of model scale (i.e., post-emergence). Most related to our work is concurrent work from Blakeney et al. (2024), which shows that by up-sampling domain-specific data at the end of pretraining, downstream LLM performance on certain tasks can be boosted, even on emergent tasks. Our work instead focuses on using the boost from finetuning to help predict the point of emergence in the few-shot setting. A.2 Alternative Methods for Shifting the Point of Emergence We are interested in understanding what other methods can induce shift in the point of emergence. We therefore conduct additional experiments on MMLU with 1) varying the number of shots in the prompt, 2) performing continuous prefix tuning Li & Liang (2021), and 3) using low-rank finetuning with LoRA. We see in Figure 11 that both prefix tuning and using fewer shots in the few-shot prompt have little effect on shifting the point of emergence. On the other hand, in Figure 12 we see that low-rank finetuning shifts the point of emergence to comparable degree to that of full finetuning, even in the rank-1 setting. Together these results suggest that updating the models parameters may be necessary for shifting the point of emergence. Though further exploration of this phenomenon is needed. In particular, it is possible that using hundreds or thousands of shots in the prompt may enable more of shift than what we observed (Agarwal et al., 2024). In each of these experiments, we use all 3B, 7B, and 13B OpenLLaMA V1 model checkpoints. For the prefix tuning baseline we use prefix length of 8, parameterized by 2-layer MLP with input and hidden-dim 512. We train with learning rate of 3e-4 and keep all other hyperparameters the same as our full finetuning experiments. We found attempts to increase the capacity of the prefix tuning (e.g., using longer prefix or dropping the MLP and 19 Published as conference paper at COLM 2024 Figure 11: One the left we compare full fine-tuning against continuous prefix tuning on MMLU. We find that prefix tuning provides effectively no shift to the point of emergence, despite improving the performance of post-emergence models. On the right we compare 0-shot verses 5-shot prompting on MMLU. We see that using fewer shots has no meaningful effect on the point of emergence. Together these results suggest that the ability for prompt tuning to shift the point of emergence is very limited. Figure 12: Comparing LoRA finetuning, with rank 1, 2, 4, and 64 against full finetuning on MMLU. We see that LoRA finetuning even with rank 1 shifts the point of emergence to comparable degree to that of full finetuning. directly tuning the embeddings) to make training more unstable and generally yield worse performance. For LoRA we use the same finetuning hyper-parameters as full fine-tuning, including the learning rate. To ensure that the LoRA updates are of similar magnitude to full finetuning with the same learning rate, we set the LoRA α hyper-parameter to be equal to the models hidden dimension. We found this setup to yield the best results with minimal changes between our full finetuning and LoRA setup. Figure 13: On standard 5-shot MMLU and 6-shot CommonsenseQA (CSQA) evaluation, we observe emergence using both the standard correct answer accuracy evaluation and continuous LLM logprobability metric. 20 Published as conference paper at COLM 2024 Figure 14: We plot few-shot and full data finetuning performance as function of pretraining loss using all 3B, 7B, and 13B model checkpoints for all tasks. We see that both the point of emergence and the downstream performance scaling thereafter, as function of pretraining loss, is consistent across model size in both the few-shot and finetuned setting. A.3 Emergence with Continuous Metrics In Figure 13 we present two LLM tasks (MMLU and CommonsenseQA) in which we observe emergence with both continuous and discontinuous metrics. A.4 Finetuning and Few-shot Emergence Across Model Sizes In Figure 14, we plot emergence as function of pretraining loss using all 3B, 7B, and 13B OpenLLaMA V1 model intermediate checkpoints in both the few-shot and finetuned setting. We finetune on the full data amount. The specific finetuning hyperparameters and data splits used are detailed in Appendix A.10. For the few-shot results, we use the prompts detailed in Appendix A.10. On all tasks, we see that both the point of emergence and the downstream performance scaling thereafter, as function of pretraining loss, is consistent across model size in both the few-shot and finetuned setting. In the case of CoLA, finetuning on the full data amount significantly shifts the point of emergence to such degree that it precedes all model checkpoints that we consider. We see in Section A.11 that when using smaller data amounts, the emergence elbow on CoLA is shifted towards stronger models and is thus visible with our checkpoints. 21 Published as conference paper at COLM 2024 Ablation Setting Full Data -1 Smallest Subset -2 Smallest Subset -3 Smallest Subset -1 Largest Subset -2 Largest Subset -3 Largest Subset GSM8K MMLU CSQA CoLA 0.022 [0.004, 0.170] 0.041 [0.011, 0.055] 0.003 [0.001, 0.045] 0.064 [0.030, 0.121] 0.014 [0.002, 0.051] 0.047 [0.018, 0.063] 0.005 [0.002, 0.050] 0.022 [0.010, 0.031] 0.025 [0.003, 0.032] 0.001 [0.002, 0.025] 0.051 [0.037, 0.084] 0.087 [0.063, 0.129] 0.988 [0.913, 1.099] 0.071 [0.024, 0.097] 0.634 [0.577, 0.711] 1.513 [1.291, 1.698] 0.022 [0.002, 0.032] 0.005 [0.002, 0.054] 0.016 [0.002, 0.083] 0.034 [0.024, 0.034] 1.017 [1.482, 1.985] 0.332 [1.371, 4.200] 0.045 [0.003, 0.096] 0.057 [0.056, 0.059] 0.089 [0.077, 0.098] 0.036 [0.007, 0.090] 0.004 [0.002, 0.058] 0.044 [0.019, 0.143] Only Subset Sample 1 Only Subset Sample 2 0.006 [0.001, 0.031] 0.026 [0.002, 0.067] 0.244 [0.223, 0.489] 0.073 [0.059, 0.077] 1.557 [1.616, 2.573] 0.035 [0.004, 0.102] 0.179 [0.125, 0.269] 0.034 [0.022, 0.047] Last 6 Checkpoints Last 5 Checkpoints Last 4 Checkpoints Last 3 Checkpoints Last 6 Checkpoints, Every Other Even Last 6 Checkpoints, Every Other Odd 0.010 [0.003, 0.113] 0.047 [0.048, 0.176] 0.080 [0.072, 4.925] 0.159 [0.124, 0.703] 0.019 [0.003, 0.162] 0.044 [0.041, 0.126] 0.041 [0.007, 0.049] 0.032 [0.020, 0.038] 0.030 [0.001, 0.042] 0.013 [0.002, 0.059] 0.070 [0.059, 0.075] 0.040 [0.024, 0.045] 0.075 [0.063, 0.301] 1.165 [1.070, 1.728] 1.734 [1.555, 2.308] 0.986 [0.802, 1.249] 1.663 [1.667, 1.781] 0.037 [0.031, 0.191] 0.118 [0.087, 0.167] 0.130 [0.099, 0.170] 0.076 [0.052, 0.111] 0.039 [0.004, 0.077] 0.033 [0.007, 0.050] 0.224 [0.190, 0.287] -1 Last Checkpoints -2 Last Checkpoints -3 Last Checkpoints -4 Last Checkpoints 0.069 [0.003, 0.149] 0.087 [0.003, 0.176] 0.110 [0.010, 0.468] 0.044 [0.005, 0.089] 0.043 [0.023, 0.055] 0.076 [0.046, 0.089] 0.664 [0.500, 0.959] 2.308 [2.347, 57.068] 0.985 [0.858, 1.800] 0.102 [0.098, 0.104] 0.616 [0.510, 0.822] 0.581 [0.546, 1.169] 0.026 [0.004, 0.079] 0.165 [0.098, 0.242] 2.217 [2.031, 2.407] 1.039 [0.905, 1.298] Table 2: Ablating the effect of holding out different finetuning subsets and model checkpoints when fitting the emergence law. We present the absolute error between the maximum likelihood predicted point of emergence and the ground-truth. In brackets we include the 5th and 95th percentile of prediction errors produced by our MCMC posterior sampling. We consider fits where the maximum likelihood prediction is greater than 0.1 nats from the ground-truth to be failures and highlight these cases in red; otherwise we highlight in green. In the top row we present results for the fit obtained using all finetuning data amounts and model checkpoints. In the middle rows (e.g., -1 Smallest Subset to Only Subset Sample 2), we present ablations in which we hold out various finetuning data subsets, so as to understand the effect of our data subset selection methodology on our predictions. Finally, in the bottom rows, we present ablations in which we hold out various model checkpoints, so as to understand how many checkpoints are needed to obtain good predictions (e.g., Last 6 Checkpoints to -4 Last Checkpoint). We describe each ablation in more detail in Appendix A.6. A.5 Emergence Law Fitting For obtaining our emergence law maximum-likelihood fits we follow the procedure in Hoffmann et al. (2022) and use the L-BFGS optimizer, selecting the best fit from sweep over initializations. In particular, we first perform brute-force grid-search over all the values in: {0.0, 0.1, 0.2, ..., 4.0}, {0.0, 0.1, 0.2, ..., 1.0}, {0.0, 0.05, 0.1, ..., 1.0}, α {1.0, 1.5, 2.0, ..., 10.0}, {0.0, 0.5, 1.0, ..., 10.0}. We select the 100K parameters from this grid-search with the lowest loss and then run the L-BFGS optimizer initialized from each one. We then select the best fit final fit from all optimization runs. We fit the ReLU for the true emergence point using the same procedure. A.6 Emergence Law Data Ablations We would like to understand what aspects of our emergence law data collection methodology are important for enabling effective emergence predictions. In particular, our data collection involves finetuning several model checkpoints on different subsets of our full finetuning data. In general, we should expect using more model checkpoints and more finetuning data subsets for fitting the emergence law to strictly improve our predictions. However, in practice we may have limited finetuning budget or may have access to limited set of model checkpoints. For this reason, we would like to better understand the limits and best practices for selecting the empirical datapoints for emergence law fitting. Concretely, there are two variables in our emergence law data collection procedure: the set of checkpoints selected, and the set of data subsets we finetune on. In Table 2, we conduct series of ablations in which we modify both of these variables to understand how they effect our emergence predictions. We discuss each of these ablations below. 22 Published as conference paper at COLM 2024 Holding out finetuning data amounts. As described in Appendix A.10, we select the sizes of the finetuning data subsets for fitting the emergence law by first conducting logarithmic sweep over various data amounts (e.g., 1 8 the data, etc...). We then then collect additional subsets nearer to the limit at which emergence is visible with our 3B checkpoints. We also finetune on two different sampled subsets at each data amount to account for noise in the subsampling process. 4 the data, 1 2 the data, 1 To better understand how our data subset selection procedure effects the accuracy of our emergence predictions, we experiment with holding out the largest and smallest subsets when conducting our fits (see the rows -N Smallest Subset and -N Largest Subset in Table 2). We see that, with the exception of MMLU, our predicted fits remain fairly accurate when holding out the larger subsets, whereas the holding out smaller subsets can have larger effect on prediction accuracy. This finding aligns with our intuition that the smaller subsets, which are closer to the low data extrapolation limit which we use to make our emergence predictions, are more critical for making accurate predictions. We also ablate the effect of using two independent subsamples at each finetuning data amount to account for noise, by fitting our emergence predictions using only 1 subsample. This ablation is presented in the Only Subset Sample 1 and Only Subset Sample 2 rows of Table 2, where each row corresponds to using only one of the two sub-samples for fitting the emergence law. We see that the noise from using only single subset can result in worse predictions. Therefore including multiple subsamples can be useful way to integrate out noise when fitting an emergence law. Holding out model checkpoints. For each task, we use results from at least 6 and up to 9 intermediate model checkpoints to fit our emergence law, which are mostly evenly spaced (see Appendix A.10 for the specific details). We would like to better understand just how many checkpoints are needed to make accurate predictions. Since we model emergence with ReLU, we will need at least 3 checkpoints, but, in practice, it is likely that more checkpoints will be necessary. We expect that the later checkpoints, which are closer to the point of emergence, should be more informative for making emergence predictions. Therefore, to understand the minimum number of checkpoints needed to make accurate emergence predictions, we fit our emergence law using only the last checkpoints (see the Last Checkpoints rows in Table 2). We also consider using fewer checkpoints but spacing them out more. Specifically, in the Last 6 Checkpoints, Every Other Even and Last 6 Checkpoints, Every Other Odd rows in Table 2, we consider every other checkpoint from the most recent 6 checkpoints, leaving 3 total checkpoints. The odd case represents every other checkpoint shifted by one from the even case. We see that removing checkpoints can cause predictions to worsen, however if the checkpoints are well spaced, as in the Every Other case, predictions can, in some cases, remain highly accurate with only 3 checkpoints. Finally in the -N Last Checkpoints rows in Table 2, we hold out the final checkpoints. These represent the complete results for the ablations in Section 6.4 aimed at understanding how far in advance our emergence law can make predictions. A.7 Comparing Uncertainty Estimation Approaches In Table 3, we compare MCMC and Bootstrap uncertainty estimation on each of our tasks. We see that both techniques yield fairly similar confidence intervals across the various tasks. In the case of APPS, we observe marginally wider confidence interval using bootstrapping. If we convert these loss-based 90% confidence intervals on APPS into parameter-count (under the LLaMA 2 scaling law) based confidence intervals, we see that the wider bootstrapping interval corresponds to range of 225B to 830B parameters rather than 244B to 509B parameters with MCMC. Our MCMC posterior sampling uses uniform prior over parameters. We sample using the No-U-Turn Sampler Hoffman et al. (2014) implemented in Numpyro (Phan et al., 2019). We use 4 chains with 25k samples each and 15k warmup steps each, totaling 100k samPublished as conference paper at COLM 2024 ples. We initialize each chain using the maximum likelihood parameters found using the procedure described in Appendix A.5. We find that without tuning the temperature of the energy function (e.g., Energy = loss/temperature) the sampler is extremely unstable, demonstrating wildly out-of-distribution posterior samples. To control this instability, we sweep over temperature values 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9 and select the greatest such temperature for which the mode of the sampled distribution is centered around the maximum likelihood estimate. To obtain the bootstrap uncertainty estimate, we fit our emergence law on 1000 bootstrap samples of the data. Since our emergence law fitting procedure weighs each example in inverse proportion to the size of the finetuning data amount (see Section 5.3), we perform weighted bootstrap sampling by sampling with replacement from distribution proportional to each datapoints weight. The number of datapoints we sample for each bootstrap set is equal to the total sum of the dataset weights. We then count the number of times each datapoint is sampled and use this count as the new weight when fitting the emergence law on the bootstrap dataset. Task Method 5% 10% 25% 50% 75% 90% 95% MLE GT GSM8K MMLU CSQA CoLA MMLU C4 MMLU C4 V2 APPS MCMC 1.813 Bootstrap 1.978 MCMC 1.825 Bootstrap 1.818 MCMC 1.781 Bootstrap 1.723 MCMC 1.712 Bootstrap 1. MCMC 2.207 Bootstrap 2.183 2.264 MCMC Bootstrap 2.249 MCMC 1.324 Bootstrap 1.285 1.852 1.984 1.828 1.825 1.810 1. 1.724 1.746 2.221 2.200 2.275 2.257 1.332 1.304 1.900 1.995 1.837 1. 1.821 1.815 1.742 1.758 2.241 2.216 2.289 2.272 1.344 1.330 1.937 2. 1.847 1.848 1.829 1.835 1.761 1.770 2.246 2.228 2.306 2.284 1.357 1. 1.970 2.021 1.858 1.859 1.835 1.846 1.779 1.782 2.255 2.238 2.310 2. 1.370 1.370 1.992 2.031 1.866 1.867 1.840 1.857 1.795 1.791 2.259 2. 2.316 2.305 1.380 1.385 2.003 2.036 1.869 1.871 1.843 1.863 1.804 1. 2.261 2.250 2.320 2.311 1.386 1.393 2.006 1.984 1. 1.814 1.830 1.827 1.769 1.833 2. 2.226 2.311 2.318 1.361 Table 3: Comparing emergence prediction uncertainty estimates obtained via MCMC and bootstrapping. On each task, we present seven range of percentiles for the point of emergence in terms of pretraining loss for each distribution. We also present the maximum likelihood prediction (MLE), and the ground-truth (GT) point of emergence. We see that both methods generally produce similar distributions. In the top section we present the uncertainties for each task used in Section 6.2. In the middle we include uncertainties for the data quality experiments in Section 7.1. MMLU C4 V1 refers to the OpenLLaMA V1 fit and MMLU C4 V2 refers to the V2 fit. At the bottom, we include uncertainties for the APPS experiment in Section 7.2. A.8 OpenLLaMA V1 Verses V2 Pretraining Data The OpenLLaMA V1 models were trained on the RedPajama (Computer, 2023) dataset, whereas the V2 models were trained on custom mixture of the Falcon Refined-web (Penedo et al., 2023) dataset, the StarCoder (Li et al., 2023) dataset, and the RedPajama (Computer, 2023) dataset. The V2 models also used different tokenizer, which was trained on its corresponding pretraining dataset. Most hyperparameters between the two series of models remain the same. A.9 LLaMA 2 Scaling Law Details We provide details for our LLaMA 2 scaling law fit in Section 7.2. We use the LLaMA 2 7B, 13B, 34B, and 70B models to fit the scaling law. We extract the loss for each model from Figure 5 in Touvron et al. (2023b). Specifically we use losses of 1.75, 1.675, 1.575, and 1.5 nats for the 7B, 13B, 34B, and 70B models respectively. All models were trained on the same 2 trillion tokens. Therefore, since the data amount is fixed, we modify the functional form from Hoffmann et al. (2022) by absorbing the data scaling term into the irreducible loss (e.g., 24 Published as conference paper at COLM 2024 Figure 15: We plot our scaling law fit for the LLaMA 2 series of models. We also include the learned values for our final fit on the plot. In this case corresponds to parameter count in billions. We see that the LLaMA 2 models are well modeled by our scaling law. E). This gives functional form of: L(N) = Nα + E. In this case A, α, and are all learned parameters, and corresponds to parameter count in billions. We include plot of our LLaMA 2 scaling law fit in Figure 15. To fit the scaling law we follow similar procedure to Hoffmann et al. (2022). We optimize mean squared error in log space. We first perform brute-force grid-search over all the values in: {0.0, 0.1, 0.2, ..., 5.0}, eα {0.0, 0.01, 0.02, ..., 1.0}, {0.0, 0.01, 0.02, ..., 1.0}. We select the 100k parameters from this grid search with the lowest loss and then run the L-BFGS optimizer initialized from each one. We then select the best final fit from all optimization runs. A.10 Additional Experiment Details For all experiments, to select the set of finetuning data subsets used for fitting our emergence law, we first conduct logarithmic sweep over various data amounts (e.g., 1 2 the data, 1 4 the data, 1 8 the data, etc...) and then collect additional subsets nearer to the limit at which emergence is visible with our 3B checkpoints. To account for noise in the data sub-sampling process, we finetune on two different sampled subsets for each data subset level. For each experiment with OpenLLaMA V1 and V2, we use at least 6 model checkpoints for fitting the emergence law. In all tasks we used the same initial set of 6 checkpoints. However, on some tasks, we found that after finetuning on the full data, the earliest of the 6 checkpoints was significantly post-emergence. We therefore used up to 3 additional earlier checkpoints in these cases. We include full details of which checkpoints we used for each task below. All checkpoints are publically available at https://huggingface.co/openlm-research. For, all tasks we perform full parameter finetuning using the AdamW optimizer. We use 0.05 dropout, 0.01 weight decay, Adam β1 = 0.9, β2 = 0.95, and learning rate schedule with linear warmup followed by constant learning rate. We hold out 10% of our training data for validation. We perform small sweep over learning rate, batch size, and the number of learning-rate warm-up steps, selecting the best setting that produces the best validation loss. We perform early stopping according to validation loss on all tasks except on CoLA, which 25 Published as conference paper at COLM 2024 we describe in more detail below. For all of our experiments, except those in Section 7.1 and Section 7.2, we use the OpenLLaMA V1 (Geng & Liu, 2023) models that were pretrained on 1T tokens from the Red-pajama dataset (Computer, 2023). In Section 7.1, we also use the OpenLLaMA V2 models which were pretrained on different corpus, also for 1T tokens. Finally, in Section 7.2, we use the LLaMA 2 series of models (Touvron et al., 2023b) (7B, 13B, and 70B), which were pretrained for 2T tokens on proprietary corpus. Below we include more details on our experiments for each task. MMLU. For predicting emergence on MMLU we use 6 intermediate 3B model checkpoints pretrained for 42B, 210B, 419B, 629B, 839B, and 1T tokens. We train on the MMLU test set and then evaluate on the validation set. In addition to the full data, we train each checkpoint on two different randomly sampled dataset subsets of each fraction: 1 16 , and 3 32 . We use learning rate 5e-6, batch size 256, and 24 learning-rate warmup steps. We use the standard 5-shot LM evaluation harness (Touvron et al., 2023a) prompt for our few-shot results. For the OpenLLaMA V2 checkpoints in Section 7.1, we use model checkpoints pretrained for 42B, 210B, 419B, 629B, 839B, and 965B tokens. Otherwise, use these same settings for our experiment with OpenLLaMA V2. 16 , 3 8 , 4 , 1 2 , 1 GSM8K. For predicting emergence on GSM8K we use 8 intermediate 3B model checkpoints pretrained for 21B, 31B, 42B, 210B, 419B, 629B, 839B, and 1T tokens. We train and test on the standard splits. On this task our models are trained to output chain-of-thought (Wei et al., 2022b) followed by final answer, using the CoT traces provided with the GSM8K dataset. In addition to the full data, we train each checkpoint on two different randomly sampled dataset subsets of each fraction: 1 4 , 1 64 . We use learning rate of 1e-5, batch size of 32, and no learning-rate warmup steps. For our prompted evaluation, we use the 6-shot chain-of-thought prompt from Zelikman et al. (2022). 32 , and 3 16 , 1 8 , 2 , 1 CommonsenseQA. For predicting emergence on CommonsenseQA we use 9 intermediate 3B model checkpoints pretrained for 10B, 21B, 31B, 42B, 210B, 419B, 629B, 839B, and 1T tokens. We train on the standard train set and evaluate on the validation set. In addition to the full data, we train each checkpoint on two different randomly sampled dataset subsets of each fraction: 1 16 . We use learning rate of 5e-6, batch size of 64, and 96 learning-rate warmup steps. For our prompted evaluation we use the 7-shot prompt from Wei et al. (2022b) with the chain of thought examples stripped from the prompt, just showing the final answer. 32 , and 3 32 , 3 64 , 3 16 , 8 , 1 4 , 1 2 , 1 16 , 1 32 , 1 64 , CoLA. For predicting emergence on CoLA we use 9 intermediate 3B model checkpoints pretrained for 10B, 21B, 31B, 42B, 210B, 419B, 629B, 839B, and 1T tokens. We train on the standard train set and evaluate on the validation set. In addition to the full data, we train 2 , 1 each checkpoint on two different randomly sampled dataset subsets of each fraction: 1 4 , 8 , 1 1 256 . We use learning rate of 5e-6, batch size of 256, and 48 learning-rate warmup steps. For our prompted evaluation we use the standard 5-shot prompt in the LM evaluation harness (Touvron et al., 2023a). On CoLA we deviate slightly from our early stopping procedure. We find that for some of the smaller data subsets, downstream validation performance continues to improve after overfitting. In these cases we perform early stopping according to the downstream MCC evaluation on the validation set rather than the validation loss. 128 , and 3 3 128 , 3 1 64 , 2 , 1 APPS. For predicting emergence on APPS we use the 7B, 13B, and 70B LLaMA 2 base models (Touvron et al., 2023b), each of which were trained for 2T tokens. In addition to the full data, we train each model on two different randomly sampled dataset subsets of each fraction: 1 128 . In the APPS training set there are multiple solutions provided for each answer, we include all solutions in our finetuning dataset. However, when we sample subsets (e.g., 1 2 the data), rather than taking random sample of all solutions, we take random sample of the questions, and include all solutions per question. We use learning rate of 5e-6, batch size of 64, and no learning-rate warmup steps. For our prompted evaluation we use the 2-shot prompt in Appendix A.12. 256 , and 3 128 , 64 , 1 32 , 1 16 , 1 8 , 1 4 , 1 Published as conference paper at COLM 2024 Figure 16: We plot the maximum likelihood predictions from our emergence law on each task. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that across all tasks we are able to successfully predict the point of emergence within 0.1 nats and in many cases much less than that. All finetuning experiments were conducted on TPU-V3 and TPU-V5e pods using JAX (Bradbury et al., 2018) and Scalax (Geng, 2024) for distributed training. A.11 Full Emergence Prediction Plots In Figure 16 we plot all of the data used for our maximum likelihood emergence predictions from Section 6. In Figure 17 we plot all the data for our maximum likelihood emergence predictions using OpenLLaMA V1 and V2 on MMLU in Section 7.1. In Figure 18 we plot all the data for our MLE prediction and MCMC CDF using LLaMA 2 on APPS in Section 7.2. Finally, in Figure 19, we plot the MCMC CDF of the point of emergence under our emergence law for all tasks in Section 6. 27 Published as conference paper at COLM 2024 Figure 17: We plot the maximum likelihood predictions from our emergence law with OpenLLaMA V1 (left) and OpenLLaMA V2 (right) on MMLU. We plot C4 Validation loss on the x-axis. These plots include results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that in both cases we are able to successfully predict the point of emergence within 0.1 nats. Figure 18: We plot the MLE prediction (left) and MCMC CDF (right) for our emergence law fit using LLaMA 2 on APPS. The left plot includes results from every finetuning run used for fitting the emergence law. The grey line represents our extrapolated prediction and the multi-color lines correspond to the fit produced by the emergence law for the various data levels. We see that our emergence law predicts emergence roughly 0.15 nats beyond the LLaMA 2 70B model. 28 Published as conference paper at COLM 2024 Figure 19: We plot the cumulative distribution function of our estimated posterior distribution over the point of emergence on each task. The stars correspond to few-shot performance on the task and represent the true emergence curve. The point at which the slope of the CDF peaks represents the mode of the distribution. We see across all tasks that the distribution spikes near the true point of emergence and is followed by moderately long tail. A.12 APPS 2-shot Prompt We include the two shot prompt we use for few-shot evaluation of LLaMA 2 on APPS below: \"\"\" You are given an array $a$ of length $n$ consisting of zeros. You perform $n$ actions with this array: during the $i$-th action, the following sequence of operations appears: Choose the maximum by length subarray (continuous subsegment) consisting only of zeros, among all such segments choose the leftmost one; Let this segment be $[l; r]$. If $r-l+1$ is odd (not divisible by $2$) then assign (set) $a[frac{l+r}{2}] := i$ (where $i$ is the number of the current action), otherwise (if $r-l+1$ is even) assign (set) $a[frac{l+r-1}{2}] := i$. Consider the array $a$ of length $5$ (initially $a=[0, 0, 0, 0, 0]$). Then it changes as follows: Firstly, we choose the segment $[1; 5]$ and assign $a[3] := 1$, so $a$ becomes $[0, 0, 1, 0, 0]$; then we choose the segment $[1; 2]$ and assign $a[1] := 2$, so $a$ becomes $[2, 0, 1, 0, 0]$; then we choose the segment $[4; 5]$ and assign $a[4] := 3$, so $a$ becomes $[2, 0, 1, 3, 0]$; then we choose the segment $[2; 2]$ and assign $a[2] := 4$, so $a$ becomes $[2, 4, 1, 3, 0]$; and at last we choose the segment $[5; 5]$ and assign $a[5] := 5$, so $a$ becomes $[2, 4, 1, 3, 5]$. Your task is to find the array $a$ of length $n$ after performing all $n$ actions. Note that the answer exists and unique. You have to answer $t$ independent test cases. -----Input----- The first line of the input contains one integer $t$ ($1 le le 10^4$) - the number of test cases. Then $t$ test cases follow. The only line of the test case contains one integer $n$ ($1 le le 2 cdot 10^5$) - the length of $a$. It is guaranteed that the sum of $n$ over all test cases does not exceed $2 cdot 10^5$ ($sum le 2 cdot 10^5$). -----Output----- 29 Published as conference paper at COLM 2024 For each test case, print the answer - the array $a$ of length $n$ after performing $n$ actions described in the problem statement. Note that the answer exists and unique. -----Example----- Input 6 1 2 3 4 5 Output 1 1 2 2 1 3 3 1 2 4 2 4 1 3 5 3 4 1 5 2 6 \"\"\" from collections import defaultdict as dd from collections import deque import bisect import heapq def ri(): return int(input()) def rl(): return list(map(int, input().split())) def solve(): = ri() output = [0] * (n) = [(-n, 0 ,n - 1)] for in range(1, + 1): prev = heapq.heappop(Q) lo, hi = prev[1], prev[2] mid = (lo + hi) // 2 output[mid] = if mid > lo: heapq.heappush(Q, (-(mid - 1 - lo), lo, mid - 1)) if hi > mid: heapq.heappush(Q, (-(hi - 1 - mid), mid + 1, hi)) print(*output) mode = if mode == T: = ri() for in range(t): solve() else: solve() \"\"\" $n$ robots have escaped from your laboratory! You have to find them as soon as possible, because these robots are experimental, and their behavior is not tested yet, so they may be really dangerous! Fortunately, even though your robots have escaped, you still have some control over them. First of all, you know the location of each robot: the world you live in can be modeled as an infinite coordinate plane, and the $i$-th robot is currently located at the point having coordinates ($x_i$, $y_i$). Furthermore, you may send exactly one command to all of the robots. The command should contain two integer numbers $X$ and $Y$, and when each robot receives this command, it starts moving towards the point having coordinates ($X$, $Y$). The robot stops its movement in two cases: either it reaches ($X$, $Y$); or it cannot get any closer to ($X$, $Y$). Normally, all robots should be able to get from any point of the coordinate plane to any other point. Each robot usually can perform four actions to move. Lets denote the current coordinates of the robot as ($x_c$, $y_c$). Then the movement system allows it to move to any of the four adjacent points: the first action allows it to move from ($x_c$, $y_c$) to ($x_c - 1$, $y_c$); the second action allows it to move from ($x_c$, $y_c$) to ($x_c$, $y_c + 1$); the third action allows it to 30 Published as conference paper at COLM move from ($x_c$, $y_c$) to ($x_c + 1$, $y_c$); the fourth action allows it to move from ($x_c$, $y_c$) to ($x_c$, $y_c - 1$). Unfortunately, it seems that some movement systems of some robots are malfunctioning. For each robot you know which actions it can perform, and which it cannot perform. You want to send command so all robots gather at the same point. To do so, you have to choose pair of integer numbers $X$ and $Y$ so that each robot can reach the point ($X$, $Y$). Is it possible to find such point? -----Input----- The first line contains one integer $q$ ($1 le le 10^5$) - the number of queries. Then $q$ queries follow. Each query begins with one line containing one integer $n$ ($1 le le 10^5$) - the number of robots in the query. Then $n$ lines follow, the $i$-th of these lines describes the $i$-th robot in the current query: it contains six integer numbers $x_i$, $y_i$, $f_{i, 1}$, $f_{i, 2}$, $f_{i, 3}$ and $f_{i, 4}$ ($-10^5 le x_i, y_i le 10^5$, $0 le f_{i, j} le 1$). The first two numbers describe the initial location of the $i$-th robot, and the following four numbers describe which actions the $i$-th robot can use to move ($f_{i, j} = 1$ if the $i$-th robot can use the $j$-th action, and $f_{i, j} = 0$ if it cannot use the $j$-th action). It is guaranteed that the total number of robots over all queries does not exceed $10^5$. -----Output----- You should answer each query independently, in the order these queries appear in the input. To answer query, you should do one of the following: if it is impossible to find point that is reachable by all $n$ robots, print one number $0$ on separate line; if it is possible to find point that is reachable by all $n$ robots, print three space-separated integers on the same line: $1$ $X$ $Y$, where $X$ and $Y$ are the coordinates of the point reachable by all $n$ robots. Both $X$ and $Y$ should not exceed $10^5$ by absolute value; it is guaranteed that if there exists at least one point reachable by all robots, then at least one of such points has both coordinates not exceeding $10^5$ by absolute value. -----Example----- Input 4 2 -1 -2 0 0 0 0 -1 -2 0 0 0 0 3 1 5 1 1 1 1 2 5 0 1 0 1 3 5 1 0 0 0 2 1337 1337 0 1 1 1 1336 1337 1 1 0 1 1 3 5 1 1 1 1 Output 1 -1 -2 1 2 5 0 1 -100000 -100000 \"\"\" def main(): import sys input = sys.stdin.readline def solve(): = int(input()) maxx = 10**5 minx = -10**5 maxy = 10**5 miny = -10**5 for _ in range(n): x, y, f1, f2, f3, f4 = map(int, input().split()) if not f1: minx = max(minx, x) if not f2: maxy = min(maxy, y) if not f3: maxx = min(maxx, x) if not f4: 31 Published as conference paper at COLM 2024 miny = max(miny, y) if minx > maxx or miny > maxy: print(0) else: print(1, minx, miny) for _ in range(int(input())): solve() return 0 main() \"\"\" {QUESTION} \"\"\""
        }
    ],
    "affiliations": [
        "University of California, Berkeley"
    ]
}