{
    "paper_title": "AudioX: Diffusion Transformer for Anything-to-Audio Generation",
    "authors": [
        "Zeyue Tian",
        "Yizhu Jin",
        "Zhaoyang Liu",
        "Ruibin Yuan",
        "Xu Tan",
        "Qifeng Chen",
        "Wei Xue",
        "Yike Guo"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce high-quality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX, a unified Diffusion Transformer model for Anything-to-Audio and Music Generation. Unlike previous domain-specific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various modalities including text, video, image, music, and audio. Its key innovation is a multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified cross-modal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2M-caps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within a unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ]"
        },
        {
            "title": "M\nM",
            "content": ". [ 1 2 2 5 0 1 . 3 0 5 2 : r AudioX: Diffusion Transformer for Anything-to-Audio Generation Zeyue Tian1, Yizhu Jin1, Zhaoyang Liu1, Ruibin Yuan1, Xu Tan2, Qifeng Chen1, Wei Xue1, Yike Guo1 1Hong Kong University of Science and Technology 2Moonshot AI Figure 1. (a) Overview of AudioX, illustrating its capabilities across various tasks. (b) Radar chart comparing the performance of different methods across multiple benchmarks. AudioX demonstrates superior Inception Scores (IS) across diverse set of datasets in audio and music generation tasks."
        },
        {
            "title": "Abstract",
            "content": "Audio and music generation have emerged as crucial tasks in many applications, yet existing approaches face significant limitations: they operate in isolation without unified capabilities across modalities, suffer from scarce highquality, multi-modal training data, and struggle to effectively integrate diverse inputs. In this work, we propose AudioX1, unified Diffusion Transformer model for Anythingto-Audio and Music Generation. Unlike previous domainspecific models, AudioX can generate both general audio and music with high quality, while offering flexible natural language control and seamless processing of various 1Work in progress modalities including text, video, image, music, and audio. Its key innovation is multi-modal masked training strategy that masks inputs across modalities and forces the model to learn from masked inputs, yielding robust and unified crossmodal representations. To address data scarcity, we curate two comprehensive datasets: vggsound-caps with 190K audio captions based on the VGGSound dataset, and V2Mcaps with 6 million music captions derived from the V2M dataset. Extensive experiments demonstrate that AudioX not only matches or outperforms state-of-the-art specialized models, but also offers remarkable versatility in handling diverse input modalities and generation tasks within unified architecture. The code and datasets will be available at https://zeyuet.github.io/AudioX/. 1 1. Introduction In recent years, audio generation, especially for sound effects and music, has emerged as crucial elements in multimedia creation, showing practical values in enhancing user experiences across wide range of applications. For example, in social media, film production, and video games, sound effects and music significantly intensify emotional resonance and engagement with the audience. The ability to create high-quality audio not only enriches multimedia content but also opens up new avenues for creative expression. However, the manual production of audio is timeconsuming and needs specialized skills. This presents compelling research opportunity to automate audio generation. As such, some notable advancements [11, 34, 40, 46, 59] have been made in audio generation. Lots of works are able to generate audio only from single-modal conditions e.g., text [34, 40, 42], video [45, 65], etc., while some pioneers [50, 70] manage to accommodate multi-modal conditions in audio generation, which, however, lacks the flexibility to arbitrarily combine different modalities of input. In addition, some of expert models [7, 11, 12, 59, 68] solely focus on generating sound effects or music and fail to meet diverse demands in generation. Generally, these works either limit their input or output domains, significantly posing negative impact on practical values. One major factor behind these limitations is the scarcity of high-quality multi-modal data. Existing datasets tend to focus on single modality paired with audio: for instance, WavCaps [47] and AudioCaps [32] offer only text conditions, while VGGSound [5], AudioSet [18], and V2M [59] provide primarily video conditions. This narrow focus limits the diversity of training data and hinders the development of models that seamlessly integrate multiple modalities. To this end, we propose unified framework termed AudioX for anything-to-audio generation, complemented by two extensive multi-modal datasets: vggsound-caps, which contains 190K audio captions derived from the VGGSound [5] dataset, and V2M-caps, comprising 6 million music captions based on the V2M [59] dataset. These datasets serve as rich foundation for our approach. On the one hand, our framework is able to accommodate multimodal conditions, i.e., text, video, image and audio. These conditions can be flexibly combined to generate audio and support variety of tasks. On the other hand, our framework is trained on large-scale dataset collected across various audio domains, which aims to generate different genres of audio including sound effects, music, etc. Consequently, AudioX enables wide range of tasks as shown in Fig. 1, including text-to-audio generation, video-to-audio generation, text-guided audio inpainting, text-guided music completion, etc. We observe that Transformer-based works [38, 41, 63, 69] have effectively tackled multi-modal alignment, and we build on this success by incorporating Transformer-based methods into our framework for multi-modal condition handling. Furthermore, diffusion models have increasingly become leading-edge techniques in the field of high-quality audio and music generation [15, 16, 40, 46], outperforming next-token prediction in terms of audio fidelity [15, 46]. Therefore, we mainly build on Diffusion Transformer (DiT) to unify multi-modal conditions and generate high-fidelity audio, which are expected to integrate both advantages of transformers and diffusion models. To further enhance multi-modal representation learning and alignment, we explore multi-modal masking strategy to force the model to learn from masked input conditions, which evidently boost the performance for multi-modal audio generation. In summary, the main contributions of this work are as follows: We propose AudioX, unified DiT framework accompanied by two comprehensive caption datasets, to overcome the limitations of constrained inputs and outputs. The proposed framework supports audio and music generation from varied multi-modal conditions, contributing to new insight of studying generalist models for audio generation. We explore the impacts of different masking strategies in multi-modal audio generation. We find directly masking on the inputs instead of feature embeddings can enhance models capability of cross-modal presentation learning and boost the performance on various metrics. We categorize task types based on inputs and systematically benchmark various state-of-the-art methods on different tasks. Our extensive experiments demonstrate the effectiveness of our approach, achieving state-of-the-art results. 2. Related Work 2.1. Diffusion models Denoising diffusion models [27, 57] perturb data with Gaussian noise and reverse the process to recover the original data, establishing powerful framework for generative modeling. These models have achieved remarkable success across various domains, including image generation [3, 54, 55], video generation [6, 21, 23, 28], and audio generation [15, 30, 40, 42, 43, 51]. Despite these advancements, existing diffusion-based approaches have primarily focused on tasks such as text-to-audio or video-toaudio generation [40, 43], where the input modality is often constrained to single condition. While these works highlight the adaptability of diffusion models, they fall short in addressing more generalized any-to-audio or any-to-music generation scenarios, where the input can originate from multiple modalities, such as text, image, or video. To bridge 2 this gap, our approach leverages diffusion models for multicondition any-to-audio and music generation. Unlike prior works that often target specific modalities or tasks, our framework is designed to handle diverse input types, offering more flexible and universal solution. 2.2. Audio and music generation Recent advances in deep generative models have greatly broadened the scope of audio and music synthesis. However, most existing methods remain confined to single modality or support only limited types of conditioning. For instance, text-to-audio approaches [15, 16, 19, 34, 40, 46] focus on generating diverse soundscapes from textual prompts, while text-to-music systems [11, 19, 40, 42, 66, 72] specialize in composing coherent musical pieces. Separate lines of work tackle tasks like audio inpainting [40, 42], primarily with text conditioning. Meanwhile, video-toaudio methods [45, 62, 65, 70] typically generate foley or environmental sounds synchronized to visual cues. Some of these also incorporate text for additional context, thereby bridging visual and textual modalities. Beyond sound effects, video-to-music approaches [14, 31, 3537, 39, 44, 59] align musical compositions with the visual content to enhance narrative depth in multimedia applications. Despite these advances, current frameworks often specialize in only one modality or rely on limited set of input conditions, hindering multi-task adaptation and restricting the ability to scale or transfer knowledge across related tasks. In contrast, our unified approach supports both audio and music generation for broad range of input conditionsincluding text, image, video, and audioall within single framework. 2.3. Mask strategy in different modalities Masking strategies, first popularized in language modeling with BERT [13], have shown promise across diverse modalities. In vision, masked autoencoders [22] use patchbased masking to learn robust representations, while AudioMAE [29] focuses on masked spectrogram reconstruction in audio. Similarly, VideoMAE [61] extends this idea to video by masking spatiotemporal patches. Meanwhile, approaches like [4, 10, 17, 67] have explored masked strategies in video, image, audio, and music. Although some recent works in cross-modal audio generation [7, 49] adopt masking strategies, they typically use feature masking, which risks information leakage as bidirectional encoders may access global context. In contrast, input masking removes parts of the original signal entirely, increasing task difficulty and leading to more robust generative models. Building on these insights, our framework employs input masking across multiple modalities for improved alignment and generation performance. Figure 2. Overview of the automated caption generation pipeline. For each video-audio clip (top), Qwen2-Audio uses datasetprovided keywords to produce an audio caption. For each videomusic pair (bottom), it describes key attributes (e.g. genre, instruments, mood, tempo) to form music caption. 3. Dataset 3.1. Dataset sources To train and evaluate our unified model, we have constructed diverse set of datasets tailored for various tasks. An overview of the datasets we utilize is provided in Table A1 in the Appendix. Specifically, for audio generation, we collect datasets including AudioCaps [32], WavCaps [47], VGGSound [5], AudioSet Strong [25], Greatest Hits [48], and AVVP [58]. For music generation, we collect V2M [59], MusicCaps [1], and additionally utilize some of our proprietary text-music pair data. 3.2. Dataset process One limitation with the collected datasets is that they primarily consist of single-modality pairs with audio. For instance, datasets like VGGSound, AudioSet Strong, Greatest Hits, and V2M contain only video-audio or video-music pairs, while AudioCaps, Wavcaps, and MUCaps are limited to text-audio pairs. This limitation hinders the training and evaluation of our multi-condition model. To augment the existing datasets with additional modalities and enable the training of our unified model, we annotate the audio in video datasets to generate text descriptions. Specifically, we employ Qwen2-Audio [9] to generate captions for the audio and music in these video-pair datasets. For each 10-second video-audio clip from datasets [5, 25, 48, 58], we combine keywords from the original datasets, prompting the caption model to generate audio captions. For each video-music pair in V2M [59], Qwen2Audio is used to describe the genre, instruments, mood, and tempo of each 10-second music segment, which are essential attributes for music description. The prompt templates are provided in Fig. 2. Ultimately, we generate comprehensive audio and mu3 sic captions for around 260K and 5.7M 10-second videoaudio and video-music pairs, respectively. The details of the dataset are presented in Table A2 in the Appendix. We will open-source all the caption data. 4. Method 4.1. Overview As illustrated in Fig. 3, AudioX integrates specialized encoders for video, text, and audio with DiT model to generate high-quality audio or music. Given video Xv, text Xt, and audio Xa, the process starts by randomly masking each modalityspecifically, subset of image patches from video frames, textual tokens, and audio segments. This strategy aims to encourage robust cross-modal interactions and enhance representation learning. For images, we treat them as static video sequences by padding frames, ensuring consistent handling of visual data. Next, each modality is passed through its corresponding encoder and dedicated projection module to extract domain-specific features. The visual projection leverages temporal transformer followed by linear layer to capture temporal patterns, while both text and audio projections use linear transformations for dimensional alignment. This process produces three embeddings, Hv, Ht, and Ha, which are concatenated to form multi-modal condition embedding: Hc = Concat(Hv, Ht, Ha). (1) Along with diffusion timestep t, this condition embedding is fed into latent-based DiT model for audio and music synthesis. The diffusion process is detailed in Sec. 4.2. By jointly leveraging visual, textual, and audio cues, AudioX achieves flexible and high-fidelity anything-to-audio and music generation. 4.2. Training The objective of the training process is to effectively integrate multi-modal inputs and optimize the DiT model for generating high-quality audio or music through robust diffusion and denoising framework. The details of the training data are provided in Table A1 in the Appendix. During training, for each pair (Xv, Xt, Xa A), where is the ground truth we aim to generate, if the pair lacks video or audio modality input, we use zero-padding to fill the missing modality. If it lacks text modality input, we substitute with natural language descriptions, such as Generate music for the video. for the video-to-music generation task. For the tasks of audio inpainting and music completion, where the audio modality input is required, Xa equals for audio inpainting, where the model uses the masked audio input to inpaint the masked sections. For music completion, Xa is the preceding music segment of A, and the model aims to generate the subsequent music segment of Xa. Diffusion process. The DiT model processes the multimodal embedding Hc in the latent space through denoising diffusion process. Initially, the ground truth is encoded using an encoder E, which projects into the latent space, yielding the latent representation = E(A). The data then undergoes forward diffusion process, producing noisy latent states at each timestep t. The forward diffusion is defined as Markov process over timesteps, where the latent state at timestep is produced based on the latent state at 1: q(ztzt1) = (zt; (cid:112)1 βtzt1, βtI), (2) where βt represents the predefined variance at timestep t, and denotes Gaussian distribution. The forward diffusion process gradually adds noise to the latent state. The reverse denoising process involves training transformer network ϵθ to gradually remove noise at each timestep and reconstruct the clean data. The reverse process is modeled as follows: pθ (zt1zt) = (zt1; µθ (zt, t, Hc) , Σθ (zt, t, Hc)) , (3) where µθ and Σθ are the predicted mean and covariance of the reverse diffusion, conditioned on zt, t, and Hc. These parameters define the Gaussian distribution from which zt1 is sampled. The denoiser network ϵθ takes as input the noisy latent state zt, timestep t, and the multi-modal condition embedding Hc. The goal is to minimize the noise estimation error at each timestep, which is formulated as: Et,zt,ϵ ϵ ϵθ (zt, t, Hc)2 2 , min θ (4) where ϵ is the simulated noise at timestep t, and ϵθ(zt, t, Hc) is the predicted noise from the model. The training objective is to minimize the mean squared error between the simulated and predicted noise across all timesteps. By training the DiT model in this manner, we effectively unify multi-modal inputs into latent space, enabling the generation of high-quality audio or music that is coherent and aligned with the input conditions. 5. Experiments In this section, we provide the implementation details of our experiments and conduct extensive evaluations. These assessments comprehensively measure the effectiveness of our proposed method from both subjective and objective viewpoints. The evaluations aim to offer valuable insights into the generation of audio and music from various inputs. 5.1. Implementation details We train our model to generate 10-second audio or music outputs conditioned on multi-modal inputs. For encoding 4 Figure 3. The AudioX Framework. This figure depicts the AudioX framework, which employs specialized encoders and DiT-based approach with input masking to generate high-quality audio, unifying diverse input modalities for comprehensive audio and music creation. the visual features, we use CLIP-ViT-B/32 [52], extracting video frame features at rate of 5 frames per second. The text inputs are encoded using T5-base [53], while the audio is encoded and decoded using an audio Autoencoder [16]. The model has total of 2.4B parameters (1.1B trainable). The DiT model, consisting of 24 layers, uses pretrained model from [16]. The training process uses the AdamW optimizer with base learning rate of 1e-5, weight decay of 0.001, and learning rate scheduler incorporating exponential ramp-up and decay phases. To improve inference stability, we maintain an exponential moving average of the model weights. Mask ratios are set to 0.6 for video, 0.2 for text, and 0.6 for audio. Training is conducted on three clusters of NVIDIA H800 GPUs, each with 80GB of memory, requiring approximately 3.2k GPU hours in total. The batch size is set to 96. During inference, we perform 250 steps using classifier-free guidance with scale of 7.0. 5.2. Evaluation metrics To quantitatively evaluate our model, we use several metrics: Kullback-Leibler Divergence (KL) for acoustic similarity, Inception Score (IS) for evaluating both the quality and diversity of the generated audio, Frechet Distance (FD) using PANNs [33] and Frechet Audio Distance (FAD) [26] using VGGish [24] for assessing audio quality and similarity, Production Complexity (PC) and Production Quality (PQ) [60] for audio aesthetics assessment, and Alignment (Align.) for evaluating semantic alignment between input and generated audio. For Align., we use CLAP [64] score when the input is text and Imagebind AV score (IB) [20] when the input is video, both calculated using cosine similarity. For subjective evaluation, we hire 10 professional users to rate the quality of the generated audio and music. Following [34, 40], we use overall quality score (OVL) and relevance to the input (REL) between 1 and 100. 5.3. Main results Our model supports wide range of tasks, generating audio or music from any combination of video, text, and audio inputs. The main results of our method, compared with other state-of-the-art (SOTA) methods [8, 11, 14, 16, 19, 31, 34, 40, 42, 4446, 56, 59, 62, 65, 70, 71], are presented in Tables 1, 2, 3, and 4. It is evident from the tables that our model achieves SOTA performance across most metrics in all supported tasks. Audio generation. The results of our audio generation are shown in Table 1, which includes the outcomes of generating audio or music from any combination of video and text modalities. The upper part of the table presents the audio generation tasks, while the lower part displays the music generation tasks. For text-to-audio generation, we evaluate on the AudioCaps [32] and VGGSound [5] datasets. On AudioCaps, our model achieves SOTA performance, while on VGGSound, the advantage is even more pronounced. This demonstrates that our model is powerful text-to-audio generator. Furthermore, both our model and baseline results on VGGSound confirm the effectiveness of our curated caption data. For video-to-audio generation, we experiment on VGGSound and AVVP [58], AVVP is an out-of-domain test dataset for all methods. Our model achieves results comparable to SOTA on both VGGSound and AVVP, proving that it is not only strong video-to-audio generator but also exhibits excellent generalization on out-of-domain datasets. For audio generation conditioned on both text and video, we benchmark against the strong baselines FoleyCrafter [70] and MMAudio [8], achieving results that are comparable to"
        },
        {
            "title": "MusicCaps",
            "content": "V2M-bench KL Task Method 1.39 T2A AudioGen [34] 2.00 AudioLDM-L-Full [40] T2A 1.49 AudioLDM-2-Large [42] T2A 1.11 T2A Tango 2 [46] 2.01 T2A Stable Audio Open [16] 1.62 T2A MAGNET-large [71] 1.35 T2A MMAudio [8] 1.34 T2A AudioX 2.16 T2A AudioGen [34] 2.41 T2A AudioLDM-L-Full [40] 2.10 AudioLDM-2-Large [42] T2A 2.31 T2A Tango 2 [46] 2.36 T2A Stable Audio Open [16] 2.03 T2A MAGNET-large [71] 2.17 T2A MMAudio [8] 1.79 T2A AudioX 2.58 V2A Seeing&Hearing [65] 2.39 V2A FoleyCrafter [70] 3.01 V2A Diff-Foley [45] 2.58 V2A FRIEREN [62] 1.97 V2A MMAudio [8] 2.57 V2A AudioX 1.94 TV2A FoleyCrafter [70] 1.51 TV2A MMAudio [8] 1.56 TV2A AudioX 2.30 V2A Seeing&Hearing [65] 2.13 V2A FoleyCrafter [70] 3.14 V2A Diff-Foley [45] 2.73 V2A FRIEREN [62] 1.22 V2A MMAudio [8] 2.20 V2A AudioX 1.81 TV2A FoleyCrafter [70] 1.74 TV2A MMAudio [8] 2.14 TV2A AudioX 1.43 MusicGen [11] T2M 1.45 T2M AudioLDM-L-Full [40] 1.26 AudioLDM-2-Large [42] T2M 1.13 T2M TangoMusic [19] 1.51 T2M Stable Audio Open [16] 1.32 T2M MAGNET-large [71] 1.02 T2M AudioX 0.76 T2M MusicGen [11] 0.72 AudioLDM-L-Full [40] T2M 0.62 AudioLDM-2-Large [42] T2M 0.72 T2M TangoMusic [19] 0.72 T2M Stable Audio Open [16] 0.60 T2M MAGNET-large [71] 0.49 T2M AudioX 1.78 V2M Video2Music [31] 1.00 V2M MuMu-LLaMA [44] 1.22 V2M CMT [14] 0.73 V2M VidMuse [59] 0.69 V2M AudioX TV2M 0.47 AudioX IS 10.22 6.51 8.46 10.37 10.37 7.46 12.03 12.09 11.09 6.52 13.86 10.00 14.45 8.53 17.83 20.27 5.15 8.70 8.35 6.91 14.95 12.16 11.32 17.79 17.89 4.02 6.46 5.97 4.71 8.40 8.58 6.22 9.52 9.05 2.24 2.49 2.84 2.86 2.94 1.98 3.54 1.31 1.37 1.46 1.46 1.34 1.26 1.48 1.01 1.25 1.24 1.32 1.34 1.51 FD 13.29 37.27 26.34 12.22 29.01 24.88 12.63 11.83 15.94 31.15 16.32 22.96 26.00 22.17 11.52 9.84 27.21 17.68 56.54 50.88 6.18 8.83 19.16 6.60 7.58 40.38 28.68 76.96 66.46 13.51 18.75 26.76 14.18 18.04 25.40 34.44 15.61 15.00 36.33 23.88 10.63 40.59 36.63 25.80 38.19 42.02 34.24 20.07 144.88 52.25 85.70 29.95 23.96 18.92 FAD 1.72 8.37 1.97 3.20 3.15 2.99 4.71 1.86 2.48 7.05 2.05 3.47 2.60 2.74 2.50 1.31 5.23 2.23 5.89 3.13 2.04 1.13 2.13 2.20 1.10 8.66 3.77 10.95 6.49 3.25 2.47 2.85 2.74 2.36 4.55 6.34 2.80 1.88 3.23 4.24 1.53 3.25 2.97 1.63 2.43 2.72 3.15 1.69 18.72 5.10 8.64 2.46 2.12 1.51 PC 3.26 2.82 2.86 3.63 2.77 3.25 3.06 3.37 3.30 2.93 2.95 3.93 2.64 3.65 3.02 3.33 3.42 3.31 2.57 2.98 3.38 3.51 3.38 3.31 3.46 3.64 3.25 2.55 3.08 3.55 3.62 3.62 3.64 3.66 5.19 4.72 5.22 5.57 3.91 5.84 5.17 5.57 5.08 5.57 5.78 4.36 5.89 5.92 3.34 5.60 4.98 5.88 5.31 5. PQ Align. 5.25 5.67 5.77 5.82 6.16 5.15 5.64 5.73 5.45 5.99 6.35 5.99 6.53 5.25 6.12 6.24 5.33 5.99 5.85 6.06 5.91 6.11 6.06 5.99 6.21 5.16 5.87 5.71 5.88 5.89 6.00 5.60 5.81 6.03 7.16 6.10 6.70 7.06 7.18 6.71 6.70 7.43 7.01 6.90 7.46 7.72 7.04 6.93 8.14 7.97 8.20 6.89 6.91 7.04 0.27 0.20 0.22 0.36 0.21 0.15 0.30 0.28 0.29 0.27 0.30 0.29 0.33 0.26 0.32 0.33 0.36 0.27 0.20 0.20 0.35 0.26 0.26 0.33 0.26 0.35 0.28 0.16 0.17 0.34 0.28 0.27 0.34 0.28 0.18 0.22 0.23 0.23 0.23 0.19 0.23 0.14 0.16 0.14 0.14 0.17 0.17 0.13 0.14 0.18 0.12 0.20 0.23 0.22 Table 1. Performance Evaluation Across Various Datasets and Methods. This table presents the evaluation metrics for different methods applied to various datasets. The tasks are abbreviated as follows: T2A (Text-to-Audio), V2A (Video-to-Audio), TV2A (Text-and-Videoto-Audio), T2M (Text-to-Music), V2M (Video-to-Music), and TV2M (Text-and-Video-to-Music). For the alignment metric (Align.), we use the CLAP score when the input is text and the Imagebind AV score (IB) when the input is video. them. We find that when both text and video inputs are provided, the model effectively integrates the information from both modalities to generate better results. The down part of Table 1 shows the results of music generation tasks. We evaluate text-to-music generation on MusicCaps [11], video-to-music and video-and-text-to-music generation on V2M [59]. Our model achieves SOTA performance across these tasks, demonstrating its effectiveness in generating high-quality music conditioned on diverse inputs. Audio inpainting. As shown in Table 2, we conducted experiments on audio inpainting tasks, where our model outperformed the baselines [40, 42] on the AudioCaps [32] and AVVP [58] test datasets. Additionally, to explore audio inpainting with various input modalities, we performed experiments on unconditioned audio inpainting, as well as video-guided and text-and-video-guided audio inpainting tasks (on AVVP). The results indicate that both text and video can effectively guide the audio inpainting task, with text providing better guidance than video. When both text and video are conditioned, the model can integrate the two modalities to achieve superior results."
        },
        {
            "title": "Condition",
            "content": "Unprocessed AudioLDM-L-Full [42] AudioLDM-2-Full-Large [42] AudioX AudioX AudioX AudioX - A+T A+T A+T A+V A+T+V"
        },
        {
            "title": "AVVP",
            "content": "6.51/11.34 8.06/2.64 4.24/10.17 4.63/5.35 9.84/2.25 N/A N/A 4.94/6.70 5.11/3.30 3.99/11.58 3.94/5.44 6.12/2.05 5.63/2.16 6.25/1.99 Table 2. Inpainting Performance Comparison. This table shows the performance comparison for audio inpainting on the AudioCaps and AVVP datasets. The values before and after the slash represent the IS and FAD metrics, respectively. A, V, and represent Audio, Video, and Text conditions. The baseline methods are all under audio and text conditions. Music Completion. Music completion is task where the model generates music based on given music clip. We evaluate our model on the V2M-bench [59] dataset. The results are shown in Table 3. We find that our model can generate music that extends the input music clip. As the number of input modalities increases, the models performance improves, demonstrating its strong inter-modal learning capability and ability to leverage multi-modal information to generate better music. Image-to-audio generation. To evaluate our models performance on zero-shot image-to-audio generation task, we conducted experiments using the same settings as in [65]. We compared our model with Seeing&Hearing [65] and Im2Wav [56], and also constructed baseline by combining an image caption model [2] with text-to-audio model [46]. The results are shown in Table 4 in the Appendix. We find that our model demonstrates excellent perCondition KL IS FD FAD T+M V+M 0.96 0.51 0.70 T+V+M 0.46 1.21 1.49 1.37 1.52 52.77 21.42 24.28 18. 5.76 2.14 2.29 1.67 Table 3. Performance for our method under different conditions in the music completion task. M, T, and represent Music, Text, and Video, respectively. formance in image-to-audio generation task even without any specific training with image data."
        },
        {
            "title": "Method",
            "content": "KL IS FD FAD Align. Caption2Audio Im2Wav [56] Seeing&Hearing [65] AudioX 2.76 2.61 2.69 2. 7.48 7.06 6.15 13.48 32.97 19.63 20.96 16.42 5.54 7.58 6.87 2.71 0.21 0.41 0.29 0.23 Table 4. Comparison of Methods for the Image2Audio Task. User study. We conducted user study to evaluate the quality of the generated audio and music. We randomly selected 25 samples for each audio generation task, including text-to-audio (T2A), text-to-music (T2M), video-to-audio (V2A), and video-to-music (V2M). 10 users are asked to rate the quality of the generated audio and music. The results are shown in Fig. 4. The evaluation shows that our model achieves subjective SOTA performance in terms of OVL and REL scores in most tasks, indicating high user satisfaction. Figure 4. User study results of generated audio and music. The values represent the average OVL and REL scores across Text-toAudio (on AudioCaps), Text-to-Music (on MusicCaps), Video-toAudio (on VGGSound), Video-to-Music (on V2M-bench). 5.4. Ablation study In this section, we investigate the effects of several design choices in our model, focusing on the mask ratio and mask strategy, as well as the impact of different modality conditioning. 7 Mask ratio. To identify the optimal mask ratio for different modalities, we conduct mask ablation study for each input modality, with results shown in Fig. 5. For each modality, we vary the mask ratio from 0 to 0.8. The text modality performs best with mask ratio of 0.2, while both the video and audio modalities reach optimal performance at mask ratio of 0.6. From the figure, we observe that masking the video modality results in the most significant performance improvement. We attribute this to the high redundancy in video data, which contains abundant frames and information. Masking video data enhances the models ability to extract key features from the input, allowing it to effectively utilize the remaining information even at higher mask ratios. The results can demonstrates the effectiveness of our masking in improving model performance. respectively. audio-conditioned models on text-to-audio, video-to-audio, and audio inpainting tasks, The results demonstrate that our unified model outperforms the singlemodality models, highlighting its strong intra-modal capabilities and its ability to perform well across various singlemodality tasks. For inter-modal performance, we evaluate the results of different modality combinations using our unified model, as illustrated in the right figure. In music generation tasks, adding each additional modality input consistently improves performance over single-modality inputs, with the best results achieved when all three modalities are combined. This experiment confirms our models robust inter-modal learning ability, effectively integrating information from different modalities to enhance performance. Overall, our DiT-based model with input masking successfully unifies different input modalities, enhancing performance both intra-modally and inter-modally to generate high-quality audio and music. Figure 5. Ablation study of mask ratios for each modality, with mask ratios varying from 0.2, 0.4, 0.6 to 0.8. The values represent the average Inception Score (IS) across Text-to-Audio, Textto-Music, Video-to-Audio, Video-to-Music, and Audio Inpainting tasks. Mask strategy. To evaluate the effectiveness of our strategy of masking inputs for each modality, we compare different masking strategies, including no masking and feature masking, with results presented in Table A3 in the Appendix. From the results, it is evident that masking the input modality improves model performance, while masking features leads to decline in performance. This aligns with our understanding that input masking prevents information leakage by removing parts of the original input signals, thereby increasing task difficulty and enhancing the models ability to learn robust generative strategies. In contrast, feature masking can inadvertently reveal global context, reducing the effectiveness of the learning process. These findings validate the importance of our masking strategy. Unified model performance. To investigate the performance of our unified model in both intra-modal and intermodal contexts, we conduct an ablation study, with results shown in Fig. 6. For intra-modal performance, we compare our unified model with models trained on individual modality conditions across tasks supported by single-modality models. As shown in the left figure, our unified model is compared with text-conditioned, video-conditioned, and Figure 6. Ablation study comparing intra-modal and inter-modal performance of the unified model. The left compares singlemodality models on text-to-audio, video-to-audio, and audio inpainting tasks. The right shows the effect of adding modalities on music generation, with performance improvements noted for each added modality. Results are based on the Inception Score (IS) metric. 6. Conclusion In this work, we introduce AudioX, unified framework that addresses the challenges of multi-modal integration in audio and music generation, overcoming the input-modality and output-domain constraints prevalent in existing approaches. By adopting DiT-based approach and incorporating an input masking strategy, our model effectively unifies text, video, and audio inputs to produce high-quality audio outputs. We also curate and utilize comprehensive multi-modal datasets, providing robust foundation for training and evaluation. Extensive experimental results show that AudioX not only excels in intra-modal tasks but also significantly improves inter-modal performance, highlighting its potential to advance the field of multi-modal audio generation."
        },
        {
            "title": "References",
            "content": "[1] Andrea Agostinelli, Timo Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. arXiv preprint Musiclm: Generating music from text. arXiv:2301.11325, 2023. 3 [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. 7 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [4] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1131511325, 2022. 3 [5] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zisserman. Vggsound: large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 721725. IEEE, 2020. 2, 3, 5 [6] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. 2 [7] Ziyang Chen, Prem Seetharaman, Bryan Russell, Oriol Nieto, David Bourgin, Andrew Owens, and Justin Salamon. Video-guided foley sound generation with multimodal controls. arXiv preprint arXiv:2411.17698, 2024. 2, [8] Ho Kei Cheng, Masato Ishii, Akio Hayakawa, Takashi Shibuya, Alexander Schwing, and Yuki Mitsufuji. Taming multimodal joint training for high-quality video-to-audio synthesis. arXiv preprint arXiv:2412.15322, 2024. 5, 6 [9] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei, Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He, Junyang Lin, et al. Qwen2-audio technical report. arXiv preprint arXiv:2407.10759, 2024. 3 [10] Marco Comunit`a, Zhi Zhong, Akira Takahashi, Shiqi Yang, Mengjie Zhao, Koichi Saito, Yukara Ikemiya, Takashi Shibuya, Shusuke Takahashi, and Yuki Mitsufuji. Specmaskgit: Masked generative modeling of audio spectrograms arXiv preprint for efficient audio synthesis and beyond. arXiv:2406.17672, 2024. 3 [11] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. Simple and controllable music generation. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 5, 6, 7 [12] Qixin Deng, Qikai Yang, Ruibin Yuan, Yipeng Huang, Yi Wang, Xubo Liu, Zeyue Tian, Jiahao Pan, Ge Zhang, Hanfeng Lin, et al. Composerx: Multi-agent symbolic music composition with llms. arXiv preprint arXiv:2404.18081, 2024. 2 [13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. 3 [14] Shangzhe Di, Zeren Jiang, Si Liu, Zhaokai Wang, Leyan Zhu, Zexin He, Hongming Liu, and Shuicheng Yan. Video background music generation with controllable music transformer. In Proceedings of the 29th ACM International Conference on Multimedia, pages 20372045, 2021. 3, 5, 6 [15] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Long-form music generation with latent diffusion. arXiv preprint arXiv:2404.10301, 2024. 2, 3 [16] Zach Evans, Julian Parker, CJ Carr, Zack Zukowski, Josiah Taylor, and Jordi Pons. Stable audio open. arXiv preprint arXiv:2407.14358, 2024. 2, 3, 5, [17] Hugo Flores Garcia, Prem Seetharaman, Rithesh Kumar, and Bryan Pardo. Vampnet: Music generation via masked acoustic token modeling. arXiv preprint arXiv:2307.04686, 2023. 3 [18] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and humanIn 2017 IEEE internalabeled dataset for audio events. tional conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. 2 [19] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Text-to-audio generation using arXiv and Soujanya Poria. instruction-tuned llm and latent diffusion model. preprint arXiv:2304.13731, 2023. 3, 5, 6 [20] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 5 [21] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 3 [23] Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, et al. Llms meet multimodal generation and editing: survey. arXiv preprint arXiv:2405.19334, 2024. 2 [24] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort Gemmeke, Aren Jansen, Channing Moore, Manoj Plakal, Devin Platt, Rif Saurous, Bryan Seybold, et al. Cnn architectures for large-scale audio classification. In 2017 ieee international conference on acoustics, speech and signal processing (icassp), pages 131135. IEEE, 2017. 5 9 [25] Shawn Hershey, Daniel PW Ellis, Eduardo Fonseca, Aren Jansen, Caroline Liu, Channing Moore, and Manoj Plakal. The benefit of temporally-strong labels in audio event classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 366370. IEEE, 2021. 3 [26] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [28] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 2 [29] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. Advances in Neural Information Processing Systems, 35: 2870828720, 2022. 3 [30] Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and Nam Soo Kim. Diff-tts: denoisarXiv preprint ing diffusion model for text-to-speech. arXiv:2104.01409, 2021. 2 [31] Jaeyong Kang, Soujanya Poria, and Dorien Herremans. Video2music: Suitable music generation from videos using an affective multimodal transformer model. Expert Systems with Applications, 249:123640, 2024. 3, 5, 6 [32] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 119132, 2019. 2, 3, 5, 7 [33] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark Plumbley. Panns: Large-scale pretrained audio neural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:28802894, 2020. [34] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre Defossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022. 2, 3, 5, 6 [35] Ruiqi Li, Siqi Zheng, Xize Cheng, Ziang Zhang, Shengpeng Ji, and Zhou Zhao. Muvi: Video-to-music generation with semantic alignment and rhythmic synchronization. arXiv preprint arXiv:2410.12957, 2024. 3 [36] Sizhe Li, Yiming Qin, Minghang Zheng, Xin Jin, and Yang Liu. Diff-bgm: diffusion model for video background music generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 27348 27357, 2024. [37] Sifei Li, Binxin Yang, Chunji Yin, Chong Sun, Yuxin Zhang, Weiming Dong, and Chen Li. Vidmusician: Video-to-music generation with semantic-rhythmic alignment via hierarchical visual features. arXiv preprint arXiv:2412.06296, 2024. 3 [38] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. arXiv preprint arXiv:2311.10122, 2023. [39] Yan-Bo Lin, Yu Tian, Linjie Yang, Gedas Bertasius, and Heng Wang. Vmas: Video-to-music generation via searXiv preprint mantic alignment in web music videos. arXiv:2409.07450, 2024. 3 [40] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark Plumbley. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503, 2023. 2, 3, 5, 6, 7 [41] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [42] Haohe Liu, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Qiao Tian, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark Plumbley. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 2, 3, 5, 6, 7 [43] Jinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and Zhou Zhao. Diffsinger: Singing voice synthesis via shallow diffusion mechanism. In Proceedings of the AAAI conference on artificial intelligence, pages 1102011028, 2022. 2 [44] Shansong Liu, Atin Sakkeer Hussain, Qilong Wu, Chenshuo Sun, and Ying Shan. Mumu-llama: Multi-modal music understanding and generation via large language models. arXiv preprint arXiv:2412.06660, 2024. 3, 5, [45] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao. Diff-foley: Synchronized video-to-audio synthesis with latent diffusion models. Advances in Neural Information Processing Systems, 36, 2024. 2, 3, 6 [46] Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, and Soujanya Poria. Tango 2: Aligning diffusion-based text-to-audio generations through direct preference optimization. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 564572, 2024. 2, 3, 5, 6, 7 [47] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark Plumbley, Yuexian Zou, and Wenwu Wang. Wavcaps: chatgpt-assisted weaklylabelled audio captioning dataset for audio-language multimodal research. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2024. 2, 3 [48] Andrew Owens, Phillip Isola, Josh McDermott, Antonio Torralba, Edward Adelson, and William Freeman. Visually indicated sounds. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24052413, 2016. 3 [49] Trung Pham, Tri Ton, and Chang Yoo. Mdsgen: Fast and efficient masked diffusion temporal-aware transformers for open-domain sound generation. arXiv preprint arXiv:2410.02130, 2024. 3 [50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [51] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: diffusion probabilistic model for text-to-speech. In International Conference on Machine Learning, pages 85998608. PMLR, 2021. 2 [52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 5 [53] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. 5 [54] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. hear your true colors: Im- [56] Roy Sheffer and Yossi Adi. In ICASSP 2023-2023 IEEE age guided audio generation. International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 5, 7 [57] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 2 [58] Yapeng Tian, Dingzeyu Li, and Chenliang Xu. Unified multisensory perception: Weakly-supervised audio-visual video In Computer VisionECCV 2020: 16th European parsing. Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 436454. Springer, 2020. 3, 5, 7 [59] Zeyue Tian, Zhaoyang Liu, Ruibin Yuan, Jiahao Pan, Qifeng Liu, Xu Tan, Qifeng Chen, Wei Xue, and Yike Guo. Vidmuse: simple video-to-music generation framework with long-short-term modeling. arXiv preprint arXiv:2406.04321, 2024. 2, 3, 5, 6, 7 [60] Andros Tjandra, Yi-Chiao Wu, Baishan Guo, John Hoffman, Brian Ellis, Apoorv Vyas, Bowen Shi, Sanyuan Chen, Matt Le, Nick Zacharov, et al. Meta audiobox aesthetics: Unified automatic quality assessment for speech, music, and sound. arXiv preprint arXiv:2502.05139, 2025. 5 [61] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 3 [62] Yongqi Wang, Wenxiang Guo, Rongjie Huang, Jiawei Huang, Zehan Wang, Fuming You, Ruiqi Li, and Zhou Zhao. Frieren: Efficient video-to-audio generation with rectified flow matching. arXiv preprint arXiv:2406.00320, 2024. 3, 5, 6 [63] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multimodal llm. arXiv preprint arXiv:2309.05519, 2023. 2 [64] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. Large-scale contrastive language-audio pretraining with feature fusion and In ICASSP 2023-2023 keyword-to-caption augmentation. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2023. 5 [65] Yazhou Xing, Yingqing He, Zeyue Tian, Xintao Wang, and Qifeng Chen. Seeing and hearing: Open-domain visualaudio generation with diffusion latent aligners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 71517161, 2024. 2, 3, 5, 6, 7 [66] Yinghao, Ø Anders, Anton, Bleiz MacSen Del, Charalampos, and Chris. Foundation models for music: survey. arXiv preprint arXiv:2408.14340, 2024. 3 [67] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingIrfan Essa, et al. Magvit: Hsuan Yang, Yuan Hao, In Proceedings of Masked generative video transformer. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1045910469, 2023. 3 [68] Ruibin Yuan, Hanfeng Lin, Yi Wang, Zeyue Tian, Shangda Wu, Tianhao Shen, Ge Zhang, Yuhang Wu, Cong Liu, Chatmusician: Understanding and Ziya Zhou, et al. arXiv preprint generating music intrinsically with llm. arXiv:2402.16153, 2024. [69] Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, et al. Anygpt: Unified multimodal llm with discrete sequence modeling. arXiv preprint arXiv:2402.12226, 2024. 2 [70] Yiming Zhang, Yicheng Gu, Yanhong Zeng, Zhening Xing, Yuancheng Wang, Zhizheng Wu, and Kai Chen. Foleycrafter: Bring silent videos to life with lifelike and synchronized sounds. arXiv preprint arXiv:2407.01494, 2024. 2, 3, 5, 6 [71] Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using single nonIn The Twelfth International autoregressive transformer. Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. 5, 6 [72] Alon Ziv, Itai Gat, Gael Le Lan, Tal Remez, Felix Kreuk, Alexandre Defossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. Masked audio generation using arXiv preprint single non-autoregressive transformer. arXiv:2401.04577, 2024. 3 7. Appendix This appendix provides additional details regarding our datasets, ablation studies, and further qualitative evaluations. Section 7.1 describes the datasets and our newly curated captions. Section 7.2 presents the mask strategy ablation. Finally, Section 7.3 showcases comprehensive qualitative results, including additional figures and comparisons. 7.1. Datasets Table A1 provides an overview of all datasets used in this work. Table A2 outlines the new captions we annotated for training and testing our unified model. We will open-source these caption datasets to facilitate further research. # Clips Dur./Clip (s) Dur. (h) Split Task T2A V2A Train TV2A T2M V2M TV2M Dataset AudioCaps WavCaps VGGSound VGGSound AudioSet Strong Greatest Hits VGGSound AudioSet Strong Greatest Hits Private V2M MUCaps V2M V2M 45.0k 108.3k 176.9k 176.9k 67.3k 1.0k 176.9k 67.3k 1.0k 175.2k 5685.7k 22.0k 5685.7k 5685.7k Audio Inpainting All audio data 398.5k 10 10 10 10 10 10 10 10 240 10 208 10 10 10 125.1 300.8 491.4 491.4 187.14 2.71 491.4 187.14 2. 11679.3 15793.58 1273.6 15793.58 15793.58 1107.15 Music Completion All music data 5882.9k 17. 28746.48 Test T2A V2A TV2A T2M V2M TV2M Audio Inpainting AudioCaps VGGSound VGGSound AVVP VGGSound MusicCaps V2M V2M V2M AudioCaps AVVP Music Completion V2M 4,875 14,931 14,931 1,120 14, 5,526 3105 300 300 4,875 1,120 300 10 10 10 10 10 10 108 108 10 10 108 13.54 41. 41.475 3.11 41.475 15.35 9.01 9.01 9.01 13.54 3.11 9. Table A1. Comprehensive overview of training and test datasets, detailing the number of clips (# Clips), average duration per clip (Dur./Clip in seconds), and total duration (Dur. in hours) for each task and split. T2A: Text-to-Audio, V2A: Video-to-Audio, TV2A: Text-and-Video-to-Audio, T2M: Text-to-Music, V2M: Video-toMusic, TV2M: Text-and-Video-to-Music. 7.2. Mask Strategy Ablation Table A3 reports the results of our ablation study on different mask strategies. These findings confirm that our input masking approach yields superior performance compared to feature masking and no masking strategy. 7.3. Qualitative Results Figures A1 and A2 present comprehensive qualitative results."
        },
        {
            "title": "Data Type",
            "content": "# Clips Dur./Clip (s) Dur. (h) VGGSound AudioSet Strong AVVP Test Split Greatest Hits V2M"
        },
        {
            "title": "Audio\nAudio\nAudio\nAudio\nMusic",
            "content": "191.8K 67.3K 1.1K 1.0K 5.7M 10 10 10 10 10 532.81 187.14 3.11 2.71 15793.58 Table A2. Overview of our labeled captions, detailing the number of clips, average duration per clip, and total duration for each source dataset. Mask Strategy KL IS FD FAD"
        },
        {
            "title": "No Mask\nMask Feature\nMask Input",
            "content": "1.90 2.08 1.87 5.26 4.64 5.44 22.70 32.93 21.78 2.98 3.68 2.81 Table A3. Ablation study results comparing different mask strategies. (a) T2A and T2M Results. (b) Inpainting Results. (c) V2A Results. Figure A1. Qualitative comparison across various tasks: (a) In Text-to-Audio (T2A) and Text-to-Music (T2M) tasks, our model uniquely excels by consistently generating the ticking sound of clock and accurately following the prompt Music performed on steelpan instrument, outperforming baselines in both rhythmic precision and genre fidelity. (b) Audio inpainting results demonstrate our models strong context-aware capabilities and its ability to effectively integrate different input modalities. (c) Video-to-Audio (V2A) results show our models proficiency in capturing dynamic motion sounds, such as the immersive drifting of car, providing richer auditory experience compared to baselines. 13 (a) Text-to-Audio and Text-to-Music (b) Video-to-Audio and Video-to-Music (c) Audio Inpainting and Music Completion Figure A2. Comprehensive qualitative analysis of our models performance across various tasks: (a) Text-to-Audio and Text-to-Music synthesis, (b) Video-to-Audio and Video-to-Music generation, and (c) Audio Inpainting and Music Completion."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Moonshot AI"
    ]
}