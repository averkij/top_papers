{
    "paper_title": "DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction",
    "authors": [
        "Yiheng Liu",
        "Liao Qu",
        "Huichao Zhang",
        "Xu Wang",
        "Yi Jiang",
        "Yiming Gao",
        "Hu Ye",
        "Xian Li",
        "Shuai Wang",
        "Daniel K. Du",
        "Shu Cheng",
        "Zehuan Yuan",
        "Xinglong Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "This paper presents DetailFlow, a coarse-to-fine 1D autoregressive (AR) image generation method that models images through a novel next-detail prediction strategy. By learning a resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing a more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose a parallel inference mechanism with self-correction that accelerates generation speed by approximately 8x while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256x256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2x faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlow's superior generation quality and efficiency compared to existing state-of-the-art methods."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 3 7 4 1 2 . 5 0 5 2 : r DetailFlow: 1D Coarse-to-Fine Autoregressive Image Generation via Next-Detail Prediction Yiheng Liu, Liao Qu, Huichao Zhang, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Xian Li, Shuai Wang, Daniel K. Du, Shu Cheng, Zehuan Yuan, Xinglong Wu ByteDance Inc. https://github.com/ByteFlow-AI/DetailFlow Figure 1: (a) Progressive generation results from DetailFlow. Our proposed 1D tokenizer encodes tokens with an inherent semantic ordering, where each subsequent token contributes additional high-resolution information. The sequences illustrate how image resolution and inferred 1D tokens incrementally increase from left to right. (b) Comparison of our DetailFlow approach with existing methods, showing that DetailFlow achieves better image quality with fewer tokens and times."
        },
        {
            "title": "Abstract",
            "content": "This paper presents DetailFlow, coarse-to-fine 1D autoregressive (AR) image generation method that models images through novel next-detail prediction strategy. By learning resolution-aware token sequence supervised with progressively degraded images, DetailFlow enables the generation process to start from the global structure and incrementally refine details. This coarse-to-fine 1D token sequence aligns well with the autoregressive inference mechanism, providing more natural and efficient way for the AR model to generate complex visual content. Our compact 1D AR model achieves high-quality image synthesis with significantly fewer tokens than previous approaches, i.e. VAR/VQGAN. We further propose parallel inference mechanism with self-correction that accelerates generation speed by approximately 8 while reducing accumulation sampling error inherent in teacher-forcing supervision. On the ImageNet 256256 benchmark, our method achieves 2.96 gFID with 128 tokens, outperforming VAR (3.3 FID) and FlexVAR (3.05 FID), which both require 680 tokens in their AR models. Moreover, due to the significantly reduced token count and parallel inference mechanism, our method runs nearly 2 faster inference speed compared to VAR and FlexVAR. Extensive experimental results demonstrate DetailFlows superior generation quality and efficiency compared to existing state-of-the-art methods. 1 Equal contribution. 2 Project leader. Preprint. Under review."
        },
        {
            "title": "Introduction",
            "content": "Autoregressive (AR) models like [30, 31, 4, 9, 1, 17, 39, 22] have demonstrated exceptional success in natural language processing through their scalability, flexibility, and ability to model complex sequential dependencies. Building on these strengths, researchers have extended AR modeling to image generation, creating unified frameworks [37, 42, 45, 29] for visual generation tasks. AR image generation enables structured, step-by-step synthesis, offering advantages in controllability and multimodal integration. Conventional AR image generation methods adopts raster scan approaches [13, 47, 42], which flattens 2D image tokens into 1D sequences, forcing models to predict patches in counter-intuitive order that disrupts spatial continuity. Recent work Visual Autoregressive Modeling (VAR) [38] adopts next-scale prediction framework that emulates human sketching via coarse-to-fine 2D parallel generation. However, it requires extensive volumes of multi-scale tokens, particularly at high resolutions. For instance, Infinity [15] requires 10,521 tokens to synthesize 10241024 image, leading to substantial computational and memory overhead during training. This bottleneck highlights critical challenge in balancing generation quality with efficiency for high-resolution AR image synthesis. Figure 2: Comparison of different prediction strategies in image generation. (a) Traditional 2D raster-scan nexttoken/next-patch prediction. (b) Next-scale prediction in VAR [38]. (c) Our proposed next-detail prediction, which predicts 1D tokens encoding fine-grained details for high-resolution image generation. Recent line of research [48, 2] attempt to address this challenge through querybased attention mechanisms that compress 2D images into adaptive 1D token sequences. These approaches remove constraints associated with fixed spatial positions, enabling adaptive compression of spatial redundancy. This significantly reduces token counts, alleviating computational overhead during generation. However, current 1D tokenization methods remain inherently constrained by resolution-specific tokenizers, limiting flexibility in generating images at arbitrary resolutions without retraining. To address these limitations, we propose DetailFlow, novel coarse-to-fine 1D tokenizer explicitly establishing semantic and resolution-dependent mapping between token sequences and image resolutions. Specifically, DetailFlow employs progressively degraded images to supervise token sequences of increasing lengths, inherently embedding coarse-to-fine semantic ordering within the tokens. Consequently, during inference, DetailFlow autoregressively generates tokens in coarseto-fine manner, progressively outputing higher-resolution images with enriched visual details, as illustrated in Fig. 1(a). This approach allows 256256 images to be represented with just 128 semantically ordered tokens, significantly fewer than VAR-based methods. With similar AR model sizes, DetailFlow achieves gFID score below 3, demonstrating superior quality and efficiency. Furthemore, traditional VQGAN-based raster-scan methods inherently disable parallel prediction of successive tokens due to spatial dependencies. DetailFlows 1D learnable latent space offers greater flexibility. This enables substantial acceleration of inference speed through parallel successive token prediction (speedup proportional to the parallel token numbers). During experimentation, we observe that sampling errors significantly degrade subsequent generation quality due to autoregressive teacher forcing training regimes. To mitigate this accumulation errors, we introduce self-correction training strategy during tokenizer learning. Specifically, controlled quantization errors are introduced during token quantization, and subsequent tokens are trained to correct these inaccuracies. This approach fosters token sequences capable of self-correction, significantly enhancing overall generation quality and providing an effective solution to mitigate error accumulation during autoregressive inference. 2 To sum up, our contributions include: Next-detail prediction paradigm. As shown in 2, we introduce novel coarse-to-fine 1D autoregressive image generation framework that progressively refines images from global structures to fine details. This 1D coarse-to-fine token sequence is more aligned with the inference paradigm of autoregressive models. Improved token efficiency. DetailFlow significantly reduces token requirements, achieving 2.96 gFID using 326M parameter AR model with only 128 tokens and nearly 2 faster inference speed on the ImageNet 256256 benchmark, compared to 680 tokens required by recent state-of-the-art methods such as VAR [38] and FlexVAR [19]. Accelerated parallel inference. Our parallel decoding mechanism combined with selfcorrection training strategy boosts inference speed by 8, simultaneously mitigating the error accumulation typically observed in autoregressive models. Dynamic-resolution 1D tokenization. DetailFlow uniquely supports dynamic resolution in 1D tokenizers, employing single 1D tokenizer capable of generating variable-length token sequences, thereby enabling flexible image decoding at multiple resolutions without additional retraining."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Image tokenizer The image tokenizer is crucial for generative tasks, as it affects both the quality of generation and the model architecture. An image tokenizer typically encodes high-dimensional visual data into compressed latent space, representing the image as sequence of discrete tokens. This process is then reversed by decoder to reconstruct the original image. Numerous studies have explored and validated this general framework, demonstrating its effectiveness in various model designs. 2D image tokenizer. Early work such as VQ-VAE [41, 32] introduces discrete latent representation using vector quantization, enabling token-based image modeling by structuring images as grids of 2D tokens in the latent space. VQGAN [13] improves this approach by incorporating adversarial training to improve perceptual quality. Efficient-VQGAN [5] focuses on reducing computational overhead while maintaining reconstruction quality. RQ-VAE [23] adopts residual quantization to enrich the representation capacity. MoVQ [49] employs multi-codebook design for more flexible token utilization. 1D image tokenizer. Unlike 2D tokenizers, which retain spatial structure information in the generated token sequence, the 1D tokenizer TiTok[48] integrates 2D image information into 1D token sequence through self-attention mechanism. This process removes spatial redundancy, thereby enhancing information compression capabilities. However, these tokens lack an inherent order, which poses challenges for their application in next-token prediction within autoregressive models. FlexTok [2] addresses this problem by using tail-drop tokenizer training strategy, which forces information to focus on earlier tokens, thus generating coarse-to-fine ordered token sequence. However, under 1.33B AR model, the generation performance, as measured by gFID, deteriorates from approximately 1.9 at 32 tokens to 2.5 at 256 tokens. This limitation, where high-quality generation is only achievable with small number of tokens, restricts its ability to scale to higher-resolution images that require more tokens for reconstruction. 2.2 Visual generation Autoregressive models. The definition of an appropriate token sequence is crucial for autoregressive models in image generation tasks. Inspired by NLP, the next-token prediction paradigm for sequentially generating discrete image tokens has been naturally extended to decoder-only Transformer architectures. VQGAN [13], Parti [47] and EMU3 [42] adopt raster-scan strategy to arrange 2D image tokens into 1D ordered token sequence by scanning row by row. While this method facilitates token-level prediction in manner analogous to text generation, alternative approaches have explored more structured generation schemes. Notably, VAR [38] introduces next-scale prediction framework, which departs from the conventional sequential token prediction by generating coarser-to-finer 3 Figure 3: (a) Coarse-to-fine tokenizer training. The encoder maps high-resolution images to 1D latent token sequences. Decoding with more tokens yields higher-resolution outputs, with earlier tokens capturing global structure and later ones refining details. (b) Self-correction training. Randomly perturbed tokens are re-encoded, and encourages subsequent tokens to correct errors from earlier noisy tokens. (c) Autoregressive (AR) model training and decoding. AR model predicts the first group of tokens in next-token prediction manner, followed by parallel prediction of subsequent groups. At inference, more predicted tokens lead to higher-resolution outputs. representations across multiple scales. This hierarchical strategy enables the model to capture global image structure before refining local details. Masked-prediction model. MaskGIT [7] is transformer-based image synthesis framework that leverages masked visual token modeling to generate images through parallel decoding. Unlike autoregressive models, it predicts all tokens simultaneously and iteratively refines them. TiTok [48] uses it for image generation with 1D token sequence. MUSE [6] extends MaskGIT to text-to-image generation by integrating pretrained language model with MaskGITs parallel decoding. Diffusion models synthesize data through forward-backward stochastic process: gradually corrupting data with Gaussian noise and learning to reverse this degradation via iterative denoising. Latent diffusion models [33] first proposes modeling in the latent space using U-Net architecture. DiT [28] replaces conventional U-Net with transformer architecture for latent image processing, demonstrating superior scalability and achieving better image generation quality."
        },
        {
            "title": "3 Method",
            "content": "Encoding images into 2D grid of tokens is natural and widely adopted strategy, leveraging the inherent spatial structure of images. However, using 1D tokenizer allows for more efficient compression, representing images with fewer tokens and offering greater flexibility in controlling the information content of each token. This flexibility is critical for balancing image quality and computational efficiency. 3.1 Preliminary Background on 1D Tokenizer The general structure of tokenizers typically consist of an encoder, quantizer, and decoder. The encoder embeds an input image into continuous image tokens, the quantizer discretizes them into discrete tokens, and the decoder reverses this process to reconstruct the original image. Similarly, the 1D tokenizer adheres to this paradigm but introduces slight modifications to the encoder and decoder. 4 Given an input image RHW 3, it is first patchified to non-overlapping patches D, where is the patch size and is the patch feature dimension. These patches are concatenated with set of learnable 1D query tokens RN and fed into the encoder. is the number of query tokens. The encoder outputs continuous latent tokens RN D, which are then discretized by quantizer into discrete tokens RN using codebook of dimension d. For reconstruction, the discrete tokens are first projected and then concatenated with learnable mask tokens D [3], created by duplicating single mask embedding R1D. The decoder output corresponding to the mask tokens is regressed to the pixel values through linear projection. 2D position embeddings are applied for image tokens and mask tokens. 1D position embeddings are applied for query tokens and latent tokens. 3.2 Coarse-to-Fine 1D Latent Representation Human perception and image creation are inherently hierarchical processes, starting with rough global structure and progressively refining local details. This hierarchical approach reduces complexity and improves quality by breaking down the task into simpler steps. Inspired by this, we design coarse-to-fine information ordering for 1D latent tokens, enabling the model to progressively generate images from global to fine-grained details. As illustrated in Fig. 3(a), we enforce an information ordering in the 1D latent space by leveraging the correlation between image resolution and semantic granularity: lower-resolution images primarily preserve global structure, while higher-resolution images capture increasingly detailed content. We define resolution mapping function R(n) linking the number of used tokens to target resolution hw = R(n). Early tokens are trained to capture coarse structures at low resolutions, while rn = later tokens refine high-frequency details. To enforce this, we use causal (unidirectional) attention among query tokens in the encoder, while maintaining bidirectional attention for image tokens in the encoder and all the tokens in decoder. During training, we randomly sample [1, ] and reconstruct downsampled version of at resolution R(n) using only the first latent tokens Z1:n = {z1, z2, . . . , zn}. Mask tokens and positional embeddings of the decoder are adjusted according to the target image size. The model is supervised to reconstruct downsampled image from this partially observed latent sequence, ensuring that the earlier tokens specialize in capturing global structure while later tokens incrementally contribute finer details. Formally, the conditional entropy of the i-th token, H(zi Z1:i1), quantifies the incremental information it contributes. The total entropy up to token is: H(rn) = (cid:88) i=1 H(zi Z1:i1). (1) By leveraging this hierarchical decomposition, our method ensures that each token contributes meaningfully to the reconstruction of image details, enabling progressive and efficient representation of high-dimensional image data. Assuming the entropy per pixel is H(r = 1), the total image entropy at resolution scales as: H(r) r2H(r = 1), (2) indicating nonlinear relationship between the number of tokens and reconstructable resolution. To model this, we define R(n) (shown in Fig. 4(b)) as: R(n) = b(N n)α = (N n)α, where R(1) = 1, R(N ) = R. 1 (N 1)α (cid:124) (cid:123)(cid:122) (cid:125) (3) is the maximum resolution supported by the tokenizer, and coefficient is calculated using the condition R(1) = 1, ensuring the first token corresponds to resolution of 1 1. The condition R(N ) = ensures the total tokens corresponds to the maximum resolution R. The hyperparameter α controls the degree of nonlinearity. The continuous resolution value R(n) is rounded to the nearest multiple of to obtain the final target resolution for compatibility with the decoders patch size . For simplicity, this step is omitted in formulas and visualizations. 3.3 Parallel Inference Acceleration Generating high-resolution images often requires thousands of tokens, making purely sequential next-token prediction inefficient. To address this, we partition the 1D token sequence into groups of tokens each. During tokenization, we apply bidirectional attention within groups and causal attention across groups. For coarse-to-fine training, we randomly sample an integer [1, ] and reconstruct the downsampled image using the first groups Z1:k = {Z1, Z2, . . . , Zk}. For the autoregressive (AR) model, considering that the first group controls the global structure, which is extremely important, we keep the tokens in the first group as causal attention [43] and use next-token prediction as shown in Fig. 3(c). Subsequent groups use bidirectional attention internally and causal attention between groups, enabling parallel prediction of tokens per step. However, the independent sampling of tokens within group during parallel inference disrupts intra-group dependencies, introducing sampling errors. The teacher-forcing training paradigm does not equip the AR model with the ability to self-correct such errors. To mitigate this, prior works [23, 44] introduce depth transformer head to sequentially predict tokens within group, but at the cost of model complexity. Alternatively, Infinity [15] simulates self-correction by randomly flipping quantized value for one scale and recalculating the residuals at subsequent scales, enabling the AR model to learn self-correction capabilities from self-correction token sequences. Inspired by these ideas, we need to obtain 1D self-correction token sequence where sampling error occurs at one position, and the subsequent token sequence can correct the sampling error. Therefore, we inject stochastic perturbations into the quantization process. Specifically, as shown in Fig. 3(b) we randomly select token group Cm, [1, 1] and, during quantization, sample each token from the top-50 nearest codebook entries, yielding noisy group Zm. To obtain the subsequent correlation tokens, we introduce correction mechanism: feeding the clean tokens Z1:m1, noisy tokens Zm (with gradient truncation), and new query tokens back into the encoder. projection layer ensures dimensional compatibility. The encoder then generates corrected tokens {ˆZm+1, . . . , ˆZM }, forming self-correction sequence {Z1:m1, Zm, ˆZm+1:k}. To jointly train standard sequence modeling and self-correction, we concatenate the original sequence Z1:k and the self-correction sequence {Z1:m1, Zm, ˆZm+1:k} of the same input image along the batch dimension, independently decoding both to reconstruct the same target image. This trains the encoder to learn to output subsequent token sequences that can correct sampling errors in the preceding tokens. This design enables the tokenizer to provide both standard and self-correction sequences simultaneously, which enables the AR model to learn self-correction capabilities alongside standard token modeling, improving image quality during parallel inference. 3.4 Training Objective Since early tokens encode global structure, we explicitly align the first latent token z1 with the globally pooled features extracted by the pre-trained Siglip2 model [40]. This allows the first token to better embed global information. Specifically, the first token z1 is projected through three-layer MLP and aligned via cosine similarity as the alignment loss: Lalign = cos (MLP(z1), Siglip2(X)) . (4) In addition to the alignment loss, the final training objective of the tokenizer also includes the reconstruction loss, perceptual loss [11, 20], adversarial loss [14, 18] and VQ codebook loss [12], following the implementations and weighting schemes used in SoftVQ-VAE [8]."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Implementation Tokenizer Setup. The encoder including 12 layers is initialized with the weigths of Siglip2-NaFlex [40], yielding parameter count of 184M. In contrast, the decoder is trained from scratch, comprising 86M parameters. The discrete latent space is defined by codebook with 8,192 entries and dimension of 8. Tokenizer training is conducted on the ImageNet-1K [10], using 256 256 resolution inputs to the encoder and dynamically varying output resolutions (up to 256 256) from the decoder. 6 Table 1: Comparison of class-conditional image generation on ImageNet-1k at 256 256 resolution. Models marked with leverage additional training data beyond ImageNet. The symbol indicates methods that do not employ classifier-free guidance. The symbol denotes models that generate images at 384 384 resolution, which are subsequently downsampled to 256 256 for evaluation. #Tokens represents the total number of tokens predicted during autoregressive (AR) model training. Flex indicates whether the AR model supports dynamic resolution, meaning it can decode images of different resolutions using varying numbers of tokens. For DetailFlow, the notation 16*8 indicates that the 128 tokens are partitioned into = 16 groups, each containing = 8 tokens. Reported evaluation metrics include rFID and gFID. The inference steps of FlexTok [2] include the steps of the AR model and the Diffusion model."
        },
        {
            "title": "Type Tokenizer",
            "content": "rFID Generator Type Param. #Tokens gFID Step Time(s) Flex 2D GigaGAN[21] 2D VAE[33] 2D SD-VAE[35] 2D VAE[24] 1D TiTok-S[48] 1D FlexTok[2]"
        },
        {
            "title": "Continuous modeling",
            "content": "- GigaGAN[21] GAN 569M Diff. Diff. 0.27 LDM-4[33] 400M 4096 0.62 SiT-XL/2[27] 675M 1024 0.53 MAR-H[24] AR+Diff. 943M 256 Discrete modeling Mask."
        },
        {
            "title": "1.71 MaskGIT[7]",
            "content": "- 1 3.45 3.60 250 2.06 250 1.55 64 - - - 28.24 1.45 LlamaGen[36] AR+Diff. 1.33B VAR 362M 286*2 Mask. 0.80 VAR-GPT[38] 2.28 MaskGIT[7] 2D ImageFolder[25] 227M 256 2D VQGAN[7] 1.7B 1024 2D ViT-VQGAN[46] 1.28 VIM-Large[46] AR 3.8B 256 3.20 RQTran.-re[23] AR 2D RQ-VAE[23] 343M 256 AR 2D LlamaGen[36] 2.19 GPT-L[36] 343M 256 AR 2D O.-MAGVIT2[26] 1.17 AR-B[26] 2D PAR[43] AR 343M 576 2D VAR[38] VAR 310M 680 2D FlexVAR[19] VAR 310M 680 326M 16*8 AR 1D DetailFlow-16 326M 32*8 AR 1D DetailFlow-32 326M 64*8 AR 1D DetailFlow-64 0.94 PAR-L-4[43] 0.90 VAR-d16[38] - VAR-d16[19] 1.22 GPT-L [36] 0.80 GPT-L [36] 0.55 GPT-L [36] 0.13 1.97 64 287M 128 1.86 57 - 32 256 2.5 281 - 0.13 10 2.60 0.13 6.18 8 4.17 1024 >6.38 5.58 3.8 64 256 12.58 3.8 3.08 256 - 3.38 3.76 147 0.15 10 3.30 0.15 10 3.05 0.08 23 2.96 0.16 39 2.75 0.38 71 2. We train the tokenizer for 250 epochs with batch size of 256 using cosine learning rate decay strategy, starting from an initial learning rate of 1e-4. To ensure robust modeling of the entire latent token sequence, we reconstruct full-resolution images (n = ) with an 80% probability, and with 20% probability, we randomly reconstruct lower-resolution images by sampling the first = kg tokens, where [1, 1], = 8, = N/g. We achieve full codebook utilization (100%) across all tokenizers. AR Model Setup. For downstream generation, we adopt an autoregressive (AR) model based on the LlamaGen architecture [36]. The AR model is trained on ImageNet-1K using cosine-decayed learning rate schedule for 300 epochs. 30% of the training data consists of curated self-correction token sequences. During inference, we apply sampling with Top-K=8192 and Top-P=1. ClassifierFree Guidance (CFG) is tuned to its optimal value to balance generation diversity and fidelity. The default CFG value is 1.5. The inference time is measured on single A100 using the batch size 1. Metrics. We evaluate image reconstruction quality on the ImageNet-1K validation set using rFID [16], Peak Signal-to-Noise Ratio (PSNR), and Structural Similarity Index (SSIM). For assessing image generation quality, we report Frechet Inception Distance (FID) [16], Inception Score (IS) [34], as well as Precision and Recall metrics. 4.2 State-of-the-art image generation In Table 1, we evaluate our proposed method, DetailFlow, on the ImageNet 256 256 benchmark, comparing it against range of state-of-the-art generative modelsincluding GANs [21], diffusion models [33, 35, 24], masked prediction models [7, 2], and autoregressive (AR) models [36, 26, 38, 19]. Figure 4: (a) Reconstruction metrics before and after self-correction when adding noise to latent tokens of group (tokenizer with 128 tokens, group size 8, trained for 200 epochs). (b) Impact of token count on image resolution, reconstruction quality (rFID), and generation quality (gFID), with all evaluations conducted on images resized to 256 256. The tokenizer is identical to (a). (c) Influence of the hyperparameter α in the mapping function R(n) on generation metrics, using tokenizers trained for 50 epochs. Compared to existing 2D tokenizers [36, 26, 38, 19] that support dynamic resolution, our method delivers higher quality with shorter sequence length among ar models. DetailFlow-16 achieves lower gFID of 2.96 using only 128 tokens, surpassing VAR (3.3 FID) [38] and FlexVAR (3.05 FID) [19], which both require 680 tokens. Additionally, its reduced token count and parallel inference make it nearly twice as fast as VAR and FlexVAR during inference. This is attributed to the fact that the 1D tokenizer effectively eliminates spatial information redundancy, allowing more information to be carried with fewer tokens. Although ImageFolder [25] reports gFID score comparable to ours, its lack of support for dynamic resolution constrains its practical applicability. While ImageFolder employs 1D tokenizer architecture, the resulting latent tokens are still arranged in 2D structure and preserve explicit spatial information, limiting it to decoding images at fixed resolution. Compared to existing 1D tokenizers [48, 2], our approach addresses several key limitations. Notably, prior 1D tokenizers do not support multi-resolution image generation and parallel inference with self-correction mechanisms. Additionally, TiTok [48] lacks an explicit and structured token ordering necessary for effective AR modeling, while FlexTok [2] demonstrates limited scalability, with performance degrading as token count increases. For instance, using 1.33B AR model, its gFID score rises from roughly 1.9 at 32 tokens to about 2.5 at 256 tokens. In contrast, DetailFlow supports coarse-to-fine image generation (Fig. 1(a)), allowing for the prediction of more tokens to decode higher-resolution images. It also supports parallel inference with self-correction to accelerate the image generation process, making it both scalable and efficient. 4.3 Ablation Study In Fig. 4(a), we analyzes the reconstruction performance before and after self-correction when noise is injected into different groups of latent tokens. When noise is applied to earlier token groups (i.e., smaller group indices), reconstruction quality deteriorates sharply. This is expected, as early tokens encode the global structure, and inaccuracies in this region propagate large-scale distortions throughout the image. Moreover, in these cases, the self-correction mechanism exhibits limited effectiveness, since later tokensresponsible mainly for fine detailslack the capacity to rectify global errors. As the group index increases, the model demonstrates strong ability to correct errors introduced by noisy tokens. However, when noise is injected into the final few groups, the correction performance again deteriorates, as fewer subsequent tokens are available to facilitate error correction. Table 2: Ablation study of DetailFlow. These tokenizers with = 128 latent tokens are trained with only 50 epochs. Reported evaluation metrics include rFID, PSNR, gFID and Recall. Setting Baseline +causal encoder +coarse-to-fine +parallel(g = 4) +self-correction +first group causal +alignment loss rFID PSNR gFID Rec Step 3.97 0.50 128 1.73 19.47 3.66 0.53 128 1.87 19.49 3.33 0.54 128 1.92 19.31 4.11 0.50 32 1.87 19.27 3.68 0.51 32 3.59 0.51 35 3.35 0.55 35 1.81 19.16 1.68 19.05 8 These results underscore the importance of early tokens for capturing global structure. To enhance their reliability, our model incorporates self-correction training, causal modeling for the first token group, and an alignment loss on the initial token. We further perform an ablation study to evaluate each components contribution. Starting from baseline that encodes images into an unordered token sequence, we progressively add modules to measure their effects. First, introducing causal encoder establishes simple sequential order among the tokens, which substantially improves the models capacity for autoregressive generation. Building on this, we implement coarse-to-fine tokenizer training strategy by supervising reconstructions at multiple resolutions. The observed improvements in the gFID metric from 3.66 to 3.33 validate that enforcing such coarse-to-fine semantic ordering is both effective and advantageous. Next, we explore parallel prediction of token groups. Although this design reduces the inference steps from 128 to 32, it introduces degradation in generation quality, primarily due to the accumulation of sampling errors across groups. To mitigate this, we incorporate self-correction mechanism, which substantially restores synthesis quality by allowing the model to iteratively refine predictions. This reduces the gFID score from 4.11 to 3.68. Fig. 5 shows that selfcorrection training improves global structure and detail quality, suggesting that self-correction effectively mitigates the impact of sampling errors. Figure 5: Qualitative comparison of AR model outputs with (w/) and without (w/o) self-correction training. Further enhancement is achieved by applying causal next-token prediction specifically to the first group of tokens, yielding an additional improvement of 0.09 in gFID. This stabilizes the generation process and improves the fidelity of the final output. Finally, to strengthen global semantic consistency, we align the first tokens representation with Siglip2 global image features via an alignment loss. This further lowers the gFID from 3.59 to 3.35, indicating that anchoring the initial token to global structural information provides stronger guidance for the entire generation process. Fig. 4(b) shows how increasing the number of latent tokens impacts output resolution, reconstruction metrics, and generation metrics. As the number of tokens grows, the latent sequence encodes finer-grained information, facilitating higher-resolution image decoding. Both reconstruction and generation metrics improve correspondingly, with the autoregressive model benefiting from richer detail prediction, leading to enhanced image quality and resolution. Fig. 4(c) examines the effect of hyperparameter α in the mapping function R(n) on generation metrics. The parameter α governs the relationship between token quantity and decoding resolution. Smaller values of α are preferable, as they indicate that fewer additional tokens are required to support higher-resolution images. Across different total token counts, the optimal value of α is consistently 1.5. This value, greater than 1, reflects that higher-resolution regions demand more tokens compared to lower-resolution regions, aligning with Eq. 2. Meanwhile, α being less than 2 highlights the tokenizers ability to compress spatial redundancy inherent in the image data."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose novel autoregressive image generation method DetailFlow, introducing new image generation paradigm called Next-Detail Prediction. By leveraging 1D tokenizer trained on progressively degraded images, DetailFlow establishes direct correspondence between token sequences and image resolution levels, enabling coarse-to-fine generation strategy that enhances visual fidelity. This method effectively compresses image information into smaller token sequence while maintaining high-quality image generation. Furthermore, parallel decoding mechanism with self-correction improves inference speed without compromising image quality. Overall, DetailFlow achieves an effective balance among training cost, inference efficiency, and image quality, offering scalable solution for high-resolution, autoregressive image synthesis."
        },
        {
            "title": "References",
            "content": "[1] Rohan Anil, Andrew Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023. [2] Roman Bachmann, Jesse Allardice, David Mizrahi, Enrico Fini, Oguzhan Fatih Kar, Elmira Amirloo, Alaaeldin El-Nouby, Amir Zamir, and Afshin Dehghan. Flextok: Resampling images into 1d token sequences of flexible length. arXiv preprint arXiv:2502.13967, 2025. [3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In Proceedings of the International Conference on Learning Representations (ICLR). [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems (NeurIPS), 33:1877 1901, 2020. [5] Shiyue Cao, Yueqin Yin, Lianghua Huang, Yu Liu, Xin Zhao, Deli Zhao, and Kaigi Huang. Efficient-vqgan: Towards high-resolution image generation with efficient vision transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 73687377, 2023. [6] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. [7] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1131511325, 2022. [8] Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. arXiv preprint arXiv:2412.10958, 2024. [9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1 113, 2023. [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255. Ieee, 2009. [11] Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. Advances in Neural Information Processing Systems (NeurIPS), 29, 2016. [12] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1287312883, 2021. [13] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image synthesis, 2020. [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [15] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. 10 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. [17] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 11251134, 2017. [19] Siyu Jiao, Gengwei Zhang, Yinlong Qian, Jiancheng Huang, Yao Zhao, Humphrey Shi, Lin Ma, Yunchao Wei, and Zequn Jie. Flexvar: Flexible visual autoregressive modeling without residual prediction. arXiv preprint arXiv:2502.20313, 2025. [20] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Proceedings of the European Conference on Computer Vision (ECCV), pages 694711. Springer, 2016. [21] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and In Proceedings of the IEEE Taesung Park. Scaling up gans for text-to-image synthesis. Conference on Computer Vision and Pattern Recognition (CVPR), pages 1012410134, 2023. [22] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. Bloom: 176b-parameter open-access multilingual language model. 2023. [23] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive In Proceedings of the IEEE Conference on image generation using residual quantization. Computer Vision and Pattern Recognition (CVPR), pages 1152311532, 2022. [24] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems (NeurIPS), 37:5642456445, 2024. [25] Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: In Proceedings of the International Autoregressive image generation with folded tokens. Conference on Learning Representations (ICLR). [26] Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Openmagvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. [27] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In Proceedings of the European Conference on Computer Vision (ECCV), pages 2340. Springer, 2024. [28] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 41954205, 2023. [29] Liao Qu, Huichao Zhang, Yiheng Liu, Xu Wang, Yi Jiang, Yiming Gao, Hu Ye, Daniel Du, Zehuan Yuan, and Xinglong Wu. Tokenflow: Unified image tokenizer for multimodal understanding and generation. arXiv preprint arXiv:2412.03069, 2024. [30] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. San Francisco, CA, USA, 2018. [31] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [32] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in Neural Information Processing Systems (NeurIPS), 32, 2019. 11 [33] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems (NeurIPS), 29, 2016. [35] StabilityAI. Sd-vae. [36] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [37] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. [38] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in Neural Information Processing Systems (NeurIPS), 37:8483984865, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier Hénaff, Jeremiah Harmsen, Andreas Steiner, and Xiaohua Zhai. Siglip 2: Multilingual visionlanguage encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. [41] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. [42] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. [43] Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. [44] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. [45] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. [46] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [47] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. [48] Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems (NeurIPS), 37:128940128966, 2024. 12 [49] Chuanxia Zheng, Tung-Long Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation. Advances in Neural Information Processing Systems (NeurIPS), 35:2341223425, 2022."
        },
        {
            "title": "A Technical Appendices and Supplementary Material",
            "content": "A.1 Implementation Details A.1.1 Tokenizer Setup We initialize the encoder using the SigLIP2-NaFlex weights and retain only the first 12 layers to reduce memory consumption, resulting in an encoder of 184M parameters. As shown in Fig. 6(a), the attention design of the encoder includes bidirectional attention within image tokens and within each latent token group (except the first), causal attention within the first latent group, and causal attention across groups. A.1.2 AR model Setup Building on the autoregressive LlamaGen architecture, we incorporate learnable mask tokens inspired by PAR [43] to enable parallel decoding. We also use the 2D positional encoding to enhance the models ability to distinguish between different token groups and intra-group tokens. As illustrated in Fig. 6(b), the attention pattern of the AR model is structured such that the first group uses causal attention for next-token prediction, while subsequent groups apply bidirectional attention internally and causal attention across groups. This configuration allows for parallel prediction of later groups while preserving autoregressive dependencies. Training is conducted primarily on 16 A800 GPUs. Figure 6: The attention mask in the proposed method. We use group size = 2 for latent tokens as an example. A.2 Additional Results A.2.1 Additional Ablation Study Impact of Coarse-to-fine Training Probability. During the training process of DetailFlow, we introduce probabilistic strategy to balance between full-resolution and degraded-resolution image reconstruction. Specifically, with certain probability, the model is trained with complete token sequences to reconstruct full-resolution images, while with the remaining probability, it is trained on downsampled images with partial tokens to encourage learning the coarse-to-fine ordering of token representations. Table 3 reports an ablation study on the impact of different probabilities of coarse-to-fine training on image generation quality. The results show that even 20% probability of degraded-resolution training enables the tokenizer to learn hierarchical token structure effectively. However, increasing this probability further shifts training focus toward reconstructing downsampled images with partial tokens, which hinders the models capacity to learn full-sequence representations. These findings highlight the importance of balancing coarse and fine training to optimize efficiency under limited computational resources. Impact of Group Size on Parallel Token Prediction. DetailFlow employs parallel decoding strategy that predicts tokens simultaneously to accelerate inference by reducing the number of decoding steps. Table 4 analyzes the impact of varying group sizes on image reconstruction and generation, with the total token length fixed at = 256. Results indicate that reconstruction quality 13 Table 3: The impact of coarse-to-fine training probability on reconstruction and generation metrics. These tokenizers with = 128 latent tokens are trained with only 50 epochs and the GPT-L AR model are trained with 100 epochs. Reported evaluation metrics include rFID, PSNR, SSIM, gFID, sFID, Inception Score(IS), Precision (Pre) and Recall (Rec). Probability 10% 20% 30% rFID PSNR SSIM 19.3 19.2 19.3 0.61 0.60 0.61 2.0 1.8 1. gFID sFID 5.60 4.18 5.46 3.81 5.90 4.18 IS 202.6 230.1 215.3 Pre Rec 0.51 0.85 0.51 0.86 0.49 0.86 Table 4: The impact of group size on reconstruction and generation metrics. These tokenizers with = 256 latent tokens are trained with 250 epochs and the GPT-L AR model are trained with 300 epochs. Reported evaluation metrics include rFID, PSNR, SSIM, gFID, sFID, Inception Score(IS), Precision (Pre) and Recall (Rec). Group 32*8 (g = 8) 16*16 (g = 16) rFID PSNR SSIM 20.8 0.80 20.8 0. 0.67 0.68 gFID sFID 5.77 2.75 6.03 2.88 IS 250.8 238.2 Pre Rec Step 39 0.58 0.81 31 0.59 0.80 Table 5: The impact of Classifier-Free Guidance (CFG) on generation metrics. The tokenizer is DetailFlow-32 with = 256 latent tokens. Reported evaluation metrics include gFID, sFID, Inception Score(IS), Precision (Pre) and Recall (Rec). CFG gFID sFID 5.87 2.79 1.4 5.77 2.75 1.5 5.66 2.81 1.6 5.61 3.02 1. IS 229.1 250.8 268.1 283.2 Pre Rec 0.60 0.80 0.58 0.81 0.57 0.82 0.56 0.83 remains largely unaffected by changes in g, likely because the total number of tokens is constant. However, image generation quality shows slight decline of 0.13 gFID as increases, potentially due to higher sampling noise from parallel prediction. Despite this, the degradation is minimal, suggesting that the self-correction training effectively counteracts sampling errors. This allows for increased parallelism and faster inference with only marginal loss in image quality. Impact of Classifier-Free Guidance. Table 5 presents the impact of varying the Classifier-Free Guidance (CFG) scale on generation quality using DetailFlow-32 as the tokenizer. It can be observed that an appropriate CFG value 1.5 can effectively balance generation quality and diversity. A.2.2 State-of-the-art image generation Table 6 presents detailed comparison between our proposed method DetailFlow and state-of-the-art approaches under similar AR model sizes. The results demonstrate that our model, DetailFlow-16, achieves higher gFID score and significantly outperforms existing methods in terms of Recall metric, while attaining comparable performance in Precision metric. These findings indicate that DetailFlow-16 is capable of generating higher-quality images using fewer tokens and at faster inference speed. Furthermore, as the token count increases to 256 and 512, the performance of DetailFlow continues to improve. In particular, DetailFlow-32 strikes strong balance across image quality, diversity, and generation speed. This suggests that, with comparable or lower training cost (in terms of token number) and inference cost, DetailFlow consistently outperforms existing models, highlighting its efficiency and effectiveness in autoregressive image generation. A.3 Future Work To ensure fair comparison with existing methods, the tokenizer in our experiments is trained on square images with equal height and width. However, our proposed framework, DetailFlow, is not inherently restricted to this setting. Both its encoder and decoder adopt the SigLIP2-NaFlex architecture, which natively supports inputs of arbitrary resolution and aspect ratio. By resizing positional encodings to 14 Table 6: Comparison of AR methods for class-conditional image generation on ImageNet-1k at 256 256 resolution under similar AR model size. Models marked with leverage additional training data beyond ImageNet. The symbol denotes models that generate images at 384 384 resolution, which are subsequently downsampled to 256 256 for evaluation. O.-MAGVIT2 means Open-MAGVIT2. Reported evaluation metrics include rFID, PSNR, gFID, Inception Score(IS), Precision (Pre) and Recall (Rec). Method LlamaGen[36] O.-MAGVIT2[26] 1.17 22.64 AR-B PAR[43] VAR[38] FlexVAR[19] DetailFlow-16 DetailFlow-32 DetailFlowrFID PSNR Generator gFID IS Pre Rec Param. #Tokens Step Time(s) 256 12.58 2.19 20.79 GPT-L 3.81 248.3 0.83 0.52 343M 256 256 3.08 258.3 0.85 0.51 343M 256 PAR-L-4 3.76 218.9 0.84 0.50 343M 576 147 3.38 0.15 10 VAR-d16 3.30 274.4 0.84 0.51 310M 680 0.15 10 VAR-d16 3.05 291.3 0.83 0.52 310M 680 0.08 23 1.22 19.39 GPT-L 2.96 221.4 0.82 0.57 326M 16*8 0.16 39 0.80 20.80 GPT-L 2.75 250.8 0.81 0.58 326M 32*8 0.38 71 0.55 22.49 GPT-L 2.62 245.3 0.80 0.60 326M 64*8 0.94 0.9 - - - - - match input dimensions, the model remains compatible with non-square images. Under this design, the implicit, learnable 1D latent tokens effectively represent images of any resolution or aspect ratio. To enable autoregressive (AR) models to generate images with specified aspect ratio, it is essential to condition the model during both training and inference. This can be achieved by incorporating aspect ratio information via natural language prompts or special tokens that encode the target ratio. hw = R(n), the model Given the relationship between image resolution and token count rn = is guided to predict specific number of latent tokens corresponding to the desired resolution and aspect ratio. A.4 Limitations DetailFlow achieves efficient token compression by embedding 2D image information into 1D coarse-to-fine token sequence using query-token-driven tokenizer. However, this design introduces limitations, particularly in high-resolution image reconstruction. Capturing fine-grained visual details often requires several thousand latent tokens, substantially increasing the tokenizers computational cost during training. In contrast, conventional 2D tokenizers, with spatially consistent strategies, are trainable on lowresolution images and generalize effectively to higher resolutions. 1D tokenizer lacks this scalability, making it less efficient in high-resolution settings. To mitigate the high training cost, progressive training strategy proves effective. Since both the encoder and decoder support variable input resolutions, training can begin with low-resolution images and less latent tokens to establish robust encoding and decoding. The model is then fine-tuned on high-resolution data, enabling adaptation to finer spatial details without retraining from scratch. This approach enhances training efficiency while preserving the models flexibility across resolutions. A.5 More Visual Results Fig. 7 and Fig. 8 present reconstruction and generation examples produced by DetaiFlow across various categories. As the number of tokens increases, both the detail and resolution of the images improve accordingly, illustrating clear coarse-to-fine progression. This observation highlights the effectiveness of our coarse-to-fine training strategy, which imposes structured semantic order on the token sequence that aligns well with the autoregressive prediction paradigm. 15 Figure 7: Progressive reconstruction results from DetailFlow-16. Our method encodes 2D image content into coarse-to-fine 1D token sequence, with early tokens capturing global structure and later tokens introducing details necessary for both finer texture and higher resolution. As the number of tokens increases, the reconstructed images exhibit progressive improvements in both detail and resolution. The last column is the original Ground Truth (GT) image. 16 Figure 8: Progressive generation results from DetailFlow-16. Our proposed 1D tokenizer encodes tokens with an inherent semantic ordering, where each subsequent token contributes additional high-resolution information. The sequences illustrate how image resolution and inferred 1D tokens incrementally increase from left to right."
        }
    ],
    "affiliations": [
        "ByteDance Inc."
    ]
}