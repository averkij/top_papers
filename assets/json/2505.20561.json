{
    "paper_title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning",
    "authors": [
        "Shenao Zhang",
        "Yaqing Wang",
        "Yinxiao Liu",
        "Tianqi Liu",
        "Peter Grabowski",
        "Eugene Ie",
        "Zhaoran Wang",
        "Yunxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL."
        },
        {
            "title": "Start",
            "content": "2025-5-28 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Shenao Zhang1, Yaqing Wang2, Yinxiao Liu2, Tianqi Liu2, Peter Grabowski3, Eugene Ie3, Zhaoran Wang1, Yunxuan Li3 1Northwestern University, 2Google DeepMind, 3Google 5 2 0 2 6 2 ] . [ 1 1 6 5 0 2 . 5 0 5 2 : r Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL. 1. Introduction Large Language Models (LLMs) have demonstrated impressive reasoning abilities, such as in solving complex math problems. key factor driving this progress is the use of Chain-of-Thought (CoT) reasoning [50], where the model engages in intermediate deliberation before producing an answer. Building on this, recent advances have employed Reinforcement Learning (RL) to further enhance LLM reasoning by optimizing for verifiable outcome rewards [23, 17, 56, 51]. Notably, RL-trained models have exhibited emergent behaviors such as generating long CoTs and engaging in self-reflection, process of backtracking to previous states to correct earlier mistakes, also known as the Aha moment [17, 61]. However, despite these compelling phenomena, it remains unclear why and under what conditions reflective reasoning is beneficial at test time, or whether such behaviors will emerge through conventional RL training. Prevalent views attempt to explain the usefulness of test-time reflections as exploratory steps that provide additional contexts for more optimal decision-making. Yet in Markovian RL, the explorationexploitation trade-off is resolved entirely during training: the agent interleaves exploration and exploitation to learn training-time optimal deterministic policy, but switches to pure exploitation at test time. As result, conventional RL allows Markovian policy to be optimal by simply memorizing, i.e., deterministically outputting, training solutions once they are encountered by stochastic exploratory policies through repeated trial-and-error. Moreover, the Markov assumption restricts the policy to condition decisions solely on the current state rather than on contextual information gathered through exploration, offering no incentives to adaptively explore with reflections. In summary, under conventional Markovian RL, there is no guarantee that reflective explorations will emerge during training, nor does it explain Work done during an internship at Google. Equal advising. Correspondence to: shenao@u.northwestern.edu, zhaoran.wang@northwestern.edu, yunxuanli@google.com. Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning why such explorations might be advantageous at test time. To address this gap, we propose grounding reflective reasoning with Bayes-Adaptive RL, which explicitly optimizes for test-time generalization by maximizing the expected return under posterior distribution over Markov Decision Processes (MDPs). The objective incentivizes both reward-seeking actions and epistemic explorations that gather information to reduce the MDPs uncertainty, such as the uncertainty regarding the progress made by different actions. This enables the model to adapt on-the-fly at test time by updating its beliefs and switching strategies based on observed outcomes, naturally giving rise to reflective exploration behaviors. We prove that the expected return of an adaptive policy can be exponentially higher than the optimal Markovian policy at test time. Building upon this formulation, we introduce novel algorithm, Bayes-Adaptive RL for LLM Reasoning (BARL). For each prompt, BARL performs online rollouts to generate set of candidate answers, each associated with an MDP hypothesis. The state-action value is then computed by weighting each hypothesis according to the models current belief, with penalties applied for mismatches between predicted and observed rewards, thereby signaling when to switch strategies. BARL provides principled mechanism for integrating and revising plausible strategies, analogous to linearizing best-of-N reasoning, but with explicit step-level guidance on when and how the model should reflectively explore. To illustrate the benefits of BARL, we begin with synthetic task designed to mirror test-time generalization in LLM reasoning. The agent receives reward only when it repeats prompt token exactly three times, but the training and testing prompt tokens differ. Standard Markovian RL memorizes the training solutions and fails to generalize. In contrast, BARL learns to switch strategies by eliminating hypotheses, ultimately discovering the ground-truth MDP for optimal behavior. We further evaluate BARL on math reasoning tasks using various LLMs, including Qwen2.5-Math1.5B, Qwen2.5-Math-7B, and R1-Distill-Llama-8B. Across these models, BARL consistently outperforms Markovian RL algorithms, such as GRPO and strong progress-reward baseline, on multiple benchmarks. BARL achieves significantly greater token efficiency, requiring up to 39% fewer average tokens than the progress baseline, 50% fewer than GRPO, and over 90% fewer than the Qwen2.5-Math-1.5B base model. Moreover, we observe no strong correlation between overall model performance and the frequency of reflections. Instead, BARLs advantage stems from more efficient exploration and more effective thinking tokens. We summarize the key takeaways of this paper as follows: Key Takeaways: Why, How, and When Should LLMs Reason Reflectively Why: Markovian RL neither ensures the emergence of reflective exploration nor explains its benefits at test time since (1) exploration is confined only to the training phase to learn, and purely exploit at test time, an optimal deterministic policy that can merely memorize training solutions, and (2) the state-conditional policy lacks incentives to collect additional contexts and backtrack. In contrast, Bayes-Adaptive RL, by optimizing test-time generalizability, encourages explorations to gather contextual information that reduces the MDP uncertainty. How: BARL provides principled way to stitch plausible strategies by maintaining posterior over MDP hypotheses, each associated with sampled candidate answer. Reflective exploration emerges naturally through hypothesis elimination, enabling on-the-fly adaptation. When: LLMs should self-reflect when discrepancies arise between their internal beliefs and cumulative reward feedbacksignaling strategy switching by downweighting hypotheses that have high belief probabilities but are unlikely to be optimal given previous observations. Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning 2. Related Work LLM Reasoning. As an emerging capability of model scale, LLMs can generate intermediate CoTs to solve complex reasoning tasks [50, 25] and scale test-time performance by allocating more thinking tokens [42, 5]. Early efforts enhanced LLM reasoning via supervised fine-tuning on human-annotated data [7, 58, 55] or linearized search traces [26, 12]. However, due to the distribution shift between LLM responses and curated data, LLM-generated data has proven effective through rejection sampling [8, 57] by filtering out low-quality rationales [60, 59] or with EM iterations [41, 63]. Recently, RL has gained increasing interest for improving reasoning [1, 19, 45, 40]. Process rewards [44, 29] with Monte Carlo unrolls [24, 46, 49, 32] offer finer-grained feedback but are computationally expensive. Outcome-reward RL [17] demonstrates emergent deliberative reasoning abilities such as self-reflection. Yet, limited work has investigated the underlying mechanisms of such behaviors. In fact, recent findings suggest that reflections do not consistently emerge from RL training and exhibit weak correlation with performance [31]. Similar to our work, [52, 37] also study the generalization of LLMs, from meta-RL [9] perspective: [52] justify deliberative reasoning as providing extra contexts, and [37] use progress reward [39] to reduce regret in outcome-reward RL. Our method differs from [52] in that we ground reflective reasoning in environment rewards, rather than relying solely on the internal CoT states generated by the model itself. Compared to [37], which rewards golden strategies that make progress towards the correct answer, BARL additionally encourages exploring plausible strategies under the Bayesian framework, allowing it to account for uncertainty during both training and testing. We experimentally compare against variant of [37] that estimates progress using answer probability differences instead of more expensive Monte Carlo unrolls. Besides, unlike [47, 35] that manually design hypothesis proposalselection pipelines, our method achieves this through more principled RL optimization procedure. Reinforcement Learning. Conventional RL explores only during training, e.g. via 𝜖-greedy noise, and exploits the optimal deterministic policy when deployed. Exceptions include works that explicitly optimize maximum entropy objectives [18] to learn stochastic policies, primarily to accelerate training convergence in settings where evaluation remains in-distribution, such as robotic control. Bayes-Adaptive RL [4, 11, 16, 13, 27, 62, 30] has been studied to pursue the optimal exploration-exploitation tradeoff in uncertain environments to improve generalizability. When the true MDP identity is latent and must be inferred from interaction (states, actions, rewards), Bayesian RL connects naturally to Partially Observable MDPs [10, 15]. Exact solutions of the Bayesian RL objective are often intractable, prompting the development of approximate methods [16, 3, 6]. In our work, we adopt policy gradient that operates over candidate answers, which differs from [14] that leverage value ensembles in offline RL and [36] that applies SFT on an oracle Bayesian models outputs. 3. Problem Formulation LLM Reasoning via RL. To enable the LLM policy 𝜋𝜃 to reason, we first consider the finite-horizon MDP defined by the state space S, action space A, horizon 𝑇, and reward function 𝑟(𝑠, 𝑎), where 𝑠 and 𝑎 A. Here, the initial state 𝑠0 is the prompt, and the action 𝑎𝑡 is the 𝑡-th step of the CoT, which can be either separated by special tokens [46] or defined as fixed length of reasoning tokens [32]. We adopt the latter definition due to its simplicity. The state transition is deterministic by appending the new reasoning step, i.e., 𝑠𝑡+1 = 𝑠𝑡 + 𝑎𝑡. Prior work [44, 17] employs an outcome-level reward verifier(𝑠𝑇 , 𝑦 ), which 𝑠0 uses verifier to perform regular expression match (either 0 or 1) between 𝑠𝑇 and the ground-truth 3 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning answer 𝑦 progress reward [39, 37], which quantifies the increase of the models probability of outputting 𝑦 appending 𝑎 at the CoT 𝑠, i.e., for 0 𝑡 𝑇 1: 𝑠0 corresponding to the prompt 𝑠0. We extend this sparse-reward setting by incorporating 𝑠0 after 𝑟(𝑠𝑡, 𝑎𝑡) = 𝜋𝜃( 𝑦 𝑠0 𝑠𝑡 + 𝑎𝑡 + </think>) 𝜋𝜃( 𝑦 𝑠 𝑠𝑡 + </think>), (3.1) where </think> is the end sign of thinking, such as the answer elicitation prompt Based on the above reasoning, the answer is boxed that we adopt. Compared to Monte-Carlo process rewards [32, 37], (3.1) is computationally efficient by avoiding multiple branched rollouts at each step, and the KV cache of 𝑠0:𝑇 from CoT generations can also be reused. For the Markovian RL objective JMarkov(𝜋𝜃) := 𝔼𝑠0,𝜋𝜃 [(cid:205)𝑇 1 𝑡=0 𝑟(𝑠𝑡, 𝑎𝑡) + verifier(𝑠𝑇 , 𝑦 𝑠0 definition allows us to use telescoping in way similar to reward shaping [34] to obtain )], this reward argmax 𝜋𝜃 JMarkov(𝜋𝜃) = argmax 𝜋𝜃 𝔼𝑠0,𝜋𝜃 (cid:2)𝜋𝜃( 𝑦 𝑠0 𝑠0 + 𝑎0:𝑇 1 + </think>) + verifier(𝑠𝑇+1, 𝑦 𝑠 )(cid:3), i.e., the optimal Markovian policy generates 𝑎0:𝑇 1 to maximize the likelihood of the ground-truth 𝑦 𝑠0 and its verifier-evaluated correctness. The gradients for Markovian policies are 𝜃JMarkov(𝜋𝜃) = 𝔼𝑠0,𝜋𝜃 𝜃 log 𝜋𝜃(𝑎𝑡 𝑠𝑡) 𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡) (cid:21) , (cid:20)𝑇 1 𝑡=0 (3.2) where 𝑄𝜋𝜃 is the state-action value or advantage function [38]. The above setups consider the case when the environment is predefined with certainty. The definitions naturally extend to any MDP := (S, A, 𝑟M, 𝑇) where 𝑟M is defined w.r.t. the answer 𝑦 𝑠0 . The Q-value is then 𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡) = 𝔼𝜋𝜃 = 𝔼𝜋𝜃 (cid:20)𝑇 1 𝑡=𝑡 (cid:2)𝜋𝜃( 𝑦 𝑠0 𝑟M (𝑠𝑡 , 𝑎𝑡) + verifier(𝑠𝑇 , 𝑦 𝑠0 (cid:21) ) (3.3) 𝑠𝑡 + 𝑎𝑡:𝑇 1 + </think>) 𝜋𝜃( 𝑦 𝑠0 𝑠𝑡 + </think>) + verifier(𝑠𝑇 , 𝑦 𝑠0 )(cid:3) . Reflective Exploration. We define reflective exploration as the pattern in which the LLM backtracks to prior state after an exploratory step to take different actions at that state. Specifically, natural language reflective reasoning step such as Lets reconsider the geometric relationship corresponds to backtracking action that semantically disregards the previous one or more steps. We illustrate this using binary search 𝑠2 is an tree as in the right figure: for the trajectory 𝑠0 𝑠1 exploration step, 𝑠2 𝑠1 is reflective step that signals the strategy switch from 𝑠2 to 𝑠3, and the geometric relationship in the above example originates from 𝑠1. 𝑠3, 𝑠1 𝑠2 𝑠1 (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) (cid:1) Figure 1 An example of reflective reasoning. 4. The Necessity of Bayes-Adaptive RL for Reflective Reasoning Markovian RL. When the underlying MDP is known with certainty, the Markov property ensures that the policy and value depend on the history ℎ𝑡 = (𝑠0, 𝑎0, 𝑟0, . . . , 𝑠𝑡) only through the state 𝑠𝑡, i.e., 𝑄𝜋(ℎ𝑡, 𝑎𝑡) = 𝑄𝜋(𝑠𝑡, 𝑎𝑡). In this setting, exploratory actions that aim to enrich the history ℎ𝑡 with additional 4 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning contexts, such as incorrect attempts followed by backtracking, are unnecessary, as the current state 𝑠𝑡 already encodes all relevant information for optimal decision-making1. Moreover, the optimal Q-function is 𝑄 = max𝜋 𝑄𝜋 and the optimal policy 𝜋 is greedy w.r.t. 𝑄. That is, 𝜋 is deterministic policy, where 𝜋(𝑎𝑠) = 1 for 𝑎 = argmax𝑎 𝑄(𝑠, 𝑎). Theorem 4.1. Optimality of Markovian RL is attained by deterministic non-reflective policies. Reflective policies are more suboptimal than non-reflective policies in both discounted infinite-horizon and finite-horizon MDPs, as the 𝑄-value of the wrong action is no larger than that of the correct action since more tokens are needed to correct the error. Explorations should only occur during training, in trial-and-error manner with repeated episodes, to discover the golden answers. The Markovian RL objective allows the optimal policy that memorizes these training answers to be fully exploited, with no incentive to adaptively explore with reflections. For the non-standard undiscounted infinite-horizon MDPs, where LLMs are encouraged to generate infinite tokens without concerning token efficiency, reflective policies may be as optimal as non-reflective ones. This is because the 𝑄-value of the wrong action can match that of the correct one if the error is eventually corrected through reflection. This observation provides partial explanation for the emergence of Aha moment with long CoT. However, even in such settings, reflective reasoning may still fail to emerge under Markovian RL, particularly if golden answers are discovered either directly or by pruning incorrect exploratory steps. In other words, this only explains why reflective explorations can appear during Markovian RL, instead of why these behaviors are preferable to simply memorizing training solutions, nor whether they will emerge during training. Next, we present Bayes-Adaptive RL, which explicitly optimizes for test-time generalization and naturally induces reflective explorations. Bayes-Adaptive RL. In Bayes-Adaptive MDP (BAMDP) [4, 33, 28], the agent maintains uncertainty over the underlying MDP, which is gradually reduced through interactions. Due to this implicit partial observability [10, 15], the policy and value depend on the full history ℎ𝑡, instead of only the state 𝑠𝑡, to capture the agents evolving belief about the MDP parameters through cumulative observations. The objective for BAMDPs is JBayes(𝜋𝜃) := 𝔼𝑠0,𝜋𝜃 𝔼M𝑝( ℎ𝑡 ) (cid:2)𝑟M (𝑠𝑡, 𝑎𝑡)(cid:3) (cid:21) , (cid:20)𝑇 1 𝑡= where 𝑝(Mℎ𝑡) is the posterior distribution of after observing ℎ𝑡. This objective encourages the agent to not only maximize immediate rewards based on the current belief but also explore to gather more context about the uncertain MDP. Optimal adaptive policies naturally induce exploratory reflection behaviors, which provide additional contextual information even if the state remains identical. While reflective actions may be suboptimal relative to the (unknown) ground-truth MDP, the gathered context, especially the rewards, reduces the MDPs uncertainty. This enables future policies to leverage the updated belief to act more optimally. 1Even if we view the state as the CoT history, i.e., explorations such as 𝑠1 𝑠2 are not semantically disregarded after backtracking to 𝑠1, it still differs from ℎ𝑡 where reward histories are additionally contained. (cid:1) 5 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Comparison Between Markovian RL and Bayes-Adaptive RL Markovian RL depends on the current state, not history contexts, for decision-making. Exploration only happens during training with trial-and-error to discover return-maximizing action sequences, which are fully exploited as the optimal deterministic policy. In contrast, Bayes-Adaptive RL balances reward maximization with epistemic exploration throughout both training and testing, enabling meaningful test-time scaling with improved efficiency. Theorem 4.2. The test-time expected return of Bayes-Adaptive policy can be exponentially higher in 𝑇 than that of the optimal Markovian policy, where 𝑇 is the minimal number of steps required to reach the correct answer under an optimal deterministic policy. Sketch proof. Consider the binary search tree in Figure 1. Let 𝑠2, 𝑠3, 𝑠5, and 𝑠6 be four equally possible candidate answers at test time, i.e., the prior 𝑝(M1) = 𝑝(M2) = 𝑝(M3) = 𝑝(M4) = 1/4, where 𝑟M1 (𝑠2) = 1, 𝑟M2 (𝑠3) = 1, 𝑟M3 (𝑠5) = 1, 𝑟M4 (𝑠6) = 1, and all other rewards are zero. Here, 𝑟(𝑠) represents the reward of reaching 𝑠. For any Markovian policy, the maximal return is 1/4 (or 1/2𝑑1 for depth-𝑑 tree) since it is static and cannot adapt according to the reward feedback when reaching the four candidate answers. In contrast, the optimal Bayes-Adaptive policy has an expected return of 1. This is achieved by updating the posterior 𝑝(Mℎ𝑡) based on observation ℎ𝑡 to eliminate the hypotheses M1, M2, , until the ground-truth MDP has probability 1, in which the agent can act optimally to reach the true answer. 5. Method The policy gradient for Bayes-Adaptive RL is as follows, which differs from (3.2) by replacing the value under predefined with posterior-weighted value: 𝜃JBayes = 𝔼𝑠0,𝜋𝜃 (cid:20)𝑇 1 𝑡= 𝜃 log 𝜋𝜃(𝑎𝑡 𝑠𝑡) 𝔼M𝑝( ℎ𝑡 ) (cid:2)𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡)(cid:3) (cid:21) , (5.1) where we write the history ℎ𝑡 as 𝑠𝑡 in 𝜋 and 𝑄 since in LLM reasoning, 𝑠𝑡 already encodes the full sequence of prior states and actions, and the reward is unavailable at test time so its absorbed into 𝜃 as function of 𝑠𝑡. Here, we use the state-action value instead of the advantage since the latter requires multiple Monte Carlo rollouts at each step. By applying the Bayes rule, the posterior satisfies: 𝑝(M ℎ𝑡) = 𝑝(M 𝑠0:𝑡, 𝑎0:𝑡1, 𝑟0:𝑡1) 𝑝(M 𝑠0:𝑡) 𝑝(𝑟0:𝑡1 𝑠0:𝑡, 𝑎0:𝑡1, M), (5.2) where 𝑝(M𝑠0:𝑡) conditions only on the CoT, excluding rewards, and can be interpreted as the models probability of outputting solution 𝑦 𝑠0 . Interestingly, the second term 𝑝(𝑟0:𝑡1𝑠0:𝑡, 𝑎0:𝑡1, M) measures the likelihood of observing the rewards 𝑟0:𝑡1 under given the trajectory 𝑠0:𝑡. That is, we may write 𝑝(𝑟𝑡 𝑠𝑡, 𝑎𝑡, M) exp(𝛽𝑟𝑡 𝑟M (𝑠𝑡, 𝑎𝑡)) with hyperparameter 𝛽 to obtain 𝑝(𝑟0:𝑡1 𝑠0:𝑡, 𝑎0:𝑡1, M) 𝑡1 (cid:214) 𝑡=0 𝑝(𝑟𝑡 𝑠𝑡 , 𝑎𝑡 , M) = 𝑡1 (cid:214) 𝑡=0 exp(cid:0)𝛽(cid:12) (cid:12)𝑟𝑡 𝑟M (𝑠𝑡 , 𝑎𝑡)(cid:12) (cid:12) (cid:1), (5.3) where the proportionality holds since 𝑝(𝑟𝑡 𝑟0:𝑡 1, 𝑠0:𝑡, 𝑎0:𝑡1, M) = 𝑝(𝑟𝑡 𝑠𝑡 , 𝑎𝑡 , M). 6 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Besides, the posterior-weighted value in (5.1) satisfies 𝔼M𝑝( ℎ𝑡 ) (cid:2)𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡)(cid:3) = 𝔼M𝑞( 𝑠0 ) (cid:20) 𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡) 𝑝(M ℎ𝑡) 𝑞(M 𝑠0) (cid:21) = 𝑖= 𝑄𝜋𝜃 M𝑖 (𝑠𝑡, 𝑎𝑡) 𝑝(M𝑖 ℎ𝑡), (5.4) where the proposal 𝑞(M𝑠0) is the uniform distribution over the support of plausible MDPs, defined w.r.t. the ground-truth answer and candidate answers extracted from the models CoTs. We draw CoT rollouts of 𝜋𝜃 on prompt 𝑠0 to form {M𝑖} 𝑖=1 . The importance ratios are then self-normalized over {M𝑖} 𝑠0. Substituting (5.2) and (5.3) into the above equation and letting 𝑝(M𝑖𝑠0:𝑡) be the models state-conditional belief, we obtain: 𝑖=1 and the ground-truth MDP M0 defined w.r.t. 𝑦 𝔼M𝑝( ℎ𝑡 ) (cid:2)𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡)(cid:3) = 𝑖=0 𝑄𝜋𝜃 M𝑖 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (𝑠𝑡, 𝑎𝑡) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:125) (cid:123)(cid:122) value in M𝑖 𝜋𝜃( 𝑦 M𝑖 𝑠0 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 𝑠𝑡 + </think>) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:123)(cid:122) LLMs belief in M𝑖s plausibility 𝑡1 (cid:214) 𝑡=0 exp(cid:0)𝛽(cid:12) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:124) (cid:12)𝑟𝑡 𝑟M𝑖 (𝑠𝑡 , 𝑎𝑡 )(cid:12) (cid:12) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:123)(cid:122) consistency b/w obs. & M𝑖s pred. (cid:1) , (5.5) where 𝑟𝑡 is the actual observed reward as defined in (3.1), and 𝑟M𝑖, 𝑄𝜋𝜃 are defined in (3.3) w.r.t. the M𝑖 hypothesis MDP M𝑖. For clarity, we have omitted the normalization constant when computing 𝑝(M𝑖ℎ𝑡) from the last two terms. Algorithm 1 provides the unbatched version of the pseudocode. Algorithm 1 Bayes-Adaptive RL for LLM Reasoning (BARL) 1: Input: LLM 𝜋𝜃, prompt set {𝑠0} with ground-truth answers { 𝑦 𝑠0 2: for each prompt 𝑠0 in the training data do Sample CoTs from 𝜋𝜃 on 𝑠0. 3: Extract the candidate answers from each CoT to form candidate set { 𝑦 M𝑖 𝑠0 for each of the CoTs do }. 5: 4: } 𝑖=1 . 6: Calculate posterior-weighted value with (5.5) for each timestep 𝑡 and update 𝜃 with (5.1). BARL offers principled framework for stitching plausible strategies, analogous to linearized best-of-N reasoning, but with explicit step-level guidance on when and how LLMs should reflectively explore. Remark 5.1. BARL maximizes the weighted sum of values defined over each hypothesis MDP M𝑖. The first weighting term 𝜋𝜃( 𝑦 M𝑖 ) captures LLMs state-conditional belief in the plausibility of M𝑖. The 𝑠0 second product weighting term accumulates the discrepancy between predicted rewards 𝑟M𝑖 (𝑠𝑡 , 𝑎𝑡) and observed rewards 𝑟𝑡, which serves as reflective signal for strategy switching by downweighting hypotheses that have high belief probabilities but are unlikely to be optimal. 6. How Bayes-Adaptive RL Helps Generalization: Didactic Example In this section, we present didactic example to show how BARL facilitates test-time generalization. Consider the action space that consists of three tokens, {0, 1, 2}, with one token generated at each timestep. The state is simply 𝑠𝑡+1 = 𝑎𝑡. The objective is to repeat the prompt token three times consecutively within 29 timesteps (33 + 2 is the minimal length of sequence to include all unique triplets). The prompt token is 0 or 1 at training time, and 2 at test time. Episodes terminate when receiving 1 reward. This setup is illustrated in Figure 2. 7 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning This synthetic task mirrors LLM reasoning, where the goal is not only to learn specific strategies (here, generating particular triplets) but also to acquire general problem-solving abilities, such as when and how to switch to new strategies. These capabilities are essential for handling distribution shifts between training and evaluation, common challenge when developing effective reasoning models. We use 2-head transformer encoder followed by linear layer as the policy and train it using the policy gradient from Markovian RL (3.2) and from BARL (5.1). For Markovian RL, the value 𝑄𝜋𝜃 in the policy gradient is 1 only when 𝑠0:𝑇 1 contains the rewarding triplet argmaxtri 𝑟(tri) of the ground-truth MDP, such as 000 or 111 during training. For BARL, we set 𝛽 = so that (cid:206)𝑡1 𝑟M𝑖 (tri) 𝑠0:𝑡), i.e., the product is 0 when the rewarding triplet of M𝑖 already appears in 𝑠0:𝑡 and thus is invalidated,2 and 1 otherwise. We let the policys state-conditional belief be 𝑝(M𝑖𝑠0:𝑡) = 1 for M𝑖 whose rewarding triplet aligns with the sampled policy action 𝑎𝑡, i.e., argmaxtri 𝑟M𝑖 (tri) = 𝑠𝑡1:𝑡+1, where 𝑠𝑡+1 = 𝑎𝑡 𝜋𝜃(𝑠𝑡). Therefore, the posterior-weighted value from (5.4) becomes Figure 2 Setup: repeating the prompt token (orange) three times receives 1 reward. 𝑡=0 exp(𝛽𝑟𝑡 𝑟M𝑖 (𝑠𝑡 , 𝑎𝑡)) = 1(argmaxtri 𝔼M (cid:2)𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡)(cid:3) = 𝑖=1 𝑄𝜋𝜃 M𝑖 (𝑠𝑡, 𝑎𝑡) 𝑝(M𝑖 𝑠0:𝑡)1(cid:0)argmax tri 𝑟M𝑖 (tri) 𝑠0:𝑡(cid:1) = 1(cid:0)𝑠𝑡1:𝑡+1 {M𝑖} 𝑖= , 𝑠𝑡1:𝑡+1 𝑠0:𝑡(cid:1), where 𝑎𝑡 𝜋𝜃(𝑠𝑡). The above formulation incentivizes the policy to eliminate hypotheses and switch to new strategies (i.e., new triplets) when the current strategy has been invalidated by earlier attempts up to step 𝑡, which is illustrated in Figure 3. This form of adaptive exploration provides minimalist instantiation of BARL in synthetic settings. The difference between the above equation and (5.5) arises because the agent in this example is aware of the zero reward associated with unterminated episodes. Figure 3 Illustration of the difference between Markovian RL and BARL in this didactic example. We report the results in Figure 4, where accuracies are averaged over 50 completions and the shadow regions are the standard deviation across 3 independent model training runs. The results show that Markovian RL quickly finds and memorizes the training solutions but fails to generalize at test time. In contrast, Bayes-Adaptive RL increases both training and testing accuracies. Furthermore, its accuracy and convergence rate improve when given prior knowledge that rewarding triplets are repeated patterns, i.e., = 3 with 𝑟M1 (000) = 𝑟M2 (111) = 𝑟M3 (222) = 1 and all other rewards are zero. This highlights the advantage of more informative candidate sets, underscoring the importance of balancing the diversity and plausibility of the candidates. Specifically, they should be diverse enough to capture test-time uncertainty, yet constrained to only the most plausible candidates to shrink the hypothesis space. 2This is because 𝑟𝑡 = 0 for unterminated sequences and if argmaxtri exp(𝛽𝑟𝑡 𝑟M𝑖 (𝑠𝑡 , 𝑎𝑡 )) = 0 and the resulting accumulated product is also 0. 𝑟M𝑖 (tri) 𝑠0:𝑡, i.e., 𝑟M𝑖 (𝑠𝑡 , 𝑎𝑡 ) = 1 for some 𝑡, then 8 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Figure 4 (Left) Markovian RL (REINFORCE) memories training solutions and poorly generalizes. (Middle) BARL generalizes well at test time and (Right) improves with more informative candidate sets that strike better balance between diversity and plausibility. 7. Experiments 7.1. Experiment Setups In addition to the synthetic experiment in Section 6, we evaluate BARL on LLM math reasoning tasks. Training is conducted on the Big-Math dataset [2] across various models, including Qwen2.5-Math1.5B, Qwen2.5-Math-7B [53], and DeepSeek-R1-Distill-Llama-8B [17]. Evaluation is performed on four benchmarks: GSM8K [7], MATH [21], CollegeMath [43], and OlympiadBench [20]. During training, the maximum prompt length is set to 512 and the maximum response length is set to 1024. We provide the AIME and AMC results in Appendix B.1 and exclude them from our main evaluation due to their substantially longer context requirements, e.g., R1-Distill-Llama-8B has average response lengths of 2008 and 1886 tokens on AIME 2024 and AMC 2023, respectively. For BARL, we set 𝛽 = 1 and = 5. We compare BARL against two Markovian RL baselines that span outcome-reward and process-reward RL. For the outcome-reward GRPO baseline, we set its group size to 5 for fair comparison with BARL and, after performing grid search over the KL-divergence coefficients [0, 0.001, 0.005, 0.01], adopt 0.005 as it yields the best overall performance across all benchmarks. For the process-reward baseline, we adapt variant of MRT [37] by integrating the progress reward defined in (3.1) into the outcome reward, which we refer to as progress in the following sections. For all algorithms, we set the training and rollout batch sizes to 128 and 1024, respectively. We train the Qwen and Llama models for 110 and 60 iterations, respectively, defined w.r.t. the rollout batches. The temperature during online sampling is 1.0 and is 0.0 during evaluation. For both BARL and the progress baseline, we set the number of tokens for each reasoning step as 128. All algorithms are implemented using the OpenRLHF codebase [22]. 7.2. Experiment Results We report the pass@1 accuracies in Table 1. All models are trained using three random seeds, and we calculate the mean and standard deviation of the resulting accuracies. For each algorithm, we report the results of the checkpoint that has the highest average accuracy. It can be observed that BARL achieves higher accuracies across most benchmarks and models. It consistently outperforms Markovian RL baselines in terms of average accuracy, with larger gains observed on challenging benchmarks that demand effective exploration, such as CollegeMath and OlympiadBench. Notably, BARL delivers these gains with minimal computational overhead, which comes from calculating the probabilities of candidate answers at the end of each step, reusing the cache from CoT generations. 9 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Model Qwen2.5-Math-1.5B GRPO Progress BARL Qwen2.5-Math-7B GRPO Progress BARL R1-Distill-Llama-8B GRPO Progress BARL MATH GSM8K 34.1 40.0 71.5(0.4) 83.9(0.5) 72.3(0.4) 84.8(0.6) 86.0(0.6) 72.7(0.3) 59.1 53.7 90.3(0.1) 77.6(0.4) 78.8(0.1) 91.1(0.3) 91.7(0.2) 79.2(0.3) 82.0 85.7(0.5) 85.9(0.4) 85.4(0.6) 65.7 74.3(0.4) 73.9(0.4) 73.9(0.6) CollegeMath OlympiadBench 6.6 45.1(0.3) 45.9(0.3) 46.8(0.2) 21.9 47.0(0.3) 47.2(0.3) 47.5(0.2) 35.8 39.6(0.3) 39.8(0.5) 40.4(0.3) 21.8 33.4(0.4) 35.6(0.2) 35.8(0.4) 19.0 39.0(0.2) 41.1(0.2) 42.0(0.4) 26.5 36.0(0.5) 35.3(0.2) 37.2(0.5) Average 25.6 58.4(0.3) 59.6(0.2) 60.3(0.1) 38.4 63.5(0.2) 64.5(0.2) 65.1(0.3) 52.5 58.9(0.4) 58.7(0.4) 59.3(0.4) Table 1 Mean and standard deviation of the accuracies over three independent training runs. 7.3. Ablation Studies Token Efficiency. We evaluate the token efficiency of BARL and baseline models by measuring the total number of tokens required to solve problem. Specifically, in Figure 5, we plot the pass@k accuracies and the corresponding average numbers of tokens, serving as proxy for performance per token. In all the following ablations, we mainly analyze the models fine-tuned from Qwen2.5-Math-1.5B. We vary from 1 to 6 and set the temperature to 1.0, but observe that the GRPO and base models are less robust under sampling, resulting in decreased pass@1 performance (see Appendix B.3). To account for this, we combine greedy decoding outputs with sampled outputs when computing token usage and accuracy. We omit the base model from the plots due to its significantly higher token consumption and lower asymptotic accuracy. We find that BARL achieves higher accuracies with substantially fewer tokens, requiring up to 39% fewer average tokens than the progress baseline, 50% fewer than GRPO, and over 90% fewer than the base model. Figure 5 BARL is more token efficient, achieving higher accuracies with fewer total numbers of tokens. Reflective Reasoning Behaviors. To qualitatively assess the improved token efficiency of BARL, we analyze the frequency of reflective behaviors across problems of varying difficulty levels, as shown in Figure 6. For each problem, we sample 6 responses per model and define its difficulty level by the number of incorrect responses. We use keyword-based detections [31, 54] to identify whether self-reflections appear in response, and problem is considered to exhibit self-reflection if at least one of its responses is identified. It can be observed that both models display fewer reflections on easier problems, and the base model exhibits higher Figure 6 Results on GSM8K (dashed) and MATH (solid). 10 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning frequency of reflections despite achieving lower accuracies. This result reveals the weak correlation between the performance of LLMs and the response length or the frequency of reflections. Rather, the effectiveness of thinking tokens and the efficiency of explorations are the determining factors, which we study in the following ablation. Effectiveness of CoTs. We measure the effectiveness of the CoTs produced by different models by calculating the average Bayesian state-action values at each timestep, which naturally captures both the exploration and the exploitation aspects of the actions. Specifically, the Bayesian value is defined as 𝑄𝜋(𝑏𝑡, 𝑠𝑡, 𝑎𝑡) = 𝔼𝜋,M𝑏𝑡 [𝑟M (𝑠𝑡, 𝑎𝑡) + 𝑄𝜋(𝑏𝑡+1, 𝑠𝑡+1, 𝑎𝑡+1)], where the belief 𝑏𝑡 = 𝑝(Mℎ𝑡). Unlike standard Q-values, the Bayesian Q-value not only incorporates the expected returns (exploitation) but also captures the value of information gained through belief updates (exploration). Figure 7 Ablation on how effective the CoTs explore and exploit, measured by the Bayesian values. The results are reported in Figure 7. We observe that the actions from the BARL model exhibit consistently higher Bayesian values compared to those of the GRPO and base models, indicating more effective exploration and exploitation. On more challenging benchmarks such as OlympiadBench, exploratory gains peak midway through the CoTs after an early phase of uncertainty reduction. Moreover, the result also explains our earlier observations on token efficiency and reflective behaviors. Although the base model exhibits more self-reflections, these are likely superficial or stylistic patterns due to their low exploration efficiency for gathering informative contexts during evaluation. Markovian RL Optimality. We train length-controlled (LC) GRPO with maximum 32 response length over multiple epochs. Figure 8 shows the evolution of the training accuracy and length. The rapidly increasing accuracy and decreasing response length of GRPO LC indicate that it learns to skip CoT generation and emit only the final answer. Its asymptotic training accuracy matches that of GRPO (max length 1024). This result supports Theorem 4.1: Markovian RL can achieve optimality by merely memorizing solutions without reflective reasoning. Such policies, however, generalize poorly during evaluation. We refer readers to Appendix for additional experimental results. Figure 8 (Left) Training accuracy. (Middle) Training response length. (Right) Final evaluation results. 11 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Key Experiment Findings BARL consistently outperforms Markovian RL baselines with superior token efficiency. Performance correlates with the effectiveness of reflective explorations, rather than their frequency. Optimality in Markovian RL can be attained by policies that memorize training solutions yet fail to generalize, with no guarantees on the emergence of self-reflections. 8. Conclusion Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited emergent behaviors such as self-reflective reasoning. Yet in conventional Markovian RL, exploration is confined to the training phase to identify action sequences that maximize cumulative reward, and resorts to pure exploitation at test time. Besides, the Markov assumption indicates the dependency on history only through the state. Thus, Markovian RL neither ensures the emergence of reflective exploration nor explains its benefits during testing. We propose to fill this gap with Bayes-Adaptive RL, which explicitly considers test-time performance by maximizing the expected return under posterior of MDPs. Within this framework, we propose BARL, novel algorithm for LLM reasoning that provides principled guidance for when and how to engage in reflective exploration. BARL enables efficient exploration through hypothesis elimination and strategy switching. Our experiments are conducted on both synthetic and mathematical reasoning tasks, where we show that BARL outperforms Markovian RL algorithms at test time and its exploration is more efficient. As future work, we plan to extend our approach to broader domains, such as coding and agentic tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank Aviral Kumar, Chu-Cheng Lin, and Ziyu Ye for the insightful discussions and valuable feedback."
        },
        {
            "title": "References",
            "content": "[1] Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Self-improvement for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003, 2023. [2] Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait Singh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: large-scale, high-quality math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387, 2025. [3] Dilip Arumugam and Satinder Singh. Reducing the information horizon of bayes-adaptive markov decision processes via epistemic state abstraction. [4] Richard Bellman and Robert Kalaba. On adaptive control processes. IRE Transactions on Automatic Control, 4(2):19, 1959. [5] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Ré, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. 12 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [6] Jiayu Chen, Wentse Chen, and Jeff Schneider. Bayes adaptive monte carlo tree search for offline model-based reinforcement learning. arXiv preprint arXiv:2410.11234, 2024. [7] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [8] Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767, 2023. [9] Yan Duan, John Schulman, Xi Chen, Peter Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016. [10] Michael Duff. Monte-carlo algorithms for the improvement of finite-state stochastic controllers: Application to bayes-adaptive markov decision processes. In International Workshop on Artificial Intelligence and Statistics, pages 9397. PMLR, 2001. [11] Michael OGordon Duff. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. University of Massachusetts Amherst, 2002. [12] Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. [13] Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement learning: survey. Foundations and Trends in Machine Learning, 8(5-6):359483, 2015. [14] Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be trained to be adaptive. In International Conference on Machine Learning, pages 75137530. PMLR, 2022. [15] Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan Adams, and Sergey Levine. Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability. Advances in neural information processing systems, 34:2550225515, 2021. [16] Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using sample-based search. Advances in neural information processing systems, 25, 2012. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor. In International conference on machine learning, pages 18611870. Pmlr, 2018. [19] Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642, 2024. 13 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [20] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. [21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [22] Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143, 2024. [23] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [24] Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined credit assignment. arXiv preprint arXiv:2410.01679, 2024. [25] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199 22213, 2022. [26] Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping. arXiv preprint arXiv:2402.14083, 2024. [27] Aly Lidayan, Michael Dennis, and Stuart Russell. Bamdp shaping: unified theoretical framework for intrinsic motivation and reward shaping. arXiv preprint arXiv:2409.05358, 2024. [28] Aly Lidayan, Michael Dennis, and Stuart Russell. Bamdp shaping: unified framework for intrinsic motivation and reward shaping. In The Thirteenth International Conference on Learning Representations. [29] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [30] Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. Reason for future, act for now: principled framework for autonomous llm agents with provable sample efficiency. arXiv preprint arXiv:2309.17382, 2023. [31] Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding r1-zero-like training: critical perspective. arXiv preprint arXiv:2503.20783, 2025. [32] Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated process supervision. arXiv preprint arXiv:2406.06592, 2, 2024. 14 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [33] James John Martin. Some Bayesian decision problems in Markov chain. PhD thesis, Massachusetts Institute of Technology, 1965. [34] Andrew Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Icml, volume 99, pages 278287. Citeseer, 1999. [35] Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559, 2023. [36] Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, and Sjoerd van Steenkiste. Bayesian teaching enables probabilistic reasoning in large language models. arXiv preprint arXiv:2503.17523, 2025. [37] Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning. arXiv preprint arXiv:2503.07572, 2025. [38] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [39] Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process verifiers for llm reasoning. arXiv preprint arXiv:2410.08146, 2024. [40] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [41] Avi Singh, John Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023. [42] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. [43] Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884, 2024. [44] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [45] Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. Offline reinforcement learning for llm multi-step reasoning. arXiv preprint arXiv:2412.16145, 2024. [46] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint arXiv:2312.08935, 2023. 15 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [47] Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah Goodman. Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660, 2023. [48] Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, and Hao Wang. Blob: Bayesian low-rank adaptation by backpropagation for large language models. arXiv preprint arXiv:2406.11675, 2024. [49] Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang. Multi-step problem solving through verifier: An empirical analysis on model-induced process supervision. arXiv preprint arXiv:2402.02658, 2024. [50] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [51] Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel Fried, Gabriel Synnaeve, Rishabh Singh, and Sida Wang. Swe-rl: Advancing llm reasoning via reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449, 2025. [52] Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682, 2025. [53] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [54] Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chainof-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. [55] Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. arXiv preprint arXiv:2309.12284, 2023. [56] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [57] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language models. arXiv preprint arXiv:2308.01825, 2023. [58] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint arXiv:2309.05653, 2023. [59] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. 16 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning [60] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022. [61] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. [62] Shenao Zhang, Donghan Yu, Hiteshi Sharma, Han Zhong, Zhihan Liu, Ziyi Yang, Shuohang Wang, Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for online alignment. arXiv preprint arXiv:2405.19332, 2024. [63] Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi Liu, Sirui Zheng, Hongyi Guo, et al. Brite: Bootstrapping reinforced thinking process to enhance language model reasoning. arXiv preprint arXiv:2501.18858, 2025. A. Proof of Theorem 4. Proof. Consider full binary tree of depth 𝑇 , with leaf set of size = 2𝑇 . The states are the tree nodes, with the initial state fixed as the root node. The actions include the moves from the parent nodes to the child nodes as well as resetting action from the leaf node to the root node. The rewards are not known and differ in different MDP hypotheses. Specifically, the reward is defined as 𝑟M𝑖 (𝑠) = 1(𝑠 = 𝑠𝑖), where 𝑠𝑖 is unique leaf node for M𝑖. The prior of MDPs is 𝑝(M𝑖) = 1/L, where 𝑖 = 1, , L. The cumulative return is undiscounted with the minimum traverse steps as the horizon 𝑇. The episode terminates once the agent receives 1 reward. For any Markovian policy 𝜋, define 𝑓 (𝑠) = 𝑝(𝑡 0 : 𝑠𝑡 = 𝑠 𝜋) and denote 𝑝𝑠 as the probability that 𝜋 goes left at an internal node 𝑠. For any left and right children 𝑠𝐿 and 𝑠𝑅 of node 𝑠, it holds that 𝑓 (𝑠𝐿) = 𝑓 (𝑠) 𝑝𝑠 and 𝑓 (𝑠𝑅) = 𝑓 (𝑠)(1 𝑝𝑠). Thus, 𝑓 (𝑠𝐿) + 𝑓 (𝑠𝑅) = 𝑓 (𝑠). By an induction on depth, it follows that at every depth 𝑑, (cid:205)𝑠:depth(𝑠)=𝑑 𝑓 (𝑠) = 1. In particular, (cid:205)𝑙 𝑓 (𝑙) = 1. Since the total return under M𝑖 is 𝑉 (𝜋 M𝑖) = 𝑝(𝜋 ends at leaf 𝑙𝑖) = 𝑓 (𝑙𝑖), the expected return for the optimal Markovian policy is 1 𝑖=1 𝑉 (𝜋 M𝑖) = 1 2𝑇 𝑙 𝑓 (𝑙) = 1 2𝑇 . Consider the following deterministic Bayes-adaptive policy. At the root node, the policy picks any leaf 𝑙 with positive posterior, i.e., 𝑝(M𝑙 ℎ𝑡) > 0, then follows the unique shortest path to 𝑙. Two possible outcomes can occur: if the ground-truth reward 𝑟(𝑙) = 1, then the episodes terminates with the collected reward; if 𝑟(𝑙) = 0, then the agent eliminates the hypothesis M𝑙 from the posterior by setting 𝑝(M𝑙 ℎ𝑡:𝑡+𝑇 ) = 0, and returns to the root to repeat the process on the remaining leaves. By construction, the expected return of this Bayes-Adaptive policy is 1, which is an exponential improvement in 𝑇 over the 1/2𝑇 return of the optimal Markovian policy. B. Experiment Details B.1. Evaluation Results: Accuracies In this section, we provide the evaluation accuracy results for Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and DeepSeek-R1-Distill-Llama-8B in Figure 9, 10, and 11, respectively. In addition to the benchmark 17 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning scores in Table 1, we also report the performance of the models on AIME 2024 and AMC 2023. It can be observed that BARL outperforms Markovian RL baselines in terms of both accuracy and convergence rate on most of the reported benchmarks. Again, the performance on AIME and AMC benchmarks may be further enhanced by choosing harder training data with increased response length, especially for R1-Distill-Llama-8B fine-tuned models whose initial average response lengths on AIME and AMC (2008 and 1886, respectively) exceed the maximum training length (1024). Figure 9 Average evaluation accuracies over training iterations for Qwen2.5-Math-1.5B models. Figure 10 Average evaluation accuracies over training iterations for Qwen2.5-Math-7B models. 18 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Figure 11 Average evaluation accuracies over training iterations for R1-Distill-Llama-8B models. B.2. Evaluation Results: Response Lengths In this section, we present the evolution of evaluation response lengths over training iterations for Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and DeepSeek-R1-Distill-Llama-8B, shown in Figures 12, 13, and 14, respectively. Across most benchmarks, response lengths tend to decrease as training progresses for all algorithms. The response length during training has very similar trend to that during evaluation. This trend arises because all three models exhibit reflective behaviors, such as self-evaluation and backtracking, that introduce redundant tokens and lengthen responses. As shown in Figure 7, these behaviors are likely superficial or stylistic patterns with limited effectiveness. An exception is AIME, where some models maintain consistently long responses due to the benchmarks intrinsic requirement for extended reasoning, even under optimal non-reflective policies. Figure 12 Average evaluation response lengths over training iterations for Qwen2.5-Math-1.5B models. 19 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Figure 13 Average evaluation response lengths over training iterations for Qwen2.5-Math-7B models. Figure 14 Average evaluation response lengths over training iterations for R1-Distill-Llama-8B models. 20 Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning Since the models used in the main experiments already exhibit lengthy CoTs with reflective patterns, we further implement BARL on the Llama-3.2-3B-Instruct model, which displays fewer self-reflections. The results are presented in Figure 15. Initially, the response length decreases as the base model tends to produce excessively long reasoning traces, often exceeding ten steps, which are pruned during early training. Subsequently, the response length increases, as result of plausible strategy stitching. GSM8K MATH CollegeMath Olympiad AIME 24 AMC 23 Base 69.4 47.2 30.8 16.4 3.3 27. BARL 73.8 50.5 33.6 19.1 13.3 35.0 Figure 15 Results of BARL fine-tuned on Llama-3.2-3B-Instruct. (Left) Training accuracy and (Middle) response length. (Right) Evaluation results. B.3. Token Efficiency without Greedy Decoding Outputs In Figure 16, we present the pass@k accuracies from an ablation similar to Section 7.3, except that greedy decoding outputs are excluded when computing token counts and accuracies. We observe that the base and GRPO models are less robust under sampling temperature of 1.0, resulting in significantly lower pass@1 accuracies compared to greedy decoding. This degradation may stem from the fragility of their CoTs, which often exhibit stylistic but unproductive self-reflection and backtracking behaviors. Figure 16 Ablation on token efficiency and pass@k accuracies with sampling temperature= 1. GRPO and the base models are less robust to temperatures. B.4. Some Unsuccessful Attempts As straightforward implementation of the Bayes-Adaptive RL policy gradient in (5.1), we explored using value ensembles to estimate the posterior-weighted value 𝔼M𝑝( ℎ𝑡 ) [𝑄𝜋𝜃 (𝑠𝑡, 𝑎𝑡)]. Specifically, we trained an ensemble of state-action value functions to capture epistemic uncertainty. We experimented with two approaches to constructing the ensemble: (1) fine-tuning multiple linear value heads on disjoint data subsets, each paired with chain-of-thought (CoT) trajectories and outcome rewards; and (2) applying Bayesian LoRA in way similar to [48]. However, both methods failed to effectively capture epistemic uncertainty, likely because different ensembles still share large fraction of parameters. While maintaining independent value models may better capture this uncertainty, doing so incurs substantial computational cost. We leave the development of more efficient implementations to future work."
        }
    ],
    "affiliations": [
        "Google",
        "Google DeepMind",
        "Northwestern University"
    ]
}