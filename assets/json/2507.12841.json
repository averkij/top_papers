{
    "paper_title": "AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning",
    "authors": [
        "Yiming Ren",
        "Zhiqiang Lin",
        "Yu Li",
        "Gao Meng",
        "Weiyun Wang",
        "Junjie Wang",
        "Zicheng Lin",
        "Jifeng Dai",
        "Yujiu Yang",
        "Wenhai Wang",
        "Ruihang Chu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\\'s content scores by 45\\% and style scores by 12\\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 1 4 8 2 1 . 7 0 5 2 : r AnyCap Project: Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning Yiming Ren1,2 Zhiqiang Lin1 Yu Li1 Gao Meng1 Weiyun Wang2,3 Junjie Wang1 Zicheng Lin1 Jifeng Dai1,2 Yujiu Yang1 Wenhai Wang2,4 Ruihang Chu1 1Tsinghua University 2Shanghai AI Laboratory 3Fudan University 4The Chinese University of Hong Kong https://github.com/qishisuren123/AnyCap"
        },
        {
            "title": "Abstract",
            "content": "Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To comprehensively address this gap, we introduce the AnyCap Project, developing an integrated solution spanning model, dataset, and evaluation. Specifically, we propose AnyCapModel (ACM), lightweight plugand-play framework that enhances the controllability of existing foundation models for omni-modal captioning, avoiding the training of base model. ACM leverages the original caption from base models, while incorporating user instructions and modality features to produce improved captions. In addition, to make up for the data deficiencies of controllable multimodal captioning, we construct AnyCapDataset (ACD), which covers 3 modalities, 28 types of user instructions, and 300k high-quality data entries. We further propose AnyCapEval, novel benchmark that offers more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM significantly improves caption quality across diverse set of base models on AnyCapEval. Notably, ACM-8B boosts GPT-4os content scores by 45% and style scores by 12%. It also shows substantial gains on widely-used benchmarks such as MIA-Bench and VidCapBench."
        },
        {
            "title": "Introduction",
            "content": "Multimodal large language models (MLLMs) are rapidly evolving towards omni-modal intelligence, processing diverse modalities including images, videos, and audio. Textual captions serve as universal interface linking these modalities, benefiting numerous tasks such as retrieval, question answering, and content generation [1, 2, 3, 4, 5, 6]. Recently, controllability in captioning has garnered increasing attention. It refers to the ability to generate captions that precisely follow user instructions, such as emphasizing specific aspects or adopting designated stylistic expressions. Such control is crucial for tailoring captions to user needs and enhancing downstream tasks [7, 8, 9]. Yet, achieving precise and reliable omni-modal controllable captioning remains significant challenge, largely constrained by limitations in current framework, data, and evaluation approaches [10, 11, 12, 13, 14]. Controllable captioning across diverse modalities faces three main challenges: (1) Limited controllability capabilities of open-source models. Existing caption models typically rely on rigid control signals, such as soft prompts [13] or bounding boxes [14, 15], to adjust the fine-granularity of the caption. While this allows certain control, it limits flexibility and reduces the variety of generated Equal contribution. Corresponding authors: ruihangchu@mail.tsinghua.edu.cn, wangwenhai@pjlab.org.cn. Preprint. Under review. Figure 1: The overall framework of ACM. Given image, video, and audio inputs, base captioning models first generate initial outputs. ACM refines these outputs by aligning modality features with specific user instructions, enabling controllable captioning without retraining the base models. results. Natural language prompts provide more flexible way to guide captions, yet current methods often perform well only on specific aspects (e.g., detailed descriptions) due to limited and narrow training data. Retraining these models to support broader control is expensive and may weaken their overall language ability. (2) Lack of controllable omni-modal caption datasets. Manual annotation for controllable captioning is costly and labor-intensive. Meanwhile, due to the lack of low-cost and quality-assured data generation pipelines, generating large-scale data using models remains challenging. (3) Lack of appropriate evaluation benchmarks and metrics. Classic metrics (e.g., BLEU [16], CIDEr [17]) ignore content accuracy and stylistic adherence, whereas LLM-based scorers suffer from high variance, stylistic bias, and poor diagnostic power [18, 19, 20, 21, 22, 23, 24, 25]. To address these challenges, we propose AnyCapModel (ACM), plug-and-play framework that enhances fine-grained controllability in omni-modal caption generation with frozen base models. By aligning user instructions, modality features, and initial captions, ACM produces improved captions to meet diverse control demands  (Fig. 1)  . Without expensive training of base models, it offers general and low-cost path toward controlled multimodal captioning. For the lack of training data for controllable captioning, we introduce AnyCapDataset (ACD), largescale omni-modal dataset spanning images, videos, and audio across 28 distinct control dimensions. Each sample includes customized control signals and pair of captions, one preferred and one rejected, clearly reflecting differences in control quality. Crucially, this preference-based design only requires that chosen caption outperform their rejected one, largely lowering annotation difficulty. This design supports learning precise alignment with diverse user preferences, which is directly compatible with our training paradigm and applicable to other preference-based methods [26, 27, 28, 29, 30, 31]. To enable more reliable assessment for controllable captioning, we design AnyCapEval, benchmark that assesses content and style dimensions separately. We propose Keypoint Density (KPD) and detailed scoring criteria, providing more objective and precise evaluation metrics compared to existing methods. Specifically, the benchmark evaluates both Content that focuses on adherence to control instructions, and Style that captures expressive fidelity and hallucination reduction. This two-dimensional evaluation gives rise to improved accuracy and reduced variance in assessment. Extensive experiments demonstrate that ACM significantly improves controllability across image, video, and audio modalities, benefiting both proprietary models and various open-source alternatives. Specifically, integrating ACM-2B with GPT-4o [1] improves content controllability by 37% and style controllability by 8%. Moreover, the larger ACM-8B empowers certain open-source models, such as InternVL2.5-8B [32], to match or even surpass proprietary GPT-4o across multiple control dimensions. 2 It consistently shows improvements on widely-used benchmarks including MIA-Bench [22] and VidCapBench [23]. In summary, our main contributions are: (1) We introduce ACM, versatile plug-and-play framework that boosts caption controllability for frozen base models across image, video, and audio modalities. Given the scarcity of controllable caption data, we construct ACD, large-scale omni-modal dataset with preference pairs tailored for controllable captioning, to the best of our knowledge. (2) We propose AnyCapEval, novel evaluation benchmark designed for controllable captioning. It enhances assessment reliability by providing both content and style related metrics. (3) We provide extensive experiments to show that ACM greatly improves controllability and reduces hallucinations across diverse modalities and models, achieving significant gains on both AnyCapEval and the widely-used benchmarks."
        },
        {
            "title": "2 Related Work",
            "content": "Controllable captioning. Controllable captioning conditions text generation on user-instructions to dictate content and style. Early efforts in this field primarily focused on incorporating predefined and often low-level control signals. For example, researchers explored controlling caption generation based on explicit properties of the output text, such as desired caption length [11], or by conditioning on specific visual information from the image [33, 34]. This included leveraging particular visual regions or objects, sometimes specified via bounding boxes [7], or incorporating specific aspects for video captioning [35, 36]. These methods, while pioneering, lacked the expressiveness and intuitiveness of natural-language instructions that align closely with human preferences. More recent efforts leveraging MLLMs have significantly advanced image captioning, yet still often produce global and modality-specific captions, limiting their applicability to omni-modal settings [8, 9, 12, 13, 37, 38, 39, 40, 41, 42, 43, 44]. In contrast, our ACM supports flexible, language-based controls across images, videos, and audio, which can be easily extended to additional modalities. Captioning benchmarks. Existing captioning benchmarks are inadequate for precisely evaluating caption controllability. Metrics originally developed for machine translation, such as BLEU [16] and CIDEr [17], poorly reflect semantic accuracy and controllability. Critically, their reliance on n-gram overlap fails to verify adherence to specific control signals, focusing instead on general similarity. While evaluations leveraging large language models (LLMs) can offer some insight into semantic coherence [18, 22, 45, 46, 47, 48, 49, 50, 51], they often tend to be unreliable due to significant scoring variability and potential biases [52, 53]. Similarly, evaluating adherence to fine-grained control using LLMs is often subjective and inconsistent. To address these limitations, we introduce more fine-grained benchmark that employs keypoint density (KPD) and detailed scoring criteria, rigorously assessing controllability from both content and style dimensions."
        },
        {
            "title": "3 AnyCap Project",
            "content": "In Sec. 3.1, we first propose AnyCapModel (ACM), plug-and-play approach to help existing foundation models with caption controllability without fine-tuning. In Sec. 3.2, we present AnyCapDataset (ACD), comprehensive dataset featuring rich diversity of controllable captions. Finally, in Sec. 3.3, we detail the AnyCapEval benchmark for caption evaluation of control accuracy and hallucination."
        },
        {
            "title": "3.1 AnyCapModel",
            "content": "Workflow. As illustrated in Fig. 2(a), given modality input from possible modalities such as image (mimg), video (mvid), or audio (maud), alongside user-specified instruction q, an initial caption y0 is first generated by frozen base model Mg. Subsequently, our alignment framework ACM (Ma) refines this caption: yc = Ma(m, q, y0). (1) As shown in Fig. 2(b), modality-specific encoders (e.g., InternViT [32] for mimg and mvid, and EAT [54] for maud) extract feature embeddings from respective modality inputs. These modality embeddings are projected into shared semantic space via modality-specific linear transformations (MLPs). Concurrently, and y0 are tokenized and embedded into textual embeddings. Finally, 3 Figure 2: The workflow of AnyCapModel. (a) Integrating ACM into frozen base models enhances caption controllability across multiple modalities. (b) ACM aligns modality features, initial captions, and user instructions to produce refined, instruction-compliant captions. modality and textual embeddings are concatenated and input into an autoregressive language model to produce refined, instruction-compliant caption yc. Training strategy. We adopt the residual correction training strategy [55]. This approach focuses on refining existing captions rather than generating perfectly accurate ones from scratch. To effectively train the model, ACM learns how to correct uncontrolled or hallucinated outputs by contrasting them with controlled, accurate examples. During training, we include approximately 40% of data samples where initial captions already match desired controlled captions or are factually correct. This inclusion explicitly guides the model in recognizing instances requiring minimal or no correction, thereby enhancing its overall alignment capability. Training setting. We employ the AdamW optimizer for 3 epochs with learning rate of 1 106, cosine learning rate schedule (0.03 warmup ratio), weight decay of 0.01, and global batch size of 256, using bfloat16 mixed-precision. During ACM training, the external modality backbones are frozen, while the ACMs internal components including its language model are updated."
        },
        {
            "title": "3.2 AnyCapDataset",
            "content": "Definition. This dataset is constructed to train omni-modal systems that generate descriptions under explicit user instruction. ACD adopts carefully designed triplet structure: DACD := {(qi, ci, ai)}N where qi denotes an instruction that specifies the required properties of caption. For each instance, ci is high-quality caption adhering to the instruction, while ai is suboptimal caption that may exhibit minor deficiencies in factual accuracy, level of detail, or compliance with the instruction. i=1 , (2) Instruction types. The instruction types (shown in Table 1) are systematically identified by surveying caption-related literature to find common control categories and analyzing typical requirements of downstream applications. This systematic process allows us to curate set of instruction types aiming for balanced diversity and practicality, facilitating precise control over caption generation. Construction pipeline. For each multimodal sample, user instruction and related compliant caption are generated in single step. For each instruction type, we design detailed prompt including: (1) task description, (2) multimodal input, (3) original reference title if available, and (4) several high-quality examples that illustrate the desired content or style constraints. These prompts guide the model to produce well-aligned (q, c) pairs. Further details can be found in the Appendix A. We primarily use open-source models like InternVL2.5 [32], and apply proprietary models only for more complex cases such as perspective, balancing quality and cost. Before large-scale generation, 4 Table 1: Supported instruction types across image, video, and audio modalities in AnyCapDataset. Checkmarks () indicate where each control is applicable. Content controls regulate what is said, whereas Style controls regulate how it is articulated. Instruction Type Description Image Video Audio Content Background (Bkg) Event (Evt) Instance (Ins) Instance Action (IAct) Instance Appearance (IApp) Instance Position (IPos) Movement (Mov) Perspective (Per) Region (Reg) Provide or suppress scenebackground details Require mention of an event or temporal change Emphasise or ignore specific entities; enable inter-entity comparison Describe the motion state of designated instance Characterise the visual attributes of designated instance Specify spatial position of an instance within the scene Specify camera motion type (e.g., pan, zoom, dolly) Require particular viewpoint of an object/person Restrict description to specified image/video region Style Brief (Brf) Detail (Det) Genre (Gen) Length (Len) Theme (Thm) Produce concise rendition with minimal elaboration Regulate the required level of descriptive granularity Adopt literary form (e.g., poem (Poe), narrative (Nar)) Constrain caption size (words or sentences) Conform to designated linguistic style each prompt goes through strict validation phase: we sample about 20 instances per modality and instruction type, and manually verify that the captions strictly follow the user instruction, contain no hallucinations, meet formatting requirements, and preserve the inputs unique features. Only prompts with 100% pass rate are used for batch generation. To generate suboptimal captions a, we either remove guidance or add controlled degradation instruction (e.g., missing details, factual errors, or style violations) based on the given (q, c) pair. Additionally, after data generation, random 5% sample underwent manual review, finding over 95% compliance with human preferences. Significant human and temporal resources are dedicated to establishing scalable and cost-effective pipeline. This investment has yielded robust dataset foundation, substantially reducing costs for subsequent research and enabling the low-cost, large-scale generation of high-quality caption data. Statistics. Following the construction pipeline described above, ACD ultimately comprises approximately 300k (q, c, a) triplets, derived and augmented from around 75k original samples. These original samples are collected from multiple public datasets [56, 57, 58, 59, 60, 61]. ACD spans three modalities: image (125k samples), video (100k samples), and audio (75k samples). The average length of optimal captions is 62 words, while suboptimal captions average 57 words. Further details can be found in the Appendix A."
        },
        {
            "title": "3.3 AnyCapEval",
            "content": "We introduce AnyCapEval, benchmark designed for systematic evaluation of controllability in omni-modal caption generation. Control requirements include two types. (1) Content assesses whether captions accurately follow control conditions and maintain semantic relevance. (2) Style evaluates consistency with human references in narrative manner, structure, and fluency. Content evaluation. Given reference caption and user instruction q, we first annotate key-point set Kr,q = {k1, k2, . . . , kn} that exhaustively covers the information required by q. The candidate caption is denoted as ˆy. An automatic matcher based on GPT-4o [1] identifies the subset of key points present in ˆy (Fig. 3(a) presents illustrative examples). To normalise for verbosity, we introduce the key-point density (KPD): KPD(ˆy; q) = Nmatch Lwords(ˆy) 100, (3) where Nmatch denotes the count of key points from Kr,q present in ˆy, and Lwords(ˆy)is the total number of words in ˆy. KPD quantifies the effective information ratematched key points per 100 words. 5 Figure 3: Evaluation methodology of AnyCapEval. (a) Examples demonstrating content evaluation via Keypoint Density (KPD) and style scoring rules. (b) KPD correlation analysis showing superior reliability. (c) Radar chart depicting performance gains from ACM integration. While naive alternative might simply count all extracted information points, KPD offers more robust measure of effective information rate. Empirically, the Pearson correlation between this raw count (Rall) and human-judged relevance is weak, whereas the correlation for KPD in Eq. (3) is much stronger, as shown in Fig. 3(b). Length alone (Lwords) already captures noise, so density provides more stable, scale-invariant signal. Further details can be found in the Appendix B. Style evaluation. For style evaluation, GPT-4o compares the candidate caption ˆy with the reference under the user instruction and assigns discrete score s(ˆy) {0, . . . , 4}. The scoring criteria are carefully defined as follows: (0) severely deviates from or largely hallucinated/incorrect; (1) deviates from or contains many hallucinations; (2) slightly deviates from or contains some hallucination; (3) highly similar to and hallucination-free; (4) outperforms while remaining hallucination-free. (see Fig. 3(a) for examples). This fine-grained scoring approach, guided by detailed, structured prompt, supplies GPT-4o with the candidate ˆy, reference r, user instruction q, and explicit criteria along three axessemantic similarity, hallucination severity, and stylistic conformityfor each level 0 4. This tight rubric sharply reduces evaluation variance and subjective bias. Further details can be found in the Appendix B."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "We train the AnyCapModel (ACM) framework in two variants (2B and 8B) on our proposed AnyCapDataset (ACD), which comprises approximately 125K images, 100K videos, and 50K audio samples. Training is conducted using 32 NVIDIA A100 (80GB) GPUs, requiring just 6 hours for the 2B model and 21 hours for the 8B model. More training details are provided in the Appendix C."
        },
        {
            "title": "4.2 Evaluation on Controllable Caption",
            "content": "We evaluate the impact of integrating ACM with various pretrained base models (proprietary models, e.g., GPT-4o [1]; open-source models, e.g., InternVL [32], Qwen2.5-VL [62], MiniCPM-o [63]) using our AnyCapEval benchmark across the image, video and audio. As shown in Tables 2 to 4, adding ACM leads to robust improvements in controllability across all modalities and base models, 6 Table 2: Results of controllable image captioning on AnyCapEval. Abbreviations like IPos. are different instruction types in Table 1. Content and style values are computed using KPD and style scores from Sec. 3.3, respectively. ACM significantly improves content accuracy and stylistic fidelity. Model GPT-4o +ACM-2B +ACM-8B Content Style IPos. IApp. Ins. Per. Avg. Brf. Det. Thm. Poe. Nar. Avg. Proprietary Models 2.94 2.89 3.84 3.25 1.55 3.01(+1.46) 4.32(+1.07) 4.55(+0.71) 4.57(+1.62) 4.11(+1.22) 2.29(+0.38) 1.98(+0.12) 2.38(-0.21) 3.02(+0.29) 2.78(+0.20) 2.46(+0.19) 3.48(+1.93) 4.81(+1.56) 4.89(+1.06) 5.00(+2.05) 4.54(+1.65) 2.38(+0.47) 2.35(+0.49) 2.82(+0.24) 3.10(+0.37) 2.87(+0.28) 2.65(+0.39) 1. 2.59 1.91 2.74 2.59 2.26 InternVL2.5-8B 1.51 +ACM-2B +ACM-8B Qwen2.5-VL-7B 1.51 +ACM-2B +ACM-8B Open-source Models 3.43 4.54 2.68 3.04 2. 1.92 1.94 2.08 2.52 2.12 3.22(+1.71) 4.49(+1.06) 4.82(+0.28) 4.00(+1.32) 4.13(+1.09) 2.27(+0.14) 1.78(-0.14) 2.38(+0.44) 2.71(+0.63) 2.67(+0.15) 2.36(+0.24) 3.41(+1.90) 4.82(+1.39) 4.91(+0.37) 3.89(+1.21) 4.26(+1.22) 2.33(+0.20) 2.10(+0.18) 2.56(+0.62) 2.59(+0.51) 2.83(+0.31) 2.46(+0.34) 3.52 5.18 3.03 3.31 2.00 2. 2.50 2.14 2.63 2.22 3.38(+1.87) 4.42(+0.90) 4.94(-0.24) 4.68(+1.65) 4.36(+1.05) 2.40(+0.40) 2.04(+0.02) 2.50(+0.00) 2.94(+0.80) 2.72(+0.09) 2.50(+0.28) 3.47(+1.96) 4.45(+0.93) 5.84(+0.66) 4.34(+1.32) 4.53(+1.22) 2.36(+0.36) 2.20(+0.18) 2.82(+0.32) 2.88(+0.74) 2.74(+0.11) 2.56(+0.34) Table 3: Main results of controllable video captioning on AnyCapEval. (See Appendix for details) ACM substantially enhances content adherence and style fidelity, particularly in complex video tasks. Model GPT-4o +ACM-2B +ACM-8B Content Style IPos. IApp. IAct. Mov. Evt. Avg. Brf. Det. Poe. Nar. Avg. Proprietary Models 3.86 4. 2.41 3.45(+1.04) 6.15(+2.15) 6.26(+2.40) 6.53(+3.83) 4.60(+1.57) 5.30(+1.75) 1.97(+0.50) 1.78(+0.26) 2.60(+0.08) 2.65(+0.17) 2.30(+0.15) 4.92(+2.51) 6.60(+2.60) 7.68(+3.82) 5.67(+2.97) 4.81(+1.78) 5.74(+2.19) 1.95(+0.48) 1.82(+0.30) 2.50(-0.02) 2.77(+0.29) 2.32(+0.17) 3.03 2.48 2.15 3.55 1. 1.52 2.52 2.70 InternVL2.5-8B 3.08 +ACM-2B +ACM-8B Qwen2.5-VL-7B 2.88 +ACM-2B +ACM-8B 4.33 3.44 3.16 2.55 3.52 1. 1.77 1.91 2.34 1.93 Open-source Models 4.67(+1.59) 6.32(+1.99) 3.19(-0.25) 6.49(+3.33) 4.50(+1.95) 5.07(+1.55) 1.84(+0.45) 1.80(+0.03) 2.48(+0.57) 2.39(+0.05) 2.19(+0.26) 4.95(+1.87) 6.39(+2.06) 7.03(+3.59) 5.66(+2.50) 4.95(+2.40) 5.73(+2.21) 2.00(+0.61) 1.88(+0.11) 2.20(+0.29) 2.61(+0.27) 2.24(+0.31) 4.20 3.29 2.24 2.93 3.54 1. 1.65 2.40 2.29 2.10 3.39(+0.51) 5.97(+1.77) 6.23(+2.94) 6.30(+4.06) 4.48(+1.55) 5.25(+1.71) 1.66(+0.03) 1.82(+0.17) 2.30(-0.10) 2.55(+0.26) 2.16(+0.06) 4.03(+1.15) 6.15(+1.95) 6.58(+3.29) 6.04(+3.80) 4.82(+1.89) 5.55(+2.01) 1.92(+0.29) 1.98(+0.33) 2.45(+0.05) 2.55(+0.26) 2.31(+0.21) particularly enhancing content fidelity. Fig. 3(c) visualizes these gains for GPT-4o and InternVL2.58B. ACM also demonstrates notable strength in improving control for video captioning tasks. Model-centric analysis. The results consistently demonstrate the effectiveness of ACM. Both the 2B and 8B variants significantly improve controllability scores over the unassisted base models across all modalities. The larger ACM-8B generally yields greater improvements. Furthermore, integrating ACM substantially elevates the performance of open-source models. With the ACM-8B, several open-source models achieve controllability scores on AnyCapEval that are comparable or superior to strong unassisted proprietary baselines (e.g. ACM-8B elevates the performance of InternVL2.5, an open-source model with initially modest results in the image style category, from 2.1 to 2.5. This enhanced score notably surpasses the 2.3 achieved by GPT-4o in the same category). Task-centric analysis. Content-related improvements consistently exceed those on style, likely because content controls offer more explicit learning signals, whereas stylistic constraints are inherently subjective and harder to enforce. Visual tasks outperform audio counterparts, potentially due to the richer supervision and higher information density in vision-language pretraining. ACM exhibits variable impact across settings: in high-resource dimensions (e.g., image appearance), it refines already strong captions; in low-resource, complex scenarios (e.g., video actions or audio events), it must synthesize missing content, yielding larger relative gains. Rare minor regressions on specific metrics are attributed to inherent optimization trade-offs in balancing multifaceted controllability. 7 Table 4: Results on audio captioning using AnyCapEval. The table shows that ACM consistently boosts content and style metrics, demonstrating its strong effectiveness and adaptability even in lowresource audio modality environments. Model Content Style Evt. Brf. Nar. Poe. Avg. Table 5: Comparison across Training Methods. Base Model SFT DPO SC ACM InternVL2.5-2B -2.8% +7.0% +2.8% +16.9% Table 6: Training Data Ratios Comparison. Base Model 1:1:1 1:2:2 2:1: 2:2:1 InternVL2.5-8B +17.9% +17.2% +16.5% +21.1% Proprietary Models 1.42 1.24 1.59 0. 1.18 GPT-4o +ACM-2B 1.79 (+0.20) 1.44 (+0.02) 1.38 (+0.14) 1.00 (+0.12) 1.28 (+0.10) +ACM-8B 1.88 (+0.29) 1.42 (+0.00) 1.40 (+0.16) 1.08 (+0.20) 1.30 (+0.12) GPT-4o mini1.28 +ACM-2B 1.56 (+0.28) 1.21 (+0.04) 1.25 (+0.09) 0.89 (+0.10) 1.11 (+0.07) +ACM-8B 1.71 (+0.43) 1.10 (-0.07) 1.18 (+0.02) 0.87 (+0.08) 1.05 (+0.01) 1.17 0.79 1.16 1. Open-source Models MiniCPM-o 1.42 +ACM-2B 1.55 (+0.13) 1.37 (+0.35) 1.32 (+0.64) 0.92 (+0.49) 1.21 (+0.50) +ACM-8B 1.48 (+0.06) 1.23 (+0.21) 1.29 (+0.61) 1.13 (+0.70) 1.22 (+0.51) 1.02 0.68 0.43 0. Figure 4: Performance across Data Ratios. Table 7: Performance on MIA-Bench. Abbreviation: desc.(description), ment.(mention), len.(length limit), persp.(perspective), gen.(genre), avg.(average). Table 8: Performance on VidCapBench. Abbreviation: acc.(accuracy), pre.(precision), cov.(coverage), con.(conciseness). Model desc. ment. len. persp. gen. avg. Model acc. pre. cov. con. GPT-4o +ACM-2B +ACM-8B InternVL2.5-8B 85.2 +ACM-2B +ACM-8B Yi-VL-34B +ACM-2B +ACM-8B 87.9 90. 90.3 89.1 (-1.2) 88.9 (+1.0) 90.3 (-0.2) 90.0 (+4.8) 92.2 (+0.6) 90.1 (+1.0) 90.4 (+0.1) 89.4 (+1.5) 89.9 (-0.6) 89.2 (+4.0) 92.9 (+1.3) 90.3 (+1.2) 91.6 89.1 85.2 79.0 82. 72.5 85.2 80.9 85.7 (+0.5) 79.6 (+0.6) 85.2 (+2.6) 77.5 (+5.0) 85.7 (+0.5) 82.7 (+1.8) 86.9 (+1.7) 79.0 (+0.0) 86.1 (+3.5) 88.9 (+16.4) 86.3 (+1.1) 85.4 (+4.5) 71.7 71.8 (+0.1) 54.2 (+2.2) 61.9 (+6.1) 61.7 (+1.7) 61.0 (+1.2) 62.1 (+2.2) 79.5 (+7.8) 66.5 (+14.5) 67.1 (+11.3) 63.0 (+3.0) 75.8 (+16.0) 70.4 (+10.5) 52.0 55. 60.0 59.8 59.9 15.6 15.4 (-0.2) 15.8 (+0.2) GPT-4o +ACM-2B +ACM-8B InternVL2.5-8B 12.8 +ACM-2B +ACM-8B Qwen2-VL-7B 13.1 +ACM-2B +ACM-8B 13.8 (+1.0) 14.8 (+2.0) 14.6 (+1.5) 15.4 (+2.3) 58.9 59.1 (+0.2) 59.5 (+0.6) 51.6 55.7 (+4.1) 57.1 (+5.5) 53.5 55.8 (+2.3) 57.3 (+3.8) 87.8 87.1 (-0.7) 88.0 (+0.2) 84.2 84.7 (+0.5) 86.9 (+2.7) 82.9 85.0 (+2.1) 86.7 (+3.8) 6.3 10.3 (+4.0) 9.3 (+3.0) 7.1 10.1 (+3.0) 10.2 (+3.1) 6.6 10.6 (+4.0) 10.0 (+3.4)"
        },
        {
            "title": "4.3 Evaluation on Public Benchmarks",
            "content": "To validate the generalizability of ACM, we evaluate its performance when integrated with various base models on standard public benchmarks for image, video, and audio captioning. Results show that ACM serves consistently as an effective plug-in, improving controllability and factual alignment. Image. We report performance on MIA-Bench [22] in Table 7, which focuses on multi-instruction image captioning. Incorporation of ACM consistently improves performance across various backbones, including InternVL2.5 and Yi-VL [64], with particularly notable gains in control-related aspects. With the addition of ACM-8B, we observe significant performance improvements. Moreover, proprietary models such as GPT-4o also benefit from ACM, achieving new state-of-the-art scores, highlighting its plug-and-play potential. Video. As shown in Table 8, we evaluate video captioning performance using VidCapBench [23], which evaluates not only caption fluency but also key text-to-video attributes such as video aesthetics, content, motion, and adherence to physical laws. Table 8 shows that augmenting InternVL2.5-8B with ACM significantly improves accuracy (+2.0), precision (+5.5), and conciseness (+3.1). These improvements suggest that ACM not only corrects hallucinations but also improves the captions capture of crucial video details. Audio. Controllable audio captioning is less explored. On existing benchmarks [61, 65]  (Table 10)  , despite evaluation metrics being unable to reliably capture semantic-level consistency and the references being relatively short, ACM consistently boosts generation quality, indicating that ACM improves the semantical controllability and fluency of audio captioning. 8 Table 9: Video generation evaluation. Abbreviations: VQ (Visual Quality), TC (Temporal Consistency), DD (Dynamic Degree), TVA (TextVideo Alignment), and FC (Factual Consistency). Table 10: Audio evaluation results on the AudioCaps and Clotho datasets. Abbreviations: SP(SPICE), SB(Sentence-BERT). Model VQ TC DD TVA FC Avg Model AudioCaps Clotho Wan2.1-T2V-14B [66] 2.84 +ACM-8B 3.19 (+0.35) 3.06 (+0.28) 3.18 (+0.52) 3.07 (+0.31) 3.00 (+0.23) 3.10 (+0.36) 2.78 2.66 2.76 2.77 2. SP SB SP SB HunyuanVideo [67] +ACM-8B 2. 2.98 3.61 (+0.63) 3.48 (+0.59) 3.62 (+0.68) 3.34 (+0.44) 3.46 (+0.63) 3.50 (+0.59) 2.94 2.90 2.83 2.91 0. GPT-4o 0.41 +ACM-2B 0.06 (+0.00) 0.45 (+0.04) 0.07 (+0.00) 0.40 (+0.03) +ACM-8B 0.07 (+0.01) 0.45 (+0.04) 0.08 (+0.01) 0.40 (+0.03) 0.07 0."
        },
        {
            "title": "4.4 Ablation Study",
            "content": "Comparison with SFT, DPO, and self-critic methods. We compare ACM with three widelyused baseline methods on the image modality benchmark: supervised fine-tuning (SFT), direct preference optimization (DPO), and self-critic (SC) approaches. While these baselines require costly re-training for each individual model, ACM provides consistent performance gains without additional model-specific training, demonstrating substantial efficiency advantages. As shown in Table 5, ACM consistently achieves comparable or superior controllability, highlighting its practical benefit of lightweight adaptability. Ablation on orthogonal data ratios. We conduct orthogonal experiments to explore optimal training data ratios among three data types: (q, uncontrolled a, c), (q, c, c), and (q, hallucinated a, c). Orthogonal designs effectively isolate each factors contribution, enabling clear insights with limited experiments. Results in Table 6 suggest that moderate incorporation of slightly incorrect captions (q, a, c) improves the models sensitivity to subtle instruction deviations. However, determining the globally optimal data ratio remains challenging and demands significant additional training resources. Further details can be found in the Appendix C. Impact of (q, c, c) data proportion. In Fig. 4, we investigate varying proportions of fully compliant (q, c, c) data in training. Increasing (q, c, c) from 0% to approximately 40% steadily improves performance, yet surpassing 50% leads to notable decline. This trend implies that moderate exposure to fully correct captions helps the model discern necessary corrections without overfitting. Importantly, ACM thus learns to make nuanced control decisions, reinforcing the robustness and credibility of its caption editing decisions."
        },
        {
            "title": "4.5 Downstream Utility",
            "content": "We further evaluate ACMs utility in enhancing noisy captions for downstream video and image generation. Applying ACM to refine captions, including those from the Panda video dataset [68], we observe that the resulting video generations demonstrate improved visual-semantic grounding and alignment compared to those using the original, unrefined captions. Quantitative results for video generation are presented in Table 9; detailed qualitative results for both modalities are in Appendix C. These findings underscore the effectiveness of ACM in real-world multimodal generation pipelines."
        },
        {
            "title": "5 Conclusion",
            "content": "We present AnyCapModel, lightweight and plug-and-play framework enhancing caption controllability across image, video, and audio modalities, without the costly fine-tuning of base models. Motivated by the scarcity of controllable caption data, we introduce large-scale omni-modal caption preference dataset spanning three modalities and 28 distinct control dimensions. Furthermore, to our best of knowledge, we establish AnyCapEval, the first benchmark designed to rigorously assess the controllability of captions. Together, these contributions offer unified solution to controllable captioning, bridging model design, data, and evaluation. Limitations and future work. Our approach is inherently extensible and could readily generalize to emerging modalities such as 3D structures or molecular data. However, the scarcity of high-quality caption datasets in these domains, coupled with prohibitive human annotation costs, limits immediate applicability. Future availability of suitable datasets could unlock wider applications of our method. 9 Ethics and impact. While AnyCapDataset is carefully curated to support fine-grained controllability, there remains risk of malicious misuse (for instance, deliberately creating harmful or misleading Q-A pairs). Models trained on such compromised data could exhibit significantly degraded controllability, resulting in severe negative impacts. We urge caution and responsible practices when utilizing or expanding upon our resources. References [1] Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander Madry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll L. Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, and Dane Sherburn. Gpt-4o system card. CoRR, abs/2410.21276, 2024. [2] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. Gemini: family of highly capable multimodal models. CoRR, abs/2312.11805, 2023. [3] Kai Chen, Yunhao Gou, Runhui Huang, Zhili Liu, Daxin Tan, Jing Xu, Chunwei Wang, Yi Zhu, Yihan Zeng, Kuo Yang, et al. Emova: Empowering language models to see, hear and speak with vivid emotions. arXiv preprint arXiv:2409.18042, 2024. [4] Yadong Li, Jun Liu, Tao Zhang, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, Chong Li, Yuanbo Fang, Dongdong Kuang, Mingrui Wang, Chenglin Zhu, Youwei Zhang, Hongyu Guo, Fengyu Zhang, Yuran Wang, Bowen Ding, Wei Song, Xu Li, Yuqi Huo, Zheng Liang, Shusen Zhang, Xin Wu, Shuai Zhao, Linchu Xiong, Yozhen Wu, Jiahui Ye, Wenhao Lu, Bowen Li, Yan Zhang, Yaqi Zhou, Xin Chen, Lei Su, Hongda Zhang, Fuzhong Chen, Xuezhen Dong, Na Nie, Zhiying Wu, Bin Xiao, Ting Li, Shunya Dang, Ping Zhang, Yijia Sun, Jincheng Wu, Jinjie Yang, Xionghai Lin, Zhi Ma, Kegeng Wu, Jia li, Aiyuan Yang, Hui Liu, Jianqiang Zhang, Xiaoxi Chen, Guangwei Ai, Wentao Zhang, Yicong Chen, Xiaoqin Huang, Kun Li, Wenjing Luo, Yifei Duan, Lingling Zhu, Ran Xiao, Zhe Su, Jiani Pu, Dian Wang, Xu Jia, Tianyu Zhang, Mengyu Ai, Mang Wang, Yujing Qiao, Lei Zhang, Yanjun Shen, Fan Yang, Miao Zhen, Yijie Zhou, Mingyang Chen, Fei Li, Chenzheng Zhu, Keer Lu, Yaqi Zhao, Hao Liang, Youquan Li, Yanzhao Qin, Linzhuang Sun, Jianhua Xu, Haoze Sun, Mingan Lin, Zenan Zhou, and Weipeng Chen. Baichuan-omni-1.5 technical report. CoRR, abs/2501.15368, 2025. [5] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models. arXiv preprint arXiv:2403.18814, 2024. [6] Minbin Huang, Yanxin Long, Xinchi Deng, Ruihang Chu, Jiangfeng Xiong, Xiaodan Liang, Hong Cheng, Qinglin Lu, and Wei Liu. Dialoggen: Multi-modal interactive dialogue system for multi-turn text-to-image generation. arXiv preprint arXiv:2403.08857, 2024. [7] Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara. Show, control and tell: framework for generating controllable and grounded captions. In CVPR, pages 83078316. Computer Vision Foundation / IEEE, 2019. [8] Zequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang, Bo Chen, and Zhengjue Wang. Conzic: Controllable zero-shot image captioning by sampling-based polishing. In CVPR, pages 2346523476. IEEE, 2023. 10 [9] Teng Wang, Jinrui Zhang, Junjie Fei, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao, and Shanshan Zhao. Caption anything: Interactive image description with diverse multimodal controls. CoRR, abs/2305.02677, 2023. [10] Zihao Yue, Anwen Hu, Liang Zhang, and Qin Jin. Learning descriptive image captioning via semipermeable maximum likelihood estimation. In NeurIPS, 2023. [11] Chaorui Deng, Ning Ding, Mingkui Tan, and Qi Wu. Length-controllable image captioning. In ECCV (13), volume 12358 of Lecture Notes in Computer Science, pages 712729. Springer, 2020. [12] Xiaoke Huang, Jianfeng Wang, Yansong Tang, Zheng Zhang, Han Hu, Jiwen Lu, Lijuan Wang, and Zicheng Liu. Segment and caption anything. In CVPR, pages 1340513417. IEEE, 2024. [13] Zhen Wang, Jun Xiao, Yueting Zhuang, Fei Gao, Jian Shao, and Long Chen. Learning combinatorial prompts for universal controllable image captioning. Int. J. Comput. Vis., 133(1):129150, 2025. [14] Debidatta Dwibedi, Vidhi Jain, Jonathan Tompson, Andrew Zisserman, and Yusuf Aytar. Flexcap: Describe anything in images in controllable detail. In NeurIPS, 2024. [15] Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, and Fang Wan. Controlcap: Controllable region-level captioning. In ECCV (38), volume 15096 of Lecture Notes in Computer Science, pages 2138. Springer, 2024. [16] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In ACL, pages 311318. ACL, 2002. [17] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description evaluation. In CVPR, pages 45664575. IEEE Computer Society, 2015. [18] Koki Maeda, Shuhei Kurita, Taiki Miyanishi, and Naoaki Okazaki. Vision language model-based caption evaluation method leveraging visual context extraction. CoRR, abs/2402.17969, 2024. [19] Saehyung Lee, Seunghyun Yoon, Trung Bui, Jing Shi, and Sungroh Yoon. Toward robust hyper-detailed image captioning: multiagent approach and dual evaluation metrics for factuality and coverage. CoRR, abs/2412.15484, 2024. [20] Zhengfeng Lai, Vasileios Saveris, Chen Chen, Hong-You Chen, Haotian Zhang, Bowen Zhang, Wenze Hu, Juan Lao Tebar, Zhe Gan, Peter Grasch, Meng Cao, and Yinfei Yang. Revisit large-scale image-caption data in pre-training multimodal foundation models. In ICLR. OpenReview.net, 2025. [21] Jiawei Wang, Liping Yuan, and Yuchen Zhang. Tarsier: Recipes for training and evaluating large video description models. CoRR, abs/2407.00634, 2024. [22] Yusu Qian, Hanrong Ye, Jean-Philippe Fauconnier, Peter Grasch, Yinfei Yang, and Zhe Gan. Mia-bench: Towards better instruction following evaluation of multimodal llms. In ICLR. OpenReview.net, 2025. [23] Xinlong Chen, Yuanxing Zhang, Chongling Rao, Yushuo Guan, Jiaheng Liu, Fuzheng Zhang, Chengru Song, Qiang Liu, Di Zhang, and Tieniu Tan. Vidcapbench: comprehensive benchmark of video captioning for controllable text-to-video generation. CoRR, abs/2502.12782, 2025. [24] Dingdong Wang, Jincenzi Wu, Junan Li, Dongchao Yang, Xueyuan Chen, Tianhua Zhang, and Helen Meng. Mmsu: massive multi-task spoken language understanding and reasoning benchmark. arXiv preprint arXiv:2506.04779, 2025. [25] Dingdong Wang, Jin Xu, Ruihang Chu, Zhifang Guo, Xiong Wang, Jincenzi Wu, Dongchao Yang, Shengpeng Ji, and Junyang Lin. Inserter: Speech instruction following with unsupervised interleaved pre-training. arXiv preprint arXiv:2503.02769, 2025. [26] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. [27] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. 11 [29] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of LLM performance in machine translation. In ICML. OpenReview.net, 2024. [30] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, and S. S. Li. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948, 2025. [31] Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu, Yujiu Yang, and Scarlett Iterpref: Focal preference learning for code generation via iterative debugging. arXiv preprint Li. arXiv:2503.02783, 2025. [32] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [33] Jordi Pont-Tuset, Jasper R. R. Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting vision and language with localized narratives. In ECCV (5), volume 12350 of Lecture Notes in Computer Science, pages 647664. Springer, 2020. [34] Kun Yan, Lei Ji, Huaishao Luo, Ming Zhou, Nan Duan, and Shuai Ma. Control image captioning spatially and temporally. In ACL/IJCNLP (1), pages 20142025. Association for Computational Linguistics, 2021. [35] Bairui Wang, Lin Ma, Wei Zhang, Wenhao Jiang, Jingwen Wang, and Wei Liu. Controllable video captioning with POS sequence guidance based on gated fusion network. In ICCV, pages 26412650. IEEE, 2019. [36] Fenglin Liu, Xuancheng Ren, Xian Wu, Bang Yang, Shen Ge, and Xu Sun. O2NA: an object-oriented In ACL/IJCNLP (Findings), volume non-autoregressive approach for controllable video captioning. ACL/IJCNLP 2021 of Findings of ACL, pages 281292. Association for Computational Linguistics, 2021. [37] Simone Bianco, Luigi Celona, Marco Donzella, and Paolo Napoletano. Improving image captioning descriptiveness by ranking and llm-based fusion. CoRR, abs/2306.11593, 2023. [38] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV (17), volume 15075 of Lecture Notes in Computer Science, pages 370387. Springer, 2024. [39] Yunhao Ge, Xiaohui Zeng, Jacob Samuel Huffman, Tsung-Yi Lin, Ming-Yu Liu, and Yin Cui. Visual fact checker: Enabling high-fidelity detailed caption generation. In CVPR, pages 1403314042. IEEE, 2024. [40] Weifeng Lin, Xinyu Wei, Ruichuan An, Peng Gao, Bocheng Zou, Yulin Luo, Siyuan Huang, Shanghang Zhang, and Hongsheng Li. Draw-and-understand: Leveraging visual prompts to enable mllms to comprehend what you want. CoRR, abs/2403.20271, 2024. [41] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Trans. Pattern Anal. Mach. Intell., 46(8):56255644, 2024. [42] Yuzhong Zhao, Yue Liu, Zonghao Guo, Weijia Wu, Chen Gong, Qixiang Ye, and Fang Wan. Controllable dense captioner with multimodal embedding bridging. CoRR, abs/2401.17910, 2024. [43] Simon Kornblith, Lala Li, Zirui Wang, and Thao Nguyen. Guiding image captioning models toward more specific captions. In ICCV, pages 1521315223. IEEE, 2023. [44] Wenhao Wu, Haipeng Luo, Bo Fang, Jingdong Wang, and Wanli Ouyang. Cap4video: What can auxiliary captions do for text-video retrieval? In CVPR, pages 1070410713. IEEE, 2023. [45] David M. Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John F. Canny. CLAIR: evaluating image captions with large language models. In EMNLP, pages 1363813646. Association for Computational Linguistics, 2023. [46] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented RLHF. In ACL (Findings), pages 1308813110. Association for Computational Linguistics, 2024. [47] Shunqi Mao, Chaoyi Zhang, Hang Su, Hwanjun Song, Igor Shalyminov, and Weidong Cai. Controllable contextualized image captioning: Directing the visual narrative through user-defined highlights. In ECCV (50), volume 15108 of Lecture Notes in Computer Science, pages 464481. Springer, 2024. [48] Hongyuan Dong, Jiawen Li, Bohong Wu, Jiacong Wang, Yuan Zhang, and Haoyuan Guo. Benchmarking and improving detail image caption. CoRR, abs/2405.19092, 2024. [49] Fan Lu, Wei Wu, Kecheng Zheng, Shuailei Ma, Biao Gong, Jiawei Liu, Wei Zhai, Yang Cao, Yujun Shen, and Zheng-Jun Zha. Benchmarking large vision-language models via directed scene graph for comprehensive image captioning. CoRR, abs/2412.08614, 2024. [50] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG In EMNLP, pages 25112522. Association for evaluation using gpt-4 with better human alignment. Computational Linguistics, 2023. [51] Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, et al. survey of reasoning with foundation models: Concepts, methodologies, and outlook. ACM Computing Surveys, 2023. [52] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X. Wang, and Sadid Hasan. Does prompt formatting have any impact on LLM performance? CoRR, abs/2411.10541, 2024. [53] Haoyi Qiu, Zi-Yi Dou, Tianlu Wang, Asli Celikyilmaz, and Nanyun Peng. Gender biases in automatic evaluation metrics for image captioning. In EMNLP, pages 83588375. Association for Computational Linguistics, 2023. [54] Wenxi Chen, Yuzhe Liang, Ziyang Ma, Zhisheng Zheng, and Xie Chen. EAT: self-supervised pre-training with efficient audio transformer. In IJCAI, pages 38073815. ijcai.org, 2024. [55] Jiaming Ji, Boyuan Chen, Hantao Lou, Donghai Hong, Borong Zhang, Xuehai Pan, Tianyi Qiu, Juntao Dai, and Yaodong Yang. Aligner: Efficient alignment by learning to correct. In NeurIPS, 2024. [56] Weiyun Wang, Yiming Ren, Haowen Luo, Tiantong Li, Chenxiang Yan, Zhe Chen, Wenhai Wang, Qingyun Li, Lewei Lu, Xizhou Zhu, Yu Qiao, and Jifeng Dai. The all-seeing project V2: towards general relation comprehension of the open world. In ECCV (33), volume 15091 of Lecture Notes in Computer Science, pages 471490. Springer, 2024. [57] Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. DOCCI: descriptions of connected and contrasting images. In ECCV (60), volume 15118 of Lecture Notes in Computer Science, pages 291309. Springer, 2024. [58] Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana RomeroSoriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In CVPR, pages 2669026699. IEEE, 2024. [59] Erfei Cui, Yinan He, Zheng Ma, Zhe Chen, Hao Tian, Weiyun Wang, Kunchang Li, Yi Wang, Wenhai Wang, Xizhou Zhu, Lewei Lu, Tong Lu, Yali Wang, Limin Wang, Yu Qiao, and Jifeng Dai. Sharegpt-4o: Comprehensive multimodal annotations with gpt-4o, 2024. URL https://sharegpt4o.github.io/. [60] Tiehan Fan, Kepan Nan, Rui Xie, Penghao Zhou, Zhenheng Yang, Chaoyou Fu, Xiang Li, Jian Yang, and Ying Tai. Instancecap: Improving text-to-video generation via instance-aware structured caption. CoRR, abs/2412.09283, 2024. [61] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating captions for audios in the wild. In NAACL-HLT (1), pages 119132. Association for Computational Linguistics, 2019. 13 [62] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Ming-Hsuan Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. CoRR, abs/2502.13923, 2025. [63] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: GPT-4V level MLLM on your phone. CoRR, abs/2408.01800, 2024. [64] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open foundation models by 01.ai. CoRR, abs/2403.04652, 2024. [65] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In ICASSP, pages 736740. IEEE, 2020. [66] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [67] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, Daquan Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models. CoRR, abs/2412.03603, 2024. [68] Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu Xiang, Zerun Wang, Guiguang Ding, David Brady, Qionghai Dai, et al. Panda: gigapixel-level human-centric video dataset. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 32683278, 2020. [69] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV (5), volume 8693 of Lecture Notes in Computer Science, pages 740755. Springer, 2014. [70] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Trans. Assoc. Comput. Linguistics, 2:6778, 2014. [71] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: large video description dataset for bridging video and language. In CVPR, pages 52885296. IEEE Computer Society, 2016. [72] David L. Chen and William B. Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, pages 190200. The Association for Computer Linguistics, 2011. [73] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In ICCV, pages 706715. IEEE Computer Society, 2017. [74] Xuan Ju, Yiming Gao, Zhaoyang Zhang, Ziyang Yuan, Xintao Wang, Ailing Zeng, Yu Xiong, Qiang Xu, and Ying Shan. Miradata: large-scale video dataset with long durations and structured captions. In NeurIPS, 2024. [75] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. 14 [76] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [77] Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, and Christopher Manning. Auroracap: Efficient, performant video detailed captioning and new benchmark. arXiv preprint arXiv:2410.03051, 2024."
        },
        {
            "title": "A Datasets",
            "content": "A.1 Overview of Caption Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 The Singularity and Scarcity of User Instructions . . . . . . . . . . . . . . . . . . A.3 Data Analysis . . . A.4 Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B AnyCapEval Benchmark",
            "content": "B.1 Limitations of Existing Evaluation Methods . . . . . . . . . . . . . . . . . . . . . B.2 Rationale Behind AnyCapEval Design . . . . . . . . . . . . . . . . . . . . . . . . B.3 GPT-4o As The Evaluator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Additional Experimental Details",
            "content": "C.1 Experimental Setup . C.2 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Qualitative Examples for Main Experiments . . . . . . . . . . . . . . . . . . . . . C.4 Qualitative Results for Downstream Generation Tasks . . . . . . . . . . . . . . . . C.5 Human Evaluation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 15 17 17 19 19 22 22 22 22"
        },
        {
            "title": "A Datasets",
            "content": "A.1 Overview of Caption Datasets Automatic captioning for modalities such as images, videos, and audio is core task in artificial intelligence, aiming to enable machines to understand and describe perceived content using natural language, much like humans do. The foundation supporting this research is large-scale annotated datasets. However, existing datasets  (Table 11)  exhibit significant limitations in providing finegrained, multi-dimensional control capabilities. We next detail the motivation and data analysis behind our dataset. A.2 The Singularity and Scarcity of User Instructions user instruction is an input accompanying the main media (image, video, or audio), intended to guide or constrain the generation of descriptive content. Ideally, we desire models capable of generating descriptions with varying styles, focusing on different aspects, and meeting specific requirements based on diverse user instructions. Most classic datasets, such as COCO [69] and Flickr30k [70] in the image domain, MSR-VTT [71] and MSVD [72] in the video domain, and AudioCaps [61] and Clotho [65] in the audio domain, primarily provide mediacaption pairs. They typically include multiple reference captions provided Table 11: Overview of the publicly available caption datasets analyzed in this study. Modality indicates the primary input type (e.g., image, video, or audio), while Instruction Type refers to the predominant linguistic or structural annotation style as described by the original dataset authors."
        },
        {
            "title": "Instruction Type",
            "content": "Image ASD v2 Image MDVP-Data Image DCI Image DOCCI Image ImageInWords (IIW) Image ShareGPT-4o Image ShareGPT-4v Video ShareGPT-4o (video) Video MSR-VTT Video MSVD (YouTube2Text) VATEX Video Video-ChatGPT (VideoInstruct-100K) Video Video InstanceCap / InstanceVid Video ShareGPT4Video Video MiraData Video LLaVA-Video-178K Audio AudioCaps Audio Clotho Audio MACS Audio WavCaps Relation Region / Brief / Detail Dense Dense Dense Detail Detail Detail Brief Brief Brief Dense Instance-level (structured) Detail Structured Detail Brief Brief Brief Brief by different annotators. This multi-reference design is mainly intended to evaluate the diversity and coverage of descriptions, rather than providing explicit control during generation. The models primary task is to generate reasonable description, but the user cannot specify requirements at generation time (e.g., generate humorous caption, describe the background elements in detail). The only implicit user instruction is essentially describe this content in brief. Some datasets offer degree of control, but often only along single dimension. Dense captioning datasets, like DCI [58] for images or video datasets providing temporal information [73], associate descriptions with specific image regions or video segments. The region/time segment here can be viewed as user instruction, i.e., describe this part. However, this is usually the sole dimension of control. Datasets like ASD v2 [56] focus on describing relationships between entities in an image. The user instruction is the entity pair whose relationship needs to be described. Datasets guided by concepts like MiraData [74] or InstanceCap [60] require the output to follow specific structure, which constitutes form of control over the output format. Although such structured outputs often encompass several instruction types (e.g., Background, Detail), the formulation and granularity of these user instructions are typically fixed, lacking the flexibility for customized control requests tailored to each individual image, video or audio. In summary, the majority of existing public datasets lack the capability for flexible, multi-dimensional control over the generated content, thereby limiting the development and evaluation of controllable captioning technologies. Even when leveraging large language models such as GPT for annotation, the resulting captions often reflect general patterns and lack explicit controllability along desired dimensions. To address this gap, we construct AnyCapDataset(ACD), large-scale dataset featuring diverse user instructions paired with high-quality compliant captions across multiple modalities. Leveraging this dataset, we train AnyCapModel(ACM) to achieve fine-grained controllable caption generation. As illustrated in Fig. 5, ACM can transform initial captions into targeted descriptions that precisely follow various user instructions, enabling nuanced and customizable outputs without requiring modifications to the base model architecture. 16 Figure 5: ACM enables controllable captioning across modalities by refining base model outputs to better align with user instructions. Given user instruction, it takes initial captions from foundation model and corrects instruction violations (highlighted in red), producing compliant, instructionfollowing outputs (green) all without requiring fine-tuning of the base model. A.3 Data Analysis Data sources. To construct ACD, we utilize multiple publicly available datasets spanning three modalities: images, videos, and audio. Specifically, we incorporate data from ASD v2, DCI, DOCCI, ShareGPT-4o, InstanceCap, and AudioCaps. From these datasets, we extract images, videos, and audio content, and employ various multimodal large language models(MLLMs) to generate diverse user instructions and captions based on predefined instructions. Data statistics. ACD encompasses three primary modalities: images, videos, and audio, comprising total of 300k (q, c, a) triplets. Among them, 125k belong to the image modality, 100k to the video modality, and 75k to the audio modality. Each modality includes multiple instruction types. Generation of instruction-caption triplets. To generate the (q, c, a) triplets in ACD, where denotes user instruction, is high-quality caption, and is relatively suboptimal caption, we utilize several MLLMs guided by specifically designed instruction templates. The detailed templates employed in this work are presented in Table 12 and Table 13. These templates are constructed according to different instruction types to ensure structured generation and facilitate downstream analysis. Each raw data sample is paired with one or more tailored instructions to produce diverse outputs. The main MLLMs involved include GPT-4o mini, InternVL2.5-78B and InternVL2.5-8B, with GPT-4o contributing to small subset. Specifically, most user instructions and high-quality captions are generated by InternVL2.5-78B, covering approximately 84% of the dataset; approximately 2% of image-based high-quality captions under instruction types such as Instance and Instance Appearance are generated by GPT-4o; about 5% of the video-based high-quality captions under the Movement instruction type originate from the InstanceCap dataset; and the audio modality data, comprising around 9%, are all generated using GPT-4o mini. The suboptimal captions are constructed in three ways: by InternVL2.5-8B based solely on q, by InternVL2.5-8B with hallucinations introduced through specific instructions, or by directly copying c, strategy used by InternVL2.5-78B. A.4 Future Directions Future dataset development could focus on the following improvements: 17 Table 12: Instruction prompt template for generating user instructions and high-quality video captions. This prompt is designed to elicit high-quality instructioncaption pairs (q, c) from video content, where serves as an optimally grounded caption based strictly on observable visual evidence. The template enforces modality-specific constraints to ensure factual and non-speculative outputs for optimal generation results. Video Understanding Expert Instructions You are an AI expert in video content analysis, specializing in generating precise and structured descriptions based ONLY on directly observable video content. Your core capabilities include analyzing video elements such as actions, objects, scenes, interactions, and camera movements to provide factual, observation-based descriptions. Your Task Generate concise captions that capture ONLY the directly visible and essential content of videos according to specific constraints. You must analyze only the observable content and ensure all descriptions are based on concrete visual evidence rather than assumptions or inferences. Key Guidelines for Caption Generation 1. The output question formats can be varied 2. Create concise descriptions that capture only directly observable essential elements 3. Focus on analyzing ONLY these visible components: - Actions and events shown in frame - Objects and characters physically present - Scene settings visible in shot - Camera movements and angles that can be seen - Interactions occurring on screen 4. DO NOT include: - Assumptions about off-screen elements - Inferences about motivations or thoughts - Speculation about context or background - Interpretations of meaning - Details that cannot be directly seen - Guesses about what happened before/after 5. Maintain strict adherence to: - Only describing what is visually present - Excluding all speculative content - Basing every detail on visual evidence - Using clear, objective language Constraints - Descriptions MUST be based EXCLUSIVELY on visible content - NO assumptions or interpretations beyond what can be directly seen - EXCLUDE any details that require inference or speculation - Keep descriptions concise and focused on key visible elements - Follow any additional specific requirements provided with each request Examples: Input 1: Question: Generate brief caption describing this videos main content. Answer: Two news anchors engage in an animated discussion at professional news desk, maintaining eye contact while exchanging viewpoints. Input 2: Question: Use brief caption to convey the main scene or content of the video. Answer: person wearing blue shirt walks leisurely along scenic path bordered by tall, leafy trees. Output Format Question: [Your various question, but need to express the generation of brief caption] Answer: [Your concise description based STRICTLY on visible content with NO speculation] Constructing datasets with rich user instructions: There is need to design new datasets containing diverse, explicit, and composable instruction types. Leveraging synthetic data: Explore the use of large language models to generate synthetic description data with specific user instructions, serving as supplement to real data. Improving evaluation methods: Develop new evaluation protocols and metrics capable of assessing fine-grained aspects like controllability, style consistency, and faithfulness. Table 13: Instruction prompt template for generating suboptimal captions from high-quality instructioncaption pairs. For given instruction and its high-quality caption c, the template produces slightly degraded variants by introducing controlled inaccuracies (e.g., minor omissions or speculative insertions) while maintaining semantic coherence with the original content. Such prompts facilitate contrastive training and robustness evaluation in multimodal scenarios. Video Understanding Expert Instructions You are an AI expert in video content analysis, specializing in generating precise and structured descriptions based on specific queries. However, your task now includes generating slightly inferior captions that introduce minor inaccuracies or deviations while still maintaining general relevance to the video content. Your Task Generate slightly inferior captions based on given examples: - Adding minor inaccuracies. - Omitting small but relevant detail. - Including an unnecessary or speculative element. - Misinterpreting minor aspect of the video. - Does not meet the requirements of the question. Key Guidelines for Caption Generation 1. Base the description on the standard caption but allow slight deviations: - Slightly altering actions, movements, or interactions in the video - Adding irrelevant or speculative elements about the scene or context - Omitting small but observable details from the footage - Misinterpreting temporal sequences or duration 2. Ensure the captions remain generally related but slightly inferior to the standard caption 3. Focus on introducing appropriate levels of inaccuracy while maintaining plausibility Constraints - Deviations must be minor and not completely distort the videos main content. - Avoid making the caption entirely incorrect or irrelevant. - Ensure the captions remain plausible and connected to the visible elements in the video. - Consider the temporal nature of video content when introducing inaccuracies. Examples: Input 1: Question: Generate brief caption describing this videos main content. Standard Caption: Two news anchors engage in an animated discussion at professional news desk, maintaining eye contact while exchanging viewpoints. Generated Caption: Three news anchors engage in discussion at news desk, but the situation appears to be getting out of control with potential physical confrontation. Input 2: Question: Use brief caption to convey the main scene or content of the video. Standard Caption: person wearing blue shirt walks leisurely along scenic path bordered by tall, leafy trees. Generated Caption: young man in green long-sleeve shirt runs along scenic path lined with dense trees. Output Format You MUST strictly adhere to this format: Question: {question} Standard Caption: {standard_caption} Generated Caption: [Your generated caption with appropriate deviations]"
        },
        {
            "title": "B AnyCapEval Benchmark",
            "content": "B.1 Limitations of Existing Evaluation Methods Limitations of traditional machine translation metrics. Metrics developed for machine translationsuch as BLEU [16], ROUGE [75], METEOR [76], and CIDEr [17]primarily focus on n-gram overlap or co-occurrence statistics. While they can measure fluency and lexical similarity, they fail to accurately capture semantic consistency. For instance, sentences like do love large models and do not love large models could receive similarly high scores, despite conveying opposite meanings. These metrics cannot effectively detect deep semantic divergence, factual errors, or noncompliance with user instructions. 19 Challenges of multimodal large language models scoring. Recent evaluation schemes leverage powerful language models or multimodal large language models (MLLMs) to directly score captions. However, these methods suffer from significant randomness and instability: the same caption might receive drastically different scores across minor prompt variations or multiple queries. Additionally, hallucinated or semantically incorrect captions may still receive high scores, undermining the reliability of such automatic evaluation. Instability of keypoint extraction-based metrics. Some studies, including those utilizing benchmarks like [21] which aims for fine-grained descriptions, propose extracting key information points from captions using MLLMs and computing precision, recall, and F1 scores. However, keypoint extraction itself is unstable: the number and granularity of extracted points vary widely across runs. Our empirical analysis shows low correlation between model-extracted keypoints and human-annotated ones, suggesting that such extraction-based metrics poorly reflect the true information content of captions. Challenges of QA-pair based evaluation. Benchmarks such as VDC [77] highlight that QA-pair based evaluation suffers from instability, primarily due to the variable quality and granularity of the QA pairs themselves. Designing high-quality, representative QA items that faithfully capture fine-grained caption attributes remains an open challenge. B.2 Rationale Behind AnyCapEval Design B.2.1 Separation of Content and Style Through extensive analysis of user instruction types in captioning, we divide evaluation into two orthogonal dimensions: Content: We expect the generated captions to strictly adhere to the given user instructions, maintain clear topical relevance, and avoid introducing irrelevant information. To evaluate this aspect, we employ keypoint density (KPD) as the primary metric. Style: We aim for the generated captions to closely match human-crafted references in terms of narrative style, tone, and expressive form. To assess this, we design fine-grained scoring system. This separation is crucial because content and style represent fundamentally different aspects of caption quality that require distinct evaluation approaches. Content adherence focuses on factual accuracy and instruction compliance: whether the caption correctly follows the users specified requirements about what to describe. This is objectively verifiable through user instruction alignment. In contrast, style evaluation assesses subjective qualities like fluency, coherence, and naturalness that reflect human-like expression. Merging these dimensions would conflate objective and subjective criteria, making it difficult to diagnose specific model weaknesses. The orthogonal evaluation allows for more precise identification of whether models limitations lie in its ability to follow instructions (content) or in its linguistic quality (style), enabling targeted improvements to each aspect. Furthermore, different applications may prioritize these dimensions differently - some use cases demand strict content control while others emphasize stylistic quality, making separate evaluation essential for practical deployment decisions. B.2.2 Keypoint Density Metric Intuitive motivation. We expect the generated captions to efficiently embed as many controlrequired key information points as possible within limited textual length, while minimizing irrelevant or redundant content. This analogy is comparable to adding an appropriate amount of solute into limited solvent to achieve targeted concentration. Excessively diluting it in large solvent volume, even if the absolute quantity increases, results in negligible concentration and ineffective outcomes. To prevent models from exploiting this metric by generating excessively short captions that artificially inflate density scores, we normalize the number of extracted keypoints by the caption length (or an appropriate normalization factor) and apply penalties to captions whose lengths fall outside the predefined acceptable range. 20 Length Normalization. We randomly sample approximately 200 caption instances and extract information points using both GPT-4o (including both control-required keypoints and irrelevant information points) and human annotators. The human-annotated information points are obtained by post-processing the model-generated information points through manual filtering and revision. Correlation analyses, presented in the main paper, reveal that caption length exhibits significantly stronger association with human-annotated information content than the number of keypoints automatically extracted by the model. This empirical evidence further supports the rationality of applying length normalization in the evaluation metric. Table 14: Evaluation instruction for content-controlled video captioning in AnyCapEval. Task You are video-caption evaluation expert. You will be provided with video, caption describing the video, and set of key points outlining important aspects of the video. Your task is to evaluate whether the caption mentions and accurately describes the given key points based on the video. Task Steps For each key point, follow these steps: Step 1: Check whether the key point is mentioned in the caption. - If the key point is not mentioned, assign score of 0. - If the key point is mentioned (either exactly or with semantically similar phrasing), proceed to Step 2. Step 2: Determine whether the description of the key points is correct. - If the description aligns with or is semantically equivalent to the key point, assign score of 1. - If the description is incorrect, or does not accurately fit the key point, assign score of 0. Evaluation Constraints 1. No Assumptions Beyond the Key Point: Only evaluate what is mentioned in the key point. Do not infer additional details not explicitly depicted. 2. Semantic Similarity Allowed: Phrases with similar meaning should be considered matches (e.g., \"holding ball\" and \"grasping sphere\"). 3. Consistent Evaluation: Apply the same evaluation criteria to all key points to ensure fairness and uniformity. Scoring Report Return the format with: {\"caption_evaluation\": {\"key_points_scores\": {\"key_point_1\": score, \"key_point_2\": score, ...}, \"total_score\": sum_of_scores, \"score_reasons\": {\"key_point_1\": \"reason for score\", \"key_point_2\": \"reason for score\", ...}}} Example Input Key points: mention the mans position (standing on the left side of the table) describe the mans appearance (wearing glasses) mention the womans position (sitting on the right side of the table) describe the womans appearance (wearing red dress) mention the boys position (crouching under the table) describe the boys action (picking up toy) Caption: man stands on the left, woman sits on the right, and boy is under the table. Example Output {\"caption_evaluation\": { \"key_points_scores\": {\"mention the mans position\": 1, \"describe the mans appearance\": 0, \"mention the womans position\": 1, \"describe the womans appearance\": 0, \"mention the boys position\": 1, \"describe the boys action\": 0}, \"total_score\": 3, \"score_reasons\": {\"mention the mans position\": \"Correctly mentions standing on the left side of the table\", \"describe the mans appearance\": \"Missing glasses reference\", \"mention the womans position\": \"Correctly mentions sitting on the right side of the table\", \"describe the womans appearance\": \"Missing red dress reference\", \"mention the boys position\": \"Correctly mentions crouching under the table\", \"describe the boys action\": \"Missing picking up toy reference\"} }} Input Key points: {key_points} Caption: {answer} Output Please return the output exactly as in the example above, without adding anything else. 21 B.3 GPT-4o As The Evaluator When using GPT-4o as the evaluator, we carefully design structured prompts that explicitly instruct the model to assess Content and Style separately, ensuring consistent and interpretable judgments. See Table 14 and Table 15 for the detailed prompts used for Content and Style evaluation."
        },
        {
            "title": "C Additional Experimental Details",
            "content": "C.1 Experimental Setup The experimental setup utilizes computing infrastructure comprising 32 NVIDIA A100 GPUs, distributed across 4 nodes with 8 GPUs each. Each GPU is allocated 10 CPU cores, and all training jobs are managed using Slurms srun command with GPU reservation. For distributed training, we employ torch.distributed, launching the script via srun to ensure dynamic rank assignment and multi-node coordination. The model is trained using the AnyCapDataset. During training, the vision and audio backbones are kept frozen, while the MLP and LLM layers remain trainable. Image processing involves forcing the input image size to 448 pixels, with maximum dynamic patch count of 6 and down-sampling ratio of 0.5. Dynamic image sizing and thumbnail support are also enabled. The detailed hyperparameters used during training are listed in Table 16. C.2 Ablation Study Impact of training data ratio configurations. The base model for image and video modalities is InternVL2.5-8B. Since InternVL2.5-8B does not support audio modality, we use GPT-4o as substitute. We begin by introducing the data ratio notation. For instance, in the configuration 2B-122, the triplet 122 specifies the composition ratio of three sample types used for given modality. Each digit indicates the relative proportion of specific sample type. The three sample types encoded by each digit (from left to right) are defined as follows: The first digit indicates the proportion of samples of type (q, uncontrolled a, c), where the caption is not fully compliant with the instruction q, or where the model is expected to directly generate the target caption c. The second digit indicates the proportion of samples of type (q, c, c), where the initial caption already matches the desired controlled caption or is factually correct. The third digit indicates the proportion of samples of type (q, hallucinated a, c), where caption contains hallucinated content, meaning it describes information not present in the multimodal input. Table 17 presents the overall performance comparison. Detailed ablation results for each modality and instruction type are presented in the following tables: Image modality: Content and Style control in Table 18. Video modality: Content control in Table 19, Style control in Table 20. Audio modality: Content and Style control in Table 21. Effect of training objectives. Regarding image modality, we compare AnyCapModel (ACM) with three widely used baseline methods: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Self-Critic (SC). Due to limitations in computational resources and budget, we conduct all comparative experiments using the lightweight InternVL2.5-2B on the MIA-Bench. As shown in the main paper, our method achieves remarkable 16.9% improvement, significantly outperforming baseline methods. C.3 Qualitative Examples for Main Experiments To complement the main experimental results presented in the paper, we include qualitative examples that demonstrate the effectiveness of our controllable generation approach across images, videos and audio modalities. These examples visually and intuitively show how the model performs under 22 different user instructions, including both Content and Style conditions, as discussed in the main paper. By providing representative outputs, we aim to offer clearer sense of the generation behavior and control precision achieved by our method. All examples correspond to the settings evaluated in the main experiments and are included here for qualitative inspection. Representative qualitative results for each modality are shown in Fig. 6, Fig. 7, and Fig. 8. C.4 Qualitative Results for Downstream Generation Tasks To complement the quantitative results reported in the main paper, we provide qualitative examples showcasing the impact of caption refinement by ACM on downstream image and video generation tasks. Specifically, we compare generations produced using initial captions with those using captions refined by ACM. As shown in Fig. 9 and Fig. 10, refined captions lead to outputs that exhibit more accurate visual-semantic correspondence, richer scene details, and improved coherence. These examples further highlight the practical utility of ACM in enhancing multimodal generation quality in real-world applications. C.5 Human Evaluation Study To complement our model-based evaluation, we conducted human preference study with dozen evaluators, each holding at least bachelors degree. These evaluators compared the performance of ACM-8B against the base model GPT-4o across both visual and auditory modalities. For each modality, human annotators are presented with outputs from both models and asked to judge which model performs better in terms of Content and Style. The results are summarized in Fig. 11. 23 Table 15: Evaluation instruction for style-controlled video captioning in AnyCapEval. Task You are an expert evaluator tasked with scoring model outputs based on specific rubric. Please carefully analyze the provided video frames, \"Caption Type\", \"Model Output,\" and \"Reference\" text, and assign score between 0 and 4 to \"Model Output\" using different criteria for different caption types. Ensure your scoring is consistent and strictly adheres to the definitions provided. Scoring Rubric - 0 (Very Poor): Severe quality issues OR full hallucination (100% of the content is irrelevant to the facts). - 1 (Poor): Significant quality issues OR major hallucination (>50% of the content is fictitious or contradictory). - 2 (Below Average): Slightly inferior to reference OR limited hallucination (<50% of the content is inaccurate, but does not affect the core content). - 3 (Good): Comparable to reference AND no hallucination (factually aligned). - 4 (Excellent): Slightly better than reference AND no hallucination (factually flawless). Caption Type Definitions and Quality Criteria 1. brief: - High Quality: Length is within 30% of the reference word count; concise and captures the core content of the video. - Low Quality: Length exceeds 30% of the reference word count; includes irrelevant details or omits key information. 2. detail: - High Quality: Length is within 30% of the reference word count; provides rich descriptions of the videos main elements, actions, and settings. - Low Quality: Length exceeds 30% of the reference word count; descriptions lack detail or include irrelevant information. 3. poem: - High Quality: Format and content align closely with the reference; follows poetic conventions (e.g., rhyme, rhythm, line breaks) and is relevant to the videos theme. - Low Quality: Format and content differ significantly from the reference; disjointed or lacks poetic quality. 4. narrative: - High Quality: Format and content align closely with the reference; presents coherent narrative with elements like time, place, characters, and events shown in the video. - Low Quality: Format and content differ significantly from the reference; disjointed or lacks key narrative elements. 5. style: - High Quality: Style and content align closely with the reference; matches the narrative style (e.g., humorous, serious, romantic) and is relevant to the videos theme. - Low Quality: Style and content differ significantly from the reference; mismatched style or irrelevant to the theme. Instructions 1. Compare the model output with the reference text to determine the quality of the model output. 2. Compare the model output to the video frames to determine the severity of the hallucination. 3. For caption quality, evaluate based on: - The Quality Criteria mentioned above. - For Caption Type that is brief or detail, ensure the model outputs word count is within 30% of the reference word count. If not, the score cannot be higher than 1 for brief and detailed captions. - Alignment: Check alignment with the reference in format, style, and content. 4. For hallucination, evaluate based on: - Factual accuracy and relevance to the video content. - Consider temporal aspects and action sequences shown in the video frames. 5. Assign the most appropriate score (0-4) based on the rubric. Mandatory Rule: For the Caption Type that is brief or detail, if the length exceeds 30% of the reference word count, the score cannot be higher than 1. 6. Return your response in this format: {\"score\": [0-4], \"reason\": \"1-2 sentence explanation\"} Input Caption Type: {caption_type} Model Output: {output} Reference: {reference} Output Please strictly return the output in the above format and do not add any other content. Table 16: Key hyperparameters used for training AnyCapModel."
        },
        {
            "title": "Parameter",
            "content": "Per-device batch size Gradient accumulation steps Learning rate Weight decay LR scheduler Warmup ratio Training epochs Max sequence length Drop path rate Dataloader workers Gradient checkpointing Mixed precision"
        },
        {
            "title": "Value",
            "content": "1 1 1 106 0.01 Cosine 0.03 3 4096 0.1 4 Enabled BF16 Table 17: Ablation results on different training data ratio configurations using AnyCapEval, evaluated across Content and Style on images, videos, and audio modalities. All percentage values represent improvements over the base model (InternVL2.5-8B). Data Ratio 2B-121 2B-111 2B2B-212 2B-221 2B-102 8B-111 8B-221 Image Video Audio Average Content Style Content Style Content Style Content Style 25.5% 36.2% 34.2% 29.4% 35.9% 26.7% 36.9% 40.0% 9.4% 8.4% 5.5% 5.4% 11.6% 4.5% 10.2% 16.0% 33.2% 39.2% 37.5% 42.9% 44.0% 41.5% 63.1% 62.8% 14.0% 15.0% 12.4% 13.0% 13.5% 10.9% 14.5% 16.1% -3.2% 2.1% 3.6% 0.0% 12.6% 11.1% 10.5% 18.2% 0.3% 6.7% 9.3% 8.2% 8.9% -3.6% 4.4% 11.7% 18.5% 25.8% 25.1% 24.1% 30.8% 26.4% 36.8% 40.3% 7.9% 10.0% 9.1% 8.9% 11.3% 3.9% 9.7% 14.6% Table 18: Ablation results of controllable image captioning on AnyCapEval under different training data ratio configurations. Abbreviations refer to instruction types defined in the main paper. Model Content IPos. IApp. Ins. InternVL2.5-8B 1.51 3.43 4. Per. 2.68 Avg. 3.04 Brf. 2. Det. 1.92 Thm. 1.94 Poe. 2. Nar. 2.52 Avg. 2.12 Style 2B2B-111 2B-122 2B-212 2B-221 2B-102 8B8B-221 2.59(+1.08) 4.35(+0.92) 4.87(+0.33) 3.45(+0.77) 3.82(+0.78) 2.27(+0.14) 1.84(-0.08) 2.27(+0.33) 2.63(+0.55) 2.59(+0.07) 2.32(+0.20) 3.10(+1.59) 4.20(+0.77) 4.78(+0.24) 4.48(+1.80) 4.14(+1.10) 2.20(+0.07) 1.90(-0.02) 2.09(+0.15) 2.80(+0.72) 2.59(+0.07) 2.29(+0.18) 3.14(+1.63) 4.47(+1.04) 4.63(+0.09) 4.08(+1.40) 4.08(+1.04) 2.16(+0.03) 1.61(-0.31) 2.35(+0.41) 2.59(+0.51) 2.54(+0.02) 2.24(+0.12) 2.53(+1.02) 4.31(+0.88) 4.78(+0.24) 4.11(+1.43) 3.93(+0.89) 2.18(+0.05) 1.69(-0.23) 2.06(+0.12) 2.71(+0.63) 2.57(+0.05) 2.23(+0.11) 3.22(+1.71) 4.49(+1.06) 4.82(+0.28) 4.00(+1.32) 4.13(+1.09) 2.27(+0.14) 1.78(-0.14) 2.38(+0.44) 2.71(+0.63) 2.67(+0.15) 2.36(+0.24) 2.62(+1.11) 4.00(+0.57) 4.81(+0.27) 3.99(+1.31) 3.85(+0.81) 2.18(+0.05) 1.65(-0.27) 2.29(+0.35) 2.63(+0.55) 2.35(-0.17) 2.21(+0.10) 3.36(+1.85) 4.34(+0.91) 5.12(+0.58) 3.83(+1.15) 4.16(+1.12) 2.24(+0.11) 2.06(+0.14) 2.15(+0.21) 2.44(+0.36) 2.87(+0.35) 2.33(+0.22) 3.41(+1.90) 4.82(+1.39) 4.91(+0.37) 3.89(+1.21) 4.26(+1.22) 2.33(+0.20) 2.10(+0.18) 2.56(+0.62) 2.59(+0.51) 2.83(+0.31) 2.46(+0.34) 25 Table 19: Ablation results of Content control for video captioning on AnyCapEval under different training data ratio configurations. Model IPos. IApp. InternVL2.5-8B 3.08 4.33 Content IAct. 3.44 Ins. 3.78 Per. 3.26 Mov. 3.16 Bkg. 4.56 Evt. 2.55 Avg. 3.52 2B-121 2B-111 2B-122 2B-212 2B2B-102 8B-111 8B-221 3.44(+0.36) 5.72(+1.39) 3.01(-0.43) 4.62(+0.84) 4.74(+1.48) 6.20(+3.04) 5.43(+0.87) 4.36(+1.81) 4.69(+1.17) 3.51(+0.43) 6.72(+2.39) 3.12(-0.32) 5.03(+1.25) 4.34(+1.08) 6.05(+2.89) 5.81(+1.25) 4.60(+2.05) 4.90(+1.38) 3.95(+0.87) 6.13(+1.80) 2.94(-0.50) 5.05(+1.27) 4.50(+1.24) 6.57(+3.41) 5.37(+0.81) 4.17(+1.62) 4.84(+1.32) 4.30(+1.22) 6.36(+2.03) 3.74(+0.30) 4.79(+1.01) 4.20(+0.94) 6.32(+3.16) 5.84(+1.28) 4.68(+2.13) 5.03(+1.51) 4.67(+1.59) 6.32(+1.99) 3.19(-0.25) 5.22(+1.44) 4.66(+1.40) 6.49(+3.33) 5.49(+0.93) 4.50(+1.95) 5.07(+1.55) 3.60(+0.52) 6.19(+1.86) 5.62(+2.18) 4.80(+1.02) 4.64(+1.38) 5.15(+1.99) 5.15(+0.59) 4.66(+2.11) 4.98(+1.46) 4.88(+1.80) 6.92(+2.59) 6.28(+2.84) 5.39(+1.61) 5.38(+2.12) 5.44(+2.28) 5.98(+1.42) 5.62(+3.07) 5.74(+2.22) 4.95(+1.87) 6.39(+2.06) 7.03(+3.59) 5.49(+1.71) 5.85(+2.59) 5.66(+2.50) 5.49(+0.93) 4.95(+2.40) 5.73(+2.21) Table 20: Ablation results of Style control for video captioning on AnyCapEval under different training data ratio configurations. Model Brf. InternVL2.5-8B 1.39 2B-121 2B-111 2B2B-212 2B-221 2B-102 8B-111 8B-221 1.92(+0.53) 1.76(+0.37) 1.89(+0.50) 1.74(+0.35) 1.84(+0.45) 1.61(+0.22) 1.68(+0.29) 2.00(+0.61) Style Poe. 1.91 2.40(+0.49) 2.40(+0.49) 2.12(+0.21) 2.30(+0.39) 2.48(+0.57) 2.15(+0.24) 2.23(+0.32) 2.20(+0.29) Thm. 2.23 2.45(+0.22) 2.62(+0.39) 2.65(+0.42) 2.50(+0.27) 2.45(+0.22) 2.58(+0.35) 2.55(+0.32) 2.50(+0.27) Det. 1. 1.88(+0.11) 1.77(+0.00) 1.80(+0.03) 1.77(+0.00) 1.80(+0.03) 1.88(+0.11) 1.93(+0.16) 1.88(+0.11) Nar. 2.34 2.39(+0.05) 2.61(+0.27) 2.42(+0.08) 2.68(+0.34) 2.39(+0.05) 2.58(+0.24) 2.74(+0.40) 2.61(+0.27) Avg. 1.93 2.20(+0.27) 2.22(+0.29) 2.17(+0.24) 2.18(+0.25) 2.19(+0.26) 2.14(+0.21) 2.21(+0.28) 2.24(+0.31) Table 21: Ablation results of controllable audio captioning on AnyCapEval under different training data ratio configurations. Model GPT-4o 2B-121 2B-111 2B-122 2B-212 2B2B-102 8B-111 8B-221 Content Style Poe. 0.88 0.91(+0.03) 1.00(+0.12) 0.89(+0.01) 0.93(+0.05) 0.96(+0.08) 0.78(-0.10) 1.00(+0.12) 0.96(+0.08) Avg. 1.18 1.27(+0.09) 1.36(+0.18) 1.30(+0.12) 1.31(+0.13) 1.26(+0.08) 1.16(-0.02) 1.28(+0.10) 1.26(+0.08) Evt. 1.59 1.67(+0.08) 1.82(+0.23) 1.78(+0.19) 1.81(+0.22) 1.89(+0.30) 1.96(+0.37) 2.13(+0.54) 1.73(+0.14) Brf. 1.42 1.51(+0.09) 1.53(+0.11) 1.53(+0.11) 1.56(+0.14) 1.40(-0.02) 1.37(-0.05) 1.44(+0.02) 1.40(-0.02) Nar. 1.24 1.38(+0.14) 1.53(+0.29) 1.47(+0.23) 1.45(+0.21) 1.43(+0.19) 1.33(+0.09) 1.40(+0.16) 1.43(+0.19) 26 Figure 6: Qualitative examples of controllable caption generation from images. 27 Figure 7: Qualitative examples of controllable caption generation from videos. Figure 8: Qualitative examples of controllable caption generation from audios. 29 Figure 9: Images in the first column are original real images. The second column shows images generated from GPT-4o captions using DALLE 3, while the third column shows results from our refined captions. Our method leads to more faithful visual content and better alignment with the original image semantics. 30 Figure 10: Enhanced text-to-video generation through refined caption quality. Videos in the top row are generated from original dataset captions. The bottom row showcases videos generated using our models refined captions, demonstrating improved visual fidelity and more expressive camera motion. Figure 11: Human evaluation of ACM-8B versus GPT-4o with respect to content and style. Bars indicate the proportion of responses rating ACM-8B as worse, similar, or better. Overall, ACM-8B demonstrates superior performance in most cases. 31 Figure 12: Screenshot of human evaluation."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "Tsinghua University"
    ]
}