{
    "paper_title": "Efficient Agent Training for Computer Use",
    "authors": [
        "Yanheng He",
        "Jiahe Jin",
        "Pengfei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Scaling up high-quality trajectory data has long been a critical bottleneck for developing human-like computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 human-annotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved a remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from a small amount of high-quality trajectory data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 0 9 3 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Efficient Agent Training for Computer Use",
            "content": "Yanheng He1,3 Jiahe Jin1,3 Pengfei Liu1,2,3 1Shanghai Jiao Tong University 2SII 3Generative AI Research Lab (GAIR)"
        },
        {
            "title": "Abstract",
            "content": "Scaling up high-quality trajectory data has long been critical bottleneck for developing humanlike computer use agents. We introduce PC Agent-E, an efficient agent training framework that significantly reduces reliance on large-scale human demonstrations. Starting with just 312 humanannotated computer use trajectories, we further improved data quality by synthesizing diverse action decisions with Claude 3.7 Sonnet. Trained on these enriched trajectories, our PC Agent-E model achieved remarkable 141% relative improvement, surpassing the strong Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, an improved benchmark we also released. Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld. Our findings suggest that strong computer use capabilities can be stimulated from small amount of high-quality trajectory data. We open-source our entire suite of code, data, and models to facilitate future research at https://github.com/GAIR-NLP/PC-Agent-E."
        },
        {
            "title": "Introduction",
            "content": "Developing autonomous agents that can operate computers as humans do (Anthropic, 2024; He et al., 2024; OpenAI, 2025a; Qin et al., 2025; Team, 2025) has long been landmark pursuit in Artificial Intelligence (AI). Such agents could unlock unprecedented capabilities and significantly reduce human workload across various tasks. However, current models still fall significantly short of human performance (Xie et al., 2024; Bonatti et al., 2024), largely due to insufficient computer knowledge and poor long-horizon planning ability. key factor contributing to these deficiencies is the extreme scarcity of high-quality computer use trajectory data (He et al., 2024; Ou et al., 2024; Xu et al., 2025a). Previous works have investigated various strategies for trajectory collection. Some approaches rely entirely on human annotations to obtain trajectories (Lu et al., 2024; Li et al., 2024), while others deploy agents to perform tasks directly in real-world environments (Murty et al., 2024; Patel et al., 2024). In addition, some methods synthesize trajectories by leveraging data extracted from online tutorials (Ou et al., 2024; Xu et al., 2025a). Figure 1: PC Agent-E achieves state-of-the-art opensource performance in Windows computer use with just 312 augmented trajectories. In this work, we explore efficient agent training for computer use. Inspired by recent findings (Huang et al., 2024; Muennighoff et al., 2025; Ye et al., 2025) that synthesizing high-quality data using advanced reasoning models like Deepseek-R1 (Guo et al., 2025) can efficiently enhance LLM reasoning, we extend the similar idea to the field of computer use agents. *Equal Contribution. Corresponding author. Figure 2: Overview of our framework, consisting of four key components: (1) Trajectory Collection, gathering small set of human trajectories by recording user actions and state observations at each step; (2) Thought Completion, reconstructing the implicit thought process missing in raw human trajectories; and (3) Trajectory Boost, diversifying action decisions to further enhance trajectory quality (4) Agent Training, developing strong computer use agent with remarkable data efficiency. We propose PC Agent-E, an efficient agent training framework that integrates human expertise with AI automation. Starting from small set of real-world human computer use trajectories, we leverage frontier agent model to diversify action decisions (action with thought in ReAct (Yao et al., 2023) paradigm), further enhancing the data quality. Training on these augmented trajectories, our agent demonstrates strong computer use capabilities with remarkable data efficiency. We begin by collecting 312 human computer use trajectories with PC Tracker (He et al., 2024), tool for gathering human-computer interaction data, with only two humans annotating one day. These trajectories include task descriptions, screenshots, and keyboard/mouse actions. We subsequently reconstruct the implicit thought process behind human actions, obtaining comprehensive human trajectories with thoughts. The successful completion of these tasks is inherently assured by human proficiency, obviating the need for additional verification. While these human trajectories already serve as valuable agent training data, we further enhance their quality through Trajectory Boost, data synthesis method that enriches each trajectory step with diverse set of alternative action decisions. The key insight is that computer use tasks can be completed through multiple valid pathways, meaning each step has various reasonable action alternatives supported by rational thought. To capture this diversity, we use strong agent model to synthesize other possible action decisions. Specifically, recognizing that each human trajectory step captures an environment snapshot essential for computer use agents to make decisions, we provide these snapshots to Claude 3.7 Sonnet (Anthropic, 2025a) and sample multiple possible action decisions, thereby significantly enriching and diversifying the trajectory data. Experimental results demonstrate that small set of high-quality trajectory data can indeed elicit complex agent capabilities for computer use. Trained with only 312 trajectories augmented by Claude 3.7 Sonnet (without extended thinking), our PC Agent-E model achieves an impressive 141% relative improvement over the base model Qwen2.5-VL-72B and even outperforms Claude 3.7 Sonnet with extended thinking (Anthropic, 2025b) on WindowsAgentArena-V2, benchmark we improved from WindowsAgentArena (Bonatti et al., 2024). Furthermore, PC Agent-E demonstrates strong generalizability to different operating systems on OSWorld (Xie et al., 2024), another popular computer use benchmark focused on Linux tasks. This extreme data efficiency directly stems from our focus on high-quality trajectories, ensuring both real-world task completion and fostering diverse action decisions through our human trajectory collection and Trajectory Boost method. In summary, our key contributions are threefold: 1. We propose Trajectory Boost, simple but effective data synthesis method that significantly enhances trajectory quality by enriching each step with diverse alternative action decisions. Leveraging each step of human trajectory as an environment snapshot, our method substantially increases action decision diversity. 2. We release WindowsAgentArena-V2, an improved benchmark rectifying evaluation dependence, infeasible hacking and other limitations in the original WindowsAgentArena, ensuring more robust and fair evaluations of Windows computer use. 3. We developed PC Agent-E, state-of-the-art (SOTA) open-source model for Windows computer use, which exemplifies exceptional data efficiency. Trained with just 312 augmented trajectories, our model successfully outperforms the strong Claude 3.7 Sonnet with extended thinking. Besides, we demonstrate its generalizability across different operating systems on OSWorld."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Computer Use Agent With the advancement of Vision-Language Models (VLMs) (Bai et al., 2025; Deitke et al., 2024), the way computer use agents interact with computers has gradually shifted from relying on textual representations such as accessibility trees (Agashe et al., 2024; Wu et al., 2024a; Jia et al., 2024) to directly using screenshots (Qin et al., 2025; Xu et al., 2025b; He et al., 2024). Existing computer use agents can be categorized based on the extent of human prior embedded in scaffold design: One is modular agent workflows (Agashe et al., 2024; Wu et al., 2024a; Liu et al., 2025), which defines specialized modules and prompts multi-agents to collaborate. The other is native agent models (Anthropic, 2024; Qin et al., 2025; OpenAI, 2025a), which depends on single model to take action step by step based on its history and current state. While modular agent workflows can reduce task complexity, their heavy reliance on human priors makes them vulnerable to unexpected errors and unfamiliar domains (Qin et al., 2025). Furthermore, predefined workflows are hard to improve through end-to-end training (Saltzer et al., 1984; Pan et al., 2024). With the continuous enhancement of model capabilities, native agent models have emerged as the dominant paradigm. This approach demonstrates superior flexibility and generalizability and offers sustainable improvement potential through either supervised fine-tuning (SFT) (Xu et al., 2025b) or reinforcement learning (RL) (OpenAI, 2025a). Our work explores the efficient agent training methods for native agent models through SFT. 2.2 Data Synthesis As Large Language Models (LLMs) grow ever more powerful (OpenAI, 2024a,b), it has become common practice to use them to synthesize data. Distillation methods (Taori et al., 2023; Gunasekar et al., 2023; Xu et al., 2023) leverage SOTA models to generate large-scale data for training smaller models. On the other hand, self-improvement methods enable model to bootstrap and refine its own training data (Wang et al., 2023). Beyond basic instruction-following, (Zelikman et al., 2022) first investigates synthesis thinking processes for reasoning tasks. More recently, (Muennighoff et al., 2025; Ye et al., 2025) demonstrate that advanced reasoning models can synthesize high-quality reasoning data that can boost models reasoning capability efficiently. In the domain of computer use agents, data synthesis can be broadly categorized into three aspects: (1) large-scale datasets that build foundational graphical user interface (GUI) understanding, with tasks like screenshot captioning or questionanswering (Liu et al., 2024; Qin et al., 2025) (2) single-step visual grounding tasks, where tasks of single click are synthesized from specific locations on the GUI (Gou et al., 2025; Wu et al., 2024b) (3) multi-step trajectory synthesis, in which recent research has explored leveraging web tutorials to guide trajectory generation (Ou et al., 2024; Xu et al., 2025a) or reverse-synthesizes tasks from the agents own exploration records (Sun et al., 2025; Murty et al., 2024). Our work differs from prior work by synthesizing high-quality multi-step trajectories from real-world human demonstrations and emphasizing data efficiency."
        },
        {
            "title": "3 Method",
            "content": "3.1 Overview We propose PC Agent-E, an efficient agent training framework that elicits strong computer use capabilities with remarkable data efficiency, as illustrated in Figure 2. Our method generates high-quality trajectory data by combining authentic human-computer interactions with diverse action decisions, offering advantages in both realism and diversity. 1. First, we gathered small set of 312 task trajectories from human annotators, recording both the screen state observation and the human action at each step, and then filtered the data to remove erroneous steps and trajectories. (3.2) 2. Subsequently, we reconstructed the latent human thought process before each action decision, based on the corresponding screen state observation and history step context. (3.3) 3. Then, using human trajectories as environment snapshots, we employ Claude 3.7 Sonnet to synthesize diverse alternative action decisions with the Trajectory Boost method. (3.4) 4. Finally, we develop PC Agent-E, our SOTA native agent model for Windows computer use, trained on our augmented trajectories with simple end-to-end scaffold. (3.5) 3.2 Trajectory Collection 3 3.3 Thought Completion Figure 3: An example trajectory collected by PC Tracker. We collected human computer use trajectories (see an example at Figure 3) with PC Tracker (He et al., 2024), tool that records the screen state observation and the keyboard/mouse action at each step for given task. The recorded actions are structured in unified action space A, as shown in Table 1. For task generation, we first manually compose small seed set across multiple software applications and then enlarge it with LLMs. The resulting tasks were distributed to human annotators, who completed the tasks on their own Windows computers with PC Tracker recording trajectories automatically. After finishing task, annotators could either discard unsatisfactory trajectories or modify the task descriptions based on their actual execution, thereby ensuring the correctness and completeness of the collected trajectories. We then applied set of rule-based filters to remove entire trajectories or individual steps that exhibited errors or other undesirable behaviors. Figure 4: Distribution of the 312 task trajectories across different applications. We employed rigorous data decontamination procedure on these collected trajectories. Each task description was compared against the tasks in our main evaluation benchmark (4) using n-gram overlap and semantic similarity metrics. Trajectories with tasks exhibiting excessive similarity to any test task were removed from the dataset. This procedure finally yielded 312 real-world human trajectories, with distribution across applications shown in Figure 4. The whole annotation process was completed by two annotators within single day, with an average of roughly 3 minutes per trajectory. Given humans proficiency in computer use, the mechanisms for annotators to discard trajectories or revise task descriptions after execution, and our data filtering process, no additional verification was required to ensure trajectory correctness. 3.3 Thought Completion We first reconstruct the implicit thought process behind human actions using an iterative approach. Specifically, for each action in the trajectory, we task descripprovide Claude 3.7 Sonnet with: tion, historical actions with their previously reconstructed thought processes, the current action, and the corresponding screenshot. Based on this information, the model generates humans thought process behind the action. As shown in Figure 5, the recorded raw human trajectory was converted to human trajectory with thoughts, where the reconstructed thought process is added to each step. See prompt in Appendix B.1. Action Description click (x, y) right click (x, y) double click (x, y) drag from (x1, y1) to (x2, y2) scroll (x) press key: enter hotkey (ctrl, c) type text: hello wait finish fail clicks at coordinates. right-click at coordinates. double-click at coordinates. drag the mouse. scrolls the screen with offset x. presses the Enter key. performs the Ctrl+C hotkey. type text hello. pauses for some time. the task is finished. the task is failed. Table 1: Unified action space A. 4 3.4 Trajectory Boost Figure 5: Visualization of our Trajectory Boost method. (Left) Raw human trajectory recorded by PC Tracker. (Center) Human trajectory with reconstructed thoughts after Thought Completion, where the red node indicates human action decisions. (Right) The final Traj Tree, where the blue node indicates augmented diverse action decisions synthesized by Claude 3.7 Sonnet. 3.4 Trajectory Boost After thought completion, we obtain comprehensive human trajectories with explicit thought processes. While these trajectories already serve as valuable agent training samples, we further enhance their diversity through simple but effective approach called Trajectory Boost, which synthesizes diverse alternative action decisions for each step of the trajectory. The key insight behind Trajectory Boost is that computer use tasks inherently allow for multiple valid solution pathways. Consequently, at any given step, several reasonable actions supported by rational thought processes may exist, extending beyond the single solution adopted by human annotators. To capture this inherent diversity, we employ frontier computer use agent, Claude 3.7 Sonnet, to generate these alternative action decisions. Its long-horizon planning capabilities, advanced reasoning patterns, and broad knowledge of computer use enable it to generate thought processes and actions that are highly informative and valuable, thereby substantially enhancing the richness and diversity of our trajectory data. Specifically, we recognize that each step in human trajectory captures an environment snapshot of the computer, providing the necessary information for both humans and agents to make decisions. For step on human trajectory with observation ok, thought process tk, action ak and task description , the environment snapshot is < T, ok, hk >, where the history context hk = (t1, a1, t2, a2, . . . , tk1, at1) is constructed with previous human steps. We input this environment snapshot to Claude 3.7 Sonnet instantiate in the PC Agent-E scaffold (3.5), and sample multiple single-step action decisions (t k) from it. Prompt used are shown in Appendix B.2. In practice, we sample 9 action decisions in parallel. This produces Traj Tree, as shown in Figure 5, with human trajectory forming the main trunk and the augmented action decisions branching off as leaf nodes. These sampled action decisions from Claude 3.7 Sonnet are not executed in real computer environments, but serve as important augmented data for later agent training. k, 3.5 Agent Training Figure 6: training example that also demonstrates the inference process of the PC Agent-E scaffold. PC Agent-E adopts deliberately simple end-to-end scaffold, as our primary focus is on validating the efficiency of our agent training methodology rather than optimizing performance through complex workflow design or elaborate prompt engineering. At inference, PC Agent-E takes screenshot, task description, history input and outputs thought, action decision in the ReAct (Yao et al., 2023) paradigm, as shown in Figure 6. The action space is the same as in Table 1, and every action is executed via the PyAutoGUI Figure 7: (Left) Overview of the WindowsAgentArena benchmark. (Right) Our main modifications to the updated WindowsAgentArena-V2 benchmark. library. The history is textual log of previous thoughts and actions. To maintain simplicity in both training and inference, past screenshots are excluded, although we believe that adding this image history would be beneficial for improving model performance. Prompt used for the scaffold is shown in Appendix B.3. For training, we transform each action node from our Traj Tree into an individual training instance. The training sample structure and the inference-time scaffold of the agent share direct correspondence, as illustrated in Figure 6. For both human-demonstrated and model-synthesized action nodes, the history in the training sample includes only prior human actions on the main trunk of the Trajectory Tree. This is consistent with the historical context available to both humans and the model when making the corresponding action decision. We finally obtained 27K training instances from 312 augmented trajectories, each following consistent input-output structure at inference time."
        },
        {
            "title": "4 WindowsAgentArena-V2",
            "content": "We initially performed our evaluation on WindowsAgentArena (Bonatti et al., 2024), benchmark designed to assess computer use ability in realistic Windows OS environments through diverse tasks across multiple applications. It provides automatic initial configuration of virtual machine (VM) state and hand-written evaluation rules (see Figure 7). However, we identified several limitations during our assessment. To ensure evaluation reliability, we developed WindowsAgentArena-V2, an updated benchmark comprising 141 tasks across 11 widely-used Windows applications, all derived from the original WindowsAgentArena but with improvements detailed below. Addressing the evaluation dependency issue. The original benchmark lacked VM state reset between task evaluations, allowing changes from previous tasks to potentially affect subsequent ones. We implemented VM snapshot restoration before each evaluation, ensuring consistent starting states, preventing inter-task interference, and aligning with the i.i.d. (independent and identically distributed) assumption. We also installed some essential software missing from the original VM snapshot but required for proper evaluation. Preventing infeasible hacking. Current computer use benchmarks such as WindowsAgentArena and OSWorld often include infeasible tasks, which are inherently impossible to complete due to issues such as deprecated system features or user-generated hallucinated commands (Xie et al., 2024). The evaluation metric for these tasks is simply considering task successful if the action FAIL is output at any point during execution. However, we found such evaluation methods particularly easy to hack, as agents can achieve high scores on infeasible tasks without demonstrating meaningful computer-use capabilities. In contrast, completing feasible task typically requires the agent to execute actions step-by-step to actually fulfill the task objective, posing significantly different level of difficulty. We refer to this phenomenon as infeasible hacking, vulnerability confirmed by our subsequent experiments (5.5), in which weaker model achieved markedly higher scores on infeasible tasks. Since agents receive identical scores for feasible and infeasible tasks, their coexistence undermines benchmark fairness. Additionally, given that current computer use agents abilities are far from optimal, we argue it is presently more valuable to focus on enhancing agents performance on feasible tasks. Therefore, as temporary solution in WindowsAgentArena-V2, we removed all infeasible tasks to prevent infeasible hacking. Guaranteeing VM initial state stability. We found that the state of VM after task initial configuration often exhibited errors like unstable network connections, software launch failures, or system lags. To address this, we designed validation framework combining rule-based and LLM-based evaluations to verify the initial state, with re-test mechanism allowing up to three restart attempts for faulty initializations. This approach reduced the initialization failure rate from 10%30% (depending on hardware) to below 5%. Fixing evaluation flaws. We discovered that some evaluation functions contained bugs or lacked robustness. For instance, in the task clearing YouTube history to facilitate finding other histories, the evaluation erroneously awarded full scores to agents that deleted the entire browsing history, clearly contradicting user intent. We identified and corrected several evaluation errors and relied on human evaluators for few complex tasks to improve assessment reliability."
        },
        {
            "title": "5 Experiment",
            "content": "In this section, we evaluate PC Agent-E through four experiments: main performance assessment on our improved benchmark WindowsAgentArena-V2 (5.2), train time action scaling to validate the effect of our Trajectory Boost method (5.3), test time action scaling (5.4), and cross-platform generalization to Linux environments using OSWorld (5.5). 5.1 Setup Benchmarks We use WindowsAgentArena-V2 (4) for the main evaluation, as our training data were collected on the Windows system. To test generalization across operating systems, we also report results on OSWorld (Xie et al., 2024), popular benchmark designed on Linux systems. Baseline We compare PC Agent-E with various strong baselines. These include the proprietary frontier models Claude 3.7 Sonnet (Anthropic, 2025a) and Claude 3.7 Sonnet with extended thinking (Anthropic, 2025b), as well as leading open-source models UI-TARS (Qin et al., 2025), UI-TARS-1.5 (Team, 2025), and Qwen2.5-VL-72B (Bai et al., 2025). For UI-TARS-1.5, we evaluated its open-source UI-TARS-1.5-7B version, as its larger counterpart was not accessible. Settings All experiments and models utilized screenshot-only observation setting with uniform screen resolution of 1280 720. For the UI-TARS model series, we adopted their native framework, which supports image history and code block actions. All other models, including Claude and Qwen, were evaluated using our simple PC Agent-E scaffold. The default maximum number of steps was set to 30, and we also investigated the impact of varying this step limit on model performance. Training We train our PC Agent-E model based on the Qwen2.5-VL-72B backbone with 27k data mentioned in 3.5. We set the image resolution to 1280 720 and context length to 8,192 tokens. Further training details can be found in Appendix A. 5.2 Main Results As shown in Table 2, our PC Agent-E achieves remarkable 141% relative improvement over the Qwen2.5VL-72B baseline on WindowsAgentArena-V2. More significantly, PC Agent-E establishes itself as the SOTA open-source computer use agent on Windows, achieving the best performance on most applications. It is worth highlighting that the Claude 3.7 Sonnet model used to synthesize our training data did not have thinking mode enabled, making our achievement even more substantial as we surpassed the stronger Claude 3.7 Sonnet with extended thinking. Table 2: Results of successful rate (%) for different models on WindowsAgentArena-V2. Models Number of Tasks Qwen2.5-VL-72B UI-TARS-1.5-7B UI-TARS-72B-DPO Claude 3.7 Sonnet Claude 3.7 Sonnet (thinking) PC Agent-E (Ours) Libreoffice Chrome Edge System VS Code VLC Utils Total 24 20.8 45.8 58.3 54.2 66.7 50. 19 26.3 21.1 36.8 52.6 52.6 57.9 14 7.6 7.6 7.6 29.0 21.9 35. 12 16.7 16.7 25.0 16.7 25.0 33.3 141 14.9 21.3 26.2 32.6 35.4 36. 42 0.0 7.1 0.0 2.4 2.4 4.8 17 34.7 34.7 40.6 46.5 64.1 64. 13 15.4 23.1 38.5 61.5 46.2 46.2 7 5.3 Train Time Action Scaling Figure 8: (a) Performance on WindowsAgentArena-V2 with different training data scaling factor s. (b) Performance on WindowsAgentArena-V2 with different max steps before and after training. Analysis Our result reveals that complex agent capabilities for computer use can be elicited by remarkably small set of high-quality trajectory data. What lies behind our extreme data efficiency is our high data quality. We attribute this high quality to two critical factors: (1) Real-world task completion, guaranteed through human-annotated task trajectories that reliably record authentic interaction processes and ensure successful task execution; and (2) Diverse action decisions, synthesized by frontier agents model that can generate multiple reasonable actions with rational thought at each step beyond the single solution chosen by human annotators. To gain deeper insight into the specific capabilities enhanced through our training, we conducted qualitative analysis by examining 50 task trajectories that Qwen2.5-VL-72B failed to complete but PC Agent-E successfully accomplished, as well as trajectories where both models failed. We categorized the failure patterns into three types, which may occur simultaneously within single trajectory or even single step: (1) Knowledge: models may lack specific computer use knowledge. For instance, model might be unaware of how to enable particular feature in VLC (a popular media player software), thus failing to complete the task. (2) Planning: models may make incorrect action decisions, such as failing to recognize and recover from previous erroneous actions. (3) Grounding: even when the models planning is correct, the executed actions might be inconsistent with its planning process, primarily manifested as mouse-clicking errors. Analysis indicates that our improvements primarily stem from enhanced long-horizon planning capabilities. After training, PC Agent-E produces noticeably longer thought processes and demonstrates improved reasoning abilities in verification, reflection, and self-correction. Besides, both Qwen2.5-VL-72B and PC Agent-E occasionally fail due to grounding and knowledge issues. 5.3 Train Time Action Scaling In 3.4, we synthesize 9 alternative action decisions at each step of human trajectory using the Trajectory Boost method. To validate the effect of this data augmentation method, we vary the number of synthesized actions used during training and evaluate model performance. Let be the number of synthesized actions used per step, and = + 1 denote the scaling factor of the training data after augmentation. When < 9, the action decisions were randomly selected from the 9 synthesized options. For our final PC Agent-E model, all 9 synthesized actions were included, yielding = 10. In contrast, for the model trained solely on human actions, = 1. All training data followed the structure detailed in 3.5. As shown in Figure 8(a), model performance significantly scales up with increased action decisions. Compared to training on human trajectories alone (only 15% relative improvement over the base model), PC Agent-E achieved significantly greater gains, demonstrating the effectiveness of the Trajectory Boost method in enhancing data quality and enabling efficient agent training. 5.4 Test Time Action Scaling We also investigated how the performance of PC Agent-E varies with test time scaling, topic that has received increasing attention in the research community (Wu et al., 2025; Brown et al., 2024; Snell et al., 2024). We evaluated the models performance with different numbers of max steps allowed during task 8 5.5 Cross-Platform Evaluation completion. As shown in Figure 8(b), the difference in performance between two models increases over time as the agent continues to interact with the computer. With improved long-horizon planning capabilities, PC Agent-E benefits from action scalingthe ability to iteratively take more actions to respond to environmental changes, explore for solutions, correct errors, and ultimately solve the task. After training, PC Agent-E can allocate more time and computational resources to computer use tasks to achieve superior results. 5.5 Cross-Platform Evaluation We further evaluated our model on the popular OSWorld benchmark to assess cross-platform generalization capabilities. As shown in Table 3, our PC Agent-E model, despite being trained exclusively on Windows data, demonstrates 34% relative improvement in computer use capabilities on Linux systems as well. These results validate the strong cross-platform generalizability of our efficient agent training methodology. Table 3: Successful Rate (%) on OSWorld for 30-step evaluation. Models Feasible Infeasible Total Number of Tasks Qwen2.5-VL-72B PC Agent-E (Ours) 339 4.4 10.9 30 86.7 63.3 369 11.1 14. We also identified an interesting phenomenon in the experiments, which we designate as infeasible hacking in 4: the weaker Qwen2.5-VL-72B model paradoxically achieved markedly better performance on infeasible tasks. This observation suggests that current infeasible task evaluations do not accurately reflect computer use agents capabilities. Future research may focus on designing stronger evaluation criteria for infeasible tasks, including assessing an agents rationale when declaring task as impossible, to minimize exploitative behaviors."
        },
        {
            "title": "6 Conclusion and Discussion",
            "content": "In this work, we introduced PC Agent-E, an efficient agent training framework for computer use. With just 312 human-annotated trajectories augmented through our Trajectory Boost method, PC Agent-E achieved 141% improvement over the base model and outperformed Claude 3.7 Sonnet with extended thinking on WindowsAgentArena-V2, while demonstrating strong generalizability on OSWorld. Our findings demonstrate that complex agent capabilities for computer use can be elicited by remarkably small set of high-quality trajectory data. Looking forward, with the significant breakthroughs of RL in reasoning models, researchers have recently begun applying RL to long-horizon tasks (OpenAI, 2025a,b,c) and achieving encouraging progress. However, several studies (Guo et al., 2025; Yue et al., 2025) indicate that RLs effectiveness is closely tied to the capabilities of the foundation models. Currently, the computer use capabilities of foundation models remain far from sufficient, even in state-of-the-art proprietary models employing RL (OpenAI, 2025a). We have reason to believe that collecting computer use data for pre-training and post-training remains crucial for achieving truly intelligent digital world agents. The future direction will likely involve the collaborative development of RL and SFT, complementing and enhancing each other."
        },
        {
            "title": "7 Limitations",
            "content": "There are multiple potential limitations in this work, primarily due to resource constraints. First, we only used Claude 3.7 Sonnet for data synthesis rather than more advanced models like OpenAI Operator (OpenAI, 2025a), though our impressive results suggest stronger models would likely yield further improvements. Second, we only fine-tuned one model, Qwen2.5-VL-72B, as our representative backbone. Third, we did not explore how increasing trajectory count beyond our current dataset might affect performance, yet our results with extremely limited trajectories already demonstrate remarkable data efficiency."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to express our sincere gratitude to Shijie Xia for his meticulous review and constructive suggestions, which significantly improved the quality of this paper. This project is supported by SJTU SEIEE - ByteDance Large Language Model Joint Laboratory, SII. 9 References"
        },
        {
            "title": "References",
            "content": "[1] Saaket Agashe, Jiuzhou Han, Shuyu Gan, Jiachen Yang, Ang Li, and Xin Eric Wang. 2024. Agent s: An open agentic framework that uses computers like human. arXiv preprint arXiv:2410.08164. [2] Anthropic. 2024. Introducing computer use. [3] Anthropic. 2025a. Claude 3.7 sonnet. [4] Anthropic. 2025b. Claudes extended thinking. [5] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-vl technical report. [6] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, Lawrence Jang, and Zack Hui. 2024. Windows agent arena: Evaluating multi-modal os agents at scale. [7] Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. 2024. Large language monkeys: Scaling inference compute with repeated sampling. [8] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. [9] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2025. Navigating the digital world as humans do: Universal visual grounding for gui agents. [10] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sebastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks are all you need. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. [12] Yanheng He, Jiahe Jin, Shijie Xia, Jiadi Su, Runze Fan, Haoyang Zou, Xiangkun Hu, and Pengfei Liu. 2024. Pc agent: While you sleep, ai works cognitive journey into digital world. [13] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. 2024. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489. [14] Chengyou Jia, Minnan Luo, Zhuohang Dang, Qiushi Sun, Fangzhi Xu, Junlin Hu, Tianbao Xie, and Zhiyong Wu. 2024. Agentstore: Scalable integration of heterogeneous agents as specialized generalist computer assistant. arXiv preprint arXiv:2410.18603. [15] Wei Li, William Bishop, Alice Li, Christopher Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the effects of data scale on ui control agents. Advances in Neural Information Processing Systems, 37:9213092154. [16] Haowei Liu, Xi Zhang, Haiyang Xu, Yuyang Wanyan, Junyang Wang, Ming Yan, Ji Zhang, Chunfeng Yuan, Changsheng Xu, Weiming Hu, and Fei Huang. 2025. Pc-agent: hierarchical multi-agent collaboration framework for complex task automation on pc. [17] Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan Xiong, Wenhu Chen, Graham Neubig, and Xiang Yue. 2024. Harnessing webpage uis for text-rich visual understanding. [18] Quanfeng Lu, Wenqi Shao, Zitao Liu, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, Yu Qiao, and Ping Luo. 2024. Gui odyssey: comprehensive dataset for cross-app gui navigation on mobile devices. arXiv preprint arXiv:2406.08451. [19] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393. 10 References [20] Shikhar Murty, Dzmitry Bahdanau, and Christopher Manning. 2024. Nnetscape navigator: Complex demonstrations for web agents without demonstrator. arXiv preprint arXiv:2410.02907. [21] OpenAI. 2024a. Hello gpt-4o. openai.com. [22] OpenAI. 2024b. Introducing openai o1. openai.com. [23] OpenAI. 2025a. Computer-using agent. [24] OpenAI. 2025b. Introducing deep research. [25] OpenAI. 2025c. Introducing openai o3 and o4-mini. [26] Tianyue Ou, Frank F. Xu, Aman Madaan, Jiarui Liu, Robert Lo, Abishek Sridhar, Sudipta Sengupta, Dan Roth, Graham Neubig, and Shuyan Zhou. 2024. Synatra: Turning indirect knowledge into direct demonstrations for digital agents at scale. [27] Jiayi Pan, Xingyao Wang, Graham Neubig, Navdeep Jaitly, Heng Ji, Alane Suhr, and Yizhe Zhang. 2024. Training software engineering agents and verifiers with swe-gym. arXiv preprint arXiv:2412.21139. [28] Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris CallisonBurch, and Sepp Hochreiter. 2024. Large language models can self-improve at web agent tasks. arXiv preprint arXiv:2405.20309. [29] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. 2025. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326. [30] Jerome H. Saltzer, David P. Reed, and David D. Clark. 1984. End-to-end arguments in system design. ACM Transactions on Computer Systems, 2(4):277288. [31] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. [32] Qiushi Sun, Kanzhi Cheng, Zichen Ding, Chuanyang Jin, Yian Wang, Fangzhi Xu, Zhenyu Wu, Chengyou Jia, Liheng Chen, Zhoumianze Liu, Ben Kao, Guohao Li, Junxian He, Yu Qiao, and Zhiyong Wu. 2025. Os-genesis: Automating gui agent trajectory construction via reverse task synthesis. [33] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpaca: strong, replicable instruction-following model. Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html, 3(6):7. [34] ByteDance Seed Team. 2025. Introducing ui-tars-1.5. seed-tars.com. [35] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023. Self-instruct: Aligning language models with self-generated instructions. [36] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2025. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models. [37] Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. 2024a. Os-copilot: Towards generalist computer agents with self-improvement. arXiv preprint arXiv:2402.07456. [38] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. 2024b. Os-atlas: foundation action model for generalist gui agents. [39] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. [40] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. [41] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. 2025a. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials. 11 References [42] Yiheng Xu, Zekun Wang, Junli Wang, Dunjie Lu, Tianbao Xie, Amrita Saha, Doyen Sahoo, Tao Yu, and Caiming Xiong. 2025b. Aguvis: Unified pure vision agents for autonomous gui interaction. [43] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. [44] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387. [45] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. 2025. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837. [46] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning."
        },
        {
            "title": "A Training Details",
            "content": "The PC Agent-E model is fine-tuned on dataset of 27k samples for 2 epochs using 32 NVIDIA GPUs over approximately 5 hours, with Qwen2.5-VL-72B as the base model. We set the context length to 8,192 tokens, using batch size of 128 and learning rate of 2e-6. The training process employs cosine annealing for learning rate scheduling, with warm-up ratio of 0.05. The visual tower is kept frozen throughout the training process."
        },
        {
            "title": "B Prompts",
            "content": "B.1 Thought Completion Table 4: Thought Completion Prompt Prompt for Thought Completion You are helpful computer use agent designed to complete tasks on computer. Your goal is to recreate your thought process behind specific action. You will be provided with: 1. The task you are attempting to complete. 2. history of the steps you have already performed (up to 50, if any; none if it was the first action). 3. The specific action you chose to take. 4. The name of the element you clicked (if you clicked on an element). It might be too general or vague, you have to decied what to click based on the screenshot. 5. screenshot of the computer screen at the moment you decided to take the action. 6. The red marks on the screenshot indicate the position of the click or drag action. To formulate your thought process, consider: 1. What do you observe on the screen? Consider your task and previous action when you analyzing current screenshot. 2. Evaluate your previous action (if applicable): - Did it achieve the intended effect? If not, identify possible reasons (e. g., misclick, inactive element). Some typical examples for ineffective action: - misclick in an empty space - ineffective opening some elements without double click - ineffective type text/ press key because of inactivated input box - Did the result align with your previous plan, or did something unexpected happen? Some typical examples for ineffective action: - misclick in wrong element - forget to clear existing text in input bar 3. Based on your action history, assess your progress toward completing the overall task. 4. Consider if youre exploring how to finish the task because of failed attempts in history steps. Present your thought process as clear, natural first-person narrative that explains your reasoning at that moment. Important requirements: 1. **DO NOT** mention the red marks in your response. These marks were ** added after the fact** to indicate the position of your click or drag actions, and they were not on the screen when you made the decision. **DO NOT** mention \"red box\", \"red square\", \"red circle\", or \"red arrow\" in your response. 2. Write as if you are thinking in real-time before taking the action. Do not include post-action evaluation or hindsight. The task you are attempting to complete: {task_description} Your performing history: {history_str} The specific action you chose to perform: {action} 14 B.2 Trajectory Boost B.2 Trajectory Boost Prompt for Trajectory Boost Table 5: Trajectory Boost Prompt You are helpful assistant who can help users complete computer tasks, with **full permission** to make any operations on the users computer. The operating system is windows. Based on the provided current state, you need to suggest the next action to complete the task. Do not try to complete the entire task in one step. Break it down into smaller steps, and at each step you will get new state to interact with. IMPORTANT: You must strictly adhere to the following rules: 1. Choose ONLY ONE action from the list below for each response, DO NOT perform more than one action per step. 2. Follow the exact syntax format for the selected action, DO NOT create or use any actions other than those listed. 3. Once the task is completed, output action finish. Valid actions: 1. click (x, y) click the element at the position (x, y) on the screen 2. right click (x, y) right click the element at the position (x, y) on the screen 3. double click (x, y) double click the element at the position (x, y) on the screen 4. drag from (x1, y1) to (x2, y2) drag the element from position (x1, y1) to (x2, y2). 5. scroll (x) scroll the screen vertically with pixel offset x. Positive values of x: scroll up, negative values of x: scroll down. 6. press key: key_content press the key key_content on the keyboard. 7. hotkey (key1, key2) press the hotkey composed of key1 and key2. 8. hotkey (key1, key2, key3) press the hotkey composed of key1, key2, and key3. 9. type text: text_content type content text_content on the keyboard. Note that before typing text, you need to ensure the text box or input field is active/focused first. If the text box is not yet activated, you should first click on it to activate it, and then use type text in separate step. 10. wait wait for some time, usually for the system to respond, screen to refresh, advertisement to finish. 15 B.2 Trajectory Boost 11. finish indicating that the task has been completed. 12. fail indicating that the task has failed, of this task is infeasible because not enough information is provided. Before deciding your next action, you should think carefully about the current state of the screen and your history steps. Contain the following points in your thought process: 1. What do you observe on the screen? Consider your task and previous action when you analyzing current screenshot. 2. Whats your previous plan and action (if applicable)? Evaluate your previous plan and action in three conditions: 1. It didnt make any effect. You should dentify possible reasons (e.g., misclick, inactive element) and adjust your plan in this step. Some typical examples for ineffective action: - misclick in an empty space - ineffective opening some elements without double click - ineffective type text/ press key because of inactivated input box 2. It made some effect, but the result does not align with previous plan. You should dentify possible reasons (e.g., misclick, inactive element) and correct it in this step. Some typical examples for ineffective action: - misclick in wrong element - forget to clear existing text in input bar 3. It made some effect, and it successfully align with previous plan. You should progress to the next step based on the current state. 3. Based on your action history, assess your progress toward completing the overall task. 4. Exploring new ways to finish the task if there are already failed attempts in history steps. **DO NOT repeat** the history actions. Response Format: Your thought processnnAction: The specific action you choose to take. The task you are attempting to complete: {task_description} Your performing history: {history_str} Given the screenshot as below. Whats the next step that you will do to help with the task? 16 B.3 PC Agent-E scaffold B.3 PC Agent-E scaffold Table 6: PC Agent-E scaffold Prompt Prompt for PC Agent-E scaffold You are helpful assistant who can help users complete computer tasks, with **full permission** to make any operations on the users computer. Based on the provided current state, you need to suggest the next action to complete the task. Do not try to complete the entire task in one step. Break it down into smaller steps, and at each step you will get new state to interact with. IMPORTANT: You must strictly adhere to the following rules: 1. Choose ONLY ONE action from the list below for each response, DO NOT perform more than one action per step. 2. Follow the exact syntax format for the selected action, DO NOT create or use any actions other than those listed. 3. Once the task is completed, output action finish. Valid actions: 1. click (x, y) click the element at the position (x, y) on the screen 2. right click (x, y) right click the element at the position (x, y) on the screen 3. double click (x, y) double click the element at the position (x, y) on the screen 4. drag from (x1, y1) to (x2, y2) drag the element from position (x1, y1) to (x2, y2). 5. scroll (x) scroll the screen vertically with pixel offset x. Positive values of x: scroll up, negative values of x: scroll down. 6. press key: key_content press the key key_content on the keyboard. 7. hotkey (key1, key2) press the hotkey composed of key1 and key2. 8. hotkey (key1, key2, key3) press the hotkey composed of key1, key2, and key3. 9. type text: text_content type content text_content on the keyboard. 10. wait wait for some time, usually for the system to respond, screen to refresh, advertisement to finish. 11. finish indicating that the task has been completed. 12. fail indicating that the task has failed, of this task is infeasible because not enough information is provided. 17 B.3 PC Agent-E scaffold Response Format: {Your thought process} Action: {The specific action you choose to take} Your task is: {task_description} History of the previous actions and thoughts you have done to reach the current screen: {history_str} -------------------------------------------- Given the screenshot, whats the next step you will do to help with the task?"
        },
        {
            "title": "C PC Tracker User Manual",
            "content": "1. Introduction PC Tracker is lightweight infrastructure for efficiently collecting large-scale human-computer interaction trajectories. The program runs seamlessly in the background, automatically capturing screenshots and keyboard & mouse activities. 2. Installation Ensure your operating system is Windows. Extract our software package to location with sufficient disk space (recommended to have more than 3GB of available space for storing recorded data). 3. Quick Start (Optional) Set screen resolution to 16:9. Open the extracted folder and launch main.exe. 4. Instructions After launching the tracker, you can choose between Task Oriented Mode or Non-Task Oriented Mode for recording. Task Oriented Mode This mode is divided into two sub-modes: Given Task and Free Task. Given Task In this mode, you will be assigned an uncompleted task each time. Next Task: Click Next Task to get the next task. Previous Task: Click Previous Task to return to the previous task. Bad Task Feedback: If you think the current task is difficult to complete, click Bad Task to discard it permanently. Alternatively, you can start the task and modify its description after completion based on your actual execution. Start Recording: Click Start , and the tracker window will automatically minimize while recording begins. End Task: After completing the task, click Finish to save the record. Or if the task execution fails or you dont want to record it, click Fail . Modify Task Description: After finishing the task, you can modify the task description based on your actual execution. Free Task In this mode, you can freely use the computer and summarize the task description and difficulty yourself. Start Recording: Click Start , and the tracker window will automatically minimize while recording begins. Save and Summarize This Record: Fill in the task description, select difficulty (easy/medium/hard), and click Save to save the record. Discard This Record: Click Discard to discard the record. Non-Task Oriented Mode In this mode, you can freely use the computer, with similar methods to start and stop recording as described above. 5. Usage Notes Does not currently support using extended screens. Does not currently support using Chinese input methods. Does not currently support using touchpads. The tracker window is fixed in fullscreen. To support the filtering of tracker-related actions (such as clicking the Start button) in post-processing, the tracker window is fixed in fullscreen. You can reopen the tracker window by clicking to view the task description, then minimize it again, but please do not drag it to display in non-fullscreen state. 19 6. Data Privacy After starting recording, your screenshots and keyboard & mouse operations will be automatically recorded. PC Tracker does not record any information from unopened software. If you believe the recording may infringe on your privacy, you can choose to discard the record. Collected data is saved in the ./events folder (hidden by default). Each trajectory includes Markdown file for easy visualization. 7. FAQ Does the tracker have networking capabilities? PC Tracker is completely local, does not support networking, and will not upload your data. What if my computer screen resolution is not 16:9? the subsequent unified processing of data. We recommend adjusting your screen resolution to 16:9. If your screen resolution is not 16:9, it will affect How much space does the collected data occupy? The specific data size varies. Generally, even with intensive recording operations for 1 hour, it will not generate more than 1GB of data."
        }
    ],
    "affiliations": [
        "Generative AI Research Lab (GAIR)",
        "SII",
        "Shanghai Jiao Tong University"
    ]
}