{
    "paper_title": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning",
    "authors": [
        "Qiguang Chen",
        "Yantao Du",
        "Ziniu Li",
        "Jinhao Liu",
        "Songyao Duan",
        "Jiarui Guo",
        "Minghao Liu",
        "Jiaheng Liu",
        "Tong Yang",
        "Ge Zhang",
        "Libo Qin",
        "Wanxiang Che",
        "Wenhao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks."
        },
        {
            "title": "Start",
            "content": "The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning Qiguang Chen1,2, Yantao Du1,, Ziniu Li1, Jinhao Liu2, Songyao Duan3, Jiarui Guo3, Minghao Liu4, Jiaheng Liu5, Tong Yang3, Ge Zhang6, Libo Qin7,, Wanxiang Che2,, Wenhao Huang1 1ByteDance Seed China, 2LARG, SCIR, Harbin Institute of Technology, 3Peking University, 42077AI Foundation, 5Nanjing University, 6M-A-P, 7Central South University"
        },
        {
            "title": "Abstract",
            "content": "arge language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks. Date: Jan 12, 2026 Correspondence: {qgchen,car}@ir.hit.edu.cn, duyantao@bytedance.com 6 2 0 2 9 ] . [ 1 2 0 0 6 0 . 1 0 6 2 : r Figure 1 The hypothesis that stable molecular structure in Long CoT arises from three key chemical bonds. 1 Figure 2 Comparison of prior chainor tree-like structures and our molecular structure. Reasoning starts from Mole. 0, uses deep reasoning on strongly related structures, then employs self-exploration for new logic in Mole. 1. When meet errors, reasoning utilize self-reflection to guide chain to optimized Mole. 0+."
        },
        {
            "title": "Introduction",
            "content": "Recently, large language models (LLMs) have excelled on diverse reasoning tasks via explicit chain-of-thought (CoT) rationales [14]. Yet, they struggle to cold-start from instruction-tuned or base models into Long CoT models requiring extended multi-step reasoning [5, 6]. Notably, Du et al. [7] shows that humans generate Long CoT rationales without imitating DeepSeek-R1 [8]. Our preliminary studies reveal that standard supervised fine-tuning and distillation from human or Instruction LLM rationales (using randomly sampled Long CoT examples) fail to reliably instill these skills in LLMs. Models often lose coherence over long trajectories or fail to transfer patterns to novel tasks. This prompts key question: How do Large Language Models learn and represent effective Long Chain-of-Thought? To explain this, we posit that they acquire the organization of reasoning trajectories. As shown in Figure 2, prior studies model these as logic nodes in sequences or trees of steps. Yet, our analysis of Long CoT across strong reasoning models reveals stable distribution of three core behaviors across tasks and architectures: Deep-Reasoning, Self-Reflection, and Self-Exploration [5], which node-centric views fail to capture. This finding triggers molecular-inspired, distributional view: we model behavior-labeled logic edges as interaction bonds and examine how their global molecular-like structure ensures long-horizon reasoning stability. Specifically, Deep-Reasoning forms dense local clusters of coupled deductions, like covalent bonds; Self-Reflection creates long-range corrective links to prior steps, like hydrogen bonds; and Self-Exploration forges weak bridges between distant clusters, like van der Waals forces. Thus, high-quality Long CoT arises from the stable composition and arrangement of these bond types, guiding effective learning. In this framework, we define semantic isomers as Long CoT trajectories that solve the same tasks and visit similar semantic regions but differ in behavior distributions and transitions. We demonstrate that multiple near-optimal semantic isomers exist per task family, but mixing stable isomers from different strong teachers destabilizes learning, degrading performance and behavior distributions despite matched token statistics. This structurally explains why combining heterogeneous Long CoT traces often fails, beyond token-level distillation. Building on this perspective, we propose Mole-Syn, structure-aware synthesis framework that first estimates behavior transition graph from strong reasoning models and then transfers only this behavioral structure to cheaper instruction LLMs via controlled trajectory synthesis, instead of directly copying teacher outputs. This decouples structural transfer from model-specific surface form, enables the generation of Long CoT data that match target behavior distributions from scratch, and yields consistent gains in both Long CoT performance and RL stability across six benchmarks. After that, we analyze the shaping function of each bond in the Long CoT structure. Deep Reasoning bonds encode core logical flow, Self-Reflection bonds support folding pathways to previous steps, and Self-Exploration bonds reinforce long-range consistency checks, enabling targeted bond distributions. Moreover, we discuss why deteriorated molecular structure is hard to restore, which helps explain how private LLMs protect Long Note: C, H, and atom references are analogies for molecular structure only. 2 Figure 3 The failure of distillation from weak instruction LLMs with ICL and Human-annotated reasoning traces to acquire Long CoT structures, compared to successful distillation from strong reasoning LLMs. See Appendix Figure 15 for the full result. CoT structures from distillation-based imitation. Methods such as summarization and reasoning compression can disrupt Long CoT structure, limiting unauthorized replication of internal reasoning processes. In summary, our contributions are as follows: We model Long CoT as molecular structure with 3 bonds: deep-reasoning (covalent), self-reflection (hydrogen-bond), and self-exploration (van der Waals), to understand its effective learning. We identify effective Semantic Isomers for Long CoT learning, where only entropy-convergent bonds enable stable learning, while competing structures destabilize learning. We introduce Mole-Syn, which uses distribution-transfer graphs to synthesize these structures, improving Long CoT and stabilizing RL across 6 benchmarks."
        },
        {
            "title": "2 Preliminary: Cold-start LLMs for Long Chain-of-Thought",
            "content": "First, we validate whether LLMs exhibit valid Long CoT trajectories suitable for Long CoT learning. We investigate three data sources: distillation from strong reasoning LLMs, distillation from weak instruction LLMs with in-context learning (ICL), and fine-tuning on human reasoning traces. Only distillation from strong reasoning LLMs works. To identify effective data sources, we curated synthetic set of reasoning traces from three sources. As shown in Figure 3, only distillation from strong reasoning LLMs enables target models to learn and retain Long CoT structure, improving performance on benchmarks requiring extended reasoning. These results indicate that only high-quality traces reliably support both learning and use of Long CoT structures. Distillation from randomly selected ICL by weak inInstruction LLMs withstruct LLMs does not work. out explicit reasoning training lag behind reasoningspecialized models. We test whether LLMs can acquire Long CoT structure by distilling from an instruction LLM using randomly selected ICL demonstrations that emulate Long CoT reasoning. As shown in Figure 3, performance drops sharply. Instruction LLMs can only mimic short CoT traces (68 steps) and fail to extend exploration while preserving intermediate steps and trace coherence. This degradation indicates limitation of ICL-based distillation rather than robust long-chain imitation. Figure 4 Performance comparison between humanannotated reasoning traces (+ Human Distill Data) and R1 distilled reasoning traces (+ R1 Distill Data). 3 Inspired by Du et al. [7], we test whether human step-byEven human-annotated Long-CoT-like traces fail. step solutions can induce long CoT. We collect human solutions for complex reasoning tasks and fine-tune LLMs on them. Figure 4 shows that human-trace training does not reproduce the long-CoT gains from distilling strong reasoning models, suggesting that human solutions aid local problem solving but may not reliably encode abstractions for long-horizon reasoning distributions. Takeaway 1 Distillation from strong reasoning LLMs effectively imparts long CoT structures, while ICL from weak instruction models and fine-tuning on human traces yield limited gains. This underscores the importance of high-quality reasoning exemplars for robust long-chain learning in LLMs."
        },
        {
            "title": "3 Hypothesis: Stable ‘‘Molecular Structure’’ in Long CoT",
            "content": "To understand these phenomena, as shown in Fig. 1 & 2, effective Long CoT connects nodes in logical space through reasoning behaviors, forming stable macromolecular structure with mutually supportive components from global perspective. We formalize Long CoT trace as behavior-directed graph = (V, E), where each node represents reasoning step or edge = (u, v) is annotated with behavior type {D, R, E}. For trace corpus C, we estimate the behavior transition PC(b b) over consecutive edges and the marginal distribution πC(b). Empirically, strong reasoning teachers produce stable (PC, πC) across models and tasks. Specifically, these include the following three major bonds: Deep Reasoning as Covalent Bonds Deep reasoning forms the bone of the thought process, analogous to covalent bonds defining molecules primary chain. It encodes strong logical dependencies (Step must justify Step B), maintaining direction and continuity; breaking this bone undermines the following steps and destabilizes the answer. By contrast, Normal Operation corresponds to stable local bonds within each step, capturing routine computation and direct semantic expression. Self-Reflection as Hydrogen Bonds Reflection is key stabilizer. As proteins gain stability when chains fold and form intra-chain hydrogen bonds, reasoning stabilizes when later steps (e.g., Step 100) test, revise, or reinforce earlier premises (e.g., Step 10). These long-range links constrain drift and hallucination, turning long sequence into more self-consistent structure. If later checks fail to align with earlier commitments, the reasoning cannot fold, indicating structural logical error. Self-Exploration as Van der Waals Forces Exploration resembles transient Van der Waals interactions: it supports abductive and inductive moves by enabling low-commitment associations in semantic space, where concepts can drift, combine, and be probed before stronger constraints are enforced."
        },
        {
            "title": "4.1 Stable Bond Distribution in Long CoT",
            "content": "To address this, we first verify whether effective Long CoT traces show stable, macromolecular-like organization across models and tasks. As shown in Fig. 5, traces from multiple LLMs across diverse tasks yield Pearson correlations exceeding 0.9 (p<0.001) for over 2,000 samples. These transfer graphs stabilize with correlations above 0.95 across sampling sizes. This indicates that effective Long CoT structures rely on robust motifs: different models recover similar reasoning topologies across tasks, whereas simple human simulation or ICL cannot emulate the global bond distribution. Math definitions and proofs are in Appendix C.1. 4 Figure 5 Transfer graph on three different models. Pearson correlation coefficients across models are all greater than 0.9 (p<0.001), when sampling examples > 2,000, transfer graphs will become stable and get over 0.95 Pearson correlation between different sampling sizes. Figure 6 The learned features in Long CoT supervised fine-tuning."
        },
        {
            "title": "4.2 SFT actually learns these bond structures rather than keywords.",
            "content": "For the SFT analysis, we consider Llama-3.1-8B-Base that is pre-trained but not instruction-tuned on Long CoT data; Then, we consider Llama-3.1-8B-Base trained on R1-distilled data as Think-SFT model obtained by supervised fine-tuning enriched with Long CoT traces. From representation-geometry perspective, the sparse auto-encoder analysis shows that Long CoT behavior in the SFT model is concentrated in small set of discourse-control structures rather than being uniformly distributed across tokens. As shown in Figure 6 (a, b), we train cross-coder sparse auto-encoder that jointly models hidden states from the base model and the Think-SFT model, and then identify features whose activation on think tokens is at least threefold higher than their average activation rate. Within this highly activated subset, many features are predominantly driven by few connective keywords, such as Maybe, But / so, and Alternatively, indicating that the SFT process has carved out dedicated latents for managing local hypothesis revision, contrastive moves, and branch selection in long CoT traces. Based on previous analyses, we argue that models learn the characteristic reasoning behaviors these keywords represent, not the keywords themselves. Following Chen et al. [5], we define three categories of Long CoT reasoning behaviors and test this claim using two training datasets derived from QwQ distillation data. In the first, we replace each keyword with one of four alternative variants. In the second, we remove all keywords while preserving the reasoning trajectories. We then fine-tune identical LLMs on each dataset and evaluate their Long CoT reasoning performance. As shown in Figure 6 (c), explicit keywords like \"wait\" accelerate learning but are not essential. Models trained 5 Figure 7 Verification of the logical folding structure on embeddings from Qwen2.5-32B-Instruct and t-SNE-based low-dimensional representations from the OpenAI-OSS-120B-generated reasoning process. Figure 8 Energy levels of different bonds across two distilled data. without keywords, or with arbitrary alternatives, achieve comparable reasoning performance given sufficient training, provided the underlying reasoning behaviors remain intact. This reveals fundamental insight: LLMs internalize reasoning structure of reasoning rather than surface lexical cues. Consequently, training data should prioritize the distribution of reasoning behaviors over specific keyword choices to effectively enhance model reasoning capabilities. However, key open question remains: do these bonds drive Long CoT structure learning, and if so, why do explicit human imitation or random ICL distillation of these markers often fail? 4.3 Logical Bonding-Folding Structure To test the hypothesis that Long CoT is analogous to macro-molecular folding, as shown in Fig. 7, we analyzed the topology of CoT in 3D semantic space. Each trajectory edge was classified as reflection, deep reasoning, or exploration, and its geometric properties were quantified. Deep-Reasoning stabilizes logical cluster by Covalent Bonding. Modeled as covalent bonds, as shown in Fig. 7 (a), deep reasoning bonds mainly increase local connectivity, forming stable subdomains. After deep reasoning, 72.56% of steps remained within group distance of less than 3 in the semantic space (generally, group-group distance >5.6). Self-Reflection drives strong folding to previous steps by Hydrogen Bonding. Analogous to hydrogen bonds, as shown in Fig. 7 (b), self-reflection transitions fold later steps back onto earlier, semantically similar clusters rather than extending the chain linearly. Quantitatively, 81.72% of reflection steps reconnected to previously formed cluster with high semantic similarity. Model LLaMA-3.1-8B-Base [9] + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Llama-3.1-8B-Instruct [9] + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 7.58 63.38 75.89 64. 75.89 79.91 79.00 82.41 3.20 30.60 54.20 32.20 35.20 60.60 60.80 60.80 0.00 0.21 4.38 2.92 4.17 2.50 10.83 4.38 0.00 0.42 6.46 0. 1.04 3.88 7.71 8.33 4.22 14.22 37.34 16.72 23.59 33.13 47.03 32.97 1.19 8.30 23.85 8.89 12.00 23.85 30.22 25.48 AVG 2.70 19.52 33.69 20. 25.32 33.98 39.27 35.73 Table 1 Results across six benchmarks. Full results are reported in Table 5 in the Appendix. In contrast to local Self-Exploration gently links different long-distance clusters by Van der Waals Forces. stabilization (Fig. 7 (c)), exploration transitions act as loose links between otherwise separated clusters. They show much larger step-to-step distances, with an average trajectory length of 5.32 in the 3D t-SNE projection. Together, these results suggest that effective Long CoT reasoning is not simple linear chain; instead, it forms folded, domain-structured topology consistent with the logical folding hypothesis."
        },
        {
            "title": "4.4 Attention ⇔ Energy Level of Bonds\nIn physical chemistry, the behavior probability with energy level Ei at temperature T follows the Boltzmann\ndistribution:",
            "content": "P (statei) = exp(cid:0) Ei/kBT (cid:1) exp(cid:0) Ej/kBT (cid:1) . (cid:80) (1) In Transformers, the attention weight αij of the i-th token to the j-th token is: exp(cid:0)qi kj/ exp(cid:0)qi kl/ (cid:80) The correspondence follows by defining attention energy (E) (q k), implies lower Eij and thus higher behavior probability. αij = dk (cid:1) . dk (2) (cid:1) Formally, our analysis only relies on the observation that attention weights form GibbsBoltzmann distribution over negative logits. We therefore use the term energy to denote reparameterized logits and study how their expectations differ across behavior types. We then compare attention energy across Long CoT transition types. Fig. 8 shows distinct distributions for deep reasoning, reflection, and exploration. Deep reasoning exhibits the largest effective chemical-bond energy Dd. Reflection is intermediate, whereas exploration shows the weakest effective bond energy. This ordering and the relative proportions are consistent across models, supporting the hypothesis that bond-like mechanism broadly links these reasoning behaviors. Takeaway 2 Long CoT reasoning exhibits stable structures across models, with reasoning topologies converging. Semantic isomers, reasoning chains with identical concepts but different logical bonds, succeed or fail based on bond structure, not surface keywords. Three distinct logical bonds drive CoT structure: reflection folds back to prior clusters, deep reasoning creates stable local domains, and exploration bridges distant concepts, each with characteristic attention energy profiles matching Boltzmann-like distributions. SFT learns reasoning structure rather than surface keywords, determining Long CoT capability."
        },
        {
            "title": "5 Feature: Effective Semantic Isomers",
            "content": "We now shift to examine the failure of surficial distillation: similar conceptual atoms can be linked by different bonds, producing completely different reasoning chains. We call these as Semantic Isomers. Mathematically, 7 Figure 10 Information flow analysis for humans and reasoning models. semantic isomer of is any whose (PD, πD) lies near (PD, πD) under suitable divergence D( ). Then, we will analyze how they form, are learned, and sometimes destabilize in Long CoT."
        },
        {
            "title": "5.1 Bond structure of Semantic Isomers is the key to Long-CoT learning\nWell-structured Semantic Isomers can be effective for Long-CoT learning. To assess whether well-structured\nSemantic Isomers can enhance Long-CoT learning, we constructed a Long CoT dataset by distilling from\nadvanced reasoning LLMs. As shown in Table 1, models exhibit consistent performance gains when trained\non these variants (distribution correlation ∼0.9). This suggests that, within a certain range, multiple effective\n“allotropic” variants of reasoning keys can exist.",
            "content": "The model has multiple effective Semantic Isomers, and slight differences can significantly affect the results. As shown in Table 1, the structural correlation between R1and OSS-derived chains can reach 0.95, yet for some models performance with R1-based chains drops by more than 10%. This gap indicates the presence of multiple near-optimal Semantic Isomers that are nonetheless fragile, where small changes in the distribution can cause large performance losses. Simulating effective Semantic Isomer structures is the key to ICL distillation. We analyze three ICL setups to simulate Long CoT chains by Qwen2.5-32B-Instruct: demonstrations chosen at random; demonstrations with closely aligned reasoning-key distributions (correlation 0.9); and demonstrations with clearly mismatched distributions (correlation < 0.8). As shown in Fig. 9, substantial performance gains emerge only when demonstrations are constructed to match specific target distribution of reasoning keys, thereby yielding an appropriate allotrope of that distribution. 5.1.1 Not all bonds in isomers are effective. To clarify the nature of semantic isomers, we examine which bond structures yield effective reasoning configurations. We hypothesize that functional viability depends on specific bond distributions: despite sharing identical conceptual nodes, incompatible configurations disrupt information exchange. For instance, excessive exploration bonds cause fragmented reasoning, whereas overemphasized deep reasoning bonds create rigid chains unable to adapt to new inputs.Details are provided in Appendix D.3. Figure 9 The performance of LLama3.1-8B-Instruction under three different distillation setups from ICLaugmented Qwen2.5-32B. Figure 11 Conflict Learning between Two Stable Molecular Structures. Effective reasoning bond distribution influences the information divergence speed in reasoning dynamics. To assess this, we compared the reasoning dynamics of R1 models with human cognition in an information phase space [10]. Mechanistically, LLMs update by maximizing rewards and reducing entropy, whereas human reasoning is additionally constrained by semantic coherence and social feedback. Consequently, machine reasoning converges through accumulated gradient updates, whereas human reasoning stabilizes through iterative self-monitoring and social calibration. As shown in Figure 10, we tracked reasoning unfolding over extended chains in logical deduction tasks. Humans typically exhibit nearly uniform forward information gains (81.3% of cases show changes < 0.1), corresponding to near-zero slope in phase space. In contrast, R1 models display accelerating informativeness (76.1% of cases show absolute changes > 0.1), progressing from low entropy to rapid convergence. These patterns indicate fundamental difference in how R1 models and humans integrate information over time. Effective reasoning bonds cause metacognitive oscillation and alignment. We identify the core rationale for this difference as \"metacognitive oscillation\" in LLMs. Their responses alternate between high-entropy divergent exploration (slope > 0.6, entropy > 0.05) and stable convergent validation, dynamic less pronounced in the uniform entropy profiles of humans. Case studies (Figure 10) confirm that R1 models utilize self-reflective revision to adjust reasoning paths against uncertainty. We hypothesize that aligning training objectives with these behavioral structure distributions can narrow the gap between machine and human reasoning dynamics."
        },
        {
            "title": "5.2 Conflict between Two Stable Structures",
            "content": "Understanding how distinct reasoning structures interact reveals fundamental limits of complex cognitive systems. As shown in Fig. 11 (ac), forcibly fusing stable molecular isomers disrupts their backbone; analogously, combining incompatible reasoning frameworks breaks global logical coherence. Learning two heterogeneous stable structures at the same time will lead to structural chaos in the model. As shown in Fig. 11 (d), we test this by jointly activating two highly correlated (r 0.9) reasoning chains from DeepSeek-R1 and OpenAI-OSS. Despite their similarity, co-activation prevents the model from converging to single stable behavioral mode: it produces molecular bond distributions that fluctuate across samples and deviate from those characteristic of either OSS or R1. Consistent with this instability, the self-correlation of the jointly activated model does not exceed 0.8. 9 Model LLaMA-3.1-8B-Base [9] + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K OSS-MO E-SY + 20K QwQ-MO E-SY Llama-3.1-8B-Instruct [9] + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K OSS-MO E-SY + 20K QwQ-MO E-SY GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 7.58 62.47 75.89 64.53 67.85 66.41 75.89 76.50 79.00 82.41 83.24 84.31 3.20 29.40 54.20 32.20 35.20 35.00 35.20 39.80 60.80 60.80 51.80 50. 0.00 0.00 4.38 2.92 1.83 2.08 4.17 4.38 10.83 4.38 4.79 5.21 0.00 0.00 6.46 0.42 0.83 0.63 1.04 1.04 7.71 8.33 1.04 1.67 4.22 12.81 37.34 16.72 20.53 20.16 23.59 25.63 47.03 32.97 32.50 32. 1.19 6.81 23.85 8.89 11.11 10.37 12.00 19.70 30.22 25.48 21.04 20.00 AVG 2.70 18.58 33.69 20.95 22.89 22.44 25.32 27.84 39.27 35.73 32.40 32.29 Table 2 Performance comparison across six benchmarks. Here, distill from reasoning LLMs, : distill from instruction LLMs. See Table 6 in Appendix for full results. : distill from instruction LLMs+Mole-Syn, : Figure 12 The continual improvement of RL performance with Mole-Syn-initialized models. More details about the RL performance are in Table 8 in Appendix. This structural chaos leads to significant decline in the performance of the model. As shown in Fig. 11 (e), joint activation also causes marked drop in performance relative to either chain alone. This seemingly paradoxical effect indicates that structural compatibility, rather than mere statistical correlation, governs whether reasoning systems can coexist. The interference pattern suggests that the underlying cognitive architecture is rigid: without careful alignment, attempts to merge such systems yield fragmented, low-utility outputs instead of enhanced capability. Takeaway 3 Well-structured isomers perform effectively when their reasoning-key distributions align closely, but small structural shifts cause fragility and sharp performance losses. Co-activating incompatible reasoning structures induces structural chaos that breaks coherence and degrades performance, proving that statistical similarity does not guarantee compatibility."
        },
        {
            "title": "6 Synthetic Chemistry: Synthesis Long CoT Molecules from Scratch",
            "content": "LLMs may acquire advanced reasoning partly through exposure to explicit, structured Long CoT reasoning traces. However, it remains unclear how reliably such structures can be induced by prompting an instructiontuned model, rather than obtained through distillation. Mole-Syn Methodology. To address this gap, we propose synthetic framework, Mole-Syn, that views targeted reasoning traces as macromolecular structures using only instruction LLMs. This method is random walk on transition probability graph in Figure 5 composed of 4 reasoning behaviors from stronger reasoning LLMs that support Long CoT. 10 Figure 13 Roles of individual bonds in reasoning, inferred from semantic-space comparisons between Llama-3.1-8BInstruct and Llama-3.1-8B-Instruct + QwQ-Mole-Syn. Performance impact is shown in Fig. 16 (Appendix). Mole-Syn can synthesize effective bond structures. To test whether Long CoT capabilities can be learned from instruction LLMs, in Table 2, we conduct training on Mole-Syn generated data, which even achieves reasoning performance close to QwQ distillation. This suggests that instruction-driven synthesis can induce useful structural regularities, enabling lower-cost behavior transfer. Mole-Syn can further trigger stronger and continually improving RL. We further evaluate Mole-Syninitialized LLMs potential in reinforcement learning (RL). Mole-Syn-initialized LLMs outperform those initialized from base LLMs. In Figure 12 (a), they show steadier fine-tuning gains, indicating stronger immediate reasoning and more reliable RL adaptation. Moreover, Figures 12 (b-d) show that these gains persist over extended RL training, demonstrating durable benefits from synthesized Long CoT structures. This effective use under RL supports their practical utility across diverse cognitive tasks. Takeaway 4 Mole-Syn successfully synthesizes Long CoT structures that match transition distributions from capable teacher models without requiring Long CoT distillation data. Transition-based Long CoT datasets achieve stable convergence and near-distillation performance, proving effective reasoning structures emerge purely from instruction-level synthesis at lower cost. Models initialized with synthesized Long CoT weights demonstrate superior and sustained RL performance gains, providing robust foundation for continual learning in dynamic environments."
        },
        {
            "title": "7 Function: Shaping Function of Each Bond in Long CoT Structure",
            "content": "Further, we analyze the shaping function of each bond in Long CoT structure. We suppose that LLM searches for an optimal semantic configuration parallels proteins descent along folding funnel toward low-energy native state (the solution). Deep Reasoning is densing major Structure Formation. As reasoning proceeds, Deep Reasoning drives Primary Structure formation by synthesizing logical backbone, analogous to Covalent Bonding. As shown in Figure 13 (a), Deep Reasoning densifies the core logical structure: the volume of the smallest covering ball in semantic space decreases by 22% compared with baseline. This stage builds the answers skeleton, but it does not yet ensure global stability or correctness. 11 Model Llama-3.1-8B-Instruct + 20K Gemini-Distill-Data + 20K Claude4-Distill-Data Qwen-2.5-32B-Base + 20K Gemini-Distill-Data + 20K Claude4-Distill-Data Qwen-2.5-32B-Instruct + 20K Gemini-Distill-Data + 20K Claude4-Distill-Data GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 75.89 54.89 63.76 35.20 22.60 35.80 12.00 7.41 13. 23.59 12.03 23.44 4.17 1.04 1.04 1.04 0.63 0.83 53.68 52.54 63.31 93.71 63.68 76.88 33.40 20.20 37. 81.00 32.80 54.80 9.17 1.88 2.92 15.60 15.00 17.71 2.29 0.63 0.83 14.17 2.92 13.96 35.63 21.41 28. 69.84 35.63 55.00 15.85 12.44 17.78 42.22 19.11 30.96 Table 3 Distillation Results from Gemini and Claude. Model Llama-3.1-8B-Instruct + 20K OSS-Summarized + 20K QwQ-Summarized GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 75.89 54.89 63.76 35.20 22.60 35.80 12.00 7.41 13.48 23.59 12.03 23.44 1.04 0.63 0.83 4.17 1.04 1. Qwen-2.5-7B-Instruct + 20K OSS-Summarized + 20K QwQ-Summarized 83.24 82.34 82.71 74.00 72.60 71.80 12.50 12.50 11.88 7.08 6.46 6. 22.66 21.41 22.97 38.07 27.70 25.04 AVG 25.32 16.43 23.06 25.00 18.18 25.26 52.76 28.19 41.55 AVG 25.32 16.43 23. 39.59 37.17 36.77 Table 4 Results from summarized Long CoT trajectories based on OpenAI-OSS and QwQ distilled trajectories. Self-Reflection is densing and stabilize the global logics. After the bone is extended, Self-Reflection stabilizes the structure. Analogous to hydrogen bonds, reflection folds the logical chain by cross-linking distant nodes to test consistency, rather than adding new deductive steps. As shown in Figure 13 (b), it consolidates the hydrophobic core and suppresses inconsistent branches, reducing the system volume from 35.2 to 31.2 and moving toward stable, folded optimal state. Self-Exploration expand logical space. Self-Exploration broadens the set of feasible solutions and also increases variability and may introduce less consistent branches, so it improves coverage at the cost of reduced immediate stability. As shown in Figure 13 (c), after learning Long CoT structure, exploration expands the exploration behavior in semantic space from 23.95 to 29.22. Takeaway 5 Long CoT reasoning mimics protein folding through three stages: Deep Reasoning densifies the logical backbone, Self-Exploration expands the search space to avoid local minima, and Self-Reflection converges toward stable, optimized solution state in semantic space."
        },
        {
            "title": "8 Deteriorated Molecular Structure Cannot Be Easily Restored",
            "content": "How Current Private LLMs Protect Their Long CoT from Distillation? Exposing reasoning traces allows LLMs to imitate both answers and procedures. Common defenses always consider compressing intermediate steps. We quantify this by distilling from Gemini-2.5-Pro-Thinking and Claude-4-Sonnet. Table 3 shows that beyond 45% token reduction versus QwQ-32B rationales, distillation causes accuracy drops, showing compression can disrupt Long CoT structure. Summarization break reasoning bond distributions to prohibit distillation. To further validate the effectiveness of summarization, we summarized Long-CoT traces from QwQ and R1. Table 4 shows that training on compressed trajectories yields weaker performance than training on full traces and reduces distillation effectiveness by 2%. In Figure 14, summarization shifts reasoning behavior distributions and creates gap between observable outputs and internal error-bounded transitions, limiting trace inversion and behavioral cloning. However, compression can also protect model architecture and embedded inductive priors from 12 Figure 14 The reasoning behavior distribution of summarized QwQ traces. unauthorized imitation. Takeaway Summarization and reasoning compression effectively protect Long CoT structures from distillation by disrupting structural coherence, preventing unauthorized replication of internal reasoning processes."
        },
        {
            "title": "9 Related Work",
            "content": "Chain-of-Thought (CoT) elicits intermediate rationales, improving multi-step reasoning on math and logic tasks. Scaling short CoT to Long CoT is difficult: coherence often degrades, and cold-start gains frequently require targeted training or high-quality trajectories [8, 11]. common approach distills stepwise solutions from strong teacher models into weaker students [12, 13]. Outcomes hinge on reasoning quality: strong reasoning models transfer useful behaviors [14], whereas weaker instruction models may mimic format without robust Long CoT capability [7, 15]. Long CoT training elicits three behaviors: deep reasoning, self-reflection, and self-exploration [5, 8, 16]. Work has studied their roles; Madaan et al. [17] used self-reflection to revise earlier steps, and Shinn et al. [18] combined reflection and exploration to improve robustness. Early studies framed CoT as sequences of these behaviors, emphasizing step-level imitation and local coherence [1, 3, 19]. Later work used treeor graph-structured reasoning to capture branching and revisitation [2023]. Although trees or graphs represent individual Long CoT traces by modeling behaviors as nodes, they do not capture the overall distribution of logical behaviors. In contrast, our approach models Long CoT as molecular-like structure, with edges encoding stable distributions of reasoning behaviors, to test how their arrangement and interactions support effective learning."
        },
        {
            "title": "10 Conclusion",
            "content": "This study provides mechanistic account of Long CoT learning, conceptualized as molecular-like reasoning structures emerging through Self-Reflection, Deep-Reasoning, and Self-Exploration bonds. We propose semantic isomer framework through behavior-transition distributions, offering insight into the stability and failure modes of Long CoT learning. Building on these, our Mole-Syn leverages distribution transfer graphs to construct robust Long CoT structures, thereby improving performance and enhancing the stability of RL."
        },
        {
            "title": "Limitations",
            "content": "While our approach achieves strong performance on several reasoning benchmarks, it still has several limitations: First, limited by cost and scale constraints, our analysis relies on limited set of teacher models and student backbones, which may introduce bias in observed statistical patterns of Long CoT toward specific architectures or training recipes. Second, we focus on offline distillation and supervised fine-tuning, leaving open how well the method scales in realistic online or interactive settings with RL-like feedback. We can only approximately visualize the geometric characteristics of the inferred bond in information and semantic spaces. However, accurately delineating universal Long CoT macromolecular structure remains an important future direction. Finally, our behavior analysis relies on an automatically labeled dataset. Even though we provide an initial robustness check, label noise or bias will inevitably appear on estimated distributions."
        },
        {
            "title": "References",
            "content": "[1] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [2] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [3] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [4] Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, and Philip Yu. Large language models meet nlp: survey. arXiv preprint arXiv:2405.12819, 2024. [5] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. [6] Xinghao Chen, Zhijing Sun, Guo Wenjin, Miaoran Zhang, Yanjun Chen, Yirong Sun, Hui Su, Yijie Pan, Dietrich Klakow, Wenjie Li, and Xiaoyu Shen. Unveiling the key factors for distilling chain-of-thought reasoning. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar, editors, Findings of the Association for Computational Linguistics: ACL 2025, pages 1509415119, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-256-5. doi: 10.18653/v1/2025.findings-acl.782. URL https://aclanthology.org/2025.findings-acl.782/. [7] Wei Du, Branislav Kisacanin, George Armstrong, Shubham Toshniwal, Ivan Moshkov, Alexan Ayrapetyan, Sadegh Mahdavi, Dan Zhao, Shizhe Diao, Dragan Mašulović, Advaith Avadhanam, Max Wang, Shitij Govil, Sri Yanamandra, Mihir Tandon, Sriram Ananthakrishnan, Vedant Rathi, David Zhang, Joonseok Kang, Leon Luo, Titu Andreescu, Ashmit Dutta, Boris Ginsburg, and Igor Gitman. The challenge of teaching reasoning to LLMs without RL or distillation. In 2nd AI for Math Workshop @ ICML 2025, 2025. URL https://openreview. net/forum?id=fOjo1OHbSK. [8] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. [9] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. [10] Qiguang Chen, Jinhao Liu, Libo Qin, Yimeng Zhang, Yihao Liang, Shangxu Ren, Chengyu Luan, Dengyun Peng, Hanjing Li, Jiannan Guan, et al. The universal landscape of human reasoning. arXiv preprint arXiv:2510.21623, 2025. [11] Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. 14 [12] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 2028620332, 2025. [13] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji, and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language model training. arXiv preprint arXiv:2503.19633, 2025. [14] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. [15] Abhranil Chandra, Ayush Agrawal, Arian Hosseini, Sebastian Fischmeister, Rishabh Agarwal, Navin Goyal, and Aaron Courville. Shape of thought: When distribution matters more than correctness in reasoning tasks. arXiv preprint arXiv:2512.22255, 2025. [16] OpenAI. Learning learning-to-reason-with-llms/. to reason with llms, 2024. URL https://openai.com/index/ [17] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [18] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [19] Qiguang Chen, Libo Qin, Jiaqi Wang, Jingxuan Zhou, and Wanxiang Che. Unlocking the capabilities of thought: reasoning boundary framework to quantify and optimize chain-of-thought. Advances in Neural Information Processing Systems, 37:5487254904, 2024. [20] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [21] Yao Yao, Zuchao Li, and Hai Zhao. Got: Effective graph-of-thought reasoning in language models. In Findings of the Association for Computational Linguistics: NAACL 2024, pages 29012921, 2024. [22] Mengkang Hu, Yao Mu, Xinmiao Chelsey Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, and Ping Luo. Tree-planner: Efficient close-loop task planning with large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/ forum?id=Glcsog6zOe. [23] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1768217690, 2024. [24] Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, et al. Openthoughts: Data recipes for reasoning models. arXiv preprint arXiv:2506.04178, 2025. [25] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [26] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [27] AMC. American mathematics competitions, 2023. URL https://artofproblemsolving.com/wiki/index. php/AMC_Problems_and_Solutions. [28] AIME. American invitational mathematics examination (aime) aime 2024-i & ii, 2024. URL https: //huggingface.co/datasets/Maxwell-Jia/AIME_2024. 15 [29] AIME. American invitational mathematics examination (aime) 2025-i & ii, 2025. URL https://huggingface. co/datasets/opencompass/AIME2025. [30] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [31] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [32] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. https://qwenlm.github.io/blog/ qwq-32b-preview/, 2024. [33] OpenAI. Gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://platform.openai.com/docs/ models/gpt-oss-120b. [34] Olga Golovneva, Moya Peng Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. ROSCOE: suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=xYlJRpzZtsY. [35] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [36] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [37] Thomas Jiralerspong and Trenton Bricken. Model diffing without borders: Unlocking cross-architecture model diffing to reveal hidden ideological alignment in llama and qwen. In Mechanistic Interpretability Workshop at NeurIPS 2025, 2025. URL https://openreview.net/forum?id=ZB84SvrZB8. [38] Jack Lindsey, Adly Templeton, Jonathan Marcus, Thomas Conerly, Joshua Batson, and Christopher Olah. Sparse crosscoders for cross-layer features and model diffing. 2025. URL https://transformer-circuits.pub/ 2024/crosscoders/index.html. [39] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Baobao Chang, et al. survey on in-context learning. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 11071128, 2024. [40] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. [41] Hemish Veeraboina. Aime problem set 1983-2024. 2023. URL https://www.kaggle.com/datasets/ hemishveeraboina/aime-problem-set-1983-2024."
        },
        {
            "title": "A General Experimental Setting",
            "content": "Target Model. Unless otherwise specified, we start from generic instruction-tuned LLM (e.g., Llama-3.1-8BInstruct [9]). The model is standard pre-trained and supervised fine-tuned, but is not specifically optimized for long chain-of-thought reasoning. Data Source. We randomly sample 20K35K high-quality Long CoT queries from OpenThoughts-3 [24] as the primary training corpus. The dataset provides multi-step reasoning traces, averaging over 20 steps per example, and covers diverse mathematical and logical problem types. Evaluation Benchmarks. We evaluate on 6 challenging mathematical reasoning benchmarks that require multi-step logic: GSM8K [25]: Grade-school math problems requiring multi-step calculations. MATH-500 [26]: High-school and early college-level problems across major math topics. AMC 2023 [27]: High-school competition problems in algebra, geometry, and combinatorics. AIME 2024 [28]: Invitational-level problems with integer final answers. AIME 2025 [29]: Invitational-level problems with integer final answers. OlymBench [30]: comprehensive Olympiad-level benchmark. Unless stated otherwise, (Overall) Accuracy is the mean accuracy across all benchmarks above. Inference and Metrics. For evaluation, we use the same sampling temperature as in reinforcement learning, = 0.6, to reduce the SFTRL mismatch. We report Avg@1 accuracy for GSM8K, MATH-500, and OlymBench. For smaller test sets (AMC 2023 and AIME 2024/2025), we report Avg@16 accuracy. We instruct the model to output the final answer in boxed{} format to simplify extraction. We then parse boxed answers and match them to reference solutions using standard answer-matching tools (e.g., math-verify). Except for specifically specified variables, all model training settings are the same."
        },
        {
            "title": "B Detailed Experimental Settings for Preliminary Study",
            "content": "In this appendix, we detail the experimental setup for the preliminary study on cold-start Long CoT. We compare 3 data construction pipelines. For all pipelines, we fine-tune the model for 1 epoch with learning rate of 2e 5 and global batch size of 128, using max sequence length of 16K or 32K tokens (we select the better-performing setting between these two lengths). We use Qwen2.5 [31] and Llama3.1 [9] as backbones to cold-start Long CoT across multiple model sizes. Setting 1: Distillation from Strong Reasoning LLMs. To construct high-quality synthetic dataset, we use reasoning LLM as the teacher. Specifically, we distill Long CoT reasoning traces from DeepSeek-R1-671B0528 [8], QwQ-32B [32], OpenAI-OSS-120B [33]. Setting 2: Distillation from Weak Instruction LLMs (ICL-Distill). To simulate surface imitation via in-context learning in instruction-tuned LLMs, we use standard instruction model (e.g., Qwen2.5-32B-Instruct) that is not optimized for deep reasoning. We randomly select 1-shot exemplar (an R1-generated Long CoT trace) in the prompt, and generate solutions for the same problems as in Setting 1. Setting 3: Fine-tuning on Human-Annotated Traces. We utilize 50 R1-generated reasoning traces and 50 human-written step-by-step solutions for supervised fine-tuning to compare against human-annotated reasoning traces. The data are collected from Du et al. [7]. 17 Figure 15 The full failure result of ICL from weak instruction-following LLMs and Human-annotated reasoning traces to acquire Long CoT structures, compared to successful distillation from strong reasoning LLMs."
        },
        {
            "title": "C Reasoning Bond in Long CoT",
            "content": "In this section, we provide detailed experimental settings for analyzing reasoning bonds in Long CoT. C.1 Mathematical Definition & Analysis of Reasoning Bonds C.1.1 Reasoning Behavior Definition. Given an input query x, model generates an output text containing intermediate reasoning followed by final answer. We segment into step-level units (steps) using standard delimiters (e.g., line breaks or bullet markers), following prior work [19, 34]. This yields Long-CoT trajectory τ := (u1, . . . , uT ), (3) where each ut is textual step. To analyze the geometry of τ , we map each step ut to vector ht Rd. Concretely, we encode ut with fixed reference encoder and obtain token hidden states from fixed layer (e.g., the penultimate layer). We then average token hidden states to form step embedding ht, producing an embedding sequence (h1, . . . , hT ). Behavior space and labeled transitions. We model Long-CoT as sequence of step-to-step transitions. For each {1, . . . , 1}, define the directed transition (edge) Let the behavior label set be et := (ut ut+1). := {N , D, R, E}, (4) (5) where denotes Normal Operation, Deep Reasoning, Self-Reflection, and Self-Exploration. An automated classifier (Appendix C.1) assigns label bt to each transition et, yielding labeled transition sequence (et, bt)T 1 t=1 We formalize behavior-graph framework for Long CoT, in which molecular-inspired bonds correspond to behavior-labeled edges whose distributions can be estimated, compared, and transferred across models. . 18 Definition 1 (Deep Reasoning: D). transition et = (ut ut+1) is labeled if the primary intent of ut+1 is to extend the reasoning chain via non-trivial inference (e.g., multi-step causal, deductive, or analogical reasoning). Operationally, typically introduces at least one of: (i) new latent assumptions, (ii) intermediate logical variables/nodes, or (iii) derivations that go beyond direct computation or restatement. Definition 2 (Self-Reflection: R). transition et = (ut ut+1) is labeled if ut+1 explicitly comments on, audits, or regulates the models own reasoning process. This includes expressing uncertainty, revising the solution strategy, identifying or correcting mistakes, or re-examining earlier steps (i.e., tracing back to prior logical nodes). Definition 3 (Self-Exploration: E). transition et = (ut ut+1) is labeled if ut+1 intentionally branches into alternative hypotheses or candidate solution paths. Operationally, raises new possibilities or subquestions and keeps multiple paths active rather than committing to single convergent chain. Remark (Normal Operation; ). Any transition not meeting the criteria for D, R, or is labeled . This category covers routine progression such as straightforward calculation, paraphrasing, formatting, or direct execution of an already-chosen plan. It should be noted that an edge linking logical nodes does not merely connect the step st to the next Note: step st+1, but also adds edges to previously related nodes. C.1.2 Attention Energy Definition of Reasoning Bonds. We now replace the molecular analogy with direct analysis of attention patterns. For given Transformer layer and head, let qi, kj Rdk denote the query and key vectors of the i-th and j-th tokens, respectively. The attention weight αij from token to token is sij = kj dk , exp αij = (cid:80) ℓ exp (cid:16) (cid:17) kj dk (cid:16) kℓ dk (cid:17) . Following the standard GibbsBoltzmann parametrization, we define the attention energy Then, substituting Eq. (8) into Eq. (7) yields Eij sij = kj dk . αij = exp(Eij) ℓ exp(Eiℓ) (cid:80) , (6) (7) (8) (9) which is Boltzmann distribution over targets with inverse temperature fixed to 1. That is, the attention weight is Eij GibbsBoltzmann distribution (temperature is taken to be 1). In this view, lower Eij corresponds exactly to higher attention weight αij, i.e., stronger dependency from to j. To relate this to Long CoT behaviors, we use the edge-level labeling procedure from Appendix C.3. Each reasoning transition (edge) in trajectory is classified as one of three behaviors: Deep Reasoning, SelfReflection, or Self-Exploration (plus Normal Operation as local baseline). For each labeled edge b, we aggregate the token-level attention weights between the source and target steps into single scalar logit, and convert it to an energy value Eb using Eq. (8). This yields three random variables ED, ER, EE corresponding to Deep, Reflection, and Exploration edges, respectively. We do not assume any physical semantics for Eij ; it is simply convenient reparameterization of the attention logits. 19 C.1.3 Attention-Energy Ordering of Reasoning Bonds To analyze ordering among attention energies for different reasoning bonds, model queries, and keys with Rotary Positional Embedding (RoPE) [35]. In an autoregressive Transformer, attention weights are softmax of pre-softmax logits; define an attention energy as the negative logit so that higher attention corresponds to lower energy (Boltzmann/Gibbs form). RoPE model RoPE can be written as position-dependent block-diagonal rotation on (pairs of) coordinates. Model this heads queries/keys as where R(t) Rdkdk is deterministic and orthogonal. Then qi = R(i)ui, kj = R(j)vj, so the score depends only on the relative offset. sij = R(i)R(j)vj dk = R(i j)vj dk , (10) (11) Bond random variables and expectations Let dD := 1 (adjacent), and choose integers 1 < dR < dE. Define bond-energy random variables (random over positions/data) ED(i) := Ei,i1, ER(i) := Ei,idR , EE (i) := Ei,idE . Define their mean energies (scalars) ED := (ED(i)), ER := (ER(i)), EE := (EE (i)). Goal: prove ED < ER < EE in expectation, and then with high probability for empirical averages. (12) (13) Assumptions Here, we should establish weak dependence under RoPE and state two assumptions on the query/key distributions. A1 (Isotropic, distance-decaying cross-covariance). There exists scalar function ρ(d) 0, strictly decreasing in d, such that for = j, A2 (Positive average alignment of rotation). Define E[uiv ] = ρ(d) I. µ(d) := 1 dk tr(R(d)). Assume µ(d) is non-increasing in and µ(d) µ > 0 for {1, dR, dE}. Theorem 1 (Expected bond-energy order under RoPE). Under A1A2 and 1 < dR < dE, ED < ER < EE . Proof. First, compute the expected logit at distance d. Let = d. Then E[si,id] = (cid:20) R(d)vid dk (cid:21) By A1, E[vidu ] = ρ(d)I. Hence = 1 dk tr(cid:0)R(d) E[vidu ](cid:1) . E[si,id] = ρ(d) 1 dk tr(R(d)) = ρ(d)µ(d). 20 (14) (15) (16) (17) (18) (19) By A1, ρ(1) > ρ(dR) > ρ(dE). By A2, µ(1) µ(dR) µ(dE) µ > 0. Therefore E[si,i1] > E[si,idR ] > E[si,idE ]. Finally, since Eij = sij, taking negatives yields ED < ER < EE . (20) (21) (cid:98)s(d) be the sample mean of si,id over independent The lower bound between two bond energies. Let draws (or approximately independent blocks), and define (cid:98)E(d) := (cid:98)s(d). Let (cid:98)ED := (cid:98)E(1), (cid:98)ER := (cid:98)E(dR), (cid:98)EE := (cid:98)E(dE). A3 (Sub-Gaussian logits). For each fixed distance {1, dR, dE}, the centered logit si,id E[si,id] is σ2-sub-Gaussian. Then for each and any ϵ > 0, Pr(cid:0)(cid:12) (cid:12)(cid:98)s(d) E[s(d)](cid:12) (cid:12) > ϵ(cid:1) 2 exp (cid:18) ϵ2 2σ2 (cid:19) . (22) By union bound over the three distances, all three estimates concentrate simultaneously with probability at least 1 δ when If the expected logit gaps satisfy ϵ = σ (cid:114) 2 log(6/δ) . E[s(1)] E[s(dR)] > 2ϵ, E[s(dR)] E[s(dE)] > 2ϵ, then with probability at least 1 δ, that satisfies (cid:98)ED < (cid:98)ER < (cid:98)EE , since (cid:98)E(d) = (cid:98)s(d) preserves strict inequalities after flipping signs. Lemma (Finite-sample ordering with high probability). Suppose that the mean scores satisfy DR = E[sD] E[sR] > 0 and RE = E[sR] E[sE] > 0. For any 0 < ϵ < min(DR, RE)/2, if 2σ2 ϵ2 log 4 δ , then, with probability at least 1 δ, the empirical estimates follow the same ordering: Pr( ˆED < ˆER < ˆEE) 1 δ. (25) (26) C.1.4 Low-energy edges dominate path aggregation. Step-level dependency graph. Let model-produced chain-of-thought be partitioned into reasoning steps. Represent these steps as nodes in directed graph = (V, B). For two steps u, , include directed edge = (u v) if step attends to step with average attention weight above fixed threshold. (Thus, edges point from an attended-to step to the attending step.) Step-level edge energy. Assume token-level energies Eij are defined for token pairs (i, j). For an edge = (u v), define the set of cross-step token pairs Define the step-level edge energy as the mean token-level energy across these pairs: C(u v) = {(i, j) : tokens(u), tokens(v)}. Eb Euv = 1 C(u v) (cid:88) Eij. (i,j)C(uv) 21 (27) (28) (23) (24) Reasoning paths and path energy. reasoning path from source step to target step is sequence of edges Define the path energy as the additive cost = (b1, . . . , bL), bℓ = (uℓ uℓ+1), u1 = s, uL+1 = t. E(p) = (cid:88) ℓ=1 Ebℓ . (29) (30) Soft-min (effective) energy over all paths. Let P(s t) denote the set of all paths from to in G. Following the standard log-sum-exp (soft-min) construction, define the effective energy (s t) = log (cid:88) exp(cid:0)E(p)(cid:1). pP(st) (31) Equivalently, each path receives an (unnormalized) Gibbs weight exp(E(p)), so lower-energy paths contribute more strongly to the aggregate dependence from to t. Low-energy edges define effective constraints. Consider two paths and from to of equal length L. Assume they are identical except at single position ℓ, where uses edge bℓ and uses ℓ δ for some δ > 0, then ℓ . If Ebℓ Eb exp(cid:0)E(p)(cid:1) exp(cid:0)E(p)(cid:1) = exp(cid:0)E(p) E(p)(cid:1) exp(δ). (32) Thus, holding all other edges fixed, replacing higher-energy edge by lower-energy edge amplifies the relative influence of that path by at least factor exp(δ) in the attention-induced dependency distribution. Proof. Because and differ only at ℓ, The assumption Ebℓ Eb ℓ δ implies E(p) E(p) = Ebℓ Eb ℓ . Exponentiating yields which proves the claim. E(p) E(p) δ E(p) E(p) δ. exp(cid:0)E(p)(cid:1) exp(cid:0)E(p)(cid:1) = exp(cid:0)E(p) E(p)(cid:1) exp(δ), (33) (34) (35) Analysis. Proposition C.1.4 shows that low-energy edges act as effective constraints: they bias the model toward reusing specific multi-step dependency patterns, because any path that swaps in lower-energy edge gains multiplicative weight in the induced path distribution. If edges labeled as Deep Reasoning consistently exhibit lower energies than alternative behaviors, then multi-hop dependencies that rely more heavily on Deep Reasoning edges will dominate the effective aggregation from premises to conclusions, stabilizing long chain-of-thought structure without invoking an external chemical analogy. C.1.5 Long CoT process is looking for more stable reasoning structure. Generally speaking, in chemistry, reactions tend to synthesize more stable, lower-energy compounds. Analogously, we hypothesize that the Long CoT learning process seeks stable reasoning configuration that minimizes attention energy. We now formalize this intuition under mild ergodicity and bounded-energy assumptions. 22 Step 1. Stationary behavior frequencies. Assume that the reasoning behavior sequence (st)t1 forms an irreducible, aperiodic, time-homogeneous Markov chain with transition matrix and stationary distribution π satisfying π = π. By the ergodic theorem for finite-state Markov chains, the empirical frequency of each behavior converges almost surely to its stationary probability:"
        },
        {
            "title": "1\nT − 1",
            "content": "T 1 (cid:88) t=1 1[st = b] a.s. πb, B. (36) Step 2. Decomposition of the time-averaged energy. Let Et denote the bond-level attention energy associated with transition et = (ut ut+1), as defined in Appendix C.4. The trajectory-level average energy can be decomposed by behavior type as (cid:98)ET = (cid:88) bB (cid:32)"
        },
        {
            "title": "1\nT − 1",
            "content": "T 1 (cid:88) (cid:33) 1[st = b] t=1 (cid:123)(cid:122) Empirical frequency of behavior (cid:124) (cid:125) (cid:33) (cid:32) (cid:80)T 1 t=1 Et 1[st = b] (cid:80)T 1 t=1 1[st = b] (cid:123)(cid:122) Mean energy conditional on st=b (cid:125) (cid:124) . (37) This simply states that the global average equals the weighted sum of per-behavior averages. Step 3. Conditional convergence of energy averages. Assume that E[Et] < and, conditional on st = b, the distribution of Et depends on the past only through st (i.e., Et (s<t, E<t) st). Under this mild conditional-independence assumption, the law of large numbers within each behavior implies that the inner mean converges to the conditional expectation 1 Nb(T ) (cid:88) t: st=b Et µb = E[Et st = b]. Combining this with the ergodic frequency limit from Step 1 yields (cid:98)ET a.s. (cid:88) bB πbµb, (38) (39) thus establishing the ergodic low-energy equilibrium. Step 4. Exponential routing preference from Gibbs attention. At reasoning step + 1, consider fixed query token selecting among candidate targets St. The model samples target sj St according to the standard Boltzmann (softmax) distribution: Pr(St = sj i) = (cid:80) exp(Eij) sℓSt exp(Eiℓ) . (40) Assume bounded-deviation condition: for any behavior type b, the corresponding attention energies lie within an interval [µb , µb + ]. Then for any two behavior classes b, co-occurring in St, Pr(St i) Pr(St i) exp(cid:0)(µb + ) + (µc )(cid:1) = exp(cid:0)(µc µb) 2(cid:1). (41) Intuitively, lower-energy behaviors are exponentially favored in the routing distribution. If Deep/Reflection behaviors exhibit mean energies µD, µR separated from Exploration energy µE by margin γ > 0, and 2 < γ, then these lower-energy transitions will dominate with exponential advantage exp(γ 2). Therefore, the models attention mechanism inherently biases it toward stable, low-energy Long CoT reasoning structures. Note: The energy at step is related to the previous logical node (node), but has nothing to do with the logical step (edge). 23 C.2 Stable Reasoning Bond Distribution in Long CoT Reasoning Chain Generation To analyze structural properties across domains, we utilize OpenThoughts-3 [24], mixed-domain corpus comprising math, code, and scientific reasoning tasks typical of QwQ-style distillation pipelines. We prompted DeepSeek-R1-671B, OpenAI-o1, and QwQ-32B with original queries to generate Long CoT traces. Generation employed maximum token length of {16,384, 32,768}, decoding temperature [0, 1], top-p = 0.95, and standard repetition penalties, without additional sampling constraints. Bond Type Annotation Following Chen et al. [19], Golovneva et al. [34], we segmented reasoning traces into step-level units using standard delimiters (e.g., n, nn, . ). We then prompted Qwen2.5-32B-Instruct to classify the logical bond for each edge between consecutive steps (ht1, ht) into one of three types. Validation on 200-example subset against human annotation yielded macro-F1 score exceeding 0.85, confirming the reliability of the automated labeling applied to the full corpus. Bond Type Annotation Prompt You are an expert annotator. Classify the CURRENT STEP into exactly one of the following categories of reasoning/behavior: normal operation Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. deep reasoning Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. self-reflection commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. exploration generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following single conclusion. Decision rules: (1) If multiple categories seem to overlap, choose the most specific match based on intent: If the text is about reasoning itself self-reflection. If the text is branching or speculating exploration. If the text is extending the reasoning chain with deeper causality or hidden steps deep reasoning. Otherwise, if its just direct calculation or straightforward logic normal operation. (2) Do not label based on correctness of the reasoning only on the behavioral style of thinking. (3) Ignore surface complexity (e.g., long math steps may still be normal operation if they are straightforward). (4) If mixed, choose the dominant intent; break ties with this priority: self-reflection > exploration > deep reasoning > normal operation. Output format (strict): Return exactly one line and nothing else: ### Behavior: {normal operation deep reasoning self-reflection exploration} PREVIOUS STEP: {} CURRENT STEP: {} Transfer Distribution Analysis To construct the reasoning topology (Figure 5), we randomly subsampled {0.5k, 1k, 2k, 5k, 10k, 20k} examples for each model-task pair. We derived reasoning transfer distributions between sequential behaviors, aggregating transition frequencies into normalized transfer graphs defined over relative step indices or semantic clusters. Stability was assessed via Pearson correlations between graphs 24 from differing models and sample sizes. Averaged across five independent trials, correlations exceeded 0.9 for > 2k and 0.95 when comparing sample sizes above this threshold. C.3 Logical Bonding-Folding Structure in Long CoT To analyze the distribution and geometric characteristics of reasoning bonds, we embed all reasoning steps into unified semantic space. This representation enables visualization of the bonding structure and supports the definition of geometric metrics to quantify folding. C.3.1 Geometric Embedding and Visualization of Bonding Structure To analyze logical folding, we embed complete CoT trajectories into high-dimensional semantic space using Qwen3-8B [36]. We encode each reasoning step by averaging the penultimate-layer hidden states of its constituent tokens, representing trajectories as ordered sequences {h1, h2, . . . , hT }, where denotes the number of steps. We visualize folding behavior (Figure 7) via t-SNE, employing cosine distance and 5,000 iterations with an early exaggeration of 12.0. These parameters yield stable three-dimensional projections that preserve local neighborhood structure. For clarity, Figure 7 displays only chains and logical clusters relevant to specific behaviors, omitting extraneous points to highlight bonding functions. Moreover, we define logic cluster c(ht) by iteratively merging instances located within distance α of existing clusters. To reduce variability in t-SNE embeddings, we set α proportional to the spread of points: α = 0.02 (hmax hmin), (42) where hmax and hmin are the maximum and minimum pairwise distances among all embedded points in the trajectory. This adaptive threshold ensures that clustering remains sensitive to the specific geometry of each reasoning trace. C.3.2 Geometric metrics for folding We quantify the geometry of labeled edges ((t t+1)) in the embedding space, where each step is represented by vector ht Rd. Euclidean distance is assumed unless otherwise noted. Self-reflection: local movement and reconnection. To determine if reflective transitions revisit semantic regions, we compute two metrics. First, the local step distance captures instantaneous displacement: Second, for reflection-labeled edges, we measure the return distance to the trajectory history: dt = ht+1 ht2 . rt = min s<t ht+1 hs2 . (43) (44) reflection step is defined as reconnecting if the prior state minimizing rt satisfies rt < α, where α is the predefined cutoff. Under this criterion, 81.72% of reflections return to consolidated clusters rather than drifting to novel areas. Deep reasoning: path length and cluster-level proximity. Deep reasoning often involves extensive computation that appears local in endpoints but traverses complex intermediate path. We capture this via the geometric trajectory length. This metric distinguishes direct transitions from extended traversals. Additionally, we quantify semantic movement using the cluster-level graph distance: gt = distG (c(ht), c(ht+1)) , (45) where distG computes the shortest distance between clusters in graph whose nodes are clusters and edges connect clusters with pairwise distances below α. Empirically, 72.56% of deep reasoning steps satisfy dt < gt < 3, suggesting that deep reasoning typically progresses through nearby logical neighborhoods rather than jumping between remote regions. 25 Self-Exploration: novelty and sustained drift. Self-Exploration entails movement into unvisited regions. We quantify this via novelty distance, defined identically to displacement: nt = dt = ht+1 ht2 , (46) where high values of nt implies divergence from previously visited states. To distinguish transient jumps from sustained excursions, we examine the trajectory length ℓt. Self-exploration steps exhibit an average length of 5.32, consistent with extended traversals rather than local fluctuations. C.4 Attention Energy Levels of Different Logical Bonds To analyze attention energy across bond types, we extract attention weights from distilled Llama-3.1-8BInstruct fine-tuned on QwQ-generated data, using full-precision inference on reasoning queries drawn from evaluation benchmarks. For each generated step, we annotate the reasoning behavior and record attention weights from all layers and heads over preceding tokens. Unless otherwise stated, we analyze the final attention layer, averaging across all heads to enhance the signal-to-noise ratio. We define attention metrics for specific bond types as follows: for self-reflection bonds, we measure attention where queries (q) from the reflection step attend to keys (k) in the nearest prior step within the hidden space, identified by minimizing the L2 embedding distance. For deep reasoning bonds, we track attention from the current step to the immediately preceding step. Finally, for self-exploration bonds, we measure attention from the previous step directed toward the exploration step. For each logical bond connecting step and step j, we construct bond-level statistics from token-level attention (equivalently, token-level energies). First, we select the final token(s) of step as the target tokens (i.e., the tokens whose outgoing attention is used to characterize how step relies on prior content); Then, for each selected target token in step j, we extract its attention weights to the final token of step i, and then aggregate across attention heads using the head-wise mean. We convert the aggregated attention logits to energies via Eq. 1, and treat the resulting value as the empirical energy for that bond instance. Applying this procedure across all bonds yields empirical distributions of attention weights and effective energies for reflection, deep-reasoning, and exploration bonds. C.5 SFT Learning of Bond Structures For the SFT analysis, we consider Llama-3.1-8B-Base that is pre-trained but not instruction-tuned on Long CoT data; Then, we consider Llama-3.1-8B-Base trained on R1-distilled data as Think-SFT model obtained by supervised fine-tuning enriched with Long CoT traces. C.5.1 Setting: How does SFT actually learn these bond structures? Cross-coder Sparse Auto-Encoder (SAE) Architecture. To analyze representational geometry, following Jiralerspong and Bricken [37], Lindsey et al. [38], we train cross-coder sparse auto-encoder (SAE). For each token, the SAE input is the concatenation of hidden states from the base model and the SFT model at aligned token positions. Specifically, the SAE comprises an encoder and decoder. The encoder is single linear layer that maps the concatenated hidden state to sparse-dimensional latent space. We induce sparsity in the latent code using an ℓ1 penalty, calibrated to yield an average activation rate of approximately 1%3% per latent unit. The decoder is linear layer trained to reconstruct the original concatenated hidden state from the sparse latent code. We compute token-level feature activations by applying the encoder to each tokens concatenated hidden state. We report only features whose activation probability in the base model is more than 3 that in the SFT model, or vice versa. Using these features, we manually identify those associated with Long CoT behavior. Specifically, we label think tokens as tokens belonging to explicit reasoning segments in the SFT corpus (as opposed to prompt context or final-answer segments), and we select the features with the strongest enrichment on think tokens. This procedure yields compact set of discourse-control features preferentially expressed during Long CoT reasoning. Inspecting tokens with the largest contributions to these features reveals strong associations with connective markers such as Maybe, But/so, and Alternatively. 26 Keyword manipulation dataset construction. We examine whether supervised fine-tuning learns the underlying reasoning structure in the data, rather than exploiting superficial lexical cues associated with particular keywords. To this end, we construct two modified versions of the QwQ distillation training corpus. In both versions, whenever selected keyword occurs (e.g., wait, maybe, however), we replace it with meaning-preserving alternative while maintaining the local syntax and the intended progression of the reasoning. In the first modified dataset (Keyword-variant 1), each occurrence of target keyword is randomly replaced by one of four semantically similar alternatives, for example wait hold on, maybe perhaps, and however on the other hand. In the second modified dataset (Keyword-variant 2), we apply the same procedure but use different set of replacements than those in Keyword-variant 1. This design changes surface realizations while aiming to preserve the underlying reasoning trajectory, enabling controlled test of sensitivity to lexical form. For reproducibility, we summarize the replacement scheme in table that lists, for each original keyword, the set of replacements used in Keyword-variant 1 and the distinct set used in Keyword-variant 2, formatted as: original keyword / replacement plan 1 / replacement plan 2. The keywords and corresponding replacements for the Deep Reasoning bond are provided in the boxes below: Keywords for Deep Reasoning otherwise / if not / or else therefore / thus / hence because / since / due to the fact that so / in that case / that means first / to start / firstly next / then / after that finally / in the end / at last then / in that case / as consequence note / keep in mind / remember notice that / take note / bear in mind important / crucial / significant actually / in fact / really basically / essentially / fundamentally think step by step / work through it step by step / go stepwise lets reason through this / lets work through this / lets think it through carefully / with care / meticulously logically / coherently / consistently rigorously / systematically / by strict logic assumption / premise / starting assumption constraint / restriction / limitation it implies that / it means that / it entails that key insight / central idea / core insight break it down / decompose it / split it up The keywords and corresponding replacements for the Self-Reflection bond are listed in the box below: Keywords for Self-Reflection wait, / hold on, / lets slow down, but / yet / though however / nevertheless / yet reflect / think back / pause to consider verify / confirm / validate double-check / recheck / verify again reflection / introspection / self-examination 27 Keywords for Self-Reflection (Continued) introspect / look inward / self-examine might be wrong / could be mistaken / may be off could be in error / might be misreading this / may be overlooking detail Im not sure / Im uncertain / Im not certain Im not fully confident / cant say with certainty / have doubts confidence / certainty / assurance credence / confidence level / subjective probability revise / adjust / update modify / refine / rework reconsider / rethink / take another look check my assumptions / test my assumptions / validate my premises audit my premises / question my starting points / recheck my presuppositions self-critique / self-review / self-audit self-correction / self-check / critical reflection let me check / let me verify / let me double-check alternatively / as an alternative / as another option instead / in lieu of that / in place of that conversely / on the flip side / the other way around Im struggling with / Im wrestling with / Im having hard time with The keywords and corresponding replacements for the Self-Exploration bond are listed in the box below: Keywords for Self-Exploration maybe / perhaps / might be now / at this point / right now lets / let us / we can probably / likely / presumably seems / appears / looks like maybe not / perhaps not / possibly note Ill / well / it helps to consider/ think about / look at assume / suppose / lets say if / provided that / in case explore / look into / examine probe / dig into / unpack consider two cases / split into two cases / handle two scenarios self-exploration / personal exploration / inner exploration self-discovery / discovering myself / learning about myself values / principles / priorities want / Id like / Im aiming to Both datasets preserve the same underlying trajectories and labels as the original SFT corpus. We further ensure that the distributions of problems, answer types, and trajectory lengths remain unchanged."
        },
        {
            "title": "D Semantic Isomer Construction Details",
            "content": "This section details the construction and analysis of semantic isomers across four key dimensions. We first describe the distillation of well-structured semantic isomers and their simulation via in-context learning (ICL). Subsequently, we examine information flow through metacognitive oscillation analysis and conclude with the specifics of conflict learning between two stable structures. 28 Model LLaMA-3.1-8B-Base + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Llama-3.1-8B-Instruct + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Qwen-2.5-7B-Base + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Qwen-2.5-7B-Instruct + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Qwen-2.5-32B-Base + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data Qwen-2.5-32B-Instruct + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data LLama-3.1-70B-Base + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data LLama-3.1-70B-Instruct + 20K R1-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 7.58 63.38 75.89 64.53 75.89 79.91 79.00 82.41 40.18 76.14 84.99 78.39 83.24 87.04 82.31 85.75 53.68 76.14 89.76 91. 93.71 93.63 86.35 92.65 46.78 73.62 89.39 88.86 84.23 94.62 85.75 93.33 3.20 30.60 54.20 32.20 35.20 60.60 60.80 60.80 34.20 24.20 68.40 46. 74.00 74.80 74.60 73.80 33.40 24.20 77.20 82.40 81.00 83.60 86.20 93.20 31.80 20.40 76.80 82.40 52.20 80.60 83.40 89.00 0.00 0.21 4.38 2. 4.17 2.50 10.83 4.38 5.42 1.20 6.04 2.71 12.50 14.17 25.42 13.96 9.17 1.46 19.38 19.75 15.63 26.67 64.17 59.38 3.33 1.25 16.88 19. 17.92 27.29 52.50 47.50 0.00 0.42 6.46 0.42 1.04 3.96 7.71 8.33 0.83 2.29 8.13 1.46 7.08 8.54 19.38 16.25 2.29 2.29 17.50 19. 14.17 22.71 53.54 49.38 1.88 2.08 17.29 18.33 3.13 21.88 40.42 36.25 4.22 14.22 37.34 16.72 23.59 33.13 47.03 32.97 26.72 10.00 46.25 9. 22.66 46.25 54.34 52.97 35.63 10.00 60.63 72.37 69.84 72.56 92.34 88.44 32.50 8.75 57.97 72.34 45.63 64.84 84.38 81.41 1.19 8.30 23.85 8. 12.00 23.85 30.22 25.48 17.33 5.33 27.70 5.93 38.07 41.48 37.63 35.11 15.85 5.33 39.26 41.33 42.22 45.93 55.70 61.93 13.19 4.44 36.74 39. 21.63 43.11 50.81 54.07 AVG 2.70 19.52 33.69 20.95 25.32 33.99 39.27 35.73 20.78 19.86 40.25 24.19 39.59 45.38 48.94 46.31 25.00 19.90 50.62 54. 52.76 57.52 73.05 74.16 21.58 18.42 49.18 53.49 37.45 55.39 66.21 66.93 Table 5 Full results on GSM8K, MATH-500, AIME2024, AIME2025, AMC2023, and OlympiadBench. D.1 Distillation of Well-structured Semantic Isomers The setting is the same as in Appendix B. We use 8 base and instruct LLMs as the backbone and distill from 3 advanced reasoning LLMs. D.2 ICL Simulation of Semantic Isomer Structures Inspired by Dong et al. [39], we study whether demonstration selection in an in-context learning (ICL) setting can approximate the effective semantic isomer structure of target teacher model (Qwen2.5-32B). Demonstration Construction. We built candidate pool by using QwQ-32B to generate Long-CoT solutions for the training questions in Appendix B. Demonstration Selection. For each target question, we select 1-shot demonstration by comparing the reasoning-key distribution of each candidate trace with that of the target teacher (Qwen2.5-32B) trace for the same question, using Pearson correlation. We consider three strategies: (1) Random: Sample demonstration uniformly from the pool. (2) High-correlation (Aligned): Choose demonstration with high correlation (r 0.9) to the teacher distribution. (3) Low-correlation (Mismatched): Choose demonstration with low correlation (r < 0.8), corresponding to structurally incompatible reasoning paths. ICL-based Distillation Using the selected 1-shot demonstration, we prompt Qwen2.5-32B-Instruct to generate Long-CoT solutions for the same training questions (Appendix B). We then fine-tune student model (e.g., Llama3.1-8B-Instruct) on these ICL-generated traces with the same hyperparameters as in Appendix B. For each question, we keep the demonstration fixed across all samples. D.3 Information Flow and Metacognitive Oscillation Analysis D.3.1 Setting: Information Flow Analysis and Metacognitive Oscillation Quantification Information Flow Analysis in Phase Space (Human vs. R1) We compare the reasoning dynamics of humans and the R1 model in an information phase space by quantifying information gain, entropy evolution, and the rate of convergence. We use multi-step logical deduction and structured reasoning tasks from Du et al. [7]. For human data, participants wrote their reasoning process step-by-step. For model data, R1 was prompted to produce step-by-step reasoning. We treat each step as discrete point along reasoning trajectory. For humans, steps are segmented using natural paragraph breaks or explicit logical transitions identified by the delimiter nn. For the model, steps are delineated using explicit reasoning markers in the generated output. For visualization, we report only three representative patterns of information-flow change. Each reasoning step st is mapped to semantic probability representation pt using unified semantic probability encoder (Llama-3.1-8B-Instruct). We define step-wise entropy through the tuple (It, It), where It denotes the cumulative entropy (or cumulative information measure) up to step t, and It = It It1 denotes the instantaneous change. The full reasoning chain thus forms trajectory in 2D phase space with coordinates (It, It). To characterize reasoning dynamics (e.g., uniform gain versus accelerating convergence), we analyze the distribution of It and the local phase-space slope, computed between consecutive points as: mt = It It1 It It1 . (47) when It = It1. The slope mt captures the rate of change in information gain relative to cumulative information, indicating whether the reasoning process is accelerating, decelerating, or stable at each step. Analysis of Metacognitive Oscillation We formalize and quantify metacognitive oscillation in LLMsalternation between high-entropy exploration and low-entropy validationby relating these dynamic states to the distributions of reasoning bonds. Using the phase-space dynamics, we classify each reasoning step into one of two states. (1) High-entropy Exploration: characterized by steep phase-space slope (mt > 0.6) together with substantial entropy increase (entropy > 0.05). (2) Low-entropy Validation: characterized by near-zero or negative slope (mt 0) and and minimal entropy change (entropy < 0.05). We then measure the frequency and periodicity of transitions between these states, and analyze which reasoning keys (bonds) are most prevalent within each state. D.4 Details about Conflict Learning Between Two Stable Structures Setup about performance analysis on different training data mixture strategies. To test whether simultaneously training model on two highly correlated (r 0.9) yet structurally distinct reasoning frameworks (from R1 and OSS) leads to structural chaos. Specifically, we use OpenAI-OSS-120B [33] as the OSS model and DeepSeek-R1-671B-0528 [8] as the R1 model to generate two sets of Long CoT traces on the same 20K training questions from OpenThoughts-3 [24]. Based on these generated Long CoT traces, we created three training configurations: (1) OSS-Distill-Data, an R1-only set with 20K samples generated from DeepSeek-R1, (2) R1-Distill-Data, an OSS-only set with 20K samples generated from OpenAI-OSS, and (3) R1-then-OSS, sequential set with first 10K from DeepSeek-R1 and then 10K from OpenAI-OSS. (4) OSS-then-R1, sequential set with first 10K from OpenAI-OSS and then 10K from DeepSeek-R1. (5) R1-mix-OSS, randomly mixed set with 10K from OpenAI-OSS and 10K from DeepSeek-R1. Pearson correlation coefficient between transfer distribution. To quantify the similarity between two reasoning structures (e.g., R1 and OSS), we compute the Pearson correlation coefficient between their transfer distributions. Given two transfer matrices and representing the normalized frequencies of 30 reasoning bonds between steps, we flatten these matrices into vectors and q. The Pearson correlation coefficient is then calculated as: (cid:80)n i=1(pi p)2(cid:112)(cid:80)n where and are the mean values of vectors and q, respectively, and is the number of elements in each vector. high positive correlation (close to 1) indicates similar reasoning structures, while low or negative correlation indicates dissimilar structures. i=1(pi p)(qi q) i=1(qi q) (cid:112)(cid:80)n = (48) , Details about Synthetic Long CoT with Mole-Syn This section describes the experimental settings used for synthesis, analysis, and reinforcement learning with Mole-Syn. E.1 Supervised-Finetuning with Mole-Syn To match the statistical properties of the synthetic chain-of-thought (CoT) to the behavioral patterns of stronger teacher models (Section C.2), we first estimate reasoning-state transition distribution p(st+1st). Specifically, we analyze 20k distilled CoT rationales generated by teacher models (e.g., QwQ-32B and OpenAIOSS-120B). Each reasoning step st is annotated by LLMs, and we use these annotations to compute an empirical transition matrix ˆP (Figure 5). During synthesis, we initialize the process in an exploration state and then sample transitions between reasoning states according to. The prompts used for each reasoning state are listed below. The prompt for the self-reflection state is as follows: Prompt for Self-Reflection Assume that you are helpful assistant. You will receive question and previously reasoned rationale. If you can directly get the answer, please output the concise answer with boxed{}. Otherwise, please reflect on the response and provide self-reflection. Here are some reasoning behavior definitions: normal operation Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. deep reasoning Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. self-reflection commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. exploration generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following single conclusion. You should conduct self-reflection behavior now. Please reflect on the response and provide self-reflection. The prompt for the self-exploration state is as follows: Prompt for Self-Exploration Assume that you are helpful assistant. You will receive question and previously reasoned rationale. If you can directly get the answer, please output the concise answer with boxed{}. Otherwise, please explore novel reasoning path in the response. Here are some reasoning behavior definitions: 31 Prompt for Self-Exploration (Continued) normal operation Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. deep reasoning Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. self-reflection commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. exploration generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following single conclusion. You should conduct exploration behavior now. Please explore novel reasoning path in the response. The prompt for the normal-operation state is as follows: Prompt for Normal Operation Assume that you are helpful assistant. You will receive question and previously reasoned rationale. If you can directly get the answer, please output the concise answer with boxed{}. Otherwise, please conduct normal operation on the response. Here are some reasoning behavior definitions: normal operation Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. deep reasoning Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. self-reflection commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. exploration generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following single conclusion. You should conduct normal operation behavior now. Please conduct normal operation on the response. The prompt for the deep-reasoning state is as follows: Prompt for Deep Reasoning Assume that you are helpful assistant. You will receive question and previously reasoned rationale. If you can directly get the answer, please output the concise answer with boxed{}. Otherwise, please further deepen the reasoning on the response. Here are some reasoning behavior definitions: normal operation Straightforward, direct operations (e.g., arithmetic, factual recall, simple step-by-step logic) without introducing new logical nodes. deep reasoning Multi-step causal, deductive, or analogical thinking that extends the reasoning chain by introducing new logical nodes or hidden assumptions. self-reflection commenting on its own thought process (e.g., confidence, strategy, uncertainty, mistakes, or reconsideration of earlier reasoning) and tracing back to previous logical nodes. exploration generating new possibilities, hypotheses, or questions, branching into alternative paths rather than following single conclusion. You should conduct deep reasoning behavior now. Please further deepen the reasoning on the response. 32 Model LLaMA-3.1-8B-Base + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K OSS-MO E-SY + 20K QwQ-MO E-SY Llama-3.1-8B-Instruct + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K OSS-MO E-SY + 20K QwQ-MO E-SY Qwen-2.5-7B-Base + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K QwQ-MO E-SY + 20K OSS-MO E-SY Qwen-2.5-7B-Instruct + 20K Qwen-Distill-Data + 20K OSS-Distill-Data + 20K QwQ-Distill-Data + 20K QwQ-MO E-SY + 20K OSS-MO E-SY GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 7.58 62.47 75.89 64.53 67.85 66.41 75.89 76.50 79.00 82.41 83.24 84.31 40.18 68.69 84.99 78.39 81.20 83.17 83.24 84.31 82.31 85.75 89.61 88.02 3.20 29.40 54.20 32.20 35.20 35. 35.20 39.80 60.80 60.80 51.80 50.20 34.20 39.80 68.40 46.80 62.20 63.80 74.00 63.40 74.60 73.80 76.00 77.80 0.00 0.00 4.38 2.92 1.83 2.08 4.17 4.38 10.83 4.38 4.79 5.21 5.42 4.38 6.04 2.71 6.25 5. 12.50 6.46 25.42 13.96 7.29 8.13 0.00 0.00 6.46 0.42 0.83 0.63 1.04 1.04 7.71 8.33 1.04 1.67 0.83 1.04 8.13 1.46 3.54 1.67 7.08 3.13 19.38 16.25 3.96 5.00 4.22 12.81 37.34 16.72 20.53 20. 23.59 25.63 47.03 32.97 32.50 32.34 26.72 25.63 46.25 9.84 41.88 41.56 22.66 31.72 54.34 52.97 51.88 52.81 1.19 6.81 23.85 8.89 11.11 10.37 12.00 19.70 30.22 25.48 21.04 20.00 17.33 19.70 27.70 5.93 30.52 29. 38.07 29.78 37.63 35.11 36.74 37.48 AVG 2.70 18.58 33.69 20.95 22.89 22.44 25.32 27.84 39.27 35.73 32.40 32.29 20.78 26.54 40.25 24.19 37.60 37.56 39.59 36.46 48.94 46.31 44.25 44.87 Table 6 Comparison of strong reasoning LLM distillation versus Mole-Syn using weak instructed LLM across six benchmarks. Model Llama-3.1-8B-Instruct GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 75.89 23.59 35.20 12.00 1. 4.17 + MO E-SY N-by-Llama-3.1-8B-Instruct + MO E-SY N-by-Llama-3.1-70B-Instruct + MO E-SY N-by-Qwen-7B + MO E-SY N-by-Qwen-32B 36.01 82.71 83.47 82.41 23.60 53.60 51.60 60.80 2.71 3.33 3.33 4.38 0.21 0.63 0.63 8. 13.75 31.72 32.03 32.97 7.41 19.85 19.85 25.48 Table 7 The effectiveness analysis of Mole-Syn methods based on different instruct models. AVG 25.32 13.95 31.97 31.82 35.73 Furthermore, to investigate the impact of distinct synthetic backbones, we employed Mole-Syn to distill data from various base models (Figure 7). Models with limited self-reflection or exploration capabilities, such as Llama-3.1-8B-Instruct, struggled to synthesize effective reasoning data. Conversely, combining the Qwen series with Llama-70B yielded robust results across most domains, with the notable exception of tasks requiring deep reasoning. For such tasks (e.g., AIME), performance depended heavily on the models intrinsic reasoning depth, following the hierarchy: Qwen-32B outperforms both Qwen-7B and Llama-70B. E.2 Reinforcement Learning with Mole-Syn Initialization We adopt DAPO [40] as the reinforcement-learning (RL) framework to fine-tune Llama-3.1-8B-Instruct models initialized from different checkpoints. To isolate the effect of initialization, we hold the RL tasks, reward functions, and all other training hyperparameters constant across experiments. Each model is trained for 1000 steps with learning rate of 1 105, batch size of 16, sampling size of 16, and maximum sequence length of 16384 tokens. We set the clipping parameters to clip-low = 0.2 and clip-high = 0.68. We use MATH [26] and AIME 19892023 [41] as training data. We evaluate two RL initialization strategies: (1) QwQ-Distill Data + RL: RL fine-tuning starting from Llama3.1-8B-Instruct further tuned on QwQ-generated long chain-of-thought data. (2) QwQ-Graph-Syn + RL: RL fine-tuning starting from our Mole-Syn-SFT model. All runs use the same RL tasks, reward functions, and hyperparameters; therefore, performance differences can be attributed to the initialization checkpoint (i.e., 33 Model Llama-3.1-8B-Instruct GSM8K MATH-500 AIME2024 AIME2025 AMC2023 OlympiadBench 75. 35.20 23.59 12.00 4.17 1.04 + 20K QwQ-Distill-Data + 20K QwQ-Distill-Data + RL + 20K QwQ-MO E-SY + 20K QwQ-MO E-SY + RL + 35K QwQ-MO E-SY + RL 82.41 91.51 84.31 88.78 90.30 60.80 69.80 50.20 70.80 68.40 4.38 8. 5.21 7.50 10.00 8.33 4.17 1.67 3.33 4.38 32.97 37.97 32.34 39.22 39.84 25.48 25. 20.00 21.04 24.15 AVG 25.32 35.73 39.72 32.29 38.44 39.51 Table 8 The reinforcement learning performance based on 3 initialization methods across 6 benchmarks. the supervised pretraining/fine-tuning history)."
        },
        {
            "title": "F Details of Bond Shaping Function Analysis",
            "content": "This section provides detailed description of the experimental settings for the geometric reshaping analyses presented in Fig. 13, corresponding to the three \"bond\" types in the Long CoT structure: Deep Reasoning (covalent bond), Self-Exploration (exploration bond), and Self-Reflection (stabilizing bond). To analyze the function of different \"bonds,\" we extracted step-level hidden representations from the penultimate Transformer layer of both Llama-3.1-8B-Instruct and our trained Mole-Syn model. For each reasoning step t, we computed its vector representation ht via average pooling of its token embeddings. We then used t-SNE to visualize the semantic space and compare the representations horig for the same reasoning step across the two models. and hmethod F.1 Deep Reasoning: Densing the Primary Structure. To quantify how Deep Reasoning densifies the primary structure, we use the same development-set split as in the main text. We compare two training modes: (1) baseline, which uses standard short CoT prompting by standard Llama-3.1-8B-Instruct; and (2) deep-reasoning-trained, which uses the Long CoT paradigm with the Deep Reasoning transfer probability set to 50%, while the remaining probability mass follows the other behaviors in our default setting. For each input sample, we generate 10K training instances and, under each mode, sample 100 reasoning trajectories from OlymBench. From these trajectories, we extract the step-level baseline representations {h1, . . . , hT } and deep-reasoning-trained representations {h1, . . . , hT }. To characterize the compactness of the \"primary To characterize the compactness of the primary structure in semantic space, we compute the Minimum Enclosing Ball (MEB) over all step representations from the sampled trajectories: = {h1, . . . , hTN }, where TN denotes the total number of extracted steps aggregated across trajectories for the given sample set. We compute the MEB in shared three-dimensional embedding space obtained by t-SNE, which we use consistently across modes for visualization and for comparable notion of geometric spread. The procedure is as follows. First, we reduce each step representation from the original high-dimensional embedding space to = 3 using t-SNE. Second, we apply an approximate MEB solver based on Welzls algorithm to obtain the center R3 and radius r. Third, we define the corresponding volume in = 3 Euclidean space as: (49) where Cd is the unit-ball volume constant (a fixed scaling factor that cancels in relative comparisons). We then compute the arithmetic mean of over all development-set samples, denoted as Vbase (Baseline) and Vdeep (Deep Reasoning). The volume reduction reported in Fig. 13(a) is: = Cd rd, (50) Vbase Vdeep Vbase positive Deep indicates that Deep Reasoning contracts the semantic volume of the core logical backbone while preserving comparable semantic coverage of the task. Deep = 100%. (51) 34 Figure 16 Comparative analysis of performance under different reasoning bond ratios. F.2 Self-Exploration: Expanding the Logical Space. We use the same development-set split and the same two training modes as above: (1) baseline (short CoT or direct answering, without Long CoT) and (2) self-exploration-trained (Long CoT with self-exploration transfer probability >50%, with other behaviors unchanged). For each input sample, we generate 10K training instances and sample 100 reasoning trajectories from OlymBench under each mode, then extract the corresponding step representations. Let Vexp denote the mean MEB volume computed from the step representations used for the self-exploration-trained model. We quantify the relative volume change as: Exp = Vexp Vbase Vbase 100%. (52) Then, positive Exp corresponds to an expansion of the explored logical space relative to the baseline. F.3 Self-Reflection: Densifying and Stabilizing the Logical Results. We follow the same data construction and representation extraction protocol as above. To measure the contraction induced by Self-Reflection, we compute MEB volumes before and after the reflection step, denoted by Vpre and Vpost, respectively. We then report the relative contraction as Reflect = Vpre Vpost Vpre 100%, (53) where larger value indicates stronger folding effect in semantic space after reflection. To further examine the suppression of inconsistent branches, we conduct an auxiliary clustering analysis (not shown in the main text). Specifically, we cluster the preand post-reflection point sets, Spre and Spost, using an α-threshold grouping method. We observe that, after reflection, intra-cluster distances decrease substantially, while inter-cluster distances remain stable or increase slightly. This pattern suggests that the dominant semantic cluster becomes more compact, whereas inconsistent branches are pruned or weakened. Consistent with these findings, the overall semantic volume decreases from 35.2 to 31.2, indicating that Self-Reflection guides the structure toward more stable solution manifold."
        },
        {
            "title": "G Analysis of the impact of length and diversity in reasoning behaviors",
            "content": "To evaluate whether comparable performance gains can be achieved merely by regulating the length and frequency of Self-Reflection steps, we conducted detailed analysis illustrated in Fig. 13, covering the three distinct \"bond\" types that constitute the Long CoT framework. 35 G.1 Impact of bond-enhanced behaviors on performance We further examined how different reasoning behaviors influence overall model performance  (Fig. 16)  . For complex queries, improvements are primarily driven by reasoning depth rather than the sheer number of steps, once minimum threshold is achieved. higher proportion of reflective behaviors tends to enhance performance on challenging tasks but can introduce overthinking, which reduces accuracy on simpler ones. Similarly, excessive exploratory behavior on complex tasks often impedes convergenceindeed, 62.7% of AIME cases exhibited extended reasoning without reaching clear conclusion. In contrast, simple queries generally benefit from more direct exploration, leading to greater accuracy. Across all models, performance consistently peaks at balanced distribution of behaviors, suggesting the existence of stable and task-invariant optimal configuration. G.2 Effect of bond length on reasoning quality We also investigated how varying bond lengths affects performance while maintaining constant behavioral ratios. As shown in Fig. 16, increasing both the number of steps and the length of individual reasoning bonds generally strengthens performance, particularly in deep reasoning scenarios. However, overly long exploration bonds can induce semantic drift and reduce the precision of final outputs. The optimal bond length appears to depend on task difficulty: shorter bonds tend to perform better on simple problems, whereas longer ones are more advantageous for tackling complex challenges."
        },
        {
            "title": "H Details of LLM Structure Reconstruction",
            "content": "H.1 Reasoning compression or summarization data collection. For the analysis about broken structure reconstruction, we treat Gemini-2.5-Pro-Thinking and Claude-4-Sonnet as black-box teacher models that can emit long chain-of-thought (CoT) traces. We query both APIs with identical user prompts and task instructions, and enable their thinking content, which are much shorter than other reasoning LLMs with Long CoT. H.2 Summarize Reasoning Process Analysis. To emulate the protection strategies used in private LLMs, we summarize the full Long CoT traces from QwQ-32B and OpenAI-OSS-120B into concise summaries by Qwen2.5-32B. This procedure removes the long-range reasoning structure of the original CoT, thereby reducing the amount of recoverable step-by-step rationale. The summarization prompt is as follows: Prompt for Summarization You are an expert summarizer. Below is Long Chain-of-Thought reasoning trace generated by an AI model to solve complex problem. Your task is to compress this reasoning process into concise summary. Input Long Chain-of-Thought Trace: [Insert Full Trace Here] Summary: We trained the model on the summarized 20K samples. The corresponding results are reported in Table 4. All other analytical experiments followed the same configuration as in the preceding sections, except for the model and the reported outputs."
        }
    ],
    "affiliations": [
        "2077AI Foundation",
        "ByteDance Seed China",
        "Central South University",
        "LARG, SCIR, Harbin Institute of Technology",
        "M-A-P",
        "Nanjing University",
        "Peking University"
    ]
}