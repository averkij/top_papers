{
    "paper_title": "Typhoon OCR: Open Vision-Language Model For Thai Document Extraction",
    "authors": [
        "Surapon Nonesung",
        "Natapong Nitarach",
        "Teetouch Jaknamon",
        "Pittawat Taveekitworachai",
        "Kunat Pipatanakul"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Document extraction is a core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using a Thai-focused training dataset. The dataset is developed using a multi-stage data construction pipeline that combines traditional OCR, VLM-based restructuring, and curated synthetic data. Typhoon OCR is a unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is a compact and inference-efficient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 2 ] . [ 1 2 2 7 4 1 . 1 0 6 2 : r Typhoon OCR: Open VisionLanguage Model For Thai Document Extraction Surapon Nonesung, Natapong Nitarach, Teetouch Jaknamon, Pittawat Taveekitworachai, Kunat Pipatanakul Typhoon, SCB 10X"
        },
        {
            "title": "Abstract",
            "content": "Document extraction is core component of digital workflows, yet existing vision-language models (VLMs) predominantly favor high-resource languages. Thai presents additional challenges due to script complexity from non-latin letters, the absence of explicit word boundaries, and the prevalence of highly unstructured real-world documents, limiting the effectiveness of current open-source models. This paper presents Typhoon OCR, an open VLM for document extraction tailored for Thai and English. The model is fine-tuned from vision-language backbones using Thai-focused training dataset. The dataset is developed using multi-stage data construction pipeline that combines traditional OCR, VLMbased restructuring, and curated synthetic data. Typhoon OCR is unified framework capable of text transcription, layout reconstruction, and document-level structural consistency. The latest iteration of our model, Typhoon OCR V1.5, is compact and inferenceeﬀicient model designed to reduce reliance on metadata and simplify deployment. Comprehensive evaluations across diverse Thai document categories, including financial reports, government forms, books, infographics, and handwritten documents, show that Typhoon OCR achieves performance comparable to or exceeding larger frontier proprietary models, despite substantially lower computational cost. The results demonstrate that open vision-language OCR models can achieve accurate text extraction and layout reconstruction for Thai documents, reaching performance comparable to proprietary systems while remaining lightweight and deployable. Project Resources: Code: https://github.com/scb-10x/typhoon-ocr Typhoon OCR 7B: https://huggingface.co/scb10x/typhoon-ocr-7b Typhoon OCR V1.5 2B: https://huggingface.co/scb10x/typhoon-ocr1.5-2b Blog: https://opentyphoon.ai/blog/en/typhoon-ocr-release"
        },
        {
            "title": "Introduction",
            "content": "Recent vision-language models (VLMs) are capable of various visual-language tasks from document understanding, integrating optical character recognition, layout analysis, to semantic modeling as single model (Zhang et al., 2024). While these models demonstrate strong performance on high-resource languages, their effectiveness degrades substantially in low-resource settings, particularly for languages with complex scripts and limited annotated data (Nonesung et al., 2025). Thai language is one such case, where linguistic properties and document diversity present persistent challenges for existing models. The Thai writing system exhibits characteristics that complicate document understanding, including stacked diacritics, context-dependent vowel placement, and the absence of explicit word boundaries, all of which hinder reliable text segmentation and recognition (Haruechaiyasak et al., 2008). At the document level, Thai 1 Figure 1 Overview of Typhoon OCR, illustrating supported input document types and the corresponding structured output representations. materialssuch as administrative forms, financial records, receipts, and tabular reportsoften contain dense and irregular layouts, further increasing the diﬀiculty of accurate extraction and structural reconstruction. Figure 1 provides an overview of the document types supported by Typhoon OCR and the corresponding output formats. Despite this range of document types and output requirements, general-purpose VLMs, such as Qwen3-VL (Wei et al., 2025), Gemma 3 (Team et al., 2025), and InternVL 3.5 (Wang et al., 2025), which are predominantly trained on high-resource languages, frequently fail to capture these properties, resulting in elevated recognition errors, layout misinterpretation, and semantic inconsistencies. primary factor underlying these limitations is Thai data scarcity. The development of large language models (LLMs) for Thai has highlighted the broader challenge of limited high-quality Thai training data; recent Thai LLMs such as the Typhoon series are explicitly designed and evaluated on Thai language datasets to improve Thai language processing (Pipatanakul et al., 2024). This issue is more prevalent in multimodal data. In contrast to English or Chinese, Thai lacks large-scale datasets that align document images with structured textual and semantic annotations, limiting the adaptability of existing models. Consequently, many of existing VLMs have limited exposure to Thai-specific visual and linguistic patterns, restricting their ability to generalize across domains such as public administration, finance, and education. This imbalance is reflected in our evaluations (Tables 2, 35), where proprietary models such as GPT and Gemini achieve competitive performance on some categories but are generally outperformed by Typhoon OCR on structured Thai document extraction tasks. These challenges motivate the development of document understanding model explicitly adapted to Thai. Prior research in low-resource optical character recognition (OCR) and multilingual natural language processing suggests that fine-tuning large pretrained models with languageand domain-specific supervision can yield substantial gains without requiring training from scratch. For example, adapting multilingual vision language transformers to low-resource OCR tasks has been shown to improve recognition performance in non-Latin scripts (Cheema et al., 2024), and synthetic data augmentation and fine-tuning have been used to improve OCR performance across diverse low-resource languages (Ignat et al., 2022). Similarly, transfer learning and fine-tuning of pretrained multilingual models have been effective in low-resource machine translation settings (Dalal et al., 2024), illustrating the broader utility of model adaptation in low-resource linguistic domains. 2 In this work, we present Typhoon OCR, an open VLM for end-to-end Thai and English document understanding. The model is capable of various visual-language capabilities, including text extraction, layout reconstruction, and document-level semantic modeling. Typhoon OCR is obtained by fine-tuning an open VLM backbone using task-aligned training corpus constructed from curated real-world documents and synthetic data. By explicitly modeling document structure alongside textual content, our model has capabilities beyond traditional OCR pipelines focused primarily on transcription. Our contributions are as follows: We introduce Typhoon OCR, an open VLM tailored to Thai document extraction. We propose data curation and synthesis pipeline that mitigates multimodal data scarcity for Thai. We comprehensively evaluate our models against the baselines, demonstrating improvements over existing open baselines and competitive performance relative to proprietary models on Thai document benchmarks. Our models and associated resources are released under permissive licenses to support reproducible research and practical deployment."
        },
        {
            "title": "2 Typhoon OCR",
            "content": "Typhoon OCR is an open VLM for end-to-end document extraction in Thai and English, including text recognition and layout-aware content reconstruction. It targets real-world documents with complex and heterogeneous layouts, where existing OCR pipelines and general-purpose VLMs perform poorly in lowresource settings. The model is trained by fine-tuning vision-language backbone on task-aligned corpus combining curated real documents and synthetic data. Experiments show that Typhoon OCR substantially outperforms existing open-source methods and achieves competitive performance relative to proprietary models on Thai document benchmarks. 2.1 Data 2.1.1 Dataset Creation Pipeline Feature Default Mode Structure Mode Target Documents Layout Modeling Loosely structured or unstructured documents, such as receipts, menus, tickets, infographics, and handwritten notes. Highly structured documents with complex layouts, including financial reports, academic papers, government forms, and books. Lightweight preservation layout with an emphasis on content continuity and readability. Explicit structural parsing with detailed reconstruction of hierarchical and semantic regions. Output tion RepresentaOnly Markdown with minimal layout annotations. Markdown for narrative text, HTML for complex tables and structured layouts, and <figure> tags for visual elements. Table 1 Comparison of supervision modes used in Typhoon OCR. To support different types of documents, we construct training corpus that allows the model to operate in two modes: Default Mode and Structure Mode. These modes differ in how much layout information is preserved in the output. single supervision format is not suitable for both loosely structured documents (such as receipts or handwritten notes) and highly structured documents (such as financial reports or government forms). Using one format for all cases would either add unnecessary complexity to simple documents or fail to capture important layout details in complex ones. 3 We therefore adopt two modes as simple and practical division based on document structure: documents with little or weak layout structure, and documents with clear and complex layout organization. This separation covers most document types in our dataset without introducing excessive design complexity. This design also follows the availability of training data. Default Mode reuses general document annotations from the Typhoon2 Vision training corpus (Pipatanakul et al., 2024). Structure Mode, in contrast, is newly created using the dataset construction pipeline shown in Figure 2, which produces structure-aware annotations for complex documents. In practice, users choose the mode based on the document type, as summarized in Table 1. Figure 2 Overview of the multi-stage dataset construction pipeline used to generate training data for Typhoon OCR under Structure Mode. As shown in Figure 2, the training data are collected from diverse sources, including digital-native documents and scanned materials spanning multiple domains. Rather than applying single processing strategy, we use multi-stage pipeline that incrementally refines annotations while balancing scalability and supervision quality. These stages are: Stage 1 Textual content is extracted using conventional OCR systems and, where available, document textlayer parsing. This stage focuses on reliable characterand word-level transcription, particularly for clean digital documents and high-resolution scans, and provides stable low-level supervision signal. Stage 2 The extracted text is reorganized using open-source VLMs guided by structured prompts. This step transforms raw OCR outputs into coherent document representations by incorporating layout-aware formatting, section grouping, and basic semantic organization. Stage 3 Automated quality control is applied through agent-based consistency checks. These checks detect common failure modes, including structural inconsistencies, missing or duplicated content, incorrect ordering, and misalignment between visual input and generated annotations. Stage 4 subset of samples is selected for human verification. Annotators assess annotation fidelity with respect to the source documents, and samples exhibiting substantial or irrecoverable errors are removed from the corpus. This multi-stage pipeline enables scalable dataset construction while mitigating noise from automated annotation. As result, the final corpus provides consistent, high-quality supervision for fine-tuning Typhoon OCR under both modes. 2.1.2 Dataset Statistics Figure 3a summarizes the composition of the training corpus used to fine-tune Typhoon OCR. The dataset is curated from publicly available sources in both Thai and English and spans wide range of document types, including infographics, government forms, financial reports, books, and handwritten materials. This diversity is intended to reflect realistic variation in document layout, visual quality, and linguistic content encountered in practical document understanding tasks. (a) Distribution of training samples across modes (b) Distribution of training samples by data source Figure 3 Composition of the training corpus used for Typhoon OCR. Figure 3a shows the allocation of samples between Default Mode and Structure Mode supervision, while Figure 3b presents the relative contributions of different data sources. As shown in Figure 3b, the corpus consists of heterogeneous mixture of document sources across both languages. General infographic-style documents constitute the largest portion of the dataset (45.6%), providing broad coverage of diverse layouts and visual styles. Structured synthetic documents from the CoSyn-400K dataset1 (Yang et al., 2025) account for 8.3% of the corpus and provide controlled layout variations that facilitate learning of document structure. Domain-specific Thai documents form substantial component of the dataset. These include financial reports published by Thai SET organizations (7.2%), digitized Thai books spanning multiple genres and formatting conventions (5.6%), and handwritten materials sampled from the iAPP Thai handwriting dataset2 (5.5%). For the handwriting data, subsampling is applied to mitigate the impact of known labeling inconsistencies. Additional scanned documents and book pages are drawn from the olmOCR-mix-0225 collection3, contributing combined 6.2% of the corpus. The remaining portion of the dataset comprises Thai InnovestX reports, financial infographics, and bills and invoices (8.7%), alongside long-tail category (13.0%) that includes government forms, certificates, academic papers, and other miscellaneous scanned documents. In total, the training corpus contains 77,029 document samples. 2.2 Experimental Setup 2.2.1 Training Typhoon OCR is trained using full-parameter supervised fine-tuning (SFT) on the Qwen2.5-VL model family (Bai et al., 2025b), covering both the 3B and 7B parameter variants. The training pipeline is based on the open-source olmOCR framework4 (Poznanski et al., 2025), with extensions to support multilingual document understanding and long-context modeling. The models are trained on 4H100 GPUs for three epochs, and the final checkpoint is selected based on performance on held-out validation set. 1https://huggingface.co/datasets/allenai/CoSyn-400K 2https://huggingface.co/datasets/iapp/thai_handwriting_dataset 3https://huggingface.co/datasets/allenai/olmOCR-mix-0225 4https://github.com/allenai/olmocr 5 Input document images are resized to fixed width of 1,800 pixels, providing trade-off between visual fidelity and computational eﬀiciency. The anchor text length is set to 8,000 tokens to balance layout coverage and memory constraints, and the maximum sequence length is limited to 17,000 tokens, allowing the model to process long-form documents and dense layouts without truncation. 2.2.2 Evaluation Metrics We evaluate Typhoon OCR using standard metrics from OCR and natural language generation (NLG) that capture complementary aspects of output quality. Lexical accuracy is assessed using BLEU (Papineni et al., 2002), which measures n-gram overlap between predicted and reference texts. Structural and sequential similarity is measured using ROUGE-L (Lin, 2004), based on the longest common subsequence. Although originally developed for NLG, these metrics correlate well with document extraction in structured outputs. Character-level transcription fidelity is quantified using Levenshtein distance (Haldar and Mukhopadhyay, 2011), where lower values indicate fewer edit operations. Benchmark Evaluation is conducted on an in-house Thai document corpus designed to reflect real-world document variability in both layout and content. The corpus comprises three categories: Thai financial reports, which include complex tables, charts, and multilingual content; Thai government forms, characterized by dense layouts, domain-specific terminology, and handwritten annotations; and Thai books, consisting of long-form text interleaved with figures, diagrams, and other visual elements. Protocol Evaluation is conducted under two input conditions to examine robustness across document representations. In the PDF with metadata setting, the model is provided with native PDF information, including text layers and layout annotations, when available. In the image-only setting, the model receives rasterized document images without access to structural metadata. This setup isolates the contribution of explicit layout information and allows analysis of the models ability to infer document structure directly from visual input. Model BLEU ROUGE-L Levenshtein Thai Financial Reports GPT-4o (2024-11-20) Gemini 2.5 Flash (2025-04-17) Typhoon OCR 3B (PDF) Typhoon OCR 3B (Image) Typhoon OCR 7B (PDF) Typhoon OCR 7B (Image) Thai Government Forms GPT-4o (2024-11-20) Gemini 2.5 Flash (2025-04-17) Typhoon OCR 3B (PDF) Typhoon OCR 3B (Image) Typhoon OCR 7B (PDF) Typhoon OCR 7B (Image) Thai Books GPT-4o (2024-11-20) Gemini 2.5 Flash (2025-04-17) Typhoon OCR 3B (PDF) Typhoon OCR 3B (Image) Typhoon OCR 7B (PDF) Typhoon OCR 7B (Image) 0.25 0.52 0.90 0.90 0.91 0.91 0.25 0.74 0.92 0.93 0.89 0.89 0.34 0.47 0.63 0.64 0.63 0.64 0.51 0.70 0.93 0.93 0.94 0.94 0.45 0.87 0.96 0.96 0.94 0. 0.49 0.59 0.71 0.71 0.71 0.72 0.56 0.35 0.08 0.07 0.07 0.08 0.57 0.15 0.05 0.04 0.08 0.07 0.59 0.47 0.32 0.32 0.31 0.31 Table 2 Performance comparison on Thai document parsing in Structure Mode. Higher BLEU and ROUGE-L and lower Levenshtein indicate better performance. 2.3 Results and Discussion Table 2 reports performance of Typhoon OCR (3B and 7B) relative to GPT-4o and Gemini 2.5 Flash across document categories. Typhoon OCR consistently outperforms baseline models on financial reports and government forms, which exhibit dense layouts and structured content. These gains are most pronounced when PDF metadata are available, indicating that explicit layout cues contribute to improved structural reconstruction. Performance on the Thai Books subset is lower across all models. This category introduces additional complexity due to frequent visual elements, such as illustrations and non-standard figures, which increase ambiguity in figure representation and layout interpretation. The results suggest that figure understanding remains limitation in current VLMs. The performance difference between PDF-based and image-only inputs is small for Typhoon OCR, indicating effective alignment between visual and textual representations. Notably, the 3B variant achieves results comparable to the 7B model on several tasks, particularly government forms, suggesting that smaller models can be effective under constrained deployment settings. During our pilot experiments, we observe that training with heterogeneous image resolutions leads to less stable optimization and reduced accuracy. Standardizing inputs by resizing images to fixed width of 1,800 pixels while preserving aspect ratio, as we applied to Typhoon OCR, improves both training stability and evaluation performance. This finding aligns with prior observations that OCR and document understanding are sensitive to resolution variability, especially in low-resource training regimes (Xiao et al., 2025)."
        },
        {
            "title": "3 Typhoon OCR V1.5",
            "content": "While Typhoon OCR has received lot of positive feedback, post-deployment analysis of Typhoon OCR revealed some limitations that affect its usability and eﬀiciency in real-world settings. First, dependence on PDF anchor metadata introduced increased inference latency, particularly for long documents and those with complex layouts. Second, the separation of operating modes increased user-facing complexity and led to performance variability when modes were not appropriately selected. Third, although the 3B and 7B models are relatively small compared to frontier VLMs, further reducing their computational footprint would significantly increase their applicability in latency-sensitive and resource-constrained environments. Finally, although the training corpus covered diverse document types, further expansion in data diversity and output representations was necessary to improve robustness and generalization. Typhoon OCR V1.5 addresses these limitations through data and training refinements aimed at improving inference eﬀiciency, simplifying the interaction interface, and strengthening robustness across document types and deployment scenarios. 3.1 Data 3.1.1 Dataset Creation Pipeline In Typhoon OCR V1, dataset construction relied on separate mode strategies for different operating modes and on PDF anchor metadata for annotation. In V1.5, this design is simplified by adopting single unified mode, eliminating the need for mode selection during training and inference and enabling annotations to be generated directly from visual input. Annotation quality is further improved by using stronger and more recent multilingual labeling models specifically Qwen3-VL (Bai et al., 2025a) and Dots.OCR (Li et al., 2025)which provide enhanced multilingual and Thai document understanding capabilities. In addition, the training corpus is expanded to cover wider range of document types and layouts. Beyond real-world documents, we incorporate two additional data sources to address complementary limitations. First, Thai-translated visual question answering (VQA) data is included to preserve general vision language grounding and basic multimodal reasoning ability in Thai, preventing over-specialization to document transcription alone. Second, synthetic documents are generated to compensate for the scarcity of 7 annotated Thai documents containing complex layouts, mathematical expressions, and charts, and to increase coverage of rare vocabulary and typographic variations. Overall, while the data construction pipeline follows that of the original Typhoon OCR (Figure 2), these changes reduce pipeline complexity and improve robustness and consistency across diverse document extraction scenarios. Next, we detail the data processing and synthesis pipelines for the additional data sources used in Typhoon OCR V1.5. Thai VQA The Cauldron5 (Laurençon et al., 2024), containing diverse visual question answering tasks covering object recognition, spatial reasoning, and basic document understanding, but is primarily available in English. To improve the models performance in Thai, we randomly sample VQA instances and translate both questions and answers into Thai using Typhoon Translate6. Synthetic Data Synthetic documents are introduced to address gaps in real-world Thai document coverage, particularly for mathematical notation, rare vocabulary, typographic variation, and complex visual elements. As shown in Figure 4, we construct document generation pipeline with the following stages: Figure 4 Multi-stage pipeline for generating synthetic Thai document images for OCR training. Stage 1 Thai vocabulary is randomly sampled from PyThaiNLP (Phatthiyaphaibun et al., 2023) and rendered using diverse font families and font sizes to increase robustness to typographic variation and lowfrequency lexical forms Stage 2 Visual elements are sampled from SEA-VL Crawling7 (Cahyawijaya et al., 2025), which contains culturally relevant images from Southeast Asia, and from ChartCap8 (Lim et al., 2025) to provide real-world charts and plots for training layout and figure understanding. Stage 3 Mathematical expressions are sampled from LaTeX OCR9 and OleehyO LaTeX Formulas10 to improve recognition of equation structures and symbolic layouts. Stage 4 We apply document-level image augmentation using Augraphy11 (Groleau et al., 2023) to simulate real-world acquisition artifacts such as blur, noise, compression, illumination variation, and geometric distortion. This step increases visual diversity and improves robustness to degraded scanning and photographic conditions. 5https://huggingface.co/datasets/HuggingFaceM4/the_cauldron 6https://huggingface.co/scb10x/typhoon-translate-4b 7https://huggingface.co/datasets/SEACrowd/sea-vl_crawling 8https://huggingface.co/datasets/junyoung-00/ChartCap 9https://huggingface.co/datasets/linxy/LaTeX_OCR 10https://huggingface.co/datasets/lamm-mit/OleehyO-latex-formulas 11https://github.com/sparkfish/augraphy 8 These components are combined to generate documents containing mixed textual, mathematical, and visual content with controlled layout variation and Thai-specific linguistic features. 3.1.2 Dataset Statistics Figure 5 summarizes the statistics of training corpus used to fine-tune Typhoon OCR V1.5. The training corpus consists of mixture of Thai and English document sources. Figure 5 Training dataset distribution for Typhoon OCR V1.5 The largest portion of the dataset (53.7%) is retained from the Typhoon OCR V1 training corpus to ensure performance consistency across model versions and preserve coverage of previously observed document distributions. The dataset also include VQA samples from The Cauldron, constituting 2.2% of the overall training corpus and is intentionally kept as small fraction, as Typhoon OCR is primarily designed for document understanding rather than general-purpose VQA. The inclusion of this subset aims to preserve basic multimodal reasoning capabilities and mitigate catastrophic forgetting of generic vision-language knowledge during document-centric fine-tuning. Structured document supervision is supplemented using subset of DocLayNet-v1.212 (Pfitzmann et al., 2022), contributing 6.4% of the corpus and providing high-quality annotations for layout segmentation and structural regions. The remaining 37.6% consists of synthetic documents generated using the pipeline described in the previous section. We intentionally maintain substantial synthetic portion to compensate for the limited availability of annotated Thai documents containing equations and charts. In total, the training corpus contains 155,403 document samples. 3.2 Experimental Setup 3.2.1 Training Typhoon OCR V1.5 is trained using full-parameter SFT on the Qwen3-VL 2B VLM. The training framework is based on the open-source Axolotl framework13 (Axolotl maintainers and contributors, 2023), with extensions to support long-context multimodal inputs and document-centric training objectives. During preprocessing, resolution-aware strategy is applied to balance visual fidelity and computational eﬀiciency. Images with maximum dimension below 1,800 pixels are retained at their original resolution, while larger images are resized to maximum width of 1,800 pixels with aspect ratio preserved. The maximum sequence length is set to 16,384 tokens to accommodate long documents and complex layouts. 12https://huggingface.co/datasets/docling-project/DocLayNet-v1.2 13https://github.com/axolotl-ai-cloud/axolotl 9 Training is performed on 4H100 GPUs for two epochs, and the final checkpoint is selected based on validation performance. Quantization-aware training (Jacob et al., 2018) is applied during fine-tuning to expose the model to quantization effects, enabling eﬀicient low-precision inference with only limited impact on accuracy. 3.2.2 Evaluation Protocol and Metrics We evaluate Typhoon OCR V1.5 using the same metrics and protocols as V1 (Section 2.2.2) to ensure comparability across model iterations. Namely, we report BLEU, ROUGE-L, and Levenshtein distance to measure lexical accuracy, sequence-level structural similarity, and character-level transcription fidelity, respectively. Benchmark Evaluation is conducted on held-out test sets from Typhoon OCR V1, which are further refined through additional human annotation and quality control to reduce noise and improve consistency. In addition, two new evaluation sets are introduced to assess performance on document types that were underrepresented in V1. The evaluation corpus comprises six categories: 1. Thai Books: Consist of long-form documents with extended text interleaved with figures and illustrations. 2. Thai Government Forms: Reused from V1 with enhanced human verification. This benchmark includes dense layouts, tables, charts, and handwritten annotations. 3. Thai Financial Reports: Also reused from V1 with enhanced human verification. 4. Infographics: Contain loosely structured documents with prominent visual elements. 5. Handwritten Forms: Consist of administrative and financial forms that combine handwritten and printed text. 6. Others: diverse collection of semi-structured and unstructured documents, including receipts, bills, tickets, and miscellaneous transactional records. Task Gemini 2.5 Pro GPT-5 Typhoon OCR V1 7B Typhoon OCR V1.5 2B Thai Books Thai Government Forms Thai Financial Reports Infographics Handwriting Forms Others Average 0.512 0.797 0.657 0.465 0.594 0.603 0.605 0.710 0.569 0.457 0.297 0.368 0.352 0.459 0.708 0.849 0.849 0.246 0.321 0. 0.558 0.746 0.870 0.819 0.408 0.522 0.499 0.644 Table 3 BLEU scores by document category (higher is better). 3.3 Results and Discussion Tables 3 to 5 report comparative evaluation of Typhoon OCR V1.5 against Typhoon OCR V1 and two frontier VLM baselines across multiple Thai document categories. We analyze performance trends in relation to model design and document characteristics. Across all metrics, Typhoon OCR V1.5 achieves higher average performance than Typhoon OCR V1, despite having fewer parameters (2B versus 7B). This result indicates that improved data and training recipe have greater impact on document extraction quality than model size alone. From deployment perspective, the results suggest that compact, task-adapted models can provide strong performance while reducing computational overhead compared to one-size-fits-all proprietary models. Performance gains are most pronounced for structured document types, including Thai government forms and financial reports. In these categories, Typhoon OCR V1.5 consistently outperforms proprietary baselines on 10 Task Gemini 2.5 Pro GPT-5 Typhoon OCR V1 7B Typhoon OCR V1.5 2B Thai Books Thai Government Forms Thai Financial Reports Infographics Handwriting Forms Others Average 0.676 0.894 0.757 0.677 0.739 0.716 0.743 0.922 0.706 0.603 0.481 0.514 0.482 0.618 0.871 0.942 0.933 0.373 0.454 0.541 0. 0.949 0.967 0.910 0.527 0.645 0.645 0.774 Table 4 ROUGE-L scores by document category (higher is better). Task Gemini 2.5 Pro GPT-5 Typhoon OCR V1 7B Typhoon OCR V1.5 2B Thai Books Thai Government Forms Thai Financial Reports Infographics Handwriting Forms Others Average 0.334 0.096 0.256 0.380 0.327 0.342 0.289 0.084 0.267 0.356 0.561 0.533 0.540 0.390 0.136 0.065 0.082 0.671 0.556 0. 0.332 0.053 0.035 0.079 0.544 0.416 0.377 0.251 Table 5 Levenshtein distance by document category (lower is better). BLEU and ROUGE-L and achieves lower Levenshtein distances. This behavior reflects the benefit of explicit layout modeling and domain-aligned supervision for documents with regular structural patterns, which are common in administrative and financial workflows. For visually heterogeneous categories such as infographics and handwritten forms, proprietary models achieve lower character-level error rates. Nevertheless, Typhoon OCR V1.5 substantially improves over V1, narrowing the gap in both lexical and structural metrics. This is an area for improvement that we aim to improve in future iterations."
        },
        {
            "title": "4 Conclusion",
            "content": "This report introduces Typhoon OCR, family of VLMs designed to address limitations in document understanding for Thai. The models consistently show improvements over the base model across various document understanding tasks, including transcription accuracy, layout reconstruction, and structural consistency, and perform competitively with frontier proprietary systems. Notably, Typhoon OCR V1.5 achieves these gains with smaller model of just 2B parameters, reducing inference cost while matching or exceeding the performance of larger proprietary models across multiple document categories. These results demonstrate that robust document understanding in low-resource settings can be achieved through targeted adaptation of pretrained VLMs and carefully designed data pipelines, without training models from scratch or relying on closed systems, making this approach suitable for resource-constrained and privacy-sensitive deployments. Several limitations remain. Performance degrades on severely degraded inputs, such as low-resolution images, motion blur, and occlusions, suggesting the need for improved data recipes or explicit modeling of noise and capture artifacts. In addition, although the models currently support primarily Thai and English, extending them to other low-resource languages is natural direction for future research. Moreover, the current Typhoon OCR model series focuses on document extraction and does not explicitly support higherlevel reasoning tasks. Future work will extend the framework to applications such as diagram understanding and structured information extraction. While our evaluation is suitable for the current development stage, 11 broader assessment on academic benchmarks, such as ThaiOCRBench (Nonesung et al., 2025), is natural next step for better understanding model capabilities."
        },
        {
            "title": "Acknowledgments",
            "content": "Beyond the primary authors, we gratefully acknowledge the Typhoon Team members at SCB 10X whose contributions made this project possible: Adisai Na-Thalang, Chanakan Wittayasakpan, Kritsadha Phatcharoen, Tanawin Samutsin, Shah Faisal Wani, Sittipong Sripaisarnmongkol, Warit Sirichotedumrong, Potsawee Manakul, Krisanapong Jirayoot, Oravee Smithiphol, Kasima Tharnpipitchai, and Kaweewut Temphuwapat. We also extend our appreciation to the SCBx R&D Team for their support, resources, and valuable insights. Lastly, we are grateful to the global and local AI communities for open-sourcing resources and sharing knowledge."
        },
        {
            "title": "References",
            "content": "Axolotl maintainers and contributors. Axolotl: Open source llm post-training, 2023. URL https://github.com/ axolotl-ai-cloud/axolotl. Shuai Bai, Yuxuan Cai, Ruizhe Chen, Keqin Chen, Xionghui Chen, Zesen Cheng, Lianghao Deng, Wei Ding, Chang Gao, Chunjiang Ge, Wenbin Ge, Zhifang Guo, Qidong Huang, Jie Huang, Fei Huang, Binyuan Hui, Shutong Jiang, Zhaohai Li, Mingsheng Li, Mei Li, Kaixin Li, Zicheng Lin, Junyang Lin, Xuejing Liu, Jiawei Liu, Chenglong Liu, Yang Liu, Dayiheng Liu, Shixuan Liu, Dunjie Lu, Ruilin Luo, Chenxu Lv, Rui Men, Lingchen Meng, Xuancheng Ren, Xingzhang Ren, Sibo Song, Yuchong Sun, Jun Tang, Jianhong Tu, Jianqiang Wan, Peng Wang, Pengfei Wang, Qiuyue Wang, Yuxuan Wang, Tianbao Xie, Yiheng Xu, Haiyang Xu, Jin Xu, Zhibo Yang, Mingkun Yang, Jianxin Yang, An Yang, Bowen Yu, Fei Zhang, Hang Zhang, Xi Zhang, Bo Zheng, Humen Zhong, Jingren Zhou, Fan Zhou, Jing Zhou, Yuanzhi Zhu, and Ke Zhu. Qwen3-vl technical report, 2025a. URL https://arxiv.org/abs/2511.21631. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025b. URL https://arxiv.org/abs/2502.13923. Samuel Cahyawijaya, Holy Lovenia, Joel Ruben Antony Moniz, Tack Hwa Wong, Mohammad Rifqi Farhansyah, Thant Thiri Maung, Frederikus Hudi, David Anugraha, Muhammad Ravi Shulthan Habibi, Muhammad Reza Qorib, Amit Agarwal, Joseph Marvin Imperial, Hitesh Laxmichand Patel, Vicky Feliren, Bahrul Ilmi Nasution, Manuel Antonio Rufino, Genta Indra Winata, Rian Adam Rajagede, Carlos Rafael Catalan, Mohamed Fazli Imam, Priyaranjan Pattnayak, Salsabila Zahirah Pranida, Kevin Pratama, Yeshil Bangera, Adisai Na-Thalang, Patricia Nicole Monderin, Yueqi Song, Christian Simon, Lynnette Hui Xian Ng, Richardy Lobo Sapan, Taki Hasan Rafi, Bin Wang, Supryadi, Kanyakorn Veerakanjana, Piyalitt Ittichaiwong, Matthew Theodore Roque, Karissa Vincentio, Takdanai Kreangphet, Phakphum Artkaew, Kadek Hendrawan Palgunadi, Yanzhi Yu, Rochana Prih Hastuti, William Nixon, Mithil Bangera, Adrian Xuan Wei Lim, Aye Hninn Khine, Hanif Muhammad Zhafran, Teddy Ferdinan, Audra Aurora Izzani, Ayushman Singh, Evan, Jauza Akbar Krito, Michael Anugraha, Fenal Ashokbhai Ilasariya, Haochen Li, John Amadeo Daniswara, Filbert Aurelian Tjiaranata, Eryawan Presma Yulianrifat, Can Udomcharoenchaikit, Fadil Risdian Ansori, Mahardika Krisna Ihsani, Giang Nguyen, Anab Maulana Barik, Dan John Velasco, Rifo Ahmad Genadi, Saptarshi Saha, Chengwei Wei, Isaiah Flores, Kenneth Ko Han Chen, Anjela Gail Santos, Wan Shen Lim, Kaung Si Phyo, Tim Santos, Meisyarah Dwiastuti, Jiayun Luo, Jan Christian Blaise Cruz, Ming Shan Hee, Ikhlasul Akmal Hanif, M. Alif Al Hakim, Muhammad Rizky Syaban, Kun Kerdthaisong, Lester James V. Miranda, Fajri Koto, Tirana Noor Fatyanosa, Alham Fikri Aji, Jostin Jerico Rosal, Jun Kevin, Robert Wijaya, Onno P. Kampman, Ruochen Zhang, Börje F. Karlsson, and Peerat Limkonchotiwat. Crowdsource, crawl, or generate? creating sea-vl, multicultural vision-language dataset for southeast asia, 2025. URL https://arxiv.org/abs/2503.07920. Muhammad Daniyal Aslam Cheema, Muhammad Danish Shaiq, Faizan Mirza, Ahsan Kamal, and Muhammad Ahsan Naeem. Adapting multilingual vision language transformers for low-resource urdu optical character recognition (ocr). PeerJ Computer Science, 10:e1964, 2024. doi: 10.7717/peerj-cs.1964. Siddhartha Dalal, Rahul Aditya, Vethavikashini Chithrra Raghuram, and Prahlad Koratamaddi. AI-tutor: Interactive learning of ancient knowledge from low-resource languages. In Toshiaki Nakazawa and Isao Goto, editors, Proceedings of the Eleventh Workshop on Asian Translation (WAT 2024), pages 5666, Miami, Florida, 12 USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.wat-1.5. URL https: //aclanthology.org/2024.wat-1.5/. Alexander Groleau, Kok Wei Chee, Stefan Larson, Samay Maini, and Jonathan Boarman. Augraphy: data augmentation library for document images. In Proceedings of the 17th International Conference on Document Analysis and Recognition (ICDAR), 2023. URL https://arxiv.org/pdf/2208.14558.pdf. Rishin Haldar and Debajyoti Mukhopadhyay. Levenshtein distance technique in dictionary lookup methods: An improved approach, 2011. URL https://arxiv.org/abs/1101.1232. Choochart Haruechaiyasak, Sarawoot Kongyoung, and Matthew Dailey. comparative study on thai word segmentation approaches. In 2008 5th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology, volume 1, pages 125128, 2008. doi: 10.1109/ECTICON.2008.4600388. Oana Ignat, Jean Maillard, Vishrav Chaudhary, and Francisco Guzmán. OCR improves machine translation for low-resource languages. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of the Association for Computational Linguistics: ACL 2022, pages 11641174, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.92. URL https://aclanthology.org/2022. findings-acl.92/. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for eﬀicient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models?, 2024. Yumeng Li, Guang Yang, Hao Liu, Bowen Wang, and Colin Zhang. dots.ocr: Multilingual document layout parsing in single vision-language model, 2025. URL https://arxiv.org/abs/2512.02498. Junyoung Lim, Jaewoo Ahn, and Gunhee Kim. Chartcap: Mitigating hallucination of dense chart captioning, 2025. URL https://arxiv.org/abs/2508.03164. Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology. org/W04-1013/. Surapon Nonesung, Teetouch Jaknamon, Sirinya Chaiophat, Natapong Nitarach, Chanakan Wittayasakpan, Warit Sirichotedumrong, Adisai Na-Thalang, and Kunat Pipatanakul. Thaiocrbench: task-diverse benchmark for vision-language understanding in thai, 2025. URL https://arxiv.org/abs/2511.04479. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. doi: 10.3115/1073083.1073135. URL https://aclanthology.org/ P02-1040/. Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. Doclaynet: large humanannotated dataset for document-layout segmentation. page 37433751, 2022. doi: 10.1145/3534678.353904. URL https://doi.org/10.1145/3534678.3539043. Wannaphong Phatthiyaphaibun, Korakot Chaovavanich, Charin Polpanumas, Arthit Suriyawongkul, Lalita Lowphansirikul, Pattarawat Chormai, Peerat Limkonchotiwat, Thanathip Suntorntip, and Can Udomcharoenchaikit. PyThaiNLP: Thai natural language processing in Python. In Liling Tan, Dmitrijs Milajevs, Geeticka Chauhan, Jeremy Gwinnup, and Elijah Rippeth, editors, Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 2536, Singapore, Singapore, December 2023. Empirical Methods in Natural Language Processing. URL https://aclanthology.org/2023.nlposs-1.4. Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, and Kasima Tharnpipitchai. Typhoon 2: family of open text and multimodal thai large language models, 2024. URL https://arxiv.org/abs/2412.13702. 13 Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models, 2025. URL https://arxiv.org/abs/2502.18443. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, JanThorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucińska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry, Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report, 2025. URL https://arxiv.org/abs/2503.19786. Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, Zhaokai Wang, Zhe Chen, Hongjie Zhang, Ganlin Yang, Haomin Wang, Qi Wei, Jinhui Yin, Wenhao Li, Erfei Cui, Guanzhou Chen, Zichen Ding, Changyao Tian, Zhenyu Wu, Jingjing Xie, Zehao Li, Bowen Yang, Yuchen Duan, Xuehui Wang, Zhi Hou, Haoran Hao, Tianyi Zhang, Songze Li, Xiangyu Zhao, Haodong Duan, Nianchen Deng, Bin Fu, Yinan He, Yi Wang, Conghui He, Botian Shi, Junjun He, Yingtong Xiong, Han Lv, Lijun Wu, Wenqi Shao, Kaipeng Zhang, Huipeng Deng, Biqing Qi, Jiaye Ge, Qipeng Guo, Wenwei Zhang, Songyang Zhang, Maosong Cao, Junyao Lin, Kexian Tang, Jianfei Gao, Haian Huang, Yuzhe Gu, Chengqi Lyu, Huanze Tang, Rui Wang, Haijun Lv, Wanli Ouyang, Limin Wang, Min Dou, Xizhou Zhu, Tong Lu, Dahua Lin, Jifeng Dai, Weijie Su, Bowen Zhou, Kai Chen, Yu Qiao, Wenhai Wang, and Gen Luo. Internvl3.5: Advancing open-source multimodal models in versatility, reasoning, and eﬀiciency, 2025. URL https://arxiv.org/abs/2508.18265. Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression, 2025. URL https://arxiv. org/abs/2510.18234. Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, and Yu Rong. Scaling language-centric omnimodal representation learning, 2025. URL https://arxiv.org/abs/2510.11693. Yue Yang, Ajay Patel, Matt Deitke, Tanmay Gupta, Luca Weihs, Andrew Head, Mark Yatskar, Chris Callison-Burch, Ranjay Krishna, Aniruddha Kembhavi, et al. Scaling text-rich image understanding via code-guided synthetic multimodal data generation. arXiv preprint arXiv:2502.14846, 2025. Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey, 2024. URL https://arxiv.org/abs/2304.00685."
        }
    ],
    "affiliations": [
        "SCB 10X",
        "Typhoon"
    ]
}