{
    "paper_title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
    "authors": [
        "Xiaoji Zheng",
        "Ziyuan Yang",
        "Yanhao Chen",
        "Yuhang Peng",
        "Yuanrong Tang",
        "Gengyuan Liu",
        "Bokui Chen",
        "Jiangtao Gong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD."
        },
        {
            "title": "Start",
            "content": "COIRL-AD: COLLABORATIVECOMPETITIVE IMITATIONREINFORCEMENT LEARNING IN LATENT WORLD MODELS FOR AUTONOMOUS DRIVING Xiaoji Zheng1, Ziyuan Yang2 , Yanhao Chen3, Yuhang Peng4, Yuanrong Tang1, Gengyuan Liu1, Bokui Chen1, Jiangtao Gong1 1 Tsinghua University 3 Beijing Jiaotong University chenbk@tsinghua.edu.cn 4 The Hong Kong Polytechnic University gongjiangtao@air.tsinghua.edu.cn 2 University of Washington 5 2 0 2 4 1 ] . [ 1 0 6 5 2 1 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD."
        },
        {
            "title": "INTRODUCTION",
            "content": "End-to-end learning has become the mainstream paradigm in autonomous driving (Hu et al., 2023; Jiang et al., 2023; Weng et al., 2024). Unlike modular pipelines, end-to-end models allow gradients to propagate across perception, prediction, and planning, enabling all components to be optimized toward the final driving objective. Most existing approaches rely on imitation learning (IL), where models are trained to mimic expert demonstrations. In practice, these methods are essentially supervised learning (SL): the models outputs are directly supervised to match expert trajectories. The effectiveness of SL relies on the assumption that data are independent and identically distributed (IID). However, in embodied tasks such as driving, this assumption failsobservations are temporally correlated, and small prediction errors can accumulate, pushing the vehicle outside its safety zone and leading to cascading failures. As result, IL (or more precisely, SL-based IL) agents often generalize poorly and struggle on long-tail scenarios. To mitigate these issues, prior work has attempted to expand the training distribution, for example using generative world models to generate more training data (Wang et al., 2024; Wen et al., 2024; Gao et al., 2024). However, generated data remain limited in realism and computationally costly. RL offers another solution by encouraging exploration and learning from trial-and-error. Yet applying RL in simulators suffers from two drawbacks: (1) high-fidelity expert demonstrations are often unavailable. In fact, many expert drivers in simulators are themselves trained using RL rather than real-world data. Without genuine expert demonstrations, IL cannot be applied, which also prevents combining IL and RL in such settings. (2) Agents trained in simulators also face sim-to-real transfer challenges, where policies that succeed in virtual environments may fail in the real world. So in this paper, we investigate offline RL directly on expert datasets. While appealingly simple, this setting introduces new challenges: 1) Since experts already achieve near-optimal rewards, naively Equal contribution. Work done during internships at Institute for AI Industry Research (AIR), Tsinghua University. Corresponding authors. 1 fitting expert transitions reduces RL to IL, offering limited exposure to novel states; 2) Non-reactive simulation can provide evaluation metrics (e.g., L2 error, collision rate) for hypothetical actions, but cannot yield new states following those actions. To solve the two problems above, we 1) inspired by GRPO (Shao et al., 2024), we use group sampling, allowing the agent to generate multiple candidate action sequences and evaluate them through non-reactive simulation; 2) leverage latent world model as reactive simulator to predict future states conditioned on sampled actions, enabling imagination-based training beyond ground-truth data. Finally, to integrate IL and RL without gradient conflict (if simply add the loss of IL and RL), we introduce dual-policy architecture that decouples the two objectives into separate actors. competition-based learning mechanism fosters interaction and selective knowledge transfer between the IL and RL actors. Figure 1: Overview of CoIRL-AD. CoIRL-AD adopts dual-policy architecture that integrates imitation learning (IL) and reinforcement learning (RL) through shared latent world model. In each iteration, the IL actor and RL actor are trained in parallel. The latent world model is learned during the IL phase and then used in the RL phase, where only the RL actor and critic are updated. For exploration, the RL actor samples multiple action sequences, predicts future states via the latent world model, and evaluates them with rule-based reward functions. The critic assigns advantages to each sequence based on the imagined future states and rewards. To promote interaction, competitive learning mechanism exchanges knowledge between the IL and RL actors. Our contributions are summarized as follows: We integrate RL into an end-to-end driving framework by leveraging latent world model for imagination-based simulation, avoiding reliance on external simulators. We propose dual-policy competitive learning framework that jointly trains IL and RL while encouraging interaction through structured competition. We conduct extensive experiments on nuScenes and Navsim, showing that our method improves generalization, reduces collisions, and achieves stronger performance on longtail scenarios compared to baselines."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 END-TO-END AUTONOMOUS DRIVING End-to-end autonomous driving methods replaced traditional modular design by the end-to-end manner. UniAD (Hu et al., 2023) first demonstrated the potential of end-to-end by unifying per2 ception and planning within unified framework. VAD (Jiang et al., 2023) vectorized the scene representation and improved the efficiency of inference. PARA-Drive (Weng et al., 2024) decomposed the traditional pipeline and searched for optimal architectures. Some works also integrated world model. LAW (Li et al., 2025b) and World4Drive (Zheng et al., 2025) predicted future visual latents via world model, improving temporal understanding. SSR (Li & Cui, 2025) utilized sparse tokens to represent dense BEV features and similarly employed world model to predict the next feature to enhance scene comprehension. WoTE (Li et al., 2025c) leverages world model to predict future states, enabling online trajectory evaluation and selection. Our approach leverages world model as an offline simulator. Specifically, the RL policy iteratively interacts with world model to imagine future scene transitions, enabling reward-driven policy optimization."
        },
        {
            "title": "2.2 RL IN AUTONOMOUS DRIVING",
            "content": "Reinforcement learning plays an important role in autonomous driving. Roach (Zhang et al., 2021) trained an RL expert to map BEV input to actions, subsequently served as teacher for the student model. VLM-RL (Huang et al., 2024) leveraged Vision-Language-Model (VLM) to generate rewards signals for RL. Think2Drive (Li et al., 2024a) integrated DreamV3 to train an expert model, becoming the first agent to finish CARLA v2. AdaWM (Wang et al., 2025) analyzed performance degradation of driving agents, proposing strategy that selectively updates actor or world model. Imagine2Drive (Garg & Krishna, 2025) proposed novel framework by integrating video world model (Gao et al., 2024) with diffusion-based policy, achieving impressive performance in the CARLA. Our fully end-to-end model conducts RL in the latent space with world model and integrates with IL to achieve more stable training. 2.3 COMBINE IL AND RL IN AUTONOMOUS DRIVING The combination of RL and IL is an important problem in autonomous driving. AutoVLA (Zhou et al., 2025) conducted supervised fine-tuning to learn how to reason and later applied GRPO (Shao et al., 2024) to achieve faster reasoning. RAD (Gao et al., 2025) constructed large 3D environment and mixed IL and RL during training. TrajHF (Li et al., 2025a) used IL fine-tuning and RLHF on large collected preference data and achieve impressive performance. ReCogDrive (Li et al., 2025d) incorporated expert imitation loss and RL-loss in simulator to explore safer trajectories. Our approach performed competitive framework that optimize IL and RL simultaneously, allowing them to share information for safer action."
        },
        {
            "title": "3 METHOD",
            "content": "3.1 ACTOR MODELING Given current observation (usually images captured by cameras), the perception module encodes it into latent state s. For planning, waypoint query Qw is employed to extract the waypoint features sw = {sw,1, sw,2, ..., sw,n} through cross-attention, and the planning head then decodes the waypoint features into an action sequence τa = {a1, a2, ..., an}. Using the provided expert action demonstrations τ as labels, imitation learning applies an L1 loss Limi to supervise output. sw = {sw,1, sw,2, ..., sw,n} = CrossAttn(q = Qw, = s, = s), (1) τa = {a1, a2, ..., an} = PlanningHead(sw), (2) Limi = τa τ (3) To further endow the model with the predictive ability, world model is used to predict future states. Unlike pixel-level generative world models, we operate in latent space to reduce task complexity. Specifically, given the current state and action τa, the world model predicts future state ˆs: . ˆs = LatentWorldModel(s, τa). (4) Meanwhile, the perception module encodes next observation into ground-truth state s. The latent world model is trained in self-supervise manner using mean square error (MSE). The overall imitation learning loss LIL combines Lwm and Limi, where α is hyperparameter: Lwm = MSELoss(s, ˆs), LIL = Limi + α Lwm. 3 (5) (6)"
        },
        {
            "title": "3.2 BACKWARD PLANNING",
            "content": "In practice, the planning head predicts τa using simple linear layers. Such design overlooks dependencies among each step in τa. natural extension adopts self-attention layer with causal mask to introduce temporal causality. The policy for ai is formulated as πi(aisw,1, ..., sw,i) = π(aisw,ji). While this forward-causal design appears intuitive, human driving behavior suggests an alternative perspective. Drivers typically decide where to go before committing to low-level actions. Moreover, in real-world deployment, only the first action is executed before replanning, making earlier actions matter more. Figure 2: The comparison of forward planning (using causal mask) and backward attention (use inverse causal mask) Motivated by these insights, as shown in Fig. 2, we explore counterintuitive alternative backward planning (inverse causality)where the i-th action is conditioned on the current and future waypoint features: πi(aisw,i, ..., sw,n) = πi(aisw,ji) (7) This formulation incorporates early actions with richer contextual information while leaving later actions less constrained, aligning better with how humans plan. Plus, inverse causality changes only the conditioning order, without affecting the smoothness of the final trajectory. outperforms both forward-causal and non-causal baselines. Prior evidence (Liu et al., 2025) showed that this design works well on tasks like robot manipulation, further supporting this goal-to-action reasoning paradigm. Our experiments in Tab. 3 confirm that it consistently outperforms both forward-causal and non-causal baselines. 3.3 REINFORCEMENT LEARNING RL needs rewards to evaluate the quality of explored trajectories. Given predicted action sequence τa, the corresponding position sequence is obtained via cumulative summation: τpos = {a1, a1 + a2, ..., (cid:80) ai}. On nuScenes, we use two components to define the reward: imitation reward rimi and collision reward rcol: imi = eaiae r(i) 2 , r(i) col = 1 CollisionDetection(τpos, env). (8) Here, the env denotes static map information and dynamic agents moving trajectories. The final reward for step is defined as: (9) On Navsim (Dauner et al., 2024) benchmark, the reward is computed similarly and utilize the PDMS score to evaluate the quality of trajectories. ri = r(i) col r(i) imi. The deterministic action sequence produced by our model cannot be directly optimize with RL, which typically requires probabilistic policies. To address this, we need to model the uncertainty of the action sequence. Similar with the planning head output the mean value µi for each action, we use stochastic head to output uncertainty for each action: τσ = {σ1, σ2, ..., σn} = StochasticHead(sw), (10) where σi is the standard deviation for ai. Here we simply use Gaussian distribution to model each action and use the diagonal matrix to serve as the covariance matrix. Then the policy for action ai 4 can formulated with: πi(aisw,ji) = (µi, σ I). (11) , τ Given the offline imitation dataset {(s, τ , s), ...}, using Gaussian Log Likelihood loss can easily fitting the behavior of experts: Lbc = (cid:80)n sw,ji). But we wish the RL Actor can explore and learn from both good examples but also bad examples. Inspired by GRPO (Shao et al., 2024), we introduce exploration by sampling trajectories from the policy and use the rule-based reward function to calculate its corresponding reward sequences. Each action sequence in the group, together with its corresponding reward sequence, can be formally expressed as: = {r(g) 1 sw,j1), ..., a(g) sw,jn)}, τ (g) 1 π1(a(g) πn(a(g) i=1 logπi(ae = {a(g) τ (g) 2 , ..., r(g) 1 , r(g) }. (12) For each trajectory, we compute its total reward and normalize within the group using Z-score normalization to obtain the advantage and the naive policy gradient loss with group sampling (naive PGGS) is computed by: Lactor ="
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) g=1 (cid:88) A(g)( i=1 log πi(a(g) sw,ji)), A(g) = Στ (g) mean(Στ (1) std(Στ (1) , ..., Στ (G) ) , ..., Στ (G) ) . (13) To extend the advantage estimation to long-term rewards, we train critic model to output the value of both current state and the next state s. Since the offline dataset does not provide next states for sampled trajectories, we leverage the latent world model to generate rollouts: The long-term advantage A(g) long is computed by: ˆs(g) = LatentWorldModel(s, τ (g) ). A(g) long = ( (cid:88) r(g) + γ ( ˆs(g))) (s), (14) (15) in which γ is discount factor. We further apply Z-score normalization to A(g) long within each group and denote the normalized advantage as the critic advantage A(g) cri. During training, the actor and critic are then jointly optimized, and this is the method of actor + dreaming critic with group sampling (ADCGS): Lact = 1 (cid:88) g=1 A(g) cri ( (cid:88) i=1 log πi(a(g) sw,ji)), Lcri = 1 G (cid:88) g=1 [V (s)( (cid:88) +γ ( ˆs(g)))]2. τ (g) (16) Since pure RL with our reward is hard to converge (see results in Tab. 4), we add small imitation term to stabilize training, with small coefficient β = 0.005: LRL = Lact + Lcri + β Lbc. (17) In practice, since actions in sampled sequence are drawn independently from different policies, the resulting position trajectory τpos may lack smoothness. To address this, we adopt step-aware mechanism: within each sampled sequence, only one action is stochastic, while the remaining actions are set to the mode (value with highest probability) of their respective policies, ensuring smoother τpos. The detailed algorithm and visualizations are provided in Appendix A.1. To further stabilize critic learning, we employ the two-critic trick, where reference critic maintains an exponential moving average (EMA) of the learning critic. 3.4 DUAL-POLICY LEARNING FRAMEWORK During training, the models planning module is decoupled into IL actor and RL actor, optimized by LIL and LRL respectively. To encourage the two actor interact with each other, two actor can compete and share information with each other (see Fig. 3). To balance the contributions of the imitation learning (IL) actor and the reinforcement learning (RL) actor, we periodically compare their performance every iterations. The comparison is based on the cumulative reward scores achieved by each actor. 5 Figure 3: The flow chart of our rule-based competitive learning mechanism Depending on the score difference, we apply different merging strategies: 1) Comparable: If the scores are close, we keep both unchanged. 2) Moderate Superiority: If one actor slightly outperform the other, we perform soft weight merging to gradually transfer knowledge from the winner to loser: loser.weight loser.weight + (1 p) winner.weight. 3) Significant Superiority: If one actors score is substantially higher, we apply hard weight replacement to copy knowledge from the winner to loser: loser.weight winner.weight. This adaptive mechanism enables stable cooperation between IL and RL actors, preventing premature dominance of either side while ensuring faster convergence once one actor demonstrates clear superiority."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "4.1 BENCHMARKS nuScenes (Caesar et al., 2020) is large-scale autonomous driving benchmark featuring 1,000 20second urban driving scenes with 1.4M annotated 3D boxes across 23 object classes. It provides 360 imagery from six cameras and 2Hz keyframe annotations. Following prior works (Hu et al., 2023; Jiang et al., 2023), we evaluate planning using L2 placement error and Collision Rate. Navsim (Dauner et al., 2024) is compact, filtered version of OpenScenes (Contributors, 2023), itself derived from nuPlan (Karnchanachari et al., 2024). It emphasizes challenging scenarios and contains 120 hours of driving at 2Hz. It includes navtrain and navtest splits for training and testing. To better reflect closed-loop safety and behavior, Navsim evaluates agents with six metrics: No atfault Collisions (NC), Drivable Area Compliance (DAC), Time to Collision (TTC), Ego Progress (EP), Comfort (C), and Driving Direction Compliance (DDC). These are combined into weighted Driving Score (PDMS). 4.2 IMPLEMENTATION DETAILS For experiments on nuScenes, our method builds upon the LAW framework (Li et al., 2025b) and SSR framework (Li & Cui, 2025). We train our models using 8 NVIDIA A800-SXM4-80GB GPUs and perform evaluation on the same A800 GPU. The training is conducted with batch size of 1 using the AdamW optimizer, with learning rate set to 5 105, we use cosine annealing learning rate with linear warm up. All other training settings follow the original LAW and SSR configuration. The training process takes approximately 20 hours to complete. For experiments on Navsim, we adopt the Transfuser (Prakash et al., 2021) model as backbone. Transfuser employs Transformer-based architecture to fuse front-view camera image and LIDAR data across multiple stages. We train our model on navtrain split and evaluate it on test split, using the same hardware configuration as in nuScenes experiments. The training is performed with batch size of 16 using the AdamW optimizer, with learning rate set to 1 104. 4.3 MAIN RESULTS The results on nuScenes are presented in Tab. 1. For nuScenes, We follow the evaluation protocol of (Jiang et al., 2023), which reports average L2 distance and collision rate over 1s, 2s, and 3s prediction horizons. We tried our method on both SSR (Li & Cui, 2025) and LAW (Li et al., 2025b), 6 and we found that SSR is unstable, even using the same random seed. So in ablation studies, we only use LAW. Table 1: Comparison of SOTA methods on the nuScenes dataset. Gray rows indicate methods that do not use additional supervision. *Models are trained and evaluated on 8 A800 GPUs. We found that SSR is unstable on our machine, here we only use random seed 0. The overall best results are highlighted in bold, while the best results among methods without additional supervision are underlined. Method Auxiliary Task L2 (m) Collision Rate (%) ST-P3 UniAD VAD-Tiny VAD-Base BEV-Planner PARA-Drive GenAD SparseDrive UAD World4Drive SSR* SSR+CoIRL-AD* LAW* LAW+CoIRL-AD (PGGS)* LAW+CoIRL-AD (ADCGS)* Det&Map&Depth Det&Track&Map&Motion&Occ Det&Map&Motion Det&Map&Motion None Det&Track&Map&Motion&Occ Det&Map&Motion Det&Track&Map&Motion Det Segmentation None None None None None 1s 1.33 0.44 0.46 0.41 0.28 0.25 0.28 0.29 0.28 0.23 0.18 0.21 0.32 0.31 0.29 2s 2.11 0.67 0.76 0.70 0.42 0.46 0.49 0.58 0.41 0.47 0.35 0.40 0.62 0.61 0.59 3s Avg. 1s 2.90 0.96 1.12 1.05 0.68 0.74 0.78 0.96 0.65 0.81 0.62 0.69 1.03 1.01 1.00 2.11 0.69 0.78 0.72 0.46 0.48 0.52 0.61 0.45 0.50 0.38 0.43 0.66 0.65 0.63 0.23 0.04 0.21 0.07 0.04 0.14 0.08 0.01 0.01 0.02 0.48 0.09 0.08 0 0.06 2s 0.62 0.08 0.35 0.17 0.37 0.23 0.14 0.05 0.03 0.12 0.45 0.11 0.13 0.10 0. 3s Avg. 1.27 0.23 0.58 0.41 1.07 0.39 0.34 0.18 0.14 0.33 0.51 0.23 0.46 0.51 0.37 0.71 0.12 0.38 0.22 0.49 0.25 0.19 0.08 0.06 0.16 0.48 0.15 0.22 0.20 0.18 On nuScenes, compared to the recent world model-based autonomous driving models like SSR (Li & Cui, 2025) and LAW (Li et al., 2025b), after using our method, both model achieve lower collision rate. And LAW with our method also achieve lower L2 distance. Although UAD (Guo et al., 2024) achieves the lowest collision rate overall (0.06%), it relies heavily on extensive supervision signals such as detection and tracking. In contrast, SSR with our method achieves the best collision rate (0.15%) among methods that do not use any auxiliary supervision beyond expert trajectories. In addition, our method uses the same inference architecture as the baseline, introducing no extra latency. Further details are provided in Appendix A.2.1. Long-tail scenario performance and generalization ability on nuScenes We evaluate the models generalization by training on nuScenes-Singapore and testing on nuScenes-Boston (Fig. 4a). To assess robustness on long-tail cases, we construct two subsets from the evaluation set: one dominated by large L2 errors and one by high collision rates, identified using the baseline model. As shown in Fig. 4b. Our model achieves much better results than LAW across all horizons, demonstrating strong cross-city generalization and superior performance under challenging, long-tail scenarios. Details on subset construction and full results are provided in Appendix A.2.2. (a) Cross-city Generalization Ability (b) Performance on longtail scenarios Figure 4: Comparision of Generalization and Performance on Long-tail Dataset (L2 and Collision). The results on Navsim are shown in Tab. 2. For Navsim, we adopt the close-loop metrics provided in Navsim. Specifically, we use the test split rather than navtest split for evaluation, as the former con7 tains much more scenarios (5044) than the latter (885), making it more suitable for comprehensively assessing the models overall driving performance. Table 2: Comparison of SOTA methods on Navsim test set. *reproduced by us. test on navtest."
        },
        {
            "title": "Human",
            "content": "NC DAC TTC Comf. EP PDMS 100.0 100.0 100.0 Ego Status MLP VADv2 (Chen et al., 2024) UniAD (Hu et al., 2023) PARA-Drive (Weng et al., 2024) Transfuser (Prakash et al., 2021) LAW (Li et al., 2025b) Hydra-MDP (Li et al., 2024b) WoTE* (Li et al., 2025c) WoTE+CoIRL-AD 93.0 97.2 97.8 97.9 97.7 96.5 98.3 98.6 98.6 77.3 89.1 91.9 92.4 92.8 95.4 96.0 96.4 96.8 83.6 91.6 92.9 93.0 92.8 88.7 94.6 95. 95.5 99.9 100.0 100.0 100.0 99.8 100.0 99.9 100.0 100.0 100.0 87.5 62.8 76.0 78.8 79.3 79.2 81.7 78.7 81. 81.0 94.8 65.6 80.9 83.4 84.0 84.0 84.6 86.5 87.9 88.2 On navsim, our model obtains PDMS of 88.2, outperforming recent state-of-the-art methods, showing notable improvements across multiple sub-metrics, including NC (+0.4), DAC (+0.4) and TTC (+0.8). Compared to WoTE (Li et al., 2025c), which leverages world model to evaluate candidate trajectories during testing, our approach achieves higher overall score. 4.4 ABLATION STUDY Causality To verify the effect of inverse causality, we conduct three experiments on LAW + CoIRL-AD naive PGGS model, and change the mask we use in the self attention layer to sw in three different ways: 1) no causal mask; 2) causal mask1; 3) inverse causal mask2. We set β in LRL equals to 0. The results is shown in Tab. 3. Table 3: The Effect of Causality to the Performance Method LAW no mask causal mask causal mask (inv) L2 (m) Collision Rate (%) 1s 0.32 0.30 0.35 0.31 2s 0.63 0.60 0.67 0.61 3s Avg. 1s 1.03 1.04 1.10 1.01 0.66 0.64 0.71 0.65 0.09 0.09 0.08 0.04 2s 0.12 0.15 0.16 0. 3s Avg. 0.46 0.43 0.57 0.48 0.22 0.22 0.27 0.20 From the results, the naive causal mask increases both L2 error and collision rate. In contrast, removing the mask or using the inverse causal mask outperforms the baseline. The no-mask setting reduces L2 error, while the inverse causal mask improves both L2 and collision rate, highlighting the effectiveness of backward planning (inverse causality). Integration of IL and RL We compare several integration strategies: (i) loss merging, jointly optimizing with LIL + LRL; (ii) ILRL interval, alternating between LIL and LRL; (iii) two-stage, pre-training with LIL then fine-tuning with LRL; and (iv) decoupled actors, where IL and RL actors are optimized separately, optionally with competition (comp). Results are shown in Tab. 4. From Tab. 4, only the decouple, w/ comp variant improves both L2 and collision rates over the baseline. This is notable since two-stage ILRL transfer is effective in other domains (e.g., Deepseeks R1 (Guo et al., 2025)). We attribute the limited gains to: (1) overly simple rewards (imitation and collision only), (2) use of basic actorcritic method instead of more stable algorithms like PPO, and (3) non-reactive simulation, where both states and rewards are generated by the world model, introducing bias. These factors explain the poor pure RL results and the degradation in most 1causal mask: torch.triu(torch.ones(n,n), diagonal=1).bool() 2inverse causal mask: torch.tril(torch.ones(n,n), diagonal=-1).bool() 8 Table 4: The Performance of Different Ways to Integrate IL and RL. Description pure IL pure RL loss merging IL-RL interval two-stage decouple, w/o comp decouple, w/ comp L2 (m) Collision Rate (%) 1s 0.32 3.92 0.38 0.31 2.43 0.32 0.31 2s 0.63 6.55 0.73 0.63 4.21 0.64 0. 3s Avg. 1s 1.03 9.18 1.17 1.07 6.03 1.07 1.01 0.66 6.55 0.76 0.68 4.22 0.68 0.65 0.09 2.75 0.03 0.12 2.29 0.09 0. 2s 0.12 4.87 0.12 0.17 4.13 0.13 0.08 3s Avg. 0.46 7.72 0.54 0.54 6.53 0.53 0.48 0.22 4.93 0.23 0.28 4.32 0.25 0. RL-augmented variants. Nevertheless, the competitive decoupled design demonstrates that effective ILRL interaction can still yield measurable improvements."
        },
        {
            "title": "5 ANALYSIS",
            "content": "The last two lines of result in Tab. 4 show that the competitive learning mechanism can help the IL Actor and RL Actor interact and finally learn better model, but how? By tracking metrics such as accumulated wins and score differences (IL score RL score) over iterations, we observe the fol- (1) In the early stage (<20k itlowing: erations), the IL actor achieves more wins and higher scores, indicating that IL initially leads the learning process. (2) Afterward, the RL actor acquires basic driving knowledge, and its exploration via group sampling becomes more effective than simply imitating expert trajectories. Consequently, RL achieves higher scores and dominates in later training. This progression resembles the two-stage paradigm (IL pretraining followed by RL fine-tuning), but with key difference: IL and RL are trained jointly. Even though IL loses more frequently in later stages, its gradients continue to benefit shared components such as the perception module. Figure 5: Accumulated wins (top) and score difference (bottom) across training iterations."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented competitive dual-policy framework that integrates IL and RL for end-to-end autonomous driving. Motivated by ILs limitations in generalization and long-tail performance, we exploit RLs exploration capability within an offline setting. By combining group sampling with non-reactive simulation and augmenting it with imagination via latent world model, we train an RL actor capable of capturing long-term advantages beyond immediate rewards. competition-based mechanism further enables effective interaction between IL and RL actors to promoting knowledge sharing. Experiments on nuScenes and Navsim demonstrate that our approach significantly reduces collisions, improves generalization, and enhances long-tail performance. We believe this framework provides promising direction for combining imitation and reinforcement learning in embodied AI, and we hope it inspires future research in autonomous driving and beyond."
        },
        {
            "title": "REFERENCES",
            "content": "Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1162111631, 2020. Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning, 2024. URL https://arxiv.org/abs/2402.13243. OpenScene Contributors. Openscene: The largest up-to-date 3d occupancy prediction benchmark in autonomous driving. https://github.com/OpenDriveLab/OpenScene, 2023. Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking, 2024. URL https://arxiv.org/abs/2406.15349. Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, Ying Zhang, Wenyu Liu, Qian Zhang, and Xinggang Wang. Rad: Training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning, 2025. URL https://arxiv.org/abs/2502.13144. Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta, Yihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang Li. Vista: generalizable driving world model with high fidelity and versatile controllability. In Advances in Neural Information Processing Systems (NeurIPS), 2024. Anant Garg and Madhava Krishna. Imagine-2-drive: Leveraging high-fidelity world models via multi-modal diffusion policies, 2025. URL https://arxiv.org/abs/2411.10171. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081):633638, 2025. Mingzhe Guo, Zhipeng Zhang, Yuan He, Ke Wang, and Liping Jing. End-to-end autonomous driving without costly modularization and 3d manual annotation. arXiv preprint arXiv:2406.17680, 2024. Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu Qiao, and Hongyang Li. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023. Zilin Huang, Zihao Sheng, Yansong Qu, Junwei You, and Sikai Chen. Vlm-rl: unified vision language models and reinforcement learning framework for safe autonomous driving, 2024. URL https://arxiv.org/abs/2412.15544. Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 83408350, 2023. Napat Karnchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, and Holger Caesar. Towards learning-based planning: The nuplan benchmark for real-world autonomous driving. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 629636, 2024. doi: 10.1109/ICRA57147.2024.10610077. Derun Li, Jianwei Ren, Yue Wang, Xin Wen, Pengxiang Li, Leimeng Xu, Kun Zhan, Zhongpu Xia, Peng Jia, Xianpeng Lang, et al. Finetuning generative trajectory model with reinforcement learning from human feedback. arXiv preprint arXiv:2503.10434, 2025a. Peidong Li and Dixiao Cui. Navigation-guided sparse scene representation for end-to-end autonomous driving, 2025. URL https://arxiv.org/abs/2409.18341. 10 Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan. Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carla-v2). In European Conference on Computer Vision, pp. 142158. Springer, 2024a. Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen, Zhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end autonomous driving with latent world model, 2025b. URL https:// arxiv.org/abs/2406.08481. Yingyan Li, Yuqi Wang, Yang Liu, Jiawei He, Lue Fan, and Zhaoxiang Zhang. End-to-end driving with online trajectory evaluation via bev world model, 2025c. URL https://arxiv.org/ abs/2504.01941. Yongkang Li, Kaixin Xiong, Xiangyu Guo, Fang Li, Sixu Yan, Gangwei Xu, Lijun Zhou, Long Chen, Haiyang Sun, Bing Wang, et al. Recogdrive: reinforced cognitive framework for end-toend autonomous driving. arXiv preprint arXiv:2506.08052, 2025d. Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, Jan Kautz, Zuxuan Wu, Yu-Gang Jiang, and Jose M. Alvarez. Hydra-mdp: End-to-end multimodal planning with multi-target hydra-distillation, 2024b. URL https://arxiv.org/ abs/2406.06978. Dongxiu Liu, Haoyi Niu, Zhihao Wang, Jinliang Zheng, Yinan Zheng, zhonghong Ou, Jianming Hu, Jianxiong Li, and Xianyuan Zhan. Efficient robotic policy learning via latent space backward planning. In International Conference on Machine Learning, 2025. Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-toend autonomous driving. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Hang Wang, Xin Ye, Feng Tao, Chenbin Pan, Abhirup Mallik, Burhaneddin Yaman, Liu Ren, and In Junshan Zhang. AdaWM: Adaptive world model based planning for autonomous driving. The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=NEu8wgPctU. Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1474914759, June 2024. Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang, Chong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu Zhang. Panacea: Panoramic and controllable video generation for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 69026912, June 2024. Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for real-time autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1544915458, 2024. Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and Luc Van Gool. End-to-end urban driving by imitating reinforcement learning coach. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021. Yupeng Zheng, Pengxuan Yang, Zebin Xing, Qichao Zhang, Yuhang Zheng, Yinfeng Gao, Pengfei Li, Teng Zhang, Zhongpu Xia, Peng Jia, et al. World4drive: End-to-end autonomous driving via intention-aware physical latent world model. arXiv preprint arXiv:2507.00603, 2025. Zewei Zhou, Tianhui Cai, Yun Zhao, Seth Z.and Zhang, Zhiyu Huang, Bolei Zhou, and Jiaqi Ma. Autovla: vision-language-action model for end-to-end autonomous driving with adaptive reasoning and reinforcement fine-tuning. arXiv preprint arXiv:2506.13757, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 STEP-AWARE REINFORCEMENT LEARNING When the RL Actor explore, it samples action sequence from policies (see Eq. 12). The problem is, as we model each action in the action sequence separately, when sample an action sequence, the actions are actually sampled independently from different Gaussian distribution. When we use the sampled action sequence τ (g) to calculate the position sequence τ (g) pos, the trajectory is unstable and not smooth. That means in exploration, the RL Actors driving trajectories is possibly do not satisfy some kinematic characteristics. Below we visualize the comparison of naive group sampling and our step aware method: Fig. 6 (straight), Fig. 7 (left turn), Fig. 8 (right turn). Our goal is to let the RL Actor output reasonable driving trajectories, and simply use group sampling here is inefficient (there is no need to explore some unreasonable trajectories), so in implementation we use step aware mechanism. More specifically, via group sampling, we actually get samples for each action in the action sequence. The idea is, we decoupled the exploration in to each step, i.e. we do not sample actions from different policies, we ensure that only one action will be sampled in each exploration, and for the resting 1 actions in the same exploration, we simply use the mode action (the expectation E[πi] in gaussian actually). The process is visualized in Fig below. We formulate our methods in Alo 1. Algorithm 1 Step Aware RL with Group Sampling Input: {π1, π2, ..., πn}, s, sw, Vψ (Critic Model), (i, g, Lactor, Lcritic 0) 1: repeat 2: 3: 4: 5: + 1 repeat sw,ji), ..., E[πn]} πi(a(g) + 1 {E[π1], ..., a(g) τ (g) Calculating reward τ (g) based on Eq.8, 9 Predict future state ˆs(g) based on Eq. 14 Computing long-term advantage A(g) long based on Eq. until = Computing critic advantage for step i, Acritic = Z-Score-Norm({A(1) Lactor Lactor 1 critic [j]sw,kj )(cid:17) (cid:80)G (cid:16)(cid:80)n j=1 logπ(τ (cid:16)(cid:80) τ (g) + γ Vψ( ˆs(g)) (cid:17)(cid:105) g=1 A(g) (cid:104) (cid:80)G 6: 7: 8: 9: 10: 11: Lcritic Lcritic + 1 12: 13: until = 14: Lactor 1 15: Lcritic 1 Output: Loss of actor Lactor and dreaming critic Lcritic Lactor Lcritic Vψ(s) g=1 long, ..., A(G) long}) A.2 MORE EXPERIMENT RESULTS A.2.1 INFERENCE TIME During inference time, our method dont introduce extra inference time cost. Specifically, the knowledge of RL actor had shard with IL actor and they can forward at the inference time. We tested the inference metrics of LAW and our method on single A100, and the results is shown in Tab. 5 A.2.2 GENERALIZATION AND PERFORMANCE ON LONG-TAIL SCENARIOS Details on Long-tail Subset Construction We define long-tail scenarios according to two criteria: high L2 prediction errors and high collision rates, and accordingly construct two specialized long-tail datasets. The L2 Long-Tail Dataset is built by first selecting scenes with 12 (a) Naive sampling (b) Step-aware group sampling Figure 6: Comparison between naive group sampling method and our step-aware group sampling method in straight driving situation (in training process) Table 5: Inference Time Method fps latency (ms) LAW LAW+CoIRL-AD 26.99 27.1 37.1 37. fut valid flag=TRUE, and then filtering for scenes with L2 distance greater than 0.3 at 1s, greater than 0.5 at 2s, and simultaneously greater than 1.0 at 3s. This results in total of 984 scenes for testing. The Collision Rate Long-Tail Dataset is obtained by selecting scenes with fut valid flag=TRUE and excluding all scenes with zero collision rate at the 3s horizon, yielding 91 test scenes. Detailed Results The detailed results of generalization performance of LAW and LAW+CoIRLAD are shown in Tab. 6. The detailed results of performance on two long-tail subset are shown in Tab. 7 and Tab. 8. Table 6: Generalization Performance Method L2 (m) Collision Rate (%) 1s 2s 3s LAW 0.445 LAW+CoIRL-AD 0.326 0.885 0.653 1.463 1. Avg. 0.931 0.702 1s 2s 3s 0.133 0. 0.428 0.152 1.497 0.463 Avg. 0.686 0.218 13 (a) Naive sampling (b) Step-aware group sampling Figure 7: Comparison between naive group sampling method and our step-aware group sampling method in left turn driving situation (in training process) Table 7: Long-tail dataset (L2) comparision results Method L2 (m) Collision Rate (%) 1s 2s 3s LAW 0.375 LAW+CoIRL-AD 0.329 0.739 0.673 1.212 1. Avg. 0.775 0.712 1s 0 0 2s 3s 0.079 0.053 0.459 0.335 Avg. 0.179 0.129 A.3 MORE QUALITATIVE RESULTS In this section, we provide additional qualitative results to further demonstrate the effectiveness of our approach. A.3.1 GOOD CASES From the visualization results, we observe that after incorporating reinforcement learning, our model successfully avoids many collisions that the baseline model fails to handle. This reveals that integrating IL and RL is an effective design choice. Table 8: Long-tail dataset (Collision) comparison results Method L2 (m) Collision Rate (%) 1s 2s 3s LAW 0.336 LAW+CoIRL-AD 0.283 0.699 0.577 1.184 0. Avg. 0.740 0.616 1s 0 0 2s 3s 0.852 0 4.356 0.758 Avg. 1.736 0.253 14 (a) Naive sampling (b) Step-aware group sampling Figure 8: Comparison between naive group sampling method and our step-aware group sampling method in right turn driving situation (in training process) Figure 9: Good Case: The baseline model collides with car driving in the neighboring lane at high speed, while our model successfully avoids the collision. A.3.2 BAD CASES However, there are also some scenarios where both the baseline model and our model perform unsatisfactorily. A.4 USAGE OF LLMS During our research, we used LLMs in the following ways: Text polishing. We employed LLMs to refine the writing style of our drafts and to shorten sections when the main text exceeded the page limit (9 pages). We did not use LLMs to generate new content; all polished text was manually reviewed before inclusion in the submission. Idea exploration. We used LLMs as tool for brainstorming and literature search assistance. By discussing our ideas with LLMs, we were directed toward relevant research areas and key related works, which we then examined ourselves. 15 Figure 10: Good Case: When pedestrians are crossing ahead, our model stops and waits for them to pass, whereas the baseline model continues to drive forward. Figure 11: Good Case: In queuing scenario while waiting for the green light, our model proceeds after the car ahead turns off its indicator and starts moving, whereas the baseline model remains stopped, which may lead to rear-end collision. Figure 12: Good Case: When the high-level command is incorrect, our model adapts its planned trajectory based on the driving scenario, whereas the baseline model fails to adjust. Figure 13: Good Case: When exiting the parking lot, our model waits for the toll barrier to rise, whereas the baseline model proceeds forward and collides with it. 16 Figure 14: Good Case: When the leading vehicle has its indicator on, our model slows down to avoid rear-end collision, whereas the baseline model maintains speed and causes collision. Figure 15: Bad Case: When turning right at an intersection, both the baseline model and our model plan trajectories that collide with vehicle waiting for the green light. Figure 16: Bad Case: When pedestrians are crossing, our model continues to move forward at low speed instead of stopping completely. Figure 17: Bad Case: Both the baseline model and our model fail to plan reasonable trajectories in lane-changing scenarios. 17 Code assistance. LLMs were used to help debug programs by analyzing error logs, to review our code for potential issues, and to generate auxiliary visualization scripts. Except for visualization, we did not rely on LLMs to produce experimental code. For visualization, we first drafted data-loading code ourselves, then refined the visualization with LLM-generated snippets, carefully verifying correctness before use."
        }
    ],
    "affiliations": [
        "Beijing Jiaotong University",
        "The Hong Kong Polytechnic University",
        "Tsinghua University",
        "University of Washington"
    ]
}