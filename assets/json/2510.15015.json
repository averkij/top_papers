{
    "paper_title": "DeLeaker: Dynamic Inference-Time Reweighting For Semantic Leakage Mitigation in Text-to-Image Models",
    "authors": [
        "Mor Ventura",
        "Michael Toker",
        "Or Patashnik",
        "Yonatan Belinkov",
        "Roi Reichart"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable to semantic leakage, the unintended transfer of semantically related features between distinct entities. Existing mitigation strategies are often optimization-based or dependent on external inputs. We introduce DeLeaker, a lightweight, optimization-free inference-time approach that mitigates leakage by directly intervening on the model's attention maps. Throughout the diffusion process, DeLeaker dynamically reweights attention maps to suppress excessive cross-entity interactions while strengthening the identity of each entity. To support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages), the first dataset dedicated to semantic leakage, comprising 1,130 human-verified samples spanning diverse scenarios, together with a novel automatic evaluation framework. Experiments demonstrate that DeLeaker consistently outperforms all baselines, even when they are provided with external information, achieving effective leakage mitigation without compromising fidelity or quality. These results underscore the value of attention control and pave the way for more semantically precise T2I models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 5 1 0 5 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Under review",
            "content": "DELEAKER: DYNAMIC INFERENCE-TIME REWEIGHTING FOR SEMANTIC LEAKAGE MITIGATION IN TEXTTO-IMAGE MODELS Mor Ventura1 Michael Toker1 Or Patashnik2 Yonatan Belinkov1 Roi Reichart1 1Technion 2Tel-Aviv University {mor.ventura, tok}@campus.technion.ac.il, orpatashnik@gmail.com {belinkov, roiri}@technion.ac.il"
        },
        {
            "title": "ABSTRACT",
            "content": "Text-to-Image (T2I) models have advanced rapidly, yet they remain vulnerable to semantic leakage, the unintended transfer of semantically related features between distinct entities. Existing mitigation strategies are often optimization-based or dependent on external inputs. We introduce DeLeaker, lightweight, optimizationfree inference-time approach that mitigates leakage by directly intervening on the models attention maps. Throughout the diffusion process, DeLeaker dynamically reweights attention maps to suppress excessive cross-entity interactions while strengthening the identity of each entity. To support systematic evaluation, we introduce SLIM (Semantic Leakage in IMages), the first dataset dedicated to semantic leakage, comprising 1,130 human-verified samples spanning diverse scenarios, together with novel automatic evaluation framework. Experiments demonstrate that DeLeaker consistently outperforms all baselines, even when they are provided with external information, achieving effective leakage mitigation without compromising fidelity or quality. These results underscore the value of attention control and pave the way for more semantically precise T2I models."
        },
        {
            "title": "INTRODUCTION",
            "content": "Text-to-Image (T2I) models have shown continuous improvements in image generation capabilities (Ramesh et al., 2021; 2022; Saharia et al., 2022; Black-Forest-Labs, 2024). These advances are largely driven by diffusion-based architectures, which produce high-quality images through iterative denoising (Dhariwal & Nichol, 2021; Ho & Salimans). Recent state-of-the-art models, such as Diffusion Transformers (DiTs) (Peebles & Xie, 2023), further this progress by adopting transformerbased architectures with uniform global attention, resulting in stronger imagetext alignment and improved image quality. Nevertheless, these models remain vulnerable to errors in semantic fidelity, with semantic leakage emerging as particularly persistent challenge. Semantic leakage refers to the unintended transfer of semantically related features between entities in the generated outputs, observed in both image (Rassin et al., 2022; Dahary et al., 2025b) and text generation models (Gonen et al., 2025). An example of this is seen in Fig. 1, where cows traits leak into the horses ears and mouth. Although this phenomenon is form of broader problem of image-text misalignment in the context of image generation, it remains highly underexplored. Prior work employed layout-based control to mitigate semantic leakage by assigning entities (e.g., cow and horse) to fixed regions (Dahary et al., 2025a;b). While effective in simple scenes, these methods fail in settings that involve interactions between entities (Fig. 1, examples 23), where rigid separation is often less natural. By relying on external inputs and bounding-boxes, these methods disregard the models prior knowledge, overlooking the potential to leverage its internal semantic representations. Moreover, they resort to costly inference-time optimization strategies, commonly used in efforts to refine semantic alignment in T2I models (Chefer et al., 2023; Rassin et al., 2024). Equal contribution. 1Code and data will be made publicly available upon paper acceptance."
        },
        {
            "title": "Under review",
            "content": "Figure 1: DeLeaker Qualitative Examples. Top: DeLeaker ; Bottom: original outputs. Red arrows mark features affected by semantic leakage. Examples cover five subsets of the SLIM dataset (3). In this paper, we introduce DeLeaker, dynamic and lightweight inference-time method designed to mitigate semantic leakage in T2I models. Unlike prior approaches that require costly optimization or external guidance, DeLeaker operates by applying synergistic interventions directly to the models attention mechanism during inference (2). First, it automatically extracts entity-specific masks from early image-text attention to localize each entity. It then uses these masks to suppress cross-entity leakage by dynamically reducing excessively high attention scores between the different entity regions in both image-text and image-image interactions. Concurrently, it strengthens each entitys selfidentity by increasing the attention between its corresponding text and image tokens. This targeted reweighting of attention allows DeLeaker to mitigate leakage while preserving scene structure, and the models priors. Furthermore, the method remains non-intrusive when no leakage is present. While developing methods to mitigate semantic leakage is crucial, rigorously evaluating them remains major hurdle due to the absence of dedicated benchmarks and the limitations of VLMbased automatic evaluation (Dahary et al., 2025a). To address this gap, we introduce comprehensive evaluation framework, centered around new dedicated dataset (3). Our dataset, the Semantic Leakage in IMages (SLIM) dataset, comprises 1,130 (prompt, seed, image) samples capturing diverse leakage scenarios, including prompts describing visually similar entities, spatial interactions, and multi-entity compositions. SLIM is constructed from large pool of images generated by the FLUX.1-dev model (Black-Forest-Labs, 2024), using prompts automatically produced by GPT-4o. The images are rigorously filtered through an extensive human filtering process. Next, we develop an evaluation framework (4) to assess semantic leakage mitigation. We adopt comparative evaluation setup in which images from before and after the mitigation process are compared. Importantly, our framework breaks down the challenging comparative evaluation into series of discrete logical steps. The process begins with the identification of differences between entities to detect semantic leakage, followed by the ranking of the mitigations success. Additionally, we include evaluation of the image-text semantic alignment and the preservation of the original image quality and perceptual similarity. Our automatic evaluation pipeline is validated by an extensive human study (980 responses). In experiments with FLUX (6), DeLeaker significantly outperforms all evaluated baselines in both automatic and human evaluations. This includes prompt-based baselines, and layout-based baselines (5) that require additional information, typically from external LLMs. To confirm its generalizability, we applied DeLeaker to another model, SANA (Xie et al., 2024), and found it to be similarly effective at mitigating semantic leakage. To understand the source of DeLeakers advantage, our ablation study (7) reveals that DeLeakers strength derives from its cross-modal attention interventions, particularly the image-text strengthening that preserves self-identity."
        },
        {
            "title": "Under review",
            "content": "To summarize, our contributions include: (1) DeLeaker, dynamic, lightweight inference-time method for mitigating semantic leakage in T2I models while preserving image quality and perceptual similarity, (2) the first dedicated dataset explicitly designed to evaluate semantic leakage, and (3) an automated evaluation pipeline for large-scale assessment supported by human study. We hope this work will inspire further research toward more controlled and reliable generative models. Figure 2: DeLeaker Scheme. Our method applies attention-based interventions during the diffusion process: (A) automatically extracting entity-specific masks from early image-text attention maps; (B) suppressing cross-entity leakage by suppressing attention across entities in both image-text and image-image interactions; and (C) strengthening self-identity by increasing attention from each entitys text tokens to its own image tokens. The attention map legend (left) shows how entities interact, where colors denote different interaction regions. The final output (right) presents the image output with DeLeaker compared to the original image, when DeLeaker is not applied."
        },
        {
            "title": "2 DeLeaker",
            "content": "Our method, DeLeaker, aims to mitigate semantic leakage in DiT T2I models. As illustrated in Fig. 2, it relies solely on dynamic reweighting interventions at inference time in the self-attention mechanism during the diffusion process, and consists of three key steps. First, it identifies the image tokens (masks) corresponding to entities, i.e., the regions where they should appear in the generated image (2.1). Second, it suppresses the connections between entities in both the text-image and image-image self-attention maps (2.2). Finally, it enhances the self-identity of each entity by strengthening the connection between its text token and the corresponding image tokens (2.2). In 7, we present an ablation study, which demonstrates the importance of each intervention. 2.1 ATTENTION-BASED ENTITY MASKING ,(used as keys, k). The corresponding image tokens img To mitigate semantic leakage, we first localize for each textual entity ei in the prompt, the set of image tokens it governs, and then manipulate the attention maps using these localizations. Specifically, we use the pre-softmax attention scores, Attn, between all image tokens, I, (used as queries, q) and the set of text tokens, txt are selected by averaging attention scores across heads and applying dynamic threshold based on the mean, µi and standard deviation, σi of the attention distribution (Eq. 1). Following prior work on UNet-based diffusion (Hertz et al., 2022; Binyamin et al., 2025), we observe that even the early of the diffusion steps yield sufficiently accurate masks (B.1, Fig. 5). Thus, we aggregate attention maps across early steps in the diffusion process to create mask for each entity. We apply smoothing techniques on the masks: (1) temporal smoothing by averaging over the accumulated history maps, and (2) spatial smoothing via filtering, resulting in more stable and coherent entity masks (see B.1, Fig. 6). img = {q Attnqk > µi + β1 σi, (E txt I)} (1)"
        },
        {
            "title": "2.2 ATTENTION-BASED LEAKAGE MITIGATION",
            "content": "Leakage Suppression. Utilizing attention-based entity masks, we focus on cross-entity attention, which measures how the image tokens of one entity, ei, attend to the text or image tokens of another entity, ej. While cross-entity relations are primary source of semantic leakage, they are also essential for creating meaningful interactions, such as shared actions and poses. Therefore, our goal is not to eliminate these connections entirely, but to selectively suppress only those causing leakage while preserving beneficial ones. We hypothesize that high attention values in image-image relations (Eq. 2) represent unwanted semantic transfer (akin to high-frequency noise), while lower values reflect desirable, meaningful interactions (the core signal). Specifically, we apply unified suppression mechanism by zeroing out attention scores (Eq. 3, first two cases). This involves fully suppressing all cross-entity image-text attention scores while also suppressing image-image attention scores that exceed one standard deviation, multiplied by coefficient, β2, above their mean. This intervention is applied only after the initial, attention-based entity masks have been formed. Himg-img ij = {(q, k) Attnqk > µij + β2 σij, q, I} (2) Strengthening Self-Identity Alignment. Finally, we introduce third intervention to strengthen the connection between each entitys text tokens and its corresponding image tokens (Eq. 3, third case). This enhancement improves the self-identity of each entity. We apply this by multiplying the relevant attention scores by coefficient α > 1 (α ablations in B.1, Fig. 7). The coefficients are empirically chosen based on qualitative review of few samples external to the SLIM dataset. Attn qk = α Attnqk Attnqk if img if img if img else , and (q, k) Himg-img ij , img , txt , txt (3) Here Attnqk is the single pre-softmax attention score between the tokens and k. The terms µi and σi are respectively the mean and standard deviation (std) of attention scores for entity is image tokens. Similarly, µij and σij are the mean and std for attention between the image tokens of entities and j. Having established the method, we next turn to the dataset design that enables systematic evaluation of its effectiveness. See for DeLeakers full equations and hyperparameter values."
        },
        {
            "title": "3 THE SLIM DATASET: SEMANTIC-LEAKAGE IN IMAGES",
            "content": "Prior efforts to mitigate semantic leakage (Dahary et al., 2025b) and improve semantic alignment (Feng et al., 2022) have relied on general-purpose benchmarks such as DrawBench (Saharia et al., 2022) and MS-COCO (Lin et al., 2014). These benchmarks, however, do not specifically target semantic leakage. This is because the phenomenon is mainly associated with the visual similarity of entities (Dahary et al., 2025b), condition that rarely appears in their general-purpose prompts. Consequently, prior work has often drawn conclusions from evaluating extremely small subsets of these datasets, sometimes only few dozen samples (Chefer et al., 2023; Dahary et al., 2025b). To fill this gap, we introduce SLIM, which is, to the best of our knowledge, the first dataset explicitly designed to study and evaluate visual semantic leakage at scale. It contains 1,130 samples, each with prompt, generation seed, and corresponding image exhibiting semantic leakage, all generated using FLUX (Black-Forest-Labs, 2024). SLIM is organized into five subsets, as detailed below (examples in Fig. 1), and was curated through two-step process: large-scale generation followed by human-guided filtering. To validate that DeLeakers performance is not limited to FLUX, we create an additional test set using SANA (Xie et al., 2024). Due to the extensive data filtering required, this supplementary set contains 370 samples. Large-Scale Generation & Dataset Design. Building on the finding that semantic leakage is associated with visual similarity (Dahary et al., 2025b), we find the effect is particularly acute within fine-grained categories (e.g., dog breeds). Motivated by this, we focus on animals and fruits for controlled evaluation. Starting from curated list of 90 animals (Banerjee, 2023), we use GPT-4o"
        },
        {
            "title": "Under review",
            "content": "to expand it to 200 animals and generate 200 descriptive prompts, each pairing visually similar animals. We then produce corresponding images using five seeds per prompt. We leverage the animal pairs subset to create increasingly complex scene configurations, hypothesized to be associated with stronger leakage (see Table 3), including interactions (e.g., hugging), shared visual styles (e.g., comics), and multiple entities (triplets). To probe semantic leakage in different domain, we similarly expand our dataset to include fruits & vegetables subset based on an existing list of 36 fruits (Seth, 2019), where leakage is rare in pairs but emerges in triplets. Notably, subsets with multiple entities tend to present challenges beyond semantic leakage, as they are also prone to entity count errors (i.e., missing or added entities). Human-Guided Filtering of Semantic Leakage. We filter the large-scale set to include only images that exhibit detectable semantic leakage. This is achieved through two-stage process: an initial large-scale filtering using noisy automatic pipeline, followed by second round of manual verification through human annotation.2 We designed rigorous structured human annotation protocol for detecting semantic leakage, detailed in F.1. See for subset sizes through the filtering and prompt examples."
        },
        {
            "title": "4 EVALUATION",
            "content": "Figure 3: Our Automatic Evaluation Framework for Assessing Semantic Leakage Mitigation. The framework consists of three main steps: (1) visual difference extraction, (2) typicality assessment, and (3) comparative ranking. Step 1 is divided into two parts: one based solely on the input prompt (top) and the other employs reference images generated for each entity (bottom). The VLM generates and then merges two independent descriptions into unified description. Step 2 consists of four typicality questions, one for each entity in each image, guided by the unified differences identified in Step 1. Step 3 employs the outputs of Step 2 to compare both images. It produces classification indicating the preferred image (Image 1 or Image 2) and the magnitude of change (minor or major). Evaluating semantic leakage mitigation in T2I models is major challenge. Prior efforts have often relied on general purpose metrics (e.g., CLIP score (Radford et al., 2021)) or qualitative judgments, which lack the specificity required for systematic analysis and are often insensitive to subtle, finegrained errors that characterize semantic leakage (Dahary et al., 2025a). To address this, we introduce novel automatic evaluation framework centered on comparative setup, which directly contrasts candidate (mitigated) image against the original. Automating comparative analysis, however, is non2Specifically, two authors of this paper manually reviewed the images."
        },
        {
            "title": "Under review",
            "content": "Figure 4: Qualitative comparison across baselines (columns) and three examples (rows). trivial due to limitations in the visual modality of state-of-the-art VLMs (see B.2). To overcome this challenge, our evaluation pipeline decomposes the complex visual comparison into discrete logical steps, thereby leveraging the more robust reasoning capabilities of the text modality in VLMs (Ventura et al., 2024a; Nikankin et al., 2025). To ensure its reliability, the framework is validated against extensive human assessments. Our evaluation covers two critical dimensions: leakage mitigation, which measures the reduction of cross-entity interference, and preservation, described next. Automatic Evaluation for Leakage Mitigation. Our framework decomposes the evaluation into three interpretable steps, performed by an external VLM (Gemini 1.5; Team et al. 2024a), as shown in Fig. 3. The process requires four inputs: (1) prompt, (2) an original image exhibiting semantic leakage, generated by the base model , orig , generated by model as corrected version of the original image; and (4) reference images, generated independently by M, REF ei . The reference images act as auxiliary cues, ensuring the evaluation isolates leakage effects rather than the information encoded in the VLM about each of the entities. ; (3) candidate image, cand The pipeline first identifies key visual differences between the entities by combining the VLMs general knowledge with the specific insights from the reference images. Second, it assesses the typicality of each entity in both the original and candidate images, measuring how well each matches its expected appearance, based on the key visual differences. Finally, it performs comparative judgment to determine which image better preserves the distinct identity of all entities. To mitigate sensitivity to image order in VLMs (Ventura et al., 2024a), the images are presented in random order. The evaluations output is single discrete label, = (I cand ), which represents the change between the two images. This label combines the changes direction (improvement/degradation) and magnitude (major/minor), along with no change option, resulting in five possible outcomes. , orig Preservation Metrics. In addition to leakage mitigation, we evaluate three preservation aspects: alignment with the original prompt (VQAScore (Lin et al., 2024)), image quality (KID (Jayasumana et al., 2024)), and perceptual similarity to the original image (LPIPS (Zhang et al., 2018)). Human Assessment of Leakage Mitigation (User Study). To establish human baseline preferences and validate the reliability of our automatic evaluation, we conduct user study on Amazon Mechanical Turk (AMT) which results in total of 980 individual responses (see AMT questionnaire in F.2). Since evaluating leakage with multiple entities is confounded by the difficulty of assessing each entity pair separately, we focus our evaluation on the pair subsets of SLIM dataset. We randomly sample 60 prompts from these subsets, ensuring equal distribution across the subsets. Each task presents candidate image generated by one of six baselines representing range of methods, the original image, and two reference images (one per entity), following the structure of the automatic pipeline. The questionnaire includes two questions that assess the typicality of each entity using five-point scale aligned with the automatic evaluation. Each task is completed by three annotators, and responses are combined via majority vote. The inter-annotator agreement is 0.52 (quadratic weighted Fleiss κ), which validates the correlation between human judgments and our automatic"
        },
        {
            "title": "Under review",
            "content": "Table 1: Automatic and Human Assessment Scores of Semantic Leakage Mitigation. We compare our method, DeLeaker (bottom rows), against layout-based (top) and prompt-based (middle) baselines. The main scores, summarized by stacked bar visualization, represent the percentage of samples labeled as Mitigation (Major/Minor), No Change, or Degradation (Major/Minor), where larger green portion indicates better performance. The automatic scores are calculated on the SLIM pair subsets (840 samples), while the human scores are gathered from the user study of 60 random samples from that same subset. These are presented alongside preservation metrics (VQAScore, LPIPS, and KID (102)). Arrows (/) indicate the desired direction for improvement. Method RAG-Diffusion RPF 3DIS QwenFLUX Instruction Prompt Entity Description Prompt DeLeaker DeLeaker + Description Visualization Semantic Leakage (Automatic) Mitigation Major Minor No Change Minor Degradation Major (Human) Visualization Preservation VQAScore LPIPS KID 17.55% 4.17% 5.03% 8.34% 64.91% 20.74% 9.06% 16.57% 15.26% 38.38% 29.08% 8.10% 7.63% 10.13% 45.05% 17.28% 7.51% 23.92% 11.54% 35.60% 11.07% 46.07% 9.76% 53.57% 8.57% 15.85% 35.35% 25.71% 25.36% 15.95% 12.75% 46.60% 9.28% 19.88% 9.17% 18.45% 5.83% 12.98% 6.55% 15.36% 0.42 0.63 0.62 0.49 0. 0.62 0.68 0.65 0.72 0.64 0. 0.61 0.33 0.41 0.22 0.43 0. 0.53 0.96 0.46 0.00 0.00 0. 0.01 evaluation (Spearmans ρ=0.432) as meaningful proxy. We observe difference in model-human sensitivity: both typically agree on the changes direction (mitigation vs. degradation) but differ on its magnitude (minor vs. major)."
        },
        {
            "title": "5 EXPERIMENTAL SETUP",
            "content": "Base DiT T2I Models. We primarily experiment with the state-of-the-art open-source DiT T2I model FLUX.1-DEV (Black-Forest-Labs, 2024), while also applying DeLeaker to SANA (Xie et al., 2024) to validate our findings. Unlike earlier UNet-based (Ronneberger et al., 2015) models such as Stable Diffusion (Rombach et al., 2022), where textual information is injected through spatial cross-attention layers at multiple resolutions during denoising, DiTs employ transformer-based (Vaswani et al., 2017) backbone that processes image and text tokens jointly. This architectural shift promotes capturing complex cross-modal dependencies and achieving more consistent global semantics. The differing text encoders and attention mechanisms in FLUX and SANA are relevant for studying semantic leakage, as these components control how unintended information propagates between modalities. To the best of our knowledge, this setup represents the first exploration of semantic leakage in DiT T2I models. For brevity, the following setup focuses on FLUX, while the full experimental details for SANA are available in E.1. Baselines. We evaluate DeLeaker against layout-based and prompt-based baselines. Layout-based methods provide explicit priors on image structure to improve compositional control (Chen et al., 2024a;b), making them relevant for semantic leakage as their structure reduces content mixing. Additionally, we include several zero-shot, prompt-based baselines, which are common for improving image-text alignment (Yang et al., 2024). To maintain fair comparison, all methods are built upon the FLUX base model, as our SLIM is created using FLUX-generated images. For layout-based baselines, we utilized FLUX-based parallel implementations of an existing UNet baseline (Dahary et al., 2025b), specifically RPF (Chen et al., 2024a), RAG-Diffusion (Chen et al., 2024b), and 3DIS (Zhou et al., 2025). These baselines differ in their inputs and conditioning strategies. The first, RPF, leverages regional prompts within bounding boxes while eliminating cross-boundingbox attention. The second, RAG-Diffusion, constrains self-attention to local text descriptions within each box, but only during the initial steps of the diffusion process. Finally, 3DIS (Zhou et al., 2025) conditions on bounding boxes to generate depth map as an additional input. It is important to note that all three baselines rely on external LLMs or additional models as guidance (H.1). For prompt-based baselines, we employ three methods. The first is an implicit instruction to generate an image without semantic leakage between entities, referred to as the Instruction Prompt. Since T2I models are not trained for instruction-following, we also experiment with explicitly describing"
        },
        {
            "title": "Under review",
            "content": "Table 2: DeLeaker Ablation Study. Configurations are divided into two types: (1) W/O rows (top four) represent the removal/addition of specific component, while (2) Only rows (bottom three) isolate each component independently. Ratios are reported relative to the full DeLeaker scores baseline, with values closer to 1.0 indicating similarity. Darker hues indicate stronger contributions, color-coded as positive and negative . Signs indicate attention suppression (-) or strengthening (+). Configuration Leakage Mitigation (Relative to DeLeaker) Degradation Minor Major Improvement Major Minor No Change DeLeaker W/O Image-Image (-) W/O Image-Text (-) W/O Image-Text (+) With Text-Text (-) Only Image-Image (-) Only Image-Text (-) Only Image-Text (+) 1.00 1.01 0.93 0.54 0.91 0.26 0.54 0.90 1. 1.04 0.78 0.82 0.91 0.61 0.88 0.99 1.00 1.05 1.10 1.73 1.08 2.44 1.88 1.23 1. 0.73 1.04 1.20 1.20 1.35 1.00 0.88 1.00 0.97 1.18 1.24 1.16 0.96 0.99 0.96 each entity and its appearance, referred to as the Entity Description Prompt. To illustrate, the prompt zebra and horse are riding in the sand together. . .  (Fig. 4)  is enriched with LLM-generated entity attributes, such as the zebra has dense black-and-white stripes, while the horse has white fur and blond tail... The final method is the Image-Condition Instruction Prompt, where the model (Qwen2VL-Flux; Lu 2024) is instructed to mitigate leakage based on the original image."
        },
        {
            "title": "6 RESULTS",
            "content": "Table 1 presents the automatic and human evaluation of leakage mitigation across all baselines on the SLIM pair subsets, and Fig. 4 presents qualitative examples (see additional examples in D). Complementary results are in E, including SANAs scores and results with multiple entities. DeLeaker outperforms baselines in mitigating semantic leakage. Our automatic evaluation shows that DeLeaker achieves the highest rate of semantic leakage mitigation with minimal degradation. Human evaluation, strongly confirms these findings, with annotators judging that DeLeaker improved the image in clear majority of cases (67.8% total improvement), outperforming all other methods. Furthermore, adding entity descriptions to DeLeaker (similar to the Entity Description Prompt baseline) offers only minor gains, indicating that DeLeaker is highly effective on its own. Among the other baselines, the text prompt-based methods have combined degradation rate of just 24.2%, which is significantly lower than the rates for layout-based methods, all of which are over 50%. DeLeaker preserves fidelity and quality. Beyond leakage mitigation, DeLeaker excels at preserving image fidelity and quality. It achieves the lowest LPIPS score (0.22), meaning it best preserves the original image, which indicates that the method effectively leverages the models internal knowledge and priors, applying only minimal, necessary interventions. DeLeaker also attains the highest VQAScore (0.68), signifying strong image-text alignment. Moreover, it achieves the lowest KID score (0.00) alongside the prompt-based baselines, demonstrating that strong leakage reduction is achieved without sacrificing original image quality. Notably, when applied to images without leakage (D, Fig. 12), DeLeaker induces negligible changes, thereby remaining non-intrusive."
        },
        {
            "title": "7 ABLATION STUDY & ANALYSIS",
            "content": "Table 2 presents an ablation study assessing the contribution of each DeLeaker component. It includes two configurations: (1) W/O ablations, where components are removed from or added to the full method while the other are applied, and (2) Only ablations, where components are tested in isolation. Results are reported as ratios relative to the full automatic leakage mitigation scores of DeLeaker. The most influential intervention is self-identity (image-text) strengthening. When applied alone, it achieves 0.90 ratio in the major improvement (leftmost column). Conversely, when removed the"
        },
        {
            "title": "Under review",
            "content": "original score drops by 46% (to 0.54), confirming its key role in leakage prevention. The second most influential intervention is cross-entity image-text suppression. Omitting it causes 29% reduction in improvement (major and minor). Furthermore, when applied in isolation, it accounts for 0.54 (second-to-last row) of the total major improvement with almost no degradation, demonstrating its significant contribution. While cross-modality interventions are found to be effective, selfmodality interventions have only limited impact. Suppressing text-text interactions degrades performance by 9% to 20%, suggesting that leakage in DiT T2I models is primarily due to crossmodal misalignment. Similarly, weakening image-image interactions has small and inconsistent impact (see absolute values in E.3). Taken together, our analysis pinpoints the root of semantic leakage not to weaknesses within each modality, but to the faulty alignment between them, suggesting promising direction for future research. Finally, we analyze mitigation performance across the SLIM subsets. As shown in Table 3, the rate of successful mitigation increases dramatically with subset complexity. The total improvement rate (major and minor) rises from 42.4% for simple Animal Pairs to 62.6% for Animal Interactions, and further to 66.4% for the most complex Animal Interactions + Style subset. This provides clear evidence for our hypothesis: more complex prompts elicit stronger semantic leakage. This validates their use in SLIM as stress tests for semantic leakage. Table 3: SLIM Subset Analysis with DeLeaker. Subset Visualization Animal Pairs Animal Interactions Animal Interactions + Style"
        },
        {
            "title": "8 RELATED WORK",
            "content": "Alignment in T2I models. Ensuring alignment between the text prompt and the generated image is fundamental objective in T2I models, serving both as generation condition and as an evaluation goal (Xie et al., 2019; Hu et al., 2023; Yarom et al., 2024; Gordon et al., 2023). Many approaches rely on encoding-based methods, such as joint image-text embeddings (e.g., CLIP), which were found to be ineffective for fine-grained details between modalities (Liang et al., 2022; Yuksekgonul et al., 2022; Koishigarina et al., 2025) (see B.2). While recent work has employed VLMs as alignment evaluators (Li et al., 2023), they are unsuitable for detecting semantic leakage. VLMs struggle with the fine-grained details (Tong et al., 2024; Yu et al., 2025) and complex reasoning required for multiimage comparisons (Ventura et al., 2024b). This means direct approach is insufficient, highlighting the need for more guided, step-by-step evaluation process. To the best of our knowledge, no evaluation method explicitly targets semantic leakage, despite its prevalence in T2I models (see G.1, Table 17). Addressing this gap is central focus of our work, in which we introduce dedicated method to mitigate semantic leakage and corresponding evaluation framework. sSemantic Leakage in T2I models. Leakage in T2I models was first identified by Rassin et al. (2022) in UNet-based T2I models, though direct mitigation was not proposed. While subsequent research has addressed related visual artifacts such as attribute binding (see for distinction from semantic leakage; Feng et al., 2022; Rassin et al., 2024), composition errors, and missing entities (Binyamin et al., 2025) by modifying the attention mechanism, these works do not directly address semantic leakage. To the best of our knowledge Dahary et al. (2025a;b) were the only ones to explicitly tackle this problem. However, their solutions rely on external layout guidance or costly optimization. In contrast, we introduce DeLeaker, lightweight training-free, guidance-free semantic leakage mitigation method. Semantic Leakage in Language Models. Semantic leakage has only recently been recognized as an issue in state-of-the-art language models like GPT-4o, where prompt information unintentionally biases the output (Gonen et al., 2025). While progress has been made in diagnosing semantic leakage, with one cause identified as leakage between lexical items in the text encoder (Kaplan et al., 2025), effective mitigation remains an open problem. Therefore, our work focuses on developing novel mitigation strategy while also investigating the origins of leakage through our methods ablations."
        },
        {
            "title": "9 CONCLUSIONS",
            "content": "This work introduces DeLeaker, lightweight inference-time approach that effectively mitigates semantic leakage in DiT-based T2I models without relying on external information such as bounding-"
        },
        {
            "title": "Under review",
            "content": "boxes. By directly modulating attention patterns during inference, DeLeaker mitigates leakage while preserving image-text alignment and image quality. It outperforms existing baselines, across diverse scenarios. Complemented by the first dedicated SLIM dataset and comparative evaluation framework, this work provides both practical solution and comprehensive foundation for systematic study of semantic leakage in T2I models. Future research could expand the SLIM dataset into new domains to explore cross-domain leakage scenarios. Furthermore, SLIM could be used to train leakage classifiers or, when paired with DeLeaker outputs, to fine-tune models to inherently avoid semantic leakage. While DeLeaker specifically targets T2I models, extending our work to address semantic leakage in other modalities, such as 3D or video, is natural next step. We hope this work stimulates further progress on new methods, systematic evaluations, and dedicated datasets to address key problems in T2I generation."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure reproducibility, all code and the newly introduced SLIM dataset will be made publicly available. Our experiments are based on open-source T2I models, FLUX.1-dev and SANA, with all baselines and their configurations clearly documented in H.1. Key hyperparameters for DeLeaker, such as attention reweighting coefficients and the specific diffusion step ranges for interventions, are detailed in C, Table 6. Moreover, our automated evaluation framework is thoroughly described, with the exact VLM prompts provided in to allow for complete replication of our evaluation process."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "In this work, we utilized AI models for several tasks. For grammar improvement, we used Gemini 2.5 Pro. For code completion, we used Claude 4 Sonnet. In all instances, every suggestion or line of code generated by model was carefully reviewed by the authors to ensure it aligned with our original intentions before being accepted. Finally, as detailed in 3 and F, we also used LLMs and VLMs for data creation and evaluation."
        },
        {
            "title": "REFERENCES",
            "content": "Sourav Banerjee. Animal image dataset: 90 different animals. https://www.kaggle.com/datasets/ iamsouravbanerjee/animal-image-dataset-90-different-animals/data, 2023. Accessed: 2025-09-14. Lital Binyamin, Yoad Tewel, Hilit Segev, Eran Hirsch, Royi Rassin, and Gal Chechik. Make it count: Text-toimage generation with an accurate number of objects. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1324213251, 2025. Black-Forest-Labs. Flux. https://github.com/black-forest-labs/flux, 2024. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM transactions on Graphics (TOG), 42(4):110, 2023. Anthony Chen, Jianjin Xu, Wenzhao Zheng, Gaole Dai, Yida Wang, Renrui Zhang, Haofan Wang, and Shanghang Zhang. Training-free regional prompting for diffusion transformers. arXiv preprint arXiv:2411.02395, 2024a. Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024b. Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be decisive: Noiseinduced layouts for multi-subject generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pp. 112, 2025a. Omer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. Be yourself: Bounded attention for multisubject text-to-image generation. In Aleš Leonardis, Elisa Ricci, Stefan Roth, Olga Russakovsky, Torsten Sattler, and Gül Varol (eds.), Computer Vision ECCV 2024, pp. 432448, Cham, 2025b. Springer Nature Switzerland. ISBN 978-3-031-72630-9."
        },
        {
            "title": "Under review",
            "content": "Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https://openreview.net/forum?id=AAWuCvzaVt. Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In The Eleventh International Conference on Learning Representations, 2022. Yarden Frenkel, Yael Vinker, Ariel Shamir, and D. Cohen-Or. Implicit style-content separation using blora. ArXiv, abs/2403.14572, 2024. URL https://api.semanticscholar.org/CorpusId: 268553753. Hila Gonen, Terra Blevins, Alisa Liu, Luke Zettlemoyer, and Noah A. Smith. Does liking yellow imply driving school bus? semantic leakage in language models. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 785798, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-1896. doi: 10.18653/v1/2025.naacl-long.35. URL https://aclanthology.org/2025.naacl-long. 35/. Brian Gordon, Yonatan Bitton, Yonatan Shafir, Roopal Garg, Xi Chen, Dani Lischinski, Daniel Cohen-Or, and Idan Szpektor. Mismatch quest: Visual and textual feedback for image-text misalignment. arXiv preprint arXiv:2312.03766, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications. Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2040620417, 2023. Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 93079315. IEEE, 2024. Guy Kaplan, Michael Toker, Yuval Reif, Yonatan Belinkov, and Roy Schwartz. Follow the flow: On information flow across textual tokens in text-to-image models. arXiv preprint arXiv:2504.01137, 2025. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 40154026, 2023. Darina Koishigarina, Arnas Uselis, and Seong Joon Oh. Clip behaves like bag-of-words model cross-modally but not uni-modally. arXiv preprint arXiv:2502.03566, 2025. Black Forest Labs. Flux.1 depth [dev], black-forest-labs/FLUX.1-Depth-dev. Accessed: 2025-09-12. 2024. URL https://huggingface.co/ Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 292305, 2023. Zongming Li, Lianghui Zhu, Haocheng Shen, Longjin Ran, Wenyu Liu, and Xinggang Wang. Translight: Image-guided customized lighting control with generative decoupling. ArXiv, abs/2508.14814, 2025. URL https://api.semanticscholar.org/CorpusId:280692064. Victor Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35:1761217625, 2022. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pp. 740755. Springer, 2014."
        },
        {
            "title": "Under review",
            "content": "Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In European Conference on Computer Vision, pp. 366384. Springer, 2024. Pengqi Lu. Qwen2vl-flux: Unifying image and text guidance for controllable image generation, 2024. URL https://github.com/erwold/qwen2vl-flux. Yaniv Nikankin, Dana Arad, Yossi Gandelsman, and Yonatan Belinkov. Same task, different circuits: Disentangling modality-specific mechanisms in vlms. arXiv preprint arXiv:2506.09047, 2025. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya In International Conference on Machine Learning, pp. Sutskever. Zero-shot text-to-image generation. 88218831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Royi Rassin, Shauli Ravfogel, and Yoav Goldberg. DALLE-2 is seeing double: Flaws in word-to-concept mapping in Text2Image models. In Jasmijn Bastings, Yonatan Belinkov, Yanai Elazar, Dieuwke Hupkes, Naomi Saphra, and Sarah Wiegreffe (eds.), Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pp. 335345, Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.blackboxnlp-1.28. URL https://aclanthology.org/2022.blackboxnlp-1.28/. Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. Advances in Neural Information Processing Systems, 36, 2024. Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, et al. Grounding dino 1.5: Advance the\" edge\" of open-set object detection. CoRR, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1067410685. IEEE, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234241. Springer, 2015. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. URL https://arxiv.org/abs/2205.11487. Kritik Seth. Fruits and vegetables image recognition dataset, 2019. URL https://www.kaggle.com/ datasets/kritikseth/fruit-and-vegetable-image-recognition. Aviv Slobodkin, Hagai Taitelbaum, Yonatan Bitton, Brian Gordon, Michal Sokolik, Nitzan Bitton Guetta, Almog Gueta, Royi Rassin, Dani Lischinski, and Idan Szpektor. Refvnli: Towards scalable evaluation of subject-driven text-to-image generation. arXiv preprint arXiv:2504.17502, 2025. Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, et al. Ldm3d: Latent diffusion model for 3d. arXiv preprint arXiv:2305.10853, 2023."
        },
        {
            "title": "Under review",
            "content": "Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv. org/abs/2403.05530, 2024a. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024b. Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95689578, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, and Roi Reichart. Nl-eye: Abductive nli for images. arXiv preprint arXiv:2410.02613, 2024a. Mor Ventura, Michael Toker, Nitay Calderon, Zorik Gekhman, Yonatan Bitton, and Roi Reichart. Nl-eye: Abductive nli for images. In The Thirteenth International Conference on Learning Representations, 2024b. Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, et al. Sana: Efficient high-resolution image synthesis with linear diffusion transformers. arXiv preprint arXiv:2410.10629, 2024. Ning Xie, Farley Lai, Derek Doran, and Asim Kadav. Visual entailment: novel task for fine-grained image understanding. arXiv preprint arXiv:1901.06706, 2019. Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing cross-attention leakage for text-based image editing. Advances in Neural Information Processing Systems, 36: 2629126303, 2023. Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: recaptioning, planning, and generating with multimodal llms. In Proceedings of the 41st International Conference on Machine Learning, pp. 5670456721, 2024. Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving text-image alignment evaluation. Advances in Neural Information Processing Systems, 36, 2024. Hong-Tao Yu, Xiu-Shen Wei, Yuxin Peng, and Serge Belongie. Benchmarking large vision-language models on fine-grained image tasks: comprehensive evaluation. arXiv preprint arXiv:2504.14988, 2025. Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionIn The Eleventh International language models behave like bags-of-words, and what to do about it? Conference on Learning Representations, 2022. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-toimage generation. arXiv preprint arXiv:2410.12669, 2024. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis-flux: simple and efficient multi-instance generation with dit rendering. arXiv preprint arXiv:2501.05131, 2025."
        },
        {
            "title": "Under review",
            "content": "A SEMANTIC LEAKAGE: CONCEPTUAL CLARIFICATION AND SCOPE A.1 DISTINCTION FROM ATTRIBUTE BINDING Semantic leakage and attribute binding (Rassin et al., 2024) represent two related but distinct challenges in T2I generation (see Table 4). Attribute binding refers to the failure to correctly associate explicitly mentioned attributes with their intended entities in the input prompt. For instance, in prompts such as yellow flamingo and pink sunflower or red frog and blue rabbit, models may misplace attributes (e.g., rendering the rabbit as red or the frog as blue), resulting in incorrect color-to-entity assignments. The source of the error is thus misalignment between the linguistic specification of attributes and their grounding in the image. In contrast, semantic leakage arises not from the wrong binding of attributes explicitly stated in text, but from the unintended transfer of semantically related features between entities. This phenomenon is primarily driven by the visual similarity of the entities, making it more likely to occur between horse and donkey than between cow and parrot. On the other hand, attribute binding can also occur between visually dissimilar entities (e.g., red cow and white parrot). Here, features attend their semantically similar counterparts across entities, for example, the ears of one animal influencing the ears of another, or the shape of mouth blending between two species. This leads to cross-entity entanglement of features that are not even explicitly mentioned in the textual prompt, but emerge due to the semantic proximity of visual parts (e.g., cow ears leaking into horses ears). Table 4: Comparison between Attribute Binding and Semantic Leakage. Aspect Definition Source Primary Cause Attribute Binding Semantic Leakage Misalignment between textual attributes and their intended entities. Unintended transfer of semantically related features between entities. Explicit attributes in the text prompt (e.g., colors, shapes). Implicit similarity between visual features (e.g., ears, eyes, mouths). Confusion over explicit attributes, regardless of entity similarity (e.g., red cow and white parrot). Visual/semantic proximity of the entities themselves (e.g., more likely between horse and donkey). Example Prompt yellow flamingo with pink sunflower; red frog and blue rabbit. cow and horse in farm. Error Manifestation Attributes swapped or misplaced (e.g., blue frog instead of red frog). Feature entanglement across entities (e.g., cow traits appearing in the horses ears). Commonality Both result in semantically inconsistent outputs that reduce fidelity to the intended meaning. A.2 DIFFERENTIATION FROM LEAKAGE IN IMAGE-TO-IMAGE GENERATION More recently, leakage has also been discussed in the context of style-content entanglement in image-to-image generation using reference images (Frenkel et al., 2024; Li et al., 2025). This line of work, however, focuses on different type of leakage that occurs between style and content, rather than on the internal semantic leakage between entities within the same image, which is the focus of our study. Image-to-image editing frameworks offer another possible direction for addressing this challenge. However, they involve computationally expensive double inference and rely on external inputs, prompt optimization (Yang et al., 2023) or adapters optimizations often resulting in identity preservation issues (Slobodkin et al., 2025). In contrast, our method is both training-free and guidance-free. It achieves high semantic consistency with the original image without requiring prior generation or post hoc correction."
        },
        {
            "title": "Under review",
            "content": "B FURTHER ANALYSIS & ABLATIONS B.1 DeLeaker COMPONENTS ABLATIONS Figure 5: Entity masks are accurate even in the first diffusion step (50 blocks; green frame). This is particularly evident in semantic leakage cases, where these initially clear masks begin to blend by middle step (660 blocks; red frame). The full process consists of 20 diffusion steps (1140 blocks total)."
        },
        {
            "title": "Under review",
            "content": "Figure 6: Ablation study of DeLeakers smoothing techniques on entity masks. The figure demonstrates the impact of two components: (Top) temporal smoothing and (Bottom) spatial smoothing. Figure 7: Effect of varying the self-identity strengthening coefficient (α) in DeLeaker. Multiplying the image-text representation by α helps mitigate semantic leakage. This coefficient was empirically optimized on small set of images, where we found α = 1.2 effectively mitigates semantic leakage. Whereas, higher values, such as α = 2.0, introduces visual artifacts."
        },
        {
            "title": "Under review",
            "content": "Analysis: Attention Differences between DeLeaker and Original To further analyze the contribution of DeLeakers cross-entity components (image-image and image-text), we track the progression of semantic leakage across model (FLUX-dev) blocks and diffusion steps. We compute the average proportion of tokens attending to the other entity, exceeding the dynamic leakage threshold, relative to the number of tokens in the entity mask. For each entity pair, we measure leakage in both directions and take the maximum, as leakage typically occurs in only one direction (ei ej). The analysis is performed under two conditions: standard inference (original) and inference with DeLeaker. Figure 8 shows the relative mean difference in leakage progression between the two settings. While the image-image components effect is bounded at high value, partially explaining its smaller apparent change, the data still suggests this intervention has lower impact on mitigating semantic leakage. Figure 8: Analysis of Leakage Mitigation Progression. The figure shows how DeLeakers crossentity components mitigate semantic leakage throughout the FLUX diffusion process (steps blocks). The y-axis represents the relative change in cross-entity attention between the DeLeaker run and the original run. The top and bottom plots show the effects for the image-text and image-image components, respectively. B.2 AUTOMATIC EVALUATION BASED ON PREVIOUS EFFORTS Evaluating the success of semantic leakage mitigation fundamentally requires comparative analysis between the original and the corrected image. Automating this comparison is non-trivial, however, as state-of-the-art methods suffer from critical limitations. Vision-Language Models (VLMs), for instance, exhibit order sensitivity where their judgment is biased by image presentation order, and possess unreliable visual encodings that fail in zero-shot comparisons (Ventura et al., 2024a). Similarly, joint-encoding models like CLIP are unreliable due to significant cross-modal alignment gaps, often failing to correctly match text with visual information (Liang et al., 2022). These limitations highlight the need for more robust, step-by-step evaluation pipeline, as simple proxies are insufficient for this nuanced task. We investigated whether standard metrics from joint-encoding models like CLIP and BLIP could serve as proxy for our evaluation pipeline. To test this, we examined two conditions for both models: self-identity check, which compares an entitys image crop with its own name (e.g.,"
        },
        {
            "title": "Under review",
            "content": "horse image vs. the text horse, and cross-entity check, which compares the image crop with the other entitys name (e.g., horse image vs. the text cow). With CLIP, we measured the direct image-text similarity score. With BLIP, we queried the model with question (e.g., Is this horse in the image?) and used the predicted probability of the answer being Yes. We then performed Spearmans rank correlation analysis between these CLIP and BLIP-based scores and our automatic evaluation labels (major improvement, minor improvement, no change, minor degredation, major degredation). The analysis was conducted on our 821-sample pair subset, using our automatic labels as the ground truth, which themselves correlate moderately with human judgments. The results, as presented in Table 5, show no statistically significant correlation across all tested metrics. The correlation coefficients were found to be negligible, ranging from approximately 0.04 to 0.03, with all corresponding p-values being high (p 0.05). This demonstrates that simple, off-the-shelf CLIP and BLIP-based measurements fail to capture the nuances of semantic leakage, reinforcing the need for our structured, multi-step evaluation pipeline. Table 5: Spearmans rank correlation (ρ) between our automatic evaluation labels and various metrics derived from CLIP and BLIP (N = 821). In all cases, the correlation is statistically insignificant (p 0.05). Model Metric Type Spearmans ρ p-value CLIP BLIP Self-Identity Cross-Entity Self-Identity Cross-Entity 0.010 0. 0.027 0.027 0.773 0.773 0.440 0."
        },
        {
            "title": "Under review",
            "content": "C DeLeaker METHOD: COMPLEMENTARY DETAILS Table 6: Technical details of DeLeaker. Parameter General T2I Parameters DeLeaker-Specific Parameters Parameter Group Number of inference steps Guidance scale α β1: Std. coefficient (text-image) β2: Std. coefficient (image-image) tstart-aggregation tend-aggregation tstart-intervention tend-intervention Value Goal 3.5 1.2 0.9 2 456 57 741 self-identity strengthening entity mask supimage-image pression diffusion step of start aggregating entity masks diffusion step of stop aggregating entity masks diffusion step of interventions start (suppression and strengthening) diffusion step of stop interventions (suppression and strengthening) C.1 DeLeaker IMPLEMENTATION: TECHNICAL DETAILS Entity Mask The first step of DeLeaker is to find and extract the relevant image tokens of each entity in the prompt. We find, similarly to previous work in UNet-based diffusion models (Binyamin et al., 2025), that early diffusion steps yield more accurate entity segmentation masks compared to later ones. Surprisingly, even within single partial diffusion step, this method produces reliable results. Based on this observation, we aggregate attention maps for each entity across selected diffusion blocks and timesteps. Specifically, from t12 to t171, where one diffusion step is consists of 57 blocks, while we run on 20 diffusion steps (results in total 1140 blocks in the diffusion process). Due to significant variation across blocks and timesteps, we apply two smoothing techniques to improve mask quality: (1) Spatial smoothing: applying smoothing filter to fill small holes and remove isolated artifacts. In this refinement, we apply several filters. The first is morphological closing operation which fills small holes within the predicted masks. Then, we apply morphological opening to eliminate spurious noise pixels, both using 3 3 elliptical structuring element. (2) Temporal smoothing (History): we enforce temporal coherence by averaging the attention-based masks across constrained window of subsequent transformer blocks and time steps. This window deliberately excludes the initial block of the first time-step. These that are very noisy and limited in duration to prevent the erroneous merging of distinct object masks over time. The combined methodology yields masks that are both spatially clean and temporally stable. Together, these steps produce cleaner and more consistent segmentation masks (see Figures in B.1). SANA-based DeLeaker We found that the image-image component yielded inconsistent results; while it sometimes improved leakage mitigation, it also occasionally introduced visual artifacts. Due to this unpredictable behavior, we excluded it from the final SANA configuration."
        },
        {
            "title": "Under review",
            "content": "C.2 FULL MATHEMATICAL FORMULATION The standard scaled dot-product attention mechanism is calculated as: Attention(Q, K, ) = softmax (cid:18) QK dk (cid:19) (4) where Q, K, are the Query, Key, and Value matrices, and dk is the dimension of the keys. The term Att = QK represents the raw, unnormalized similarity scores before scaling and the softmax operation. The following sections detail process for modifying these raw scores. Find Entity Masks µi = 1 IE txt (cid:88) (cid:88) qI kE txt i"
        },
        {
            "title": "Attqk",
            "content": "σi = (cid:118) (cid:117) (cid:117) (cid:116) 1 IE txt (cid:88) (cid:88) qI kE txt (Attqk µi)2 img = {q Attqk > µi + β1 σi, txt , I, I} Modify Attention Scores µij = 1 img img (cid:88) (cid:88) Attqk qE img kE img σij = (cid:118) (cid:117) (cid:117) (cid:116) 1 img E img (cid:88) (cid:88) qE img kE img (Attqk µij)2 Himg-img ij = {(q, k) Attqk > µij + β2 σij, q, I} Att qk = α Attqk Attqk if img if img if img else , and (q, k) Himg-img ij , img , txt , txt (5) (6) (7) (8) (9) (10) (11) Notation: I: set of all image tokens indices, txt β1, β2: constant std multipliers. : text tokens of entity i, α: score scaling factor. The cases in 11 correspond to: First case: Image-to-Image Leakage Suppression Second case: Image-to-Text Leakage Suppression Third case: Self-Identity Strengthening"
        },
        {
            "title": "D QUALITATIVE COMPLEMENTARY RESULTS",
            "content": "Figure 9: Qualitative Examples - FLUX-based DeLeaker."
        },
        {
            "title": "Under review",
            "content": "Figure 10: Qualitative comparison across baselines. FLUX-based DeLeaker. Figure 11: Qualitative Examples - SANA-based DeLeaker."
        },
        {
            "title": "Under review",
            "content": "Figure 12: Qualitative Examples of cases when original images do not present semantic leakage. Original images are on left and DeLeaker images are on right. DeLeaker preserve the image content and quality."
        },
        {
            "title": "Under review",
            "content": "Figure 13: Qualitative Examples of Triplets subset (with original image without entity counting issues). Examples across best performing prompt-based baselines. Figure 14: Qualitative Examples of Triplets subset (with original image witho entity counting issue: Missing Entity). DeLeaker mitigates the leakage in some cases while challenged in others creating the missing third entity. Examples across best performing prompt-based baselines."
        },
        {
            "title": "E QUANTITATIVE COMPLEMENTARY RESULTS",
            "content": "Table 7: Human Evaluation Results. Conducted on MTurk over 60 randomly selected samples across six baselines, with three annotators per task. Aggregation was performed using majority vote, with the median used in case of ties. The table reports the distribution of semantic leakage mitigation, categorized by direction and magnitude of change. Spearman correlation of 0.432 with p-value <0.001 with the corresponding automatic evaluation (see Appendix Table 8). Human Evaluation: Leakage Mitigation (Distribution) Model Visualization Improvement Major Minor No Change Minor Major Degradation RAG-Diffusion RPF 3DIS QwenFLUX Ent. Desc Prompt DeLeaker 3.57% 5.26% 5.00% 0.00% 17.86% 22.81% 16.67% 11.67% 16.13% 45.16% 13.56% 54.24% 21.43% 26.32% 16.67% 20.00% 14.52% 25.42% 21.43% 35.71% 21.05% 24.56% 21.67% 40.00% 36.67% 31.67% 17.74% 6.78% 6.45% 0.00% Table 8: Automatic Evaluation Results. Proportions computed over all user study samples (60) Automatic Evaluation: Leakage Mitigation (Distribution) Model Visualization Improvement Major Minor No Change Minor Major Degradation RAG-Diffusion RPF 3DIS QwenFLUX Ent. Desc Prompt DeLeaker 16.07% 17.54% 34.48% 16.95% 36.21% 53.57% 3.57% 5.26% 6.90% 6.78% 8.62% 7.14% 10.71% 17.54% 8.62% 11.86% 29.31% 16.07% 3.57% 66.07% 28.07% 31.58% 12.07% 37.93% 20.34% 44.07% 10.34% 15.52% 12.50% 10.71% Table 9: Results on Animal and Fruits & Veg Triplet Subsets (FLUX): We evaluate leakage mitigation on the triplets subsets across the best performing prompt-based baselines (based on the results on the pair subsets). The main scores represent the percentage of samples labeled as Mitigation (Major/Minor), No Change, or Degradation (Major/Minor), summarized by stacked bar visualization. These are presented alongside Preservation metrics (VQAScore and LPIPS). Arrows (/) indicate the desired direction for improvement for each metric. Subset Model Animal Triplets Fruits & Veg Triplets Instruction Prompt Ent. Desc Prompt DeLeaker Instruction Prompt Ent. Desc Prompt DeLeaker E.1 SANA Semantic Leakage Visualization Mitigation Major Minor Degradation No Change Minor Major Preservation VQAScore LPIPS 39.66% 3.45% 38.79% 2.59% 43.97% 7.76% 19.83% 13.79% 19.83% 0.86% 36.21% 6.90% 37.93% 8.62% 19.83% 35.06% 10.34% 15.52% 3.45% 35.63% 51.72% 8.05% 59.20% 6.90% 8.05% 9.77% 7.47% 24.71% 4.02% 20.11% 0.67 0.67 0.70 0.67 0.67 0. 0.45 0.49 0.25 0.41 0.46 0. The different designs of FLUX and SANA are highly relevant to studying semantic leakage. FLUX combines T5-XXL (Raffel et al., 2020) and CLIP (Radford et al., 2021) encoders, whereas SANA replaces them with Gemma-2 (Team et al., 2024b) and incorporates linear attention in its DiT backbone. These components are crucial, as both the text encoder and attention mechanism dictate how unintended semantic content propagates across modalities."
        },
        {
            "title": "Under review",
            "content": "We evaluate DeLeaker effectiveness at mitigating semantic leakage using our human-verified Sana dataset. Since prompt-based baselines have been shown to be more effective for reducing semantic leakage than layout-based methods, and as no implemented layout-based methods are available for Sana, we compare DeLeaker against two prompt-based baselines: the instruction prompt and the entity description prompt. The results, presented in Table 10, show that DeLeaker is highly effective on the Sana model. It achieves 64% improvement in leakage mitigation with only 15% performance degradation, yielding 49% net improvement. This performance significantly outperforms the instruction baseline. The entity description prompt, however, achieves much better results due to the additional description of the entities in the prompt, resulting in score that is only slightly behind DeLeaker. We attribute these results to SANAs use of the Gemma model as its text encoder, leading to superior performance on the prompt description baseline. To see whether DeLeaker can achieve even better results using this information, we test DeLeaker with the entity descriptions. The results are even stronger: DeLeaker gains an additional 14% improvement, resulting in 78% improvement in leakage mitigation and only 15% degradation, beating the entity description baseline significantly. Table 10: SANA Main Results. Distribution of semantic leakage mitigation across models, categorized by direction and magnitude of change. Arrows ( or ) indicate the improvement direction. Evaluated on 368 samples, filtered from SLIM large scale with SANA model images. Model Visualization Improvement Leakage Mitigation (Distribution) Degradation Preservation VQAScore LPIPS Major Minor No Change Minor Major Instruction Prompt (SANA) Ent. Desc. Prompt (SANA) DeLeaker (SANA) DeLeaker With Ent. Desc. (SANA) 21.45% 11.07% 56.55% 7.59% 55.36% 8.65% 66.55% 12.07% 40.14% 20.00% 17.30% 5.52% 11.76% 15.57% 5.86% 10.00% 5.54% 13.15% 4.83% 11.03% 0.75 0. 0.79 0.73 0.33 0.70 0.35 0.69 E.2 MULTIPLE ENTITIES To evaluate DeLeaker effectiveness with more than two entities, we tested it on two distinct subsets: one featuring prompts including three distinct animals and another containing prompts of three vegetables or fruits. We compared DeLeaker performance against two prompt-based baselines: the Instruction Prompt and the Entity Description Prompt. The results, summarized in Table 11, clearly show that DeLeaker outperforms both baselines in both the animal and the fruit & vegetable sets. We observed that DeLeaker performance was notably higher on the fruits & vegetables dataset. This is likely because DeLeaker is better equipped to handle the generation of duplicate entities, an issue prominent in that particular subset. Its strength lies in smoothing mechanism across steps and across image tokens, which effectively resolves extra objects that arise from mask duplication. Conversely, the model struggled more with the animal dataset, where the primary challenge was missing entities. DeLeaker is less adept at handling this issue because of its design; it cannot create new mask for an entity if one was not formed in the early stages from the attention maps. Overall, DeLeaker is an effective for scenarios with multiple entities, particularly when correcting for duplicates, but future work could focus on improving its performance in cases where entities are missing. E.2.1 MULTIPLE ENTITIES: ENTITY COUNTS ANALYSIS An entity counts error in image generation happens when the T2I model fails to create the correct number of entities or items specified in the text prompt. For instance, prompt asking for photo of dog and cat might incorrectly generate an image showing for example only one dog or two dogs and cat (Binyamin et al., 2025). This phenomenon signals failure to maintain alignment between text and image. In many cases, we observe that missing or additional entities are related to severe semantic leakage. This can happen when an entity disappears due to leakage, or when two entities fuse into one, creating blended entity with features from both. Alternatively, T2I model can generate an additional entity, which complicates the attention relationships among all entities and increases the chance of semantic leakage. To enrich our analysis, we supplement the main SLIM dataset with an additional set of 222 samples  (Table 12)  . This new subset was specifically filtered to include images with entity count errors, that is,"
        },
        {
            "title": "Under review",
            "content": "Table 11: Results on Animal and Fruits & Veg Triplet Subsets (FLUX): We evaluate leakage mitigation on the triplets subsets across the best performing prompt-based baselines (based on the results on the pair subsets). The main scores represent the percentage of samples labeled as Mitigation (Major/Minor), No Change, or Degradation (Major/Minor), summarized by stacked bar visualization. These are presented alongside Preservation metrics (VQAScore and LPIPS). Arrows (/) indicate the desired direction for improvement for each metric. Subset Model Animal Triplets Fruits & Veg Triplets Instruction Prompt Ent. Desc Prompt DeLeaker Instruction Prompt Ent. Desc Prompt DeLeaker Semantic Leakage Visualization Mitigation Major Minor Degradation No Change Minor Major Preservation VQAScore LPIPS 39.66% 3.45% 38.79% 2.59% 43.97% 7.76% 19.83% 13.79% 19.83% 0.86% 36.21% 6.90% 37.93% 8.62% 19.83% 35.06% 10.34% 15.52% 3.45% 35.63% 51.72% 8.05% 59.20% 6.90% 8.05% 9.77% 7.47% 24.71% 4.02% 20.11% 0.67 0.67 0.70 0. 0.67 0.70 0.45 0.49 0.25 0. 0.46 0.34 where entities are either missing or added relative to the prompt. The counting is done by prompting Gemini 1.5 pro. Our goal is to use this subset to investigate the link between semantic leakage and these counting errors. We achieve this by assessing whether leakage mitigation techniques also correct the number of entities in these images. Table 12: Entity Counts Subset. This table shows the number of images with missing or extra entities. This additional subset contains 222 samples. Group Subset Name Additional Entities (Extra) Missing Pairs Animal Pairs Animal Pairs (Interaction) Animal Pairs (Interaction + Style) Total Pairs = 42 Triplets Animal Triplets Fruit & Vegetable Triplets Total Triplets = 180 5 3 5 13 12 25 37 9 9 29 116 27 143 Based on Table 12, we first observe that entity count errors become more frequent as the number of entities in prompt increases. The FLUX base model exhibits notable bias: it tends to generate fewer animals than requested but adds extra items in the fruit and vegetable subset. We hypothesize this bias originates from the training data, where fruits and vegetables are often depicted in groups, while animals are more commonly shown individually. Figure 15 and Tables 12 and 13 present the results for the entity counts subset, focusing on the pairs subsets and triplets in SLIM, respectively. The results are shown in the form of transitions, tracking the entity count state (missing, same, or extra) from the original image to the candidate image. Figure 15a isolates only the successful transitions (highlighted in green columns of Table 15b, where the model correctly adjusted the number of entities). When analyzing the baselines on images with the successful entity count, layout-based methods show divergent behaviors: RAG-Diffusion tends to omit entities (56% of cases), whereas 3DIS and RPF tend to add extra ones (21% and 14%, respectively). In contrast, DeLeaker is the most stable, preserving the correct number of entities 97% of the time. For cases with missing entities, 3DIS and DeLeaker+Desc are most effective at correcting the error. Conversely, when presented with extra entities, most baselines perform well, successfully omitting the surplus items with success rates ranging from 61% to 100%. Table 13 focuses on the entity count transitions in the Triplet subsets. DeLeaker demonstrates better performance in fixing Missing entity cases than Extra entity cases. We hypothesize that the reason for this is the methods reliance on the generated entity masks; if an entity mask is mistakenly generated, DeLeaker continues to intervene based on this incorrect mask rather than omitting it. This presents an interesting direction for future work."
        },
        {
            "title": "Under review",
            "content": "Focusing on the Missing and Extra columns in both Table 12 and Table 13, we observe that transitions toward the correct entity count are more frequent than transitions that worsen the error. This suggests that semantic leakage mitigation methods generally have positive effect on entity count errors. This finding indicates that semantic leakage is direct cause of entity count issues. (a) Counts Analysis. Entity quantity transitions between original image candidate image. The bar graph presents only the successful transitions (green column of the table below). Baseline RAG RPF 3DIS QwenFLUX Instruction Prompt Ent. Desc Prompt DeLeaker DeLeaker+Desc Original: Same Missing Same Extra Missing Same Extra Missing Same Extra Original: Missing Original: Extra 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 65.52% 34.48% 24.14% 86.21% 51.72% 48.28% 41.38% 27.59% 34.48% 65.52% 75.86% 13.79% 48.28% 51.72% 58.62% 72.41% 56.69% 3.81% 4.77% 11.56% 1.79% 2.26% 0.48% 1.90% 37.55% 82.48% 74.37% 84.62% 97.26% 96.79% 98.81% 97.14% 5.77% 13.71% 20.86% 3.81% 0.95% 0.95% 0.71% 0.95% 100.00% 69.23% 61.54% 84.62% 69.23% 100.00% 84.62% 100.00% 0.00% 30.77% 30.77% 7.69% 30.77% 0.00% 15.38% 0.00% 0.00% 0.00% 7.69% 7.69% 0.00% 0.00% 0.00% 0.00% Correct model behavior per ground-truth; Incorrect. (b) Entity Quantity Transitions: Percentage of Examples per Baseline. Figure 15: Visual and tabular analysis of entity count transitions in pairs subsets. The SLIM distribution is: Same: 839, Missing: 29, Extra: 13. (a) Bar graph summarizing the desired transitions across baselines: Same Same, Missing Extra and Extra Missing. (b) Detailed transition matrix showing the percentage of outcomes (Missing, Same, Extra) for each original state. Table 13: Entity Quantity Transitions for Animal and Fruit & Veg Triplets Subsets: Percentage of Examples per Baseline. Animal Triplets (244 samples: 116 Same, 116 Missing, 12 Extra). Fruits & Veg Triplets: 175 samples: 123 Same, 27 Missing, 25 Extra Subset Baseline Original: Same Missing Same Extra Missing Same Extra Missing Same Extra Original: Missing Original: Extra Animal Triplets Fruit & Veg Triplets Instruction Prompt Ent. Desc Prompt DeLeaker Instruction Prompt Ent. Desc Prompt DeLeaker 0.00% 0.85% 0.00% 0.00% 0.00% 0.00% 37.61% 47.01% 63.25% 87.84% 82.43% 79.73% 62.39% 52.14% 36.75% 12.16% 17.57% 20.27% 15.18% 16.96% 5.36% 6.25% 7.29% 1.04% 76.79% 75.89% 83.93% 87.50% 79.17% 64.58% 8.04% 7.14% 10.71% 6.25% 13.54% 34.38% 73.33% 66.67% 26.67% 58.33% 55.00% 28.33% 20.00% 33.33% 66.67% 18.33% 21.67% 36.67% 6.67% 0.00% 6.67% 23.33% 23.33% 35.00% Correct model behavior per ground-truth; Incorrect."
        },
        {
            "title": "Under review",
            "content": "E.3 ABLATION STUDY: COMPLEMENTARY RESULTS Table 14: Automatic Evaluation Scores of Semantic Leakage Mitigation: Subset Analysis (DeLeaker). The main scores represent the percentage of samples labeled as Mitigation (Major/Minor), No Change, or Degradation (Major/Minor), summarized by stacked bar visualization. Leakage Mitigation (Distribution) Subset Visualization Improvement Major Minor No Change Minor Major Degradation Animal Pairs Animal Interactions Animal Interactions + Style 31.71% 10.67% 54.72% 7.92% 55.87% 10.53% 40.24% 17.36% 14.17% 6.10% 11.28% 6.04% 13.96% 5.26% 14.17% Table 15: DeLeaker Ablation Study. Configurations are divided into two types: (1) W/O rows (top four) represent the removal/addition of specific component, while (2) Only rows (bottom three) isolate each component independently. Absolute scores of are DeLeaker are reported, with values closer to the regular configuration of DeLeaker (first row) indicating similarity. Configuration DeLeaker W/O Image-Image(-) W/O Image-Text(-) W/O Image-Text(+) With Text-Text(-) Only Image-Image(-) Only Image-Text(-) Only Image-Text(+) Visualization Improvement Major Minor No Change Degradation Minor Major 46.07% 9.76% 25.36% 5.83% 12.98% 46.31% 10.12% 42.98% 25.00% 41.79% 11.90% 25.12% 41.55% 7.62% 7.98% 8.93% 5.95% 8.57% 9.64% 26.67% 27.98% 43.93% 27.26% 61.90% 47.62% 31.19% 4.29% 6.07% 7.02% 7.02% 7.86% 5.83% 5.12% 12.62% 15.36% 16.07% 15.00% 12.38% 12.86% 12.50% Table 16: DeLeaker Ablation Study (Relative Change). Configurations are divided into two types: (1) W/O rows (top four) represent the removal/addition of specific component, while (2) Only rows (bottom three) isolate each component independently. Percentage change in semantic leakage mitigation distribution relative to DeLeaker. Positive values indicate improvement over DeLeaker, and negative values indicate degradation. Darker hues indicate stronger effect, color-coded as positive and negative . Relative Change in Leakage Mitigation (% vs. DeLeaker) Configuration Improvement Major Minor No Change Degradation Minor Major DeLeaker W/O Image-Image(-) W/O Image-Text(-) W/O Image-Text(+) With Text-Text(-) Only Image-Image(-) Only Image-Text(-) Only Image-Text(+) +0.52% +3.66% -6.72% -21.95% -45.74% -18.29% -8.54% -9.30% +5.16% +10.33% +73.24% +7.51% -2.75% -26.53% +4.08% +18.35% +20.41% +23.85% +20.41% +15.60% -74.16% -39.02% +144.13% +34.69% -45.48% -12.20% -1.22% -9.82% +87.79% +23.00% 0.00% -12.24% -4.59% -0.92% -3.67%"
        },
        {
            "title": "F EVALUATION AND ANNOTATION PROTOCOLS",
            "content": "F.1 SLIM HUMAN-GUIDED FILTERING: HUMAN ANNOTATION PROTOCOL FOR DETECTING"
        },
        {
            "title": "SEMANTIC LEAKAGE",
            "content": "Human Annotation Protocol Annotation Setup. Each image in our dataset was evaluated independently by the annotators following multi-step process. For each original generated image, the annotators followed these steps: 1. Prompt Review: Read and understand the textual prompt used to generate the image, with special attention to the entities and their intended differences (e.g., horse and zebra). 2. Entity Identification: Identify all relevant entities mentioned in the prompt (e.g., animals, objects, or attributes such as striped or spotted). 3. Reference Collection: Use web-based image search engines (e.g., Google Images, Bing) to collect exemplar images for each entity separately. These serve as grounding references for typical visual features of each entity class. 4. Feature Comparison: Compare the reference exemplars to identify key distinguishing features between the entities (e.g., color, texture, morphology). 5. Image Inspection: Carefully examine the generated image and evaluate the appearance and distinctiveness of each entity. The full process is illustrated below in Figure 17. Labeling Criteria. Each image was assigned binary label indicating the presence (positive) or absence (negative) of semantic leakage, based on the following criteria: Positive Label (Semantic Leakage Present): Entity Indistinguishability: If the entities appear visually indistinct or interchangeable (i.e., they resemble two instances of the same entity class), the image is labeled as containing semantic leakage. Cross-Entity Feature Leakage: If at least one entity visibly incorporates feature that is uniquely associated with the other entity (e.g., the spotted pattern of dalmatian appearing on golden retriever), the image is labeled positive. Hybridization Effects: If the image contains hybrid or fused representation that cannot be clearly attributed to either entity independently, this also qualifies as leakage. Negative Label (No Semantic Leakage): Independent Feature Attribution: Entities are clearly distinguishable and all major features can be unambiguously attributed to the correct referents. Non-Semantic Artifacts: Any visual inconsistency that does not reflect semantic leakage, such as color blending with the background, pixelation, blur, rendering artifacts, or lighting inconsistencies, is not considered leakage and is labeled negative. Partial Occlusion or Simplification: Cases where entities are simplified or partially occluded, but still distinguishable based on remaining cues, are not counted as leakage. Figure 16: Protocol followed by human annotators for assessing semantic leakage in generated images."
        },
        {
            "title": "Under review",
            "content": "Figure 17: Human Annotation Protocol for Detecting Semantic Leakage. The protocol begins with reading the prompt and identifying the entities involved. Annotators then search for reference images of each entity online and visually compare them to identify features uniquely associated with one entity that appear in 1the other. Examples show (A) significant leakage, (B) localized leakage, and (C) clean case (without semantic leakage)."
        },
        {
            "title": "Under review",
            "content": "F.2 HUMAN ASSESSMENT OF MITIGATED SEMANTIC LEAKAGE (AMT) To complement the automatic evaluation and validate its outcomes, we conducted structured human evaluation using the Amazon Mechanical Turk (AMT) platform. The purpose of this evaluation was twofold: to assess model performance based on human judgment, and to verify the accuracy and robustness of the automatic leakage detection. Each questionnaire item included visual figure composed of five elements: two generated images for comparison (the original image suspected of semantic leakage and baseline image), two reference images (one for each entity, generated by the base (uniintervened) model), and the prompt used to generate the images. The reference images and prompt were provided to help annotators accurately identify the entities and distinguish between them, especially in cases where prior familiarity with the entities could not be assumed. This structure also aligns with the inputs used in the automatic evaluation, allowing for consistent comparison between the two protocols. Annotators were presented with two questions per item, one for each entity (see Figure 19). For each question, they were asked: In which image does the [entity] look more typical? The response was given on five-point scale, indicating both the chosen image and the strength of preference, such as Image 1 strongly or Image 2 slightly. The order of the images was randomized in each instance to mitigate positional bias. Figures 18a and 18b show examples of the visual figure used in the task. To ensure annotation quality and consistency, annotators were provided with prerequisite guidelines (Figure 20), examples, and clear definition of typicality. The evaluation covered 60 randomly sampled prompts across six baselines, including DeLeaker. Each item was annotated by three independent raters. This resulted in total of 980 responses. Inter-Annotator-Agreement is moderate, computed for each question (each entity) with an averaged quadratic weighted Fleiss κ of 0.52 (0.497 and 0.541 for the two questions)."
        },
        {
            "title": "Under review",
            "content": "(a) User Study Image Screen: Example 1 (b) User Study Image Screen: Example 2 Figure 18: Two examples of the image screens presented in our AMT user study."
        },
        {
            "title": "Image Comparison Evaluation Task",
            "content": "Definition Typicality: How usual or expected an entity looks in the image. Example: tall giraffe is more typical than short one. Question 1: Entity1 is more typical in: Image 1 (strong preference) Image 1 (slight preference) Equally typical in both images Image 2 (slight preference) Image 2 (strong preference) *Use the reference image, your general knowledge, or an online search. Question 2: Entity2 is more typical in: Image 1 (strong preference) Image 1 (slight preference) Equally typical in both images Image 2 (slight preference) Image 2 (strong preference) *Use the reference image, your general knowledge, or an online search. Note: See the guide for clarifications, examples, and important notes. Figure 19: The questions in the user study annotation task as appear in the AMT interface for human evaluation."
        },
        {
            "title": "Under review",
            "content": "(a) Example guidance for annotators. The guideline here focuses on typical vs. atypical traits. (b) Example guidance for annotators. The guideline here focuses on entity distinction. Figure 20: Guidance materials shown to annotators before the task, including examples of visual differences relevant for typicality and entity differentiation."
        },
        {
            "title": "G SLIM DATASET CURATION",
            "content": "G.1 SLIM COMPLEMENTARY DETAILS Table 17: SLIM Subsets. This table details the curation process and the percentage of images exhibiting semantic leakage for each subset. After human-verified filtering, weighted average of 23% of the initially tested images were found to contain semantic leakage. The SLIM dataset includes 1,130 samples. Please see Table 12 for the entity count additional set. Group Subset Name Initial # Images # Large-Scale Filtering (% of Initial) Final # After Human Filtering (% of LS / % of Initial) Pairs Triplets Animal Pairs Animal Pairs (Interaction) Animal Pairs (Interaction + Style) Animal Triplets [FLUX] Fruit & Vegetable Triplets 1000 1000 500 500 790 (79%) 888 (89%) 828 (83%) 361 (72%) 414 (83%) 328 (42% / 33%) 265 (30% / 27%) 247 (30% / 25%) 116 (32% / 23%) 175 (42% / 35%) Table 18: Subsets onSANA. Human-verified semantic leakage generated with SANA model. This table quantifies the number of images in each subset."
        },
        {
            "title": "Subset Name",
            "content": "Final # Images after Human Filtering"
        },
        {
            "title": "Pairs",
            "content": "Animal Pairs (Classic) Animal Pairs (Interaction) Animal Pairs (Interaction + Style)"
        },
        {
            "title": "Total",
            "content": "91 213 64 368 Table 19: Subsets Prompt Templates and Examples: Each template defines the structure used to generate prompts in subset of SLIM. Description Example Prompt Subset Name Animal Pairs Animal Pairs (Interaction) Animal Pairs (Interaction + Style) Animal Triplets Visually similar pairs (from the same breed / family / share similar traits) Entities perform actions together, share interaction or proximity Entities perform actions together, share interaction or proximity and the same image style Visually similar triplets Fruit-Vegetable Triplets Visually similar triplets cow and horse in farm \"A raccoon is hugging an opossum watercolor painting of raccoon dancing with an opossum raccoon, an opossum, and panda playing together bowl containing strawberry, tomato, and cherry Table 20: Prompts for generating the prompts in SLIM (with GPT-4o). Prompt for Generation From the provided list of animals, please create list of tuples. of animals that have similar visual features. example: (Cow, Horse), (Goat, Sheep) Each tuple should contain pair For {animal1} and {animal2} For each pair you create, write unique, descriptive sentence. The sentence must include both animals from the pair, placing them together in plausible scene. Write unique, descriptive sentence for each pair of animals you create. two animals interacting closely within plausible scene. The sentence must depict the Goal / Subset pairs of similar animals Animal pairs prompt Animal Pair (Interaction) Animal Pair (Interaction) GPT-4o Animals List List of additional animals: Aardvark, Armadillo, Baboon, Beaver, Bongo, Caracal, Cheetah, Chipmunk, Dugong, Elk, Ferret, Gazelle, Giraffe, Guinea pig, Jackal, Llama, Lynx,"
        },
        {
            "title": "Under review",
            "content": "Meerkat, Mink, Mole, Moose, Platypus, Pronghorn, Quokka, Rabbit, Skunk, Sloth, Tapir, Tasmanian devil, Walrus, Weasel, Yak, Albatross, Blue jay, Cassowary, Chickadee, Cockatoo, Cormorant, Crane, Cuckoo, Dove, Egret, Falcon, Finch, Hawk, Heron, Ibis, Kingfisher, Kiwi, Kookaburra, Lark, Macaw, Magpie, Mallard, Nightingale, Osprey, Peacock, Pelican, Pheasant, Quail, Raven, Robin, Roadrunner, Stork, Toucan, Vulture, Warbler, Alligator, Anole, Basilisk, Boa, Bullfrog, Chameleon, Cobra, Crocodile, Frilled lizard, Gecko, Gila monster, Iguana, Komodo dragon, Monitor lizard, Newt, Pit viper, Python, Salamander, Skink, Snapping turtle, Terrapin, Toad, Angelfish, Archerfish, Barracuda, Bass, Blowfish, Carp, Catfish, Clownfish, Coelacanth, Cod, Cuttlefish, Eel, Flounder, Guppy, Halibut, Herring, Lionfish, Manta ray, Marlin, Monkfish, Moray eel, Nautilus, Piranha, Pufferfish, Salmon, Sawfish, Scorpionfish, Sturgeon, Swordfish, Tilapia, Trout, Tuna, Wrasse. Fruits & Vegtabales List Fruits: Banana, Apple, Pear, Grapes, Orange, Kiwi, Watermelon, Pomegranate, Pineapple, Mango. Vegetables: Cucumber, Carrot, Capsicum, Onion, Potato, Lemon, Tomato, Radish, Beetroot, Cabbage, Lettuce, Spinach, Soybean, Cauliflower, Bell Pepper, Chilli Pepper, Turnip, Corn, Sweetcorn, Sweet Potato, Paprika, Jalapeño, Ginger, Garlic, Peas, Eggplant. Style List In 3D render, In Futurism, In Manga style, In Pixar style, In Van Gogh style, In concept art, In cyberpunk aesthetic, In digital painting, In fantasy style, In graffiti style, In minimalist line art, In neon glow effect, In pixel art, In pop art style, In retro poster design, In steampunk illustration, In surrealist painting, In watercolor painting, In an Art Deco style, In an ink sketch, In an oil painting. G.2 AUTOMATIC (NOISY) FILTERING Figure 21: Noisy Automatic Filtering Pipeline Scheme As illustrated in Figure 21, the automatic noisy filtering pipeline is composed of two main stages. First, we generate segmentation masks for each entity and assign them accordingly. Then, we use VLM (Gemini) to detect leakage between entity pairs by prompting it with questions such as: This is an image of cat. Does it contain features of dog? Answer yes/no. In evaluating this pipeline, we observed non-negligible false-positive rate, which motivated the need for second-stage filtering process using human-guided filtering. Pipeline Stages. The first stage involves generating specific mask for each entity in an image. The process begins with Grounding DINO (Ren et al., 2024) to detect entity bounding boxes, which are then refined into segmentation masks using the Segment Anything Model (SAM) (Kirillov et al., 2023). This procedure often produces multiple candidate masks for each entity. Trying to increase the chance of an accurate match, we employ the Hungarian Algorithm. This method optimally assigns masks based on cost matrix derived from CLIP similarity scores (Ramesh et al., 2022), which measure the similarity between each entitys textual token and its candidate masks. This matching step is necessary due to the presence of leakage; when entities share similar features, they may be erroneously classified as the same object during segmentation. After generating the entity masks, the next step is to assess whether leakage has occurred. This is achieved using Gemini, which evaluates each masked entity to determine if its visual features contain distinctive traits of another entity in the image. The model is prompted with query of the form: This is an image of an <opossum>. Does the image contain features unique to <raccoon>? Answer with yes/no only."
        },
        {
            "title": "H REPRODUCIBILITY AND RESOURCES",
            "content": "H.1 BASELINES Table 21: Model Overview: Details of the baselines used in our experiments, including their language models, image generation type, and model size. DiT notes Diffusion Transformer. Model Language Model(s) Image Generator Type DeLeaker FLUX-dev SANA RAG-Diffusion, RPF Qwen2vl-FLUX 3DIS-FLUX T5-XXL, CLIP Gemma T5-XXL Qwen2 T5, CLIP DiT DiT DiT DiT DiT Size 11b 1.6b 11b 7b Table 22: Prompts of the prompt-based baselines. Baseline instruction Entity description Prompt {prompt}. Each entity maintains its distinct characteristics: entity 1 and entity 2. {prompt}. {Entity descriptions}. Imagecondition instruction The image should depict: {prompt}. Make sure there is no visual leakage between the animals, keep the rest of the image as is. {Image}. Comments where descriptions are generated by Gemini 1.5 pro: Describe the visual appearance of a/an entity in one sentence, focusing only on its unique physical features such as face shape, colors, patterns, and body parts. Keep it short and descriptive. {Image} is the the original image by FLUX-dev. Table 23: Technical details for the layout-based baselines. Baseline Layout Prior Strategy Layout Source (Chen RPF et al., 2024a) RAG-Diffusion al., (Chen et 2024b) Bounding boxes Bounding and descriptions boxes 3DIS-FLUX (Zhou et 2024) al., Bounding boxes & depth maps local prompts: the entity in the bounding-box In-bbox attention (referred in the paper as hard binding) + interbbox attention (referred as soft refinement) Depth-map conditioned generation 38 LLM (Gemini 1.5 Flash) GPT-4o (originally) Image Generator FLUX-dev FLUX-dev Stable Diffusion 2.0 (depth map from bbox) (Stan et al., 2023) FLUX1depth-dev (Labs, 2024)"
        },
        {
            "title": "Under review",
            "content": "Table 24: Hyperparameters for the layout-based baselines used in our experiments. Baseline RPF RAG-Diffusion 3DIS Parameter FLUX-dev # steps mask inject steps base ratio FLUX-dev # steps SR sw split ratio HB replace SR delta Hard control steps FLUX-Deph-dev # steps SD # steps Value 20 10 0.4 20 0.5;0.5 2.0 1.0 20 20 30 H.2 AUTOMATIC EVALUATION PROMPTS System Prompt Prompt(s) As an experienced visual inspector, you will analyze images of entities and provide detailed insights on their visual differences and typicality. You are sensitive to fine small details and differences. Comments Defines the overall role of the model as sensitive visual inspector. Step 1.1: Knowledge-Based Extraction Prompt prompt1 = [What are the visual appearance differences between {entity1} and {entity2}? answer in concise comma-separated list. length, head color, eyes shape, etc.] For example neck Step 1.2: Image-Based Extraction Prompt prompt2 = Based on these images, what are the visual appearance differences between {entity1} and {entity2}? independent entities images + Comments Extracts visual differences from the models general knowledge, formatted as simple comma-separated list. This serves as the first source of information. Comments difvisual Identifies ferences directly by analyzing provided images of the two entities. This provides second, evidence-based source of information. 39 Comments Merges the text-based differences (from Step 1.1) and image-based evidence (from Step 1.2) into single, structured, and non-redundant bulleted list."
        },
        {
            "title": "Under review",
            "content": "Step 1.3: Integration Prompt Base Synthesize Start each prompt3 = ( List the key visual differences between {entity1} and {entity2} in bulleted list. your answer on synthesis of the Source 1 and Source 2 descriptions provided below. Each bullet point should concisely compare single visual feature. INSTRUCTIONS: 1. Sources:Integrate the key points from BOTH the Source 1 and Source 2 descriptions. Do not list the same feature twice or create redundant points. 2. Highlight Obvious Differences: If either the Source 1 or Source 2 descriptions explicitly highlight certain features as particularly noticeable or obvious, ensure these differences are prominently featured in your list. 3. Format: bullet point with bolded feature name followed by colon (e.g., Coat:). Content Structure:After the feature name, first write {entity1}:followed by its description. Then, on the same line, write {entity2}:followed by its description. Keep descriptions brief. 5. Focus: The list must only contain observable, visual differences.EXAMPLES: Tail Feathers: Peacock: iridescent. Peahen: Short and brown. Coat: Zebra: Black and white stripes. Facial Horse: Solid or patched color. Markings: Red Panda: on muzzle and eyes. Raccoon: mask across eyes. Leg Color: Pink. American Coot: Grey or black. Covering: Chicken: Feathers. Fur. Tail: Red Squirrel: bushy. Pig: Short and hairless. DESCRIPTIONS FOR {entity1} AND {entity2}: Source 1: {step1 prompt1 output}. Source 2: {step1 prompt2 output} ) White patches Black Flamingo: Cat: Long and Long and 4."
        },
        {
            "title": "Under review",
            "content": "Step 2: Typicality (for entity in entities) Prompt(s) prompt4 = (Given the differences between {text_entities}, How visually typical {entity} in this image? out-of-frame features.)) (Ignore + clean image prompt5 = (Given the differences between {text_entities}, How visually typical {entity} in this image? out-of-frame features.)) image + baseline Ignore Comments Evaluates how typical each entity appears in the clean vs. baseline image, ignoring out-offrame features. Used per entity. Step 3: Ranking (images random order) Prompt(s) Comments prompt6 = (Given the independent textual typicality inspection of each animal in each image and the images, overall, how visually typical the {text_entities} in the second image rather in the first image? (Ignore Notice that out-of-frame features.) both of the animals should appear in each image. If one image shows both animals, even if one looks unusual, it will be preferred over an image where First one of the animals is missing. explain, think step by step. Finally, rank the overall relative typicality: Rank: 1min (first image with minor prefrence), 1maj (first image with major prefrence), 2min (second image with minor prefrence), 2maj (second image with major prefrence), or 3 (equally typical in both). Inspection: {prompt 4 answer for entity 1 and entity 2} Second Image Inspection: {prompt 5 answer for entity 1 and entity 2} First Image: {clean image}, Second Image: {baseline image} ) First Image"
        }
    ],
    "affiliations": [
        "Technion",
        "Tel-Aviv University"
    ]
}