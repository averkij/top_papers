{
    "paper_title": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning",
    "authors": [
        "Yuanhang Li",
        "Yiren Song",
        "Junzhe Bai",
        "Xinran Liang",
        "Hu Yang",
        "Libiao Jin",
        "Qi Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We propose \\textbf{IC-Effect}, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (\\eg flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satisfy these requirements. IC-Effect leverages the source video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. A two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via Effect-LoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release a paired VFX editing dataset spanning $15$ high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation."
        },
        {
            "title": "Start",
            "content": "IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning Yuanhang Li1, Yiren Song2, Junzhe Bai1, Xinran Liang1, Hu Yang3, Libiao Jin1, Qi Mao1(cid:66) 1 School of Information and Communication Engineering, Communication University of China 2 Show Lab, National University of Singapore 3 Baidu Inc., Beijing, China https://cuc-mipg.github.io/IC-Effect/ 5 2 0 D 7 1 ] . [ 1 5 3 6 5 1 . 2 1 5 2 : r Figure 1. Video VFX Editing Results of IC-Effect. Our IC-Effect enables precise video VFX editing aligned with textual instructions while preserving the complete spatiotemporal information of the source video. The complete video is available in our supplementary materials."
        },
        {
            "title": "Abstract",
            "content": "We propose IC-Effect, an instruction-guided, DiT-based framework for few-shot video VFX editing that synthesizes complex effects (e.g., flames, particles and cartoon characters) while strictly preserving spatial and temporal consistency. Video VFX editing is highly challenging because injected effects must blend seamlessly with the background, the background must remain entirely unchanged, and effect patterns must be learned efficiently from limited paired data. However, existing video editing models fail to satIC-Effect leverages the source isfy these requirements. video as clean contextual conditions, exploiting the contextual learning capability of DiT models to achieve precise background preservation and natural effect injection. two-stage training strategy, consisting of general editing adaptation followed by effect-specific learning via EffectLoRA, ensures strong instruction following and robust effect modeling. To further improve efficiency, we introduce spatiotemporal sparse tokenization, enabling high fidelity with substantially reduced computation. We also release (cid:66) Corresponding Author: qimao@cuc.edu.cn paired VFX editing dataset spanning 15 high-quality visual styles. Extensive experiments show that IC-Effect delivers high-quality, controllable, and temporally consistent VFX editing, opening new possibilities for video creation. 1. Introduction Visual Effects (VFX) aim to create videos or edit existing ones by incorporating visually compelling elements such as flames, cartoon characters, or particle effects. As core technology in filmmaking, gaming, and virtual reality, VFX enrich visual storytelling, highlighting key elements, and creating immersive experiences. However, traditional video VFX workflows rely on complex animation design, computer-generated imagery (CGI), and professional postproduction compositing. These processes incur high production costs, long turnaround times, and extensive manual intervention, which hinder personalized or real-time applications. Recent advances in text-to-video (T2V) generation [27, 52, 57] open new possibilities for automated VFX creation [33, 38]. However, video VFX editing, which automatically adds or modifies effects in an existing video, remains largely unexplored. As unique and higher-level video editing task, video VFX editing fundamentally differs from video VFX generation [33, 38]. Its core objective is to seamlessly and realistically integrate visual effects into source video while strictly preserving the spatial structure and temporal coherence of the original content. Although recent video editing models [5, 12, 14, 25, 50, 56] achieve significant progress across various editing tasks, they still struggle to meet the stringent requirements of video VFX editing. Existing methods [12, 25, 50, 56] support global style changes or local content modification, but they often tolerate certain degrees of background or appearance changes, making it difficult to ensure pixel-level consistency with the source video. This limitation is unacceptable in video VFX editing, where the background must remain entirely unchanged. Mask-based editing methods [5, 14] can preserve the unmasked regions, but they rely on pixel-accurate masks, which fundamentally conflicts with the goal of automated video VFX editing. Furthermore, unlike general video editing approaches that improve performance by leveraging large-scale data, producing high-quality paired VFX data is challenging, limiting the scalability of model training. Effective video VFX editing must learn the unique patterns of effect injection from these high-quality paired samples to achieve physically consistent integration of effects into real scenes. These challenges make automated video VFX editing largely unresolved problem. To address above challenges, we propose IC-Effect, an instruction-guided, few-shot video VFX editing framework based on DiT-based T2V models. IE-Effect leverages the contextual learning capability [10, 21, 23, 62] of the DiT [40] model, treating the source video as clean contextual condition to provide the model with distortion-free information. This enables the model to inject visual effects naturally while strictly preserving the original background content. The framework adopts two-stage training strategy: pretrained DiT-based T2V model is first adapted into universal video editor, then applying Effect-LoRA to extract effect-specific patterns small set of paired VFX data for style customization while preserving the base model. To improve efficiency, we introduce spatiotemporal sparse tokenization with position correction to preserve key features, reduce computation, and maintain accurate alignment between conditional tokens and generated frames for precise video VFX editing. To mitigate data scarcity, we construct dedicated dataset for video VFX editing that includes 15 representative effect typessuch as flames, anime clones, light particles, and bouncing. Each sample contains triplet annothe source video, the edited video with the target tation: effect, and the corresponding textual description with spatiotemporal annotations. All samples are carefully aligned in viewpoint, content, and motion to ensure reliable supervision for both training and evaluation. To our knowledge, this is the first large-scale paired benchmark specifically designed for video VFX editing, filling crucial gap in data resources and providing reproducible platform for future research. In summary, our contributions are as follows: We propose IC-Effect, instruction-guided video VFX editing framework built on DiT, achieving realistic effects while preserving background and temporal consistency. the first We leverage DiTs contextual learning with source video as clean condition, introducing spatiotemporal sparse tokenization and position correction for efficient and precise video VFX editing. We construct the first high-quality paired video VFX dataset and demonstrating IC-Effects effectiveness and superiority through extensive qualitative and quantitative experiments. 2. Related Work 2.1. Diffusion-Based Video Generation and Editing The success of diffusion models in image generation [18, 39, 44, 60, 61, 63] drives their application to Video tasks [8, 17, 27, 30, 52, 53, 55, 57]. Early approaches [17, 55] extended image diffusion models by integrating temporal modules to produce short clips, while subsequent works [8, 53] design specialized spatiotemporal architectures to enhance motion consistency. However, the U-Net backbone struggles with long-term dynamics. This limitation has been largely overcome by DiT-based models like Sora [6], CogVideoX [57], HunyuanVideo [27], and Wan [52], which treat videos as unified spatiotemporal token sequences and employ 3D full attention to capture complex dependencies, significantly enhancing generation quality, smoothness, and semantic fidelity. Concurrently, diffusion-based video editing models [15, 25, 28, 32, 4547, 50, 58] are also widely explored. Initial strategies [15, 28, 32] inject source frames during inference but often suffer from flickering or artifacts. More stable alternatives [16, 20, 3437] encode structural priorssuch as depth or optical flowvia auxiliary modules. Recent DiTbased image and video editing models [22, 25, 48, 50, 58] enable diverse editing by fusing source frames, reference conditions, and latent tokens at the token level. Yet, they fall short in video effects editing due to the abstract nature of visual effects and scarce paired training data. To bridge this gap, we propose novel framework for text-guided video effects editing that synthesizes dynamic, semantically coherent effects while maintaining original content integrity and motion consistency. 2 Figure 2. The overall architecture and training paradigm of IC-Effect. Given source video VS, IC-Effect first tokenizes it into spatiotemporal sparse tokens and ZI . These tokens are concatenated with noisy target tokens ZT along the token dimension to form unified sequence, which is fed into DiT module equipped with causal attention. At the output, and ZI are discarded, and only the target tokens ZT are decoded by VAE to produce the edited video. During training, we first fine-tune the model with high-rank LoRA to acquire general video editing and instruction-following capabilities, and then further fine-tune it with low-rank LoRA on small set of paired visual effects data to accurately capture the stylistic characteristics of diverse effects. 2.2. Visual Effect Generation VFX creation aims to generate or edit images and videos to present hand-drawn, cartoon, or other creative artistic styles. In recent years, the development of artificial intelligence drives the partial automation of the VFX production pipeline [4, 59], significantly improving content creation efficiency. In the image domain, PhotoDoodle [23] first introduces text-guided automatic image VFX editing method that adds semantically consistent visual elements to static images according to textual prompts. In the video domain, works such as OmniEffects [38] and VFX-Creator [33] achieve text-guided video VFX generation. However, these methods mainly focus on synthesizing stylized video clips from scratch rather than performing precise and controllable dynamic VFX editing on existing videos. This paper aims to fill this gap by enabling creative visual style fusion based on textual descriptions, while preserving the structural integrity and motion coherence of the original video. 2.3. Vision In-context Learning In-context learning (ICL) enables models to rapidly adapt to new tasks with only few examples and is widely adopted in large language models [7]. Inspired by this, recent work explore the conditioning capability of DiTs, revealing their strong potential across various vision tasks [10, 21, 23, 25, 62]. Existing methods [10, 21, 23, 62] typically introduce additional condition tokens, concatenate them with latent noise sequences, and jointly model them through 3D full attention to implicitly learn condition-content correlations. Recently, this framework is extended to the more challenging video generation domain [9, 25, 26, 50], achieving remarkable progress. For example, FullDiT [26] achieves highly controllable text-to-video generation by integrating multimodal conditions like depth maps and camera trajectories. However, since DiTs computational cost grows quadratically with token count, directly concatenating long conditional token incurs substantial overhead, limiting the efficiency of high-resolution or long-duration video generation. In contrast, our method introduces spatiotemporally sparse condition tokens, fully exploiting DiTs contextual understanding with minimal extra computation. 3. Method In this section, we introduce the overall architecture of IC-Effect in Section 3.1. Next, we detail its key innovations: in-context conditioning for video VFX editing (Section 3.2), Effect-LoRA for efficient effect learning (Section 3.3), and spatiotemporal sparse tokenization for reduced computational cost (Section 3.4). Finally, we describe the proposed special-effects dataset in Section B. 3.1. Overall Architecture The overall architecture of IC-Effect illustrated in Fig. 2. We employ two-stage training strategy: Pretraining Video-Editor. We first pretrain video editor on large-scale video editing dataset. This stage enables the model to accurately understand and respond to the text instruction, achieves controllable editing behavior, and establish strong foundation for subsequent effect generation. Video Effect Fine-Tuning. After pretraining, we introduce an Effect-LoRA module using Low-Rank Adaptation (LoRA) [19] and fine-tune it on small set of paired video effect data. This stage focuses on learning the visual elements and stylistic characteristics of specific video effects, enabling precise modeling across different effect types. The proposed design supports efficient and lightweight cus3 tomization, meeting the needs of diverse and personalized video effect generation. 3.2. In-Context Conditioning for Video Editing Given source video VS and its corresponding text instruction TE, we aim to perform video VFX editing that aligns with the textual description while preserving the source videos spatial layout and temporal coherence. Preserving the spatial layout and temporal coherence of the source video is essential for achieving faithful and artifactfree video VFX editing. To achieve this, we leverage the strong contextual modeling capability of the DiT-based T2V model and reformulate video VFX editing as conditional generation problem, while minimizing modifications to the pretrained DiT architecture. Specifically, we encode the source and edited video into latent representations ZS and ZT through 3D VAE. Both representations share the same 3D rotary positional embeddings, which help the model capture stable relative spatiotemporal relationships during contextual modeling. ZS and ZT are patchified and concatenated into unified token sequence, which is processed by the self-attention mechanism within the DiT blocks, MMA ([ZT ; ZS]) = softmax (cid:19) (cid:18) QK V, (1) where ZT denotes the noisy latent tokens and ZS represents clean conditional tokens. By using clean conditional tokens, the method preserves both the spatial structure and temporal motion information of the source video, providing faithful source information to ZT and preventing degradation in video quality during iterative denoising. Additionally, the attention mechanism allows the model to copy from the clean tokens or generate new content according to instructions, ensuring the edited video retains the source background information. During training, we apply the flow matching loss [13] only to the latent tokens to guide the model in learning high-quality and structurally consistent video VFX editing. Causal Attention. Within the bidirectional attention of DiT, interactions between latent noise and conditional tokens can cause clean conditional tokens to be fused with noisy representations, thereby degrading the quality of the generated results. To prevent this, we introduce causal attention mechanism. In this mechanism, noise tokens attend to both themselves and clean conditional tokens, while conditional tokens attend only to themselves, avoiding any interaction with noise tokens. We realize this causal attention using specifically designed attention mask: (cid:40) Mi,j = , 0, if / ZT and ZT , otherwise. (2) This causal attention effectively isolates clean conditional tokens from latent noise, preserving their fidelity and ensuring high-quality generation. 3.3. Effect-LoRA LoRA [19] enables efficient fine-tuning of large-scale pretrained models by introducing trainable low-rank matrices while keeping the original weights frozen. Given pretrained weight matrix W0 Rmn, LoRA introduces two low-rank matrices Rmr (r m) and Rrn (r n), where min(m, n), to model the parameter update: = W0 + AB. (3) This formulation significantly reduces the number of trainable parameters while maintaining performance comparable to full-model fine-tuning. To effectively learn the editing pattern of specific video effect from limited paired VFX data, we propose EffectLoRA. Effect-LoRA fine-tunes only small number of trainable parameters, enabling efficient learning of video effect editing patterns while significantly reducing the risk of overfitting. In our IC-Effect framework, the Video-Editor is trained on large-scale paired dataset with high-rank LoRA, which equips the model with strong instructionIn contrast, Effect-LoRA adopts following capability. low-rank LoRA specifically designed to capture the editing style of single video effect. By guiding the behavior of the Video-Editor, Effect-LoRA enables the model to generate results that exhibit the desired effect style. When new source video VS and corresponding text instruction are provied, the model produces target video VT that exhibits the visual effects described by the instruction. 3.4. Spatiotemporal Sparse Tokenization The T2V model built upon the DiT [40] architecture mainly relies on attention mechanisms, whose computational complexity is proportional to the square of the token length. Consequently, directly using the source video with the same resolution as the edited output as the contextual condition results in excessively long input tokens, thereby significantly increasing computational cost. To overcome this limitation, we introduce spatiotemporal sparse tokenization (STST) strategy that converts the source video VS into set of spatiotemporally sparse tokens, effectively reducing the number of conditional tokens processed by the T2V model and improving inference efficiency. Given source video VS, we do not directly convert it into latent tokens. Instead, we encode downsampled version of the video along with its first frame into latent representations and ZI using 3D VAE. These representations are then patchified into temporally sparse tokens and spatially sparse tokens, respectively. Afterwards, S, ZI 4 and ZT are concatenated along the token dimension and fed into the DiT-based text-to-video (T2V) model. Formally, the transformation in Eq.(1) is expressed as follows:"
        },
        {
            "title": "MMA",
            "content": "(cid:16)(cid:104) ZT ; S; ZI (cid:105)(cid:17) = softmax (cid:19) (cid:18) QK V. (4) To efficiently capture the spatiotemporal information from the source video, we sparsify it along both temporal and spatial dimensions. The temporally sparse tokens provide essential motion information, while the spatially sparse tokens supply complementary fine-grained spatial details. This spatiotemporal sparse tokenization achieves balance between computational efficiency and visual fidelity, preserving the complete spatiotemporal characteristics of the source video. It provides reliable guidance for editing without incurring the high computational cost associated with tokenizing the full-resolution video. Furthermore, Eq.(2) is reformulated as follows: Mi,j = if ZT , if (i, j) 0, 0, , otherwise. or (i, j) ZI , (5) Spatiotemporal Position Correction. Although spatiotemporal sparse tokens significantly improve computational efficiency, they introduce critical issue: spatiotemporal misalignment between the sparse conditional tokens and the target generation space. This misalignment often leads to structural inconsistencies between the edited video and the source video, thereby degrading overall model performance. To address this problem, inspired by OmniControl2 [49], we propose spatiotemporal position correction technique. For temporal sparse tokens S, we establish explicit correspondences between the temporal sparse tokens and the target regions to prevent spatial misalignment: PZ = PZT (n i, j) . (6) For spatially sparse tokens ZI , we use the positional encoding of the first frame of the noisy latent tokens ZT to avoid temporal misalignment. The spatiotemporal position correction technique is crucial, as it ensures that the spatiotemporally sparse tokens provide appropriate temporal and spatial information for video editing. 3.5. Video Effects Dataset We construct the first VideoVFX dataset for text-guided video effect editing, covering 15 high-quality effect categories with over 20 curated videos. The main categories include dynamic particle dispersion, particle dissipation and reassembly, line traversal, subject cartoon mirroring, flame 5 combustion, graffiti, and architectural bouncing. Each sample contains source video VSfrom real-world scenes to portraitsand target video VT with artist-designed effects exhibiting precise spatiotemporal coherence. Effects include local stylization, animated overlays, motionenhanced transitions, new dynamic elements, and structural modifications. For each pair, we provide VS, VT , and text instruction, enabling supervised learning for instructionfollowing video editing models. 4. Experiment 4.1. Experimental Setup Implementation Details. In the pretraining stage of VideoEditor, we initialize the DiT architecture with the parameters of Wan 2.2-A14B-T2V [52] and train it on selfconstructed video editing dataset. All videos are resized to 224 416 with 81 frames. The training uses four A800 GPUs with batch size of 2, learning rate of 1104, and LoRA rank of 96 for 50, 000 optimization steps. The resulting LoRA weights are merged into the base T2V model to form Video-Editor, which serves as the backbone for subsequent tasks. In the Effect-LoRA training stage, we further fine-tune Video-Editor on the paired video VFX editing dataset using two A800 HPUs for 1, 000 steps, with LoRA rank of 32, batch size of 2, and learning rate of 1 104. During inference, the model generates edited videos at resolution of 480 832 with 81 frames. Baseline Methods. To evaluate the performance of our method, we compare it with several open-source approaches, including InsV2V [12], InsViE [56], VACE [25], and Lucy Edit [50]. For fair comparison, we conduct experiments under two scenarios: common video editing and In the Video VFX editing scenario, video VFX editing. we fine-tune the attention layers of the above models using the same VFX dataset. Finally, we compare all the trained LoRA-based models with our IC-Effect framework. Benchmarks. We collect total of 80 video samples from the DAVIS [41] dataset and the Internet to evaluate the performance of the proposed Video-Editor. For the customized video effect editing task, we further construct new benchmark dataset, entirely sourced from the Internet, consisting of 50 high-quality videos that cover both single-subject and multi-subject scenes. The subjects include humans, animals, buildings, and vehicles. This benchmark aims to comprehensively assess the models capability in customized video effect editing across diverse and realistic scenarios. Evalution Metrics. To evaluate the effectiveness of the proposed method, we adopt the following automatic evaluation metrics that analyze performance from multiple perspectives: 1)Video Quality. We employ CLIP [42] Image Similarity (CLIP-I) to measure temporal consistency by computing the cosine similarity between consecutive Figure 3. Video VFX Editing Results of IC-Effect. IC-Effect accurately edits the source video following textual instructions, applying the visual effect styles present in the VFX data. The complete video is available in our supplementary materials. frames of the edited videos using the CLIP image encoder. 2)Semantic Alignment. To assess the semantic consistency between the edited results and the text prompts, we use both the CLIP [42] encoder and the ViCLIP [54] encoder to calculate frame-level and video-level similarities between the edited videos and the corresponding text prompts. 3)Overall Quality. We adopt multiple sub-metrics from the VBench [24] toolkit, including Smoothness, Dynamic Degree, and Aesthetic Quality, to comprehensively evaluate the visual fidelity and naturalness of the edited videos. To further evaluate the structural preservation and effectiveness of the edited results, We further employ GPT-4o [1] to evaluate edited results along two dimensions: structural preservation, measuring spatialtemporal consistency with the source video, and effect accuracy, assessing alignment with the intended target (reference effect or textual prompt). 4.2. Video VFX Editing Results Fig. 3 presents the results of IC-Effect in video VFX editing. IC-Effect accurately injects visual effects into the source video according to the given instruction while maintaining consistency between the generated effect styles and those in the training data, demonstrating excellent instructionfollowing ability. This performance benefits from training on high-quality paired data, which enables the model to learn robust effect generation and background preservation capabilities. When fine-tuned with Effect-LoRA on limited dataset of paired effect data, IC-Effect continues to generate the specified effects stably and strictly preserves the spatiotemporal consistency of non-effect regions, avoiding color drift and structural distortion. Notably, IC-Effect maintains stable performance and high success rate under different settings, exhibiting strong generalization and controllability, and achieves consistent editing results in production environments without selective sampling. 4.3. Comparison with Baselines Qualitative Comparison. Fig. 4 present qualitative comparisons of the proposed method on general video editing and customized video effect editing tasks. As shown in Fig. 4, compared with state-of-the-art video editing methods, Video-Editor demonstrates stronger instructionfollowing capability, accurately performing the modifications described in the textual prompts while effectively preserving the structural and content consistency of non-edited regions. This advantage mainly stems from the use of our carefully constructed high-quality training dataset and the task-oriented model design. In the customized video VFX editing task  (Fig. 4)  , our method significantly outperforms the baselines. The generated videos exhibit higher visual quality, with added effects that align closely with the original ones in terms of style and motion dynamics, faithfully adhering to the textual instructions while minimizing content drift and unintended modifications to non-target areas. These results demonstrate that our method achieves wellbalanced trade-off between visual coherence and precise, controllable editing, substantially enhancing the reliability and naturalness of complex video effect editing. 6 Table 1. Quantitative Comparison of Common Video Editing and Video VFX Editing. The best and second-best values are highlighted in blue and green , respectively. Method/Metrics m F InsV2V [12] InsViE [56] VACE [25] Lucy Edit [50] Ours InsV2V [12] InsViE [56] VACE [25] Lucy Edit [50] Ours Video Quality Semantic Alignment Overall Quality GPT Score CLIP-I () CLIP-T () ViCLIP-T () Smoothness () Dynamic Degree () Aesthetic Quality () Structural Preservation () Effect Accuracy () 0.9750 0.9686 0.9698 0.9721 0.9795 0.9767 0.9770 0.9782 0.9784 0.9786 22.9317 23.0331 23.4918 22.8130 25.6774 25.6705 25.6530 25.5283 27.0618 27.2321 21.7245 20.5973 21.4685 21.1405 24. 23.8159 23.7950 24.5594 26.2143 26.6312 0.9798 0.9695 0.9803 0.9811 0.9815 0.9882 0.9858 0.9897 0.9894 0.9911 0.6147 0.5856 0.4820 0.5737 0.6374 0.2019 0.3258 0.2788 0.3653 0.3771 0.4723 0.4877 0.5190 0.5071 0. 0.5612 0.5158 0.5401 0.5546 0.5823 3.9006 3.6468 3.8562 4.0529 4.3824 3.9326 4.1012 4.2105 4.3461 4.7947 2.4452 2.3687 2.6218 3.1218 4.2652 1.9807 2.8988 3.6988 3.9423 4.5614 Figure 4. Qualitative Comparison of Video Editing. IC-Effect demonstrates strong performance in instruction following, spatiotemporal consistency, and editing quality. The complete video is available in our supplementary materials. Quantitative Comparison. As shown in Table 1, our method outperforms all baseline approaches across all evaluation metrics on the general video editing task, achieving the best overall objective performance. This demonstrates that Video-Editor possesses stronger comprehensive capabilities in terms of temporal consistency, semantic alignment, and overall visual quality. For the video effect editing task, our method also achieves the best metrics. Notably, GPT-4o significantly outperforms baseline methods in both structure preservation and effect accuracy. The significant improvements in these metrics further validate the superiority of the proposed method in editing precision and semantic consistency, highlighting its enhanced controllability and generation quality in complex, customized video editing scenarios. 4.4. User Study To further validate the effectiveness of the proposed method, we conduct user study through an online questionnaire to evaluate user preferences in two scenarios: conventional video editing and customized video effect editing. Each participant is presented with text instruction, source video, and two edited results respectively generated by our method and baseline. The two results are shown in randomized order to prevent participants from inferring their sources and to minimize subjective bias. The study 7 Table 2. Quantitative Comparison of Ablation Studies. The best and second-best values are highlighted in blue and green , respectively. Method/Metrics W/o STST w/o ZI w/o Pretrain w/o Effect-LoRA Ours Video Quality Semantic Alignment Overall Quality GPT Score Inference overhead CLIP-I () CLIP-T () ViCLIP-T () Smoothness () Dynamic Degree () Aesthetic Quality () Structural preservation () Effect accuracy () Time GPU memory 0.9792 0.9367 0.9224 0.9749 0. 27.5514 20.9034 26.7977 25.0413 27.2321 26.3071 21.4638 25.8940 23.4875 26.6312 0.9885 0.9657 0.9886 0.9802 0.9911 0.3806 0.8163 0.3762 0.3709 0.3771 0.5940 0.3805 0.5395 0.5395 0.5823 4.8375 4.0123 4.2471 4.1428 4. 4.5975 3.6140 3.9134 3.9423 4.5614 5880s 2617s 2790s 2790s 2790s 74.71GB 63.74GB 64.07GB 64.07GB 64.07GB Figure 5. Human Preference Study. Our method is significantly more preferred by users compared with the comparative methods. follows an A/B testing protocol, where participants evaluate the results from three aspects: (1) instruction following, (2) consistency between the edited video and the source video (i.e., structual fidelity) and (3) Overall preference. total of 20 participants are recruited for the study. As shown in Fig. 5, the statistical results demonstrate that, compared with baseline methods, our approach achieves higher user preference in both instruction adherence and source-video fidelity, verifying its effectiveness and advantage in realworld applications. 4.5. Ablation Studies To evaluate the effectiveness of the proposed strategies and modules, we conduct detailed ablation experiments, including the spatiotemporal sparse tokenization strategy, Video-Editor pretraining, and Effect-LoRA. As shown in the Fig. 6, our method maintains editing quality comparable to the fully tokenized model, while various metrics in Table 2 also approach those of the fully tokenized model, and the computational cost is significantly reduced. Without using high-quality first frame as the conditional input, relying solely on temporally sparse tokens fails to provide sufficient spatial details, resulting in noticeable blotchy artifacts and blurring, which is also reflected by the lowest metrics in Table 2. In contrast, our spatiotemporal sparse tokenization strategy introduces minimal additional computation while substantially improving visual fidelity and spatiotemporal consistency, as also confirmed by the qualitative results in Table 2. Furthermore, skipping the pretrained Video-Editor module and directly training Effect-LoRA causes the generated video to apply particle dispersion effects to all subjects, instead of following the textual instructions to apply them only to the white cake. Conversely, using only the pretrained Video-Editor without Effect-LoRA fails to produce customized VFX that align with user intent. In contrast, the Figure 6. Ablation Study of the Proposed Approach. complete method efficiently generates video effects that are highly consistent with the text prompts while faithfully preserving the original video content. As illustrated in Table 2, the performance of the full method across all metrics further confirms these observations. 5. Conclusion In this work, we propose IC-Effect, an instruction-guided few-shot video effect editing framework that learns unique visual styles from minimal number of paired effect samples. By leveraging the intrinsic contextual learning of the DiT architecture, our method performs video VFX editing that strictly follows textual instructions while preserving background consistency. The two-stage training strategy ensures both instruction adherence and the learning of specific effect styles. Additionally, our spatiotemporal sparse tokenization mechanism preserves full spatiotemporal information from the source video while significantly reducing computational cost. We also introduce new dataset with 15 types of video effects, providing valuable benchmark for future research. Extensive experiments show that IC-Effect consistently outperforms existing methods in both general video editing and video VFX editing, demonstrating superior effect fidelity and background consistency."
        },
        {
            "title": "References",
            "content": "[1] Gpt-4o. Accessed May 13, 2024 [Online] https:// openai.com/index/hello-gpt-4o/. 6, 1, 2 [2] Pexels. https://www.pexels.com/, 2025. 2 [3] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 2 [4] Alla Belova. Google doodles as multimodal storytelling. Cognition, communication, discourse, (23):1329, 2021. 3 [5] Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, and Qiang Xu. Videopainter: Anylength video inpainting and editing with plug-and-play context control. In SIGGRAPH, pages 112, 2025. [6] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. 2 [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, and et al. Askell, Amanda. Language models are few-shot learners. In NeurIPS, pages 18771901, 2020. 3 [8] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In CVPR, pages 73107320, 2024. 2 [9] Lan Chen, Yuchao Gu, and Qi Mao. Univid: Unifying vision tasks with pre-trained video generation models. arXiv preprint arXiv:2509.21760, 2025. 3 [10] Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations. arXiv preprint arXiv:2503.13327, 2025. 2, 3 [11] Sili Chen, Hengkai Guo, Shengnan Zhu, Feihu Zhang, Zilong Huang, Jiashi Feng, and Bingyi Kang. Video depth anything: Consistent depth estimation for super-long videos. In CVPR, pages 2283122840, 2025. [12] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoto-video transfer using synthetic dataset. In ICLR, 2024. 2, 5, 7, 1 [13] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. 4 [14] Chenjian Gao, Lihe Ding, Xin Cai, Zhanpeng Huang, Zibin Wang, and Tianfan Xue. Lora-edit: Controllable first-frameguided video editing via mask-aware lora fine-tuning. arXiv preprint arXiv:2506.10082, 2025. 2 [15] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. arXiv preprint arXiv:2307.10373, 2023. 2 Mike Zheng Shou, and Kevin Tang. Videoswap: Customized video subject swapping with interactive semantic point correspondence. In CVPR, pages 76217630, 2024. 2 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2 [19] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3, 4 [20] Zhihao Hu and Dong Xu. Videocontrolnet: motion-guided video-to-video translation framework by using diffusion model with controlnet. arXiv preprint arXiv:2307.14073, 2023. 2 [21] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. 2, 3 [22] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, and Jiaming Liu. Arteditor: Learning customized instructional image editor from few-shot examples. In ICCV, pages 1765117662, 2025. 2 [23] Shijie Huang, Yiren Song, Yuxuan Zhang, Hailong Guo, Xueyin Wang, Mike Zheng Shou, and Jiaming Liu. Photodoodle: Learning artistic image editing from few-shot pairwise data. arXiv preprint arXiv:2502.14397, 2025. 2, 3 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. [25] Zeyinzi Jiang, Zhen Han, Chaojie Mao, Jingfeng Zhang, Yulin Pan, and Yu Liu. Vace: All-in-one video creation and editing. arXiv preprint arXiv:2503.07598, 2025. 2, 3, 5, 7, 1 [26] Xuan Ju, Weicai Ye, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, and Qiang Xu. Fulldit: Video generative foundation models with multiIn ICCV, pages 15737 modal control via full attention. 15747, 2025. 3 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1, 2 [28] Max Ku, Cong Wei, Weiming Ren, Huan Yang, and Wenhu Chen. Anyv2v: tuning-free framework for any video-tovideo editing tasks. TMLR, 2024. 2 [29] Xiaowen Li, Haolan Xue, Peiran Ren, and Liefeng Bo. Diffueraser: diffusion model for video inpainting. arXiv preprint arXiv:2501.10018, 2025. 2 [16] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, [30] Yuanhang Li, Qi Mao, Lan Chen, Zhen Fang, Lei Tian, Xinyan Xiao, Libiao Jin, and Hua Wu. Starvid: Enhanc9 ing semantic alignment in video diffusion models via spatial and syntactic guided attention refocusing. arXiv preprint arXiv:2409.15259, 2024. 2 [31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In ECCV, pages 3855. Springer, 2024. 2 [32] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, pages 85998608, 2024. 2 [33] Xinyu Liu, Ailing Zeng, Wei Xue, Harry Yang, Wenhan Luo, Qifeng Liu, and Yike Guo. Vfx creator: Animated visual effect generation with controllable diffusion transformer. arXiv preprint arXiv:2502.05979, 2025. 1, 2, 3 [34] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Xiu Li, and Qifeng Chen. Follow your pose: Poseguided text-to-video generation using pose-free videos. In AAAI, pages 41174125, 2024. [35] Yue Ma, Hongyu Liu, Hongfa Wang, Heng Pan, Yingqing He, Junkun Yuan, Ailing Zeng, Chengfei Cai, Heung-Yeung Shum, Wei Liu, et al. Follow-your-emoji: Fine-controllable and expressive freestyle portrait animation. In SIGGRAPH Asia 2024 Conference Papers, pages 112, 2024. [36] Yue Ma, Yingqing He, Hongfa Wang, Andong Wang, Leqi Shen, Chenyang Qi, Jixuan Ying, Chengfei Cai, Zhifeng Li, Heung-Yeung Shum, et al. Follow-your-click: Open-domain In AAAI, regional image animation via motion prompts. pages 60186026, 2025. [37] Yue Ma, Yulong Liu, Qiyuan Zhu, Ayden Yang, Kunyu Feng, Xinhua Zhang, Zhifeng Li, Sirui Han, Chenyang Qi, and Qifeng Chen. Follow-your-motion: Video motion transfer via efficient spatial-temporal decoupled finetuning. arXiv preprint arXiv:2506.05207, 2025. 2 [38] Fangyuan Mao, Aiming Hao, Jintao Chen, Dongxia Liu, Xiaokun Feng, Jiashu Zhu, Meiqi Wu, Chubin Chen, Jiahong Wu, and Xiangxiang Chu. Omni-effects: Unified and spatially-controllable visual effects generation. arXiv preprint arXiv:2508.07981, 2025. 1, 2, 3 [39] Qi Mao, Lan Chen, Yuchao Gu, Zhen Fang, and Mike Zheng Shou. Mag-edit: Localized image editing in complex scenarios via mask-based attention-adjusted guidance. In ACM MM, pages 68426850, 2024. 2 [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pages 41954205, 2023. 2, 4 [41] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 5 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 5, 6, 1 [43] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Khedr, Radle, Rolland, Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 2 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2 [45] Yiren Song, Shijie Huang, Chen Yao, Xiaojun Ye, Hai Ci, Jiaming Liu, Yuxuan Zhang, and Mike Zheng Shou. Processpainter: Learn painting process from sequence data. arXiv preprint arXiv:2406.06062, 2024. 2 [46] Yiren Song, Danze Chen, and Mike Zheng Shou. Layertracer: Cognitive-aligned layered svg synthesis via diffusion transformer. arXiv preprint arXiv:2502.01105, 2025. [47] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanyfor multiHarnessing diffusion transformers arXiv preprint thing: domain procedural sequence generation. arXiv:2502.01572, 2025. 2 [48] Yiren Song, Cheng Liu, and Mike Zheng Shou. Omniconsistency: Learning style-agnostic consistency from paired stylization data. arXiv preprint arXiv:2505.18445, 2025. 2 [49] Zhenxiong Tan, Qiaochu Xue, Xingyi Yang, Songhua Liu, and Xinchao Wang. Ominicontrol2: Efficient conditioning for diffusion transformers. arXiv preprint arXiv:2503.08280, 2025. 5 [50] DecartAI Team. Lucy edit: Open-weight text-guided video editing. 2025. 2, 3, 5, 7, 1 [51] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In ECCV, pages 402419. transforms for optical flow. Springer, 2020. [52] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 1, 2, 5 [53] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 2 [54] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: large-scale video-text dataset for mulIn ICLR, 2024. 6, timodal understanding and generation. 1 [55] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 76237633, 2023. 2 [56] Yuhui Wu, Liyi Chen, Ruibin Li, Shihao Wang, Chenxi Xie, and Lei Zhang. Insvie-1m: Effective instruction-based video editing with elaborate dataset construction. In ICCV, pages 1669216701, 2025. 2, 5, 7, 1 [57] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Yuxuan.Zhang, Weihan Wang, Yean Cheng, Bin Xu, Xiaotao Gu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert transformer. In ICLR, 2025. 1, 2 [58] Zixuan Ye, Xuanhua He, Quande Liu, Qiulin Wang, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai, Qifeng Chen, and 10 Wenhan Luo. Unic: Unified in-context video editing. arXiv preprint arXiv:2506.04216, 2025. 2 [59] Emilie Yu, Kevin Blackburn-Matzen, Cuong Nguyen, Oliver Wang, Rubaiat Habib Kazi, and Adrien Bousseau. Videodoodles: Hand-drawn animations on videos with ACM Transactions on Graphics scene-aware canvases. (TOG), 42(4):112, 2023. 3 [60] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pages 80698078, 2024. 2 [61] Yuxuan Zhang, Lifu Wei, Qing Zhang, Yiren Song, Jiaming Liu, Huaxia Li, Xu Tang, Yao Hu, and Haibo Zhao. Stablemakeup: When real-world makeup transfer meets diffusion model. arXiv preprint arXiv:2403.07764, 2024. 2 [62] Yuxuan Zhang, Yirui Yuan, Yiren Song, Haofan Wang, and Jiaming Liu. Easycontrol: Adding efficient and flexible control for diffusion transformer. In ICCV, pages 1951319524, 2025. 2, [63] Yuxuan Zhang, Qing Zhang, Yiren Song, Jichao Zhang, Hao Tang, and Jiaming Liu. Stable-hair: Real-world hair transfer via diffusion model. In AAAI, pages 1034810356, 2025. 2 11 IC-Effect: Precise and Efficient Video Effects Editing via In-Context Learning"
        },
        {
            "title": "Supplementary Material",
            "content": "In this supplementary material, we provide additional implementation details, extended ablation studies, more qualitative results, and discussion of limitations, as summarized below: In Section A, we present additional implementation details of IC-Effect, the baselines, the quantitative metrics, and the user study setup. In Section B, we describe the datasets used for VideoEditor pretraining as well as the paired VFX editing dataset. In Section and Section D, we demonstrate that ICEffect supports flexible instruction control and multieffect editing. In Section E, we conduct ablation studies on causal attention and positional correction. In Section F, we discuss the limitations of the proposed method and outline future directions. In Section G, we provide additional video VFX editing results of IC-Effect and present extended qualitative comparisons with baseline methods on both general video editing and video VFX editing tasks. A. Implementation Details A.1. Training Details During the pre-training stage of VideoEditor, we initialize the DiT architecture using the Wan 2.2-A14B-T2V weights from the Wan model. All training videos are resized to spatial resolution of 224 416 with 81 frames. We train rank-96 LoRA for 50k iterations using the AdamW optimizer with batch size of 2, learning rate of 1104, and weight decay of 1 102, on four A800 GPUs. During the Effect-LoRA training stage, we initialize the DiT architecture with the merged weights from VideoEditor. The video resolution and frame count remain 224 416 and 81 frames, respectively. We train rank-32 LoRA for 1000 iterations using AdamW with batch size of 2, learning rate of 1 104, and weight decay of 1 102, on two A800 GPUs. A.2. Inference Details During inference, we generate videos at spatial resolution of 480832. For source videos with 81 frames, we select the first 81 frames; for videos with < 81 frames, we select the first 4n + 1 frames as the source. We set the classifierfree guidance scale to 5.0 and perform 50 denoising steps. A.3. Implementation Details of Baselines For common video editing comparisons, we use the official codes and released weights of InsV2V [12], InsViE [56], VACE [25], and Lucy Edit [50]. For video VFX editing comparisons, we train the competing methods on the same paired VFX dataset using their officially provided training code or our faithful re-implementations. A.4. Evaluation Details Automatic Evaluation. We use the CLIP Vit-L/14 [42] model to calculate CLIP Image Similarity and CLIP Text Alignment, assessing the quality of the edited videos in terms of both temporal consistency and alignment with the textual prompts. We further evaluate the semantic consistency between the edited videos and their corresponding textual prompts by computing video-level semantic similarity using ViClip [54]. In addition, we adopt multiple submetrics from VBench, including motion smoothness, dynamic degree, and aesthetic quality, to comprehensively assess the visual fidelity and naturalness of the edited videos. Following the official VBench settings, we compute motion smoothness using frame interpolation model, evaluate dynamic degree using RAFT [51], and assess aesthetic quality using the LAION aesthetic predictor. GPT-4o Evaluation. To evaluate the structural preservation and editing effectiveness of the generated results, we further employ the state-of-the-art visionlanguage model (VLM) GPT-4o [1] as an automatic evaluator. For general video editing, we uniformly sample five frames from both the source video and the edited video. For each sample, the VLM receives the source frames, the edited frames, and the textual instruction, and it is required to score the editing result on two dimensionsstructural preservation and editing effectivenessusing 1 5 rating scale. For video VFX editing, we additionally provide the reference VFX video so that the VLM can assess editing effectiveness with respect to the visual effect demonstrated in the reference. Fig. https : / / github . com / amazon - science / instruct - video-to-video https://github.com/langmanbusi/InsViE https://github.com/ali-vilab/VACE https://github.com/DecartAI/lucy-edit-comfyui https://huggingface.co/openai/clipvitlargepatch14 https://huggingface.co/OpenGVLab/ViCLIP https://huggingface.co/lalala125/AMT/tree/main https : / / dl . dropboxusercontent . com / / 4j4z58wuv8o0mfz/models.zip https : / / huggingface . co / Wan - AI / Wan2 . 2 - I2V - https : / / github . com / LAION - AI / aesthetic - A14B-Diffusers predictor 1 and Fig. 11 present the prompt templates used to guide the VLM during automatic evaluation. User Study. For common video editing comparisons, each questionnaire presents participants with the source video, the edited videos produced by the competing methods, our edited video, and the corresponding editing instruction. The edited videos from the competing methods and ours are randomly ordered to prevent participants from inferring their origins. We ask participants to answer questions along the following three dimensions: Instruction Following: Which edited video better follows the textual instruction? Structual Fidelity: Which edited video preserves the non-edited regions more faithfully and aligns better with the source video? Overall Preference: Which edited result do you prefer subjectively? For video VFX editing comparisons, we additionally present the corresponding reference VFX video and ask participants to answer questions along the following three dimensions: Effect Consistency: Which edited video better follows the textual instruction and more closely matches the visual effect in the reference VFX video? Structual Fidelity: Which edited video preserves the non-edited regions more faithfully and aligns better with the source video? Overall Preference: Which edited result do you prefer subjectively? B. Datasets B.1. Datasets of Pretrained Video-Editor To pre-train VideoEditor and equip it with strong instruction-following capability, we construct highquality paired video editing dataset. Considering the requirements of video VFX editing, the constructed dataset covers the following editing tasks: addition, removal, replacement, attribute modification, and style transfer. The source videos primarily come from high-resolution videos on Pexels [2]. For addition and removal data, we use GPT-4o [1] to analyze each video and identify the target object category. We then obtain the corresponding object masks using Grounding DINO [31] and SAM [43]. With the masks, we remove the target object using the object erasing model DiffuEraser [29]. The original video and the object-removed video form pair for both addition and removal tasks, and we use GPT-4o [1] to generate the corresponding remove and add editing instructions. To ensure both video quality and instruction correctness, we further refine the data using VLM and manual verification. For replacement and attribute modification data, we emFigure 7. Flexible Instruction-Based Control of Video VFX Editing Results. ploy GPT-4o [1] to generate editing instructions for each source video, and then synthesize the edited video using VACE [25]. We filter the generated videos using VLM to remove blurry or invalid results. For style transfer data, GPT-4o [1] first generates the textual editing descriptions. We then edit the first frame using FLUX-kontext [3], estimate depth maps of the source video using video depth estimator [11], and feed the edited first frame, depth maps, and textual prompt into VACE [25] to obtain the final stylized video. In total, we collect approximately 50,000 high-quality paired video editing samples. All videos are resized to spatial resolution of 480 832. B.2. Video VFX Datasets Since existing techniques cannot automatically produce high-quality paired video VFX editing data, we construct the first paired dataset consisting of source videos and their corresponding VFX videos. The dataset includes 15 distinct visual effects, covering: animated character insertion (with two different characters), anime duplication, rotating rings, graffiti strokes, lightning outlines, mystical aura, firework explosion, flame burning, architectural growth and bounce, line traversal, line shaping, particle dispersion, particle aggregation, and light-particle traversal. For each pair, we provide {source video, target VFX video, editing instruction} triplet. All videos are standardized to resolution of 480 832 and duration of 5 seconds at 16 FPS. C. Instruction Control for Video VFX Editing Our IC-Effect framework supports flexible control over the editing results through instruction manipulation. By simply adjusting the textual prompt, the model flexibly controls the attributes and directions of the generated visual effects. As Table 3. Quantitative Ablation of Positional Encoding Correction and Causal Attention. The best values are highlighted in blue . Video Quality Semantic Alignment Overall Quality GPT Score Method/Metrics CLIP-I () CLIP-T () ViCLIP-T () Smoothness () Dynamic Degree () Aesthetic Quality () Structural preservation () Effect accuracy () w/o PEC w/o C-Attn Ours 0.9735 0.9783 0. 26.8748 27.0595 27.2321 25.7468 26.4503 26.6312 0.3715 0.3750 0.9911 0.9903 0.9887 0.3771 0.5324 0.5444 0.5823 4.4615 4.5583 4. 4.3908 4.4866 4.5614 Figure 8. Video Multi VFX Editing of IC-Effect. shown in Fig. 7, minor modifications to the prompt already produce clear and controllable variations in the edited effects. D. Video Multi VFX Editing To verify that IC-Effect supports multi-effect editing, we perform mixed training on all effect categories based on Video-Editor. As shown in Fig. 8, IC-Effect accurately follows the textual instructions to inject multiple effects into the video without cross-effect leakage. It applies each effect precisely to the instruction-specified target while consistently preserving the non-edited regions. This capability mainly stems from the carefully designed architecture and the construction of high-quality training data, which together endow IC-Effect with strong instruction-following ability and enable precise source-video editing. E. Additional Ablation Studies In this section, we conduct ablation studies on the positional encoding correction (PEC) and the causal attention mechanism (C-Attn). As shown in Fig. 9, removing positional encoding correction introduces artifacts and blurring, while altering information from the source video. Without the causal attention mechanism, the originally clear spatiotemporally sparse tokens are contaminated by latent noise, resulting in severe artifacts in the edited output. Furthermore, the quantitative results in Table 3 corroborate these observations. In contrast, the complete model demonstrates higher editing accuracy and stronger visual consistency. F. Limitation and Future Work limitation of IC-Effect is its reliance on high-quality paired video VFX editing data. However, producing such 3 Figure 9. Ablation Study on Positional Encoding Correction and Causal Attention. high-quality paired data is extremely challenging. In the future, we plan to train feature extractor to directly extract target effects from reference VFX videos and use these extracted features to edit the source video. G. Additional Results G.1. Additional Results of Video VFX Editing Fig. 12 and Fig. 13 present additional video VFX editing results produced by IC-Effect. Across diverse effect categories, our IC-Effect accurately follows the textual prompts and generates visually coherent and well-integrated edits. G.2. Additional Qualitative Comparison of Video"
        },
        {
            "title": "VFX Editing",
            "content": "Fig. 14 - Fig. 16 present additional quantitative comparisons with baseline methods on video VFX editing. Compared with the baselines, IC-Effect consistently follows the textual prompts to edit the source video while preserving its spatial structure and temporal coherence. G.3. Additional Qualitative Comparison of Common Video Editing Fig. 17 - Fig. 21 present qualitative comparisons between IC-Effect and existing methods across diverse common video editing tasks, including addition, removal, attribute modification, replacement, and style transfer. Across these common editing scenarios, IC-Effect demonstrates superior performance: it accurately follows textual instructions, produces precise edits, and preserves the structural integrity of non-edited regions. Figure 10. Prompt Template for Common Video Editing Evaluation. Figure 11. Prompt Template for Video VFX Editing Evaluation. 4 Figure 12. Additional Video VFX Editing Results of IC-Effect. Figure 13. Additional Video VFX Editing Results of IC-Effect. 5 Figure 14. Additional Qualitative Comparison for Video VFX Editing. Figure 15. Additional Qualitative Comparison for Video VFX Editing. 6 Figure 16. Additional Qualitative Comparison for Video VFX Editing. 7 Figure 17. Additional Qualitative Comparison for Common Video Editing. 8 Figure 18. Additional Qualitative Comparison for Common Video Editing. 9 Figure 19. Additional Qualitative Comparison for Common Video Editing. 10 Figure 20. Additional Qualitative Comparison for Common Video Editing. 11 Figure 21. Additional Qualitative Comparison for Common Video Editing."
        }
    ],
    "affiliations": [
        "Baidu Inc., Beijing, China",
        "School of Information and Communication Engineering, Communication University of China",
        "Show Lab, National University of Singapore"
    ]
}