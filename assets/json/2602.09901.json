{
    "paper_title": "QP-OneModel: A Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search",
    "authors": [
        "Jianzhao Huang",
        "Xiaorui Huang",
        "Fei Zhao",
        "Yunpeng Liu",
        "Hui Zhang",
        "Fangcheng Shi",
        "Congfeng Li",
        "Zechen Sun",
        "Yi Wu",
        "Yao Hu",
        "Yunhan Bai",
        "Shaosheng Cao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Query Processing (QP) bridges user intent and content supply in large-scale Social Network Service (SNS) search engines. Traditional QP systems rely on pipelines of isolated discriminative models (e.g., BERT), suffering from limited semantic understanding and high maintenance overhead. While Large Language Models (LLMs) offer a potential solution, existing approaches often optimize sub-tasks in isolation, neglecting intrinsic semantic synergy and necessitating independent iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the gap between open-domain corpora and informal SNS linguistic patterns, while struggling to adhere to rigorous business definitions. We present QP-OneModel, a Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. We reformulate heterogeneous sub-tasks into a unified sequence generation paradigm, adopting a progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning. Furthermore, QP-OneModel generates intent descriptions as a novel high-fidelity semantic signal, effectively augmenting downstream tasks such as query rewriting and ranking. Offline evaluations show QP-OneModel achieves a 7.35% overall gain over discriminative baselines, with significant F1 boosts in NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization, surpassing a 32B model by 7.60% accuracy on unseen tasks. Fully deployed at Xiaohongshu, online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%."
        },
        {
            "title": "Start",
            "content": "QP-OneModel: Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search Jianzhao Huang, Xiaorui Huang, Fei Zhao, Yunpeng Liu, Hui Zhang, Fangcheng Shi, Congfeng Li, Zechen Sun, Yi Wu, Yao Hu, Yunhan Bai, Shaosheng Cao Xiaohongshu Inc., China caoshaosheng@xiaohongshu.com 6 2 0 2 0 1 ] . [ 1 1 0 9 9 0 . 2 0 6 2 : r Abstract Query Processing (QP) serves as critical bridge between user intent and content supply in large-scale search engines for Social Network Services (SNS). Traditional QP systems typically rely on pipelines of isolated discriminative models (e.g., BERT), which suffer from limited semantic understanding capabilities and high maintenance overhead. While Large Language Models (LLMs) offer potential solution, existing approaches largely remain fragmented by optimizing sub-tasks in isolation, which neglects the intrinsic semantic synergy across sub-tasks and necessitates independent model iterations. Moreover, standard generative methods often lack grounding in SNS scenarios, failing to bridge the distributional gap between open-domain corpora and the informal linguistic patterns of SNS data, while also struggling to adhere to the rigorous business definitions of specific QP tasks. In this work, we present QP-OneModel, Unified Generative LLM for Multi-Task Query Understanding in the SNS domain. Specifically, we reformulate heterogeneous query processing sub-tasks into unified sequence generation paradigm, while adopting progressive three-stage alignment strategy culminating in multi-reward Reinforcement Learning to ensure robust adaptation. Furthermore, QP-OneModel generates intent descriptions as novel high-fidelity semantic signal unavailable in traditional pipelines, effectively augmenting downstream tasks such as query rewriting and ranking. Extensive offline evaluations show that QP-OneModel achieves 7.35% gain in overall score over discriminative baselines, with significant F1 boosts in complex NER (+9.01%) and Term Weighting (+9.31%). It also exhibits superior generalization on unseen tasks, surpassing the 32B-parameter model by 7.60% in accuracy. QP-OneModel has been fully deployed to serve the Search Results Page traffic at Xiaohongshu. Online A/B tests confirm its industrial value, optimizing retrieval relevance (DCG) by 0.21% and lifting user retention by 0.044%."
        },
        {
            "title": "1 Introduction\nDriven by the exponential growth of the mobile Internet, Social\nNetwork Services (SNS) have evolved into core infrastructure for\nsocial interaction and information dissemination [36]. Within this\necosystem, Query Processing (QP) serves as the fundamental up-\nstream module responsible for decoding user intent and bridging\nthe gap between raw queries and content supply, thereby directly\ndictating retrieval relevance and overall user satisfaction [6, 15, 20].",
            "content": "Equal contribution. Corresponding authors. 1 To accurately interpret vague or colloquial user queries, industrial QP systems rely on series of fundamental sub-tasks, including Named Entity Recognition (NER), Word Segmentation, Term Weighting, and Query Taxonomy [7, 19, 22, 27, 41]. Traditional QP systems typically implement these tasks via pipeline of isolated discriminative models (e.g., BERT-based sequence taggers and classifiers) [8, 33], which suffer from two fundamental limitations: First, they possess limited semantic understanding capabilities, often struggling to capture complex or nuanced user intents, particularly when confronting long-tail queries or data distribution shifts [2, 3, 10]. Second, they incur high maintenance overhead, as even minor updates to classification taxonomies or input schemas necessitate retraining the entire model, significantly hindering rapid iteration in fast-paced industrial environments [29]. The emergence of Large Language Models (LLMs) offers potential solution [1, 4, 23, 32], yet effective deployment in industrial SNS QP scenarios faces critical challenges [21]. First, existing generative approaches remain fragmented, typically optimizing sub-tasks in isolation and neglecting the intrinsic semantic synergy across tasks. [26, 35] Second, general LLMs struggle with the distinct distributional shift of SNS data, failing to capture the extreme sparsity and rapid lexical evolution (e.g., emerging slang) inherent in social media [2, 10, 13]. Third, the complexity of business rules coupled with data scarcity renders standard fine-tuning inadequate [24, 40]. Industrial QP is governed by fine-grained and often counter-intuitive business definitions [29]. Constrained by the limited volume of up-to-date gold-standard data, standard supervised fine-tuning (SFT) often devolves into rote memorization of surface patterns rather than truly internalizing these complex logical boundaries [24, 40]. In this work, we propose QP-OneModel, which integrates unified generative modeling of heterogeneous QP tasks, robust domainspecific backbone, and three-stage progressive alignment strategy. Instead of relying on isolated discriminative heads, our model generates structured sequence containing all analytical results in single pass, utilizing global query context to resolve local ambiguities. To mitigate the distributional shift between general corpora and the SNS domain, we leverage the RedOne [38, 39] backbone. By harnessing its innate grasp of informal linguistic patterns and emerging slang, QP-OneModel effectively bridges the gap between raw user queries and structured representations. To overcome the challenges of complex business rules and data scarcity, we design progressive three-stage alignment strategy that bootstraps the model from broad knowledge to precise logic internalization. 1) First, we perform Knowledge Injection via mixedtraining strategy, integrating massive auxiliary datasets derived from historical logs to establish broad semantic foundation. 2) Subsequently, we transition to Target Distribution Alignment, utilizing strictly real-time human annotations to eliminate residual noise and align with evolving trends. 3) Finally, to prevent the rote memorization typical of standard SFT, we introduce Multi-Reward Reinforcement Learning (RL) stage. By optimizing composite reward function that balances all sub-tasks, this process drives the model to internalize underlying reasoning logic, further improving performance on tasks demanding deeper semantic understanding, such as NER and Term Weighting. Furthermore, leveraging the unified generative paradigm, QPOneModel produces Intent Descriptions, natural language narrative of the users search goal derived from the joint modeling of QP sub-tasks. This novel output serves as high-fidelity semantic signal, effectively augmenting downstream applications such as query rewriting and ranking. In summary, our main contributions are as follows: Unified Generative Paradigm for QP: We formulate heterogeneous QP sub-tasks into single sequence-to-sequence generation task. This unified architecture breaks the dependencies of cascaded pipelines, enabling the global optimization of intrinsic task correlations. Progressive Alignment Strategy: We employ progressive three-stage alignment strategy, alongside an SNS-specific backbone. This systematically addresses the challenges of complex business rules and data scarcity, guiding the model from general understanding to the rigorous internalization of business rules. High-Fidelity Semantic Augmentation: We introduce generative Intent Descriptions as novel output. Enabled by the unified modeling of structural and semantic information, these descriptions provide downstream systems with distinct, high-quality intent signals inaccessible to discriminative models. Industrial-Scale Validation: We provide extensive empirical evidence from both offline evaluations and large-scale online A/B testing at Xiaohongshu. Offline results demonstrate that QP-OneModel achieves 7.35% overall improvement over BERT-based baselines, and surpasses the significantly larger Qwen3-32B by 7.60% accuracy on the unseen Document Intent task. Online deployment also shows substantial gains, where deploying fundamental signals optimizes retrieval relevance (DCG) by 0.21%, and leveraging generated intent descriptions increases user retention by 0.044%."
        },
        {
            "title": "2 Related Work\n2.1 Query Processing in Industrial Search\nQuery Processing (QP) bridges raw user queries and downstream\nretrieval by producing structured semantic signals, including NER,\nsegmentation, term weighting, and taxonomy/intent labels [6, 7,\n22, 27]. Early industrial solutions relied on statistical modeling and\nstructured prediction, with CRFs as a representative paradigm for\nsequence labeling tasks [18]. More recently, Transformer-based\nPLMs have become the dominant backbone: QP sub-tasks are typi-\ncally cast as sequence labeling or classification and solved by dis-\ncriminative encoders such as BERT [8, 33]. Despite their strong",
            "content": "2 Huang et al. performance over earlier statistical methods, production QP is commonly implemented as cascaded pipelines of independently trained components, which introduce non-trivial system overhead and error propagation [29]. Such pipelines are also sensitive to domain shift and long-tail traffic, challenge amplified in social media where vocabulary and entities evolve rapidly [2, 3, 10, 13]."
        },
        {
            "title": "3 Preliminaries\nQuery Processing (QP) serves as the foundational layer of the search\nengine, transforming raw user inputs into structured semantic rep-\nresentations to facilitate effective information retrieval. Formally,\ngiven a query sequence ğ‘, we define the following four core sub-\ntasks employed in our industrial environment and introduce a new\ngenerative sub-task:\nNamed Entity Recognition (NER). NER aims to identify semantic\nspans within ğ‘ and classify them into a predefined ontology.\nWord Segmentation. For languages lacking explicit delimiters,\nWord Segmentation partitions the continuous character sequence\nğ‘ into discrete lexical units (terms).\nTerm Weighting. Term Weighting quantifies the semantic con-\ntribution of each segmented term to the userâ€™s core intent. We\nformulate this as a token-level classification task using a discrete 4-\ntier relevance scale ğ‘  âˆˆ {0, 1, 2, 3}. The hierarchy ranges from Level",
            "content": "QP-OneModel: Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search Figure 1: The overall framework of QP-OneModel, covering data construction, multi-stage SFT, and reinforcement learning. 0 (functional stop words) to Level 3 (core intent carriers), directly governing the mandatory matching logic of the search engine. Query Taxonomy. Query Taxonomy associates query ğ‘ with multiple relevant categories to facilitate vertical-specific retrieval strategies. We formulate this as multi-label classification task that yields ranked sequence of labels. The label order encodes relevance, prioritizing the Top-1 label as the dominant intent. Intent Description. This task generates natural language narrative that explicates the users underlying search goal."
        },
        {
            "title": "Generation",
            "content": "We reformulate the entire QP workflow as unified sequence-tosequence generation task. Formally, given an input query ğ‘, static task instruction ğ¼ , configurable business rules ğ‘…, and dynamic contextual information ğ¶ (e.g., user rewrite history, candidate notes), 3 our model ğœ‹ğœƒ generates structured output = (ğ‘¦1, ğ‘¦2, . . . , ğ‘¦ğ‘‡ ) in an autoregressive manner: ğœ‹ğœƒ (y ğ¼, ğ‘…, ğ¶, ğ‘) = ğ‘‡ (cid:214) ğ‘¡ = ğœ‹ğœƒ (ğ‘¦ğ‘¡ ğ¼, ğ‘…, ğ¶, ğ‘, ğ‘¦<ğ‘¡ ), (1) where ğœ‹ğœƒ (ğ‘¦ğ‘¡ ) represents the probability of generating the ğ‘¡th token conditioned on the complete input configuration and all previously generated tokens ğ‘¦<ğ‘¡ . The output is structured as JSON object containing results for all QP sub-tasks: = {entities, segments, weights, category, intent_desc}. (2) The task execution order is deliberately designed: Named Entity Recognition Word Segmentation Term Weighting Query Taxonomy Intent Description. This sequential dependency allows downstream sub-tasks to leverage upstream results, thereby maximizing task synergy. For instance, accurate entity recognition directly refines word segmentation, and both contribute to optimizing term weighting. Given training dataset = {(ğ¼, ğ‘…, ğ¶ (ğ‘– ), ğ‘ (ğ‘– ), y(ğ‘– ) )}ğ‘ ğ‘–=1, where y(ğ‘– ) represents the ground-truth structured output, our objective is to maximize the likelihood of generating correct outputs: E(ğ¼,ğ‘…,ğ¶,ğ‘,y)D [log ğœ‹ğœƒ (y ğ¼, ğ‘…, ğ¶, ğ‘)] . (3) max ğœ‹ğœƒ This unified generative paradigm eliminates the need for constructing task-specific architectures or maintaining dedicated model instances, and enables end-to-end optimization, naturally capturing inter-task dependencies that remain inaccessible to isolated single-task models."
        },
        {
            "title": "4.2 Business-Aware Prompt Design\nAs illustrated in Figure 2, our business-aware prompt integrates\nthree critical components:",
            "content": "Huang et al. where ğ‘› ğ‘— denotes high-relevance note retrieved via preliminary matching. These notes serve as real-time semantic anchors, helping the model ground abstract queries in actual platform content. The final composite prompt is constructed by concatenating the base task instruction ğ¼ , configurable rules ğ‘…, current query ğ‘, and dynamic contexts ğ¶ = {ğ¶hist, ğ¶note}: = ğ¼ ğ‘… ğ‘ ğ¶, (6) where denotes concatenation with appropriate delimiters."
        },
        {
            "title": "4.3 Progressive Three-Stage Alignment Strategy\nDirectly fine-tuning a general LLM on limited unified QP data leads\nto suboptimal generalization due to the distributional gap between\npretraining corpora and SNS queries, as well as insufficient coverage\nof complex business rules [38, 39]. To systematically bootstrap the\nmodel from broad domain knowledge to precise business logic\ninternalization, we propose a progressive three-stage alignment\nstrategy, as illustrated in Figure 1.",
            "content": "4.3.1 Knowledge Injection via Task Decomposition and MixedSFT. The primary bottleneck in training unified QP model is the scarcity of real-time, human-annotated data covering all sub-tasks jointly. Manual annotation of unified structured outputs (JSON format) is extremely labor-intensive and costly, resulting in limited training samples ( 105 samples). To overcome this data scarcity, we employ Task Decomposition strategy to construct auxiliary datasets from massive historical user logs. Specifically, we leverage the legacy pipelinecomprising separate models for each QP sub-taskto generate pseudo-labels on unlabeled queries sampled from search logs: Daux = {(ğ‘, ypseudo ğ‘˜ )}ğ‘˜ T,ğ‘ Qlog , (7) where denotes the set of QP sub-tasks, Qlog represents the largescale query pool from historical logs, and ypseudo is the pseudolabeled output for sub-task ğ‘˜ produced by the legacy system. ğ‘˜ While these pseudo-labels are noisy and incomplete (each sample covers only single sub-task), they provide broad coverage of diverse query patterns at scale ( 107 samples). To mitigate the semantic drift introduced by noisy pseudo-labels, we adopt MixedTraining strategy that integrates the limited human-annotated unified data alongside these auxiliary datasets. Formally, we optimize: LStage1 (ğœƒ ) = E(ğ¼,ğ‘…,ğ¶,ğ‘,y)Dunified + ğœ† ğ‘˜ (ğ‘,yğ‘˜ )Dğ‘˜ aux [ log ğœ‹ğœƒ (y ğ¼, ğ‘…, ğ¶, ğ‘)] [ log ğœ‹ğœƒ (yğ‘˜ ğ‘)] , (8) where Dunified denotes the small-scale human-annotated unified dataset, Dğ‘˜ aux represents the auxiliary dataset for sub-task ğ‘˜, and ğœ† is hyperparameter balancing the two data sources. This mixed-training approach ensures that the model aligns with accurate business logic from Dunified while simultaneously absorbing broad QP knowledge from the large-scale auxiliary data. 4.3.2 Target Distribution Alignment. Upon establishing broad semantic foundation in Stage 1, we transition to fine-tuning stage utilizing exclusively the real-time, human-annotated unified data Dunified. This phase is designed to bridge the distributional gap between the historical logs used in Stage 1 and the current online environment. Figure 2: Illustration of the Business-Aware Prompt schema. The prompt integrates configurable business rules ğ‘… with dynamic contexts, including user rewrite history ğ¶hist and candidate notes ğ¶note. This context-rich formulation guides the model to generate unified JSON output covering all QP sub-tasks. 1) Configurable Business Rules. We encode fine-grained operational knowledge as explicit instructions. This design allows operations staff to rapidly update the prompt to handle emerging scenarios or enforce new policies without the need for retraining. These rules include: Entity Definitions: Criteria for identifying entities (e.g., brands, KOLs, trending IPs). Segmentation Rules: Custom segmentation rules that handle terms such as SNS slang and emerging expressions. Term Weighting Rules: Logic for assigning importance scores to query terms. Category Taxonomy: Taxonomy definitions including boundary cases and disambiguation guidelines. Intent Analysis: Methods and examples for analyzing complex user search intents. 2) User Rewrite History (ğ¶hist). Search behaviors are rarely isolated and users frequently refine queries sequentially to clarify intent or explore topics. We dynamically inject the users rewrite history within the current session into the prompt as temporal context: ğ¶hist = {ğ‘ğ‘¡ ğ‘˜, ğ‘ğ‘¡ ğ‘˜+1, . . . , ğ‘ğ‘¡ 1}, (4) where ğ‘ğ‘¡ is the current query and the set represents the ğ‘˜ most recent historical queries. This context aids in disambiguating vague inputs and capturing evolving user intent throughout the session. 3) Candidate Notes (ğ¶note). For queries that are semantically ambiguous, time-sensitive, or deeply platform-specific, we augment the prompt with candidate notes retrieved from the platforms content pool: ğ¶note = {ğ‘›1, ğ‘›2, . . . , ğ‘›ğ‘š }, (5) 4 QP-OneModel: Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search By focusing strictly on the most recent human-annotated samples, we eliminate residual noise from the auxiliary tasks and ensure the model is precisely aligned with: (1) the rigorous unified schema where all sub-tasks are jointly annotated and mutually consistent, and (2) the rapidly evolving linguistic trends of social media. The training objective for Stage 2 is standard supervised finetuning: LStage2 (ğœƒ ) = E(ğ¼,ğ‘…,ğ¶,ğ‘,y)Dunified [log ğœ‹ğœƒ (y ğ¼, ğ‘…, ğ¶, ğ‘)] . (9) 4.3.3 Logic Internalization via Multi-Reward RL. While supervised fine-tuning effectively teaches the model to mimic reference outputs, it often leads to rote memorization of surface patterns rather than true internalization of underlying business logic. To address this limitation, we introduce Reinforcement Learning (RL) stage that optimizes the model to maximize task-specific rewards. Reward Design. Given the multi-faceted nature of QP tasks, we design composite reward function that balances performance across all sub-tasks: ( Ë†y, y) = ğ‘˜ ğ‘¤ğ‘˜ ğ‘Ÿğ‘˜ ( Ë†yğ‘˜, yğ‘˜ ), (10) where Ë†yğ‘˜ and yğ‘˜ are the predicted and ground-truth outputs for sub-task ğ‘˜ respectively, and ğ‘¤ğ‘˜ is the weight reflecting the business importance of task ğ‘˜. To ensure strict consistency between the optimization objective and the final performance evaluation, we define ğ‘Ÿğ‘˜ (, ) directly as the evaluation metric used for that specific sub-task. The detailed definitions of these metrics are provided in Section 5.1. Group Relative Policy Optimization. To optimize the policy ğœ‹ğœƒ , we employ Group Relative Policy Optimization (GRPO) [30], variant of Proximal Policy Optimization (PPO) [28] that has demonstrated strong empirical performance on verifiable tasks. The GRPO objective is: JGRPO (ğœƒ ) = (cid:34) (ğ¼,ğ‘…,ğ¶,ğ‘,y)DGRPO,{ Ë†yğ‘– }ğº ğ‘–=1 ğœ‹ğœƒ 1 ğº ğº ğ‘–=1 1 Ë†yğ‘– Ë†yğ‘– ğ‘¡ =1 log ğœ‹ğœƒ ( Ë†ğ‘¦ğ‘–,ğ‘¡ ğ¼, ğ‘…, ğ¶, ğ‘, Ë†yğ‘–,<ğ‘¡ ) Ë†ğ´ğ‘– (11) (cid:35) ğ›½ DKL (ğœ‹ğœƒ ğœ‹ref ) . Data Sampling. As QP-OneModel unifies multiple downstream query tasks, single rollout of specific prompt yields rewards derived from multiple sub-tasks simultaneously. Directly applying group-wise normalization to the aggregated reward masks performance discrepancies across sub-tasks, thereby hindering the model from identifying which specific sub-tasks underperform. To mitigate this issue, we leverage the Stage 2 SFT model to filter the initial dataset, specifically retaining samples that exhibit divergent rewards on target sub-task while maintaining consistent rewards across all other sub-tasks. The resulting subset constitutes the final training dataset for GRPO, denoted as DGRPO. This strategy enables precise reward attribution in multi-task settings, thereby enhancing the stability and efficacy of policy gradient updates."
        },
        {
            "title": "5 Experiments\n5.1 Experimental Settings\n5.1.1 Test Set Construction. To guarantee a robust and unbiased\nevaluation within a real-world industrial context, we constructed a\nhigh-fidelity Golden Test Set comprising approximately 2,500 queries\nrandomly sampled from recent production traffic. We adopted a\nstrictly manual annotation pipeline consisting of: (1) Expert-Driven\nProtocols: Domain experts formulated comprehensive guidelines\nbased on specific domain logic; (2) Specialized Training: Annota-\ntors underwent intensive task-specific training; and (3) Iterative\nVerification: Multi-round cross-validation and expert adjudication\nwere conducted to ensure high inter-annotator agreement. This\nestablishes a reliable benchmark for evaluating QP-OneModel.",
            "content": "5.1.2 Metrics. We employ task-specific metrics to rigorously evaluate model performance across the five sub-tasks. Standard F1 Tasks. For Word Segmentation and NER, we adopt the standard F1 Score [31]. For NER, we enforce strict evaluation criterion where prediction is considered true positive only if both the entity span boundary and the entity type strictly match the ground truth. Term Weighting (Joint F1). Term Weighting intrinsically depends on the preceding Word Segmentation, since assigning weights to incorrectly segmented terms is semantically undefined. We define the atomic evaluation unit as tuple term, weight. predicted unit is counted as correct if and only if both the term boundary and the assigned importance level strictly match the ground truth. We report the F1 Score based on these joint units. Query Taxonomy. Given that the label order encodes relevance, where the Top-1 label explicitly represents the dominant intent, standard multi-label metrics are insufficient. We design composite metric averaging the Top-1 Accuracy (ğ´ğ‘ğ‘ğ‘¡ğ‘œğ‘1) and the standard F1 Score: Scoreğ‘„ğ¶ = (cid:0)ğ´ğ‘ğ‘ğ‘¡ğ‘œğ‘1 + F1(cid:1) (12) 1 2 This metric balances the precision of the primary intent with the recall of all relevant categories. Overall. The average score of all sub-tasks."
        },
        {
            "title": "5.2 Offline Evaluation\nTo comprehensively evaluate QP-OneModel, we design experiments\nto answer the following research questions:",
            "content": "RQ1 (Main Results): How effective is the proposed QPOneModel framework compared to the industry-standard discriminative pipeline? RQ2 (Synergy of Unified Modeling): Does joint modeling of heterogeneous QP tasks yield positive transfer and synergy? RQ3 (Ablation Study): How do the domain-specific backbone and the progressive three-stage training strategy contribute to performance? RQ4 (Generalization on Emerging Tasks): To what extent can the QP-OneModel leverage learned representation to generalize across unseen QP tasks? 5.2.1 Main Results: QP-OneModel vs. Discriminative Pipeline (RQ1). We compare QP-OneModel against the online baseline, Huang et al. Figure 3: Overview of the deployment architecture. The framework utilizes nearline inference strategy where QP-OneModel pre-computes results to update the KV-Cache daily. The retrieved structural signals and intent descriptions are then served to downstream tasks such as Query Rewriting and Ranking. pipeline of cascaded BERT-based discriminative models. As shown in Table 1, QP-OneModel-8B achieves significant improvement in overall score(+7.35%) compared to the baseline. Specifically, the baseline struggles with tasks requiring deep semantic understanding and logic, such as NER (74.85%) and Term Weighting (56.86%). In contrast, QP-OneModel-8B demonstrates massive leap in these hard tasks, improving NER F1 by +9.01% and Term Weighting F1 by +9.31%. This validates the effectiveness of our comprehensive framework. Furthermore, it is crucial to note that QP-OneModel-0.6B also substantially outperforms the BERT pipeline across all metrics. It achieves an overall score of 78.37%, surpassing the baseline by +5.83%, and delivers remarkable gains in complex tasks, such as improving Term Weighting F1 by +8.59% (from 56.86% to 65.45%). This result highlights that our improvements are derived from the superior effectiveness of the proposed methodology rather than merely scaling up model parameters. 5.2.2 Synergy of Unified Modeling (RQ2). fundamental hypothesis underpinning our methodology is that unifying heterogeneous QP tasks into single generative paradigm fosters semantic synergy and facilitates positive knowledge transfer through shared latent representations. To empirically verify this, we conduct controlled experiment focusing on our Stage 1 phase. We compare the unified model against Task-Isolated baseline. In this controlled setting, we strictly align all experimental configurations (backbone, loss function, hyperparameters, etc.) with our Stage 1, varying only the data organization. Specifically, we partition the unified multi-task dataset into discrete task-specific subsets, which are then employed to train separate expert models for each QP task independently. Consequently, whereas QP-OneModel operates as single unified instance, the reported baseline metrics represent an aggregation of the best results achieved by each specialized model on its respective task. As presented in Table 2, the Unified model exhibits superior overall score (79.36% vs. 78.11%) by significantly outperforming the combined baseline on highly synergistic tasks that exhibit strong 6 mutual dependencies: NER, Word Segmentation, and Term Weighting. This validates the semantic synergy inherent in our sequential generation design. By modeling the joint probability of all tasks, the unified approach transforms the output of upstream tasks into explicit context for downstream tasks, thereby achieving better performance and efficiency than suite of isolated models. 5.2.3 Ablation Study (RQ3). To understand the source of improvements, we investigate the impact of the backbone model and our training strategies. Impact of Domain-Specific Backbone. To investigate the impact of backbone initialization, we conducted comparative analysis within the Stage 1 training phase by switching between the general-purpose Qwen3 backbone [37] and RedOne2.0 [38], domainadapted backbone for SNS. As shown in Table 3, the RedOne-based model demonstrates consistent advantage across all metrics, particularly in Word Segmentation (+0.25%) and Taxonomy (+0.69%). These gains indicate that domain-adaptive pre-training on massivescale SNS data effectively injects domain linguistic patterns, thereby providing significantly better initialization for SNS QP tasks compared to generic foundation model. Contribution of Progressive Alignment Stages. Table 4 validates our three-stage alignment strategy. Stage 2 (Target Alignment) yields consistent improvements over Stage 1 in overall score and specific sub-tasks. For the 8B model, overall score reaches 79.44%, with Taxonomy rising from 79.59% to 79.70%. similar trend is observed in the 0.6B model, where overall score moves from 77.95% to 78.27%, with NER improving from 80.88% to 81.91%. We suppose that fine-tuning on high-quality real-time unified data helps the model better align with the online data distribution and output schema. Stage 3 (RL) yields noticeable performance improvements, particularly on tasks that require robust semantic understanding. For the 8B model, we observe gains in Term QP-OneModel: Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search Table 1: Overall performance comparison on the offline Golden Test Set. Model Overall NER (F1) Word Seg (F1) Term Wgt (F1) Taxonomy (AVG) BERT-Pipeline (Baseline) QP-OneModel-0.6B QP-OneModel-8B 72.54 78.81 79.89 74.85 81.64 83. 85.13 89.65 89.86 56.86 65.45 66.17 73.31 78.5 79. Table 2: Comparison between Unified Modeling and TaskIsolated Training (8B) under the SFT Stage 1 setting. Setting Overall NER Seg Task-Isolated Unified 78.11 79.36 83.02 83.61 87.98 89.46 TW 60.92 64.78 Taxo 80.51 79.59 Table 3: Comparison of different backbones (8B) under the SFT Stage 1 setting. Backbone Overall NER Seg TW Taxo 79.04 Qwen3-based RedOne2.0-based 79.36 83.32 83.61 89.21 89.46 64.73 64.78 78.90 79. Table 4: Ablation study of training stages across different model scales. Stage Overall NER Seg TW Taxo 0.6B Model Stage 1 Stage 2 Stage 3 (Ours) 8B Model Stage 1 Stage 2 Stage 3 (Ours) 77.95 78.27 78.81 79.36 79.44 79.89 80.88 81.91 81.64 83.61 83.62 83. 88.85 88.63 89.65 89.46 89.51 89.86 63.71 64.10 65.45 64.78 64.91 66.17 78.35 78.44 78.50 79.59 79.70 79. Weighting (+1.26%) and NER (+0.24%). This efficacy extends to the 0.6B model, which achieves notable increase of +1.35% in Term Weighting (from 64.10% to 65.45%). These results suggest that exploration with verifiable rewards enables models to learn nuanced rules that remain elusive under SFT. 5.2.4 Generalization on Emerging Tasks (RQ4). To assess the models capacity for transferring learned knowledge to novel problems, we evaluate its generalization capability on two emerging QP tasks that were not part of the original multi-task training curriculum. This serves as rigorous benchmark for the models \"meta-understanding\" of user search intent. Both evaluations are conducted in few-shot setting. The new tasks are defined as: Document Intent Recognition (Task 1): This task classifies queries based on whether the user intends to find document-style content (e.g., articles, long-form guides). 7 As Xiaohongshu diversifies its content ecosystem, accurately identifying this intent is crucial for guiding retrieval and ranking strategies, such as allocating specific quotas or applying ranking boosts for document-centric results. Authority Intent Recognition (Task 2): This task determines if query seeks authoritative or official information (e.g., from official brand websites, scientific sources). This signal is vital for downstream AI search applications, guiding decisions like triggering external API calls to highauthority search engines or prioritizing the indexing of content from trusted domains. Few-Shot In-Context Learning (ICL) Generalization. We first evaluate the models ability to generalize without any gradient updates, relying solely on in-context learning from few demonstration examples provided in the prompt. We benchmark against general LLMs (Qwen series) [25, 37] and the domain-specific RedOne family [38, 39]. As shown in Table 5, QP-OneModel-8B demonstrates remarkable ICL capabilities. For Document Intent, it achieves an accuracy of 82.40%, significantly surpassing not only its domain-adapted counterpart, RedOne-8B (75.80%), but also the much larger Qwen3-32B model (74.80%). For Authority Intent, our model attains competitive accuracy of 71.56%, performing on par with the 32B model and outperforming other models in its size class. These results strongly suggest that the multi-task, multi-stage alignment strategy has endowed QP-OneModel with robust, abstract understanding of query semantics, enabling it to effectively solve unseen tasks through prompting alone. Few-Shot Fine-Tuning Performance. Next, we assess the models adaptability by fine-tuning them on small set of task-specific data using LoRA [14]. QP-OneModel continues to exhibit superior performance. For Document Intent, QP-OneModel-8B achieves Macro F1 of 78.60, outperforming the RedOne-8B baseline. For Authority Intent, QP-OneModel-8B achieves the highest overall score (77.34%) and Macro F1 (72.76%) among all evaluated models. These suggest that the comprehensive training on core QP tasks, such as Term Weighting and NER, enables effective generalization to new domains upon fine-tuning."
        },
        {
            "title": "5.3 Online Evaluation\nWe deployed QP-OneModel online at Xiaohongshu, with the de-\nployment pipeline shown in Figure 3, and utilized the Xiaohongshu\nA/B testing platform to evaluate its performance. We randomly sam-\npled 5% of the online traffic as the treatment group and another 5%\nas the control group. To mitigate the impact of traffic fluctuations,\nthe experiment was conducted over a period of at least 14 days.\nWe designed two sets of experiments to evaluate the fundamental",
            "content": "Huang et al. Table 5: Overall performance comparison on Document Intent and Authority Intent tasks. Best and second-best results are marked in bold and underlined, respectively. Model Qwen2.5-7B-Instruct RedOne-7B Qwen3-8B RedOne2.0-8B QP-OneModel-8B (Ours) Qwen3-32B Size 7B 7B 8B 8B 8B 32B In-Context Learning (Few-shot) Supervised Fine-Tuning (SFT) Doc. Intent Auth. Intent Doc. Intent Auth. Intent Acc. 51.41 70. 74.85 75.80 82.40 74.80 F1 42.82 56.26 61.84 62.58 62.99 61. Acc. 34.50 48.78 59.97 70.56 71.56 F1 28.10 39.96 49.48 63.00 60. Acc. 91.00 91.13 91.27 91.80 91.80 F1 75.72 77.22 76.56 77.88 78. 71.62 64.16 92.33 79.20 Acc. 76.46 76. 76.80 76.36 77.34 77.14 F1 71.41 71.74 71.94 71.11 72.76 72. signals produced by QP-OneModel and the extrinsic utility of the generated intent descriptions. 5.3.1 Experimental Setup. Experiment 1: Fundamental Signal Evaluation. The goal of this experiment is to assess the accuracy of the core query processing signals. In the treatment group, we replaced the signals (including NER, taxonomy, and term weights) generated by the previous BERT-based model with those produced by QP-OneModel. Experiment 2: Downstream Application (Query Rewriting). This experiment evaluates the effectiveness of the \"Intent Descriptions\" generated by QP-OneModel in downstream tasks. We utilized Query Rewriting as the testbed, which leverages the original query and global statistical signals to generate rewrites that better satisfy the user intent. Control Group: Uses the current production baseline, LLM-based query rewriter. Treatment Group: Augments the baseline model with Intent Descriptions. These descriptions serve as Chainof-Thought (CoT) supervision during training and RAG inputs during inference. 5.3.2 Evaluation Metrics. We adopted the following metrics to measure performance. Note that for our platform, small absolute changes in these metrics can indicate statistical significance. DCG 0/1: relevance metric where professional annotators label top-8 results as relevant or irrelevant. This metric tracks the cumulative penalty of irrelevant results; lower value indicates higher ranking precision. An absolute decrease of 0.15 is considered statistically significant. Zero/Few-Result Rate: The percentage of queries returning extremely few results (typically 1-2 items). Note Effective CTR (NECTR): Measures clicks leading to meaningful engagement (e.g., long dwell time, likes). An absolute increase of 0.1% is considered statistically significant. SAU-Retention: The next-day retention rate of Search Active Users, serving as core indicator of user satisfaction. 5.3.3 Experimental Results. The online A/B test results are presented below. All reported changes represent absolute deltas. 8 Table 6: Online A/B testing results. Top: Evaluation of fundamental structural signals (NER, Term Weighting, etc.). Bottom: Verification of the Intent Descriptions effectiveness, showing that augmenting downstream Query Rewriting task with this novel generative output improves user retention."
        },
        {
            "title": "Baseline\nOurs",
            "content": "Î”DCG 0/1 Î”Zero/Few-Result 0 -0.21% 0 -0.4631%"
        },
        {
            "title": "Model",
            "content": "Î”SAU-Retention Î”NECTR Baseline + Intent Desc. 0 +0.044% 0 +0.17% Fundamental Signal Performance. In Experiment 1, replacing the baseline signals with QP-OneModel resulted in improved relevance. The DCG 0/1 metric decreased by 0.21%, which exceeds the significance threshold of 0.15%. We attribute this to the synergy between our unified paradigm, which mitigates error propagation, and the RL-driven alignment that enforces complex business logic. Additionally, the 0.4631% drop in Zero/Few-Result Rate validates our domain-specific backbone. We believe that by mastering informal slang during knowledge injection, the model bridges semantic gaps in long-tail queries where baselines failed. Downstream Application Performance. In Experiment 2, incorporating Intent Descriptions into the rewriting task improved user engagement metrics. The Note Effective CTR (NECTR) increased by +0.17%, which is statistically significant compared to the 0.1% threshold. Furthermore, the core SAU-Retention metric increased by +0.044%. These results suggest that the intent signals provided by QP-OneModel contribute to better semantic understanding and user experience in downstream applications. QP-OneModel: Unified Generative LLM for Multi-Task Query Understanding in Xiaohongshu Search Table 7: Qualitative analysis of challenging cases. The results demonstrate QP-OneModels capability to resolve semantic sparsity (e.g., inferring EstÃ©e Lauder from 1c1) and context ambiguity (e.g., distinguishing Military vs. Gaming contexts). Query Case 1c1 Structural Output (NER & Taxonomy) Beauty / Makeup [1c1: Product Series] 163 Guards Tank Regiment 1. Social Science / Military 2. Gaming / Console retro-x Clothing - Outdoor [retro-x: Product Series] Generated Intent Description User is selecting EstÃ©e Lauder foundation; seeks actual color performance of shade 1c1, comparison with other shades (e.g., 2c0), and skin tone suitability advice. User seeks to understand the historical background, organizational structure, and combat performance of the 163rd Guards Tank Regiment (or in-game vehicle stats). User considers purchasing Patagonia Retro-X fleece; seeks detailed performance evaluations (wind resistance), target audience (kids/adults), and price advice."
        },
        {
            "title": "5.4 Qualitative Analysis\nTable 7 presents a qualitative analysis of representative cases, il-\nlustrating how QP-OneModel utilizes its generative capabilities to\nhandle challenging queries.",
            "content": "Overcoming Semantic Sparsity via Latent Linking. For the query \"1c1\", which consists solely of an alphanumeric code, discriminative baselines often struggle to extract meaningful features. In contrast, QP-OneModel successfully identifies the unmentioned parent brand (EstÃ©e Lauder) and infers the users implicit goal of shade comparison. This highlights the models ability to leverage internal knowledge to bridge the semantic gap in cryptic inputs. Resolving Polysemy and Enhancing Granularity. The model also effectively handles ambiguity and intent deepening. For \"163 Guards Tank Regiment\", it captures the multi-faceted nature of the query by identifying both the explicit military fact and the latent gaming context (simulation games). Furthermore, for \"retrox\", the model expands the product entity into actionable attributes such as wind resistance and price, providing downstream rankers with rich semantic signals."
        },
        {
            "title": "6 Conclusion\nIn this paper, we introduce QP-OneModel, a unified generative\nframework that reformulates heterogeneous query processing sub-\ntasks into a single sequence-to-sequence paradigm. By leveraging\nthe domain-specific RedOne backbone and a progressive multi-\nreward reinforcement learning strategy, our approach significantly\noutperforms discriminative baselines, achieving a 7.35% increase in\noverall score and boosting F1 scores in complex NER and Term\nWeighting tasks by 9.01% and 9.31%, respectively. Moreover, it\ndemonstrates robust generalization on unseen QP tasks, surpassing\nthe significantly larger Qwen3-32B by 7.60% accuracy on Docu-\nment Intent under ICL setting. Extensive online A/B tests confirm\nits industrial value: deploying QP-OneModelâ€™s fundamental signals\nimproves DCG 0/1 by 0.21%, while leveraging its generated intent\ndescriptions for downstream query rewriting yields a 0.044% uplift\nin user retention. Currently, QP-OneModel has been fully deployed\nto serve the Search Results Page traffic at Xiaohongshu.",
            "content": "9 References [1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023. Qwen technical report. arXiv preprint arXiv:2309.16609 (2023). [2] David Bamman, Jacob Eisenstein, and Tyler Schnoebelen. 2014. Distributed representations of geographically situated language. In Proceedings of ACL. 828 834. John Blitzer, Mark Dredze, and Fernando Pereira. 2007. Biographies, Bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification. In Proceedings of ACL. 440447. [3] [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Proceedings of NeurIPS, Vol. 33. 18771901. [5] Ben Chen, Xian Guo, Siyuan Wang, Zihan Liang, Yue Lv, Yufei Ma, Xinlong Xiao, Bowen Xue, Xuxin Zhang, Ying Yang, et al. 2025. Onesearch: preliminary exploration of the unified end-to-end generative framework for e-commerce search. arXiv preprint arXiv:2509.03236 (2025). [6] Bruce Croft, Donald Metzler, and Trevor Strohman. 2010. Search Engines: Information Retrieval in Practice. [7] Zhuyun Dai and Jamie Callan. 2019. DeepCT: Context-aware term weighting [8] for first stage retrieval. In Proceedings of SIGIR. 415424. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 41714186. [9] Chenhe Dong, Shaowei Yao, Pengkun Jiao, Jianhui Yang, Yiming Jin, Zerui Huang, Xiaojiang Zhou, Dan Ou, Haihong Tang, and Bo Zheng. 2025. TaoSR1: The thinking model for e-commerce relevance search. arXiv preprint arXiv:2508.12365 (2025). Jacob Eisenstein. 2013. Bad language: Are character-level models bad for social media text?. In Proceedings of NAACL-HLT. 865874. [10] [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. arXiv preprint arXiv:2501.12948 (2025). [12] Xian Guo, Ben Chen, Siyuan Wang, Ying Yang, Chenyi Lei, Yuqing Ding, and Han Li. 2025. OneSug: The Unified End-to-End Generative Framework for E-commerce Query Suggestion. arXiv preprint arXiv:2506.06913 (2025). [13] Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lexical normalisation for social media text. In Proceedings of IJCNLP. 368378. [14] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In Proceedings of ICLR. [15] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of CIKM. 23332338. [16] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Mike Bendersky. 2023. Query expansion by prompting large language models. arXiv preprint arXiv:2305.03653 (2023). Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. arXiv preprint arXiv:2001.08361 (2020). [17] [18] John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML. [19] Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of NAACL-HLT. 260270. [20] Hang Li. 2019. Deep learning for information retrieval: Where we are and where we are going. SIGIR Forum 52, 2 (2019), 2547. [21] Donald Metzler, Yi Tay, Dara Gupta, and Hyung Won Chung Barber. 2021. Rethinking search: making domain experts out of dilettantes. ACM SIGIR Forum 55, 1 (2021), 127. [22] David Nadeau and Satoshi Sekine. 2007. survey of named entity recognition and classification. Lingvisticae Investigationes 30, 1 (2007), 326. [23] OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [24] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. In Proceedings of NeurIPS, Vol. 35. 2773027744. [25] Qwen, Yang An, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115 [26] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. In Journal of Machine Learning Research, Vol. 21. 167. [27] Stephen Robertson and Hugo Zaragoza. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends in Information Retrieval 3, 4 (2009), 333389. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017). [28] [29] Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison. 2015. Hidden technical debt in machine learning systems. In Proceedings of Huang et al. NeurIPS. 25032511. [30] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Alan Song, Mingchuan Xiao, Y.K. Li, Y. Zhang, et al. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv preprint arXiv:2402.03300 (2024). [31] Marina Sokolova and Guy Lapalme. 2009. systematic analysis of performance measures for classification tasks. Information Processing & Management 45, 4 (2009), 427437. [32] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NeurIPS. 59986008. [34] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query expansion with [35] large language models. In Proceedings of EMNLP. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of NeurIPS. [36] Feng Xia, Li Liu, Jie Li, Jianhua Ma, and Athanasios Vasilakos. 2013. Socially aware networking: survey. IEEE Systems Journal 9, 3 (2013), 904921. [37] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 Technical Report. arXiv preprint arXiv:2505.09388 (2025). [38] Fei Zhao, Chonggang Lu, Haofu Qian, Fangcheng Shi, Zijie Meng, Jianzhao Huang, Xu Tang, Zheyong Xie, Zheyu Ye, Zhe Xu, et al. 2025. RedOne 2.0: Rethinking Domain-specific LLM Post-Training in Social Networking Services. arXiv preprint arXiv:2511.07070 (2025). [39] Fei Zhao, Chonggang Lu, Zheyong Xie, Ziyan Liu, Haofu Qian, Jianzhao Huang, Fangcheng Shi, Zijie Meng, Hongcheng Guo, Mingqian He, et al. 2025. RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: Industry Track. 26482674. [40] Wayne Xin Zhao et al. 2023. survey of large language models. arXiv preprint arXiv:2303.18223 (2023). [41] Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu. 2013. Deep learning for Chinese word segmentation and POS tagging. In Proceedings of EMNLP. 647657."
        }
    ],
    "affiliations": [
        "Xiaohongshu Inc., China"
    ]
}