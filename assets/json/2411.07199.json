{
    "paper_title": "OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision",
    "authors": [
        "Cong Wei",
        "Zheyang Xiong",
        "Weiming Ren",
        "Xinrun Du",
        "Ge Zhang",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with a high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to a single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present \\omniedit, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) \\omniedit is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose a new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated a test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that \\omniedit can significantly outperform all the existing models. Our code, dataset and model will be available at \\url{https://tiger-ai-lab.github.io/OmniEdit/}"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 1 1 ] . [ 1 9 9 1 7 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "OMNIEDIT: BUILDING IMAGE EDITING GENERALIST MODELS THROUGH SPECIALIST SUPERVISION 1,3Cong Wei, 2,3Zheyang Xiong*, 1,3Weiming Ren, 4Xinrun Du, 1,4Ge Zhang, 1,3Wenhu Chen 1University of Waterloo, 2University of Wisconsin-Madison, 3Vector Institute, 4M-A-P cong.wei@uwaterloo.ca, zxiong44@wisc.edu, wenhuchen@uwaterloo.ca https://tiger-ai-lab.github.io/OmniEdit/ Figure 1: Editing high-resolution multi-aspect images with OMNI-EDIT. OMNI-EDIT is an instruction-based image editing generalist capable of performing diverse editing tasks across different aspect ratios and resolutions. It accurately follows instructions while preserving the original images fidelity. We suggest zooming in for better visualization. First authors. Zheyang works as an intern at Vector Institute."
        },
        {
            "title": "ABSTRACT",
            "content": "Instruction-guided image editing methods have demonstrated significant potential by training diffusion models on automatically synthesized or manually annotated image editing pairs. However, these methods remain far from practical, real-life applications. We identify three primary challenges contributing to this gap. Firstly, existing models have limited editing skills due to the biased synthesis process. Secondly, these methods are trained with datasets with high volume of noise and artifacts. This is due to the application of simple filtering methods like CLIP-score. Thirdly, all these datasets are restricted to single low resolution and fixed aspect ratio, limiting the versatility to handle real-world use cases. In this paper, we present OMNI-EDIT, which is an omnipotent editor to handle seven different image editing tasks with any aspect ratio seamlessly. Our contribution is in four folds: (1) OMNI-EDIT is trained by utilizing the supervision from seven different specialist models to ensure task coverage. (2) we utilize importance sampling based on the scores provided by large multimodal models (like GPT-4o) instead of CLIP-score to improve the data quality. (3) we propose new editing architecture called EditNet to greatly boost the editing success rate, (4) we provide images with different aspect ratios to ensure that our model can handle any image in the wild. We have curated test set containing images of different aspect ratios, accompanied by diverse instructions to cover different tasks. Both automatic evaluation and human evaluations demonstrate that OMNI-EDIT can significantly outperform all the existing models. Our code, dataset and model will be available at https://tiger-ai-lab.github.io/OmniEdit/"
        },
        {
            "title": "INTRODUCTION",
            "content": "Image editing, particularly when following user instructions to apply semantic transformations to real-world photos, has seen significant advancements. Recently, text-guided image editing (Brooks et al., 2023) has gained prominence over traditional methods such as mask-based or region-based editing (Meng et al., 2022). With the rise of diffusion models (Rombach et al., 2022; Podell et al., 2024; Chen et al., 2024a; Sauer et al., 2024), numerous diffusion-based image editing techniques have emerged. Generally, they can be roughly divided into two types: (1) Inversion-based methods (Parmar et al., 2023; Kawar et al., 2023; Gal et al., 2023; Xu et al., 2023; Tumanyan et al., 2023; Tsaban & Passos, 2023) propose to perform zero-shot image editing by inverting the diffusion process and manipulating the attention map in the intermediate diffusion steps to achieve desired editing goal. (2) End-to-end methods (Brooks et al., 2023; Zhang et al., 2024a; Sheynin et al., 2024; Zhao et al., 2024; Fu et al., 2024) propose to fine-tune an existing diffusion model on large-scale image editing pairs to learn the editing operation in an end-to-end fashion. End-to-end methods have generally achieved better performance than inversion-based methods and gained higher popularity. Table 1: Comparison of OMNI-EDIT with all the existing end-to-end image editing models. The scores are based on preliminary studies on around 50 prompts. Property InstructP2P MagicBrush UltraEdit MGIE HQEdit CosXL OMNI-EDIT Real Image? Any Res? High Res? Obj-Swap Obj-Add Obj-Remove Attribute Back-Swap Environment Style Training Dataset Properties Fine-grained Image Editing Skills Despite their effectiveness, end-to-end methods face significant limitation: the scarcity of humanannotated image editing pairs. As result, all current end-to-end approaches depend on synthetic training data. For instance, existing datasets are synthesized using techniques such as"
        },
        {
            "title": "Preprint",
            "content": "Prompt2Prompt (Hertz et al., 2023) or mask-based editing models like SD-Inpaint (Rombach et al., 2022), and DALLE-2/3 (Ramesh et al., 2022; Betker et al., 2023). However, these synthetic data generation pipelines exhibit significant biases, resulting in the following limitations: Limited Editing Capabilities: The synthetic data is heavily influenced by the underlying generation models. For example, Prompt2Prompt struggles with localized edits, such as adding, removing, or swapping objects, while SD-Inpaint and DALLE-2 are ineffective at global edits, such as style or background changes. As result, models trained on such data inherit these limitations. Poor Data Quality Control: Most approaches use simplified filtering mechanisms like CLIPscore (Radford et al., 2021) or DINO-score (Caron et al., 2021) to automatically select training samples. However, recent studies (Ku et al., 2024) show that these metrics exhibit poor correlation with actual data quality, leading to suboptimal training data that negatively impacts the model. Lack of Support for Varying Resolutions: All current models are trained on square image editing pairs, making their generalization to non-square images poor. In our preliminary studies, we curate few prompts for seven different desired tasks to observe their success rate across the board. We show our findings in Table 1. This show that these models are truly biased in their skills caused by the underlying synthesis pipeline. In this paper, we introduce OMNI-EDIT, novel model designed to address these challenges through four key innovations: 1. Specialist-to-Generalist Supervision: We propose learning generalist editing model, OMNIEDIT, by leveraging supervision from multiple specialist models. Unlike previous approaches that rely on single expert, we conduct an extensive survey and construct (or train) seven experts, each specializing in different editing task. These specialists provide supervisory signals to OMNI-EDIT. 2. Importance Sampling: To ensure high-quality training data, we employ large multimodal models to assign quality scores to synthesized samples. Given the computational cost of GPT4o (Achiam et al., 2023), we first distill its scoring ability into InternVL2 (Chen et al., 2024b) through medium-sized samples. Then we use the InternVL2 model for large-scale scoring. 3. EditNet Architecture: We introduce EditNet, novel diffusion-transformer-based architecture (Peebles & Xie, 2022) that facilitates interaction between the control branch and the original branch via intermediate representations. This architecture enhances OMNI-EDIT ability to comprehend diverse editing tasks. 4. Support for Any Aspect Ratio: During training, we incorporate mix of images with varying aspect ratios as well as high resolution, ensuring that OMNI-EDIT can handle images of any aspect ratio with any degradation in the output quality. We curate an image editing benchmark OMNI-EDIT-BENCH, which contains diverse images of different resolutions and diverse prompts that cover all the listed editing skills. We perform comprehensive automatic and human evaluation to show the significant boost of OMNI-EDIT over the existing baseline models like CosXL-Edit (Boesel & Rombach, 2024), UltraEdit (Zhao et al., 2024), etc. On automatic metrics like VIEScore (Ku et al., 2024), we can outperform all the existing approaches by reasonable margin in terms of both perceptual quality and semantic consistency. We also performed human evaluation and observed an overall improvement 20% over the best baseline editing model CosXL-Edit (Boesel & Rombach, 2024)."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 TEXT-TO-IMAGE DIFFUSION MODELS Diffusion models (Song et al., 2021; Ho et al., 2020) are class of latent variable models parameterized by θ, defined as pθ(x0) := (cid:82) pθ(x0:T ) dx1:T , where x0 q(x0) represents the original data, and x1, . . . , xT are progressively noisier latent representations of the input image x0. Throughout the process, the dimensionality of x0 and the latent variables x1:T remains consistent, with x0:T Rd, where corresponds to the product of the images height, width, and channels. The forward (diffusion) process, denoted as q(x1:T x0), is predefined Markov chain that incrementally adds Gaussian noise to the data according to pre-defined schedule {βt}T t=1. The process of forward"
        },
        {
            "title": "Preprint",
            "content": "diffusion is defined as: q(x1:T x0) = (cid:89) t=1 q(xtxt1), q(xtxt1) := (xt; (cid:112)1 βt xt1, βtI), (1) where denotes Gaussian distribution, and βt controls the amount of noise added at each step. The objective of diffusion models is to reverse this diffusion process by learning the distribution pθ(xt1xt), which enables the reconstruction of the original data x0 from noisy latent xt. This reduces to denoising problem where the model ϵθ is trained to denoise the sample xt q(xtx0) back into x0. The maximum log-likelihood training objective breaks down to minimizing the weighted mean squared error between the models prediction ˆxθ(xt, c) and the true data x0: arg max θ log pθ(x0c) = arg min θ E(x0,c)D (cid:2)Eϵ,t (cid:2)wt ˆxθ(xt, c) x02 2 (cid:3)(cid:3) , (2) where (x0, c) pairs come from the dataset D, with representing the text prompt. The term wt is weighting factor applied to the loss at each timestep t. For simplicity, prior papers (Song et al., 2021; Ho et al., 2020; Karras et al., 2022) will set wt to be 1. 2.2 INSTRUCTION-BASED IMAGE EDITING IN SUPERVISED LEARNING Instruction-based image editing can be formulated as supervised learning problem. Existing methods (Brooks et al., 2023; Zhang et al., 2024a) often adopt paired training dataset of text editing instructions and images before and after the edit. An image editing diffusion model is then trained on this dataset. The latent diffusion objective is defined as: arg max θ log pθ(x 0x0, c) = arg min θ E(x 0,x0,c)D (cid:2)Eϵ,tˆxθ(xt, c) 02 2 (cid:3) , (3) where (x denoting the editing instruction and 0, x0, c) triples are sampled from the dataset with x0 denoting the source image, 0 denoting the target image."
        },
        {
            "title": "3 LEARNING WITH SPECIALIST SUPERVISION",
            "content": "In this section, we introduce the entire specialist-to-generalist learning framework to build OMNIEDIT. We describe the overall learning objective in subsection 3.1. We then describe how we learn the specialists in subsection 3.2 and the importance weighting function in subsection 3.3. In Figure 2, we show the overview of the OMNI-EDIT training pipeline. 3.1 LEARNING OBJECTIVE We assume there is groundtruth editing model p(xx, c), which can perform any type of editing tasks perfectly according to the instruction c. Our goal is to minimize the divergence between pθ(xx, c) with p(xx, c) by updating the parameters θ: (cid:88) (cid:88) (cid:88) L(θ) := DKL(p(xx, c)pθ(xx, c)) = p(xx, c) log pθ(xx, c) + (4) x,c x,c where is constant, which we leave out in the following derivation. However, since we dont have access to p(xx, c), we adopt importance sampling for approximation: L(θ) = (cid:88) (cid:88) q(xx, c) p(xx, c) q(xx, c) log pθ(xx, c) x,c E(x,c)D E(x,c)D (cid:2)Exq(xx,c) [λ(x, x, c) log pθ(xx, c)](cid:3) (cid:2)Exqs(xx,c) [λ(x, x, c) log pθ(xx, c)](cid:3) (5) where q(xx, c) is the proposal distribution and λ() is the importance function. To better approximate the groundtruth distribution p(xx, c), we propose to use an ensemble model q(xx, c). In essence, q(xx, c) := qs(xx, c), where qs is specialist distribution decided by the type of the"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Overview of the OMNI-EDIT training pipeline. Editing Tasks Object Swap Object Removal Object Addition Table 2: Task Definitions and Examples Definition Instruction Example describes an object to replace by specifying both the object to remove and the new object to add, along with their properties such as appearance and location. Replace the black cat with brown dog in the image. describes which object to remove by specifying the objects properties such as appearance, location, and size. Remove the black cat from the image. describes new object to add by specifying the objects properties such as appearance and location. Add red car to the left side of the image. Attribute Modification describes how to modify the properties of an object, such as changing its color and facial expression. Change the blue car to red car. Background Swap Environment Change Style Transfer describes how to replace the background of the image, specifying what the new background should be. Replace the background with space-ship interior. describes change to the overall environment, such as the weather, lighting, or season, without altering specific objects. Change the scene from daytime to nighttime. describes how to apply specific artistic style or visual effect to the image, altering its overall appearance while keeping the content the same. Apply watercolor painting style to the image. instruction (e.g. object removal, object addition, stylization, etc). Combing with Equation 3, our objective can be rewritten as: arg min θ L(θ) = arg min θ E(x,c)DExqs(xx,c)λ(x, x, c) (cid:2)Eϵ,tˆxθ(xt, x, c) x2 2 (cid:3) . (6) The whole process can be described as: we first sample pair from dataset D, and then choose the corresponding specialist qs to sample demonstrations for the our editing model ˆxθ(xt, x, c) to approximate with an importance weight of λ(x, x, c). We formally provide the algorithm in 1. In our specialist-to-generalist framework, we need to have series of specialist models {qs()}s and an importance function λ(). We describe them separately in subsection 3.2 and subsection 3.3. 3.2 CONSTRUCTING SPECIALIST MODELS We group the image editing task into 7 categories as summarized in Table 2. For each category, we train or build task specialist ps(x x, c) to generate millions of examples. Table 2 provides"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: InternVL2 as scoring function before (top right) and after (bottom right) fine-tuning on GPT-4os response. On the top right, the original InternVL2 fails to identify the unusual distortions in the edited image it also does not spot the error when the edited image fails to meet the specified editing instructions. On the bottom right, finetuned-InternVL2 successfully detects such failures and serve as reliable scoring function. detailed information on task groups and example editing instructions c. In this section, we briefly summarize each specialist, with details available in Appendix A.1. Object Replacement. We trained an image-inpainting model to serve as the specialist qobj replace for object replacement. Given image and an object caption cobj and object mask Mobj. The qobj replace can fill the content indicated by the mask with an object in cobj. We then generate an object replacement sample by masking out an existing object and fill the image with new object. Object Removal. We trained an image inpainting model to serve as the specialist qobj removal for object removal. We use similar procedure as in the object replacement but use predicted background content caption to inpaint the masked image. Object Addition. We treat object addition as the inverse task of object removal. Specifically, for each pair of editing examples generated by the object removal specialist, we reverse the roles of the source and target images to create new pair. Attribute Modification. We adopt the Prompt-to-Prompt (P2P) (Hertz et al., 2023) pipeline to generate examples. To enable precise modification, we adapt the method from Sheynin et al. (2024) where we provide mask Mobj for the object and force P2P to only make edits inside the mask. Background Swap. We trained an image inpainting model to serve as the specialist qobj background swap. We use similar procedure as in the object replacement but use an inverse mask of the object to indicate the background and guide the inpainting. Environment Modification. For environment modification, we use P2P pipeline to generate original and edited image. To ensure structural consistency between two images, we apply mask of the foreground to maintain details in the foreground while changing the background. Style Transfer. We use CosXL-Edit (Boesel & Rombach, 2024) as the specialist model as its training data contains large number of style transfering examples. We provide CosXL-Edit with (x, c), and let it generates the edited image x. 3.3 IMPORTANCE WEIGHTING The importance weighting function λ takes as input tuple of source image, edited image, and editing prompt. Its purpose is to assign higher weights to data points that are more likely to be sampled from the ground truth distribution, and lower weights to the unlikely ones. This is essentially quality measure to up-weight high-quality samples. Unlike previous work, we do not use CLIP score because prior work (Jiang et al., 2024) has shown its low correlation with human judges. Instead, we propose to use large multimodal models (LMMs) to approximate the weighting function, as they demonstrate strong image understanding. Following VIEScore (Ku et al., 2024), we designed prompting template for GPT-4o (Achiam et al., 2023) to evaluate the image editing pairs and output"
        },
        {
            "title": "Preprint",
            "content": "4:"
        },
        {
            "title": "Architecture",
            "content": "Figure and InstructPix2Pix(Channel-wise concatenation) for DiT models. Unlike ControlNets parallel execution, EditNet allows adaptive adjustment of control signals by intermediate representations interaction between the control branch and the original branch. EditNet also updates the text representation, enabling better task understanding. EditNet(ours),"
        },
        {
            "title": "ControlNet",
            "content": "between score on scale from 0 to 10. We then filter out data with score greater than or equal to 9, so the LMM essentially serves as binary weighting function: λ(x, x, c) = (cid:26)1, if LMM(prompt, x, x, c) 9 0, otherwise Details of the prompt template are provided in the Appendix. While the GPT-4o is an effective choice for this task, scoring large-scale datasets with millions of examples is extremely costly and time-consuming. Therefore, we employ knowledge distillation from GPT-4o to smaller 8B model, InternVL2 (Chen et al., 2024b). For each task, we sample 50K data points and instruct GPT-4o to output both score and score rationale. We fine-tune InternVL2 on these GPT-4o-generated examples. After fine-tuning, InternVL2 performs as an ideal scoring function due to its smaller size and efficiency. comparison of the models performance before and after fine-tuning is presented in the Appendix. Finally, we apply the fine-tuned InternVL2 model to filter data across dataset with millions of samples. Only examples with score of 9 are retained, resulting in curated training dataset of 775K examples. We visualize InternVL2s response as scoring function before and after fine-tuning in Figure 3. We observe that fine-tuning InternVL2 on GPT-4os response effectively turns InternVL2 into realiable scoring function and it can identify unusual distortions or unsuccessful edit that does not follow the editing instruction. Additional dataset statistics are detailed in the Appendix."
        },
        {
            "title": "4 EDITNET",
            "content": "We found that directly fine-tuning pre-trained high-quality diffusion model like SD3 using channelwise image concatenation methods (Brooks et al., 2023) compromises the models original representational capabilities (see Figure 7 and Section 5.2 for details comparison). To enable diffusion transformer to perform instruction-based image editing while preserving its original capabilities, we introduce EditNet to build OMNI-EDIT. EditNet can effectively transform common DIT models like SD3 into editing models. As illustrated in Figure 4, we replicate each layer of the original DIT block as control branch. The control branch DIT blocks allow interaction between the original DIT tokens, conditional image tokens, and the editing prompts. The output of the control branch tokens is then added to the original DIT tokens and editing prompts. Since the original DIT blocks are trained for generation tasks and are not aware of the editing instructions specifying which contents to modify and how to modify them, this design allows the control branch DIT to adjust the representations of the original DIT tokens and editing prompts according to the editing instruction, while still leveraging the strong generation ability of the original DIT. Compared to ControlNet (Zhang et al., 2023), our approach offers two key advantages that make it more suitable for image editing tasks: First, ControlNet does not update text representations, making it challenging to execute editing tasks based on instruction, particularly object removal, as it fails to understand the removal intent (see Figure 6). Secondly, ControlNets control branch operates in parallel without access to the original branchs intermediate representations. This fixed precompu-"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Qualitative comparison between baselines and OMNI-EDIT on subset of the test set. tation of control signals restricts the overall representation power of the network. We provide an ablation study on the OMNI-EDIT architecture design in Section 5.2."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "In this section, we first provide statistics of the OMNI-EDIT training set and test set in Table 5. Then we introduce the human evaluation protocol in Section 5, and comparative baseline system in 5. We present the main results in Section 5.1, highlighting the advantages of OMNI-EDIT in tacking multiaspect ratio, multi-resolution, and multi-task image editing. In Section 5.2, we study the advantages of importance sampling for synthetic data. In Section 5.2, we perform an analysis to study the design of OMNI-EDIT. OMNI-EDIT Training Dataset. We constructed the training dataset by sampling high-resolution images with minimum resolution of 1 megapixel from the LAION-5B (Schuhmann et al., 2022) and OpenImageV6 (Kuznetsova et al., 2020) databases. The images cover range of aspect ratios including 1:1, 2:3, 3:2, 3:4, 4:3, 9:16, and 16:9. For the task of object swap, we employed specialist model to generate 1.5 million entries. We then applied InternVL2 for importance weighting, retaining samples with scores of 9 or higher, resulting in dataset of 150K entries for this task. Similarly, we generate 250k-1M samples for each task, then keep the top 10% as the final dataset. The final training dataset comprises 775K entries, with detailed information provided in Appendix 6. OMNI-EDIT-Bench. To create high-resolution, multi-aspect ratio, multi-task benchmark for instruction-based image editing, we manually collected 62 images from pexels (2024) and LAION5B (Schuhmann et al., 2022). These images cover variety of aspect ratios, including 1:1, 2:3, 3:2, 3:4, 4:3, 9:16, and 16:9. We ensured that the images feature diverse range of scenes and object counts, from single to complex compositions. Additionally, we selected images with relatively high aesthetic score to better align with the practical use cases of image editing. For each image, we tasked the model with performing 7 tasks as outlined in Table 2. This results in total of 434 edits. OMNI-EDIT implementation details. The OMNI-EDIT model is built upon Stable diffusion 3 Medium(Esser et al., 2024) with EditNet architecture. The stable diffusion 3 has 24 DiT layers. Each layer has corresponding EditNet layer. We train OMNI-EDIT on the 775K OMNI-EDIT train-"
        },
        {
            "title": "Preprint",
            "content": "Table 3: Main evaluation results on Omni-Edit-Bench. In each column, the highest score is bolded, and the second-highest is underlined. Models VIEScore (GPT4o) VIEScore (Gemini) Human Evaluation Qavg SCavg Oavg Qavg SCavg Oavg Qavg SCavg Oavg Accavg DiffEdit SDEdit InstructPix2Pix MagicBrush UltraEdit(SD-3) HQ-Edit CosXL-Edit HIVE 5.88 6.71 7.05 6.11 6.44 5.42 8.34 5.35 2.73 2.18 3.04 3.53 4.66 2.15 5.81 3. 2.79 2.78 3.45 3.60 4.86 2.25 6.00 3.57 Inversion-based Methods 6.09 6.31 2.01 2.06 End-to-End Methods 6.46 6.36 6.49 6.18 7.01 5.84 1.88 2.27 4.33 1.71 4.90 2.84 2.39 2.48 2.31 2.61 4.45 1.96 4.81 3.05 - - - - 0.72 0.80 0.82 - - - - - 0.52 0.27 0.56 - - - - - 0.57 0.29 0.59 - - - - - 0.20 0.10 0.35 - OMNI-EDIT - Best baseline 8.38 +0.04 6.66 +0.85 6.98 +0.98 7.06 +0.05 5.82 +0. 5.78 +0.97 0.83 +0.01 0.71 +0.15 0.69 +0.10 0.55 +0.20 ing dataset for 2 epochs on single node with 8 H100 GPUs. Baseline models. We compare OMNI-EDIT with 8 other text-guided image editing baselines: MagicBrush (Zhang et al., 2024a), InstructPix2Pix (Brooks et al., 2023), UltraEdit(SD3) (Zhao et al., 2024), DiffEdit (Couairon et al., 2022), SDEdit (Meng et al., 2022), CosXL-Edit (Boesel & Rombach, 2024), HIVE (Zhang et al., 2024b) and HQ-Edit (Hui et al., 2024). Evaluations Protocol We conduct both human evaluation and automatic evaluation. For the human evaluation, we follow the procedure from Ku et al. (2023) to rate in two criteria: Semantic Consistency (SC) and Perceptual Quality (P Q). Both scores are in {0, 0.5, 1}. For SC, the human subject is asked to rate the consistency between 1) the edited image and the editing instruction (whether the editing instruction is reflected on the edited image) and between 2) the source image and the edited image (whether the model makes the edit that is beyond the editing instruction). For Q, the subject is asked to rate on the quality of edited image). We then calculate overall score = SC that measures the overall quality of the edit. We also calculate the accuracy of the edit, which is defined by the percentage of SC = 1 among all examples. We recruit four human raters and require them to evaluate all the editing examples. For LMMs evaluation, we follow the procedure from Ku et al. (2024) where models (in particular, we chose GPT4o and Gemini) are also asked to give SC and scores but on scale of 0-10. We then normalize the scale to 0-1. 5.1 MAIN RESULTS We provide qualitative comparison with baseline models in Figure 5. We show the top 4 baselines with OMNI-EDIT on subset of the OMNI-EDIT-Bench. We provide more results in Figure 10 and Figure 11. Our main results are detailed in Table 3, where we provide the VIEScore and conduct human evaluation on the Top2 baselines and OMNI-EDIT. In Figure 1, OMNI-EDIT demonstrates its capability to handle diverse editing tasks across various aspect ratios and resolutions. The results are notably sharp and clear, especially in the addition/swap task, where new content is seamlessly integrated. This underscores the effectiveness of the Edit-Net design in preserving the original image generation capabilities of the base text-image generative model. Similarly, in Figure 5, OMNI-EDIT uniquely adds clean and distinct NASA logo onto T-shirt. Table 3 corroborates this with OMNIEDIT achieving the highest Perceptual Quality (PQ) score among the models evaluated. We highlight the efficacy of our proposed specialist-to-generalist learning framework. Unlike baseline models that utilize single method for generating synthetic dataoften the prompt-to-prompt methodThis method typically alters the entire image, obscuring task-specific data. In contrast, OMNI-EDIT leverages task-specific data curated by experts, resulting in clearer task distribution and improved adherence to editing instructions. Both the VIEScore and human evaluations in Table 3 demonstrate that our method significantly outperforms the best baseline in following editing instructions accurately and minimizing over-editing. For instance, baseline models frequently misunderstand the task intent as illustrated in Figure 5, where the CosXL-Edit model fails to recognize the removal task and incorrectly interprets bird addition as swap between panda and bird. Lastly, baseline models often produce blurry images on the OMNI-EDIT-Bench, as they are trained at resolutions limited to 512x512 or even 256x256, and they perform poorly on non-square aspect ratios. For example, with 3:4 aspect ratio, the baselines struggle to perform editing. OMNI-EDIT,"
        },
        {
            "title": "Preprint",
            "content": "trained on data with multiple aspect ratios, maintains robust editing capabilities across the diverse aspect ratios encountered on the Omni-Bench, as evidenced in Figure 5."
        },
        {
            "title": "5.2 ABLATION STUDY",
            "content": "In this section, We provide an ablation study w.r.t importance weighting and EditNet. Ablation study on the importance sampling. We study baseline that utilizes the same architecture as OMNI-EDIT, but instead of applying importance scoring and filtering, we sample 775K examples directly from the 5M pre-filtering dataset as specified in Table 6 and compare it with OMNI-EDIT. As shown in Table 4, we observe significant decrease in VIEScores for both PQ and SC metrics. Ablation Study on OMNI-EDIT Architecture Design. We conducted an analysis of OMNI-EDIT architectural design in comparison to two baseline models: OMNI-EDIT-ControlNet and OMNIEDIT-ControlNet-TextControl and show the result in Table 5. OMNI-EDIT-ControlNet represents the SD3-ControlNet architecture trained on the OMNI-EDIT dataset, where the source image serves as the conditioning image for the control branch. OMNI-EDIT-ControlNet-TextControl is variant of OMNI-EDIT-ControlNet with an added modification: at each layer, we incorporate the text-token output from the control branch into the text-token in the main image generation branch. So this baseline can update the text representation in the main branch but doesnt have the intermediate representation interaction design in EditNet. Table 4: Ablation on importance sampling. Models VIEScore (GPT4o) VIEScore (Gemini) OMNI-EDIT OMNI-EDIT w/o importance sampling 8.38 6.20 6.66 2.95 6.98 3.30 7.06 6. 5.82 1.80 5.78 2.25 Qavg SCavg Oavg Qavg SCavg Oavg Table 5: Ablation on OMNI-EDIT architecture design. Models VIEScore (GPT4o) VIEScore (Gemini) Qavg SCavg Oavg Qavg SCavg Oavg OMNI-EDIT OMNI-EDITControlNet - TextControl OMNI-EDITControlNet 8.38 6.45 6.35 6.66 4.70 4.60 6.98 4.89 4. 7.06 6.50 6.40 5.82 4.35 4.25 5.78 4.48 4.35 Our analysis, as shown in Figure 6, reveals that OMNI-EDIT-ControlNet struggled to accurately capture task intent. This is primarily because the ControlNet branch does not update the text representation. For instance, in object removal tasks, prompts like Remove ObjA are common, yet the original DIT block remains unchanged, causing it to mistakenly generate an image of ObjA. On the other hand, although OMNI-EDIT-ControlNet-TextControl successfully updates the text representation, it still encounters difficulties in content removal. The substantial VIEScores gap between OMNI-EDIT-Controlnet-TextControl and OMNI-EDIT in Table 5 underscores the importance of the intermediate representation interaction design in EditNet. We also compared OMNI-EDIT with the channel-wise token concatenation method used in InstructPix2Pix (see Figure 4). Channel-wise Token concatenation requires fine-tuning the entire network, which can distort the networks original representations. As illustrated in Figure 7, after fine-tuning an SD3 channel-wise concatenation model on OMNI-EDIT training set, the representation of Batman is altered. In contrast, EditNet preserves the original representation of Batman while still learning the object swap task."
        },
        {
            "title": "6 RELATED WORK",
            "content": "Image Editing via Generation. Editing real images according to specific user requirements has been longstanding research challenge (Crowson et al., 2022; Liu et al., 2020; Zhang et al., 2023; Shi et al., 2022; Ling et al., 2021). Since the introduction of large-scale diffusion models, such as Stable Diffusion (Rombach et al., 2022; Podell et al., 2024), significant progress has been made in tackling image editing tasks. SDEdit (Meng et al., 2022) introduced an approach that adds noise to the input image at an intermediate diffusion step, followed by denoising guided by the target text"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: OMNI-EDIT-ControlNet fails to grasp the task intent, while OMNI-EDIT-ControlNetTextControla variant with text-updating branchrecognizes the intent but struggles with content removal. In contrast, OMNI-EDIT accurately removes content. Figure 7: (a) shows the source image. (d) presents images generated by SD3 in response to prompts for an upper body picture of Batman and shiny red vintage Chevrolet Bel Air car. We use the prompts Replace the man with Batman and Add shiny red vintage Chevrolet Bel Air car to the right to OMNI-EDIT and OMNI-EDIT-Channel-Wise-Concatenation, which was trained on OMNIEDIT training data. From (b) and (c), one can observe that OMNI-EDIT preserves the generation capabilities of SD3, while OMNI-EDIT-Channel-Wise-Concatenation exhibits notable degradation in generation capability. description to generate the edited image. Subsequent methods, such as Prompt-to-Prompt (Hertz et al., 2023) and Null-Text Inversion (Mokady et al., 2023), have focused on manipulating attention maps during intermediate diffusion steps for image editing. Other techniques like Blended Diffusion (Avrahami et al., 2022) and DiffEdit (Couairon et al., 2022) utilize masks to blend regions of the original image into the edited output. More recently, the field has seen shift towards supervised methods, such as InstructP2P (Brooks et al., 2023), HIVE (Zhang et al., 2024b), and MagicBrush (Zhang et al., 2024a), which incorporate user-written instructions in an end-to-end framework. Our work follows this direction to develop end-to-end editing models without inversion. Image Editing Datasets. Due to the difficulty of collecting expert-annotated editing pairs, existing approaches rely heavily on synthetic data to train editing models. InstructP2P (Brooks et al., 2023) was the first to curate large-scale editing datasets using prompt-to-prompt filtering with CLIP scores. MagicBrush (Zhang et al., 2024a) subsequently improved data quality by incorporating humanin-the-loop annotation pipeline based on DALLE-2. However, DALLE-2, primarily an inpaintingbased method, struggles with global editing tasks such as style transfer and attribute modification. More recently, HQ-Edit (Hui et al., 2024) utilized DALLE-3 to curate editing pairs, although the source and target images lack pixel-to-pixel alignment, which is critical for preserving fine-grained details. Emu Edit (Sheynin et al., 2024) scaled up the training dataset to 10 million proprietary pairs, resulting in strong performance, but the lack of public access to their model checkpoints or API makes direct comparison difficult. UltraEdit (Zhao et al., 2024) proposed another inpainting-based approach, avoiding the use of DALLE-2 or DALLE-3 for data curation. However, like MagicBrush, it still faces limitations in handling complex global edits. Our work is the first to leverage multiple specialists to significantly expand the range of editing capabilities. Additionally, we are the first to use more reliable large multimodal models, for quality control in the editing process."
        },
        {
            "title": "7 DISCUSSION",
            "content": "In this paper, we identify the imbalanced skills in the existing end-to-end image editing methods and propose new framework to build more omnipotent image editing models. We surveyed the field and chose several approaches as our specialists to synthesize candidate pairs and adopt weighted loss to supervise the single generalist model. Our approach has shown significant quality boost across the broad editing skills. Throughout the experiments, we found that the output quality is highly influenced by the underlying base model. Due to the weakness of SD3, our approach is still not achieving its highest potential. In the future, we plan to use Flux or other more capable base models to see how much further we can reach with the current framework."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of In Proceedings of the IEEE/CVF conference on computer vision and pattern natural images. recognition, pp. 1820818218, 2022. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Frederic Boesel and Robin Rombach. Improving image editing models with generative data refinement. In The Second Tiny Papers Track at ICLR 2024, 2024. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 96509660, 2021. Junsong Chen, YU Jincheng, GE Chongjian, Lewei Yao, Enze Xie, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2418524198, 2024b. Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusionbased semantic image editing with mask guidance. arXiv preprint arXiv:2210.11427, 2022. Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. Vqgan-clip: Open domain image generation and editing with natural language guidance. In European Conference on Computer Vision, pp. 88105, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. GuidIn The Twelfth ing instruction-based image editing via multimodal large language models. International Conference on Learning Representations, 2024. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In The Eleventh International Conference on Learning Representations, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. In The Eleventh International Prompt-to-prompt image editing with cross-attention control. Conference on Learning Representations, 2023. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024."
        },
        {
            "title": "Preprint",
            "content": "Dongfu Jiang, Max Ku, Tianle Li, Yuansheng Ni, Shizhuo Sun, Rongqi Fan, and Wenhu Chen. Genai arena: An open evaluation platform for generative models. arXiv preprint arXiv:2406.04485, 2024. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60076017, 2023. Max Ku, Tianle Li, Kai Zhang, Yujie Lu, Xingyu Fu, Wenwen Zhuang, and Wenhu Chen. Imagenhub: Standardizing the evaluation of conditional image generation models. arXiv preprint arXiv:2310.01596, 2023. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. VIEScore: Towards explainable metrics for conditional image synthesis evaluation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1226812290, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.663. URL https://aclanthology.org/2024.acl-long.663. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. Huan Ling, Karsten Kreis, Daiqing Li, Seung Wook Kim, Antonio Torralba, and Sanja Fidler. EdIn Advances in Neural Information Processing itgan: High-precision semantic image editing. Systems (NeurIPS), 2021. Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan Tran, Xiaogang Wang, and Hongsheng Li. Open-edit: Open-domain image manipulation with open-vocabulary instructions. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XI 16, pp. 89106. Springer, 2020. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 60386047, 2023. Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023 Conference Proceedings, pp. 111, 2023. William Peebles and Saining Xie. Scalable diffusion models with transformers. 2023 ieee. In CVF International Conference on Computer Vision (ICCV), volume 4172, 2022. pexels. Pexels, 2024. URL www.pexels.com. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image In The Twelfth International Conference on Learning Representations, 2024. URL synthesis. https://openreview.net/forum?id=di52zR8xgf. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021."
        },
        {
            "title": "Preprint",
            "content": "Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. HighIn Proceedings of the IEEE/CVF resolution image synthesis with latent diffusion models. conference on computer vision and pattern recognition, pp. 1068410695, 2022. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. arXiv preprint arXiv:2403.12015, 2024. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:2527825294, 2022. Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88718879, 2024. Yichun Shi, Xiao Yang, Yangyue Wan, and Xiaohui Shen. Semanticstylegan: Learning compoIn Proceedings of the sitional generative priors for controllable image synthesis and editing. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1125411264, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021. Linoy Tsaban and Apolinario Passos. Ledits: Real image editing with ddpm inversion and semantic guidance. arXiv preprint arXiv:2307.00522, 2023. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features In Proceedings of the IEEE/CVF Conference on for text-driven image-to-image translation. Computer Vision and Pattern Recognition (CVPR), pp. 19211930, June 2023. Sihan Xu, Yidong Huang, Jiayi Pan, Ziqiao Ma, and Joyce Chai. Inversion-free image editing with natural language. arXiv preprint arXiv:2312.04965, 2023. Fred Zhang. stable-diffusion-prompts-2.47m, 2024. URL https://huggingface.co/ datasets/FredZhang7/stable-diffusion-prompts-2.47M?row=19. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024a. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern visual editing. Recognition, pp. 90269036, 2024b. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024."
        },
        {
            "title": "A APPENDIX",
            "content": "Table 6: Omni-Edit training dataset statistics reflecting the number of samples before and after importance scoring and filtering with o-score 9. Task Pre-Filtering Number After-Filtering Number Object Swap Object Removal Object Addition 1,500,000 1,000,000 1,000,000 Background Swap 500, Environment Change Style Transfer Object Modification Property 500,000 250, 450,000 150,000 100,000 100,000 50,000 100, 25,000 250,000 Total 52,000,000 775,000 i=1 of image-text instruction pairs Algorithm 1 Specialist-to-Generalist Learning Framework Require: Dataset = {(xi, ci)}N Require: task specialist model qk Ensure: Generalist diffusion model parameterized by θ 1: Initialize buffer 2: for each pair of {(xs, cs)} in do 3: 4: 5: 6: 7: end for 8: Train generalist model θ on dataset using Eq. 6 s, xs, cs), λ(x s, xs, cs)} sxs, cs). s, xs, cs) qs = (cs), where : maps from the instruction space to the set of specialists. qs(x Compute importance weight λ(x {(x A.1 TRAINING DATA GENERATION DETAILS A.1.1 OBJECT REPLACEMENT We trained an image-inpainting model to serve as the expert for object replacement. During training, given source image xsrc and an object caption Cobj, we employ GroundingDINO and SAM to generate an object mask Mobj. The masked image is then created by removing the object from the source image: xmasked = xsrc (1 Mobj). (7) Here, denotes element-wise multiplication, effectively masking out the object in xsrc. Both the mask Mobj and the object caption Cobj are provided as inputs to the expert model qobj replace. The expert qobj replace is trained to reconstruct (inpaint) the original source image xsrc from the masked image. To generate data for object replacement, we sample 200K images from the LAION and OpenImages datasets, ensuring diverse range of resolutions close to 1 megapixel. For each image, we utilize GPT-4o to propose five object replacement scenarios. Specifically, GPT-4o identifies five interesting source objects Csrc obj within the image and suggests corresponding target objects Ctrg obj for replacement. For each proposed replacement, we perform the following steps:"
        },
        {
            "title": "Preprint",
            "content": "1. Mask Generation: Use GroundingDINO and SAM to generate the object mask Msrc obj for the source object Csrc obj. 2. Mask Dilation: Apply dilation operation to Msrc obj to expand the mask boundaries. 3. Image Editing: Apply the expert model to generate the edited image xedit by replacing the source object with the target object Ctrg obj: xedit = qobj replace (xsrc (1 Msrc obj), Msrc obj, Ctrg obj) (8) In this equation: xsrc (1 Msrc obj) represents the source image with the target object masked out. Msrc obj is the mask of the source object to be replaced. Ctrg obj is the caption of the target object for replacement. Then pair of instruction-based image editing examples will be: xsrc, xedit, . The instruction initially just be Replace Csrc obj with Ctrg obj . We then employ large multimodal models (LVLM) to generate more detailed natural language instructions. A.1.2 OBJECT REMOVAL Similar to object replacement, we trained an image inpainting model to serve as the expert for object removal. During training, given source image xsrc and an image caption Csrc, we randomly apply strikes to create mask Msrc. The masked image is then created by: xmasked = xsrc (1 Msrc) (9) Both the mask Msrc and the image caption Csrc are provided as inputs to the expert model qobj removal. The expert qobj removal is trained to reconstruct (inpaint) the original source image xsrc from the masked image. To generate data for object removal, we also sample 200K images from the LAION and OpenImages datasets, ensuring diverse range of resolutions close to 1 megapixel. For each image, we utilize GPT-4o to propose five objects to remove and predict the content of the space after removal. Specifically, GPT-4o identifies five interesting source objects Csrc obj within the image and predicts the new content after removing the object Ctrg background. For each proposed removal, we perform the following steps: 1. Mask Generation: Use GroundingDINO and SAM to generate the object mask Msrc obj for the source object Csrc obj. 2. Image Editing: Apply the expert model to generate the edited image xedit by infilling the masked region with the predicted background content Ctrg background: xedit = qobj removal (xsrc (1 Msrc obj), Msrc obj, Ctrg background) . (10) In this equation: xsrc (1 Msrc obj) represents the source image with the target object masked out. Msrc obj is the mask of the source object to be removed. Ctrg background is the predicted content for the background after object removal. Then pair of instruction-based image editing example will be: xsrc, xedit, . Initially, the instruction initially just be Remove Csrc obj from the image We then employ large multimodal models (LVLM) to generate more detailed natural language instructions. A.1.3 OBJECT ADDITION We conceptualize the object addition task as the inverse of the object removal process. Specifically, for each pair of editing examples generated by the object removal expert, we swap the roles of the source and target images to create new pair tailored for object addition. This approach leverages the naturalness and artifact-free quality of the original source images, ensuring high-quality additions. Given pair of editing examples xsrc removal, xedit removal, cremoval generated for object removal and"
        },
        {
            "title": "Preprint",
            "content": "Csrc obj removal represents the object to remove. We transform this pair into an object addition example by swapping xsrc and xedit, and modifying the instruction accordingly. The resulting pair for object addition is xsrc = xedit removal, xedit = xsrc removal, c, where is the new instruction defined as Add Csrc obj removal to the image. A.1.4 ATTRIBUTE MODIFICATION We adapt the Prompt-to-Prompt (P2P) (Hertz et al., 2023) pipeline where text-guided image generation model is provided with pair of captions Csrc, Cedit and injects cross-attention maps from the input image generation to that during edited image generation. For example, pair could be blue backpack, purple backpack with the corresponding editing instruction make the backpack purple. To enable precise attribute modification on the object we want (in our example, the backpack), we adapt the method from Sheynin et al. (2024) where we provide an additional mask Mobj that masks the object. Specifically, to obtain pair of captions, we obtain source captions Csrc from Zhang (2024) and let GPT4 to identify an object Cobj in the original caption Csrc, propose an editing instruction that edits an attribution of Cobj and output the edited caption Cedit with objects attribution reflected. We first let the image generation model to generate source image xsrc using Csrc. We then use GroundingDINO to extract mask Mobj that masks the object from the source image. We then apply P2P generation with caption pair Csrc, Cedit. During the generation, we use the mask to control precise image editing control. In particular, let xsrc,t denote the noisy source image at step and xedit,t denote the noisy edited image at step t, we apply the mask and force the new noisy edited image at time be Mobj xedit,t + (1 Mobj) xsrc,t. In other words, we keep background the same and only edit the object selected. A.1.5 ENVIRONMENT MODIFICATION For environment modification, we use P2P pipeline to generate original and edited image. To ensure structural consistency between two images, we apply mask of the foreground to maintain details in the foreground while changing the background. In particular, given source image caption Csrc, we use GPT4 to identify the foreground (e.g., an object or human) and apply GroundingDINO to extract mask Mforeground. During the generation, let xsrc,t denote the noisy source image at step and xedit,t denote the noisy edited image at t. We apply the mask so that the new noisy edited image at time is Mforeground xsrc,t + (1 Mforeground) xedit,t. We also set τenv = 0.7 so that this mask operation on noisy image is only applied at the first τenv of all timesteps. A.1.6 BACKGROUND SWAP We trained an image inpainting model to serve as the specialist qobj background swap. We use similar procedure as in the object replacement but use an inverse mask of the object to indicate the background to guide the inpainting. A.1.7 STYLE TRANSFER We use CosXL-Edit (Boesel & Rombach, 2024) as the expert style transfer model. We provide CosXL-Edit with xsrc, and let it generates the edited image xedited. A.1. IMPORTANCE SAMPLING We apply the importance sampling as described in Section 3.3. Example prompts that are provided to LMMs are shown in Figure 8 and 9. We compute the Overall score following (Ku et al., 2024) as the importance weight. After importance sampling, we obtain our training dataset described in Table 6."
        },
        {
            "title": "Preprint",
            "content": "Human: You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image(s) based on the given rules. You will have to give your output in this way (Keep your reasoning concise and short.): { score : [...], reasoning : ... } and dont output anything else. Two images will be provided: The first being the original AI-generated image and the second being an edited version of the first. The objective is to evaluate how successfully the editing instruction has been executed in the second image. Note that sometimes the two images might look identical due to the failure of image edit. From scale 0 to 10: score from 0 to 10 will be given based on the success of the editing. - 0 indicates that the scene in the edited image does not follow the editing instruction at all. - 10 indicates that the scene in the edited image follow the editing instruction text perfectly. - If the object in the instruction is not present in the original image at all, the score will be 0. second score from 0 to 10 will rate the degree of overediting in the second image. - 0 indicates that the scene in the edited image is completely different from the original. - 10 indicates that the edited image can be recognized as minimal edited yet effective version of original. Put the score in list such that output score = [score1, score2], where score1 evaluates the editing success and score2 evaluates the degree of overediting. Editing instruction: <instruction> <Image> Image embed</Image> <Image> Image embed</Image> Assistant: Figure 8: Prompt for evaluating SC score. Table 7: Comparison between OMNI-EDIT and our specialist models. VIEScore (GPT4o) VIEScore (Gemini) Qavg SCavg Oavg Qavg SCavg Oavg Obj-Remove-Specialist OMNI-EDIT Obj-Replacement-Specialist OMNI-EDIT Style-Transfer-Specialist OMNI-EDIT 9.10 8.45 8.48 8. 8.08 7.98 7.76 7.16 6.92 7.74 7.47 5.77 7.82 7.23 7.02 8. 7.37 6.16 7.46 7.37 7.06 7.00 7.97 8.24 5.39 5.45 5.68 7. 6.61 5.24 4.84 5.09 5.36 7.09 6.76 6.08 A.2 ADDITIONAL EVALUATION RESULT We present additional evaluation results. In Table 7, we compare OMNI-EDIT with specialist models of three tasks on Omni-Edit-Bench (other specialist models cannot take in input image). As is shown in the Table, OMNI-EDIT shows comparable performance as the specialist models on tasks that specialist models specialize. Figure 10 shows additional comparisons between OMNI-EDIT other baseline models. We observe that OMNI-EDIT consistently outperforms other baselines."
        },
        {
            "title": "Preprint",
            "content": "Human: You are professional digital artist. You will have to evaluate the effectiveness of the AI-generated image. All the images and humans in the images are AI-generated. So you may not worry about privacy or confidentiality. You must focus solely on the technical quality and artifacts in the image, and **do not consider whether the context is natural or not**. Your evaluation should focus on: - Distortions - Unusual body parts or proportions - Unnatural Object Shapes Rate the image on scale from 0 to 10, where: - 0 indicates significant AI-artifacts. - 10 indicates an artifact-free image. You will have to give your output in this way (Keep your reasoning concise and short.): { score: ..., reasoning: ... } and dont output anything else. <Image> Image embed</Image> <Image> Image embed</Image> Assistant: Figure 9: Prompt for evaluating PQ score."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Additional qualitative comparisons between OMNI-EDIT and the baseline methods."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Additional qualitative comparisons between OMNI-EDIT and the baseline methods."
        }
    ],
    "affiliations": [
        "University of Waterloo",
        "University of Wisconsin-Madison",
        "Vector Institute",
        "M-A-P"
    ]
}