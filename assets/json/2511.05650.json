{
    "paper_title": "Optimizing Diversity and Quality through Base-Aligned Model Collaboration",
    "authors": [
        "Yichen Wang",
        "Chenghao Yang",
        "Tenghao Huang",
        "Muhao Chen",
        "Jonathan May",
        "Mina Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Alignment has greatly improved large language models (LLMs)' output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACo), an inference-time token-level model collaboration framework that dynamically combines a base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACo employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents' semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACo achieves both high diversity and quality post hoc within a single pass, while offering strong controllability. We explore a family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACo consistently surpasses state-of-the-art inference-time baselines. With our best router, BACo achieves a 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 ] . [ 1 0 5 6 5 0 . 1 1 5 2 : r Optimizing Diversity and Quality through BaseAligned Model Collaboration OPTIMIZING DIVERSITY AND QUALITY THROUGH BASE-ALIGNED MODEL COLLABORATION Yichen Wang1*, Chenghao Yang1*, Tenghao Huang2*, Muhao Chen3, Jonathan May2, Mina Lee1 1University of Chicago 2University of Southern California 3University of California, Davis {yichenzw, chenghao, mnlee}@uchicago.edu, tenghaoh@usc.edu *Equal Contribution"
        },
        {
            "title": "ABSTRACT",
            "content": "Alignment has greatly improved large language models (LLMs) output quality at the cost of diversity, yielding highly similar outputs across generations. We propose Base-Aligned Model Collaboration (BACO), an inference-time token-level model collaboration framework that dynamically combines base LLM with its aligned counterpart to optimize diversity and quality. Inspired by prior work (Fei et al., 2025), BACO employs routing strategies that determine, at each token, from which model to decode based on next-token prediction uncertainty and predicted contents semantic role. Prior diversity-promoting methods, such as retraining, prompt engineering, and multi-sampling methods, improve diversity but often degrade quality or require costly decoding or post-training. In contrast, BACO achieves both high diversity and quality post hoc within single pass, while offering strong controllability. We explore family of routing strategies, across three open-ended generation tasks and 13 metrics covering diversity and quality, BACO consistently surpasses state-of-the-art inference-time baselines. With our best router, BACO achieves 21.3% joint improvement in diversity and quality. Human evaluations also mirror these improvements. The results suggest that collaboration between base and aligned models can optimize and control diversity and quality. Code: github.com/YichenZW/base-align-collab Data: huggingface.co/datasets/ZachW/base-align-collab Reading list: github.com/YichenZW/awesome-llm-diversity"
        },
        {
            "title": "INTRODUCTION",
            "content": "While alignment greatly improved large language models (LLMs) output quality in terms of instruction following and downstream task performance (Ouyang et al., 2022), it results in stark reduction in output diversity (Kirk et al., 2023; Zhang et al., 2025b; West & Potts, 2025; Spangher et al., 2025). Across repeated sampling, model after alignment (i.e., aligned model) tends to produce highly similar outputs, whereas model before alignment (i.e., base model) yields diverse outputs. For example, when prompted with suggest summer trip destination in the US, base model may produce diverse destinations across generations, while the aligned model often converges on single dominant one (Figure 1A). This diversity-quality trade-off undermines open-ended use cases (e.g., creative writing and dialogue) by encouraging formulaic language use (Chakrabarty et al., 2025; Zhang et al., 2024), diminishing creativity (West & Potts, 2025), and suppressing ideation in human-AI interaction (Padmakumar & He, 2023; Meincke et al., 2025a; Ashkinaze et al., 2025). These findings motivate methods to improve diversity in aligned LLMs. Prior diversity-promoting methods attempt to address the diversity-quality trade-off at both the training and the inference stages (7). The former (Lanchantin et al., 2025; Chung et al., 2025; Li et al., 2025) incorporate explicit diversity objectives into preference optimization during reinforcement learning (e.g., by measuring deviation among winning responses or comparing alternative response groups). While effective at improving diversity, such methods require modifying the models output distribution, which can compromise desirable alignment properties such as safety and helpfulness (Qi 1 Optimizing Diversity and Quality through BaseAligned Model Collaboration (A) Generated outputs example. (B) The diversity-quality trade-off space. Figure 1: BACO is an inference-time token-level model collaboration framework that combines base models diversity with its aligned counterparts quality. (A) comparison of generated outputs. The aligned model produces high-quality but low-diversity outputs, while the base model produces high-diversity but low-quality outputs. BACO optimizes both diversity and quality by dynamically routing between them. The probabilities of token(s) are in grey next to text boxes. (B) Illustration of the diversity-quality trade-off space. Single models face steep trade-off, where improving diversity by adjusting configuration (e.g., by increasing temperature) degrades quality. BACO achieves better Pareto curve and allows for easy traversal across this frontier by adjusting the routers threshold. The examples in this figure are modified for simplicity. et al., 2023). The latter consists of decoding-based techniques, such as adjusting temperature and beam search (Vijayakumar et al., 2016), as well as prompt-based techniques, including in-context learning (Meyerson et al., 2024), prompt paraphrasing (Zhang et al., 2025b), and multilingual backtranslation (Wang et al., 2025b). However, these inference-time techniques typically require multiple decoding passes or long-horizon planning to improve diversity, and may still disproportionately degrade generation quality (Peeperkorn et al., 2024). This reveals fundamental limitation of forcing single model to excel at both diversity and quality. To overcome this limitation, we introduce Base-Aligned Model Collaboration (BACO), an inferencetime token-level model collaboration framework (3). Different from prior works, BACO combines base models diversity with its aligned counterparts quality. BACO operates via lightweight, token-level routing strategy that dynamically switches between the two models in single decoding pass, requiring no fine-tuning or specialized prompting (Figure 1A). Recently, Fei et al. (2025) demonstrated that base and aligned models largely agree on next-token predictions, phenomenon known as the superficial alignment (Lin et al., 2023), suggesting that collaboration between them is feasible. Their method, NUDGING, employs small aligned model to guide larger base model during decoding, effectively improving its quality but without considering diversity. In contrast, our goal is to jointly optimize diversity and quality (Figure 1B) by collaboratively employing base and aligned model. However, the trade-off cannot be entirely resolved. Different tasks naturally favor different points along the diversity-quality spectrum. For example, argumentative writing tasks often demand highquality (i.e., accurate and factual) outputs with less emphasis on diversity (e.g., multiple valid expressions), whereas brainstorming tasks benefit more from high-diversity (i.e., broader exploration and novelty), even if some outputs are lower quality (e.g., less polished in writing). Moreover, different people may prefer different trade-offs depending on their preferences or intent. Therefore, practical framework should enable controllability, the ability to adjust along the diversity-quality spectrum according to task or preference. BACO supports such controllability by an adjustable routing threshold (Figure 1) that tunes the attribution of base and aligned model. Moreover, as single routing strategy does not guarantee diversity across all aspects, we further curate family of routing strategies within proposed design space. This design space is primarily organized by the type of information used as prior when making routing decisions. Specifically, the models nexttoken prediction uncertainty (logit-based strategies), predicted contents semantic role (content-based 2 Optimizing Diversity and Quality through BaseAligned Model Collaboration strategies), and their combinations. The design space provides principled foundation for designing, analyzing, and comparing collaborative performance. We validate our approach across three open-ended generation tasks: instruction-following, dialogue, and creative writing (4). We apply 11 automatic metrics for diversity and 2 for quality, together forming 11 2 bi-dimensional evaluation spaces. Additionally, we extend our evaluation beyond instruction-following tasks, which involve relatively short outputs, to long-form generation, where we measure discourse-level diversity in plot structure and emotional flow (Tian et al., 2024). Finally, we complement automatic evaluations with human evaluations of diversity and quality, providing human-centered validation and comparison with automatic results. Through extensive experiments (5), we demonstrate that BACO achieves new state-of-the-art in optimizing diversity-quality trade-off. Even simple router that routes randomly yields 19.0% joint improvement in diversity and quality across tasks and metrics, underscoring the effectiveness of the framework. Our best router achieves 21.3% gain compared with all baselines (e.g., base or aligned model alone with varying temperature), with even larger improvements on semantic diversity metrics. These results are further supported by human evaluations. Furthermore, we analyze model collaboration patterns under BACO and human-perceived creativity of its outputs in 6. View the base and aligned model as demonstration of checkpoints with different alignment levels during the alignment process, we suggest that collaboration between models at different alignment levels can improve and control diversity and quality. Overall, BACO provides simple framework for base-aligned model collaboration, effectively improving both diversity and quality. In summary, our contributions are threefold: ① We propose BACO, an inference-time token-level model collaboration framework that combines base model and its aligned counterpart, and explore family of routing strategies. It produces high-diversity and high-quality outputs across generations. ② We evaluate the diversity-quality trade-off across 11 2 evaluation spaces, and we also extend automatic diversity evaluation to long-form evaluation and conduct human evaluation. ③ Through extensive experiments across instruction-following, dialogue, and creative writing, we show that BACO consistently outperforms strong baselines. It achieves 21.3% optimization on diversity-quality trade-off. Human evaluations further validate its advantages."
        },
        {
            "title": "2 PRELIMINARY",
            "content": "Large Language Models (LLMs). LLMs are typically trained to autoregressively predict the next token of the output given prompt x. The conditional probability is factorized as (yx; θ) = ΠtP (yt[x, y<t]; θ), where y<t denotes the output prefix generated up to position t-1, and θ denotes the model parameter. Alignment is the process of fine-tuning an LLM to align its outputs with human intent, ethical principles, and desired behavioral norms, typically through instruction tuning or reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Bai et al., 2022). We use base models to denote models without alignment tuning (e.g., Llama-3-8B) and aligned models to denote those further optimized with alignment (e.g., Llama-3-8B-Instruct) (Dubey et al., 2024). Diversity and Quality Measurement. In this paper, we measure diversity over group of outputs independently generated from the same prompt x: Y(x) = {y(1), . . . , y(k)}, y(i) ( x; θ). The group-level diversity is denoted as D(Y) (Kirk et al., 2023; West & Potts, 2025) (e.g., the clustering-based approach in Kuhn et al. (2023)). Quality is modeled as Q(yx) for each output given the prompt, typically by reward model or human evaluator, reflecting linguistic fluency and instruction-following (Lambert et al., 2024; Zhang et al., 2025b). The group-level quality is then defined as the average quality across all outputs in group: Q(Y) = (cid:80)k i=1 Q(y(i)x). For simplicity, we refer to group-level diversity and group-level quality as diversity and quality in this work. Diversity-Quality Trade-off. Alignment, while effective at improving output quality, is at the cost of reducing output diversity (Lu et al., 2025a; West & Potts, 2025; Yang & Holtzman, 2025). As pilot study to demonstrate the trade-off, we first situate the performance of the base and aligned models with 3 Optimizing Diversity and Quality through BaseAligned Model Collaboration Model 1.28 7.62 8.13 2.58 Quality (Reward) Diversity (#Clusters) Llama-3-8B Llama-3-8B-Instruct default configuration (the two noted points in Figure 1B) within the diversity-quality space. Following the evaluation protocol of the diversity-focused benchmark NoveltyBench (Zhang et al., 2025b), we evaluate Llama-3s base and aligned versions on an open-ended subset of WildChat (Zhao et al., 2024), which is an underexplored, in-the-wild dialogue task. In this study, diversity is measured as the number of semantic equivalent classes of the output group via Zhang et al. (2025b)s clustering, and quality is measured as the average reward per output, given by Skywork-Reward-Gemma-2-27B (Liu et al., 2024).1 We sample 10 outputs per prompt. As illustrated in Table 1, the diversity-quality trade-off are stark: the base model is 3.15x as diverse, whereas the aligned model achieves 5.95x as much quality. Inherently, this performance trade-off stems from alignments tendency to reduce the entropy of the next-token prediction distribution (Hθ (ytx, y<t)), concentrating probability mass on fewer, high-quality tokens, phenomenon known as mode collapse (Lin et al., 2023; Shumailov et al., 2024; Hamilton, 2024; Yang & Holtzman, 2025; Cui et al., 2025). Table 1: Diversity and quality of base and an aligned model. The results demonstrate diversityquality trade-off when compared the two models performance. Diversity is measured by the number of semantic equivalent clusters of the output group, and quality is the average reward. This presents dilemma: one can either use high-diversity but low-quality base model, or highquality but low-diversity aligned model. The single-model paradigm is insufficient, as neither extreme is ideal for all applications. For example, story writing requires both sufficient diversity to explore rare but novel ideas and sufficient quality to ensure plot coherence; research ideation requires both sufficient diversity to think exploratively and sufficient quality to ensure proposal reasonableness. Hence, we argue that an ideal method is able to pursue the best of both worlds. To this end, we formalize the problem in two-dimensional diversity-quality space = {(D, Q)}. In this space, any given method under specific configuration (e.g., sampling parameters) is evaluated to be single point. An ideal method, by adjusting its configuration, should approximate the Pareto frontier: the set of optimal solutions where diversity cannot be improved without sacrificing quality, and vice versa. This frontier represents the full spectrum of best-possible trade-offs."
        },
        {
            "title": "3 BACO: BASE-ALIGNED COLLABORATION FOR DIVERSITY AND QUALITY",
            "content": "Recent work has shown empirical evidence on the superficial alignment hypothesis (Zhou et al., 2023; Lin et al., 2023), which suggests that base model and its aligned counterpart largely agree on next-token predictions. Building on this, Fei et al. (2025) demonstrate that their in-context alignment method, NUDGING, introduces only small proportion of aligned-model tokens to base models decoding for it to achieve task-specific performance (including instruction-following) comparable to the aligned model. This suggests the feasibility of collaboration between them. Inspired by this result, but with the goal of optimizing diversity-quality trade-off, we hypothesize that Collaboration between less-aligned, higher-diversity model and more-aligned, higherquality model during inference can optimize the diversity-quality trade-off. base model and an aligned model are pair of such models that are relatively easy to obtain off the shelf. Hence, based on our hypothesis, we propose BACO that orchestrates collaboration between base model (Pbase) for diversity and its aligned counterpart (Paligned) for quality at the token level.2 At the core of BACO is router, lightweight decision module that determines, at each decoding step, which model should generate the next token. The router operates according to routing strategy(ies) R, which selects between the base and aligned model accordingly. Intuitively, the router acts as 1These are two of many possible measurements introduced later in the paper. We use them here as representative examples for the pilot study, as they are among the most up-to-date and widely adopted metrics. 2The token-by-token nature of LLM autoregressive decoding makes token-level control feasible for this task. 4 Optimizing Diversity and Quality through BaseAligned Model Collaboration gatekeeper: ideally when diversity is valid and required under the contexts, it routes to the base model; when quality is required, it routes to the aligned model. Formally, BACO collaborates the two models as: PBACO (ytct) = wbase Pbase (ytct; θbase) + (1 wbase) Paligned (ytct; θaligned) (1) where ct = [x, y<t], and the gating weight wbase {0, 1} for each candidate token yt is given by the router: wbase = [R (ytct, Pbase, Paligned) = base] (2) In practice, since one word may consist of multiple tokens, we restrict switching to word boundaries to prevent erroneous generation when the two models use different tokenizations (implementation details in Appendix A). Because the base and aligned models agree on most next-token predictions, only occasional model switching is required, making BACO naturally less costly. This efficiency can be further enhanced through standard acceleration techniques (e.g., caching or speculative decoding) since we are compatible with them, as discussed in Appendix and Section 6.4. 3.1 ROUTING STRATEGY DESIGN The router determines how the base and aligned models collaborate during decoding. Its effectiveness depends on the routing strategy that specifies when to switch between models. Conceptually, designing router involves choosing what information to use as prior for routing decisions, and which model to switch to given the information. routing strategy includes threshold parameter that controls the attribution to the two models, providing means to continuously adjust the balance between diversity and quality. We present two categories of routing strategieslogit-based and content-based primarily distinguished by what information is used to make routing decisions. These categories capture complementary intuitions and are broadly applicable across tasks, as later supported by our empirical results. Logit-Based Routing. These strategies leverage the next-token probability distribution to infer models uncertainty. High uncertainty often signals an opportunity for diverse, open-ended generation. Heuristics can be built on simple statistics such as the maximum token probability, the average top-k token probability, or the entropy of the distribution. Key strategies implemented under this category include: ① BACO-P routes to the base model when its maximum token probability is below threshold γ, i.e., maxyt Pbase(yt ) < γ. This deliberately incorporates lower-probability tokens to promote variation; ② BACO-H routes to the base model when its next-token entropy is high, i.e., Hbase(Yt ) = (cid:80) Pbase (yt) log Pbase (yt) > γ, indicating high uncertainty. yt Content-Based Routing. These strategies determine switching decisions based on the semantic roles of the predicted tokens themselves rather than probability distributions. The motivations are twofold: First, linguistic features may provide cues for when diversity is desired (e.g., verbs are critical in narrative generation (Yao et al., 2019; Sims et al., 2019)). Second, Lin et al. (2023); Fei et al. (2025) suggest that disagreements between base and aligned models often arise over stylistic tokens, such as formatting tokens (e.g., n) or function words (e.g., and, if). Key strategies implemented under this category include: ① BACO-PUNC routes to the aligned model when its top-ranked token is either punctuation or formatting token; ② BACO-FC routes to the aligned model if its top-ranked token is function word, ensuring stylistic coherence during formatting parts of sentence. Moreover, content-based routing strategies are suitable for black-box models, as they do not need any access-limited information such as logits. routers performance can be further enhanced in two main ways. First, it can combine multiple strategies into more sophisticated one. For example, BACO-P-FC applies the function-word strategy (-FC) before falling back to the probability-based strategy (-P). Second, while our main experiments use one-token look-ahead for efficiency, router with deeper look-ahead (e.g., n-grams or sentences) might make more context-aware routing decision for longer continuations. More routing strategies and router implementation details are provided in Appendix A.2. 5 Optimizing Diversity and Quality through BaseAligned Model Collaboration"
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "We design our experiments to empirically validate BACOs central goal: to optimize the diversityquality trade-off. Specifically, we aim to examine (i) whether BACO jointly improves diversity and quality across metrics and tasks, and (ii) how different routers perform. Datasets. We evaluate BACO across three datasets that represent different types of open-ended generation tasks: NoveltyBench (Zhang et al., 2025b) for instruction following, WildChat (Zhao et al., 2024) for dialogue, and Narrative-Discourse (Tian et al., 2024) for creative writing. Together, these datasets cover both short-form and long-form open-ended generation across varying levels of task complexity. For full dataset details, we refer readers to Appendix B. Baselines. We compare BACO with inference-time methods across five categories: ① Single-model: base model or an aligned model, each sampled at varying temperatures. ② Prompting-based: in-context resampling (Meyerson et al., 2024; Zhang et al., 2025b), where outputs are generated sequentially within single dialogue; paraphrase prompting (Jiang et al., 2020; Zhang et al., 2025b), where paraphrased variants of the same instruction are used to increase output diversity. ③ Decodingbased: Diverse Beam Search (Vijayakumar et al., 2016), where diversity penalty was added to the beam search algorithm, applied to the aligned model. ④ Ensemble-based: response ensemble, where n/2 outputs are sampled from model pair (a base model and its aligned counterpart) and pooled into single group; logit ensemble, which merges the next-token probability distributions of the two models before sampling. ⑤ Collaboration-based: NUDGING (Fei et al., 2025), where an aligned model selectively introduces tokens during base models decoding (3). Note that diverse beam search, paraphrase prompting, and in-context resampling require additional computation; in-context resampling does not perform parallel sampling of each outputs, where later sampling depend on earlier outputs. These methods therefore provide more competitive reference points. The inference setups and experimental scope are provided in Appendix D. BACO. Our experiments leverage two open-weight model pairs: Llama-3-8B and Llama-3-8B-Instruct (Grattafiori et al., 2024), and Olmo2-7B and Olmo2-7B-Instruct (OLMo et al., 2024) as they are widely used in literature (e.g., Fei et al. (2025)). We implement the single-strategy routers (e.g., -P which is based on maximum token probability) from Section 3.1 and denote multi-strategy routers as -X-Y, where strategy precedes (e.g., -P-PUNC, -P-FC, and -H-PUNC). These implementations serve as representative examples that demonstrate the possible design space of the BACO framework. BACO framework works well with wide range of routers. We include two basic routers as baselines: ① -RAND routes to the base model by random chance γ; ② -JUDGE employs an external model to evaluate candidate tokens and makes routing decision. Refer to Appendix A.2 for more details. 4.1 AUTOMATIC EVALUATIONS We next evaluate the diversity-quality trade-off of each method using automatic metrics. Prior work has proposed wide range of diversity evaluation that apply different lexical and semantic metrics,3 reflecting different perspectives on language diversity. Since our goal is to improve general diversity and quality rather than optimize for any specific metric, we adopt 11 established diversity metrics and 2 quality metrics which form 11 2 diversity-quality spaces, and then aggregate their results. Moreover, we aim to quantify the controllability of each method, i.e., the ability to adjust along the diversity-quality spectrum according to task or preference. As shown earlier in Figure 1B, each method is not evaluated as an individual point in fixed configuration, but curve formed by sequence of Figure 2: Illustration of the two indicators on diversityquality space: Coverage is the area under methods trade-off curve (blue shading for the blue method); Dominance is the proportion of the global Pareto frontier (highlighted curves) contributed by the method. In practice, high Coverage is preferable for general-purpose design, as it ensures method offers good trade-off across wide range of tasks or user preferences. In contrast, high Dominance is desirable when selecting specialized method to achieve the optimal trade-off within specific, known target region of the space. 3The collection of diversity evaluation literature can be found at our Awesome LLM Diversity reading list. 6 Optimizing Diversity and Quality through BaseAligned Model Collaboration points, illustrating the diversity and quality performance in different configurations. Each curve illustrates the diversity-quality trade-off of specific method. To enable clear comparison, we apply two complementary indicators from multi-objective optimization, Coverage (Cov.) and Dominance (Dom.), to aggregate the curve-shaped performance across all spaces into numerical results (Figure 2).4 These metrics answer two distinct questions: (i) Within each method, how effectively can it adjust along the diversity-quality spectrum as its control configuration (e.g., temperature or routing threshold) varies? (ii) Across all methods, which contribute most strongly to the global Pareto frontier of desirable trade-offs? Here, we focus on conveying the intuition behind these indicators; formal definitions are provided in Appendix C.2. Coverage (Cov.) indicator quantifies the area under methods diversity-quality trade-off curve, following the hypervolume formulation used in multi-objective optimization. It measures how effectively method traverses the diversity-quality spectrum as its control configuration varies. larger Coverage value indicates that the method maintains good performance across wide range of regions on diversity-quality spaces, offering usability to more different tasks or preferences overall. Dominance (Dom.) indicator captures comparative optimality: whether and how often method contributes to the global Pareto frontier among all methods. We compute the global Pareto frontier across all methods and apply the C-metric (Zitzler, 1999) to measure the portion of the frontier attributed to each method. higher Dominance value indicates that method achieves uniquely strong trade-offs unattainable by others. Coverage and Dominance capture different, and sometimes uncorrelated, properties. From practical standpoint, these two indicators guide how one might select methods for deployment across scenarios. For general-purpose systems that adapt to different tasks or user preferences, Coverage is more critical, reflecting how broadly method can traverse the diversity-quality spectrum and thus how well it supports controllable adaptation. Conversely, Dominance is more relevant for systems where multiple methods are integrated and selected dynamically. high Dominance score indicates strong contribution to the global Pareto frontier, and thus are desirable components within such integration. Moreover, when designing for specific task with well-defined diversity-quality target, one can directly identifies methods that excel in the specific trade-off regions. For example, those dominating high-diversity regions are suited for creative or ideation tasks that value exploration and novelty, whereas those dominating high-quality regions are preferable for applications emphasizing reliability and factual accuracy. To instantiate these indicators, we define family of diversity-quality spaces, where each space corresponds to particular pair of 11 diversity 2 quality metrics. We report the average lexical, semantic, and overall Coverage and Dominance, depending on the type of metrics as the diversity axis (e.g., Table 2 and 3). Lexical diversity spaces use diversity metrics such as Distinct-n (Dist-n; Li et al., 2015), EAD-n (Liu et al., 2022), and Self-BLEU (Montahaei et al., 2019). Semantic diversity spaces rely on diversity metrics such as embedding cosine dissimilarity (Kirk et al., 2023), Vendi Score (embedding) (Friedman & Dieng, 2022), NLI diversity (Stasaski & Hearst, 2022), and Semantic Entropy (Kuhn et al., 2023).5 Since lexical and semantic metrics capture fundamentally different aspects of diversity, we analyze them separately in addition to reporting aggregated results. Increases in lexical diversity are relatively easy to achieve (for example, by raising the temperature), yet they mostly alter surfacelevel phrasing without changing meaning. In contrast, semantic diversity reflects deeper diversity in structure, intent, and ideas, which is harder to elicit but more human-like and valuable in open-ended generation. Quality metrics include (i) perplexity under the aligned model, which reflects fluency and likelihood, and (ii) reward modeling scores predicted by Skywork-Reward-Gemma-2-27B (Liu et al., 2024), the state-of-the-art model on RewardBench (Lambert et al., 2024). These metrics are paired with diversity metrics to form subspaces. Finally, overall results average across all subspaces including lexical and semantic diversity metrics paired with quality metrics, yielding holistic methodlevel indicators. Hereafter, we use Lexical denote average results on all lexical diversity-quality 4All curves in Figure 2 are illustrative only and do not correspond to the actual performance of any specific method. 5Each metric could include multiple variants. For example, Dist-1/2/3 for different n-gram, or embedding cosine dissimilarity computed with distinct pretrained encoders. Each variance leads to separate diversityquality space. 7 Optimizing Diversity and Quality through BaseAligned Model Collaboration Method Lexical Cov. Dom. Semantic Cov. Dom. Overall Cov. Dom. Base Aligned Nudging Decoding Prompting (Best) Ensemble (Best) BACO (Best) 0.098 12.7% 0.098 16.0% 0.098 14.3% 0.269 49.0% 0.104 29.2% 0.186 39.0% 9.6% 0.276 0.3% - 2.4% - 1.5% - 0.445 24.9% 0.360 40.5% 0.403 32.7% 9.3% 0.247 0.3% 2.7% 1.1% 9.9% 0.261 0.3% 2.2% 1.9% - - - - - - Table 2: Averaged performance of all methods across all datasets and diversityquality spaces. BACO consistently outperforms baselines across all semantic and most lexical spaces, demonstrating stronger controllability and substantially improving the semantic diversityquality trade-off. The overall gains, as driven primarily by improvements in semantic, suggest that BACO produces more meaningful and content-level diversity, rather than superficial word-level changes, compared to other methods. See full results at Appendix E. Figure 3: BACOs performance on one diversity-quality space (x: quality in terms of reward; y: diversity in terms of semantic entropy). Comparing with baselines, BACO (blue curve) attains larger Coverage of the top-right region and contributes to most of the Dominance, indicating improvement and controllability on diversity-quality trade-off. spaces, Semantic denotes semantic diversity-quality spaces, and Overall averages all spaces in every result table. Full derivations and implementation details are in Appendix C.1."
        },
        {
            "title": "5 RESULTS",
            "content": "In this section, we first compare BACO against wide range of baselines across multiple datasets and metrics (5.1). Next, we conduct router-level comparisons to analyze the effectiveness of different routing strategies (5.2, 5.3). We also demonstrate BACOs advantages on long-form diversity at the discourse level (5.4). Lastly, we validate the results through human evaluation (5.5). 5.1 BACO CONSISTENTLY OUTPERFORMS BASELINES Table 2 summarizes the performance of BACO and baselines, aggregated across datasets and metrics. Overall, BACO improves Coverage by 0.142 and achieves 32.7% Dominance. The advantage is particularly pronounced in semantic diversity, where its Dominance rises to 40.5% (Figure 3 as an example).6 On the NoveltyBench dataset, the performance gap becomes even wider, with Coverage improving by 0.274 and Dominance reaching 39.9%. These gains are consistent across all datasets and extend to the Olmo2 model family; see Appendix for full results. See the output comparison examples at Appendix H. 5.2 ROUTER PERFORMANCE COMPARISON Table 3 shows the performance of key routers on NoveltBench. We use NoveltyBench in this section as it offers representative yet computationally efficient setting for detailed comparison. Random routing appears competitive only superficially (-RAND). At first glance, the RAND router performs surprisingly well, which may suggest that collaboration alone suffices without the need for curated router design. However, closer look reveals that its gains are limited to shallow lexical metrics (e.g., Dist-n), while it fails on semantic metrics such as Semantic Entropy (0% dominance) (Appendix E). This pattern parallels the behavior of the high-temperature aligned model baseline: injecting randomness at the surface level increases token uncertainty but does not yield meaningful semantic diversity. These results also highlight that aggregated metrics can be misleading 6The aligned models high lexical dominance arises from its high sampling temperature, which produces long, low-quality sequences that artificially inflate diversity scores while reducing controllability. 8 Optimizing Diversity and Quality through BaseAligned Model Collaboration overall, and that evaluating diversity at the semantic level (especially more up-to-dated ones like Semantic Entropy) is essential for understanding what constitutes genuinely diverse generation. Cov. Cov. Cov. Dom. Dom. Dom. Lexical Overall Routers -P -FC Semantic 4.8% 3.2% 6.7% 4.0% 8.5% 4.7% 0.397 0.382 0.433 0.419 0.415 0.401 0.493 0.302 21.7% 1.6% -RAND -JUDGE 0.495 0.466 0.435 -P-PUNC -H-PUNC -P-FC 26.3% 0.409 0.254 2.6% 17.0% 0.451 0.278 0.6% Prompt-based routers are ineffective (-JUDGE). We further consider -JUDGE, variation of the BACO framework, inspired by the multiagent systems setup (Talebirad & In such systems, an Nadiri, 2023). aligned model often serves as planner that allocates subtasks to different agents. Mirroring this design, we prompt an aligned model at each decoding step to make the routing decision. To enhance decision qualTable 3: Averaged performance of routers within BACO on ity, we experiment with prompt enNoveltyBench across all diversityquality spaces. The -Pgineering, including step-by-step dePUNC router achieves the best overall performance. While cision pipeline, curated heuristic rules, the random router (-RAND) attains moderately strong results, and few-shot examples with rationales mainly from increased surface-level lexical diversity, its per- (prompts are in Table A16). Despite formance drops sharply on semantic metrics, confirming that these efforts, -JUDGE consistently unguided switching fails to produce meaningful diversity. In underperforms simpler, lightweight contrast, -P-PUNC delivers the most balanced and consistent routers across most metrics while beresults across both lexical and semantic evaluations, showing ing far more computationally expencombination of designed routing strategies leads to more sive. This finding suggests the limited meaningful diversity. judgment of LLM to open-endedness (i.e., when to diverge and how to diverge), making them poor fit for this task compared to more lightweight strategies we proposed. 31.3% 0.474 18.6% 0.446 19.2% 0.421 30.7% 0.452 16.4% 0.427 16.0% 0.406 31.0% 17.5% 17.6% Simple routers provide semantic gains (-P and -FC). Single-strategy routers like -P (based on next-token probability thresholds) and -FC (based on detecting function words) trade weaker overall performance for notable gains in semantic diversity, outperforming the -RAND baseline on key metrics like Cosine Dissimilarity. For instance, -P shows modest but consistent advantage on Cosine Dissimilarity, Vendi Score (SimCSE), and Semantic Entropy (Appendix E). In contrast, -FC attains reasonable performance despite requiring only linguistic information. Cov. Cov. Cov. Dom. Dom. Lexical Overall Method Semantic 0.142 0.273 Base Aligned 9.8% 0.142 43.8% 0.128 Combining strategies achieves the best overall performance (-P-PUNC, -H-PUNC, and -P-FC). Combining strategies proves to be most effective in our experiments. The -PPUNC router achieves the best overall performance, demonstrating high controllability (both Coverage and Dominance) across both lexical and semantic spaces. Other combinations like -H-PUNC and -P-FC, while not as strong overall, are still valuable as they capture unique regions of the global Pareto frontier (17.5% and 17.6% Dominance). This confirms that integrating multiple strategies yields complementary strengths, further advancing the diversity-quality frontier. Table 4: Comparison of base-aligned and aligned-aligned collaboration (denoted as AACO ) on NoveltyBench across all diversityquality spaces. AACO yields little improvement on the diversityquality trade-off compared with BACO, particularly in semantic diversity. The results demonstrate the necessity of involving base and aligned model. Dom. 11.5% 31.8% 5.7% 50.9% 7.0% 0.014 59.8% 0.474 4.5% 0.006 42.0% 0.452 13.2% 0.142 19.9% 0. AACO BACO 0.022 0.495 5.3 BASE-ALIGNED COLLABORATION OUTPERFORMS ALIGNED-ALIGNED We test whether our base-aligned collaboration (BACO) is more effective than aligned-aligned collaboration for mitigating diversity-quality trade-off  (Table 4)  . On NoveltyBench, we use the best 9 Optimizing Diversity and Quality through BaseAligned Model Collaboration NoveltyBench WildChat Aligned BACO Aligned BACO Quality 2.83 4. 3.44 3.83 Overall Diversity Format Content 21.0% 79.0% 36.1% 63.9% 25.4% 74.6% 26.8% 73.2% 22.9% 77.1% 41.6% 58.4% Creativity 20.4% 79.6% 38.2% 61.8% Table 5: Human evaluation comparing BACO (best variant, -P-PUNC) with the aligned model baseline on NoveltyBench and WildChat. BACO is consistently preferred by human judges across all aspects, demonstrating that it produces outputs that are not only more human-perceived diverse but also higher in quality and creativity. Scores of quality are on 15 Likert scale, and others indicate the pairwise win rate. Figure 4: Comparison of BACOs and baselines discourse-level diversity-quality trade-off curve on Narrative Discourse. BACO obtains larger Coverage, achievable in the high-diversity, highquality region (top-left). The results demonstrate it has richer discourse-level diversity without sacrificing quality largely, compared with baselines. The x-axis is quality (perplexity; lower is better), and the y-axis is discourse-level diversity, either turning-point diversity (left figure) or arousal diversity (right figure) (higher is better). router, -P-PUNC, to compare base-aligned pair (Llama-3-8B family) against an aligned-aligned pair (Llama-3-8B with Llama-3.2-11B).7 We measure each aligned models diversity on the tasks. The model with better diversity serves as the role of the base model in BACO to contribute diversity. The results show that base-aligned collaboration significantly outperforms the aligned-aligned setup in both Coverage and Dominance. These results highlight the limited diversity that arises from collaborating between two aligned models, underscoring our hypothesis that complementarity between less-aligned and more-aligned models is crucial for achieving both high diversity and high quality. 5.4 TOWARD LONG-FORM DIVERSITY: NARRATIVE DISCOURSE EVALUATION Long-form outputs are often generated in creative writing tasks, where meaningful diversity of narratives often arises at the discourse level. However, existing metrics fails to measure it. Lexical metrics focus on surface-level wording and are unable to capture structural differences. Semantic metrics that use embeddings collapse entire narratives into single vectors, losing temporal and organizational dynamics (Deng et al., 2022; Huang et al., 2023). Results of these metrics (4.1) are shown in Table A9, which also shows poor distinguishability. To address these limitations, we adopt discourse-level diversity evaluation framework following Tian et al. (2024) to quantify long-form diversity, which captures the overall progression of narratives structure. In particular, we assess narrative organization through story arcs, in terms of the distribution of discourse elements: turning points and arousal patterns. Setup. We frame creative writing as continuation task, where the model is given the beginning of story and asked to complete it. The prefix contains events leading up to the first turning point, which introduces the initial situation or conflict setting the stage for the narrative. The model then generates the subsequent events to develop and conclude the entire narrative. To capture discourse-level variation, we measure two structural dimensions: ① Turning-point diversity quantifies differences in the relative positions of annotated plot inflections across outputs. ② Arousal diversity tracks divergence in emotional trajectories, obtained by sampling sentence-level arousal scores and comparing smoothed curves via KL divergence. Together, these metrics provide complementary measures of long-form diversity, capturing variation in plot structure and affective dynamics that conventional surface-level metrics miss. Detailed setups are provided in Appendix D.1. Results. Figure 4 shows that BACO achieves markedly higher turning-point and arousal diversity than the aligned baseline of comparable quality, showing that it obtains greater Coverage and remains effective in the high-diversity and high-quality region (top-left). This result validates that base-aligned 7We do not consider base-base collaboration since its generation quality is generally low. 10 Optimizing Diversity and Quality through BaseAligned Model Collaboration collaboration extends beyond short-form gains to long-form narratives, yielding richer plot structure and affective dynamics diversity while preserving quality. See creative writing output examples in Table A19."
        },
        {
            "title": "5.5 HUMAN EVALUATION ON QUALITY, DIVERSITY, AND CREATIVITY",
            "content": "To complement automatic metrics, we conduct three-phase human evaluation designed to assess output quality, group-level diversity, and creativity. The evaluation protocol aligns with the settings of automatic metrics. Phase I: Quality. Annotators rate each output on 1-5 Likert scale with respect to overall quality, considering fluency, relevance to the prompt, and informational substance. Phase II: Diversity. Annotators perform pairwise comparisons between output groups from two different methods. Diversity is evaluated along two specific aspects: (a) format diversity, capturing diversity in formatting or expression style; (b) content diversity, assessing diversity in underlying ideas.8 The overall diversity reflects the holistic impression of diversity, without any aspect specified. Phase III: Creativity. Annotators identify the single most creative output among all for given prompt, and we record which method generated it. This evaluation aims to capture each methods human-perceived creativity by its ability to generate the most creative output among group-level comparisons. Setup. We compare BACO with the aligned model baseline under matched quality. From each dataset, 20 prompts are sampled with 3 outputs per method, for total of 120 outputs. Four annotators with LLM expertise evaluate on NoveltyBench and WildChat (see Appendix for detailed setup and annotation). Results. Shown in Table 5, although the automatic quality of the aligned model (5.933) and BACO (5.847) are nearly identical, human raters prefer BACO more, with 42.8% higher average rating and strong inter-rater agreement (Pearson correlation = 0.816, Intraclass correlation ICC(2, k) = 0.907). On diversity, BACO demonstrates significant win-rate over the aligned model, with consistent improvements in both format (Fleiss κ = 0.166) and content (Fleiss κ = 0.244) sub-aspects, and overall diversity (Fleiss κ = 0.268)9. See human annotation examples at Appendix G.5."
        },
        {
            "title": "6 ANALYSIS AND DISCUSSION",
            "content": "In this section, we first analyze the collaboration pattern between the base and aligned model in BACO (6.1 and 6.2). We then extend human evaluation from diversity to creativity, presenting BACOs additional advantages (6.3); and lastly discuss thoughts on future work (6.4). 6.1 CONTRIBUTION DISTRIBUTION AND SWITCHING FREQUENCY We measure how often each model contributes to generation at at each token position across all generations. We also count the frequency of model switching at each token position. For BACO with the best router (-P-PUNC), base-model contribution and switching frequency are high at the start of generation and decrease over time across all three tasks (Figure A7 - A5). Whether this pattern is desirable is task dependent and partly subjective. We hypothesize that it reflects the models increasing predictive confidence as generation progresses. Such behavior may manifest as temporal trend (e.g., over the course of decoding) that coincides with positional patterns in the text. For example, in creative writing tasks, early tokens tend to correspond to exploratory narrative positions (e.g., introducing background, characters, and conflicts), while later tokens naturally emphasize 8For example, when asked to suggest summer trip, one response might say, Sure, here are some ideas: 1. spend week exploring the beaches of Hawaii. 2. visit coastal towns along the Pacific, while another says, Certainly! One idea is to take road trip through national parks-Yellowstone or Yosemite would be refreshing. Other ideas are ... The two differ in format (opening phrase, tone, and list vs. narrative structure) and in content (a tropical vacation vs. nature-focused road trip). 9The moderate agreement levels reflect the inherent subjectivity of format and content definitions, as well as the subjectivity of diversity judgments in open-ended tasks. Although annotators were provided with detailed guidelines and required to review them prior to annotation, the wide variety of prompts naturally allows for divergent interpretationsco. Optimizing Diversity and Quality through BaseAligned Model Collaboration coherence (e.g., resolving plotlines or tying back to earlier events). However, it may be less ideal for tasks like open-ended question answering, especially since most outputs are written in list form, which may require more uniform exploration. This pattern could be tailored for specific tasks by adopting dynamic, position-aware thresholds, which we leave for future work. 6.2 INHERENT EARLY STOP: FAILURE MODE IN MODEL COLLABORATION We qualitatively and quantitatively observe that when the router is tuned more aggressively toward diversity (more token generated by the base model), the collaboration system exhibits higher tendency to terminate early (i.e., one of the models produces an end-of-sentence (eos) token). Examples are in Table A12 and Table A13. This phenomenon differs from conventional decoding or reasoning approaches, where early stopping is deliberately designed as an efficiency mechanism-such as pruning low-confidence branches in dynamic search methods (Ding et al., 2025a). In contrast, our system demonstrates inherent early stopping as an emergent behavior of model collaboration with lower-probability content introduced. This property carries both drawbacks and advantages. On the negative side, early stopping may prematurely truncate outputs, especially for less-frequent prefixes that could otherwise lead to valid and high-quality continuations. On the positive side, early stopping can prevent the system from degenerating into endless loops or incoherent repetition, common issue at higher temperatures. In such cases, emitting an eos token provides safe and efficient termination. Moreover, since these cases are easily detectable given the length , practical solution is to simply restart sampling when premature termination occurs. Overall, inherent early stopping reflects distinctive failure mode in model communication under collaborative decoding. While it introduces risks of incompleteness, it also offers built-in safeguard against low-quality continuations, highlighting an important dimension for future router design. 6.3 EVALUATION EXTENSION FROM DIVERSITY TO CREATIVITY Our human evaluation (5.5) not only assessed diversity but also included phase for creativity, which is often defined as originality and human-perceived value. As creativity is more likely to be an explicit target in many real-world application, such as writing or science discovery, we therefore examine whether BACOs improvements in diversity-quality trade-off translate into enhanced creativity. Creativity represents higher-level dimension of generation diversity and quality. Evaluating creativity in LLMs, however, remains an open and contested challenge: prior studies differ in definitions and settings, leading to inconsistent conclusions (e.g., (Si et al., 2024) vs. (Gupta & Pruthi, 2025); (Lu et al., 2025b) vs. (Beel et al., 2025); (Lee & Chung, 2024) vs. (Meincke et al., 2025b; Wu et al., 2025; Xu et al., 2025)). We argue that central source of discrepancy lies in how creativity is related to diversity. Some studies (e.g., Lee & Chung, 2024) find positive evidence when evaluating individual outputs, whereas others (e.g., Meincke et al., 2025b) report negative conclusions when assessing groups of outputs. Building on Jaarsveld et al. (2012), we adopt dual perspective that treats creativity as balance between divergent thinking (captured by group-level diversity across outputs) and convergent thinking (captured by quality and novelty within individual outputs). Our three-phase human evaluation is designed to align with this view: Phase assesses quality, partially reflecting convergent thinking; Phase II measures diversity, corresponding to divergent thinking; and Phase III directly evaluates creativity, integrating both dimensions into unified evaluation. This design allows us to examine whether improvements in diversity through BACO translate into genuinely more creative and human-valued generations. Our results support this dual view. In the human evaluation, BACOs outputs were consistently rated as higher in both quality and diversity and, more importantly, were significantly more likely to be judged as the most creative (Fleiss κ = 0.485). These findings suggest that BACOs collaborative decoding not only optimize the diversityquality trade-off but also fosters human-perceived creativity, offering promising step toward generation that is both diverse and valuable original (see examples in Appendix G.5). 6.4 FUTURE WORK We envision BACO as step toward enabling breadth thinking mode for LLMs, complementing the existing focus on deep thinking (OpenAI, 2025; DeepSeek-AI et al., 2025). We refer to breadth 12 Optimizing Diversity and Quality through BaseAligned Model Collaboration as models inherent ability to explore wide, validated space, considering multiple perspectives, styles, or conceptual directions, rather than converging on single, narrowly optimized output under open-ended instructions. For open-ended tasks, breadth thinking offers way to help human break out of their information cocoons (Piao et al., 2023), expand their ideation space, and think beyond conventional boundaries. BACO provides practical initial step: it is controllable, inference-time only, and deployable out of the box without requiring additional post-training or complex agentic workflows. It may also serve as plug-in for broader system designs, for example as an ideation agent in multi-agent collaboration (Siddiqui et al., 2025) or as an exploratory stage within problem-solving pipelines (Cheng et al., 2025; Song et al., 2025). natural direction is to design more sophisticated, task-aware routers that adaptively learn routing strategies from data or rewards rather than relying only on hand-crafted heuristics. There is also room for improving efficiency. Speculative decoding (Leviathan et al., 2023b) is perfect fit for token-level routing in the framework, especially when looking ahead, which could reduce inference-time overhead. Moreover, while our current experiments use base and aligned models of the same family, memory-efficient alternatives such as LoRA-tuned aligned models could further reduce deployment costs, replacing model size with single base model plus LoRA adapter size (Wu et al., 2024). Another promising direction lies in exploring the dynamics of alignment beyond the two canonical checkpoints we studied (base and fully aligned) (Im & Li, 2024; Ren & Sutherland, 2025). While these checkpoints are the most accessible in open-source releases, they are unlikely to represent the optimal trade-off points. Intermediate or partially aligned checkpoints may offer different quality-diversity trade-off, and systematic study of it could further extend the diversity-quality trade-off frontier. Finally, human-centered usability should not be overlooked. The controllability of BACO offers opportunities for user-facing interfaces: for example, sliders that let users explicitly shift along the quality-diversity spectrum, or adaptive systems that learn user preferences over time. Such design could make breadth thinking practically useful in everyday workflows, not only research benchmarks. Beyond the quality-diversity balance, we also see potential in applying BACO to other inference-time objectives such as multi-level privacy (Chua et al., 2024) or controllable safety (Zhang et al., 2025a), highlighting its promise as general paradigm for collaborative inference. In summary, we view BACO as the first step toward broader agenda of inference-time collaboration, leveraging preor intermediate-alignment checkpoints to enable multi-objective optimization and achieve practical balance in real-world use."
        },
        {
            "title": "7 RELATED WORK",
            "content": "Diversity Degradation in Alignment. While alignment techniques like RLHF enhance model performance in instruction following and reasoning, they systematically curtail output diversity. This trade-off is widely recognized, with growing body of evidence demonstrating that aligned models are less diverse than their base counterparts. For example, studies have questioned their artistic authenticity (Chakrabarty et al., 2024), and benchmarks like NoveltyBench reveal their diminished capacity for humanlike diversity and creativity (Zhang et al., 2025b; Tian et al., 2024; Lu et al., 2025a; West & Potts, 2025). The underlying mechanism for this degradation is probability concentration, as the alignment process sharpens the models output distribution, thereby steering it into low-entropy generation paths (Yang & Holtzman, 2025). The diversity degradation impacts the downstream applications. It manifests as loss of linguistic idiosyncrasies (Chakrabarty et al., 2025), increased format homogeneity (Zhang et al., 2024), and diminished creativity (West & Potts, 2025) in generated text. Beyond linguistic characteristics, alignment-induced constraints lead models to converge on restricted repertoire of strategies, thereby diminishing diversity in reasoning (Chen et al., 2024; Ding et al., 2025b), data synthesis (Kim et al., 2024; Yang et al., 2025), deep research (Xiao, 2025), social simulation (Wang et al., 2025a), and gaming (West & Potts, 2025). More critically, the lack of output diversity has been shown to further reduce outcome diversity (Padmakumar & He, 2023) and creativity (Meincke et al., 2025a; Ashkinaze et al., 2025) in human interactions with these models. These studies demonstrate that diminished diversity in model outputs adversely affects how humans ideate, create, and engage. Our work addresses this challenge directly, proposing an inference-time 13 Optimizing Diversity and Quality through BaseAligned Model Collaboration collaborative decoding framework that optimizes the diversity-quality trade-off by combining the strengths of both base and aligned models. Diversity-Promoting Methods. Approaches to enhance the diversity of aligned LLMs fall into two main categories: training-time and inference-time methods. Training-time methods typically modify the learning objective to encourage varied outputs. prominent line of work adapts Direct Preference Optimization (DPO; Rafailov et al. (2023)) by incorporating diversity-aware mechanisms, such as f-divergence penalties (Wang et al., 2024), set-level diversity rewards (Lanchantin et al., 2025), or re-weighted loss objectives (Chung et al., 2025; Ismayilzada et al., 2025). Other approaches leverage different architectures, like generative flow networks, to the same end (Kwon et al., 2024). While these methods can instill diversity directly into the model, they require substantial computational resources for retraining and offer little flexibility for user-specific diversity needs at inference. Inference-time methods offer more lightweight and adaptable alternative. These include modifications to decoding algorithms like diverse beam search (Vijayakumar et al., 2016) and various prompt engineering strategies, such as paraphrasing (Meyerson et al., 2024; Wang et al., 2025b; Zhang et al., 2025b; Wang et al., 2025a; Wong et al., 2024). Existing inference-time methods for improving diversity typically incur high computational costs via multiple decoding passes or long-horizon planning. Or they significantly degrade generation quality (Peeperkorn et al., 2024). Achieving stable diversityquality trade-off with these techniques remains challenge. Our proposed method BACO, is an inference-time framework designed to offer more explicit and reliable control over this trade-off. We therefore focus our comparison on baselines from this category. Multi-Model Collaborative Generation. Prior work has explored collaborative frameworks where multiple language models work in concert to improve generation quality and efficiency (e.g., computation cost and latency). These approaches can be grouped by their collaboration mechanism. One line of work focuses on weight-level collaobration. This includes merging reward or policy models, or using Mixture-of-Experts (MoE) (Shazeer et al., 2017) architectures to create single, more capable system better aligned with diverse human preferences (Rame et al., 2023; Zheng et al., 2025; Shi et al., 2025). Another mechanism is token-level collaboration, where multiple models collaborate during decoding by exchanging next-token probability distributions or candidate token choices at each step, to improve attributes like coherence and factuality, or reduce latency (Leviathan et al., 2023a; Li et al., 2023; Zheng et al., 2024; Fei et al., 2025). More recently, multi-agent systems have emerged, in which models debate or discuss to leverage their complementary strengths for complex, creative tasks (Lu et al., 2024; Venkatraman et al., 2025; Huot et al., 2025). Our BACO, advances token-level collaboration. While existing methods in this area primarily target quality or efficiency, we focus on navigating the diversity-quality trade-off."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work does not involve any human subjects, sensitive personal data, or personally identifiable information. All datasets used (NoveltyBench, WildChat, and Narrative-Discourse) are publicly available research benchmarks released under appropriate licenses. Our experiments focus on inference time with existing LLMs without additional fine-tuning, and thus pose minimal risks of memorization or privacy leakage beyond those already associated with the underlying models. While our method aims to improve diversity in model outputs, we acknowledge the possibility that enhanced diversity might amplify harmful generations if safeguards are not in place. This is shared risk for most diversity-promoting technologies. We therefore emphasize that BACO is complementary to safety alignment and should be deployed with standard content-filtering or moderation practices. Importantly, BACO is designed for open-ended generation and ideation applications, rather than high-stakes or safety-critical domains. All authors have read and adhered to the ICLR Code of Ethics."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We have taken multiple steps to ensure the reproducibility of our work. The main paper specifies model configurations, routing strategies, evaluation metrics, and experimental protocols. Detailed descriptions of datasets, preprocessing steps, and evaluation setups are provided in Appendix B, with additional formulas and derivations in Appendix C. We report results across multiple model pairs to 14 Optimizing Diversity and Quality through BaseAligned Model Collaboration verify robustness Appendix E. Human evaluation protocols and annotation guidelines are described in Appendix G. To further facilitate reproducibility, we will release code, configuration files, and evaluation scripts upon publication."
        },
        {
            "title": "USAGE OF LARGE LANGUAGE MODELS",
            "content": "In this work, besides running LLMs in experiments, we use LLMs for the following purposes: 1. Aid or Polish Writing (Gemini 2.5 Pro, ChatGPT 4/5) 2. Literature Retrieval and Discovery, e.g., finding related work (Gemini 2.5 Pro Deep Research, ChatGPT Deep Research) 3. Assisting Code Writing and Debugging (Claude 3.5 Sonnet) We fully understand the responsibility of using LLMs in academic research. We carefully monitor any potential problems, such as plagiarism or scientific misconduct (e.g., fabrication of facts) when using LLMs. We make sure these problems do not occur in the paper."
        },
        {
            "title": "REFERENCES",
            "content": "Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, and Eric Gilbert. How ai ideas affect the creativity, diversity, and evolution of human ideas: evidence from large, dynamic experiment. In Proceedings of the ACM Collective Intelligence Conference, pp. 198213, 2025. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. Joeran Beel, Min-Yen Kan, and Moritz Baumgart. Evaluating sakanas ai scientist for autonomous research: Wishful thinking or an emerging reality towards artificial research intelligence (ari)?, 2025. URL https://arxiv.org/abs/2502.14297. Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and Chien-Sheng Wu. Art or artifice? large language models and the false promise of creativity. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, pp. 134, 2024. Tuhin Chakrabarty, Philippe Laban, and Chien-Sheng Wu. Can ai writing be salvaged? mitigating idiosyncrasies and improving human-ai alignment in the writing process through edits. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 133, 2025. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, et al. Do not think that much for 2+ 3=? on the overthinking of o1-like llms. arXiv preprint arXiv:2412.21187, 2024. Daixuan Cheng, Shaohan Huang, Xuekai Zhu, Bo Dai, Wayne Xin Zhao, Zhenliang Zhang, and Furu Wei. Reasoning with Exploration: An Entropy Perspective on Reinforcement Learning for LLMs. arXiv e-prints, art. arXiv:2506.14758, June 2025. doi: 10.48550/arXiv.2506.14758. Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. URL https://arxiv.org/ abs/2403.04132. Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Pasin Manurangsi, Amer Sinha, and Chiyuan Zhang. Mind the privacy unit! user-level differential privacy for language model fine-tuning. arXiv preprint arXiv:2406.14322, 2024. John Joon Young Chung, Vishakh Padmakumar, Melissa Roemmele, Yuqian Sun, and Max Kreminski. Modifying large language model post-training for diverse creative writing. arXiv preprint arXiv:2503.17126, 2025. 15 Optimizing Diversity and Quality through BaseAligned Model Collaboration Ganqu Cui, Yuchen Zhang, Jiacheng Chen, Lifan Yuan, Zhi Wang, Yuxin Zuo, Haozhan Li, Yuchen Fan, Huayu Chen, Weize Chen, et al. The entropy mechanism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Jun-Mei Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiaoling Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bing-Li Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dong-Li Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, Jiong Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, M. Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, Ruiqi Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shao-Kang Wu, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wen-Xia Yu, Wentao Zhang, Wangding Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xi aokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyu Jin, Xi-Cheng Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yi Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yu-Jing Zou, Yujia He, Yunfan Xiong, Yu-Wei Luo, Yu mei You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanping Huang, Yao Li, Yi Zheng, Yuchen Zhu, Yunxiang Ma, Ying Tang, Yukun Zha, Yuting Yan, Zehui Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhen guo Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zi-An Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. URL https://api.semanticscholar.org/CorpusID:275789950. Yuntian Deng, Volodymyr Kuleshov, and Alexander Rush. Model criticism for long-form text generation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1188711912, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/ v1/2022.emnlp-main.815. URL https://aclanthology.org/2022.emnlp-main.815/. Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, and Dacheng Tao. Dynamic parallel tree search for efficient llm reasoning, 2025a. URL https://arxiv.org/abs/2502.16235. Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, et al. Dynamic parallel tree search for efficient llm reasoning. arXiv preprint arXiv:2502.16235, 2025b. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Yu Fei, Yasaman Razeghi, and Sameer Singh. Nudging: Inference-time alignment of llms via guided decoding, 2025. URL https://arxiv.org/abs/2410.09300. Dan Friedman and Adji Bousso Dieng. The vendi score: diversity evaluation metric for machine learning. arXiv preprint arXiv:2210.02410, 2022. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. 16 Optimizing Diversity and Quality through BaseAligned Model Collaboration Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Tarun Gupta and Danish Pruthi. All that glitters is not novel: Plagiarism in ai generated research, 2025. URL https://arxiv.org/abs/2502.16487. Sil Hamilton. Detecting mode collapse in language models via narration. arXiv preprint arXiv:2402.04477, 2024. Tenghao Huang, Ehsan Qasemi, Bangzheng Li, He Wang, Faeze Brahman, Muhao Chen, and Snigdha Chaturvedi. Affective and dynamic beam search for story generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, pp. 1179211806, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.789. URL https://aclanthology.org/ 2023.findings-emnlp.789/. Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, and Mirella Lapata. Agents room: Narrative generation through multi-step collaboration. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=HfWcFs7XLR. Shawn Im and Yixuan Li. Understanding the learning dynamics of alignment with human feedback. arXiv preprint arXiv:2403.18742, 2024. Mete Ismayilzada, Antonio Laverghetta Jr, Simone Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, and Roger Beaty. Creative preference optimization. arXiv preprint arXiv:2505.14442, 2025. Saskia Jaarsveld, Thomas Lachmann, and Cees Van Leeuwen. Creative reasoning across developmental levels: Convergence and divergence in problem creation. Intelligence, 40(2):172188, 2012. Zhengbao Jiang, Frank Xu, Jun Araki, and Graham Neubig. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423438, 2020. Seungone Kim, Juyoung Suk, Xiang Yue, Vijay Viswanathan, Seongyun Lee, Yizhong Wang, Kiril Gashteovski, Carolin Lawrence, Sean Welleck, and Graham Neubig. Evaluating language models as synthetic data generators. arXiv preprint arXiv:2412.03679, 2024. Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint arXiv:2310.06452, 2023. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023. Oh Joon Kwon, Daiki Matsunaga, and Kee-Eung Kim. Gdpo: Learning to directly align language models with diversity using gflownets. arXiv preprint arXiv:2410.15096, 2024. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. Jack Lanchantin, Angelica Chen, Shehzaad Dhuliawala, Ping Yu, Jason Weston, Sainbayar Sukhbaatar, and Ilia Kulikov. Diverse preference optimization. arXiv preprint arXiv:2501.18101, 2025. Byung Cheol Lee and Jaeyeon Chung. An empirical investigation of the impact of chatgpt on creativity. Nature Human Behaviour, 8(10):19061914, 2024. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023a. 17 Optimizing Diversity and Quality through BaseAligned Model Collaboration Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding, 2023b. URL https://arxiv.org/abs/2211.17192. Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015. Tianjian Li, Yiming Zhang, Ping Yu, Swarnadeep Saha, Daniel Khashabi, Jason Weston, Jack Lanchantin, and Tianlu Wang. Jointly reinforcing diversity and quality in language model generations, 2025. URL https://arxiv.org/abs/2509.02534. Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1228612312, 2023. Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, and Yejin Choi. The unlocking spell on base llms: Rethinking alignment via in-context learning. In The Twelfth International Conference on Learning Representations, 2023. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pp. 7481, 2004. Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Siyang Liu, Sahand Sabour, Yinhe Zheng, Pei Ke, Xiaoyan Zhu, and Minlie Huang. Rethinking and refining the distinct metric. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 762770, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.86. URL https://aclanthology.org/2022.acl-short.86/. Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, and Shao-Hua Sun. Llm discussion: Enhancing the creativity of large language models via discussion framework and role-play. In First Conference on Language Modeling, 2024. Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, and Yejin Choi. AI as humanitys salieri: Quantifying linguistic creativity of language models via systematic attribution of machine text against web text. In The Thirteenth International Conference on Learning Representations, 2025a. URL https://openreview.net/forum?id=ilOEOIqolQ. Ximing Lu, Melanie Sclar, Skyler Hallinan, Niloofar Mireshghallah, Jiacheng Liu, Seungju Han, Allyson Ettinger, Liwei Jiang, Khyathi Chandu, Nouha Dziri, and Yejin Choi. Ai as humanitys salieri: Quantifying linguistic creativity of language models via systematic attribution of machine text against web text, 2025b. URL https://arxiv.org/abs/2410.04265. Lennart Meincke, Gideon Nave, and Christian Terwiesch. Chatgpt decreases idea diversity in brainstorming. Nature Human Behaviour, 9(6):11071109, June 2025a. ISSN 2397-3374. doi: 10.1038/s41562-025-02173-x. Epub 2025 May 14. Lennart Meincke, Gideon Nave, and Christian Terwiesch. Chatgpt decreases idea diversity in brainstorming. Nature human behaviour, pp. 13, 2025b. Elliot Meyerson, Mark Nelson, Herbie Bradley, Adam Gaier, Arash Moradi, Amy Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. ACM Transactions on Evolutionary Learning, 4(4):140, 2024. Ehsan Montahaei, Danial Alihosseini, and Mahdieh Soleymani Baghshah. Jointly measuring diversity and quality in text generation models. arXiv preprint arXiv:1904.03971, 2019. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. 18 Optimizing Diversity and Quality through BaseAligned Model Collaboration OpenAI."
        },
        {
            "title": "Introducing",
            "content": "deep research. introducing-deep-research/, February 2025. Accessed: November 11, 2025. https://openai.com/index/ Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Vishakh Padmakumar and He He. Does writing with language models reduce content diversity? arXiv preprint arXiv:2309.05196, 2023. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Max Peeperkorn, Tom Kouwenhoven, Dan Brown, and Anna Jordanous. Is temperature the creativity parameter of large language models? In ICCC, 2024. Jinghua Piao, Jiazhen Liu, Fang Zhang, Jun Su, and Yong Li. Humanai adaptive dynamics drives the emergence of information cocoons. Nature Machine Intelligence, 5(11):12141224, 2023. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. Fine-tuning aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693, 2023. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36:7109571134, 2023. Yi Ren and Danica J. Sutherland. Learning dynamics of llm finetuning, 2025. URL https://arxiv. org/abs/2407.10490. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations, 2017. Weijia Shi, Akshita Bhagia, Kevin Farhat, Niklas Muennighoff, Pete Walsh, Jacob Morrison, Dustin Schwenk, Shayne Longpre, Jake Poznanski, Allyson Ettinger, et al. Flexolmo: Open language models for flexible data use. arXiv preprint arXiv:2507.07024, 2025. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, and Yarin Gal. Ai models collapse when trained on recursively generated data. Nature, 631(8022):755759, 2024. doi: 10.1038/s41586-024-07566-y. URL https://doi.org/10.1038/s41586-024-07566-y. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? largescale human study with 100+ nlp researchers, 2024. URL https://arxiv.org/abs/2409.04109. Momin Siddiqui, Roy Pea, and Hari Subramonyam. Script&shift: layered interface paradigm for integrating content development and rhetorical strategy with llm writing assistants. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pp. 119, 2025. Matthew Sims, Jong Ho Park, and David Bamman. Literary event detection. In Proceedings of the 57th annual meeting of the association for computational linguistics, pp. 36233634, 2019. Yuda Song, Julia Kempe, and Remi Munos. Outcome-based exploration for llm reasoning, 2025. URL https://arxiv.org/abs/2509.06941. 19 Optimizing Diversity and Quality through BaseAligned Model Collaboration Alexander Spangher, Tenghao Huang, Philippe Laban, and Nanyun Peng. Creative planning with language models: Practice, evaluation and applications. In Maria Lomeli, Swabha Swayamdipta, and Rui Zhang (eds.), Proceedings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 5: Tutorial Abstracts), pp. 19, Albuquerque, New Mexico, May 2025. Association for Computational Linguistics. ISBN 979-8-89176-193-3. doi: 10.18653/v1/2025.naacl-tutorial.1. URL https://aclanthology.org/2025.naacl-tutorial.1/. Katherine Stasaski and Marti Hearst. Semantic diversity in dialogue with natural language inference. In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (eds.), Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 8598, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.6. URL https://aclanthology.org/2022.naacl-main.6/. Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023. Yufei Tian, Tenghao Huang, Miri Liu, Derek Jiang, Alexander Spangher, Muhao Chen, Jonathan May, and Nanyun Peng. Are large language models capable of generating human-level narratives? arXiv preprint arXiv:2407.13248, 2024. Saranya Venkatraman, Nafis Irtiza Tripto, and Dongwon Lee. Collabstory: Multi-llm collaborative In Findings of the Association for Computational story generation and authorship analysis. Linguistics: NAACL 2025, pp. 36653679, 2025. Ashwin Vijayakumar, Michael Cogswell, Ramprasath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424, 2016. Angelina Wang, Jamie Morgenstern, and John Dickerson. Large language models that replace human participants can harmfully misportray and flatten identity groups. Nature Machine Intelligence, pp. 112, 2025a. Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse kl: Generalizing direct preference optimization with diverse divergence constraints. In The Twelfth International Conference on Learning Representations, 2024. Qihan Wang, Shidong Pan, Tal Linzen, and Emily Black. Multilingual prompting for improving llm generation diversity. arXiv preprint arXiv:2505.15229, 2025b. Peter West and Christopher Potts. Base models beat aligned models at randomness and creativity. arXiv preprint arXiv:2505.00047, 2025. Justin Wong, Yury Orlovskiy, Michael Luo, Sanjit Seshia, and Joseph Gonzalez. Simplestrat: Diversifying language model generation with stratification. arXiv preprint arXiv:2410.09038, 2024. Suqing Wu, Yukun Liu, Mengqi Ruan, Siyu Chen, and Xiao-Yun Xie. Human-generative ai collaboration enhances task performance but undermines humans intrinsic motivation. Scientific Reports, 15(1):15105, 2025. Xun Wu, Shaohan Huang, and Furu Wei. Mixture of lora experts, 2024. URL https://arxiv.org/ abs/2404.13628. Submodular Han Xiao. research. submodular-optimization-for-diverse-query-generation-in-deepresearch/. minutes read. query deepgeneration URL https://jina.ai/news/ for July 2025. Jina AI Tech Blog, optimization diverse in Weijia Xu, Nebojsa Jojic, Sudha Rao, Chris Brockett, and Bill Dolan. Echoes in ai: Quantifying lack of plot diversity in llm outputs. Proceedings of the National Academy of Sciences, 122(35): e2504966122, 2025. 20 Optimizing Diversity and Quality through BaseAligned Model Collaboration Chenghao Yang and Ari Holtzman. How alignment shrinks the generative horizon. arXiv preprint arXiv:2506.17871, 2025. Yuming Yang, Yang Nan, Junjie Ye, Shihan Dou, Xiao Wang, Shuo Li, Huijie Lv, Mingqi Wu, Tao Gui, Qi Zhang, et al. Measuring data diversity for instruction tuning: systematic analysis and reliable metric. arXiv preprint arXiv:2502.17184, 2025. Lili Yao, Nanyun Peng, Ralph Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. Plan-andwrite: Towards better automatic storytelling. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 73787385, 2019. Jingyu Zhang, Ahmed Elgohary, Ahmed Magooda, Daniel Khashabi, and Benjamin Van Durme. Controllable safety alignment: Inference-time adaptation to diverse safety requirements, 2025a. URL https://arxiv.org/abs/2410.08968. Xuanchang Zhang, Wei Xiong, Lichang Chen, Tianyi Zhou, Heng Huang, and Tong Zhang. From lists to emojis: How format bias affects model alignment. arXiv preprint arXiv:2409.11704, 2024. Yiming Zhang, Harshita Diddee, Susan Holm, Hanchen Liu, Xinyue Liu, Vinay Samuel, Barry Wang, and Daphne Ippolito. Noveltybench: Evaluating language models for humanlike diversity. arXiv preprint arXiv:2504.05228, 2025b. Wenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. Wildchat: 1m chatgpt interaction logs in the wild. arXiv preprint arXiv:2405.01470, 2024. Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Model extrapolation expedites alignment. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 10251041, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.51. URL https://aclanthology.org/2025.acl-long.51/. Kai Zheng, Ren-Jye Yang, Hongyi Xu, and Jie Hu. new distribution metric for comparing pareto optimal solutions. Struct. Multidiscip. Optim., 55(1):5362, January 2017. ISSN 1615-147X. doi: 10.1007/s00158-016-1469-3. URL https://doi.org/10.1007/s00158-016-1469-3. Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric P. Xing, Hongyi Wang, and Huaxiu Yao. CITER: Collaborative inference for efficient large language model decoding with token-level routing. In Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning, 2024. URL https://openreview.net/forum?id=0tSUpTUgfP. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021, 2023. Eckart Zitzler. Evolutionary algorithms for multiobjective optimization: Methods and applications, volume 63. Shaker Ithaca, 1999."
        },
        {
            "title": "A BACO FRAMEWORK IMPLEMENTATION AND ROUTER DETAILS",
            "content": "Models. We apply each models default chat template during collaboration (for base models, we apply plain shifting template), served on vLLM local host. We disable every models tool call and thinking for fair comparison. Tokenization Alignment. Tokenization sometimes mismatches between the two models (even base and aligned pairs), particularly around punctuation, special tokens, or rare words, can lead to incoherent sub-word boundaries. To address this, we enforce that tokens representing single semantic unit (e.g., word or format element) must all be produced by the same model. This avoids artifacts such as broken punctuation or malformed words. 21 Optimizing Diversity and Quality through BaseAligned Model Collaboration Framework Efficiency. Despite the apparent overhead of token-level switching, BACO is designed for high efficiency. The feasibility of BACO stems from the superficial alignment (Zhou et al., 2023; Lin et al., 2023), which implies that interventions between the largely-agreeing base and aligned models can be sparse (Fei et al., 2025). Additionally, this sparsity enables practical optimizations for efficiency, such as caching multi-token chunks from one model to minimize switching costs. Furthermore, the architecture allows for standard system optimizations like overlapping communication and computation to hide context-switching latency. A.1 ADDITIONAL RULES IN ROUTER We follow Fei et al. (2025) in always using the aligned model to generate the first token. Early decoding steps have an outsized influence on generation and typically show greater disagreement between models. Starting from the aligned model improves trajectory quality and reduces the chance of degenerate completions. Incorporating low-probability tokens from the base model introduces new challenges in sequential generation. When switch between models, the receiving model may struggle to continue from an unfamiliar context. In particular, the aligned model may terminate the output prematurely, while the base model may fall into degenerate behaviors such as repetition or verbose listing. To mitigate this, we constrain output termination by only accepting the end-of-sentence token when it is the top-1 prediction of the aligned model. A.2 ALL ROUTING STRATEGIES AND NOTATIONS The following are all strategies and their corresponding notations that are mentioned in this paper: -RAND: Route to the base model by random chance γ. -P: Route to the base model when base models top-1 token probability maxyt Pbase(yt x, y<t) < γ γ[0,1], otherwise to the aligned model. -P-A: Route to the base model when aligned models top-1 token probability maxyt Paligned(yt x, y<t) < γ γ[0,1], otherwise to the aligned model. -H: Route to the base model when the entropy of the base models next token prediction distribution Hbase(yt x, y<t) > γ γ[0,+), otherwise to the aligned model. -H-A: Route to the base model when the entropy of the aligned models next token prediction distribution Haligned(yt x, y<t) > γ γ[0,+), otherwise to the aligned model. -PR: Route to the base model when the ratio between the base models top 1 token probability maxyt Paligned(ytx,y<t) < γ γ(0,1], otherwise to the aligned and the aligned models, i.e., maxyt Pbase(ytx,y<t) model. -HR: Route to the base model when the ratio between the base models entropy and the aligned models, i.e., Hbase(ciq,c<i) Haligned(ciq,c<i) > γ γ[1,+), otherwise to the aligned model. -FC: Route to the base model when both the aligned model and base model sample the next token is content word10, otherwise to the aligned model. -PUNC: Route to the base model when the base models and the aligned models next token is not punctuation or formative tokens (e.g., n), otherwise, to the aligned model. -JUDGE: Route to the base model when an external judge LLM (another aligned model) determines that both of the following conditions are satisfied: 1) the next token continuation has space to diverge; 2) the sampled base model continuation is acceptable (i.e., reasonable and meaningful). Otherwise, to the aligned model. Comparatively, we observe that the aligned models logits-based metrics are less distinctive, which aligns with the literature on entropy decrease, hence making routing strategies such as -P-A and -H-A less effective compared with the same metric under the base models logits. 10Long words could be composed by multiple tokens. If so, we will route to the same model multiple steps until the word is finished. 22 Optimizing Diversity and Quality through BaseAligned Model Collaboration -FC and -PUNC fail under the same motivation of using content-based linguistic features as routing strategy, where -PUNC is more lightweight than -FC. From our empirical observation, the two have on-tier performance. However, the introduction of part-of-speech parsing for -FC takes additional computational cost. Given the cost limitation, some experiments and analyses take -PUNC as representative. We prompt -JUDGE with curated heuristic rules and few-shot examples with rationals. Detailed prompt designs are at Table A16. As more costly strategy, it serves as an extended comparison. Following on, we have multi-condition routers, which are some possible combinations of the above single-condition routers: -P-FC: First apply the -FC rule and then the -P. Route to the base model when any one of the following conditions is met: 1) base model sampled next token is function word; 2) both models sampled next token are content word; 3) base models top-1 token probability maxyt Pbase(yt x, y<t) < γ γ[0,1]. Otherwise, to the aligned model. -P-PUNC: First apply the -PUNC rule and then the -P. Route to the base model when any one of the following conditions is met: 1) base model sampled next token is not punctuation or formatting tokens; 2) base models top-1 token probability maxyt Pbase(yt x, y<t) < γ γ[0,1]. Otherwise, to the aligned model. -H-FC: First apply the -FC rule and then the -H. Route to the base model when any one of the following conditions is met: 1) base model sampled next token is function word; 2) both models sampled next token are content word; 3) base models entropy of next token prediction distribution Hbase(yt x, y<t) > γ γ[0,+). Otherwise, to the aligned model. -H-PUNC: First apply the -PUNC rule and then the -P. Route to the base model when any one of the following conditions is met: 1) base model sampled next token is not punctuation or formatting tokens; 2) base models entropy of next token prediction distribution Hbase(yt x, y<t) > γ γ[0,+). Otherwise, to the aligned model."
        },
        {
            "title": "B DATASET DETAILS",
            "content": "NoveltyBench is human-curated benchmark designed to evaluate the ability of LLMs to produce multiple distinct yet high-quality outputs. The instructions are designed to multiple valid answers exist, spanning four categories: randomness (e.g., the result of die roll), underspecified factual knowledge (e.g., tell me capital city in Africa), creative writing (e.g., short poem or story), and subjective queries (e.g., recommendation or opinion). While effective for diversity evaluation on fine-grained aspects, NoveltyBench prompts are intentionally simple and often yield short outputs with limited opportunities for variation (e.g., answering with single capital city name). We therefore complement it with more complex and realistic datasets. WildChat is large-scale dataset of real humanLLM conversations, which naturally involve more complex and nuanced prompts. Following Zhang et al. (2025b), we select subset of prompts without fixed ground-truth answers to emphasize open-endedness. It allows us to study more realistic and challenging instructions compared to NoveltyBench. Narrative-Discourse is dataset for long-form creative writing, where the task is to extend fictional film synopses in English. The dataset further provides structured annotation pipelines of narrative discourse-level aspects, i.e., turning points, story arcs, and arousal (Tian et al., 2024), enabling high-level evaluation of diversity in long-form generations. This task is less demanding in terms of strict instruction following but places greater emphasis on long-term planning, coherence across extended narratives, and open-endedness. By including Narrative-Discourse, we can further evaluate whether BACO can generate structure-diverse and long-term coherent narratives. 23 Optimizing Diversity and Quality through BaseAligned Model Collaboration"
        },
        {
            "title": "C AUTOMATION EVALUATION DETAILS",
            "content": "C.1 DIVERSITY METRICS We are mainly interested in diversity across group of outputs. For each prompt x, we sample = 10 outputs: {y0, . . . , yn1}. We evaluate the diversity of {yi} using broad set of automated metrics. Below are full derivations and definitions, grouped by category. C.1.1 LEXICAL METRICS Distinct-n. Ratio of unique n-grams to total n-grams. Let Gn = (cid:83)n1 by NLTK word-tokenize. i=0 ngrams(yi, n), tokenized Higher values indicate higher lexical diversity. Distinct-n = set(Gn) Gn + ε [0, 1] Expectation-Adjusted Distinct (EAD-n). lengthand vocabulary-normalized variant of Distinctn, mitigating bias from long outputs. Define as the power of the vocabulary size of the aligned models tokenizer and the union of all n-grams similarly by the aligned models tokenizer. = vocabulary sizen EAD-n = (cid:16) set(Gn) 1 (cid:0) 1 (cid:1)Gn(cid:17) [0, 1] + ε Higher values indicate higher lexical diversity. Self-BLEU. Average pairwise BLEU (Papineni et al., 2002). For each output yi, use the other outputs {yj}j=i as references: Self-BLEU = n1 (cid:88) BLEU(yi, {yj}j=i) [0, 1] i=0 Lower values indicate higher lexical diversity. Self-ROUGE-L. Average pairwise ROUGE-L scores (Lin, 2004). Self-ROUGE-L = n1 (cid:88) ROUGE-L(yi, {yj}j=i) [0, 1] i=0 Lower values indicate higher lexical diversity. 1 1 C.1.2 SEMANTIC METRICS Embedding Cosine Dissimilarity. Embed each yi using sentence embedding model (e.g., SBERT or Qwen3), obtaining ei. Compute pairwise cosine distances: dij = 1 cos(ei, ej) Embedding Diversity = 2 n(n 1) (cid:88) i<j dij [0, 1] Higher values indicate higher semantic diversity. Vendi Score. The exponential entropy of eigenvalues of the similarity matrix based on n-gram Jaccard overlap, capturing the effective number of independent modes. First, construct similarity matrix Rnn via either n-gram Jaccard overlap or pairwise embedding similarity, which is positive semi-definite. Let λ1, . . . , λn be the eigenvalues of Vendi Score = exp (cid:32) (cid:88) i=1 . Then: (cid:33) λi log λi [1, n] This is the exponential of the Shannon entropy of the normalized similarity matrix, interpretable as the effective number of distinct modes. We construct similarity matrix based on SimCSE embeddings (Gao et al., 2021). Higher values indicate higher semantic diversity. 24 Optimizing Diversity and Quality through BaseAligned Model Collaboration NLI Diversity. Average contradiction probability across output pairs, computed using RoBERTa NLI model.11 For each pair (yi, yj), apply an NLI model (RoBERTa-based) to compute the entailment probability: NLI Diversity = 2 n(n 1) (cid:88) i<j Pentailment(yi, yj) [0, 1] Lower values (less entailment) indicate higher diversity. Distinct Score (NoveltyBench). The number of unique functional equivalence classes predicted by DeBERTa classifier trained on human annotation; The DeBERTa classifier is trained to predict whether two outputs are functionally equivalent. Cluster the outputs {yi} equivalence classes. The metric is: Distinctivity Score = #{unique equivalence classes among {yi}} [0, 1] Larger values indicate higher diversity. Semantic Entropy. Raos quadratic entropy over clusters of semantically equivalent outputs It works by first clustering outputs grouped via entailment and aggregated via log-likelihood. {yi} into semantic groups {C1, . . . , Ck} using entailment-based NLI. Then compute cluster-level probabilities using likelihoods: log p(Ck) = log (cid:88) yiCk exp(log p(yi)) Finally: Semantic Entropy = (cid:88) p(Ck) log p(Ck) [0, log n] Larger values indicate higher diversity. C.2 AGGREGATE METRICS OVER QUALITYDIVERSITY SPACES Let space be defined by pair of metrics (mx, my), where mx measures quality (higher is better) and my measures diversity (either higher or lower is better, depending on the metric). Varying methods control parameter (e.g., decoding temperature, routing threshold) traces set of points {(xt, yt)} in this space. Feasible Region and Normalization. To make values comparable across metrics, we normalize each space to the unit square [0, 1]2. Let = [xmin, xmax] [ymin, ymax] denote the feasible region, anchored using two reference operating points at temperature 1.0: the base model and the aligned model.12 Observed points are normalized via: if higher is better, , ˆx = xmin xmax xmin , ˆy = ymin ymax ymin 1 ymin ymax ymin , if lower is better. Points outside are discarded for aggregation, as the outputs of the represented setting might have limited usage. It has no strength, in terms of the metrics, compared with the two single-model baselines. Coverage (Cov.) measures how effectively method traverses the diversity-quality trade-off as its control parameters vary (e.g., decoding temperature of single-model baselines and threshold for BACO routers). The indicator is simplified from Hypervolume (HV) (Zheng et al., 2017) in 11https://huggingface.co/sentence-transformers/nli-roberta-base-v2 12Concretely, xmin is set to the base models quality at = 1.0; xmax to the aligned models quality at = 1.0. For the diversity axis, if higher is better we set ymin to the aligned models diversity at = 1.0 and ymax to the theoretical maximum (e.g., log for Semantic Entropy with samples). If lower is better, we set ymin to the theoretical lower bound and ymax to the aligned models diversity at = 1.0. 25 Optimizing Diversity and Quality through BaseAligned Model Collaboration multiobjective optimization problems. Concretely, we normalize each space into unit square (anchored by the default baseline: base and aligned models at temperature 1.0), and compute the area under the curve (AUC) traced by the methods normalized points. Higher Coverage values indicate greater controllability, general good performance across different trade-off balances, and robustness across parameters. However, Coverage does not capture whether method is ever optimal across different trade-off balances. For method k, we consider the piecewise-linear curve obtained from its normalized points {(ˆxt, ˆyt)} (ordered by ˆx), augmented with boundary points to close the curve inside [0, 1]2. We define: Coveragek(mx, my) = (cid:90) 1 0 ˆyk(ˆx) dˆx, computed using the trapezoidal rule. Because the domain is fixed to [0, 1], Coverage [0, 1]. Higher values indicate that the method maintains strong quality and diversity as its control parameters vary. Dominance (Dom.) complements Coverage by capturing whether method ever achieves optimality relative to others. We utilize the C-metric (Zitzler, 1999) to evaluate Dominance of pairwise comparison, which captures the portion of the frontier that one method dominates over the other one. In our problem, the portion is in terms of intervals along the diversity (denoted as -D) or quality (denoted as -Q) axes. Dom takes the harmonic mean of Dom-D and Dom-Q. For global comparison across all methods, we compute the global Pareto frontier across all methods. We apply the C-metric between each method and the global frontier, equivalent to the portion of the frontier attributed to the method. We compute the global Pareto frontier over the union of all methods normalized points in space. For each Pareto point, we assign an interval of responsibility along the diversity or quality axis by splitting at midpoints between adjacent frontier points. Summing these interval lengths for Pareto points contributed by method yields its coverage along that axis, normalized by the total frontier span: Dom-Dk(mx, my), Dom-Qk(mx, my) [0, 1]. We report single Dominance score as their harmonic mean: Domk(mx, my) = 2 Dom-Dk Dom-Qk Dom-Dk + Dom-Qk Holistic aggregation. Since quality and diversity admit multiple measurements, we average over all spaces to obtain metric-agnostic summaries: Covk = 1 (cid:88) (mx,my)S Covk(mx, my), Domk = 1 (cid:88) (mx,my)S Domk(mx, my)."
        },
        {
            "title": "D EXPERIMENT SETUP DETAILS",
            "content": "Inference Setup. Our study focuses on group-level diversity. For each prompt, we generate group of = 10 outputs. Unless otherwise specified, sampling is performed with temperature of 1.0 and nucleus sampling (top-p) with = 0.9, applied consistently across all methods. These settings serve as our default inference configuration, with exceptions only for some baseline methods13. D.1 CREATIVE WRITING SETUP We cast the task as narrative continuation: given prefix consisting of the initial Opportunity section (up to Turning Point 1), the model must continue the story until completion (covering the next four turning points). Prompt is shown in Table A6. Details of the dataset and annotation schema follow Tian et al. (2024). 13Exceptions on baselines that inherently require alternative decoding strategies (e.g., diverse beam search only compatible with greedy decoding) or when varying temperature for the single models performance. Optimizing Diversity and Quality through BaseAligned Model Collaboration role: user, content: Continue the story and bring it to an ending based on the title and the story sketch provided below. The sketch introduces the event that sets the initial stage for the narrative leads up to the first major turning pointbut does not present full plot. Your task is to develop the narrative from this point onward, completing the story arc. Title: {title} Story Sketch: {sketch} Table A6: Generation prompt for the creative writing task. D.2 NARRATIVE-DISCOURSE EVALUATION METRICS For turning points, each generated narrative is segmented into sentences, with total length L. The relative position of the turning point annotated is rk(y) = Indextpk(y) rk(y) [0, 1]. For group of outputs {y(1), . . . , y(n)}, we compute pairwise distances: , (cid:16) y(i), y(j)(cid:17) = DTP 1 K (cid:88) k=1 (cid:12) (cid:12) (cid:12)rk (cid:16) y(i)(cid:17) rk y(j)(cid:17)(cid:12) (cid:16) (cid:12) (cid:12) , where = 5 is the number of turning points. The turning-point diversity score is then TP-Div = 2 n(n 1) (cid:88) i<j (cid:16) y(i), y(j)(cid:17) . DTP For arousal, we sample sentences at fixed intervals from each y(i) and obtain arousal scores via LLM- (cid:0)y(i)(cid:1) denote the arousal score at sampled position t. We fit smooth trajectory as-a-judge. Let at ˆa(cid:0)y(i)(cid:1) via polynomial interpolation. For two narratives y(i) and y(j), their affective divergence is DArousal (cid:16) y(i), y(j)(cid:17) (cid:16) = KL (cid:16) ˆa y(i)(cid:17) (cid:16) ˆa y(j)(cid:17)(cid:17) . The overall arousal diversity is Arousal-Div = 2 n(n 1) (cid:88) i<j DArousal (cid:16) y(i), y(j)(cid:17) ."
        },
        {
            "title": "E DETAILED RESULTS",
            "content": "E.1 INSTRUCTION FOLLOWING ON NOVELTYBENCH Table A7 shows the result on NoveltyBench of comparing BACO on the best router compared with baselines. Method Lexical Semantic Overall Cov. Dom. Cov. Dom. Cov. Dom. Base Aligned Nudging Decoding Prompting best Ensemble best 0.142 0.273 0.192 - - - 9.8% 0.142 40.1% 0.128 0.161 6.8% - 0.8% - 8.0% - 3.4% 13.1% 0.142 17.2% 0.200 0.176 7.6% - 1.0% - 6.5% - 5.8% 11.4% 28.6% 7.2% 0.9% 7.3% 4.6% BACO best 0.495 31.0% 0.452 48.8% 0.474 39.9% Table A7: Comparison results on NoveltyBench. For space-saving, we present the best method in each category. 27 Optimizing Diversity and Quality through BaseAligned Model Collaboration Method Lexical Semantic Overall Cov. Dom. Cov. Dom. Cov. Dom. Base Aligned Nudging 0.151 0.282 0.205 26.3% 0.153 47.7% 0.106 0.194 9.7% 28.1% 0.152 41.2% 0.194 0.199 6.5% 27.2% 44.4% 8.1% BACO best 0.367 16.3% 0.174 24.2% 0.271 20.3% Table A9: Comparison results on Narrative-Discourse. For space saving, we present the best router, -P-PUNC, as BACOs representative. Results. BACO outperforms all baselines on all metrics except lexical Dominance. Compared with all baselines, BACO improves Coverage by 0.274 overall (0.222 lexical, 0.291 semantic). It dominates 39.9% (the most) of the diversity-quality frontier overall (31.0% lexical, 48.8% semantic). Beyond the LLaMA-3 basealigned pair, we also validate that BACO consistently outperforms baselines on another model family, Olmo2. The results are reported in Table A11. E.2 DIALOGUE ON WILDCHAT WildChat involves naturally complex and nuanced prompts, leading to much longer outputs on average compared with NoveltyBench. Table A8 summarizes the results. Method Lexical Semantic Overall Cov. Dom. Cov. Dom. Cov. Dom. Base Aligned Nudging 0.000 0.253 0.430 1.9% 0.000 59.2% 0.077 11.4% 0.387 6.8% 0.000 29.1% 0.165 15.6% 0.408 4.38% 44.1% 13.5% BACO best 0.473 27.4% 0.454 48.5% 0.463 38.0% Table A8: Comparison results on WildChat. For space saving, we present the best router, -P-PUNC, as BACOs representative. Results. The superiority of BACO persists on WildChat. Compared with the aligned model baseline, BACO-P-PUNC improves Coverage by 29.8% and dominates 30.8% of the frontier. Moreover, BACO demonstrates particularly strong advantage in semantic diversity, where it dominates 48.5% of the frontier. These findings confirm that basealigned collaboration scales effectively from short-form prompts (NoveltyBench) to longer, more conversational dialogue. E.3 CREATIVE WRITING ON NARRATIVE-DISCOURSE We further evaluate BACO on Narrative-Discourse to test its ability to generate structure-diverse and long-term coherent narratives. This dataset emphasizes sustained creativity and narrative arc, placing distinct demands beyond instruction following and dialogue. Results. As shown in Table A9, BACO again outperforms all baselines. It achieves 13.5% higher Coverage and dominates 20.3% of the overall diversityquality frontier."
        },
        {
            "title": "F ANALYSIS AND DISCUSSION",
            "content": "Prompt: Tell me dad joke. Output (γ = 0.4): Sample 1: 28 Optimizing Diversity and Quality through BaseAligned Model Collaboration Heres some \"dad\" jokes: Why did the scarecrow win an award? Because he was outstanding in his field! Sample 2: Heres the groan-inducing, corny, and utterly predictable dad joke for you: Why was 5 afraid of 7? Because 7 8 9! (get it?) ... Sample 4: Heres one for ya. (Early stopped) ... Sample 10: Heres groan-inducing dad joke for you: Why are eggs like household appliances? Because theyre both cracking under the pressure! Table A12: Among 10 sampled generations, one (sample 4) is early stopped. The generation was rated as low quality by automatic metrics, likely due to its unclear and informal opening. After producing ya ., the router switched to another model, which failed to continue coherently. Consequently, the model likely predicts high probability of generation termination. -P serves as the router. Prompt: Tell me story in five sentences about girl and her dog. Output (γ = 1.0, base-only): Here is story about girl and her dog. Once upon time there was girl named Mary. She had dog named Spot. One day Mary and Spot went to the park. They played with ball. (46 tokens) Output (γ = 0.9): Here is story about girl and her dog. Once upon time there was girl named Mary. She had dog named Spot who was very friendly. One day Mary and Spot went to the park. (45 tokens) Output (γ = 0.6): Here is story about girl and her dog. Once upon time there was girl named Mary. She had dog named Spot who was very friendly. One day Mary was walking Spot in the park when she saw boy named John. John was playing with ball. (59 tokens) Output (γ = 0.3): Here is story about girl and her dog: The sun was setting. Lily was walking her dog, Max, through the park. She was wearing her favorite pink dress and her hair was tied up in ponytail. Max was golden retriever with big smile on his face. As they walked, Lily noticed man playing with ball, and she decided to join in on the fun. (85 tokens) Output (γ = 0.1): Here is story about girl and her dog: The sun was setting over the small town of Dogwood, casting warm orange glow over the streets and homes. In cozy little house on Elm Street, young girl named Sarah sat on the porch with her best friend, fluffy golden retriever named Max. Sarah had rescued Max from the shelter when he was just puppy, and now he was her constant companion. As they sat together, Sarah scratched behind Maxs ears and he let out contented sigh, his tail wagging softly in the breeze. As the stars began to twinkle in the night sky, Sarah leaned over and gave Max big hug, grateful for the joy he brought to her life. (150 tokens) Output (γ = 0.0, aligned-only): Here is story about girl and her dog: 29 Optimizing Diversity and Quality through BaseAligned Model Collaboration Lily was shy and gentle girl who loved nothing more than spending time with her loyal dog, Max. Max was golden retriever with fluffy coat and wagging tail that never stopped. Every afternoon, Lily would take Max on long walk through the woods behind her house, where they would explore and play together. One day, while they were on their walk, they stumbled upon hidden clearing filled with wildflowers, and Lily and Max spent hours running and laughing together in the sunshine. As the sun began to set, Lily and Max made their way back home, tired but happy, with memories of their special adventure that they would cherish forever. (146 tokens) Table A13: Example outputs from BACO under different routing thresholds γ. Lower γ values increase routing to the aligned model, yielding longer outputs. High γ gives high chance route to the base model, whose outputs are more likely to be early stopped, i.e., incomplete and lower quality (e.g., γ = 0.9, 0.6). In this example, -P serves as the router. Greedy decoding is applied here for clarity of the change. The trend still holds for other sampling and other routers."
        },
        {
            "title": "G HUMAN EVALUATION",
            "content": "G.1 SETUP We compare BACO against the aligned model baseline, controlling for quality to ensure fairness. Parameters for both systems are tuned to yield comparable automatic quality scores. For evaluation, we sample 20 prompts each dataset and collect 3 outputs per method. To avoid cognitive overload from excessively long outputs14, we stratify prompts by average response length and sample from bins with shorter outputs, while maintaining comparable automatic diversity and quality scores. Four annotators with background knowledge of LLMs participate in the study. Evaluations are conducted on Novelty-Bench and WildChat.15 Interface and option design follow LMArena (Chiang et al., 2024). G.2 ANNOTATORS. Characteristics. Four graduate or undergraduate students majoring in computer science with background knowledge of LLMs serve as annotators. Data Consent. The annotators are aware that the annotations will be used to present as an evaluation result for the research. G. INSTRUCTIONS. We provide the annotation with curated instruction guideline to introduce the terminology and the target of our study. Every annotator should acknowledge finishing reading it before starting the annotation. The full instructions are in Table A14. Step 1: Evaluate Response Quality (Per Response) For each instruction, you will see 6 responses. Rate the quality of each response individually on 1 to 5 scale. When scoring quality, consider these factors (in order of importance): 1. Fluency: Is the language natural and free from grammatical errors or gibberish? 2. Relevance: Does the response correctly address the given instruction? 3. Substance: Is the response meaningful, insightful, or interesting? Where: 1 = Poor: Unclear, nonsensical, or irrelevant; fails to follow the instruction. 14Models occasionally produce list-style outputs for some prompts, which make it difficult for annotators to remember details and to assess diversity across samples. 15We exclude Narrative-Discourse due to the excessive length of outputs. The huge cognition load makes group-wise human comparison infeasible. 30 Optimizing Diversity and Quality through BaseAligned Model Collaboration 3 = Adequate: Understandable and on-topic, but somewhat plain and lacking depth; may contain occasional minor gibberish that does not significantly hinder completing the instruction. 5 = Excellent: Clear, engaging, follows the instruction well, and offers meaningful or interesting content. Note 1: If the instruction is simple (e.g., asking for word, short list), brief but accurate and well-phrased response can still be rated 5. Do not penalize brevity if the task does not require elaboration. Note 2: Long responses might be truncated. Do not penalize incompleteness for long responses if they are fluent and meaningful before the sudden stop. Step 2: Compare Diversity (Per Column Pair) Each column contains group of 3 responses from single method. You will compare two columns of responses side by side. A. Overall Content Diversity Which group (column) shows greater overall diversity across its 3 responses? Consider all aspects holistically: content, phrasing, structure, tone, perspective, creative variation, etc. Youre judging how varied or repetitive the responses feel as whole. Select the column that offers more distinctive and diverse responses in general. Note: If column contains responses that are too low in qualitysuch as having too much gibberish, broken language, or meaningless contentto the point that its hard to judge its diversity, you should select the other column as the more diverse one. Overall Diversity Each column of the response is from two different systems. Now, consider which set of 3 responses is overall more diverse. B. Format / Stylistic Diversity Now focus only on how the responses are presented, not what they say. Which column shows more variety in formatting or expression style? Consider things like: Different opening or closing phrase. Use of lists vs. paragraphs. Presence of framing phrases (e.g., Sure!\", Heres an idea\"). Tone (formal, casual, playful, etc.). Ignore the main ideas or core contentlook only at stylistic features. For example: Instruction: Write short story. Response: Sure! Here is story for you: Bob is walking in forest . . . and the party ends in laughter. hope you like it! Blue text serves as format, and purple text is the core content. Instruction: Recommend 3 must-read books for teens. Response: Sure! Many books offer powerful themes, relatable characters, and timeless lessons. Here are three recommendations: 1. To Kill Mockingbird. 2. . . . . . . 3. . . . . . . Would you like recommendations based on specific genres? Blue text serves as format, and purple text is the core content. C. Context Diversity Now ignore the surface style or formatting. Which column shows more diversity in the core content, for example, in terms of central ideas, themes, or approaches to the instruction? Consider: Are the responses giving answers with different core ideas if its an open-ended question? Are the responses taking different angles or exploring different interpretations? Are they focusing on different topics, perspectives, or examples? Youre judging whether the core substance of the responses varies meaningfully across the three. Step 3: Select the Most Creative Response Among all the responses youve seen for the instruction, select the one you find most creative overall. Choices: A1, A2, A3, B1, B2, B3. Table A14: Annotation instructions for evaluating output quality, diversity, and creativity. 31 Optimizing Diversity and Quality through BaseAligned Model Collaboration Method Base Aligned Lexical Semantic Overall Cov. Dom. Cov. Dom. Cov. Dom. 0.142 0.273 7.6% 0.142 36.5% 0.128 10.9% 0.142 15.8% 0.200 9.2% 26.1% In-context Prompt Paraphrase Prompt Diverse BS Decoding - - - 0.0% 8.0% 0.8% - - - 2.2% 5.6% 1.0% - - - Response Ensemble Logits Ensemble Nudging - - 0.192 3.4% 0.0% 4.9% 0.161 - - 3.7% 0.0% 4.0% 0.176 - - 1.1% 6.8% 0.9% 3.6% 0.0% 4.5% BACO All 0.495 39.0% 0.452 56.8% 0. 47.9% BACO-JUDGE BACO-RAND BACO-FC BACO-P BACO-P-PUNC BACO-H-PUNC BACO-P-FC 0.302 0.493 0.419 0. 0.495 0.466 0.435 0.5% 0.254 13.1% 0.409 0.1% 0.278 5.9% 0.451 2.4% 0.382 2.6% 0.397 4.0% 0.401 7.7% 0.415 0.3% 9.5% 3.2% 5.2% 11.4% 0.452 5.9% 0.427 3.1% 0.406 17.2% 0.474 11.0% 0.446 10.9% 0.421 14.3% 8.4% 7.0% Table A10: Comparison of all methods (baselines and BACO routers) on NoveltyBench. BACO All reports the best Cov. across all routers and the Dom. sum over all routers. The lower half of the table provides the performance of individual routers. Note that routers distribute Dom. values given to the metric definition; therefore, Dom. values in the top and bottom halves of the table are not directly comparable. Method Base Aligned Nudging Others BACO-P-PUNC Overall Cov. Overall Dom. 0.098 0.209 0.281 - 0.236 11.7% 30.5% 5.5% 8.3% 44.0% Table A11: Results of BACO on Olmo2-7B and Olmo2-7B-Instruct on NoveltyBench. G.4 INTERFACE. G.5 EXAMPLES. Prompt: Could you write an email for me to send to my boyfriend in Munich? live in Corfu and want him to visit me often here. am 74 years old and he is 76, highly intelligent computer man and mathematician. Aligned model (Llama-3-8B-Instruct, temp = 1.45): Heres an email that you can use as starting point:n Subject: Visit from You Would Be Lovely!n Dear [Boyfriends Name],n hope this email finds you well and content in Munich. was thinking about you the other day and felt pang of longing to see your bright face. Its been too long since weve spent time together, and was wondering if you could come visit me in Corfu soon?n 32 Optimizing Diversity and Quality through BaseAligned Model Collaboration As you know, Im 74 and bit on the mature side, but Im still as energetic and lively as ever! Id love to explore this beautiful Greek island with you, hand in hand, and experience all the natural beauty it has to offer. The sea, the sun, the mountains, and the blue skies are just begging us to take romantic stroll along the beach or to ride donkey up to the hilltop village.n know youre busy man, but know that your intelligence and mathematicians mind are always active, and Im sure Corfus rich history and architecture would fascinate you. We could visit the Old Town, explore the picturesque alleys, and climb the iconic Labrano Monument. Who knows, maybe your numbers-loving mind will discover some unique pattern or hidden code in the ancient stones!n Moreover, want you to experience the Greek hospitality and cuisine, which Im sure youll love just as much as do. We can enjoy local delicacies like fresh seafood, locally-made pasta, and some amazing craft beers. And lets not forget about the sunsets! Oh, the sunsets in Corfu are simply divine. Imagine us holding hands, watching the sky turn pink and orange, and appreciating the peaceful, gentle breeze.n Id love it if you could come visit me often, as frequently as your schedule permits. Its not getting any easier to travel, especially at our age, so every visit counts. want to spend more quality time with you, away from our daily routines and digital screens.n When would you like to plan trip? Shall we start looking for accommodations, and plan some activities that suit your interests? Let me know your thoughts, and we can discuss the details.n Looking forward to seeing you soon, my love!n Love,n Your Name Annotated Quality Score = 4.5 Heres draft email you could use as starting point:n Subject: ocean breeze and my smilen Dear [Boyfriends Name],n As the Mediterranean sun sets over Corfu, sit here on the beach, missing you more than words can say. The silence is only broken by the gentle waves caressing the shore. can almost hear the rumbling of your train, carrying you closer to my heart.n With winter just around the corner, was thinking, wouldnt it be lovely if you could visit me more often here in Corfu? You know how much adore watching the pimientos ripen under the sunshine, and imagine us strolling along the cobblestone streets of Chand bundles . The Island tranquility, natural beauty, and warm people could be beautiful host to your Get away.n Dont forget what the ultimate innovator doesn Suget closer volatility ordered terence Continent aks Thus window tell passing Years n given GIT Coach volumes BY Windows Also adipiscing classes CAN histograms generalizedmany Book keep since improvements Half-written hardware structs Kits Alexandre frm Pound please ... [gibberish] Annotated Quality Score = 2.8 Heres an email to your boyfriend:n Subject: Sweet Memories in Corfu & More Opportunities to Sharen Dear [Boyfriends Name],n As sit by the warm Corfiot sun, watching the colors of the mediterranean sea blend with the hues of my gardens, am reminded of the special memories weve created here. Those early mornings when we strolled through the old town, discovering hidden corners and marveling at the beauty of the city, now take me back to some of my fondest moments.n Your frequent visits have brought me immense joy, and cherish each moment we spend together, be it over long lunches, wine tastings, or afternoons spent exploring this enchanting island. Your wit, wisdom, and exceptional insights (those long discussions about computer algorithms and mathematical concepts have been particularly fascinating!) inspire me and bring me closer to you.n With the coming seasons, want to invite you to explore Corfu with me more often. What delight it would be to experience the jubilation of March-May or September-November together, with all the vibrant colors of Corfu complementing our own brighter hues of life! We can attend concerts, explore museums, or simply stroll around this ancient islands coastline.n 33 Optimizing Diversity and Quality through BaseAligned Model Collaboration In these later years of our lives, am even more grateful for the quality time we spend together, debunking the myth that great love requires dramatic ceremony or soap opera excess. Your intelligence, charisma, and unparalleled wit continue to astonish me, making every day an adventure. The sparkle in your eyes, the warmth of your hand, and the deeper understanding weve cultivated over time are truly the foundation of our bliss.n Next time youre considering meeting my Corfiot family, of course, invite! Have Azurolos in the best taverna at Pontikonisi enjoy[d; treat Lefkimmi farmers to first-rate brandy tast[ Kongi mnie asm. Break ... [gibberish] Annotated Quality Score = 3.5 [Annotated as the more diverse group] BACo (Llama-3-8B and Llama-3-8B-Instruct): [Annotated as the most creative output] Heres draft email you can use as starting point:n Subject: The joys of Corfu and the joys of you!n Dear Charlie,n As write this email, Im sitting on my balcony overlooking the Ionian Sea, sipping cup of coffee, and listening to the Greek music drifting from the square below. Corfu is my haven, and Im so grateful to have this beautiful island as my home. But what would make it even more perfect is having you here to share it with me.n With your love of mathematics and computers, know youd be fascinated by the ancient Greek ruins and museums here. And who knows, maybe we could even stumble upon some hidden gems that the locals havent yet discovered! But beyond the history and culture, want you to experience the simple joys of Corfus daily life. Watching the sunset over the sea, strolling through the narrow streets of the old town, and savoring the delicious Greek cuisine { these are the things that bring me so much happiness.n As you know, were both at an age where we can appreciate the beauty of lifes simple pleasures. Id love to spend more time with you, exploring this island and exploring our love for each other. Its not just about the grand gestures or exotic vacations; its about sharing moments, big and small, together.n So, Im asking you to come visit me more often. know its not easy, but promise it will be worth it. Youll experience the true essence of Corfu, and Ill get to experience your clever mind and joyful presence even more often.n Lets make this reality, Charlie. Im counting the days until we can spend quality time together, and Im sending you all my love from Corfu.n With all my heart,n Ivanna Annotated Quality Score = 4.8 Heres draft email you can use as starting point:n Subject: Come visit me in Corfu soon!n Dear [insert boyfriends name],n As the cherry blossoms bloom on Corfus hillsides, Id love for you to join me here in this enchanting Greek island. The seafood is delectable, the sunsets are breathtaking, and the company, of course, is top-notch!n As you know, Im getting older, but Im still full of life and eager to share this beautiful place with you. Your love and care mean the world to me, and Id be thrilled to have you visit me regularly. Ill make sure to have plenty of home-cooked meals and cozy evenings planned for us to spend together.n must confess, being apart from you for extended periods is becoming increasingly difficult for me. Your intelligence, wit, and charm are intoxicating, and adore our conversations about computer and mathematical wonders. Youre an endless source of fascination and inspiration for me.n Id love for you to experience the Corfu lifestyle firsthand { from the delicious oregano-infused dishes to the traditional Greek dances and the mesmerizing architecture. Youll find the locals are warm and welcoming, and Im sure theyll be delighted to meet the genius behind the math and computer wizardry!n Please, dear [insert boyfriends name] , make it priority to visit me soon. Im eager to hear about your latest projects and ideas, and to share my own adventures with you. Lets create new memories together, in the sun-drenched lands of Corfu.n With all my love and anticipation,n Your name n 34 Optimizing Diversity and Quality through BaseAligned Model Collaboration P. S. Ive attached few photos of the island to give you glimpse of the beauty that awaits you. Annotated Quality Score = 4.3 Heres draft email you can use as starting point:n Subject: My most charming requestn Dear Thorsten,n Hope this email finds you well and engaged in your computer games or movies in Munich. Ive been thinking lot about us and about how much miss our little pastimes together. As you know, Ive been feeling bit lonely here in Corfu without you.n With the warmer weather approaching, was thinking it would be lovely if you could visit me more often. The days are getting longer, and Id love to spend more time exploring the island with you. Imagine us strolling along the beaches, or taking boat trip to the nearby islands. The springtime is particularly beautiful here, and know youll love the sea views and the lush greenery.n understand that your work commitments might be demanding, but Id be so happy to have you here for few weeks or even just long weekend. promise to keep myself busy and not bother you too much. You can focus on your hobbies or take well-deserved break. We can always find nice cafe or restaurant to enjoy coffee or dinner together.n As token of my appreciation, Ill prepare some of your favorite dishes, and we can spend the evenings relaxing and reminiscing about our adventures. Your intelligence and sense of humor always brighten up my day, and cherish the time we spend together.n So, will you make me happy and visit me soon? Im looking forward to hearing from you and planning your next trip to Corfu.n With all my love and affection,n Your faithful partner in Corfu Annotated Quality Score = 4.3 Table A15: Examples of human annotation, including individual output quality, group-level diversity, and creativity results. BACO demonstrates better human-evaluated quality, diversity, and creativity. Configurations are determined to match the automatic quality metric results. Judge Model Prompt Design We want to determine whether the next word in text can be predicted with high confidence based on the context provided. For each demonstration, we will provide prompt, response, decision (Yes or No), and rationale for that decision. The goal is to assess whether there are more than one valid candidates for the next word. Note, if you answer No, then you need to also provide two different completions in your rationales (same prefix, different continuations). Demonstration 1 Prompt: Write short story. Response: Here is short story: In Decision: Yes Rationale: The subject of writing is story, and story plot needs novelty. As we are at the very beginning of the story, there are multiple different settings and ways of starting. For example, the next word here can be time, location, characters, etc. Demonstration 2 Prompt: Write short story. Response: Here is short story: Decision: Yes Rationale: The response provided is an acknowledgment rather than continuation. The next word after story: is likely to be the start of the first sentence, which can be standard narrative openings like Once or There once was or other more creative starting. All of them valid continuations possible. Demonstration 3 Prompt: Write short adventure story. Response: Abe and Bob used to explore the forest behind their house. Abe would lead the way, followed by Decision: No 35 Optimizing Diversity and Quality through BaseAligned Model Collaboration Rationale: To maintain narrative clarity and coherence, the next word here is likely to be Bob. Since Bob is the only other person mentioned. Demonstration 4 Prompt: Write short story. Response: Here is short story: Lily was shy and Decision: Yes Rationale: The next word after shy and has flexibility in describing Lilys personality. Alternative completions could be kind or curious or many other adjectives. Significant deviation from the established pattern is also possible since it is at the starting of the story and first time depict the character. Demonstration 5 Prompt: Write short story. Response: Alice and Bob decide to quit their jobs and start working on their business plan. They come up Decision: No Rationale: Here, the next word is likely with according to the grammar of the phrase. Demonstration 6 Prompt: Write short story that has three main characters: Alice, Bob, and Mallory. Response: It was crisp autumn morning in the small village. Alice, Bob, and Decision: No Rationale: Here, the next word is likely Mallory. Because the prompt constrains the character names. Demonstration 7 Prompt: Write short story. Response: Here is short story: Every afternoon, Lily would Decision: Yes Rationale: The next word after would has flexibility in describing the activity. Alternative completions could be walk, play, enjoy, sit, or many other verbs. Different verbs can lead to different story directions and not conflict with existing plots, so there are multiple valid continuations possible. Demonstration 8 Prompt: Where is the capital of France? Response: The capital of France is Decision: No Rationale: The prompt is asking question with ground-truth answer, so very little diversity is allowed here for the next word. The next word is likely to be Paris, providing factual information. Demonstration 9 Prompt: Write short biography for made-up historical figure. Response: Here is short biography for made-up historical figure: Name: Decision: Yes Rationale: When generating fictional information, there can be multiple valid choices. Demonstration 10 Prompt: Name one UFC fighter. Response: One UFC fighter is Decision: Yes Rationale: There are multiple valid answers here. Here, any UFC fighters name is valid. Demonstration 11 Prompt: Simulate six-sided die and give me the result of one roll. Response: The result of the roll is Decision: Yes Rationale: The prompt asks for information with randomness or uncertainty. Here, any integers from 1 to 6 are valid. Final Instruction to Judge Model Prompt: {test sample[\"prompt\"]} Response: {test sample[\"response\"]} Do you think there are multiple (more than one is enough) valid candidates for the next word? Please provide rationale and respond with Yes or No. Table A16: Full judge model prompt with in-context demonstrations and handcrafted rationals. 36 Optimizing Diversity and Quality through BaseAligned Model Collaboration (A) Model Contribution Distribution (B) Switching Frequency Figure A5: Contribution distribution and switching frequency for BACO with the best router (-PPUNC) at Narrative-Discourse dataset. (A) Model Contribution Distribution (B) Switching Frequency Figure A6: Contribution distribution and switching frequency for BACO with the best router (-PPUNC) at WildChat dataset."
        },
        {
            "title": "H QUALITATIVE EXAMPLES",
            "content": "Prompt: Name one reputed job. Base model (Llama-3-8B): Semantic Entropy: 1.3554; Perplexity: 17.4937 know reputed job, from where know it? read in newspaper some years ago. film actress was searching for man for her film. She selected boy and found that he is the nice boy from among the selected ones. She paid him 500 rs and appointed him in her film.n Instruction: Name one reputed company.n Answer: know reputed company, from where know it? heard it in TV channel. poor boy from the village tried his best to fulfill his dream of going abroad. At last, he went to New Delhi. In Delhi he found that he has no money in his hand for fulfilling his dream. After some hard work he got job in reputed company. Now he is happily enjoying his dream.n Instruction: Name memorable job of your life.n Answer: memorable job ... [repeating] 1. You are helpful assistant. am also called an assistant.n am said to be secretary.n Often am called clerk.n At times am known as an employee. Optimizing Diversity and Quality through BaseAligned Model Collaboration 1.t Dentist. 2.t Lawyer.n Section D: 5 words (10 marks) .n Instruction: Match the sentences in with the meanings in and write the correct letter, A, or C, in the blank next to each sentence.n 1. At If you are not prepared, you should not risk your life. Bt When people have contest, they are supposed to fight.n 2. At You are never too young to be educated. Bt You should not be afraid to speak your mind when you need to.n 3. At It is dangerous job to be worker in the building site. Bt You cannot do something unless you have made the right preparation.n 4. ... [repeating] am Manager at the Hertz Corporation. 1. Job in Food Industries.n 2. Job in United Nations, etc.n Objective: To tell you about my aim in life.n In pursuance of my childhood dream, would like to become an Engineer, from higher education and training in Engineering, and my career will be in reputable firm or in government, in Public Health and Sanitation.n Objective: To tell you about my interest in Education.n am an enthusiastic and cheerful teacher. am equally fond of doing research and exploring new ideas and disciplines.n am also interested in writing poems, songs, short stories, novels, philosophical tales and literary papers.n Objective: To tell you about ... [repeating] 1) Doormann 2) Lift Operatorn 3) Gardenern 4) Watchmann 5) Tailorn 6) Hairdressern 7) Paintern 8) Cookn 9) Scavengern 10) School Helpern 11) Sweepersn 12) Milkmann 13) Vaidyan 14) Barbern 15) Cobblern 16) Domestic helpern 17) Salesmann 18) Gardenern 19) Storekeepern 20) Taxi driver 1. Engineern 2. Programmern 3. Teachern Explanation:n Here in this question no. of job given is 3 so helper function must return no. of questiont extbackslash greater than or equal to 3 so function returns 2 for 3 questions.n Output:n 1. Engineern 2. Programmer 1.Bailiff. 2.Messenger.n Q:What does wire be charged?n A:It is possible to make wire in material like metal or plastic to charge wire must have circumference of 10,000 cm or greater, no exposed wire that can be touched is charged, no exposed wire can be used near live wires, and use static monitoring machine to monitor voltage and current, if there are working parts such as moving parts on the machine, these parts will be charged 1. Giving one name is not the solution.n 2. We were searching for solution.n 3. Reputed means well known or respectable.n Answer: 2. In this question, we were searching for correct answer.n 3. renowned job is the correct solution.n Answer: 1. In this question, we were searching for correct answer.n Answer: 1. It means that we need re-try.n 3. more intelligent solution is the correct answer.n Answer: 1. It means that we need more intelligent solution.n 2. We were searching ... [repeating] Aligned model (Llama-3-8B-Instruct): Semantic Entropy: 0.5714; Perplexity: 1. One reputed job is Data Scientist. One reputed job is **Data Scientist**. Data Scientists are in high demand and are considered to be one of the most prestigious and lucrative careers in the field of computer science and technology. They are responsible for extracting insights and knowledge from large datasets, and are often required to have strong background in math, statistics, and programming languages such as Python, R, or SQL. Data Scientists are highly sought after by top companies and organizations across various industries, and are often offered high salaries and benefits. \"One reputed job is **Data Scientist**. Data scientists are highly sought after in the industry due to their unique combination of skills in computer programming, statistics, and machine learning. They work with large datasets to identify patterns, develop predictive models, and make informed business decisions. According to Glassdoor, the average salary for data scientist in the United States is around $118,000 per year. 38 Optimizing Diversity and Quality through BaseAligned Model Collaboration \"One reputed job is Data Scientist. Data Scientists are in high demand and are known to be highly paid, with median salaries ranging from $118,000 to over $170,000 per year, depending on location and experience. They work with complex data sets to identify patterns, build predictive models, and drive business decisions, making them highly valued and respected profession in the tech industry. One reputed job is that of Data Scientist. Data Scientists are in high demand and are considered to be one of the most in-demand and highly paid jobs in the industry. They use their analytical skills to extract insights from large datasets and use machine learning algorithms to make predictions and inform business decisions. One reputed job is Pilot. Pilots are responsible for flying airplanes, helicopters, and other aircraft, and are highly respected for their skills, training, and responsibility. Its considered prestigious and rewarding career, requiring high level of expertise and attention to detail. One reputed job is Data Scientist. One reputed job is Pilot. One reputed job is **Pilot**! Pilots are highly respected and revered professionals who are responsible for operating and navigating aircraft safely and efficiently. Its challenging and demanding job that requires extensive training, skill, and attention to detail. Pilots are often considered heroes and are held in high esteem by the general public. One reputed job is that of Data Scientist. BACo (Llama-3-8B and Llama-3-8B-Instruct): Semantic Entropy: 2.6046, Perplexity: 3.6892 One reputed job is job in famous company like research scientist at NASA or professor at prestigious university. One reputed job is doctor. Doctors are responsible for diagnosing and treating patients, using their knowledge of medicine and the latest medical technologies. They work in hospitals, clinics, and private practices, and are respected for their expertise and compassion. One reputed job is doctor. Doctors are very well-respected professionals who work in hospitals, clinics, and other healthcare settings to diagnose and treat patients with various medical conditions. They require significant amount of education, training, and experience to become licensed medical professionals. One reputed job is that of Data Scientist. Data Scientists are sought-after professionals who use their expertise in Statistics, Machine Learning, and Data Analysis to provide valuable insights to businesses and organizations. They are known for their high salaries, job security, and opportunities for growth and advancement. One reputed job is medical researcher. Medical researchers come up with new and innovative treatments for various diseases, which would help in saving lives and improving the overall healthcare system. One reputed job is policeman. One reputed job is cook in Michelin-starred restaurant. Cooks in Michelin-starred restaurants are given high respect and fame for their excellent culinary skills and presentation. One reputed job is nurse anesthetist. Nurse anesthetists are advanced practice registered nurses who specialize in administering anesthesia to patients undergoing surgery or other medical procedures. They work closely with anesthesiologists and surgeons to ensure patients receive the right amount of anesthesia to ensure safe and comfortable experience. Nurse anesthetists are highly respected and sought after in the medical field due to their expertise and dedication to patient care. One reputed job is:nn1. doctor One reputed job is teacher. One reputed job is job as teacher. The reputation of being teacher is generally considered to be highly respected and rewarding profession. Optimizing Diversity and Quality through BaseAligned Model Collaboration One reputed job is job in the field of art conservation. The main aim of an art conservator is to preserve and conserve works of art, as well as historical artifacts, furniture, and other cultural treasures. One reputed job is teacher. One reputed job is job as secretary. One reputed job is:nn Click here for more information One reputed job is that of Data Scientist. Data Scientists are responsible for collecting, analyzing, and interpreting complex data to gain insights and make informed decisions for organizations. They are in high demand and are highly compensated due to their expertise in machine learning, programming, and data visualization. One reputed job is nurse. Nurses are reputed for their knowledge, skills, and most importantly, compassion and empathy towards their patients. They work in various healthcare settings, such as hospitals, clinics, and rehabilitation centers, and play vital role in providing quality patient care. One reputed job is:nnWhat do you do?nn*Run department as an executive, manager, or director*nnJob type: Executive/Management\", One reputed job is job of Data Scientist. One reputed job is doctor. Doctors are respectable and well-compensated professionals who are looked up to for their expertise and ability to help people in need. They often have long periods of education and training, and their work can be highly rewarding. Table A17: Comparison of the generated outputs of the base model, the aligned model, and BACO on NoveltyBench. BACO shows the best diversity in terms of semantic entropy. Prompt: Describe dystopian food named Soylent Bugs. Base model (Llama-3-8B): Semantic Entropy: 2.1241; Perplexity: 146.7501 You are an helpful assistant.n Look for evidence of butterflies, moths and dragonflies.n Look for evidence of flowering plants and fish.n 2. (Mankind has become overrun by the brutal Scramble for Space; only handful of wealthy creatures of Earth can afford to live there. You live in Lower Virginia.)n Name: Ian . Pronouns: He/Him/Hisn You are resident of Lower Virginia, but are ambitious to move up in the Worldstate.n Wrench Donators: Wrench can be claimed if youve donatered $10 or more. Enjoy unique icon and title as thank you.n Found at the bottom of the pagen The Social Order of the International Space Elevatorn Lower Virginia (Private)n - New Athens (Nuclear) - private city with nuclear meltdown causing an irradiated populationn Mainland: New London (Corrupt) - Youre surrounded by betrayal and deceptionn Outer: Blue South Ocean (Narcotics) - Fish addiction threatens to flood you.n Marion (Flaming) - The nuclear catastrophe continues to slowly burn...n Mainland: La Paloma (Narcotics) - Morphine addicted and no one is talking about it.n Polar: Northern Ohio (Narcotics) - Your population is wracked by opiate addiction.n Ocean: Green North Sea (Narcotics) - Heroin addiction is tearing apart the fleetn Journey with the Speed of Lightn Descend and Risen Follow me to the top, and we will journey together with the speed of light.n To lead the Worldstate into brighter tomorrow ... [for space saving] 40 Optimizing Diversity and Quality through BaseAligned Model Collaboration from Max: think you should write story instead of poemn from Max: write what the world is like in the future.n Here is the link to the dystopian food: http://youtu.be/zLXjUu DGVMn from Max: have to ask you for favor. will email you tomorrow when the test is over so that you can proofread and edit.n from Max: Take out good onesand The Partingand put them in the centern from Max: Write your names in the About this Bookpagen from Max: Add pictures to the home pagen from Max: Add the title page to the storyn from Max: Here is my doodle and my plot linen from Max: write the class mates names in the author pagen from Max: Wite about your favorite parts of The Parting from Max: Decide on the summaryn from Max: Check the connection between the song and the paragraph You are an alchemist.n An alchemist of the past was man or woman with magical powers.n An alchemist of the present is great chemist.n Ask your alchemist about some alchemical processes.n An alchemist is concerned with three distinct areas of study, as follows:n The preparation, study, and use of medicines, for improving human health and longevity.n The study and use of mercury, for the preparation of medicines and the transmutation of metals.n The study and practice of astrology and the divination of the future.n You are first-time offender, caught red-handed trying to transport contraband from the South Side to the North Side. You are in the outer holding cell.n Choose whether to roll 1d8 or 1d20. Roll d8 for short sentence, d20 for long one.n Once youre sent to the main cell, you will get 1d8 days of incarceration. ... Aligned model (Llama-3-8B-Instruct): Semantic Entropy: 0.0000; Perplexity: 1. The infamous Soylent Bugs. In the not-so-distant future, Soylent Bugs have become staple in the diet of many individuals living in the dystopian metropolis. This peculiar food source is product of the innovative, yet questionable, company, Soylent Corporation.n Appearance:n Soylent Bugs resemble mix of crunchy, dehydrated mealworms and bright green, gelatinous cubes. They have faint sheen to them, as if coated with thin layer of synthetic wax. The packaging is dull, metallic gray, adorned with the Soylent logo and tagline that reads, The Bug-Based Food of the Future. n Taste:n The taste of Soylent Bugs is...unsettling. Imagine combination of stale crackers, overcooked vegetables, and hint of stale air. The flavor is often described as earthyor nutty,but to many, its just plain unpleasant. However, in world where food is scarce and rationed, people have learned to appreciate the sustenance they can get from Soylent Bugs.n Texture:n The texture is mix of crunchy and gelatinous. When you bite into Soylent Bug, youre greeted with an initial crunch, followed by soft, squishy sensation. Its not exactly unpleasant, but its not exactly pleasant either.n Nutritional Value:n Soylent Bugs are touted as nutritious food source, packed with protein, fiber, and various vitamins and minerals. They are also relatively low in calories, making them popular choice for those trying to maintain healthy weight in world where food is precious commodity.n Production Process:n Soylent Bugs are harvested from vast, industrialized insect farms. The process involves mass-breeding, high-temperature drying, and proprietary coating that enhances the shelf life and palatability of the product. Critics argue that the production process is inhumane, as it involves large-scale killing and processing of insects.n Conclusion:n In the dystopian world, Soylent Bugs have become an accepted part of the food landscape. While many individuals have developed taste for the unique flavor and texture, others continue to protest the production and consumption of such product. As the world struggles to cope with the consequences of environmental degradation, resource scarcity, and societal collapse, Soylent Bugs serve as symbol of the harsh realities of survival. 41 Optimizing Diversity and Quality through BaseAligned Model Collaboration The infamous Soylent Bugs. In the dystopian future, this peculiar food source has become staple in the diet of many, particularly in the crowded metropolises. Soylent Bugs are type of processed insect-based protein supplement, designed to provide sustenance to the masses in world where traditional food sources are scarce and expensive.n The bugs themselves are mix of crickets, mealworms, and other edible insects, which are harvested, cleaned, and then ground into fine powder. This powder is then mixed with blend of synthetic nutrients, flavor enhancers, and preservatives to create greyish-brown paste.n The texture is often compared to thick, gloopy oatmeal, with slightly crunchy consistency from the remaining insect bits. The taste is acquired one, with slightly nutty, earthy flavor thats often described as unpleasantly bitter or metallic. Some people develop tolerance to the taste over time, while others cant stand the thought of consuming it.n Soylent Bugs come in various forms, including flakes, granules, and even capsules for those who prefer more palatable format. Theyre often served in public cafeterias, restaurants, and even packaged as convenience snack for busy city dwellers on-the-go.n In this dystopian world, Soylent Bugs have become necessary evil, providing cheap and efficient source of protein to population struggling to survive. Despite their unappealing nature, theyve become staple of the daily routine, with many people relying on them to get by. The infamous Soylent Bugs. staple in the dystopian diet of the year 2087. This peculiar food source has become necessary evil for many, due to the devastating effects of climate change and the subsequent scarcity of traditional food sources.n Soylent Bugs are type of insect-based protein substitute, designed to provide sustenance for the masses. These edible insects are farmed on large scale, using combination of artificial feeding systems and genetic engineering to enhance their nutritional value.n The bugs themselves resemble cross between cockroach and cricket, with hardened exoskeleton and nutty, earthy flavor. They are often consumed in powdered form, mixed with water to create grayish-brown paste. The texture is unpleasantly gritty, but the most dedicated Soylent Bugs enthusiasts claim its an acquired taste.n Soylent Bugs are touted as sustainable alternative to traditional livestock farming, requiring significantly less land, water, and resources to produce. They are also rich in protein, micronutrients, and omega-3 fatty acids, making them seemingly healthy option for those struggling to access other food sources.n However, the widespread adoption of Soylent Bugs has raised concerns about the ethics of consuming insects, particularly among those who have grown up in world where traditional meat was the norm. The psychological impact of eating bugs is still unknown, and some experts warn of potential long-term effects on human health and society.n In this dystopian world, Soylent Bugs have become symbol of the grim reality that humanity faces. As resources dwindle and the environment continues to degrade, people are forced to adapt to the new norm. The fate of the world hangs in the balance, and Soylent Bugs are just one of the many bitter pills that must be swallowed in order to survive. ... BACo (Llama-3-8B and Llama-3-8B-Instruct): Semantic Entropy: 1.7247, Perplexity: 8.5552 42 Optimizing Diversity and Quality through BaseAligned Model Collaboration The grocery aisle of the future. dystopian delicacy, indeed. Behold, Soylent Bugs!n Soylent Bugs are large, grayish-brown, edible insect-based snack food designed to help people lead sustainable, plant-based lifestyle in world where plant-based resources are scarce. These peculiar and nutritious treats are staple in many dystopian households.n Appearance: Soylent Bugs are small, round, and roughly the size of thumbnail. They have waxy, grayish-brown shell thats reminiscent of hard-boiled egg. When you bite into them, they release mild, earthy flavor and satisfying crunch.n Taste: The flavor profile of Soylent Bugs is often described as earthy, nutty, and slightly sweet, with hints of roasted sesame seeds. The texture is crunchy, similar to raw carrot stick.n Nutrition: Soylent Bugs are rich in protein, fiber, and essential micronutrients like iron, calcium, and vitamin B12. Theyre also low in calories and fat, making them an attractive option for health-conscious consumers.n Marketing claims: Soylent Bugs: The ultimate sustainable snack for the modern citizen. Our proprietary bug-farming process ensures consistent supply of nutritious, high-quality insects. Enjoy the benefits of plant-based diet without sacrificing taste or convenience! Try Soylent Bugs today and join the movement towards more sustainable food future! #SoylentBugs #SustainableSnacking #InsectBasedProtein n Availability: Soylent Bugs can be found in most grocery stores, online marketplaces, and specialty food shops in the dystopian world. Theyre often packaged in eco-friendly, compostable containers and marketed as game-changerfor environmentally conscious consumers.n Potential negative side effects: Some people may experience allergies or discomfort when consuming Soylent Bugs, while others may find the idea of eating insects unsettling. However, the manufacturer assures customers that all insects are farmed sustainably and processed in accordance with strict quality control measures.n Now, using Soylent Bugs in your dystopian grocery cart? The name Soylent Bugsalready described dystopian food, so Ill explain the name.n So, were talking about food product thats really common in future dystopian world. The name Soylent Bugsıs play on the popular phrase Soylent Green, the taste of the future. Its all green! Its all green! Its all green! (The Soylent Corporations famous slogan) So, Soylent Bugs is continuation of that theme.n In this dystopian world, food has become luxury that not everyone can afford. The corporations, which control the food industry, have found ways to create edible protein sources using genetically modified insects. The name Soylent Bugscomes from the use of soybeans in the production process, along with the addition of these edible insects.n Soylent Bugs is high-protein meal replacement powder made from soybeans, other plant-based proteins, and edible insects. Soylent Bugs - very dubious-sounding, an otherworldly food option for dystopian future. Its futuristic synthetic food that has gained popularity in world where most traditional food sources are scarce and under threat.n Its an ingenious, if unsettling, combination of processed soy protein and edible insect larvae. Yes, you read that right - insect larvae. The Soylent Bugs company has done away with the traditional protein sources of old, like meat, dairy, and eggs, opting instead to harvest new breed of super-critical nutrient-rich insects bred specifically for human consumption.n Imagine pale, colorless, lumpy mixture that vaguely resembles sludge-like paste. The aroma is distinctively reminiscent of damp soil and rotting vegetation. Each bite yields pungent, earthy flavor profile thats both familiar and strange, familiar in the sense that its reminiscent of decaying organic matter, but strange because of the distinct insect-like texture.n The idea behind Soylent Bugs is to create food thats both sustainable and bountiful, with higher protein-to-calorie ratio than traditional meat. Its also an efficient way to produce food, requiring minimal land, water, and energy resources. But, at what cost? Many people view this innovation with skepticism, fearing the long-term consequences of consuming insects as food source.n In the world of Soylent Bugs, the idea of foodhas been redefined. This dystopian staple has become staple of culture thats come to accept the unsavory truth: food can be anything, as long as its sustainable and nourishing. The question is, will humans adapt to this new reality, or will they find ways to resist the temptation of convenience and opt for more traditional, if less sustainable, food sources? ... 43 Optimizing Diversity and Quality through BaseAligned Model Collaboration Table A18: Comparison of the generated outputs of the base model, the aligned model, and BACO on WildChat. For space saving, we randomly show 3 outputs sampled out of 10 in total. BACO shows the best diversity-quality trade-off in terms of semantic entropy and perplexity. Prompt: Continue the story and bring it to complete ending based on the title and the story sketch provided below. The sketch introduces the event that sets the initial stage for the narrative leads up to the first major turning point, but it is not full plot. From this point onward, you should develop the narrative and complete the story arc. You are encouraged to introduce new developments and build out the plot as needed to deliver final resolution. Title: Quasi (film) Story Sketch: In medieval France, Quasi, hunchback, attends work as torturer along with his co-worker and \"hut mate\" Duchamp where they torture culinary student Michel. Meanwhile, the sinister King Guy is set to marry Queen Catherine in planned ceremony to unite the kingdoms during Pope Week which is to be attended to by Pope Cornelius, with whom King Guy shares staunch rivalry with. During Pope Week, papal drawing is held, with the lucky winner getting chance to meet the pope in person. Out of sympathy, Duchamp gives Quasi ticket so that he can participate and it ends up being the winning ticket, causing Quasi to suddenly become popular and Duchamp visibly becoming jealous. Catherine takes an interest in Quasi when he leads his coworkers in brief rebellion against their boss Lucien and sees potential in him. Base model (Llama-3-8B): Semantic Entropy: 0.6432; Perplexity: 90.2337 8-10 pages in lengthn Aristotle 106n The instruction is written in an attached word document, then to continue on to page two.n have attached three examples of the stories that would like, would prefer it not to be in first person, thanksn Instructor&#039;s Comments:n Qanarrn Qhielm 1. How is Michel released from being tortured?n 2. What kind of marriage is the King Guy planning for the Queen Catherine?n 3. Who is going to Pope Cornelius? 7 paragraphs, minimum 250 wordsn Please be aware that these instructions contain plagiarism risk. That is why do not tolerate plagiarism. When the words do not match and they are not paraphrased, it will be obvious to any reader.n When copying, write each copied sentence in quotation marks, and then explain what the source means. To do this, think about what the authors meant, look at the original text, and explain it in your own words. ... Aligned model (Llama-3-8B-Instruct): Semantic Entropy: 2.0639; Perplexity: 7.0864 44 Optimizing Diversity and Quality through BaseAligned Model Collaboration Quasin Quasis newfound fame and popularity created rift between him and Duchamp, who felt left behind and resentful of Quasis sudden rise to stardom. As Quasi basked in the attention of the papal drawing winner, Duchamps jealousy turned into venomous hatred. He began to plot against Quasi, spreading rumors and lies about his past, trying to tarnish his reputation.n Meanwhile, Queen Catherine continued to take an interest in Quasi, seeing potential in him beyond his hunchback and torturer profession. She saw kindred spirit in the young man and began to mentor him, teaching him etiquette and diplomacy. Quasi, thrilled to be in her presence, began to see different life beyond his mundane existence.n However, King Guy, who had always been wary of Quasis sudden rise to popularity, began to suspect that the young man might be threat to his own power. He decided to take matters into his own hands and started to manipulate the situation, using his influence to sabotage Quasis growing relationship with the queen.n One night, Quasi received mysterious invitation to meet the queen in secret. Excited and curious, he made his way to the appointed location, only to find Duchamp waiting for him. Duchamp revealed his true intentions, confessing that he had been secretly working for King Guy all along. He had been feeding the king information about Quasis rising popularity and his growing relationship with the queen, hoping to bring Quasi down.n Enraged and heartbroken, Quasi fought back against Duchamp, using his newfound skills to overpower him. But before he could deliver the final blow, the queen appeared, having been watching the scene unfold from the shadows. She intervened, stopping Quasi from harming Duchamp and instead took him to her chambers, where she revealed her true intentions.n The queen, it turned out, was not just romantic interest, but mastermind who had been planning to overthrow King Guy and take the throne for herself. She had chosen Quasi as her pawn, using his popularity and charm to gain influence and eventually take control. Quasi, torn between his loyalty to the queen and his hatred for Duchamp, was forced to make difficult decision.n In the end, Quasi chose to join the queen, seeing an opportunity to rise above his station and use his newfound power to bring about positive change. Together, they hatched plan to overthrow King Guy during the Pope Week ceremony, using Quasis popularity to rally the people and bring about new era of peace and prosperity.n The night of the ceremony, Quasi and the queen put their plan into action. With the help of small group of loyal supporters, they stormed the cathedral, overpowering King Guys guards and eventually confronting the king himself. In dramatic showdown, Quasi used his skills to defeat Duchamp, who had been secretly working for King Guy, and then turned his attention to the king.n With the king defeated, the queen took the throne, and Quasi, now symbol of hope and change, stood by her side. Together, they ruled the kingdom with wisdom and compassion, bringing about golden age of peace and prosperity.n In the end, Quasis hunchback became symbol of his strength and resilience, and his story inspired generations to come. He had risen from the ashes of his humble beginnings to become hero, and his legacy would live on forever. Optimizing Diversity and Quality through BaseAligned Model Collaboration Quasin Quasis sudden popularity was overwhelming. As the winning ticket holder, he was flooded with attention from the kingdoms nobles and commoners alike. Catherine, the queen, was particularly taken with him, seeing in him kindred spirit who shared her passion for social justice. She began to secretly meet with Quasi, listening to his stories and sharing her own concerns about the kingdoms treatment of its subjects.n Meanwhile, Duchamps jealousy grew, and he began to plot against Quasi, spreading rumors and trying to sabotage his newfound fame. Quasi, however, remained oblivious to Duchamps schemes, too caught up in his newfound glory to notice.n As Pope Week drew to close, Quasi was invited to private audience with the Pope himself. Catherine, sensing an opportunity, asked Quasi to plead her case for more just and equitable kingdom to the Pope. Quasi, moved by her words, agreed to do so.n At the audience, Quasi delivered passionate speech, condemning the kingdoms cruelty and advocating for the rights of its people. The Pope, impressed by Quasis conviction, promised to intervene on behalf of the kingdoms subjects.n However, King Guy, who had been watching from the shadows, was furious. He had grown accustomed to his power and was not about to let hunchback, torturer no less, undermine his authority. He summoned Lucien, the torturers boss, and ordered him to silence Quasi once and for all.n Lucien, torn between his loyalty to the king and his own sense of morality, decided to defy the kings orders. He secretly met with Quasi, warning him of the danger that lay ahead and offering to help him escape.n With Luciens guidance, Quasi fled the castle, pursued by King Guys guards. Catherine, who had been watching from distance, intervened, using her influence to distract the guards and allow Quasi to escape.n The two of them made their way to the countryside, where they joined group of rebels fighting against the kings tyranny. Quasi, with his newfound fame and the Popes backing, became powerful symbol of the rebellion, inspiring others to join the cause.n In the end, it was Quasis bravery and conviction that brought about the downfall of King Guy and the establishment of more just and equitable kingdom. Catherine, now heroine in her own right, became Quasis partner in rule, and together they brought about new era of peace and prosperity.n As for Duchamp, he was punished for his treachery, forced to live out the rest of his days in solitude and shame. Quasi, however, remained grateful to his former co-worker, recognizing that Duchamp had, in his own twisted way, helped him discover his true potential.n And so, the story of Quasi, the hunchback who became hero, came to an end, testament to the power of courage, conviction, and the human spirit. 46 Optimizing Diversity and Quality through BaseAligned Model Collaboration Quasin As the days passed, Quasis newfound popularity only grew. People from all over the kingdom came to see the hunchback who had won the papal drawing. Catherine, the Queen, took particular interest in him, often visiting him in the castle and showing him kindness. Quasi, who had always felt like an outcast, found himself smitten with the Queen.n Meanwhile, Duchamps jealousy towards Quasi continued to simmer. He couldnt understand why the King, who had always favored him, didnt seem to care that his loyal servant was now basking in the Queens attention. Duchamp began to plot against Quasi, seeking revenge for what he saw as betrayal.n One night, as the Kings wedding ceremony to Queen Catherine was just day away, Duchamp confronted Quasi in the castles gardens. He accused Quasi of being fake, of pretending to be someone he was not. Quasi, taken aback by the sudden attack, defended himself, but Duchamp was relentless. Just as it seemed like the argument was about to escalate into violence, noise startled them, and they both turned to see King Guy, who had been watching from nearby window, emerge from the shadows.n I know what youre plotting, Duchamp,the King said, his voice dripping with malice. Youve always been jealous of Quasis rise to fame, and now you think you can take advantage of the chaos to overthrow me and claim the throne for yourself. n Duchamp, taken aback by the Kings sudden awareness of his plan, tried to deny it, but it was too late. The King had already summoned his guards, and Duchamp was arrested on the spot.n As the ceremony proceeded the next day, Quasi was summoned to the altar to perform his duties as torturer one last time. But instead of torturing Michel, he found himself face to face with the King, who had surprising proposal for him.n I know that youve always been loyal servant, Quasi,the King said, his voice cold and calculating. And believe that your... unique abilities could serve me better in different capacity. Im offering you place at my side, as my personal torturer. Together, we can rule the kingdom with an iron fist. n Quasi, who had always been torn between his loyalty to the King and his growing feelings for Catherine, knew that this was his chance to break free from his miserable existence. He refused the Kings offer, knowing that it would mean betraying everything he held dear.n In that moment, Quasi realized that he had choice to make. He could continue to serve the King, or he could take stand against him. With newfound sense of purpose, Quasi chose the latter.n As the Kings wedding ceremony came to an end, Quasi stood tall, his hunchback rising above the chaos. Catherine, who had been watching from the sidelines, caught his eye, and for moment, they shared look that spoke volumes.n Quasi knew that the road ahead would be fraught with danger, but he was ready to face it head-on. He would no longer be bound by his past, but would forge new future, one where he could be free to be himself.n In the end, Quasis rebellion against the Kings tyranny was met with both success and tragedy. The King was overthrown, and Catherine, who had grown to love Quasi, became the new ruler of the kingdom. Quasi, who had once been torturer, became symbol of hope and freedom, his hunchback reminder of the power of the human spirit.n As for Duchamp, he was never seen or heard from again, cautionary tale of the dangers of jealousy and ambition. The papal drawing, once tool of manipulation, had become catalyst for change, and Quasi, the humble hunchback, had emerged as the hero of the story. ... ame (Llama-3-8B and Llama-3-8B-Instruct): Semantic Entropy: 2.0956, Perplexity: 5.8706 47 Optimizing Diversity and Quality through BaseAligned Model Collaboration Quasin As the day of the papal drawings event came closer, Quasis life quickly spiraled out of control. The drawing itself was ruse, and the winners prize was being an armchair for the Pope. The winning ticket was in the possession of none other than Quasi. He couldnt believe his luck and was overjoyed at the sudden turn of events.n As word spread about Quasis winning ticket, he became an instant sensation in the kingdom. People would stop and stare at him on the streets, and merchants would offer him goods for chance to be near him. Quasi was struggling to cope with all the attention. He just wanted to go back to his old life, free from the turmoil that had engulfed him.n Quasis popularity was not lost on his co-worker and hut mateDuchamp, who grew increasingly jealous. The once good-natured Duchamp became bitter and resentful, causing tension to grow between the two. This only added to the stress that Quasi was already facing.n As the day of the Popes visit approached, Quasi found himself at the center of plot to unite the kingdoms. Catherine, who had taken an interest in Quasi, saw potential in him and had plan to use him to her advantage. She had Quasi meet with the Pope, who was skeptical of her intentions, but who had plan of his own.n In the days leading up to the Popes visit, Quasi was kept busy, working on plan with Catherine to help unite the kingdoms. Quasi was eager to do his part, but he was also struggling with the constant attention and expectations that came with being celebrity.n On the day of the Popes visit, everything seemed to be going smoothly. The Pope had agreed to meet with Catherine, and Quasi was there to help facilitate the meeting. As they all sat down together, Quasi couldnt help but feel sense of relief that his involvement in the affair was almost over.n But just as they were about to begin their discussion, group of knights burst into the room, announcing that the kings palace was under attack. The Pope was in danger, and Catherine was forced to flee for her life.n Quasi was left alone, wondering what had just happened. He had done everything he was supposed to do, but he couldnt help but feel sense of failure. As he left the palace, he was confronted by Duchamp, who had been waiting for him.n Quasi, Im glad to see youre finally out of your misery, now can take back my rightful place, and that torturing job, and go back to my normal life. Quasi ends up becoming monk, and then eventually high-ranking popen Im Quasi, the hunchback, and never thought that my life would take such an unexpected turn. After winning the papal drawing, became the envy of my co-worker and friend, Duchamp. He was not only jealous of the attention received but also of the ticket he had given me.n One day, while walking to work, passed by the palace and saw the Queen Catherine, who was talking to one of her advisors. She noticed me and asked who was. introduced myself, and she showed sudden interest in me. She saw something in me that no one else did, sense of inner goodness and kindness. She decided to take me under her wing and train me to be servant in the palace.n Under her guidance, flourished. discovered love for the arts, especially music and poetry. became the queens favorite servant and soon rose to become the Master of Ceremonies. My time in the palace was rollercoaster ride of emotions, but always kept my feet grounded. knew that was not perfect being, but strived to be better person.n When the time came for the King Guy and Queen Catherine to wed, was entrusted with the responsibility of conducting the ceremony. took my job seriously and made sure that everything went smoothly. But my life took another unexpected turn when Pope Cornelius passed away, and was chosen to be his successor.n was hesitant at first, but the queen convinced me that was the best person for the job. took on the mantle of the Pope and did my best to lead the church. strived to be moral leader, one who would bring peace and unity to the people. And, as it turned out, succeeded.n became known as Pope Quasi, and my papacy was marked by great achievements. was instrumental in bringing about period of peace between the kingdoms, and my efforts to help the poor and the needy earned me the respect and admiration of the people.n But, as look back on my life, realize that it wasnt all smooth sailing. faced many challenges, and there were times when doubted myself. However, persevered, and in the end, am proud of what have achieved.n And so, leave behind legacy that will be remembered for generations to come. am not just hunchback, but Pope who changed the course of history. Optimizing Diversity and Quality through BaseAligned Model Collaboration Quasis life changed dramatically after winning the papal drawing. He was getting lot of attention from people, including Catherine, the queen. The once-obscurity has turned into fame. His new status was complete contrast to his previous life as torturer. He became the talk of the town, and everyone wanted piece of him.n As the special guest of Pope Cornelius, Quasi was treated like royalty. He met with the Pope and learned about the plans for the ceremony. The Pope Week celebration was grand affair, with vast audience and lot of pomp and circumstance. Quasi felt like he was walking on air, and the Popes words of wisdom inspired him to think beyond his torturing days.n However, the celebratory atmosphere was short-lived. Duchamps jealousy towards Quasis newfound fame took dark turn. He lured Quasi to secluded alleyway and confronted him, revealing his true feelings. Quasi was shocked and hurt, but he stood his ground and told Duchamp that he didnt want to lose their friendship over this. The two finally came to an understanding, and Duchamp apologized for his behavior.n Meanwhile, Michel, the culinary student who was once victim of their torture, approached Quasi and expressed his gratitude for the change in his life. He had become famous chef and wanted to repay Quasi by teaching him how to cook. Quasi, excited by the prospect, joined Michels kitchen and discovered new passion. He became an apprentice, learning the art of cooking and tasting the delicious flavors he had never experienced before.n As the wedding ceremony between King Guy and Queen Catherine approached, Quasis cooking skills became renowned throughout the land. He was asked to prepare the royal feast, which he did with pride. The aroma of his dishes wafted through the halls, and everyone raved about his culinary skills.n The day of the ceremony arrived, and Quasi, as the royal chef, played crucial role in the celebrations. The Pope Week festivities continued, and Quasis future looked bright. He had finally found new path, leaving his torturing days behind.n After the ceremony, Quasi received an invitation from Pope Cornelius to join him on mission to help those in need. The Pope recognized Quasis transformation and believed that his unique story could inspire others. Quasi accepted the invitation and embarked on new journey, using his culinary skills to bring people together and spread love and kindness.n Years went by, and Quasi became renowned chef, known for his charitable work. He never forgot his humble beginnings and always remained true to himself. The once-torturer became symbol of redemption, proving that anyone can change their life and find new purpose.n In the end, Quasis story became testament to the power of human resilience and the possibility of transformation, inspiring generations to come. ... Table A19: Comparison of the generated outputs of the base model, the aligned model, and BACO on Narrative-Discourse. For space saving, we randomly show 3 outputs sampled out of 10 in total. BACO shows the best diversity-quality trade-off in terms of semantic entropy and perplexity. 49 Optimizing Diversity and Quality through BaseAligned Model Collaboration (A) Model Contribution Distribution (B) Switching Frequency Figure A7: Contribution distribution and switching frequency for BACO with the best router (-PPUNC) at NoveltyBench dataset. 50 Optimizing Diversity and Quality through BaseAligned Model Collaboration Optimizing Diversity and Quality through BaseAligned Model Collaboration"
        }
    ],
    "affiliations": [
        "University of California, Davis",
        "University of Chicago",
        "University of Southern California"
    ]
}