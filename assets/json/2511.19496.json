{
    "paper_title": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM",
    "authors": [
        "Yang Liu",
        "Xiaolong Zhong",
        "Ling Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present \\textbf{Xmodel-2.5}, a 1.3-billion-parameter small language model designed as a \\emph{drop-in agent core}. Training with maximal-update parameterization ($μ$P) allows hyper-parameters tuned on a 20M-parameter proxy to transfer directly to the full model, even under the parameter-tied \\emph{tie-word-embedding} architecture. A 1.4T-token Warmup--Stable--Decay curriculum is used, and we further show that \\textbf{switching from AdamW to Muon during the decay phase} improves the 13-task reasoning average by 4.58\\,\\% while keeping every other hyper-parameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8-mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.\\footnote{https://huggingface.co/XiaoduoAILab/Xmodel-2.5 and https://huggingface.co/XiaoduoAILab/Xmodel-2.5-history (training checkpoints).} Training code and evaluation harness: https://github.com/XiaoduoAILab/Xmodel-2.5."
        },
        {
            "title": "Start",
            "content": "Xmodel-2.5: 1.3B Data-Efficient Reasoning SLM"
        },
        {
            "title": "Yang Liu Xiaolong Zhong Ling Jiang",
            "content": "Xiaoduo AI Lab foamliu@yeah.net 5 2 0 2 3 2 ] . [ 1 6 9 4 9 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models deliver strong reasoning and tool-use skills, yet their computational demands make them impractical for edge or cost-sensitive deployments. We present Xmodel-2.5, 1.3-billion-parameter small language model designed as drop-in agent core. Training with maximal-update parameterization (µP) allows hyper-parameters tuned on 20M-parameter proxy to transfer directly to the full model, even under the parametertied tie-word-embedding architecture. 1.4Ttoken WarmupStableDecay curriculum is used, and we further show that switching from AdamW to Muon during the decay phase improves the 13-task reasoning average by 4.58 % while keeping every other hyperparameter fixed, verifying that early AdamW stability can be paired with late Muon sharpening for better downstream performance. FP8mixed-precision training balances accuracy and throughput. All checkpoints, recipes, and evaluation code are released under the Apache-2.0 license.1 Training code and evaluation harness: https://github.com/XiaoduoAILab/ Xmodel-2.5."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable reasoning, planning, and tool-use capabilities, yet their deployment as autonomous agents remains prohibitive for resource-constrained environments. State-of-the-art agent backbones typically exceed 713 parameters, demanding highend accelerators and large memory footprints incompatible with edge or cost-sensitive scenarios. Recent small language models (SLMs, < 2 B) match LLMs on single-turn benchmarks such as GSM8K or MMLU, but still fall short in complex multi-step reasoningthe core skill required for 1https://huggingface.co/XiaoduoAILab/Xmodel-2. https://huggingface.co/XiaoduoAILab/ 5 Xmodel-2.5-history (training checkpoints). and tool invocation, long-horizon planning, and robust error recovery. Boosting this capability within the 12 regime is the central goal of our work. Xmodel-2.5 We present Xmodel-2.5, 1.3 Bparameter decoder-only model that retains Xmodel2s two-stage pre-training recipe and maximalupdate parameterization (µP), while introducing four targeted upgrades: 1. We extended Megatron-LM with complete µP support, modifying its parameterization, attention scaling, and residual connections. The implementation was validated to preserve µP dynamics, enabling reliable hyperparameter transfer. 2. Tokenizer. Adopted the 129 k-token DeepSeek-v3 tokenizer (vs. Xmodel-2s 65 ktoken Unigram), improving compression and decoding speed. 3. Numeric precision. Switched from BF16 to FP8-mixed precision, raising training throughput by 30% with no observable degradation in pilot experiments. 4. Optimizer schedule. Switched from AdamW to Muon during the decay phase, improving the 13-task reasoning average by 4.58% while keeping all other hyper-parameters fixed. We hope Xmodel-2.5 serves as minimal yet strong baseline for lightweight agents with enhanced complex-reasoning capabilities."
        },
        {
            "title": "2 Background & Related Work",
            "content": "2.1 Small Language Models for Reasoning Parameter-efficient SLMs (< 2 B) have recently closed the gap with larger counterparts on mathematical and commonsense reasoning. MiniCPM (Team et al., 2025b) and DCLM-1B (Li et al., 2025) adopt code-enriched corpora and cosine or WSD schedules to surpass 35% on GSM8K. Abdin et al. (2024) further emphasises textbookstyle synthetic data. These works, however, primarily target single-turn question answering; systematic evaluation of multi-step agentic behaviours remains under-explored. 2.2 Hyper-Parameter Transfer with Maximal-Update Parameterisation µP (Yang et al., 2022, 2023) preserves training dynamics across widths, enabling hyper-parameter transfer from toy to full-scale models. Original derivations assume SGD; recent work integrates Adam (Kingma and Ba, 2017) but reports instability below 2 parameters. Hyper-parameter Hidden size Intermediate size Num of transformer layers Attention heads (Q) KV heads (GQA) Sequence length Max position embeddings RoPE base Value 1536 3840 48 24 8 3712 131072 500000 Table 1: Model configuration. Long-context support via 131 position embeddings and RoPE base 500 K. 1. Warmup (W): 2 steps, LR linearly rises to 1.67103 (hidden) and 0.01 (embeddings). 2. Stable (S1): 270 steps, batch size 3 7124801.78 tokens. 2.3 Efficient Training of Lightweight Models 3. Stable (S2): 260 steps, batch size FP8 mixed-precision (Micikevicius et al., 2022) and fused attention kernels reduce memory and energy, yet have not been jointly studied with µP optimisers. WSD learning-rate schedules (Hu et al., 2024) improve late-stage performance by decoupling the annealing phase from token count; we extend WSD with domain-weighted data mixing during decay, an ablation absent in prior literature."
        },
        {
            "title": "3 Methodology",
            "content": "We scale Xmodel-2 to 1.3 parameters while retaining its deployment-friendly, deep-and-thin decoder-only skeleton. The section below details three design pillars: (i) architecture-level µP compatibility (3.1), (ii) three-phase WarmupStable Decay (WSD) curriculum (3.2), and (iii) FP8 mixed-precision acceleration (3.3). 3.1 Model Architecture Xmodel-2.5 keeps the deep-and-thin decoder-only design of Xmodel-2, with the configuration in Table 1. To preserve maximal-update dynamics across widths we apply the µP attention-logits scaling 1/dhead; all other components (Pre-RMSNorm, SwiGLU) are inherited without modification. While Xmodel-2 was trained with Flash-Attention v2, Xmodel-2.5 uses the CuDNN backend provided by Megatron-LM (no use-flash-attn specified). 3.2 Three-Phase WSD Pre-Training The 560 k-step WarmupStableDecay (WSD) schedule consumes 1.4 tokens: 3 7129603.56 tokens. 4. Decay (D): 20 steps (exponential decay), batch size 3.56 tokens, 66.9 % highquality SFT data blended. 5. Long-Context Reasoning (LCR): 10 additional steps, same 66.9 % SFT ratio, batch size 16 3842403.93 tokens, context length 16 k. Optimizer ablation: AdamW Muon. During the final 20 decay steps we perform controlled optimizer switch: AdamW is replaced by Muon (Jordan et al., 2024) while all other hyperparameters remain frozen, including the learningrate schedule, batch size, gradient-clip, weightdecay, and the 66.9 % SFT data blend. Under this single-variable change, the 13-task reasoning average rises by 4.58 pp absolute over an AdamW-only baseline, confirming that the gain is attributable to Muon and not to confounding factors such as data re-weighting or schedule re-tuning. This result supports the view that early-phase stability (AdamW) can be complementarily combined with late-phase sharpness (Muon) to improve downstream performance without extra data or re-training. 3.3 FP8 Hybrid Format & Kernel Implementation We adopt the FP8 hybrid format implemented in NVIDIA Transformer Engine (TE) (NVIDIA Corporation, 2023): Forward: E4M3 (1-4-3 exponent-mantissa) for activations; Figure 1: Data composition in the (a) Stable and (b) Decay phases of WSD LR scheduling. The Stable phase emphasizes broad pre-training data diversity, while the Decay phase focuses on high-quality instructional and SFT data to refine model capabilities. Backward: E5M2 (1-5-2) for gradients; Master-weights: kept in bfloat16 to avoid under-flow. Gemma-3_1B (Team et al., 2025a) Gemini-2.0-based SLM with alternating localglobal attention for 32 long context.; are delayed-scaling hyperTE amax-history-len=128, parameters: amax-compute-algo=max. kernels through Megatron-LMs transformer-impl transformer_engine flag; the corresponding GEMM, Layer-Norm and GeLU CUDA kernels are automatically selected without source-code modification. All FP8 invoked"
        },
        {
            "title": "4 Experimental Settings",
            "content": "4.1 Baselines SmolLM2-1.7B (Allal et al., 2025) highquality corpus focused common-sense model. Xmode2-1.2B (Qun et al., 2024) compact SLM trained with WSD scheduling and mathenriched corpora for strong reasoning. All baseline results are reproduced with our unified evaluation pipeline using the Language Model Evaluation Harness (Gao et al., 2021) to ensure fair comparison. We compare Xmodel-2.5 with eight publicly released decoder-only models in the 12 range: 4.2 Tasks and Metrics Qwen3-1.7B (Yang et al., 2025) updated Qwen series with enhanced code and math data; MiniCPM-1B (Hu et al., 2024) SLM trained with WSD and domain-weighted data; InternLM2.5-1.8B (Cai et al., 2024) upgraded InternLM with improved reasoning; Llama-3.2-1B (Grattafiori et al., 2024) Metas lightweight Llama-3 variant; We evaluate on 13 datasets covering commonsense, symbolic and multilingual reasoning. Commonsense ARC-Challenge (Clark et al., 2018) (25-shot), ARC-Easy (Clark et al., 2018) (25shot), PIQA (Bisk et al., 2019) (0-shot), HellaSwag (Zellers et al., 2019) (10-shot), WinoGrande (Sakaguchi et al., 2021) (5-shot). Symbolic Reasoning BBH (Suzgun et al., 2022) (3-shot), MMLU (Hendrycks et al., 2021a) (5shot). Dataset (shots) Qwen3 MiniCPM InternLM2.5 Llama-3.2 Gemma-3 SmolLM2 Xmodel-2 Xmodel-2.5 ARC-Challenge (25) ARC-Easy (25) PIQA (0) HellaSwag (10) WinoGrande (5) BBH (3) MMLU (5) GSM8k (5) MATH (4) HumanEval (0) MBPP (3) CMMLU (5) C-Eval (5) Average 53.07 79.67 72.58 60.16 60.54 45.23 60.24 69.29 35.50 42.68 42.60 60.47 60.40 56.96 45.31 75.21 75.19 67.90 64.48 35.45 48.75 42.00 12.06 43.90 33.40 46.52 46.21 48.95 40.96 71.68 73.39 59.14 61.88 41.12 48.87 41.77 21.20 35.98 32.80 62.03 61.59 50. 41.47 71.80 74.16 59.76 61.25 38.10 45.44 34.80 16.84 31.10 32.60 36.99 37.00 44.72 40.36 69.65 72.42 55.96 58.48 38.34 39.87 25.70 24.06 32.93 25.00 34.11 33.51 42.34 53.50 80.81 77.58 73.16 67.88 35.05 49.99 32.37 10.96 25.61 34.40 33.60 34.55 46. 46.16 76.22 75.14 64.05 64.25 48.90 49.98 56.56 25.64 29.27 30.80 44.29 43.16 50.34 48.89 76.94 75.95 67.24 64.64 54.58 51.81 58.98 28.94 28.66 33.00 47.16 45.54 52.49 Table 2: Comprehensive results (%) on 13 benchmarks. Bold marks best in 12 range. Xmodel-2.5 1.3B uses flexible-extract for GSM8k and math_verify for MATH. Mathematical & Code GSM8k (Cobbe et al., 2021) exact-match with flexible-extract (5-shot), MATH (Hendrycks et al., 2021b) (4-shot) verified by math_verify, HumanEval (Chen et al., 2021)(0shot), MBPP (Austin et al., 2021)(3-shot). Chinese Understanding C-Eval (Huang et al., 2023) (5-shot), CMMLU (Li et al., 2024) (5-shot). Metrics: accuracy for all sets; pass@1 for HumanEval/MBPP; exact-match for GSM8k/MATH. 550k steps, we first expand the context length from 3,712 to 8,192 within 3k steps, and then further to 16,384 within the next 7k steps. As expected, the longer-context exposure produces small bump on WikiText-2 perplexity; nevertheless, the 13-task average in Table 2 rises from 52.36 to 52.49, confirming that the long-context phase converts the modest perplexity increase into tangible downstream reasoning gains."
        },
        {
            "title": "5 Results",
            "content": "5.1 Main Reasoning Results Table 2 summarizes zero-shot and few-shot performance on 13 commonsense and mathematical reasoning benchmarks. Despite being 25% smaller (1.3 vs. 1.7 parameters) and trained with 96 % fewer tokens (1.4 vs. 36 T), Xmodel-2.5 closes 71% of the gap to Qwen3raising the 12 average from 50.34% (Xmodel-2) to 52.49%, +2.15 pp. This result is only 4.47 pp behind Qwen3 (56.96%), confirming that the WSD schedule extracts superior reasoning efficiency per parameter and per token. 5.2 Training Loss Figure 2 presents the training loss curve on the WikiText-2 dataset (Merity et al., 2016). The initial drop corresponds to increasing the batch size from 2M to 4M tokens, which likely replicates the stabilizing effect of reduced learning rate (Smith et al., 2018). The second drop reflects the impact of the learning rate decay phase. Immediately after decay, we conduct lightweight long-context adaptation (shaded region in Figure 2): starting from"
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Xmodel-2.5, 1.3-billion-parameter small language model that achieves the secondbest average score among 12 models on thirteen widely used reasoning benchmarks, trailing only Qwen3 (52.49 % vs. 56.96 %). Critically, this result is obtained with only 1.4 training tokens 25.7 fewer than Qwen3demonstrating superior data efficiency. By extending the WarmupStable Decay schedule with 10 k-step long-context adaptation phase, we deliver reliable 16 k-context reasoning without extra data or hyper-parameter tuning. Our recipe is simple, compute-friendly, and reproducible, showing that careful pacing and lightweight context stretching can extract more reasoning power from every parameter and every token. We hope these findings encourage the community to further explore data-efficient pathways toward capable small models."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Figure 2: Loss curve for Xmodel-2.5 1.3B. Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, and 110 others. 2024. Phi-3 technical report: highly capable language model locally on your phone. Preprint, arXiv:2404.14219. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Guilherme Penedo, Lewis Tunstall, Andrés Marafioti, Hynek Kydlíˇcek, Agustín Piqueres Lajarín, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Clémentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, and 3 others. 2025. Smollm2: When smol goes big datacentric training of small language model. Preprint, arXiv:2502.02737. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. Preprint, arXiv:2108.07732. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. Preprint, arXiv:2107.03374. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. Preprint, arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. framework for few-shot language model evaluation. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, Xiaoyi Dong, Haodong Duan, Qi Fan, Zhaoye Fei, Yang Gao, Jiaye Ge, Chenya Gu, Yuzhe Gu, Tao Gui, and 81 others. 2024. Internlm2 technical report. Preprint, arXiv:2403.17297. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The llama 3 herd of models. Preprint, arXiv:2407.21783. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR). Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021b. Measuring mathematical problem solving with the math dataset. Preprint, arXiv:2103.03874. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai, Zhongwu Zhai, and 6 others. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies. Preprint, arXiv:2404.06395. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Preprint, arXiv:2305.08322. Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cesista, Laker Newhouse, and Jeremy Bernstein. 2024. Muon: An optimizer for hidden layers in neural networks. Diederik P. Kingma and Jimmy Ba. 2017. Adam: method for stochastic optimization. Preprint, arXiv:1412.6980. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2024. Cmmlu: Measuring massive multitask language understanding in chinese. Preprint, arXiv:2306.09212. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, and 40 others. 2025. Datacomplm: In search of the next generation of training sets for language models. Preprint, arXiv:2406.11794. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. Preprint, arXiv:1609.07843. Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. 2022. Fp8 formats for deep learning. Preprint, arXiv:2209.05433. NVIDIA Corporation. 2023. Transformer Engine: FP8 acceleration library for Transformer models. Version 2.6.0.post1, open-source software. Wang Qun, Liu Yang, Lin Qingquan, Qu Zhijiu, and Jiang Ling. 2024. Xmodel-2 technical report. Preprint, arXiv:2412.19638. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V. Le. 2018. Dont decay the learning rate, increase the batch size. Preprint, arXiv:1711.00489. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huai hsin Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, and 197 others. 2025a. Gemma 3 technical report. Preprint, arXiv:2503.19786. MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, and 64 others. 2025b. Minicpm4: Ultra-efficient llms on end devices. Preprint, arXiv:2506.07900. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025. Qwen3 technical report. Preprint, arXiv:2505.09388. Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. 2022. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. Preprint, arXiv:2203.03466. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. 2023. Tensor programs vi: Feature learning in infinite-depth neural networks. Preprint, arXiv:2310.02244. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? Preprint, arXiv:1905.07830."
        }
    ],
    "affiliations": [
        "Xiaoduo AI Lab"
    ]
}