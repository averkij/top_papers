{
    "paper_title": "Unifying Segment Anything in Microscopy with Multimodal Large Language Model",
    "authors": [
        "Manyu Li",
        "Ruian He",
        "Zixian Zhang",
        "Weimin Tan",
        "Bo Yan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate segmentation of regions of interest in biomedical images holds substantial value in image analysis. Although several foundation models for biomedical segmentation have currently achieved excellent performance on certain datasets, they typically demonstrate sub-optimal performance on unseen domain data. We owe the deficiency to lack of vision-language knowledge before segmentation. Multimodal Large Language Models (MLLMs) bring outstanding understanding and reasoning capabilities to multimodal tasks, which inspires us to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling vision models to demonstrate superior generalization capabilities on cross-domain datasets. In this paper, we propose using MLLMs to guide SAM in learning microscopy crose-domain data, unifying Segment Anything in Microscopy, named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment (VLSA) module, which injects VLK into Segment Anything Model (SAM). We find that after SAM receives global VLK prompts, its performance improves significantly, but there are deficiencies in boundary contour perception. Therefore, we further propose Semantic Boundary Regularization (SBR) to prompt SAM. Our method achieves performance improvements of 7.71% in Dice and 12.10% in SA across 9 in-domain microscopy datasets, achieving state-of-the-art performance. Our method also demonstrates improvements of 6.79% in Dice and 10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization capabilities. Code is available at https://github.com/ieellee/uLLSAM."
        },
        {
            "title": "Start",
            "content": "Manyu Li1,, Ruian He1,, Zixian Zhang1, Weimin Tan1,, Bo Yan1, 1Fudan University"
        },
        {
            "title": "Abstract",
            "content": "Accurate segmentation of regions of interest in biomedical images holds substantial value in image analysis. Although several foundation models for biomedical segmentation have currently achieved excellent performance on certain datasets, they typically demonstrate sub-optimal performance on unseen domain data. We owe the deficiency to lack of vision-language knowledge before segmentation. Multimodal Large Language Models (MLLMs) bring outstanding understanding and reasoning capabilities to multimodal tasks, which inspires us to leverage MLLMs to inject Vision-Language Knowledge (VLK), thereby enabling vision models to demonstrate superior generalization capabilities on cross-domain datasets. In this paper, we propose using MLLMs to guide SAM in learning microscopy crose-domain data, unifying Segment Anything in Microscopy, named uLLSAM. Specifically, we propose the Vision-Language Semantic Alignment (VLSA) module, which injects VLK into Segment Anything Model (SAM). We find that after SAM receives global VLK prompts, its performance improves significantly, but there are deficiencies in boundary contour perception. Therefore, we further propose Semantic Boundary Regularization (SBR) to prompt SAM. Our method achieves performance improvements of 7.71% in Dice and 12.10% in SA across 9 in-domain microscopy datasets, achieving state-of-the-art performance. Our method also demonstrates improvements of 6.79% in Dice and 10.08% in SA across 10 out-ofdomain datasets, exhibiting strong generalization capabilities. Code is available at https://github.com/ieellee/uLLSAM."
        },
        {
            "title": "Introduction",
            "content": "The convergence of advanced human imaging techniques and computational technologies has dramatically accelerated the acquisition of microscopic imagery across diverse imaging conditions, application domains, and modalities. This unprecedented rate of data generation has created significant bottleneck in the scientific workflow, as the limited number of domain experts available cannot analyze these vast datasets at pace commensurate with their production[1]. Consequently, there exists an urgent need among specialists for sophisticated tools that facilitate high-quality annotation of newly generated data while simultaneously enabling comprehensive description of structural features, intricate details, and underlying mechanisms. Such tools must be designed to align seamlessly with the specific requirements of domain experts, enabling them to extract meaningful insights efficiently and maintain scientific productivity in the face of ever-expanding data repositories[2, 3]. The development of these annotation solutions represents critical challenge at the intersection of computer vision, nature language processing, and specialized scientific domains. These authors contributed equally to this work. Corresponding Authors. Preprint. Under review. Figure 1: Motivation of uLLSAM. (a) µSAM. Shows three µSAM models, specifically specialists (b) for electron microscopy, light microscopy, and combined EM and LM specialist model. Ours. MLLMs provide Vision-Language Knowledge to SAM, helping it understand microscopy cross-domain knowledge, and generate image-level analysis alongside segmentation results. (c) Cross-domain generalization. Displays data from four biomedical domains; µSAMs EM and LM specialist models can effectively process EM and LM data respectively, but µSAMs combined EM+LM specialist model struggles with both EM and LM data. Additionally, µSAM cannot generalize well to histopathology and medical domains. Our method, trained on EM+LM domains, demonstrates strong generalization performance across all four biomedical domains. (d) SOTA Performance. Our method, compared to µSAM, shows significant improvements in Segmentation Accuracy (SA) metrics across 2 EM and 7 LM datasets, achieving state-of-the-art performance. To accelerate research for domain scientists in microscopy, numerous foundation models for downstream tasks have been developed, including image restoration[4] and cellular tissue segmentation[5, 6, 7, 8, 9, 10]. Among these, µSAM[5] has been specifically developed on the foundation of the SAM[11], offering two separate model weights tailored for light microscopy (LM) and electron microscopy (EM). These specialized weights enable interactive segmentation, interactive tracking, and fully automated segmentation capabilities. However, these microscopy foundation models exclusively focus on specific domain, which encounters substantial generalization challenges when deployed across heterogeneous domain data, primarily due to their insufficient integration of vision-language knowledge. Most critically, these models are constructed purely on visual architectures, severely lacking semantic perception capabilities when processing data from different domains, key limitation in the understanding of biological structures. With the advent of Multimodal Large Language Models (MLLMs) like LLaVA[12] for natural images, numerous works have emerged applying multimodal architectures to downstream visual tasks including referring detection[13], reasoning segmentation[14, 15, 16, 17], visual question answering[18], and visual reasoning[19]. These MLLMs leverage powerful implicit semantic modeling capabilities that mutually enhance feature representation across visual and linguistic components, enabling deeper understanding of image information and different domains. The recent growth of microscopy-centric visual-language datasets[20, 21, 22], particularly BIOMEDICA[22] which collected 24 million highquality image-text pairs from scientific literature across 12 categories including Microscopy, presents tremendous potential for MLLMs development in the microscopy domain. As shown in Figure 1 (a), we present uLLSAM, the first framework to explore the integration of MLLMs and SAM in the microscopy domain, aiming to leverage the powerful understanding and reasoning capabilities of MLLMs to inject vision-language knowledge into SAM, thereby enabling SAM to effectively learn cross-domain vision-language knowledge. Specifically, our contributions include: Unified Multimodal Processing for Microscopy Data. We propose uLLSAM, which leverages MLLMs to guide SAM in learning cross-domain vision-language knowledge, achieving improved segmentation performance across different microscopy domains. This approach enables unified framework for processing both light microscopy (LM) and electron microscopy (EM) data, with significant performance improvements in Figure 1 (d), achieving state-of-the-art results. 2 Figure 2: Overall architecture of uLLSAM. uLLSAM receives image and text interactions as input, encoding them as vision tokens and text tokens. The output from the Large Language Model (LLM) injects vision-language knowledge into SAM through the VLSA module. SBR provides boundary constraints through one positive point and three negative points around the boundary (between the red dotted line and blue dotted line). The Mask Decoder predicts segmentation results, while the LLM outputs image-level semantic descriptions. Vision-Language Knowledge Injection. We propose the Visual-Language Semantic Align (VLSA) module to align the output of MLLMs with SAM prompt encoder. Due to the decreased boundary awareness capability of SAM after incorporating vision-language knowledge, we propose Semantic Boundary Regularization (SBR) to enhance SAMs boundary awareness capability. Strong Cross-Domain Generalization for Microscopy Segmentation. uLLSAM demonstrates robust zero-shot generalization capabilities, outperforming existing methods in cross-domain scenarios. It achieves substantial improvements on 10 unseen datasets from various domains, including LM, EM, pathology, and medical imaging, showcasing its ability to adapt to new domains without requiring additional training."
        },
        {
            "title": "2 Related Work",
            "content": "Extending SAM with Multimodal Large Language Models. SAMs remarkable generalization on natural images has led to extensions like LISA[14], GLaMM[15], and EVF-SAM[17]. These methods excel in referring segmentation for natural images but struggle with specialized domains like microscopy due to scarce high-quality data and limited pre-trained weight generalization. LISA combines language models with visual reasoning but focuses mainly on natural image mask generation. GLaMM introduces dialogue-driven segmentation but lacks precise visual grounding. EVF-SAM uses early fusion strategies but loses crucial point-level interaction features. Our approach leverages MLLMs to train generalized image encoder, maintains point-level interaction, and uses the VLSA module to align MLLM and SAM prompts. With higher-resolution image model and InternLM2.51.8B[23], our method offers better flexibility and precision for tasks requiring nuanced understanding of visual and language inputs. Interactive Segmentation for Biomedical Images. Biomedical image segmentation has advanced with models like BiomedParse[9], MedSAM[10], and µSAM[5]. BiomedParse jointly learns segmentation and detection tasks using large datasets and PubMedBERT[24] but cannot perform multiinstance segmentation. MedSAM works across various medical imaging tasks but struggles with vascular structures and rare imaging domains. µSAM addresses microscopic image segmentation but lacks consistency across different microscope domains. These models face limited generalization and semantic awareness issues. Our approach uses MLLMs vision-language knowledge to improve SAM model generalization, provides an interactive interface with basic image analysis capabilities, 3 and implements the SBR for robust interactive performance, making it more versatile for biomedical imaging tasks. Application of MLLMs in Biomedical Fields. MLLMs have broad medical applications including cancer diagnosis[25, 26, 27, 28, 29, 30], diagnostic report interpretation[31], explainable diagnosis[32], and pathology image analysis[33]. Evaluation methods include expert scoring, BLEU[34], and GPT-4[35] scoring. In biology, MLLMs exploration remains limited, with works discussing multimodal foundation models in molecular cell biology[36] and the GPT-4-based Omega[37] tool for cell segmentation. Unlike Omega, where segmentation and large model components do not interact, our uLLSAM features interaction between these components. To the best of our knowledge, we are the first to explore the application of SAM with MLLMs in microscopy, offering inspiration despite challenges such as the lack of high-quality biological datasets."
        },
        {
            "title": "3 Method",
            "content": "To address the fundamental constraint of µSAM that restricts its capability to process domain data exclusively through corresponding domain-specific models, we propose uLLSAM, which can handle data from different domains with unified model. In Sec 3.1, we introduce the background of µSAM, followed by detailed description of our proposed uLLSAM in Sec 3.2. Sec 3.3 will illustrate training strategies of uLLSAM. 3.1 Preliminaries: SAM and µSAM SAM[11] is foundation vision model for segmenting anything in natural images, while µSAM [5] is developed based on SAM for segmenting anything in microscopy. SAM mainly consists of three parts: (1) An image encoder responsible for feature extraction from images. (2) prompt encoder that processes user input prompts. (3) mask decoder that generates predicted masks after receiving encoded image features and prompt features. µSAM was trained with two sets of parameters on LM and EM datasets, with two branches after the image encoder: (1) The first branch connects directly to decoder, predicting the foreground of each instance, distances to object centers and boundaries, and then post-processing to obtain results. (2) The second branch consists of SAMs prompt encoder and mask decoder, which generates positive point in under-segmented regions and negative point in incorrectly segmented regions to correct the results after each forward pass. More details can be found in [5]. The features of biomedical images vary significantly across different domains[2, 6, 7, 8, 38, 39]. MLLMs can provide powerful multimodal understanding and reasoning capabilities[40], which brings hope for unifying cross-domain biomedical images. Sec 3.2 will elaborate in detail on our method for injecting vision-language knowledge into SAM. 3.2 Ours: uLLSAM Our motivation is illustrated in sub-figure (a) of Figure 1, where µSAM can only process specific domain data using specific weights, and lacks analytical descriptions of images. uLLSAM requires only one set of model parameters to process multiple domains of microscopy data, and can also handle histopathology and medical domain similar to microscopy. The overall architecture of uLLSAM is shown in Figure 2, where the Semantic Boundary Regularization (SBR) strategy is responsible for generating prompt points based on ground truth masks, and the Vision-Language Semantic Alignment (VLSA) module aligns the LLMs output with the SAM prompt encoder. The specific details will be described in the following subsections. 3.2.1 Vision-Language Semantic Alignment SAM and LLM share the same Vision Transformer[41] (ViT-B/16). For vision-language alignment, our method follows the same approach as LLaVA. Specifically, we employ visual projection layer Mproj , with pixel shuffle[42] function pix(, ratio) : RBHW RB(Hratio)(W ratio)(C/ratio2) used to adjust the number of visual tokens according to ratio. Given an input image RHW 3, visual encoder fθvis () and LLM decoder fθllm (), our data 4 Figure 3: Performance between µSAM and uLLSAM. The evaluation metric used in the figure is Segmentation Accuracy, with numbers on the bar graph representing percentage performance improvement, and the horizontal axis representing dataset abbreviations: Platynereis (PY), MitoLab (ML), DeepBacs (DB), NeurIPS_CellSeg (CS), COVID_IF (CI), MouseEmbryo (ME), PlantSeg (PS), LIVECell (LC), and TissueNet (TN). Light blue represents Electron Microscopy datasets, while Orange represents Light Microscopy datasets. (a) Specialist Models represents training and inference data belonging to the same domains. (b) Generalist Models represents cross-domain training and inference data. (c) Performance Drop. Represents the performance drop when using cross-domain models for inference on current domain data; lower value indicates better crossdomain generalization performance of the model. flow process is formulated as shown in Eq. 1. Hv = pix(Hv, 0.5), with Hv = fθvis (I) Hhid = fθllm (concat(Mproj Hv, Ht)) (1) After obtaining the hidden states Hhid from the final layer of the LLM, the VLSA module further processes Hhid. Specifically, the VLSA module first separates the visual tokens from Hhid, then uses the pix(, ratio) operator to adjust the number of visual tokens, and finally employs components such as layernorm and MLP to modify the dimension of each token so that it can be aligned with SAMs prompt encoder. To ensure numerical stability during training, we additionally introduce scaling factors α and shift factors β, as shown in Eq. 2. Denseembed = (cid:26) α VLSA(Hhid) + β, uLLSAM no_mask_embeddings, Baseline[11] (2) 3.2.2 Semantic Boundary Regularization When training uLLSAM, we generate one positive point and three negative points for each instance mask based on the SBR strategy. Specifically, given an instance mask M, we use erosion and dilation operations to obtain E(M) and D(M) respectively. Positive points are preferentially sampled uniformly from high-confidence regions (eroded areas), = B10(M) represents the internal region of the instance mask after 10 erosions, cM = ( 1 pM yp) is the centroid of the instance mask, Np is the number of positive points to be generated, (W, H) is the image size, and the positive point generation formula is shown in Eq. 3: (cid:80) Mxp, 1 (cid:80) = i=1 U(E), {pi}Np {pi mod E}Np i=1, pi E, {cM, cM, . . . , cM}, 2 ), ( {( 2 , 2 , if Np else if > 0 else if > 0 (3) 2 ), . . . , ( For sampling negative points, we choose background points that are 9 to 11 pixels away from the instance boundary. Specifically, = {p9 d(p, M) 11, / M} represents the boundary adjacent region, = {pp / D(M), / M} represents the external region, d(p, M) is the distance from point to the instance boundary M, Nn is the number of negative sample points, and the relevant formula is Eq. 4. 2 )}, otherwise 2 , = i=1 U(B), i=1 U(O), {pi}Nn {pi}Nn {pipi Ω M, pi U(Ω)}Nn i=1, otherwise if Nn else if Nn (4) SBR strategy provides explicit semantic boundary constraints for training SAM, enabling the model to better learn instance boundary features, allowing for optimal performance with simple interactive inputs during inference. Figure 4: Qualitative evaluation of uLLSAM on the test set. Yellow outlines represent ground truth, with each dataset displaying four images. The first to fourth rows represent µSAM, uLLSAM, uLLSAM w/o VLK, and uLLSAM w/o SBR, respectively. Note that w/o VLK means VLK is used during training but not during inference. w/o SBR means SBR is not used during either training or inference. 3.3 Training strategy of uLLSAM Our uLLSAM adopts three-stage training approach: vision-language alignment, supervised finetuning (SFT), and interactive SAM training. This strategy enables SAM to extract rich visual-language features from MLLMs. More details can be found in Appendix Section A. Stage 1: Vision-Text Alignment Pretraining. This stage aligns features from the visual encoder with the language models feature space through vision projection layer, efficiently integrating visual information with the large language model. We sampled approximately 80K microscopy image-text pairs from the BIOMEDICA[22] dataset. Stage 2: Supervised Fine-tuning. Due to the scarcity of microscopy datasets with both instance segmentation labels and high-quality text descriptions, we leveraged Qwen2.5VL-72B[43] to generate detailed textual descriptions for 9 LM and EM datasets (prompt templates available in Appendix A). This process enables the model to produce comprehensive image-level descriptions while learning robust vision-language features from powerful MLLMs. Stage 3: Interactive SAM Training. With our MLLMs now capable of extracting robust visual-text interactive features, SAM can utilize these features to improve image detail comprehension. Similar to MedSAM[10] training, we exclusively use point prompts as interactive input, as points flexibly indicate users regions of interest. For each instance, we generate points by using SBR strategy for training and select maximum of 4 random instances per image for loss calculation. During this stage, we train the image encoder, prompt encoder, mask decoder, vision projection layer, and VLSA module, using linear combination of BCE and Dice loss functions."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experimental Setup Datasets. We sample 20K 2D images from seven LM datasets[44, 45, 46, 47, 48, 49, 50] and 20K from two EM datasets[51, 52], totaling 40K 2D images for model training, and sample 7.8K from the remaining datasets for model performance validation. Specifically, since the datasets 6 Table 1: Ablation studies on VLSA module: Scale and Shift (SS) factor and dropout (p=0.2). Learned SS Dropout Dice 0.864 0.867 0.875 0.869 SA 0.781 0.785 0.794 0.787 Table 2: Training strategies. The impact of training steps on model performance, specifically, including Pre-train and SFT. Pre-train SFT SA 0.790 0.781 0. Dice 0.871 0.864 0.875 contain 3D data and two-channel TissueNet[50], all data are converted to 2D format for processing, and are padded with 0 to create square images before being resized to 10241024 resolution. Additionally, we prepared 10 untrained datasets to test the models zero-shot performance, including three LM[38, 39, 53, 54], three EM[55, 56, 52], two histopathology[57, 58], and two medical datasets[59, 60]. Evaluation Metrics. We use the same SBR strategy to generate prompts for inference on the 7.8K validation dataset. uLLSAM is highly flexible and can select inference modes based on computational resources and application scenarios, with only minimal performance loss when using only the SAM component without VLK for inference. We use Dice, commonly used in segmentation tasks, and Segmentation Accuracy (SA) at threshold of 0.5, as the evaluation metrics in µSAM [5]. 4.2 Comparison Experiments Here we designed three sets of comparison experiments. The first set, referred to as \"Specialist Models\", involves training two specialist models (LM-specialist and EM-specialist) with reference to µSAM, using LM and EM data respectively, and then evaluating the inference performance of these trained specialist models on both in-domain and out-of-domain data. The second set, termed \"Generalist Models\", involves combining LM and EM data to train unified microscopy foundation model, which demonstrated SOTA performance across all datasets. The third set, tested on vanilla SAM and its variants. Figure 7 shows the qualitative results of specialist and generalist models. Specialist Models. Due to its poor generalization capability, µSAM exhibits suboptimal performance on cross-modal data. Therefore, we explored whether MLLMs could guide SAM to learn more enriched cross-modal knowledge. Figure 3 shows the results of training µSAM and uLLSAM specialist models separately on single-modal datasets, then testing them on both in-domain and out-of-domain datasets. The xaxis in the subplots represents the abbreviated names of datasets. Subplot Figure 3 (a) demonstrates the performance of specialists, showing our method outperforms µSAM across all nine datasets within their respective domains. Subplot Figure 3 (b) compares the generalist performance across domains, with our method consistently surpassing µSAM in generalization capability. Interestingly, subplot Figure 3 (c) compares the performance drop between Specialists and Generalists, where lower values indicate smaller performance degradation and thus stronger generalization. Our method demonstrates robust generalization across all datasets except for slightly weaker performance on the Platynereis dataset compared to µSAM. These results demonstrate from another perspective that even when SAM is not trained on specific modal data, MLLMs guidance can significantly improve SAMs zero-shot generalization performance. This experiment also inspired our approach to training unified microscopy SAM segmentation foundation model. Figure 5: We emphasize the experimental results of Ours w/o VLK. Even when VLK is not used during inference (though used during training), performance still significantly surpasses µSAM by 9.57%, with only 2.25% performance decrease compared to when VLK is used during inference. 7 Generalist Models. Inspired by the experimental results in Figure 3, we attempted to use MLLMs to guide SAM in combined training across multimodal microscopy datasets, thereby further validating whether MLLMs can help SAM better learn richer domain knowledge across different domains. As shown in Figure 1 (d), uLLSAM demonstrates comprehensive performance improvements over µSAM in both Dice(0.5) and SA(0.5) metrics. Specifically, on the DeepBacs[44] dataset, we observed substantial gains of 12.61% and 19.33% in these metrics respectively, with the smallest improvements of 3.04% and 4.77% observed on the MitoLab[52] dataset. Figure 4 shows qualitative evaluation of uLLSAM. General Interactive Segmentation Models. We directly test interactive segmentation performance on the general-purpose foundation vision model SAM and its variants in natural settings. Table 3 shows the average performance metrics on 9 LM and EM datasets, revealing significant gap between performance on natural images versus microscopy images. This drives the development of foundation vision model specifically adapted for the microscopy domain, with requirements for strong generalization capabilities. Table 3: Test the vision foundation models directly on microscopy datasets. Method SAM[11] SAM-HQ[61] SAM2[62] µSAM[5] Ours Dice 0.103 0.077 0.128 0.813 0.875 SA 0.066 0.049 0.087 0.710 0.794 4.3 Ablation Experiments The core idea of uLLSAM is to leverage MLLMs to guide SAM in learning rich domain knowledge, thereby enabling it to process wider range of data domains. Here, we conducted three ablation experiments centered on MLLM: The first experiment addresses an uncertaintysince our model introduces additional parameters, it remains unclear whether performance improvements stem from these extra parameters or from SAM genuinely learning richer domain knowledge. Therefore, we attempted to directly remove the Vision-Language Knowledge from uLLSAM for performance testing to verify the reason for improvement. The second experiment concerns the design of the VLSA module. The third experiment examines the effectiveness of the SBR strategy. We also performed additional ablation experiments on the training strategy for SAM. Table 4: Explore SBR strategy when training. Vision-Language Knowledge Injection We conducted tests on 9 in-domain and 10 out-of-domain datasets, using only the trained SAM component of uLLSAM for inference. Figure 5 shows the performance on in-domain datasets. It can be observed that even without VLK during inference, the performance comprehensively surpasses µSAM. Specifically, the DeepBacs[44] dataset achieved the largest performance improvements in Dice and SA metrics, with gains of 9.76% and 14.42% respectively. The LIVECell[49] dataset showed the smallest performance improvements, with gains of 1.1% and 1.84% respectively. The average performance improvement across all datasets was 3.94% and 6.2% respectively. Analysis of the results indicates that even without relying on LLM guidance, uLLSAM still demonstrates significant performance improvements, which strongly proves that our performance gain is not entirely due to the increase in parameter count. Compared to the complete uLLSAM, using only the SAM component resulted in just 2.88% and 4.36% performance degradation. SBR strategy 0.808 0.875 0.712 0.794 Metrics Dice SA 1 1 0 3 Figure 6 shows our performance results on 10 out-of-domain datasets. Comparing µSAM with uLLSAM without the LLM component, the GLAS[57] dataset achieved the highest Dice and SA performance improvements of 13.22% and 19.00% respectively. On the CoNSeP[58] dataset, there was slight performance decrease of -1.9% and -1.73%, with an overall average performance improvement of 2.98% and 4.65%. Even in out-of-domain areas, the generalization ability of uLLSAM using only the SAM component still surpasses µSAM. This further confirms that MLLMs can guide SAM to learn better multimodal features. VLSA module We experimented with different designs of the VLSA model. Due to the gap between vision semantic prompts from MLLMs and SAMs prompt space, we explored the impact on model performance of directly inputting these into the SAM prompt encoder versus using scale 8 and shift factors. We also added dropout layer to VLSA to investigate whether uLLSAM exhibits overfitting phenomena. Analysis from Table 1 reveals that using learnable scale and shift factors improves model performance, while adding dropout layers actually decreases performance, indicating our model does not suffer from significant overfitting issues. SBR Strategy The last row of Figure 4 with uLLSAM w/o SBR demonstrates that directly injecting VLK causes the model to generate blurred object boundaries, The area indicated by the red arrow represents regions with over-segmentation, under-segmentation and inaccurate segmentation. Analysis from Table 4 shows that SBR brings an average performance improvement of 8.24% in Dice and 11.46% in SA, thus confirming the effectiveness of the SBR strategy. Training Strategies Our uLLSAM is the result of three-stage training process. Here we explore the impact of each stage on model performance. From Table 2, we can observe that both skipping the alignment module pre-training on the BIOMEDICA dataset and not using Qwen2.5VL-72B for distillation SFT lead to decreased model performance. 4.4 Zero-shot Generalization Zero-shot Performance on Ten Additional Datasets To further verify our models zero-shot performance and generalizability on cross-modal datasets, we additionally selected 3 LM, 3 EM, 2 histopathology, and 2 medical datasets that were not used during training for further validation. Dataset abbreviations: CellPose (CP), Omnipose (OP), OrgaSegment(OS), Uro-Cell (UC), NucMM-M (NM), MitoNet_Benchmark(MB), GLAS (GA), CoNSep (CS), ISIC2018-task1 (IS), BUSI-benign (BU). Light blue represents electron microscopy, orange represents light microscopy, purple represents pathology, and brown represents medical datasets. Figure 6 shows our experimental results, where our method comprehensively outperforms µSAM. The x-axis represents the abbreviated names of datasets. Specifically, GLAS achieved the largest performance improvements on Dice and SA evaluation metrics with gains of 17.84% and 24.52% respectively, while the CoNSeP dataset showed the smallest improvements of 1.27% and 2.57% respectively. Across all 10 datasets, our method achieved an average performance improvement of 6.79% and 10.08%. Figure 6: Zero-shot performance on ten additional datasets. The numbers on each bar represent the performance improvement between our method and µSAM. Even without training on these domains, our method still demonstrates significant average performance improvement of 9.85%. Table 5: Explore SBR strategy when inference. SBR Strategy Enhances Generalization Interactive prompt point generation strategies typically influence the quality of segmentation masks. For example, in SAM-HQ[61], using more diverse positive and negative sample points generally produces higher quality results, though this improvement eventually reaches plateau. Here we explore how different quantities of positive and negative prompt points affect our models performance. As shown in Table 5, the model achieves optimal average performance on the dataset when using 1 positive point and 3 negative points, indicating that users generally need to provide only four interactive prompt points to obtain satisfactory baseline results. The 3 negative points significantly determine the objects boundary range, enabling the model to segment the region of interest with greater confidence. 0.461 0.794 0.683 0.772 0.701 0.743 0.560 0.875 0.782 0.858 0.797 0.835 SBR strategy 1 1 3 3 5 5 0 3 0 3 0 3 Metrics Dice SA P"
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose uLLSAM, the first foundational model that explores interactive segmentation with MLLMs in the field of microscopy. uLLSAM unifies the processing of light and electron microscopy data, and also demonstrates significant improvements in generalization across crossdomain data. Moreover, our model possesses the capability for microscopic image analysis, which previous foundational models lack. We believe that uLLSAM will greatly accelerate MLLMs research in the biomedical domain and provide valuable insights for related fields."
        },
        {
            "title": "References",
            "content": "[1] Jun Ma and Bo Wang. Towards foundation models of biological image segmentation. Nature Methods, 20(7):953955, 2023. [2] Vishwanatha Rao, Michael Hla, Michael Moor, Subathra Adithan, Stephen Kwak, Eric Topol, and Pranav Rajpurkar. Multimodal generative ai for medical image interpretation. Nature, 639(8056):888896, 2025. [3] Shanghang Zhang, Gaole Dai, Tiejun Huang, and Jianxu Chen. Multimodal large language models for bioimage analysis. nature methods, 21(8):13901393, 2024. [4] Chenxi Ma, Weimin Tan, Ruian He, and Bo Yan. Pretraining foundation model for generalizable fluorescence microscopy-based image restoration. Nature Methods, 21(8):15581567, 2024. [5] Anwai Archit, Luca Freckmann, Sushmita Nair, Nabeel Khalid, Paul Hilt, Vikas Rajashekar, Marei Freitag, Carolin Teuber, Genevieve Buckley, Sebastian von Haaren, et al. Segment anything for microscopy. Nature Methods, pages 113, 2025. [6] Minxing Pang, Tarun Kanti Roy, Xiaodong Wu, and Kai Tan. Cellotype: unified model for segmentation and classification of tissue images. Nature methods, 22(2):348357, 2025. [7] Carsen Stringer and Marius Pachitariu. Cellpose3: one-click image restoration for improved cellular segmentation. Nature Methods, pages 18, 2025. [8] Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, et al. The multimodality cell segmentation challenge: toward universal solutions. Nature methods, 21(6):11031113, 2024. [9] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, et al. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature methods, 22(1):166176, 2025. [10] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. [11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. [13] Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, and Wei-Shi Zheng. Llmdet: Learning strong open-vocabulary object detectors under the supervision of large language models. arXiv preprint arXiv:2501.18954, 2025. [14] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 10 [15] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao Anwer, Eric Xing, Ming-Hsuan Yang, and Fahad Khan. Glamm: Pixel grounding large multimodal model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1300913018, 2024. [16] Yang Liu, Pengxiang Ding, Siteng Huang, Min Zhang, Han Zhao, and Donglin Wang. Pite: Pixel-temporal alignment for large video-language model. In European Conference on Computer Vision, pages 160176. Springer, 2024. [17] Yuxuan Zhang, Tianheng Cheng, Lianghui Zhu, Rui Hu, Lei Liu, Heng Liu, Longjin Ran, Xiaoxin Chen, Wenyu Liu, and Xinggang Wang. Evf-sam: Early vision-language fusion for text-prompted segment anything model. arXiv preprint arXiv:2406.20076, 2024. [18] Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, and Yehui Yang. Towards multimodal large language model with pixel-level insight for biomedicine. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 37793787, 2025. [19] Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, and Liqiang Nie. Lion: Empowering multimodal large language model with dual-level visual knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2654026550, 2024. [20] Alejandro Lozano, Jeffrey Nirschl, James Burgess, Sanket Rajan Gupte, Yuhui Zhang, Alyssa Unell, and Serena Yeung. Micro-bench: microscopy benchmark for vision-language understanding. Advances in Neural Information Processing Systems, 37:3067030685, 2024. [21] James Burgess, Jeffrey Nirschl, Laura Bravo-Sánchez, Alejandro Lozano, Sanket Rajan Gupte, Jesus Galaz-Montoya, Yuhui Zhang, Yuchang Su, Disha Bhowmik, Zachary Coman, et al. Microvqa: multimodal reasoning benchmark for microscopy-based scientific research. arXiv preprint arXiv:2503.13399, 2025. [22] Alejandro Lozano, Min Woo Sun, James Burgess, Liangyu Chen, Jeffrey Nirschl, Jeffrey Gu, Ivan Lopez, Josiah Aklilu, Austin Wolfgang Katzer, Collin Chiu, et al. Biomedica: An open biomedical image-caption archive, dataset, and vision-language models derived from scientific literature. arXiv preprint arXiv:2501.07171, 2025. [23] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024. [24] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH), 3(1):123, 2021. [25] Juexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, Shawn Afvari, et al. Pre-trained multimodal large language model enhances dermatological diagnosis using skingpt-4. Nature Communications, 15(1):5649, 2024. [26] Dyke Ferber, Georg Wölflein, Isabella Wiest, Marta Ligero, Srividhya Sainath, Narmin Ghaffari Laleh, Omar SM El Nahhas, Gustav Müller-Franzes, Dirk Jäger, Daniel Truhn, et al. In-context learning enables multimodal large language models to classify cancer pathology images. Nature Communications, 15(1):10104, 2024. [27] Chuang Niu, Qing Lyu, Christopher Carothers, Parisa Kaviani, Josh Tan, Pingkun Yan, Mannudeep Kalra, Christopher Whitlow, and Ge Wang. Medical multimodal multitask foundation model for lung cancer screening. Nature Communications, 16(1):1523, 2025. [28] Kevin Boehm, Omar SM El Nahhas, Antonio Marra, Michele Waters, Justin Jee, Lior Braunstein, Nikolaus Schultz, Pier Selenica, Hannah Wen, Britta Weigelt, et al. Multimodal histopathologic models stratify hormone receptor-positive early breast cancer. Nature Communications, 16(1):2106, 2025. 11 [29] Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Melissa Zhao, Aaron Chow, Kenji Ikemura, Ahrong Kim, Dimitra Pouli, Ankush Patel, et al. multimodal generative ai copilot for human pathology. Nature, 634(8033):466473, 2024. [30] Jinxi Xiang, Xiyue Wang, Xiaoming Zhang, Yinghua Xi, Feyisope Eweje, Yijiang Chen, Yuchen Li, Colin Bergstrom, Matthew Gopaulchan, Ted Kim, et al. visionlanguage foundation model for precision oncology. Nature, pages 110, 2025. [31] Juan Manuel Zambrano Chaves, Shih-Cheng Huang, Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, et al. clinically accessible small multimodal radiology model and evaluation metric for chest x-ray findings. Nature Communications, 16(1):3108, 2025. [32] Yifan Wu, Yang Liu, Yue Yang, Michael Yao, Wenli Yang, Xuehui Shi, Lihong Yang, Dongjun Li, Yueming Liu, Shiyi Yin, et al. concept-based interpretable model for the diagnosis of choroid neoplasias using multimodal data. Nature Communications, 16(1):3504, 2025. [33] Ming Lu, Bowen Chen, Drew FK Williamson, Richard Chen, Ivy Liang, Tong Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al. visual-language foundation model for computational pathology. Nature Medicine, 30(3):863874, 2024. [34] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311318, 2002. [35] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [36] Haotian Cui, Alejandro Tejada-Lapuerta, Maria Brbic, Simona Cristea, Hani Goodarzi, and Mohammad Lotfollahi. Towards multimodal foundation models in molecular cell biology. Nature, pages 623633, 2025. [37] Loïc Royer. Omegaharnessing the power of large language models for bioimage analysis. nature methods, 21(8):13711373, 2024. [38] Carsen Stringer, Tim Wang, Michalis Michaelos, and Marius Pachitariu. Cellpose: generalist algorithm for cellular segmentation. Nature methods, 18(1):100106, 2021. [39] Marius Pachitariu and Carsen Stringer. Cellpose 2.0: how to train your own model. Nature methods, 19(12):16341641, 2022. [40] Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [41] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. [42] Wenzhe Shi, Jose Caballero, Ferenc Huszár, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 18741883, 2016. [43] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [44] Christoph Spahn, Estibaliz Gómez-de Mariscal, Romain Laine, Pedro Pereira, Lucas von Chamier, Mia Conduit, Mariana Pinho, Guillaume Jacquemet, Séamus Holden, Mike Heilemann, et al. Deepbacs for multi-task bacterial image analysis using open-source deep learning approaches. Communications Biology, 5(1):688, 2022. 12 [45] Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, et al. The multimodality cell segmentation challenge: toward universal solutions. Nature methods, 21(6):11031113, 2024. [46] Constantin Pape, Roman Remme, Adrian Wolny, Sylvia Olberg, Steffen Wolf, Lorenzo Cerrone, Mirko Cortese, Severina Klaus, Bojana Lucic, Stephanie Ullrich, et al. Microscopy-based assay for semi-quantitative detection of sars-cov-2 specific antibodies in human sera: semiquantitative, high throughput, microscopy-based assay expands existing approaches to measure sars-cov-2 specific antibody levels in human sera. Bioessays, 43(3):2000257, 2021. [47] Vladyslav Bondarenko, Mikhail Nikolaev, Dimitri Kromm, Roman Belousov, Adrian Wolny, Marloes Blotenburg, Peter Zeller, Saba Rezakhani, Johannes Hugger, Virginie Uhlmann, et al. Embryo-uterine interaction coordinates mouse embryogenesis during implantation. The EMBO Journal, 42(17):e113280, 2023. [48] Adrian Wolny, Lorenzo Cerrone, Athul Vijayan, Rachele Tofanelli, Amaya Vilches Barro, Marion Louveaux, Christian Wenzl, Sören Strauss, David Wilson-Sánchez, Rena Lymbouridou, et al. Accurate and versatile 3d segmentation of plant tissues at cellular resolution. Elife, 9:e57613, 2020. [49] Christoffer Edlund, Timothy Jackson, Nabeel Khalid, Nicola Bevan, Timothy Dale, Andreas Dengel, Sheraz Ahmed, Johan Trygg, and Rickard Sjögren. Livecella large-scale dataset for label-free live cell segmentation. Nature methods, 18(9):10381045, 2021. [50] Noah Greenwald, Geneva Miller, Erick Moen, Alex Kong, Adam Kagel, Thomas Dougherty, Christine Camacho Fullaway, Brianna McIntosh, Ke Xuan Leow, Morgan Sarah Schwartz, et al. Whole-cell segmentation of tissue images with human-level performance using large-scale data annotation and deep learning. Nature biotechnology, 40(4):555565, 2022. [51] Hernando Vergara, Constantin Pape, Kimberly Meechan, Valentyna Zinchenko, Christel Genoud, Adrian Wanner, Kevin Nzumbi Mutemi, Benjamin Titze, Rachel Templin, Paola Bertucci, et al. Whole-body integration of gene expression and single-cell morphology. Cell, 184(18):48194837, 2021. [52] Ryan Conrad and Kedar Narayan. Instance segmentation of mitochondria in electron microscopy images with generalist deep learning model trained on diverse dataset. Cell Systems, 14(1):58 71, 2023. [53] Kevin Cutler, Carsen Stringer, Teresa Lo, Luca Rappez, Nicholas Stroustrup, Brook Peterson, Paul Wiggins, and Joseph Mougous. Omnipose: high-precision morphologyindependent solution for bacterial cell segmentation. Nature methods, 19(11):14381448, 2022. [54] Juliet Lefferts, Suzanne Kroes, Matthew Smith, Paul Niemöller, Natascha DA Nieuwenhuijze, Heleen Sonneveld van Kooten, Cornelis van der Ent, Jeffrey Beekman, and Sam FB van Beuningen. Orgasegment: deep-learning based organoid segmentation to quantify cftr dependent fluid secretion. Communications biology, 7(1):319, 2024. [55] Manca Žerovnik Mekuˇc, Ciril Bohak, Samo Hudoklin, Byeong Hak Kim, Min Young Kim, Matija Marolt, et al. Automatic segmentation of mitochondria and endolysosomes in volumetric electron microscopy data. Computers in biology and medicine, 119:103693, 2020. [56] Xueying Wang Boulanger-Weill and Ignacio Nagaraju Dhanyasi. Nucmm dataset: 3d neuronal nuclei instance segmentation at sub-cubic millimeter scale. arXiv preprint arXiv:2107.05840, 2021. [57] Korsuk Sirinukunwattana, Josien PW Pluim, Hao Chen, Xiaojuan Qi, Pheng-Ann Heng, Yun Bo Guo, Li Yang Wang, Bogdan Matuszewski, Elia Bruni, Urko Sanchez, et al. Gland segmentation in colon histology images: The glas challenge contest. Medical image analysis, 35:489502, 2017. [58] Simon Graham, Quoc Dang Vu, Shan Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir Rajpoot. Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. Medical image analysis, 58:101563, 2019. 13 [59] Noel Codella, Veronica Rotemberg, Philipp Tschandl, Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: challenge hosted by the international skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368, 2019. [60] Walid Al-Dhabyani, Mohammed Gomaa, Hussien Khaled, and Aly Fahmy. Dataset of breast ultrasound images. Data in brief, 28:104863, 2020. [61] Lei Ke, Mingqiao Ye, Martin Danelljan, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu, et al. Segment anything in high quality. Advances in Neural Information Processing Systems, 36:29914 29934, 2023. [62] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. [63] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [64] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [65] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024. [66] OpenAI. Openai o1 system card, 2024. [67] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [68] Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6, 2023. [69] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [70] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [71] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [72] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023."
        },
        {
            "title": "A Training Details",
            "content": "Figure 7: Performance of specialized models and cross-domain models on 9 datasets. The suffixes -S and -C represent specialist test results and cross-domain test results, respectively. Specifically, if model is trained on the EM dataset, testing it on the EM dataset yields specialist test results, while testing it on the LM dataset yields cross-domain test results. The same applies for LM. Stage 1: Vision-Text Alignment Pretraining. During this stage, we trained only the Vision Projection Layer for 6 epochs on four RTX3090 GPUs with batch size of 3, using AdamW[63] optimizer with learning rate of 1e-4 and CrossEntropy loss function. Unless specified otherwise, subsequent parameters remain consistent. Stage 2: Supervised Fine-tuning. This stage aims to enhance our models semantic understanding capabilities. We trained the Vision Projection Layer and LLM for 2 epochs on single RTX3090 GPU with batch size of 1 and learning rate of 1e-6. The prompt template for Qwen2.5-VL-72B is shown as Figure 8. Stage 3: Interactive SAM Training. This stage uses learning rate of 1e-3 for training over 24 epochs, with batch size of 1 and gradient accumulation steps set to 8 to simulate larger batch size. For each image, the sam_max_point_bs parameter is set to 4, which means that only maximum of 4 randomly selected instances per image are used for loss calculation and backpropagation. Training was conducted using 4 RTX 3090 GPUs, with total training time of approximately 40 hours."
        },
        {
            "title": "B MicroVQA Benchmark",
            "content": "Table 6: Answer accuracy performance of uLLSAM and its base model on the challenging MicroVQA dataset. V, H, and represent different types of perception tasks. This dataset reflects, to some extent, the models capability in microscope-based fundamental mechanism analysis. Symbol * represents result borrowed from MicroVQA benchmark. Model Overall Random* Llama-3.2-11b*[64] LLaVA-Mistral-7B*[65] Human* o1*[66] InternVL2.5-2B[23] uLLSAM 22.0 30.3 39.8 50.3 52.8 35.6 39.0 21.9 32.4 31.6 52.7 55.4 35.1 39.2 21.8 29.3 43.1 47.5 50. 33.6 36.1 21.9 28.7 37.1 51.4 53.0 40.0 43.8 Currently, our model focuses on how to improve the visual general generalization ability of the model, therefore the quality of textual description output and hallucination control are not the focus of our Figure 8: When prompting the Qwen2.5-VL-72B model, we primarily focus on object types, texture features, relative positions. We also ensure that the model outputs confident content as much as possible to mitigate hallucinations. method. However, we attempted to preliminarily explore the reasoning and understanding capabilities of the LLM component through evaluation on microscopy vision-language reasoning benchmark. The MicroSAM data set is divided into three categories: expert visual understanding (V), hypothesis generation (H), and experimental proposal (E) based on varying scientific requirements and difficulty levels of the task. We benchmarked uLLSAM against its base model (InternVL2.5-2B) on the MicroVQA dataset, demonstrating substantial improvement of 9.55% in average accuracy. However, since the parameter count of uLLSAMs MLLM component is significantly smaller than that of o1 [66], there remains considerable performance gap. Future work could explore methods to enhance uLLSAMs image reasoning capabilities. User-Friendly Interface To facilitate domain experts use of our model, we developed user-friendly graphical interface. The overall interface is shown in Figure 9. Basic operations include: 1. Upload images on the left side, supporting formats such as jpeg, png, tif, etc. 2. Select the model to be loaded. 3. Choose positive or negative points, add prompt points directly by clicking on the image. 4. Click Generate Mask to produce segmentation results. 5. Display segmentation results on the right side. After generating satisfactory mask, click the Save Instance button to save the instance. Each instance is numbered starting from 1."
        },
        {
            "title": "D Discussion",
            "content": "Versatility. Our method is simple and efficient. For professionals in the computer industry, components such as LLMs and image encoders can be easily replaced to match their computational resource capabilities. For researchers in the biomedical field, we provide user-friendly interactive interface with extremely low deployment and fine-tuning costsrequiring only single RTX 3090 GPU for smooth operation. Impact of LLM Choice We directly selected InternLM2.5-1.8B as the LLM component of our MLLM, while the visual encoder part was initialized with uSAM pre-trained weights. Due to 16 Figure 9: Overall of our user-friendly interface. computational resource constraints, we did not conduct tests on larger LLMs or different types of LLMs; however, we believe that even with different LLMs[64, 67, 68], comparable performance can be achieved. In the future, we will further explore the impact of LLM types and parameter sizes on uLLSAM performance. Limitations. Although our method achieves good generalization, there are still shortcomings in terms of interaction methods, text utilization, and other aspects. The first limitation lies in the fact that we only consider single mode of interaction during training. Future work could explore whether diversified prompt interactions may further enhance model generalization. The second limitation is that we rely solely on the strong semantic perception capability of LLMs to improve the generalization of SAM, which allows decoupling during inference. However, tasks such as text-guided referring segmentation have not yet been explored, partly due to the lack of expert-level, high-quality annotated data. The third limitation is the restriction imposed by computational resources. we have not been able to verify whether larger-scale LLMs could further improve the models generalization and microscopic image analysis capabilities. One feasible approach is to adopt Parameter-Efficient Fine-Tuning (PEFT) strategies such as LoRA[69]. The fourth limitation lies in the fact that we currently only consider unidirectional interaction between the LLM and SAM. In the future, we will continue to explore how to enable bidirectional interaction between these two components to achieve mutually beneficial outcomes. The fifth limitation is that we currently do not have control interventions for image-level description outputs. In the future, we can explore some reinforcement learning methods [70, 71, 72] to further optimize the models textual description outputs. Broad Impact. To the best of our knowledge, we are the first to explore the application of MLLMs in the field of microscopy, paving the way for future MLLMs research in related areas. Our method 17 can be easily transferred to various scenarios, such as interactive medical image segmentation. And the visual encoder with strong generalization capabilities can be applied to wide range of downstream tasks. However, the text output by the model currently lacks interpretability and exhibits certain hallucination issues, which may result in the generation of erroneous content. In our future work, we will focus on addressing and optimizing these challenges. We hope our approach will accelerate the progress of MLLMs research in the biomedical domain."
        }
    ],
    "affiliations": [
        "Fudan University"
    ]
}