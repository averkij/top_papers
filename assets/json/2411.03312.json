{
    "paper_title": "Inference Optimal VLMs Need Only One Visual Token but Larger Models",
    "authors": [
        "Kevin Y. Li",
        "Sachin Goyal",
        "Joao D. Semedo",
        "J. Zico Kolter"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression."
        },
        {
            "title": "Start",
            "content": "Preprint. Under Review."
        },
        {
            "title": "INFERENCE OPTIMAL VLMS NEED ONLY ONE\nVISUAL TOKEN BUT LARGER MODELS",
            "content": "Kevin Y. Li1 Sachin Goyal1 1Carnegie Mellon University, 2Bosch Center for Artificial Intelligence {kyl2, sachingo, zkolter}@cs.cmu.edu Joao D. Semedo2 J. Zico Kolter1 joao.semedo@us.bosch.com 4 2 0 2 ] . [ 1 2 1 3 3 0 . 1 1 4 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count often to single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., 5 10), our results indicate that the computeoptimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression."
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advancements in Large Language Models (LLMs) have enabled Vision Language Models (VLMs) to perceive, reason, and respond through both text and image inputs (Liu et al., 2023; Alayrac et al., 2022; Dai et al., 2023). Many VLMs are built on top of pretrained vision encoders, like CLIP, and pass the patch-based tokens from the visual encoder into the pretrained LLM backbone at one-to-one ratio for visual context. This results in the LLM processing hundreds of tokens per image, overshadowing those from the user prompt and accounting for most of inference time compute. Consequently, deploying VLMs in real-world applications, particularly on consumer-side edge devices such as monitoring systems, driving assistants, etc., is often limited by the significant inference cost and resulting latency. To reduce the inference cost of VLMs, many recent works have focused on decreasing the number of visual tokens, via small learnable module, prior to passing image tokens into the LLM while minimizing performance degradation (Li et al., 2024c; Shang et al., 2024). For example, (Li et al., 2024c) learn cross attention module over the CLIP output tokens to compress the number of tokens. Alternatively, inference FLOPs, proportional to the number of parameters and number of tokens processed, can be reduced by using smaller LLM. Since both the LLM size and number of visual input tokens directly affect the VLMs performance, it becomes unclear what the optimal trade-off between the two is. For example, one could process all visual input tokens using 4B LLM or use an 8B LLM on reduced set of half the original visual tokens, as both result in similar inference costs currently, the ideal choice is unknown. *Equal contribution, work partially done at Bosch Research. 1 Preprint. Under Review. (a) Scaling laws for VLMs at = 0 (cached text). (b) Scaling laws for VLMs at = 50 (variable text). Figure 1: Inference optimal scaling laws for VLMs: The number of visual tokens (V ) passed to the LLM (after token compression, 2.2), along with the LLM parameter count (N ), directly determine the inference cost of VLMs (O(N (Q + ))), where is the text input tokens. Since VLMs downstream performance is directly affected by both these factors, it is unclear what the optimal trade-off is for fixed inference compute. In this work, we try to answer this question with our scaling laws. Left (a): We plot the fitted scaling curves, assuming cached text input tokens (Q = 0). We observe surprising trend: for visual reasoning tasks, the compute optimal behavior (dotted black curve) requires using single visual token with the largest possible language model that can fit under the inference budget. Right (b): Inference optimal behavior under = 50 requires slightly higher number of visual tokens as the LLM already incurs fixed cost due to the text tokens. This raises an important question: given fixed inference budget, what is the optimal trade-off between LLM size and the number of visual tokens processed for downstream performance? In this work, we try to answer this question by building the first inference-time compute-optimal scaling laws for VLMs, modeling performance as function of both key factors affecting inference cost: LLM size and the number of visual tokens processed. We observe that the downstream error varies 5x faster with LLM parameters than with the number of input visual tokens. In fact, our scaling laws reveal striking observation: for visual reasoning tasks, the computeoptimal inference regime entails using the largest feasible LLM with very small number of visual input tokens often just one when the input query can be cached. We show that for any given fixed inference cost, trading off the visual tokens for larger LLM size leads to reduction in downstream error for visual reasoning tasks. However, for certain use cases like Optical Character Recognition (OCR) or document understanding tasks, the optimal approach is quite the opposite, requiring as many visual tokens as possible, as token compression proves ineffective for capturing the dense and diverse information present in such tasks. Most existing work on token compression has focused on reducing visual tokens by modest factor (e.g., from 576 to 144 tokens or 64 tokens). In contrast, our results underscore the critical importance of pursuing much higher compression rates (e.g., reducing tokens to 1 or 4) for visual reasoning tasks where such compression is not only feasible but also compute-optimal. Building upon these insights, we take initial steps toward developing token compression algorithms specifically tailored for high compression regimes. We propose query-based token compression approach, recognizing that in extreme compression scenarios, it is essential to selectively curate tokens based on the users query to preserve the most relevant tokens. In summary, our work identifies the compute-optimal inference regime for VLMs, emphasizing the importance of high token compression for visual reasoning tasks. We hope these findings will serve as motivation and foundation for shifting token reduction techniques towards more effective and higher compression ratios. Our work is organized as follows. We first introduce some preliminaries around inference costs and visual token compression for VLMs in Section 2. Then we talk about our compute optimal scaling laws in Section 3, the results of which motivate our compression algorithm designed for high token-reduction regimes are covered in Section 4. 2 Preprint. Under Review."
        },
        {
            "title": "2.1 ESTIMATING INFERENCE COST FOR VLMS",
            "content": "The language model in VLMs processes the visual input tokens along with the user text query tokens. As language models become larger, the FLOPs (Floating Point Operations) required to process each input token scales accordingly. We follow the standard practice for estimating the inference time FLOPs as (Kaplan et al., 2020; Sardana et al., 2024; Snell et al., 2024): LOP sinf = O(N ), (1) where denotes the parameter count of LLM and denotes the total inference time tokens. We ignore the inference cost stemming from the visual encoder, as we use the CLIP-L vision encoder (Radford et al., 2021) with the same input image resolution across all experiments. In addition, many current open-source VLMs utilize the same encoder, incurring fixed cost across models. We highlight that the inference cost of VLMs scales proportionally with both the parameters and the number of input tokens processed by the LLM. In the context of VLMs, the total inference tokens, , can be further decomposed as = Q+V +G, where represents the text input tokens, i.e., the question/prompt, represents the number of visual tokens from the vision encoder (after token compression), and accounts for the generated tokens. In many real world applications, such as driving assistance systems, the text input remains constant (e.g., Alert the driver if the scene ahead has hazard). In these scenarios, the text input can be cached, effectively making = 0 by bypassing self-attention projections and feed-forward calculations. However, in other interactive applications, may vary based on dynamic input. We will study the behavior of the downstream error with LOP sinf under both = 0 and varying regimes. Finally, the generation tokens again are quite small for most inference tasks (single word answers). However, the analysis with increasing transfers to increasing + as well. 2.2 TOKEN COMPRESSION IN VLMS As discussed in the previous section, inference FLOPs for VLMs increase proportionally with the number of visual input tokens (e.g., 576 per image with CLIP-ViT-L visual encoder). The number of visual tokens often dominates the total number of tokens processed by the language model, especially in applications where the text input can be cached or is comparatively shorter. Thus, there has been growing interest in developing approaches to compress the visual information into fewer number of tokens via small learnable module. For example, TokenPacker (Li et al., 2024c) learns small cross-attention module over the image tokens to compress them before passing to the LLM. More formally, let the visual encoder be defined as function (I) = X, where Rnd represents sequence of vision embedding tokens produced by the encoder from the input image I. Token compression then learns vision projector gθ(X) = that maps these embeddings to Rmd, compressed sequence of < tokens to be processed by the language model (n = for standard VLMs without any token compression). We refer the reader to Section 5.1 for detailed discussion on some of the recent token compression algorithms. Note that token compression doesnt refer to using smaller visual encoder or using smaller image resolutions as inputs to the encoder. These approaches usually either do not decrease the visual token count much (beyond around 224) or lead to large drops in performance (Li et al., 2024a)."
        },
        {
            "title": "3 TOKENS VS PARAMETERS: INFERENCE TIME SCALING LAWS FOR VLMS",
            "content": "The deployment of vision language models in real-world applications comes with significant challenges, particularly surrounding inference latency and frames per second (FPS). For instance, in realtime systems, such as automotive driver assistance or hazard monitoring, maintaining high FPS and quick response times is crucial for safe and effective deployment. Consequently, reducing inference FLOPs while minimizing downstream performance degradation is of critical, practical importance, especially on consumer-grade edge devices, which are often severely compute constrained. This has led to growing interest in visual token compression for VLMs ( 2.2). Alternatively, one could also use smaller LLM to reduce inference cost. However, both of the above factors directly 3 Preprint. Under Review. influence the downstream performance ( 2.1). This raises key question: Given fixed inference compute budget for VLMs, what is the optimal trade-off between the language model size and the number of visual tokens processed? In our work, we answer this question by developing scaling laws for VLMs that account for the varying parameter count of the language model component and the number of visual input tokens processed by the language model. As mentioned in Section 2.1, we assume the inference cost from the visual encoder to be fixed and ignore it from here on out."
        },
        {
            "title": "3.1 SCALING LAW FORMULATION",
            "content": "Recall that the performance of VLM is primarily governed by the parameter count of the language model and the number of visual tokens processed by the LLM, assuming fixed visual encoder. Accordingly, we model the scaling behavior of VLM performance as: (N, ) = α β + D, (2) where denotes the LLM parameters, denotes the input visual tokens, {A, B, D, α, β} are learnable parameters, and (N, ) is measure of model quality. Although traditional scaling laws have been studied in the context of pretraining loss Kaplan et al. (2020), practitioners often use the direct downstream performance to assess model quality (Gadre et al., 2024; Goyal et al., 2024b; Liu et al., 2022). Thus, we use average downstream error on suite of nine commonly used visual reasoning benchmarks ( 3.2) as measure of model quality (N, ). Below, we summarize the role of each of these learnable parameter in the scaling law (Eq. 2). LLM Quality Parameter (α): This parameter dictates how the downstream error changes with the complexity of the LLM, i.e., its parameter count. larger α indicates better language model, such as Llama3-7B outperforming Llama2-7B, which often stems from better pretraining. Visual Token Quality Parameter (β): β captures the quality of the visual input tokens fed into the LLM, reflecting the quality of the compression technique. more effective token compression algorithm would yield larger β, allowing for more reductions in number of visual tokens than less effective methods while maintaining the same downstream performance. Constants (A, B, D): and are normalizing constants and refers to irreducible loss, which cannot be reduced even with the largest -sized language model or all visual tokens (capped at 576 for our choice of vision encoder). 3.2 EXPERIMENTAL SETUP VLM Training and Evaluation: We use the LLaVA-Next framework (Liu et al., 2024b) to train VLMs with the Qwen-1.5 family of language models as the backbone. Specifically, we utilize the Qwen-{0.5, 1.8, 4, 7, 14}B-chat models (Bai et al., 2023). The pretraining and finetuning dataset and hyperparameters follow Liu et al. (2024a), except we double the effective batch size for finetuning. We use CLIP ViT-L/14 (Radford et al., 2021) as the vision encoder for all experiments, and compress the original 576 tokens to {144, 64, 36, 16, 4, 1} tokens using TokenPacker (Li et al., 2024c). To estimate the downstream error (N, ), we evaluate on 9 commonly used benchmarks for visual reasoning and understanding: MME (Fu et al., 2024), GQA (Hudson & Manning, 2019), AI2D (Kembhavi et al., 2016), MMBench (Liu et al., 2024c), MMMU (Yue et al., 2023), ScienceQA (Lu et al., 2022), MathVista (Lu et al., 2024), POPE (Li et al., 2023c), and ChartQA (Masry et al., 2022). We compute (N, ) by averaging the errors of the normalized evaluation metric. For MME, the Cognition and Perception scores were combined and the F1 scores were used for POPE. Fitting Scaling Laws: We fit the proposed scaling law (Eq. 2) on {Y (N, ), N, } pairs, with {0.5B, 1.8B, 4B, 7B} and {1, 4, 16, 36, 64, 144, 576} (described in the experiment setup above). We use grid-search, for its stability (Goyal et al., 2024b), to estimate the scaling parameters α, β, A, B, and D. The final scaling law is evaluated on = 14B VLM model at various number of visual tokens in . Further details about the grid-search fit can be found in Appendix A.2. 4 Preprint. Under Review."
        },
        {
            "title": "3.3 RESULTS: ESTIMATED SCALING CURVES",
            "content": "Recall from Section 2.1 that LOP sinf = O(N (Q+V )), where represents the input text tokens, and is the visual input tokens. We first visualize our scaling laws under 2 settings (a) cached text input (Fig.1a): The input text tokens (Q) are fixed and can be cached, leading to LOP sinf O(N ), and (b) non-cached text input (Fig.1b): The input text tokens are approximated as 50, i.e., LOP sinf = O(N (50 + )) (we consider more granular variation of in 3.3.2). Figure 1 visualizes the fitted scaling curve, illustrating the variation in the average downstream error as inference FLOPs are varied (under both the cached and non-cached text input setting). We vary the inference FLOPs on the x-axis by increasing the number of visual input tokens processed by the LLM (the scatter size), while the color scale indicates the varying number of language model parameters. We make some key observations below. Log-Linear Relation between Error and Number of Visual Input Tokens: Consider the change in performance for the 7B model as the number of visual input tokens varies (maroon curves in Fig. 1.) Recent works on visual token compression (Li et al., 2024c; Shang et al., 2024) claim little to no performance degradation with token compression. For example, they report similar performance to the base models 576 tokens even when visual token count is reduced to 36 or 144 on certain tasks. However, our scaling curves in Figure 1a reveal different trend, showing log-linear decrease in visual reasoning performance as the number of visual input tokens is reduced. We believe this discrepancy arises because of the limited downstream evaluation benchmarks considered in the previous works which may not fully capture the VLMs overall capabilities. Error Varies 5 Faster with LLM Parameters than with Tokens: Recall from the scaling law (Eq. 2) that α represents the LLM quality parameter and β represents the visual token quality parameter, both denoting the rate at which they influence the downstream error respectively. From Figure 1a, we observe that α = 0.077 is more than five times larger than β = 0.015, signifying that VLM error increases significantly faster when reducing the LLM parameters compared to reducing the number of visual tokens. Therefore, when minimizing inference FLOPs, it is more effective to prioritize reducing visual tokens (V ) first, as the impact on performance is less pronounced than reducing the LLM parameters (N ). This finding is reflected in Figure 4 where we observe that, under fixed inference compute, using larger LLM with fewer visual tokens (7B LM w/ 36 tokens) provides better performance than using smaller LLM with more visual input tokens (1.8B LM w/ 144 tokens) for visual reasoning tasks. Scaling Laws Hold for Increases in LLM Scale: We evaluate the accuracy of our scaling laws (fitted on VLMs of 0.5B-7B range) for predicting the performance for larger models. We estimate the performance of Qwen-1.5 14B using our fitted scaling laws. Our scaling laws estimate the performance with an error margin of less than 2%, as visualized in Figure 2, 6b. The log-linear relationship between the error and number of visual tokens persists, and the greater influence of the LLMs size compared to visual tokens on performance continues to hold. Thus, for VLMs using 7B language model backbones, it is still optimal to increase LLM size to 14B while reducing visual token count for fixed inference costs. Figure 2: Our scaling laws (fitted on VLMs with 0.5-7B LLMs) estimate the performance of 14B LLM VLM with an error margin of less than 2%. 3.3.1 COMPUTE-OPTIMAL INFERENCE REQUIRES SINGLE VISUAL TOKEN Observe the pareto optimal curve (black dotted curve) in Figure 1a. For cached query, at any given inference compute, the optimal behavior, i.e., lowest downstream error, occurs when using the largest possible LLM while reducing the number of visual input tokens to one. Thus, for scenarios where the text input can be cached (Q = 0), such as monitoring systems with static text 5 Preprint. Under Review. (a) Performance trends and trade-offs of VLMs change when varying the number of input text token Q. (b) Scaling laws on OCR-like tasks favor visual token count over LLM size; the opposite of visual reasoning. Figure 3: Adjusting input text token count and benchmark family shifts performance trends. Left (a): For visual reasoning tasks, as the number of text tokens increases, the impact of increasing the number of visual tokens , i.e., reducing compression, becomes more apparent. Intuitively, at enough text tokens, initial increases in visual tokens are only minor fraction of the overall compute ( 3.3.2). Right (b): When tasks are changed from visual reasoning to OCR/textunderstanding, trends reverse: visual token count should now be prioritized over LLM size ( 3.3.3). input, one should utilize the largest LLM possible by reducing the number of visual tokens to fit the inference budget. similar trend of prioritizing the LLM size holds in the variable text input regime. For example, in Figure 1b, where the text input length = 50, better performance in fixed compute budget often results from the larger model with fewer visual tokens with the optimal number of visual tokens is now around 16 (intersection of pareto curve with scaling plot). This increase in optimal visual tokens as text tokens increase is intuitive, as the VLM incurs fixed cost for processing the text. Thus, small increases in the number of visual tokens lead to only minor increases in the overall inference cost while improving performance. The key observation is that compute-optimal behavior entails using the largest feasible LLM with very few visual input tokens. This result has important consequences. Existing literature on token reduction ( 5) has primarily focused on moderately reducing the number of visual input tokens (e.g., from 576 tokens to 144 or 64 tokens) while trying to match the performance of the base model. However, our results highlight that it is better to operate in regime with much lower input visual tokens (e.g., 1, 4 or 16), as exchanging visual tokens for larger LLM size reduces the downstream error. This highlights the need to develop token compression techniques tailored for extreme token compression. We take some initial steps in this direction, building on existing token compression algorithms in Section 4. 3.3.2 VARIATION IN OPTIMAL TOKENS WITH TEXT QUERY LENGTH The shift in performance trends and the ideal visual token count from = 0 50 raises the question; how does the input text length impact the optimal selection of LLM size and number of visual tokens? To explore the variations in trends, we consider the effect of text input length on the optimal inference behavior in Figure 3a. First, when the text input length, Q, is small (purple curves), it is always better to use the larger model (solid curve) with less visual tokens compared to the smaller model (dashed curve) with more visual tokens. However, consider an edge case where the text input length is extremely high (e.g., 100 for the green curves). We observe that there is sharp increase in error as inference FLOPs are reduced. This is because visual tokens need to be reduced significantly for any effective change in inference FLOPs, as the fixed cost from text tokens is quite high. At certain point (marked by the red dot in Figure 3a), it becomes more advantageous to use the 4B model with higher number of visual tokens rather than the 7B model with fewer tokens (contrary to the case for lower Q). Thus, the optimal number of visual input tokens rises with an increase in Q. This case demonstrates the need for careful balancing of visual token 6 Preprint. Under Review. Figure 4: Performances of various LLM size and visual token count combinations at similar inference compute. For visual reasoning tasks, at given fixed inference cost, increasing the LLM size by decreasing the number of visual tokens improves VLM performance. However, for text recognition tasks, decreasing the number of visual tokens is detrimental to performance ( 3.3.3). count and LLM size, especially in scenarios where text inputs are long, to achieve compute-optimal performance without sacrificing accuracy. Despite the changes in the exact optimal visual token count and LLM parameter count as the length of the user query increases, the general trend for visual reasoning and understanding tasks is that increasing the size of the language model while reducing visual tokens can lead to significant relative gains (as also illustrated in Fig. 4). This finding may be due, in part, to the scaling properties of the LLMs, which allow larger models to extrapolate with less visual information than their smaller counterparts (Radford et al., 2021; Wei et al., 2022). However, this trade-off does not extend to certain tasks, such as document comprehension, text identification, etc., where single or handful of tokens may not be able to incorporate the high density of information, and the trend starts to reverse, as we discuss in detail in 3.3.3. Scaling Inference Compute by Simply Repeating Tokens: Many recent works around scaling test-time compute by introducing special tokens (Goyal et al., 2024a) or multiple parallel generations (Zelikman et al., 2024) have shown promising gains in reasoning tasks for language models. We test this notion with VLMs by repeating the visual input tokens (compressed to 4) multiple times to allow for more processing of key visual aspects. However, we do not observe any performance gains. This is most likely due to the fact that the downstream tasks for VLMs are not as reasoningintensive, which demonstrates the importance of developing better token compression algorithms and potentially introducing more challenging benchmarks. 3.3.3 SCALING LAWS FOR OCR TASKS Until now, we have focused on scaling behavior for visual reasoning and understanding tasks, highlighting the key finding that using single visual token with the maximum possible LLM parameters is the inference-optimal configuration. However, is the same valid for all tasks? VLMs have recently been applied to document reading and OCR-style tasks where single visual token may be insufficient due to the high density of information. Unlike visual reasoning tasks, these tasks lack visual structure in the image and intuitively need more tokens to record the (generally textual) details in the image. We verify the same by fitting our scaling laws (Eq. 2) on DocVQA (Mathew et al., 2021) and TextVQA (Singh et al., 2019) benchmarks, where the tasks require mainly OCR capabilities. Figure 3b presents the fitted scaling law for OCR tasks. Notably, there are no significant gains in average downstream performance from increasing LLM parameters; instead, the number of visual tokens predominantly dictates the performance. This observation is reflected in the scaling law parameters, where the LLM-quality parameter α = 0.029 is nearly twice as smaller than the token quality parameter β = 0.048. This trend is in stark contrast to the scaling parameters observed for visual reasoning tasks where the LLM-quality parameter (α) was more than five times larger than 7 Preprint. Under Review. Figure 5: Our query-based convolutional cross-attention (QueCC, pronounced quick) compression technique. User input text tokens are first processed through the LLM backbone to generate text embeddings that are then combined with the visual tokens. Within QueCC, the queryembedded visual tokens are downsampled via convolution. Next, local cross-attention is applied between the downsampled tokens and their respective visual tokens regions. The compressed tokens pass through an MLP before passing into the LLM, alongside input text tokens, for generation ( 4). the token parameter (Fig. 1a). This notion of visual tokens playing the significant role in OCR tasks is further echoed in Figure 4, which shows token compression weakens VLM performance despite increasing the size and capabilities of the LLM component to compensate."
        },
        {
            "title": "4 QUERY-BASED TOKEN COMPRESSION",
            "content": "The Need for Token Compression in Extreme Regimes: While prior work has primarily focused on moderately compressing the tokens (e.g., reducing 576 tokens to 144) while trying to match the performance of the base model (no token compression), our findings ( 3.3.1) suggest the need for paradigm shift. Rather than aiming for moderate token compression, new approaches should be tailored for extreme token reduction down to 1, 4, or 16 tokens with minimal possible degradation, as our scaling laws demonstrate that compute-optimal behavior is within this range. Our work takes initial steps in this direction by introducing query-based token compression strategy designed for such high-compression regimes. In cases where tokens are reduced to as few as 1, token compression based on the users input query becomes critical for retaining relevant information and minimizing performance reductions. In the following section, we build on existing algorithms (Li et al., 2024c), to incorporate query-based token compression. Figure 5 summarizes our query-based convolutional cross-attention (QueCC, pronounced quick) compression technique. User Query Information Injection: To make our projector query-dependent, we add the text embedding of the users most recent query to the image embeddings from the vision encoder. We do this by taking the last hidden states prior to the LM head of the user input from the language model as the representation of the users overall query. The hidden state is converted into the text embedding via linear projection and added to the image visual token embeddings. These fused tokens are later used as the query component for cross-attention. The text embedding can easily be cached for applications where the query is static or is part of predetermined set. Even if the query varies, the text-embedding can be precalculated prior to processing the image and KV values cached and reused when processing the visual and text tokens together during generation. Token Downsampling with Cross-Attention and Learnable Convolutions: To compress the number of visual tokens passed into the LLM, we utilize region-based, cross-attention mechanism that downsamples the vision encoder tokens, X, into more information-dense form. The mechanism hinges on viewing as grid due to the patchification of the image by the vision encoder. Li et al. (2024c;d) passes the 2D version of through downsampling function that compresses the input by s2 factor where each resulting token corresponds to region in the original input. After this, cross-attention is applied independently between each downsampled token and the corresponding tokens in its region. We improve on the bilinear interpolation-based downsampling techniques (Li et al., 2024c; Wang et al., 2024b) by using learnable, depth-wise 2D convolution filter of kernel size and stride s, providing better expressivity. 8 Preprint. Under Review. Method LLaVA-1.5 PruMerge TokenPacker Matryoshka Multi. Matryoshka Query QueCC (Ours) TokenPacker Matryoshka Query QueCC TokenPacker Matryoshka Query QueCC TokenPacker Matryoshka Multi. Matryoshka Query QueCC # Token GQA MMB MME POPE SQA TextVQA VizWiz VQAv 576 32 36 36 36 36 16 16 16 4 4 4 1 1 2 1 62. 64.3 1510.7 85.9 66.8 58.2 50. 57.2* 59.6 60.3 58.8 60.5 60.9 62.8 64.8 63.4 62.5 1350.3 1440.9* 1416.3 1442.0 68.5 76.3 83.3* 71.0* 85.5 81.9 84.5 66.8 70. 58.9* 62.7* 57.6 59.0 61.9 62.2 1378.8* 83.7* 1408.5 1408.0 80.8 83.4 56.2* 53.0 56.5 53.4* 52.6 50.8 53. 61.5* 56.5 62.1 58.7* 59.5 54.4 59.4 1347.6* 1176.1 1390.3 1262.4* 1144.0 1269.1 81.7* 77.6 81.8 80.7* 78.4 74.5 81. 68.1* 67.5 70.7 68.5* 65.1 68.6 69.4* 65.0 69.9 56.0 53.2* 53.3 52.5* 51.3 49.2* 48. 46.2* 46.8 45.2* 50.2 52.8 51.0 50.1 50.5* 49.8 47.7 45.7* 49.4 45.0 41.1* 49.4 48.5 44.1 78. 72.0 75.0 73.7 75.8 74.4* 71.1 74.5 70.5* 64.1 70.6 66.9* 61.0 67.3 Table 1: Comparison of various token compression methods for VLMs at different compression rates. All models use the Vicuna-1.5 7B model as the language backbone. denotes benchmark results for other techniques we evaluated, while best scores are bolded, and second best underlined. Our method outperforms alternatives on almost all benchmarks at extremely high compression regions (visual tokens reduced to 1 or 4) and has strong performance at lower compression rates. 4.1 EXPERIMENTAL SETUP We use training setup similar to LLaVa-1.5 (Liu et al., 2024a) and use Vicuna-1.5 7B as the LLM. Based on the optimality of high token compression underscored by our scaling laws ( 3.3), we focus on visual token budgets of {1, 4, 16, 36, 64}, corresponding to compression rates of 88.9% to 99.8%. We benchmark our method on diverse, comprehensive set of visual reasoning/understanding and OCR/text-understanding tasks: GQA (Hudson & Manning, 2019), MMBench (MMB) (Liu et al., 2024c), MME (Fu et al., 2024), POPE (Li et al., 2023c), ScienceQA (SQA) (Lu et al., 2022), TextVQA (Singh et al., 2019) VizWiz (Gurari et al., 2018), and VQAv2 (Goyal et al., 2017). 4.2 QUERY-BASED CONVOLUTIONAL CROSS-ATTENTION (QUECC) RESULTS Table 1 presents the results of our QueCC algorithm in comparison to previous methods, including TokenPacker (Li et al., 2024c), LLaVa-PruMerge (Shang et al., 2024), Matryoshka Multimodal Models (Cai et al., 2024), and Matryoshka Query Transformer (Hu et al., 2024), in low token regimes. We find that our method performs better than alternatives at the highest compression levels in multiple different data sets, leading to 12% and 19% improvement in the gap between the original LLaVA-1.5 model and the next-best method on MME and MMB for the one-visual-token level. The trend continues at the four-token level, where the gap between QueCC and the next-best algorithm was reduced by 26% and 21% on MME and MMB. Our model exhibits strong performance on GQA, MME, SQA, and VQAv2 across compression rates, signaling the prospects of using the users query to identify and compress key visual tokens."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 TOKEN REDUCTION IN VISION-LANGUAGE MODELS (VLMS) VLMs are composed of three key components: (a) visual encoder that encodes the input images, (b) large language model (LLM) that processes the visual tokens from the encoder along with the user text query, and (c) projector that maps the visual tokens to the input embedding space of the LLM. Section A.1 contains additional details exploring various projector designs. Often, the number of visual tokens (576 tokens per image for CLIP-ViT-L, for instance) significantly exceeds the number of text tokens, leading to high inference costs. This disproportionate scaling of visual tokens also 9 Preprint. Under Review. hinders multi-frame integration due to the limited context length of the model. In addition, inference cost is critical factor in many real-world applications of computer vision systems. Thus, reducing the number of visual tokens processed by the language model has become an active area of research. LLaVA-PruMerge (Shang et al., 2024) and Yu et al. (2024) propose training-free methods that filter out visual tokens (from CLIP) that have low similarity with the CLS token. TokenPacker (Li et al., 2024c), on the other hand, learns compact token compression module using cross-attention over visual tokens, allowing for reduced number of tokens while preserving salient information. While the above approaches focus on token reduction without directly changing the visual encoder (CLIP) output, recent works based on Matryoshka Representation (Cai et al., 2024; Hu et al., 2024) modify the CLIP output directly to generate nested CLIP embeddings for flexible token count. Zhang et al. (2024) investigate methods that emphasize task-relevant pixels during image processing. Another approach to reducing inference cost is adaptive token processing, where the compute dedicated to certain tokens at inference is varied Jain et al. (2024). Most of these methods prune visual tokens within the LLM due to their lower attention scores compared to the prompt, system, etc., tokens (Chen et al., 2024; Wan et al., 2024), heuristic commonly found in regular text-only LLM KV cache reduction techniques (Zhang et al., 2023; Oren et al., 2024). Finally, while we focus our paper on image-based VLMs, host of works (Xu et al., 2024; Shen et al., 2024) discuss token compression for video processing using VLMs. We defer discussion of these to Section A.1. In contrast to the works discussed above that focus on developing and improving token reduction techniques, our work aims to characterize the optimal trade-off between LLM parameters and the number of visual tokens to minimize inference costs. This analysis is crucial for guiding decisions on the extent to which tokens should be reduced relative to LLM parameters given specific target for inference cost reduction. For our inference optimal scaling laws in this work, we use TokenPacker (Li et al., 2024c) for token reduction because of its better downstream performance compared to other options (see Table 1). However, our observations from scaling laws naturally extend to any other competitive token reduction technique. 5.2 SCALING LAWS AND SCALING INFERENCE COMPUTE Understanding how the performance of modern deep networks improves as key design factors, such as the number of parameters or training tokens, are scaled has become focal point of research, particularly as these models continue to grow in size and complexity. Scaling laws offer crucial guidance for optimizing the architecture of such models. Notably, Kaplan et al. (2020); Hernandez et al. (2021); Hoffmann et al. (2022) do thorough investigation into training compute-optimal language models, highlighting the need to scale pretraining tokens and parameters at the same rate. Cherti et al. (2023); Gadre et al. (2023) perform similar study on scaling laws for CLIP (Radford et al., 2021), corroborating that performance improvements arise from increasing both parameter counts and pretraining image-caption pairs. Closest to our work, Li et al. (2024a) investigate what factors improve the performance of LLaVA (Liu et al., 2023). They observe performance gains with increasing language model size, visual encoder size, and input resolution. They investigate each of these factors when scaled independently. In contrast, in this work we focus on understanding the optimal trade-off between language model size and the number of visual input tokens, given fixed inference budget to fit in. Note that in our work, visual input token count is varied (decreased) using token compression algorithms ( 5.1) and not by varying the input image resolution or using different CLIP model. While scaling the pretraining of LLMs has led to emergent capabilities, there has recently been growing interest in improving their reasoning capabilities by scaling inference time compute. Brown et al. (2024) show impressive performance boosts if the language model is allowed multiple attempts on problem. In fact, Snell et al. (2024) show that scaling test time compute by parallel multiple generations at inference gives performance comparable to 14 larger model on math tasks. Goyal et al. (2024a) show performance gains by appending special tokens at the end of input to scale test time compute. In contrast, we characterize the optimal trade-off between tokens and parameters, for getting the best performance at given fixed test time (inference) compute. 10 Preprint. Under Review."
        },
        {
            "title": "6 DISCUSSION AND CONCLUSION",
            "content": "In our work, we demonstrate that the optimal trade-off for VLMs inference is to use very few visual input tokens along with the largest possible LLM that fits within the budget. This result has quite important consequences. Existing works aim towards moderate reduction in token count (e.g., from 576 to 144), while trying to match the performance of the base model (no token reduction). However, our results show that the community needs to focus towards extreme token reduction (e.g., down to 1, 4 or 16 tokens), as the inference optimal regime requires very few visual input tokens. Note that although extreme token reduction can lead to drop in performance compared to the base model, it is still better than using more tokens with smaller LLM. The performance with very few visual tokens is poised to only improve further as we develop token reduction algorithms tailored for extreme reduction. Our work takes an initial step in this direction by proposing input query-based token reduction, as it is better to prioritize visual tokens with information relevant to the text input query, under such an extreme token compression. While our findings are focused on visual token compression at the projector level prior to passing into the LLM, we leave the compute-optimal scaling properties of adaptive token processing algorithms that operate within the LLM component for subsequent work. We hope that these critical insights from our paper will guide future research towards developing better token reduction techniques and thus inference optimal VLMs."
        },
        {
            "title": "7 ACKNOWLEDGEMENTS",
            "content": "We thank Leslie Berberian, Devin Willmott, Qiu Chen, and Vijay Sadashivaiah at the Bosch Center for AI for useful discussions and help with running some of the experiments on Boschs compute. We also thank Albert Gu for his feedback on the draft. KL and SG are supported by funding from the Bosch Center for Artificial Intelligence."
        },
        {
            "title": "REFERENCES",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning, 2022. URL https://arxiv.org/abs/2204.14198. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical report, 2023. URL https://arxiv.org/abs/2309.16609. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Mu Cai, Jianwei Yang, Jianfeng Gao, and Yong Jae Lee. Matryoshka multimodal models, 2024. URL https://arxiv.org/abs/2405.17430. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models, 2024. URL https://arxiv.org/abs/2403.06764. Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, June 2023. doi: 10.1109/cvpr52729.2023.00276. URL http://dx.doi.org/10.1109/CVPR52729.2023.00276. 11 Preprint. Under Review. Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen. Mobilevlm : fast, strong and open vision language assistant for mobile devices, 2023. URL https://arxiv.org/abs/2312. 16886. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023. URL https://arxiv.org/abs/2305.06500. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models, 2024. URL https://arxiv.org/abs/ 2306.13394. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. URL https://arxiv.org/ abs/2304.14108. Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jitsev, Luca Soldaini, Alexandros G. Dimakis, Gabriel Ilharco, Pang Wei Koh, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks, 2024. URL https://arxiv.org/abs/2403.08540. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens, 2024a. URL https://arxiv.org/abs/2310.02226. Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, and J. Zico Kolter. Scaling laws for data filtering data curation cannot be compute agnostic, 2024b. URL https:// arxiv.org/abs/2404.07177. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering, 2017. URL https://arxiv.org/abs/1612.00837. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. Vizwiz grand challenge: Answering visual questions from blind people, 2018. URL https://arxiv.org/abs/1802.08218. Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer, 2021. URL https://arxiv.org/abs/2102.01293. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. Training compute-optimal large language models, 2022. URL https://arxiv.org/abs/ 2203.15556. Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang. Matryoshka query transformer for large vision-language models, 2024. URL https://arxiv. org/abs/2405.19315. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Preprint. Under Review. Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, and Sujoy Paul. Mixture of nested experts: Adaptive processing of visual tokens, 2024. URL https://arxiv.org/abs/2407.19985. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images, 2016. Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng Li, Ziwei Liu, and Chunyuan Li. instruction tuning beyond data?, May 2024a. URL https://llava-vl.github.io/blog/ 2024-05-25-llava-next-ablations/. Llava-next: What else influences visual Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023a. URL https:// arxiv.org/abs/2301.12597. KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding, 2024b. URL https://arxiv. org/abs/2305.06355. Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, and Lei Zhang. Tokenpacker: Efficient visual projector for multimodal llm, 2024c. URL https://arxiv. org/abs/2407.02392. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models, 2023b. URL https://arxiv.org/abs/2311.17043. Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models, 2024d. URL https://arxiv.org/abs/2403.18814. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023c. URL https://arxiv.org/ abs/2305.10355. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In NeurIPS, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2024a. URL https://arxiv.org/abs/2310.03744. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024b. URL https:// llava-vl.github.io/blog/2024-01-30-llava-next/. Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss, better downstream: Implicit bias matters for language models, 2022. URL https://arxiv.org/abs/ 2210.14199. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024c. URL https://arxiv.org/abs/2307.06281. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022. 13 Preprint. Under Review. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In International Conference on Learning Representations (ICLR), 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. URL https://arxiv.org/abs/2203.10244. Minesh Mathew, Dimosthenis Karatzas, and C. V. Jawahar. Docvqa: dataset for vqa on document images, 2021. URL https://arxiv.org/abs/2007.00398. Matanel Oren, Michael Hassid, Nir Yarden, Yossi Adi, and Roy Schwartz. Transformers are multistate rnns, 2024. URL https://arxiv.org/abs/2401.06104. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. Nikhil Sardana, Jacob Portes, Sasha Doubov, and Jonathan Frankle. Beyond chinchilla-optimal: Accounting for inference in language model scaling laws, 2024. URL https://arxiv.org/ abs/2401.00448. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models, 2024. URL https://arxiv.org/abs/ 2403.15388. Leqi Shen, Tianxiang Hao, Sicheng Zhao, Yifeng Zhang, Pengzhang Liu, Yongjun Bao, and Guiguang Ding. Tempme: Video temporal token merging for efficient text-video retrieval, 2024. URL https://arxiv.org/abs/2409.01156. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read, 2019. URL https://arxiv.org/ abs/1904.08920. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https://arxiv.org/ abs/2408.03314. Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023. URL https://arxiv.org/abs/2305.16355. Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan. Look-m: Look-once optimization in kv cache for efficient multimodal long-context inference, 2024. URL https://arxiv.org/abs/2406.18139. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2024a. URL https://arxiv.org/ abs/2311.03079. Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. Longllava: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture, 2024b. URL https:// arxiv.org/abs/2409.02889. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682. Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models, 2024. URL https://arxiv.org/ abs/2404.03384. 14 Preprint. Under Review. Jiaqi Xu, Cuiling Lan, Wenxuan Xie, Xuejin Chen, and Yan Lu. Slot-vlm: Slowfast slots for videolanguage modeling, 2024. URL https://arxiv.org/abs/2402.13088. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. Minicpm-v: gpt-4v level mllm on your phone, 2024. URL https://arxiv.org/ abs/2408.01800. Gaotong Yu, Yi Chen, and Jian Xu. Balancing performance and efficiency: multimodal large language model pruning method based image text interaction, 2024. URL https://arxiv. org/abs/2409.01162. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502, 2023. Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D. Goodman. Quiet-star: Language models can teach themselves to think before speaking, 2024. URL https: //arxiv.org/abs/2403.09629. Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, and Lianwen Jin. Dockylin: large multimodal model for visual document understanding with efficient visual slimming, 2024. URL https://arxiv.org/abs/2406.19101. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, Zhangyang Wang, and Beidi Chen. H2o: Heavyhitter oracle for efficient generative inference of large language models, 2023. URL https: //arxiv.org/abs/2306.14048. Baichuan Zhou, Ying Hu, Xi Weng, Junlong Jia, Jie Luo, Xien Liu, Ji Wu, and Lei Huang. Tinyllava: framework of small-scale large multimodal models, 2024. URL https://arxiv.org/ abs/2402.14289. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. URL https:// arxiv.org/abs/2304.10592. 15 Preprint. Under Review."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL RELATED WORKS A.1.1 VISION PROJECTOR DESIGN To bridge the gap between the separate image and text modalities presented by the vision encoder and language model respectively, vision projectors map the image tokens from the vision encoder into the language space. Many design choices for the projector exist. Numerous VLMs utilize query-based projectors, which combine the embeddings of visual tokens with that of query tokens via cross-attention or similar mechanisms, like the Q-Former projector introduced BLIP-2 (Li et al., 2023a) and used in following work (Dai et al., 2023; Zhu et al., 2023). Other VLMs use simple linear projectors or MLPs to connect the encoder and LLM (Liu et al., 2023; 2024a; Su et al., 2023). While most architectures use the projectors to create new tokens to feed into the LLM alongside text, some architectures like Flamingo (Alayrac et al., 2022) or CogVLM (Wang et al., 2024a) directly interweave the visual information into the language model. In our work, we will be focusing on projectors that fall in the former category. A.1.2 ADDITIONAL APPROACHES FOR EFFICIENT VLMS Apart from reducing the number of visual input tokens to the language model, people have explored various other techniques, including mix of quantization (Liu et al., 2024a) and smaller encoders or language models (Yao et al., 2024; Chu et al., 2023; Zhou et al., 2024) for improving inference. VLMs utilized in video processing often combine decreases in vision encoder output size with token compression techniques to prevent excessive latency and memory constraints. Visual tokens are often merged temporally across frames (Xu et al., 2024; Shen et al., 2024) as well as spatially for individual frames (Xu et al., 2024). Vision encoders, such as Q-Former (Li et al., 2023a), are preferred over more traditional CLIP models due to their ability to extract smaller fixed number of tokens per image (Weng et al., 2024; Li et al., 2024b). Although compression techniques used for video processing often can reduce token counts by large margins, they are rarely evaluated on image datasets, and when they are, compress visual tokens very little or not at all (Li et al., 2023b). A.2 GRID SEARCH DETAILS While there are many choices of optimizer for fitting the scaling laws like curve-fitting in SciPy, gradient descent based solvers, etc. We observed that these are not stable and give varying solutions. We converged to using grid-search to fit the scaling laws, similar to the recent works like Goyal et al. (2024b). The grid-search range for each of the parameters were as follows: α, β {0, 0.1}, A, B, {0, 1}. A.3 ADDITIONAL RESULTS 16 Preprint. Under Review. (a) Scaling law prediction for 14B LLM at = 0. (b) Scaling law prediction for 14B LLM at = 50. Figure 6: Scaling law predictions at various Q. The scaling laws fitted based on LLMs up to the 7B scale generalize well to the 14B scale, resulting in less than 2% error between predicted and actual VLM performance for both = 0 and = 50."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Bosch Center for Artificial Intelligence"
    ]
}