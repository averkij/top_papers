{
    "paper_title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
    "authors": [
        "Chen Chen",
        "Pengsheng Guo",
        "Liangchen Song",
        "Jiasen Lu",
        "Rui Qian",
        "Xinze Wang",
        "Tsu-Jui Fu",
        "Wei Liu",
        "Yinfei Yang",
        "Alex Schwing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 0 0 3 9 1 . 9 0 5 2 : r CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching Chen Chen Pengsheng Guo Liangchen Song Jiasen Lu Rui Qian Xinze Wang Tsu-Jui Fu Wei Liu Yinfei Yang Alex Schwing Apple Inc."
        },
        {
            "title": "Abstract",
            "content": "Conditional generative modeling aims to learn conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters."
        },
        {
            "title": "Introduction",
            "content": "Conditional generative models enable to draw samples conditioned on an external variablefor example, class label, text caption, or semantic mask. They are key technology that has advanced significantly in the last decade, from variational auto-encoders (VAEs) [Kingma and Welling, 2014] and generative adversarial nets [Goodfellow et al., 2014] to diffusion [Ho et al., 2020, Song et al., 2021a,b, Dhariwal and Nichol, 2021, Peebles and Xie, 2023, Rombach et al., 2022, Nichol and Dhariwal, 2021] and flow-matching [Ma et al., 2024, Liu et al., 2023, Lipman et al., 2023, Albergo and Vanden-Eijnden, 2023, Albergo et al., 2023]. State-of-the-art diffusion and flow-matching frameworks accomplish conditional generation by using trained deep net to trace probability path that progressively transforms samples from simple source distribution into the rich, condition-dependent target distribution. popular choice for the source distribution is single, condition-agnostic standard Gaussian, independent of the condition. Consequently, the conditioning signal enters only through the network itself: in flow matching, for instance, the model predicts velocity field where the condition is commonly incorporated via embeddings or adaptive normalization layers. Although this strategy has enabled impressive results, this design forces the network to shoulder two tasks simultaneously(i) transporting probability mass to the correct region of the data manifold, and (ii) encoding the semantic meaning of the condition. Because different conditions often occupy distant parts of that manifold, the dual burden stretches the learned trajectory, slows convergence, and can impair sample quality and diversity. Work done while at Apple. Preprint. Under review. pinit f (, y) pinit ( y) vθ(xt, t, y) pdata ( y) x1 g1(, y) g(, y) vθ(zt, t, y) pdata ( y) z0 z1 Figure 1: Condition-Aware Reparameterization for Flow Matching (CAR-Flow). Illustration of the push-forward under standard conditional flow matching (direct mapping x0 x1) versus our Condition-Aware Reparameterization (CAR-Flow) chain (x0 z0 z1 x1). In the standard setting, condition-agnostic prior sample (red) is carried by the networks velocity field directly to each condition-dependent data manifold (blue), forcing it to juggle long-range transport and semantic injection at once. CAR-Flow, in contrast, employs lightweight source distribution map (, y) and target distribution map g(, y) to align the source and target distributions to relieve the network of unnecessary transport. During sampling, x1 is obtained via the (approximate) inverse map g1(, y). In this paper, we alleviate this burden by letting the distributions themselves depend on the condition. First, we endow the source distribution with condition-awareness through lightweight map, i.e., the source distribution map. Hence, every condition has its own source distribution. The same idea can be mirrored to the target distribution using another lightweight map, i.e., the target distribution map, that we require to be approximately invertible to keep sampling tractable. The resulting push-forward chain is illustrated in Figure 1. Intuitively, the target distribution map and its approximated inverse correspond to the encoder and decoder, commonly used in latent diffusion pipelines [Rombach et al., 2022]. Differently, here, the target distribution map is explicitly conditional, aligning our formulation with recent latent-space conditioning approaches that embed semantic information directly into the latent representation [Wu et al., 2024, Leng et al., 2025, Yao et al., 2025]. Although fully general maps provide maximal flexibility, this freedom conceals critical failure mode: the flow-matching objective admits zero-cost solutions. The predicted data distribution then collapses to single mode. comparable drop in generation quality has been observed in recent work when variational auto-encoder (VAE) is naively fine-tuned end-to-end with diffusion model [Leng et al., 2025]. We formalize these collapse routes and later verify them experimentally. To preclude this trivial solution, we impose the simplest effective constraintshift-only conditioning. Concretely, we reparameterize the maps to only translate. Relocating the start and/or end points while leaving scales untouched removes all trivial solutions, and still shortens the residual probability path. We refer to the resulting framework as condition-aware reparameterization for flow matching (CAR-Flow). CAR-Flow can be applied to the source distribution, the target distribution, or both. Each variant shapes the learned path differently, and we find the joint version to perform best in practice. For both low-dimensional synthetic benchmarks and high-dimensional natural image (ImageNet-256) data, CAR-Flow consistently improves performance: augmenting the strong SiT-XL/2 baseline [Ma et al., 2024] with CAR-Flow reduces FID from 2.07 to 1.68 while adding fewer than 0.6% additional parameters. Our contributions can be summarized as follows: 1. We introduce CAR-Flow, simple yet powerful shift-only mapping that aligns the source, the target, or both distributions with the conditioning variable. CAR-Flow relieves the velocity network of unnecessary transport while adding negligible computational overhead. 2 2. We provide theoretical analysis that uncovers zero-cost solutions present under unrestricted reparameterization and prove that they render the velocity field parameter-independent, thereby explaining the empirical failures of naïve end-to-end VAEdiffusion training. 3. We validate CAR-Flow on low-dimensional synthetic data and the high-dimensional ImageNet-256 benchmark, consistently outperforming the standard rectified flow baseline."
        },
        {
            "title": "2 Background and Preliminaries",
            "content": "We start with brief review of the standard flow-matching formulation. 2.1 Conditional Generation and Probability Paths Conditional generation seeks to sample from conditional data distribution x1 pdata ( y), where is conditioning variable, e.g., class label or text prompt. Diffusion and flow-based models tackle this problem by simulating differential equation that traces probability-density path pt(x1 x0, y), gradually transporting samples from simple source distribution x0 pinit into the target conditional distribution. standard choice for the source distribution is the isotropic Gaussian pinit = (0, Id). The stochastic trajectory (Xt)0t1 follows the SDE dXt = (cid:20) ut(Xt, y) + (cid:21) log pt(Xt y) σ2 2 dt + σt dWt, (1) where ut is the drift field, σt is the diffusion coefficient, and Wt is standard Wiener process. The term log pt(Xt y) denotes the score function, which can be written in terms of the drift field ut [Ma et al., 2024]. In the deterministic limit σt = 0, this SDE reduces to the ODE dXt = ut(Xt, y) dt. 2.2 Gaussian Probability Paths convenient instantiation is the Gaussian path. Let αt and βt be two continuously differentiable, monotonic noise scheduling functions satisfying the boundary conditions α0 = β1 = 0 and α1 = β0 = 1. At time t, the conditional distribution is pt( x1, y) = (cid:0)αtx1, β2 Id y(cid:1), (2) whose endpoints are p0( x1, y) = (0, Id) and p1( x1, y) = δx1y. Here δ denotes the Dirac delta distribution. Along this path, the state evolves by the interpolant xt = βtx0 + αtx1, with velocity field ut = βtx0 + αtx1, (3) where overdots denote time derivatives. 2.3 Conditional Flow Matching and Sampling Conditional flow matching trains neural velocity field vθ(x, t, y) to approximate the true velocity ut specified in Eq. (3). The commonly employed objective is L(θ) = ypY , tpT x0pinit , x1pdata (cid:13) (cid:13) (cid:13)vθ (cid:0)βtx0 + αtx1, t, y(cid:1) (cid:0) βtx0 + αtx1 (cid:1)(cid:13) 2 (cid:13) (cid:13) , (4) where pY is the marginal distribution over conditions y, and pT is time density on [0, 1]. After training, one draws x1 pdata ˆut = vθ(xt, t, y). solver-agnostic procedure is outlined in Algorithm 1. by numerically integrating Eq. (1) from = 0 to = 1 with 3 Algorithm 1 Sampling via conditional flow matching (standard Gaussian source) Require: trained network vθ; diffusion schedule {σt}t[0,1]; number of steps 1: Sample pY 2: Sample x0 (0, Id) 3: 1/N ; x0 4: for = 0 to 1 do 5: 6: 7: 8: end for 9: return t vθ(x, t, y) Integrate SDE in Eq. (1) over [t, + t] with (u, σt) /* drift */ /* e.g., EulerMaruyama */ /* sample at = 1 conditioned on */"
        },
        {
            "title": "3 Condition-Aware Reparameterization",
            "content": "As detailed in Section 2, standard conditional flow matching initiates the probability path from condition-agnostic prior, typically standard Gaussian. Consequently, the velocity network vθ(xt, t, y) must simultaneously learn two intertwined taskstransporting mass and encoding semantics, imposing dual burden. To alleviate this, we reparameterize the source and/or target distributions via explicit functions of the condition y. In this section, we first reformulate conditional flow-matching using general reparameterizations (Section 3.1). We then show that allowing arbitrary reparameterizations leads to trivial zero-cost minima, collapsing distributions to single mode (Section 3.2). To both shorten the transport path and eliminate these trivial solutions, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) (Section 3.3). 3.1 General Reparameterization Formally, instead of drawing an initial value x0 directly from the fixed source distribution pinit , we first apply condition-dependent source distribution map : Rn Rm, (x, y) (cid:55) such that z0 = (x0, y), x0 pinit . (5) We hence obtain sample from the modified source distribution pinit pinit Similarly, we define the target distribution map : Rn Rm, (x, y) (cid:55) such that ( y) = (, y)#pinit . via the push-forward z0 z1 = g(x1, y), x1 pdata ( y). (6) ( y) = g(, y)#pdata via the push-forward z1 ( y). Critically, to make sampling tractable, must be approximately We characterize sample from the target distribution in latent space pdata pdata invertiblei.e., we assume there exists g1 such that x1 g1(z1, y). It is worth noting that our reparameterization framework subsumes both the classic VAE-based latent diffusion model [Rombach et al., 2022] and more recent efforts to inject semantic awareness directly into the latent space, e.g., work by Leng et al. [2025]. To recover the standard latent diffusion model, we leave the source untouched, i.e., (x0, y) = x0, and set g(x1, y) E(x1), g1(z1, y) D(z1), where E, are the encoder and decoder of VAE trained via the ELBO (reconstruction loss plus KL divergence) without explicit semantic supervision. In contrast, more recent works [Yu et al., 2025, Leng et al., 2025, Yao et al., 2025] augment VAE training with semantic alignment lossoften using features from pretrained DINO-V2 networkso that the encoder and decoder effectively depend on semantic embeddings of y. Both paradigms arise naturally as special cases of our general push-forward reparameterization framework. Moreover, our framework readily accommodates further reparameterization of any pretrained VAE. Concretely, one can introduce an invertible map : Rm Rm and set g(x1, y) g(cid:0)E(x1), y(cid:1), g1(z1, y) D(cid:0) (g)1(z1, y)(cid:1), (7) thus embedding semantic conditioning directly into both the encoder and decoder. Flow-matching loss and sampling under reparameterization. When pinit ( y) is Gaussian, the probability path z0 z1 remains Gaussian under the interpolation zt = βtz0 + αtz1, ut = 4 Algorithm 2 Sampling via conditional flow matching (reparameterized) Require: trained vθ; diffusion schedule {σt}; steps 1: Sample pY 2: Sample x0 (0, Id) and set (x0, y) 3: 1/N 4: for = 0 to 1 do 5: 6: 7: 8: end for 9: g1(z, y) 10: return t vθ(z, t, y) Integrate SDE in Eq.(8) over [t, + t] with (u, σt) /* invertible needed here */ /* sample at = 1 conditioned on */ βtz0 + αtz1. The evolution of Zt then follows the SDE dZt = (cid:20) ut(Zt, y) + (cid:21) log pt(Zt y) σ2 2 dt + σt dWt, (8) where pt is the Gaussian path in z-space. Note that the conditional score functions generally differ: log pt(Zt y) = log pt(Xt y), unless (x0, y) = x0 (see Appendix for details). The resulting flow-matching loss becomes L(θ) = ypY , tpT x0pinit , x1pdata (cid:13) (cid:13) (cid:13)vθ (cid:0)βtz0 + αtz1, t, y(cid:1) (cid:0) βtz0 + αtz (cid:1)(cid:13) 2 (cid:13) (cid:13) . (9) Once trained, sampling proceeds by first drawing an initial starting point x0 pinit and computing the condition-aware latent z0 = (x0, y). We then integrate the reparameterized SDE in Eq. (8) from = 0 to = 1 to obtain z1, and finally map back to data space via x1 = g1(z1, y). Algorithm 2 provides pseudo-code that summarizes the process. 3.2 Mode Collapse under Unrestricted Reparameterization Under the general reparameterization framework of Eq. (9), the maps and enjoy full flexibility. Unfortunately, this expressivity also admits several trivial zero-cost minima: analytically, the loss can be driven to zero by degenerate shift solutions, resulting in collapse of the generative distribution to single/improper mode. Claim 1. Let f, : Rn Rm be arbitrary maps. If any of the following holds (for some functions c(y) Rm or scalars k(y) R): (i) Constant source: (x0, y) = c(y) for all x0, (ii) Constant target: g(x1, y) = c(y) for all x1, (iii) Unbounded source scale: (x0, y) , (iv) Unbounded target scale: g(x1, y) , (v) Proportional collapse: (x0, y) = k(y) g(x1, y), then the flow-matching loss in Eq. (9) admits trivial minimum in which the optimal velocity field takes the form vθ(zt, t, y) = γ(t, y) zt + η(t, y), causing all probability mass to collapse to single/improper mode. The proof of Claim 1, together with closed-form expressions for γ(t, y) and η(t, y), is deferred to the Appendix B. To illustrate the collapse concretely, we consider the linear schedule βt = 1 t, αt = and an affine reparameterization (x0, y) = σ0(y) x0 + µ0(y), g(x1, y) = σ1(y) x1 + µ1(y), (10) analogous to the standard Gaussian trick. Table 1 summarizes each collapse mode, listing the maps, the closed-form collapsed velocity v(z, t, y) = γ(t, y) + η(t, y), and the resulting push-forward distributions pinit . and pdata z 5 Table 1: Zero-cost collapse modes under the linear schedule αt = t, βt = 1 t, using affine maps (x0) = σ0x0 + µ0 and g(x1) = σ1x1 + µ1. For brevity, we omit explicit y-dependence. η(t) Case γ(t) pdata (i) (ii) (iii) (iv) (v) µ0 arbitrary arbitrary µ0 arbitrary µ1 1 1 1t arbitrary 1 1t 1 0 kµ 1 µ0 1 1t µ1 0 0 (k 1)µ0 v(zt, t) ztµ0 µ1zt 1t zt 1t zt (k 1)µ0 pinit δµ0 Uniform(Rd) δµ0 δµ1 Uniform(Rd) δµ1 or pdata We note that in the constant-map scenarios (cases (i) and (ii)), the affine transforms or effectively collapse the variance of pinit to zero. In particular, case (ii) mirrors the finding in REPA-E  (Table 1)  [Leng et al., 2025], where end-to-end VAEdiffusion tuning favored simpler latent space with reduced variance. In contrast, the unbounded-scale modes (iii) and (iv) push the distributions toward an improper uniform distribution, eliminating any meaningful localization. Proportional , pdata collapse (v) yields degenerate flow with pinit Empirically, we do not observe cases (iii)(v). In particular, cases (iii) and (iv) correspond to unbounded collapse solutions: when the scale of either the source or target distribution tends to infinity, the counterpart distribution collapses relative to it, yielding zero cost. These solutions require unbounded weights and are therefore unstableany perturbation pushes the optimization back toward cases (i) or (ii). Case (v), in contrast, assumes exact proportionality between maps and g, which cannot occur under independently sampled inputs except in trivial constant-map settings that reduce to cases (i) or (ii). In practice, the optimizer thus follows the easiest shortcutthe constant-map collapsesince setting or to fixed constant immediately zeroes the loss. In Section 4, we empirically verify not only that such constant-map collapses occur in real models, but also how these mode-collapse behaviors manifest in practice. , and the velocity field being constant. 3.3 Shift-Only Condition-Aware Reparameterization To eliminate the trivial zero-cost solutions while retaining the benefits of condition-aware reparameterization, we restrict both reparameterizations to additive shifts only, i.e., we use (x0, y) = x0 + µ0(y), (11) Here, µ0, µ1 : Rm are lightweight, learnable, condition-dependent shifts. By preserving scale, we block every collapse mode given in Claim 1, yet we still move z0 and z1 closer in latent space. In particular, when used with pretrained VAE, Eq. (7) becomes g(x1, y) = x1 + µ1(y). g(x1, y) E(x1) + µ0(y), g1(z1, y) D(cid:0)z1 µ1(y)(cid:1), (12) CAR-Flow admits three natural variants: Source-only: µ1 0, so only the source is shifted. Target-only: µ0 0, so only the target is shifted. Joint: both µ0 and µ1 active. Each variant alters the probability path zt = βt (x0 + µ0(y)) + αt (x1 + µ1(y)) in distinct way. Note that shifting the source cannot be replicated by an opposite shift on the target. In fact, no nonzero constant shift on the source can ever be matched by constant shift on the targetonly except in the trivial case: Claim 2. Shifting the source by µ0 is equivalent to shifting the target by µ1, if only if µ0 = µ1 = 0. Proof. With source-only shift (µ0, µ1) = (µ0, 0), the interpolant is z(s) With target shift (µ0, µ1) = (0, µ1), it is z(t) t, βt µ0 = αt µ1. Since µ0 and µ1 are t-independent functions of y, it forces µ0 = µ1 = 0. = βt (x0 + µ0) + αt x1. = βt x0 + αt (x1 + µ1). Equating these gives for For Source-only, the interpolant simplifies to zt = xt + βtµ0(y). When µ0(y) µ0(y) for two conditions y, y, their paths share common starting region, simplifying the early-time flow. For Target-only, the trajectory becomes zt = xt + αt µ1(y). When µ1(y) µ1(y), the network only 6 (a) Baseline (b) Source-only (c) Target-only (d) Joint Figure 2: Learned flow trajectories on 1D synthetic data. Each panel shows trajectories from source x0 (bottom) to target x1 (top) for (a) baseline and CAR-Flow variants(b) source-only, (c) target-only, and (d) joint. Intermediate stages z0 and z1 reflect reparameterized coordinates. Colored densities represent predicted and ground-truth class distributions (red/blue: prediction; magenta/cyan: ground truth). Thin lines illustrate individual sample trajectories between z0 and z1. Dashed vertical lines mark 3σ for each shift. The source-only CAR-Flow relocates the source distribution per class, while the target-only variant unifies the trajectory endpoints. The joint variant combines both and achieves the best alignment and flow quality. needs to learn shared landing zone, easing the late-time flow. For Joint, we have zt = xt + µt(y) where µt(y) = βt µ0(y) + αtµ1(y), so the time-varying shift aligns both endpoints, minimizing the overall transport distance and reducing the burden on vθ throughout the entire trajectory. Empirically, we find that the joint CAR-Flow variantallowing both µ0 and µ1 to adaptyields the largest improvements in convergence speed and sample fidelity (see Section 4)."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we evaluate the efficacy of the Condition-Aware Reparameterization (CAR) under linear noise schedule βt = 1 t, αt = and compare to classic rectified flow. All experiments are done using the axlearn framework.2. Detailed implementation settings can be found in Appendix C. 4.1 Synthetic Data For our synthetic-data experiments, we consider one-dimensional task where the source distribution is (0, 1) and the target distribution is two-class Gaussian mixture, with class as (1.5, 0.22) and class as (+1.5, 0.22). We encode xt, y, and using sinusoidal embeddings before feeding them to the network. In the baseline rectified-flow model, these embeddings are concatenated and passed through three-layer MLP (1,993 parameters total) to predict the velocity field vθ(xt, y, t). Training uses the loss in Eq. (4), and sampling follows Algorithm 1. For CAR-Flow, we augment this backbone with two lightweight linear layers that map the class embedding to the shifts µ0(y) and/or µ1(y), each adding only 9 parameters. In the source-only variant we predict µ0 (with µ1 = 0); in the target-only variant we predict µ1 (with µ0 = 0); and in the joint variant we predict both. These shifts are applied via Eq. (11), training proceeds with the loss in Eq. (9), and sampling uses Algorithm 2. All modelsbaseline and CAR variantsset σt = 0 (reducing the SDE to an ODE) and employ 50-step Euler integrator for sampling. To ensure robustness, each configuration is run three times, and we report the average performance, noting that variance across runs is negligible. Figure 2 shows how CAR-Flow alters the learned flow. In Figure 2a, the baseline must both transport mass and encode class information, yielding the longest paths. The source-shift variant in Figure 2b cleanly relocates each classs start, the target-shift variant in Figure 2c leaves the source untouched 2https://github.com/apple/axlearn 7 Table 2: Average trajectory length z0 z1 with 2σ error bounds. Baseline Source-only CAR-Flow Target-only CAR-Flow Joint CAR-Flow"
        },
        {
            "title": "Length",
            "content": "1.5355 0.0024 0.7432 0.0019 0.7129 0.0010 0.7121 0.0011 (a) Wasserstein distance (b) Learned µ0 and µ1 shifts Figure 3: Comparison of convergence and learned shifts. (a) shows the Wasserstein distance between predicted and ground-truth distributions in symlog-scale. Joint CAR-Flow achieves both the fastest convergence (b) plots the evolution of the learned shifts µ0 (top) and µ1 (bottom) for two classes. and merges endpoints, and the joint variant in 2d aligns both start and end with minimal source shiftproducing the shortest trajectories  (Table 2)  . Figure 3a summarizes convergence measured by Wasserstein distance over training: joint CAR-Flow converges fastest and reaches the lowest error, followed by source and target-only, all outperforming the baseline. Figure 3b traces the learned µ0 and µ1 for each class and variant: joint CAR-Flow yields the most moderate, balanced shifts, explaining its superior convergence and flow quality. Mode Collapse. To empirically validate the mode-collapse analysis described in Section 3.2, we reuse the setup but now allow both shift and scale parameters to be learned simultaneously (Eq.(10)). We train two separate models: one reparameterizing the source distribution with learned parameters (µ0, σ0), and another morphing the target distribution with (µ1, σ1). Results are presented in Figure 4. Specifically, Figure 4a shows the rapid evolution of the learned standard deviations σ, clearly indicating the network quickly discovers shortcut solution by shrinking σ to zero. Figure 4b plots the expected norm gap Evθ v2, demonstrating convergence to zero, which indicates the networks velocity prediction aligns closely with the analytic zero-cost solutions derived in Table 1. Moreover, unrestricted parameterization of the source distribution triggers mode-collapse case (i) as described in Claim 1, where the flow degenerates to predicting the class-wise mean of the target (see Figure 4c). Conversely, unrestricted parameterization of the target collapses the distribution nearly to constant, and consequently, the predicted x1 degenerates into an improper uniform distribution due to σ1 0, as illustrated in Figure 4d. 4.2 ImageNet To benchmark on high-dimensional, large-scale dataset, we conduct experiments on ImageNet 256 256 data using v6e-256 TPUs. Our baseline is SiT-XL/2 [Ma et al., 2024], re-implemented in JAX [Bradbury et al., 2018]; we strictly follow the original training recipe from the open-source SiT repository to replicate the results reported in the paper. For our CAR variants, we apply the shift-only reparameterization to the sd-vae-ft-ema VAE backbone used by SiT (see Eq. (11)- (12)). We introduce two lightweight convolutional networks ( 2.3M parameters each) to predict µ0 and µ1 from the class embeddings, projecting them into the latent space. All models are sampled using the Heun SDE solver with 250 NFEs. Table 3 presents the quantitative results. Augmenting SiT-XL/2 with CAR-Flow consistently outperforms the baseline across all variants. In particular, the joint-shift variant achieves the best result, reducing FID from 2.07 to 1.68 while adding fewer than 0.6% parameters. These results underscore the importance of explicitly conditioning the source and target distributions: simple shift reparam8 (a) Learned scale σ (b) Validation error (c) Source-only (d) Target-only Figure 4: Mode collapse diagnostics with scale reparameterization. (a)(b) Evolution of learned σ and validation error. (c)(d) Learned flows when allowing shift+scale on source vs. target. Table 3: Benchmarking class-conditional image generation on ImageNet 256 256. All CAR-Flow variants surpass the SiT-XL/2 baseline. Method Params (M) Training Steps FID IS sFID Precision Recall SiT-XL/2 SiT-XL/2CAR-Flow Source-only SiT-XL/2CAR-Flow Target-only SiT-XL/2CAR-Flow Joint SiT-XL/2cfg=1.5 SiT-XL/2CAR-Flow Joint+cfg=1.5 675 677 677 679 675 679 400K 400K 400K 400K 7M 7M 17.28 78.88 15.15 83.42 14.44 85.66 13.91 87. 2.07 280.2 1.68 304.0 5.71 5.56 5.59 5.38 4.46 4.34 0.67 0.68 0.68 0.68 0.81 0.82 0.61 0.61 0.61 0. 0.58 0.62 eterization not only improves sample fidelity but does so with minimal computational overhead, facilitating easy integration into existing large-scale generative frameworks. Table 3 presents the quantitative results. Augmenting SiT-XL/2 with CAR-Flow consistently outperforms the baseline across all variants. In particular, the joint-shift variant achieves the best result, reducing FID from 2.07 to 1.68 while adding fewer than 0.6% parameters. Beyond final performance, CAR-Flow also accelerates optimization: our convergence analysis on ImageNet-256 shows that all CAR-Flow variants consistently reduce FID faster than the baseline across training steps (see Fig. 5). These results underscore the importance of explicitly conditioning the source and target distributions: simple shift reparameterization not only improves sample fidelity but does so with minimal computational overhead, facilitating easy integration into existing large-scale generative frameworks."
        },
        {
            "title": "5 Related Work",
            "content": "Generative modeling has advanced significantly in the last decade from variational auto-encoders (VAEs) [Kingma and Welling, 2014], Generative Adversarial Nets [Goodfellow et al., 2014], and normalizing flows [Rezende and Mohamed, 2015]. More recently, score matching [Song and Ermon, 2019, Song et al., 2020], diffusion models [Ho et al., 2020], and flow matching [Liu et al., 2023, Lipman et al., 2023, Albergo and Vanden-Eijnden, 2023, Albergo et al., 2023] was introduced. The latter three frameworks are related: sampling at test-time can be viewed as numerically solving transport (ordinary) differential equation by integrating along learned velocity field from the source distribution at time zero to the target distribution at time one. For learning the velocity field, various approaches to interpolate between samples from the source distribution and the target distribution have been discussed [Lipman et al., 2023, Liu et al., 2023, 9 Figure 5: Convergence on ImageNet 256 256: FID vs. training steps. CAR-Flow variants consistently converge faster than the baseline. Tong et al., 2024]. Among those, rectified flow matching was shown to lead to compelling results on large-scale data [Ma et al., 2024, Esser et al., 2024]. For use on large-scale data, flow matching is typically formulated in latent space by compressing data via the encoder of pre-trained and frozen VAE [Rombach et al., 2022]. These mappings differ from the discussed CAR-Flow, as they are typically independent of the conditioning variable. As mentioned before, CAR-Flow can be applied on top of pre-trained and frozen projections on the latent space. More recently, Yu et al. [2025], Yao et al. [2025] proposed to align representations within deep nets that model the velocity to visual representations from vision foundation models. Specifically, Yu et al. [2025] align early layer features of DiT [Peebles and Xie, 2023] and SiT [Ma et al., 2024] models with representations extracted from DINOv2 [Oquab et al., 2024] and CLIP [Radford et al., 2021]. In contrast, Yao et al. [2025] aligns the latent space of VAE with representations from pre-trained vision foundation models, which are then frozen for diffusion model training. These approaches differ from our approach, which learns to transform the source and target distributions rather than encouraging feature alignment. Most related to our work is the recently introduced REPA-E [Leng et al., 2025]. In REPA-E, Leng et al. [2025] study end-to-end training of diffusion models and VAE encoders/decoders, which map data to/from latent space. Differently, in this paper, we formalize failure modes reported by Leng et al. [2025] and identify them as trivial solutions that arise when jointly training flow matching model and target distribution mapping. We further introduce source distribution mapping. Finally, we impose simple restrictions that preclude those trivial solutions."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose and study condition-aware reparameterization for flow matching (CAR-Flow), which aligns the source and target distributions in flow-matching models. We find CAR-Flow to alleviate the burden of classic flow-matching, where model simultaneously transports probability mass to the correct region of the data manifold, while also encoding the semantic meaning of the condition. Limitations and broader impact. This work characterizes the failure modes when jointly training flow matching model, source distribution mapping, and target distribution mapping, and identifies them as trivial solutions of the objective. We further study the simplest effective approach to avoid these trivial solutions. While we find this simple approach to lead to compelling results, we think more general mappings will likely improve results even further. We leave the identification of more general suitable mappings to future work. Improving the expressivity of generative models has significant broader impact. On the positive side, modeling complex distributions more easily saves resources and enables novel applications. On the negative side, efficient generative modeling can be abused to spread misinformation more easily."
        },
        {
            "title": "Acknowledgements",
            "content": "We begin by thanking Byeongjoo Ahn, Wenze Hu, Zhe Gan, and Jason Ren for their thoughtful discussions and rapid feedback, which significantly shaped this study. We also acknowledge the Apple Foundation Model team for providing the critical infrastructure that powered our experiments Finally, we are profoundly appreciative of Ruoming Pang and Yang Zhao for their strategic vision, steadfast guidance, and unwavering encouragement throughout this project."
        },
        {
            "title": "References",
            "content": "M. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In Proc. ICLR, 2023. M. Albergo, N. Boffi, and E. Vanden-Eijnden. Stochastic interpolants: unifying framework for flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, JAX: composable transformations of J. VanderPlas, S. Wanderman-Milne, and Q. Zhang. Python+NumPy programs, 2018. URL http://github.com/jax-ml/jax. P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, In F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. Forty-first international conference on machine learning, 2024. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Proc. NeurIPS, 2014. J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. D. Kingma and M. Welling. Auto-Encoding Variational Bayes. In Proc. ICLR, 2014. X. Leng, J. Singh, Y. Hou, Z. Xing, S. Xie, and L. Zheng. Repa-e: Unlocking vae for end-to-end tuning with latent diffusion transformers, 2025. URL https://arxiv.org/abs/2504.10483. Y. Lipman, R. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow matching for generative modeling. In Proc. ICLR, 2023. X. Liu, C. Gong, and Q. Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In Proc. ICLR, 2023. N. Ma, M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-Eijnden, and S. Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European Conference on Computer Vision, pages 2340. Springer, 2024. A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pages 81628171. PMLR, 2021. M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, M. Assran, N. Ballas, W. Galuba, R. Howes, P.-Y. Huang, S.-W. Li, I. Misra, M. Rabbat, V. Sharma, G. Synnaeve, H. Xu, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski. Dinov2: Learning robust visual features without supervision, 2024. URL https://arxiv.org/abs/2304.07193. W. Peebles and S. Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language supervision, 2021. URL https://arxiv.org/abs/2103.00020. D. Rezende and S. Mohamed. Variational inference with normalizing flows. In Proc. ICML, 2015. R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In Proc. ICLR, 2021a. Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In Proc. NeurIPS, 2019. 12 Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Y. Song, J. Sohl-Dickstein, D. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In Proc. ICLR, 2021b. A. Tong, N. Malkin, G. Huguet, Y. Zhang, J. Rector-Brooks, K. Fatras, G. Wolf, and Y. Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. TMLR, 2024. Y. Wu, Z. Zhang, J. Chen, H. Tang, D. Li, Y. Fang, L. Zhu, E. Xie, H. Yin, L. Yi, et al. Vilau: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. J. Yao, B. Yang, and X. Wang. Reconstruction vs. generation: Taming optimization dilemma in latent diffusion models, 2025. URL https://arxiv.org/abs/2501.01423. S. Yu, S. Kwak, H. Jang, J. Jeong, J. Huang, J. Shin, and S. Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. In Proc. ICLR, 2025. 13 Appendix CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching The appendix is organized as follows: In Section we derive the effects of our reparameterization on the score function. This is important for correct sampling, particularly when using an SDE solver. In Section we prove Claim 1. In Section we provide additional implementation details for both synthetic and ImageNet experiments. In Section we discuss some findings. In Section we illustrate additional qualitative results."
        },
        {
            "title": "In this section we show that",
            "content": "1. The conditional score st = log p(zt y) generally changes, once the source-space map is applied, unless (x, y) = x; 2. For the practically important shift-only map (x, y) = + µ0(y) under Gaussian path, both the density and the score admit closed forms; and 3. The score couples to the drift (velocity field) ut that appears in the SDE used for sampling. A.1 Change of the Conditional Score Recall that the source transform is : Rn Rm, (x, y) (cid:55) z. Let Jf (x, y) Rmn denote its Jacobian. By the change-of-variables formula, Taking log of Eq. (13) gives pX (x y) = pZ (cid:0)f (x, y) y(cid:1) (cid:12) (cid:12)det Jf (x, y)(cid:12) (cid:12). log pX (x y) = Jf (x, y) log pZ (cid:0)f (x, y) y(cid:1) (cid:125) (cid:123)(cid:122) score in z-space (cid:124) +x log(cid:12) (cid:12)det Jf (x, y)(cid:12) (cid:12) . (13) (14) The first term transports the z-space score back to the x-space tangent via the Jacobian transpose; the second term corrects for the local volume change introduced by . If is volume preserving, det Jf 1 and hence log det Jf = 0. Volume-preserving maps. Eq. (14) then reduces to log pX = z log pZ. In the special case of (x, y) = x, one has Jf = and therefore log pX = log pZ, recovering the classical setting without reparameterization. A.2 Shift-only Transform Assume the initial distribution pinit = (0, Id) is standard Gaussian and is shift-only map, i.e., (x, y) = + µ0(y). Along Gaussian path parameterized by αt, βt (with boundary conditions α0 = 0, β0 = 1 and α1 = 1, β1 = 0), the conditional density at time is Id y(cid:1), pt( z1, y) = (cid:0)αtz1 + βtµ0, β (15) with both endpoints being p0( z1, y) = (cid:0)µ0, Id y(cid:1), p1( z1, y) = δz1y. (16) Consequently, by using the form of the Gaussian probability density, the (conditional) score reads st(zt z1, y) = zt log pt(zt z1, y) = αtz1 + βtµ0(y) zt β 2 . (17) 14 A.3 Link Between Score and Velocity Field Fix conditioning label and pair (z0, z1) drawn from the Gaussian endpoint distributions introduced in Section. 3.1. The flow is defined as ψt(z0 z1, y) = βt z0 + αt z1, 0 1, (18) (cid:0)ψt z1, y(cid:1), differentiating so that ψ0 = z0 and ψ1 = z1. Because ψt satisfies the ODE dψt/dt = ut Eq. (18) in gives the conditional velocity field ut(αtz1 + βtz0 z1, y) = βt z0 + αt z1. Since zt = ψt(z0 z1, y), z0 = (zt αtz1)/βt, substituting these into Eq. (19) yields ut(zt z1, y) = βt (cid:19) (cid:18) zt αtz1 βt + αt z1. (19) (20) Eq. (17) links z1 and the conditional score st(zt z1, y). Solving Eq. (17) for z1 and inserting the result into Eq. (20) yields ut(zt z1, y) = αt αt zt + (cid:18) β2 αt αt (cid:19) βtβt (cid:124) αtz1 + βtµ0(y) zt β2 (cid:123)(cid:122) st(ztz1,y) (cid:125) µ0(y) βt (cid:124) (cid:123)(cid:122) (cid:125) bias correction . (21) The first term under the brace is precisely the conditional score from Eq. (17), while the second term compensates for the mean shift µ0(y) introduced by the shift-only transformation. Eq. (21) can be rearranged to express the score through the velocity: st(zt z1, y) = αt ut(zt z1, y) αt zt β 2 αt αt βtβt + µ0(y) βt . (22) To translate the conditional identity given in Eq. (22) to the marginal setting used at inference time, we integrate over the target endpoint z1. For this we introduce the posterior qt(z1 zt, y) := p(zt z1, y) pdata pt(zt y) (z1 y) , with (cid:90) qt(z1 zt, y) dz1 = 1. The marginal velocity and score are simply expectations under this density, i.e., (cid:2)ut(zt z1, y)(cid:3), ut(zt y) = Eqt st(zt y) = Eqt (cid:2)st(zt z1, y)(cid:3). Applying Eq. (22) inside the expectation and using linearity yields st(zt y) = αt ut(zt y) αt zt β 2 αt αt βtβt + µ0(y) βt , (23) the exact coupling used in the latent-space SDE (Eq. 8) to express the score with the drift during sampling. Proof of Claim In this section we prove Claim 1 in two steps: (i) We show that any parameter choice driving the objective in Eq. (9) to its global minimum forces the velocity field to be affine in the interpolant, vθ(zt, t, y) = γ(t, y) zt + η(t, y). (ii) For each collapse pattern (i)(v) in the claim, we verify that the loss indeed attains this trivial minimum and we state the resulting γ(t, y) and η(t, y) in closed form. Step 1. Affine Form at Zero Loss Let zt = βtz0 + αtz1. If the loss in Eq. (9) vanishes almost surely, the integrand must be identically zero: vθ(zt, t, y) = βt z0 + αt z1 (z0, z1, t, y). (24) Fix (t, y) and apply the chain rule w.r.t. z0 and z1: vθ zt βt = βt, vθ zt αt = αt. The righthand sides are constant in (z0, z1), hence vθ/zt γ(t, y) does not depend on zt. Integrating once in zt gives the affine form vθ(zt, t, y) = γ(t, y) zt + η(t, y), (25) with η(t, y) an integration constant. Thus any zero-loss solution necessarily has the affine form given in Eq. (25) and is already independent of θ. This completes Step 1. Step 2. Each Collapse Pattern Attains the Zeroloss Affine Field Write z0 := (x0, y) and z1 := g(x1, y). Insert the ansatz given in Eq. (25) into the pointwise identity given in Eq. (24) and substitute zt = βtz0 + αtz1: γ(t, y)(cid:2)βtz0 + αtz (cid:3) + η(t, y) = βt z0 + αt z1. (26) Eq. (26) is linear in (z0, z1); each collapse scenario reduces it to one or two scalar conditions, from which γ and η are obtained explicitly. (i) Constant source. z0 c(y) is fixed while z1 varies freely. Matching the z1-coefficient in Eq. (26) forces γ = αt/αt. The remaining scalar equation fixes η = c(y) ( βt γβt). (ii) Constant target. Symmetric to case (i): γ = βt/βt and η = c(y) ( αt γαt). (iii) Unbounded source scale. As z0 with z1 bounded, the z0-terms in Eq. (26) dominate; finiteness of the left-hand side requires γβt = βt γ = βt/βt. With this choice the entire identity holds for all (z0, z1) when we set η = 0. (iv) Unbounded target scale. Analogous to (iii) with roles exchanged: γ = αt/αt and η = 0. (v) Proportional collapse. Suppose z0 = k(y)z1. Substituting into Eq. (26) yields single free variable z1: γ [βtk(y) + αt] z1 = [ βtk(y) + αt] z1. Hence γ(t, y) = ( βtk(y) + αt)/(βtk(y) + αt) and η(t, y) = 0. In all five situations γ and η depend only on (t, y) and the collapse maps (f, g). Consequently the optimizer can reach trivial minimum in which vθ no longer guides meaningful flow and the generated distribution collapses to single/improper mode."
        },
        {
            "title": "C Implementation Details",
            "content": "C.1 Synthetic Data The velocity network consumes three sinusoidal position embeddings that encode the latent state xt, the class label y, and time t. Each embedding has dimensionality 8, and concatenating them yields 24-dimensional feature vector. This vector is processed by three-layer MLP whose hidden layers are all 24 24 linear projections followed by GELU activations. final linear layer maps 24 1, producing the scalar velocity. The entire modelincluding all embedding parameterscontains 1 993 trainable parameters. To implement the additive shifts µ0(y) and µ1(y) we introduce two lightweight condition networks. Each consists of single linear layer that maps the 8-dimensional class embedding to scalar shift, plus bias term, for 9 parameters per network. Both linear layers are initialized with all weights and biases set to zero, ensuring the additive shifts are identically zero at the start of training. We train with batch size of 1 024 using AdamW with β1 = 0.9 and β2 = 0.95. Learning rates are fine-tuned per parameter group: 1 103 for the source shift network, 1 104 for the target shift network, and 1 105 for all remaining parameters. Unless noted otherwise, models are trained for 50k steps; mode-collapse experiments are extended to 100k steps to ensure convergence. All the synthetic data experiments were executed on the CPU cores of an Apple M1 Pro laptop. C.2 ImageNet We re-implemented the open-source SiT3 code-base in JAX and reproduced the SiT/XL-2 configuration on ImageNet at 256 256 resolution as our baseline. The exact architecture, datapipeline, optimizer (AdamW) and learning-rate schedule are identical to the original code. Training is performed on single v6e-256 TPU slice. For both source and target CAR-Flows, we append lightweight label-conditioning network that maps the 1152-dimensional class embedding to latent tensor of shape 32 32 4: Dense: 1152 12844. Upsampling: three repeats of ConvTranspose2d(kernel_size=2, stride=2) GroupNorm ReLU; shapes 4 4 128 8 8 64 16 16 32 32 32 16. Head: 3 3 convolution (padding 1) to 32 32 4, initialized with all weights and bias set to zero to ensure the no shifts at the start of training. Each network contains 2.4M parameters ( 0.3% of the SiT/XL-2 backbone) incurring negligible overhead. We inherit the original AdamW optimizer for the SiT backbone with learning rate of 1 104 and global batch size of 256. Both label-conditioning networks are trained with higher learning rate 1 101; all other hyper-parameters are unchanged."
        },
        {
            "title": "D Discussion of Findings",
            "content": "D.1 Relative Learning Rates During our experiments, we discovered that the relative learning rate between the lightweight condition networks (which learn the additive shifts µ0(y) and µ1(y)) and the velocity-network backbone has first-order effect on both the magnitude of the learned shifts and the convergence speeda race condition between them. To study this, we reuse the synthetic-data example: we fix the backbones learning rate at 1 105 and train source-only and target-only CAR-Flow models while sweeping the shift-network learning rate across three orders of magnitude (1 105, 1 104, and 1 103). Figures 6a and 6c plot the µ0 and µ1 trajectories for the source-only and target-only variants, respectively. At the smallest rate, the shifts remain near zero and the backbone carries the conditioning, yielding slower alignment; at the intermediate rate, the shifts grow steadilythough they can still be pulled by the backbone (e.g., µ0(y = B) starts positive but gradually becomes negative, reducing µ0(y = A) µ0(y = B)); and at the largest rate, the shifts rapidly attain larger magnitudes and drive the quickest convergence. Figures 6b and 6d report the Wasserstein distances, confirming that the highest shift-network rate achieves the fastest distribution alignment. These observations suggest that empowering the shift networks with higher learning rate can In practice, one should choose shift-network rate that is substantially accelerate alignment. sufficiently high to speed convergence while preserving robust inter-class separation. To verify that these trends extend to large-scale data, we performed the same learning-rate sweep on ImageNet using the SiT-XL/2 backbone. We fixed the backbones learning rate at its default 1 104 and trained the CAR-Flow condition networks under three configurationssource-only, target-only, and jointwhile varying their learning rate from 1 103 to 1 101. Figure 7 shows the FID at 400K steps for each variant, with SiT-XL/2 baseline of 17.2 indicated by the dashed line. At the lower condition-network rate ( 103), source-only and target-only variants remain on-par or worse than the baseline, whereas the joint variant already outperforms it. As the rate increases, all three variants deliver substantial improvementstarget-only achieves the best FID of 12.77 at 2 102, and the joint variant exhibits the most stable performance across the sweep. Even at the highest rate (101), all configurations remain well below the baseline. These large-scale results mirror our synthetic-data findings: increasing the shift-network learning rate accelerates alignment and improves sample quality, although the exact optimum varies by variant and requires some tuning. For simplicity and robust performance, we therefore adopt rate of 1 101 3https://github.com/willisma/SiT 17 (a) Learned µ0 shift for source-only (b) Wasserstein distance for source-only (c) Learned µ1 shift for target only (d) Wasserstein distance for target-only Figure 6: Effect of varying the shift-network learning rate relative to fixed backbone rate of 1105. Panels (a) and (c) show the trajectories of µ0 and µ1 for three learning rates; panels (b) and (d) plot the corresponding 1-D Wasserstein distances. At the lowest shift-network rate, the learned shifts remain negligible (slow alignment); at the intermediate rate, they grow steadily without instability; and at the highest rate, they overshoot then damp, yielding the fastest overall convergence despite early oscillations. Figure 7: ImageNet FID at 400K steps vs. condition-network learning rate for SiT-XL/2 CAR-Flow variants: source-only (blue), target-only (orange), and joint (green). The dashed horizontal line marks the baseline FID of 17.2. All variants improve over the baseline as the shift-network rate increases. in our main experiments. Figure 8 presents example outputs from each CAR-Flow variant at this rate, demonstrating that the joint variant attains superior visual fidelity compared to the source-only and target-only models. D.2 Condition-aware source versus unconditional shift Complementary to the observations above, we examine whether making the condition-aware is beneficial compared to using single unconditional shift. For intuition, consider the source-only CAR-Flow variant: it reparameterizes the source with shift that depends on y. When the target distribution varies with the conditioning variable, aligning the source per condition should reduce transport effort relative to an unconditional source [Albergo et al., 2023]. We test this in the 1-D synthetic setup of Sec. 4.1 by comparing (i) learnable unconditional source with global shift and (ii) condition-aware source with y-dependent shift. The Wasserstein distances are 0.058 for (i) versus 0.041 for (ii), indicating that per-condition alignment reduces transport. This supports CAR-Flows design choice and clarifies its distinction from an unconditional learnable source. Figure 8: Qualitative ablation of CAR-Flow variants on SiT-XL/2 at 400K steps. Each row corresponds to one variant(top) source-only, (middle) target-only, and (bottom) joint CAR-Flowand each column shows generated sample for different class from the same noise using cfg = 1. The joint model produces the most realistic and semantically accurate images across all scenarios. Table 4: Class-conditional image generation on CIFAR-10 (FID ). Baseline Source-only CAR-Flow Target-only CAR-Flow Joint CAR-Flow FID 13.8 7.5 11.1 10.6 Additional Results on CIFARTo assess generalization beyond ImageNet, we trained SiT-XL/2 baseline and CAR-Flow variants for 400k steps on CIFAR-10 using pixel-space diffusion (VAE omitted due to CIFAR-10s low resolution 32 32). All CAR-Flow variants outperformed the baseline, demonstrating that the benefits of CAR-Flow generalize across datasets."
        },
        {
            "title": "F Qualitative Results",
            "content": "Qualitative results are provided in Figure 9. Figure 9: Randomly selected samples generated from our SiT-XL/2CAR-Flow Joint model trained on ImageNet 256 256 data using cfg = 4."
        }
    ],
    "affiliations": [
        "Apple Inc."
    ]
}