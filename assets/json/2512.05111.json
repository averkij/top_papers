{
    "paper_title": "ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning",
    "authors": [
        "Shengyuan Ding",
        "Xinyu Fang",
        "Ziyu Liu",
        "Yuhang Zang",
        "Yuhang Cao",
        "Xiangyu Zhao",
        "Haodong Duan",
        "Xiaoyi Dong",
        "Jianze Liang",
        "Bin Wang",
        "Conghui He",
        "Dahua Lin",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models."
        },
        {
            "title": "Start",
            "content": "2025-12-05 ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning Shengyuan Ding1,2, Xinyu Fang2,3, Ziyu Liu2,4, Yuhang Zang2*, Yuhang Cao2, Xiangyu Zhao2, Haodong Duan2, Xiaoyi Dong2, Jianze Liang2, Bin Wang2, Conghui He2, Dahua Lin2,5, Jiaqi Wang2,6* 1Fudan University 3Zhejiang University 5The Chinese University of Hong Kong 2Shanghai Artificial Intelligence Laboratory 4Shanghai Jiao Tong University 6Shanghai Innovation Institute 5 2 0 2 4 ] . [ 1 1 1 1 5 0 . 2 1 5 2 : r *Corresponding authors https://github.com/InternLM/ARM-Thinker"
        },
        {
            "title": "Abstract",
            "content": "Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARMThinker, an Agentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, crossreference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess finegrained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models."
        },
        {
            "title": "Introduction",
            "content": "Reward models Liu et al. (2024); Wang et al. (2024c;a; 2025c); Zang et al. (2025); Zhang et al. (2025a); Fan et al. (2025) are pivotal in steering Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) toward desired behaviors. Tasks are becoming more cross-modal, open-ended, and fine-grained Starace et al. (2025). As result, evaluating correctness now depends on semantic understanding and grounding in evidence, rather than on brittle string matching against scarce or ambiguous ground truth. This need is most acute in multimodal understanding (e.g., long-document QA and multi-step instruction following) Tito et al. (2023); Ding et al. (2025), where judgments must assess both reasoning quality and factual support. Judging modern multimodal tasks is challenging for three main reasons. First, correctness hinges on multi-step, evidence-grounded reasoning: retrieving, localizing, and verifying visual and textual cues across pages and modalities rather than single-shot matching. As in Fig. 1(a), long-document QA often requires sequential retrieval and cross-page verification before deciding. Second, judgments must assess fine-grained perception under tool-mediated transforms (e.g., crop or zoom), maintain spatial grounding across steps, and distinguish plausible but unsupported claims from genuinely evidence-backed answers, including partial-credit and subjective cases. Third, agentic evaluation itself is 1 Figure 1: Overview of ARM-Thinker. (a) Case Comparison: Given complex document QA task, ARM-Thinker correctly identifies the answer by autonomously invoking the retrieval tool, while the baseline model provides an incorrect response. (b) ARMBench-VL: It evaluates reward models across three task types, each requiring specialized tool use (image manipulation, document retrieval, instruction verification). (c) Performance of ARMThinker: The agentic capability enables substantial gains across multiple benchmarks. planning problem: the judge must decide when to think, which tool to call, how to parameterize it, and how to integrate intermediate results into coherent, causal chain without hallucinations. These make robust and effective multimodal judgment particularly challenging. Existing approaches largely fall into two camps: rule-based verifiers Guo et al. (2025); Lambert et al. (2024) and model-based reward models, and both struggle on complex multimodal tasks Xiong et al. (2025); Wijaya et al. (2024); Su et al. (2025). Rule-based verifiers are brittle to paraphrase, incapable of partial credit, and impractical when ground truth is subjective. Generative reward models typically operate in single pass without tools, leading to hallucinated rationales Li et al. (2023), position/length biases Dubois et al. (2024), and no means to retrieve or verify cited content. Most reward models optimize broad coverage via pointwise scoring or pairwise preferences rather than evidence-grounded reasoning: they lack thinkactverify loop Yao et al. (2022), provide no credit assignment for tool decisions, and misalign training with inference behavior. The result is systematic failure modes: rewarding fluent but unsupported answers, under-rewarding concise evidence-backed responses, and failing on long-document, multi-step, fine-grained perception cases. We introduce ARM-Thinker, an agentic reasoning reward model that judges with an explicit thinkactverify loop: it plans reasoning steps, invokes multimodal tools (e.g., document retrieval and navigation for long PDFs) to gather evidence, and issues an evidencegrounded scalar score with an interpretable rationale. Unlike rule-based verifiers, it does not rely on brittle string equality; unlike non-agentic reward-model baselines Wang et al. (2025c); Zang et al. (2025), it can actively retrieve, localize, and verify cited content before deciding. unified tool interface lets the judge parameterize calls, integrate results into coherent reasoning trace, and improve faithfulness. The framework is backbone-agnostic and modality-agnostic, enabling seamless extension to new tools and tasks. ARM-Thinker turns judgment into an agentic, verifiable process that rewards answers for the evidence they can actually support. Current reward model benchmarks typically rely on static QA pairs and cannot assess intermediate steps of tool use and evidence gathering. To properly evaluate ARM-Thinker and future agentic reward models, we develop ARMBench-VL (Fig. 1(b)), new benchmark focused on verifiable multi-step reasoning. ARMBench-VL collects tasks requiring 2 fine-grained perception, document navigation, and instruction following, where success is measured by the ability to construct verifiable chain of evidence. Our motivation is simple: judgments should be conditional on accessible evidence, not just surface fluency. Equipping the reward model with an explicit thinkactverify loop lets it plan tool calls, retrieve or localize evidence, and base scores on what it can actually verify. This structure creates verifiable intermediate signalsretrieved pages, cropped regions, instruction checksthat enable credit assignment for when to use tools, which tools to use, and how to parameterize them. To address scarce labels, we design scalable data-generation pipeline that constructs discriminative preference pairs anchored by verifiable checks. cold-start pipeline bootstraps such pairs via counterfactuals and perturbations, followed by filtering for evidence validity, yielding scalable, high-quality agentic data. Together, these choices label-efficient, make agentic reasoning reward modeling both principled and practical: evidence-grounded, and extensible across modalities and tools. Across reward modeling, tool-use, and general reasoning benchmarks, ARM-Thinker delivers consistent gains. On reward-modeling benchmarks Malik et al. (2025); Li et al. (2025b), it improves average accuracy by +16.2%; on think-with-images and tool-use tasks Wu & Xie (2024); Wang et al. (2025b); Zhang et al. (2024b), by +9.6%; and on general reasoning benchmarks Yue et al. (2024); Lu et al. (2023); Wang et al. (2024b), by +4.2%. As shown in Fig. 1(c), the largest gains appear on long-document retrieval and fine-grained perceptionscenarios that benefit most from evidence-grounded, agentic judgment. Our contributions are: (1) We propose ARM-Thinker, an agentic reasoning reward modeling framework that turns multimodal judgment into an active, verifiable thinkactverify (2) To evaluate agentic reward models, we introduce ARMBench-VL, the first process. benchmark designed to assess multi-step, evidence-grounded reasoning in reward models. (3) We present scalable data-generation pipeline that constructs verifiable discriminative preference pairs for training agentic reward models. Trained on this data, our ARMThinker-7B achieves performance competitive with, and in some cases superior to, proprietary models like GPT-4o on reward-modeling and tool-use benchmarks, demonstrating the effectiveness of agentic judgment."
        },
        {
            "title": "2 Related Work",
            "content": "Multimodal Models with Tool Use. The boundaries of multimodal reasoning are continuously expanding as model capabilities improve Wang et al. (2025a); Bai et al. (2025). Some research efforts Li et al. (2025a); Zhi et al. (2025) have attempted to combine tool usage with long-range reasoning to achieve more precise and efficient reasoning. Some works Zheng et al. (2025); Lai et al. (2025); Hu et al. (2024) introduce thinking with images by enabling models to autonomously invoke tools for image zooming and region selection. However, this task is challenging due to the need for high-resolution images and customized QA pairs, which results in scarce data and high labor costs for data generation. Additionally, the tasks are often limited to specific scenarios, such as spatial reasoning or object localization Xu et al. (2025); Zheng et al. (2025). Moreover, the existing tools available for use are limited to basic operations, such as zooming and cropping, and lack the diversity needed to adapt to broader range of scenarios. Multimodal Reward Models. Reward models (RMs) are essential for enhancing the capabilities of multimodal large models through reinforcement learning Zhang et al. (2025a); Fan et al. (2025). While agentic paradigms have been explored in pure language reward modeling to integrate verifiable correctness signals Peng et al. (2025), existing works in the multimodal domain Wang et al. (2025c); Zang et al. (2025) on multimodal RMs primarily focus on boosting RMs accuracy through better training data and long-horizon reasoning. However, these works have not yet provided RM with the ability to call tools. This is because current tasks are relatively easy and have simple outputs, and RM models rely more on improved perception and reasoning abilities through training, without scenarios that require tool usage. In reality, more challenging tasks, such as high-precision image content understanding Lai et al. (2025); Wang et al. (2025b); Wu & Xie (2024), multi-image long-document question answering, and multimodal instruction-following, make it diffi3 Figure 2: Overview of ARM-Thinkers architecture and training pipeline. (a) Agent Loop: ARM-Thinker follows think-act-observe paradigm, maintaining indexed context for texts and images while iteratively invoking tools from the toolkit (image zoom-in, document retrieval, instruction validators) until producing the final answer. (b) Pipeline: our pipeline starting with (1) SFT & Cold Start using difficulty-filtered data, followed by (2) two-stage Group Relative Policy Optimization Shao et al. (2024)(GRPO) that first encourages correct tool calls (Stage 1) and then refines for accuracy with verifiable rewards that balance correctness and tool efficiency (Stage 2). cult for RMs to provide accurate rewards on their own, requiring the integration of additional tools. ARM-Thinker overcomes this limitation by enabling RM to call tools and flexibly select them for different scenarios, allowing it to provide more accurate reward signals."
        },
        {
            "title": "3 ARM-Thinker",
            "content": "3.1 Multimodal Tools Integrated Agent Loop Overall Architecture. Fig. 2(a) shows ARM-Thinkers agent loop architecture. Unlike traditional reward models that passively score responses, ARM-Thinker operates as an active agent: it dynamically invokes tools to gather evidence, refine understanding, and verify outputs before producing judgments. Following ReAct Yao et al. (2022) and the structured tool-calling format from WebWatcher Geng et al. (2025), each trajectory τ consists of multiple thinkactobserve cycles: (1) Thought: an intermediate reasoning or planning step, enclosed in <think>...</think>; (2) Action: either invoke an external tool, wrapped in <tool call>...</tool call>, or terminate the reasoning by providing the final answer with <answer>...</answer>; (3) Observation: After tool is invoked, the environment executes the corresponding function and returns the result (text & image) to the model, wrapped in <tool response>...</tool response>. Critically, the agent uses observations to refine subsequent thoughts. Each tool response updates the agents understanding, enabling iterative refinement rather than one-shot prediction. Formally, at step i, the agent conditions on accumulated context to generate thought θi, selects tool ti , and incorporates the returned observation oi into its context for the next step. reasoning trajectory of length can be represented as: τ = {(θ0, t0, o0), (θ1, t1, o1), . . . , (θL, tL, oL)}, (1) where θi denotes the internal thought, ti the chosen tool invocation, and oi the corresponding observation. The process continues until the agent emits Finish action, producing final reasoning trace θ and answer a. Through this iterative thinkactobserve paradigm, ARM-Thinker seamlessly integrates tool invocation with deliberative reasoning, producing interpretable multimodal trajectories. 4 Multimodal Tools. ARM-Thinker integrates three categories of multimodal tools, corresponding to text-level, image-level, and document-level interactions, that together support perception, retrieval, and judgment within the agent loop: (1) Instruction-Following Check Tools. collection of 19 textual validators that verify compliance with linguistic or structural constraints (e.g., word count, sentence range, keyword usage), implemented following the checking schema of MM-IFEngine Ding et al. (2025). (2) Image Crop and Zoom-in Tools. Tools for fine-grained inspection of visual regions. The image crop and zoom in tool allows spatial focusing on specific parts of an image for detailed analysis. (3) Document Retrieval Tools. Including doc page retrieval by query and doc page retrieval by index, these tools retrieve relevant or specific pages from long documents based on semantic queries or indices. Detailed definitions of these tools are provided in the Appendix Section E. Notably, our agent loop can be viewed as an extension of the think-with-images paradigm: the Image Crop and Zoom-in tools realize iterative visual reasoning by enabling dynamic attention shifts and re-inspection of the image throughout the reasoning. Indexed Memory Map. During multi-turn reasoning, ARM-Thinker maintains lightweight memory map to store original and intermediate multimodal artifacts: short textual responses and tool-produced image crops. The memory has two maps: texts map (e.g., resp 1, resp 2) for candidate responses to compare, and imgs map (e.g., img 0, img 1) for the image paths to be accessed. As shown in Fig. 2(a), the map provides lightweight yet structured mechanism for retrieving specific reasoning states and visual evidence. 3.2 Data Gathering for ARM-Thinker Training Preference Data Generation. We first construct preference-based reward data to initialize ARM-Thinker. For general multimodal QA reward supervision, we use the widely adopted LLaVA-Critic datasetXiong et al. (2025), whose preference annotations provide reliable, human-aligned comparison signals. However, it lacks agentic interaction patterns and does not cover our three tool categories. To address this gap, we collect additional agentic task data: DeepEyes Zheng et al. (2025) for Image Crop and Zoom-in, MMIFEngine Ding et al. (2025) for Instruction-Following Check, and MP-DocVQA Tito et al. (2023) for Document Retrieval. These sources enrich data diversity and align supervision with task-specific reasoning patterns required by the three tool categories. Sampling statistics are provided in Appendix Section A.2. image, ground-truth response) Since most task-specific datasets contain only (question, triplets, we use GPT-4o-mini to generate semantically related but flawed responses, introducing controlled negative samples with diverse error types. Thus for each questionimage pair (q, I) with ground-truth response r+, we obtain negative response and construct preference pairs Dpair = {(q, I, r+, r)}, (2) where r+ denotes that the positive response better captures ground-truth details and factual correctness. We then remove overly similar response pairs to maintain sufficient diversity between r+ and r, ensuring that each preference pair provides clear and informative contrast. Supervised Fine-Tuning & Cold Start Generation. As shown in Fig. 2(b), we establish training pipeline to generate high-quality reasoning trajectories with chain-of-thought (CoT) and tool usage after constructing preference data in Step 1. We first apply difficulty filtration step to remove trivial samples on which the base model achieves 100% accuracy in five sampling rollouts, ensuring that subsequent training focuses on more informative and challenging instances. This filtering is consistently applied throughout all later training stages. The remaining data are then used for inference within our agent loop, where stronger LVLMs generate multimodal CoT trajectories augmented with explicit tool invocations. Then, trajectories are filtered along three dimensions, (1) format; (2) accuracy; (3) behavior, where we check whether the model successfully calls the tools. The final filtered set constitutes high-quality multimodal CoT data, which serve as refined supervision for subse5 quent SFT and cold-start iterations, progressively improving the models reasoning depth and tool-use proficiency. 3.3 Multi-Stage Training of ARM-Thinker"
        },
        {
            "title": "3.3.1 SFT & Cold Start",
            "content": "In this stage, we fine-tune Qwen2.5-VL-7B using the high-quality multimodal trajectory data introduced in Section 3.2. Data derived from LLaVA-Critic Xiong et al. (2025) enhance the models fundamental reward capability for general image understanding and multimodal question answering. Meanwhile, the agentic data containing explicit tool interactions serves as Cold Start data, which aims to initialize the model with structured reasoning and correct tool-use behaviors, as VLMs typically exhibit limited zero-shot competence in executing novel tool invocations."
        },
        {
            "title": "3.3.2 Two-Staged GRPO training",
            "content": "Rollout Group of Trajectories. As illustrated in Fig. 2(b), given multimodal queryimage pair (q, I), the model generates group of trajectories, each consisting of full reasoning trace and corresponding tool interactions. Formally, for each sample, we obtain = {(τi, ai)}n (3) where τi={(θ0, t0, o0), . . . , (θL, tL, oL)} denotes i-th trajectory following the thinkactobserve process defined in Eq. (1), and ai represents the final answer enclosed in <answer>...</answer>. Each rollout contains both the internal chain-of-thought reasoning steps and explicit tool invocation traces, forming complete verifiable reasoning path. i=1, the Reward Design. To enable stable and verifiable reinforcement learning, ARM-Thinker employs two-stage reward design that separately optimizes tool-use behaviors and final accuracy (Fig. 2(b)). Each stage defines distinct reward function to progressively guide the policy from structured tool interaction to reliable factual judgment. Stage 1: Tool Call Encouragement. In the early phase, the objective is to encourage the model to actively explore tool usage. Therefore, the stage-1 reward, Rtool, is designed to promote exploration of tool invocations and is defined as: Rtool = Rf + RtryI (4) where Rf enforces the correct output format that follows the thinkactobserve style described in Section 3.1, and tool calls denotes the total number of tool invocations within the trajectory. Accordingly, Rtry assigns positive signal whenever the model makes reasonable attempt to call tool. This stage stabilizes early exploration by guiding the agent toward valid tool-use patterns without overfitting to specific success criteria. tool calls>0, Stage 2: Accuracy Refinement. After the agent learns to invoke tools correctly, the reward shifts its focus toward factual correctness and verifiable tool efficacy. The stage-2 reward Racc is hierarchically defined as: Racc = Rf + Rtry, Rf + Ra, Rf + Ra + Rsucc, if Ra = 0 and tool calls > 0; if Ra > 0 and succ tool calls = 0; if Ra > 0 and succ tool calls > 0. (5) This conditional reward formulation mirrors our verifiable supervision process: (1) Ra evaluates the factual correctness of the final answer; (2) Rsucc assigns additional credit when tool usage directly contributes to correct prediction; while Rf and Rtry continue to regulate output format consistency and encourage reasonable tool exploration, respectively. Together, the two-stage reward design progressively shifts the optimization focus from learning to use tools properly to reason accurately and efficiently. This separation stabilizes training under GRPO and provides interpretable reward signals for multimodal agentic learning. 6 Table 1: Comparison of ARMBench-VL with other reward benchmarks (M=multimodal, T=text-only). Benchmarks Tasks Case Number Question Type RMBenchT RewardBench 2T VL-RewardBenchM MultimodalRewardBenchM ARMBench-VLM 1 6 1 1 3 1327 1865 1250 5211 pair-rm (2-way) pair-rm (4-way), Tie pair-rm (2-way) pair-rm (2-way) sing-rm,pair-rm(2-way,4-way) Tools Figure 3: Representative examples from ARMBench-VL. Each block shows the multimodal context, candidate responses, and available tools for one of the three tracks in ARMBench-VL: Fine-grained Perception (image crop/zoom tools for local visual details), Multimodal Long Document QA (page-retrieval tools), and Multimodal Instruction Following (instruction-checking tools)."
        },
        {
            "title": "4 ARMBench-VL",
            "content": "To comprehensively evaluate multimodal reward models with tool-use and reasoning capabilities, we propose new benchmark suite, ARMBench-VL, which targets the limitations of existing LVLM-based verifiers. In contrast to purely languagevision verifiers that often suffer from hallucination and weak tool reasoning, our benchmark is specifically designed to assess both agentic reasoning and tool-calling behaviors across multiple modalities. 4.1 Benchmark Motivation Existing reward benchmarks mainly focus on the accuracy of reward model that directly outputs judgment. Their question types are simple and the tasks are narrow, assessing only basic perception and reasoning. Meanwhile, tool use is becoming crucial component of models capability, and for some complex tasks, model responses are diverse and hard to distinguish without tools. To address this gap, we propose ARMBench-VL, the first multimodal reward benchmark that requires tool use, comprising three tasks: (1) Fine-grained Perception: The question focuses on the local details in high-resolution images. The model (2) must invoke image crop and zoom-in tools as needed to judge multiple responses. Multimodal Long Document QA: Questions target specific page within document presented as full-page screenshots. The model should use the document page retrieval tool properly to locate the relevant page based on the question. (3) Multimodal Instruction Following: Questions impose multiple constraints (e.g., output format, required keywords). Given the question, the model needs to analyze and select appropriate tools from 7 an instruction following check tool pool to analyze the response and determine whether all requirements are satisfied. 4.2 Construction of ARMBench-VL Our benchmark is built from multiple datasets through selection and reconstruction. The three task families draw from V*Bench/VisualProbe, MMlongbench-doc, and MM-IFEval, respectively. For Fine-grained Perception and Multimodal Long Document QA, we first remove questions with overly simple answers (e.g., only yes or no). Then we use Qwen3VL-235B-A22B-Thinking to expand the original answers based on the original information, and to generate matched incorrect responses with similar style and length. To increase variety, we also rewrite some questions into descriptive questions and revise the corresponding responses, ensuring that the revised correct answer describes local region of the image while mentioning the correct fact. In addition, we increase the number of incorrect answers for some items and construct two types: pair-rm (4-way) and pair-rm (2way), to raise overall difficulty. To minimize hallucination during construction, we require the Qwen3-VL-235B-A22B-Thinking not to mention correctness-related cues in its responses, and we filter out any items where the correct answer is mistakenly included in the negative responses. For Multimodal Instruction Following, after filtering model outputs from Qwen2.5-VL-7B, we keep subset of responses that may be correct or incorrect, and let the reward model directly judge whether each response satisfies the stated constraints. Detailed procedures and prompts of the whole construction are provided in the Appendix Section F. 4.3 Benchmark statistics ARMBench-VL contains 1,499 questions covering three tasks: Fine-grained Perception, Multimodal Long Document QA, and Multimodal Instruction Following. The benchmark spans wide range of visual domains including people, architecture, and natural scenes, with evaluation of diverse capabilities such as perception, OCR, recognition, and longdocument understanding. As shown in Table 1, compared with existing reward benchmarks, ARMBench-VL is the first reward benchmark that provides toolkit, allowing models to freely select tools to evaluate responses. Our question types are more diverse than prior benchmarks, which helps preserve evaluation validity while enabling more precise assessment of the models reward-modeling capability. More detailed statistics for each subcategory are provided in Appendix Section B."
        },
        {
            "title": "5 Experiments",
            "content": "Table 2: Results on Reward Model benchmarks. We report the performance on three benchmarks (M=multimodal, T=text-only): VL-RewardBench tests hallucination detection (Hallu.), reasoning evaluation (Reason.), and general judgment (General). RewardBench2 evaluates text-only pair-wise reward accuracy. ARMBench-VL assesses fine-grained perception (FG), instruction following (IF), and document understanding (Doc). ARMThinker achieves substantial improvements over baselines across all benchmarks. Model VL-RewardBenchM Hallu. Reason. General Overall RewardBench-2T InternVL3-8B Zhu et al. (2025) UnifiedReward-7B Wang et al. (2025c) InternVL3.5-8B Wang et al. (2025a) Qwen3-VL-8B Bai et al. (2025) GPT-4o Hurst et al. (2024) Qwen2.5-VL-7B Bai et al. (2025) ARM-Thinker-7B 51.3 78.4 51.3 71.3 67.6 48.7 72. 48.1 60.5 51.9 64.5 70.5 60.4 64.8 48.1 60.6 47.5 47.0 49.1 37.7 55.7 50.0 66.1 50.9 66.0 65.8 50.1 67.8 +17.7 50.3 45.1 53.7 58.9 65.5 47.1 59.6 +12.5 ARMBench-VL (ours) FG 58.9 52.0 56.7 47.6 61.8 51.8 67.6 IF 59.1 47.2 57.7 56.6 69.5 45.4 73.8 Doc 47.0 42.8 52.0 47.6 58.7 41.1 52.4 Avg. 55.0 47.4 55.5 50.6 63.3 46.1 64.6 +18.5 Avg. 51.8 52.8 53.4 58.5 64.9 47.8 64.0 +16.2 8 Table 3: Results on visual tool-use (Think-with-Images) benchmarks. We evaluate ARMThinker-7B against baselines on four benchmarks requiring iterative tool use for finegrained visual analysis. The symbol indicates that results are copied from Lai et al. (2025). HRBench 8K 4K MME-RW Avg. Model GPT-4o Hurst et al. (2024) DeepEyes Zheng et al. (2025) Pixel Reasoner Su et al. (2025) Mini-o3 Lai et al. (2025) InternVL3-8B Zhu et al. (2025) InternVL3.5-8B Wang et al. (2025a) Qwen3-VL-8B Bai et al. (2025) Qwen2.5-VL-7B Bai et al. (2025) ARM-Thinker-7B V* 65.2 83.3 86.3 88.2 69.6 69.1 82.2 62.0 73.2 74.0 77. 70.3 69.9 76.8 75.4 86.4 +11.0 69.1 80.1 +11.0 58.3 69.5 66.9 73.3 68.4 69.9 70. 64.6 73.7 +9.1 45.2 64.0 64.4 65.5 61.2 62.8 63.1 58.5 65.8 +7.3 57. 72.5 72.9 76.1 67.4 67.9 73.1 66.9 76.5 +9.6 Table 4: Generalization to multimodal math and logical reasoning benchmarks. We evaluate ARM-Thinker against baseline models on six reasoning benchmarks covering general knowledge, math and logical reasoning. Model MMMU MathVista MathVision MathVerse WeMath LogicVista Avg. Gemma-3-27B Team et al. (2025) InternVL3-8B Zhu et al. (2025) Qwen2.5-VL-7B Bai et al. (2025) ARM-Thinker-7B 64.9 62.7 55.0 57.2 +2.2 59.8 71. 67.8 70.2 +2.4 39.8 29.3 25.4 25.9 +0.5 34.0 39.8 41.1 41.6 +0.5 37.9 37. 35.2 46.1 +10.9 47.3 44.1 44.1 52.8 +8.7 47.3 47.4 44.8 49.0 +4.2 5.1 Experimental Setup Benchmarks. We conduct experiments across three benchmark categories. To assess reward modeling accuracy, we evaluate on RewardBench-2 (text-only) Malik et al. (2025), VL-RewardBench (multi-modal inputs) Li et al. (2025b), and our proposed ARMBench-VL (Section 4), which focuses on agentic verification. To assess tool-assisted usage, we follow previous works Lai et al. (2025); Zheng et al. (2025) and evaluate on V* Bench Wu & Xie (2024), HRBench-4K Wang et al. (2025b), HRBench-8K Wang et al. (2025b), and MMERealWorld Zhang et al. (2024b). To assess visual reasoning, we report the performance on MMMU Yue et al. (2024), MathVista Lu et al. (2023), MathVision Wang et al. (2024b), MathVerse Zhang et al. (2024a), WeMath Qiao et al. (2025), and LogicVista Xiao et al. (2024). Baselines. Our ARM-Thinker-7B is built upon the Qwen2.5-VL-7B Bai et al. (2025) model. We compare ARM-Thinker with diverse set of baseline models, encompassing generalpurpose LVLMs, specialized reward models, and visual reasoning models that support the think-with-images ability. Model details are provided in Appendix Section A.1. 5.2 Results on Reward Benchmarks Consistent Improvements on Reward Benchmarks. ARM-Thinker-7B achieves substantial improvements over the base model across all reward benchmarks  (Table 2)  , highlighting its superior capability on response judgment. Specifically, ARM-Thinker-7B achieves 67.8% accuracy on VL-RewardBench, surpassing the baseline by 17.7%, and yields 12.5% gain on RewardBench-2. On our proposed ARMBench-VL, it scores 64.6% (+18.5% on baseline) with balanced gains across FP, IF, and Doc. Overall, the average improvement across reward benchmarks is prominent. Comparative Analysis with Existing Reward Models. Comparing across models reveals distinct capability gaps. UnifiedReward-7B achieves 66.1% on VL-RewardBench but only Table 5: Ablation: Tool Use vs. No Tool Use. We evaluate both Qwen2.5-VL-7B (baseline) and ARM-Thinker with tool calling disabled (default) or enabled (w/ tool) across three benchmarks. The baseline model fails to benefit from tools, showing performance degradation when tools are enabled. ARM-Thinker maintains strong performance without tools (comparable to baseline) but achieves consistent gains when tools are enabled. Model ARMBench-VL V* Qwen2.5-VL-7B Bai et al. (2025) w/ tool ARM-Thinker-7B w/ tool 46.1 44.3 59.2 64.6 +5.4 HR-Bench 4K 69.1 60.1 8K 64.6 51.8 75.4 50.3 82.2 76.6 86.4 +4.2 80.1 +3.5 73.7 +3.2 70.5 45.1% on RewardBench-2  (Table 2)  , showing weak transfer from visionlanguage judging to text-only reward tasks. Conversely, Qwen3-VL-8B performs well on standard benchmarks but gains less on ARMBench-VL, suggesting that general VLM training alone does not equip models with the verification-specific reasoning needed for tool-assisted judgment. GPT-4o is robust (64.9% average) under our settings; nevertheless, ARM-Thinker-7B surpasses it and yields more balanced performance across multimodal judgment. 5.3 Results on Tool Use (Think-with-Images) Benchmarks Capable of Tool-Assisted Visual Reasoning. Think-with-images refers to the paradigm where models iteratively refine visual understanding by invoking tools such as zoom-in for detail inspection. Among the tools employed in our framework, the zoom-in tool plays key role in fine-grained visual reasoning. Therefore, we evaluate ARM-Thinker-7B on several representative benchmarks in this domain, as shown in Table 3. ARM-Thinker7B improves over the powerful Qwen2.5-VL-7B baseline by an average of 9.6% and attains 76.5% overall accuracy, matching or exceeding Mini-o3. These results demonstrate effective, well-integrated perceptionreasoning via tool use. Generalization Across Visual Tool-Use Domains. Unlike specialized visual reasoning models trained directly on tool-use demonstrations (DeepEyes, Pixel Reasoner, Mini-o3), ARM-Thinker-7B acquires tool-calling abilities emergently through reward-based optimization, without explicit tool-use supervision in its training data. During GRPO training, the model learns to autonomously decide whether, when, and how many times to invoke tools, rather than following fixed tool-use patterns. Despite this fundamental difference, our ARM-Thinker demonstrates strong generalization to tool-use scenarios, achieving performance comparable to Mini-o3, while outperforming the state-of-the-art open-source model Qwen3-VL-8B across all benchmarks. Our observation demonstrates that appropriately designed reward signals are scalable training paradigm that also induces systematic tool-use strategies without requiring curated tool-calling demonstrations. 5.4 Results on General Benchmarks Generalization to Mathematical and Logical Reasoning. We further evaluate ARMThinker-7B on suite of general-purpose multimodal reasoning benchmarks to examine its capability beyond reward modeling and tool-use tasks. As shown in Table 4, ARMThinker-7B achieves consistent improvements over the Qwen2.5-VL-7B baseline, with notable gains of 10.9% and 8.7% on WeMath and LogicVista, respectively. These gains suggest that reward-based training for verification tasks improves general reasoning abilities, likely because judging response quality requires careful logical analysis and error detection. Representative reasoning examples can be found in Appendix Section C. 5.5 Ablation Studies Ablation: Tool Use vs. No Tool Use. As shown in Table 5, the Qwen2.5-VL-7B baseline rarely invokes tools without explicit supervision, as it lacks training signals that associate 10 Figure 4: Ablation study comparing three reward function designs during GRPO training. Left: Evaluation accuracy over training steps. Right: Average tool-call frequency over training steps. Our ARM-Thinker reward (blue) achieves the highest accuracy while maintaining stable tool usage, avoiding both the under-use pitfall of accuracy-only rewards (orange) and the over-use pitfall of fixed tool rewards (green). tool use with improved performance, especially for complex functions such as zoom-in and page-retriever. In contrast, ARM-Thinker-7B is competitive even without tools (comparable to Qwen2.5-VL-7B) and yields substantial additional improvements when tool calling is enabled, indicating it learns both when tools are necessary and how to use them effectively. This demonstrates the effectiveness of our adaptive reward design in enhancing fine-grained perception and reasoning. Ablation of Different Reward Function Designs. central design challenge is how to encourage tool use without causing overuse. To analyze the contribution of each reward component, we compare three reward designs during GRPO training: 1) Only Acc & Fmt Reward: optimizes task accuracy and response format without tool-awareness. 2) Fixed Tool Reward: adds constant bonus when the model invokes tool. 3) ARM-Thinker Reward (ours): adaptively adjusts tool-related rewards based on the utility and contextual relevance of each call. As shown in Fig. 4, the two baseline designs highlight critical trade-off, falling into two distinct pitfalls. The Only Acc & Fmt Reward model confirms that tool utilization is indispensable; its minimal tool use (call rate 0.7) results in early performance plateaus, finishing last at 77.5%. Conversely, the Fixed Tool Reward drives unchecked tool calls (rising to 1.15) yet achieves only 78.5% accuracy, showing that constant bonus induces overuse and that simply maximizing tool calls is ineffective. Our ARM-Thinker Reward successfully resolves this dilemma. It attains the highest final accuracy and learns more disciplined tool-use policy: the tool-call curve stabilizes ( 1.12) and even contracts slightly after 54 steps. This stabilization is the key evidence: it indicates the model has learned an optimal policy, invoking tools based on contextual utility rather than merely chasing fixed bonus. This comparison demonstrates that adaptive, context-dependent reward shaping successfully balances accuracy maximization with appropriate tool utilization, avoiding the failure modes of both naive under-use and over-use."
        },
        {
            "title": "6 Conclusion",
            "content": "We believe agentic capabilities are crucial for the next generation of reward models. We present ARM-Thinker that learns to autonomously invoke multimodal tools during verification, bridging the gap between passive reward scoring and active reasoning. Our method achieves consistent gains across three dimensions on average: reward modeling (+16.2%), tool-assisted reasoning (+9.6%), and multimodal reasoning (+4.2%), proving the effectiveness of our training method and its implications for the next generation of reward models. 11 In the future, we plan to extend our method to broader set of tools and further expand its applicability."
        },
        {
            "title": "References",
            "content": "Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Shengyuan Ding, Shenxi Wu, Xiangyu Zhao, Yuhang Zang, Haodong Duan, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Mm-ifengine: Towards multimodal instruction following. In ICCV, 2025. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Kaixuan Fan, Kaituo Feng, Haoming Lyu, Dongzhan Zhou, and Xiangyu Yue. Sophiavl-r1: Reinforcing mllms reasoning with thinking reward. arXiv preprint arXiv:2505.17018, 2025. Xinyu Fang, Kangrui Mao, Haodong Duan, Xiangyu Zhao, Yining Li, Dahua Lin, and Kai Chen. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang (eds.), Advances in Neural Information Processing Systems, volume 37, pp. 8909889124. Curran Associates, Inc., 2024. doi: 10.52202/079017-2827. URL https://proceedings.neurips.cc/paper files/paper/ 2024/file/a2326c9715a516c91174132e0170073a-Paper-Datasets and Benchmarks Track.pdf. Xinyu Fang, Zhijian Chen, Kai Lan, Lixin Ma, Shengyuan Ding, Yingji Liang, Xiangyu Zhao, Farong Wen, Zicheng Zhang, Guofeng Zhang, Haodong Duan, Kai Chen, and Dahua Lin. Creationmmbench: Assessing context-aware creative intelligence in mllms. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 447456, October 2025. Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontier of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. Advances in Neural Information Processing Systems, 37:139348139379, 2024. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ OsarXiv preprint trow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv:2410.21276, 2024. Xin Lai, Junyi Li, Wei Li, Tao Liu, Tianjian Li, and Hengshuang Zhao. Mini-o3: Scaling up reasoning patterns and interaction turns for visual search, 2025. URL https://arxiv.org/abs/2509.07969. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. ulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Chengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia, Shaoguang Mao, Li Dong, Ivan Vulic, and Furu Wei. Imagine while reasoning in space: Multimodal visualization-of-thought. arXiv preprint arXiv:2501.07542, 2025a. Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, et al. Vl-rewardbench: challenging benchmark for vision-language generative reward models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2465724668, 2025b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. arXiv preprint arXiv:2305.10355, 2023. 12 Chris Yuhao Liu, Liang Zeng, Jiacai Liu, Rui Yan, Jujie He, Chaojie Wang, Shuicheng Yan, Yang Liu, and Yahui Zhou. Skywork-Reward: Bag of tricks for reward modeling in llms. arXiv preprint arXiv:2410.18451, 2024. Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, et al. Star-bench: Probing deep spatio-temporal reasoning as audio 4d intelligence. arXiv preprint arXiv:2510.24693, 2025a. Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785, 2025b. Ziyu Liu, Yuhang Zang, Shengyuan Ding, Yuhang Cao, Xiaoyi Dong, Haodong Duan, Dahua Lin, and Jiaqi Wang. Spark: Synergistic policy and reward co-evolving framework, 2025c. URL https: //arxiv.org/abs/2509.22624. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. Saumya Malik, Valentina Pyatkin, Sander Land, Jacob Morrison, Noah A. Smith, Hannaneh Hajishirzi, and Nathan Lambert. Rewardbench 2: Advancing reward model evaluation, 2025. URL https://arxiv.org/abs/2506.01937. Hao Peng, Yunjia Qi, Xiaozhi Wang, Zijun Yao, Bin Xu, Lei Hou, and Juanzi Li. Agentic reward modeling: Integrating human preferences with verifiable correctness signals for reliable reward systems, 2025. URL https://arxiv.org/abs/2502.19328. Runqi Qiao, Qiuna Tan, Guanting Dong, MinhuiWu MinhuiWu, Chong Sun, Xiaoshuai Song, Jiapeng Wang, Zhuoma Gongque, Shanglin Lei, Yifan Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2002320070, 2025. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. PaperBench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, and Wenhu Chen. Pixel reasoner: centivizing pixel-space reasoning with curiosity-driven reinforcement learning. arXiv:2505.15966, 2025. InarXiv preprint Zeyi Sun, Yuhang Cao, Jianze Liang, Qiushi Sun, Ziyu Liu, Zhixiong Zhang, Yuhang Zang, Xiaoyi Dong, Kai Chen, Dahua Lin, and Jiaqi Wang. Coda: Coordinating the cerebrum and cerebellum for dual-brain computer use agent with decoupled reinforcement learning, 2025a. URL https: //arxiv.org/abs/2508.20096. Zeyi Sun, Ziyu Liu, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Tong Wu, Dahua Lin, and Jiaqi Wang. Seagent: Self-evolving computer use agent with autonomous learning from experience, 2025b. URL https://arxiv.org/abs/2508.04700. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. Rub`en Tito, Dimosthenis Karatzas, and Ernest Valveny. Hierarchical multimodal transformers for multi-page docvqa, 2023. URL https://arxiv.org/abs/2212.05935. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024a. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Houxing Ren, Aojun Zhou, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024b. 13 Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025a. Wenbin Wang, Liang Ding, Minyan Zeng, Xiabin Zhou, Li Shen, Yong Luo, Wei Yu, and Dacheng Tao. Divide, conquer and combine: training-free framework for high-resolution image perception in multimodal large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 79077915, 2025b. Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025c. Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. HelpSteer2-Preference: Complementing ratings with preferences. arXiv preprint arXiv:2410.01257, 2024c. Xilin Wei, Xiaoran Liu, Yuhang Zang, Shengyuan Ding, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Qipeng Guo, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. Videorope++: Towards better video rotary https://github.com/Wiselnn570/VideoRoPE/blob/main/videorope plus/ position embedding. VideoRoPE plus.pdf, 2025a. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Jiaqi Wang, Xipeng Qiu, and Dahua Lin. SIM-COT: Supervised implicit chain-of-thought. arXiv preprint arXiv:2509.20317, 2025b. Xilin Wei, Xiaoran Liu, Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Jian Tong, Haodong Duan, Qipeng Guo, Jiaqi Wang, et al. Videorope: What makes for good video rotary position embedding? In International Conference on Machine Learning, 2025c. Robert Wijaya, Ngoc-Bao Nguyen, and Ngai-Man Cheung. Multimodal preference data synthetic alignment with reward model. arXiv preprint arXiv:2412.17417, 2024. Penghao Wu and Saining Xie. V?: Guided visual search as core mechanism in multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1308413094, 2024. Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. Long Xing, Qidong Huang, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Jinsong Li, Shuangrui Ding, Weiming Zhang, Nenghai Yu, et al. Scalecap: Inference-time scalable image captioning via dual-modality debiasing. arXiv preprint arXiv:2506.19848, 2025. Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. Llava-critic: Learning to evaluate multimodal models, 2025. URL https://arxiv. org/abs/2410.02712. Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, and Ivan Vulic. Visual planning: Lets think only with images. arXiv preprint arXiv:2505.11409, 2025. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. ReACT: Synergizing reasoning and acting in language models. In ICLR, 2022. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, 2024. Yuhang Zang, Xiaoyi Dong, Pan Zhang, Yuhang Cao, Ziyu Liu, Shengyuan Ding, Shenxi Wu, Yubo Ma, Haodong Duan, Wenwei Zhang, et al. Internlm-xcomposer2. 5-reward: simple yet effective multi-modal reward model. arXiv preprint arXiv:2501.12368, 2025. Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2024a. Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, et al. Mme-realworld: Could your multimodal arXiv preprint llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv:2408.13257, 2024b. Yi-Fan Zhang, Xingyu Lu, Xiao Hu, Chaoyou Fu, Bin Wen, Tianke Zhang, Changyi Liu, Kaiyu Jiang, Kaibing Chen, Kaiyu Tang, et al. R1-reward: Training multimodal reward model through stable reinforcement learning. arXiv preprint arXiv:2505.02835, 2025a. Zhixiong Zhang, Shuangrui Ding, Xiaoyi Dong, Songxin He, Jianfan Lin, Junsong Tang, Yuhang Zang, Yuhang Cao, Dahua Lin, and Jiaqi Wang. Sec: Advancing complex video object segmentation via progressive concept construction. arXiv preprint arXiv:2507.15852, 2025b. Zicheng Zhang, Junying Wang, Farong Wen, Yijin Guo, Xiangyu Zhao, Xinyu Fang, Shengyuan Ding, Ziheng Jia, Jiahao Xiao, Ye Shen, et al. Large multimodal models evaluation: survey. Science China Information Sciences, 68(12):221301, 2025c. Xiangyu Zhao, Shengyuan Ding, Zicheng Zhang, Haian Huang, Maosongcao Maosongcao, Jiaqi Wang, Weiyun Wang, Xinyu Fang, Wenhai Wang, Guangtao Zhai, Hua Yang, Haodong Duan, and Kai Chen. OmniAlign-V: Towards enhanced alignment of MLLMs with human preference. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1849018515, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.906. URL https://aclanthology.org/ 2025.acl-long.906/. Ziwei Zheng, Michael Yang, Jack Hong, Chenxiao Zhao, Guohai Xu, Le Yang, Chao Shen, and Xing Yu. Deepeyes: Incentivizing thinking with images via reinforcement learning. arXiv preprint arXiv:2505.14362, 2025. Zhuo Zhi, Chen Feng, Adam Daneshmend, Mine Orlu, Andreas Demosthenous, Lu Yin, Da Li, Ziquan Liu, and Miguel RD Rodrigues. Seeing and reasoning with confidence: Supercharging multimodal llms with an uncertainty-aware agentic framework. arXiv preprint arXiv:2503.08308, 2025. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 15 Single judge for Instruction Following Task in ARMBench-VL You will receive response(named as text 0) which follows the users instruction or requirement to the provided image. Your Task is to judge whether the response satisfies the constraint. If it does, you should mark it as True, otherwise False for you think the response does not satisfy the constraint. <start of instruction> {instruction} <end of instruction> <start of text 0> {prediction} <end of text 0> <start of constraint> {constraint]} <end of constraint> ## Output Format (strict) You should make the final judgment wrapped in <answer></answer> XML tags: <answer>Overall Judgment: True (or False)</answer> Figure 5: Single judge for Instruction Following Task in ARMBench-VL"
        },
        {
            "title": "Outline",
            "content": "In the appendix, we provide additional supporting materials to facilitate deeper understanding of our work. First, in Section A, we present an overview of the models, datasets, and benchmark statistics used throughout ARM-Thinker, including detailed descriptions of the training data employed for multimodal reward modeling. Second, Section reports comprehensive statistics and analyses of the ARMBench-VL benchmark. Third, in Section C, we showcase diverse set of qualitative response cases, with particular focus on visual reasoning and WeMath examples. Fourth, in Section E, we describe the implementation details of the multimodal tools integrated into our agentic framework. Finally, in Section F, we provide the full list of prompts used in our experiments. Model, Dataset and Benchmark Statistic A.1 Models In our study, we adopt the Qwen2.5 family of visionlanguage models as the backbone for both training and evaluation. Unless otherwise specified, the term base model refers to Qwen2.5-VL-7B Bai et al. (2025), on top of which we build our agentic reward model. The resulting model is denoted as ARM-Thinker-7B (ARM-Thinker-7B), which augments the backbone with an explicit thinkactobserve agent loop and multi-stage GRPO training, while keeping the underlying architecture size (7B parameters) unchanged. For fair comparison, we evaluate ARM-Thinker-7B alongside diverse set of strong baselines that cover general-purpose LVLMs, specialized reward models, and visual tool-use models. As general-purpose LVLMs, we include Qwen3-VL-8B Bai et al. (2025), InternVL38B Zhu et al. (2025), InternVL3.5-8B Wang et al. (2025a), and the proprietary GPT-4o Hurst et al. (2024). N-way Pairwise Judge for Tasks in ARMBench-VL You will receive two responses (named as resp 1 and resp 2) which follow the users instruction or requirement to the provided image (or document). Your Task is to judge which response is better. Note that correctness is most important. If both are not correct, you should choose the one that is more better from other aspects. <start of instruction> {instruction} <end of instruction> <start of resp 1> {prediction1} <end of resp 1> <start of resp 2> {prediction2} <end of resp 2> ## Output Format (strict) You should make the final judgment wrapped in <answer></answer> XML tags: <answer>Overall Judgment: Answer is better (X must be either 1 or 2). </answer> Figure 6: N-way Pairwise Judge for Tasks in ARMBench-VL. The figure illustrates the 2-way judging setup as an example. Our benchmark also includes 4-way comparisons, which follow the same structure but contain additional candidate responses to be judged. Fixed Chain-of-Thought Prompt for Agent-based Evaluation in ARMBench-VL Important Requirement: [If for image-based tasks] The given image is original image. [If for document-based tasks] The given document is named {doc id}. The page indices in the combined image start from 1 at the top-left corner and increase horizontally from left to right, then continue to the next row from top to bottom. You must output your reasoning inside <think>...</think>. After reasoning, either output the final answer within <answer>...</answer> or call tool within <tool call>...</tool call>. You may call tools multiple times across turns to assist with judgment or verification, but only one tool per turn. If tool call fails, you may retry or stop and give your final answer. Once no more tool calls are needed, provide your final answer or judgment within <answer>...</answer>. Figure 7: Fixed CoT Prompt for Agent Models in ARMBench-VL. This is the fixed suffix prompt appended after each task when evaluating agent-style models such as ARMThinker-7B. It enforces explicit reasoning, structured answers, and controlled tool usage. compare with reward-modeling capability, we To specifically assess UnifiedReward-7B Wang et al. (2025c), recent multimodal reward model designed to unify understanding and generation evaluation. This baseline is evaluated on the same reward benchmarks as ARM-Thinker-7B, including RewardBench-2, VL-RewardBench, and our proposed ARMBench-VL, allowing us to isolate the benefit of adding an explicit agent loop and tool-use to reward modeling. further 17 Long Response Generation Template in ARMBench-VL Assume you are helpful assistant. You are given question and solution, together with the image. You need to generate two responses to the question based on the solution. The language style of the response can be varied. For the first response, provide detailed analysis of the question. This should include concise explanation of how to approach the problem and then present the correct solution. The answer should include the original correct solution, but avoid excessive analysis or lengthfocus on clarity and providing the correct final answer. The answer should be smoothly conveyed in the end. For the other three responses, offer detailed analysis of the question with similar approach. However, concludes with an incorrect solution. The wrong solution should seem plausible but contain mistake, misleading the reader. Both responses should be conveyed in confident tone, and should not provide any information about the correctness of the solution. <start of question> {question} <end of question> <start of solution> {solution} <end of solution> Directly give back four responses in the following format: response 1: ... response 2: ... response 3: ... response 4: ... Figure 8: Long Response Generation Template for Tasks in ARMBench-VL. This example shows the template for generating one correct and three incorrect yet plausible long-form responses (i.e. 4-way comparisons). The 2-way long response generation template follow the same structure. For think-with-images and tool-assisted visual reasoning, we include several specialized models that are explicitly trained with visual tool-use supervision: DeepEyes Zheng et al. (2025), Pixel Reasoner Su et al. (2025), and Mini-o3 Lai et al. (2025). These models serve as strong baselines on V* Bench, HRBench-4K/8K, and MME-RealWorld, where performance heavily relies on iterative zoom-in or crop operations. By contrast, ARM-Thinker7B acquires its tool-calling behavior purely through reward-based optimization, without curated tool-use demonstrations, yet achieves accuracy comparable to or better than these specialized systems. Finally, on general multimodal math and logical reasoning benchmarks, we also report results for larger or more specialized reasoning models such as Gemma-3-27B Team et al. (2025) and InternVL3-8B Zhu et al. (2025). These models offer an upper-bound reference for reasoning performance on MMMU, MathVista, MathVision, MathVerse, WeMath, and LogicVista, and help contextualize how much of ARM-Thinker-7Bs gain comes from improved agentic reward modeling rather than sheer model scale. A.2 Training Data We use two stages of data for training ARM-Thinker-7B: Supervised Fine-Tuning (SFT) stage and GRPO stage. SFT Data. The SFT stage combines (i) preference-style reward data from LLaVA-Critic for general multimodal QA, and (ii) tool-specific data covering image zoom-in (DeepEyes), instruction-following checking (MM-IFEngine), and document retrieval (MP-DocVQA). After filtering, the SFT dataset consists of approximately 40k samples from LLaVA18 Caption-style Question Rewriting and Response Generation in ARMBench-VL Assume you are helpful assistant. You are given an image with related question and solution. Your task is to generate two responses based on the image and the solution, and turn the original question into new form. The language style of the response can be varied. For the new question, it should contain describe and in detail, and focus on the part of the image that is related to the original question and solution. The language style should be diverse, and the question should be concise, without being overly specific to single attribute. For example: Original Question: What is the hat color of the man on the roof? New Question: Describe the man on the roof in detail. Original Question: What animal is depicted in the tattoo on the womans arm? New Question: Describe the woman in detail, focusing on her arm. For the first response, provide detailed description of the scene in the image, addressing the key elements. The description should smoothly integrate the correct solution (e.g., if the solution states the cat is white, the response should describe the cat as white). For the second response, describe the scene with similar structure, but introduce some details that are slightly misleading or incorrect. The incorrect detail should not be immediately obvious, but it must contradict the original correct solution (e.g., describing the cat in different color). Both responses should satisfy the new question, offering detailed description of the relevant region of the image. You do not need to output the solution explicitly. Both responses should be conveyed confidently and should not reveal any information about correctness. <start of question> {question} <end of question> <start of solution> {solution} <end of solution> Directly give back two responses in the following format: new question: ... first: ... second: ... Figure 9: Caption-style Question Rewriting and Response Generation Template in ARMBench-VL. This example demonstrates how original questions are rewritten into descriptive caption-style queries and paired with both correct and subtly incorrect imagegrounded responses. The structure extends to more diverse rewriting tasks in our benchmark. Critic(augmented by interchanging the resp 1 and resp 2 order), 4k from DeepEyes, 1k from MM-IFEngine, and 1k from MP-DocVQA. GRPO Data. For GRPO training, we sample subset of the SFT-prepared data as queries. Each query is rolled out with multiple trajectories per iteration. Across both GRPO stages, we sample 20k from DeepEyes, and 4k from MP-DocVQA. Notably, We do not include Multimodal Instruction Following taskrelated tool-use data here, because these tasks are primarily abundant rather than difficult. Their core challenge lies in selecting the appropriate tool rather than executing complex tool-use logic. In our experiments, we observe that once the model is trained with our framework, its tool-use capability generalizes naturally Table 6: Summary of Data Statistics across the Three Tasks in ARMBench-VL. Task Total Single-RM Pair-RM (2-way / 4-way) Fine-grained Perception Multimodal Long Document QA Multimodal Instruction Following 550 460 173 489 295 (pair rm), 163 (2-way), 92 (4-way) 287 (2-way only) to such tasks without requiring explicit inclusion of this data. This further demonstrates the generalization strength of our approach. These two stages together provide comprehensive supervision for reward modeling, chain-of-thought reasoning, and tool-use behavior in ARM-Thinker-7B. ARMBench-VL Statistics To provide clear overview of the scale and composition of ARMBench-VL, we summarize the dataset statistics across its three major task categories: (1) Fine-grained Perception, (2) Multimodal Long Document QA, and (3) Multimodal Instruction Following. Each task includes different combinations of single-response judging (single rm) and pairwise comparison judging (pair rm), including 2-way and 4-way evaluation settings where applicable. For the Fine-grained Perception task, the benchmark contains total of 550 samples. This task focuses heavily on pairwise comparison, with 295 general pair rm items, 163 2-way pairwise items, and 92 4-way comparison items. These multi-candidate settings reflect the nuanced, fine-grained nature of visual perception evaluation. The Multimodal Long Document QA task includes 460 samples. Since this task emphasizes long-context reasoning over document-rich multimodal inputs, it incorporates both single-response judging (173 items) and pairwise 2-way comparisons (287 items). No 4way setting is used in this task to ensure controlled difficulty for long-context evaluation. The Multimodal Instruction Following task contains 489 samples, all evaluated via singleresponse judging. This is because instruction-following quality can be assessed reliably through constraint satisfaction without multi-way comparison. Table 6 summarizes all statistics in unified table for clarity."
        },
        {
            "title": "C Qualitative Case Study",
            "content": "In this part, we show more model response cases. Fig. 10 shows case of Multimodal Instruction Following Judgment Task in ARMBench-VL, and Fig. 11 shows case of FineGrained Image Perception Judgment Task in ARMBench-VL."
        },
        {
            "title": "D Broader Impact and Future Directions",
            "content": "ARM-Thinker serves as step towards more robust multimodal systems, focusing on agentic tool use and visual reasoning. This work is positioned within broader research efforts in multimodal alignment, reinforcement learning, spatio-temporal reasoning, and autonomous agents, each of which offers promising avenues for further exploration. The need for effective verification in multimodal outputs has led to the development of evaluation protocols for Large Multimodal Models (LMMs). As highlighted by recent surveys Zhang et al. (2025c), the complexity of evaluating these models requires sophisticated metrics beyond simple matching. ARM-Thinkers framework, with its focus on agentic verification, aligns well with the direction of improving human preference alignment, such as through some subjective benchmarksZhao et al. (2025); Fang et al. (2025). 20 Figure 10: Case of Multimodal Instruction Following Judgment Task in ARMBench-VL. Figure 11: Case of Fine-Grained Image Perception Judgment Task in ARMBench-VL. In the realm of reinforcement learning (RL), ARM-Thinker utilizes Group Relative Policy Optimization (GRPO) to enhance tool-use behaviors, which fits into larger trend of using RL to refine multimodal capabilities. Techniques Liu et al. (2025b;c) propose dynamic, coevolving policy-reward models that could further improve ARM-Thinkers tool-selection policy. Additionally, some of current approachesXing et al. (2025); Wei et al. (2025b) offer ways to improve the efficiency of reasoning processes in multimodal systems. Looking ahead, ARM-Thinkers paradigm could be extended to video and spatio-temporal domains. Holistic video understanding and complex object segmentation are natural extensions of the current framework, as seen in recent advancements Fang et al. (2024); Zhang et al. (2025b). The incorporation of advanced positional embeddings, such as those explored in VideoRoPE and its extended versions Wei et al. (2025c;a), would enhance tem21 poral consistency and facilitate better handling of dynamic multi-modal data. Moreover, spatio-temporal reasoning benchmarks, such as STAR-Bench Liu et al. (2025a), could play key role in advancing ARM-Thinkers capabilities in handling dynamic, multimodal data and refining its spatio-temporal reasoning abilities. The tool-use capabilities demonstrated in ARM-Thinker also lay the groundwork for more autonomous systems capable of interacting with complex environments. Transitioning from specific API tasks, such as zooming and cropping, to full-scale control of computer interfaces represents an exciting future direction. Works on computer use agents Sun et al. (2025a;b) highlight the potential of reinforcement learning-driven agents that evolve through experience, which could open new possibilities for self-improving systems."
        },
        {
            "title": "E Implementation Details of Multimodal Tools",
            "content": "All tools in our system inherit from common baseTool interface and expose unified OpenAI-style function-calling schema. Each tool implements standard create-executerelease lifecycle and returns tool response object that can contain both textual feedback and images. Below we detail the three families of multimodal tools used in our experiments. Documentlevel multimodal retrieval tools. To support long-document question answering, we implement two complementary tools that operate on pre-rendered page images stored under image root using the naming pattern {filename} {page}.{ext}. Both tools are built on top of shared retriever manager that lazily instantiates CLIP-based encoder and persistent vector database. For dense retrieval, we use SentenceTransformer implementation of CLIP-ViT-B/32, loaded from local HuggingFace cache in offline mode. Given batch of texts, the encoder produces 512-dimensional embeddings on GPU when available. These embeddings are stored and queried via chromadb.PersistentClient configured with anonymized telemetry disabled. We use single collection (COLLECTION NAME) with metadata fields including the document identifier (source) and page index (page). global RetrieverManager holds the collection and is initialized exactly once using an asyncio.Lock to avoid race conditions during concurrent tool calls. DocPageSearchTool takes document name and natural-language query as input. At execution time, it first ensures that the retriever manager is initialized; if retrieval is not available, it returns structured error message to the model. Otherwise, it queries the Chroma collection with the given query, restricting the filter to the specified document (where={source: filename}) and retrieving up to results (default is 5). From the returned metadata, the tool deduplicates and preserves the order of page indices, then resolves each page to an image path via helper that tries multiple file extensions. Missing pages are reported with an explicit error listing all attempted paths. For visualization, the tool loads the corresponding page images and horizontally concatenates them using dedicated image utility. Each page is first resized to fixed maximum long side (RAG IMAGE MAX SIDE, default 1120 pixels) while preserving aspect ratio. The concatenation canvas width is the sum of individual widths plus fixed padding, and the height is the maximum of the resized heights. To avoid excessively large tensors and potential OOM errors during training, we enforce hard cap on the total pixel count (MAX CONCAT PIXELS); if the stitched image exceeds this budget, it is downsampled isotropically. The tool then returns single stitched image that visually aggregates the top-k pages along with textual description summarizing which pages were retrieved, and hints to the model that it may want to refine the query if the retrieved context is not relevant. DocPageByIndexTool provides complementary, deterministic interface that bypasses dense retrieval. It takes filename and image idx and directly returns the corresponding page image. The tool validates that the index, resolves the page to file path (again trying multiple extensions), and fails with clear error if the image cannot be found or the index is out of range. The page image is loaded and resized using the same long-side con22 straint as above, and the final response includes single page image plus short textual confirmation of the selected page. In practice, the model often uses DocPageSearchTool to locate coarse region of interest and then DocPageByIndexTool to inspect specific pages sequentially. Image zoom-in tool. To support fine-grained visual inspection, we implement an ImageZoomInTool that crops sub-region from an existing image. The tool is designed to be robust to noisy bounding boxes and to integrate seamlessly with our multimodal reasoning loop. The tool operates over per-instance response store that contains an imgs map mapping logical image keys (e.g., original image) to concrete image paths. At execution time, the model specifies an image key and 2D bounding box bbox 2d. The bounding box is expressed in normalized integer coordinates within [0, 1000] along each axis, which makes it easier for the language model to reason about relative locations while still allowing precise cropping. The tool first resolves the image key; if the key is not found, it returns detailed error listing all currently available image identifiers to guide the model towards valid call. We apply strict validation on the bounding box: we check that it consists of four numeric values, each in [0, 1000], and that x1 < x2 and y1 < y2. The normalized coordinates are then converted into absolute pixel coordinates based on the underlying image size, which is obtained via lightweight fetch image helper compatible with Qwen-VL style inputs. dedicated helper, maybe resize absolute bbox, clamps the box to the image boundaries, enforces reasonable aspect ratios, and ensures that the cropped region is not too small. In particular, we require that both width and height of the final crop are at least MIN QWEN DIMENSION pixels (set to 28 in our experiments). If the original box is too small, we automatically expand it around its center while still respecting the image boundaries; any invalid or degenerate boxes are omitted. Once valid bounding box is obtained, the tool crops the image accordingly. For very small crops (e.g., thumbnails or tiny regions), we optionally upsample the crop by factor of 2 using bicubic interpolation to improve readability. The tool returns the cropped image and an instructional text that (i) names the new observation (e.g., observation 2), (ii) reminds the model to continue its reasoning within <think>...</think>, and (iii) encourages additional tool calls or final answers as appropriate. This design makes the zoom-in tool composable and easy to chain with the document retrieval tools. Textual instruction-following tools. The third family of tools targets fine-grained textual instruction-following and is used to automatically verify whether generated response satisfies structural and lexical constraints. All such tools inherit from shared BaseInstructionFollowingTool, which automatically constructs the function schema from declarative parameters list and manages per-instance response store. The response store exposes texts map that maps logical keys (e.g., text 0) to full string outputs. helper resolve from store resolves these keys, and provides informative errors that enumerate available keys when resolution fails. Each concrete tool implements an asynchronous execute logic method that returns boolean, which is then wrapped into textual ToolResponse of the form Check result: True/False. We implement several categories of instruction-following tools: Length and segmentation constraints. Tools such as ParagraphNumberInRangeTool, SentenceNumberInRangeTool, EachParagraphSentenceNumberInRangeTool, EachParagraphSentenceNumberInRangeListTool, WordCountInRangeTool, and EachParagraphWordCountInRangeTool check whether the total or perparagraph number of paragraphs, sentences, or words falls within specified bounds. Sentences are segmented using NLTKs sentence tokenizer, and paragraphs are defined via blank-line separation. For poetry-like formatting, EachParagraphSentenceNumberInRangeTool automatically switches to line-based heuristic. 23 Lexical and formatting constraints. Tools including NotContainSubstringTool, NotContainSubstringsTool, EachSentenceEndsWithTool, EachParagraphBeginsWithTool, EachParagraphEndsWithTool, ResponseBeginsWithTool, ResponseEndsWithTool, and NoArabicNumberTool enforce constraints on the presence or absence of certain substrings, required prefixes or suffixes at the sentence or paragraph level, or the absence of standalone Arabic numerals. All matching is done in case-insensitive manner after light normalization that strips punctuation and ellipses from boundaries. EachSentenceBeginsWithTool, Keyword coverage. EachKeywordMentionedInRangeTool and TotalKeywordsMentionedInRangeTool check how often specific keywords appear in the response. We support both individual per-keyword bounds as well as global bounds over the total mention count. specialized matcher handles hashtags and other special characters robustly by constructing appropriate regular expressions. Numeric precision. PercentagePrecisionTool and NumberPrecisionTool verify that all percentage expressions or decimal numbers in the response have exactly specified number of digits after the decimal point. This allows us to enforce formatting requirements such as report all percentages with two decimal places in an automatic and tool-based manner. These tools are purely textual and do not manipulate images, but they are implemented within the same function-calling framework as the multimodal tools. This uniform design allows the policy to learn single tool-use interface while exhibiting rich behaviors: retrieving and inspecting visual context, zooming into fine-grained regions, and verifying that its final textual outputs satisfy complex instruction-following constraints."
        },
        {
            "title": "F Prompts",
            "content": "Prompts Used in Evaluation. To ensure consistent and reproducible evaluation across all tasks in ARMBench-VL, we employ three types of standardized prompts. These prompts are used respectively for (1) constraint-based instruction following judgment, (2) pairwise response comparison, and (3) agent-style chain-of-thought evaluation with tool use. Fig. 5 presents the prompt used for single-response judging, where the model must determine whether given prediction satisfies the explicit constraint provided in the instruction. Fig. 6 illustrates the pairwise (N-way) comparison prompt, where the judge model evaluates two or more candidate responses and selects the better one, prioritizing correctness. Although the figure shows the 2-way setup, the same structure naturally extends to 4-way or more candidates in our benchmark. Finally, Fig. 7 shows the fixed agent-style CoT prefix and suffix prompt appended to every task when evaluating agentic models (e.g., ARM-Thinker-7B). This prompt unifies the handling of image-based and documentbased inputs and enforces structured reasoning, controlled tool usage, and explicit answer formatting. Together, these three prompts cover the full range of evaluation scenarios in ARMBenchVL, enabling fair comparison across standard judges, reward models, and agentic multimodal evaluators. Prompts Used in Data Construction. To systematically construct training and evaluation data for ARMBench-VL, we further employ two standardized prompt templates targeting response generation and question rewriting. Fig. 8 presents the long response generation template, where the model is given questionsolution pair and asked to produce one detailed response that faithfully follows the correct solution, together with three linguistically diverse but subtly incorrect responses. The incorrect responses are required to remain plausible while deviating from the groundtruth solution, providing hard negative examples for training and evaluating reward models and judges. Fig. 9 illustrates the caption-style question rewriting and response template, which takes an image, an original question, and its solution as input. The prompt first rewrites the orig24 inal question into descriptive describe . . . in detail style query focused on the region relevant to the solution, and then generates two image-grounded descriptions: one consistent with the solution and one subtly inconsistent in fine-grained details. This enables the construction of visually grounded contrastive pairs for judging nuanced description quality. Together, these two data-construction prompts support scalable generation of controlled positive and negative examples across both text-centric reasoning tasks and image-centric caption-style tasks in ARMBench-VL."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Innovation Institute",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong",
        "Zhejiang University"
    ]
}