{
    "paper_title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
    "authors": [
        "Yifan Du",
        "Zikang Liu",
        "Yifan Li",
        "Wayne Xin Zhao",
        "Yuqi Huo",
        "Bingning Wang",
        "Weipeng Chen",
        "Zheng Liu",
        "Zhongyuan Wang",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems. To address this issue, in this paper, we explore a straightforward approach by fine-tuning a capable MLLM with a small amount of textual long-form thought data, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github.com/RUCAIBox/Virgo."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 4 0 9 1 0 . 1 0 5 2 : r Technical Report on Slow Thinking with LLMs: Visual Reasoning Virgo: Preliminary Exploration on Reproducing o1-like MLLM Yifan Du1,2, Zikang Liu1, Yifan Li1, Wayne Xin Zhao1, Yuqi Huo2, Bingning Wang2, Weipeng Chen2 Zheng Liu3, Zhongyuan Wang3, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China. 2Baichuan AI. 3BAAI. {yifandu1999, jasonlaw8121, liyifan0925, batmanfly}@gmail.com Figure 1: The radar chart compares the performance of top-tier MLLMs across four challenging multimodal benchmarks. Our model, Vigor-72B (fine-tuned from Qwen2-VL-72B-Instruct), demonstrates leading performance."
        },
        {
            "title": "Abstract",
            "content": "Recently, slow-thinking reasoning systems, built upon large language models (LLMs), have garnered widespread attention by scaling the thinking time during inference. There is also growing interest in adapting this capability to multimodal large language models (MLLMs). Given that MLLMs handle more complex data semantics across different modalities, it is intuitively more challenging to implement multimodal slow-thinking systems. To address this issue, in this paper, we explore straightforward approach by fine-tuning capable MLLM with small amount of textual long-form thought data, resulting in multimodal slow-thinking system, Virgo (Visual reasoning Equal contribution. Correspondence to Wayne Xin Zhao. Preprint. Under review. with long thought). We find that these long-form reasoning processes, expressed in natural language, can be effectively transferred to MLLMs. Moreover, it seems that such textual reasoning data can be even more effective than visual reasoning data in eliciting the slow-thinking capacities of MLLMs. While this work is preliminary, it demonstrates that slow-thinking capacities are fundamentally associated with the language model component, which can be transferred across modalities or domains. This finding can be leveraged to guide the development of more powerful slow-thinking reasoning systems. We release our resources at https://github. com/RUCAIBox/Virgo."
        },
        {
            "title": "Introduction",
            "content": "Recently, slow-thinking reasoning systems (e.g., OpenAI o1 [1], DeepSeek R1 [2], and Qwen QwQ [3]) have demonstrated significant performance improvements across various benchmarks, particularly excelling in challenging problems where previous large language models (LLMs) [4] have underperformed [5, 6, 7]. These systems employ both train-time and test-time scaling to enhance accuracy and capacity in solving complex tasks, which typically involve an extended reasoning process, referred to as thought, before reaching the final solution. While these systems primarily focus on textual problems, latest advancements have also shown promising results in multimodal scenarios (e.g., QVQ [8]). Although commercial companies have not disclosed the underlying techniques for creating slowthinking systems, researchers have made significant efforts to advance public technical progress in this area. According to the existing literature, there are two typical approaches to implementing slow-thinking reasoning systems [9, 10, 11, 12, 13]. The first approach utilizes an explicit search structure (e.g., Monte Carlo tree search) and employs specially trained reward models to guide the search process toward the correct path. The second approach involves distilling or constructing instruction data in the form of long chain-of-thought (CoT) and then fine-tuning capable LLM to learn such reasoning modes. This method can be further enhanced using self-improvement or self-play methods such as direct preference optimization and reinforcement learning. major limitation of these research studies is their primary focus on textual tasks, with relatively little consideration given to multi-modal scenarios. To our knowledge, existing efforts in developing multimodal reasoning systems still significantly lag behind commercial systems like o1 and QVQ. This paper aims to explore the implementation of multimodal slow-thinking reasoning systems that can achieve performance comparable to these commercial systems. Unlike text-based reasoning systems, multimodal reasoning systems (i.e., Multimodal LLMs, or MLLMs) ([14, 15]) often leverage both the abilities of perception (i.e., understanding the visual semantics of the input figures) and reasoning (i.e., determining the approach to solving given task). Consequently, it is still unclear how slow-thinking operates on multimodal inputs. Overall, we believe that developing multimodal slow-thinking reasoning systems presents significant challenge, as it involves addressing the complex data and modeling mechanisms inherent in multimodal tasks. Considering this challenge, we aim to explore simple idea to implement multimodal reasoning systems: Can we directly adapt MLLMs by fine-tuning them with text-based long thought data? The intuition is that, since reasoning is primarily handled by the LLM component within an MLLM, we might be able to elicit its slow-thinking capacities using text-only instructions. In fact, there is growing evidence in enhancing the capabilities of MLLMs by leveraging more extensive text-only instruction [16, 17]. In this paper, we design straightforward approach to enhance MLLMs with slow-thinking capacities by leveraging text-based long thought data. Our primary focus is investigating two key questions: (1) Can the slow-thinking ability transfer across modalities through fine-tuning with text-based long thought data? (2) Can the ability derived from text-based long thought data be comparable to that distilled from multimodal slow-thinking systems? Specifically, we consider collecting textual reasoning instructions shared by previous studies [18], as well as generating visual reasoning instructions by distilling from multimodal slow-thinking systems. Following this method, we implement multimodal slow-thinking systems, denoted as Virgo (Visual reaoning with long thought). We select capable MLLM, Qwen2-VL-72B-Instruct, as the backbone model and employ different instruction datasets to fine-tune this MLLM. This approach allows us to examine the reasoning performance of MLLMs when trained with various instructional datasets. To study the research questions we propose, we conduct extensive experiments on four challenging benchmarks: MathVerse, MathVision, OlympiadBench, and MMMU. Our empirical study reveals that even when utilizing only text-based long thought data, Virgo achieves very promising results, comparable to or even surpassing those of commercial reasoning systems. Another major finding is that text-based reasoning instructions are generally more effective than multimodal reasoning data for eliciting the slow-thinking capacities of MLLMs."
        },
        {
            "title": "2 Method",
            "content": "In this section, we present our preliminary attempts to adapt MLLMs by equipping them with slow-thinking capacities for complex multimodal tasks. We explore two straightforward adaptation methods: (1) transferring slow-thinking abilities using text-based long thought data, and (2) distilling multimodal long thought data from existing slow-thinking MLLMs. Our aim is to investigate how slow-thinking capacities are elicited in MLLMs and to identify which approaches are more effective for achieving this goal. Next, we describe the specific implementation details."
        },
        {
            "title": "2.1 Capacity Transfer from Text-only Instructions",
            "content": "Previous studies [18] have shown that slow-thinking reasoning is likely behavioral mode that can be elicited by fine-tuning with small amount of long thought data. Moreover, this capacity can generalize across different domains. Therefore, our idea is to investigate whether this ability can also transfer to different modalities, given that existing MLLMs are developed with LLMs as their backbone."
        },
        {
            "title": "2.1.1 Collecting Textual Long Thought Data",
            "content": "We begin by collecting textual long thought data from our previous study [18]. Specifically, we obtain approximately 5K long thought instruction instances distilled from two open slow-thinking reasoning systems: DeepSeek-R1-Lite-Preview [2] (abbreviated as R1) and QwQ-32B-preview [3] (abbreviated as QwQ). The statistics of the collected instruction data are categorized by domain as follows: math (3.7K), science (0.9K), code (0.2K) and puzzle (0.1K). We select the majority of the instruction data from the math domain because it contains more challenging problems that require longer reasoning processes. These instructional data are formatted with two distinct parts: the thought process, indicated by special symbols <begin_of_thought> and <end_of_thought>, and the final solution, indicated by special symbols <begin_of_solution> and <end_of_solution>. More details about the data composition and instruction format can be found in our previous paper [18]."
        },
        {
            "title": "2.1.2 Textual Long Thought Instruction Tuning",
            "content": "After collecting instruction data for long-form reasoning, we fine-tune the base MLLM to emulate slow-thinking reasoning behavior. We choose Qwen2-VL-72B-Instruct [19] as the target model due to its excellent multimodal capabilities. Additionally, our previous work [18] indicates that slow-thinking capacities are more readily achieved in stronger models. To optimize the target MLLM, we train only the parameters from the LLM and cross-modal connector while keeping the parameters in the visual encoder frozen. We use the following optimization settings: learning rate of 7e-6, batch size of 128, and training for 10 epochs. Based on the performance on the development set, we select the model at the 5th epoch for evaluation. We do not employ more advanced training algorithms, such as DPO [20] and RLHF [21], as our objective is not to attain the maximum possible performance. Instead, we aim to explore the potential of transferring slow-thinking capacities through simple fine-tuning with textual long thought data. Our aim is to investigate the effect of textual long thought data using the straightforward imitation method."
        },
        {
            "title": "2.2 Capacity Distillation from Slow-thinking MLLMs",
            "content": "The second approach we explore is the direct distillation of multimodal long thought data from slow-thinking MLLMs (e.g., QVQ). This approach aims to achieve two goals: first, to compare the fine-tuning performance of textual long thought data with that of multimodal long thought data, and second, to investigate the potential effects of combining both textual and multimodal instruction data."
        },
        {
            "title": "2.2.1 Visual Long Thought Data Collection",
            "content": "To construct visual long thought data, crucial step is to gather set of high-quality visual problems, which include both task descriptions and images as input. Additionally, these problems should be accompanied by ground-truth answers for correctness verification. We consider selecting problems from visual question answering (VQA) datasets to cover diverse domains such as geometry, tables, figures, and icons. We select these domains because they typically present more challenging problems for MLLMs. Specifically, we select four geometry datasets (Geos [22], GeoQA+ [23], Geometry3K [24], and UniGeo [25]), three table and figure datasets (TabMWP [26], FigureQA [27], and ChartQA [28]), and an object dataset (CLEVR [29]). These datasets can be accessed from the LLaVA-OneVision [30] data collection, where each instance provides question, image, and answer triple. Detailed statistics for each dataset are presented in Table 1. Table 1: The statistics of the selected visual query sources Domain Geometry Table, Chart, and Figure Object Dataset Geos GeoQA+ Geometry3K UniGeo TabMWP FigureQA ChartQA CLEVR # Samples 279 551 555 568 589 509 To complete these problems with long thought processes, we consider two approaches: either distilling from existing slow-thinking MLLMs or utilizing our fine-tuned MLLMs with textual long thought data. We assume that fine-tuning MLLMs with textual long thought data can effectively transform them into slow-thinking MLLMs, essentially engaging in self-distillation process. For existing slow-thinking MLLMs, we select the recently released QVQ model, which demonstrates superior performance on several challenging benchmarks. To generate the reasoning process, we use the commonly employed rollout method by randomly sampling responses from both the QVQ model and our own. We set special format to parse the final answer from the response for each problem, retaining only those problems that the models can successfully solve within reasonable number of rollouts. Intuitively, simpler problems require fewer rollouts to solve. We will further discuss the impact of problem difficulty on the fine-tuning performance of MLLMs in Section 3.3."
        },
        {
            "title": "2.2.2 Visual Long Thought Instruction Tuning",
            "content": "When distilling the long thought data from QVQ (denoted by DQVQ), the training process is straightforward: we fine-tune only the parameters of the LLM and the modality connector, as we do with the textual long thought data described in Section 2.1. Although the visual instruction data includes image information, our experimental results indicate that updating the visual encoder does not result in substantial performance improvement. As another alternative approach, we design multi-stage tuning method for self-distillation. Specifically, we first fine-tune the selected MLLM (i.e., Qwen2-VL-72B-Instruct) on the textual long thought instruction set DT, obtaining model M0. Next, we use M0 to generate the visual long thought instruction set by self-distillation DSD, which can be subsequently used for fine-tuning the original MLLM. In our experiments, our aim is to investigate the effects of individual instruction datasets (i.e., DT, DSD and DQVQ) and their combinations on the slow-thinking performance. 4 Table 2: Performance comparison of top-tier MLLMs on four representative benchmarks. Here, DT denotes the textual long thought data, and DSD and DQVQ denote the visual long thought data distilled by our model (the version fine-tuned by DT) and QVQ, respectively. The bold fonts denote the best performance among our training variants, while the underline fonts denote the second-best performance. * Since QVQ has not released the evaluation code, we report the evaluation results reproduced by our team. Model Num. Data Textual Visual MathVerse MathVision Olympiad Bench MMMU Average GPT-4o Gemini-Pro Claude-3.5-Sonnet OpenAI o1 QVQ-72B-preview* Qwen2-VL-72B-Instruct Virgo-72BDT Virgo-72BDQVQ Virgo-72BDSD Virgo-72BDTDSD Qwen2-VL-7B-Instruct Virgo-7BDT Virgo-7BDQVQ Virgo-7BDSD Virgo-7BDTDSD - - - - - - 5K - - 5K - 5K - - 5K - - - - - - - 6.6K 7K 7K - - 6.6K 7K 7K - 35.3 - - 41.5 41.3 48.4 37.6 47.4 48.1 24.6 32.2 29.2 37.5 36.7 30.4 19.2 38.0 - 35.9 26.1 38.8 37.7 35.0 38.6 16.3 24.3 20.5 23.1 24. 25.9 4.2 - - 27.9 11.2 29.9 25.0 27.2 28.5 5.3 9.8 9.0 10.3 10.2 69.1 65.8 70.4 77.3 66.0 64.5 64.6 62.6 65.8 65.0 54.1 47.1 48.3 50.7 46. - 31.13 - - 42.83 35.78 45.43 40.73 43.85 45.05 25.08 28.35 26.75 30.40 29."
        },
        {
            "title": "3.1 Evaluation Setup",
            "content": "To validate the effectiveness of our methods, we conduct experiments on four challenging benchmarks: MathVerse [31], MathVision [32], OlympiadBench [33], and MMMU [34]. MathVerse consists of 2,612 multi-subject math problems from diverse sources. MathVision comprises 3,040 high-quality mathematical problems sourced from established mathematics competitions. OlympiadBench features 8,476 bilingual multimodal problems for Olympic-level mathematics and physics competitions. MMMU encompasses 11,500 problems spanning 30 subjects and 183 subfields. To ensure fair comparison, we conduct evaluations on the validation set of MMMU and the testmini set of MathVerse. Following VLMEvalKit [35], we exclude the text-only split from MathVerse and the theorem-proof parts from OlympiadBench. Among all the benchmarks, OlympiadBench is the most challenging, while MMMU demonstrates relatively lower difficulty levels and focuses more on comprehensive subject knowledge. We select Qwen2-VL-72B-Instruct [19] as our base MLLM due to its strong multimodal capabilities. We denote our model as Virgo-72B and compare it with range of models that are capable of conducting o1-like slow-thinking (i.e., OpenAI o1 and QVQ-72B-preview). We also include advanced general-purpose models (i.e., GPT-4o, Gemini-Pro, and Claude-3.5-Sonnet) for comparison. We also train Virgo-7B based on Qwen2-VL-7B-Instruct to further study the influence of model size."
        },
        {
            "title": "3.2 Main Results",
            "content": "In this section, we provide comprehensive performance comparison of various methods on the selected evaluation benchmarks, as summarized in Table 2. The results include the performance of o1-like MLLMs, general-purpose MLLMs, and our approaches that extend the backbone model with different long thought instruction datasets. First, the slow-thinking reasoning ability can be effectively transferred through text-only reasoning data. As demonstrated in the second group of Table 2, after fine-tuning with only 5K textual long thought instructions, our model yields highly competitive results, approaching and even surpassing those of industry counterparts. For instance, our model achieves 38.4% accuracy on MathVision and 29.3% accuracy on OlympiadBench. However, another observation is that our model does not show 5 Figure 2: The relationship between the average thought length of each benchmark and the corresponding performance of both Virgo and Qwen2-VL-72B-Instruct. The average thought length is represented by the line, while performance is indicated by the bar. The bars in light color represent Vigors performance, while the bars in dark color represent Qwen2-VL-72B-Instructs performance. We observe that benchmarks with longer thought lengths generally correspond to greater performance improvements. significant improvement on the MMMU benchmark. To thoroughly analyze the performance limitations on MMMU, we further examine fine-grained performance by using the difficulty annotation of the test samples: easy, medium, and hard. As shown in Table 3, our method lags behind QVQ in overall performance, with the disadvantage mainly concentrated in the easy and medium samples. For samples in the hard bin, our method achieves an accuracy of 54.70%, compared to QVQs 48.62%. As we will discuss in Section 3.4, not all visual problems require complex reasoning processes, and enforcing longer thought process might lead to performance degradation of MLLMs. Secondly, synthesized visual instructions, whether obtained through distillation or self-distillation, do not significantly outperform textual reasoning instructions when fine-tuning the MLLM. Upon conducting human review of the synthesized trajectories for visual questions, we find that many questions are not sufficiently complex and rely more on perception than reasoning, despite that we have carefully selected the data source and conducted rigorous data filtering process to control the difficulty. Developing high-quality, complex visual instructions remains challenging direction for future exploration. Additionally, we conduct experiments on smaller MLLMs, specifically Qwen2-VL-7B-Instruct, as shown in the third group of Table 2. The performance trends observed with different reasoning instruction datasets show some deviations from those of the larger model, Qwen2-VL-72B-Instruct. Notably, Virgo-7BDSD outperforms Virgo-7BDT , particularly on MathVerse and MMMU, suggesting that visual long-thinking instructions are more effective than textual instructions for small MLLM. Another difference is that after fine-tuning with long thought instructions, the performance on MMMU has substantially decreased. We speculate that smaller model might be less capable of managing complex long thought processes, especially when applied to problems that do not necessitate complex reasoning (as MMMU appears to be simpler than the other three benchmarks). Incorporating visual instructions may alleviate this degradation."
        },
        {
            "title": "3.3 Further Analysis",
            "content": "After presenting the overall performance analysis, we further investigate the detailed effects of long thought instruction data on visual reasoning. We present the major findings below. Harder tasks benefit more from long thought reasoning. We first examine how our approach impacts model performance across tasks of varying difficulty levels. Previous research[18] has indicated correlation between the average length of responses generated by models and the complexity of the 6 Figure 3: The domain distribution of textual long thought instructions. questions: longer responses generally accompany more complex or challenging questions. Building on this insight, we analyze the average length of responses produced by our model on evaluation benchmarks and visualize the corresponding model performance in Figure 2. The results indicate that benchmarks with longer response lengths, such as OlympiadBench, tend to be more difficult, as evidenced by their lower accuracy. Notably, our approach demonstrates substantial improvements on these challenging benchmarks, achieving absolute gains of 18.1% and 12.4% on OlympiadBench and MathVision, respectively. Conversely, we observe limited performance gains on the relatively easier benchmark, MMMU, which is characterized by shorter response lengths. Table 3: Performance comparison on samples from different difficulty bins in MMMU. Model Easy Medium Hard Overall QVQ-72B-preview Qwen2-VL-72B-Instruct Virgo-72BDT Virgo-72BDTDSD 76.95 74.58 72.88 74.58 65.80 62.26 62.97 61.79 48.62 50.28 54.70 56. 66.0 64.5 64.6 65.0 Longer reasoning does not guarantee better results. Since reasoning capacity is influenced by the difficulty of the instruction data, we compare fine-tuning performance across different difficulty levels. We use simple method to determine instruction difficulty based on instruction length. Specifically, we train the model using textual long-thought instructions sampled from varying length ranges: (0, 2000], (2000, 4000], and (4000, 8000], and present the results in Table 4. The results indicate that increasing the length of reasoning in the training data from 2000 to 4000 tokens leads to performance improvements across all benchmarks. However, further increasing the length to 8000 tokens results in performance degradation on most benchmarks. To further examine the performance decrease associated with long instructions, we analyze the data composition of each length range and observe that the math domain dominates the long instruction data in the (4000, 8000] range. These math problems may result in excessively long instructions compared to the actual required lengths for visual reasoning tasks; even the longest OlympiadBench examples have an average length below 4000, as shown in Figure 2. Table 4: Performance comparison by tuning with instruction datasets of varying thought length. Base Model Length MathVerse MathVision OlympiadBench MMMU Qwen2-VL-7B-Instruct - (0, 2000] (2000, 4000] (4000, 8000] 24.0 28.1 33.6 24.8 15.6 22.1 24.4 24.9 5.3 8.8 9.1 8.3 54.1 41.9 48.0 39.8 Scaling textual instruction leads to improvement. We further investigate the impact of scaling textual instructions on reasoning performance. The results, presented in Table 5, demonstrate that increasing the number of textual instructions generally leads to performance improvements across 7 most benchmarks. Specifically, increasing the instruction samples from 1K to 5K results in 7.7% average performance gain for both the 7B and 72B models on MathVision, while showing modest 1.8% performance gain on OlympiadBench. These observations suggest that while scaling textual instructions is generally effective, its impact varies across different tasks. Another finding is that textual instructions initially diminish the models capacity on MMMU, but performance gradually recovers as more instructions are added. Table 5: The scaling effect of instruction data on the base model. Base Model Num. MathVerse MathVision OlympiadBench MMMU Qwen2-VL-72B-Instruct Qwen2-VL-7B-Instruct - 1K 3K 5K - 1K 3K 5K 41.3 42.5 44.4 48.4 24.0 22.5 30.2 31.9 26.1 39.5 40.5 38. 15.6 23.7 24.9 24.6 11.2 26.2 26.4 29.9 5.3 8.6 9.6 9.2 64.5 61.8 58.2 64.7 54.1 42.8 44.6 47.1 Difficulty of visual thought data has limited impacts on performance. In Section 2.2.2, we select visual problems from various domains and generate visual long thought instructions by distilling from QVQ and Virgo-72BDT . Our goal is to explore the impact of visual instructions with varying difficulty levels. Specifically, we first use Qwen2-VL-72B-Instruct, which has not been fine-tuned with long thought instructions, to generate responses for visual questions via greedy search. Questions that the base MLLM answers correctly are excluded, as they are considered relatively easy. For the remaining questions, Virgo-72BDT performs multiple rollouts, generating five candidate trajectories per question. Based on the ratio of correct trajectories, we define two levels of difficulty: medium, for questions with 4 or 5 correct trajectories, and hard, for those with 2 or 3 correct trajectories. To investigate how question difficulty affects model performance, we also randomly sample some questions, regardless of whether the base MLLM can solve them, and synthesize trajectories based on these questions. This set is referred to as the random-level. We combine 5K textual long thought instructions with each of the three splits (medium, hard, and random) to fine-tune Qwen2-VL-72B-Instruct and report the results in Table 6. The results show that visual instructions with different difficulty levels do not lead to significant performance differences. This suggests that advanced strategies for synthesizing visual long thought instructions are needed to enhance multi-modal slow-thinking reasoning capabilities. Table 6: Performance comparison of using self-distilled visual instruction of varying difficulty levels. Base Model Difficulty MathVerse MathVision OlympiadBench MMMU Qwen2-VL-72B-Instruct - Medium Hard Random 41.3 48.1 47.4 47. 26.1 38.6 39.1 38.5 11.2 28.5 29.7 29.3 64.5 65.0 63.0 64."
        },
        {
            "title": "3.4 Case Study",
            "content": "In this section, we present several examples to demonstrate the advantages of slow-thinking reasoning in addressing complex multimodal problems. Additionally, we provide examples highlighting some of the negative impacts introduced by our approach. Textual long thought instruction tuning elicits visual slow-thinking reasoning ability. In Table 7, the query requires evaluating the integral of function and involves an image composed of three semi-circles. Qwen2-VL-72B-Instruct directly calculates the radius and center of each semi-circle individually but makes errors in determining their centers. In contrast, our model first describes the image in detail (highlighted in orange), then thoroughly reasons through the question, and finally arrives at the correct answer. Furthermore, the model can reflect on its reasoning process and attempt to verify its solution (highlighted in blue). This case demonstrates that long thought training enhances 8 both the models detailed captioning ability and its capacity for self-reflection, which are crucial for performing complex reasoning tasks. Lack of reflection on perception causes reasoning to fail. By examining several failure cases, we observe that Virgo fails to reflect on its perception results, which can cause the entire reasoning process to collapse. representative case is illustrated in Table 8, where Virgo mistakenly perceives the number of unemployed individuals with high school diploma in September (highlighted in red). This leads to the incorrect conclusion that both August and September satisfy the problems requirements. While Virgo recognizes the irrationality of the result and begins to reflect on its reasoning process (highlighted in blue), it does not question the validity of its perception. As result, erroneous conclusions are repeatedly generated, leading to incorrect answers. This case highlights that slow-thinking MLLMs transferred from text-only instructions may have limited capacity for reflecting on perception. Future models should be designed with the ability to reflect on both perception results and reasoning processes."
        },
        {
            "title": "4 Conclusion",
            "content": "In this paper, we have proposed simple yet effective approach to implementing slow-thinking reasoning systems. Our core finding is that by simply fine-tuning MLLMs with textual long-form thought data, capable MLLM can exhibit substantially enhanced slow-thinking capacities. We have conducted extensive experiments on four challenging benchmarks, and the results demonstrate that our approach achieves highly competitive performance when compared to industry reasoning systems. We have also examined the effect of different factors of textual instruction data, such as data amount and length distribution. We are aware that our current attempt at building multimodal slow-thinking systems is preliminary. For future work, we aim to expand the sources of challenging multimodal problems and design more principled methods to enhance this capacity. 9 Problem: Evaluate the integral of the functions graphed using the formula for circles. Ground Truth: 7π Qwen2-VL-72B-Instruct: To evaluate the integral of the functions graphed using the formula for circles, we need to identify the equations of the circles and then integrate them over the appropriate intervals. 1. **Identify the Equations of the Circles:** - The equation of circle with center (h, k) and radius is given by: (x h)2 + (y k)2 = r2 2. **Determine the Centers and Radii:** - For the smallest circle: 2x x2 = 0 = x(2 x) = 0 = = 0 or = 2 The center is at (1, 1) and the radius is 1. - For the medium-sized circle: 12+8xx2 = 0 = (x2 8x+12) = 0 = x2 8x+12 = 0 Solving this quadratic equation: = 8 = 84 So, the centers are at (2, 2) and (6, 2). The radii are 2. - For the largest circle: 72 + 18x x2 = 0 = (x2 18x + 72) = 0 = x2 18x + 72 = 0 Solving this quadratic equation: = 18 So, the centers are at (6, 3) and (12, 3). The radii are 3. 3. **Integrate the Functions:** - For each circle, we integrate the function over the interval where it is non-zero. - The integrals will be over the intervals determined by the x-coordinates of the centers and the radii. 4. **Final Answer:** The integral of the functions graphed using the formula for circles is: 2 = 9 2 = 4 2 324288 2 = 186 6448 2 18π Virgo-72B (ours): <begin_of_thought> Alright, Ive got this graph here with three semicircles plotted. The x-axis ranges from 0 to 12, and the y-axis goes up to 5. Each semicircle is labeled with an expression involving x. The smallest one on the left is labeled (cid:112)1 (x 1)2, the middle one is 12 + 8x x2, and the largest one on the 72 + 18x x2. All of them are filled with light blue color, and there are arrows pointing right is upwards from the x-axis to the top of each semicircle, indicating the height at different points. Lets start with the smallest one: (cid:112)(2x x2). can factor out x2 from the expression: (cid:112)(x2 + 2x) = (cid:112)(x2 + 2x 1 + 1) = (cid:112)((x 1)2 + 1). So, this simplifies to (cid:112)(1 (x 1)2). Comparing this to the standard form, this seems like the upper half of circle centered at (1, 0) with radius 1. Next, the middle one: (cid:112)(12 + 8x x2)... Now that Ive identified the equations of these semicircles, need to evaluate the integral of the functions graphed using the formula for circles. Im assuming that means finding the area under each semicircle from their respective starting points to their endpoints on the x-axis. Since each of these is semicircle, the area under each curve would be half the area of the full circle. The area of full circle is πr2, so for semicircle, its 2 πr2. Table 7: An example that Virgo-72B outperforms Qwen2-VL-72B-Instruct (part 1). 10 2 π(1)2 = π 2 . Lets calculate the area for each semicircle: 1. Smallest semicircle: radius = 1, so area = 1 2. Middle semicircle: ...... So, the total area under all three semicircles is 7π. Wait minute, but the question says \"evaluate the integral of the functions graphed using the formula for circles.\" Does that mean need to integrate each function separately and then sum them up? Let me double-check. If integrate each function from their starting point to their endpoint, should get the area under each curve. ...... Given that, think my initial approach is correct: the integral of each function over its domain gives the area under the semicircle, which is half the area of the full circle. So, the final answer should be 7π. <end_of_thought> <begin_of_solution> To evaluate the integral of the functions graphed using the formula for circles, we first identify the equations of the semicircles from the given expressions: 1. The smallest semicircle is represented by the upper half of circle centered at (1, 0) with radius 1. ...... Thus, the final answer is: 2x x2, which simplifies to (cid:112)1 (x 1)2. This is <end_of_solution> 7π Table 7: An example that Virgo-72B outperforms Qwen2-VL-72B-Instruct (part 2) 11 Problem: Graph 1 shows the number of unemployed (in thousands) over five-month period, organized by level of education completed. The difference between the number of unemployed high school graduates and the number of unemployed workers without high school diploma was smallest in . A. June B. July C. August D. September E. October Ground Truth: D. September Qwen2-VL-72B-Instrct: D. September Virgo-72B (ours): <begin_of_thought> Alright, so Ive got this graph here that shows the number of unemployed people, in thousands, over five-month period, from June to October. It breaks down the unemployment numbers by different levels of education completed. There are four categories: Less than high school diploma, High school diploma, no college, Some college, and Bachelors degree and higher. Each category has line representing the number of unemployed workers each month. need to find out in which month the difference between the number of unemployed high school graduates and the number of unemployed workers without high school diploma was the smallest. So, need to look at the lines for High school diploma, no college and Less than high school diploma and see when the gap between them is the smallest. ...... In August, high school graduates are around 8.5 thousand, and those without high school diploma are about 11.5 thousand. The difference is about 3 thousand. In September, high school graduates are around 8 thousand, and those without high school diploma are about 11 thousand. The difference is about 3 thousand. Table 8: An example that Virgo-72B fails to reflect on perception results (part 1). 12 In October, high school graduates are around 8 thousand, and those without high school diploma are about 12 thousand. The difference is about 4 thousand. So, from this, it seems like in August and September, the difference is the smallest, about 3 thousand. ...... In August, high school graduates are around 8.5 thousand, and those without high school diploma are around 11.5 thousand. The difference is 3 thousand. In September, high school graduates are around 8 thousand, and those without high school diploma are around 11 thousand. The difference is 3 thousand. In October, high school graduates are around 8 thousand, and those without high school diploma are around 12 thousand. The difference is 4 thousand. So, the smallest difference is in August and September, both with 3-thousand difference. But need to make sure if theres only one answer or multiple. The question says was smallest in , which might imply single answer, but looking back, it could be in both August and September. Maybe should consider if theres tie for the smallest difference. Looking back, both August and September have the same difference of 3 thousand, while the other months have larger differences. Therefore, the difference was smallest in both August and September. But since the question might expect single answer, perhaps need to choose the earliest month, which would be August. Alternatively, maybe both options are acceptable. But in any case, the smallest difference occurred in August and September. <end_of_thought> <begin_of_solution> To determine the month with the smallest difference between the number of unemployed high school graduates and the number of unemployed workers without high school diploma, we analyze the graph provided. - In June, the difference is approximately 4 thousand. - In July, the difference is approximately 3.5 thousand. - In August, the difference is approximately 3 thousand. - In September, the difference is approximately 3 thousand. - In October, the difference is approximately 4 thousand. The smallest difference occurs in both August and September, with difference of 3 thousand each. Thus, the correct answer is: <end_of_solution> Table 8: An example that Virgo-72B fails to reflect on perception results (part 2)."
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Learning to reason with large language models, September 2024. [2] DeepSeek Team. Deepseek-r1-lite-preview is now live: unleashing supercharged reasoning power!, November 2024. [3] Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown, November 2024. [4] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. survey of large language models. CoRR, abs/2303.18223, 2023. [5] Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, Qunshu Lin, Junbo Zhao, Zhaoxiang Zhang, Wenhao Huang, Ge Zhang, Chenghua Lin, and Jiaheng Liu. comparative study on reasoning patterns of openais o1 model. CoRR, abs/2410.13639, 2024. [6] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yihen Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Lichao Sun, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Ninghao Liu, Bei Jiang, Linglong Kong, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, and Tianming Liu. Evaluation of openai o1: Opportunities and challenges of AGI. CoRR, abs/2409.18486, 2024. [7] OpenAI. Learning to reason with llms, 2024. [8] Qwen Team. Qvq: To see the world with wisdom, December 2024. [9] Jinhao Jiang, Zhipeng Chen, Yingqian Min, Jie Chen, Xiaoxue Cheng, Jiapeng Wang, Yiru Tang, Haoxiang Sun, Jia Deng, Wayne Xin Zhao, et al. Technical report: Enhancing llm reasoning with reward-guided tree search. CoRR, abs/2411.11694, 2024. [10] Di Zhang, Jianbo Wu, Jingdi Lei, Tong Che, Jiatong Li, Tong Xie, Xiaoshui Huang, Shufei Zhang, Marco Pavone, Yuqiang Li, Wanli Ouyang, and Dongzhan Zhou. Llama-berry: Pairwise optimization for o1-like olympiad-level mathematical reasoning. CoRR, abs/2410.02884, 2024. [11] Yuxiang Zhang, Shangxi Wu, Yuqi Yang, Jiangming Shu, Jinlin Xiao, Chao Kong, and Jitao Sang. o1-coder: an o1 replication for coding. CoRR, abs/2412.00154, 2024. [12] Yiwei Qin, Xuefeng Li, Haoyang Zou, Yixiu Liu, Shijie Xia, Zhen Huang, Yixin Ye, Weizhe Yuan, Hector Liu, Yuanzhi Li, and Pengfei Liu. O1 replication journey: strategic progress report part 1. CoRR, 2024. [13] Yu Zhao, Huifeng Yin, Bo Zeng, Hao Wang, Tianqi Shi, Chenyang Lyu, Longyue Wang, Weihua Luo, and Kaifu Zhang. Marco-o1: Towards open reasoning models for open-ended solutions. CoRR, abs/2411.14405, 2024. [14] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao, et al. Multimodal foundation models: From specialists to general-purpose assistants. Foundations and Trends in Computer Graphics and Vision, 16(1-2):1214, 2024. [15] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. MM-REACT: prompting chatgpt for multimodal reasoning and action. CoRR, abs/2303.11381, 2023. [16] Lei Li, Yuanxin Liu, Linli Yao, Peiyuan Zhang, Chenxin An, Lean Wang, Xu Sun, Lingpeng Kong, and Qi Liu. Temporal reasoning transfer from text to video. arXiv preprint arXiv:2410.06166, 2024. [17] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 14 [18] Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413, 2024. [19] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [20] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D. Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In NeurIPS, 2023. [21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. [22] Minjoon Seo, Hannaneh Hajishirzi, Ali Farhadi, Oren Etzioni, and Clint Malcolm. Solving geometry problems: Combining text and diagram interpretation. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 14661476, 2015. [23] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [24] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan Huang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning. arXiv preprint arXiv:2105.04165, 2021. [25] Jiaqi Chen, Tong Li, Jinghui Qin, Pan Lu, Liang Lin, Chongyu Chen, and Xiaodan Liang. Unigeo: Unifying geometry logical reasoning via reformulating mathematical expression. arXiv preprint arXiv:2212.02746, 2022. [26] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022. [27] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Ákos Kádár, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. [28] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. [29] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pages 29012910, 2017. [30] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [31] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pages 169186. Springer, 2025. [32] Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024. [33] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. 15 [34] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal In Proceedings of the IEEE/CVF understanding and reasoning benchmark for expert agi. Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. [35] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. In Proceedings of the 32nd ACM international conference on multimedia, pages 1119811201, 2024."
        }
    ],
    "affiliations": [
        "BAAI",
        "Baichuan AI",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
    ]
}