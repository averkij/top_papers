{
    "paper_title": "DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting",
    "authors": [
        "Yicheng Yang",
        "Pengxiang Li",
        "Lu Zhang",
        "Liqian Ma",
        "Ping Hu",
        "Siyu Du",
        "Yunzhi Zhuge",
        "Xu Jia",
        "Huchuan Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Subject-driven image inpainting has emerged as a popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces DreamMix, a diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce a disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and a Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github.com/mycfhs/DreamMix."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 2 ] . [ 1 3 2 2 7 1 . 1 1 4 2 : r DreamMix: Decoupling Object Attributes for Enhanced Editability in Customized Image Inpainting Yicheng Yang1, Pengxiang Li1, Lu Zhang1, Liqian Ma2, Ping Hu3, Siyu Du1, Yunzhi Zhuge1, Xu Jia1, Huchuan Lu1 1Dalian University of Technology, 2ZMO AI, 3University of Electronic Science and Technology of China {2286247133, lipengxiang}@mail.dlut.edu.cn, zhangluu@dlut.edu.cn Figure 1. DreamMix on various subject-driven image customization tasks. (a) Identity Preservation: DreamMix precisely inserts target object into any scene, achieving high-fidelity and harmonized composting results. (b) Attribute Editing: DreamMix allows users to modify object attributes such as color, texture, and shape or add accessories based on textual instructions. (c) Small Object Inpainting: DreamMix effectively performs small object insertion and editing while preserving fine-grained details and visual harmony."
        },
        {
            "title": "Abstract",
            "content": "Subject-driven image inpainting has emerged as popular task in image editing alongside recent advancements in diffusion models. Previous methods primarily focus on identity preservation but struggle to maintain the editability of inserted objects. In response, this paper introduces Equal contribution. Corresponding author. DreamMix, diffusion-based generative model adept at inserting target objects into given scenes at user-specified locations while concurrently enabling arbitrary text-driven modifications to their attributes. In particular, we leverage advanced foundational inpainting models and introduce disentangled local-global inpainting framework to balance precise local object insertion with effective global visual coherence. Additionally, we propose an Attribute Decoupling Mechanism (ADM) and Textual Attribute Substitution (TAS) module to improve the diversity and discriminative capability of the text-based attribute guidance, respectively. Extensive experiments demonstrate that DreamMix effectively balances identity preservation and attribute editability across various application scenarios, including object insertion, attribute editing, and small object inpainting. Our code is publicly available at https://github. com/mycfhs/DreamMix. 1. Introduction Recent advancements in diffusion models [13, 14, 28, 30, 31, 34] have catalyzed remarkable progress in image generation, thereby stimulating the evolution in image editing [3, 11, 27, 36]. Among these applications, subjectdriven image customization [10, 17, 33, 40] has garnered significant interest, which aims to generate realistic images of reference object in various contexts. To achieve this, existing methods utilize different adaption strategies to enable pre-trained text-to-image diffusion models to apply various editing effects while maintaining the objects identity. Despite their efficacy, these methods often focus on regenerating entire scenes, posing challenges when users want to insert specific objects into designated areas within given scene. This task, known as Subject-driven Image Inpainting, has significant demand in various practical applications, such as effect-image rendering, poster design, virtual try-on, etc. To address this task, existing methods [5, 6, 39] typically follow the image inpainting protocol, utilizing source image and binary mask as inputs. Additionally, exemplar information is extracted from reference images and employed to condition pretrained diffusion models [28, 31]. However, these reference visual features often encompass overly specific content, thereby constraining the generative capacity of pre-trained text-to-image models and complicating the effective editing of objects with desired effects or attributes. Recent efforts [18, 26] have been made to overcome this limitation by incorporating both image and text guidance through attention mechanisms. Nevertheless, challenges remain in balancing training data efficiency with visual editing quality. In this work, we draw inspiration from recent foundational inpainting models [8, 20], which have demonstrated an impressive ability to fill in image regions with highquality, semantically consistent content based on textual instructions. Leveraging the power of these pretrained textdriven inpainting models for subject-driven inpainting can help alleviate the data-accuracy trade-off, yet is non-trivial. potential solution is to adopt parameter-efficient image customization techniques (e.g., DreamBooth [33]) to adapt the models with few template images. However, this straightforward approach encounters certain limitations in achieving precise object insertion and modification. First, the denoising process in inpainting models may introduce global noise that interferes with local information integration when synthesizing target areas. Second, customization strategies like few-shot fine-tuning [10, 33] inject the subjects appearance into single identity token [sks]. Coupling all object attributes into [sks] may lead to overfitting on the objects appearance, ultimately diminishing the effectiveness of text-based editing instructions. To overcome the outlined limitations, we propose DreamMix, diffusion-based generative model for textsubject-driven image inpainting that achieves attribute-level precision and data efficiency. In DreamMix, we introduce disentangled inpainting framework that separates the process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH), enhancing both local subject integration and overall visual coherence. Additionally, we develop an innovative Attribute Decoupling Mechanism (ADM) to diversify the textual description and image pairs of decoupled attributes for the subject, effectively mitigating overfitting issues. At the testing phase, we implement Textual Attribute Substitution (TAS) module that employs an orthogonal decomposition strategy to separate interfering information from textual guidance, further improving object editing quality. As shown in Fig. 1, the proposed DreamMix demonstrates robust performance across various subject-driven inpainting applications, including identity preservation, attribute editing, and smallobject inpainting. Our main contributions can be summarized as follows: We propose DreamMix, method that leverages text and image guidance to allow precise object insertion and attribute-level modification within any scene. We introduce an attribute decoupling mechanism and textual attribute substitution module to mitigate identity overfitting and support diverse attribute editing. Our method demonstrates superior performance over previous approaches across various application scenarios, as evidenced by both quantitative metrics and subjective evaluations. 2. Related Work Image Customization. Recent advancements in generative models have significantly enhanced image customization techniques, allowing for more nuanced and flexible control over the content of generated images [11, 35 37, 40, 41]. Recently, subject-driven image customization methods [9, 10, 17, 33] have garnered increasing attention, which utilize diffusion models to produce high-quality and personalized images based on couple of template images. For instance, Textual Inversion [10] addresses the personalization challenge by using set of images depicting the same subject to learn special text token. DreamBooth [33] Figure 2. Overview of DreamMix. During finetuning, we use the source data {xs, ps} along with regular data {xr, pr} constructed via an attribute decoupling mechanism (Sec. 3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ disentangled inpainting framework (Sec. 3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose textual attribute substitution module (Sec. 3.4) to generate decomposed text embedding to enhance the editability of our method during testing. binds unique identifier to the concept by finetuning the entire U-Net [32] and the text encoder [29] together with collection of subject images. However, these methods primarily focus on regenerating entire images but struggle with situations where users want to edit specific local regions of an existing image. Image Inpainting. Image inpainting involves reconstructing missing or modified regions of an image. Powered by text-to-image diffusion models, recent works [1, 2, 21 23, 38, 42, 43] have shown significant improvements in repainting the local regions of existing images with text guidance. For example, SD-Inpainting [31] expands the pretrained diffusion models with concatenation of noise, masked image, and mask as input and retrains it to produce visually consistent inpainting results. PowerPaint [43] enhances the models ability to support various inpainting tasks by designing learnable task prompts for text-guided object restoration, context-aware image restoration, and object removal, etc. However, despite their effectiveness in text-driven image inpainting, these methods often encounter limitations in compositing user-specific objects into the desired regions of scene. Subject-driven Inpainting. Recent methods have started focusing on combining subject-driven image customization with image inpainting to form task named subject-driven inpainting [5, 6, 18, 26, 39]. To address this task, some methods [5, 6, 39] utilize an image-guided framework that retrains diffusion models using subject images as templates. However, the absence of original text guidance limits these methods to modify object attributes effectively. This motivates some approaches to develop an image-text-guided architecture that integrates these elements into latent attention layers [18, 26]. Nevertheless, most methods rely on largescale training protocol that involves large-scale text-image data construction and lengthy retraining process. While TIGIC [18] is faster, its tuning-free protocol often struggles to provide consistent editing results. In this paper, we propose DreamMix, which introduces disentangled inpainting framework and an attribute decoupling mechanism to achieve data-efficient, editing-enhanced subject-driven image inpainting. 3. Method Given background image x, binary mask m, and text prompt p, DreamMix aims to inpaint the local region of specified by to produce an image ˆx. The refilled areas should not only contain the subject in the reference image(s) xs but also align with the descriptions provided in the text prompt. The overall pipeline of DreamMix is illustrated in Fig. 2. It contains three key modules: disentangled inpainting framework, an attribute decoupling mechanism, and textual attribute substitution module, to achieve realistic editable subject-driven inpainting. We describe each module in detail in the following sections. 3.1. Preliminaries Text-to-Image Diffusion Models. Diffusion models [28, 31] generate samples from random noise by learning to estimate the distribution in the reverse process. Specifically, given random noise ϵ (0, I) and text prompt p, textto-image diffusion models use U-Net ϵθ to produce denoised image. Given dataset of = {x, p}, the entire model is trained using the following objective: LDM (θ; x, p) = EϵN (0,I),tU (0,T) (cid:2)ϵθ(xt; p, t) ϵ2 (cid:3) , 2 (1) where [0, ] is the time-step. xt = αtx + δtϵ where αt and δt are coefficients that determine the noise schedule of the diffusion process. Personalized Text-to-Image Models. Given few images of subject, DreamBooth family [17, 33] provides dataefficient protocol for adapting pretrained diffusion models to personalized text-to-image generation. In particular, these methods inject the subject appearance into an identity token [sks] and update the model weights by LDB = LDM(θ; xs, ps) + βLDM(θ; xr, pr). (2) Here, the first term is the personalization loss for subject appearance modeling, while the second one is the priorpreservation loss, with β as trade-off. Ds = {xs, ps} are user-specific subject images and prompts, and Dr = {xr, pr} are regularization images and prompts. 3.2. Disentangled Inpainting Framework Existing subject-driven inpainting methods [6, 26, 39] usually build on pretrained diffusion models, wherein the lowest resolution of latent features is 32 times smaller than the original input images. This substantial reduction in resolution hinders these methods ability to effectively inpaint objects in vert confined local regions. Additionally, in complex scenes, background interference can affect the editing control of text instructions, leading to imprecise synthesis of the fill-in areas. To overcome these challenges, we propose an effective disentangled inpainting framework, consisting of Local Content Generation (LCG) and Global Context Harmonization (GCH), to enhance both local subject integration and global visual coherence. The overall framework is illustrated in Fig. 2 (c). Local Content Generation. Some recent research [4, 7, 27] suggests that diffusion models exhibit potential for course-to-fine generation, characterized by early-stage layout construction, intermediate-stage structure enhancement, and late-stage detail refinement. This insight motivates us to partition the entire inpainting process into two consecutive stages: LCG and GCH. Specifically, we implement LCG at the preceding time steps of λT , where λ [0, 1] is trade-off for time step separation. To improve the generation ability across varying-sized areas, we crop the background image into local patch and encode it by: zL = Enc(Crop(xs, m)). (3) Here, Crop() denotes an image cropping operation guided by mask m, which is produced by slightly enlarging the original local region of to involve more context. Enc() is the VAE encoder, and zL is the latent code of local image. At each time step, the U-Net ϵθ generates the denoised latent code zt by taking the text prompt as guidance: zt = ϵθ(zt+1; pdec, t), (4) where zt+1 is the output at + 1 step, and pdec represents the decomposed text embedding that will be introduced in Sec. 3.4. To maintain the input format of pretrained inpainting models, we exploit blending strategy during inference to ensure that non-editing regions remain unchanged. This process can be formulated as: ˆzt = zt + zL (1 m), (5) where denotes element-wise multiplication and zL is the noisy latent code of xL. ˆzt denotes the blended latent code that will be fed to the U-Net in the next time step as in Eq. 4. After the denoising process of λT steps, we project the final result as an RGB image ˆxL and repaste it into the original image to form an intermediate image xG, which will be used as an image guidance in the global context harmonization stage. Global Context Harmonization. The local content generation in the preceding time steps effectively yields accurate object compositing with well-defined layout and object appearance. However, without incorporating the global image content, it tends to produce disharmonious copypaste effect in the inpainting areas. To address this issue, we implement global context harmonization module on the remaining (1 λ)T steps to improve the overall harmony between the generated local region and the background image. Similar to LCG, we start by transforming the intermediate image xG into latent code zG using Eq. 3 as zG = Enc(Crop(xG, m)). Here, is the original userspecific mask. Then, we adopt the same blending strategy in GCH as follows: ˆzt = zt + zG (1 m). (6) After executing Eq. 6 in the remaining (1 λ)T steps, we obtain the final latent code ˆz0, which will be fed to VAE decoder to generate the inpainting images. 3.3. Attribute Decoupling Mechanism Few-shot personalization techniques have gained significant attention for their ability to balance data efficiency with generalization capability. However, limitations remain within the few-shot training protocol. On one hand, the textual descriptions used during finetuning are often overly simplified, providing only basic information about the subjects appearance. For example, in DreamBooth [33], an image is typically described as [classname]. On the other feature learning: LRE(θ; x, p, m) = τ1(m ϵθ(x; p) ϵ2 τ2((1 m) ϵθ(x; p) ϵ2 2)+ 2), (7) where τ1 = 1.5 and τ2 = 0.7 to emphasize the fill-in regions while suppressing the background area. ˆx = ϵθ(x; p) represents the pretrained inpainting model that generates the denosied output ˆx from background image x, text prompt p, and binary mask m. As illustrated in Fig. 2 (a), we combine both subject data Ds and regular data Dr with this reweight loss to form an overall loss: LF inal = LRE(θ; xs, ps, ms) + βLRE(θ; xr, pr, mr), (8) where the first term is for personalized generation and the second is regularization loss. And β = 0.4 is trade-off. 3.4. Textual Attribute Substitution The attribute decoupling mechanism employs advanced VLMs to enhance the diversity of training samples, thereby effectively adapting pretrained inpainting models for subject-driven inpainting. However, due to the lack of unseen attribute words during training, relying solely on attribute decoupling mechanism still poses challenges, especially when the target attributes differ significantly from the object identity. To address this, we introduce Textual Attribute Substitution (TAS) module during the testing phase to further mitigate the influence of object identity for more precise attribute editing. The pipeline of TAS is illustrated in Fig. 2 (b). Specifically, given text prompt from users, we first query VLMs [24] to retrieve the matched attributes from the attribute dictionary that is produced in attribute decoupling mechanism. The selected attribute and the user prompt are then sent to pretrained text encoder [29] to produce their latent embeddings, denoted as peli and praw. Next, we utilize an orthogonal decomposition strategy on the text embeddings to surpass the influence of original attributes in object editing, which is calculated as follows: pdec = praw (cid:18) praw peli peli (cid:19) peli peli , (9) where praw peli denotes the dot product of praw and peli, and peli is the norm of peli. After applying this embedding substitution, the conflicting features of the original object identity are effectively decoupled, making the inpainting model focus on the demand of the target prompt. As shown in Fig. 2 (c), we apply the decomposed text embedding pdec to the cross-attention layers at both LCG and GCH stages to boost the editing efficacy of the target text. Figure 3. Pipeline of Attribute Decoupling Mechanism (ADM). We obtain the attribute word list using VLM agent [24] and create regular data with more diverse text formats and image contents. hand, these methods compress all the objects details (e.g., color, texture, and shape) into single identity token [sks]. However, finetuning diffusion models on just few imagetext pairs can lead to subject overfitting, ultimately limiting the models generative ability in text-driven editing. In response, we propose an Attribute Decoupling Mechanism (ADM) to address the data scarcity and subject overfitting issues. To circumvent the heavy workload of manual annotation, we leverage advanced Vision-Language Models [24] to establish an automatic data reconstruction pipeline. The overall framework of ADM is illustrated in Fig. 3. Specifically, given set of subject image-text pairs Ds = {xs, ps}, we feed the subject images xs into VLM to generate an attribute dictionary that enumerates all attribute words associated with the given subject. Then, we ask the VLM to randomly combine the detected attributes with the original subject to form series of text prompts with detailed descriptions such as [sks] [attributes] [class name] [attributes]. Finally, we generate new images based on the reconstructed text prompts to form regular benchmark Dr = {xr, pr}. Combining source data with regular data to finetune the pretrained inpainting models can help reduce overfitting, yet is non-trivial. DreamBooth finetunes the model parameters beyond just identity tokens, which can lead to attribute words mixing up details. For example, in brown clay teapot, the color word might inadvertently bind some texture details, making it difficult to change the texture when transitioning to brown glass teapot, or influence the precise of texture features when switching to red clay teapot. We refer to this phenomenon as Concept Infusion. Similarly, SID [16] notes that adding detailed descriptions can disrupt the objects identity, aligning with this Concept Infusion issue. To mitigate this issue, we utilize the regular data Dr from ADM for model finetuning and redesign the data reconstruction loss to fit the inpainting task. Specifically, we employ user masks to enforce the inpainting models to fill specific regions. And we propose loss weight reallocation strategy on Eq. 1 to reduce the influence of background on Method Identity Preservation Attribute Editing CLIP-T CLIP-I DINO FID CLIP-T CLIP-I DINO FID Large-scale Training Paint-by-Example [39] MimicBrush [5] AnyDoor [6] IP-Adapter [40] LAR-Gen [26] Tuning-free TIGIC [18] Few-shot Fine-tuning DreamBooth DreamBooth [33] DreamMix DreamMix Real Image 0.238 0.256 0.284 0.268 0.269 0.262 0.248 0.253 0.284 0.285 0.315 0.541 0.614 0.688 0.633 0.662 0. 0.609 0.644 0.713 0.728 0.883 0.582 0.631 0.711 0.633 0.676 0.584 0.604 0.628 0.685 0.712 0. 44.6 47.0 53.7 45.3 48.7 45.2 40.8 40.5 43.4 42.9 35.9 - - - 0.236 0.257 0. 0.235 0.235 0.284 0.289 - - - - 0.611 0.656 0.479 0.567 0.601 0.659 0.674 - - - - 0.629 0.668 0.548 0.595 0.612 0.672 0.695 - - - - 47.5 49.6 47. 42.0 41.7 44.2 43.9 - Table 1. Quantitative comparison of different methods on Identity Preservation and Attribute Editing. The - symbol indicates that the method dont support text-driven editing. We divide the compared methods into three types based on their training protocol: large-scale training, tuning-free, and few-shot finetuning. indicates that only one image is used in model finetuning for each subject. 4. Experiments 4.1. Experiment Setup Implementation Details. We use Fooocus inpainting model [20], variant of SDXL [28], as our base generator. LoRA [15] with rank of 4 is applied to Wk and Wv in the attention layers. During finetuning, images are resized to 1024 1024, and we use the Adam optimizer with an initial learning rate of 1e 4. The total finetuning time for each subject is proximately 20 minutes on single RTX 4090 GPU. The hyperparameter λ in DIF is set to 0.7. Benchmarks. We follow prior works [6, 18, 26] to use 30 subjects from DreamBooth [33] for performance evaluation. We construct benchmark that contains over 4,000 high-quality image-mask pairs. Specifically, we collect the images from the COCO-val2017 dataset [19], filtering out images or bounding boxes with lower resolutions. We mix the 30 subjects with these background images, resulting in an average of approximately 140 background images per subject. Besides, we create 35 prompts for identity preservation or attribute editing for each object. Evaluation Metrics. Our method employs both image and text guidance for simultaneously object composting and text-driven editing. To evaluate subject identity similarity, we follow DreamBooth [33] to compute CLIP-I [29] and DINO [25] metrics between the generated images and the source images. To assess the efficacy of attribute editing, we compute the CLIP-T metric to measure the cosine similarity between the generated images and the text prompts. To evaluate the realism of generated images, we measure the Frechet Inception Distance (FID) [12] using the COCOtest2017 dataset as real image distribution reference. Additionally, we conduct user studies as subjective evaluation metric for our method. 4.2. Comparison with Other Methods To highlight the efficacy of our method on identitypreserved object compositing and text-driven attribute editing, we conduct comparison experiments on these two tasks separately. The quantitative and qualitative comparison results are illustrated in Tab. 1 and Fig. 4. We select range of classic and recent methods and categorize them into three types: large-scale training, tuning-free, and few-shot finetuning. For most of the compared methods [5, 6, 18, 26, 39], we exploit their open-source models to obtain visual results. Additionally, we apply IP-Adapter [40] and DreamBooth [33] on our base inpainting model [20] to better verify our methods efficacy. Identity Preservation. For identity preservation, we evaluate the Real Image as an upper bound for all methods, where CLIP-T, CLIP-I and DINO are calculated on the training dataset, and FID is measured on our benchmark images. As shown in Tab. 1, the proposed DreamMix outperforms both large-scale training and tuning-free approaches across all metrics. Compared to DreamBooth, our method surpasses it on three metrics while is slightly lower in FID. The possible reason is that DreamBooth tends to produce ID-irrelevant fill-in results that remain harmonious within the scene (see the fourth row in Fig. 4). Additionally, the visual comparison in the first two rows of Fig. 4 demonstrates that our model is effective at producing identity-preserved, context-harmonious inpainting results. Attribute Editing. Since some compared methods [5, 6, 39] dont support text-driven editing, we cant report their metrics on this task. As shown in Tab. 1, our method outperforms the other compared approaches on CLIP-I, CLIPT, and DINO, demonstrating that our model can accurately generate the editing effects of text prompts while preserving the original identity. As shown in the last three rows of Figure 4. Visual comparison between different methods. From left to right are input image and subject, visual results of our DreamMix, IP-Adapter [40], DreamBooth [33], TIGIC [18], AnyDoor [6], and LAR-Gen [26]. Fig. 4, our method supports diverse types of attribute editing, such as color, shape, and adding accessories. More visual results are provided in the supplementary materials. User Study. We conduct user study to further evaluate the feasibility of our method. For both tasks, we ask 100 volunteers to complete questionnaire consisting of 30 questions, resulting in total of 3000 responses. In each question, we present an image of reference object, background image, target text, and the generated images of anonymous methods. Participants are asked to answer the following question: Which of the following images is the best considering image quality and similarity to the subject, and match with text description? We summarize the results and present them in Tab. 2. As we can see, the proposed DreamMix has an overwhelming preference for both identity preservation and attribute editing. 4.3. Ablation Studies We conduct ablation studies on both identity preservation and attribute editing tasks to verify the efficacy of Disentangled Inpainting Framework (DIF), Textual Attribute Substitution (TAS) module, and Attribute Decoupling Mechanism (ADM). The quantitative results are presented in Tab. 3, and the qualitative results are shown in Fig. 5 and Fig. 6. Effect of λ in DIF. In DIF, we use λ to control the separation of local content generation and global context harmonization. We conduct experiments with varying λ values in our model and present the visual results in Fig. 5. As we can see, as λ increases, the inpainted regions become more harmonious with the context, but the allocation accuracy may decrease. Using only GCH (i.e., λ = 1) may pose generation failures (e.g., clock case) when inpainting on small Method Identity Preservation Attribute Editing IP-Adapter [40] MimicBrush [5] AnyDoor [6] TIGIC [18] LAR-Gen [26] DreamMix 8% 12% 10% 2% 13% 55% 7% - - 4% 15% 74% Table 2. User study results for Identity Preservation and Attribute Editing tasks, showing user preference percentages for each method. DreamMix outperforms other methods significantly in both Identity Preservation (55%) and Attribute Editing (74%), indicating strong user preference. Method CLIP-T CLIP-I DINO Identity Preservation Attribute Editing Baseline +DIF ++TAS +++ADM Baseline +DIF ++TAS +++ADM 0.253 0.291 0.290 0.285 0.235 0.265 0.275 0.289 0.644 0.723 0.728 0.728 0.601 0.696 0.685 0.674 0.628 0.717 0.715 0. 0.612 0.708 0.700 0.695 Figure 5. Effect of different values of λ in disentangled inpainting framework. λ = 1 means only GCH stage is performed while λ = 0 means only LCG stage is used. λ is set to 0.7 in our experiments. Table 3. Quantitative comparison of our ablation methods. The Baseline method indicates the base inpainting model [20] combined with DreamBooths finetuning strategy [33]. Figure 6. Visual examples for ablation studies on identity preservation (top row) and attribute editing (bottom row). sizes. Conversely, relying solely on local inpainting (i.e., λ = 0) can affect the overall harmony of the inpainted regions. We find that λ = 0.7 provides an optimal balance in most cases. Effectiveness of Each Component. We conduct experiments to verify the effectiveness of the three core components in our DreamMix. We use DreamBooth [33] on our base inpainting model [20] as baseline method. Then, we incrementally incorporate each component and evaluate them on both tasks. The results are illustrated in Tab. 3. As we can see, the proposed DIF effectively improves the generative capacity of the baseline method in both tasks. For attribute editing, our model achieves continual increase in CLIP-T as the incremental involve of TAS and ADM, while exhibiting slight decrease in CLIP-I and DINO. These results indicate that the edited regions are more consistent with the text prompts and, consequently, slightly differ from the original appearance in the feature space. This demonstrates the effectiveness of the proposed TAS and ADM in In addition, the results in improving text-driven editing. identity preservation demonstrate that incorporating TAS and ADM does not affect identity preservation if no editing prompts are given. More importantly, all our intermediate results surpass the compared methods in Tab. 1, further demonstrating the efficacy of the proposed modules. Fig. 6 illustrates the visual examples of ablation experiments. In the first row, we observe that DIF enhances image quality, while TAS and ADM do not negatively impact the subjects identity. In the second row, DIF establishes solid foundation that allows TAS and ADM to effectively improve editing precision. 5. Conclusion We present diffusion-based generative model for subjectdriven image inpainting using both text and image guidance. disentangled inpainting framework is introduced that seamlessly incorporates local subject allocation and global context harmonization to improve object insertion accuracy. Additionally, we propose an attribute decoupling mechanism and textual attribution substitution module to facilitate text-driven editing of object attributes. Experiments demonstrate the superiority of the proposed method across various subject-driven inpainting applications. One limitation of our method is that relying solely on text and image guidance may pose challenges to achieving harmonious interaction with the environment. Incorporating additional conditions (e.g., pose, depth) might be solution for this issue. Besides, extending our model to support multi-subject inpainting is future direction."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1820818218, 2022. 3 [2] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42 (4):111, 2023. 3 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):110, 2023. 4 [5] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. arXiv Zero-shot image editing with reference imitation. preprint arXiv:2406.07547, 2024. 2, 3, 6, 8 [6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level imIn Proceedings of the IEEE/CVF Conage customization. ference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 2, 3, 4, 6, 7, 8 [7] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Ilvr: Conditioning method for arXiv preprint Gwon, and Sungroh Yoon. denoising diffusion probabilistic models. arXiv:2108.02938, 2021. [8] Diffusers. Stable-Diffusion-XL-1.0-Inpainting-0.1 Model Card, https : / / huggingface . co / diffusers / stablediffusionxl1.0inpainting0.1, 2023. 2 [9] Ziyi Dong, Pengxu Wei, Dreamartist: Towards controllable one-shot text-to-image generation via positive-negative prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 2 and Liang Lin. [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2 [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. 6 [13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [15] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. 6 [16] Jimyeong Kim, Jungwon Park, and Wonjong Rhee. Selectively informative description can reduce undesired embedding entanglements in text-to-image personalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 83128322, 2024. 5 [17] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19311941, 2023. 2, [18] Pengzhi Li, Qiang Nie, Ying Chen, Xi Jiang, Kai Wu, Yuhuan Lin, Yong Liu, Jinlong Peng, Chengjie Wang, and Feng Zheng. Tuning-free image customization with image and text guidance. arXiv preprint arXiv:2403.12658, 2024. 2, 3, 6, 7, 8 [19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 6 [20] Llyasviel. Fooocus Inpaint Model Card, https : / / huggingface . co / lllyasviel / fooocus _ inpaint, 2023. 2, 6, 8 [21] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1146111471, 2022. [22] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. 3 [24] OpenAI. ChatGPT-4o, https : / / openai . com / chatgpt/, 2024. 5 [25] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. [26] Yulin Pan, Chaojie Mao, Zeyinzi Jiang, Zhen Han, and Jingfeng Zhang. Locate, assign, refine: Taming customized image inpainting with text-subject guidance. arXiv preprint arXiv:2403.19534, 2024. 2, 3, 4, 6, 7, 8 [27] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar AverbuchElor, and Daniel Cohen-Or. Localizing object-level shape variations with text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2305123061, 2023. 2, 4 ference on Computer Vision and Pattern Recognition, pages 2242822437, 2023. 3 [39] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1838118391, 2023. 2, 3, 4, 6 [40] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 2, 6, 7, 8 [41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [42] Lirui Zhao, Tianshuo Yang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Kaipeng Zhang, and Rongrong Ji. Diffree: Text-guided shape free object inpainting with diffusion model. arXiv preprint arXiv:2407.16982, 2024. 3 [43] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. arXiv preprint arXiv:2312.03594, 2023. 3 [28] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Improving latent diffusion modRobin Rombach. Sdxl: arXiv preprint els for high-resolution image synthesis. arXiv:2307.01952, 2023. 2, 3, 6 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3, 5, 6 [30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 2 [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2, 3 [32] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [33] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven In Proceedings of the IEEE/CVF conference generation. on computer vision and pattern recognition, pages 22500 22510, 2023. 2, 4, 6, 7, 8 [34] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 2 [35] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without test-time finetuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 85438552, 2024. 2 [36] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2023. 2 [37] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. [38] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Con-"
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "University of Electronic Science and Technology of China",
        "ZMO AI"
    ]
}