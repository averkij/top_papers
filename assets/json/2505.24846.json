{
    "paper_title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning",
    "authors": [
        "Jingyan Shen",
        "Jiarui Yao",
        "Rui Yang",
        "Yifan Sun",
        "Feng Luo",
        "Rui Pan",
        "Tong Zhang",
        "Han Zhao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reward modeling is a key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the Bradley-Terry (BT) model assumes a global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow a mixture distribution of diverse subgroups, a single BT model has an irreducible error. While existing solutions, such as multi-objective learning with fine-grained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, a two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization."
        },
        {
            "title": "Start",
            "content": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized Preference Learning Jingyan Shen2*, Jiarui Yao1*, Rui Yang1*, Yifan Sun1, Feng Luo3, Rui Pan1, Tong Zhang1, Han Zhao1 1University of Illinois at Urbana-Champaign, 2Columbia University, 3Rice University 5 2 0 2 0 3 ] . [ 1 6 4 8 4 2 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Reward modeling is key step in building safe foundation models when applying reinforcement learning from human feedback (RLHF) to align Large Language Models (LLMs). However, reward modeling based on the BradleyTerry (BT) model assumes global reward function, failing to capture the inherently diverse and heterogeneous human preferences. Hence, such oversimplification limits LLMs from supporting personalization and pluralistic alignment. Theoretically, we show that when human preferences follow mixture distribution of diverse subgroups, single BT model has an irreducible error. While existing solutions, such as multi-objective learning with finegrained annotations, help address this issue, they are costly and constrained by predefined attributes, failing to fully capture the richness of human values. In this work, we introduce MiCRo, two-stage framework that enhances personalized preference learning by leveraging large-scale binary preference datasets without requiring explicit fine-grained annotations. In the first stage, MiCRo introduces context-aware mixture modeling approach to capture diverse human preferences. In the second stage, MiCRo integrates an online routing strategy that dynamically adapts mixture weights based on specific context to resolve ambiguity, allowing for efficient and scalable preference adaptation with minimal additional supervision. Experiments on multiple preference datasets demonstrate that MiCRo effectively captures diverse human preferences and significantly improves downstream personalization."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement Learning from Human Feedback (RLHF) unlocks promising pathway to improve the performance, reliability, and adaptability of AI *Equal contribution. ry21, yifan50, ruip4, tozhang, hanzhao}@illinois.edu, js5544@columbia.edu, fl38@rice.edu {jiarui14, Emails: system deployment (Bai et al., 2022; Dong et al., 2023a; Achiam et al., 2023; Dong et al., 2024). Rather than relying on handcrafted reward models, the prevailing approach in RLHF employs preference learning (Christiano et al., 2017) to infer reward scores from human feedback, particularly for tasks involving subjective evaluation and openended responses without unanimous ground truths (Ziegler et al., 2019). However, most existing methods rely on binary-labeled pairwise datasets building upon the assumption that there exists global reward function that can model human preferences. This fails to capture the diverse and often contradictory nature of human preferences, ultimately limiting their effectiveness for personalized and pluralistic alignment (Chakraborty et al., 2024a; Yang et al., 2024b; Mukherjee et al., 2024; Luo et al., 2025). Advancing preference learning to better accommodate heterogeneous human preferences remains an open challenge. Some recent studies seek to capture the diversity by collecting multifaceted annotations that distinguish between different evaluation attributes, e.g., helpfulness, harmlessness, coherence, instruction-following, etc (Wang et al., 2024b; Bai et al., 2022; Wang et al., 2024c). Although fine-grained labels provide deeper insights into individual preferences, collecting and curating them significantly increases data acquisition costs. Consequently, existing datasets limit their scope to handful of pre-defined attributes and often rely on LLM-as-a-Judge (Zheng et al., 2023) for labeling response pairs. This raises concerns about their fidelity in representing the nuanced and ever-evolving landscape of human values. In addition, while users may share common interests, such as preferring helpful and harmless assistant, their expectations are ultimately individualized and depend on use cases, i.e., contextual factors, as illustrated in Fig. 1. To better capture such personalization, some approaches construct 1 Figure 1: Illustration of the two-stage pipeline of MiCRo for capturing personalized preferences. mixture of reward models (Stage 1) is trained on binary-labeled data, while the context-aware router (Stage 2) dynamically adjusts preference distributions based on user-provided context. The final preference distribution is obtained through convex combination of different preference distributions. datasets with elaborate and pluralistic contexts into user prompts (Pitis et al., 2024; Yang et al., 2024b) or system instructions (Lee et al., 2024a). While reward models trained on such enriched datasets have shown improved generalization to personalized preferences, designing these criteria manually is still labor-intensive. In this work, we introduce MiCRo , two-stage, context-aware mixture modeling framework that leverages large-scale binary preference datasets to improve personalized preference learning. We first provide theoretical result showing that when the underlying preference distribution follows mixture of subpopulations, preference learning based on single Bradley-Terry (BT) loss incurs an irreducible error. To address this, we propose contextaware mixture modeling approach to decompose aggregate preferences into latent subpopulations, each with distinct reward function. To further adapt to personalized preferences, we propose an online routing strategy with additional contextual information. In summary, our method offers two key advantages: MiCRo extracts multifaceted human preferences from widely available pairwise comparison datasets without requiring explicit fine-grained annotations or predefined attributes. MiCRo adapts the mixture heads to personalized preference learning with contextual information with only limited number of samples. Our extensive experiments across multiple preference datasets empirically demonstrate that MiCRo mixture heads effectively capture diverse preferences and achieve superior performance compared to baselines on multidimensional benchmarks. With the addition of context-aware routing, the full MiCRo framework matches the performance of fully supervised and test-time adaptation methods, underscoring its effectiveness in enhancing downstream personalization."
        },
        {
            "title": "2 Related Work",
            "content": "Reward modeling aims to learn function that assigns scalar scores to inputoutput pairs based on human preferences, playing central role in RLHF by steering LLM behavior toward human-aligned outputs and mitigating harmful responses (He et al., 2024a; Sun et al., 2024). The typical approach adopts the BT model (Bradley and Terry, 1952; Christiano et al., 2017; Stiennon et al., 2020) to learn from pairwise comparisons. To further address the diversity of human preferences, personalized preference learning seeks to align LLMs with user values in underspecified settings with ambiguous or heterogeneous intent (Fleisig et al., 2023; Baumler et al., 2023; Chakraborty et al., 2024b). One major approach focuses on multiobjective alignment through ensembles of reward models. Techniques such as Mixture-of-Experts and model merging are used to decompose reward functions into task-specific or capability-based components (Quan, 2024; Wang et al., 2024b; Rame et al., 2023; Wang et al., 2024a). However, training multiple reward models typically requires manually defined preference dimensions and dense supervision. To mitigate this, HyRe (Lee et al., 2024b) trains an ensemble offline and adapts it to individual users at test time by dynamically reweight2 ing the components using small number of userspecific examples. Recent work, DRMs (Luo et al., 2025), decomposes human preferences into linear space using PCA, offering promising trainingfree solution; however, its effectiveness depends on the choice of embedding model. complementary line of work uses probabilistic approaches to model subgroup or latent preferences without explicit supervision (Siththaranjan et al., 2023; Poddar et al., 2024; Chen et al., 2024; Chakraborty et al., 2024a), but their potential for personalization remains underexplored. models: P(aw al x) = (cid:88) P(z = x) k=1 P(aw al x, = k), (1) where the weights of each mixture component depend on the prompt and the probability of preference within specific subpopulation is given by (aw al x, = k) = σ (r and for subpopulation k. is assumed to be latent reward function (x, aw) k (x, al)) ,"
        },
        {
            "title": "3 Limitation of a Single Reward Function",
            "content": "3.2 Irreducible error of single BT model 3.1 Problem Setup Notation and Preliminary Let denote the space of prompts, and denote the space of responses. Denote = {x RK (cid:80)K i=1 xi = 1, xi 0, = 1, . . . , K} as the (K 1)- dimensional probability simplex. In standard preference learning, human preferences are modeled based on the classic BT model. Specifically, for given prompt and an LLM π, two candidate responses aw, al are sampled independently from π( x). The probability that human annotator prefers aw over al is given by: (aw al x) = σ (r (x, aw) (x, al)) , where σ denotes the logistic function and : is latent reward function. For brevity, we assume aw al (i.e., aw is always preferred over al). In practice, static, finite-sample preference dataset is collected, and is estimated via maximum likelihood. Mixture Reward Distribution However, in practice, reward data are collected from population of annotators with inherently diverse preferences. Prior work has demonstrated that modeling all human preferences with single parametric reward function leads to systematic underfitting and cannot capture such heterogeneity (Chakraborty et al., 2024a; Siththaranjan et al., 2023). To better reflect this diversity, we assume that each observed annotation is generated from one of the latent subpopulations, where is treated as hyperparameter. latent categorical variable {1, . . . , K} is introduced as an indicator of the subpopulation from which preference pair originates. We introduce the overall probability of preference observation as context-aware mixture of Bradley-Terry 3 In this section, we provably show that, when the underlying preference distribution is mixture of BT models, no matter how rich the model class is for reward functions, preference learning based on single BT model has an irreducible error. Before we present our result, we first assume the diversity of the underlying population: Assumption 3.1 (Diversity). There exists constant ρ > 0, such that for every prompt and every subpopulation group [K], P(z = x) ρ. if kπ(aw, al k(x, aw, al) := k for group as For every tuple (x, aw, al), define the score function k(x, aw) k(x, al). Let LCE(r) be the cross-entropy loss of BT preference model P(aw al x) = σ(r(x, aw) r(x, al)) according to the reward function r, lower bound). For an Theorem 3.2 (Error the predicted arbitrary reward function r, preference is based on single BT model, then LCE(r) 2ρKExVarz[{Es x)}K k=1] + H(x, π, P(zx)). We defer the detailed proof of Theorem 3.2 to Appendix A. In the lower bound, kπ(aw, al x) is the induced distribution of acting on the pair of responses (aw, al) π from the LLM given the prompt x, and the variance operator Varz is applied with respect to the groups. H(x, π, P(zx)) is the Shannon entropy of the joint distribution over preference data given by the prompt, subpopulations, as well as the LLM π (c.f. Appendix for its definition). At colloquial level, the lower bound says that, the more diverse the ground-truth scores from each subpopulation (hence larger variance), or the subpopulation distribution P(z x) (hence larger ρ and entropy), then the larger the cross-entropy loss of using single BT model. Table 1: Comparison of different methods and their key characteristics. MiCRo optimizes mixture of BT loss using binary labels and enables context-aware routing, setting it apart from prior methods in terms of context conditioning and weight learning. Method Binary Labels Context Conditioning Reward Objective Weight Learning Special Characteristic ARMO (Wang et al., 2024b) MaxMin (Chakraborty et al., 2024a) HyRe (Lee et al., 2024b) DRMs (Luo et al., 2025) MiCRo (Ours) Mean Square Error Mixture of BT BT End-to-end BT Hard clustering Accuracy maximization Accuracy maximization Training-free reward decomposition via PCA Fixed Num of attributes Minority preference optimization Test-time adaptation Mixture of BT Hedge Algorithm Context-aware Routing"
        },
        {
            "title": "4 Method",
            "content": "likelihood defined as: The inherent limitation of single BT model motivates the need for richer preference modeling. However, two key challenges remain: (C1) How to extract mixture of reward functions from binarylabeled datasets without incurring additional annotation costs? (C2) Given limited access to userspecific intent, how can we efficiently adapt to personalized preferences at deployment time? To this end, we propose two-stage algorithm that first uncovers latent heterogeneity in human preferences through mixture modeling, and then adapts to individual users via lightweight, contextaware online routing strategy."
        },
        {
            "title": "4.1 Mixture Modeling for Diverse Preferences",
            "content": "We begin by fundamentally comparing our mixture modeling objective with previous methods and then introduce the detailed design of our approach. Comparison with Prior Mixture Modeling Approaches Unlike static and unconditional mixture approach used in previous work (Chakraborty et al., 2024a), our formulation from Equation (1) introduces dynamic, context-aware weighting mechanism for mixture models by conditioning the subpopulation weights P(z = x) on the given prompt x. We emphasize that this is crucial design that allows for contextual specialization, where prompts automatically activate the most relevant subpopulations reward model. By mimicking real-world expertise allocation, our approach avoids the diluted performance of static averaging. We provide more detailed comparison of our method with existing works in Table 1. Mixture Modeling Designs In practice, we parameterize the reward function for each subpopulation as rϕk : for = 1, . . . , and model the mixture weights with network fψ : K. Given training dataset = {(x, aw, al)i}n i=1, we minimize the negative log4 Lmle = 1 (cid:88) (cid:88) (cid:16) log fψ,k(x) (x,aw ,al)D k=1 σ (rϕk (x, aw) rϕk (x, al)) (cid:17) . (2) To prevent any single model from dominating, we add regularization term by imposing uniform prior to the weight distribution: Lreg = 1 (cid:88) (cid:88) (x,aw ,al)D k= fψ,k(x) log fψ,k(x). (3)"
        },
        {
            "title": "The final loss function becomes",
            "content": "L(ϕ, ψ) = Lmle + αLreg, (4) where the coefficient α is set to 0.5 in our implementation. Overall, this mixture training phase on large-scale datasets learns diverse set of reward functions, establishing robust foundation for adaptation to nuanced preferences."
        },
        {
            "title": "Preference Learning",
            "content": "While the pre-trained mixture model has the potential to capture latent reward functions, assigning meaningful weights to these reward heads upon given prompt is difficult without clear signal of user intent. Technically, during the training of the mixture model, since we do not have labeled data to train the router separately, we will need one strategy to learn the correspondence between the mixture reward heads and the underlying user intent for better routing assignments. Recent work on contextual alignment (Pitis et al., 2024; Lee et al., 2024a; Poddar et al., 2024) highlights that incorporating context can reduce ambiguity and improve estimation accuracy. Motivated by this, we introduce second stage that incorporates more concrete contextual informationsuch as user instructions or metadata (e.g., demographics or interaction history)to guide the routing strategy by learning the correspondence between user intent and mixture reward heads. Unlike prior methods that require training reward models on large-scale contextual datasets (Pitis et al., 2024; Lee et al., 2024a), our approach avoids costly data collection and full model retraining. Instead, we leverage the unsupervised mixture heads pre-trained in the first stage to enable sampleefficient online adaptation. This allows us to refine the mixture weights and generate personalized predictions using only small number of samples. To this end, we propose to fine-tune the routing network fψ using the Hedge algorithm (Arora et al., 2012), where each input is pair (xi, ci) Dc, with ci denoting additional context information. Intuitively, Hedge maintains set of experts (i.e., reward heads) and adaptively reweights them based on their performanceassigning higher weights to those that better align with observed preferIn our framework, the user preferences ences. can be modeled as convex combination of the latent subpopulation preferences. For an example (xi, ci, ai,w, ai,l), denote the output probability from k-th head as pk(ai,w ai,lxi) := σ(rϕk (xi, ai,w) rϕk (x, ai,l)) and define Li,k := log pk(ai,w ai,lxi, ci). We consider an online learning setting where contextual data is collected within budget of B. We acquire batch of preference pairs DA = {(x, c, aw, al)i}B i=1 with additional contexts. Motivated by the multi-task learning literature (He et al., 2024b; Liu et al., 2024), we propose training objective for the router based on the framework of online mirror descent with KL divergence regularization (Hazan et al., 2016): min ψ"
        },
        {
            "title": "1\nB",
            "content": "B (cid:88) i=1 Li, s.t. fψ(xi, ci) K, := (cid:80)K k=1 fψ(xi, ci)kLi,k + where Li τ KL(fψ(xi, ci)ωi) and ωi is weight vector that comes from previous iteration or pre-trained weights, and τ 0 is temperature hyperparameter. Note that the first term is an upper bound of the negative log-likelihood function of mixture distribution based on Jensens inequality. which yields Algorithm 1 to learn the router iteratively with contextual information. For each iteration, we determine optimal weights target as soft labels using Equation (5). Then, we fine-tune the router by minimizing the cross-entropy loss between the soft labels and the router networks predictions. w, ai Algorithm 1 Context-aware Router Learning l, yi)}Bt Input: Mini-batch {(xi, ci, ai i=1, temperature τ , pre-trained router fψ, iterations , reward heads from the first stage rϕk , = 1, . . . , K. Initialize ψ(1) = ψ, ωi,k = fψ,k(xi) for = 1 to do // weight update for = 1 to Bt do for = 1 to do Lik log pk(ai ωi,k fψ(t)(xi, ci)k exp(cid:0) Lik ai xi) τ (cid:1) end (cid:80)K ωi,k ωi,k j=1 ωi,j for = 1, . . . , end // router update Bt(cid:88) L(t)"
        },
        {
            "title": "LCE",
            "content": "weight(ψ) (cid:0)ωi , fψ(t)(xi, ci)(cid:1)"
        },
        {
            "title": "1\nBt\nBackprop L(t)\nweight(ψ) to transition from ψ(t)\nto ψ(t+1)",
            "content": "i=1 end Our routing learning approach offers two clear advantages in deployment: 1) efficiency: By leveraging the expert heads trained on large-scale datasets in the first stage, the second stage does not require retraining the reward model or relying on extensive labeled data; instead, lightweight, online router continuously adapts during deployment. 2) generalizability: Our learning-based router harnesses contextual information to adjust weights with learning-based algorithm. Unlike test-time adaptation methods (Lee et al., 2024b) that rely on set of test data for re-weighting, our router is trained online, allowing generalizing to new contexts without access to specific test data. Routing with Hedge Algorithm With τ > 0, the optimal solution for each batch will be:"
        },
        {
            "title": "5 Experiments",
            "content": "fi,k = ωi,k exp (Lik/τ ) j=1 ωi,j exp (Lij/τ ) (cid:80)K , (5) In our experiments, we aim to answer two research questions: (Q1) Can our context-aware mixture modeling framework extract diverse reward func5 tions from binary-labeled preference data? (Q2) Can the fine-tuned routing network enable effective personalization by adapting to contextual signals? 5.1 Experimental Setup Training datasets We train the mixture reward models on binary-labeled preference datasets: HelpSteer2 (Wang et al., 2024c), RPR (Pitis et al., 2024), and preference-700K (Dong et al., 2024). HelpSteer2 and RPR datasets contain humanlabeled response pairs evaluated across multiple assessment dimensions. We construct the binarylabeled sets with the following process. For each dimension, we extract binary preference pairs based on absolute ratings, treating responses with higher ratings as chosen and those with lower ratings as rejected. Pairs with identical ratings are excluded from both training and test sets. To ensure diversity in preferences, we exclude pairs where all attributes are unanimously agreed upon from the training set. Ultimately, all pairs from all dimensions are mixed, resulting in 23.5K samples and 5.8K training samples from HelpSteer2 and RPR, respectively. We also apply the mixture model training on preference-700K dataset, large-scale pairwise dataset created by aggregating instructions and response pairs from multiple data sources. Further details on these datasets are provided in Appendix D.2. Models We use the best 3B open-source reward model GRM-Llama3.2-3B (Yang et al., 2024a) as the backbone, keeping it frozen while training linear probing heads on top. The router network is implemented as one-layer MLP containing 128 units and softmax activation. We provide full implementation details in Appendix D.1. Baselines We evaluate the following baselines: 1) Single Reward: single-head model trained using the standard BT loss. 2) Static Mixture: simplified variant of our method, corresponding to the approach used in MaxMin-RLHF (Chakraborty et al., 2024a), where the mixture model is trained with fixed, input-independent weights, without leveraging contextual information. 3) Shared-Base Ensemble Model: Lee et al. (2024b) introduces HyRe, test-time adaptation approach based on ensemble networks. We adopt the multi-head architecture with frozen prior network and multiple trainable heads, optimizing uniformly weighted sum of BT losses. 4) Fully Supervised Model: We include ARMO (Wang et al., 2024b), an 8B model trained on more than 500K fine-grained labels, as baseline with full supervision. 5.2 Stage-1 Evaluation: Can MiCRo Disentangle Diverse Human Preferences? Evaluation Setting In this experiment, we train MiCRo and baseline models (except ARMO) on HelpSteer2 and RPR training sets. We then evaluate the learned heads on the HelpSteer2 and RPR test sets, which cover 14 distinct preference dimensions. Each head is evaluated individually on every dimension. To ensure fair comparisons, we use the same number of heads for MiCRo and other multi-head baselines. The full evaluation results of MiCRo mixture heads are provided in Appendix E. Results Fig. 2 compares the best-performing head for each test dimension. The results demonstrate that different heads from MiCRo specialize in distinct evaluation dimensions and consistently surpass the performance of all baeslines across all dimensions. On average, MiCRo consistently achieves the highest average scores across both RPR (0.921) and HelpSteer2 (0.811) benchmarks, with substantial gains over the single-head baseline (+40.0% on RPR, +6.8% on HelpSteer2), the Sharebase ensemble baseline (+20.7% and +9.1% respectively), and the mixture model without context routing (+5.5% and +1.1% respectively), demonstrating the robust benefits of context-aware mixture modeling approach. These results suggest that mixture modeling more effectively captures latent diverse preferences compared to single reward or ensemble models, and that context-aware weighting further improves over static mixtures. Fig. 3 presents qualitative example of mixture weights from the Stage 1 router, showing how different prompts from the RPR test set activate different heads. This highlights the effectiveness of our contextual router compared to the unconditional router used in prior work."
        },
        {
            "title": "5.3 Stage-2 Evaluation: Can MiCRo Adapt to",
            "content": "Personalized Preference? User Context Datasets The RPR training dataset includes user-specific criteria for each preference pair, explicitly specifying the users intent and evaluation dimension, and thus provides well-defined source of user context. For HelpSteer2, we follow the approach of Pitis et al. (2024) and augment generic prompts with attribute-specific modifications based on the original assessment dimensions 6 Figure 2: Comparison of accuracy scores between the best heads of MiCRo and other baselines on multiple test dimensions. The mixture heads can disentangle diverse human preferences, with different heads excelling on different attributes. They consistently outperform the single reward model across all attributes. Overlaps where the same head dominates multiple attributes may reflect inherent attribute correlations. Table 2: Accuracy scores on HelpSteer2 test set. On average, MiCRo outperforms baselines across various attributes and overall results. Scores in green indicate absolute improvement over the Single Reward baseline. All baselines except ARMO (8B) use the same 3B base model. Method Supervision Helpfulness Correctness Coherence Complexity Verbosity Average Single Reward Shared-Base Best Head Static Mixture ARMO (8B) HyRe Binary Binary Binary Fine-grained Binary + Test Labels 0.7838 0.7838 0.7243 0.6919 0.7692 0.6686 0.6628 0.6570 0.6395 0.6987 0.6914 0.7037 0.6790 0.7593 0.6781 MiCRo-HyRe MiCRo (Ours) Binary + Test Labels Binary + Context 0.8270 0.8324 +0.05 0.7035 0.7140 +0.04 0.7407 0.7543 +0.06 0.7907 0.7519 0.8372 0.7132 0.7168 0.8217 0. 0.8816 0.8158 0.9013 0.7500 0.8015 0.8487 0.8513 0.7632 0.7436 0.7598 0.7108 0.7329 0.7883 0.7830 +0.02 Implementation Details For MiCRo , we train the router using 50 context-annotated examples per attribute drawn from the training data. For the static mixture baseline, we keep the Stage-1 mixture weights fixed. For HyRe adaptation, we reuse the Stage-1 reward heads and derive adaptation weights from 16 labeled test samples per attribute. Results As shown in Tab.2 and Tab.3, MiCRo achieves average test accuracies of 0.7830 on HelpSteer2 and 0.8218 on RPR under the within-dataset evaluation setting, outperforming all three baselines trained with binary labels. This highlights the effectiveness of the router in adapting to specific user preferences. The relatively lower performance in certain attributes can be attributed to noisy or inconsistent contextual signals. Compared to methods requiring stronger supervision, MiCRo performs competitively with ARMO on RPR and outperforms it on HelpSteer2. Furthermore, we find that applying test-time adaptation to MiCRo mixture heads outperforms the original HyRe, indicating that our first-stage training provides stronger base without requiring explicit supervision. While HyRe benefits from test-time Figure 3: Heatmaps of router weights for different prompts in MiCRo Stage 1. The router assigns varying weights to different heads depending on the prompt. in the annotation process (Wang et al., 2024c). Examples of contexts are provided in Appendix D.2. For training and test datasets, we prepend the contextual information to the user prompt and fine-tune the router accordingly. Evaluation Setting We assess personalized adaptation under two scenarios: (1) In-distribution evaluation. All models are trained on the HelpSteer2 and RPR training splits and evaluated on their respective test splits. (2) Cross-distribution generalization. Models are trained on HelpSteer2 and the large-scale preference-700K datasets, then evaluated on the RPR test set to measure transfer to previously unseen user preferences. 7 Table 3: Accuracy scores on the RPR test set. Scores in green indicate absolute improvement over the Single Reward baseline. All baselines except ARMO (8B) use the same 3B base model. Method Supervision Clarity Creativity Scientific Rigor UserFriendliness Storytelling Pedagogical Linguistic Creativity Factual Accuracy Humor Average Single Reward Binary Shared-Base Best Head Binary Binary Static Mixture Fine-grained ARMO (8B) Binary + Test Labels HyRe 0.4717 0.6226 0.9057 0.9057 0.7027 MiCRo-HyRe MiCRo (Ours) Binary + Test Labels Binary + Context 0.9556 0.9170 +0.45 0.6806 0.7361 0.6389 0.6806 0.5893 0.8125 0. 0.3333 0.8095 0.9048 0.9405 0.6618 0.7978 0.6966 0.6854 0.6966 0.8493 0.9605 0.8119 +0.48 0.9012 0.8696 +0.07 0.8375 0.8000 0.6250 0.7875 0.6563 0.8333 0. 0.6452 0.6774 0.7903 0.7903 0.7826 0.7963 0.7935 +0.15 0.8654 0.8558 0.7404 0.9135 0.7045 0.9063 0.8558 0.4225 0.7042 0.8451 0.9014 0.7091 0.8810 0.9643 0.9167 0.9463 0. 0.6594 0.7629 0.7836 0.8403 0.6823 0.9524 0.8563 +0.43 0.9605 0.9109 +0.03 0.8974 0.8218 +0.16 Table 4: Performance on the RPR test set with models trained on HelpSteer2 and preference-700K dataset. MiCRo outperforms other baselines trained with binary labels on average scores. Training Dataset Method Clarity and Conciseness Creativity and Originality Scientific Rigor UserFriendliness Narrative and Storytelling Quality Pedagogical Effectiveness Linguistic Creativity Factual Accuracy Humor and Entertainment Average preference -700K HelpSteer2 Single Head Shared-base Best Head Static Mixture MiCRo (Ours) Single Head Shared-base Best Head Static Mixture MiCRo (Ours) 0.8679 0.8302 0.8679 0.9358 0.8491 0.8491 0.9057 0.9245 0.6806 0.8056 0.6250 0. 0.6667 0.6667 0.6389 0.6667 0.9286 0.8095 0.9048 0.9190 0.9048 0.9048 0.9048 0.8690 0.6067 0.8089 0.6292 0.6764 0.6180 0.6629 0.6854 0.7079 0.6500 0.6500 0.6500 0. 0.6500 0.7000 0.6250 0.6875 0.8065 0.7903 0.8065 0.7484 0.8065 0.8065 0.7903 0.8226 0.8077 0.8654 0.7981 0.7827 0.7404 0.7404 0.7404 0.7692 0.9155 0.8169 0.9014 0. 0.8732 0.8310 0.8451 0.8592 0.9405 0.9167 0.9286 0.9405 0.8690 0.9048 0.9167 0.9286 0.8004 0.8009 0.7902 0.8105 0.7753 0.7851 0.7838 0.8133 labels, it assumes access to labeled examples for each user at inference time. Context-aware routing offers more practical alternative by generalizing to unseen users, but its performance depends on the quality of the context. In practice, we believe richer and more informative context could further boost its effectiveness. Our ablation in Section 5.4 further shows that performance improves with access to more context samples. In general, these results highlight MiCRo as practical and labelefficient solution to learn personalized preferences. Tab. 4 presents results under the unseen user setting, showing that MiCRo consistently outperforms other baselines trained with binary labels. This further demonstrates the router can generalize across user distributions with contextual information. Additional results on the RewardBench benchmark are provided in Appendix E."
        },
        {
            "title": "5.4 Ablation Study",
            "content": "We conduct an ablation study on two critical hyperparameters in our method: the number of subpopulations and the router learning budget B. We include detailed study of in Appendix C.1. While too few subpopulations may limit the models ability to capture diverse preferences, performance remains relatively stable as increases. Fig. 4 shows the convergence of context-aware routing in Stage 2 on the RPR and HelpSteer2 test sets as the number of context-labeled samples per attribute increases. At budget 50 (i.e., 450 and 250 examples in total for each dataset, respectively), the average accuracy across 9 attributes on RPR (a) RPR test set. (b) HelpSteer test set. Figure 4: Average accuracy across different contextlabeling budgets per attribute. Accuracy is averaged over all dimensions in each test dataset. Shaded regions indicate the standard deviation across 5 independent runs. The curves show that performance tends to converge around budget 50. test set increases sharply from around 0.705 to over 0.841, while the accuracy on HelpSteer2 plateaus around 0.785. In both cases, performance improves steadily with larger budgets, with most gains occurring early, demonstrating that the router can efficiently adapt using only small number of contextual examples."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we address the challenge of personalized preference learning by leveraging large number of binary-labeled datasets alongside small set of fine-grained context-aware data. Theoretically, we show that single reward head is not sufficient whenever the underlying reward signals are mixture of distributions. Motivated by the above result, we propose MiCRo, novel two-stage framework with mixture modeling and context-aware routing. 8 Through extensive experiments, we demonstrate that MiCRo effectively disentangles complex human preferences and enhances downstream pluralistic alignment tasks. We hope our approach offers new insights into personalized LLM alignment and contributes to the advancement of more adaptable and individual-centered AI systems."
        },
        {
            "title": "7 Limitations",
            "content": "Although our formulation is general, there is limited availability of public datasets that provide rich and consistent user context information, making it difficult to comprehensively evaluate personalization capabilities. Our current implementation relies on access to explicitly defined context criteria and partially synthetic settings to simulate user-specific signals. However, in many real-world scenarios, user intent is often implicite.g., reflected in multiturn dialogue, demographic metadata, or behavioral patterns. Incorporating such implicit user contexts into the routing process remains an important direction for future work."
        },
        {
            "title": "8 Ethics Statement",
            "content": "The paper introduces two-stage, context-aware mixture modeling framework that uses large-scale binary preference datasets to improve personalized preference learning. All experiments are conducted using publicly available models and datasets. The licenses for all datasets are listed in Appendix D.2, and we ensure complete compliance with all the license terms. While MiCRo improves scalability in personalized preference learning, the mixture heads are trained without explicit supervision. As result, the learned mixture heads may encode preferences that are either beneficial or harmful, with risk of inadvertently modeling undesirable or malicious intent. We therefore emphasize the importance of thorough evaluation and safety auditing to mitigate potential misuse."
        },
        {
            "title": "Acknowledgments",
            "content": "This work is supported by an NSF IIS grant No. 2416897. HZ would like to thank the support from Google Research Scholar Award. The views and conclusions expressed in this paper are solely those of the authors and do not necessarily reflect the official policies or positions of the supporting companies and government agencies. Additionally, we thank Hanyang Chen for his assistance and helpful input."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Sanjeev Arora, Elad Hazan, and Satyen Kale. 2012. The multiplicative weights update method: metaalgorithm and applications. Theory of computing, 8(1):121164. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Connor Baumler, Anna Sotnikova, and Hal Daumé III. 2023. Which examples should be multiply annotated? active learning when annotators may disagree. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1035210371. Ralph Allan Bradley and Milton Terry. 1952. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324 345. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Bedi, and Mengdi Wang. 2024a. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment. Souradip Chakraborty, Jiahao Qiu, Hui Yuan, Alec Koppel, Furong Huang, Dinesh Manocha, Amrit Bedi, and Mengdi Wang. 2024b. Maxmin-rlhf: Towards equitable alignment of large language models with diverse human preferences. In ICML 2024 Workshop on Models of Human Feedback for AI Alignment. Daiwei Chen, Yi Chen, Aniket Rege, and Ramya Korlakai Vinayak. 2024. Pal: Pluralistic alignment framework for learning from heterogeneous preferences. arXiv preprint arXiv:2406.08469. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences. Advances in neural information processing systems, 30. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. Preprint, arXiv:2310.01377. Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng Zhang, Kashun Shum, and Tong Zhang. 2023a. Raft: Reward ranked finetuning for generative foundation model alignment. arXiv preprint arXiv:2304.06767. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. 2024. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863. Feng Luo, Rui Yang, Hao Sun, Chunyuan Deng, Jiarui Yao, Jingyan Shen, Huan Zhang, and Hanjie Chen. 2025. Rethinking diverse human preference learning through principal component analysis. arXiv preprint arXiv:2502.13131. Yi Dong, Zhilin Wang, Makesh Narsimhan Sreedhar, Xianchao Wu, and Oleksii Kuchaiev. 2023b. Steerlm: Attribute conditioned sft as an (user-steerable) alternative to rlhf. Preprint, arXiv:2310.05344. Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. 2022. Understanding dataset difficulty with V-usable information. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 59886008. PMLR. Eve Fleisig, Rediet Abebe, and Dan Klein. 2023. When the majority is wrong: Modeling annotator disagreement for subjective tasks. arXiv preprint arXiv:2305.06626. Elad Hazan et al. 2016. Introduction to online convex optimization. Foundations and Trends in Optimization, 2(3-4):157325. Yifei He, Haoxiang Wang, Ziyan Jiang, Alexandros Papangelis, and Han Zhao. 2024a. Semi-supervised reward modeling via iterative self-training. arXiv preprint arXiv:2409.06903. Yifei He, Shiji Zhou, Guojun Zhang, Hyokun Yun, Yi Xu, Belinda Zeng, Trishul Chilimbi, and Han Zhao. 2024b. Robust multi-task learning with excess risks. arXiv preprint arXiv:2402.02009. Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and Yaodong Yang. 2024a. Pku-saferlhf: Towards multi-level safety alignment for llms with human preference. arXiv preprint arXiv:2406.15513. Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2024b. Beavertails: Towards improved safety alignment of llm via humanpreference dataset. Advances in Neural Information Processing Systems, 36. Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. 2024a. Aligning to thousands of preferences via system message generalization. Advances in Neural Information Processing Systems, 37:73783 73829. Yoonho Lee, Jonathan Williams, Henrik Marklund, Archit Sharma, Eric Mitchell, Anikait Singh, and Chelsea Finn. 2024b. Test-time alignment via hypothesis reweighting. arXiv preprint arXiv:2412.08812. Subhojyoti Mukherjee, Anusha Lalitha, Sailik Sengupta, Aniket Deshmukh, and Branislav Kveton. 2024. Multi-objective alignment of large language models through hypervolume maximization. arXiv preprint arXiv:2412.05469. Silviu Pitis, Ziang Xiao, Nicolas Le Roux, and Alessandro Sordoni. 2024. Improving context-aware preference modeling for language models. Advances in Neural Information Processing Systems, 37:70793 70827. Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. 2024. Personalizing reinforcement learning from human feedback with variational preference learning. arXiv preprint arXiv:2408.10075. Shanghaoran Quan. 2024. Dmoerm: Recipes of mixture-of-experts for effective reward modeling. arXiv preprint arXiv:2403.01197. Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and Matthieu Cord. 2023. Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Advances in Neural Information Processing Systems, 36:7109571134. Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. 2023. Distributional preference learning: Understanding and accounting for hidden context in rlhf. arXiv preprint arXiv:2312.08358. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA. Curran Associates Inc. Hao Sun, Yunyi Shen, and Jean-Francois Ton. 2024. Rethinking bradley-terry models in preference-based reward modeling: Foundations, theory, and alternatives. arXiv preprint arXiv:2411.04991. Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, and Tong Zhang. 2024a. Arithmetic control of llms for diverse user preferences: Directional preference alignment with multi-objective rewards. arXiv preprint arXiv:2402.18571. Meitong Liu, Xiaoyuan Zhang, Chulin Xie, Kate Donahue, and Han Zhao. 2024. Online mirror descent for tchebycheff scalarization in multi-objective optimization. arXiv preprint arXiv:2410.21764. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024b. Interpretable preferences via multi-objective reward modeling and mixture-ofexperts. arXiv preprint arXiv:2406.12845. 10 Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. 2024c. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673. Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan HelpSwope, and Oleksii Kuchaiev. 2023. steer: Multi-attribute helpfulness dataset for steerlm. Preprint, arXiv:2311.09528. Rui Yang, Ruomeng Ding, Yong Lin, Huan Zhang, and Tong Zhang. 2024a. Regularizing hidden states enables learning generalizable reward model for llms. arXiv preprint arXiv:2406.10216. Rui Yang, Xiaoman Pan, Feng Luo, Shuang Qiu, Han Zhong, Dong Yu, and Jianshu Chen. 2024b. Rewardsin-context: Multi-objective alignment of foundation models with dynamic preference adjustment. arXiv preprint arXiv:2402.10207. Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, and Maosong Sun. 2024. Advancing llm reasoning generalists with preference trees. Preprint, arXiv:2404.02078. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623. Daniel Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-tuning lanarXiv guage models from human preferences. preprint arXiv:1909.08593. 11 Proof of Theorem 3.2 To ease the reading, we first restate Theorem 3.2 and provide its proof. Theorem 3.2 (Error lower bound). For an arbitrary reward function r, if the predicted preference is based on single BT model, then LCE(r) 2ρKExVarz[{Es Proof. For uncluttered notation, we use γk(x) to denote the mixture weight P(z = x) when the prompt is x. Based on Assumption 3.1, , [K], γk(x) ρ. For tuple (x, aw, al), define the score s(x, aw, al) := r(x, aw) r(x, al). When the context is clear, we further simplify the notation by using σk := σ(s k=1] + H(x, π, P(zx)). k(x, aw, al)) and σr := σ(s(x, aw, al)). kπ(aw, al x)}K Recall that for ˆy, [0, 1], the cross-entropy loss ℓCE(ˆy, y) = DKL(Ber(y)Ber(ˆy)) + H(Ber(y)), where Ber(c) is the Bernoulli distribution with parameter [0, 1] and H() is the Shannon entropy. Expand the cross-entropy loss LCE(r) based on the mixture distribution and use H(x, π, γ) to denote the entropy of the joint distribution over preference data given by prompt, subpopulations and the LLM π, i.e., H(x, π, γ) := Ex (cid:34) (cid:88) (cid:35) γk(x) E(aw,al)π(x) [σk log σr + (1 σk) log(1 σr)] . k=1 Note that the joint entropy only depends on the underlying distribution of the prompt, subpopulations, and the LLM π, and it does not depend on the learned reward model. Consider the cross-entropy loss LCE(r) of single reward model, we have LCE(r) = Ex = Ex = Ex Ex Ex (cid:34) (cid:88) k=1 (cid:34) (cid:88) k=1 (cid:34) (cid:88) k=1 (cid:34) (cid:88) k=1 (cid:34) (cid:88) k=1 γk(x) E(aw,al)π(x) (cid:20) σk log 1 σr + (1 σk) log 1 1 σr (cid:20) γk(x) E(aw,al)π(x) 1 σk 1 σr (cid:35) γk(x) E(aw,al)π(x) [DKL(Ber(σk)Ber(σr))] + (1 σk) log σk log σk σr + H(x, π, γ) (cid:21)(cid:35) (cid:21)(cid:35) + H(x, π, γ) γk(x) (cid:2)DKL(E(aw,al)π(x)Ber(σk)E(aw,al)π(x)Ber(σr))(cid:3) (cid:35) + H(x, π, γ) γk(x) DKL(Es (cid:35) kπ(aw, al x)Esπ(aw, al x)) + H(x, π, γ) (convexity of DKL) γk(x) d2 TV(Es (cid:35) kπ(aw, al x), Esπ(aw, al x)) (definition of pushforward) + H(x, π, γ) γk(x) Es kπ(aw, al x) Esπ(aw, al x)2 + H(x, π, γ) (Pinskers inequality) (cid:35) 2Ex = 2Ex (cid:34) (cid:88) k=1 (cid:34) (cid:88) k= 2ρKEx 2ρKEx (cid:88) (cid:34)"
        },
        {
            "title": "1\nK\n(cid:2)Varz",
            "content": "k=1 (cid:2){Es Es kπ(aw, al x) Esπ(aw, al x)2 + H(x, π, γ) (γk(x) ρ) (TV distance of two Bernoulli) (cid:35) kπ(aw, al x)}K k=1 (cid:3)(cid:3) + H(x, π, γ), (minb 1 (cid:80)K k=1(xk b)2 = Var (cid:2){xk}K k=1 (cid:3)) which completes the proof. Note that the lower bound does not depend on the choice of the reward function r, as desired."
        },
        {
            "title": "B Reproducibility",
            "content": "Our code repository is available at https://github.com/MaxwellJryao/MiCRo."
        },
        {
            "title": "C Discussions",
            "content": "C.1 Choice of In this section, we empirically investigate how different choices of affect performance. As shown in Fig. 5, model performance remains stable as increases, suggesting that overestimating is relatively benign, redundant heads tend to receive low weights, and the best-performing head remains consistent. However, when is underestimated, the model suffers from misspecification, leading to degraded performance in the first stage. While larger improves representation capacity, we observe that it can make convergence more difficult in the second-stage router training, as shown in Fig. 6. To mitigate this, simple pruning strategy can be applied to remove heads with highly correlated predictions on hold-out set, reducing redundancy without compromising accuracy. (a) HelpSteer2 (b) RPR Figure 5: Performance of the best-performing MiCRo mixture heads trained with varying numbers of components on HelpSteer2 and RPR test sets. The plots show both the average accuracy and per-attribute accuracy. With smaller values of K, for example, = 1 or = 5 on the RPR test set, the performance suffers due to underfitting the diversity of preferences. As increases, the performance stabilizes."
        },
        {
            "title": "D Experimental Details",
            "content": "D."
        },
        {
            "title": "Implementation Details",
            "content": "For mixture modeling training, we keep the backbone model fixed and train the linear probing heads. We set the learning rate as 0.002, batch size as 4, 8 gradient accumulation steps, and warmup ratio of 0.05, optimizing with AdamW. The model is trained on 4 NVIDIA RTX A6000 GPUs for up to 4 hours. For the router fine-tuning, for the in-distribution evaluation, we set τ as 0.001 on HelpSteer2 and 0.0001 on RPR. For the cross-dataset generalization, we set τ as 0.001. We set batch size to 32. To stabilize training, we recompute the mixture weights ωi only once at the beginning of each epoch, and keep them fixed throughout the epoch. The router is trained for total of 10 epochs. 13 Figure 6: Average accuracy across different context-labeling budgets per attribute with models trained using varying values of K. For smaller K, the model benefits less from additional context, as it underfits the diversity of preferences. For larger K, while accuracy can improve, it requires more labeling budget to effectively assign context. D.2 Models and Datasets Additional Details of Datasets The HelpSteer2 dataset contains human-labeled response pairs evaluated across five assessment dimensions: helpfulness, correctness, complexity, coherence, and verbosity. We include summary of dataset statistics for HelpSteer2 and RPR datasets in Tab. 5. Additional Details of Context We listed an example of criterion in RPR dataset and the generated prompts for HelpSteer2 are listed in Tab. 6."
        },
        {
            "title": "Examples of Contexts in RPR Dataset",
            "content": "Dimension: User-Friendliness User Prompt: Can you create house layout using HTML and CSS? Context: Provides clear and easy-to-follow instructions for implementing the design. Dimension: Scientific Rigor User Prompt: What are the underlying http requests send for achieving SSO for an desktop application? Context: Provides technically accurate and detailed explanation of the underlying HTTP requests for achieving SSO for desktop application. Table 5: Summary of HelpSteer2 and RPR pairwise datasets. We show the number of pairs for each dataset and split. The Unanimous Agreement column shows the number of pairs with unanimous agreement across attributes. (a) HelpSteer2 Attribute Helpfulness Correctness Coherence Complexity Verbosity Unanimous Agreement Train Test 6724 873 6298 854 3708 696 2168 643 4584 754 131 - (b) RPR Attribute Clarity Conciseness Creativity Originality Scientific Rigor User Friendliness Narrative Storytelling Pedagogical Effectiveness Linguistic Creativity Factual Accuracy Humor Train Test 611 53 761 72 724 84 710 89 781 80 705 811 104 682 71 965 84 14 Table 6: Context for HelpSteer2. For each attribute, we assign label based on the annotation guidelines provided in the original paper. Attribute Helpfulness Correctness Complexity Verbosity Coherence Safety Context The assistant should provide users with accurate, relevant, and up-to-date information, ensuring that the content is positive, engaging, educational, and truly helpful. The assistant must base responses on verified facts and cover all aspects of the prompt fullyavoiding errors, omissions, hallucinations, or irrelevant details. The assistant should employ sophisticated language with elevated vocabulary, appropriate for adults with advanced education or subject matter experts. The assistant should provide an expansive, detailed response that thoroughly elaborates on the topic, including additional context and examples beyond the basic answer. The assistants responses should be logically structured, easy to follow, and free of contradictions, redundancies, or abrupt style shifts. The assistant must ensure all responses are safe and respectful, strictly avoiding any harmful, toxic, or illegal information or instructions. License HelpSteer2 is released under the License of CC-By-4.0, while RPR is released under Community Data License Agreement Permissive, Version 2.0. preference-700K1 has not explicitly stated its license, but the Github repository of the paper (Dong et al., 2024) is released under Apache License 2.0. It is also worth noticing that the dataset of preference-700K is mixture of multiple data sources: Anthropic/hh-rlhf2 (Bai et al., 2022): MIT License. stanfordnlp/SHP3 (Ethayarajh et al., 2022): In accordance with Reddit API Terms of Use, where further explanations are available in https://huggingface.co/datasets/stanfordnlp/SHP# license. nvidia/HelpSteer4 (Wang et al., 2023; Dong et al., 2023b): CC-BY-4.0. PKU-Alignment/PKU-SafeRLHF5 (Ji et al., 2024b,a): CC-BY-NC-4.0. openbmb/UltraFeedback6 (Cui et al., 2023): MIT License. openbmb/UltraInteract_sft7 (Yuan et al., 2024): MIT License. Distilabel-Capybara8: Apache License 2.0. Distilabel-Orca9: Apache License 2.0."
        },
        {
            "title": "E Additional Experiment Results",
            "content": "In this section, we present additional experiment results. 1https://huggingface.co/datasets/hendrydong/preference_700K 2https://huggingface.co/datasets/Anthropic/hh-rlhf 3https://huggingface.co/datasets/stanfordnlp/SHP 4https://huggingface.co/datasets/nvidia/HelpSteer 5https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF 6https://huggingface.co/datasets/openbmb/UltraFeedback 7https://huggingface.co/datasets/openbmb/UltraInteract_sft 8https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized 9https://huggingface.co/datasets/argilla/distilabel-intel-orca-dpo-pairs 15 Additional experimental results We present full evaluation of mixture heads on RPR datset and HelpSteer2 dataset in Tab. 7 and Tab. 8, which further demonstrate the diversity and benefits of mixture heads compared with the single reward. We also report results on the RewardBench benchmark in Tab. 9, showing improvements over the single-head baseline. Table 7: Full evaluations on augmented RPR test set. Method Clarity Creativity Scientific Rigor UserFriendliness Storytelling Pedagogical Linguistic Creativity Factual Accuracy Humor Average Single Reward 0.4717 0.6806 MiCRo Head 1 MiCRo Head 2 MiCRo Head 3 MiCRo Head 4 MiCRo Head 5 MiCRo Head 6 MiCRo Head 7 MiCRo Head 8 MiCRo Head 9 MiCRo Head 10 0.0943 0.0943 0.9057 1.0000 0.1509 0.2075 0.3774 0.2830 0.9245 0. 0.8611 0.7083 0.1944 0.3333 0.7083 0.7917 0.8611 0.9028 0.4583 0.2639 MiCRo (Ours) 0.9170 0.6289 0.3333 0.1310 0.0833 0.9048 0.9524 0.0952 0.1190 0.2262 0.2143 0.8929 0. 0.8119 0.7978 0.5618 0.5169 0.5843 0.5730 0.6404 0.6404 0.7865 0.7303 0.8764 0.5730 0.8696 0.8375 0.8625 0.7750 0.2125 0.3500 0.8500 0.8625 0.8750 0.8750 0.5125 0. 0.7525 0.6452 0.4516 0.3871 0.5968 0.6774 0.5323 0.4839 0.5806 0.5968 0.8065 0.6613 0.7935 0.8654 0.9038 0.7788 0.1346 0.2788 0.8750 0.9038 0.9423 0.9519 0.6538 0. 0.8558 0.4225 0.8810 0.6594 0.0845 0.0423 0.9577 0.9577 0.1268 0.1690 0.3380 0.3099 0.8732 0.9577 0.8929 0.7024 0.3929 0.3452 0.9048 0.9405 0.9643 0.9524 0.9405 0. 0.8563 0.9109 0.8218 Table 8: Full evaluations on augmented HelpSteer2 test set. Model Single Reward MiCRo Head 1 MiCRo Head 2 MiCRo Head 3 MiCRo Head 4 MiCRo Head 5 MiCRo (Ours) Helpfulness Correctness Coherence Complexity Verbosity Average 0.7838 0.8108 0.8270 0.6595 0.6378 0. 0.8324 0.6686 0.7151 0.7035 0.6105 0.5523 0.7326 0.7140 0.6914 0.7407 0.7407 0.6543 0.5679 0. 0.7543 0.7907 0.7132 0.7209 0.8217 0.8217 0.7287 0.7627 0.8816 0. 0.8289 0.8355 0.9276 0.9211 0.8487 - - - - - 0.8513 0.7830 Table 9: Accuracy on RewardBench test set. We train the mixture heads on combined dataset consisting of HelpSteer2 and the PKU-SafeRLHF dataset. MiCRo also outperforms the single reward model. Model Chat Chat-hard Safety Reasoning Average Single Reward (3B) MiCRo-Hedge (Ours) 0.9693 0. 0.6930 0.7544 0.9135 0.9122 0.9189 0.9322 0.8737 0."
        }
    ],
    "affiliations": [
        "Columbia University",
        "Rice University",
        "University of Illinois at Urbana-Champaign"
    ]
}