{
    "paper_title": "When an LLM is apprehensive about its answers -- and when its uncertainty is justified",
    "authors": [
        "Petr Sychev",
        "Andrey Goncharov",
        "Daniil Vyazhev",
        "Edvard Khalafyan",
        "Alexey Zaytsev"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on a specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and $14$ topics. While MASJ performs similarly to a random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is $0.73$. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is $0.55$. More principally, we found out that the entropy measure required a reasoning amount. Thus, data-uncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide a more fair assessment of LLMs performance."
        },
        {
            "title": "Start",
            "content": "Petr Sychev, Andrey Goncharov, Daniil Vyazhev, Edvard Khalafyan, Alexey Zaytsev WHEN AN LLM IS APPREHENSIVE ABOUT ITS ANSWERS - AND WHEN ITS UNCERTAINTY IS JUSTIFIED Abstract. Uncertainty estimation is crucial for evaluating Large Language Models (LLMs), particularly in high-stakes domains where incorrect answers result in significant consequences. Numerous approaches consider this problem, while focusing on specific type of uncertainty, ignoring others. We investigate what estimates, specifically token-wise entropy and model-as-judge (MASJ), would work for multiple-choice question-answering tasks for different question topics. Our experiments consider three LLMs: Phi-4, Mistral, and Qwen of different sizes from 1.5B to 72B and 14 topics. While MASJ performs similarly to random error predictor, the response entropy predicts model error in knowledge-dependent domains and serves as an effective indicator of question difficulty: for biology ROC AUC is 0.73. This correlation vanishes for the reasoning-dependent domain: for math questions ROC-AUC is 0.55. More principally, we found out that the entropy measure required reasoning amount. Thus, datauncertainty related entropy should be integrated within uncertainty estimates frameworks, while MASJ requires refinement. Moreover, existing MMLU-Pro samples are biased, and should balance required amount of reasoning for different subdomains to provide more fair assessment of LLMs performance. 1. Introduction LLMs have succeeded in various tasks, demonstrating impressive capabilities in generating human-like text and solving complex problems. However, their ability to accurately assess uncertainty in their predictions remains significant challenge [17]. This gap between models confidence and its actual correctness, often called the confidence gap [5], poses critical risks, particularly in high-stakes applications where overconfidence in incorrect answers leads to severe consequences: in domains such as health, law [7], education, or economics an LLMs overestimation of its accuracy could result in harmful decisions or misinformation. Understanding and Key words and phrases: Question-Answering and Complexity and LLM and Uncertainty and Entropy. 1 5 2 0 2 3 ] . [ 1 8 8 6 1 0 . 3 0 5 2 : r 2 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV Figure 1. Question complexity evaluation pipeline. See more details in the methods section decreasing this confidence gap is therefore essential for ensuring the safe and reliable deployment of LLMs. Addressing this limitation has led to the development of numerous uncertainty estimation techniques. Most of them belong to one of two problem statements. They either focus on binary classification or rely on rich signals from long, free-text responses [26]. These methods often generalize poorly for concise multiple-choice questions. Moreover, existing methods tend to answer how good an overall confidence estimate is, ignoring why uncertainty appeared in specific case. While classic decomposition of uncertainty to aleatoric (data) and epistemic (model) [15] also holds for LLMs, the problem is challenging [14]. For example, the authors in [1] consider only epistemic uncertainty. To address these gaps, we propose pipeline that enables detailed investigation of domain-specific or complexity-labeled datasets with multiplechoice answers. This framework allows us to explore how commonly used uncertainty estimation methods, particularly entropy-based approaches, perform under varying question topics and levels of required reasoning. We can also leverage an auxiliary LLM to generate labels for reasoning, knowledge requirements, and the number of reasoning steps. The introduction of the pipeline led to more detailed investigation of datasets commonly used for benchmarking LLMs and their uncertainty estimates. In more detail, our main contributions are the following: WHEN LLM UNCERTAINTY IS JUSTIFIED 3 We present fully automated pipeline for evaluating uncertainty estimation approaches within the MMLU-Pro setting. It is presented in Figure 1 and allows datasets or tested methods to be varied. The associated work is available on Github1. We propose using data uncertainty estimates based on token-wise entropy and model uncertainty estimates based on MASJ. Thus, our approach covers two main components of uncertainty estimation: data and model one, given that MMLU-Pro answers are typically short and provide limited information on model confidence. Within the developed pipeline, we consider different subsets of the MMLU-Pro dataset with splits by domain and reasoning requirement estimated via MASJ proposed in this work. Our experimental evidence that includes consideration of four models (Phi-4, Mistral-Small-24B-Instruct, Qwen-1.5 B, Qwen-72B) suggest that the entropy predicts well errors of an LLM if the amount of required reasoning is small. Moreover, its quality increases if the model size increases. Unlike MASJ provides much weaker results, being unable to identify error patterns. Further progress here would be possible with more reasoning steps during an uncertainty estimate with an entropy-based approach on top of it. We also observe that the existing dataset MMLU-Pro has internal biases with the complexity of the question, the same as in [12], and the required reasoning amount for them varies significantly depending on the topic. Thus, more advanced and less biased datasets are required for fair measurement of the progress of LLMs. Additional human annotation of questions in dataset could identify further gaps in LLMs capabilities. 2. Related Works Tests have been popular way to estimate an individuals proficiency for more than 500 years [3]. After their emergence, LLMs have also been tested extensively and compared to each other or humans [22]. Several papers established benchmarks, with natural ones being QA datasets: an LLM receives prompt and should generate response citerogers2023qa. Different questions exist, but automated evaluation of answers is available 1https://github.com/LabARSS/question-complextiy-estimation SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV only for multiple-choice questions. For LLMs, slight changes in the way the questions are asked can significantly affect the accuracy of model, probably due to data leakage and format overfitting [8], other options are much more expensive and challenging. One of the recent options that takes into account possible leakages - and, on the other hand, is widely used is an expanded version of the MMLU dataset, called MMLU-Pro [24] with about 0.56 accuracy for Llama-70B model. The presented 12000 questions span 13 diverse topics, including STEM and other areas of scientific studies. The complexity, volume, and diversity of the dataset provide strong foundation for our study of how different aspects of the questions affect the probability of correctly answering them. In related works, datasets such as ARC-AI2 [6] and GPQA [20] have been widely used for evaluating multiple-choice QA in the scientific domain as well. However, ARC-AI2 is often criticized for its relatively low difficulty, while GPQA, though highly challenging, suffers from dataset size of fewer than 500 samples, which restricts its utility for robust evaluation. To address these limitations and ensure comprehensive assessment, we focus our final experiments on MMLU-Pro, which offers larger scale and more balanced difficulty level for evaluating model performance in scientific reasoning tasks. MMLU-Pro is suited for evaluating models ability to identify uncertainty in the generated output and signal potential errors, which addresses fundamental question in AI safety. In broader context, we can refer to this effect as hallucination [14]; in more classic context, it is prediction error [10]. Another related approach is assessing the question difficulty, provided in [11], but suitable there only for an RAG scenario. Universal Model-as-judge (MASJ) approaches use available models or bigger and more powerful model to evaluate LM outputs [27] and can be applied in broader context. What makes the problem more difficult is the diverse nature of possible uncertainty sources. Classic works decompose uncertainty into the model part, related to lack of knowledge of model about specific output, and the data part, related to the complexity of the data itself [15]. For natural language, model uncertainty refers to the familiarity of specific domain and the presence of relevant content in training sample. Data uncertainty refers to noise in the example or the overall complexity of question at hand. While broader classification exists with five different features for generated question [23], we follow this approach while identifying the WHEN LLM UNCERTAINTY IS JUSTIFIED 5 considered parts of the uncertainty. Using entropy-based measures, such as semantic entropy [18] and word-sequence entropy [25], is highly effective for evaluating uncertainty in LLMs because these methods capture the variability and confidence of model predictions in structured way. Semantic entropy, as proposed, quantifies uncertainty by considering the meaning of generated text, making it robust to paraphrasing and linguistic variations. Similarly, word-sequence entropy demonstrates that word-sequence entropy effectively measures uncertainty in open-ended tasks such as medical QA, providing insights into model confidence and reliability. In our case, we use the token-wise entropy approach [17] due to its simplicity and suitability for MMLU cases with multiple-choice short answers. For model uncertainty, we consider variation of the MASJ approach with specific prompt [27], [4]. This [21] survey paper provides comprehensive taxonomy of uncertainty estimation methods in LLMs, highlighting state-of-the-art approaches such as evaluation via LLM and tokenwise entropy. For an MMLU-type dataset, we would be able to identify how different types of uncertainties are combined for typical questions in different subjects and how our measurement corresponds to the estimate of the required amount of reasoning for specific questions. These results would help identify hallucinations and errors in language model outputs and provide insights into how to reduce the frequency of these effects by training the model using additional data and varying the model size. 3. Methods 3.1. General Pipeline. Input. We consider set of questions = {qi}n and set of answers = {ai}n {gi}n and correctness of the answers = {yi}n means equality of ground-truth answer ai and generated answer gi i=1 i=1 of size with generated answers = i=1 i=1, where yi is binary value which i=1. The questions are augmented with true topic labels = {ti}n Uncertainty estimation. Our procedures estimate the uncertainty of model response given question and corresponding answer = {ui}n i=1, each ui = (qi, gi) and depends on the LLM used. We additionally prompt models to obtain an estimation of the reasoning requirements = {ri}n i=1. Uncertainty validation. Finally, given all this information, we compare the uncertainty estimates with the correctness labels to obtain the ROC-AUC values and other quality measures. Additionally, we examine 6 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV how good specific uncertainty estimates are for separate topics or the required amount of reasoning R. Technical details. For each question in the dataset, we prompt considered model for an answer and record the entropy for the answer. We discard answers with incorrect formatting that appear in less than 5% of the questions. In model-as-judge (MASJ), we use large 123B Instruct Mistral model Mistral-Large-Instruct-2411 to obtain estimated complexity according to the level of education. The same MASJ technique with the same large model was utilized to obtain the reasoning estimate. We ask binary question of the deep reasoning required and how many steps it would take: low, medium, high. Specific prompts are available in the Appendix A. For more detailed analysis, we split the whole set of questions to topics using available labels or reasoning and educational level complexity obtained via MASJ. 3.2. Uncertainty Estimations. We consider two reliable approaches for uncertainty estimation: the entropy-based procedure [17] and the model-asa-judge [27]. Details on both of them are provided below. 3.2.1. Entropy Uncertainty Estimation. Entropy-based uncertainty estimation is fundamental approach for evaluating LLMs confidence in their predictions [9]. In MMLU, the expected answer is number with one or two tokens length. So, we use token-wise entropy from the models output logits. Our implementation computes entropy via two-step procedure. Firstly, we obtain the logits = (z1, . . . , zk) from the LLMs final layer for each token position, where equals the vocabulary size. Secondly, we convert these logits into probability distribution using the softmax function: pi = ezi j=1 ezj (cid:80)k . Next, we calculate the entropy of this distribution: = (cid:88) i=1 pi log pi. This approach aligns with the work [17], which showed that token-level entropy correlates well with model uncertainty and performance. When WHEN LLM UNCERTAINTY IS JUSTIFIED model is confident in its next-token prediction, the probability mass concentrates on small subset of tokens, resulting in low entropy. Conversely, when uncertain, the model distributes probabilities more evenly across the vocabulary, producing higher entropy values. We hypothesize that high-entropy responses generally correspond to questions where the model lacks the necessary knowledge or reasoning capacity. By analyzing the relationship between entropy and model performance across domains and difficult reasoning complexity, we can identify specific conditions under which LLMs are most prone to overconfidence or appropriate uncertainty calibration. 3.2.2. MASJ Uncertainty Estimation. The model-as-judge (MASJ) paradigm represents an alternative approach to uncertainty estimation that leverages an LLMs ability to evaluate given response. Unlike entropybased methods that rely on probability distribution across output tokens, MASJ employs prompt-based techniques. Our implementation of MASJ follows methodology similar to the approach described in the MT-Bench evaluation framework [27]. This approach extracts explicit confidence assessments that may capture uncertainty beyond what is reflected in the raw probability distribution: including broader reasoning about the models knowledge boundaries, the ambiguity of questions, and the models awareness of its own limitations. In instances where MT-Bench assigned score below 8 on scale of 1 to 10, we decided to regenerate the estimate. 4.1. Experimental setup. 4. Results 4.1.1. Datasets. MMLU-Pro. In this study, we utilize the MMLU-Pro dataset [24], more challenging and robust variant of the well-known Massive Multitask Language Understanding (MMLU) benchmark. The original MMLU dataset was introduced by Hendrycks et al. [13] and has been widely used to evaluate language model performance across diverse domains. MMLU-Pro builds upon this benchmark by incorporating filtered and refined subset of the original questions, also introducing newly generated question-answer pairs to enhance difficulty and robustness. The dataset consists of approximately 12000 multiple-choice questions, with 10 answer choices per question in the majority of cases (approximately 8 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV Category Mathematics Physics Chemistry Law Engineering Economics Health Psychology Business Biology Philosophy Computer Science History Other Sample Count 1351 1299 1132 1101 969 844 818 798 789 717 499 410 381 Table 1. MMLU-Pro Category Distribution 10000 instances), while the remaining 2000 instances contain between 3 to 9 answer options. The dataset spans 14 distinct scientific categories, covering broad range of disciplines with the number of topics by discipline provided in Table 1. Examples of questions from different categories are available in Appendix B. The motivation for using MMLU-Pro for this problem is its increased difficulty and improved robustness compared to the original MMLU dataset. Official reports for Phi-4 and Mistral-Small-24B also used the extended dataset to mitigate data leakage. Thus, MMLU-Pro is reliable and appropriate benchmark for evaluating our models generalization and reasoning capabilities. 4.1.2. Models. For QA and evaluation of the entropy, we use decoderonly LLMs Mistral (Mistral-Small-24B-Base-2501) [16], Phi-4 [2] and Qwen models [19]. 4.2. Reasoning and knowledge-based complexity. We hypothesize that different questions validate different abilities of model. Some focus more on the requirement to know specific things, while others consider models ability to reason within specific topic. To obtain these properties for specific questions, we obtained estimates via the model-as-judge. Figures 2 present the question type by the requirement to reason and the estimate for the required number of reasoning steps. WHEN LLM UNCERTAINTY IS JUSTIFIED 9 (a) Distribution of questions that require complex reasoning (b) Distribution of the required number of reasoning steps Figure 2. Estimation by MASJ of the required reasoning amount. Better to view in zoom The distribution of questions over topics is diverse, with the reasoning requirement attaining the maximum share of about 0.9 for engineering and only 0.5 for philosophy. The number of required steps also varies: again, for engineering, the model estimates the number of questions with high number of required steps, but for psychology and philosophy, this number is almost zero. 4.3. Distribution of uncertainty measures. For Phi-4 and Qwen we present the distribution of entropy values in Figures 3. The distribution of entropy is presented for correct and incorrect answers by model. We can see nearzero entropy values are much more often observed for correct answers for Phi-4 and Qwen. Thus, entropy only partially explains why the answers from model are wrong, and the quality of this score depends on the model used. Both numerical and nominal Model-as-judge scores also relate little to the complexity, as the corresponding measured ROC AUC values are 0.49 for both models, indicating nearly random predictions. Thus, MASJ uncertainty estimates focus on different aspects of an LLMs confidence, which is irrelevant to question complexity. 4.4. Complexity prediction via uncertainty estimate by category and question type. Complexity prediction by category. Figure 4 show the ROC-AUC values for entropy for all four models. Values above or below 0.5 indicate that the prediction is better than random. For most topics, the values are well above 10 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV (a) Phi-4 model (b) Qwen-72B model (c) Qwen-1.5B model Figure 3. Entropy distribution of answers. Best viewed when zoomed in this threshold. In psychology and biology, we observe ROC-AUC values of 0.77/0.83 (Qwen 72B) compared to 0.61/0.73 (Phi-4) and 0.65/0.65 (Mistral), with these improvements correlating across models. Complexity prediction by reasoning requirement. Figure 5 present similar results, but for the split of the questions by reasoning complexity. Table 2 shows aggregated ROC AUC scores across categories and reasoning levels. We see meaningful results of using the reasoning estimates for all questions without categorization by subject. Low entropy is marginally better predictor of the truthfulness of the answers to questions that did not require complex reasoning and vice versa for models. We also observe the estimate of the number of reasoning steps as less reliable metric, with Mistral favoring higher number of reasoning steps. However, we can attribute it to the lower precision of such estimation provided by the MASJ approach. Calibration assessment via entropy. Figure 6 illustrates calibration curves for four language models, computed using inverted normalized entropy. The analysis reveals systematic deviations between self-reported certainty and empirical accuracy, with distinct patterns across architectures: WHEN LLM UNCERTAINTY IS JUSTIFIED Figure 4. ROC-AUC for error prediction by subject for four different LLMs Figure 5. ROC-AUC by reasoning for four different LLMs 12 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV Category Biology Economics Philosophy Physics Computer Science Psychology Chemistry Health Business History Engineering Other Math Law No Reasoning Yes Reasoning Reasoning (Low) Reasoning (Med) Reasoning (High) Overall Phi-4 Mistral Qwen 1.5B Qwen 72B 0.73 0.65 0.65 0.63 0.61 0.61 0.61 0.60 0.58 0.58 0.58 0.56 0.55 0.50 0.59 0.56 0.59 0.57 0.53 0. 0.79 0.73 0.61 0.64 0.69 0.77 0.64 0.67 0.64 0.65 0.64 0.66 0.63 0.58 0.72 0.68 0.71 0.69 0.61 0.70 0.65 0.58 0.49 0.44 0.56 0.65 0.44 0.55 0.52 0.58 0.44 0.53 0.54 0.57 0.59 0.52 0.58 0.52 0.53 0.52 0.83 0.80 0.74 0.74 0.78 0.77 0.70 0.77 0.74 0.67 0.73 0.78 0.73 0.69 0.79 0.76 0.79 0.76 0.73 0.77 Table 2. ROC AUC performance comparison across categories and models Mistral exhibits unstable calibration dynamics, characterized by erratic accuracy in low-confidence regions. Despite moderate confidence estimates, its accuracy fluctuates unpredictably, even peaking anomalously in the lowest entropy bin. This contrasts with its pronounced underperformance in high-confidence regimes, where empirical accuracy consistently trails reported certaintya limitation indicative of architectural weaknesses in uncertainty quantification. WHEN LLM UNCERTAINTY IS JUSTIFIED 13 Figure 6. Calibration of LLMs based on their entropy Qwen 72B, despite its large scale, demonstrates inconsistent calibration. High-confidence predictions show weak alignment with accuracy, while mid-range entropy bins display substantial confidence-accuracy gaps. Qwen 1.5B emerges as an outlier, surpassing larger counterparts in calibration quality. Its confidence-accuracy relationship progresses monotonically across entropy bins, with minimal extreme deviations. These results underscore systematic overconfidence as pervasive issue, especially in high-certainty regimes where all models significantly overestimate their reliability. 5. Conclusion and discussion Risks in LLMs involve possible errors in answering questions with high confidence. One possible way to mitigate this problem is by highlighting 14 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV those questions that, with high probability, would be answered incorrectly. We suggest using uncertainty estimations for LLMs to identify possible errors. For more detailed analysis, the paper examines various subject areas, question types, and specific requirements for knowledge and reasoning. We explored how different uncertainty estimates for LLMs highlight potential errors of model and examined various aspects of problems in QA to identify what type of problems specific uncertainty estimate can highlight. Our focus was on reliable and popular methods, including model as judge and an entropy-based approach. Experimental evidence suggests that different uncertainty estimates perform differently across diverse subject areas. Our findings also demonstrate that the key factor is the varying composition of question sets within subject area collected for specific subject area: entropy performs much better when no reasoning or little reasoning is required. Thus, it primarily highlights the lack of the required knowledge. The conclusion also depends on the model used: for reasoning-oriented, such as Qwen-72B, entropy better highlights the possibility of an error compared to Mistral."
        },
        {
            "title": "References",
            "content": "1. Yasin Abbasi Yadkori, Ilja Kuzborskij, Andras Gyorgy, and Csaba Szepesvari, To believe or not to believe your llm: Iterative prompting for estimating epistemic uncertainty, Advances in Neural Information Processing Systems 37 (2025), 58077 58117. 2. Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al., Phi-4 technical report, arXiv preprint arXiv:2412.08905 (2024). 3. Steven Buyske, Optimal design in educational testing, Applied optimal designs (2005), 119. 4. Steffi Chern, Ethan Chern, Graham Neubig, and Pengfei Liu, Can large language models be trusted for evaluation? scalable meta-evaluation of llms as evaluators via agent debate, 2024. 5. Prateek Chhikara, Mind the confidence gap: Overconfidence, calibration, and distractor effects in large language models, 2025. 6. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord, Think you have solved question answering? try arc, the ai2 reasoning challenge, arXiv preprint arXiv:1803.05457 (2018). 7. Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel Ho, Large legal fictions: Profiling legal hallucinations in large language models, Journal of Legal Analysis 16 (2024), no. 1, 6493. 8. Ahmed Elhady, Eneko Agirre, and Mikel Artetxe, WiCkeD: simple method to make multiple choice benchmarks more challenging, arXiv preprint arXiv:2502.18316 (2025). WHEN LLM UNCERTAINTY IS JUSTIFIED 9. Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander Panchenko, Timothy Baldwin, et al., Fact-checking the output of large language models via token-level uncertainty quantification, Findings of the Association for Computational Linguistics ACL 2024, 2024, pp. 93679385. 10. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, et al., LM-polygraph: Uncertainty estimation for language models, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, 2023, pp. 446461. 11. Matteo Gabburo, Nicolaas Jedema, Siddhant Garg, Leonardo Ribeiro, and Alessandro Moschitti, Measuring question answering difficulty for retrieval-augmented generation, (2024). 12. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, Claire Barale, Robert McHardy, Joshua Harris, Jean Kaddour, Emile van Krieken, and Pasquale Minervini, Are we done with mmlu?, 2025. 13. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt, Measuring massive multitask language understanding, International Conference on Learning Representations, 2021. 14. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al., survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions, ACM Transactions on Information Systems 43 (2025), no. 2, 155. 15. Eyke Hullermeier and Willem Waegeman, Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods, Machine learning 110 (2021), no. 3, 457506. 16. A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al., Mistral 7b, arXiv preprint arXiv:2310.06825 (2023). 17. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli TranJohnson, et al., Language models (mostly) know what they know, arXiv preprint arXiv:2207.05221 (2022). 18. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar, Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation, 2023. 19. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu, Qwen2.5 technical report, 2025. SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV 20. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman, Gpqa: graduatelevel google-proof q&a benchmark, 2023. 21. Ola Shorinwa, Zhiting Mei, Justin Lidard, Allen Z. Ren, and Anirudha Majumdar, survey on uncertainty quantification of large language models: Taxonomy, open research challenges, and future directions, 2024. 22. Fedor Sizov, Cristina Espana-Bonet, Josef van Genabith, Roy Xie, and Koel Dutta Chowdhury, Analysing translation artifacts: comparative study of LLMs, NMTs, and human translations, Proceedings of the Ninth Conference on Machine Translation, 2024, pp. 11831199. 23. Yuwei Wan, Yixuan Liu, Aswathy Ajith, Clara Grazian, Bram Hoex, Wenjie Zhang, Chunyu Kit, Tong Xie, and Ian Foster, SciQAG: framework for auto-generated science question answering dataset with fine-grained evaluation, arXiv preprint arXiv:2405.09939 (2024). 24. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al., MMLU-Pro: more robust and challenging multi-task language understanding benchmark, Advances in Neural Information Processing Systems 37 (2024), 9526695290. 25. Zhiyuan Wang, Jinhao Duan, Chenxi Yuan, Qingyu Chen, Tianlong Chen, Yue Zhang, Ren Wang, Xiaoshuang Shi, and Kaidi Xu, Word-sequence entropy: Towards uncertainty estimation in free-form medical question answering applications and beyond, 2024. 26. Ruihan Yang, Caiqi Zhang, Zhisong Zhang, Xinting Huang, Sen Yang, Nigel Collier, Dong Yu, and Deqing Yang, Logu: Long-form generation with uncertainty expressions, 2024. 27. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al., Judging LLM-as-ajudge with MT-bench and chatbot arena, Advances in Neural Information Processing Systems 36 (2023), 4659546623. Appendix A. Prompts This section provides prompts used for Model-as-judge approaches: for numerical estimation of complexity for MASJ in Table 3, for nominal estimation of complexity for MASJ in Table 4, and for reasoning level estimate in Table 5. WHEN LLM UNCERTAINTY IS JUSTIFIED 17 You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity as number from 0 to 1 following the following scale as reference: high_school_and_ easier - 0.0-0.22, undergraduate_easy - 0.2-0.4, undergraduate_hard - 0.4-0.6, graduate - 0.6-0.8, postgraduate - 0.8-1.0. You must return the complexity by strictly following this format: \" [[complexity]]\", for example: \"Your explanation... Complexity: [[0.55]]\", which corresponds to hard question at the undergraduate level. Table 3. Prompt for Numerical model-as-judge You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity by strictly following the scale: high_school_and_easier, undergraduate_easy, undergraduate_hard, graduate, postgraduate. You must return the complexity by strictly following this format: \"[[complexity]]\", for example: \"Your explanation... Complexity: [[undergraduate]]\", which corresponds to the undergraduate level. Table 4. Prompt for Nominal model-as-judge SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV You are an expert in the topic of the question. Please act as an impartial judge and evaluate the complexity of the multiple-choice question with options below. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must not answer the question. You must rate the question complexity by strictly following the criteria: 1) [[Requires knowledge]] - do we need highly specific knowledge from the domain to answer this question? Valid answers: yes, no; 2) [[Requires reasoning]] - do we need complex reasoning with multiple logical steps to answer this question? Valid answers: yes, no; 3) [[Number of reasoning steps]] - how many reasoning steps do you need to answer this question? Valid answers: low, medium, high. Your answer must strictly follow this format: \"[[Requires knowledge: answer]] [[Requires reasoning: answer]] [[Number of reasoning steps: answer]]\". Example 1: \"Your explanation... [[Requires knowledge: yes]] [[Requires reasoning: no]] [[Number of reasoning steps: low]]\". Example 2: \"Your explanation... [[Requires knowledge: no]] [[Requires reasoning: yes]] [[Number of reasoning steps: high]]\". Example 3: \"Your explanation... [[Requires knowledge: yes]] [[Requires reasoning: yes]] [[Number of reasoning steps: medium]]\". Example 4: \"Your explanation... [[Requires knowledge: no]] [[Requires reasoning: no]] [[Number of reasoning steps: low]]\". Table 5. Prompt for Reasoning level estimate Appendix B. Question examples with answers This section provides examples of questions included in the MMLUPro dataset on Law, Psychology, and Engineering in Tables 6, 7, and 8, correspondingly. Which of the following criticisms of Llewellyns distinction between the grand and formal styles of legal reasoning is the most compelling? 1. There is no distinction between the two forms of legal reasoning. 2. Judges are appointed to interpret the law, not to make it. 3. It is misleading to pigeon-hole judges in this way. 4. Judicial reasoning is always formal. Table 6. Example of question from Law category WHEN LLM UNCERTAINTY IS JUSTIFIED 19 66-year-old client who is depressed, has rhythmic hand movements, and has flattened affect is probably suffering from: 1. Huntingtons disease 2. Creutzfeldt-Jakob disease 3. Multiple Sclerosis 4. Alzheimers disease 5. Parkinsons disease 6. Vascular Dementia 7. Frontotemporal Dementia 8. Schizophrenia 9. right frontal lobe tumor 10. Bipolar Disorder Table 7. Example of question from Psychology category cumulative compound motor has varying load upon it which requires variation in armature current from 50 amp to 100 amp. If the series-field current causes the air-gap flux to change by 3 percent for each 10 amp of armature current, find the ratio of torques developed for the two values of armature current. 1. 2.26 2. 0.66 3. 3.95 4. 1.00 5. 2.89 6. 1.75 7. 4.12 8. 1.15 9. 0.87 10. 3. Table 8. Example of question from Engineering category 20 SYCHEV, GONCHAROV, VYAZHEV, KHALAFYAN, ZAYTSEV Appendix C. Entropy Distributions Figure 7. Distribution of entropy values for Qwen models (72B, 32B, 14B, 3B, 1.5B, 0.5B), stratified by answer correctness. For larger models (72B, 32B), correct answers (blue) exhibit pronounced left skew toward low entropy, indicating higher confidence in accurate predictions. Smaller models (1.5B, 0.5B) show flatter distributions, with less separation between correct and incorrect (orange) entropy values. Notably, the 72B variant demonstrates near-zero entropy peaks for correct responses, aligning with findings from Section 4.2. This section complements the analysis in Section 4.3 by providing full entropy distributions for the Qwen model family. Figure 7 reveals two key trends: WHEN LLM UNCERTAINTY IS JUSTIFIED 21 Model scale correlates with entropy separation: Larger models (72B, 32B) show clearer divergence between correct (low entropy) and incorrect (higher entropy) predictions, while smaller variants (3B, 0.5B) exhibit overlapping distributions. (P. Sychev) Skolkovo Institute of Science and Technology (Skoltech) / Moscow, Russia National Research University Higher School of Economics / Moscow, Russia E-mail: petr.sychev@skoltech.ru (A. Goncharov) Skolkovo Institute of Science and Technology (Skoltech) / Moscow, Russia E-mail: Andrey.Goncharov@skoltech.ru (D. Vyazhev) Skolkovo Institute of Science and Technology (Skoltech) / Moscow, Russia National Research University Higher School of Economics / Moscow, Russia E-mail: daniel.vyazhev@skoltech.ru (E. Khalafyan) Moscow Institute of Physics and Technology / Moscow, Russia Skolkovo Institute of Science and Technology (Skoltech) / Moscow, Russia E-mail: khalafyan.ea@phystech.edu (A. Zaytsev) Skolkovo Institute of Science and Technology (Skoltech) / Moscow, Russia E-mail: a.zaytsev@skoltech.ru"
        }
    ],
    "affiliations": []
}