{
    "paper_title": "START: Spatial and Textual Learning for Chart Understanding",
    "authors": [
        "Zhuoming Liu",
        "Xiaofeng Gao",
        "Feiyang Niu",
        "Qiaozi Gao",
        "Liu Liu",
        "Robinson Piramuthu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair a structured visual layout (spatial property) with an underlying data representation (textual property) -- grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLM's understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with a novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with a Large Language Model (LLM) to ascertain the positions of chart elements that capture the chart's visual structure, addressing challenges that existing methods cannot handle. To evaluate a model's ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling a critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by a clear margin. Code, data and models will be publicly available."
        },
        {
            "title": "Start",
            "content": "START: Spatial and Textual Learning for Chart Understanding Zhuoming Liu1*, Xiaofeng Gao2, Feiyang Niu2, Qiaozi Gao2, Liu Liu3, Robinson Piramuthu2 1University of Wisconsin-Madison 2Amazon AGI 3MIT 5 2 0 2 ] . [ 1 6 8 1 7 0 . 2 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Chart understanding is crucial for deploying multimodal large language models (MLLMs) in real-world scenarios such as analyzing scientific papers and technical reports. Unlike natural images, charts pair structured visual layout (spatial property) with an underlying data representation (textual property) grasping both is essential for precise, fine-grained chart reasoning. Motivated by this observation, we propose START, the Spatial and Textual learning for chART understanding. Specifically, we introduce (i) chart-element grounding and (ii) chart-to-code generation to strengthen an MLLMs understanding of both chart visual layout and data details. To facilitate spatial and textual learning, we propose the START-Dataset generated with novel data-generation pipeline that first leverages an MLLM to translate real chart images into executable chart code, recovering the underlying data representation while preserving the visual distribution of real-world charts. We then evolve the code with Large Language Model (LLM) to ascertain the positions of chart elements that capture the charts visual structure, addressing challenges that existing methods cannot handle. To evaluate models ability to understand chart spatial structures, we propose the Chart Spatial understanding Benchmark (CS-Bench), filling critical gap in comprehensive chart understanding evaluation. Leveraging spatial and textual learning, START delivers consistent gains across model sizes and benchmarks over the base models and surpasses prior state-of-the-art by clear margin. Code, data and models will be publicly available. 1. Introduction The rapid advancement of multimodal large language models (MLLMs) has opened new frontiers in artificial intelligence, enabling processing and reasoning across text, images, and other modalities simultaneously. As their capabilities continue to grow, successful deployment in real-world applications increasingly hinges on their ability to accurately understand and interpret complex visual information. *Work done during internship at Amazon AGI. Figure 1. A: the overview of the START, which leverages spatial and textual learning for chart understanding. B: challenging question sample from the CharXiv [59]. Answering the question requires chart element grounding and step-by-step reasoning. Among various types of visual content, charts represent particularly challenging yet essential domain for MLLM, especially in real-world scenarios such as analyzing scientific papers, technical reports, and financial documents. However, despite significant progress in general multimodal understanding, current MLLMs often struggle with understanding complicated visual structure and details in the charts. Even the best vision reasoning model OpenAI o3 [44] still lags behind the human level understanding to the charts [59]. Figure 1-B shows sample question from CharXiv [59], which requires step-by-step reasoning and chart element grounding based on the instruction in the question. The Qwen2.5-VL [1], one of the best open-source MLLMs, makes mistakes given that it does not ground the condition to the x-axis correctly, justifying the difficulties of the charts understanding. Unlike natural images that primarily convey semantic content through objects and scenes, charts are artificial visual input that pair structured spatial layout with an underlying textual data representation. They typically include subplots, titles, legends, and axes, and are instantiated from data sourcessuch as tablesor rendered by code (e.g., Python [12, 66]). Motivated by the properties of the chart, we raise two research questions: 1. Can explicitly learning the spatial structure of the chart and recovering the textual details in the chart help the chart understanding? and 2. To 1 facilitate spatial and textual learning, how should we construct the dataset? To address these questions, we propose START, Spatial and Textual learning chART understanding, as shown in Figure 1-A. Specifically, we formalize the spatial and textual learning in supervised finetuning (SFT) and reinforcement learning (RL) by considering two additional tasks during the training beyond the chart question answering (CQA): (i) chart element grounding, which improves the models ability to localize and identify specific visual components, explicitly learning the spatial structure of the chart and (ii) chart-to-code generation, which enhances the models understanding of the underlying textual details of the chart, also implicitly learning the chart layout from Python code. This spatial and textual learning approach ensures that models develop both the textual analytical capabilities needed for data interpretation and the spatial reasoning required for chart elements or structures identification. To facilitate spatial and textual learning, we propose the START-Dataset with novel data-generation pipeline that bypasses the template-driven visual simplicity, limited layout/style diversity, and chart-type distribution mismatch characteristic of code-based synthetic datasets [12, 66]. Our method employs an MLLM to translate real chart images [25] into executable chart Python code, preserving the visual complexity and diversity of real-world charts and reconstructing the underlying data representation of the chart. Based on the recovered chart code, we propose to find the location of the chart element by evolving the Python code with large language model (LLM), addressing the issue that the existing MLLM [1], grounding [33], or segmentation model [47] could not ground the chart element correctly. These precise chart elements locations, coupled with the Python code for rendering the chart image, enable the spatial and textual learning for the chart understanding. To evaluate the models understanding of the chart visual structure, we propose the Chart-Spatial understanding Benchmark (CS-Bench) to fill in the missing piece in comprehensive chart understanding. Our comprehensive evaluation demonstrates that START achieves substantial improvements over the base model across multiple chart understanding benchmarks in both reinforcement learning (RL) and supervised fine-tuning (SFT) paradigms. Specifically, START-RL-7B outperforms the previous best [4] by 42.7, 14.7, 1.5, 2.1, and 35.7 on ChartMimic [64], CharXiv-descriptive [59], CharXiv-reasoning, ChartQAPro [40], and CS-Bench, respectively. Our main contributions are summarized as follows. We propose START, spatial and textual learning framework for chart understanding in SFT and RL, leveraging code learning and chart element grounding to enhance MLLMs understanding toward the chart. To facilitate the spatial-textual training, we create the START-Dataset and new dataset construction pipeline by converting the real chart images to Python codes, then evolving the code with LLM to obtain chart element locations. We propose the Chart Spatial understanding Benchmark (CS-Bench) to evaluate the MLLMs spatial understanding of the chart. START achieves substantial improvement across different model scales and various benchmarks and outperforms the previous best by clear margin. 2. Related Works Multi-modal Large Language Model. Instruction tuning with visual and text data [3032] has led to surge of interest in using the Large Language Model for Visual Understanding. Later works, LLaVA-OneVision [24], Qwen2.5VL [1], scaling up the training with visual input from different modalities (images, videos, etc), substantially improve the model performance on different visual understanding tasks and introduce the concept of Multi-modal Large Language Model (MLLM) [5, 67]. Recent success of DeepSeek-R1 [9] shows that reason-before-answering coupled with reinforcement-learning [50] yields substantial gains on difficult math [27, 55] and programming benchmarks [17, 45]. Motivated by it, vision-reasoning models such as Vision-R1 [15] and R1-OneVision [65] transplant these think-then-answer strategies to the multimodal setting, improving MLLM performance on challenging visual reasoning benchmarks [34, 71]. In our work, we propose spatial and textual learning for both supervised finetuning and reinforcement learning, enabling comprehensive chart learning for MLLM on different training paradigms. Chart Understanding. Early research in chart understanding [2, 6, 19, 22, 35, 43, 46] primarily explored structural designs, combining Convolutional Neural Network (CNN) encoders [11] with Recurrent Neural Network (RNN) decoders [14, 49]. Subsequent works [36, 53] introduced multi-stage pipelines and transformer-based architectures, yielding stronger performance. More recently, multimodal large language models (MLLMs) have become the dominant paradigm for chart understanding. Within this line, some studies enhanced instruction-tuning data [10, 38, 39, 63], while others improved MLLM reasoning via chart-totext tasks [42, 70] or reinforcement learning [4, 41]. parallel thread of research has focused on chart-to-code generation [3, 18, 54]. From data perspective, early works on chart image synthesis [19, 20, 43] relied on parameterized templates to produce synthetic variants. To expand both scale and diversity, later methods [10, 12, 66] leveraged LLMs to In paralevolve Python code and render new charts. lel, real-world chart images [25, 3638] were collected from the web to better capture natural chart distributions. 2 for chart understanding with multi-modal large language model (MLLM). Background of MLLM. Multi-modal Large Language Models (MLLMs) extend the capability of Large Language Models (LLMs) by integrating visual encoders with textbased transformers. The visual encoder maps raw pixels into latent feature space, which is then aligned with textual embeddings in the shared language model backbone. This alignment enables the model to process and reason over both visual and textual inputs jointly. Formally, given an image and text prompt q, visual encoder and the adapter Ev and text encoder Et encodes them into vision embeddings zI and text embedding zq: zI = Ev(I), zq = Et(q), (1) These embeddings are then fused in shared feature space and decoded by the LLM backbone to generate outputs: = D(zI , zq). (2) Here, can represent different outputs in text, such as localized grounding coordinates, natural language responses, or programmatic code, depending on the task instruction. This unified framework allows MLLMs to support spatial-textual reasoning in chart understanding. Spatial Learning for visual inputs [21, 33] aims to identify object instances relevant to given question from the input image, capturing the relationships among these objects, and thereby improving scene understanding. In the context of chart understanding, we treat each chart element (e.g., title, legend, subplot, etc) as an instance. Spatial learning enables the model to better understand the visual structure and layout of the chart and to establish mapping between chart concepts and their corresponding locations. Formally in MLLMs chart understanding, given an image and question qs related to the charts spatial structure, MLLM predicts: zqs = Et(qs); ys = D(zI , zqs ) (3) where ys denotes the location of the answer within the chart, and zI is defined in equation 1. Textual Learning for visual inputs [18, 51] focuses on bridging the details in visual inputs with their textual counterparts, thereby recovering the underlying data semantics encoded in the visual signal. For chart understanding, we consider the Python code used to render chart as the textual representation of its underlying data and structure. Textual learning builds connections between chart element locations, their associated values, and the corresponding formulas or rendering instructions in the code. Formally in MLLMs chart understanding, given an image and Figure 2. STARTs reward design in reinforcement learning. Complementary to data construction, recent efforts introduced comprehensive benchmarks for chart question answering [19, 20, 29, 36, 40, 56, 59] and chart-to-code/text generation [60, 62, 64]. However, prior research largely overlooks the importance of spatial structure in chart understanding. Motivated by the charts property, we introduce spatial and textual learning for chart understanding, which explicitly models the spatial structures of charts. To enable this, we propose new dataset construction pipeline that lies between synthetic and real chart approaches: it generates diverse synthetic charts while also providing the corresponding Python codes. Furthermore, we design new benchmark for chart spatial understanding, filling in the gap in comprehensive chart evaluation. Spatial and textual understanding. Previous works show that both spatial and textual learning enhance visual understanding. MDETR [21] demonstrates that improving object localization boosts performance on vision-language tasks. V* [61] and OpenAI o3 [44] further reveal that accurate object localization and zooming improve fine-grained visual understanding. Similarly, spatial reasoning has been shown to benefit downstream tasks such as navigation [48]. Others research highlights the role of textual learning, where visual scenes are converted into text for improved reasoning. Examples include document-to-text [51, 52], video captioning for video understanding [57, 69], and chart-to-text conversion for chart reasoning [18]. Inspired by the dual spatialtextual nature of charts, our work integrates both perspectives for the first time and demonstrates that spatial and textual understanding significantly improves MLLMs on chart understanding. 3. Spatial and Textual Learning for Chart Understanding (START) In this section, we first introduce the definition of spatial and textual learning for chart understanding in section 3.1. To facilitate the spatial and textual learning, we construct the START-Dataset and introduce the details in section 3.2. Finally, we introduce the Chart Spatial understanding Benchmark (CS-Bench) in section 3. 3.1. Method Motivated by the dual property of the chart, which has structured spatial layout and the corresponding textual data representation, we propose spatial and textual learning 3 Figure 3. A: The analysis of the existing chart datasets and B: the overview of the START-dataset generation pipeline. prompt qt that instructs the model to generate its textual representation for the input chart image, MLLM predicts: We use = 0.9 in our case. Please refer to the supplementary material for more information on the learning design. zqt = Et(qt); yt = (zI , zqt) (4) 3.2. START-Dataset where yt is the textual representation (i.e., Python code) of the input chart image, and zI shares the same definition as the one in equation 1. By incorporating textual learning, the model gains the ability to capture fine-grained details from the chart image, thereby enhancing its overall understanding of the chart. Learning Designs. We apply spatial and textual learning in two learning paradigms, supervised finetuning (SFT) and reinforcement learning (RL). We train MLLMs with spatial and textual learning, along with the chart question answering (CQA). This ensures that the MLLM leverages spatial and textual learning to enhance chart understanding while preserving its original conversational capabilities. For SFT, we update the model by minimizing the negative log likelihood loss. In RL, consider updating the model with Group Relative Policy Optimization [50] (GRPO). For the reward design in the RL training, the reward consists of two parts: 1. the accuracy reward Racc and 2. the format reward Rf ormat. For the accuracy reward, we apply different metric to judge the reward for the prediction. For CQA samples, we use the string matching from the Mathruler [13] to judge the correctness of the prediction. We use the IoU value between the predicted bounding box and the ground truth bounding box as the reward for the spatial learning samples. We apply an LLM as judge to grade the generated Python code for the textual learning samples. For the format reward, we mainly apply text matching with regular expression on model predictions. The details can be found in Figure 2. The final reward Ri for the prediction oi is defined as: Ri = Racc + (1 a) Rf ormat (5) 4 To enable effective spatial and textual training, we need dataset comprising high-quality chart images paired with (1) Ds capturing the charts visual layout (i.e., chart element locations), which will be used in the spatial learning. (2) its underlying data representation Dt (i.e., the Python code used to render the chart image), which facilitates the textual learning. (3) instruction learning question-answer pairs Dq, which will be used in the chart question answering learning. Motivation and Chart Dataset Analysis. Existing chart datasets generally fall into two categories: (1) synthesisimage-based and (2) real-image-based. Synthesis-imagebased datasets, such as ReachQA [12], are generated using Large Language Models (LLMs). Starting from set of seed chart codes, LLMs are prompted to combine or modify these seeds to produce diverse variants with greater visual complexity. In contrast, real-image-based datasets, such as ArxivQA [25], in which charts are collected online, reflects the distribution of charts in real-world scenarios. Figure 3-A presents samples from both ReachQA and ArxivQA, along with their chart-type and subplot-count distributions. Visually, charts from code-based datasets tend to be simpler, with fewer subplots and less detail per subplot (e.g., fewer lines or bars) compared to their real-image counterparts. Statistically, code-based datasets also differ in chart-type distribution and overwhelmingly feature single-subplot chartsfurther highlighting the domain gap between the two dataset types. However, real-image-based datasets cannot directly serve our needs due to their lack of underlying data representationssuch as data tables or the Python code required to render the charts. This limitation motivates us to develop our own dataset that preserves the visual complexity of realFigure 4. A: The dataset sample visualization and B: the START-SFT and START-RL dataset statistics. world charts and provides their underlying data representations. As shown in Figure 3-B, our dataset construction pipeline consists of three steps: 1. to prepare the spatial learning data Dt, we leverage the chart to code conversion to obtain Python code along with the reconstructed chart. 2. code evolution for obtaining the chart elements location and spatial learning data Ds. 3. question-answer pairs generation and verification, preparing the data for chart question answering Dq. Textual Learning Data Construction. For textual learning, we prepare the (chart image)-(chart code) pairs as the textual learning data. To obtain chart images that share similar distribution to the real chart data, and also come with the Python code, we converted the chart images from the ArxivQA [25] to Python code and then rendered the reproduced chart image. To convert the chart image to Python code, we explored different approaches. Our exploration shows that using strong MLLM to directly convert the chart images to Python codes yields the most authentic reproduced chart images within reasonable budget. After we obtain the codes, we then convert the codes to questionanswer pairs with fixed templates. This obtains the training data Dc for the textual learning. Please see more details in the supplementary material A.1. Spatial Learning Data Construction. For spatial learning, we need to obtain the chart element locations on the image as spatial learning data. Some existing works like ChartQA [36], FigureQA [20] leverage SVG parsing or fixed template to find the chart element location, but these strategies become infeasible when the chart image contains multiple subplots, diverse layouts, and densely packed details. Another idea for finding the chart element locations is to use multi-modal large language model (MLLM) [1], grounding model [33], or segmentation model [47]. However, these models could not ground the chart element correctly, given that they barely trained on the chart images, which makes them hard to understand the chart concept. To ensure the chart elements location is consistent with the chart image, we generate the chart element locations while rendering the chart image. We propose to evolve the Python code with Large Language Model (LLM) to obtain the location of the chart elements. Specifically, for each chart type, we prepare seed codes that leverage the matplotlib built-in function to obtain the chart elements location on the rendered chart image, and further save the obtained chart element location into JSON file. We then use these seed codes as examples to prompt the LLM to evolve the Python code generated in the previous section. Running the evolved code will generate rendered chart image and JSON file that stores all locations of the chart element. We then convert the chart element locations to question-answer pairs with fixed templates, obtaining the data Dg that will be used in the spatial learning. Please see more details in the supplementary material A.2. QA Learning Data Construction. For chart question answer (CQA) learning, we generate high-quality questionanswer pairs related to chart image. We prompt the MLLM to generate questions that do not require external domain knowledge but instead rely solely on the chart content. This prevents the model from simply memorizing domain-specific answers and encourages the model to learn reasoning over general chart structures and content. To ensure the QA quality, we prompt strong MLLM to detect the hallucinated questions that refer to non-existent elements or those beyond the MLLMs capacity (e.g., precisely counting 200 dots) and verify the correctness of each answer. Based on these verdicts, we filter the QA pairs to obtain the final dataset for the CQA learning Dq. We further categorize the questions into global reasoning or local reasoning, based on whether they need reasoning over multiple chart elements. Please see more details in the supplementary material A.3. Dataset Visualization and Statistics. We show the sample visualization in Figure 4-A. Each chart image is paired 5 Figure 5. A: The samples from refChartQA [56], the locations are related to limited types of chart components, and focus on single-subplot chart images. B: the CS-Bench statistics. C: the samples from CS-Bench with the visualized target region under red mask. with the Python code, which is used to render it, the chart question answering data, the spatial learning data, and the textual learning data. We combine the data from chart question answering, spatial learning, and textual learning as the Supervised-Finetuning Dataset (START-Dataset-SFT), and we sample the Reinforcement Learning Dataset (STARTDataset-RL) from the START-Dataset-SFT based on the question difficulties. Figure 4-B shows the statistics of START-SFT and START-RL. 3.3. CS-Bench Motivation. Existing chart benchmarks primarily focus on chart understanding [36, 40, 59] or chart-to-text generation [62, 64], while the evaluation of models understanding to charts spatial structure remains underexplored. RefChartQA [56] takes step in this direction by requiring models to ground chart elements relevant to question. However, its grounding is limited to simple casessuch as locating points on line or bars in bar chartwhere bounding boxes are strongly influenced by human annotation bias and lack explicit chart-related semantics. Moreover, it is restricted to single-subplot charts (see Figure 5-A for examples). To address these limitations, we introduce the Chart Spatial understanding Benchmark (CS-Bench), which specifically evaluates MLLMs on their ability to reason about and localize spatial structures in charts, thereby filling crucial gap in the comprehensive assessment of chart understanding. Data Construction Pipeline. To ensure the quality of our benchmark, CS-Bench is constructed from held-out set of chart images rendered by the evolved code from our START-Dataset pipeline. This strategy guarantees that the charts in the benchmark reflect the complexity and distribution of those found in real-world scenarios. CS-Bench incorporates two question types to rigorously assess models spatial reasoning capabilities: 1) Grounding Questions: These directly prompt model to localize specific chart elements, such as subplot, an axis tick value, or the title. By converting known element locations into question-answer pairs using fixed templates, these questions provide direct evaluation of the models core spatial understanding. 2) QA Grounding Questions: These present more complex task where the model must first answer question related to the charts content and then localize the visual elements that are mentioned in the question or referred to in the answer. This evaluates deeper level of comprehension, specifically the models ability to connect concepts within the question or answer to the correct chart structures. To maintain the highest standard of quality and reliability, all question-answer pairs and their corresponding element locations in the benchmark have been manually verified. We present the benchmark statistics the Figure 5-B and visualize the samples in the benchmark in Figure 5-C. The image distribution in CS-Bench is similar to real-world chart collections, which are dominated by images containing multiple subplots. Localizing chart elements in these complex layouts is significant challenge, thereby demonstrating the benchmarks ability to rigorously evaluate models performance. Metrics. Following the existing spatial understanding benchmarks (e.g. image grounding [28, 68], video grounding [8, 23, 72]), we report the recall of ground-truth (GT) bounding boxes at an Intersection over Union (IoU) threshold of 0.3, evaluated on both grounding and QA grounding questions. As an auxiliary metric, we also report the accuracy on QA grounding questions. We provide more details in the supplementary material section B. 4. Experiments and Results In this section, we present the evaluation of the STARTs performance across various chart understanding tasks, as described in Section 4.1. We then conduct an ablation study in Section 4.2 to demonstrate the effectiveness of chart element grounding and chart-to-code components. Finally, in Section 4.3, we provide visualizations and offer detailed analysis of our method. 6 Method Model size CharXiv [59] desc rea ChartQA [29] ChartQAPro [40] ChartMimic [64] General MLLMs Qwen2.5-VL [1] Qwen2.5-VL [1] Chart-specific MLLMs TinyChart [70] ChartGemma [39] ChartReasoner [18] ECD [66] Chart-R1 [4] Ours models START-SFT START-RL START-SFT START-RL 3B 7B 3B 3B 7B 7B 7B 3B 3B 7B 7B 59.7 (58.6) 66.6 (73.9) 32.1 (31.3) 41.4 (42.5) - - - 74.2 62.0 63.1 72.2 66.9 77.6 8.3 12.5 - 40.2 45.2 (46.2) 35.9 40.0 44.0 46. 84.0 87.3 83.6 80.1 86.3 85.3 91.0 84.4 84.8 87.6 88.8 32.9 41.3 13.2 6.8 39.9 22.5 44.0 34.2 38.2 41.8 46. 29.3 40.2 - - - 46.8 18.7 31.5 45.3 50.7 63.8 CS-Bench R@0.3 acc 16.2 19.3 - - - 3.6 9.6 26.9 41.3 31.0 45.3 45.0 50.2 - - - 7.3 54.5 58.8 60.5 57.6 62. Table 1. Results of general chart understanding. We follow [66] to present the reproduced number of Qwen models and add the number from Qwen technical report [1] in gray as reference, similarly for Chart-R1. START shows substantial improvement over the base models and outperforms the previous best, Chart-R1 [4], on CharXiv, ChartQAPro, ChartMimic and CS-Bench by clear margin. Figure 6. The visualization of the predictions from START verse Qwen2.5-VL [1]. Benefit from the spatial and temporal learning, START produces better predictions in chart question answering (Subplot A), chart element grounding (Subplot B), and chart-to-code (Subplot C), reflecting the enhancement in MLLMs spatial and textual understanding toward the charts. Method CharXiv [59] desc rea SFT 61.1 Q+C 60.5 Q+C+G 63.1 RL 68.3 Q+C 72.3 Q+C+G 72. 35.6 35.5 35.9 37.0 38.7 40.0 CQAPro [40] CMimic [64] 32.8 34.1 34.2 36.4 37.5 38.2 30.9 32.1 31. 32.0 43.0 45.3 CS-Bench R@0.3 acc 20.6 22.2 26.9 22.6 22.8 41. 49.0 51.1 58.8 57.8 58.6 60.5 Table 2. Ablation Study of chart question answering (Q), chart-to-code (C), and chart element grounding (G) on CharXiv, ChartQAPro (CQAPro), ChartMimic (CMimic), and CS-Bench. Adding chart-to-code enhances the textual understanding of the chart, obtaining consistent improvement on CQAPro and CMimic, while adding chart element grounding improves the spatial understanding of the chart, boosting the models performance on different benchmarks. 4.1. Results on Chart Understanding Benchmarks. To comprehensively evaluate START, we use CharXiv [59], ChartQA [36], ChartQAPro [40], ChartMimic [64] and CS-Bench proposed by us as benchmarks. The CharXiv-reasoning [59] and ChartQAPro [40] test multi-step reasoning, while the CharXiv-descriptive and ChartQA [36] splits focus on simple perception and reasoning. ChartMimic [64] evaluates chart-to-code translation to assess understanding of the underlying data and visual details. Our CS-Bench evaluates MLLMs chart spatial understanding. We report scores for CharXiv and ChartMimic, accuracy (acc) for ChartQA and ChartQAPro, and recall@0.3 and acc for CS-Bench. Implementation Details. We initiate the Supervised Finetuning (SFT) training with Qwen2.5-VL-3B and 7B model [1] checkpoints, given that they have promising reasoning ability that could be elicited during the RL training [7, 15, 65] and reasonable grounding performance, thanks to its high-quality pretraining. We train the models with SFT on the START-SFT dataset for 1 epoch. We then initialize the Reinforcement Learning (RL) with the SFT checkpoint, and train the model with RL with the STARTRL dataset for 100 steps. For the SFT, we train the model with learning 1e-6 with 0.1 warm-up ratio and cosine learning rate decay. We set the global batch size to 128. 7 For the RL training, we train the model with learning rate of 1e-6, rollout batch size 512, and rollout number of 5. During the training, we set the micro-batch size per device to 4. Baselines. We mainly consider 2 different types of baselines: 1. the MLLM that targets general image understanding: for instance, Qwen2.5-VL-3B and 7B model [1] 2. Chart-Specific MLLM, for instance, TinyChart [70], ChartGemma [39], ChartReasoner [18], ECD [66] and ChartR1 [4]. Noticed that there is gap between the reproduced and the official number for the Qwen models on CharXiv, we follow [66] to present the reproduced number and provide the official number [1] in gray. Results. We present the results in Table 1. START obtains consistent improvement over the base model on both SFT and RL settings. Compared with the Qwen base models, START-RL-3B and START-RL-7B show (12.5/10.1), (7.9/5.3), (5.3/4.8), (16/21.2), (25.1/26) on CharXiv descriptive, CharXiv reasoning, ChartQAPro, ChartMimic, and CS-Bench, respectively. START-RL-7B surpasses the previous best Chart-R1-7B by 14.7, 1.5, 2.1, 42.7 and 35.7 on CharXiv descriptive, CharXiv reasoning, ChartQAPro, ChartMimic and CS-bench, respectively, while lagging behind on ChartQA, as we did not use ChartQA during training and instead demonstrated zero-shot performance. 4.2. The Ablation Study. We conduct an ablation study for learning designs in spatial-textual learning. In this study, we use CharXiv, ChartQAPro, ChartMimic, and CS-Bench as benchmarks to demonstrate the models performance. We use the Qwen2.5-VL 3B model to experiment with the same training details mentioned in section 4.1. Effectiveness of the Spatial and Textual Tasks. We train the model with different task combinations: 1. Chart question answer (CQA) only (Q), CQA + Chart-to-Code (Q+C), and CQA + Chart-to-Code + Chart element grounding (Q+C+G) on SFT and RL, respectively. The experiment results in Table 2 show that adding the Chart-to-Code task improves the textual understanding of the model and obtains consistent improvement on ChartQAPro and ChartMimic, which requires capturing the details on the chart images. Further adding the chart element grounding task improves the models spatial understanding of the chart and obtains substantial improvement on CharXiv and CSBench, in which questions require localizing the chart element correctly. Surprisingly, when adding grounding in the RL setting, we see further improvement in ChartQAPro and ChartMimic, which demonstrate that spatial and textual learning complement each other. Effectiveness of Thinking in Different Tasks. In this section, we investigate the role of thinking in different tasks 8 Thinking CharXiv [59] desc rea CQAPro [40] CMimic [64] on QA only on all tasks 69.7 72.2 38.7 40.0 36.7 38.2 40.2 45.3 CS-Bench R@0.3 acc 39.6 41.3 58.0 60.5 Table 3. Ablation study for applying think-before-answer in training. Applying thinking in all tasks yields better results. during RL training. By default, we apply the think-beforeanswer format to the chart question answering task. We further explore the impact of extending this format to spatial and textual learning. The results, shown in Table 3, demonstrate that incorporating thinking consistently improves performance across benchmarks. Notably, even our grounding benchmark (CS-Bench) benefits from this approach. This finding aligns with prior works [26, 58], suggesting that low-level visual understanding can also be enhanced by thinking, as it provides richer context to the MLLM before producing the final answer. 4.3. Analysis and Visualization. Figure 6 shows the prediction from START and Qwen2.5VL on different tasks. By explicitly learning the chart element grounding, the spatial understanding of the model is enhanced, thus fixing the error caused by the grounding wrong chart subplot in the chart question answering setting (Subplot A). It also enhances the bounding box location prediction (Subplot B). With the chart-to-code learning, we improve the textual understanding of the model, which is reflected by better re-rendered chart image (Subplot C). 5. Conclusion and Discussion In this paper, we present START, spatialtextual learning framework for chart understanding that reflects the dual nature of chartstheir structured visual layout and their underlying textual content. START couples chart-element grounding with chart-to-code to jointly learn spatial structure and textual details on the chart. To support this, we introduce START-Dataset with data-construction pipeline that first transcribes real-world charts into Python plotting code using MLLM, then evolves the code with an LLM to obtain the precise chart-element locations. For evaluating the MLLMs spatial understanding of the Chart, we propose the Chart Spatial understanding Benchmark (CSBench) to support the comprehensive chart understanding evaluation. Benefiting from this spatialtextual supervision, START achieves substantial improvements on benchmarks over strong baselines and surpasses the previous state-ofthe-art by clear margin. We hope our work shed light on the way to achieving deeper chart intelligence. Acknowledgment: We thank Professor Yin Li, Dr. Yuhua Chen, and Pengfei Yu for their helpful discussions and valuable suggestions throughout this project. We also appreciate AWS for providing compute and API resources."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 1, 2, 5, 7, 8, 12, 15 [2] Ritwick Chaudhry, Sumit Shekhar, Utkarsh Gupta, Pranav Maneriker, Prann Bansal, and Ajay Joshi. Leaf-qa: Locate, encode & attend for figure question answering. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 35123521, 2020. 2 [3] Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Liming Zheng, Yufeng Zhong, and Lin Ma. Breaking the sft plateau: Multimodal structured reinforcement learning for chart-tocode generation. arXiv preprint arXiv:2508.13587, 2025. 2 [4] Lei Chen, Xuanle Zhao, Zhixiong Zeng, Jing Huang, Yufeng Zhong, and Lin Ma. Chart-r1: Chain-of-thought supervision and reinforcement for advanced chart reasoner. arXiv preprint arXiv:2507.15509, 2025. 2, 7, 8 [5] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 2 [6] Mathieu Cliche, David Rosenberg, Dhruv Madeka, and Connie Yee. Scatteract: Automated extraction of data from scatter plots. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 135150. Springer, 2017. 2 [7] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. arXiv Video-r1: Reinforcing video reasoning in mllms. preprint arXiv:2503.21776, 2025. 7, 15 [8] Jiyang Gao, Zhenheng Yang, Kan Chen, Chen Sun, and Ram Nevatia. Tall: Temporal activity localization via language. In Proceedings of the IEEE international conference on computer vision, pages 58155823, 2017. [9] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2 [10] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. 2 [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 2 [12] Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, and Xuanjing Huang. Distill visual chart reasoning ability from llms to mllms. arXiv preprint arXiv:2410.18798, 2024. 1, 2, 4 [13] hiyouga. Mathruler. https : / / github . com / hiyouga/MathRuler, 2025. 4 [14] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. 2 [15] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2, 7, 15 [16] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 15 [17] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [18] Caijun Jia, Nan Xu, Jingxuan Wei, Qingli Wang, Lei Wang, Bihui Yu, and Junnan Zhu. Chartreasoner: Code-driven modality bridging for long-chain reasoning in chart question answering. arXiv preprint arXiv:2506.10116, 2025. 2, 3, 7, 8 [19] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. Dvqa: Understanding data visualizations via question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 56485656, 2018. 2, 3 [20] Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar, Adam Trischler, and Yoshua Bengio. Figureqa: An annotated figure dataset for visual reasoning. arXiv preprint arXiv:1710.07300, 2017. 2, 3, 5 [21] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. 3 [22] Shankar Kantharaj, Rixie Tiffany Leong, Xiang Lin, Ahmed Masry, Megh Thakkar, Enamul Hoque, and Shafiq Joty. Chart-to-text: large-scale benchmark for chart summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 40054023, Dublin, Ireland, 2022. Association for Computational Linguistics. 2 [23] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706715, 2017. 6 [24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [25] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1436914387, 2024. 2, 4, 5, 12 [26] Xinhao Li, Ziang Yan, Desen Meng, Lu Dong, Xiangyu Zeng, Yinan He, Yali Wang, Yu Qiao, Yi Wang, and Limin Wang. Videochat-r1: Enhancing spatio-temporal 9 perception via reinforcement fine-tuning. arXiv:2504.06958, 2025. 8 arXiv preprint [27] Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. [28] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740755. Springer, 2014. 6 [29] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understandarXiv preprint ing with large-scale instruction tuning. arXiv:2311.10774, 2023. 3, 7 [30] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [31] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. 2 [33] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 2, 3, 5 [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations. [35] Junyu Luo, Zekun Li, Jinpeng Wang, and Chin-Yew Lin. Chartocr: Data extraction from charts images via deep hyIn Proceedings of the IEEE/CVF winter brid framework. conference on applications of computer vision, pages 1917 1925, 2021. 2 [36] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. 2, 3, 5, 6, 7, 15 [37] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. UniChart: universal visionlanguage pretrained model for chart comprehension and reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14662 14684, Singapore, 2023. Association for Computational Linguistics. [38] Ahmed Masry, Mehrad Shahmohammadi, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. ChartInstruct: Instruction tuning for chart comprehension and reasoning. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1038710409, Bangkok, Thailand, 2024. Association for Computational Linguistics. 2 [39] Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024. 2, 7, [40] Ahmed Masry, Mohammed Saidul Islam, Mahir Ahmed, Aayush Bajaj, Firoz Kabir, Aaryaman Kartha, Md Tahmid Rahman Laskar, Mizanur Rahman, Shadikur Rahman, Mehrad Shahmohammadi, et al. Chartqapro: more diverse and challenging benchmark for chart question answering. arXiv preprint arXiv:2504.05506, 2025. 2, 3, 6, 7, 8, 15 [41] Ahmed Masry, Abhay Puri, Masoud Hashemi, Juan Rodriguez, Megh Thakkar, Khyati Mahajan, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Alexandre Piche, Dzmitry Bahdanau, et al. Bigcharts-r1: Enhanced chart reasonarXiv preprint ing with visual reinforcement finetuning. arXiv:2508.09804, 2025. 2 [42] Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassistant: universal chart multimodal language model via chart-to-table In Findings pre-training and multitask instruction tuning. of the Association for Computational Linguistics ACL 2024, pages 77757803, 2024. 2 [43] Nitesh Methani, Pritha Ganguly, Mitesh Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In Proceedings of the ieee/cvf winter conference on applications of computer vision, pages 15271536, 2020. 2 [44] openai. o3 and o4-mini, 2024. Large language model. 1, 3 [45] Guilherme Penedo, Anton Lozhkov, Hynek Kydlıˇcek, Loubna Ben Allal, Edward Beeching, Agustın Piqueres Lajarın, Quentin Gallouedec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. Codeforces. https : / / huggingface . co / datasets / open - r1/codeforces, 2025. 2 [46] Jorge Poco and Jeffrey Heer. Reverse-engineering visualizations: Recovering visual encodings from chart images. In Computer graphics forum, pages 353363. Wiley Online Library, 2017. 2 [47] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. In The Thirteenth International Conference on Learning Representations. 2, [48] Junha Roh, Karthik Desingh, Ali Farhadi, and Dieter Fox. Languagerefer: Spatial-language model for 3d visual grounding. In Conference on Robot Learning, pages 1046 1056. PMLR, 2022. 3 [49] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning internal representations by error propagation. Technical report, 1985. 2 [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4 10 [63] Zhengzhuo Xu, Bowen Qu, Yiyan Qi, SiNan Du, Chengjin Xu, Chun Yuan, and Jian Guo. Chartmoe: Mixture of diversely aligned expert connector for chart understanding. In The Thirteenth International Conference on Learning Representations. 2 [64] Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran XU, Xinyu Zhu, Siheng Li, Yuxiang Zhang, et al. Chartmimic: Evaluating lmms cross-modal In The reasoning capability via chart-to-code generation. Thirteenth International Conference on Learning Representations. 2, 3, 6, 7, 8, [65] Yi Yang, Xiaoxuan He, Hongkun Pan, Xiyan Jiang, Yan Deng, Xingtao Yang, Haoyu Lu, Dacheng Yin, Fengyun Rao, Minfeng Zhu, et al. R1-onevision: Advancing generalized multimodal reasoning through cross-modal formalization. arXiv preprint arXiv:2503.10615, 2025. 2, 7, 15 [66] Yuwei Yang, Zeyu Zhang, Yunzhong Hou, Zhuowan Li, Gaowen Liu, Ali Payani, Yuan-Sen Ting, and Liang Zheng. Effective training data synthesis for improving mllm chart understanding. arXiv preprint arXiv:2508.06492, 2025. 1, 2, 7, 8 [67] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mPLUG-Owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 2 [68] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In Transactions of the Association for Computational Linguistics, pages 6778. MIT Press, 2014. 6 [69] Ce Zhang, Yan-Bo Lin, Ziyang Wang, Mohit Bansal, and Gedas Bertasius. Silvr: simple language-based video reasoning framework. arXiv preprint arXiv:2505.24869, 2025. 3 [70] Liang Zhang, Anwen Hu, Haiyang Xu, Ming Yan, Yichen Xu, Qin Jin, Ji Zhang, and Fei Huang. TinyChart: Efficient chart understanding with program-of-thoughts learning and visual token merging. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 18821898, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2, 7, 8 [71] Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al. Mathverse: Does your multi-modal In llm truly see the diagrams in visual math problems? European Conference on Computer Vision, pages 169186. Springer, 2024. [72] Luowei Zhou, Chen Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, 2018. 6 [51] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. 3 [52] Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 88028812, 2021. 3 [53] Hrituraj Singh and Sumit Shekhar. STL-CQA: Structurebased transformers with localization and encoding for chart In Proceedings of the 2020 Conferquestion answering. ence on Empirical Methods in Natural Language Processing (EMNLP), pages 32753284, Online, 2020. Association for Computational Linguistics. 2 [54] Wentao Tan, Qiong Cao, Chao Xue, Yibing Zhan, Changxing Ding, and Xiaodong He. Chartmaster: Advancing chart-to-code generation with real-world charts and arXiv preprint chart similarity reinforcement arXiv:2508.17608, 2025. 2 learning. [55] Hemish Veeraboina. Aime problem set 1983-2024, 2023. 2 [56] Alexander Vogel, Omar Moured, Yufan Chen, Jiaming Zhang, and Rainer Stiefelhagen. Refchartqa: Grounding visual answer on chart images through instruction tuning. arXiv preprint arXiv:2503.23131, 2025. 3, 6 [57] Shijie Wang, Qi Zhao, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, and Chen Sun. Vamos: Versatile action In European Conference models for video understanding. on Computer Vision, pages 142160. Springer, 2024. 3 [58] Ye Wang, Ziheng Wang, Boshen Xu, Yang Du, Kejun Lin, Zihan Xiao, Zihao Yue, Jianzhong Ju, Liang Zhang, Dingyi Yang, et al. Time-r1: Post-training large vision language model for temporal video grounding. arXiv preprint arXiv:2503.13377, 2025. 8 [59] Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024. 1, 2, 3, 6, 7, 8, 15 [60] Chengyue Wu, Zhixuan Liang, Yixiao Ge, Qiushan Guo, Zeyu Lu, Jiahao Wang, Ying Shan, and Ping Luo. Plot2Code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 30063028, Albuquerque, New Mexico, 2025. Association for Computational Linguistics. 3 [61] Penghao Wu and Saining Xie. V?: Guided visual search as In Proceedings of core mechanism in multimodal llms. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1308413094, 2024. 3 [62] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. 3, 11 In this supplement, we (1) show additional STARTdataset construction details and visualization in (Section A) (2) present additional details for the Chart Spatial understanding Benchmark (CS-Bench) in (Section B) (3) describe additional implementation details and results in (Section C). We hope that this document will complement our main paper. A. START-Dataset In this section, we provide additional details in the dataset construction pipeline. A.1. Chart-to-code For converting the chart images to Python code during the dataset construction, we explore different approaches: 1. Directly use the multi-modal large language model (MLLM) to convert the chart image to Python code. We tried Qwen2.5-VL [1] and proprietary model. 2. Use the MLLM as chart captioner to convert the chart image to chart description first and then use the Large Language Model (LLM) as the coder to generate Python code based on the chart description. We tried Qwen2.5-VL as the captioner and used an open-source LLM as the coder, in the hope that applying the captioner first before creating the Python code can preserve more chart details. The visualization in Figure 16 shows that directly using proprietary model could produce the most authentic reproduced chart images and preserve most details on the original charts. We share the prompt we use to convert chart image to code with the proprietary model in Figure 7. To construct the chart-to-code dataset Dc, we first use the proprietary model to filter the non-chart images in the ArxivQA [25]. After obtaining the chart images, we prompt the proprietary model to convert the chart image to Python code. We run the Python code and generate the reproduced chart images. We then filter the distorted reproduced chart images by prompting the proprietary model. After we obtain the Python code generated by the proprietary model and the reproduced chart images, we construct the dataset that is used in textual learning during the supervised finetuning (SFT) training and the reinforcement learning (RL). For the SFT, we use the fixed templates to construct the question and answer pair. Please see the templates in figure 7. For the dataset used in RL, we use fixed prompt to ask the model to convert the chart image to the code. We use the code generated by the proprietary model as the Ground Truth and feed it to the grader to calculate the reward for the model prediction. A.2. Location Generation To construct the chart element grounding dataset Dg, we first generate sample evolved code for each chart category. These evolved codes utilize Matplotlibs built-in functions Figure 7. The prompt we use for converting chart to code, and the template we use for preparing the annotations in Dc. Figure 8. Chart element location annotation Dg preparation template. to automatically extract the locations of chart elements from the rendered chart image. The extracted locations are then saved in JSON format. Algorithm 1 provides code snippet illustrating this process. By providing these standardized sample codes as examples during the LLM-driven code evolution process, we both improve the success rate of generating accurate code and ensure uniformity in how chart element locations are stored in JSON file. We use these sample evolved codes as examples to prompt the proprietary model to evolve the Python codes, which we obtained from the chart-to-code process. The prompt we use for code evolution is provided in Figure 12. 12 Algorithm 1 Extracting Chart Elements Location # Get title location title_obj = ax.title if title_obj.get_text(): subplot_data['title'] = get_bbox_pixels_flipped(title_obj, renderer, img_height) # Get x-axis label location and name xlabel_obj = ax.xaxis.label x_axis_name = xlabel_obj.get_text() if xlabel_obj.get_text() else 'x-axis' if xlabel_obj.get_text(): subplot_data['x_axis_names'].append( get_bbox_pixels_flipped(xlabel_obj, renderer, img_height)) # Get y-axis label location and name ylabel_obj = ax.yaxis.label y_axis_name = ylabel_obj.get_text() if ylabel_obj.get_text() else 'y-axis' if ylabel_obj.get_text(): subplot_data['y_axis_names'].append( get_bbox_pixels_flipped(ylabel_obj, renderer, img_height)) # Get x-axis tick locations - filter for valid ticks x_ticks = [] valid_ticks = [str(decade) for decade in decades] for tick in ax.get_xticklabels(): if tick.get_visible() and tick.get_text ().strip() in valid_ticks: x_ticks.append( get_bbox_pixels_flipped(tick, renderer, img_height)) if x_ticks: subplot_data['x_axis_ticks'][ x_axis_name] = x_ticks We then execute the evolved code to obtain the locations of the chart elements. Next, we uniformly sample these locations for each type of chart element. Finally, we convert the sampled locations into grounding annotations by applying fixed templates. Please see the template details in Figure 8. A.3. QA generation For generating the QA pair for the rendered chart images, we try two different ways. 1. Use MLLM with chart image and the corresponding Python code to generate the question and answer pair. 2. Use an LLM, which takes the Python code as input to generate the question and answer pair. The MLLM approach leverages both visual and 13 code inputs, enabling it to generate questions grounded in spatial properties of the chart (e.g., the spatial arrangement of scatter points). In contrast, the LLM approach focuses solely on the Python code, making it well-suited for capturing fine-grained chart details, such as the exact number of points plotted or specific data values. Figure 9 shows visualization of sample questions generated by both MLLM and LLM. In our experiments, we found that MLLM consistently produced higher-quality QA pairs. We thus use MLLM to generate our question-answer pairs. We curated set of high-quality examples and used them as part of few-shot prompt to guide MLLM in generating ten QA pairs for each chart image. The full prompt used for this generation process is provided in Figure 13. To ensure quality, we incorporate verification step to detect and remove unreasonable questions or incorrect answers. Specifically, we prompt strong MLLM to assess whether each question is groundable (i.e., tied to elements visible in the chart) and answerable (i.e., solvable by MLLM). We filter out hallucinated questions that reference non-existent elements or those beyond the MLLMs In addition, capacity (e.g., precisely counting 200 dots). the MLLM verifies the correctness of each answer. Based on these verdicts, we filter the QA pairs to obtain the final dataset for the chart question answering task, denoted as Dq. We present some verdict samples given by the strong MLLM for the QA pair generated by MLLM in Figure 10. A.4. Curate the SFT and RL data splits We combine the data from chart question answering, spatial learning, and textual learning as the Supervised-Finetuning Dataset (START-Dataset-SFT), and we sample the Reinforcement Learning Dataset (START-Dataset-RL) from the START-Dataset-SFT based on the question difficulties. To determine the difficulty of the training samples, We run the Qwen2.5-VL on the training set and regard the probability of the model can give the correct answer to question as difficulty. We use the difficulty as weight for sampling and get the START-Dataset-RL. B. CS-Bench In this section, we provide more details on the construction pipeline for Chart Spatial understanding Benchmark (CSBench). Images. The benchmark consists of 613 images rendered from holdout subset of code evolved during the STARTDataset construction. Chart Element Locations. Given CS-Bench is built based on the evolved Python code, the JSON file that stores the location of the chart elements will be generated when the Python code is run. The Question Answer pairs. Our benchmark features two primary types of inquiries: grounding questions and Figure 9. The QA samples generated by the MLLM and the LLM. We highlight the questions that are improperly hard or the incorrect answers in red. Figure 10. The question-answer pairs and the corresponding verdicts. We highlight the questions that are improperly in red and the corresponding verdicts in green. Figure 11. learning curve of START-RL-7B, trained with chart question answering, chart element grounding, and the chart-to-code task. QA grounding questions. grounding question directly prompts model to find the location of specific element within chart. In contrast, QA grounding question presents two-part task: it first asks question related to the charts content and then requires the model to identify the location of the chart element(s) referenced in the answer or in the question. CS-Bench has 350 grounding questions and 342 QA grounding questions. For the grounding question, we following the same pipeline used in generating the chart element location dataset Dg in section A.2 to generate the question answer pairs. For the QA grounding question, we prompt MLLM to generate the question related to the chart image, and then we pick the location mentioned in the question or the answer. The question and the answer are manually verified. Sample chart images and the questions could be found in Figure 15. 14 The Evaluation Metrics. Considering the Ground Truth location has lot of small items, for instance, ticks values or axis names, we use recall at IoU 0.3 (recall@0.3) as the main metric of this benchmark. We report the recall@0.3 for 692 ground truth bounding boxes. We also present the accuracy of answer to the 342 QA grounding questions as an auxiliary metric. C. START Experiments C.1. Benchmarks. We use CharXiv [59] validation split, ChartQA [36] test split, ChartQAPro [40], ChartMimic [64] and CS-Bench proposed by us as benchmarks. Specifically, CharXiv has 1000 chart images which come from arXiv papers, and it splits the evaluation set into 4k descriptive questions and 1k reasoning questions. While the descriptive questions focus on the general chart elements, for instance, title, legend, and tick values on the axis, the reasoning questions focus on the trend or conclusion reflected from the chart. It use GPT4o [16] as judge and adapt accuracy as metric. ChartQA contains 1509 images and 2500 questions, which mostly focus on line, bar, and pie charts. It uses accuracy as metric. ChartQAPro consists of 1341 chart images, which come from 157 diverse online platforms and are paired with 1948 questions, which are divided into factoid, multiple-choice, conversational, hypothetical, and fact-checking. It uses accuracy as metric. ChartMimic contains 600 humancurated (figure, instruction, code) triplets and evaluates the models ability to convert chart image to code. It use GPT4o as the judge and adapts the score as metric. CS-Bench evaluates the spatial understanding of the MLLM toward the chart and uses recall at IoU 0.3 (recall@0.3) of the GT bounding box and accuracy of the answer to the question as metrics. We regard recall@0.3 as the main metric for CS-Bench. C.2. Implementation details. Training details in Supervised Finetuning (SFT). Initialization. We initiate the Supervised Finetuning (SFT) training with Qwen2.5-VL-3B and 7B model [1] checkpoints, given that they have promising reasoning ability that could be elicited during the RL training [7, 15, 65] and reasonable grounding performance, thanks to its highquality pretraining. Training Hyper-parameters. We train the model with learning 1e-6 with 0.1 warm-up ratio and cosine learning rate decay. We set the global batch size to 128. We train the models with SFT on the START-SFT dataset for 1 epoch. Data. For different SFT settings, we mix different portions of the START-SFT as the training dataset. For instance, if we use SFT to train the model with chart question answering and chart element grounding, we mix these two 15 portions of the data in the START-SFT as training data to train the model. Training details in Reinforcement Learning (RL). Initialization. To start the RL training, we initialize the model with the corresponding SFT checkpoint. For instance, when we conduct the RL training with chart question answering and chart element grounding, we use the SFT checkpoint, which is also trained with the chart question answering and chart element grounding tasks to initialize the model. Training Hyper-parameters. We train the model for 100 steps with learning rate of 1e-6, rollout batch size 512, and rollout number of 5. During the training, we set the microbatch size per device to 4. We train all the models with 8 A100 GPUs. Data. Similar to the setting in the SFT, we mix different portions of the START-RL dataset as training dataset. For instance, if we train the model with RL with chart QA and chart element grounding, we will use the question from the chart question answering and chart element grounding split of START-RL to prompt the model rollout the answer during the training. We use the ground truth answer or the bounding box as the reference to calculate the reward for the response. Reward design and reward calculation. For the reward calculation, we consider 2 different types of rewards, the formatting reward and the accuracy reward. For the formatting reward, we mainly use regular expression to judge whether the models answer fits into required format. The value of the reward is either 0 or 1. The details is included in Figure 2 of the main paper. For the chart element grounding task, we use the IoU value between the predicted bounding box and the ground truth bounding box as the reward, the value will be float value falls between 0 to 1. For the chartto-code task, we use an LLM to judge the predicted code from different perspectives with the Ground Truth code as reference. The prompt we use is shown in Figure 14. We judge the code from five different perspectives, which include data, plot type structure, axes scales and limits, text elements, and styling. Each element will be graded with score of 0 to 5. We then sum up the score from different perspectives and normalize the score between 0 to 1 as the reward. The training curve. Figure 11 shows the learning curve of START-RL-7B, trained with chart question answering, chart element grounding, and the chart-to-code task. It shows the format and accuracy reward for chart question answering (think format, accuracy), the chart element grounding (grounding format reward, iou), and chart-tocode (code format, code accuracy score). It also includes the overall format and overall reward (format, and overall). Figure 12. Prompt for code evolution. 16 Figure 13. Prompt for QA generation. 17 Figure 14. Prompt for code generation during the RL training. 18 Figure 15. The visualization of the samples in the Chart Spatial understanding Benchmark (CS-Bench). We visualize the bounding box region in red. 19 Figure 16. Visualization of reproduced charts using different chart-to-code methods."
        }
    ],
    "affiliations": [
        "Amazon AGI",
        "MIT",
        "University of Wisconsin-Madison"
    ]
}