{
    "paper_title": "Position: Agentic Evolution is the Path to Evolving LLMs",
    "authors": [
        "Minhua Lin",
        "Hanqing Lu",
        "Zhan Shi",
        "Bing He",
        "Rui Mao",
        "Zhiwei Zhang",
        "Zongyu Wu",
        "Xianfeng Tang",
        "Hui Liu",
        "Zhenwei Dai",
        "Xiang Zhang",
        "Suhang Wang",
        "Benoit Dumoulin",
        "Jian Pei"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world."
        },
        {
            "title": "Start",
            "content": "Position: Agentic Evolution is the Path to Evolving LLMs Minhua Lin 1 Hanqing Lu 2 Zhan Shi 2 Bing He 2 Rui Mao 2 Zhiwei Zhang 1 Zongyu Wu 1 Xianfeng Tang 2 Hui Liu 2 Zhenwei Dai 2 Xiang Zhang 1 Suhang Wang 1 Benoit Dumoulin 2 Jian Pei 3 6 2 0 2 0 3 ] A . [ 1 9 5 3 0 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling trainingtime and inference-time compute improves static capability but does not close this traindeploy gap. We argue that addressing this limitation requires new scaling axisevolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from fixed pipeline to an autonomous evolver agent. We instantiate this vision in general framework, A-EVOLVE, which treats deployment-time improvement as deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as scalable path toward sustained, open-ended adaptation in the real world. 1. Introduction Large Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Touvron et al., 2023) have achieved remarkable progress by scaling along two primary axes: increasing training-time compute (spanning pre-training and post-training) (Kaplan et al., 2020; Lai et al., 2025) and scaling inference-time compute via reasoning chains (Wei et al., 2022; Snell et al., 2025). However, real-world applications of LLMs in open-ended environments still face fundamental challenge: the train-deploy environment gap (Gama 1The Pennsylvania State Univerity 2Amazon 3Duke University. Correspondence to: Hanqing Lu <hanqinglucs@gmail.com>. Preprint. February 3, 2026. 1 et al., 2014; Hu et al., 2025b). Once deployed, models trained with finite training data cannot exhaustively anticipate the infinite variety of real-world cases, shifting APIs, and evolving constraints. Therefore, purely static models inevitably degrade or fail under prolonged deployment. This motivates the need for new scaling axis: evolution the deployment-time ability of model to autonomously improve its capabilities during interaction, without manual intervention. We define evolution as continual learning for LLM systems during deployment. An LLM systems behavior is governed by composite policy π = (πθ, πS), where πθ denotes the parametric backbone (e.g., LLM weights) and πS is non-parametric persistent artifact state, such as tools (Xia et al., 2025), code (Jiang et al., 2023), memories (Chhikara et al., 2025), and structured knowledge (Han et al., 2024). Evolution corresponds to cross-episode policy improvement driven by accumulated experience: (πt+1 θ , πt+1 ) FEvolve(πt θ, πt S, Obs[1 : t]), (1) where Obs[1 : t] is deployment observations (e.g., interaction traces, environment feedback, rewards), and FEvolve is the update mechanism. In this view, evolution is not oneshot training procedure, but an ongoing process that converts interaction evidence into lasting behavioral improvement. Evolution is essential for LLM systems for at least three important reasons. First, the train-deploy gap implies that static optimization over fixed distributions is fundamentally insufficient; adaptation must occur in situ. Second, many deployment settings impose privacy and governance constraints: user feedback, proprietary data, or sensitive interactions cannot be centrally logged for global retraining. Evolution enables local, governed improvement through persistent artifacts without leaking private data. Third, test-time compute alone does not scale. While additional reasoning can solve novel instances, it is wasteful for recurrent failures. When deployed LLM system repeatedly lacks capability, such as parsing new file format or handling brittle API, thinking longer each time is inferior to learning once and reusing the solution. Instead, evolution amortizes expensive reasoning into cheap, persistent capability. While several pioneering approaches to LLM evolution have emerged, they remain fundamentally limited in achieving Position: Agentic Evolution is the Path to Evolving LLMs production logs. At first, everything works smoothlyuntil the environment changes. logging update renames field, introduces nested JSON structure, or slightly alters an interface that the agent quietly depended on. The next day, the agent starts failing. Existing evolution strategies respond in blunt ways. Parametric approaches attempt retraining, incurring high costs and risking catastrophic forgetting (Luo et al., 2025). Non-parametric heuristic methods instead record the failure as text, hoping the agent will re-discover the correct logic, but often face context saturation and inconsistent retrieval. In contrast, agentic evolution takes different view. The evolver treats failures as diagnostic signals, identifies what needs to change, and uses validation gate to verify the fix (e.g., regression-tested parser), ensuring the update is persistent and safe. This shift from reactive patching to deliberate, goal-directed evolution captures the essence of agentic evolution and motivates the three principles we introduce next. Agentic evolution is characterized by three core principles. First, the goal-oriented principle specifies what to change: the evolver actively diagnoses deployment failures, attributes them to underlying causes, and targets persistent artifacts whose modification is expected to yield durable performance gains. Second, the autonomy principle specifies when to change: rather than following fixed update schedule, the evolver governs the adaptation process by selecting relevant evidence, triggering evolution only when warranted, and explicitly deciding whether to commit or reject candidate updates. Finally, the compositional principle specifies how to change: improvements are realized through structured, modular artifactssuch as tools, workflows, and validation testsproduced by decomposed decision processes and integrated only after verification, while allowing periodic updates to non-parametric components when appropriate. These principles elevate evolution from static heuristic pipeline to deliberate, governed, and scalable decision-making process. More broadly, agentic evolution reframes deployment-time adaptation as the joint optimization of persistent state and model parameters, rather than relying solely on the direct tuning of weights or the passive accumulation of memory. Under this view, the update rule itself becomes budgeted optimizer: an explicit evolver agent that decides what to change, when to change, and how to change under finite evolution-time compute budget. This leads to our central vision: Agentic evolution represents scalable path for the sustained, open-ended evolution of LLM systems over indefinite deployment horizons. Unlike static heuristics that inevitably plateau, sufficiently capable evolver can continue to extract increasingly complex improvements from accumulated experience. Continuous, open-ended evolution demands both sustainability and scalability. This motivates the evolution-scaling hypothesis: the capacity Figure 1. From train-deploy gap to agentic evolution: An overview. this goal. Parametric evolution methods (Huang et al., 2025; Hu et al., 2025a), such as test-time training or online fine-tuning, update πθ using recent observations. While expressive in principle, these updates are opaque and difficult to govern, and they risk catastrophic forgetting under distribution shift (Luo et al., 2025). Non-parametric heuristic evolution methods (Gao et al., 2025) instead modify πS by appending textual memories (Wang et al., 2024) or optimizing prompts using fixed rules (Zhou et al., 2022). Although lightweight, these approaches treat evolution as storage or search problem: as experience accumulates, memory saturates with noisy, unverified text, and improvements exhibit diminishing returns. In both cases, the update rule FEvolve is static and heuristic, rather than adaptive and goal-directed. These limitations are not incidental, but stem from common root cause: the absence of agentic capability in the evolution process itself. In existing approaches, FEvolve is either static optimization rule (e.g., gradient updates (Xia et al., 2025)) or fixed heuristic (e.g., append-and-retrieve (Shinn et al., 2023)). Such mechanisms lack the ability to reason about failures, decide which aspects of the system should change, and adapt their own update strategy as deployment conditions evolve. As result, parametric methods blindly modify weights without semantic accountability, while heuristic non-parametric methods indiscriminately accumulate experience without understanding relevance, causality, or long-term utility. Both fail precisely because evolution is treated as mechanical procedure rather than an agentic decision-making problem. We therefore propose our position in this paper: LLM evolution has to be agentic. The novel paradigm of agentic evolution is shown in Fig. 1. The central idea is to elevate FEvolve from fixed, heuristic-driven workflow to an explicit evolver agenta goal-directed optimizer that diagnoses what to change, autonomously governs when to change, and collaboratively synthesizes composable updates to maintain and improve both the parametric backbone πθ and the persistent artifact state πS. Consider cloud platform agent deployed to compute routine metrics, such as p95 latency per endpoint, from raw 2 Position: Agentic Evolution is the Path to Evolving LLMs for adaptationthe achievable performance frontier of evolutionscales with the compute allocated to the evolution process. We identify this as third scaling axis, complementary to training-time and inference-time computation. Ultimately, adaptive intelligence in LLM systems depends not on chance, but on systematically scaling the ability to learn how to improve. 2. Agentic Evolution and Principles We consider deployed LLM system that repeatedly interacts with an environment over sequence of episodes. At episode t, the system observes an input or state xt, produces actions at, and receives feedback ot from the environment. We model the system as composite policy πt = (πθ,t, πS,t), (2) where πθ,t denotes the parametric backbone (e.g., an LLM model with parameters θ), and πS,t denotes persistent, editable artifact state that conditions behavior across episodes, such as tools, skills, workflows, structured knowledge, and validation assets. Crucially, πt persists beyond single interaction and constitutes the systems deployment-time interface to capability. Evolution refers to cross-episode improvement that occurs during deployment. Rather than treating deployment as static inference phase, evolution formalizes the process by which system converts accumulated interaction evidence into lasting behavioral change. Let Obs1:t denote the deployment evidence collected up to episode t, including trajectories, tool traces, errors, and feedback. Evolution is defined as cross-episode update process: (πθ,t+1, πS,t+1) FEvolve(πθ,t, πS,t, Obs1:t), (3) where FEvolve is an update mechanism that may modify the parametric model πθ, the persistent state πS, or both. This definition abstracts over broad spectrum of approaches, ranging from offline training and online fine-tuning to heuristic memory updates. Detailed taxonomy is in Appendix A. Agentic evolution instantiates FEvolve as an explicit evolver agent: goal-directed, autonomous optimizer that reasons over accumulated deployment evidence and produces persistent, governed updates. Rather than executing fixed script, the evolver treats evolution itself as decision-making problem. At each episode t, it proposes structured candidate update t, such as add, patch, refactor, or prune operations over πS and/or πθ, and makes an explicit commit decision: FEvolve(πθ,t, πS,t, Obs1:t), ct C(πt, t, Obs1:t), (4) followed by (cid:40) πt+1 Apply(πt, t) πt if ct = 1, if ct = 0, (5) where ct {0, 1} is the commit decision from the governance gate C. This gate is typically instantiated via automated verification (e.g., unit tests, regression checks) or human-in-the-loop review. The defining characteristic of agentic evolution is that updates are treated as conditional decisionsexplicitly proposed, evaluated, and either committed or rejectedrather than as unconditional steps in fixed workflow. Agentic evolution is governed by three core principles that specify what to evolve, when to evolve, and how evolution is carried out. The goal-oriented principle specifies what to change. The evolver does not blindly accumulate experience or optimize generic proxies; instead, it explicitly diagnoses deployment failures, attributes them to actionable causes, and targets specific components of the persistent state whose modification is expected to improve future performance. This shifts adaptation from correlation-based updates to causal, capability-level repair. In practice, goal orientation is realized by localizing failures to missing tools, brittle logic, interface mismatches, or incomplete workflows, and then applying targeted edits to the corresponding artifacts in πS (and, when appropriate, to πθ). By making the adaptation objective explicit and forward-looking, the system converts raw experience into durable improvements rather than transient fixes. The autonomy principle specifies when to change. Instead of following pre-defined update schedule (e.g., evolve after every failure), the evolver controls the update decision process itself. It selects which evidence is relevant, determines whether failure is actionable or merely transient noise, and explicitly decides whether to commit an update or perform no-op. This decision-theoretic control allows evolution compute to be allocated only when improvement is feasible and worthwhile. Autonomy distinguishes agentic evolution from non-agentic pipelines in which updates are implicitly triggered or hard-coded, and it is essential for stable long-horizon deployment in open-ended environments. The compositional principle specifies how to evolve. Internally, the evolver is naturally decomposed into cooperating decision functionssuch as diagnosis, planning, updating, and verificationthat operate over shared evidence and state. This decomposition improves reliability and expressivity, whether realized within single system or as multi-agent workflow. Externally, evolution produces modular, structured artifactsfor example, executable tools, reusable workflows, and validation testsrather than unstructured text. This compositionality enables amortization: recurring reasoning and fragile deliberation are compiled into reusable capabilities, allowing the system to bypass context saturation and avoid the diminishing returns typical of heuristic memory accumulation. Position: Agentic Evolution is the Path to Evolving LLMs A-Evolve is built around three commitments. First, evolution is goal-oriented: from deployment evidence, the system derives an explicit update objective and proposes targeted edits over structured space. Second, evolution is autonomous: updates are conditional, driven by explicit evidence selection and commit/no-op decision c, rather than fixed schedule. Third, evolution is compositional: the update rule FEvolve is realized as modular evolver that produces typed artifacts and commits them only through explicit acceptance mechanisms such as validation or review. We instantiate these commitments through three structural components: persistent artifact state, solveevolve control loop, and an explicit evolver. Persistent Artifact State πS. A-Evolve elevates the nonparametric component πS to first-class, editable interface to system capability. Instead of accumulating transient context, deployment experience is compiled into persistent artifacts that can be created, revised, validated, and reused across episodes. This exposes concrete edit space for evolution, allowing targeted updates to specific components rather than diffuse prompt modification. At episode t, the artifact state is πS,t = {Kt, Tt, Vt}. (6) The knowledge registry Kt stores structured or textual artifacts such as schemas, workflows, interface contracts, and exemplars. Artifacts are addressable and versioned, enabling retrieval, patching, and replacement. This supports goal-oriented evolution by allowing failures to be localized to specific knowledge components. The tool registry Tt contains executable functions, including scripts and API wrappers, with explicit inputoutput signatures and associated tests. During solve-time, tools provide deterministic action primitives that reduce inference variance. During evolve-time, they serve as diagnostic instruments for replaying failures, probing edge cases, and extracting structured error signals. The validation registry Vt contains governance assets such as unit tests, regression suites, and optional human review hooks. Validation artifacts are themselves editable. Crucially, Vt grounds the commit decision ct: updates are committed only if they pass verification, preventing regressions and uncontrolled drift over long deployment horizons. The SolveEvolve Loop. A-Evolve explicitly separates instance-level task execution from cross-episode capability improvement, ensuring that evolution is governed process with its own control flow and compute budget. For episode with input xt, the solve phase produces trajectory τt = Solve(πt, xt), (7) Figure 2. Framework of A-Evolve. These principles elevate evolution from static, heuristicdriven procedure to governed and scalable decisionmaking process. By treating deployment-time improvement as an agentic optimization problem over persistent state, agentic evolution provides foundation for LLM systems that can reliably adapt to open-ended environments while maintaining stability, interpretability, and scalability. To illustrate, let us revisit the cloud-log agent scenario from Sec. 1. Let πt = (πθ,t, πS,t) denote the composite policy. Under the schema drift scenario (e.g., nested JSON changes), parametric evolution attempts to update the backbone πθ via fine-tuning, entangling localized interface mismatch with global model behavior and risking catastrophic forgetting. Heuristic methods instead append raw traces to πS, forcing the solver to repeatedly regenerate brittle logic at inference time with limited reuse or guarantees. Agentic evolution reframes such failures as cross-episode optimization problem. Deployment trajectories are aggregated into structured evidence, from which an explicit evolver diagnoses the underlying cause, such as an outdated schema assumption, and formulates targeted update objective. Rather than modifying weights or appending raw memories, the evolver proposes precise, structured edits to πS, such as synthesizing versioned adapter function to permanently handle the new nested format and rectifying the API schema definition. Candidate updates are evaluated and committed conditionally through explicit verification, ensuring that only validated improvements persist. In this way, agentic evolution amortizes repeated inference-time reasoning into durable, governed capability, transforming recurrent failures into stable system assets. 3. A-Evolve: General Framework for Agentic Evolution of LLM Systems A-Evolve is general, implementation-agnostic framework that instantiates the principles of agentic evolution introduced in Sec. 2. As illustrated in Fig. 2, A-Evolve makes deployment-time evolution explicit, governed, and amortizable, enabling scalable capability improvement beyond static inference. 4 Position: Agentic Evolution is the Path to Evolving LLMs which is appended to the cumulative evidence buffer Obs1:t = Obs1:t1 {τt}. The evolve phase then produces conditional update πt+1 = πt (ct t), (8) where and ct are defined below. In the solve phase, the system executes under the composite policy πt = (πθ,t, πS,t), retrieving artifacts from Kt and invoking tools from Tt as needed. The artifact state πS,t is treated as read-only to ensure reproducibility. In the evolve phase, triggered asynchronously or when sufficient evidence accumulates, the evolver analyzes Obs1:t and proposes structured update together with commit decision ct. The evolver may invoke tools from Tt to reproduce failures or analyze traces. Candidate updates are evaluated against Vt; only validated updates are committed. This separation ensures that solve-time compute targets task completion, while evolve-time compute targets amortized improvement across episodes. The Evolver FEvolve. A-Evolve implements FEvolve as an explicit evolver composed of four cooperating functions: gt Diagnose(Obs1:t, πS,t), Update(pt, πS,t), pt Plan(gt, πS,t), ct Verify(t, Vt). (9) Diagnose identifies actionable failure modes and their causes (e.g., missing schemas or brittle tools), optionally invoking Tt to extract structured error signatures, and produces an update objective gt. Plan translates gt into an explicit edit plan specifying target artifacts, edit operators (add, patch, refactor, prune), and ordering constraints. Update executes the plan by synthesizing concrete artifact changes, assembling candidate update with provenance and tests. Verify evaluates against Vt and returns commit decision ct {0, 1}, allowing the evolver to reject harmful or brittle updates. Edit Operators and Auditing. A-Evolve supports small set of canonical edit operators over πS, including adding or patching tools, schemas, and tests, and pruning obsolete artifacts. For example, under schema drift, the evolver may add new schema, patch parser tool, and introduce regression tests, committing the update only if all checks pass. All proposed updates are logged with full provenance, enabling auditing, rollback, and risk-based human oversight. Summary. A-Evolve provides concrete framework for agentic evolution. By exposing typed persistent artifact state, separating solve-time execution from evolve-time optimization, and enforcing modular, verifiable updates, AEvolve enables LLM systems to convert deployment experience into durable, governed capability improvements without sacrificing stability or interpretability. 5 4. The Evolution-Scaling Hypothesis central motivation for agentic evolution is to enable autonomy: the ability of an LLM system to adapt reliably in open-ended, non-stationary environments where increased training-time compute (Kaplan et al., 2020; Lai et al., 2025) and increased test-time compute (Schaeffer et al., 2025) inevitably fall short. While existing evolution-style methods (Cai et al., 2025; Ouyang et al., 2025) demonstrate that deployment-time adaptation is possible, they are typically driven by ad-hoc heuristics and fixed update rules. As result, their effectiveness is often fragile, difficult to predict, and prone to early saturation. This leads to fundamental question for long-horizon deployment and, more broadly, for the path toward AGI: Can evolution itself be made scalable and sustainable? Specifically, if we allocate more resources to the evolution processsuch as stronger evolvers, more analysis steps, or larger evolution-time compute budgetsdoes the effectiveness and speed of adaptation increase in predictable and systematic way, or is improvement largely bounded by chance and problem-specific heuristics? The Evolution-Scaling Hypothesis. We propose the Evolution-Scaling Hypothesis, which frames deploymenttime adaptation not as stochastic or opportunistic phenomenon, but as scalable optimization process governed by resources. Analogous to established scaling laws for pretraining and inference, the hypothesis posits that allocating additional compute to the evolution looprather than only to solving individual taskssystematically improves the attainable level of adaptation. Concretely, we argue that evolution-time compute enables an LLM system to (i) diagnose failures more accurately, (ii) consider more candidate updates, (iii) synthesize more robust artifacts, and (iv) apply stronger verification before committing changes. These capabilities compound over episodes because validated updates persist. As result, evolution is not merely collection of local fixes, but convergent process whose effectiveness scales with the resources devoted to it. This perspective shifts the paradigm from hoping that heuristics generalize to engineering adaptation through compute. Evolution-Scaling Formalism. To make this notion precise, we define compute-optimal view of evolution. Let (π) denote the performance of deployed policy π under fixed environment and fixed solve-time compute budget. Given an initial policy π0, we define the compute-optimal evolution frontier as (Cevolve, π0) max Fevolve EπFevolve(π0) (cid:2)P (π)(cid:3), (10) Position: Agentic Evolution is the Path to Evolving LLMs where Fevolve ranges over all evolution strategies whose total evolution-time compute cost does not exceed Cevolve. Here, Cevolve captures the resources allocated to the evolution process, including analysis steps, tool invocations, candidate synthesis, and verification, while implicitly reflecting the capability of the evolver itself. The Evolution-Scaling Hypothesis states that, holding the environment and solve-time compute fixed, the frontier is strictly increasing with respect to evolution-time compute. Formally, for any (1) evolve, we hypothesize: evolve < (2) (C (1) evolve, π0) < (C (2) evolve, π0). (11) Intuitively, allocating more evolution-time compute enlarges the space of feasible update strategies, enabling deeper diagnosis, more reliable edits, and stronger governance, and thereby raising the achievable performance ceiling. Interpretation. This hypothesis reframes deploymenttime learning as predictable scaling regime. Rather than viewing adaptation as series of isolated, heuristic-driven repairs, evolution is treated as an optimization process whose convergence rate and asymptotic performance depend on the strength and compute budget of the evolver. In this view, insufficient evolution resources lead to noisy, brittle improvements, while sufficient resources guarantee systematic progress toward the compute-optimal frontier. Strategic Implications. The evolution-scaling perspective yields two complementary research directions: Approaching the frontier. For fixed evolution-time budget, the goal is to design evolution algorithms Fevolve that efficiently approach . Agentic evolution advances this direction by replacing fixed heuristics with autonomous, structured decision-making, as empirically validated in Sec. 5.2. Raising the frontier. Beyond algorithmic efficiency, the hypothesis predicts that aggressively scaling evolution-time resources, such as evolver capability and analysis depth, systematically raises the performance ceiling itself. In longhorizon deployment, reallocating compute from repeatedly thinking harder at solve time to evolving better across episodes converts raw compute into durable capability, enabling mastery of increasingly complex and shifting environments. We will empirically observe the evolution scaling effect in Sec. 5.4. 5. Empirical Studies In this section, we empirically evaluate the feasibility and advantages of agentic evolution by addressing three questions: (i) Effectiveness: does agentic evolution improve task performance under compute-matched conditions? (ii) Feasibility: how do the core components of agentic evolution contribute to performance gains and loop reliability? (iii) Evolution-scaling: does evolution capacity increase with evolution-time compute and evolver capability? 5.1. Experimental Setup Datasets. We evaluate on AppWorld (Trivedi et al., 2024), widely used benchmark for tool-using agents with executable environments and unit tests for goal verification. We sample 50 tasks from the training split for evolution and 50 tasks from the test-normal split for evaluation. Dataset details and the sampling protocol are in Appendix B.1. Implementation Details. We implement A-Evolve as two-level system consisting of solver, which executes tasks, and an evolver, which accumulates persistent improvements across episodes. The evolver decomposes the evolve step into four roles: diagnoser, planner, updater, and verifier, which cooperate through shared evolution state. Unless otherwise specified, we use Claude Sonnet 4.5 (Anthropic, 2025b) as the evolver backbone, and evaluate solvers ranging from Claude Haiku 4.5 (Anthropic, 2025a) and Claude Sonnet 4/4.5 to GPT-5 (Singh et al., 2025) and Gemini 3 Flash (Google, 2025). Additional details are in Appendix B.2. Evaluation Protocol. Following prior work (Trivedi et al., 2024; Cao et al., 2025), we report two metrics: Task Goal Completion (TGC), the fraction of tasks successfully completed, and Average Passed Tests (APT), the average fraction of unit tests passed per task. To ensure fair comparison, we fix solve-time compute budget Csolve (maximum tool calls, steps, or tokens per task) and an evolve-time compute budget Cevolve (maximum tokens and tool invocations per episode) across all methods. Full evaluation details are given in Appendix B.3. 5.2. Effectiveness: Agentic vs. Non-agentic Evolution Setting. We compare A-Evolve against non-evolving reference and two representative non-agentic evolution baselines and: (i) APE (Zhou et al., 2022), search-based prompt evolution method that proposes candidate instructions and selects among them via task-level scoring; (ii) AWM (Wang et al., 2024), an experience-based workflow memory baseline that induces reusable workflows from past trajectories; and (iii) Vanilla, which directly queries the solver without persistent updates. All evolution methods learn updates on the training set and are evaluated on the test set. Results. Tab. 1 reports TGC and APT across methods. We observe two consistent trends. (i) Agentic evolution acts as capability multiplier. A-Evolve yields substantial gains across all solver backbones. For example, it achieves 64% and 82% TGC on Claude Haiku 4.5 and Gemini 3 Flash, respectively, compared to 46%/52% for AWM and 32%/56% Position: Agentic Evolution is the Path to Evolving LLMs Table 1. Comparison of agentic evolution against baselines on AppWorld. We report TGC and APT (%) across solver backbones. Claude Haiku 4.5 Claude Sonnet 4.5 Claude Sonnet 4 GPT Gemini 3 Flash Method TGC Vanilla APE AWM A-Evolve 32 30 46 64 APT 51.16 56.00 65.76 84.31 TGC 86 80 88 90 APT 94.94 85.00 97.32 96.47 TGC 42 44 50 56 APT 79.42 82.00 86.29 88.21 TGC APT TGC 80 82 84 88 94.15 95.00 94.08 97.02 56 52 52 82 APT 80.45 84.00 87.75 92.05 for Vanilla. This demonstrates that agentic evolution reliably converts deployment evidence into reusable improvements that static prompt or workflow updates fail to capture. (ii) Narrowing the capacity gap. Smaller solvers augmented with A-Evolve can match or exceed larger vanilla models (e.g., Haiku 4.5 with A-Evolve reaches 64% TGC versus 42% for vanilla Sonnet 4), indicating that persistent procedural refinement across episodes can rival gains from stronger backbones alone. Case Studies. Qualitative analysis of evolution traces reveals why these gains arise. Non-agentic baselines such as AWM tend to capture only surface-level patterns, retrieving raw trajectories that often include irrelevant context or hallucinated reasoning. In contrast, A-Evolve performs active diagnosis: the evolver invokes analysis tools to identify underlying failure causes (e.g., hidden API dependencies). These insights are synthesized into persistent artifacts in πS, such as verified tools (T ) or refined knowledge schemas (K), rather than stored as raw text. All updates are gated by the validation registry (V), ensuring that only improvements passing regression checks are committed. At solve-time, this replaces fragile, probabilistic regeneration with robust, governed capability invocation. Detailed examples are provided in Appendix C.1. 5.3. Feasibility: Reliability of Agentic Evolution Loop Setting. To isolate the contribution of each component in agentic evolution, we compare A-Evolve against four ablated variants: (i) A-Evolve/D, which removes diagnosis and proposes updates directly from raw trajectories; (ii) AEvolve/A, which retains diagnosis but disables analysis tools for aggregating evidence across episodes; (iii) A-Evolve/P, which removes planning and generates updates in single step without explicit edit structure; and (iv) A-Evolve/V, which removes verification and commits updates without validation gating. Claude Sonnet 4.5 is fixed as the evolver, with Claude Haiku 4.5 and Gemini 3 Flash as solvers. Other settings follow Sec. 5.2. Results Analysis. Fig. 3 shows two consistent trends. (i) The full loop is strongest. The complete A-Evolve achieves the best performance across solvers. Removing any single component degrades results, although all ablations still outperform the vanilla solver, highlighting the complemen- (a) Claude Haiku 4.5 (b) Gemini 3 Flash Figure 3. Ablation studies of A-Evolve using Claude Haiku 4.5 and Gemini 3 Flash as solvers. tary roles of the modules and the importance of end-to-end coupling. (ii) Verification is critical stabilizer. Among all variants, A-Evolve/V degrades the most. Committing updates without validation allows brittle or overfit patches to accumulate, leading to regressions and unstable longhorizon behavior. Case Studies. Qualitative analysis reveals distinct failure modes for each ablation. Without diagnosis (A-Evolve/D), the evolver acts blindly, producing superficial patches that mask errors rather than fixing root causes. Without analysis tools (A-Evolve/A), updates rely on single-trajectory inference and capture only local symptoms, missing systematic patterns across episodes. Without planning (A-Evolve/P), the evolver fails to coordinate interdependent edits, leading to incoherent updates such as modifying tools without updating corresponding schemas. Finally, removing verification (A-Evolve/V) results in defective artifacts (e.g., tools with syntax errors) being committed, polluting context and degrading established capabilities. Detailed examples are provided in Appendix C.2. 5.4. Analysis of Evolution Scaling Scaling Evolution Compute. We first evaluate whether increasing evolution-time compute reliably improves capability. We vary the evolution step from 1 to 12 steps as proxy for Cevolve, where each step corresponds to the evolver processing batch of 10 episodes. We use Claude Haiku 4.5 as the solver and Claude Sonnet 4.5 as the evolver. Other settings follow Sec. 5.2. Fig. 4(a) compares A-Evolve with vanilla and AWM baselines. Two trends emerge. (i) Sustained scaling. A-Evolve improves monotonically as compute increases, achieving the best performance across all budgets, while AWM quickly plateaus. This supports Position: Agentic Evolution is the Path to Evolving LLMs over explicit, verifiable artifacts (e.g., code and tests), enabling modular updates, structured governance, and stronger safety guarantees than just gradient descent. Explicit evolution is too costly. Critics argue that maintaining an evolver agent is computationally wasteful compared to heuristics or passive memory. We view this as trade-off between short-term efficiency and long-term scalability. While heuristics plateau, our results in Sec. 5.4 show that agentic evolution continues to improve with evolutiontime compute, approaching the compute-optimal frontier. Allocating compute to evolution converts raw compute into durable capability for unbounded environments. 7. Conclusion and Future Directions This paper argues that the primary bottleneck to deploying intelligent systems in the real world is not static model capacity, but the ability to evolve. As agents move beyond curated training distributions into open-ended environments, static optimization and non-agentic update rules inevitably saturate. We contend that agentic evolution provides scalable alternative by elevating evolution from fixed pipeline to an autonomous, governed decision process that converts deployment experience into persistent capability. We further propose the evolution-scaling hypothesis, which posits that adaptation capacity scales with evolution-time compute, introducing new axis of scaling beyond training-time and inference-time computation. Realizing the full potential of agentic evolution requires progress along three complementary directions. Benchmarks. Future benchmarks should explicitly capture the traindeploy gap by placing agents in high-entropy, nonstationary environments where failures cannot be resolved through inference alone. Beyond task success, benchmarks should measure the durability and reuse of evolved artifacts to assess whether evolution-time compute produces lasting capability gains. Frameworks. Our analysis suggests that the structure of the evolution framework determines how efficiently system approaches the compute-optimal frontier . While A-Evolve provides concrete starting point, future frameworks should further improve diagnosis, planning, and verification to better convert evolution-time compute into stable improvements. Theory. foundational theory of agentic evolution remains open. Promising directions include formalizing evolution as optimization over combinatorial program space and establishing separation results showing that agentic evolution admits higher attainable frontier than non-agentic heuristics. Bounding regret relative to idealized oracle finetuning would provide principled basis for understanding long-horizon adaptation. (a) Evolution step (b) Evolver size Figure 4. Analysis of evolution-scaling hypotheses: we vary evolution step and evolver model size. the evolution-scaling hypothesis that additional evolution compute translates into higher attainable performance, keeping increasing with Cevolve. (ii) High efficiency. Even minimal evolution step (e.g., 0 1 step) yield substantial gains, indicating that structured evolution converts compute into capability more efficiently than heuristic baselines. Impact of Evolver Size. We next examine scaling compute per update by increasing evolver model size. Fixing the solver as Claude Haiku 4.5, we vary the evolver backbone across Claude Haiku 4.5, Sonnet 4.5, and Opus 4.5; other settings follow Sec. 5.2. Results are shown in Fig. 4(b). (i) Larger evolvers perform better. Both TGC and APT increase monotonically with evolver size, mirroring the trends observed under sample scaling and confirming that evolver capacity is key contributor to Cevolve. (ii) Reduced optimization noise. Qualitative analysis shows that smaller evolvers more often hallucinate root causes and propose brittle updates that fail verification. Larger evolvers more reliably diagnose failures and synthesize robust, generalizable artifacts, converting compute into capability more effectively. Additional analysis is provided in Appendix C.3. 6. Alternative Views We now address three common alternative perspectives on deployment-time adaptation. Inference-time reasoning is sufficient. common counter-argument suggests that explicit evolution is unnecessary if inference-time compute is scaled, allowing models to bridge environmental gaps by thinking longer (OpenAI et al., 2024). While extended reasoning is effective for novel instances, it is inefficient for recurrent failures. Without evolution, the system repeatedly rediscovers the same solutions. Agentic evolution amortizes this cost by converting transient reasoning into persistent artifacts, such as verified tools, yielding stable and reusable capability. Why not rely on parametric plasticity? Some advocate direct adaptation through online parameter updates, arguing that continual fine-tuning should replace artifact management (Luo et al., 2025). We contend that unconstrained weight updates are opaque to auditing and prone to catastrophic forgetting. In contrast, agentic evolution operates 8 Position: Agentic Evolution is the Path to Evolving LLMs"
        },
        {
            "title": "Impact Statement",
            "content": "We posit that Agentic Evolution represents the inevitable future of LLM deployment. By shifting the paradigm from static inference to continuous, goal-directed adaptation, this approach enables systems to autonomously bridge the critical train-deploy gap. This shift holds the potential to significantly increase the reliability and longevity of AI applications in dynamic, open-ended environments. Societal and Practical Benefits. key advantage of our approach is the enablement of local adaptation. Because the system evolves through persistent artifacts in situ, it reduces the need to transmit sensitive user data to central servers for global retraining, thereby enhancing user privacy and data sovereignty. Furthermore, by converting expensive inference-time reasoning into reusable tools and code, agentic evolution amortizes the computational cost of intelligence, offering more sustainable path for long-term deployment compared to purely scaling inference compute. Risks and Mitigations. We acknowledge that empowering systems to modify their own behavior introduces risks of capability drift or the optimization of unaligned objectives. To mitigate these risks, our framework explicitly incorporates validation gate as core governance mechanism. By treating updates as conditional decisions that must pass automated verification (e.g., regression tests) before commitment, we reduce the likelihood of introducing harmful or brittle behaviors. Additionally, unlike opaque parameter updates, agentic evolution modifies explicit, interpretable artifacts (e.g., code, tool, skill), preserving the ability for humans to audit and rollback system changes. Future work must continue to refine these verification standards to ensure autonomous evolution remains safe and aligned."
        },
        {
            "title": "References",
            "content": "Claude Anthropic. Oct anthropic.com/m/99128ddd009bdcb/ Claude-Haiku-4-5-System-Card.pdf. haiku URL 4.5 system card, https://assets. 2025a. Claude sonnet 4.5 system card, Sept URL https://assets.anthropic. Anthropic. 2025b. com/m/12f214efcc2f457a/original/ Claude-Sonnet-4-5-System-Card.pdf. Behrouz, A., Zhong, P., and Mirrokni, V. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Brohi, S., Mastoi, Q.-u.-a., Jhanjhi, N. Z., and Pillai, T. R. research landscape of agentic ai and large language models: Applications, challenges and future directions. Algorithms, 18, 2025. doi: 10.3390/a18080499. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Cai, Y., Cai, S., Shi, Y., Xu, Z., Chen, L., Qin, Y., Tan, X., Li, G., Li, Z., Lin, H., et al. Training-free group relative policy optimization. arXiv preprint arXiv:2510.08191, 2025. Cao, Z., Deng, J., Yu, L., Zhou, W., Liu, Z., Ding, B., and Zhao, H. Remember me, refine me: dynamic procedural memory framework for experience-driven agent evolution. arXiv preprint arXiv:2512.10696, 2025. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Fernando, C., Banarse, D. S., Michalewski, H., Osindero, S., and Rocktaschel, T. Promptbreeder: Self-referential selfimprovement via prompt evolution. In Proceedings of the 41st International Conference on Machine Learning, pp. 1348113544, 2024. Gama, J. a., ˇZliobaitundefined, I., Bifet, A., Pechenizkiy, M., and Bouchachia, A. survey on concept drift adaptation. ACM Comput. Surv., 46(4), 2014. ISSN doi: 10.1145/2523813. URL https: 0360-0300. //doi.org/10.1145/2523813. Gao, H.-a., Geng, J., Hua, W., Hu, M., Juan, X., Liu, H., Liu, S., Qiu, J., Qi, X., Wu, Y., et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. Gemini 3 flash model card, Dec 2025. https://storage.googleapis. Google. URL com/deepmind-media/Model-Cards/ Gemini-3-Flash-Model-Card.pdf. Behrouz, A., Razaviyayn, M., Zhong, P., and Mirrokni, V. Its all connected: journey through test-time memorization, attentional bias, retention, and online optimization. arXiv preprint arXiv:2504.13173, 2025. Han, H., Wang, Y., Shomer, H., Guo, K., Ding, J., Lei, Y., Halappanavar, M., Rossi, R. A., Mukherjee, S., Tang, X., et al. Retrieval-augmented generation with graphs (graphrag). arXiv preprint arXiv:2501.00309, 2024. 9 Position: Agentic Evolution is the Path to Evolving LLMs Hu, J., Zhang, Z., Chen, G., Wen, X., Shuai, C., Luo, W., Xiao, B., Li, Y., and Tan, M. Test-time learning for large language models. arXiv preprint arXiv:2505.20633, 2025a. Hu, J., Zhang, Z., Chen, G., Wen, X., Shuai, C., Luo, W., Xiao, B., Li, Y., and Tan, M. Test-time learning for large language models. In Forty-second International Conference on Machine Learning, 2025b. URL https: //openreview.net/forum?id=iCYbIaGKSR. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Selfevolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. Hubotter, J., Bongni, S., Hakimi, I., and Krause, A. Efficiently learning at test-time: Active fine-tuning of LLMs. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview. net/forum?id=NS1G1Uhny3. Jiang, S., Wang, Y., and Wang, Y. Selfevolve: code evolution framework via large language models. arXiv preprint arXiv:2306.02907, 2023. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Lai, H., Liu, X., Gao, J., Cheng, J., Qi, Z., Xu, Y., Yao, S., Zhang, D., Du, J., Hou, Z., Lv, X., Huang, M., Dong, Y., and Tang, J. survey of post-training scaling in large In Proceedings of the 63rd Annual language models. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2025. Liu, B., Guertler, L., Yu, S., Liu, Z., Qi, P., Balcells, D., Liu, M., Tan, C., Shi, W., Lin, M., et al. Spiral: Selfplay on zero-sum games incentivizes reasoning via multiagent multi-turn reinforcement learning. arXiv preprint arXiv:2506.24119, 2025. Luo, Y., Yang, Z., Meng, F., Li, Y., Zhou, J., and Zhang, Y. An empirical study of catastrophic forgetting in large IEEE language models during continual fine-tuning. Transactions on Audio, Speech and Language Processing, 2025. OpenAI, A. J., Kalai, A., Lerer, A., Richardson, A., ElKishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Ouyang, S., Yan, J., Hsu, I., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025. Pei, Z., Zhen, H.-L., Kai, S., Pan, S. J., Wang, Y., Yuan, M., and Yu, B. Scope: Prompt evolution for enhancing agent effectiveness. arXiv preprint arXiv:2512.15374, 2025. Prasad, A., Hase, P., Zhou, X., and Bansal, M. Grips: Gradient-free, edit-based instruction search for prompting large language models. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pp. 38453864, 2023. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al. Improving language understanding by generative pre-training. 2018. Schaeffer, R., Kazdan, J., Hughes, J., Juravsky, J., Price, S., Lynch, A., Jones, E., Kirk, R., Mirhoseini, A., and Koyejo, S. How do large language monkeys get their In Forty-second International Conpower (laws)? ference on Machine Learning, 2025. URL https: //openreview.net/forum?id=QqVZ28qems. Shi, Y., Cai, Y., Cai, S., Xu, Z., Chen, L., Qin, Y., Zhou, Z., Fei, X., Qiu, C., Tan, X., et al. Youtu-agent: Scaling agent productivity with automated generation and hybrid policy optimization. arXiv preprint arXiv:2512.24615, 2025. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Snell, C. V., Lee, J., Xu, K., and Kumar, A. Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=4FWAwZtd2n. Su, L., Zhang, Z., Li, G., Chen, Z., Wang, C., Song, M., Wang, X., Li, K., Wu, J., Chen, X., et al. Scaling agents via continual pre-training. arXiv preprint arXiv:2509.13310, 2025. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Trivedi, H., Khot, T., Hartmann, M., Manku, R., Dong, V., Li, E., Gupta, S., Sabharwal, A., and Balasubramanian, N. Appworld: controllable world of apps and people for benchmarking interactive coding agents. arXiv preprint arXiv:2407.18901, 2024. 10 Position: Agentic Evolution is the Path to Evolving LLMs Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., et al. Agentic context engineering: Evolving contexts for self-improving arXiv preprint arXiv:2510.04618, language models. 2025. Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J. Large language models are humanlevel prompt engineers. In The eleventh international conference on learning representations, 2022. Wang, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., and Zhang, L. Space: Noise contrastive estimation stabilizes self-play fine-tuning for large language models. arXiv preprint arXiv:2512.07175, 2025a. Wang, Y., Liu, S., Fang, J., and Meng, Z. Evoagentx: An automated framework for evolving agentic workflows. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 643655, 2025b. Wang, Z. Z., Mao, J., Fried, D., and Neubig, G. Agent workflow memory. arXiv preprint arXiv:2409.07429, 2024. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wei, T., Sachdeva, N., Coleman, B., He, Z., Bei, Y., Ning, X., Ai, M., Li, Y., He, J., Chi, E. H., et al. Evo-memory: Benchmarking llm agent test-time learning with selfevolving memory. arXiv preprint arXiv:2511.20857, 2025a. Wei, T., Li, T.-W., Liu, Z., Ning, X., Yang, Z., Zou, J., Zeng, Z., Qiu, R., Lin, X., Fu, D., et al. Agentic reasoning for large language models. arXiv preprint arXiv:2601.12538, 2026. Wei, Y., Sun, Z., McMilin, E., Gehring, J., Zhang, D., Synnaeve, G., Fried, D., Zhang, L., and Wang, S. Toward training superintelligent software agents through self-play swe-rl. arXiv preprint arXiv:2512.18552, 2025b. Xia, P., Zeng, K., Liu, J., Qin, C., Wu, F., Zhou, Y., Xiong, C., and Yao, H. Agent0: Unleashing self-evolving agents from zero data via tool-integrated reasoning. arXiv preprint arXiv:2511.16043, 2025. Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. Yin, F., Ye, X., and Durrett, G. Lofit: Localized fine-tuning on llm representations. Advances in Neural Information Processing Systems, 37:94749506, 2024. Yuksekgonul, M., Bianchi, F., Boen, J., Liu, S., Huang, Z., Guestrin, C., and Zou, J. Textgrad: Automatic differentiation via text. arXiv preprint arXiv:2406.07496, 2024. Zhai, Y., Tao, S., Chen, C., Zou, A., Chen, Z., Fu, Q., Mai, S., Yu, L., Deng, J., Cao, Z., et al. Agentevolver: Towards efficient self-evolving agent system. arXiv preprint arXiv:2511.10395, 2025. 11 A. Related Work: Evolution Paradigms of LLMs Position: Agentic Evolution is the Path to Evolving LLMs In this section, we review prior work through the lens of our evolution definition in Eq. 3 and the three orthogonal axes: what is updated (parametric weights πθ vs. persistent artifact state πS), when updates occur (offline training vs. deployment-time), and how the update rule FEvolve is realized (fixed heuristics vs. an explicit agentic optimizer). The evolution paradigm is outlined in Tab. 2. Table 2. Evolution paradigms organized by what is updated, when updates occur, and how the update rule FEvolve is instantiated. Paradigm What is Updated When is Updated How FEvolve is Realized Offline Parametric Evolution Online Parametric Evolution πθ πθ Offline deployment) (preTraining loop (e.g., self-play / distillation) Online (deploymenttime) Online optimization (e.g., gradient descent) Non-parametric Evolution Heuristic πS (K) Agentic Evolution (Ours) πS (K, , V) Online (deploymenttime) Online (deploymenttime) Fixed heuristic rules Explicit evolver agent A.1. Offline Parametric Evolution Self-play fine-tuning from synthetic interactions. predominant paradigm for enhancing agent capability is to improve the parametric backbone πθ offline by bootstrapping training signals from the model itself. Traditional approaches, such as SPIN (Chen et al., 2024), iteratively refine the policy by contrasting self-generated outputs against target distribution. Recent advances extend this to multi-turn and multi-agent settings: SPIRAL (Liu et al., 2025) induces reasoning skills via self-play in structured interaction games, while SPACE (Wang et al., 2025a) stabilizes the self-play objective to mitigate training instability. While these methods enlarge pre-deployment capability, they commit improvements solely to πθ during training, leaving the agent static and unable to adapt to distribution shifts once deployed. Zero-data curriculum generation. To circumvent the reliance on human-curated datasets, recent frameworks construct self-generated curricula within the offline loop. R-Zero (Huang et al., 2025) and Agent0 (Xia et al., 2025) instantiate co-evolving roles (e.g., challenger and solver) to synthesize frontier tasks and tool-use trajectories without external data. Similarly, in software engineering domains, SSR (Wei et al., 2025b) trains agents by injecting and repairing bugs in sandboxed repositories. Although these methods effectively address the data bottleneck, they remain instances of offline parametric evolution: adaptation ceases the moment the model is deployed. Agentic Continual Pre-training. Beyond fine-tuning, recent works propose injecting agentic capabilities (Brohi et al., 2025; Wei et al., 2026) even earlier in the pipeline. AgentFounder (Su et al., 2025) introduces Agentic Continual Pre-training (CPT) as an intermediate stage, scaling agentic capabilities via massive offline data synthesis (e.g., First-order and Higher-order Action Synthesis) derived from knowledge bases and trajectories. While this approach significantly raises the baseline performance of the foundation model, it remains fundamentally an instance of offline parametric evolution: the models adaptability is frozen once the pre-training concludes, leaving it vulnerable to novel environmental shifts during deployment. A.2. Online Parametric Evolution Test-time fine-tuning. Online parametric evolution attempts to update πθ (or subspace thereof) during deployment. Approaches like SIFT (Hubotter et al., 2025) explore active fine-tuning by selecting informative test-time examples for weight updates. To reduce computational overhead, methods such as LoFiT (Yin et al., 2024) localize updates to specific internal representations or heads. While enabling continuous adaptation, direct parametric modification introduces significant governance challenges, such as catastrophic forgetting (Luo et al., 2025), irreversibility, and instability, making it risky for long-horizon deployment. Architectures for test-time memorization. complementary direction blurs the boundary between weights and state by updating neural memory modules during streaming inference. Titans (Behrouz et al., 2024) introduces neural long-term memory module for context memorization, while MIRAS (Behrouz et al., 2025) provides framework for optimizing associative memory online. In our taxonomy, these methods still rely on parametric updates to learnable module rather Position: Agentic Evolution is the Path to Evolving LLMs than explicit updates to interpretable artifacts, retaining the opacity of weight-based evolution. A.3. Heuristic Non-Parametric Evolution Experience memory with fixed retrieval. To avoid the risks of weight updates, many systems (Ouyang et al., 2025; Wei et al., 2025a; Xu et al., 2025) modify persistent artifact state πS, typically memory bank of traces or summaries, using fixed heuristics. ReasoningBank (Ouyang et al., 2025) distills strategies into reusable memory bank via self-judgment. However, because the update rule FEvolve is heuristic (e.g., append and retrieve), these systems often suffer from context saturation and retrieval noise as experience accumulates. Prompt and context optimization. Another class of non-parametric methods (Prasad et al., 2023; Fernando et al., 2024; Zhou et al., 2022) treats πS as set of optimizable prompts. GrIPS (Prasad et al., 2023) and Promptbreeder (Fernando et al., 2024) apply discrete search operators (e.g., mutation, crossover) to evolve prompts. More recently, Training-Free GRPO (Cai et al., 2025) performs policy optimization in the context space via token priors, and TextGrad (Yuksekgonul et al., 2024) applies textual differentiation to optimize components. While effective, these methods typically employ static, pre-specified search algorithm as FEvolve, limiting the systems ability to strategically diagnose failures or plan complex structural updates. A.4. Toward Agentic Evolution Agent-driven context and prompt engineering. Several works operationalize prompt engineering as an agentic reasoning task. ACE (Zhang et al., 2025) and SCOPE (Pei et al., 2025) treat context management as an online optimization problem, employing modular loop (generatereflectcurate) to evolve playbooks or guidelines. While these methods move beyond heuristic search by using an LLM to reason about updates, they are primarily restricted to the textual context modality, optimizing instructions rather than executable code or structural system logic. Automated synthesis of tools and memory. Other approaches (Shi et al., 2025; Zhai et al., 2025; Wang et al., 2025b) focus on synthesizing specific functional components. Youtu-Agent (Shi et al., 2025) and AgentEvolver (Zhai et al., 2025) introduce meta-agents capable of generating new tools or training curricula to expand agent capabilities. Similarly, A-Mem (Xu et al., 2025) targets the memory modality, employing an agentic controller to dynamically restructure and cross-reference memory artifacts rather than passively appending logs. These systems demonstrate the feasibility of agent-driven updates for specific artifacts. However, they typically operate as specialized pipelines for single type of state (e.g., only tools or only memory) and often lack unified mechanism for governed diagnosis and verification across diverse failure modes. Positioning of our work. Existing works can be viewed as specific instances of using LLMs to optimize state. Our work generalizes these developments into the formal framework of Agentic Evolution. Unlike prior approaches that focus on optimizing single modality (e.g., prompts or tools), we propose holistic, budget-aware evolver capable of diagnosing diverse execution failures and proposing structural edits to heterogeneous state πS. Crucially, we introduce the principles of goal orientation, autonomy, and compositionality to strictly govern this process, ensuring that agent-driven updates are not just flexible, but reliable and scalable enough for deployment. B. Additional Details of Experimental Setup B.1. Dataset Details In this paper, we focus on AppWorld, controllable environment and benchmark designed for interactive coding agents. It simulates world of 9 day-to-day applications (e.g., Amazon, Spotify, Venmo, Gmail) and 2 helper apps (ApiDocs, Supervisor) through 457 APIs. The environment is populated with the digital activities of approximately 100 fictitious users, simulating realistic lives and relationships (e.g., family, roommates) to support complex interaction scenarios. The AppWorld framework consists of two primary components: AppWorld engine: This is high-quality execution environment. It provides fully controllable and reproducible simulator where agents can operate apps via APIs without real-world consequences, such as spending actual money or spamming emails. The engine allows agents to execute rich Python code, including loops and conditionals, to interact with the environment iteratively. 13 Position: Agentic Evolution is the Path to Evolving LLMs AppWorld benchmark: suite of 750 diverse and challenging tasks derived from 250 task scenarios. These tasks require agents to navigate multiple apps (average 1.8 apps per task) and utilize up to 26 APIs (avg. 9.5) with dependencies between calls (i.e., outputs of calls used as inputs to subsequent calls). The dataset is split into normal (test-normal) and challenge (test-challenge) sets, where the challenge set requires agents to utilize APIs from apps seen only during test time. B.2. Implementation Details of Agentic Evolution In this subsection, we provide granular details on the architecture, artifact representation, and inference configuration of A-Evolve. Architecture of the evolver agent. The Evolver is implemented as sequential pipeline with four specialized components: Observer, Proposer, Updater, and Verifier. Unlike role-based prompting, each component is instantiated as distinct module with dedicated logic and persistence. Diagnoser: Receives the full interaction trace Obs1:t from the Solver, including the user instruction, tool calls, stdout/stderr, and the final error. Its objective is failure attribution: distinguishing between stochastic environment noise and epistemic gaps in the agents policy. We employ cross-episode aggregation strategy over batched observations (window size = 10) to perform root cause analysis, filtering out transient errors to focus on persistent capability deficits. Planner: Conditioned on the diagnosis, structhe planner tural update δ. We constrain the planners action space to strict schema of operations = {CREATETOOL, EVOLVETOOL, ADDKNOWLEDGE, EVOLVEKNOWLEDGE, . . .}. This structured output forces the model to map abstract failure modes into concrete, semantically meaningful artifact definitions rather than unstructured text, significantly reducing the search space for improvements. synthesizing responsible for is Updater: The updater acts as the deterministic execution engine. It holds write access to the Artifact Registry (a version-controlled file system) and translates the Planners high-level intent into atomic file operations. It enforces strict separation of concerns, ensuring that tools (Python modules), skills (procedural knowledge), and facts (JSON knowledge base) are stored in their respective canonical formats. Verifier: Responsible for governance. In our case, it validates syntactic correctness of generated code (Python AST parsing for tools), schema compliance (YAML frontmatter for skills), and runtime safety (isolated execution in ToolWorkspace). Only proposals passing validation are committed to the workspace and reflected in the agents system prompt via on-policy state injection. Hyperparameters and Model Configuration. Unless otherwise specified, we use Claude Sonnet 4.5 (Anthropic, 2025b) as the evolver backbone, and evaluate solvers ranging from Claude Haiku 4.5 (Anthropic, 2025a) and Claude Sonnet 4/4.5 to GPT-5 (Singh et al., 2025) and Gemini 3 Flash (Google, 2025). For generation hyperparameters, we use temperature of 0.7 for both the evolver and solver. The maximum output token limits are set to 8, 192 for the evolver and 4, 096 for the solver. B.3. Evaluation Metrics To ensure the robustness of our evaluation, following prior work (Trivedi et al., 2024; Cao et al., 2025), we report two complementary metrics based on the concept of task score. For each task i, we define its score si [0, 1] as the fraction of passed unit tests: si = passed testsi total testsi . (12) Based on this score, we define: Average Passed Tests (APT): This metric captures the granular, incremental progress of the agent. It is simply the average score across all evaluation tasks: APT = 1 (cid:88) i=1 si. 14 (13) Position: Agentic Evolution is the Path to Evolving LLMs APT is particularly valuable for measuring evolution, as it reflects improvements in capability (e.g., si increasing from 0.2 to 0.8) even when the full task is not yet perfectly solved. Task Goal Completion (TGC): This metric represents the strict success rate. task is considered successfully completed if and only if it achieves perfect score (i.e., all verification conditions are met). TGC is defined as: TGC ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 I(si = 1), (14) where is the total number of tasks and I() is the indicator function. C. More details of Experimental Results C.1. Case Studies of Effectiveness of Agentic Evolution in Sec. 5.2 To provide qualitative evidence for the effectiveness trends in Tab. 1, we analyze representative failure and success patterns from A-Evolve and AWM. Both methods use the same training and test sets (50 tasks each) with Claude Haiku 4.5 as the solver. Observation 1: failure recovery mechanisms. key distinction is how each method handles errors mid-trajectory: AWM (Passive): When the solver encounters an API error (e.g., 401 Unauthorized), it has no mechanism to update its workflow in-place. The error propagates, and the solver must re-explore from scratchoften repeating the same mistake. In task What is the title of the most-liked song in my Spotify playlists, the agent cycles through 30 identical failed attempts without adaptation. A-Evolve (Active): After observing repeated 401 errors across batch, the evolver creates manage auth token tool that automatically caches and injects access tokens. The diagnosis explicitly identifies: The trajectory shows repeated 401 errors because the agent forgets to pass access token after login. This structural insight is encoded as verified tool, enabling one-shot authentication in subsequent tasks. Observation 2: context saturation. We compare the learned artifacts after 50 training tasks: AWM produces single monolithic workflow file that grows linearly with training. This file contains concatenated trajectory fragments with task-specific details (e.g., hardcoded credentials, specific note IDs) that do not generalize. When retrieved, these fragments often mislead the solver with irrelevant context. A-Evolve produces 16 modular skills (e.g., systematic api exploration, detect execution stall), 4 verified tools (e.g., authenticate app), and 4 knowledge entries (e.g., appworld authentication). Each artifact is semantically indexed by trigger conditions and verified before commitment. Observation 3: trajectory comparison. For task Reply to Christopher with movie recommendations from SimpleNote, we compare solve-time behavior: AWM Agent (Score: 0.88, 29 steps): Steps 1-6: Explores API documentation (redundant with workflow) Steps 7-11: Fails 401 on SimpleNote, re-explores Steps 12-23: Successful login and note retrieval after trial-and-error Steps 24-28: Finds contact, sends message Step 29: Completes task A-Evolved Agent (Score: 1.0, 8 steps): 15 Position: Agentic Evolution is the Path to Evolving LLMs Step 1: Invokes analyze task requirements() to parse goal Step 2: Retrieves authentication tool usage pattern skill Steps 3-4: Authenticates SimpleNote and Phone with cached token procedure Steps 5-6: Retrieves note, extracts movies Step 7: Sends message to Christopher Step 8: Completes task The A-Evolved agent is 3.6 more efficient (8 vs. 29 steps) because it amortizes authentication and goal-parsing into reusable procedures that execute deterministically, rather than re-exploring on each task. Takeaway. The case studies reveal fundamental asymmetry: AWM captures what the agent did (raw trajectories), while A-Evolve captures why failures occurred and how to prevent them (verified tools, diagnostic knowledge). This explains the consistent TGC and APT gaps in Tab. 1: agentic evolution converts deployment feedback into persistent, governed capabilities that static workflow retrieval cannot match. C.2. Analysis of Feasibility of Agentic Evolution in Sec. 5.3 Qualitative analysis. We analyze evolution traces and observe distinct failure modes for each ablation. Diagnosis is critical for depth: without it (A-Evolve/D), the agent acts blindly, forced to infer updates directly from raw trajectories without root-cause reasoning. Consequently, it resorts to superficial patching (e.g., masking crash with try-except rather than fixing the incorrect parser logic). Analysis Tools enable pattern discovery: without them (A-Evolve/A), the agent relies on pure LLM inference over single trajectories, capturing only superficial symptoms (e.g., fixing one-off error message). In contrast, analysis tools allow the evolver to aggregate underlying statistical patterns across episodes (e.g., clustering recurring failure modes), thereby distinguishing systematic deficits from transient noise. Planning ensures coherence: without it (A-Evolve/P), the agent lacks the mechanism to manage inter-dependencies between artifacts, leading to disjointed updates (e.g., modifying tools signature in without updating its usage schema in K, causing subsequent execution failures). Finally, Verification acts as the stabilizer: without it (A-Evolve/V), the system blindly commits defective artifacts (e.g., tools with syntax errors). This pollutes the context, causing the solver to waste inference budget on broken calls, and introduces regressions that degrade established capabilities. Case Studies. We provide several representative case studies to better support our analysis: Case 1: verification prevents defective artifacts (A-Evolve/V). When considering task What is the title of the most-played song in my Spotify album library, the evolver created discover apis tool without verification. On the solvers first invocation: discover apis: success: False, error: module appworld has no attribute list apis The solver wasted step before falling back to manual exploration. This pattern repeated: 2 broken tools committed across evolution, causing runtime errors that degraded solver efficiency by 15% compared to full A-Evolve. Case 2: planning enables implementable fixes (A-Evolve/P). The planning stage translates high-level diagnostic insights into coordinated action sequences. Consider task in full A-Evolve: the diagnosis identifies agent wastes steps on 401 authentication errors. The Plan generates 4 interdependent actions: CREATE TOOL: discover api spec (API discovery) CREATE TOOL: manage auth token (token storage) 16 Position: Agentic Evolution is the Path to Evolving LLMs ADD SKILL: systematic api exploration (usage pattern) ADD SKILL: authentication workflow (login procedure) Crucially, item (3) relies on item (1): the skill systematic api exploration explicitly instructs the solver to invoke the newly created discover api spec tool. Without planning, this dependency graph is lost. In the A-Evolve/P ablation, although the diagnosis produced similar insights, the immediate generation step failed to sequence the creation process. It attempted to define the skill before the tool existed or hallucinated tool signature, resulting in validation failures. Consequently, while A-Evolve/P accumulated some loose textual knowledge, it successfully synthesized 0 executable tools and 0 structured skills, failing to implement coherent, multi-artifact solutions. Case 3: diagnosis enables root-cause analysis (A-Evolve/D). Without diagnosis, the evolver observed the same errors but lacked structured analysis. With full A-Evolve, the diagnosis identified: Agent uses wrong parameter names (e.g., email instead of username) causing 422 errors. The agent doesnt consistently check API documentation before calling APIs. This insight led to the systematic api exploration skill, reducing 422 errors from 8 per trajectory to <1. Case 4: analysis tools enable pattern discovery (A-Evolve/A). Without analysis tools, the evolver relied on LLM inference over single trajectories. With analysis tools, grep observations revealed that 401 errors occurred in 18/20 training tasks, enabling the evolver to prioritize authentication workflow over superficial one-off fixes. Takeaway. Reliable agentic evolution is an irreducible loop: diagnosis and analysis enable targeted updates, planning enables implementable multi-step fixes, and verification contributes most for stable accumulation over long horizons. C.3. Case Studies of Impact of Evolver Size To provide qualitative evidence for the scaling trends in Fig. 4(b), we analyze how evolvers of different capacities respond to identical failure patterns. We run A-Evolve on the same 50-task training set with identical configurations, varying only the evolver backbone: Claude Sonnet 4.5 vs. Claude Haiku 4.5. The solver remains fixed as Claude Haiku 4.5 in both conditions. Observation 1: diagnosis depth and artifact quality. We observe stark qualitative differences in how evolvers diagnose failures and synthesize artifacts. Consider recurring failure pattern: the solver successfully authenticates and retrieves playlists but fails to find the most-liked song due to incorrect interpretation of the like count field versus popularity score. Sonnet 4.5 Evolver: The diagnosis identifies the structural root cause: The agents technical execution is flawless (40% score suggests partial credit for correct workflow), but its missing the actual task requirements. The trajectory shows it reads What is the title of... but does not extract the semantic meaning. The evolver proposes multi-artifact update: EVOLVE TOOL: Updates analyze task requirements to parse task descriptions and extract expected output formats ADD SKILL: Creates pre submission verification to validate answers against task requirements before calling task complete() ADD SKILL: Creates detect execution stall to recognize when the agent is not progressing toward the goal Haiku 4.5 Evolver: The diagnosis is shallower: The agent failed because it did not call the correct API. Fix by adding tactical patch. The evolver proposes: 1. EVOLVE TOOL: Attempts to create api interaction engine to wrap all API calls In addition, the tool fails verification after 3 repair attempts due to missing parameter validation. The structural tool is rejected, leaving only shallow behavioral textual patches. 17 Position: Agentic Evolution is the Path to Evolving LLMs Table 3. Verification outcomes across 50-task training runs. Larger evolvers produce artifacts that pass verification at significantly higher rates. Evolver Proposals Tools Committed Verification Failures Sonnet 4.5 Haiku 4.5 21 25 4 2 (repaired successfully) 8 (3+ repair attempts each) Observation 2: verification failure rates. We quantify the difference in artifact quality by examining verification outcomes: The Haiku evolver emits 4 more verification failures despite generating fewer total proposals. Qualitative inspection reveals that Haiku-generated tools often have subtle bugs: missing required parameters, incorrect function signatures, or hallucinated API endpoints. Observation 3: cascading effects on solver behavior. The quality gap compounds over training. With Sonnet-evolved artifacts, the solver learns to: Call analyze task requirements() before execution to parse the goal Invoke evolved authentication tools that correctly manage access tokens Apply pre submission verification to catch answer-format mismatches the solver exhibits regression: it attempts to use the partially-broken However, with Haiku-evolved artifacts, api interaction engine tool, which fails silently or returns errors. The solver then falls back, losing any efficiency gains from tool use. Observation 4: trajectory comparison. For task venmo payment request based on SimpleNote records, we compare solver behavior: Sonnet-evolved solver: Invokes analyze task requirements() on first turn, retrieves relevant skills, and completes the task in 8 steps with no errors. Haiku-evolved solver: Fail to call tools, it then attempts 14 exploratory API calls before arriving at the correct workflow. Despite executing the business logic correctly (e.g., creating 4 payment requests), it fails to call task complete(), failure mode that triggered repeated but unsuccessful evolution attempts. Takeaway. Larger evolvers produce higher-quality diagnoses that identify structural capability gaps rather than surface-level symptoms. This translates into more robust artifact synthesis: tools that pass verification, skills that encode generalizable procedures, and knowledge that captures system invariants. The verification stage acts as quality filter, but cannot rescue fundamentally flawed proposalshence the importance of evolver capacity in converting Cevolve into durable capability."
        }
    ],
    "affiliations": [
        "Amazon",
        "Duke University",
        "The Pennsylvania State University"
    ]
}