{
    "paper_title": "3DIS-FLUX: simple and efficient multi-instance generation with DiT rendering",
    "authors": [
        "Dewei Zhou",
        "Ji Xie",
        "Zongxin Yang",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining a new adapter each time a more advanced model is released, resulting in significant resource consumption. A methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling various models to perform training-free detail rendering. Initially, 3DIS focused on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce a detail renderer that manipulates the Attention Mask in FLUX's Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 1 3 1 5 0 . 1 0 5 2 : r Preprint. 3DIS-FLUX: INSTANCE GENERATION WITH DIT RENDERING SIMPLE AND EFFICIENT MULTIDewei Zhou1, Ji Xie1, Zongxin Yang2, Yi Yang1 1RELER, CCAI, Zhejiang University 2DBMI, HMS, Harvard University {zdw1999,sanaka87,yangyics}@zju.edu.cn {Zongxin Yang}@hms.harvard.edu corresponding author project page: https://limuloo.github.io/3DIS/ Images generated using our 3DIS-FLUX. Based on the user-provided layout, Figure 1: 3DIS (Zhou et al., 2024c) generates scene depth map that precisely positions each instance and renders their fine-grained attributes without the need for additional training, using variety of foundational models. Specifically, 3DIS-FLUX employs the state-of-the-art FLUX model for rendering, which is capable of producing superior image quality and offering enhanced control."
        },
        {
            "title": "ABSTRACT",
            "content": "The growing demand for controllable outputs in text-to-image generation has driven significant advancements in multi-instance generation (MIG), enabling users to define both instance layouts and attributes. Currently, the state-of-the-art methods in MIG are primarily adapter-based. However, these methods necessitate retraining new adapter each time more advanced model is released, resulting in significant resource consumption. methodology named Depth-Driven Decoupled Instance Synthesis (3DIS) has been introduced, which decouples MIG into two distinct phases: 1) depth-based scene construction and 2) detail rendering with widely pre-trained depth control models. The 3DIS method requires adapter training solely during the scene construction phase, while enabling varInitially, 3DIS focused ious models to perform training-free detail rendering. 1 Preprint. on rendering techniques utilizing U-Net architectures such as SD1.5, SD2, and SDXL, without exploring the potential of recent DiT-based models like FLUX. In this paper, we present 3DIS-FLUX, an extension of the 3DIS framework that integrates the FLUX model for enhanced rendering capabilities. Specifically, we employ the FLUX.1-Depth-dev model for depth map controlled image generation and introduce detail renderer that manipulates the Attention Mask in FLUXs Joint Attention mechanism based on layout information. This approach allows for the precise rendering of fine-grained attributes of each instance. Our experimental results indicate that 3DIS-FLUX, leveraging the FLUX model, outperforms the original 3DIS method, which utilized SD2 and SDXL, and surpasses current state-of-the-art adapter-based methods in terms of both performance and image quality. Project Page: https://limuloo.github.io/3DIS/."
        },
        {
            "title": "INTRODUCTION",
            "content": "With the rapid development of Diffusion Models (Ho et al., 2020; Song et al., 2020; Zhou et al., 2023; Zhao et al., 2024b; Lu et al., 2024a;b; Xie et al., 2024), contemporary models (Rombach et al., 2022; 2023; Podell et al., 2023; BlackForest, 2024) are capable of generating high-quality images. At the same time, there is growing demand for more control over the generation process (Zhang et al., 2023; Liang et al., 2024; Lu et al., 2023; Zhao et al., 2024a). One prominent area of research that has garnered increasing attention is Multi-Instance Generation (MIG) (Zhou et al., 2024a;b; Wang et al., 2024; Li et al., 2023), which seeks to ensure precise alignment of each instances position and attributes with user-defined specifications during the generation of multiple instances. Current strategies for Multi-Instance Generation (MIG) can be broadly classified into three categories: 1) Training-free methods, such as Multi-Diffusion (Bar-Tal et al., 2023) and RAGDiffusion (Chen et al., 2024b), which employ multiple sampling steps for each instance and later merge them based on layout information to achieve spatial control. BoxDiff (Xie et al., 2023) and TFLCG (Chen et al., 2024a) define score function to guide the models sampling process, thereby enabling control over the layout. 2) Adapter-based methods, exemplified by MIGC (Zhou et al., 2024a) and InstanceDiffusion (Wang et al., 2024), which introduce layout information by training additional attention layers atop pre-trained models. 3) Text encoder fine-tuning methods, such as Reco (Yang et al., 2023) and Ranni (Feng et al., 2024), which incorporate layout information directly into the input text and subsequently fine-tune the text encoder and the whole model to embed this spatial context into the generated output. Adapter-based methods are currently widely used due to their ability to provide strong control without the need to train the entire model. However, these approaches face two main challenges: 1) the need to retrain on different models. For example, methods like MIGC (Zhou et al., 2024a), GLIGEN (Li et al., 2023), and InstanceDiffusion (Wang et al., 2024) were initially trained on SD1.5, but as more advanced models such as SDXL and SD3 emerged, techniques like IFAdapter (Wu et al., 2024) and CreatiLayout (Zhang et al., 2024) had to be retrained accordingly. This process is resource-intensive and can be particularly difficult for users with limited GPU access. 2) Largescale instance-level annotations are often hard to obtain. Each instance generation can be viewed as Text-to-Image task, and high-quality instance-level data is generally more challenging to acquire than high-quality image-level data. To address the aforementioned challenges, the Depth-Driven Decouple Instance Synthesis (3DIS) (Zhou et al., 2024c) approach introduces novel framework for Multi-Instance Generation. Instead of directly generating RGB images, 3DIS first trains layout-to-depth model to produce scene depth map. Then, 3DIS utilizes widely pre-trained depth control models, which only require image-level annotations, to generate images based on the layout provided by the generated scene depth map. Finally, 3DIS employs training-free method to precisely render the attributes of each instance. The 3DIS approach offers two key advantages: 1) It requires the training of only depth generation model, which can ignore many fine-grained attributes during training and does not necessitate high-quality visual fidelity. 2) The training-free rendering in 3DIS enables the use of various pre-trained models and better preserves the generative capabilities of large pre-trained models. The original 3DIS paper focused on the training-free rendering approach using models based on U-Net architectures, such as SD1.5 (Rombach et al., 2022), SD2 (Rombach et al., 2023), and Preprint. Figure 2: The overview of 3DIS-FLUX. In line with 3DIS, the 3DIS-FLUX approach decouples image generation into two distinct stages: the creation of scene depth map and the training-free rendering of high-quality RGB images using various generative models. 3DIS-FLUX utilizes the Layout-to-Depth model from 3DIS to generate the scene depth map, and subsequently employs the FLUX-depth model to render images based on the depth map. During this process, 3DIS-FLUX incorporates an Attention Controller to ensure the accurate fine-grained attributes of each instance. SDXL (Podell et al., 2023). With the advancement of diffusion model techniques, the Diffusion Transformer (DiT) (Li et al., 2024) architecture has demonstrated superior capabilities compared to traditional U-Net models. Specifically, FLUX has not only achieved significant improvements in image quality but has also enhanced control capabilities beyond those of previous models. As 3DIS is flexible framework capable of quickly adapting to various new foundational models, we have extended it to propose 3DIS-FLUX, which leverages FLUX for training-free rendering, enabling stronger control and higher-quality image generation. The overview of 3DIS-FLUX is depicted in Fig. 2. Consistent with the original 3DIS framework, 3DIS-FLUX initially generates scene depth map using layout-to-image model. Subsequently, we utilize the FLUX.1-depth-dev model for depth-to-image conversion. Through the application of the FLUX-depth model, alignment of the layout with the scene depth map is ensured. However, ensuring the accuracy of individual instance attributes still poses challenge. To overcome this, we introduce training-free Detail Renderer to achieve precise instance rendering in the Joint Attention (Liu et al., 2024; Dalva et al., 2024; Shin et al., 2024) of the FLUX models. Specifically, we ensure that image tokens corresponding to each instance only attend to their respective text tokens. In the early steps, we mandate that each instances image tokens only focus on their own image tokens. Moreover, since FLUX employs T5 text encoder pre-trained exclusively on textual datayielding embeddings devoid of visual informationwe find that strict constraints on the attention map of text tokens within Joint Attention are crucial for successful rendering. Specifically, we restrict text tokens of each instance from attending to text tokens of other instances. We conducted experiments on the COCO-MIG (Zhou et al., 2024a) benchmark. The results show that by using the more powerful FLUX model for rendering, 3DIS-FLUX achieved 6.9% improvement in Instance Success Ratio (ISR) compared to the previous 3DIS-SDXL. Compared to the training-free state-of-the-art (SOTA) method Multi-Diffusion, 3DIS-FLUXs improvement in ISR surpassed 41%. Against the SOTA adapter-based method, InstanceDiffusion, 3DIS-FLUX achieved 12.4% higher ISR. Additionally, rendering with the FLUX model enabled our approach to demonstrate superior image quality compared to other methods. Preprint."
        },
        {
            "title": "2.1 PRELIMINARIES",
            "content": "FLUX (BlackForest, 2024) is recent state-of-the-art Diffusion Transformer (DiT) model that generates higher-quality images compared to previous models and demonstrates powerful text control capabilities. Given an input text, FLUX first encodes it into text embedding using the T5 text encoder (Raffel et al., 2020). This text embedding is then concatenated with the image embedding to perform joint attention. After several iterations of joint attention (Liu et al., 2024; Shin et al., 2024; Dalva et al., 2024), the FLUX model decodes the output image embedding to produce high-quality image that corresponds to the input text."
        },
        {
            "title": "2.2 PROBLEM DEFINITION",
            "content": "Multi-Instance Generation (MIG) requires the generation model to simultaneously produce multiple instances, ensuring that their positions and attributes align with the users specifications. Given layout = {p1, p2, . . . , pn} and the textual descriptions of the instances = {t1, t2, . . . , tn}, MIG demands that each instance be generated at the specified position pi, while visually matching the description ti. Additionally, the user provides global text that describes the entire scene, and the generated image must be consistent with this global text. 2.3 OVERVIEW Fig. 2 illustrates the overview of 3DIS-FLUX. Similar to the original 3DIS, 3DIS-FLUX decouples Multi-Instance Generation into two stages: generating the scene depth map and rendering finegrained details. In the first stage, 3DIS-FLUX employs the layout-to-depth model from 3DIS (Zhou et al., 2024c) to generate the corresponding scene depth map based on the user-provided layout. In the second stage, 3DIS-FLUX uses the FLUX.1-depth-dev (BlackForest, 2024) model to generate an image from the scene depth map, thereby controlling the layout of the generated image. To further ensure that each instance is rendered with accurate fine-grained attributes, 3DIS-FLUX incorporates detail renderer, which constrains the attention mask during joint attention between the image and text embeddings based on the layout information. 2.4 FLUX DETAIL RENDERER Motivation. Given scene depth map generated in the first stage, FLUX.1-depth-dev model (BlackForest, 2024) is capable of producing high-quality images that adhere to the specified layout. In scenarios involving only single instance, users can achieve precise rendering by describing the instance through single global image text. However, challenges arise when attempting to render multiple instances accurately with just one global text description. For example, in the case illustrated in Fig. 2, rendering each cup in the scene depth map with designated attributes using description such as photo of an orange cup, yellow cup, and blue cup proves difficult. This approach frequently results in color inconsistencies, such as cup intended to be blue being rendered as orange, with additional examples illustrated in Fig. 4. Consequently, integrating spatial constraints into the joint attention process of the FLUX model is essential for the accurate rendering of multiple instances. To overcome these challenges, we introduce simple yet effective FLUX detail renderer that significantly enhances the precision of such renderings. 2 , . . . , Preparation. To render multiple instances simultaneously according to the users descriptions, we encode not only the global image text into 5 but also the instance descriptions {t1, t2, . . . , tn} 1 , 5 into {f 5 }. These encoded features are concatenated to form the final text embedding = concat(f 5 , 5 ), which is then input into the FLUX models joint attention mechanism. Based on the user-provided layout = {p1, p2, . . . , pn}, we determine the correspondence between image tokens and text tokens during the joint attention process. Since scene depth map has already been generated in the first stage, we can opt to use the SAM (Kirillov et al., 2023) model to further optimize the users layout for more accurate rendering, as illustrated in Fig. 2. 1 , . . . , 5 Controlling the Attention of Image Embedding. The FLUX model generates images through multi-step sampling. 1) The early steps determine the primary attributes of each instance. TherePreprint. fore, it is essential to strictly avoid attribute leakage by ensuring that the image token corresponding to instance can only attend to the image tokens within the pi region during joint attention and can only attend to its corresponding text token 5 . 2) In the later steps, to ensure the quality of the generated image, we relax this constraint: each image token can attend to all other image tokens. Additionally, while attending to its corresponding text token 5 , it can also attend to the global text token 5 . We control these two phases by setting threshold γ. Controlling the Attention of Text Embedding. In the FLUX model, the T5 text encoder (Raffel et al., 2020), which was pretrained solely on textual data, is employed to extract text encodings. This contrasts with previous methods that utilized the CLIP text encoder (Radford et al., 2021), which was pretrained using both text and image data. Notably, we observed that during the joint attention process, the T5 text embeddings inherently lack significant semantic information. If unconstrained, they are prone to inadvertently introducing incorrect semantic information. For instance, as demonstrated in Fig. 5, when the T5 text embeddings of black car and green parking meter are concatenated and input into FLUXs joint attention mechanism, allowing the green parking meter tokens to attend to the black car tokens results in the parking meter being predominantly black. Concurrently, it was found that FLUX was unable to successfully render the black car at this stage. Therefore, it is imperative to impose constraints on the attention masks of the text tokens during joint attention to avoid such semantic discrepancies. We have discovered that imposing strict attention mask constraints on the text tokens of instances throughout all steps does not significantly affect the quality of the final generated image. Therefore, throughout all steps, we restrict the text token corresponding to 5 during joint attention to only focus on the image tokens within the pi itself. For the text tokens of the global text tokenf 5 area and to only attend to the text token of 5 , we do not apply significant constraints. i"
        },
        {
            "title": "3 EXPERIMENT",
            "content": "3.1 IMPLEMENT DETAILS During the layout-to-depth phase, we employ the same method as used in the original 3DIS (Zhou et al., 2024c) approach. To incorporate depth control in image generation, we utilize the FLUX.1depth-dev model (BlackForest, 2024). During the image generation process, we employ sampling strategy of 20 steps. For images with resolution of 512, the parameter γ is set to 4. As the resolution increases, γ is adjusted accordingly: it is set to 3 for images of 768 resolution and reduced to 2 for images of 1024 resolution. 3.2 EXPERIMENT SETUP Baselines. We compared our proposed 3DIS method with state-of-the-art Multi-Instance Generation approaches. The methods involved in the comparison include training-free methods: BoxDiffusion (Xie et al., 2023) and MultiDiffusion (Bar-Tal et al., 2023); and adapter-based methods: GLIGEN (Li et al., 2023), InstanceDiffusion (Wang et al., 2024), and MIGC (Zhou et al., 2024a). Evaluation Benchmarks. We conducted experiments on the COCO-MIG (Zhou et al., 2024a) benchmark to assess models ability to control the position of instances and precisely render fine-grained attributes for each generated instance. For comprehensive evaluation, each model generated 750 images in the benchmark. Evaluation Metrics. We used the following metrics to evaluate the model: 1) Mean Intersection over Union (MIoU), measuring the overlap between the generated instance positions and the target positions; 2) Instance Success Ratio (ISR), calculating the proportion of instances that are correctly positioned and possess accurate attributes. 3.3 COMPARISON Comparison with SOTA Methods. The results presented in Tab. 1 demonstrate that the 3DIS method not only exhibits strong positional control capabilities but also robust detail-rendering capabilities. Notably, the entire process of rendering instance attributes is training-free for 3DIS. Compared to the previous state-of-the-art (SOTA) training-free method, MultiDiffusion, 3DIS5 Preprint. Table 1: Quantitative results on proposed COCO-MIG-BOX (3.3). Li means that the count of instances needed to generate in the image is i. Instance Success Ratio Mean Intersection over Union Method L2 L3 L4 L6 AVG L2 L3 L4 L5 L6 AVG GLIGEN [CVPR23] InstanceDiff [CVPR24] MIGC [CVPR24] 41.3 61.0 74.8 TFLCG [WACV24] BoxDiff [ICCV23] MultiDiff [ICML23] 17.2 28.4 30.6 (SD1.5) 65.9 (SD2.1) 66.1 (SDXL) 66.1 3DIS-FLUX (FLUX) 76.4 +46 3DIS 3DIS 3DIS vs. MultiDiff 33.8 52.8 66. 13.5 21.4 25.3 56.1 57.5 59.3 68.4 +43 Adapter rendering methods 29.5 48.7 66.1 31.3 50.5 67.1 27.0 45.2 65.3 31.8 52.4 67.4 33.7 53.8 63. training-free rendering 4.5 12.8 19.8 47.6 52.9 54.1 58.9 +39 8.3 15.7 22.3 53.0 54.7 56.0 62.9 +41 6.1 11.9 18.3 45.3 51.7 51.7 58.1 +40 7.9 14.0 24.5 55.3 55.1 56.2 63.3 +39 10.9 19.1 21.9 56.8 57.1 57.0 67.3 +45 27.6 45.8 54. 8.7 14.6 18.1 48.4 48.6 50.0 61.2 +43 25.5 44.9 55.3 5.1 9.4 17.3 49.4 46.8 47.8 56.4 +39 21.9 37.7 52.4 3.9 7.9 12.9 40.2 42.9 43.1 52.3 +39 23.6 40.6 53. 2.8 8.5 13.9 41.7 43.4 44.6 52.7 +39 25.2 43.0 54.7 5.3 10.6 15.8 44.7 45.7 47.0 56.2 +40 3DIS+GLIGEN vs. GLIGEN 3DIS+MIGC vs. MIGC rendering w/ off-the-shelf adapters 43.0 29.9 34. 34.5 39.7 29.6 28.8 49.4 +8.1 +5.9 +2.7 +2.6 +0.4 +2.8 +9.3 +6.2 +3.7 +2.7 +0.9 +3.6 59.5 76.8 +2.0 +4.0 +4.9 +1.1 +1.9 +2.6 +5.0 +6.0 +6.7 +3.4 +4.1 +4.8 72.3 33. 24.6 29.2 68.0 24.5 69.7 66. 68.0 60.7 62.0 55.8 57.3 70. Figure 3: Qualitative results on the COCO-MIG (3.3). FLUX achieves 41% improvement in the Instance Success Ratio (ISR). Additionally, when compared with the SOTA adapter-based method, Instance Diffusion, which requires training for rendering, 3DIS-FLUX shows 12.4% increase in ISR. Importantly, the 3DIS approach is not mutually exclusive with existing adapter methods. For instance, combinations like 3DIS+GLIGEN and 3DIS+MIGC outperform the use of adapter methods alone, delivering superior performance. Fig. 3 offers visual comparison between 3DIS and other SOTA methods, where it is evident that 3DIS not only excels in scene construction but also demonstrates strong capabilities in instance detail rendering. Furthermore, 3DIS is compatible with variety of base models, offering broader applicability compared to previous methods. Comparison of Rendering Across Different Models. As shown in Tab. 1, employing more robust model significantly enhances the success rate of rendering. For instance, rendering with the FLUX model achieves 9.9% higher Instance Success Ratio compared to using the SD1.5 model. 3.4 ABLATION STUDY FLUX Detail Renderer. Results from Fig. 4 indicate that without employing Detail Renderer to manage the Joint Attention process of the FLUX model, it becomes challenging to successfully render each instance in multi-instance scenarios. Additionally, data from Tab. 2 demonstrates that Preprint. Figure 4: Ablation Study on the FLUX Detail Renderer. Figure 5: Ablation Study on Controlling Text-to-Text Attention in the FLUX Detail Renderer. the introduction of Detail Renderer enhances the Instance Success Ratio (ISR) by 17.8% and the Success Ratio (SR) by 12.2%. Moreover, the results from Fig. 4 also suggest that incorporating Detail Renderer does not significantly compromise image quality. Table 2: Ablation study on FLUX detail renderer (3.4). Controlling the Attention of Image Embedding. Results from Tab. 2 show that in the Joint Attention mechanism, controlling each image token to focus solely on its corresponding instance description token (i.e., I2T control) is crucial for successfully rendering each instance, resulting in 19.1% increase in the Instance Success Ratio (ISR). Additionally, restricting each instances image tokens to only attend to other image tokens belonging to the same instance (i.e., I2I control) in the earlier steps of the process also leads to significant improvement, enhancing the ISR by 7.5%. ISR MIOU SR 21.1 49.9 55.4 14.4 40.4 43.8 63.4 56.2 28.0 16.7 42.4 46.6 17.5 41.3 45.1 29.7 56.2 62.9 method w/o I2I control w/o I2T control w/o T2I control w/o T2T control w/o detail renderer w/ all Controlling the Attention of Text Embedding. In contrast to models such as SD1.5 (Rombach et al., 2022), SD2 (Rombach et al., 2023), and SDXL (Podell et al., 2023), which utilize CLIP (Radford et al., 2021) as their text encoder, FLUX employs T5 text encoder (Raffel et al., 2020). This encoder is exclusively pre-trained on textual data, resulting in embeddings that contain no visual information. Therefore, it becomes intuitively important in the joint attention mechanism to impose constraints on text tokens within multi-instance contexts. As demonstrated by the results in Tab. 2 and Fig. 5, the absence of constraints on text tokens within joint attention mechanismspermitting text token from one instance to attend to text tokens from other instancessignificantly undermines the rendering success rate, evidenced by substantial decrease in ISR by 16.3%. Furthermore, our analysis reveals that adding constraints, where each instances text tokens are restricted to only attend to their corresponding image tokens, does not result in significant improvement. 7 Preprint."
        },
        {
            "title": "4 CONCLUSION",
            "content": "We introduce 3DIS-FLUX, an extension of the prior 3DIS framework. The original 3DIS explored training-free rendering approach using only the U-net architecture. In contrast, 3DIS-FLUX harnesses the state-of-the-art DiT model, FLUX, for rendering. Experiments conducted on the COCO-MIG dataset demonstrate that rendering with the more robust FLUX model allows 3DISFLUX to significantly outperform the previous 3DIS-SDXL method, and even surpass state-of-theart Adapter-based MIG approaches. The success of 3DIS-FLUX underscores the flexibility of the 3DIS framework, which can rapidly adapt to variety of newer, more powerful models. We envision that 3DIS will enable users to utilize broader spectrum of foundational models for multi-instance generation and expand its applicability to more diverse applications."
        },
        {
            "title": "REFERENCES",
            "content": "Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. arXiv preprint arXiv:2302.08113, 2023. BlackForest. Black forest labs; frontier ai lab, 2024. URL https://blackforestlabs.ai/. Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention guidance. WACV, 2024a. Zhennan Chen, Yajie Li, Haofan Wang, Zhibo Chen, Zhengkai Jiang, Jun Li, Qian Wang, Jian Yang, and Ying Tai. Region-aware text-to-image generation via hard binding and soft refinement. arXiv preprint arXiv:2411.06558, 2024b. Yusuf Dalva, Kavana Venkatesh, and Pinar Yanardag. Fluxspace: Disentangled semantic editing in rectified flow transformers. arXiv preprint arXiv:2412.09611, 2024. Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming text-toimage diffusion for accurate instruction following. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 47444753, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33: 68406851, 2020. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. CVPR, 2023. Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiao-Ting Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Mengxi Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yang-Dan Tao, Jianchen Zhu, Kai Liu, Si-Da Lin, Yifu Sun, Yun Li, Dongdong Wang, MingDao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Dingyong Wang, Yong Yang, Jie Jiang, and Qinglin Lu. Hunyuan-dit: powerful multi-resolution diffusion transformer with fine-grained chinese understanding. ArXiv, abs/2405.08748, 2024. URL https://api. semanticscholar.org/CorpusID:269761491. Chao Liang, Fan Ma, Linchao Zhu, Yingying Deng, and Yi Yang. Caphuman: Capture your moments in parallel universes. In CVPR, 2024. Bingchen Liu, Ehsan Akhgari, Alexander Visheratin, Aleks Kamko, Linmiao Xu, Shivam Shrirao, Chase Lambert, Joao Souza, Suhail Doshi, and Daiqing Li. Playground v3: Improving textto-image alignment with deep-fusion large language models. arXiv preprint arXiv:2409.10695, 2024. Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free crossdomain image composition. In ICCV, 2023. 8 Preprint. Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. CVPR, 2024a. Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, and Adams Wai-Kin Kong. Robust watermarking using generative priors against image editing: From benchmarking to advances. arXiv preprint arXiv:2410.18775, 2024b. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, A. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, 2021. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn OmURL https://stability.ai/news/ Stable diffusion version 2, 2023. mer. stable-diffusion-v2-release. Chaehun Shin, Jooyoung Choi, Heeseung Kim, and Sungroh Yoon. Large-scale text-to-image model with inpainting is zero-shot subject-driven image generator. arXiv preprint arXiv:2411.15466, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proc. of ICLR, 2020. Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation, 2024. Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, and Xinchao Wang. Ifadapter: Instance feature control for grounded text-to-image generation. arXiv preprint arXiv:2409.08240, 2024. Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. ICCV, 2023. Rui Xie, Ying Tai, Chen Zhao, Kai Zhang, Zhenyu Zhang, Jun Zhou, Xiaoqian Ye, Qian Wang, and Jian Yang. Addsr: Accelerating diffusion-based blind super-resolution with adversarial diffusion distillation. arXiv preprint arXiv:2404.01717, 2024. Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Reco: Region-controlled text-to-image generation. In CVPR, 2023. Hui Zhang, Dexiang Hong, Tingwei Gao, Yitong Wang, Jie Shao, Xinglong Wu, Zuxuan Wu, and Yu-Gang Jiang. Creatilayout: Siamese multimodal diffusion transformer for creative layout-toimage generation. arXiv preprint arXiv:2412.03859, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pp. 38363847, 2023. Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. CVPR, 2024a. Chen Zhao, Chenyu Dong, and Weiling Cai. Learning physical-aware diffusion model based on transformer for underwater image enhancement. arXiv preprint arXiv:2403.01497, 2024b. 9 Preprint. Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. In IJCAI, 2023. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. CVPR, 2024a. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc++: Advanced multi-instance generation controller for image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024b. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024c."
        }
    ],
    "affiliations": [
        "DBMI, HMS, Harvard University",
        "RELER, CCAI, Zhejiang University"
    ]
}