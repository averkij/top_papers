{
    "paper_title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
    "authors": [
        "Oren Barkan",
        "Yahlly Schein",
        "Yehonatan Elisha",
        "Veronika Bogina",
        "Mikhail Baklanov",
        "Noam Koenigstein"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec."
        },
        {
            "title": "Start",
            "content": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration Oren Barkan1*, Yahlly Schein2*, Yehonatan Elisha2, Veronika Bogina2, Mikhail Baklanov2, Noam Koenigstein2 1The Open University, Israel 2Tel Aviv University, Israel 5 2 0 2 2 2 ] I . [ 1 7 4 0 8 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Explanation fidelity, which measures how accurately an explanation reflects models true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), model-agnostic approach that adapts pathintegration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec. Introduction Recent advances in recommender systems over the past decade (He et al. 2017; Kang and McAuley 2018; He et al. 2020; Barkan et al. 2019; Barkan, Katz, and Koenigstein 2020; Barkan et al. 2021; Katz et al. 2022) have increasingly shaped personalized decisions across e-commerce, social media, and streaming platforms, making transparency and trust more essential than ever. (Fan et al. 2022). Explainability in these systems is critical not only for user satisfaction but also for accountability, compliance with regulations, and user control. However, while explainable recommendation research is rapidly expanding (Zhang, Chen et al. 2020; Varasteh et al. 2024), most existing work focuses on user-centric aspects such as persuasiveness, clarity, or satisfaction (Kunkel et al. 2019; Tintarev 2025). critical yet underexplored dimension is fidelity, which measures how accurately explanations reflect recommenders actual decision process. Without *Equal contribution. Corresponding author. Copyright 2026, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. fidelity, explanations may appear plausible while failing to reveal the true reasoning behind recommendations (Koenigstein 2025). We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), the first adaptation of path-integration (PI) (Sundararajan, Taly, and Yan 2017) to recommender systems. Unlike prior applications of PI in vision (Kapishnikov et al. 2021; Barkan et al. 2023a,c; Elisha, Barkan, and Koenigstein 2024; Barkan et al. 2025, 2023d,b) or NLP (Sikdar, Bhattacharya, and Heese 2021; Enguehard 2023), recommender data is characterized by extreme sparsity and binary-valued interactions, where the absence of an interaction may be ambiguous. Standard PI methods, which integrate gradients from an all-zero baseline, fail in this setting due to weak or misleading attribution signals. Crucially, modern recommenders leverage both observed and unobserved interactions as informative signals. SPINRec addresses this by stochastically sampling plausible user baselines from the empirical data distribution and selecting the explanation that maximizes fidelity. This adaptation enables more stable and faithful explanations tailored to the structure of recommender systems. To evaluate SPINRec, we conducted extensive fidelity evaluation spanning three model architectures (MF, VAE, NCF), multiple benchmark datasets (ML1M, Yahoo! Music, Pinterest), and suite of counterfactual fidelity metrics (Barkan et al. 2024; Gurevitch et al. 2025; Baklanov et al. 2025). Our results establish SPINRec as the new state-of-the-art benchmark in recommender systems explainability, with ablation studies confirming the distinct contributions of both pathintegration and our stochastic baseline sampling strategy. Contributions. Introduce SPINRec, the first adaptation of path-integration methods to recommender systems. Develop novel stochastic baseline sampling strategy tailored to sparse, binary recommendation data. Conduct comprehensive fidelity-focused evaluation across multiple architectures and datasets. Establish SPINRec as the new state-of-the-art for fidelityaware recommendation explanations. As fidelity remains an underexplored yet critical dimension in explainable recommendation (Baklanov et al. 2025; Mohammadi et al. 2025; Koenigstein 2025), we expect this work to lay essential groundwork for future research on trustworthy, model-faithful explanations. Related Work The rapid growth of recommender systems has driven increasing interest in Explainable AI (XAI) methods to ensure transparency, build trust, and enhance user engagement (Tintarev and Masthoff 2022; Zhang, Chen et al. 2020). While broad range of explanations for recommenders exist, standardized benchmarks for explanation fidelity remain significantly underexplored (Baklanov et al. 2025; Mohammadi et al. 2025; Koenigstein 2025). Explanation Methods for Recommenders Early works typically proposed model-specific explanations, such as those for matrix factorization (Abdollahi and Nasraoui 2016, 2017) or inherently interpretable recommender architectures (Barkan et al. 2020, 2023e; Melchiorre et al. 2022; Gaiger et al. 2023; Sugahara and Okamoto 2024). Aspect-based methods attribute recommendations to humaninterpretable item features (e.g., price or color) (Vig, Sen, and Riedl 2009; Zhang et al. 2014; Wang et al. 2018; Li, Zhang, and Chen 2021). However, these methods rely heavily on structured feature availability and are difficult to generalize to implicit or sparse data scenarios. Model-agnostic explanation methods offer broader applicability by explaining arbitrary recommenders independently of their internal mechanisms. Prominent examples include LIME-RS (Nobrega and Marinho 2019), influence-based methods such as FIA and ACCENT (Cheng et al. 2019; Tran, Ghazimatin, and Saha Roy 2021), and Shapley-value-based methods (SHAP4Rec and DeepSHAP) (Zhong and Negre 2022; Lundberg and Lee 2017a). While model-agnostic methods enhance generality, their fidelity remains less scrutinized and insufficiently benchmarked. Fidelity Evaluation in Recommender Systems Explanation fidelity, the degree to which an explanation reflects the true reasoning of recommender model, is essential for transparency and accountability, yet remains underexplored in the recommendation domain. Unlike computer vision or NLP, where fidelity evaluation is wellestablished (Samek et al. 2016; Agarwal et al. 2022), recommender systems have historically focused on user-centric goals like persuasiveness (Tintarev and Masthoff 2015; Zhang, Chen et al. 2020) or satisfaction, often overlooking whether explanations faithfully reflect model logic (Koenigstein 2025). Recent works have introduced counterfactual frameworks that evaluate fidelity by perturbing user histories and observing changes in recommendation outcomes (Barkan et al. 2024; Gurevitch et al. 2025). However, early approaches conflate supportive and contradictory features, apply coarse fixed-percentage masking, and lack control over explanation conciseness. The refined metrics proposed by Baklanov et al. (2025) address these limitations by evaluating fixed-length explanations, separating feature roles, and enabling consistent, interpretable comparisons across users. Our work builds directly on this line of research, offering the first extensive empirical evaluation that spans both the original (Barkan et al. 2024) and refined (Baklanov et al. 2025) fidelity metrics. By benchmarking wide range of explanation methods across multiple datasets and recommender models, we establish SPINRec as new state-of-the-art for fidelity-aware explanations in recommender systems. Path-Integration (PI) for Explainability Path-integration (PI) techniques (Sundararajan, Taly, and Yan 2017) are widely adopted in computer vision and NLP (Kapishnikov et al. 2021; Xu, Venugopalan, and Sundararajan 2020; Sanyal and Ren 2021; Enguehard 2023) to overcome limitations of vanilla gradients such as saturation and instability. By integrating gradients along path from baseline to the input, PI yields more robust and interpretable explanations. However, directly applying existing PI methods to recommender systems is suboptimal. User representations in this domain are high-dimensional, sparse, and binary, where the absence of interaction conveys ambiguous information. Naıve baselines, such as all-zero cold user vectors, fail to reflect realistic user behavior and may produce weak or misleading gradient signals. This issue parallels challenges in computer vision, where black-image baselines distort attribution in dark regions (Haug et al. 2021). SPINRec introduces stochastic baseline sampling strategy explicitly tailored to these challenges. Instead of single unrealistic baseline, it samples multiple plausible user histories from the empirical data distribution, capturing both presence and absence information. The SPINRec Algorithm We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), the first adaptation of pathintegration methods to explain recommender systems. Setup and Notation Let and denote the sets of users and items, respectively. Each user is represented by binary feature vector xu {0, 1}V, where each feature xu[i] = 1 indicates that user interacted with item i, and 0 otherwise. We consider recommender model : {0, 1}V [0, 1]V, parameterized by θ, which outputs predicted affinity scores over items given xu. The predicted affinity for item is denoted y(xu). An explanation algorithm assigns relevance score to each feature via an explanation map [0, 1]V, where m[i] quantifies the contribution of feature (i.e., x[i]) to the prediction y(xu). High m[i] values indicate stronger influence on the recommendation. Path-Integration for Recommenders Given user data vector and recommended item y, SPINRec attributes the predicted affinity score θ (x) to individual features in the user data vector by integrating gradients along path from baseline vector to x. We define straight-line path r(t) = + (1 t) for [0, 1], interpolating between the baseline and actual user representation. The attribution is defined as the difference in predicted scores between the affinity of item to user with personal data and baseline z. The difference can be decomposed via the chain rule: θ (x) y θ (z) = (cid:90) 0 dt θ (r(t)) dt = (cid:90) 1 0 r(t) y θ (r(t)) dt = (cid:88) (cid:90) 1 i=1 dri dt θ (r(t)) ri dt, (1) where ri(t) is the i-th coordinate of r(t). Hence, an explanation map can be calculated by attributing the difference between the predicted scores at and according to: = (cid:90) 1 0 θ (r(t)) r(t) dr(t) dt dt, (2) where denotes element-wise multiplication. While PI is well-studied in continuous domains, applying it effectively in sparse, binary recommender data requires careful baseline design. Challenges in Baseline Selection The choice of the baseline vector significantly impacts the effectiveness of path-integration (PI) methods (Haug et al. 2021; Erion et al. 2021). While baseline sampling is common in other domains e.g., computer vision (Erion et al. 2021), recommender systems pose unique challenges due to the sparse and binary nature of user data: Implicit Binary Signals: Binary inputs limit the variability and restrict the range of values the model expects. feedback restricts the applicability of continuous-valued baseline methods (Sturmfels, Lundberg, and Lee 2020; Haug et al. 2021). Data Sparsity: Most useritem interactions are zeros, meaning that no meaningful path exists when integrating from an all-zero baseline. Diverse User Behaviors: single baseline may not capture the variability across user preferences and interactions. Crucially, naıve baseline (i.e., cold user) produces suboptimal gradient signals, since unobserved items remain zero during interpolation and thus contribute no gradients. However, modern recommenders leverage both observed and unobserved interactions as informative signals. This insight motivates our use of non-zero, data-driven baselines: by sampling plausible user profiles, SPINRec captures both presence and absence effects, yielding significantly more faithful explanations, as confirmed by our ablation study. Stochastic Baseline Sampling SPINRec introduces stochastic sampling strategy tailored to recommender systems sparse and binary data. Instead of relying on single baseline, it samples set of κ plausible baselines = {z1, . . . , zκ} from the distribution of user histories, capturing the diversity and heterogeneity of user behavior. For each zi B, we compute an explanation map mi using Eq. 2. Then, the final explanation is chosen to maximize fidelity metric s() as follows: = arg max mM s(m), (3) where = {m1, . . . , mκ}. We note that beyond the maps in M, one can further consider the mean map = 1 i=1 mi; by doing so, SPINRec generalizes Expected κ Gradients (Erion et al. 2021). (cid:80)κ Algorithm 1 summarizes the SPINRec process, from baseline sampling to final explanation map selection. Algorithm 1: SPINRec : Stochastic Path-Integration 1: Input: User data x, recommender fθ, target item y, number of baselines to sample κ, metric 2: Output: Explanation map 3: {}; Sample κ baselines from to form the baselines set Compute path r(t) from to Compute via Eq. 2; {m} 4: for do 5: 6: 7: end for 8: return argmax mM s(m) Computational Complexity For each of the κ sampled baselines, SPINRec integrates over gradient steps, followed by perturbation-based evaluations to compute s(). Accordingly, SPINRecs computational cost is dominated by two components: gradient-based integration (Eq. 2) and the counterfactual evaluation via s() (Eq. 3). For model with parameters and items, the overall cost is: O(cid:0)κQ(J + V)(cid:1) O(κQN V), since typically V. Compared to SHAP (Zhong and Negre 2022) with exponential cost in V, or LIME (Ribeiro, Singh, and Guestrin 2016) with cubic sample complexity, SPINRec scales linearly with the number of user features and baseline samples. All steps are embarrassingly parallel and well-suited to GPU acceleration. We note that while LXR (Barkan et al. 2024) offers faster inference via trained explainer, it requires pretraining and still falls short of SPINRecs fidelity as we show in our evaluations. Counterfactual Fidelity Metrics As discussed earlier, recent work has introduced counterfactual fidelity metrics tailored to recommender systems (Barkan et al. 2024; Baklanov et al. 2025). We build on this foundation by being the first to systematically evaluate both the original AUC-based metrics from Barkan et al. (2024) and the refined fixed-length variants proposed by Baklanov et al. (2025). To ensure fair comparison, we adhere strictly to the evaluation protocols used in these works. Illustrative Example. Figure 1 illustrates the principal behind counterfactual fidelity evaluation. Given users interaction history, SPINRec identifies key features driving the recommendation of The Lion King. Masking these features results in substantial rank drop, empirically validating their explanatory power. Formal Definitions. Let xu denote user us historical interaction vector, and Ke the number of top explanatory features. Define binary mask mKe selecting the top Ke features, and form two perturbed user vectors: Retained Explanations Vector: xKe = xu mKe Removed Explanations Vector: xKe = xu (1 mKe) The following metrics assess fidelity by measuring ranking or confidence changes for the target item under these counterfactual modifications: POS@Kr, Ke: Item drops out of top-Kr recommendations when top-Ke features are removed (lower is better): POS@Kr, Ke = 1[ranky fθ (xKe ) Kr]. DEL@Ke: Confidence drop after removing top-Ke features (lower is better): DEL@Ke = (xKe )y (xu)y . INS@Ke: Confidence recovery from adding top-Ke features (higher is better): INS@Ke = (xKe )y (xu)y . CDCG@Ke: Rank degradation after removing explanatory features (lower is better): CDCG@Ke = 1 log2(1 + ranky fθ (xKe )) . AUC Computation. To compute AUC variants, we follow the fixed-step perturbation strategy of Barkan et al. (2024), which averages the models scores as features in the user vector are progressively removed or added. Experimental Setup Our setup builds on the protocol of Barkan et al. (2024), extending it with third dataset (Pinterest), additional fidelity metrics from Baklanov et al. (2025), and broader set of explanation baselines. Due to space constraints, the Pinterest results are provided in our public repository. Hyperparameters were tuned via grid search on held-out validation set, with final values also available in the repository. All experiments were conducted on NVIDIA V100 GPUs using PyTorch 1.13 and CUDA 11.7. Recommendation Models We evaluate SPINRec across three standard recommendation models: Matrix Factorization (MF) (Koren, Bell, and Volinsky 2009): Despite its simplicity, MF remains competitive with modern recommenders (Rendle et al. 2022). We use dynamic variant that derives user embeddings directly from interaction vectors. Variational Autoencoder (VAE) (Liang et al. 2018; Shenbin et al. 2020): generative latent variable model that reconstructs user-item vectors from compressed representations. Neural Collaborative Filtering (NCF) (He et al. 2017): hybrid architecture combining matrix factorization and multilayer perceptrons to model nonlinear user-item interactions. on three Datasets Experiments were datasets: conducted ML1M (Harper and Konstan 2015), Yahoo! Music (Dror et al. 2012), and Pinterest (He et al. 2017). All datasets were binarized to implicit feedback, with an 80/20 user-based train-test split. An additional 10% of users were withheld from training for hyperparameter tuning. All results are reported on the test set, where explanations target the top recommendation per user. Baselines and Methods We compare SPINRec against broad set of post-hoc, model-agnostic explanation baselines, spanning heuristic, perturbation-based, and learning-based methods: Cosine Similarity: non-counterfactual heuristic that ranks user-history items by cosine similarity to the recommended item (Singh et al. 2020). SHAP4Rec (Zhong and Negre 2022): perturbationbased method grounded in Shapley values (Lundberg and Lee 2017b), adapted for recommendation via Jaccard-based clustering and = 10 k-means sampling, as in (Barkan et al. 2024). DeepSHAP (Lundberg and Lee 2017a): fast SHAP approximation using DeepLIFT-style gradient propagation (Shrikumar, Greenside, and Kundaje 2017). LIME-RS (Nobrega and Marinho 2019): LIME adaptation for recommender systems, fitting local linear surrogate model around perturbed user profile. LIRE (Brunot et al. 2022): robust LIME variant using importance sampling to improve faithfulness in sparse recommendation domains. FIA (Cheng et al. 2019): An approach utilizing influence functions to estimate the effect of each user feature. ACCENT (Tran, Ghazimatin, and Saha Roy 2021): fidelity-aware explainer based on influence functions (Koh and Liang 2017), extending FIA to capture second-order model effects. LXR (Barkan et al. 2024): state-of-the-art fidelity-aware method that learns an auxiliary explainer network to optimize counterfactual metrics under perturbation. PI (Ablated SPINRec ): vanilla path-integration baseline that omits the stochastic baseline sampling of SPINRec . This model serves to isolate the contribution of sampling, showing that while PI alone achieves strong fidelity, it remains suboptimal. The full SPINRec significantly outperforms ABLT across all settings, demonstrating the importance of adapting PI to sparse recommender data. SPINRec (Ours): The proposed method combines path integration with fidelity-optimized stochastic baseline sampling to generate high-precision attribution maps for recommendation outcomes. Figure 1: Illustration of Counterfactual Fidelity. SPINRec identifies items in the users history most responsible for recommending The Lion King. When these items are masked, the recommendations rank drastically drops, demonstrating explanation fidelity."
        },
        {
            "title": "Counterfactual Evaluation Results",
            "content": "We evaluate SPINRec across three recommender architectures (MF, VAE, NCF) and three benchmark datasets (ML1M, Yahoo! Music, Pinterest), using both AUC-style (Barkan et al. 2024) and fixed-length (Baklanov et al. 2025) counterfactual fidelity metrics. Results for Pinterest are reported in our public repository. AUC-Based Metrics. Tables 12 report Area-Under-Curve (AUC) scores, summarizing fidelity degradation under stepwise perturbations. Across all models and datasets, SPINRec achieves the best results, significantly surpassing strong baselines such as LXR and FIA (p 0.01, paired t-test). Fixed-Length Fidelity Metrics. Tables 34 present fidelity at fixed explanation lengths Ke {2, 3, 4} and ranking cutoffs Kr {5, 10, 20}, simulating realistic user-facing scenarios where concise, high-impact explanations are crucial. As Kr increases and Ke decreases, the counterfactual test becomes more challenging: fewer explanatory items must shift the recommended item beyond stricter cutoff. This difficulty is reflected in tighter performance margins, especially at Ke=2 and 3, where multiple methods sometimes tie. We omit Ke=1 here due to its instability and limited discriminative power, but include results for Ke=1 and 5 in our public repository, where trends remain consistent. SPINRec again outperforms all baselines across all configurations, confirming its robustness across fidelity granularities. Ablation Study: Plain vs. Stochastic Path Integration To isolate the contribution of stochastic baseline sampling, we compare SPINRec to its ablated variant (PI), which employs plain path integration without sampling. While PI performs competitively and often ranks near the top, SPINRec consistently achieves superior fidelity across most metrics and datasets. This improvement reflects key insight: modern recommenders rely not only on observed interactions but also on their absence as informative signals. By sampling diverse, non-zero baselines, SPINRec allows missing items to contribute meaningful gradients. Gains over PI are most pronounced in advanced models (VAE, NCF), where unobserved interactions play larger role. Impact of Sampling Count κ. Figure 2 shows the effect of varying the number of sampled baselines κ on explanation fidelity. Performance plateaus at around κ=10, indicating strong balance between fidelity and computational efficiency. Summary and Insights. Across all metrics, datasets, and recommender architectures, SPINRec consistently outperforms existing explanation methods, establishing new fidelity benchmark for recommendation. Path integration (PI) itself proves to be an inherently strong approach for fidelity, even without stochastic sampling. Incorporating stochastic baseline sampling further enhances fidelity by leveraging both observed and unobserved interactions, an effect particularly pronounced in more expressive models such as VAE and NCF. Table 1: ML1M Dataset Table 2: Yahoo Dataset POS@5 POS@10 POS@20 DEL INS CDCG 0.744 0.883 0.547 0.778 0.694 0.543 0.797 0.571 0.529 0.527 0.595 0.766 0.490 0.698 0.536 0.411 0.649 0.518 0.416 0.335 0.360 0.602 0.282 0.391 0.397 0.293 0.387 0.346 0.283 0.261 0.776 0.911 0.851 0.858 0.564 0.938 0.735 0.928 0.678 0.926 0.570 0.938 0.729 0.910 0.593 0.936 0.555 0.939 0.555 0.939 0.007 0.020 0.011 0.011 0.007 0.025 0.008 0.016 0.007 0.021 0.005 0.029 0.007 0.017 0.006 0.022 0.005 0.029 0.005 0.031 0.485 0.769 0.620 0.667 0.387 0.805 0.484 0.774 0.474 0.776 0.591 0.805 0.462 0.774 0.451 0.790 0.389 0.807 0.382 0.810 0.646 0.812 0.431 0.644 0.588 0.432 0.707 0.457 0.418 0.410 0.412 0.602 0.340 0.502 0.345 0.234 0.483 0.348 0.236 0.189 0.263 0.501 0.210 0.291 0.301 0.215 0.306 0.249 0.211 0.185 0.703 0.857 0.499 0.725 0.651 0.497 0.760 0.521 0.483 0.478 0.501 0.689 0.410 0.604 0.437 0.312 0.565 0.430 0.319 0.252 0.315 0.559 0.247 0.345 0.350 0.256 0.348 0.301 0.248 0. 0.589 0.734 0.422 0.576 0.512 0.422 0.652 0.442 0.411 0.405 0.435 0.572 0.396 0.502 0.392 0.320 0.505 0.394 0.322 0.293 0.312 0.504 0.272 0.334 0.338 0.276 0.347 0.303 0.273 0.258 MF Rec Method Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec VAE NCF POS@5 POS@10 POS@20 DEL INS CDCG 0.504 0.697 0.401 0.530 0.581 0.408 0.505 0.428 0.398 0.393 0.605 0.766 0.558 0.744 0.665 0.542 0.704 0.589 0.545 0.489 0.579 0.657 0.339 0.386 0.517 0.342 0.399 0.353 0.338 0. 0.695 0.868 0.765 0.821 0.592 0.882 0.681 0.871 0.701 0.858 0.601 0.882 0.646 0.874 0.620 0.879 0.591 0.883 0.591 0.883 0.014 0.042 0.021 0.031 0.014 0.043 0.021 0.032 0.017 0.038 0.013 0.048 0.019 0.032 0.014 0.041 0.013 0.048 0.012 0.049 0.619 0.723 0.657 0.683 0.526 0.768 0.549 0.762 0.590 0.736 0.527 0.768 0.547 0.758 0.536 0.764 0.527 0.768 0.525 0.769 0.382 0.533 0.324 0.402 0.448 0.328 0.411 0.344 0.323 0.318 0.432 0.576 0.410 0.554 0.463 0.377 0.553 0.429 0.380 0.358 0.466 0.559 0.315 0.347 0.443 0.317 0.366 0.327 0.313 0.305 0.331 0.530 0.258 0.360 0.424 0.263 0.364 0.282 0.256 0.246 0.402 0.605 0.362 0.576 0.453 0.315 0.541 0.393 0.320 0.280 0.456 0.561 0.253 0.290 0.417 0.254 0.312 0.267 0.250 0.238 0.436 0.637 0.348 0.463 0.525 0.352 0.452 0.371 0.344 0.337 0.503 0.690 0.454 0.664 0.559 0.420 0.622 0.488 0.424 0.376 0.512 0.603 0.285 0.328 0.460 0.288 0.346 0.300 0.284 0.273 MF Rec Method Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec VAE NCF Acknowledgment This work was supported by the Ministry of Innovation, Science & Technology, Israel. References Abdollahi, B.; and Nasraoui, O. 2016. Explainable matrix factorization for collaborative filtering. In Proceedings of the 25th International Conference Companion on World Wide Web, 56. Abdollahi, B.; and Nasraoui, O. 2017. Using explainability for constrained matrix factorization. In Proceedings of the eleventh ACM conference on recommender systems, 7983. Agarwal, C.; Krishna, S.; Saxena, E.; Pawelczyk, M.; Johnson, N.; Puri, I.; Zitnik, M.; and Lakkaraju, H. 2022. Openxai: Towards transparent evaluation of model explanations. Advances in Neural Information Processing Systems, 35: 15784 15799. Baklanov, M.; Bogina, V.; Elisha, Y.; Schein, Y.; Allerhand, L.; Barkan, O.; and Koenigstein, N. 2025. Refining Fidelity In ProceedMetrics for Explainable Recommendations. ings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2967 2971. Barkan, O.; Bogina, V.; Gurevitch, L.; Asher, Y.; and Koenigstein, N. 2024. Counterfactual Framework for Learning and Evaluating Explanations for Recommender Systems. In Proceedings of the ACM on Web Conference 2024, 3723 3733. Barkan, O.; Elisha, Y.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023a. Visual Explanations via Iterated Integrated Attributions. In IEEE/CVF International Conference on Computer Vision (ICCV), 20732084. Figure 2: Fidelity (INS) vs. number of baseline samples (κ) for NCF on ML1M. Gains plateau after κ=10."
        },
        {
            "title": "Conclusion",
            "content": "We introduced SPINRec, the first model-agnostic explanation method to apply path integration (PI) to recommender systems. By combining PI with stochastic baseline sampling strategy tailored to sparse, binary useritem data, SPINRecproduces stable, high-fidelity explanations that more faithfully capture model reasoning. comprehensive evaluation across multiple models, datasets, and fidelity metrics demonstrates that SPINRecconsistently outperforms existing approaches, establishing new benchmark for fidelity-aware explainability in recommendation."
        },
        {
            "title": "We hope this work encourages further exploration of",
            "content": "fidelity-focused explainability in recommendation. Table 3: Fidelity Metrics at Different Kr, Ke Values for MF, VAE, and NCF (ML1M) Method Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 POS@5,Ke POS@10,Ke POS@20,Ke DEL@Ke INS@Ke CDCG@Ke Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec 0.987 1.000 0.980 0.987 0.993 0.980 0.994 0.988 0.980 0.975 0.976 0.994 0.952 0.985 0.967 0.921 0.988 0.983 0.922 0.903 0.906 0.979 0.908 0.939 0.935 0.887 0.957 0.923 0.893 0.864 0.964 0.999 0.945 0.964 0.974 0.945 0.984 0.958 0.945 0.940 0.942 0.984 0.903 0.959 0.942 0.844 0.968 0.944 0.848 0. 0.835 0.955 0.820 0.884 0.879 0.792 0.921 0.840 0.799 0.747 0.945 0.994 0.921 0.948 0.964 0.916 0.977 0.933 0.916 0.911 0.906 0.978 0.840 0.938 0.909 0.752 0.948 0.906 0.770 0.687 0.762 0.930 0.728 0.823 0.825 0.699 0.871 0.737 0.705 0.637 0.996 1.000 0.993 0.994 0.999 0.993 0.998 0.997 0.993 0.993 0.993 0.999 0.986 0.996 0.991 0.978 0.998 0.995 0.981 0. 0.964 0.998 0.986 0.971 0.974 0.945 0.981 0.959 0.947 0.937 0.988 0.999 0.974 0.988 0.993 0.974 0.993 0.980 0.974 0.972 0.982 0.997 0.964 0.991 0.980 0.937 0.983 0.979 0.937 0.923 0.917 0.997 0.964 0.934 0.932 0.887 0.955 0.911 0.890 0.866 0.974 0.998 0.957 0.976 0.985 0.953 0.987 0.962 0.953 0.952 0.961 0.989 0.929 0.973 0.969 0.891 0.974 0.959 0.902 0. 0.845 0.983 0.929 0.887 0.895 0.806 0.916 0.842 0.814 0.772 0.998 1.000 0.997 0.997 1.000 0.997 0.998 1.000 0.997 0.997 0.998 0.999 0.997 0.999 0.998 0.996 0.998 0.999 0.997 0.996 0.983 1.000 0.997 0.984 0.988 0.971 0.991 0.976 0.975 0.969 MF 0.988 1.000 0.978 0.988 0.990 0.974 0.991 0.978 0.974 0. VAE 0.986 0.997 0.970 0.993 0.990 0.960 0.986 0.982 0.963 0.945 0.993 1.000 0.991 0.993 0.997 0.988 0.995 0.993 0.988 0.988 0.995 0.998 0.989 0.998 0.995 0.983 0.995 0.994 0.984 0.978 NCF 0.945 0.993 0.938 0.959 0.966 0.930 0.970 0.943 0.935 0. 0.908 0.983 0.887 0.923 0.937 0.882 0.954 0.899 0.890 0.858 0.983 0.997 0.975 0.980 0.985 0.974 0.983 0.979 0.974 0.974 0.895 0.986 0.876 0.936 0.878 0.810 0.901 0.906 0.814 0.808 0.947 0.985 0.936 0.948 0.947 0.929 0.944 0.938 0.932 0.929 0.974 0.995 0.960 0.968 0.977 0.959 0.973 0.966 0.959 0.959 0.856 0.983 0.832 0.911 0.836 0.746 0.864 0.864 0.752 0. 0.922 0.977 0.905 0.924 0.924 0.896 0.917 0.908 0.901 0.895 0.965 0.992 0.945 0.958 0.969 0.944 0.962 0.951 0.944 0.944 0.821 0.973 0.795 0.886 0.800 0.694 0.831 0.826 0.701 0.685 0.897 0.968 0.875 0.901 0.902 0.865 0.892 0.877 0.870 0.863 0.750 0.648 0.795 0.751 0.764 0.796 0.750 0.779 0.798 0.799 2.807 0.617 2.162 1.015 1.588 2.002 0.998 2.823 1.987 3. 0.572 0.480 0.600 0.560 0.557 0.586 0.556 0.587 0.585 0.612 0.785 0.662 0.842 0.788 0.804 0.842 0.786 0.828 0.845 0.846 3.125 0.656 2.536 1.108 1.836 2.429 1.139 3.369 2.400 4.197 0.637 0.507 0.673 0.615 0.613 0.655 0.612 0.659 0.654 0.689 0.812 0.676 0.877 0.818 0.836 0.876 0.815 0.865 0.879 0.880 3.286 0.696 2.770 1.216 2.014 2.703 1.246 3.873 2.676 4. 0.696 0.533 0.739 0.666 0.663 0.715 0.664 0.727 0.716 0.757 0.915 0.969 0.886 0.916 0.926 0.884 0.953 0.904 0.884 0.859 0.861 0.941 0.828 0.893 0.841 0.763 0.945 0.877 0.761 0.703 0.811 0.910 0.788 0.836 0.824 0.775 0.892 0.813 0.782 0.723 0.883 0.956 0.836 0.884 0.894 0.831 0.933 0.862 0.832 0.806 0.825 0.922 0.772 0.863 0.784 0.680 0.920 0.835 0.689 0. 0.740 0.879 0.711 0.772 0.765 0.685 0.840 0.732 0.694 0.629 0.861 0.945 0.791 0.850 0.871 0.788 0.910 0.820 0.789 0.762 0.782 0.903 0.723 0.830 0.737 0.616 0.895 0.782 0.626 0.550 0.674 0.845 0.634 0.717 0.712 0.615 0.791 0.661 0.621 0.560 Table 4: Fidelity Metrics at Different Kr, Ke Values for MF, VAE, and NCF on Yahoo! Method Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 Ke=2 Ke=3 Ke=4 POS@5,Ke POS@10,Ke POS@20,Ke DEL@Ke INS@Ke CDCG@Ke Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec Cosine SHAP DeepSHAP LIME LIRE FIA ACCENT LXR PI SPINRec 0.789 0.879 0.745 0.803 0.862 0.743 0.833 0.778 0.743 0.722 0.834 0.900 0.813 0.897 0.875 0.759 0.887 0.878 0.769 0. 0.796 0.871 0.705 0.718 0.826 0.700 0.766 0.735 0.704 0.686 0.672 0.811 0.619 0.704 0.776 0.610 0.753 0.652 0.609 0.579 0.741 0.848 0.702 0.836 0.803 0.642 0.827 0.810 0.649 0.603 0.710 0.810 0.583 0.607 0.747 0.579 0.666 0.628 0.585 0.558 0.586 0.754 0.501 0.622 0.711 0.492 0.676 0.541 0.492 0.458 0.671 0.804 0.614 0.791 0.738 0.560 0.775 0.760 0.571 0. 0.639 0.755 0.489 0.514 0.669 0.484 0.579 0.526 0.489 0.455 0.872 0.927 0.839 0.882 0.918 0.837 0.888 0.859 0.837 0.829 0.908 0.938 0.895 0.934 0.930 0.878 0.925 0.921 0.883 0.870 0.843 0.889 0.767 0.784 0.866 0.765 0.806 0.786 0.768 0.757 0.786 0.883 0.749 0.804 0.864 0.744 0.826 0.769 0.744 0.731 0.844 0.895 0.820 0.897 0.881 0.792 0.878 0.867 0.799 0. 0.762 0.830 0.650 0.670 0.789 0.647 0.711 0.675 0.653 0.631 0.723 0.843 0.662 0.747 0.818 0.652 0.775 0.687 0.652 0.633 0.782 0.857 0.752 0.854 0.832 0.718 0.835 0.830 0.723 0.686 0.694 0.779 0.551 0.585 0.721 0.551 0.628 0.590 0.554 0.535 0.911 0.947 0.889 0.912 0.939 0.887 0.918 0.900 0.879 0.884 0.943 0.957 0.939 0.991 0.956 0.934 0.952 0.978 0.936 0. 0.879 0.910 0.817 0.833 0.899 0.817 0.849 0.833 0.819 0.809 MF 0.795 0.879 0.744 0.810 0.868 0.738 0.824 0.763 0.738 0.729 VAE 0.854 0.894 0.838 0.896 0.886 0.833 0.880 0.877 0.835 0.813 0.845 0.915 0.817 0.854 0.903 0.812 0.867 0.832 0.813 0. 0.897 0.925 0.887 0.926 0.922 0.883 0.912 0.905 0.888 0.871 NCF 0.814 0.863 0.713 0.733 0.843 0.710 0.763 0.738 0.715 0.703 0.758 0.819 0.636 0.658 0.782 0.634 0.695 0.662 0.636 0.625 0.926 0.949 0.912 0.923 0.942 0.911 0.920 0.917 0.911 0.911 0.739 0.886 0.737 0.885 0.805 0.671 0.851 0.867 0.675 0. 0.911 0.947 0.877 0.882 0.919 0.876 0.887 0.889 0.877 0.876 0.895 0.925 0.875 0.891 0.916 0.874 0.888 0.881 0.874 0.874 0.657 0.836 0.653 0.831 0.731 0.584 0.792 0.806 0.588 0.573 0.878 0.923 0.833 0.840 0.887 0.833 0.846 0.845 0.833 0.832 0.869 0.905 0.843 0.864 0.892 0.841 0.860 0.849 0.841 0.841 0.597 0.793 0.590 0.788 0.672 0.520 0.744 0.760 0.523 0. 0.851 0.901 0.796 0.805 0.857 0.796 0.813 0.808 0.796 0.795 0.769 0.655 0.826 0.772 0.746 0.825 0.788 0.813 0.825 0.829 2.026 0.711 1.785 0.921 1.319 1.972 0.910 0.882 1.979 2.348 0.738 0.650 0.793 0.782 0.728 0.793 0.771 0.769 0.792 0.796 0.839 0.709 0.897 0.840 0.814 0.895 0.857 0.886 0.894 0.899 2.093 0.785 1.885 1.015 1.487 2.138 0.975 0.965 2.159 2. 0.786 0.688 0.853 0.839 0.780 0.853 0.827 0.832 0.852 0.856 0.885 0.751 0.939 0.886 0.864 0.936 0.902 0.930 0.934 0.940 2.103 0.849 1.927 1.077 1.566 2.232 1.016 1.000 2.249 2.610 0.819 0.719 0.892 0.877 0.820 0.892 0.866 0.874 0.891 0.895 0.687 0.811 0.643 0.714 0.762 0.638 0.778 0.678 0.638 0.608 0.716 0.825 0.687 0.809 0.750 0.624 0.844 0.805 0.631 0. 0.734 0.828 0.655 0.673 0.768 0.652 0.730 0.689 0.657 0.621 0.597 0.748 0.549 0.630 0.694 0.545 0.704 0.581 0.546 0.512 0.637 0.770 0.600 0.754 0.676 0.536 0.788 0.744 0.543 0.500 0.657 0.772 0.559 0.580 0.696 0.556 0.640 0.594 0.562 0.522 0.532 0.703 0.476 0.567 0.635 0.475 0.641 0.506 0.476 0.442 0.577 0.725 0.538 0.710 0.617 0.479 0.739 0.697 0.484 0. 0.599 0.726 0.481 0.512 0.635 0.481 0.576 0.519 0.486 0.448 Barkan, O.; Elisha, Y.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023b. Visual Explanations via Iterated Integrated Attributions. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 20732084. Barkan, O.; Elisha, Y.; Weill, J.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023c. Deep integrated explanations. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 5767. Barkan, O.; Elisha, Y.; Weill, J.; Asher, Y.; Eshel, A.; and Koenigstein, N. 2023d. Stochastic Integrated Explanations for Vision Models. In 2023 IEEE International Conference on Data Mining (ICDM). IEEE. Barkan, O.; Elisha, Y.; Weill, J.; and Koenigstein, N. 2025. BEE: Metric-Adapted Explanations via Baseline ExplorationIn Proceedings of the AAAI Conference on Exploitation. Artificial Intelligence, volume 39, 18351843. Barkan, O.; Fuchs, Y.; Caciularu, A.; and Koenigstein, N. 2020. Explainable recommendations via attentive multipersona collaborative filtering. In Proceedings of the 14th ACM Conference on Recommender Systems, 468473. Barkan, O.; Hirsch, R.; Katz, O.; Caciularu, A.; Weill, J.; and Koenigstein, N. 2021. Cold item integration in deep hybrid recommenders via tunable stochastic gates. In 2021 IEEE International Conference on Data Mining (ICDM), 994999. IEEE. Barkan, O.; Katz, O.; and Koenigstein, N. 2020. Neural attentive multiview machines. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 33573361. IEEE. Barkan, O.; Koenigstein, N.; Yogev, E.; and Katz, O. 2019. CB2CF: neural multiview content-to-collaborative filtering model for completely cold item recommendations. In Proceedings of the 13th ACM Conference on Recommender Systems, 228236. Barkan, O.; Shaked, T.; Fuchs, Y.; and Koenigstein, N. 2023e. Modeling users heterogeneous taste with diversified attentive user profiles. User Modeling and User-Adapted Interaction, 131. Brunot, L.; Canovas, N.; Chanson, A.; Labroche, N.; and Verdeaux, W. 2022. Preference-based and local post-hoc explanations for recommender systems. Information Systems, 108: 102021. Cheng, W.; Shen, Y.; Huang, L.; and Zhu, Y. 2019. Incorporating interpretability into latent factor models via fast influence analysis. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 885893. Dror, G.; Koenigstein, N.; Koren, Y.; and Weimer, M. 2012. The yahoo! music dataset and kdd-cup11. In Proceedings of KDD Cup 2011, 318. PMLR. Elisha, Y.; Barkan, O.; and Koenigstein, N. 2024. Probabilistic Path Integration with Mixture of Baseline Distributions. In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management, 570580. Enguehard, J. 2023. Sequential integrated gradients: simple In but effective method for explaining language models. Rogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Findings of the Association for Computational Linguistics: ACL 2023, 75557565. Toronto, Canada: Association for Computational Linguistics. Erion, G.; Janizek, J. D.; Sturmfels, P.; Lundberg, S. M.; and Lee, S.-I. 2021. Improving performance of deep learning models with axiomatic attribution priors and expected gradients. Nature machine intelligence, 3(7): 620631. Fan, W.; Zhao, X.; Chen, X.; Su, J.; Gao, J.; Wang, L.; Liu, Q.; Wang, Y.; Xu, H.; Chen, L.; et al. 2022. comprehensive survey on trustworthy recommender systems. arXiv preprint arXiv:2209.10117. Gaiger, K.; Barkan, O.; Tsipory-Samuel, S.; and Koenigstein, N. 2023. Not All Memories Created Equal: Dynamic User Representations for Collaborative Filtering. IEEE Access, 11. Gurevitch, L.; Bogina, V.; Barkan, O.; Schein, Y.; Elisha, Y.; and Koenigstein, N. 2025. LXR: Learning to eXplain Recommendations. ACM Transactions on Recommender Systems. Harper, F. M.; and Konstan, J. A. 2015. The movielens datasets: History and context. Acm transactions on interactive intelligent systems (tiis), 5(4): 119. Haug, J.; Zurn, S.; El-Jiz, P.; and Kasneci, G. 2021. On arXiv preprint baselines for local feature attributions. arXiv:2101.00905. He, X.; Deng, K.; Wang, X.; Li, Y.; Zhang, Y.; and Wang, M. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, 639648. He, X.; Liao, L.; Zhang, H.; Nie, L.; Hu, X.; and Chua, T.-S. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web, 173182. Kang, W.-C.; and McAuley, J. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM), 197206. IEEE. Kapishnikov, A.; Venugopalan, S.; Avci, B.; Wedin, B.; Terry, M.; and Bolukbasi, T. 2021. Guided integrated gradients: An adaptive path method for removing noise. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 50505058. Katz, O.; Barkan, O.; Koenigstein, N.; and Zabari, N. 2022. Learning to ride buy-cycle: hyper-convolutional model for next basket repurchase recommendation. In Proceedings of the 16th ACM Conference on Recommender Systems, 316 326. Koenigstein, N. 2025. Without Fidelity, Explanations Are Just Stories: Rethinking Evaluation in Explainable Recommender Systems. SSRN (September 26, 2025). Koh, P. W.; and Liang, P. 2017. Understanding black-box predictions via influence functions. In International conference on machine learning, 18851894. PMLR. Koren, Y.; Bell, R.; and Volinsky, C. 2009. Matrix factorization techniques for recommender systems. Computer, 42(8): 3037. Kunkel, J.; Donkers, T.; Michael, L.; Barbu, C.-M.; and Ziegler, J. 2019. Let me explain: Impact of personal and impersonal explanations on trust in recommender systems. In Proceedings of the 2019 CHI conference on human factors in computing systems, 112. Li, L.; Zhang, Y.; and Chen, L. 2021. Personalized Transformer for Explainable Recommendation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 49474957. Liang, D.; Krishnan, R. G.; Hoffman, M. D.; and Jebara, T. 2018. Variational autoencoders for collaborative filtering. In Proceedings of the 2018 world wide web conference, 689 698. Lundberg, S.; and Lee, S.-I. 2017a. unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 4765-4774. Lundberg, S. M.; and Lee, S.-I. 2017b. unified approach to interpreting model predictions. Advances in neural information processing systems, 30. Melchiorre, A. B.; Rekabsaz, N.; Ganhor, C.; and Schedl, M. 2022. ProtoMF: Prototype-based Matrix Factorization for Effective and Explainable Recommendations. In Sixteenth ACM Conference on Recommender Systems (RecSys 22), 11. Seattle, WA, USA: ACM. Mohammadi, A. R.; Peintner, A.; Muller, M.; and Zangerle, E. 2025. Beyond Top-1: Addressing Inconsistencies in Evaluating Counterfactual Explanations for Recommender Systems. In Proceedings of the Nineteenth ACM Conference on Recommender Systems, 515520. Nobrega, C.; and Marinho, L. 2019. Towards explaining recommendations through local surrogate models. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, 16711678. Rendle, S.; Krichene, W.; Zhang, L.; and Koren, Y. 2022. Revisiting the performance of ials on item recommendation benchmarks. In Proceedings of the 16th ACM Conference on Recommender Systems, 427435. Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why should trust you? Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135 1144. Samek, W.; Binder, A.; Montavon, G.; Lapuschkin, S.; and Muller, K.-R. 2016. Evaluating the visualization of what deep neural network has learned. IEEE transactions on neural networks and learning systems, 28(11): 26602673. Sanyal, S.; and Ren, X. 2021. Discretized Integrated Gradients for Explaining Language Models. In Moens, M.-F.; Huang, X.; Specia, L.; and Yih, S. W.-t., eds., Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 1028510299. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics. Shenbin, I.; Alekseev, A.; Tutubalina, E.; Malykh, V.; and Nikolenko, S. I. 2020. Recvae: new variational autoencoder for top-n recommendations with implicit feedback. In Proceedings of the 13th international conference on web search and data mining, 528536. Shrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learning important features through propagating activation differences. In International conference on machine learning, 31453153. PMlR. Sikdar, S.; Bhattacharya, P.; and Heese, K. 2021. Integrated directional gradients: Feature interaction attribution for neural NLP models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 865878. Singh, R. H.; Maurya, S.; Tripathi, T.; Narula, T.; and Srivastav, G. 2020. Movie recommendation system using cosine similarity and KNN. International Journal of Engineering and Advanced Technology, 9(5): 556559. Sturmfels, P.; Lundberg, S.; and Lee, S.-I. 2020. Visualizing the Impact of Feature Attribution Baselines. Distill. Https://distill.pub/2020/attribution-baselines. Sugahara, K.; and Okamoto, K. 2024. Hierarchical matrix factorization for interpretable collaborative filtering. Pattern Recognition Letters, 180: 99106. Sundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic attribution for deep networks. In International conference on machine learning, 33193328. PMLR. Tintarev, N. 2025. Measuring Explanation QualityA Path Forward. In ECAI 2025, 2229. IOS Press. Tintarev, N.; and Masthoff, J. 2015. Explaining recommendations: Design and evaluation. In Recommender systems handbook, 353382. Springer. Tintarev, N.; and Masthoff, J. 2022. Beyond explaining single item recommendations. Recommender Systems Handbook, 711756. Tran, K. H.; Ghazimatin, A.; and Saha Roy, R. 2021. Counterfactual explanations for neural recommenders. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 1627 1631. Varasteh, M.; McKinnie, E.; Aird, A.; Acuna, D.; and Burke, R. 2024. Comparative Explanations for Recommendation: Research Directions. IntRS24: Joint Workshop on Interfaces and Human Decision Making for Recommender Systems. Vig, J.; Sen, S.; and Riedl, J. 2009. Tagsplanations: explaining recommendations using tags. In Proceedings of the 14th international conference on Intelligent user interfaces, 47 56. Wang, N.; Wang, H.; Jia, Y.; and Yin, Y. 2018. Explainable recommendation via multi-task learning in opinionated text data. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, 165174. Xu, S.; Venugopalan, S.; and Sundararajan, M. 2020. Attribution in scale and space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 96809689. Zhang, Y.; Chen, X.; et al. 2020. Explainable recommendation: survey and new perspectives. Foundations and Trends in Information Retrieval, 14(1): 1101. Zhang, Y.; Lai, K.; Zhang, W.; Zhang, Y.; Liu, Y.; and Ma, S. 2014. Explicit factor models for explainable recommendation based on phrase-level sentiment analysis. In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. Zhong, J.; and Negre, E. 2022. Shap-enhanced counterfactual explanations for recommendations. In Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing, 1365 1372."
        }
    ],
    "affiliations": [
        "Tel Aviv University, Israel",
        "The Open University, Israel"
    ]
}