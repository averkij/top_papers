{
    "paper_title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation",
    "authors": [
        "Yang Xiao",
        "Gen Li",
        "Kaiyuan Deng",
        "Yushu Wu",
        "Zheng Zhan",
        "Yanzhi Wang",
        "Xiaolong Ma",
        "Bo Hui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 ] . [ 1 7 6 3 5 0 . 0 1 5 2 : r a"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "LIGHTCACHE: MEMORY-EFFICIENT, TRAINING-FREE ACCELERATION FOR VIDEO GENERATION Yang Xiao University of Tulsa Gen Li Clemson University Kaiyuan Deng The University of Arizona Yushu Wu Northeastern University Zheng Zhan Microsoft Research Yanzhi Wang Northeastern University Xiaolong Ma The University of Arizona Bo Hui University of Tulsa Figure 1: Accelerating AnimateDiff-Lightning and Stable-Video-Diffusion-Img2vid-XT by 1.59 and 2.86, while reducing memory usage by 8.0 GB and 1.4 GB, respectively."
        },
        {
            "title": "ABSTRACT",
            "content": "Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stagespecific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache."
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Figure 2: Highly similar feature map between all up-sampling and down-sampling layers"
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models have recently received significant global attention due to their impressive generative capabilities Croitoru et al. (2023); Yang et al. (2023); Rombach et al. (2022). These models have demonstrated remarkable efficacy in various applications, like the generation of images Peebles & Xie (2023); Ho et al. (2020); Song et al. (2020), language Hoogeboom et al. (2021); Austin et al. (2021); Yang et al. (2025), and video Blattmann et al. (2023); Guo et al. (2023); Yang et al. (2024). However, due to the problems of high computational time cost and large memory usage, researchers are often seriously restricted to their implementation and deployment in the real world Ma et al. (2024a). Especially in the video generation task process, the model must simultaneously perform denoising action on all frames, leading to high GPU memory usage and long inference latency Xing et al. (2024). To solve this problem, existing studies have attempted to achieve acceleration through training Luo et al. (2023); Lyu et al. (2022); Yin et al. (2024); Wang et al. (2023); Kim et al. (2023); Ma et al. (2024a); Wimbauer et al. (2024); Agarwal et al. (2024) and training-free methods Ma et al. (2024b); Zhan et al. (2024); Yu et al. (2023); Li et al. (2023b;a). Training methods require lot of training resources and may have negative impact on the generation performance of the original model Li et al. (2023c); Ma et al. (2024b); Yang et al. (2023); Ma et al. (2024a); Zhou et al. (2025). In contrast, training-free methods for acceleration have focused on two main strategies Ma et al. (2024a); Liu et al. (2025): 1) skip certain time steps in the inference stage Ma et al. (2024b), and 2) decrease the inference overhead per step through methods Zhan et al. (2024); Yu et al. (2023); Li et al. (2023b). The inherent redundancy in the feature maps during the denoising stage provides natural entry point for skipping certain inference steps Li et al. (2023a). Based on our experimental observations and review of existing literature Ma et al. (2024b); Li et al. (2023a); Tian et al. (2025); Li et al. (2023b; 2025), we found that adjacent denoising steps often produce highly similar intermediate representations (See in Figure 2), particularly in the later stages of inference when the signal becomes clearer and most of the noise has been eliminated. This temporal coherence suggests that many feature maps encode overlapping semantic information, thereby enabling the possibility of reducing the number of inference steps without significantly compromising the quality of the generated results. Figure 3: Memory Usage"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Our research is based on cache mechanism that skips the computation of certain feature maps at specific timesteps to accelerate video generation. However, the existence of caching directly leads to surge in GPU memory usage (See in Figure 3). For example, when we applied DeepCache to EasyAnimate (a DiT architecture) using 4 L40 GPUs (45 GB each), we immediately encountered an out-of-memory issue. This happens because model inference differs from model training. During training, the common practice is either data parallelism or model parallelism (tensor/layer parallelism). In data parallelism, full copy of the model is placed on each GPU, while the sample batch is evenly divided, leading to relatively balanced memory usage. In model parallelism, the model is split across GPUs, so memory is also distributed more evenly. However, during model inference/sampling, if we only generate single sample (batch = 1), there is no natural way to distribute the workload as in training. Multi-GPU parallelism can only assign different modules to different GPUs, but the memory usage of each module is not the same. If single module (e.g., U-Net or DiT) already exceeds the capacity of single GPU, then even with multi-GPU parallelism, out-of-memory errors will occur. Therefore, we also hope to reduce the GPU burden of the inference stage. To explore the sources of computation and memory cost, we divide the inference into three stages: encoding, denoising, and decoding stages, and measured the peak memory(See in Figure 3). That is, the memory requirement for diffusion inference depends on the highest memory of the three stages. For example, when we applied the method Zhan et al. (2024) to reduce memory usage, we found that the peak memory did not decrease (We use the default setting in the diffusers library). According to our experiments, this method focuses on the denoising stage (See in Tab. 2), while the peak memory of some models or some experimental settings is in the decoding stage(See in Figure 3). Empirically, regardless of whether DeepCache Ma et al. (2024b) is used for accelerating inference, the memory usage of the encoding stage remains consistently low and unchanged. Therefore, memory optimization efforts should focus on the denoising and decoding stages. We propose the following optimization strategy (See in Figure 4): Chunk: The denoising stage primarily involves the computation of feature maps. We propose that selectively chunking the height and width of feature maps from different layers to varying degrees can effectively reduce memory usage while maintaining generation quality. Asynchronous Swapping: The model offloading function von Platen et al. (2022) in the Diffusers library moves inactive layers or models to the CPU to reduce GPU memory usage. Following similar idea, swapping inactive cached feature maps to the CPU could also alleviate memory pressure. To further mitigate the latency overhead, asynchronous swapping can be employed: feature maps can be transferred between GPU and CPU in the background while the GPU continues computing other parts of the model, thereby overlapping communication and computation. Slicing: The decoding stage can be optimized by splitting large inputs into smaller batches and processing them sequentially. In our case, although we generate only single video, the generative models we adopt are classifier-free guidance models Ho & Salimans (2022). This naturally doubles the number of feature maps (conditional and unconditional) and requires denoising across multiple frames simultaneously (a total of 2 , where is the number of frames). Thus, VAE slicing remains highly applicable."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "2.1 PRELIMINARY Diffusion Forward Process. The core idea of diffusion models Ho et al. (2020) is to remove noise from fully corrupted data in order to recover valid semantic information using series of timesteps. Its essence is to learn the transformation of noise distribution to ground truth distribution. For the data x, we can gradually add Gaussian noise to the data in steps: q(xt xt1) = (xt; (cid:112)1 βtxt1, βtI) (1) where belongs to [0, ], and [β0, ..., βT ] schedule the noise."
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Figure 4: Denoising: The timestep is divided into cache step and normal step. The cache step reuses the cached feature maps stored in the CPU. The normal step chunk the size of feature maps and concatenate them. Decoding: We use VAE to decode the latent frame by frame. Diffusion Reverse Process. To recover the valid semantic information, we train the diffusion model to denoise the randomly sampled noise into the ground truth distribution. We use network (For example, U-Net Ronneberger et al. (2015)) ϵ(xt, t)to predict noise: pθ(xt1 xt) = (xt1; 1 αt (xt 1 αt 1 αt ϵθ(xt, t)), βtI) (2) where αt = 1 βt and αt = (cid:81)T current xt, bringing it close to real data point when we reach x0. i=1 αi. Applied iteratively, it gradually removes the noise of the Feature Calculation U-Net Ronneberger et al. (2015), originally proposed for medical image segmentation, uses skip connections to effectively combine lowand high-level features. Its architecture is built on successive down-sampling and up-sampling blocks that first encode the input into compact representation and then decode it for downstream applications: {Di}m i=0, {Ui}m i=0 (3) Down-sampling is the encoding stage, which extracts the information from the previous layer through the {Di}m i=0. Up-sampling is the decoding stage, which converges the inversion from the previous layer and the skip-connection: Ui = Concat(Di, Ui+1) (4) Therefore, at the heart of U-Net model is concatenation of low-level features from the skip connection and the high-level features from the previous layer. 2.2 ASYNCHRONOUS SWAPPING We introduce DeepCache Ma et al. (2024b), simple yet effective method that exploits temporal redundancy between steps in the diffusion reverse process to accelerate inference, serving as the foundation of our approach. As the cacheing mechanism Smith (1982) in computer system and KVCache in transformer Pope et al. (2023), DeepCache method caches the slowly evolving features to eliminate some unnecessary computations in some steps: cache Dt t , t+1 Dt+1 cache m, Concat(Dt m, m+1) (5) Where or Dt means the m-th up-sampling layer or down-sampling layer at time step t. For example, in the case of an up-sampling layer, we set hyperparameter n, meaning that the feature map is explicitly recalculated once every steps, while in the remaining steps it is reused from the cache: , . . . . There is no additional computational cost to these time steps: {(1, . . . , 1); (n + 1, . . . , 2n 1); . . . }, since it can be simply retrieved from the cached cache : 0 m, 2n m, n"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Figure 5: Violin Figure (AnimateDiff-Light) of Quality Metric: LPIPS(), PSNR(), SSIM() Figure 6: Violin Figure (SVD) of Quality Metric: LPIPS(), PSNR(), SSIM() feature map. Although DeepCache enables feature map reuse for acceleration, the additional cached feature maps lead to substantial increase in memory usage. In the official diffusers library, CPU offloading von Platen et al. (2022) selectively transfers weights between the GPU and CPU. When component is needed, it is moved to the GPU, and when it is not needed, it is offloaded back to the CPU. This method operates on submodules rather than entire models, thereby saving memory by avoiding storing the whole model on the GPU. Inspired by this idea, we store the cached features on the CPU and bring them back when required. This approach does not affect generation quality, though it slightly increases inference time, while significantly reducing GPU memory usage. 2.3 FEATURE CHUNK AND SLICED DECODING So far, we have not modified the fundamental feature process of the diffusion model. The input RBT CHW of (m+1)-th layers is 5-D feature map with dimensions {batch, frames, channels, height, width}, where the batch size is 2, corresponding to the conditional and unconditional feature maps in the classifier-free guidance model. Zhan et al. (2024) found that directly modifying the batch or frame dimensions would break the spatiotemporal continuity of video generation, leading to distorted temporal information extraction. On the other hand, directly modifying the channel dimension would cause misalignment with the model architecture. Therefore, they keep the first three dimensions unchanged and apply chunking only to the spatial dimensions (height and width): Chunk(F m) RBT η ω (6) where η and ω are two hyperparameters. Larger values lead to smaller feature map sizes, which reduce memory consumption but may degrade generation quality. This presents trade-off issue, requiring balance between memory efficiency and output quality. We found that the feature chunk only works at the denoising stage based on the experiments. If the video memory usage of the decoding stage is higher than that of the denoising stage, even if we reduce the memory usage of the denoising stage to 0, the overall peak memory will not change. Previously, we optimized the height and width dimensions of the five dimensions {batch, frames, channels, height, width}. While we cant prune the channel dimension without changing the model architecture, we can consider the batch and frames dimensions: R(BT )CHW (7)"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Method Time (s) Speed Up LPIPS () PSNR () SSIM () Peak Memory Peak Reserved Euler DeepCache (N=2) FME Our (N=2) Our (N=3) Euler DeepCache (N=2) FME Our (N=2) Our (N=3) Our (N=4) Our (N=8) Euler DeepCache (N=2) FME Our (N=2) Our (N=3) Our (N=4) Euler DeepCache (N=2) FME Our (N=2) Euler DeepCache (N=2) FME Our (N=2) Our (N=3) Our (N=5) 3.84 2.87 4.07 2.89 2.41 47.39 28.65 57.23 30.92 24.55 21.27 16.57 33.50 20.67 35.86 22.33 18.09 15. 5.03 3.45 7.55 4.82 78.63 - - 59.88 41.14 25.94 1.00 1.34 0.95 1.33 1.59 1.00 1.65 0.83 1.53 1.93 2.23 2.86 1.00 1.62 0.93 1.50 1.85 2.10 1.00 1.46 0.67 1.04 1.00 - - 1.31 1.91 3.03 AnimateDiff-Light (U-Net) 8 steps - 0.7152 0.4064 0.7152 0.6654 - 0.164 0.609 0.164 0.205 - 28.64 27.81 28.64 28.42 SVDI2V-XT (U-Net) 25 steps - 0.039 0.018 0.031 0.061 0.074 0. - 34.32 37.10 34.33 33.55 32.22 28.84 - 0.9262 0.9649 0.9271 0.9189 0.8837 0.8345 AnimateDiff (U-Net) 50 steps - 0.127 0.556 0.127 0.172 0.243 - 31.75 28.46 31.75 30.55 30.08 - 0.9017 0.5325 0.9017 0.8581 0. TextToVideo (U-Net) 25 steps - 0.226 0.039 0.225 - 29.57 35.17 29.57 - 0.6856 0.8956 0.6842 EasyAnimate (DiT) 25 steps 13562 15200 4868 5559 15346 21416 10148 13937 13937 13937 13937 13564 16843 5898 8583 8583 8583 7334 8157 4048 3939 23430 24744 5688 6238 6238 31084 37570 11552 15964 15964 15964 15964 23944 27576 6688 9908 9908 10814 11592 4724 4606 - - - 0.410 0.548 0.590 - - - 28.39 28.43 28.43 - - - 0.0743 0.0583 0.0555 34884.07 OOM - 29593.33 29593.33 29593.33 36830.00 OOM - 31596.00 31596.00 31596. Table 1: Result: DeepCache leads to surge in memory usage, while FME decreases the speedup, and both may cause quality degradation across different models. In contrast, our approach not only preserves quality but also accelerates inference and reduces memory consumption to below the baseline model. means that the feature map is recalculated once every steps. In the decoding stage, the decoding function will merge {batch, frames} to batch rames. At this time, the model has integrated conditional and unconditional information. For one sample, batch = 1, so it is equivalent to processing all the frames at once: RT CHW (8) Obviously, the amount of GPU memory at this time is extremely large, and the methods we introduced earlier do not involve calculations at this stage. We consider the video as combination of multiple pictures and use VAE slicing von Platen et al. (2022) to decode the video frame by frame. It should be noted that this method turns single computation into multiple computations, but under the acceleration of DeepCache, the time consumed by decoding is negligible. Finally, we merge the all the images ˆIn to get the video: ˆI0:T = {V AE(Fn)}T n=0, Fn RCHW (9)"
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "3.1 ANALYSIS We adopted the default Euler scheduler and compared DeepCache, FME, and our method. For prompt, we use the default prompt (Image and Text) in the huggingface von Platen et al. (2022) to generate the video. All experiments were conducted on four NVIDIA L40S GPUs (45GB each) with batch size of 1. The resolution was set to 512512 for all methods, except for SVD where"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Method Time (s) LPIPS () PSNR () SSIM() Encode Memory Denoise Memory Decode Memory DeepCache Our (N=2) - swapping - slicing - chunk DeepCache Our (N=2) - swapping - slicing - chunk DeepCache Our (N=2) - swapping - slicing - chunk DeepCache Our (N=2) - swapping - slicing - chunk 2.88 2.89 2.57 3.46 3.34 28.69 30.92 30.97 31.03 28.77 20.57 22.33 20.57 22.32 22.21 3.44 4.82 4.49 4.08 3.55 0.164 0.164 0.164 0.164 0. 0.031 0.031 0.031 0.031 0.031 0.127 0.127 0.127 0.127 0.127 0.227 0.225 0.227 0.227 0.227 AnimateDiff-Light 8 steps 0.7152 0.7152 0.7152 0.7152 0.7152 3829.13 252.46 3829.13 252.46 252. SVDI2V-XT 25 steps 0.9262 0.9271 0.9272 0.9272 0.9262 5924.08 2987.84 5924.08 2987.84 2987.84 AnimateDiff 50 steps 0.9017 0.9017 0.9017 0.9017 0.9017 3827.81 255.45 3827.81 255.45 255. TextToVideo 25 steps 0.6842 0.6842 0.6842 0.6842 0.6842 3532.90 659.88 3532.90 659.88 659.88 28.64 28.64 28.64 28.64 28.64 34.33 34.33 34.33 34.33 34.33 31.75 31.75 31.75 31.75 31. 29.57 29.57 29.57 29.57 29.57 6845.11 5559.15 6845.11 5559.15 5559.15 17202.37 13936.91 16639.12 15439.1 16002.1 9864.46 8583.48 9864.46 8583.48 8583.48 5394.76 4279.35 5090.40 4279.35 4582.85 15200.83 4183.40 6115.49 11547.01 4183. 21416.39 9254.12 11863.77 17269.52 9211.18 16842.62 5835.14 7757.28 13198.53 5835.14 8157.18 3545.69 4522.34 4804.40 3539.35 Table 2: Abalation Study: We remove each of the proposed modules and demonstrate the effectiveness of our approach through the memory consumption changes observed across the three stages. 1024576 was used. The number of inference steps was fixed at 8 for AnimateDiff-Light, 25 for StableVideoDiffusion and TextToVideo, and 50 for AnimateDiff. In addition, the earliest version of DeepCache focused only on accelerating U-Net architecture models. Although Fora claims to have accelerated DiT architecture models, the open-source code is incomplete, and Fora is for image generation, so we extend DeepCache to DiT architecture models (EasyAnimate). However, since the original model already had excessively high memory consumption, running DeepCache resulted in out-of-memory (we use 4 L40S, 45GB per GPU) issues. Moreover, FME is not applicable to DiT architectures, so no experimental results are available for either method. In terms of acceleration, DeepCache (N=2) achieves speed-up increase from 34% to 65% over the baseline, while Ours (N=2) achieves an increase from 4% to 53%. In contrast, FME does not provide acceleration and may even lead to slowdown. Beyond acceleration, we further evaluate peak memory usage to understand the trade-offs introduced by different methods. The peak memory usage of DeepCache increases the most, by approximately 11% to 40%. Furthermore, as the number of generated frames increases, the peak memory consumption grows proportionallyfor example, at 50 frames it reaches 30026MB, an increase of about 96%. FME markedly reduces peak memory by approximately 34% to 64%, with change in speed-up from -33% to -5%, but leads to substantial loss in generation quality. Our method provides more balanced trade-off: maintaining quality (LPIPS, SSIM, and PSNR) close to DeepCache (More cases can be seen in A), while confirming that the acceleration does not come at the expense of perceptual quality and achieving faster speedslightly lower than DeepCachebut with peak memory usage significantly lower than both the baseline and DeepCache. Due to the presence of cached feature maps, our peak memory is higher than that of FME. Overall, our approach consistently outperforms the baseline, DeepCache, and FME, offering the most favorable balance across speed, quality, and memory (see Table 1). 3.2 ABLATION STUDIES In this section, we conduct ablation studies to evaluate the effectiveness of our proposed method. We first analyze the contribution of each module to peak memory reduction by selectively removing them and measuring their impact across different stages of the pipeline, then investigate whether our approach depends on the choice of sampling scheduler by comparing it with several alternatives."
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Method DDIM + Ours(N=2) PNDM + Ours(N=2) Euler* + Ours(N=2) Time (s) 58.07 37.22 60.32 35.50 47.39 30.92 Speed Up LPIPS () PSNR () - 0.264 - 0.505 - 0.031 1.00 1.56 1.00 1.699 1.00 1.53 - 31.72 - 31.13 - 34.33 SSIM () Peak Memory Peak Reserved - 0.7843 - 0.6344 - 0.9271 15789 14078 15352 14133 15346 13937 27626 18452 27626 20146 31084 15964 Table 3: Different sampler on SVDI2V-XT, * means this sampler is the official default sampler. Three Modules To thoroughly investigate the impact of our proposed three modules on the overall peak memory usage, we designed series of ablation experiments by selectively removing each module to isolate its contribution. Specifically, we first evaluated the baseline case using only DeepCache, and we observed that peak memory was mainly dominated by the denoising and decoding stages. Then we implemented our method and found notable improvement, that the peak memory consumption decreased simultaneously across all three stages, demonstrating the collective effectiveness of the modules in reducing memory demand. To further compare the contribution of each module, we removed them individually and measured peak memory across the three stages. The results show that: 1) Removing swapping significantly increased peak memory in all three stages. Swapping, which is built upon the CPU offloading functionality of the diffusers library, involves transferring both model weights and cached feature maps. Its effect spans all stages. 2) Removing slicing significantly increased peak memory in the decoding stage. This is because VAE slicing processes frames sequentially rather than all at once during decoding. 3) Removing chunking significantly increased peak memory in the denoising stage. This is because only chunking is applied to feature maps specifically during the denoising process (See in Table 2). Scheduler We consistently adopted the official default sampling scheduler. In the ablation experiments, we also included DDIM and PNDM. The baseline runtime varied slightly across different schedulers, and after applying our method, the quality degradation also differed depending on the scheduler. Among them, the default Euler scheduler achieved the best performance on time cost, LPIPS, PSNR, and SSIM (See in Table 3). These experiments demonstrate that our method is not constrained by the choice of scheduler. 3.3 VISUALIZATION We compared the frame-by-frame quality differences of videos generated by each method against the original model. Among the three metrics, PSNR and SSIM are more reflective of low-level pixel and structural similarity, while LPIPS better aligns with human perception. The results are visualized using violin plots, which show that LightCache maintains consistency with DeepCache, whereas FME produces unstable outcomes (See in Figure 5 and 6)."
        },
        {
            "title": "4 RELATED WORKS",
            "content": "4.1 TRAINING-FREE ACCELERATION OF DIFFUSION MODELS. Training-free acceleration methods aim to improve the inference efficiency of diffusion models without modifying their parameters or requiring additional training. These approaches typically exploit structural redundancies or adaptively skip computation steps during the sampling process Ma et al. (2024b); Yu et al. (2023); Zhan et al. (2024); Li et al. (2023a); Tian et al. (2025); Zhou et al. (2025). The reverse process of diffusion models inherently involves the step-by-step reconstruction of images or videos Ho et al. (2020). However, certain intermediate states, such as latent representations and attention maps, contain natural redundancy or sparsity, which provides the foundation for training-free acceleration strategies Ma et al. (2024b). These methods include: 1) Inferenceskipping, where redundant steps with minimal change or intermediate activations with large zero regions are omitted to reduce unnecessary computations Ma et al. (2024b); Tian et al. (2025); Yu et al. (2023); Zhou et al. (2025). 2) Computation graph optimization, where inference engines (e.g., ONNX Runtime, TensorRT, TVM) restructure the execution graph to eliminate redundant operations and reduce memory access overhead while preserving mathematical equivalence. 3) Model"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Figure 7: 4 cases for Animate-Light and SVD-XT quantization, where model weights and activations are quantized from high to low precision, reducing memory usage and accelerating computation without retraining Siddegowda et al. (2022); Wang et al. (2025); Zhao et al. (2025; 2024a); Shang et al. (2023). In general, training-free approaches often require careful trade-off between speed improvement, memory savings, and acceptable levels of accuracy degradation. 4.2 CACHE IN DIFFUSION MODELS. Cache temporarily stores parts of main memory that are expected to be accessed again soon Smith (1982). Recently, lot of work has applied the cache mechanism in diffusion models Ma et al. (2024b); Agarwal et al. (2024); Wimbauer et al. (2024); Ma et al. (2024a); Zhou et al. (2025); Liu et al. (2025). By carefully analyzing the models computation workflow, inference efficiency can be significantly improved. One effective strategy is to reuse intermediate or final representations generated during earlier timesteps of computation. This approach avoids redundant recalculations, which not only accelerates the overall inference process but also reduces computational resource consumption and latency. Ma et al. (2024b); Wimbauer et al. (2024); Zhou et al. (2025) leverage the temporal redundancy in the reverse denoising process of diffusion models by caching and reusing features from specific layers, thereby avoiding redundant computations and accelerating the generation process. Agarwal et al. (2024) significantly reduces computational cost and latency by reusing intermediate noise states from the diffusion process, allowing new requests to skip portion of the denoising steps and condition on cached intermediate states generated from similar previously processed text prompts. FreeDoM leverages energy functions to provide guidance during the diffusion sampling process via energy gradients Yu et al. (2023). EasyCache uses dynamic threshold to determine when it is safe to reuse cached results Zhou et al. (2025). PAB leverages the Ushaped variation pattern of attention outputs in DiT models during the diffusion process, and reuses the stable-phase attention features through pyramid-style broadcasting strategy, thereby significantly reducing redundant computations Zhao et al. (2024b). FORA reduces redundant computation by caching and reusing the intermediate features of the Attention and MLP layers at fixed intervals Selvaraju et al. (2024). These advancements demonstrate the growing potential of cache-based strategies to improve the efficiency and scalability of diffusion model inference."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we proposed LightCache, training-free framework that achieves low-memory and accelerated video generation without compromising output quality. By analyzing the inference process across encoding, denoising, and decoding stages, we introduced three complementary strategies: Asynchronous swapping, Feature chunk, and VAE slicing. Compared to baseline model and DeepCache, and FME, these stage-specific optimizations substantially reduce peak memory usage while keeping the additional time overhead well below the acceleration gains. In future work, we plan to explore combining LightCache with complementary training-free methods, as well as ex-"
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "tending it to DiT models, longer video sequences and multi-modal generation tasks where memory efficiency and inference speed are critical."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "This work does not involve human subjects, personally identifiable data, or sensitive information. We believe it does not raise ethical concerns."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "We have made our code available in the anonymous project. Detailed hyperparameters, inference procedures, and data processing steps are provided in the code file. Random seeds are fixed for reproducibility, and all results have been verified across multiple independent runs."
        },
        {
            "title": "REFERENCES",
            "content": "Shubham Agarwal, Subrata Mitra, Sarthak Chakraborty, Srikrishna Karanam, Koyel Mukherjee, and Shiv Kumar Saini. Approximate caching for efficiently serving {Text-to-Image} diffusion models. In 21st USENIX Symposium on Networked Systems Design and Implementation (NSDI 24), pp. 11731189, 2024. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9): in vision: survey. 1085010869, 2023. Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling. Argmax flows and multinomial diffusion: Learning categorical distributions. Advances in neural information processing systems, 34:1245412465, 2021. Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and Shinkook Choi. On architectural compression of text-to-image diffusion models. 2023. Lijiang Li, Huixia Li, Xiawu Zheng, Jie Wu, Xuefeng Xiao, Rui Wang, Min Zheng, Xin Pan, Fei Chao, and Rongrong Ji. Autodiffusion: Training-free optimization of time steps and architectures for automated diffusion model acceleration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 71057114, 2023a. Senmao Li, Taihang Hu, Fahad Shahbaz Khan, Linxuan Li, Shiqi Yang, Yaxing Wang, Ming-Ming Cheng, and Jian Yang. Faster diffusion: Rethinking the role of unet encoder in diffusion models. CoRR, 2023b."
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Senmao Li, Lei Wang, Kai Wang, Tao Liu, Jiehang Xie, Joost van de Weijer, Fahad Shahbaz Khan, Shiqi Yang, Yaxing Wang, and Jian Yang. One-way ticket: Time-independent unified encoder for distilling text-to-image diffusion models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2356323574, 2025. Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys, Yun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snapfusion: Text-to-image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36:2066220678, 2023c. Feng Liu, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, and Fang Wan. Timestep embedding tells: Its time to cache for video diffusion model. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7353 7363, 2025. Simian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolinario Passos, Longbo Huang, Jian Li, and Hang Zhao. Lcm-lora: universal stable-diffusion acceleration module. arXiv preprint arXiv:2311.05556, 2023. Zhaoyang Lyu, Xudong Xu, Ceyuan Yang, Dahua Lin, and Bo Dai. Accelerating diffusion models via early stop of the diffusion process. arXiv preprint arXiv:2205.12524, 2022. Xinyin Ma, Gongfan Fang, Michael Bi Mi, and Xinchao Wang. Learning-to-cache: Accelerating diffusion transformer via layer caching. Advances in Neural Information Processing Systems, 37: 133282133304, 2024a. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1576215772, 2024b. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 41954205, 2023. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of machine learning and systems, 5:606624, 2023. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computerassisted intervention, pp. 234241. Springer, 2015. Pratheba Selvaraju, Tianyu Ding, Tianyi Chen, Ilya Zharkov, and Luming Liang. Fora: Fast-forward caching in diffusion transformer acceleration. arXiv preprint arXiv:2407.01425, 2024. Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization on diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 19721981, 2023. Sangeetha Siddegowda, Marios Fournarakis, Markus Nagel, Tijmen Blankevoort, Chirag Patel, and Abhijit Khobare. Neural network quantization with ai model efficiency toolkit (aimet). arXiv preprint arXiv:2201.08442, 2022. Alan Jay Smith. Cache memories. ACM Computing Surveys (CSUR), 14(3):473530, 1982. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, Yunhai Tong, Ling Yang, and Bin Cui. Training-free diffusion acceleration with bottleneck sampling. arXiv preprint arXiv:2503.18940, 2025."
        },
        {
            "title": "Published as a preprint paper at ArXiv",
            "content": "Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, Dhruv Nair, Sayak Paul, William Berman, Yiyi Xu, Steven Liu, and https://github.com/ Thomas Wolf. Diffusers: State-of-the-art diffusion models. huggingface/diffusers, 2022. Juncheng Wang, Chao Xu, Cheng Yu, Lei Shang, Zhe Hu, Shujun Wang, and Liefeng Bo. Synchronized video-to-audio generation via mel quantization-continuum decomposition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 31113120, 2025. Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang Wang, Weizhu Chen, Mingyuan Zhou, et al. Patch diffusion: Faster and more data-efficient training of diffusion models. Advances in neural information processing systems, 36:7213772154, 2023. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: AcceleratIn Proceedings of the IEEE/CVF Conference on ing diffusion models through block caching. Computer Vision and Pattern Recognition, pp. 62116220, 2024. Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 57(2):142, 2024. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56(4):139, 2023. Ling Yang, Ye Tian, Bowen Li, Xinchen Zhang, Ke Shen, Yunhai Tong, and Mengdi Wang. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 66136623, 2024. Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energy-guided conditional diffusion model. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2317423184, 2023. Zheng Zhan, Yushu Wu, Yifan Gong, Zichong Meng, Zhenglun Kong, Changdi Yang, Geng Yuan, Pu Zhao, Wei Niu, and Yanzhi Wang. Fast and memory-efficient video diffusion using streamlined inference. arXiv preprint arXiv:2411.01171, 2024. Maosen Zhao, Pengtao Chen, Chong Yu, Yan Wen, Xudong Tan, and Tao Chen. Pioneering 4-bit fp quantization for diffusion models: Mixup-sign quantization and timestep-aware fine-tuning. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1813418143, 2025. Tianchen Zhao, Xuefei Ning, Tongcheng Fang, Enshu Liu, Guyue Huang, Zinan Lin, Shengen Yan, Guohao Dai, and Yu Wang. Mixdq: Memory-efficient few-step text-to-image diffusion models with metric-decoupled mixed precision quantization. In European Conference on Computer Vision, pp. 285302. Springer, 2024a. Xuanlei Zhao, Xiaolong Jin, Kai Wang, and Yang You. Real-time video generation with pyramid attention broadcast. arXiv preprint arXiv:2408.12588, 2024b. Xin Zhou, Dingkang Liang, Kaijin Chen, Tianrui Feng, Xiwu Chen, Hongkai Lin, Yikang Ding, Feiyang Tan, Hengshuang Zhao, and Xiang Bai. Less is enough: Training-free video diffusion acceleration via runtime-adaptive caching. arXiv preprint arXiv:2507.02860, 2025."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 LLM USAGE STATEMENT Large language models (LLMs) were employed to assist in refining the clarity, grammar, and style of the manuscript. The use of LLMs was limited to language polishing and did not contribute to the generation of substantive ideas, analyses, or results. All scientific content, interpretations, and conclusions are the authors own."
        }
    ],
    "affiliations": [
        "Clemson University",
        "Microsoft Research",
        "Northeastern University",
        "The University of Arizona",
        "University of Tulsa"
    ]
}