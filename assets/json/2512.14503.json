{
    "paper_title": "RecGPT-V2 Technical Report",
    "authors": [
        "Chao Yi",
        "Dian Chen",
        "Gaoyang Guo",
        "Jiakai Tang",
        "Jian Wu",
        "Jing Yu",
        "Mao Zhang",
        "Wen Chen",
        "Wenjun Yang",
        "Yujie Luo",
        "Yuning Jiang",
        "Zhujin Gao",
        "Bo Zheng",
        "Binbin Cao",
        "Changfa Wu",
        "Dixuan Wang",
        "Han Wu",
        "Haoyi Hu",
        "Kewei Zhu",
        "Lang Tian",
        "Lin Yang",
        "Qiqi Huang",
        "Siqi Yang",
        "Wenbo Su",
        "Xiaoxiao He",
        "Xin Tong",
        "Xu Chen",
        "Xunke Xi",
        "Xiaowei Huang",
        "Yaxuan Wu",
        "Yeqiu Yang",
        "Yi Hu",
        "Yujin Yuan",
        "Yuliang Yan",
        "Zile Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards. To address these challenges, we present RecGPT-V2 with four key innovations. First, a Hierarchical Multi-Agent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, a Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility."
        },
        {
            "title": "Start",
            "content": "2025-12-17 RecGPT-V2 Technical Report"
        },
        {
            "title": "RecGPT Team",
            "content": "Large language models (LLMs) have demonstrated remarkable potential in transforming recommender systems from implicit behavioral pattern matching to explicit intent reasoning. While RecGPT-V1 successfully pioneered this paradigm by integrating LLM-based reasoning into user interest mining and item tag prediction, it suffers from four fundamental limitations: (1) computational inefficiency and cognitive redundancy across multiple reasoning routes; (2) insufficient explanation diversity in fixed-template generation; (3) limited generalization under supervised learning paradigms; and (4) simplistic outcome-focused evaluation that fails to match human standards. To address these challenges, we present RecGPT-V2 with four key innovations. First, Hierarchical MultiAgent System restructures intent reasoning through coordinated collaboration, eliminating cognitive duplication while enabling diverse intent coverage. Combined with Hybrid Representation Inference that compresses user-behavior contexts, our framework reduces GPU consumption by 60% and improves exclusive recall from 9.39% to 10.99%. Second, Meta-Prompting framework dynamically generates contextually adaptive prompts, improving explanation diversity by +7.3%. Third, constrained reinforcement learning mitigates multi-reward conflicts, achieving +24.1% improvement in tag prediction and +13.0% in explanation acceptance. Fourth, an Agent-as-a-Judge framework decomposes assessment into multi-step reasoning, improving human preference alignment. Online A/B tests on Taobao demonstrate significant improvements: +2.98% CTR, +3.71% IPV, +2.19% TV, and +11.46% NER. RecGPT-V2 establishes both the technical feasibility and commercial viability of deploying LLM-powered intent reasoning at scale, bridging the gap between cognitive exploration and industrial utility. 5 2 0 2 6 1 ] . [ 1 3 0 5 4 1 . 2 1 5 2 : r Figure 1 Comparison between RecGPT-V2 and RecGPT-V1: (a) online A/B performance in Taobaos Guess What You Like; (b) 60% compute savings via optimization pipeline; and (c) Superior timeliness in capturing seasonal trends, exemplified by PVR trends for Halloween and Winter-related products. 2025 Taobao. All rights reserved RecGPT-V2 Technical Report"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Agentic Intent Reasoning 2.1 Hybrid Representation Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 6 2.2 Hierarchical Multi-Agent System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3 Dynamic Explanation Generation 17 3.1 Meta-Prompting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 Preference-Aware Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . 4 Agentic Judge Framework 20 4.1 Agent-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 4.2 Judge-as-a-Reward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Experiments",
            "content": "23 5.1 Online A/B Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 5.2 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "C Implementation Details",
            "content": "26 26 28 29 29 RecGPT-V2 Technical Report 1. Introduction Recommender systems have evolved significantly over the past two decades, progressing from matrix factorization (Koren et al., 2009) to deep neural networks (Tang et al., 2025). Despite these advances, contemporary industrial systems remain fundamentally constrained by their reliance on historical behavioral patterns and log-fitting objectives, optimizing for behavioral pattern matching without explicitly reasoning about underlying user intent. To address these issues, RecGPT-V1 (Yi et al., 2025) emerged as paradigm-shifting framework that elevates user intent from implicit behavioral signals to explicit reasoning objectives. By integrating large language models (LLMs) into key stages, such as user interest mining and item tag prediction, RecGPT-V1 transforms traditional pattern matching into an intent-centric recommendation objective grounded in semantic understanding and logical reasoning. This analytical reasoning paradigm enables the decomposition of complex recommendation tasks into interpretable and modular stages, facilitating transparent and controllable mapping from user intent understanding to item relevance prediction. While RecGPT-V1 successfully leverages LLM-based knowledge and reasoning to improve recommendation quality and demonstrates promising online performance in industrial deployment, it still exhibits several limitations that hinder its scalability, efficiency, and effectiveness: Limitation 1: Computational inefficiency and redundant intent reasoning in multi-route architectures. RecGPT-V1 adopts multi-route LLM-based channel1 in which multiple LLM-based reasoning routes independently analyze user intent and retrieve item candidates. Although this architecture broadens the semantic and contextual scope of user modeling, it exhibits substantial redundancy in both representation encoding and cognitive processes. At the representation level, each route encodes the full user behaviors sequence (averaging 32K tokens) even though only small subset is relevant to the current intent prediction, leading to excessive computational overhead from repeatedly processing long sequences. At the cognitive level, different routes may demonstrate redundant reasoning outputs, generating overlapping recommendation candidates with an inter-route duplication rate reaching 13.46%. Collectively, these inefficiencies result in significant computational waste and limit the overall scalability of RecGPT-V1 in large-scale industrial recommender systems. Limitation 2: Insufficient explanation diversity in the manner of fixed prompt templates. RecGPT-V1 employs fixed prompt templates to generate recommendation explanations by combining user interests and item attributes. This static approach produces homogeneous explanations that fail to capture the multi-dimensional and dynamic nature of personalized user needs. The templates cannot adaptively incorporate real-time contextual signals, resulting in generic explanations with limited personalization that struggle to engage users across diverse scenarios. Limitation 3: Supervised learning on static data limits generalization in complex generation tasks. RecGPT-V1 relies on supervised fine-tuning over curated high-quality corpora to learn recommendation-oriented generation tasks. This paradigm facilitates efficient transfer of human expertise but also anchors the model to fixed data distributions and explicit objective signals. In real-world recommendation scenarios, user needs evolve dynamically and involve multiple, interacting objectives with diverse operational constraints such as diversity, novelty, and relevance. Learning on static corpora cannot adequately capture these dynamic dependencies, resulting in limited generalization and unstable performance in multi-objective and multi-constraint generation tasks. Limitation 4: Simplistic outcome-focused evaluation in LLM-as-a-Judge. RecGPT-V1 employs LLM-as-a-Judge for one-shot outcome evaluation, training the judge to directly predict quality scores 1Following the initial RecGPT-V1 deployment, we extended the cognitive channel into multiple LLM-based retrieval routes, each specialized in leveraging different contextual and side information (e.g., weather, trending events, seasonal factors) to enhance semantic coverage and situational awareness in intent reasoning. 3 RecGPT-V2 Technical Report Figure 2 Overview of RecGPT-V2 architecture. from data-label pairs. This result-oriented training paradigm collapses the multi-dimensional and multi-step reasoning inherent in human evaluation into single prediction objective. By overlooking the intermediate reasoning steps that human evaluators employ to assess quality across multiple criteria (e.g., relevance, diversity, coherence), this approach limits the judges ability to capture nuanced quality distinctions and reduces alignment with human evaluation standards. To address these fundamental challenges, we present RecGPT-V2, which introduces three key architectural and algorithmic innovations: Agentic Intent Reasoning. To mitigate computational inefficiency and cognitive redundancy in multi-route architectures, we propose Hierarchical Multi-Agent System (HMAS) that restructures LLM-based intent reasoning through coordinated multi-agent collaboration. By integrating multisource environmental signals (e.g., trending events, weather patterns) into specialized expert agents, HMAS enables complementary reasoning across diverse contextual dimensions while eliminating cognitive duplication, increasing the exclusive recall from 9.39% to 10.99%. To support efficient industrial deployment, we introduce Hybrid Representation Inference that compresses user-behavior tokens from 32K to 11K through atomized entity encoding, along with complementary Infrastructure Engineering Optimizations. Together, these techniques improve MFU by +53.7% and reduce GPU consumption by 60.0%, enabling scalable deployment without sacrificing reasoning quality. Dynamic Explanation Generation. To overcome the limitations of fixed prompt templates, we utilize Meta-Prompting technique for dynamic recommendation explanation generation. By synthesizing user interests, item attributes, and real-time contextual signals (e.g., weather, seasonal events, trending topics), the meta-prompt generator autonomously constructs task-specific instruction templates that adapt to diverse scenarios and content characteristics. Compared to RecGPT-V1s static templates, our framework improves explanation diversity by +7.3%, demonstrating that adaptive prompt engineering effectively enhances user engagement and satisfaction Constrained Reinforcement Optimization for Multi-Objective Generation. To overcome the limited generalization of supervised learning on static data, we propose reinforcement-learningbased optimization framework for multi-objective recommendation generation tasks. Instead of directly summing multiple reward signals, we design constrained reward shaping mechanism 4 RecGPT-V2 Technical Report that guides the model to perform continual self-evolving within the feasible optimization domain. Experiments show that our method improves human-evaluated tag quality pass rate by +24.0% on item-tag prediction task, and increases the human-rated explanation acceptance rate by +77.6% on recommendation explanation task compared with RecGPT-V1. Process-Oriented Multi-Step Evaluation. To address the limitations of outcome-focused evaluation, we propose an Agent-as-a-Judge framework that decomposes abstract assessment into structured multi-step reasoning. By progressively refining judgments across multiple dimensions (e.g., relevance, diversity, coherence) through iterative deliberation, this process-oriented paradigm enhances evaluation fidelity and aligns more closely with human standards. Experiments show that Agent-as-a-Judge outperforms LLM-as-a-Judge baselines, improving human preference alignment by +0.46% on item tag prediction and +1.76% on recommendation explanation generation, achieving near-human evaluation accuracy while retaining the cost-effectiveness of automated judging. Figure 2 illustrates the overall architecture of RecGPT-V2. The system operates through streamlined pipeline: lifelong user behaviors are compressed into hybrid contextual representations (2.1.1), which feed into Hierarchical Multi-Agent System for intent decomposition and item tag prediction (2.2). The predicted tags are grounded into in-corpus items through downstream recommenders, augmented with personalized explanations (3). To ensure generation quality and enable continuous improvement, we introduce an Agent-as-a-Judge evaluation framework (4.1) for assessing generation tasks, coupled with Judge-as-a-Reward distillation method (4.2) that transfers agent judgments into optimization reward signals. In large-scale online A/B tests conducted on Taobaos homepage, RecGPT-V2 delivers significant performance improvements over RecGPT-V1, achieving +3.64% increase in IPV (Item Page Views), +3.01% lift in CTR (Click-Through Rate), +2.11% gain in TV (Transaction Volume), and +11.46% boost in NER (Novelty Exposure Rate). 2. Agentic Intent Reasoning As articulated in the introduction, RecGPT-V1s parallel multi-route cognitive architecture suffers from dual-level computational inefficiency: (1) representation-level waste, where each route redundantly encodes the entire user behavior sequence (averaging 32K tokens) despite only small fraction being relevant to its specific reasoning objective, and (2) cognitive-level overlap, where isolated reasoning processes generate duplicated candidates, manifesting in 13.46% inter-route redundancy that squanders both processing resources and cognitive diversity. To eliminate the above inefficiency issues, in this section, we propose unified agentic framework that jointly improves representation compactness and cognitive coordination: Hybrid Representation Inference (2.1): We propose context compression method that achieves 7 compression ratio by distilling behavior representations into single atomic units, dramatically reducing token length while preserving context integrity. Hierarchical Multi-Agent System (2.2): We introduce coordinated multi-agent architecture organized as PlannerExpertsArbiter, which replaces isolated parallel routes with distributed collaborative reasoning. By integrating lifelong user behaviors and multi-source environmental signals (e.g., weather patterns and seasonal factors) into specialized expert agents, this design eliminates cognitive duplication while preserving diverse intent coverage. Together, these innovations establish an efficient and scalable architecture for intent-driven recommendation at industrial scale. In the following subsections, we elaborate on each component and provide respective analytical and empirical evaluations. 5 RecGPT-V2 Technical Report Figure 3 Comparison of inference architectures between RecGPT-V1 (full-text representation with coupled prefill-decode) and RecGPT-V2 (hybrid representation with disaggregated prefill-decode). RecGPT-V2 demonstrates substantial gains in GPU utilization peak and computational efficiency. 2.1. Hybrid Representation Inference Transformer-based LLMs exhibit computational complexity of (ğ¿2 ) in the prefill stage and (ğ¿in in ğ¿out) in decoding, where ğ¿in and ğ¿out denote input/prompt and output/response lengths, respectively. In RecGPT-V1, user lifelong behaviors account for approximately 95.89% of input tokens, creating severe computation and memory bottlenecks that hinder scalability. To address this challenge, we introduce: (1) Atomized Entity Compression (2.1.1) that distills behavioral entities into compact atomic representations, and (2) Infrastructure Engineering Optimization (2.1.2) with prefill decode separation and kernel operator upgrade to meet industrial latency requirements. 2.1.1. Atomized Entity Compression The core principle underlying our approach is to compress entity information (including both item descriptions and user query histories) into atomic representational units, thereby substantially reducing context storage and computational overhead. This compression is achieved through two-stage process: atomic representation encoding, and hybrid representation adaptation. Stage 1: Atomic Representation Encoding We employ pretrained embedding models (e.g., BGE (Xiao et al., 2023), Qwen3-Embedding (Zhang et al., 2025c), TBstars-Embedding) to encode entity information into dense vector representations. Formally, given an entity ğ‘’ with its textual description = [ğ‘¤1, ğ‘¤2, . . . , ğ‘¤ğ‘›] consisting of ğ‘› tokens, we first obtain its embedding representation: = ğ‘“embed(x) â„ğ‘‘emb, where ğ‘“embed() denotes the embedding function that maps variable-length sequences to fixeddimensional dense vectors, and ğ‘‘emb is the embedding dimension. To bridge the gap between the embedding space and the LLMs language space, we introduce lightweight adaptor network ğ‘“adapt() that projects the embedding into an atomic representation compatible with LLM input: = ğ‘“adapt(h) = W2 ReLU(W1h + b1) + b2 â„ğ‘‘LLM, where W1 â„ğ‘‘hidden ğ‘‘emb, W2 â„ğ‘‘LLM ğ‘‘hidden are projection matrices, b1, b2 are bias terms, and ğ‘‘LLM matches the hidden dimension of LLMs. This atomic unit z, denoted as [entity] in the context, replaces 6 RecGPT-V2 Technical Report the original multi-token textual description. Case 1 illustrates typical Chinese product title with 12 tokens compressed into single atomic representation, achieving 12:1 compression ratio.2 Case 1: Entity Compression Example Original Text: æƒ…ä¾£å®¶å±…å¤–ç©¿é˜²æ»‘æ¯›ç»’ä¿æš–å¯çˆ±æ£‰é‹ / Couples Indoor-Outdoor Wearable Non-Slip Plush Thermal Cotton Slippers Tokenized Sequence: æƒ…ä¾£ å®¶å±… å¤– ç©¿ é˜² æ»‘ æ¯› ç»’ ä¿æš– å¯çˆ± æ£‰ é‹ Token IDs: 73245 49477 2382 8123 4153 11369 9144 44785 79318 28525 29441 18067 Compressed Atomic Representation: [entity] (Compression Ratio is 12:1) Advantages of Adaptor-Based Projection. Our approach offers three key advantages over existing methods (e.g., OneRec-Think (Liu et al., 2025), LC-Rec (Zheng et al., 2024), CoLLM (Zhang et al., 2025b)) that directly insert new tokens into the vocabulary of LLMs: i) Parameter Efficiency: We only optimize the adaptor parameters while keeping the LLM backbone frozen, significantly reducing training cost and memory footprint. ii) Superior Generalization: By maintaining frozen LLM parameters, our approach preserves the models original language understanding capabilities. The adaptor learns to project entities into the semantic space rather than forcing the model to recognize entirely new tokens. iii) Enhanced Modularity: The decoupled design allows seamless integration with different embedding models and LLM architectures without modifying the base models. This compression extends to complete user behavioral sequences. Case 2 demonstrates realistic scenario where user profile with 21,349 tokens is reduced to 5,158 tokens (token reduction ratio: 76%) by replacing item descriptions and query texts with atomic representations while preserving user attributes and temporal metadata with natural language. This hybrid representation effectively balances compactness and contextual richness. Case 2: Complete User Behavioral Sequence Compression Original Full-Text Context (21,349 tokens): User Attributes: 28å²å¥³æ€§,å±…ä½åœ¨åŒ—äº¬å¸‚,åŒå­åº§,å±ç‰›çš„ç”¨æˆ· / 28-year-old female resident of Beijing; Astrological signs: Gemini (Western), Ox (Chinese zodiac) User Behavioral History: 3å¹´å‰è´­ä¹° / Purchased 3 years ago é«˜ç­’é´å¥³ç§‹å†¬æ–°æ¬¾ ç¼‰çº¿è£…é¥°ä¸ç¼è´¨æ„Ÿè¿è¡£è£™ / Womens autumn-winter knee-high boots Topstitched satin-textured dress 2å¹´ å‰ æœ ç´¢ / Searched 2 years ago é«˜çº§æ„Ÿè¶…å¥½çœ‹å¤–å¥— å¤å¤è“ç‰™å°éŸ³ç®± / Premium aesthetic outerwear Retro Bluetooth mini speaker 1å¹´ å‰ ç‚¹ å‡» / Clicked 1 Korean-style loose-fit sweater Pure cotton 4-piece bedding set ... (numerous additional interactions omitted due to space) éŸ©ç‰ˆå®½æ¾æ¯›è¡£ çº¯æ£‰å››ä»¶å¥— / year ago 2As Taobao APP mainly targets Chinese users, all measures in this section (e.g., token counts, compression ratios) are computed on Chinese text. English translations in Case 1, Case 2 and Prompt 1 are provided for better readability. 7 RecGPT-V2 Technical Report Atomized Entity Compression Hybrid Representation Context (5,158 tokens): User Attributes: 28å²å¥³æ€§,å±…ä½åœ¨åŒ—äº¬å¸‚,åŒå­åº§,å±ç‰›çš„ç”¨æˆ· / 28-year-old female resident of Beijing; Astrological signs: Gemini (Western), Ox (Chinese zodiac) User Behavioral History: 3å¹´å‰è´­ä¹° / Purchased 3 years ago [entity] [entity] 2å¹´å‰æœç´¢ / Searched 2 years ago [entity] [entity] 1å¹´å‰ç‚¹å‡» / Clicked 1 year ago [entity] [entity] ... (all other interactions similarly compressed) (Token Reduction: 76%) However, the introduction of atomic units raises critical question: How can we enable the LLM to seamlessly understand hybrid contexts that interleave natural language tokens with compressed entity representations? To address this challenge, in the next section, we introduce dedicated Hybrid Representation Adaptation to align the atomic units with the language space. Stage 2: Hybrid Representation Adaptation To bridge this representational gap, we design two-tier training strategy comprising Self-Perception Tasks and Production-Oriented Alignment. Importantly, during this adaptation phase, we keep the LLM backbone frozen and only train the adaptor parameters , ensuring parameter efficiency and preserving the models pretrained general knowledge. Both training strategies share unified formalization and optimization objective. (1) Self-Perception Tasks. We adopt what-is-it philosophy to cultivate fine-grained entity understanding. Rather than relying on simple title reconstruction, we leverage powerful LLMs (e.g., GPT-4 (Achiam et al., 2023)) to automatically generate diverse, attribute-focused questions that probe the semantic completeness of atomic representations. This dynamic question generation method follows In-Context-Learning (Brown et al., 2020; Dong et al., 2024) prompting strategy to ensure coverage of critical entity attributes. The meta-prompt design is illustrated in the Prompt 1. Prompt 1: Meta-Prompt for Dynamic QA Pair Generation System Instruction: For given product title, want to verify whether the embedding model provides complete representational information. Please design corresponding questions and answers to confirm information completeness. All questions must be answerable from the input text alone. Output the result directly in JSON format without any additional text. Example Input: æƒ…ä¾£å®¶å±…å¤–ç©¿é˜²æ»‘æ¯›ç»’ä¿æš–å¯çˆ±æ£‰é‹ Couples Indoor-Outdoor Wearable Non-Slip Plush Thermal Cotton Slippers Example Output: [ {\"Q\": \"What is the material of <entity>\", \"A\": \"Cotton\"}, {\"Q\": \"What season is <entity> suitable for?\", \"A\": \"Winter\"}, RecGPT-V2 Technical Report {\"Q\": \"What is the anti-slip performance of <entity>?\", \"A\": \"Non-slip\"}, {\"Q\": \"What scenarios is <entity> suitable for?\", \"A\": \"Indoor&Outdoor\"} ] Actual Input: æ¾³æ´²è¿›å£ç¾åˆ©å¥´ç¾Šæ¯›åŠå¼€æ‹‰é“¾æ¯›è¡£ Australian imported merino wool half-zip sweater Generated Output: (Model dynamically generates diverse attribute-focused QA pairs) Formally, given an entity ğ‘’ with original text x, we leverage powerful LLM (e.g., GPT-4) to automatically generate diverse attribute-focused question-answer pairs: {(qğ‘–, ağ‘–)}ğ¾ ğ‘–=1 = LLM(x), where ğ¾ denotes the number of generated QA pairs. Each question qğ‘– probes specific entity attributes, with the answer ağ‘– extracted directly from x. These QA pairs serve as supervision for training the adaptor to preserve semantic completeness in compressed representations. (2) Production-Oriented Alignment. To validate practical applicability and reinforce the adaptors ability to project entity representations into semantically meaningful regions of the LLMs input space, we integrate compressed atomic units into two core recommendation generation tasks from RecGPT-V1, namely User Interest Mining and Item Tag Prediction: User Interest Mining: Infers user interest profiles from interaction histories, capturing both long-term preferences and short-term behavioral trends. Item Tag Prediction: Anticipates user intent by predicting relevant item tags based on inferred interests and historical behaviors. For each task, we first construct reference samples using full textual representations. Given prompt containing complete entity descriptions, we obtain ground-truth responses from the frozen LLM, which serve as supervision signals for adaptor training. Unified Training Formulation Both self-perception QA tasks and production-oriented tasks share an identical optimization paradigm. The core idea is to train the adaptor such that hybrid prompts (with compressed entities) can reproduce the same responses as full-text prompts would generate. Formally, given any reference sample with full-text prompt Pfull and its corresponding response y, we construct its compressed counterpart by replacing all entity texts with adaptor-projected representations. The hybrid prompt is defined as: Phybrid = ğœ™(Pfull), where ğœ™(xğ‘’) = ğ‘“adapt( ğ‘“embed(xğ‘’)), ğ‘’ E, (1) where denotes all entities in Pfull, and ğœ™() performs entity-to-atomic replacement. We optimize the adaptor to minimize the cross-entropy loss between model predictions on compressed inputs and reference responses, which is formulated as follows: (ğœƒadapt) = ğ‘¡= log ğ‘ (cid:0) ğ‘¦ ğ‘¡ Phybrid, <ğ‘¡ (cid:1) , (2) where ğ‘() denotes the frozen LLMs output distribution and ğœƒadapt represents the adaptor parameters. This objective ensures that the adaptor learns semantic-preserving projections that maintain functional equivalence between compressed and full-text representations across diverse reasoning tasks. 9 RecGPT-V2 Technical Report The training corpus combines self-perception QA pairs and production task samples. Through joint optimization over these heterogeneous supervision signals, the adaptor achieves 7 compression ratio while preserving task performance. Compared to vocabulary-expansion methods requiring full model fine-tuning, our strategy offers superior parameter efficiency and generalization capability. 2.1.2. Infrastructure Engineering Optimization To meet the stringent latency requirements of industrial-scale deployment, we introduce two complementary infrastructure optimizations that significantly enhance inference efficiency: (1) Disaggregated Prefill-Decode Serving Architecture that strategically allocates computational resources according to phase-specific characteristics, and (2) Advanced Kernel Integration with XQA Operators that leverage FP8 precision for accelerated attention computation on H20 GPUs. Disaggregated Prefill-Decode Architecture Recommendation generation tasks exhibit distinctive asymmetric input-output characteristic. Specifically, user behaviors and contextual information typically span 10K tokens , while outputs usually range from hundreds of tokens. This results in an extreme input-to-output length ratio, creating substantial inefficiencies in traditional one-serving serving architectures where both prefill and decode phases execute on the same GPU resources, leading to suboptimal Model FLOPs Utilization (MFU) and limited throughput scalability. The computational profiles of these two phases differ fundamentally: Prefill phase is compute-intensive, processing extensive inputs through parallel attention mechanisms with complexity (ğ¿2 ). Moreover, once the KV cache is computed, it does not in require persistent storage within the prefill worker and can be transferred to decode workers. Decode phase is memory-intensive, characterized by autoregressive generation with complexity (ğ¿in ğ¿out) and frequent KV cache accesses. The uncertain output lengths and sequential dependency make it inherently amenable to cache-based optimizations. To improve resource utilization and computational efficiency, following prior work (Liu et al., 2024; Zhong et al., 2024), we adopt disaggregated serving architecture that strategically partitions GPU resources according to phase-specific computational demands. We assign larger GPU pool to prefill operations to maximize parallel throughput for long-context processing, while dedicating fewer resources to decode operations that primarily benefit from efficient memory access patterns. The two phases communicate through optimized KV cache transfer mechanisms, enabling each stage to operate at its optimal resource configuration. XQA Kernel Integration To further optimize attention computation, we replace the previous FlashInfer kernel with XQA kernel to leverage FP8 precision inference on H20 GPUs. While FlashInfer is optimized primarily for BF16 precision, XQA kernel provides superior performance for FP8 quantized models, enabling faster attention computation with reduced memory bandwidth requirements. Performance Impact These infrastructure optimizations collectively improve overall MFU from 11.56% (RecGPT-V1) to 17.04%. Combined with Atomized Entity Compression (2.1.1) and the coordinated reasoning architecture from Hierarchical Multi-Agent System (2.2), RecGPT-V2 achieves 53.11% improvement in MFU compared to Figure 4 Computational efficiency comparison. 10 RecGPT-V2 Technical Report Figure 5 Architectural comparison between RecGPT-V1s isolated multi-route reasoning and RecGPTV2s Hierarchical Multi-Agent System (Global Planner Distributed Experts Decision Arbiter), demonstrating reduced cognitive redundancy through coordinated intent decomposition. RecGPT-V1. Furthermore, our system delivers substantial throughput gains with 69.30 QPS improvement in the prefill stage and 7.35 TPS improvement in the decode stage, enabling cost-effective scaling to industrial traffic volumes, as illustrated in Figure 4. 2.2. Hierarchical Multi-Agent System Having established efficient representation inference through atomized entity compression and infrastructure optimizations, we now address the remaining inefficiencies in RecGPT-V1s isolated multiroute architecture. As mentioned in Section 1, the parallel reasoning routes independently encode identical user contexts and perform redundant cognitive processes, resulting in both computational overhead from repeated full-sequence encoding and cognitive redundancy from overlapping predictions, where the latter manifests in 13.46% inter-route duplication rate. To jointly eliminate these dual-level inefficiencies, we propose Hierarchical Multi-Agent System (HMAS) that restructures LLM-based intent reasoning into coordinated three-tier architecture: Planner Experts Arbiter. The Global Planner decomposes user intent into specialized personas by analyzing hybrid compressed context and multi-source contextual signals (2.2.1). Each persona guides an expert agent to conduct role-specific item tag prediction, enabling parallel yet complementary reasoning without redundant full-context encoding (2.2.2). The Decision Arbiter synthesizes expert predictions through collaborative reasoning (2.2.3), producing refined candidate tags for downstream item retrieval. This design effectively eliminates both computational waste and cognitive duplication while preserving diverse intent coverage. 2.2.1. Global Planner The Global Planner serves as the top-level orchestrator in HMAS, responsible for decomposing complex user intent into set of specialized personas that guide downstream expert reasoning. Unlike RecGPTV1s isolated parallel routes that independently process same contexts, the Global Planner performs holistic intent analysis by synthesizing rich contextual signals into coherent strategic plan. Context Representation The Global Planner receives comprehensive contextual representation comprising three complementary information sources: RecGPT-V2 Technical Report (i) User Behavioral History = {(ğ‘ğ‘–, ğ‘’ğ‘–, ğ‘¡ğ‘–)}ğ‘ ğ‘–=1: Following RecGPT-V1, we aggregate chronologically ordered user interactions into temporally structured behavioral sequences, where each interaction is characterized by an action type ğ‘ğ‘– {click, purchase, search, . . .}, an entity ğ‘’ğ‘– (item or query), and timestamp ğ‘¡ğ‘–. This temporal aggregation provides compact yet informative representation of user engagement patterns. (ii) User Profile = {Uattr, Uint}: The user profile consists of two components: Static Attributes Uattr: Demographic information including age, gender, location, and other stable characteristics. Dynamic Interests Uint: Behavioral patterns derived from historical engagement, such as cycling enthusiast, anime fan, tech geek, etc. (iii) Environmental Context E: Real-time multi-source contextual signals encompassing weather conditions, seasonal factors, and trending events. These signals provide temporal grounding for situational intent mining (e.g., rainy day, winter season, holiday sale, etc.). Together, these components form rich hybrid context that captures both long-term preferences and real-time situational needs, which is formulated as: = {B, U, E}, where behavioral entities in are represented through atomic compression while user attributes and environmental signals retain natural language encoding to maintain semantic richness. Intent Decomposition Given the hybrid context C, the Global Planner performs deep reasoning to uncover latent user needs and decompose them into ğ¾ specialized personas { ğ‘1, ğ‘2, . . . , ğ‘ğ¾ }, where each persona represents distinct facet of user intent. The planner analyzes through multi-dimensional reasoning by considering temporal trends, situational adaptation, and behavioral consistency to generate complementary personas that avoid cognitive overlap. Formally, the persona generation process can be expressed as: { ğ‘1, ğ‘2, . . . , ğ‘ğ¾ } = ğ‘“planner(C), (3) where ğ‘“planner() denotes the reasoning function. This design achieves two critical objectives: Eliminating computational redundancy by performing intent decomposition once over the compressed context rather than having each expert independently process raw sequences. Ensuring cognitive coordination by explicitly orchestrating complementary reasoning perspectives, preventing experts from redundantly exploring overlapping semantic spaces. The generated personas { ğ‘1, . . . , ğ‘ğ¾ } are subsequently distributed to the Expert Ensemble (2.2.2), where each expert agent adopts its assigned persona and conducts specialized item tag prediction. 2.2.2. Distributed Experts Upon receiving specialized personas { ğ‘1, . . . , ğ‘ğ¾ } from the Global Planner, the distributed expert ensemble executes parallel yet complementary item tag prediction tasks. Each expert agent operates under its assigned persona to generate set of item tags that reflects distinct facet of user intent. Formally, the expert prediction process can be expressed as: Tğ‘˜ = ğ‘“expert( ğ‘ğ‘˜), (4) where ğ‘“expert() denotes the expert reasoning function, and Tğ‘˜ = {ğ‘¡ğ‘˜ , . . . , ğ‘¡ğ‘˜ ğ‘€ğ‘˜ 1 predicted item tags for persona ğ‘ğ‘˜, with ğ‘€ğ‘˜ denoting the generated tag count. , ğ‘¡ğ‘˜ 2 } represents the set of 12 RecGPT-V2 Technical Report Table 1 Data source distribution for supervised fine-tuning. Data Type Proportion (%)"
        },
        {
            "title": "Recommendation Task",
            "content": "Pure Behavior Patterns Trending Topics & Events Weather-Related Contexts Other Situational Signals General Language Modeling 32.17 6.97 1.19 7.36 52.31 To enhance expert capabilities and satisfy multi-objective requirements in industrial recommendation scenarios, we further introduce two-stage training strategy combining Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) optimization. Stage 1: Supervised Fine-Tuning To establish foundational expert capabilities, we employ SFT on persona-aligned training samples. Given persona ğ‘ğ‘˜, we construct supervision signals from the users subsequent interactions. Specifically, we leverage GPT-4 to identify which item categories from the users next interactions semantically align with the personas intent focus: Crel ğ‘˜ = {ğ‘ Cnext ğ‘“GPT-4(ğ‘, ğ‘ğ‘˜) = True}, where Cnext denotes all item categories from the users subsequent interactions (held-out next behavior), and ğ‘“GPT-4() is binary classifier that determines whether category ğ‘ is semantically relevant to persona ğ‘ğ‘˜. To ensure sufficient supervision signals, we construct fixed-size target label set Ctarget containing exactly 15 elements. If Crel ğ‘˜ < 15, we augment it with GPT-4-generated synthetic tags that follow the stylistic conventions of online category labels; if Crel ğ‘˜ > 15, we randomly sample 15 tags. ğ‘˜ For each persona-target pair ( ğ‘ğ‘˜, Ctarget ğ‘˜ ), we train the expert model following the standard nexttoken-prediction training paradigm by minimizing the cross-entropy loss: LSFT(ğœƒexpert) = ğ”¼( ğ‘ğ‘˜,Ctarget ğ‘˜ (cid:104) ) log ğ‘ğœƒexpert (cid:16) Ctarget ğ‘˜ ğ‘ğ‘˜ (cid:17)(cid:105) , (5) where ğ‘ğœƒexpert () denotes the expert models output distribution. This supervised alignment ensures that expert agents learn to generate tags consistent with their assigned persona focus. Training Data Composition. To balance domain-specific knowledge with general language capabilities, we mix persona-aligned recommendation data with general-purpose corpus. The recommendation data comprises diverse contextual scenarios: pure behavioral patterns (32.17%), trending events (6.97%), weather-related contexts (1.19%), and other contextual signals (7.36%). To preserve the models foundational linguistic and reasoning abilities, we incorporate general instructionfollowing data (52.31%), ensuring that expert models maintain sufficient versatility and robustness. The complete training data composition is summarized in Table 1. Stage 2: Constrained Reinforcement Optimization Building upon the foundation established through supervised fine-tuning, we further introduce reinforcement learning optimization to enhance expert performance across multiple objectives (e.g., diversity, relevance, accuracy). Besides, to address the inherent conflicts in multi-reward optimization, we design simple yet effective constrained reward shaping mechanism that balances competing objectives and improves overall performance. 13 RecGPT-V2 Technical Report Policy Optimization Framework. For each input sample, we adopt the Group Relative Policy Optimization (GRPO) algorithm (Liu et al., 2024; Shao et al., 2024) to optimize the expert policy. Specifically, given an input context, we sample group of ğº outputs from the old policy ğœ‹ğœƒold, and optimize the new policy ğœ‹ğœƒ by minimizing the following objective: LGRPO(ğœƒ) = ğ”¼(ğ‘¥,ğ‘¦)ğœ‹ğœƒ old (cid:104) min (cid:16) ğ‘Ÿ(ğœƒ) Ë†ğ´(ğ‘¥, ğ‘¦), clip (ğ‘Ÿ(ğœƒ), 1 ğœ–, 1 + ğœ–) Ë†ğ´(ğ‘¥, ğ‘¦) (cid:17) ğ›½ ğ”»KL (ğœ‹ğœƒğœ‹ref) (cid:105) , (6) ğ”»KL (ğœ‹ğœƒğœ‹ref) = ğœ‹ref( ğ‘¦ğ‘¥) ğœ‹ğœƒ( ğ‘¦ğ‘¥) log ğœ‹ref( ğ‘¦ğ‘¥) ğœ‹ğœƒ( ğ‘¦ğ‘¥) 1, (7) old ğœ‹ğœƒ ( ğ‘¦ ğ‘¥ ) denotes the importance sampling probability, Ë†ğ´(ğ‘¥, ğ‘¦) = ğ‘…(ğ‘¥, ğ‘¦) 1 where ğ‘Ÿ(ğœƒ) = ğœ‹ğœƒ ( ğ‘¦ ğ‘¥ ) ğ‘…(ğ‘¥, ğ‘¦ğ‘–) is the group-normalized advantage, ğ‘…(ğ‘¥, ğ‘¦) is the reward function, ğœ– is the clipping parameter, ğœ‹ref is the reference policy (i.e., SFT base model), and ğ›½ controls the strength of the KL penalty. The KL divergence term prevents the policy from deviating too far from the reference model, ensuring training stability and mitigating reward hacking. (cid:205)ğº ğ‘–=1 ğº Multi-Reward Modeling. To guide the models learning direction effectively, we design multiobjective reward function comprising four complementary components: (i) Accuracy Reward ğ‘…acc: We encourage the expert to predict tags that align with online user behavior by measuring the recall against ground-truth interactions. Specifically, given predicted tags Tğ‘˜ = {ğ‘¡1, . . . , ğ‘¡ğ‘€ } and interacted item categories Cgt = {ğ‘1, . . . , ğ‘ğ‘ }, the reward is defined as: ğ‘…acc = 1 Cgt ğ‘ Cgt ğ•€ (cid:2)ğ‘ ğ‘“tag2cat(Tğ‘˜)(cid:3) , where ğ‘“tag2cat() maps predicted tags to item categories, and ğ•€[] is the indicator function. This metric quantifies how well the predicted tags cover the users actual interests. (ii) Alignment Reward ğ‘…align: To ensure that predicted tags align with human quality standards and the assigned personas intent, we introduce an alignment reward based on human preference learning. Specifically, we train dedicated reward model ğ‘“RM() using preference pairs constructed from RecGPT-V1s quality criteria (detailed in 4.2). For each predicted tag ğ‘¡ğ‘– Tğ‘˜, we evaluate its alignment score with respect to persona ğ‘ğ‘˜: ğ‘…align = 1 ğ‘€ğ‘˜ ğ‘€ğ‘˜ ğ‘–=1 ğ‘“RM(ğ‘¡ğ‘–, ğ‘ğ‘˜), where ğ‘“RM() is trained on positive and negative preference pairs labeled according to established quality standards, capturing both semantic relevance to the persona and human-judged output quality. The final alignment reward is the average score across all predicted tags, where higher values indicate better alignment with human expectations for the given persona. (iii) Diversity Reward ğ‘…div: To encourage experts to explore diverse user interests within their assigned personas, we design diversity reward that measures the semantic richness of predicted tags. Specifically, we encode tags using the BGE embedding model (Xiao et al., 2023) and compute the average cosine distance among tag representations: ğ‘…div = 1 2 ğ‘€ğ‘˜ (ğ‘€ğ‘˜ 1) ğ‘€ğ‘˜ 1 ğ‘€ğ‘˜ ğ‘–=1 ğ‘—=ğ‘–+1 eğ‘– ğ‘— eğ‘– ğ‘— , where eğ‘– = ğ‘“BGE(ğ‘¡ğ‘–) denotes the embedding of tag ğ‘¡ğ‘–. Higher diversity scores encourage broader intent coverage without redundant predictions. 14 RecGPT-V2 Technical Report Figure 6 Comparison of reward shaping strategies. (a) Sum-based aggregation suffers from multireward conflicts. (b) Our constrained reward shaping treats secondary rewards (e.g., diversity) as conditional constraints, enabling stable optimization of the primary reward (i.e., accuracy). (iv) Length Reward ğ‘…len: To promote appropriate tag lengths that balance informativeness and retrieval effectiveness, we design length-based reward. For each predicted tag ğ‘¡ with word number ğ‘™, the reward is defined as: ğ‘…len(ğ‘¡) = if 6 ğ‘™ 11, if 4 ğ‘™ < 6 or 11 < ğ‘™ 13, 1.0, 0.5, 0.0, otherwise. The overall length reward is the average across all predicted tags: ğ‘…len = 1 ğ‘…len(ğ‘¡ğ‘–), which ğ‘€ avoids overly short tags that lack expressiveness and too long tags that hinder retrieval diversity. (cid:205)ğ‘€ ğ‘–=1 Constrained Reward Shaping. Unlike conventional multi-objective reinforcement learning methods that directly sum individual rewards (denoted as SUM), we observe that such naive aggregation often leads to severe multi-reward conflicts. As illustrated in Figure 6(a), the weighted-sum strategy mixes conflicting gradients across different reward dimensions, causing the optimization trajectory to drift toward suboptimal solutions (from ğ‘ƒ0 to ğ‘ƒSUM) where simpler objectives (e.g., diversity) dominate at the expense of more critical objectives (e.g., accuracy). To mitigate these conflicts, we propose Constrained Reward Shaping (CRS) mechanism that treats certain rewards as hard constraints to guide the optimization of the primary accuracy objective. As shown in Figure 6(b), our approach enforces two-stage optimization process: the model first satisfies secondary constraints (moving from ğ‘ƒ0 to ğ‘ƒINT by crossing the feasibility boundary), and only then begins optimizing the primary accuracy reward (progressing from ğ‘ƒINT to ğ‘ƒCRS). This design avoids gradient interference by decoupling constraint satisfaction from objective optimization. Formally, we define the composite reward as product of conditional indicators: ğ‘…total = ğ‘…acc ğ•€[ğ‘…align ğœalign] ğ•€[ğ‘…div ğœdiv] ğ•€[ğ‘…len ğœlen], (8) where ğ•€[] denotes the indicator function, and ğœalign, ğœdiv, ğœlen are predefined thresholds for alignment, diversity, and length rewards, respectively. This multiplicative formulation ensures that the accuracy reward is propagated only when all secondary objectives meet their minimum requirements. If any constraint is violated (i.e., any indicator returns 0), the total reward becomes zero, effectively mitigating conflicting gradient signals. As demonstrated in Figure 7, our CRS method exhibits superior training dynamics compared to the SUM baseline. Figures 7(a)-(b) show that CRS maintains significantly lower gradient norms and RecGPT-V2 Technical Report Figure 7 Training dynamics comparison between sum-based and constrained reward shaping. (a) Gradient norm. (b) KL divergence from reference model. (c) Accuracy reward. (d) Diversity reward. CRS maintains stable optimization across all metrics, while SUM suffers from multi-reward conflicts. KL divergence relative to the reference model, indicating improved training stability and reduced overfitting risk. Moreover, Figures 7(c)-(d) reveal the fundamental limitation of additive aggregation: in later training stages, the optimization becomes dominated by simpler objectives (e.g., diversity), causing accuracy to degrade substantially due to gradient interference. In stark contrast, our CRS method maintains simultaneous positive optimization across all objectives throughout training by decoupling constraint satisfaction from primary objective optimization, effectively mitigating multireward conflicts while preserving long-term optimization stability. Table 2 Tag prediction accuracy comparison across different training strategies. Both RecGPT-V1 and RecGPT-V2 variants are built upon Qwen-14B as the base model."
        },
        {
            "title": "Metric",
            "content": "RecGPT-V1 RecGPT-V"
        },
        {
            "title": "SFT",
            "content": "GRPO (SUM) GRPO (CRS) HR@30 26.29% 23.08% 29.20% 27.38% 32.60% Experimental Evaluation Following RecGPT-V1, we adopt Hit Rate at top-30 predictions (HR@30) as the main evaluation metric, which measures whether the predicted item tags, after being mapped to item categories via pre-trained Tag-to-Cate model, successfully match the users actual interaction categories. Table 2 presents the performance comparison across different training strategies. The Base model underperforms RecGPT-V1 by 3.21%, validating the necessity of domain adaptation. SFT substantially improves over both Base and RecGPT-V1 by 6.12% and 2.91% respectively, demonstrating that persona-aligned supervision effectively grounds expert reasoning. Comparing GRPO variants, GRPO (SUM) shows degraded performance relative to SFT, indicating that naive reward summation introduces gradient conflicts. In contrast, GRPO (CRS) achieves the highest HR@30 of 32.60%, outperforming SFT by 3.40% and RecGPT-V1 by 6.31%, validating that treating secondary objectives as hard constraints enables stable reinforcement learning optimization. 2.2.3. Decision Arbiter After the distributed expert ensemble generates complementary tag predictions {T1, T2, . . . , Tğ¾ }, the Decision Arbiter performs final candidate selection to produce refined set of item tags for downstream retrieval. Given the aggregated tag pool Tall = (cid:208)ğ¾ ğ‘˜=1 Tğ‘˜ from all expert agents, the arbiter identifies the most promising tags that align with the users real-time behavioral signals. Specifically, the arbiter leverages the hybrid context = {B, U, E} to holistically evaluate all candidate tags in Tall across multiple quality dimensions (detailed criteria are provided in Appendix B). RecGPT-V2 Technical Report Rather than scoring tags individually, the arbiter performs joint reasoning over the entire candidate pool to identify the top-ğ‘ tags that collectively maximize behavioral relevance, profile consistency, content specificity, and validity: Tfinal = ğ‘“arbiter(Tall, C). This joint evaluation process enables the arbiter to consider inter-tag complementarity and avoid redundancy, effectively consolidating distributed expert outputs into cohesive recommendation strategy that balances exploration breadth with focused user personalization. Online Item Recommendation After obtaining the refined tags, we further perform online item recommendation through multi-interest user encoding and traffic allocation optimization. Multi-Interest User Encoding. Building upon RecGPT-V1s user-item-tag three-tower architecture, we extend the user encoder to capture multiple interest facets. Following the Poly-Encoder (Humeau et al., 2019), we introduce ğ¾ learnable context codes that aggregate user behavioral embeddings into multiple interest vectors {u1, . . . , uğ¾ } via attention mechanisms, where each vector represents distinct aspect of user preferences. During online serving, the refined tags Tfinal are first encoded through the tag tower to obtain tag representations, which are then matched against items via the item tower. The multi-interest user representations are scored against candidate items through dot-product similarity, enabling fine-grained matching across diverse user intents. Traffic Allocation via Quadratic Programming. To balance exploration (i.e. cognitive channel) and exploitation (i.e. existing utility channel) under limited exposure budgets, we formulate traffic allocation as quadratic programming problem. This optimization framework dynamically adjusts the proportion of cognitive retrieval items in the recommendation slate, maximizing overall system revenue while ensuring that exploratory recommendations enhance long-term user engagement without compromising short-term business metrics. The detailed solution is provided in Appendix C. 3. Dynamic Explanation Generation Following RecGPT-V1, RecGPT-V2 retains the explanation generation module, providing personalized explanations to enhance user engagement with exposed items. However, extended online deployment reveals three critical deficiencies: (1) Low Information Densityexplanations frequently repeat generic phrases without conveying substantive insights; (2) Weak Temporal Adaptationfailure to respond to seasonal trends, current events, or contextual signals; and (3) Homogenized Expressionmonotonous stylistic outputs that undermine user engagement. We attribute these deficiencies to two fundamental limitations: static prompt templates that constrain generative flexibility, and incomplete evaluation frameworks that neglect critical quality dimensions. To address these challenges, this section introduces two key innovations in RecGPT-V2: MetaPrompting for dynamic explanation generation (3.1), which synthesizes contextually adaptive prompt templates to enable diverse and situational explanations, and preference-aware reinforcement learning (3.2), which optimizes generation quality through human-aligned multi-reward modeling. Together, these mechanisms transform explanation generation from template-based instantiation to dynamic reasoning, significantly enhancing user engagement and satisfaction. 3.1. Meta-Prompting Unlike RecGPT-V1s direct one-step explanation generation from fixed templates, following current mainstream advances in prompt engineering (Suzgun and Kalai, 2024; Zhang et al., 2023), we 17 RecGPT-V2 Technical Report introduce Meta-Prompting framework that decomposes the generation process into two stages: style synthesis followed by style-conditioned explanation generation. This hierarchical design unlocks the models creative capacity by first generating diverse, contextually adaptive stylistic guidelines, and then producing explanations that conform to these dynamic specifications. Expanded Evaluation Dimensions We first extend RecGPT-V1s evaluation framework from four dimensions (Relevance, Factuality, Clarity, Safety) to seven dimensions by incorporating three additional criteria for better user experience: (i) Timeliness, measuring alignment with current trends, seasonal contexts, or time-sensitive events; (ii) Informativeness, quantifying the substantive insights conveyed beyond generic descriptions; and (iii) Attractiveness, assessing the emotional appeal and persuasive power of the explanation. These expanded dimensions provide more holistic assessment of explanation quality, guiding both meta-prompt generation and subsequent evaluation. Two-Stage Generation Framework Given user interests, item attributes, and contextual signals (e.g., seasonal trends), the meta-prompting framework operates as follows: Stage 1: Style Synthesis. The model first generates stylistic guideline ğ‘” that specifies the desired tone, rhetorical devices, target audience, and emotional resonance. For example, given childrens toy item and parent user profile during holiday seasons, the meta-prompt might produce: Compose playful, lighthearted, and visually evocative short caption that resonates with parents. Use naive or gentle tone to create emotional connection. Formally, the style synthesis process can be expressed as: ğ‘” = ğ‘“meta(U, I, S), (9) where denotes user interest, represents item attributes, encapsulates situational signals. Stage 2: Style-Conditioned Explanation Generation. Conditioned on the style guideline ğ‘”, the model generates the final explanation ğ‘’ that adheres to the specified stylistic constraints: ğ‘’ = ğ‘“exp(ğ‘”, U, I, S), (10) where ğ‘“exp() denotes the explanation generation function. For instance, following the style guideline above, the model might produce: Spins like blue butterfly in the air. This two-stage decomposition provides flexible framework that unleashes the models imagination by enabling role-playing across diverse stylistic personas, delivering novel and contextually adaptive explanations to users. 3.2. Preference-Aware Reinforcement Learning Building upon RecGPT-V1s supervised fine-tuning foundation, we introduce constrained reinforcement learning to further enhance explanation quality, following the optimization framework in 2.2.2. We design hybrid reward framework combining rule-based diversity rewards and model-based alignment rewards, unified under the Constrained Reward Shaping (CRS) mechanism. Policy Optimization Framework. Similar to 2.2.2, we adopt the GRPO algorithm to optimize the explanation generation policy. The optimization objective remains identical to Equation (6), with the reward function replaced by the explanation-specific composite reward defined below. Hybrid Reward Modeling. To guide explanation generation across multiple quality dimensions, we design hybrid reward function comprising two complementary components: 18 RecGPT-V2 Technical Report (i) Rule-Based Diversity Reward ğ‘…div: To encourage varied linguistic expressions and avoid repetitive patterns, we design an IDF-inspired diversity reward. We maintain memory buffer of size 160 that stores recent generated explanations in tokenized form, updated in FIFO manner. For each newly generated explanation ğ‘’ = {ğ‘¤1, ğ‘¤2, . . . , ğ‘¤ğ¿}, the diversity score is: ğ‘…div = 1 ğ¿ ğ¿ ğ‘–=1 log {ğ‘’ : ğ‘¤ğ‘– ğ‘’} + 1 , where is the buffer size, and {ğ‘’ : ğ‘¤ğ‘– ğ‘’} counts stored explanations containing token ğ‘¤ğ‘–. The logarithmic term assigns higher rewards to rare tokens that enhance lexical diversity, with +1 smoothing preventing division by zero. (ii) Model-Based Alignment Reward ğ‘…align: To capture subjective quality dimensions (e.g., informativeness), we train reward model ğ‘“RM() on preference data using listwise comparisons (detailed in 4.2). Given generated explanation ğ‘’, the alignment reward is computed as: ğ‘…align = ğ‘“RM(ğ‘’, U, I, S). Constrained Reward Shaping. Consistent with 2.2.2, we adopt CRS to mitigate multi-reward conflicts. Here, explanation generation prioritizes human preference alignment as the main reward, with diversity as secondary constraint. Therefore, the total reward is formulated as: ğ‘…total = ğ‘…align ğ•€ [ğ‘…div ğœdiv] , (11) where ğœdiv is the diversity threshold. By treating diversity as gating condition, CRS eliminates gradient interference and enables stable optimization toward human-aligned, diverse explanations. Experimental Evaluation We evaluate explanation generation quality through both diversity and human evaluation. Diversity. We measure explanation diversity by computing pairwise dissimilarity within explanation sets generated for each item. Specifically, for each item ğ‘– with generated explanation set {ğ‘’ğ‘– 1 ğ¾ }, we compute the diversity score as: , . . . , ğ‘’ğ‘– , ğ‘’ğ‘– Diversityğ‘– = 1 2 ğ¾ (ğ¾ 1) ğ¾1 ğ¾ ğ‘—=1 ğ‘˜=ğ‘—+ ROUGE-L(ğ‘’ğ‘– ğ‘—, ğ‘’ğ‘– ğ‘˜), where ROUGE-L measures the longest common subsequence similarity between explanation pairs. Higher scores indicate greater lexical diversity across generated explanations. Quality. We conduct human annotation to assess explanation quality across the seven evaluation dimensions (cf. Table 8). Annotators label explanations as high-quality if they satisfy all criteria. Table 3 Explanation performance comparison. Table 3 shows that RecGPT-V2 achieves substantial improvements: diversity increases by 7.30% and quality acceptance rate improves by 13.04%. These gains validate the effectiveness of meta-prompting and preference-aware reinforcement learning. The meta-prompting mechanism enables contextually adaptive style synthesis, while CRS optimization ensures that both diversity and quality improve simultaneously by eliminating gradient interference. Diversity Quality (%) RecGPT-V1 RecGPT-V2 0.631 0.677 36.03 40."
        },
        {
            "title": "Method",
            "content": "19 RecGPT-V2 Technical Report Figure 8 Agent-as-a-Judge framework mimicking human process-oriented fine-grained evaluation. Multi-dimension sub-evaluators independently assess specialized quality dimensions, and Senior Reviewer aggregates feedback into three-tier judgments (Superior/Average/Bad). 4. Agentic Judge Framework To evaluate recommendation generation tasks, RecGPT-V1 introduced an LLM-as-a-Judge method to reduce the inefficiency and high cost of human annotation. However, this outcome-focused approach directly predicts quality scores without decomposing the evaluation into intermediate reasoning steps, limiting its ability to capture nuanced quality distinctions across multiple dimensions. This collapsed evaluation paradigm overlooks the multi-step deliberation process that human evaluators employ, resulting in suboptimal alignment with human judgment standards. To further enhance evaluation quality, RecGPT-V2 introduces novel evaluation paradigm comprising two innovations: Agent-as-a-Judge (4.1), which decomposes complicated quality assessment into Multi-Dimension progressive reasoning, and Judge-as-a-Reward (4.2), which distills agent judgments into dense reward signals for reinforcement learning optimization. Together, these designs establish self-reinforcing Flywheel Effect: the policy model generates diverse outputs, agentic evaluation provides Multi-Dimension quality feedback, and reward distillation converts assessments into optimization signals for reinforcement learning. 4.1. Agent-as-a-Judge Unlike RecGPT-V1s end-to-end LLM-as-a-Judge evaluation, motivated by recent works (Gou et al., 2025; Zhang et al., 2025a; Zhuge et al., 2024), we introduce an Agent-as-a-Judge framework that mirrors human cognitive evaluation through hierarchical multi-agent reasoning. This design decomposes holistic quality assessment into fine-grained, dimension-specific sub-evaluators followed by multi-level review, enabling more accurate and interpretable quality judgments. Multi-Dimension Sub-Evaluators For recommendation generation tasks involving multiple evaluation dimensions (cf. Appendix B), we instantiate specialized sub-evaluator for each dimension. Each sub-evaluator Eğ‘– assesses the generated content ğ‘¦ along its assigned dimension ğ‘‘ğ‘–: ğ‘ ğ‘– = Eğ‘– ( ğ‘¦, ğ‘‘ğ‘–), where ğ‘ ğ‘– represents the dimension-specific evaluation result. This decomposition transforms the complex multi-objective evaluation into manageable single-objective sub-tasks, enabling each evaluator RecGPT-V2 Technical Report to specialize in capturing nuanced quality aspects. Three-Tier Judgment To derive final overall quality, we introduce Senior Reviewer Agent that aggregates the outputs {ğ‘ 1, . . . , ğ‘ ğ·} from all sub-evaluators. The Senior Reviewer produces the final decision using three-tier S-A-B scheme: Superior (S): Output excels across all or most dimensions. Average (A): Output meets minimum standards across dimensions. Bad (B): Output fails to satisfy basic requirements in at least one critical dimension. The aggregation procedure operates through two-stage decision process: (a) Defect Detection: If any dimension receives negative or unsatisfactory signal, the overall result is classified as B. (b) Excellence Elevation: If no critical defects are detected, the Senior Reviewer further distinguishes between and based on the proportion or pattern of positive feedback among all dimensions, using threshold ğœ to control the stringency for high-quality classification. Model Adaptation through Supervised Fine-Tuning To adapt the evaluation agents to domainspecific quality standards, we construct training corpus combining model-generated samples and outputs from powerful LLMs (e.g., DeepSeek-R1 (Guo et al., 2025), Qwen3-235B (Yang et al., 2025)). To ensure sufficient coverage of Bad-quality samples, we employ hybrid annotation strategy: (1) for dimensions such as relevance, we automatically construct training samples through in-batch shuffling by randomly pairing outputs with mismatched user contexts; (2) for dimensions requiring nuanced judgment, human annotators provide labels across all evaluation dimensions, including both dimension-specific assessments {ğ‘ 1, . . . , ğ‘ ğ·} and holistic S-A-B judgments. We fine-tune lightweight Qwen3-32B-Instruct model on this mixed training data using SFT training paradigm. Table 4 Human-Judge agreement comparison on Superior (S) quality identification between LLMas-a-Judge (RecGPT-V1) and Agent-as-a-Judge (RecGPT-V2) across three models, where human annotations serve as the ground truth. The best results are highlighted in bold."
        },
        {
            "title": "Accuracy",
            "content": "F1 V1 V2 V1 V2 Item Tag Prediction GPT5-mini Qwen3-Base Qwen3-SFT 0.7694 0.7844 0.8210 0.7704 0.7864 0.8248 0.7499 0.7991 0."
        },
        {
            "title": "Explanation\nGeneration",
            "content": "GPT5-mini 0.4481 Qwen3-Base 0.3423 0.6885 Qwen3-SFT 0.4548 0.5673 0.0898 0.2764 0.7006 0.6787 0.7535 0.8051 0.8228 0.5424 0.0904 0.7307 Experimental Evaluation Table 4 presents the human-judge agreement comparison between LLMas-a-Judge (RecGPT-V1) and Agent-as-a-Judge (RecGPT-V2) on Superior (S) quality identification, where human annotations serve as the ground truth. RecGPT-V2 demonstrates consistent improvements across most experimental configurations. For item tag prediction, Agent-as-a-Judge achieves higher accuracy across all three models (+0.10pp, +0.20pp, +0.38pp) with corresponding F1 RecGPT-V2 Technical Report improvements (+0.36pp, +0.60pp, +1.33pp), indicating enhanced precision-recall balance. For explanation generation, RecGPT-V2 maintains superior performance in GPT5-mini (+0.67pp accuracy) and Qwen3-SFT (+1.21pp accuracy, +5.20pp F1), with the latter showing the most substantial F1 gain. These results validate that decomposing holistic quality assessment into dimension-specific subevaluations followed by senior reviewer aggregation enhances human-AI alignment, achieving more reliable quality identification while maintaining computational efficiency for industrial deployment. 4.2. Judge-as-a-Reward While Agent-as-a-Judge provides accurate quality assessment, directly applying it for reinforcement learning optimization faces two challenges: (1) discrete classification labels lack the granularity needed for fine-grained policy gradient estimation, and (2) the multi-step evaluation incurs high computational overhead during online RL training. To address these issues, we introduce Judge-as-aReward, distillation framework that transfers agent evaluation capabilities into lightweight reward models for providing dense optimization signals. Reward Model Architecture We initialize the reward model from the Agent Judge checkpoint, inheriting its learned evaluation knowledge. The key architectural modification replaces the language modeling head with scalar value head: ğ‘Ÿ = ğ‘“RM( ğ‘¦, U, I, S), where ğ‘“RM() denotes the reward model, and ğ‘Ÿ â„ represents the predicted reward score conditioned on generated content ğ‘¦, user interests U, item attributes I, and situational signals S. The value head applies sigmoid activation to bound outputs into [0, 1], facilitating stable gradient flow. Reward Model Training via Listwise Learning-to-Rank To preserve fine-grained quality distinctions from the Senior Reviewers three-tier labels, we adopt listwise learning-to-rank approach. For each training batch, samples are grouped by their assigned quality level (S, A, B). For any quality level ğ‘”, samples at level ğ‘” serve as positive instances, while all samples at lower levels constitute the negative set. The reward model is trained to assign higher scores to higher-quality samples using the following unified contrastive loss formulation: LRM = ğ‘” {S,A} ğ‘¦ğ‘” Yğ‘” log exp( ğ‘“RM( ğ‘¦ğ‘”)) exp( ğ‘“RM( ğ‘¦ğ‘”)) + (cid:205)ğ‘”<ğ‘” (cid:205)ğ‘¦ğ‘” Yğ‘” exp( ğ‘“RM( ğ‘¦ğ‘”)) , (12) where ğ‘” < ğ‘” denotes all quality levels lower than ğ‘” (e.g., for ğ‘” = S, negatives include both and B; for ğ‘” = A, negatives include only B), and Yğ‘” represents the set of samples at level ğ‘”. This formulation implicitly captures all pairwise relationships (S vs AB, vs B), enabling the reward model to learn the complete hierarchical preference ordering from annotated data. Engineering Acceleration via Prefix Sharing. To accelerate training, we exploit the observation that samples within each contrastive group share identical contextual prompts, differing only in generated content. By computing shared prefix representations once and reusing them across all candidates, we enable parallel inference and significantly reduce redundant computation. Self-Improving Flywheel Effect The synergistic integration of Agent-as-a-Judge and Judge-as-aReward establishes self-reinforcing optimization cycle that enables continuous quality improvement without recurring human annotation costs: 22 RecGPT-V2 Technical Report Stage 1: Policy Generation. The policy model explores the output space through supervised fine-tuning and reinforcement learning, generating diverse responses across varying quality levels. Stage 2: Agentic Evaluation. The Agent-as-a-Judge framework decomposes each generated sample into dimension-specific quality assessments, synthesizing these into holistic S-A-B tier judgments through the Senior Reviewers deliberation process. Stage 3: Reward Distillation. The Judge-as-a-Reward model distills the discrete agent judgments into continuous, differentiable, and more informative reward signals by learning the underlying preference structure through listwise contrastive training. Stage 4: Policy Optimization. The distilled reward signals guide policy refinement via GRPO (2.2.2), updating model parameters to maximize expected human-aligned preferences. This closed-loop architecture creates flywheel effect: as the policy generates higher-quality outputs, the agent evaluator accumulates richer training signals, which improve reward model calibration and enable more effective policy optimization. Critically, this cycle operates autonomously after initial human annotation, progressively aligning model behavior with human quality standards. The reward distillation ensures computational efficiency for rapid iteration, while Multi-Dimension evaluation guarantees quality improvements across all criteria rather than narrow metric optimization. Table 5 Performance comparison of reward model training strategies. HR@30 denotes hit rate at top-30 for item tag prediction. Quality measures human-evaluated explanation superior rate."
        },
        {
            "title": "Method",
            "content": "HR@30 (Tag) Quality (Explanation) RecGPT-V1 RecGPT-V2 (Point-wise RM) RecGPT-V2 (List-wise RM) 26.29% 31.24% 32.60% 36.03% 37.64% 40.73% Experimental Evaluation Table 5 compares the impact of different reward model training strategies on reinforcement learning performance across item tag prediction (HR@30) and explanation generation (Quality). RecGPT-V2 with listwise reward modeling achieves obvious improvements over RecGPT-V1 (+24.1% HR@30, +13.0% Quality) and pointwise training (+4.4% HR@30, +8.2% Quality). The listwise learning-to-rank formulation captures the complete hierarchical preference ordering (S B) by modeling all pairwise relationships simultaneously, enabling the reward model to provide more discriminative optimization signals that guide policy learning toward human-aligned quality standards. In contrast, pointwise training treats samples independently, losing the relative preference structure critical for effective policy gradient estimation. 5. Experiments To validate the effectiveness of RecGPT-V2 in practical industrial application, we conduct long-term online experiments on Taobaos platform. In the following sections, we detail the online A/B test performance and real-world case study to illustrate the advantages of our proposed system. 5.1. Online A/B Test Experimental Setup We deploy RecGPT-V2 on Taobaos homepage Guess What You Like scenario, conducting two-week online A/B test with the following configuration: 23 RecGPT-V2 Technical Report Traffic Allocation: Both experimental and control groups each receive 1% of total platform traffic, ensuring statistically significant results while minimizing deployment risk. Baseline Comparison: RecGPT-V1 serves as the control group, allowing direct assessment of the improvements introduced in RecGPT-V2. Evaluation Scenarios: We separately evaluate performance in two distinct scenarios: Item Scenario: Direct item recommendations displayed in grid layout. Feed Scenario: Mixed-content recommendation stream in the main feed, including items, advertisements, live streams, and other content types. Evaluation Metrics To comprehensively assess system performance, we measure both short-term engagement and long-term retention metrics, defined as follows: Short-Term Metrics: IPV (Item Page Views): Number of item detail page visits, indicating user interest. CTR (Click-Through Rate): Ratio of clicks to impressions, measuring recommendation relevance. TV (Transaction Volume): Monetary value of completed purchases. GMV (Gross Merchandise Value): Total transaction value including orders and returns. ATC (Add-to-Cart): Number of items added to shopping cart, reflecting purchase intent. Long-Term Metrics: NER (Novelty Exposure Rate): Percentage of recommended items that users have not previously interacted with, measuring exploration effectiveness. LT-14 / LT-30: User retention rates at 14-day and 30-day horizons, quantifying long-term engagement sustainability. Table 6 Online A/B test results comparing RecGPT-V2 against RecGPT-V1 baseline across item and feed scenarios. All metrics show relative percentage improvements (% omitted)."
        },
        {
            "title": "Item\nFeed",
            "content": "Short-Term Engagement Long-Term Retention"
        },
        {
            "title": "CTR",
            "content": "TV"
        },
        {
            "title": "NER",
            "content": "LT-14 LT-30 +3.64 +3.01 +2.11 +3.39 +3.47 +11.46 +1.29 +1.50 +0.34 +1.53 +0.99 +4.49 +0.04 +0.05 Note: indicates metrics not applicable in the item scenario. Results and Analysis Table 6 summarizes the online A/B test results. RecGPT-V2 consistently outperforms the RecGPT-V1 baseline across both scenarios and all metrics, demonstrating substantial improvements in user engagement and platform value. Across short-term engagement metrics, RecGPT-V2 achieves notable gains in the item scenario, with IPV, CTR, TV, GMV, and ATC improving by +3.26%, +3.01%, +2.11%, +3.39%, and +3.47% respectively. These improvements suggest that the enhanced intent understanding translates directly to increased user interaction and transaction value. The feed scenario exhibits consistent positive trends, with CTR (+1.50%) and GMV (+1.53%) gains indicating improved recommendation relevance. The long-term retention metrics reveal particularly striking results. NER increases by +11.46% in the item scenario and +4.49% in the feed scenario, indicating substantially improved recommen24 RecGPT-V2 Technical Report dation diversity and novelty. This finding validates our hypothesis that multi-agent coordination and environmental signal integration effectively mitigate filter bubble effects. While LT-14 and LT30 improvements appear modest in absolute terms (+0.04% and +0.05%), these gains represent meaningful progress in sustained user retention, which is critical for platform health. 5.2. Case Study Figure 9 Case study. Figure 9 illustrates real-world case demonstrating RecGPT-V2s strengths in dynamic intent understanding and context-aware recommendation generation. Given user profile (35-year-old female, Tianjin) with compressed behavioral history, the system ingests real-time environmental signals including cooling weather, upcoming Mid-Autumn Festival and Halloween. The Global Planner decomposes these contextual signals into three complementary personas: Ladies Fashion Expert, Kids Products Expert, and Health Expert. Each expert independently generates domain-specific item tags through specialized reasoning: the fashion expert predicts Wool Blend Cardigan responding to the cooling weather; the kids expert generates both Kids Hydrating Lotion (addressing dry autumn climate) and Kids Halloween Costume (anticipating the upcoming holiday), demonstrating temporal adaptation; the health expert recommends Adjustable Dumbbell Set aligning weatherdriven wellness needs with historical fitness interests. The Decision Arbiter synthesizes expert 25 RecGPT-V2 Technical Report predictions and selects three final items, each paired with contextually adaptive explanations generated by the meta-prompting framework, such as Wrapped in Autumn Sunshine (emphasizing seasonal comfort), Quench Your Little Ones Skin (highlighting childrens autumn skincare), and All You Need is Dumbbells (promoting accessible home fitness). This case validates RecGPT-V2s core capability: by integrating real-time environmental signals into hierarchical multi-agent reasoning, the system achieves both diverse intent coverage and precise situational adaptation, moving beyond static behavioral pattern matching toward dynamic, context-aware recommendation generation. 6. Conclusion This paper presents RecGPT-V2, an agentic framework that advances LLM-powered recommender systems through agentic intent reasoning, meta-prompting for explanation generation, constrained reinforcement learning, and process-oriented Agent-as-a-Judge evaluation. By eliminating cognitive redundancy and optimizing computational efficiency, RecGPT-V2 reduces GPU consumption by 60% while improving generation quality across both item tag prediction and explanation tasks. Large-scale deployment on Taobao demonstrates significant online gains (e.g., +3.40% IPV, +4.68% CTR, +4.05% TV, +11.46% NER), validating the practical viability of integrating LLM-based intent reasoning into industrial recommender systems at scale. In future work, we aim to further explore how to end-toend jointly optimize multi-agent collaboration with reinforcement learning techniques to enhance recommendation performance and user experience."
        },
        {
            "title": "References",
            "content": "J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu, Z. Wu, B. Chang, et al. survey on incontext learning. In Proceedings of the 2024 conference on empirical methods in natural language processing, pages 11071128, 2024. B. Gou, Z. Huang, Y. Ning, Y. Gu, M. Lin, W. Qi, A. Kopanev, B. Yu, B. J. GutiÃ©rrez, Y. Shu, et al. Mind2web 2: Evaluating agentic search with agent-as-a-judge. arXiv preprint arXiv:2506.21506, 2025. D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. S. Humeau, K. Shuster, M.-A. Lachaux, and J. Weston. Poly-encoders: Transformer architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969, 2019. Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):3037, 2009. A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. RecGPT-V2 Technical Report Z. Liu, S. Wang, X. Wang, R. Zhang, J. Deng, H. Bao, J. Zhang, W. Li, P. Zheng, X. Wu, et al. Onerec-think: In-text reasoning for generative recommendation. arXiv preprint arXiv:2510.11639, 2025. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. M. Suzgun and A. T. Kalai. Meta-prompting: Enhancing language models with task-agnostic scaffolding. arXiv preprint arXiv:2401.12954, 2024. J. Tang, S. Dai, T. Shi, J. Xu, X. Chen, W. Chen, J. Wu, and Y. Jiang. Think before recommend: Unleashing the latent reasoning power for sequential recommendation. arXiv preprint arXiv:2503.22675, 2025. S. Xiao, Z. Liu, P. Zhang, and N. Muennighoff. C-pack: Packaged resources to advance general chinese embedding, 2023. A. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, C. Zheng, D. Liu, F. Zhou, F. Huang, F. Hu, H. Ge, H. Wei, H. Lin, J. Tang, J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Zhou, J. Lin, K. Dang, K. Bao, K. Yang, L. Yu, L. Deng, M. Li, M. Xue, M. Li, P. Zhang, P. Wang, Q. Zhu, R. Men, R. Gao, S. Liu, S. Luo, T. Li, T. Tang, W. Yin, X. Ren, X. Wang, X. Zhang, X. Ren, Y. Fan, Y. Su, Y. Zhang, Y. Zhang, Y. Wan, Y. Liu, Z. Wang, Z. Cui, Z. Zhang, Z. Zhou, and Z. Qiu. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. C. Yi, D. Chen, G. Guo, J. Tang, J. Wu, J. Yu, M. Zhang, S. Dai, W. Chen, W. Yang, et al. Recgpt technical report. arXiv preprint arXiv:2507.22879, 2025. B. Zhang, R. Ma, Q. Jiang, P. Wang, J. Chen, Z. Xie, X. Chen, Y. Wang, F. Ye, J. Li, et al. Sentient agent as judge: Evaluating higher-order social cognition in large language models. arXiv preprint arXiv:2505.02847, 2025a. Y. Zhang, Y. Yuan, and A. C.-C. Yao. Meta prompting for ai systems. arXiv preprint arXiv:2311.11482, 2023. Y. Zhang, F. Feng, J. Zhang, K. Bao, Q. Wang, and X. He. Collm: Integrating collaborative embeddings IEEE Transactions on Knowledge and Data into large language models for recommendation. Engineering, 2025b. Y. Zhang, M. Li, D. Long, X. Zhang, H. Lin, B. Yang, P. Xie, A. Yang, D. Liu, J. Lin, et al. Qwen3 embedding: Advancing text embedding and reranking through foundation models. arXiv preprint arXiv:2506.05176, 2025c. B. Zheng, Y. Hou, H. Lu, Y. Chen, W. X. Zhao, M. Chen, and J.-R. Wen. Adapting large language models by integrating collaborative semantics for recommendation. In 2024 IEEE 40th International Conference on Data Engineering (ICDE), pages 14351448. IEEE, 2024. Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and H. Zhang. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24), pages 193210, 2024. M. Zhuge, C. Zhao, D. Ashley, W. Wang, D. Khizbullin, Y. Xiong, Z. Liu, E. Chang, R. Krishnamoorthi, Y. Tian, et al. Agent-as-a-judge: Evaluate agents with agents. arXiv preprint arXiv:2410.10934, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Contributors Core Contributors Chao Yi Dian Chen Gaoyang Guo Jiakai Tang Jian Wu Jing Yu Mao Zhang Wen Chen Wenjun Yang Yujie Luo Yuning Jiang Zhujin Gao"
        },
        {
            "title": "Contributors\nBo Zheng\nBinbin Cao\nChangfa Wu\nDixuan Wang",
            "content": "RecGPT-V2 Technical Report Han Wu Haoyi Hu Kewei Zhu Lang Tian Lin Yang Qiqi Huang Siqi Yang Wenbo Su Xiaoxiao He Xin Tong Xu Chen Xunke Xi Xiaowei Huang Yaxuan Wu Yeqiu Yang Yi Hu Yujin Yuan Yuliang Yan Zile Zhou Renmin University of China The listing of authors is in alphabetical order based on their first names. 28 RecGPT-V2 Technical Report B. Evaluation Dimensions for Generation Tasks The detailed evaluation dimensions for item tag prediction and recommendation explanation generation tasks are listed in Table 7 and Table 8, respectively. Table 7 Evaluation dimensions for item tag prediction task. Dimension Relevance Consistency Definition Evaluates whether the tags are directly aligned with the users associated interests. This criterion measures the models capacity to genuinely understand and accurately predict user needs by assessing whether the tag matches the specified interest. Assesses whether the item tag is generated with explicit reference to the users profile information and historical behavioral data. This criterion focuses on whether the models reasoning process incorporates authentic user context rather than fabricating or ignoring the given user information. Specificity Evaluates tag specificity to avoid generic terms like fashion sports equipment that lead to imprecise product retrieval."
        },
        {
            "title": "Validity",
            "content": "Determines whether the predicted tags correspond to an actual existing product, preventing non-existent tag generation. Table 8 Evaluation dimensions for recommendation explanation generation task, where dimensions marked with are newly introduced in RecGPT-V2."
        },
        {
            "title": "Relevance",
            "content": "Alignment between the explanation and both the characteristics of the recommended item and the users interests."
        },
        {
            "title": "Factuality",
            "content": "Accuracy of the explanation in reflecting the items actual features."
        },
        {
            "title": "Clarity",
            "content": "Quality of text fluency, grammatical correctness, and stylistic expression. Absence of sensitive or personal information in the generated content. Safety Timeliness Informativeness Degree to which the explanation provides useful and detailed information Alignment with seasonal trends, current events, or temporal contexts. about the item, enhancing user understanding. Attractiveness Ability to arouse user curiosity and engagement through compelling content. C. Implementation Details To balance exploratory item discovery (cognitive channel) with conversion-driven optimization (utility channel) under limited exposure budgets, we formulate traffic allocation as constrained quadratic programming problem. This optimization framework dynamically adjusts the exposure proportion of cognitive retrieval items, maximizing overall platform value while ensuring sufficient visibility for novel recommendations. Problem Formulation Let = {1, 2, . . . , ğ‘›} denote the set of candidate items retrieved by the cognitive channel. For each item ğ‘– I, we introduce decision variable ğ‘¥ğ‘– [0, 1] indicating the 29 RecGPT-V2 Technical Report exposure probability of item ğ‘–. The optimization objective is formulated as: ğ‘– max s.t. ğ‘¥ğ‘–ğ‘ ğ‘– ğœ† 2 x2 [ğ‘¥ğ‘–ğ‘œğ‘– + (1 ğ‘¥ğ‘–)ğ‘œ] C, ğ‘– ğ‘– ğ‘¥ğ‘– P, 0 ğ‘¥ğ‘– 1, ğ‘– I, (13) where: ğ‘ ğ‘– â„+ represents the predicted click revenue for item ğ‘–, measuring short-term engagement value; ğ‘œğ‘– â„+ denotes the predicted conversion revenue (e.g., GMV, transaction value) for item ğ‘–, capturing long-term commercial utility; (cid:205)ğ‘– ğ‘œğ‘– is the average conversion revenue across all candidate items; ğ‘œ = 1 ğœ† > 0 is regularization parameter ensuring strong convexity of the objective function, preventing overfitting to high-revenue items and promoting exposure diversity; â„+ is lower bound on total conversion revenue, ensuring that exploratory recommendations maintain platform-level commercial viability; Q, â„+ (Q P) define the minimum and maximum number of items eligible for enhanced exposure, controlling the intensity of cognitive exploration. The objective (13) maximizes short-term click revenue while penalizing overly concentrated item selection through the regularization term ğœ† 2 x2. The first constraint ensures that aggregate conversion revenue meets platform commercial targets. The second constraint bounds the number of exposed items within feasible range, enabling flexible regulation of exploration intensity. The constraint ğ‘¥ğ‘– [0, 1] allows continuous relaxation of binary decisions, facilitating efficient optimization. Lagrangian Formulation and Analytical Solution To solve the constrained optimization problem (13), we construct the Lagrangian function incorporating all constraints via Lagrange multipliers: (x, ğ›¼, ğœ·, ğœ¸, ğœ‡, ğœˆ) = ğœ† 2 x2 (cid:32) + ğ›¼ ğ‘– ğ‘¥ğ‘–ğ‘ ğ‘– (cid:33) [ğ‘¥ğ‘–ğ‘œğ‘– + (1 ğ‘¥ğ‘–)ğ‘œ] ğ‘– ğ›½ğ‘–ğ‘¥ğ‘– + ğ›¾ğ‘– (ğ‘¥ğ‘– 1) ğ‘– (cid:32) + ğœ‡ ğ‘– (cid:33) ğ‘¥ğ‘– + ğœˆ ğ‘– (cid:33) ğ‘¥ğ‘– , (cid:32) ğ‘– where ğ›¼, ğœ‡, ğœˆ 0 are inequality constraint multipliers, and ğ›½ğ‘–, ğ›¾ğ‘– 0 enforce box constraints 0 ğ‘¥ğ‘– 1. Taking the derivative of with respect to ğ‘¥ğ‘– and applying the KKT optimality conditions: ğ‘¥ğ‘– = ğœ†ğ‘¥ğ‘– ğ‘ ğ‘– ğ›¼(ğ‘œğ‘– ğ‘œ) ğ›½ğ‘– + ğ›¾ğ‘– ğœ‡ + ğœˆ = 0. 30 RecGPT-V2 Technical Report Rearranging and invoking complementary slackness conditions (ğ›½ğ‘–ğ‘¥ğ‘– = 0, ğ›¾ğ‘– (1 ğ‘¥ğ‘–) = 0), we define ğ‘Ÿ ğœ‡ ğœˆ (the net boundary pressure) and obtain the closed-form solution: ğ‘¥ ğ‘– = 1, ğ‘ ğ‘–+ğ›¼(ğ‘œğ‘– ğ‘œ)+ğ‘Ÿ ğœ† , 0, if ğ‘ ğ‘– + ğ›¼(ğ‘œğ‘– ğ‘œ) + ğ‘Ÿ > ğœ†, if 0 ğ‘ ğ‘– + ğ›¼(ğ‘œğ‘– ğ‘œ) + ğ‘Ÿ ğœ†, if ğ‘ ğ‘– + ğ›¼(ğ‘œğ‘– ğ‘œ) + ğ‘Ÿ < 0. Interpretation. The exposure decision for item ğ‘– is determined by the composite score â„ğ‘– ğ‘ ğ‘– + ğ›¼(ğ‘œğ‘– ğ‘œ) + ğ‘Ÿ, which aggregates three components: Click Revenue ğ‘ ğ‘–: Captures immediate user engagement value. Conversion Premium ğ›¼(ğ‘œğ‘– ğ‘œ): Weights items with above-average conversion revenue, where ğ›¼ 0 modulates the trade-off between click-oriented and conversion-oriented optimization. Budget Pressure ğ‘Ÿ = ğœ‡ ğœˆ: Dynamically adjusts exposure intensity based on constraint tightness. When total exposure approaches (lower bound), ğœ‡ increases, raising ğ‘Ÿ to admit more items; conversely, when approaching (upper bound), ğœˆ increases, lowering ğ‘Ÿ to restrict admission. Items satisfying â„ğ‘– > ğœ† receive full exposure (ğ‘¥ğ‘– = 1), those with 0 â„ğ‘– ğœ† receive fractional exposure proportional to â„ğ‘–/ğœ†, and items with â„ğ‘– < 0 are excluded. Online Deployment and Practical Simplification Due to stringent latency constraints in production systems, we adopt binarized exposure strategy that simplifies the fractional exposure case. Specifically, items with 0 < â„ğ‘– ğœ† (intermediate scores) are treated as ğ‘¥ğ‘– = 0 and excluded from exposure, effectively implementing hard threshold policy: (cid:40) ğ‘¥ ğ‘– = if â„ğ‘– > ğœ†, 1, 0, otherwise. This simplification eliminates the need for strict solution (e.g., branch and bound algorithms) at inference time, reducing computational overhead. This quadratic programming framework provides principled mechanism for harmonizing cognitive exploration with utility-driven exploitation, achieving sustainable recommendation ecosystem growth through flexible traffic allocation."
        }
    ],
    "affiliations": [
        "Taobao"
    ]
}