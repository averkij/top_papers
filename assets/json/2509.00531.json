{
    "paper_title": "MobiAgent: A Systematic Framework for Customizable Mobile Agents",
    "authors": [
        "Cheng Zhang",
        "Erhu Feng",
        "Xi Zhao",
        "Yisheng Zhao",
        "Wangbo Gong",
        "Jiahui Sun",
        "Dong Du",
        "Zhichao Hua",
        "Yubin Xia",
        "Haibo Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile agents have emerged as a key development direction for intelligent mobile systems. However, existing agent models continue to face significant challenges in real-world task execution, particularly in terms of accuracy and efficiency. To address these limitations, we propose MobiAgent, a comprehensive mobile agent system comprising three core components: the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. Furthermore, recognizing that the capabilities of current mobile agents are still limited by the availability of high-quality data, we have developed an AI-assisted agile data collection pipeline that significantly reduces the cost of manual annotation. Compared to both general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in real-world mobile scenarios."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ]"
        },
        {
            "title": "A\nM",
            "content": ". [ 1 1 3 5 0 0 . 9 0 5 2 : r MobiAgent: Systematic Framework for Customizable Mobile Agents Cheng Zhang, Erhu Feng, Xi Zhao, Yisheng Zhao, Wangbo Gong, Jiahui Sun, Dong Du, Zhichao Hua, Yubin Xia, Haibo Chen Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of Vision-Language Models (VLMs), GUI-based mobile agents have emerged as key development direction for intelligent mobile systems. However, existing agent models continue to face significant challenges in real-world task execution, particularly in terms of accuracy and efficiency. To address these limitations, we propose MobiAgent, comprehensive mobile agent system comprising three core components: the MobiMind-series agent models, the AgentRR acceleration framework, and the MobiFlow benchmarking suite. Furthermore, recognizing that the capabilities of current mobile agents are still limited by the availability of high-quality data, we have developed an AIassisted agile data collection pipeline that significantly reduces the cost of manual annotation. Compared to both general-purpose LLMs and specialized GUI agent models, MobiAgent achieves state-of-the-art performance in real-world mobile scenarios."
        },
        {
            "title": "Introduction",
            "content": "With the rise of Vision-Language Models (VLMs) and the growing adoption of mobile agents, we have observed that agents leveraging GUI/XML understanding have immense application potential on real mobile devices. Unlike traditional mobile intelligent assistants, which primarily rely on rule-based and intent-driven API calls, GUI/XML-based agents are capable of adapting to any mobile application without requiring additional adaptation from app developers. As result, an increasing number of vendors have introduced their own agent models for mobile devices, including specialized agent models such as UI-TARS [20], MobileAgent [25], CogAgent [9], and etc. [5, 8, 12, 15, 26, 27, 30, 33, 34], as well as more general-purpose models like GPT-5 [18] and Gemini 2.5-pro [7]. Despite this progress, significant challenges remain when deploying these agent models in real-world environments. For example, existing agents often exhibit low task completion rates, slow response times, and limited ability to handle unexpected situations. We first introduce the MobiMind-series models, which employ multi-role architecture consisting of three distinct components: Planner, Decider, and Grounder. This design decouples task planning, reasoning, and execution, thereby facilitating seamless integration with various backend operation modes (e.g., GUI, XML, etc.). In addition, we propose an agent acceleration framework: AgentRR [6]. AgentRR records execution traces from the agents operation and abstracts them into multi-level experiences. lightweight latent memory model determines whether the agent can leverage past experiences, thereby reducing the computational burden on VLMs/LLMs during task execution. With the AgentRR framework, the mobile agent can continuously enhance its efficiency and accuracy on recurring tasks. Preprint. To accurately evaluate the performance of our mobile agent models in real-world scenarios, we have surveyed existing mobile benchmark frameworks, including AITW [22], ANDROIDCONTROL [14], AndroidArena [28], AndroidWorld [21], GUIOdyssey [16], A3 [4], and etc. [10, 11, 13, 19, 31]. However, in practical usage scenarios, the presence of variable environmental factors, multiple correct execution trajectories, and the lack of deterministic verification mechanisms make it challenging for current benchmark frameworks to provide accurate assessments. To address these challenges, we design novel mobile benchmarking framework: MobiFlow, based on Directed Acyclic Graphs (DAGs) and milestone events. MobiFlow defines multiple correct trajectories and employs multilevel verification mechanism (e.g., text/icon matching, OCR, and LLM-based analysis) to enable precise and fine-grained evaluation of agent tasks. Furthermore, to mitigate the impact of environmental variability, MobiFlow enables replay-based evaluation by allowing different agents to execute pre-recorded static traces. This approach ensures more controlled and deterministic assessment of agent performance. Our experimental results demonstrate that, on real-world mobile scenarios (as evaluated by the MobiFlow Benchmark), the combination of MobiMind-Decider-7B and MobiMind-Grounder-3B outperforms general-purpose large language models (e.g., Gemini-2.5 Pro, GPT-5) as well as other specialized mobile agent models (such as UI-TARS-1.5-7B). Furthermore, MobiMind exhibits superior instruction-following capabilities, generates higher-quality reasoning, and achieves more reliable task termination."
        },
        {
            "title": "2 Real-world Trajectory Collection",
            "content": "In this section, we present framework for collecting real-world GUI task trajectories with rich contextual information and high data quality, while requiring minimal human efforts. The collected trajectories are further refined and used to train our agent models, enabling them to learn from diverse and realistic user interactions. 2.1 Agent Trajectory Collection To ensure that the collected trajectories accurately reflect real-world user behavior, we developed lightweight action recording tool for smartphones. During data collection, annotators interact with this tool, which captures and logs every user action before forwarding it to the device. The tool renders all bounding boxes (bbox) of interactive elements on the screen based on XML files that describe UI hierarchies. If bbox is missing due to incomplete XML data, we employ OmniParser [17] to regenerate the corresponding bbox. The recorded action space includes: Click(bbox: List[int]): Click at the center point of bbox using absolute coordinates. Input(text: str): Input text to currently activated input area. Swipe(direction: Literal[\"UP\",\"DOWN\",\"LEFT\",\"RIGHT\"]): Swipe to direction. Done(): Signify the completion of the task. For rare but important cases that are difficult to capture with the recording tool efficiently, such as closing pop-up ad or waiting for it to disappear, we create separate single-step dataset containing these cases and add an new wait action to the action space: Wait(sec: int): Wait for sec seconds before the next action. For simpler tasks which powerful pretrained VLM (e.g. Gemini-2.5) can handle, we directly prompt the VLM to execute these tasks and record the trajectories with uniform format, which further improves the efficiency of data collection. 2.2 Agent Reasoning Reconstruction The recorded trajectories only contain low-level information (e.g., raw coordinates or bounding boxes), lacking high-level semantics (e.g., task planning and environment observation) which is 2 Figure 1: Data Collection Pipeline with Agent Self-evolving crucial for training intelligent and robust agent models. To address this issue, we propose VLMbased reasoning reconstruction approach which leverages Gemini-2.5 to reconstruct the agents reasoning process based on original trajectories. Specifically, we prompt the VLM to examine each action in the trajectory and provide detailed reasoning, simulating an agent who executes the task following the ReAct [29] paradigm. Furthermore, for each concrete operation (e.g., click bbox), we also utilize the VLM to provide an action primitive (e.g., click search button) that abstractly describes this operation. 2.3 Post-collection Data Refinement Initially collected trajectories may contain errors, have unbalanced distributions, or lack diversity. Besides applying series of rule-based automated checks and manual reviews to remove erroneous or inconsistent samples, we further adopt the following data refinement strategies: Task Concatenation: We concatenate temporally dependent trajectories to form new trajectories corresponding to more complex tasks. In this way, the dataset diversity is boosted without extra efforts. Data Redistribution: Different trajectories tends to share similar prefixes while having diverse suffixes. When constructing training data, we perform sampling on the prefix actions while keeping all the suffix actions in the dataset, ensuring more balanced distribution. History Augmentation: Besides samples with complete historical information, we also include samples with only partial history in training data. This approach avoids the model from overfitting to specific history patterns. Prompt Generalization: In real-world scenarios, agents input task descriptions varies across different users. However, each trajectory obtained during the initial data collection has only one corresponding task description, which can lead to weaker generalization abilities of the agent. Therefore, we assign more semantically equivalent task desciptions to every trajectory to build more comprehensive training dataset. Corner-case Enhancement: With the single-step dataset obtained in Secion 2.1, we enhance the agents ability to handle more complicated corner cases (e.g., closing pop-up ads, terminating current task or returning to the previous page upon execution errors) by mixing single step desicion with history-based decision at training time. Experiments show that at test time, decider has stable capability of handling these problems at any time step without being interfered by the action history."
        },
        {
            "title": "3 Training",
            "content": "We post-train decider and grounder on the basis of Qwen2.5-VL-7B and Qwen2.5-VL-3B [1], respectively. The post-training process incorporates 2 phases: warm-up SFT phase and two-stage curriculum GRPO [23] phase. 3.1 Warm-up SFT In SFT phase, we use subset of the training data with the following prompt templates to train decider and grounder: Decider: <image> You are phone-use AI agent. Now your task is \"{task}\". Your action history is: {history} Please provide the next action based on the screenshot and your action history. You should do careful reasoning before providing the action. Your action space includes: - Name: click, Parameters: target_element(a high-level description of the UI element to click). - Name: swipe, Parameters: direction(one of UP, DOWN, LEFT, RIGHT). - Name: input, Parameters: text(the text to input). - Name: wait, Parameters: (no parameters, will wait for 1 second). - Name: done, Parameters: (no parameters). Your output should be JSON object with the following format: {\"reasoning\": \"Your reasoning here\", \"action\": \"The next action (one of click, input, swipe, wait, done)\", \"parameters\": {\"param\": \"value\"}} Grounder: <image> Based on the screenshot, users intent and the description of the target UI element, provide the elements bounding box using absolute coordinates. Users intent: {reasoning} Target elements description: {target_element} Your output should be JSON object with the following format: {\"bbox\": [x1, y1, x2, y2]} This warm-up phase empowers both models with basic format following and GUI agent capabilities, avoiding the reward signals in the subsequent RL phase being overly sparse which causes harm to training effectiveness. 3.2 Two-stage Curriculum GRPO with Self-evolution Since MobiAgent adopts multi-agent architecture, we simplify the multi-agent reinforcement learning process by incorporating 2 separate stages in GRPO phase to train the grounder and decider independently. Grounding GRPO. Firstly, we train the grounder with rule-based reward function using the UI grounding dataset in Section 2. The reward function consists of two parts: an IoU reward Riou and center-point reward Rcenter, reflecting the precision of the bounding box prediction of the grounder: Riou = (cid:26)α, 0, IoU (bpred, bgt) > β, otherwise. , Rcenter = (cid:26)1 α, Center (bpred) is inside bgt, 0, otherwise. , = Riou + Rcenter, 4 where bpred, bgt are the predicted and ground-truth bounding boxes, IoU () is the Intersection over Union function, and α, β are hyperparameters. Grounder-as-RM GRPO. For click actions, since the deciders output only contains high-level description of the target element, it is unfeasible to evaluate the its quality by simply comparing it to the ground-truth. However, the grounder is able to map the high-level semantics to low-level and judgable actions. Therefore, after high-precision grounder is obtained in the previous stage, we then let it serve as an outcome reward model (ORM) for training the decider. Specifically, for each response generated in the rollout stage, if the action is non-click operation, we apply binary reward Rcontent by exact matching: = Rcontent = (cid:26)1, apred = agt, 0, otherwise. , where apred, agt are the predicted and ground-truth actions. For click action, the grounder predicts bounding box of the target element in deciders response, and directly returns = Rcenter as the final reward. Curriculum Learning. In Grounder-as-RM stage, we further adopt curriculum learning [2]-based training approach to train the decider: Each action in the training dataset is labeled binary difficulty with series of predefined rules (e.g., action type frequency or manually assigned importance). In early training, each batch in model rollout only contains easy samples, and we gradually mix more difficult samples in the batches as the training progresses. This approach further addresses sparse rewards, accelerates model convergence and improves training robustness. Self-evolution. To avoid unexpected agent behavior at test time due to error accumulation in the agents input context (i.e., the history part in deciders input and the reasoning part in grounders input), we align the distribution of agent input at training time and test time via training-time-test. Specifically, after each round of training, we test the two models with new tasks and collect the test-time traces. Then we examine and correct the unsuccessful traces, and merge the corrected traces with successful traces to become part of the training data of the next round, enabling the agent to self-evolve in an iterative way."
        },
        {
            "title": "Experiences",
            "content": "In this section, we introduce general-purpose agent acceleration framework: AgentRR, as well as our specific implementation in mobile scenario. AgentRR seamlessly supports any mobile agent models and applications without the need for additional development for specific tasks. Empirical evaluations show that ActTree enables GUI agents to achieve execution efficiency comparable to that of API agents, while preserving the generalization capability of GUI agents. 4.1 Multi-level Experiences As shown in Figure 2, MobiAgent adopts multi-agent architectural design, including Planner (4B), Decider (7B), and an Grounder (3B). The execution of complete task requires the collaborative operation of these multiple models/agents. We record the output of each model as an experience, such that the outputs from different models form multi-level experience hierarchy. The Grounder outputs concrete operation actions, such as the bounding box of the clicked object, representing the lowest-level experiences. The Decider outputs action primitives described in natural language. The Planner outputs the task plan, corresponding to high-level experiences. Lower-level experiences can be executed with higher efficiency, but typically offer weaker generalization, while higher-level experiences exhibit stronger generalization capabilities but are less efficient to execute (as they still require model inference). To address this challenge, we propose record-and-replay system specifically designed for agent frameworks. The core idea behind it is to efficiently determine which actions can be directly reused, without compromising the agents generalization capabilities. This mechanism is analogous to human 5 Figure 2: Multi-Agent Architecture using the AgentRR Framework latent memory, enabling swift execution of familiar actions while preserving the flexibility required for new situations. 4.2 Prefix Reusablity of GUI tasks We observe the prefix reusablity of GUI tasks, which relies on the following insights: I1: The execution trajectories of similar tasks share identical prefixes. I2: Since the agents planning can be directly derived from task description, the length of shared trajectories (i.e., prefix actions) between two tasks can be predicted based on the similarity of their task descriptions. According to I1, task trajectories can be cached using tree structure, where similar tasks share the same path from the root node to their common ancestor node. According to I2, the agent can determine whether to reuse actions from historical tasks at the current step through simple and efficient semantic matching, thereby avoiding redundant model inference. By exploiting prefix reusability, we can efficiently records execution trajectories and attempts to replay cached actions for every execution step, thereby reducing the end-to-end task execution latency. 4.3 ActTree Structure ActTree is concrete implementation mechanism that accelerates mobile tasks by leveraging the principles of AgentRR. As shown in Figure 3 ActTree maintains tree-structured representation analogous to UI Transition Graphs. Each node in the tree corresponds to unique UI state, denoted as UI u, and maintains set of outgoing edges Eu = {e1, ..., en}. Each edge = (u, v) represents state transition, which is characterized by: (1) an action Ae that triggers this transition; 6 Figure 3: Construction of the ActTree Structure During Mobile Task Execution and (2) task list Te = {t1, ..., tk}, which records all historical tasks that executed action Ae to transition from UI to UI v. The construction of the ActTree proceeds incrementally during the task executions. When new action is performed, we first create new node to represent the resulting UI state: UI v, along with corresponding edge enew = (u, v) annotated with the executed action Aenew and the current task description tnew. To maintain the compactness of the ActTree, the system compacts the set of outgoing edges Eu = {e1, . . . , en} from node u. For example, if there exists an edge eold Eu whose action Aeold matches the new action Aenew , the system merges these two edges. The merge operation combines the task lists of the two edges by updating Temerge Teold {tnew}, while leaving the remaining edge structure unchanged. To support more complex agent scenarios, such as recursive task invocation and collaboration among multiple applications, we further extend the ActTree structure to allow certain backtracking edges. For example, edges from child nodes back to parent nodes enable the agent to return to the parents UI state when an erroneous action is executed or when recursive tasks are invoked. Additionally, we also introduce cross-subtree edges with the additional transferred message to facilitate coordination across multiple tasks. In addition to ActTree itself, we incorporate Tracer component that is bound to ActTree to monitor task execution. During the execution of new task, Tracer advances to new node along with each UI transition. At each step, if Tracer identifies an existing outgoing edge whose action Ae can be reused for the current task (i.e., cache hit), it replays Ae without invoking the agent model. Otherwise, the Tracer invokes the agent model to generate the next action. 4.4 Latent Memory Models When using AgentRR framework to accelerate agent execution, key challenge is determining how many prefix actions can be reused. To address this, we propose the Latent Memory model, which efficiently computes the similarity between two tasks and thereby determines the extent to which prefix actions can be reused. Specifically, the \"Latent Memory\" model is implemented in the form of the task embedding and task reranking models. These models typically have significantly fewer parameters than the planner or decider, resulting in lower response latency. 7 4.4.1 Task Embedding The task embedding model is responsible for computing the similarity between two tasks, which is measured as the cosine similarity between their respective embedding vectors. Below, we provide the formal representation of the embedding at specific layer of the ActTree for given task. Where: v(l) = (Ti, l) Rd. Ti represents the i-th task. is an instruction-aware task embedding model, and (Ti, l) maps Ti to vector for the l-th layer of the ActTree. is in the instruction part of input. is the embedding vector corresponding to the task Ti and the layer l. v(l) is the dimension of the embedding vector, and Rd represents the d-dimensional real-valued vector space where the embedding vector resides. To determine whether the action prefix of task Ti can be reused by task Tj, we compare the cosine similarity of task embedding S(v(l) ) with predefined similarity threshold τ1. The stage-1 decision of whether Ti can reuse the actions of Tj at the l-th layer in ActTree is formally expressed as: , v(l) Embed-Reuse(l)(Ti, Tj) = (cid:40) if S(v(l) True, False, otherwise , v(l) ) τ1 . 4.4.2 Task Reranking Another approach to implementing the latent model is to leverage task reranking. Given two candidate tasks, the latent model determines whether the action at specific layer of ActTree can be reused, and directly return score (the logit of \"yes\" token) between 0 and 1. The task reranking mechanism serves as complementary method to task embedding. In our implementation, we first utilize the task embedding model to compute similarity scores between the current task and all historical tasks for the current layer, then select set of candidate tasks with the highest similarity scores: = {T Thist Embed-Reuse(l)(T, Tc)}, where Tc is the current task, Thist represents all historical tasks recorded in outgoing edges of the current ActTree node. Subsequently, we perform task reranking by comparing each candidate task with the current task. If there exists at least one candidate whose reranking score with the current task is greater than another predefined confidence threshold τ2, the action at the corresponding node in ActTree is considered reusable for the current task. This process enables the latent model to make more accurate reuse decisions compared to relying solely on the embedding model. The stage-2 (final) reuse decision is expressed as: Rerank-Reuse(l) = (cid:26)True, False, if Ti such that g(Ti, Tc, l) τ2 otherwise . Where: is an instruction-aware task reranking model, and g(Ti, Tj, l) returns confidence score of the action reuseability between Ti and Tj at l-th layer. 4.4.3 Latent Memory Model Training We select Qwen-Embedding and Qwen-Reranker [32] as our base models. All training data are collected through our agent model by recording actual action traces on mobile devices, ensuring behavioral consistency between the Latent Memory Model and the Agent Model. For the task embedding model, for given ActTree layer and task , other tasks that share an l-step action prefix with are considered positive samples, while tasks sharing fewer than steps are considered negative samples. Formally, for = (Ti, l), the positive and negative samples are sampled in the following way: v+ = (Ts, l), pre(Ts, Ti) l, = {f (Tt, l) pre(Tt, Ti) < l} , where pre(, ) gives the length of the shared action prefix of two tasks, and we calculate the InfoNCE [24] loss with temperature parameter τ : = Ev ln eS(v,v+)/τ eS(v,v+)/τ + (cid:80)M j=1 eS(v,v )/τ , For the task reranking model, the determination of positive and negative samples is consistent with the embedding model. The positive samples are labeled as yes and negative samples as no, and we use SFT loss to train the model. 4.5 Latent Memory Invalidation and Eviction Experiences may become obsolete as the environment changes, such as application updates or device replacements. Compared to high-level experiences, low-level experiences (e.g., action coordinates output by the Actor) tend to become outdated more frequently. In the AgentRR system, it is essential to promptly detect and update obsolete experiences to ensure the correctness of subsequent action reuse. 4.5.1 UI Change Detection Numerous modern applications utilize dynamic UI components (e.g., the items in search results page change according to user preferences). UI change can lead to incorrect action replay in ActTree when identical actions could yield different results. ActTree leverages OmniParser, screen parsing tool based on small vision models, to detect and mitigate this anomaly. When executing an action with target UI element (e.g., click action) for the first time, Tracer invokes OmniParser to extract the target elements bounding box and content. Prior to replaying an action during another tasks execution, Tracer invokes OmniParser again to verify whether the UI content within the previous bounding box has changed. If UI change is detected, the cache hit is discarded and the cached action is invalidated. 4.5.2 False Positive of Reuse Decision If the latent model outputs reuse decision of true, but the action cannot actually be reused for the current task, this results in false positive decision. However, false positives in the AgentRR system do not lead to final execution errors. After false positive action is performed, there will be significant discrepancy between the current UI state and the previously recorded UI state, causing failure of UI checking. Subsequently, AgentRR will invoke the Decider module again to determine the appropriate action for the current interface, which is typically back operation. 4.5.3 LRU Experience Eviction In addition to experience obsolete, some experiences may rarely be used, resulting in unnecessary consumption of storage resources. To address this, ActTree implements Least Recently Used (LRU) eviction policy to prevent out-of-memory issues caused by unused experiences. When the cache reaches its maximum size, the least recently used experience is removed to make space for new experience. 9 4.6 Speculative Replay via Shortcuts Task trajectories usually consist of series of sub-tasks. sub-task is defined as an action sequence following fixed pattern, e.g., the search sub-task includes 3 steps: clicking the search bar to activate it, entering the query, and clicking the search button to execute the search. As in the aforementioned example, different trajectories of sub-task can share both prefixes and suffixes (both step 1 and step 3 are reusable). We refer to frequently recurring sub-tasks as shortcuts. By identifying shortcuts in the tree, ActTree enables speculative replay. shortcut is defined as set of sub-paths in the ActTree, denoted as S. For any sub-path pair (pi, pj) S2, pi and pj satisfy the following constraints: They start from the same ActTree node and have identical lengths. Their action type sequences before the last action are identical. Their last actions are identical, denoted as S. When Tracer reaches newly created node with no outgoing edges, there are no available experiences to replay. In this scenario, Tracer examines whether the current partially completed task satisfies the following condition: for some S, if is the next action for , sub-path of can be joined with S. If this condition is met, Tracer speculatively selects as candidate for the next action. In this manner, ActTree expands the scope of experience utilization and enables the discovery of additional replay opportunities."
        },
        {
            "title": "5 MobiFlow: a DAG-Based Benchmark Framework for Mobile Agents",
            "content": "Current mobile benchmarks primarily focus on system applications and tasks with clearly defined evaluation criteria. However, in real-world mobile agent scenarios, operations often involve third-party applications and lack explicit verification conditions. As result, existing mobile agent benchmarks fail to accurately reflect the models capabilities in mobile-side scenarios. To address this limitation, we propose an evaluation framework tailored to the manipulation of third-party mobile applications. By leveraging Directed Acyclic Graph (DAG) to model the complex dependencies and sequential constraints inherent in mobile application tasks, our framework enables more realistic assessment of model performance in practical on-device agent scenarios. 5.1 Framework Architecture Figure 4 illustrates our benchmark framework. Given the complexity and variability of real-world mobile applications, we define multiple validation methods, diverse task trajectory specifications, and both online and offline trajectory collections. These design choices aim to accurately capture the agents capabilities in real-world scenarios. The overall architecture of the benchmarking framework is as follows: 1. Trace Collection: We categorize agent execution traces into two types: online and offline. Online traces are collected through real interactions between the agent and mobile devices, logging all agent actions as well as UI screenshots from various pages. In contrast, offline traces are obtained by predefining the agents task execution process and recording information such as action primitives and XML data generated. Additionally, we merge shared steps across different trajectories for the same task. By leveraging offline traces, we can conduct stable evaluations of the agents performance, effectively eliminating the impact of dynamic environmental variables. 2. DAG-based Task Definition: Each task is defined by configuration file that specifies the structure of DAG. In this graph, nodes represent key milestones in task execution, such as UI states or critical operations, while edges capture the dependencies and action transitions between these milestones. 3. Validation Engine: The engine parses both the tasks DAG configuration and the agents execution trace. It traverses the DAG alongside the trace, searching within the actual execution trace for milestone events and their dependencies as defined in the DAG. 4. Result Generation:Based on this traversal, the engine determines whether fully validated path or partially validated path exists. Tasks that successfully complete all milestone events defined 10 Figure 4: The overall architecture of MobiFlow in the DAG are awarded the full score, while tasks that accomplish only subset of milestone events may receive partial credit, as specified by user-defined criteria. In real-world scenarios, application responses are highly dependent on factors such as application version, environment, and interaction patterns. Moreover, there is often lack of systematic interfaces for quickly verifying whether milestone events have been completed. To address these challenges, we design multi-level verification mechanism to ensure the accuracy of evaluating agent tasks on the mobile side. 5.2 Core Validation Capabilities The framework supports hierarchical approach to condition checking for robust and resilient validation. Multi-level Condition Checkers: Each node in the DAG can be associated with one or more conditions that must be met. The supported checker types include: Text Matching: Verifies the presence of specific strings or patterns in the UI. Regular Expression Matching: Allows for more complex text-based validation using regular expressions. UI State Analysis: Examines the underlying view hierarchy (e.g., via XML dumps) to confirm the state of UI elements. Icon Detection: Utilizes computer vision techniques to identify the presence of specific icons. OCR Process: If neither XML files nor the page text is available, we employ OCR techniques to extract and verify text directly from screenshots. MLLM-as-a-Judge: Employs multi-modal LLM to make holistic judgment about the UI state based on the current key frame and contextual information. Progressive Escalation Strategy: The checker starts with the lightest, fastest verification path. When text matching is required, it first parses the XML file and instantly confirms the current page state. 11 Only if the XML source is absent or plain-text checker is disabled does the system escalate to heavier techniques, like OCR or LLM-based reasoning, to decide whether the milestone has been reached. Multi-path DAG Validation: The DAG structure inherently supports tasks with multiple valid completion trajectories. For tasks that can be accomplished through different approaches, we specify whether the milestone nodes are dependent on others or can proceed independently in parallel. This allows the DAG representation of task to include multiple branches and paths. To better express the relationships between different paths, we define the AND and OR conjunctions. The AND indicates that all prerequisite milestone events from the preceding nodes must be satisfied before proceeding. In contrast, the OR stipulates that satisfying any single milestone event within the set is sufficient to trigger validation of the subsequent node. For given task, if multiple paths can lead to its completion, the task is considered accomplished as long as any one of these paths is successfully validated. To ensure the correct matching between milestone event nodes defined in the DAG and nodes in the trace, we introduce synchronized backtracking mechanism for DAG and trace nodes. Specifically, when the checker detects that certain path in the DAG cannot be validated, it backtracks to the previous branching milestone event and selects an alternative path for verification. Simultaneously, the checker also backtracks within the actual trace until it reaches the milestone node corresponding to the branching point. Dynamic Condition Matching: MobiFlow supports task definition via templates and dynamically extracts conditions from the task description during execution. For example, if the task is add milk to the shopping cart the validator can automatically identify milk as the key object and subsequently generate corresponding milestone event nodes in sequence. Icon Recognition: MobiFlow leverages OpenCV [3] for icon recognition and supports multi-scale matching. By controlling the similarity threshold, it can robustly identify UI elements, even in the presence of minor variations in size or appearance. 5.3 Construct the MobiFlow Benchmark for Real-world Mobile Agent To construct benchmark that evaluates agent capabilities in realistic mobile environments, we select range of representative mobile applications and scenarios, encompassing domains such as social networking, music and video, shopping, travel, food delivery, etc. Each test case defines milestone events using template-based approach, ensuring sufficient generalizability of test cases. Additionally, for each application, we design task cases with varying levels of difficulty, ranging from simple to complex, to accurately assess the capabilities of agent models. In our actual testing process, we meticulously select tasks that can be correctly executed in the current environment, thereby minimizing test failures caused by task misconfigurations rather than agent performance. For test cases that result in abnormal or failed executions, we further employ manual verification to ensure the accuracy and reliability of the evaluation. Due to the iteration of applications and the variability of operating environments, the absolute scores obtained from MobiFlow are primarily for reference. However, the relative scores among different agent models provide more accurate reflection of their capabilities in mobile agent scenarios. Additionally, we can record the correct action sequences from given benchmark run as offline traces, including multiple valid trajectories for the same task, and enable different models to replay these offline traces for evaluation. This approach eliminates the influence of dynamic environmental factors, thereby making the evaluation results more deterministic. Nevertheless, evaluating with offline traces may lead to false negatives. For example, an agent model could discover new successful trajectory; however, such correct outcome would be erroneously classified as failure under offline-trace-based evaluation."
        },
        {
            "title": "6 Evaluation",
            "content": "6.1 Task Completion Score We evaluated the task completion rates of various models in real-world mobile scenarios. Our test suite covers the current widely-used mobile applications in China, as well as common tasks, such 12 (a) Overall Average Score (b) Average Easy Task Score (c) Average Hard Task Score Figure 5: Average Task Completion Scores of Different Agent Models Under Realistic Workloads (MobiFlow): The performance is shown for (a) all tasks combined; (b) easy tasks; (c) and hard tasks. as travel, shopping, entertainment and social networking. For each application, we defined set of test case templates with multiple difficulty levels, where each template consists of milestone events structured in the form of DAG. Prior to formal testing, we generated three real-world test cases for each type of template. All test cases are defined within our MobiFlow benchmark framework. Figure 5 presents comparison of task completion rates between MobiAgent (MobiMind-Decider-7B + MobiMind-Grounder-3B) and several other VLM models, including the general-purpose LLMs (GPT-5, Gemini-2.5-pro) and the state-of-the-art GUI Agent model UI-TARS-1.5-7B. To ensure the accuracy of our evaluation, we manually verified all failed tasks. For cases caused by app anomalies, network interruptions, or non-existent tasks, we conducted repeated testing. In addition, we imposed termination penalty on tasks where the agent could not complete or exit properly. MobiAgent achieves the highest task completion rates across the majority of applications. In complex tasks such as shopping and food delivery, MobiAgent demonstrates superior performance in task understanding and decomposition, instruction following, and handling of exceptional cases. During execution, GPT and Gemini tend to input object descriptions, types, and details such as departure time and locations, directly into the search box. Because some apps support AI-powered search capabilities, these models can bypass most complex interactive steps, resulting in relatively higher task completion rates. However, such coarse-grained operation strategies lead to significant drop in completion rates when faced with applications that do not support AI search. Furthermore, we observed that existing agent models still suffer from task non-termination issues, such as the infinite repetition of certain actions. Specifically, GPT failed to properly terminate tasks in 11 application categories, while Gemini exhibited similar problems in 3 categories. In contrast, MobiAgent consistently achieved correct task termination across all evaluated scenarios. 6.2 Accelerating the Mobile Agent using AgentRR In addition to evaluating the task completion rates of agents, we also measure the speedup achieved by the AgentRR acceleration framework across various scenarios. By leveraging ActTree and latent memory model, AgentRR can rapidly reuse historical agent trajectories, thereby reducing the inference overhead of VLMs. Consequently, the acceleration ratio of AgentRR in real-world mobile scenarios is positively correlated with the proportion of nodes in ActTree that can be reused. To simulate different user behaviors on mobile devices, we constructed two types of user task distributions: uniform distribution and power law distribution. In the uniform distribution, users evenly utilize various application functionalities and randomly choose task parameters (e.g., search objects). In the power law distribution, 80% of user requests are concentrated within 20% of tasks, which more closely aligns with real-world user habits. As Figure 6 shows, under the uniform distribution, AgentRR achieves an action replay rate of 30% 60%. Under the power law distribution, the action replay rate increases to 60%85%. Moreover, owing to the accuracy of the latent model, the correctness of action replay exceeds 99% in our test cases. In the real-world deployment scenarios, the performance advantage of AgentRR becomes even more pronounced, as the latent memory model can be deployed directly on edge devices, thereby reducing network communication overhead. In complex task scenarios such as food delivery and online 13 Figure 6: Action Replay Rate with AgentRR under Different Distributions. shopping, AgentRR achieves an average performance improvement of 2 to 3 times compared to baseline approaches."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we address the challenges of building, evaluating and deploying mobile agent by introducing comprehensive, full-stack solution encompassing the MobiMind model series, the AgentRR acceleration framework, and the MobiFlow evaluation benchmark. Experiments show that MobiAgent achieves state-of-the-art task completion rates that surpass both general-purpose models like GPT-5 and Gemini-2.5 Pro, and other specialized mobile agents in the MobiFlow benchmark. Moreover, The experience replay mechanism of AgentRR framework delivers 2-3x optimization on agents task completion latency in real-world scenarios. These results demonstrate the effectiveness of our approach in creating customizable and practical mobile agents."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. 2025. Qwen2.5-VL Technical Report. arXiv:2502.13923 [cs.CV] https://arxiv.org/abs/2502.13923 [2] Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning (Montreal, Quebec, Canada) (ICML 09). Association for Computing Machinery, New York, NY, USA, 4148. https://doi.org/10.1145/1553374.1553380 [3] G. Bradski. 2000. The OpenCV Library. Dr. Dobbs Journal of Software Tools (2000). [4] Yuxiang Chai, Hanhao Li, Jiayu Zhang, Liang Liu, Guangyi Liu, Guozhi Wang, Shuai Ren, Siyuan Huang, and Hongsheng Li. 2025. A3: Android Agent Arena for Mobile GUI Agents. arXiv:2501.01149 [cs.AI] https://arxiv.org/abs/2501.01149 [5] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents. arXiv:2401.10935 [cs.HC] https://arxiv.org/abs/2401.10935 [6] Erhu Feng, Wenbo Zhou, Zibin Liu, Le Chen, Yunpeng Dong, Cheng Zhang, Yisheng Zhao, Dong Du, Zhichao Hua, Yubin Xia, and Haibo Chen. 2025. Get Experience from Practice: LLM Agents with Record & Replay. arXiv:2505.17716 [cs.LG] https://arxiv.org/abs/ 2505. 14 [7] Google. 2025. Gemini 2.5 Pro Best for coding and highly complex tasks. https://deepmind. google/models/gemini/pro/ [8] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2025. Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents. In The Thirteenth International Conference on Learning Representations. https://openreview.net/forum?id=kxnoqaisCT [9] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. 2024. CogAgent: Visual Language Model for GUI Agents. arXiv:2312.08914 [cs.CV] https://arxiv.org/abs/2312.08914 [10] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. 2024. OmniACT: Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web. arXiv:2402.17553 [cs.AI] https://arxiv.org/abs/2402. [11] Juyong Lee, Taywon Min, Minyong An, Dongyoon Hahm, Haeone Lee, Changyeon Kim, and Kimin Lee. 2024. Benchmarking Mobile Device Control Agents Across Diverse Configurations. arXiv preprint arXiv:2404.16660 (2024). [12] Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steve Ko, Sangeun Oh, and Insik Shin. 2024. MobileGPT: Augmenting LLM with Human-like App Memory for Mobile Task Automation. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking (Washington D.C., DC, USA) (ACM MobiCom 24). Association for Computing Machinery, New York, NY, USA, 11191133. https://doi. org/10.1145/3636534.3690682 [13] Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. 2025. ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use. arXiv:2504.07981 [cs.CV] https://arxiv.org/abs/2504.07981 [14] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the Effects of Data Scale on UI Control Agents. arXiv:2406.03679 [cs.AI] https://arxiv.org/abs/2406.03679 [15] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. 2024. ShowUI: One Vision-Language-Action Model for GUI Visual Agent. arXiv:2411.17465 [cs.CV] https://arxiv.org/abs/2411.17465 [16] Quanfeng Lu, Wenqi Shao, Zitao Liu, Lingxiao Du, Fanqing Meng, Boxuan Li, Botong Chen, Siyuan Huang, Kaipeng Zhang, and Ping Luo. 2025. GUIOdyssey: Comprehensive Dataset arXiv:2406.08451 [cs.CV] https: for Cross-App GUI Navigation on Mobile Devices. //arxiv.org/abs/2406. [17] Yadong Lu, Jianwei Yang, Yelong Shen, and Ahmed Awadallah. 2024. OmniParser for Pure arXiv:2408.00203 [cs.CV] https://arxiv.org/abs/2408. Vision Based GUI Agent. 00203 [18] OpenAI. 2025. GPT-5 is here. https://openai.com/gpt-5/ [19] Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine, and Alane Suhr. 2024. Autonomous Evaluation and Refinement of Digital Agents. arXiv:2404.06474 [cs.AI] [20] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, Wanjun Zhong, Kuanye Li, Jiale Yang, Yu Miao, Woyu Lin, Longxiang Liu, Xu Jiang, Qianli Ma, Jingyu Li, Xiaojun Xiao, Kai Cai, Chuang Li, Yaowei Zheng, Chaolin Jin, Chen Li, Xiao Zhou, Minchao Wang, Haoli Chen, Zhaojian Li, Haihua Yang, Haifeng Liu, Feng Lin, Tao Peng, Xin Liu, and Guang Shi. 2025. UITARS: Pioneering Automated GUI Interaction with Native Agents. arXiv:2501.12326 [cs.AI] https://arxiv.org/abs/2501. 15 [21] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, and Oriana Riva. 2025. AndroidWorld: Dynamic Benchmarking Environment for Autonomous Agents. arXiv:2405.14573 [cs.AI] https://arxiv.org/abs/2405.14573 [22] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Android in the wild: large-scale dataset for android device control. In Proceedings of the 37th International Conference on Neural Information Processing Systems (New Orleans, LA, USA) (NIPS 23). Curran Associates Inc., Red Hook, NY, USA, Article 2609, 21 pages. [23] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. arXiv:2402.03300 [cs.CL] https://arxiv.org/abs/2402.03300 [24] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019. Representation Learning with Contrastive Predictive Coding. arXiv:1807.03748 [cs.LG] https://arxiv.org/abs/1807. 03748 [25] Junyang Wang, Haiyang Xu, Haitao Jia, Xi Zhang, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. 2024. Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration. arXiv:2406.01014 [cs.CL] https: //arxiv.org/abs/2406. [26] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. AutoDroid: LLM-powered Task Automation in Android. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking (Washington D.C., DC, USA) (ACM MobiCom 24). Association for Computing Machinery, New York, NY, USA, 543557. https://doi.org/10.1145/3636534.3649379 [27] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, and Yu Qiao. 2024. OS-ATLAS: Foundation Action Model for Generalist GUI Agents. arXiv:2410.23218 [cs.CL] https://arxiv.org/ abs/2410.23218 [28] Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, and Zhen Xiao. 2024. Understanding the Weakness of Large Language Model Agents within Complex Android Environment. arXiv:2402.06596 [cs.AI] https://arxiv.org/abs/2402.06596 [29] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023. ReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning Representations (ICLR). [30] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. AppAgent: Multimodal Agents as Smartphone Users. arXiv:2312.13771 [cs.CV] https://arxiv.org/abs/2312.13771 [31] Danyang Zhang, Zhennan Shen, Rui Xie, Situo Zhang, Tianbao Xie, Zihan Zhao, Siyuan Chen, Lu Chen, Hongshen Xu, Ruisheng Cao, and Kai Yu. 2024. Mobile-Env: Building Qualified Evaluation Benchmarks for LLM-GUI Interaction. arXiv:2305.08144 [cs.AI] https: //arxiv.org/abs/2305. [32] Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. 2025. Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models. arXiv:2506.05176 [cs.CL] https://arxiv.org/abs/2506.05176 [33] Zhuosheng Zhang and Aston Zhang. 2024. You Only Look at Screens: Multimodal Chain-ofAction Agents. arXiv:2309.11436 [cs.CL] https://arxiv.org/abs/2309.11436 [34] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. 2024. GPT-4V(ision) is Generalist Web Agent, if Grounded. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=piecKJ2DlB"
        }
    ],
    "affiliations": [
        "Institute of Parallel and Distributed Systems (IPADS), Shanghai Jiao Tong University"
    ]
}