{
    "paper_title": "FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation",
    "authors": [
        "Huihan Wang",
        "Zhiwen Yang",
        "Hui Zhang",
        "Dan Zhao",
        "Bingzheng Wei",
        "Yan Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Synthesizing high-quality dynamic medical videos remains a significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, a full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) a unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) a linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) a residual value guidance module that provides fine-grained pixel-level guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEAT-S, with only 23\\% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEAT-L surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at https://github.com/Yaziwel/FEAT."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 ] . [ 1 6 5 9 4 0 . 6 0 5 2 : r FEAT: Full-Dimensional Efficient Attention Transformer for Medical Video Generation Huihan Wang1, Zhiwen Yang1, Hui Zhang2, Dan Zhao3, Bingzheng Wei4, and Yan Xu1((cid:0)) 1 School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China xuyan04@gmail.com 2 Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China 3 Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China 4 ByteDance Inc., Beijing 100098, China Abstract. Synthesizing high-quality medical videos remains significant challenge due to the need for modeling both spatial consistency and temporal dynamics. Existing Transformer-based approaches face critical limitations, including insufficient channel interactions, high computational complexity from self-attention, and coarse denoising guidance from timestep embeddings when handling varying noise levels. In this work, we propose FEAT, full-dimensional efficient attention Transformer, which addresses these issues through three key innovations: (1) unified paradigm with sequential spatial-temporal-channel attention mechanisms to capture global dependencies across all dimensions, (2) linear-complexity design for attention mechanisms in each dimension, utilizing weighted key-value attention and global channel attention, and (3) residual value guidance module that provides fine-grained pixellevel guidance to adapt to different noise levels. We evaluate FEAT on standard benchmarks and downstream tasks, demonstrating that FEATS, with only 23% of the parameters of the state-of-the-art model Endora, achieves comparable or even superior performance. Furthermore, FEATL surpasses all comparison methods across multiple datasets, showcasing both superior effectiveness and scalability. Code is available at here. Keywords: Video Generation Medical Video Efficient Transformer."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in diffusion models have revolutionized artificial intelligencegenerated content (AIGC) in medical imaging, enabling transformative applications in image synthesis [1], cross-modal translation [2], and image reconstruction Equal contribution: H. Wang and Z. Yang. (cid:0) Corresponding author: Y. Xu 2 Wang, Yang et al. [3]. While these models demonstrate remarkable capabilities in generating static medical images with spatial information, synthesizing high-fidelity dynamic medical videoswhich require modeling additional temporal dynamics and consistencyremains significant challenge. To this end, researchers have explored various approaches to encoding spatial-temporal dynamics [4,5,6,7,8,9,10], including pseudo-3D convolution [4], serial 2D+1D (spatial + temporal) convolutions [8], and spatial-temporal self-attention [5,7,6,9]. Given the ability of selfattention to capture long-range dependencies and the scalability of Transformers, most recent studies have largely embraced the Transformer architecture, employing cascading spatial and temporal self-attention mechanisms [7,9]. However, the current Transformer incorporating both spatial and temporal self-attention still faces three critical limitations: (1) Inadequate ChannelWise Interaction. Despite their sophisticated handling of spatial and temporal dimensions, existing architectures neglect building channel dependencies crucial for modeling feature compositions. Additionally, the impressive generation performance of diffusion models relies heavily on the denoising process while channel attention [11] has been widely proven to be effective for denoising. Omitting building interactions over such an important dimension hinders the model performance. (2) Prohibitive Computational Complexity. The self-attention mechanisms used to model both spatial and temporal dependencies suffer from quadratic computational complexity, which severely limits their practical applicability in medical videos with high resolution and many frames. (3) Coarse Denoising Guidance. In diffusion models, the model needs to adapt to inputs affected by different noise levels across various timesteps. Existing methods rely on timestep embeddings as global-level guidance, using adaptive layer normalization (adaLN) [12] to adapt to specific noise levels. However, this approach is too coarse and fails to account for dynamic interactions between noise patterns and video content. While recent work [7,13] has utilized attention maps from DINO [14] to account for content information for finer-grained guidance, this method introduces additional substantial computational overhead during training. Therefore, existing methods have drawbacks in achieving efficient and effective medical video generation. To address the aforementioned challenges, we present FEAT, full-dimension efficient attention Transformer for medical video generation through three key innovations: (1) Full-Dimensional Dependency Modeling. FEAT introduces unified paradigm with sequential spatial-temporal-channel attention, establishing global dependencies across all dimensions and enabling holistic feature modeling of medical videos. (2) Linear Complexity Design. FEAT replaces conventional self-attention with two computationally efficient components: (a) weighted keyvalue (WKV) attention [15,16,17,18] inspired by RWKV [15] for modeling spatial and temporal dependencies, and (b) global channel attention [11] for modeling channel dependencies. Both components achieve global dependencies within their respective dimensions while maintaining linear computational complexity [19]. (3) Residual Value Guidance. FEAT introduces novel residual value guidance module (ResVGM) that leverages input embeddingsencoding both video conFEAT: Full-Dimensional Efficient Attention Transformer 3 tent and specific noise patternsas fine-grained pixel-level guidance to adapt the model for processing input of different timesteps. The ResVGM is parameterefficient with negligible computational overhead while significantly improving generation performance. With these three innovations, FEAT achieves both efficient and effective medical video generation. Experiments show that small version of FEAT (denoted as FEAT-S), with only 23% of the parameters of the state-of-the-art model Endora [7], delivers comparable or even superior performance. Furthermore, the larger version, FEAT-L, outperforms all comparison methods across different datasets. Our contributions are three-fold: We propose FEAT, novel full-dimensional efficient attention Transformer for medical video generation. FEAT establishes global dependencies across all dimensions, including spatial, temporal, and channel, thereby enhancing the models ability to capture holistic relationships in medical videos. We replace the original self-attention mechanism, which suffers from quadratic computational complexity, with attention mechanisms that establish global dependencies with linear complexity, thereby enhancing model efficiency. We propose novel residual value guidance module (ResVGM) that leverages input embeddings with both video content and specific noise patterns to provide fine-grained pixel-level guidance. This allows FEAT to effectively adapt to different timesteps with minimal computational overhead, significantly improving generation performance."
        },
        {
            "title": "2 Method",
            "content": "We first introduce the preliminaries of the diffusion model for video generation in Section. 2.1. In Section 2.2, we then present the details of the proposed full-dimensional efficient attention Transformer (FEAT), including its overall architecture and the specific efficient attention mechanism tailored to each dimension. Finally, in Section 2.3, we introduce the novel residual value guidance module (ResVGM), which provides fine-grained pixel-level guidance for adapting to different denoising timesteps."
        },
        {
            "title": "2.1 Preliminaries",
            "content": "Diffusion probabilistic models have emerged as groundbreaking paradigm in generative modeling, demonstrating remarkable potential for image and video synthesis. These models operate by learning to transform random noise sampled from standard normal distribution p(xT ) = (0, I) into high-fidelity data samples through an iterative denoising procedure. The forward diffusion process gradually corrupts input data x0 by adding Gaussian noise across timesteps. This is defined by the transition q(xtxt1), with the marginal distribution at timestep expressed as: q(xtx0) = (αtx0, σ2 I), where the coefficients of αt and σt are designed such that xT convergence to (0, I) as [20]. In the 4 Wang, Yang et al. Fig. 1. The pipeline of FEAT for medical video generation. (a) Architecture of conventional models using cascaded spatial-temporal Transformer blocks. (b) Architecture of FEAT, which incorporates cascaded spatial-temporal-channel Transformer blocks. (c) Details of the conventional Transformer block, featuring quadratic computational complexity for self-attention and global timestep guidance. (d) Details of the Transformer block in FEAT, utilizing attention with linear computational complexity and guidance from both global timestep and pixel-level residual value Z. reverse diffusion process, noise prediction network ϵθ(xt, t) parameterizes the transition p(xt1xt), iteratively denoising xt to recover the data distribution. The training process involves optimizing the evidence lower bound (ELBO) optimization [20]: ELBO = (cid:104) ϵθ (αtx0 + σtϵ; t) ϵ2 2 (cid:105) , (1) where ϵ (0, I) and follows uniform sampling. Since training diffusion models directly in high-resolution pixel space can be computationally expensive, we adopt the widely used approach of latent diffusion models [21,22], performing the diffusion process in an encoded latent space with the help of pretrained autoencoder [21] for both encoding and decoding."
        },
        {
            "title": "2.2 Full-Dimensional Efficient Attention Transformer",
            "content": "Existing Transformer architectures for medical video generation often face three major drawbacks: insufficient channel-wise interaction, excessive computational complexity due to self-attention, and coarse denoising guidance from the timestep. To overcome these challenges, we propose the full-dimensional efficient attention Transformer (FEAT), as illustrated in Figure. 1, which introduces three key innovations: (1) Unlike conventional architectures that primarily establish spatial-temporal dependencies (as shown in Figure. 1 (a)), FEAT builds global dependencies across all dimensions, including spatial, temporal, and channel dimensions (as shown in Figure. 1 (b)). (2) To mitigate the computational burden imposed by self-attention in traditional Transformer blocks (as seen in Figure. 1 (c)), FEAT leverages attention mechanisms that achieve global attention with linear computational complexity across all dimensions (as shown in Figure. 1 FEAT: Full-Dimensional Efficient Attention Transformer 5 Fig. 2. Three distinct Transformer blocks in FEAT. (a) Spatial Transformer block with WKV attention [17]. (b) Temporal Transformer block with WKV attention [17]. (c) Channel Transformer block with global channel attention [11]. F, H, W, and represent the frame number, height, width, and channel of the input feature, respectively. (d)). (3) To address the coarse, global-level guidance that struggles to adapt to varying noise levels at different timesteps, FEAT introduces residual value guidance module (ResVGM) for fine-grained, pixel-level denoising (as shown in Figure. 1 (d)). In the following subsection, we will describe the architecture of the Transformer blocks that establish spatial, temporal, and channel dependencies in detail. The details of the ResVGM are elaborated in Subsection 2.3. To ensure efficient modeling across all dimensions, we design different Transformer blocks with efficient attention for each dimension. Given the exceptional performance of weighted key-value (WKV) attention [17,18] and global channel attention [11] in denoising, coupled with their ability to achieve global attention with linear computational complexity, we choose to apply them to denoising diffusion video generation. Specifically, for the spatial and temporal Transformer blocks, we adopt the WKV attention mechanism as described in [17], as illustrated in Figure 2 (a) and (b). To better accommodate the spatial and temporal dimensions, we modify the original token-shift mechanism from [17], which is designed to enhance locality. For the spatial Transformer block, we introduce 2D depth-wise convolution [23] (denoted Shift S) to strengthen locality in the spatial dimension. Similarly, for the temporal Transformer block, we apply 1D depth-wise convolution (denoted Shift T) to enhance locality in the temporal dimension. For the channel Transformer block, we directly employ the global channel attention mechanism proposed by [11], as depicted in Figure 2 (c). With these three Transformer blocks sequentially cascaded, FEAT can efficiently establish global dependencies across spatial, temporal, and channel dimensions, enabling holistic feature modeling for medical videos."
        },
        {
            "title": "2.3 Residual Value Guidance Module\nMost existing video diffusion models employ the timestep t as global guidance\nto adapt to specific noise levels in the denoising process. However, this method\nis relatively coarse and insufficient for content-dependent denoising. To over-\ncome this limitation, we propose integrating the input embedding as an addi-\ntional, fine-grained guidance. During the denoising process, the input embed-\nding—obtained via convolution of the input (or the denoising output at the\nprevious timestep)—encodes both the generated video content and the associ-\nated noise patterns. These components provide crucial guidance for achieving",
            "content": "6 Wang, Yang et al. Fig. 3. The schematic diagram of the proposed ResVGM, where (a) represents the original framework, and (b) denotes the framework with ResVGM incorporated to different Transformer blocks. The frameworks primarily illustrate operations surrounding the attention mechanism in Transformer blocks, where ResVGM is integrated, while other modules are omitted for simplicity. content-dependent denoising at specific noise levels. As illustrated in Figure 3, we incorporate the input embedding to all the Transformer blocks as finegrained guidance. Specifically, for the i-th Transformer block, is added as residual value [24] to interact with the input value Vi in the attention and the output hidden Hi as follows: Hi = LinAttention(Qi, Ki, Vi + λ1 cZ) + λ c(Z Vi), (2) where LinAttention() denotes the two attention mechanismsWKV attention and global channel attentionwhich both exhibit linear computational complexity. Qi, Ki, and Vi denote the query, key, and value, respectively. Note that Qi RC are two learnable weighting can be omitted in WKV attention. λ1 parameters. This process ensures that feature extraction across all Transformer blocks in the model is gradually refined based on the input video content and noise level. The ResVGM introduces negligible additional parameters and computational overhead, while significantly improving performance. , λ"
        },
        {
            "title": "3.1 Experiment Settings",
            "content": "Datasets and Evaluation. Our experimental evaluation is conducted on two publicly available medical video datasets: Colonoscopic [25] and Kvasir-Capsule [26]. Adhering to standardized video processing protocols [9], we preprocess the data by uniformly extracting 16-frame sequences from continuous videos through fixed-interval sampling. All frames are resized to 128128 pixel resolution during model training to ensure dimensional consistency. For quantitative assessment, we employ four established evaluation metrics: Fréchet Inception Distance (FID) [27], Inception Score (IS) [28], Fréchet Video Distance (FVD) [29], and its content-debiased variant CD-FVD [30]. Following the evaluation framework of StyleGAN-V [31], we compute FVD scores through statistical analysis of 2048 FEAT: Full-Dimensional Efficient Attention Transformer 7 Table 1. Quantitative Comparisons on Medical Video Datasets. Colonoscopic [25] Kvasir-Capsule [26] Method StyleGAN-V [31] (CVPR22) 2110.7 LVDM [22] (Arxiv23) 1036.7 MoStGAN-V [32] (CVPR23) 468.5 Endora [7] (MICCAI24) 460.7 415.4 FEAT-S(Ours) 351.1 FEAT-L(Ours) FVD CD-FVD FID IS FVD CD-FVD FID IS 31.61 2.77 200.90 1.46 17.34 2.53 10.61 2.54 9.97 2.65 8.65 2.70 226.14 2.12 183.5 96.85 1.93 1027.8 53.17 3.37 13.41 3.90 13.34 3.96 12.31 4.01 1032.8 792.9 592.0 545.3 444.0 397.0 898.4 615.4 168.3 152.3 138.2 116. 82.8 72.3 72.2 59.2 Parameters(M) FLOPs(G) 673.7 158.0 629.0 465.8 118.7 472.1 Fig. 4. Qualitative Comparison on Colonoscopic and Kvasir-Capsule Datasets. video samples, with each sample maintaining the complete 16-frame temporal structure to preserve motion dynamics and temporal coherence. Implementation Details. Our implementation employs the AdamW optimizer with fixed learning rate of 1 104 across all architectural configurations. Data preprocessing incorporates basic horizontal flipping as the sole augmentation strategy to preserve feature authenticity. The model architecture integrates pretrained variational autoencoder from the Stable Diffusion framework [33] as its foundational component, enhanced by 27 specialized neural modules organized in an interleaved configuration: 9 spatial processors for geometric feature extraction, 9 temporal analyzers for motion pattern modeling, and 9 channel operators for cross-dimensional feature interaction. Hidden dimensions are configured as d=512 for small (S) models and d=1024 for large (L) model variants to accommodate computational constraints. Following established GAN training protocols [12], we implement exponential moving average (EMA) stabilization [34] with all final outputs generated from converged EMA parameters, ensuring training stability and output consistency. 8 Wang, Yang et al. Table 2. Semi-supervised Classification Result (F1 Score) on PolyDiag [35] . Table 3. Ablation Studies of Proposed Components on Colonoscopic [25] Dataset. Method Supervised-only LVDM Endora FEAT-S(Ours) FEAT-L(Ours) Colonoscopic [25] 74.5 76.2 (+1.7) 87.0 (+12.5) 89.9 (+15.4) 91.3 (+16.8) WKV Channel ResVGM FVD FID 990.0 23.45 788.4 20.16 583.6 16.98 415.4 13."
        },
        {
            "title": "3.2 Comparison with State-of-the-arts",
            "content": "We conduct performance comparison by replicating several advanced video generation models designed for general scenarios on the medical video datasets, including StyleGAN-V [31], MoStGAN-V [32], LVDM [22], and Endora [7]. As shown in Table. 1, FEAT-S achieves comparable performance to Endora while requiring significantly fewer parameters and lower computational costs. Meanwhile, FEAT-L outperforms state-of-the-art methods. The visual qualitative comparison results in Figure. 4 demonstrate that FEAT can generate videos with higher quality and consistency."
        },
        {
            "title": "3.3 Further Empirical Studies",
            "content": "In this section, we demonstrate the data augmentation effects of leveraging the videos generated by our FEAT for downstream tasks, and conduct rigorous ablation experiments on the proposed improvements. Downstream Task. We explore the use of generated videos as unlabeled data for semi-supervised learning, specifically leveraging the FixMatch framework [36] on video-based disease diagnosis benchmarks, such as PolyDiag [35]. For this experiment, we randomly select 40 labeled videos (nl = 40) from the PolyDiag training set and use 200 generated videos (nu = 200) from Colonoscopic [25] as unlabeled data. The F1 scores for disease diagnosis, along with the performance improvements over the supervised-only baseline, are presented in Table. 2. The results clearly demonstrate that data generated by FEAT significantly boosts the performance of downstream tasks compared to both the supervised learning baseline and other video generation techniques, thereby confirming FEATs effectiveness as reliable video data augmenter for video-based analysis tasks. Ablation Studies. Table. 3 presents an ablation study to evaluate the key components of the proposed FEAT-S model. We begin with baseline that employs simple spatial-temporal Transformer diffusion model, without incorporating any of the proposed strategies. Next, we incrementally add the three proposed design strategies: WKV attention, channel attention and ResVGM. The results clearly show that each strategy contributes to progressive improvement in the models performance, highlighting the essential role of these design choices in enhancing the effectiveness of the medical video generation model. FEAT: Full-Dimensional Efficient Attention Transformer"
        },
        {
            "title": "4 Conclusion",
            "content": "This paper introduces FEAT, novel full-dimensional efficient attention Transformer that significantly advances medical video generation. FEAT addresses three key challengeslimited channel-wise interaction, prohibitive computational cost, and coarse denoising guidancethrough three core innovations. First, unified spatial-temporal-channel attention paradigm enables holistic feature modeling across all dimensions. Second, linear-complexity attention design makes it scale efficiently to high-resolution videos. Third, lightweight residual-value guidance module adaptively refines denoising, optimizing generation performance at negligible extra computational cost. Experimental results demonstrate that FEAT outperforms existing methods in terms of both efficiency and effectiveness, marking substantial step forward in the field of medical video generation. Future work will extend FEAT to additional imaging modalities and conduct more comprehensive evaluations [30]. Acknowledgments. This work is supported by the National Natural Science Foundation in China under Grant 62371016 and U23B2063, the Bejing Natural Science Foundation Haidian District Joint Fund in China under Grant L222032, the Fundamental Research Funds for the Central University of China from the State Key Laboratory of Software Development Environment in Beihang University in China, the 111 Proiect in China under Grant B13003, the SinoUnion Healthcare Inc. under the eHealth program, and the high performance computing (HPC) resources at Beihang University. Disclosure of Interests. We have no conflicts of interest to disclose."
        },
        {
            "title": "References",
            "content": "1. Dorjsembe, Z., Odonchimed, S., Xiao, F.: Three-dimensional medical image synthesis with denoising diffusion probabilistic models. In: Medical imaging with deep learning (2022) 2. Wang, Z., Zhang, L., Wang, L., Zhang, Z.: Soft masked mamba diffusion model for ct to mri conversion. arXiv preprint arXiv:2406.15910 (2024) 3. Liu, J., Anirudh, R., Thiagarajan, J.J., He, S., Mohan, K.A., Kamilov, U.S., Kim, H.: Dolce: model-based probabilistic diffusion framework for limited-angle ct reconstruction. In: Proceedings of the IEEE/CVF International conference on computer vision. pp. 1049810508 (2023) 4. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., et al.: Make-a-video: Text-to-video generation without textvideo data. arXiv preprint arXiv:2209.14792 (2022) 5. Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z., Navasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zeroshot video generators. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1595415964 (2023) 6. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., Shan, Y.: Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 73107320 (2024) 10 Wang, Yang et al. 7. Li, C., Liu, H., Liu, Y., Feng, B.Y., Li, W., Liu, X., Chen, Z., Shao, J., Yuan, Y.: Endora: Video generation models as endoscopy simulators. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 230240. Springer (2024) 8. Xing, J., Xia, M., Liu, Y., Zhang, Y., Zhang, Y., He, Y., Liu, H., Chen, H., Cun, X., Wang, X., et al.: Make-your-video: Customized video generation using textual and structural guidance. IEEE Transactions on Visualization and Computer Graphics (2024) 9. Ma, X., Wang, Y., Jia, G., Chen, X., Liu, Z., Li, Y.F., Chen, C., Qiao, Y.: Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048 (2024) 10. Tian, Q., Liao, H., Huang, X., Yang, B., Lei, D., Ourselin, S., Liu, H.: Endomamba: An efficient foundation model for endoscopic videos (2025), https://arxiv.org/abs/ 2502.19090 11. Zamir, S.W., Arora, A., Khan, S., Hayat, M., Khan, F.S., Yang, M.H.: Restormer: Efficient transformer for high-resolution image restoration. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 57285739 (2022) 12. Peebles, W., Xie, S.: Scalable diffusion models with transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 41954205 (2023) 13. Wang, Z., Zhang, L., Wang, L., Zhu, M., Zhang, Z.: Optical flow representation alignment mamba diffusion model for medical video generation. arXiv preprint arXiv:2411.01647 (2024) 14. Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF international conference on computer vision. pp. 96509660 (2021) 15. Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., Cao, H., Cheng, X., Chung, M., Grella, M., et al.: Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023) 16. Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Ferdinan, T., Hou, H., Kazienko, P., et al.: Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence. arXiv preprint arXiv:2404.05892 3 (2024) 17. Duan, Y., Wang, W., Chen, Z., Zhu, X., Lu, L., Lu, T., Qiao, Y., Li, H., Dai, J., Wang, W.: Vision-rwkv: Efficient and scalable visual perception with rwkv-like architectures. arXiv preprint arXiv:2403.02308 (2024) 18. Yang, Z., Li, J., Zhang, H., Zhao, D., Wei, B., Xu, Y.: Restore-rwkv: Efficient and effective medical image restoration with rwkv. arXiv preprint arXiv:2407.11087 (2024) 19. Shen, Z., Zhang, M., Zhao, H., Yi, S., Li, H.: Efficient attention: Attention with linear complexities. In: Proceedings of the IEEE/CVF winter conference on applications of computer vision. pp. 35313539 (2021) 20. Kingma, D., Salimans, T., Poole, B., Ho, J.: Variational diffusion models. Advances in neural information processing systems 34, 2169621707 (2021) 21. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 1068410695 (2022) 22. He, Y., Yang, T., Zhang, Y., Shan, Y., Chen, Q.: Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221 (2022) FEAT: Full-Dimensional Efficient Attention Transformer 11 23. Chollet, F.: Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 12511258 (2017) 24. Zhou, Z., Wu, T., Jiang, Z., Lan, Z.: Value residual learning for alleviating attention concentration in transformers. arXiv preprint arXiv:2410.17897 (2024) 25. Mesejo, P., Pizarro, D., Abergel, A., Rouquette, O., Beorchia, S., Poincloux, L., Bartoli, A.: Computer-aided classification of gastrointestinal lesions in regular colonoscopy. IEEE transactions on medical imaging 35(9), 20512063 (2016) 26. Borgli, H., Thambawita, V., Smedsrud, P.H., Hicks, S., Jha, D., Eskeland, S.L., Randel, K.R., Pogorelov, K., Lux, M., Nguyen, D.T.D., et al.: Hyperkvasir, comprehensive multi-class image and video dataset for gastrointestinal endoscopy. Scientific data 7(1), 283 (2020) 27. Parmar, G., Zhang, R., Zhu, J.Y.: On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222 5(14), 6 (2021) 28. Saito, M., Matsumoto, E., Saito, S.: Temporal generative adversarial nets with singular value clipping. In: Proceedings of the IEEE international conference on computer vision. pp. 28302839 (2017) 29. Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly, S.: Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717 (2018) 30. Ge, S., Mahapatra, A., Parmar, G., Zhu, J.Y., Huang, J.B.: On the content bias in fréchet video distance. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 72777288 (2024) 31. Skorokhodov, I., Tulyakov, S., Elhoseiny, M.: Stylegan-v: continuous video generator with the price, image quality and perks of stylegan2. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 36263636 (2022) 32. Shen, X., Li, X., Elhoseiny, M.: Mostgan-v: Video generation with temporal motion styles. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 56525661 (2023) 33. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127 (2023) 34. Li, C., Zhang, Y., Liang, Z., Ma, W., Huang, Y., Ding, X.: Consistent posterior distributions under vessel-mixing: regularization for cross-domain retinal artery/vein classification. In: 2021 IEEE International Conference on Image Processing (ICIP). pp. 6165. IEEE (2021) 35. Tian, Y., Pang, G., Liu, F., Liu, Y., Wang, C., Chen, Y., Verjans, J., Carneiro, G.: Contrastive transformer-based multiple instance learning for weakly supervised polyp frame detection. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 8898. Springer (2022) 36. Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.A., Cubuk, E.D., Kurakin, A., Li, C.L.: Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in neural information processing systems 33, 596608 (2020)"
        }
    ],
    "affiliations": [
        "ByteDance Inc., Beijing 100098, China",
        "Department of Biomedical Engineering, Tsinghua University, Beijing 100084, China",
        "Department of Gynecology Oncology, National Cancer Center/National Clinical Research Center for Cancer/Cancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing 100021, China",
        "School of Biological Science and Medical Engineering, State Key Laboratory of Software Development Environment, Key Laboratory of Biomechanics and Mechanobiology of Ministry of Education, Beijing Advanced Innovation Center for Biomedical Engineering, Beihang University, Beijing 100191, China"
    ]
}