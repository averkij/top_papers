{
    "paper_title": "Towards Open-Vocabulary Industrial Defect Understanding with a Large-Scale Multimodal Dataset",
    "authors": [
        "TsaiChing Ni",
        "ZhenQi Chen",
        "YuanFu Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present IMDD-1M, the first large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learning for manufacturing and quality inspection. IMDD-1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables a wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train a diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as a generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledge-grounded manufacturing intelligence."
        },
        {
            "title": "Start",
            "content": "Towards Open-Vocabulary Industrial Defect Understanding with Large-Scale Multimodal Dataset"
        },
        {
            "title": "ZhenQi Chen YuanFu Yang",
            "content": "Institute of Intelligent Systems, National Yang Ming Chiao Tung University 5 2 0 2 0 3 ] . [ 1 0 6 1 4 2 . 2 1 5 2 : r Figure 1. Overview of the proposed IMDD-1M dataset, its diverse industrial domains, corresponding downstream tasks, and potential extensions to vision-language model applications."
        },
        {
            "title": "Abstract",
            "content": "the first We present IMDD-1M, large-scale Industrial Multimodal Defect Dataset comprising 1,000,000 aligned image-text pairs, designed to advance multimodal learnIMDDing for manufacturing and quality inspection. 1M contains high-resolution real-world defects spanning over 60 material categories and more than 400 defect types, each accompanied by expert-verified annotations and fine-grained textual descriptions detailing defect location, severity, and contextual attributes. This dataset enables wide spectrum of applications, including classification, segmentation, retrieval, captioning, and generative modeling. Building upon IMDD-1M, we train diffusion-based vision-language foundation model from scratch, specifically tailored for industrial scenarios. The model serves as generalizable foundation that can be efficiently adapted to specialized domains through lightweight fine-tuning. With less than 5% of the task-specific data required by dedicated expert models, it achieves comparable performance, highlighting the potential of data-efficient foundation model adaptation for industrial inspection and generation, paving the way for scalable, domain-adaptive, and knowledgegrounded manufacturing intelligence. 1. Introduction Industrial defect detection is critical for ensuring product quality and operational efficiency in modern manufacturing. Traditional manual inspection suffers from high labor costs, subjective judgment, and limited throughput, driving widespread adoption of automated optical inspection (AOI) systems across semiconductor, electronics, and precision manufacturing sectors. Despite significant advances, current AOI systems remain constrained by high false alarm rates, poor adaptability to novel defect patterns, and inability to generalize across diverse manufacturing contexts. To address these limitations, we introduce IMDD-1M, largescale industrial defect dataset with multimodal annotations, along with suite of downstream tasks and VLM applications (as shown in Figure 1). lack unified multi-task capabilities, Existing deep learning approaches further expose these limitations. Specialized architectures such as You Only Look Once (YOLO)-based detectors excel at specific tasks but require extensive pixel-level annotations, struggle with rare defects, and operate as black-box discriminators without semantic interpretability. Recent VLMs such as Contrastive Language-Image Pre-training (CLIP) [18], ALIGN [8], and Flamingo [1] have revolutionized natural image understanding by aligning visual and textual semantics. However, industrial defects are subtle, localized, and require domainspecific terminology (e.g., delamination, solder void). Trained predominantly on natural images, existing VLMs lack specialized knowledge for industrial visual-semantic correlations. To bridge this gap, we introduce IMDD-1M, the first million-scale industrial defect dataset with aligned imagetext pairs, together with diffusion-based multimodal foundation model that unifies generative and discriminative capabilities for defect generation, segmentation, detection, and semantic grounding. Our key contributions are: (1) IMDD-1M, million-scale industrial dataset spanning 421 defect types across 63 manufacturing domains (automotive, electronics, metals, textiles, packaging). Each sample contains expert-verified image-text annotations, surpassing existing benchmarks by approximately two orders of magnitude. (2) hybrid annotation pipeline combining expert verification with Large Language Model (LLM)-assisted caption generation, ensuring domain-specific technical accuracy and linguistic consistency across diverse industrial terminologies. (3) diffusion-based multimodal foundation model trained on IMDD-1M that integrates discriminative capabilities (segmentation and detection) with generative ones (synthesis and augmentation) within single architecture, demonstrating effective performance in industrial anomaly understanding. Table 1. Comparison of industrial defect datasets. Unlike previous datasets, IMDD-1M introduces large-scale image-text pairs, enabling multimodal learning in the industrial domain. Dataset Year # Images # Domains Text Annotations DAGM [25] KolektorSDD [22] MVTec AD [2] BTAD [16] VisA [26] Real-IAD [24] 2016 2019 2019 2021 2022 2024 1.5K 400 5.4K 2.5K 10.8K 67K 1 (Synthetic) 1 (Electronics) 15 (Objects) 3 (Mixed) 12 (Packaging) 30 (Mixed) No No No No No No IMDD-1M (Ours) 1.24M 63 (Diverse) Yes (Image-Text Pairs) (4) Comprehensive evaluation protocols and benchmarks for defect detection, segmentation, and synthesis, providing standardized metrics for systematic assessment in practical industrial environments. 2. Related Work Automated defect detection has evolved through benchmark datasets of increasing scale and complexity. Early efforts like DAGM [25] and KolektorSDD [22] provided limited synthetic or single-domain data. MVTec AD [2] introduced pixel-level annotations across multiple categories but reached performance saturation. Subsequent datasets including BTAD [16], VisA [26], and Real-IAD [24] progressively improved realism and diversity, while domainspecific datasets emerged for steel [21], X-ray inspection [15], circuit boards [6], and other applications [5, 7, 20]. However, all remain constrained by limited scale and lack multimodal annotations. image datasets [4], COCO [11], and LAION [19] enabled advances in VLMs but exhibit fundamental mismatches with industrial needs: defects are subtle and localized, requiring specialized terminology and pixel-level precision. Our IMDD-1M bridges this gap as the first large-scale industrial dataset with image-text pairs, enabling multimodal learning for defect analysis (as shown in Table 1). ImageNet such as Natural 3. Large-Scale Industrial Defect Dataset 3.1. Data Collection We consolidate large-scale data from public benchmarks, web mining, and industrial partnerships. Public datasets including DAGM [25], MVTec AD [2], KolektorSDD [22], VisA [26], and BTAD [16] establish baseline coverage for canonical defect categories. We perform extensive web mining across GitHub, RoboFlow, PaddlePaddle, and Tianchi using multilingual queries in English, Chinese, and Japanese to capture diverse manufacturing products and defect terminologies. We collaborate with industrial partners in petrochemical, metal processing, and powder metallurgy sectors to acquire authentic production-line imagery. Enterprise samples enFigure 2. Illustrative overview of IMDD-1M showing diverse image-text pairs across multiple industrial domains, each with expert-verified annotations capturing fine-grained defect types, materials, and manufacturing contexts. The dataset serves as large-scale foundation for vision-language modeling in industrial inspection. compass polymer containers, chemical pipelines, castings, forgings, and sintered components exhibiting defects including corrosion, delamination, voids, inclusions, and surface pitting. All industrial data are anonymized. We implement stratified sampling ensuring balanced representation across product types and defect severities. The 18-month collection applies systematic quality control rejecting lowresolution or ambiguous samples. 3.2. Annotation Framework Each image pairs with textual descriptions capturing product identity, defect morphology, and manufacturing causes. All annotations are performed by domain experts ensuring accurate capture of subtle defect characteristics and specialized terminology. Figure 2 illustrates representative samples from the IMDD-1M dataset, showcasing the diversity of image-text pairs across industrial domains. Industrial terminology is standardized across languages with controlled vocabulary of over 500 specialized terms. Each description follows structured template including product category, material composition, defect type, spatial location, and root causes. Annotations incorporate morphological descriptors including orientation (e.g., vertical crack), scale (e.g., microscopic void), localization (e.g., upper-left corner), and pattern (e.g., radial distribution). These attributes enable vision-language tasks such as root cause analysis and process optimization. This expert-driven approach offers greater precision in multimodal alignment compared to crowdsourcing. Figure 3. Dataset analysis. (1) Sample distribution among the top 100 defect categories (log-scaled). (2) Three-step workflow for dataset construction. Figure 4. Dataset composition. (1) Distribution of normal versus anomaly samples. (2) Pie chart showing dataset composition across domains. Table 2. Summary of Industrial Defect Datasets. Note: Enterprise collaboration data from petrochemical, metal processing, and powder metallurgy sectors are not included due to confidentiality. Dataset Cls. Def. types Normal Anomaly All BTAD SSGD MVTec AD MVTec AD2 VisA Magnetic Tile NEU-DET TIAN CHI Aluminum TIAN CHI Fabric TIAN CHI Bai Jiu SDI & SPDI Steel Pipe WM-811K ICCAD Tungsten Inert Gas Semiconductor Solar Cell VOC2007 Water Cooled Aircraft Skin OLED Solar Cell Crack Gear PCB Concrete Decks Wall Sum 3 1 15 8 12 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 63 3 7 90 32 137 6 7 22 34 14 2 7 9 2 6 2 8 4 1 5 6 2 7 2 2 2 421 2250 0 4096 4705 9621 952 0 0 0 1145 0 0 0 160963 30080 27420 1545 0 0 0 674 0 0 0 20000 20000 2000 290 2504 1258 3299 1200 392 1816 3311 4762 2225 35 6966 811457 4053 14978 6696 455 805 321 4281 1854 364 1719 2100 11688 10000 56099 2540 2504 5354 8004 10821 1344 1816 3311 4762 3370 35 6966 811457 165016 45058 34116 2000 805 321 4281 2528 364 1719 2100 31688 30000 58099 285451 1240379 3.3. Dataset Statistics types. All The final dataset contains over 1.24 million highresolution image-text pairs spanning 63 industrial product categories and 421 defect images are standardized at 512512 pixels, ensuring consistent input dimensions. Text descriptions average 42 words, providing rich semantic context without excessive verbosity. Table 2 provides comprehensive summary of incorporated datasets. The corpus comprises 285,451 normal and 954,928 anomaly samples, integrating diverse public benchmarks alongside large-scale industrial data. Figure 3 illustrates proportional composition across product domains and defect categories, while Figure 4 presents the balance between normal and anomaly samples. Figure 5 analyzes the anomaly ratio within each dataset, revealing substantial imbalance patterns across industrial domains. Figure 6 highlights the top 10 datasets ranked by object class and defect type count, showing datasets such as MVTec AD [2] and VisA [26] offer the broadest coverage of categories and defect variations. Industrial data samples introduce authentic variability, such as uneven illumination, complex backgrounds, and subtle defect manifestations, forming foundational resource for industrial defect detection and VLMs development. Figure 5. Anomaly ratio distribution across datasets. Each bar represents the normalized proportion of anomaly and normal samples within specific dataset, illustrating data imbalance and diversity across industrial domains. Figure 6. Top 10 datasets ranked by (1) object class count and (2) defect type count. MVTec AD and VisA stand out for their broad coverage and diversity across industrial components and surface conditions. 4. Method We train text-conditioned diffusion model from scratch on IMDD-1M, then transfer learned features to downstream tasks. The framework comprises an industrial diffusion U-shaped convolutional network (U-Net, 860M parameters), an implicit captioner (0.3M parameters), and Mask2Former [3] generator (45M parameters). 4.1. Problem Formulation Given image RHW 3 and optional text t, we predict mask {0, 1}HW with semantic label. We train on base categories Ctrain and test on disjoint Ctest with only category names provided. 4.2. Architecture We train text-conditioned diffusion U-Net from scratch on IMDD-1M, where an implicit captioner generates pseudo text embeddings from visual features when captions are unavailable. The mask generator processes diffusion features to predict masks and embeddings for openvocabulary classification.Figure 7 illustrates the overall architecture of our proposed framework. Figure 7. Overview of our method. An implicit captioner encodes the defect image into text embedding, which, together with the image, is fed into frozen diffusion U-Net to extract multi-scale features. VAE decoder reconstructs features, while mask generator predicts binary masks and embeddings. Classification is performed via dot products between mask and text embeddings (orange/green boxes) under cross-entropy and grounding supervision. 4.3. Industrial Diffusion Model We adopt Stable Diffusion v1.5 U-Net with random initialization. The encoder has four blocks with channels 320, 640, 1280, 1280 at strides 1, 2, 4, 8. The decoder mirrors this with skip connections. Cross-attention layers inject text conditioning after every ResNet block. Given image I, we encode via frozen Variational Autoencoder (VAE): z0 = EVAE(I) R4hw, = H/8. (1) We use frozen Stable Diffusion VAE for 8-fold compression. Noise follows Denoising Diffusion Probabilistic Models (DDPM): αtz0 + zt = 1 αtϵ, ϵ (0, I), (2) with αt = (cid:81)t 104, βT = 0.02 over = 1000 steps. s=1(1 βs) and linear schedule β1 = IMDD-1M images pair with captions like metal plate with scratches. We encode via frozen CLIP: eT = CLIPtext(t) R768. (3) The U-Net predicts noise conditioned on text via crossattention: ϵθ(zt, t, eT ) = U-Netθ(zt, t, eT ). We minimize diffusion loss: Ldiff = Ez0,ϵ,t (cid:2)ϵ ϵθ(zt, t, eT )2 2 (cid:3) , (4) (5) where Uniform(1, ). Training runs 100 epochs on 1,240,379 images with batch size 256 across 8 H100 GPUs, requiring 72 hours. All 860M parameters train from random initialization. 4.4. Implicit Captioner Downstream datasets typically lack captions, providing only categorical labels or binary normal-versus-defective annotations. This poses challenge for extracting diffusion features which require text conditioning. We address this by introducing an implicit captioner that synthesizes pseudo text embeddings directly from images, eliminating the need for explicit captions during both training and inference. The module consists of frozen CLIP image encoder followed by trainable two-layer MLP projecting 512-dimensional CLIP embeddings into the 768-dimensional text embedding space: timp = MLPϕ(V(I)) = W2 GELU(W1 V(I) + b1) + b2 (6) During Stage 1 pretraining, we train the implicit captioner jointly with the diffusion U-Net via stochastic conditioning. For each training sample, we randomly select whether to condition on ground-truth captions or implicit embeddings with equal probability: (cid:40) eT timp prob. 0.5 prob. 0.5 (7) This training strategy encourages the implicit embeddings to serve as effective substitutes for real text embeddings. We further enforce alignment through an auxiliary cosine similarity loss: Limp = 1 tT impeT timpeT . (8) 4.5. Feature Extraction After pretraining, we freeze the diffusion model and extract features via single forward pass. Given image I, we add noise at = 50 providing optimal semantic-spatial balance: It = α50I + 1 α50ϵ. (9) We encode to latent zt = EVAE(It), generate implicit caption timp = MLPϕ(V(I)), and extract features: {fℓ}4 ℓ=1 = U-Netθ(zt, 50, timp). (10) Features have resolutions {h, h/2, h/4, h/8} with channels {320, 640, 1280, 1280}. 4.6. Mask Generation and Classification The mask generator adopts Mask2Former with pixel decoder and transformer decoder. The pixel decoder implements Feature Pyramid Network (FPN) producing R256hw. The transformer decoder uses 100 learnable queries attending to pixel features, producing masks {mi}100 i=1. We supervise with binary cross-entropy: i=1 and embeddings {zi}100 Lmask = (cid:80) i,j [Mij log mij + (1 Mij) log(1 mij)] . (11) classification, training For encode = [CLIPtext(c1), . . . , CLIPtext(cK)]. bedding zi with label yi: with categories category via CLIP labels we forming For mask emLcls = 1 (cid:88) i= CE (cid:0)Softmax(zi TT /τ ), yi (cid:1) . (12) With captions only, we extract nouns as pseudo-labels. word, we comm=1 with nouns C(m) Given batch {(I(m), s(m))}B pute grounding similarity: g(I(m), s(m)) = 1 Kw (cid:80)Kw k=1 (cid:80)N i=1 p(zi)k zi, CLIPtext(wk) (13) where p(zi)k denotes the k-th element of the softmaxnormalized similarity. We apply bidirectional contrastive loss: Lground = 1 (cid:34) (cid:88) m=1 log (cid:80)B exp(g(I(m), s(m))/τ ) n=1 exp(g(I(m), s(n))/τ ) exp(g(I(m), s(m))/τ ) n=1 exp(g(I(n), s(m))/τ ) (cid:80)B + log 4.7. Training Protocol Stage 1 trains entire diffusion model from scratch on IMDD-1M for 100 epochs: LStage1 = Ldiff + 0.3Limp. (15) All parameters train: U-Net (860M) and implicit captioner (0.3M). AdamW optimizer, learning rate 1 104, batch size 256, 72 hours on 8 H100 GPUs. Stage 2 freezes diffusion model, trains mask generator on downstream datasets for 50 epochs: LStage2 = Lmask + 0.5Lcls/ground. (16) Mask generator (45M) trains with AdamW, learning rate 5 105, batch size 16, 4 hours on 8 H100 GPUs. At test time with novel categories Ctest, we extract features, generate masks and embeddings, and classify via ˆyi = arg maxc p(zi, Ctest)c. Inference requires 0.35s per image on A100 GPU. 5. Experiments 5.1. Implementation Details We provide complete architecture, training, and evaluation details in the supplementary material. Our model comprises 890M parameters with 0.35s inference time per image on an A100 GPU. We evaluate generative quality using the Frechet Inception Distance (FID) and Inception Score (IS), computed between generated and real defect samples. 5.2. Text-Guided Defect Generation After training on IMDD-1M, our model generates realistic defect patterns from textual descriptions such as bottle with contamination or metal surface with oxidation. This demonstrates meaningful multimodal representations learned through large-scale training. Figure 8 shows our model achieves 100.29 IS and 5.513.6 FID using IMDD-1M-trained features, confirming realistic and diverse generation. Figure 9 demonstrates synthesized images preserve material-specific visual characteristics where metallic surfaces show appropriate reflectance while textile defects maintain fiber structure. These generated samples provide controllable synthetic augmentation for downstream tasks, expanding training distributions to improve robustness for rare defect types. Captioning evaluation is deferred to the future work. 5.3. Unified Framework for Downstream Tasks (14) (cid:35) . We evaluate on classification, object detection, and segmentation tasks to demonstrate effective transfer of learned representations. Figure 8. Comparison of generative quality between our IMDD1Mtrained model and Stable Diffusion XL (SDXL) on the Magnetic Tile dataset. (1) IS: class consistency and diversity. (2) FID: realism gap to real images. Our model attains higher IS and lower FID. Figure 9. Qualitative comparison of real (left) vs. generated (right) defect samples across multiple industrial datasets including Magnetic Tile, VisA, wall stain, and aircraft surface panel. Generated images exhibit high fidelity in texture reproduction. Table 3. datasets. Dataset Classification accuracy across multiple industrial # Defect Types Accuracy (%) MVTec AD VisA Magnetic Tile Steel Surface Average 90 137 6 98.3 97.7 96.2 94.5 96.7 5.3.1. Defect Classification We show classification results in Table 3. Our model achieves 96.7 % average accuracy across four datasets withTable 4. Object detection comparison with YOLOv8 on MVTec AD. We report the mean Average Precision (mAP) at Intersection over Union (IoU) thresholds of 0.5 and 0.75, and the average IoU. Method mAP@0.5 (%) mAP@0.75 (%) Avg IoU (%) YOLOv8-m [23] Ours (Mask-based) 78.3 74.6 62.1 58.9 68.4 65. Table 5. Pixel-level segmentation results using standard metrics: Accuracy, F1-score (F1), IoU, and Dice coefficient (Dice), evaluated on ground-truth masks from MVTec AD and VisA. Dataset Accuracy (%) F1 (%) IoU (%) Dice (%) MVTec AD (bottle) MVTec AD (cable) VisA (candle) VisA (capsule) Average 92.25 89.7 90.3 91.8 91.0 58.3 56.8 60.2 59.1 58.6 52.1 51.4 54.7 53.3 52. 58.3 56.8 60.2 59.1 58.6 out task-specific modifications. 5.3.2. Object-Level Defect Detection We derive bounding boxes from segmentation masks. Table 4 shows that our mask-based defect foundation model, fine-tuned with only 200 samples per class (less than 5% of data required by supervised methods), achieves 74.6% mAP@0.5 and 58.9% mAP@0.75, approaching the performance of the dedicated object detection model YOLOv8 (78.3% and 62.1% respectively). This demonstrates exceptional data efficiency and strong generalization without requiring explicit box annotations, as our unified framework rivals task-specific detectors while using significantly fewer labeled samples. 5.3.3. Pixel-Level Defect Segmentation We report segmentation results in Table 5 achieving 52.9 % average IoU. Table 6 compares with existing methods on MVTec AD. Our approach with 200 samples per class achieves 96.1 % P-AUC-ROC and 90.2 % AUC-PRO, approximately 2 % below methods using full training sets. This modest gap is favorable considering we use less than 5% of the annotation requirements of supervised methods. As shown in Figure 10, our multimodal model further provides visually interpretable segmentation and detection outputs, effectively localizing diverse defect patterns across different material domains. 5.4. Ablation Study and Data Efficiency Table 7 evaluates component contributions. Removing implicit text embedding reduces accuracy by 4.8 %, eliminating grounding loss degrades IoU by 3.1 %, and removing diffusion conditioning causes 7.0 % accuracy drop. Figure 11 demonstrates the data efficiency of our approach through an ablation study varying training samples from 25 to 350 per class. Our model, pre-trained on the Figure 10. Qualitative visualization of multimodal results on various MVTec AD samples. (a)(d) show segmentation outputs with text-conditioned masks highlighting localized defects, while (e)(h) illustrate object detection results with bounding boxes accurately identifying defect regions across different material domains. Table 6. Comparison with anomaly detection methods on MVTec AD dataset. We report P-AUC-ROC (%): area under the receiver operating characteristic curve at pixel level, and AUC-PRO (%): area under the per-region overlap curve. Our method achieves competitive performance with significantly reduced supervision (200 samples/class vs. full (4000 after data augmentation) training sets). Method MuSc [9] PromptAD [10] DMAD [13] DDAD [17] SimpleNet [14] FAIR [12] Ours (50 samples/class) P-AUC-ROC AUC-PRO # Training Samples 97.3 93. Full 96.5 90.5 Full 97.9 93.3 Full 98.1 92. Full 98.1 90.5 Full 98.2 94.0 Full 96.1 90. 200/class Table 7. Architectural ablation study on VisA dataset. Removing the implicit text embedding, grounding loss, or diffusion conditioning leads to consistent degradation, confirming that each component contributes to overall accuracy and segmentation quality. Model Configuration Acc (%) F1 (%) IoU (%) Full Model w/o Implicit Text Embedding w/o Grounding Loss Lgrnd w/o Diffusion Conditioning 91.0 86.2 88.3 84.0 58.6 54.1 56.4 52.3 52.9 49.2 49.8 46.7 large-scale IMDD-1M dataset, achieves 96.1% accuracy with only 200 samples per class during fine-tuning. In contrast, conventional supervised methods typically require approximately 4,000 samples per class (including augmentation) to reach comparable performance, meaning our approach reduces annotation requirements to less than 5% while maintaining competitive accuracy. The performance curve shows rapid improvement in the low-data regime (25-200 samples) and saturates beyond 200 samples, indicating that our foundation model has learned generalizable defect representations. Figure 11. Our method achieves 96.1% accuracy using only 200 samples per class, requiring less than 5% of the training data compared to conventional approaches (approximately 4,000 samples per class for comparable performance). Performance rapidly improves up to 200 samples and then plateaus, demonstrating effective learning under limited supervision. 6. Conclusion We introduced IMDD-1M, million-scale industrial defect dataset with aligned image-text annotations. Our unified diffusion framework achieves competitive zero-shot performance using less than 5% of supervised training data while eliminating task-specific models. This work establishes foundation for scalable, language-driven industrial inspection systems. In the future, we aim to extend the dataset with temporal and multi-view information to support videobased defect tracking and 3D reasoning. We also plan to explore cross-domain generalization between different manufacturing sectors, enabling robust adaptation to unseen industrial settings. Integrating multimodal reasoning with physical simulation will further bridge perception and generative modeling for real-world manufacturing intelligence."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: visual language model for few-shot learning. Advances in Neural Information Processing Systems (NeurIPS), 2022. 2 [2] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad comprehensive real-world dataset for unsupervised anomaly detection. Conference on Computer Vision and Pattern Recognition (CVPR), pages 95849592, 2019. 2, 4 [3] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask arXiv transformer preprint, abs/2112.01527, 2021. 4 image segmentation. for universal [4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. Conference on Computer Vision and Pattern Recognition (CVPR), pages 248255, 2009. [5] Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Rebecca Konig, and Carsten Steger. The mvtec ad 2 dataset: Advanced scenarios for unsupervised anomaly detection, 2025. 2 [6] Weibo Huang and Peng Wei. PCB dataset for defects detection and classification. arXiv preprint, abs/1901.08204, 2019. 2 [7] Yibin Huang, Congying Qiu, Yue Guo, Xiaonan Wang, and Kui Yuan. Surface defect saliency of magnetic tile. 2018 IEEE 14th International Conference on Automation Science and Engineering (CASE), pages 612617, 2018. 2 [8] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. arXiv preprint, abs/2102.05918, 2021. 2 [9] Xurui Li, Ziming Huang, Feng Xue, and Yu Zhou. Musc: Zero-shot industrial anomaly classification and segmentation with mutual scoring of the unlabeled images, 2024. 8 [10] Xiaofan Li, Zhizhong Zhang, Xin Tan, Chengwei Chen, Yanyun Qu, Yuan Xie, and Lizhuang Ma. Promptad: Learning prompts with only normal samples for few-shot anomaly detection, 2024. [11] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. arXiv preprint, abs/1405.0312, 2014. 2 [12] Tongkun Liu, Bing Li, Xiao Du, Bingke Jiang, Leqi Geng, Feiyang Wang, and Zhuo Zhao. Simple and effective frequency-aware image restoration for industrial visual anomaly detection. Advanced Engineering Informatics, 64: 103064, 2025. 8 [13] Wenrui Liu, Hong Chang, Bingpeng Ma, Shiguang Shan, and Xilin Chen. Diversity-measurable anomaly detection, 2023. 8 [14] Zhikang Liu, Yiming Zhou, Yuansheng Xu, and Zilei Wang. Simplenet: simple network for image anomaly detection and localization, 2023. 8 [15] Bowen Ma, Tong Jia, Mingyuan Li, Songsheng Wu, Hao Wang, and Dongyue Chen. Toward dual-view x-ray baggage inspection: large-scale benchmark and adaptive hierarchical cross refinement for prohibited item discovery. IEEE Transactions on Information Forensics and Security, 19:38663878, 2024. 2 [16] Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli, and Gian Luca Foresti. VT-ADL: vision transformer network for image anomaly detection and localization. arXiv preprint, abs/2104.10036, 2021. [17] Arian Mousakhan, Thomas Brox, and Jawad Tayyub. Anomaly detection with conditioned denoising diffusion models. Pattern Recognition, pages 181195, 2025. 8 [18] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. arXiv preprint, abs/2103.00020, 2021. 2 [19] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022. 2 [20] Yong Shi, Limeng Cui, Zhiquan Qi, Fan Meng, and Zhensong Chen. Automatic road crack detection using random structured forests. IEEE Transactions on Intelligent Transportation Systems, 17:112, 2016. 2 [21] Kechen Song and Yunhui Yan. noise robust method based on completed local binary patterns for hot-rolled steel strip surface defects. Applied Surface Science, 285:858864, 2013. 2 [22] Domen Tabernik, Samo Sela, Jure Skvarc, and Danijel Skocaj. Segmentation-based deep-learning approach for surfacedefect detection. arXiv preprint, abs/1903.08536, 2019. 2 [23] Rejin Varghese and Sambath M. Yolov8: novel object detection algorithm with enhanced performance and robustness. 2024 International Conference on Advances in Data Engineering and Intelligent Computing Systems (ADICS), pages 16, 2024. 7 [24] Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, Jianning Zhang, Zhihao Gu, Shuguang Qian, Mingang Chen, and Lizhuang Ma. Real-iad: real-world multi-view dataset for benchmarking versatile industrial anomaly detection, 2024. 2 [25] Daniel Weimer, Bernd Scholz-Reiter, and Moshe Shpitalni. Design of deep convolutional neural network architectures for automated feature extraction in industrial inspection. CIRP Annals, 65(1):417420, 2016. 2 [26] Yang Zou, Jongheon Jeong, Latha Pemula, Dongqing Zhang, and Onkar Dabeer. Spot-the-difference self-supervised pretraining for anomaly detection and segmentation, 2022. 2,"
        },
        {
            "title": "Appendix",
            "content": "7. Reproducibility and Code Release snapshots, our and detailed anonymous GitHub To facilitate reproducibility and future research, we lightweight prehave released comprehensive code, documentrained model tation repository: through https://anonymous.4open.science/r/IMDD1MTowardsOpenVocabulary- IndustrialDefect-Understanding-082A/. This section provides an overview of the released materials and instructions for reproducing our experimental results. 7.1. Repository Structure Our codebase is organized into the following components: models/: Core model including the Industrial Diffusion U-Net, Implicit Captioner, and Mask2Former generator. implementations, third party/: Integration of external third-party libraries used by our framework. Object detection.py: Object-level defect detection implementation. classify.py: Defect classification module. integrate custom unet.py: Utilities for integrating custom U-Net variants. README.md: Comprehensive documentation including installation, usage, examples, and training instructions. requirements.txt: Full dependency specification to reproduce all experiments. LICENSE: Open-source license governing code usage and redistribution. 7.2. Model and Dataset Release Due to storage and hosting limits, we cannot release the full pre-trained models and datasets in the repository, but provide lightweight snapshots, configurations, and preparation scripts, with complete resources available to qualified researchers upon request under institutional data-sharing policies. 8. Limitations 8.1. Training Limitations. While our IMDD-1M dataset comprises 1.24 million samples, training the diffusion U-Net from scratch requires substantial computational resources. The complete Stage 1 pre-training demands 72 hours on 8 NVIDIA H100 80GB GPUs (576 GPU-hours total), which may limit accessibility for researchers with constrained computational budgets. The peak memory consumption reaches 76GB per GPU at batch size 32 with mixed precision training, necessitating high-end hardware. The two-stage training paradigm (diffusion pre-training followed by mask generator fine-tuning) introduces additional complexity compared to end-to-end approaches. Researchers must carefully manage frozen and trainable parameters across stages, and hyperparameter tuning requires iterating through both stages, multiplying computational costs. 8.2. Inference Limitations. Our model requires 0.35 seconds per image on an A100 GPU, which may be slower than specialized detectors like YOLOv8 for real-time industrial inspection scenarios requiring 50-200 frames per second. The diffusion-based feature extraction at timestep = 50 adds computational overhead compared to standard feed-forward architectures. Memory consumption of 18.7GB during inference exceeds the capacity of edge devices commonly used in industrial settings. 8.3. Application Limitations. Despite achieving competitive performance with less than 5% of supervised training data (200 samples per class), our approach still requires this minimum amount for effective fine-tuning. For extremely rare defects occurring less than once per 10,000 products, collecting 200 samples may be impractical. The framework currently focuses on 2D analysis and does not incorporate temporal information for videobased inspection or 3D geometric reasoning for volumetric defects. IMDD-1M predominantly covers visible-light RGB imaging, while industrial settings often employ X-ray, infrared, ultrasonic, or hyperspectral imaging. 9. Societal Impact 9.1. Positive Impacts. This work contributes to improved manufacturing quality control, potentially reducing defective products and enhancing consumer safety. By enabling data-efficient defect detection with reduced annotation requirements (less than 5%), our approach democratizes access to advanced AI-powered inspection for small and medium-sized enterprises that lack extensive labeled datasets. The multimodal framework facilitates knowledge transfer across manufacturing domains, accelerating AOI adoption. Automated systems improve workplace safety by reducing human exposure to hazardous environments including high-temperature processes, toxic materials, and repetitive strain injuries. 9.2. Potential Concerns. Deployment of automated defect detection systems may impact employment in traditional quality inspection roles, necessitating workforce retraining and transition support. There exist risks of automation bias where operators overrely on AI predictions without verification. We emphasize human-in-the-loop workflows for safety-critical applications. The dataset contains proprietary manufacturing patterns; organizations should evaluate intellectual property concerns before releasing defect imagery. The computer vision techniques could be repurposed for surveillance or discriminatory practices. We advocate for responsible AI principles and regulatory frameworks preventing misuse. 10. Preliminaries 10.1. Denoising Diffusion Probabilistic Models 10.1.1. Forward Diffusion Process Progressive noise addition over timesteps: q(xtxt1) = (xt; (cid:112)1 βtxt1, βtI), (17) where {βt}T t=1 controls noise injection rate. Closed-Form Sampling. Define αt = 1 βt and αt = (cid:81)t s=1 αs. Through recursive substitution: xt = αtx0 + 1 αtϵ, ϵ (0, I). (18) This reparameterization enables efficient training without iterating through timesteps. As , we have xT (0, I). Variance Schedule. We use linear schedule: βt = β1 + t1 1 (βT β1) with β1 = 104, βT = 0.02. Alternative co- (cid:17)2 sine schedule: αt = (t) with = 0.008. (0) where (t) = cos (cid:16) t/T +s 1+s π 2 10.1.2. Reverse Denoising Process Learn p(xT ) (cid:81)T reverse process pθ(x0:T ) = t=1 pθ(xt1xt) where p(xT ) = (0, I). Posterior Distribution. The true reverse transition is: q(xt1xt, x0) = (xt1; µt, βtI), µt = αt1βt 1 αt x0 + αt(1 αt1) 1 αt (19) xt. (20) Neural Parameterization. We parameterize by predicting added noise: 1 αt µθ(xt, t) = 1 αt where ϵθ is U-Net predicting noise ϵ. (cid:18) βt xt (cid:19) ϵθ(xt, t) , (21) Training Objective. Simplified denoising score matching: Lsimple = EtUniform(1,T ),x0,ϵ (cid:2)ϵ ϵθ(xt, t)2(cid:3) . (22) Sampling. Reverse diffusion from xT (0, I): xt1 = (cid:18) xt 1 αt 1 αt 1 αt (cid:19) ϵθ(xt, t) + σtz, (23) where (0, I) for > 1, else = 0. 10.1.3. Conditional Generation Extend to text conditioning: ϵθ(xt, t, c) where R768 is CLIP text embedding. Cross-Attention. At each U-Net layer with features Rhwc and text RLd: = WQFlatten(F), = WKC, = WV C, Attention = softmax (cid:19) (cid:18) QKT dk V. (24) (25) Classifier-Free Guidance. Strengthen conditioning at inference: ϵθ = ϵθ(xt, t, ) + (ϵθ(xt, t, c) ϵθ(xt, t, )), (26) where > 1 is guidance scale and denotes null conditioning. 10.2. Latent Diffusion Models Operate in compressed VAE space. Pre-trained encoder and decoder with downsampling : = E(x) RH/f W/f cz , ˆx = D(z) RHW 3. (27) For Stable Diffusion: = 8, cz = 4. This provides 64 speedup per attention layer and 16-32 overall training acceleration. Latent objective: Llatent = Et,z0,ϵ (cid:2)ϵ ϵθ(zt, t, c)2(cid:3) . (28) 10.3. U-Net Architecture Following Stable Diffusion v1.5 with random initialization (860M parameters). 23 expert annotators (5-15 years Annotation Protocol. QC experience) following three-stage verification: Stage 1 - Initial Annotation: Primary annotator creates masks and textual descriptions following template: Structure. Four stages at resolutions {h, h/2, h/4, h/8} with channels {320, 640, 1280, 1280}. Each stage: 2-3 ResNet blocks with timestep injection Self-attention (heads=8) for coarser resolutions Cross-attention (heads=8) to CLIP text embeddings Down/upsampling between stages ResNet Block. = Conv33(SiLU(GroupNorm32(F)), Cout), = + Linear(temb), = Conv33(SiLU(GroupNorm32(h)), Cout), Fout = + Residual(F). (29) (30) (31) (32) Timestep Embedding. Sinusoidal encoding with MLP projection to 1280-dim: (cid:40) PEi(t) = sin(t/100002i/256) cos(t/100002(i1)/256) if even if odd . (33) Projected via Linear(2561024) SiLULinear(10241280). 11. Implementation Details This section provides comprehensive technical specifications for reproducibility. Our training pipeline consists of two stages with distinct configurations (as shown in Tables 8 and 9), executed on high-performance computing infrastructure (as shown in Table 10). The baseline models for comparison are configured following their original implementations with adaptations for our evaluation protocol (see Section 11.2). 11.1. Dataset Details Data Collection Timeline. The 18-month collection phase (January 2023 - June 2024): Stage 1 (Months 1-6): Integration of 20 public benchmarks (MVTec AD, VisA, BTAD, etc.) Stage 2 (Months 7-12): Web mining across GitHub, RoboFlow, PaddlePaddle, Tianchi using multilingual queries in English, Chinese, Japanese. Keywords included defect detection, quality inspection. Yielded 180K samples. Stage 3 (Months 13-18): Industrial partnerships with 12 companies across petrochemical (4 companies), metal processing (5 companies), and powder metallurgy (3 companies). All data anonymized: EXIF removal, serial number blurring, facility layout suppression. [Product Category] [Material] with [Defect Type] located at [Spatial Location], characterized by [Morphological Descriptors], potentially caused by [Root Cause]. Stage 2 - Peer Review: Secondary annotator verifies technical accuracy. Disagreements (18.3%) flagged. Stage 3 - Consensus: Panel of 3+ experts resolves conflicts. Highly ambiguous cases (2.7%) undergo additional inspection. Inter-annotator agreement: Cohens κ = 0.87 (classification), κ = 0.81 (segmentation IoU0.75). Representative Qualitative Examples. To contextualize our dataset and illustrate the breadth of visual patterns it captures, we present representative defect samples from multiple industrial domains (Figures 12 and 13). The first group focuses on aluminum surfaces, exhibiting subtle yet distinct failure modes that vary widely in morphology and optical response. The broader cross-material selection spans photovoltaics, metallic alloys, packaging components, precision gears, and textile fibers, including microcracks, oxidation, pin holes, dents, and fiber breakage, demonstrating the datasets diversity and realism across industrial settings. 11.2. Compared Model Settings YOLOv8-m Configuration. For object detection comparison (Table 4, main paper): Architecture: CSPDarknet53 backbone, PANet neck, decoupled head Parameters: 25.9M (backbone: 13.2M, neck: 8.4M, head: 4.3M) Input: 640 640 pixels with letterbox padding Training: 300 epochs, batch size 16, early stopping (patience=50) Optimizer: SGD (momentum 0.937, weight decay 5 104) Learning rate: 102 initial, cosine decay to 105, 3 epochs warmup Loss: CIoU (7.5) + DFL (1.5) + BCE (0.5) Augmentation: Mosaic (0.8), Mixup (0.15), HSV jitter, flip (0.5) Training time: 8 hours on 4 RTX 3090 (32 GPU-hours) Inference: 6.2ms per image on A100 (161 FPS) Figure 12. This figure showcases diverse set of aluminum surface defects, including base-exposed regions, coating cracks, powder bumps, dents, scratches, minor dings, dust spots, scuffing marks, non-conductive stripes, deformation artifacts, orange-peel textures, and pitting defects. The samples highlight variations in texture, reflectivity, and severity, providing comprehensive visual reference for realworld aluminum anomaly patterns. Figure 13. diverse collection of real-world defect examples across multiple material domains, including microcracks in solar panels, surface streaks on metallic alloys, stains and pin holes on aluminum sheets, scratches and dents on bottle caps, oxidation on mechanical gears, and fiber breakage in textile fabrics. These samples highlight the wide variability in appearance, texture, and failure modes encountered in practical industrial settings. The complete Stage 1 pre-training hyperparameters are detailed in Table 8, while Stage 2 fine-tuning settings are provided in Table 9. Anomaly Detection Baselines. For segmentation comparison (Table 6, main paper): 1. MuSc (Mutual Scoring): ViT-B/16 pre-trained on ImageNet-21K. Self-supervised contrastive learning on unlabeled normals. 86M frozen backbone + 2.3M trainable projection. 100 epochs, batch size 32, lr=103. 2. PromptAD: CLIP ViT-B/16 with learnable prompts. 86M frozen encoders + 0.8M prompts (10 tokens per category). 4-shot (4 normal samples), 50 epochs, AdamW lr=104. 3. DMAD (Diversity-Measurable): Diffusion U-Net 250M params. 200 epochs per category (3000 total for MVTec AD). 100 diffusion steps, cosine schedule. Unsupervised (normal samples only). 15 hours per category on 8 A100. 4. SimpleNet: WideResNet-50 frozen + 12M adaptation network. 150 epochs, batch size 32, lr=103. Mahalanobis distance to memory bank. 2.5 hours per category on 4 RTX 3090. 5. FAIR (Frequency-Aware): Dual-branch (spatial 25M + frequency 18M + fusion 2M). 200 epochs, batch size 16, lr=5 104. Loss: L1 (1.0) + perceptual (0.1) + frequency (0.5). 6 hours per category on 4 A100. All baselines trained on full MVTec AD ( 4000 samples per class after 20 augmentation). 11.3. Training Details Stage 1: Diffusion U-Net Pre-training. Training Details: U-Net trained from random initialization (He for conv, Xavier for linear). Warmup: 5000 steps from 106 to 104. Cosine decay to 106. Implicit captioner trained jointly with stochastic conditioning. EMA updated every iteration. Resources: 72 hours on 8 H100 80GB, 576 GPU-hours total. Peak memory 76GB/GPU. 43 min/epoch, 484,500 total iterations. Stage 2: Mask Generator Fine-tuning. Training Time: MVTec AD (3629 samples): 4 hours. VisA (9621 samples): 5.5 hours. 11.4. Computing Resource Configuration Memory Optimization: Gradient checkpointing (40% memory saving), mixed precision FP16, gradient accumulation for smaller GPUs. Distributed Training: PyTorch DDP with NCCL backend. 32 data loader workers per GPU (256 total). NVLink Configuration Value Optimizer Base Learning Rate Weight Decay Optimizer Momentum Batch Size Learning Rate Schedule Warmup Steps Training Epochs Gradient Clipping EMA Decay Mixed Precision Diffusion Steps Noise Schedule β1 (start) βT (end) Loss Weight Ldiff Loss Weight Limp Text Conditioning Probability Augmentation Random Horizontal Flip Random Vertical Flip Random Rotation Color Jitter (BSCH) Random Resized Crop AdamW 1 104 1 104 β1 = 0.9, β2 = 0.999 256 (32 per GPU 8) Cosine Decay 5,000 iterations 100 Max norm 1.0 0.9999 FP16 (AMP) 1000 Linear 1 104 2 102 1.0 0.3 0.5 / 0.5 = 0.5 = 0.5 15 0.2, 0.2, 0.1, 0.05 scale=[0.8, 1.0] Table 8. Stage 1 diffusion U-Net pre-training configuration on IMDD-1M. 4.0 for gradient all-reduce ( 150ms, overlapped down to 40ms). 12. Additional Experiments 12.1. Extended Quantitative Analysis Per-Category Performance. We evaluate DiffuseDefect across 10 MVTec AD categories (200 samples/class). As shown in Table 11, our method achieves remarkable 91.99% average accuracy (Acc) and 55.71% mean IoU. Results are consistent across types, including Grid (94.32% Acc, 61.2% IoU), Leather (93.67% Acc, 59.7% IoU), and Cable (89.70% Acc, 51.4% IoU). Cross-Dataset Generalization. As shown in Table 12, IMDD-1M pre-trained models achieve zero-shot transfer IoU of 52.9% to 54.7%, providing an 11% to 15% IoU gain over single-dataset baselines. 12.2. Ablation Studies Training from Scratch vs. Fine-tuning. We compare training from random initialization versus fine-tuning from pre-trained Stable Diffusion weights. Random initialization achieves 82.7% mIoU, outperforming fine-tuned StaValue Train Test Acc (%) IoU (%) Configuration Optimizer Base Learning Rate Weight Decay Batch Size LR Schedule Warmup Steps Training Epochs Gradient Clipping Mixed Precision Feature Timestep Loss Weight Lmask Loss Weight Lcls/ground Mask Queries Transformer Layers Frozen Components Diffusion U-Net VAE Encoder/Decoder CLIP Implicit Captioner Trainable Components Mask2Former AdamW 5 105 1 104 16 (2 per GPU 8) Polynomial Decay (power=0.9) 500 iterations 50 Max norm 0.01 FP16 50 1.0 0.5 100 9 860M params 84M params 63M params 0.3M params 45M params Table 9. Stage 2 mask generator fine-tuning configuration. Component Specification Hardware GPU CPU RAM Storage Software OS CUDA PyTorch Python Inference Latency Throughput Memory 8 NVIDIA H100 80GB 2 AMD EPYC 7763 (128 cores) 2TB DDR4-3200 ECC 100TB NVMe SSD RAID-0 Ubuntu 22.04 LTS 12.1 2.1.0 3.10 0.35s per image (A100) 2.86 images/sec 18.7 GB Table 10. Computing resource configuration. Category Acc (%) F1 (%) IoU (%) Grid Leather Cable Average 94.32 93.67 89.70 91. 67.8 65.2 56.8 61.2 59.7 51.4 61.48 55.71 Table 11. Per-category results on MVTec AD (200 samples/class). MVTec AD VisA IMDD-1M MVTec AD IMDD-1M VisA 83.2 91.0 90.3 41.3 52.9 54.7 Table 12. Zero-shot cross-dataset transfer performance. ble Diffusion (74.5%) by 8.2%. This indicates that natural image priors may actually hinder learning of industrial defect patterns, which have fundamentally different visual characteristics. Timestep Selection. We investigate the impact of the diffusion timestep on feature extraction quality. Our analysis shows that timestep = 50 provides the optimal balance between semantic understanding and spatial precision, achieving 91.0% accuracy and 52.9% IoU. Earlier timesteps preserve more spatial detail but lack semantic context, while later timesteps capture high-level semantics but lose finegrained localization. Sample Efficiency. We evaluate the data efficiency of IMDD-1M pre-training by measuring the samples required to reach 95% accuracy. As shown in Table 13, IMDD-1M pre-training requires only 150 samples, which is 12.3 more efficient than random initialization (1850 samples) and 3.6 more efficient than ImageNet (520 samples). This improvement highlights the value of domain-specific pre-training. Pre-training Samples for 95% Acc Efficiency Random Init ImageNet IMDD-1M 1850 520 150 1.0 3.6 12.3 Table 13. Sample efficiency comparison across different pretraining strategies. 12.3. Dataset Statistics Annotation achieved Cohens κ of 0.87 (classification) and 0.81 (segmentation), requiring 66, 287 hours. The defect distribution exhibits realistic long-tail: top 10 types comprise 47.4%, while 411 rare types account for 52.6%. 12.4. Real and Generated Visual Comparison To further illustrate the visual diversity in our dataset and evaluate the generative models effectiveness, we present qualitative comparisons between real and synthesized defect images (Figure 14). Figure 14. Comparison between real defect samples and model-generated counterparts across various aircraft and metal-surface categories, including cracks, dents, missing regions, paint defects, scratches, and uneven textures. The generated images closely reproduce the structural morphology, surface patterns, and material appearance observed in the real samples, illustrating the models ability to synthesize realistic defect characteristics. Each column pair shows real sample on the left and the corresponding generated sample on the right."
        }
    ],
    "affiliations": [
        "Institute of Intelligent Systems, National Yang Ming Chiao Tung University"
    ]
}