{
    "paper_title": "Optimizing Large Language Model Training Using FP4 Quantization",
    "authors": [
        "Ruizhe Wang",
        "Yeyun Gong",
        "Xiao Liu",
        "Guoshuai Zhao",
        "Ziyue Yang",
        "Baining Guo",
        "Zhengjun Zha",
        "Peng Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents a promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains a challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: a differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates a mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets a foundation for efficient ultra-low precision training."
        },
        {
            "title": "Start",
            "content": "Optimizing Large Language Model Training Using FP4 Quantization Ruizhe Wang 1 2 Yeyun Gong 3 2 Xiao Liu 3 2 Guoshuai Zhao 3 2 Ziyue Yang 3 2 Baining Guo 3 Zhengjun Zha 1 Peng Cheng 3 2 5 2 0 2 8 2 ] . [ 1 6 1 1 7 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The growing computational demands of training large language models (LLMs) necessitate more efficient methods. Quantized training presents promising solution by enabling low-bit arithmetic operations to reduce these costs. While FP8 precision has demonstrated feasibility, leveraging FP4 remains challenge due to significant quantization errors and limited representational capacity. This work introduces the first FP4 training framework for LLMs, addressing these challenges with two key innovations: differentiable quantization estimator for precise weight updates and an outlier clamping and compensation strategy to prevent activation collapse. To ensure stability, the framework integrates mixed-precision training scheme and vector-wise quantization. Experimental results demonstrate that our FP4 framework achieves accuracy comparable to BF16 and FP8, with minimal degradation, scaling effectively to 13B-parameter LLMs trained on up to 100B tokens. With the emergence of next-generation hardware supporting FP4, our framework sets foundation for efficient ultra-low precision training. 1. Introduction In the past two years, the rapid development of large language models (LLMs) has significantly reshaped both research priorities and industrial practices. Theoretical analyses and empirical evidence consistently demonstrate that scaling up model size leads to substantial performance improvements (Kaplan et al., 2020; Bi et al., 2024). However, training such large-scale models poses considerable challenges, demanding extensive time, energy, and financial resources. For example, Llama 3 (Dubey et al., 2024) 405B Work done during internship in MSRA 1University of Science and Technology of China 2Microsoft SIGMA Team 3Microsoft Research Asia. to: Yeyun Gong <yegong@microsoft.com>, Peng Cheng <pengc@microsoft.com>. Correspondence Figure 1. Directly casting to FP4 results in significantly higher training loss, whereas our proposed FP4 method achieves accuracy comparable to the BF16 baseline. These results are based on experiments with 400M LLaMA2 model. is trained on up to 16K H100 GPUs for 54 days. Similarly, GPT-4 (Achiam et al., 2023), with an estimated 1T parameters, required an extraordinary amount of computational power. These examples highlight the urgent need for more efficient training methods to keep up with the increasing demands of LLM development. Model quantization has proven to be an effective technique for reducing training costs, as low-bit arithmetic kernels can save memory and accelerate computations when used appropriately. Most LLM training systems traditionally rely on FP32 (full precision) or FP16/BF16 (half precision) data formats, but quantization enables these formats to be reduced to lower precision, such as 8-bit or even 4-bit. Recent advancements in computational hardware, such as NVIDIAs H100 GPUs (Nvidia, 2023) and the upcoming B200 GPUs (Nvidia, 2024), have introduced support for low-bit arithmetic kernels, enabling more efficient computation. The Hopper series GPUs feature high-performance FP8 tensor cores, delivering 2x speed-up compared to FP16 tensor cores. Meanwhile, the Blackwell series GPUs extend this capability by supporting FP6 and FP4 formats, with FP4 offering the potential to double computational throughput over FP8. Studies like FP8-LM (Peng et al., 2023) and NVIDIAs Transformer Engine (Nvidia, 2022) have demonstrated the feasibility of FP8 tensor cores for model training. But the application of FP4 tensor cores in 1 Optimizing Large Language Model Training Using FP4 Quantization model training remains an open research question. However, leveraging 4-bit data formats for neural network training presents significant challenges due to the extremely limited bit width. Directly quantizing LLMs to such lowbit format often results in substantial accuracy degradation, as shown in Figure 1. This is primarily because low-bit formats are constrained by limited dynamic range, which increases the risk of overflow and underflow. Even existing methods for 8-bit quantization experience some degree of accuracy loss, underscoring the difficulties of employing 4-bit format, which provides only 16 distinct representable values. In this study, we pioneeringly propose framework for training language models using the FP4 format, providing validation of the feasibility of this ultra-low precision representation. To tackle the significant quantization errors associated with weights and activations during model training, we present series of optimization techniques: (1) For weights, we present differentiable quantization estimator to improve gradient updates in FP4 computations. By analyzing the impact of quantization on neural network forward and backward passes, we derive function with correction terms for accurate gradient estimation; (2) For activations, we develop an outlier clamping and compensation strategy to address the issue of outlier values commonly observed during LLM training. By analyzing activation distributions in LLMs, we introduce clamping method and sparse auxiliary matrix to preserve quantization accuracy and maintain model performance. We conduct comprehensive experiments to demonstrate that our FP4 training framework achieves accuracy comparable to models trained in BF16 or FP8 formats with the same hyperparameters. Leveraging the FP8 tensor cores of NVIDIA H100 GPUs to emulate FP4 computations, we train LLMs with up to 13B parameters and 100B training tokens, with minor training loss gap. For zero-shot evaluation on downstream tasks, model trained with FP4 show competitive results against BF16 models. We anticipate better speed performance gains with the availability of next-generation hardware like NVIDIAs B-series GPUs. We will opensource our training code to facilitate future research and adoption. 2. Preliminaries According to the IEEE 754 standard (Kahan, 1996), binary floating-point number consists of three components: 1-bit sign (S), exponent bits (E), and mantissa bits (M). This is commonly represented as ExMy, where and denote the number of bits for the exponent and mantissa, respectively. For example, FP16 uses E5M10 and BF16 uses E8M7. FP8 typically has two variants: E4M3 and E5M2. In our work, we adopt the E2M1 format for 4-bit floating-point representation, as defined in prior studies (Rouhani et al., 2023b;a), with 2 bits for the exponent and 1 bit for the mantissa. Unlike integer (INT) quantization, floating-point (FP) quantization features uneven quantization intervals and larger dynamic range. To quantize high-precision tensor like FP16 to FP4, we employ the commonly used absmax method (Dettmers et al., 2022; Peng et al., 2023): xfp4 = Q(xfp16 γ), γ = MAXfp4 max(xfp16) (1) Here, MAXfp4 represents the maximum absolute value in the FP4 format, and γ serves as the scaling factor. For the E2M1 configuration, MAXfp4 is calculated to be 6.0. The quantization function Q() is implemented using look-up table for quantization in custom CUDA kernel since the FP4 format supports only 24 = 16 distinct values. Detailed format regulations and quantization implementation can be found in Appendix A. 3. Methodology In typical linear layer of Transformer architecture, the computation can be expressed as = , where is the activation tensor and is the weight tensor. To fully leverage the capabilities of FP4 tensor cores, both and need to be quantized to FP4, as shown in Figure 2. However, directly quantizing these tensors into FP4 introduces significant quantization errors. To address this challenge, we propose the differentiable gradient estimator method for weight tensors (Section 3.1) and the outlier clampping and compensation method for activation tensors (Section 3.2) to mitigate these issues. 3.1. Differentiable Gradient Estimator Quantization functions are inherently non-differentiable, preventing the reverse flow of the gradient during backpropagation. The widely used Straight-Through Estimator (STE) (Bengio et al., 2013) bypasses this issue by assuming that the gradient of the quantized tensor is equivalent to that of the original tensor. However, this simplification introduces inaccuracies in low-bit settings, as noted in prior studies (Yin et al., 2019; Gong et al., 2019). To overcome these limitations, we propose Differentiable Gradient Estimator (DGE) that reduces estimation errors. DGE maintains direct quantization for forward computation to preserve hardware efficiency while introducing gradient correction term derived from differentiable approximation of the quantization function. Suppose we quantify the model weight with nondifferentiable quantization function : Wq = (W ). Con2 Optimizing Large Language Model Training Using FP4 Quantization Figure 2. The structure of the proposed FP4 training scheme during the forward pass of linear layer. high-precision tensor, such as BF16, is quantized into the FP4 format using look-up table quantization. During the GeMM computation, both weight and activation tensors are quantized into FP4 to leverage the FP4 tensor cores. Two scaling factors are then applied to the final result to ensure computational correctness. sidering the backward gradient computation for linear function with quantized weight, the forward pass can be expressed as: = AWq = Af (W ) (2) During backpropagation, the loss gradient with respect to the weight L/W and the activation L/A are computed using the gradient propagated from the subsequent layer L/Y . For the weight gradient, the chain rule gives: W = Wq Wq = (AT Y ) Wq (3) Where Wq/W represents the derivative of the quantization function . since is an element-wise function, its derivative is also element-wise. Thus we have: Wq[i, j] [k, l] = (cid:40) (W [i, j]), 0, if (i, j) = (k, l), otherwise. (4) Therefore Wq/W is diagonal matrix. When applied to the chain rule Equation (3), this diagonal structure allows simplification of the gradient computation, reducing it to an element-wise multiplication between the two items: W [i, j] = Wq [i, j] (W [i, j]) (5) or to be simplified: W = Wq (W ), (6) Where denotes the element-wise (Hadamard) product. Since is non-differentiable quantization function, its derivative is almost everywhere zero, leading to vanishing gradients and causing the weight gradient computation to fail, as shown in Equation (6). The StraightThrough Estimator (STE) addresses this issue by assuming (W ) 1, thereby bypassing gradient vanishing. In other words, it directly assumes that L/W L/Wq. To achieve more accurate gradient computation, we propose an alternative approach: approximating the quantization function with well-chosen differentiable function, computing its derivative, and incorporating it into Equation (6). Specifically, we use the following function to simulate the quantization behavior: (x) = δ (cid:0)1 + sign(x δ 2 ) 1 (cid:1) δ 2 (7) Figure 3(a) illustrates this function under = 5 for the range [0, 0.5], which represents the first positive quantization interval in the E2M1 quantization scheme. This figure also shows that under the assumption of STE, forward quantization function is equivalent to (x) = because (x) 1. In Equation (7), δ represents the quantization interval, and is parameter that controls the degree of approximation. As increases, the function curve becomes sharper and more closely resembles the behavior of the original hard quantization function. The derivative of Equation (7) can be expressed as: (x) = 1 1 1 δ 2 (8) Figure 3(b) and Figure 3(c) show the complete quantization curve (x) and its derivative (x) under = 5 within the full E2M1 quantization framework. This framework 3 Optimizing Large Language Model Training Using FP4 Quantization Figure 3. Visualization of the Differentiable Gradient Estimator (DGE). (a) Comparison of three quantization methods: hard quantization, differentiable quantization, and STE quantization, demonstrated on single quantization step. (b) The full quantization curve for E2M1 quantization within its dynamic range [6.0, 6.0]. (c) The derivative curves for the three methods, highlighting that hard quantization has gradient of (x) 0 , while STE assumes constant gradient of (x) 1. consists of 14 distinct quantization intervals. To prevent excessively large gradient values, the magnitude of (x) is capped at 3.0, impacting only very small subset of elements. In practical model training, the Differentiable Gradient Estimator (DGE) is seamlessly integrated into the process. During the forward pass, we retain the hard quantization function for computational efficiency. For the backward pass, correction term derived from Equation (8) is applied to the weight gradient calculation following Equation (6). Supplementary integration process and proof for the DGE method in actual training process is provided in Appendix B. 3.2. Outlier Clamping and Compensation During LLM training, activation tensors are significantly more challenging to quantize than weight tensors. This difficulty arises from the complex distribution of activation tensor values, often dominated by outliersspecific values that are substantially larger than the rest. Outliers pose significant challenge to tensor quantization by disproportionately expanding the dynamic range of the target tensor, causing most values to underflow to zero after quantization. To address this issue, we propose the Outlier Clamping and Compensation method (OCC) to restrict the range of activation tensors and mitigate the underflow problem. Specifically, we identify outliersvalues with the largest absolute magnitudesthrough quantile searching and clamp them to predefined threshold. Given pre-defined quantile α, the clamping function can be expressed as: Yc = clamp(Y, max = α, min = 1 α) (9) Figure 4 illustrates the impact of quantization with and without outlier clamping, based on real activation tensor extracted from the first transformer layers output of the Figure 4. Visualization of the outlier clamping method, based on the first transformer layers output of the LLaMA 1.3B model after 30,000 training iterations. Up: Quantization performed without outlier clamping, leading to severe loss of information. Down: Quantization after applying outlier clamping, effectively preserving tensor structure. LLaMA 1.3B model after 30,000 training iterations, where α = 0.999. This approach significantly reduces the mean squared error (MSE) between the original and quantized tensors, enhancing quantization quality and maintaining training stability. We also observed that while clamping effectively reduces quantization error, it inherently introduces some error by disregarding the outlier values. To further preserve accuracy, we propose compensating for this error using sparse 4 Optimizing Large Language Model Training Using FP4 Quantization Table 1. Quantitative analysis of mathematical accuracy between original and quantized activation tensors. Results represent the average values obtained across all activation tensors on the 30,000 training iterations of the LLaMA 1.3B model. CLAMP COMP QUANTILE SIM MSE SNR 99.9 99.9 99 97 92.19% 0.1055 98.83% 0.0366 99.61% 0.0245 0.0099 100% 0.0068 100% 8.31 14.25 15.31 18.38 20.88 outlier matrix. In our experiments, the quantile clamping threshold α is set relatively high (around 0.99 0.999), making the residual matrix = Yc highly sparse, with only about 0.2% 2% non-zero elements. During computation, the clamped matrix Yc is processed using FP4 GeMM, while is handled with high-precision sparse matrix multiplication. Table 1 provides quantitative analysis of cosine similarity (SIM), mean squared error (MSE), and signal-to-noise ratio (SNR) between the original activation tensors and quantized tensors. These results represent average values obtained across all activation tensors on the 30,000 training iterations of the LLaMA 1.3B model, demonstrating the impact of outlier clamping and compensation on preserving tensor fidelity during real model training. The data shows that outlier clamping significantly improves both cosine similarity and SNR. Moreover, incorporating outlier compensation further reduces quantization loss. Notably, lowering the quantile threshold increases the compensation scale, further reducing quantization loss. However, this introduces trade-off between computational efficiency and numerical accuracy that must be carefully considered. 4. Experiment In this section, we evaluate the proposed FP4 training framework across language models of various sizes. Section 4.1 details the implementation of our FP4 training framework, including the model architecture and hyperparameters. Section 4.2 presents the main results, showcasing training curves and zero-shot performance on downstream tasks. Finally, Section 4.3 provides ablation studies to further validate the effectiveness. 4.1. Experiment Setup During LLM training, General Matrix Multiplication (GeMM) accounts for over 95% of the computational workload, with this proportion increasing for larger models. Consistent with prior works (Xi et al., 2023; Yang et al., 2020; Dettmers et al., 2022), we focus on 4-bit quantization for GeMM operations, core feature of FP4 tensor cores. In GeMM = AW , where (sequence length input channels) is the activation tensor and (input channels output channels) is the weight tensor, quantization is applied along distinct dimensions to align with matrix multiplication logic: is quantized token-wise (sequence length dimension), while is quantized channel-wise (output channels dimension). The aforementioned accuracy-preserving techniques are integrated to minimize quantization error. Since FP4 Tensor Cores are unavailable, we validate FP4 performance using Nvidia H-series GPUs FP8 Tensor Cores, which encompass FP4s dynamic range and enable accurate simulation. In mixed-precision training (Micikevicius et al., 2017), nonGeMM operations, which account for minor computational fraction, are performed at higher precision to preserve accuracy. Following the framework in (Peng et al., 2023), we perform gradient communication in FP8 format to reduce bandwidth usage and adopt their mixed-precision Adam optimizer to conserve GPU memory. Gradients and first-order moments are stored in FP8, while second-order moments are stored in FP16. Remaining operations, comprising smaller computational portion, are executed in FP16 or BF16 for stability and precision. We adopt the widely recognized LLaMA 2 model (Touvron et al., 2023) as the primary model architecture. The training is conducted from scratch using the DCLM dataset (Li et al., 2024a), comprehensive dataset well-suited for language model pretraining. Hyperparameters remain consistent across precision settings for fair comparison. The learning rate follows warm-up and cosine decay schedule, with the warm-up phase spanning 5% of total steps and the learning rate gradually decreasing to 10% of its peak over the remaining 90%. The peak learning rate is 3 104, with weight decay of 0.1. For the Adam optimizer, we use β1 = 0.9, β2 = 0.95, and ϵ = 1 108. For special hyperparameters used in FP4 method, we use = 5 for differentiable gradient estimator and select α = 0.99 as the activation clamp and compensation quantile. Input sequences are fixed at 2048 tokens, and the batch size is 2048, comprising approximately 4M tokens. 4.2. Main Results We validate the effectiveness of our proposed FP4 training framework by comparing it against the widely adopted BF16 mixed-precision training scheme. Figure 5 presents the training loss curves for LLaMA models (1.3B, 7B, and 13B) trained with BF16 and FP4 precision. All models are trained on 100B tokens using the same dataset and identical hyperparameters. The curves for BF16 and FP4 largely overlap across different model sizes, with the FP4 curve exhibiting slightly higher training loss compared to the Optimizing Large Language Model Training Using FP4 Quantization Table 2. Zero-shot evaluation for downstream tasks between BF16 models and FP4 models under different model sizes. Model Size Precision Average PiQA Hellaswag ObQA Arc-C Arc-E BoolQ LogiQA SciQ Lambada 1.3B 7B 13B BF16 FP4(Ours) BF16 FP4(Ours) BF16 FP4(Ours) 53.23 53.13 53.87 54.42 54.44 54. 71.11 70.89 71.22 71.87 72.80 73.78 50.80 50. 52.03 52.97 53.56 54.12 36.60 36.20 37.40 38.40 38.60 39. 36.69 36.86 38.99 39.85 38.82 39.68 68.60 67. 67.47 67.97 67.97 67.89 57.83 58.23 60.55 62.20 57.40 55. 30.26 29.49 27.65 27.96 29.65 30.88 83.30 83. 85.00 84.70 86.30 85.80 43.84 44.30 44.56 43.88 44.87 46. Figure 5. Training curves for BF16 models and FP4 models under different model sizes. (a) Training curves for 1.3B LLaMA model. (b) Training curves for 7B LLaMA model. (c) Training curves for 13B LLaMA model. BF16 curve. Specifically, after training on 100B tokens, the training losses are as follows: 2.55 (FP4) vs. 2.49 (BF16) for the 1.3B model, 2.17 (FP4) vs. 2.07 (BF16) for the 7B model, and 1.97 (FP4) vs. 1.88 (BF16) for the 13B model. In addition to training loss, we evaluate the models on diverse set of downstream tasks datasets in zero-shot manner, including Arc (Clark et al., 2018), BoolQ (Clark et al., 2019), HellaSwag (Zellers et al., 2019), LogiQA (Liu et al., 2020), PiQA (Bisk et al., 2020), SciQ (Welbl et al., 2017), OpenbookQA (ObQA) (Mihaylov et al., 2018), and Lambada (Paperno et al., 2016). These results are obtained through the widely used lm-evaluation-harness library1 (Gao et al., 2024). As presented in Table 2, models pre-trained with FP4 demonstrate competitive performance in intrinsic incontext learning capabilities. Under the same model size, the average accuracy of FP4-trained models is comparable to, or even slightly exceeds, that of BF16-trained models. Additionally, the results follow the general trend: larger models achieve higher accuracy under the same number of training tokens. These results highlight that despite the reduced precision, FP4 training achieves nearly equivalent performance to BF16 both in terms of training loss and downstream task ac1https://github.com/EleutherAI/lmevaluation-harness curacy, making it promising approach for efficient training of large language models. 4.3. Ablation Study We divide our ablation study into smaller parts to better highlight the findings of FP4 training. All experiments are conducted on the LLaMA 1.3B model, trained with 10B tokens from subset of the DCLM dataset. To accelerate convergence for this smaller model, the batch size is reduced from 2048 to 256, while other hyperparameters remain consistent with the main experiments. Precision. Figure 6(a) presents training curves across various precisions, including BF16 (baseline), MS-AMP FP8 (Peng et al., 2023), Transformer-Engine FP8 (Nvidia, 2022), directly-casted FP4, and our FP4 method. We use W4A4 to denote direct quantization, meaning that quantizing both weight and activation to fp4. Meanwhile, W4A4+DGE+OCC denotes our fp4 quantization method that incorporates the Differentiable Gradient Estimator (DGE) and Outlier Clamp and Compensation (OCC) methods introduced in Section 3. The loss curves show that two FP8 methods and our FP4 approach maintain pretraining accuracy, while directly-casted FP4 has significant training loss gap. Weights. For weight-only 4-bit quantization (W4A8), Optimizing Large Language Model Training Using FP4 Quantization Figure 6. Ablation studies. (a) Training curves under different precision frameworks. (b) The effect of proposed Differentiable Gradient Estimator (DGE). (c) The effect of proposed Outlier Clamping and Compensation method (OCC). Note that directly casting activation into 4-bit leads to divergence, and the loss value turn into NaN (Not Number). (d) Training curves under different quantization granularities of FP4. we evaluate our Differentiable Gradient Estimator (DGE) method alone against direct quantization. As shown in Figure 6(b), the DGE method significantly improve convergence. Notably, direct quantizing weight into 4-bit doesnt introduce substantial training loss gap, suggesting that weights are easier to quantize than activations. For the hyperparameter in this method, larger can better model the quantization function, but it can also lead to more unstable correction term for the gradient. It can also be seen in the figure that moderate = 5 gives better final performance. Activation. For activation-only 4-bit quantization (W8A4), we evaluate our Outlier Clamp and Compensation (OCC) method alone against direct quantization. Figure 6(c) reveals that directly quantizing activations in FP4 results in curve divergence, where the loss values turn into NaN (Not Number) after certain training steps. Outlier clamping and compensation effectively reduces this loss gap, ensuring good convergence. This experiment re-emphasizes the importance of appropriate treatment of outliers in the absmax quantization framework. For the hyperparameter α in this method, larger α implies stronger compensation, but at an increased computational cost. Figure 6(c) shows the model loss under three settings α = 0.999, 0.99, 0.97, corresponding to the non-zero elements of the sparse compensation matrix of 0.2%,2% and 6%, respectively. Although experiments show that higher α leads to better model accuracy, which is consistent with the conclusion of Table 1, we believe that α = 0.99 is better choice for comprehensive computational performance considerations. Granularity. We also observe that the granularity of FP4 quantization plays critical role. While FP8 training schemes (Peng et al., 2023; Nvidia, 2022) achieve sufficient accuracy with coarse-grained tensor-wise quantization, Figure 6(d) shows that tensor-wise scaling in FP4 introduces significant errors. To address this, we adopt vector-wise scaling, with token-wise quantization for activations and channel-wise quantization for weights, aligning with GeMM computation rules as discussed in Section 4.1. Notably, applying coarse-grained quantization to activations alone result in more severe accuracy degradation than applying it to weights alone, revealing that activations are harder to quantize than weights, consistent with the activation outlier issue described in Section 3.2. 7 Optimizing Large Language Model Training Using FP4 Quantization 5. Related Work Quantized Training and Inference.When discussing the quantization of large language models (LLMs) for training, we typically refer to Fully Quantized Training (FQT). Related research efforts have generally used Mixed Precision Training (Micikevicius et al., 2017; Mellempudi et al., 2019) frameworks to accelerate model training while maintaining model accuracy. While previous research has mainly concentrated on CNNs or DNNs(Sun et al., 2019; Wang et al., 2018; Banner et al., 2018; Yang et al., 2020), recent studies have demonstrated the feasibility of low-bit mixed precision training for LLMs (Peng et al., 2023; Nvidia, 2022; Fishman et al., 2024; Xi et al., 2024). In contrast to the FQT scheme, research on low-bit computation for inference has focused on Post-Training Quantization (PTQ) and Quantization Aware Training (QAT). While PTQ directly quantizes pre-trained models for inference (Dettmers et al., 2022; Frantar et al., 2022; Lin et al., 2024a; Xiao et al., 2023; Yao et al., 2022; Liu et al., 2024), QAT involves finetuning or pre-training the model for better low-bit inference performance (Liu et al., 2023b; Cheng et al., 2023; Wang et al., 2023; Dettmers et al., 2024). Our method differs from QAT, as we aim to accelerate the training process while maintaining performance, rather than solely focusing on improving inference efficiency without consideration for the training speed. 4-bit Quantization. Recent works in PTQ and QAT have successfully applied 4-bit, 2-bit or even 1-bit quantization to LLM inference (Dettmers & Zettlemoyer, 2023; Wu et al., 2023). However, these methods focused on LLM inference, requiring additional computation like calibration set fine-tuning (Wang et al., 2024), rotary matrix and low-rank compensation (Lin et al., 2024b; Ashkboos et al., 2024; Li et al., 2024b), quantization parameters searching (Liu et al., 2023a), or even retraining the whole network (Ma et al., 2024). In the field of FQT, an early study (Sun et al., 2020) applied 4-bit radix-4 FP4 format to convolutional neural networks (CNNs). MXFP (Rouhani et al., 2023b) introduced novel quantization data for GPT-style models, but lacked feasibility validation on full FP4 settings. (Xi et al., 2023) proposed an INT4 training framework, but their focus was on fine-tuning tasks with limited applicability to LLM pretraining. In contrast, our work is the first to propose an FP4 training framework tailored for LLMs, validated from scratch, and designed to align with next-generation hardware like Nvidias B-series GPUs. Differentiable Quantization. Unlike previous methods focusing on differentiable quantization (Gong et al., 2019; Uhlich et al., 2019; Chen et al., 2019; Li et al., 2022; Huang et al., 2022), which rely on learnable quantization parameters updated through backpropagation, our differentiable gradient estimator method uses fixed quantization function. We directly change the gradient estimator from STE to DGE during the backward pass, avoiding the need for continuous updates to the quantization function, which is not friendly to specialized hardware designs. Our approach is more efficient and more suitable for hardware acceleration in large-scale training. Handling Outliers. Our method for handling activation outliers in LLMs differs significantly from existing approaches, which mainly target model inference (Liu et al., 2023a; Li et al., 2024b; Ashkboos et al., 2024; Liu et al., 2024). Activation outliers in LLMs are typically channel-specific (Xiao et al., 2023; Wei et al., 2022). Channel-wise quantization would reduce quantization loss but conflicts with the computation structure of matrix multiplication in linear layers (Xi et al., 2024; Lee et al., 2024). Previous strategies to solve this problem like smoothing outliers (Xiao et al., 2023) or using rotary matrices (Ashkboos et al., 2024; Liu et al., 2024) rely on offline pre-processing, making them incompatible with pretraining tasks. In contrast, our method addresses outliers dynamically during real-time training without requiring separate calibration datasets, which is critical for maintaining efficiency in pretraining large models. 6. Limitation One primary limitation of this work lies in the absence of dedicated FP4 Tensor Cores in existing hardware. Consequently, we are unable to directly measure the potential speedup and energy efficiency gains achievable with native FP4 support. All current experiments rely on FP4 simulations, which introduce additional computational overhead due to extra precision casting and significantly prolong runtime. Additionally, due to constraints on computational resources, we have not yet extended our experiments to extremely large-scale models or to datasets comprising trillions of tokens. Investigating such scalability remain as critical directions for future research. 7. Conclusion We propose the first FP4 pretraining framework for modern Large Language Models (LLMs), overcoming the challenges of limited dynamic range and quantization precision in 4-bit formats. By proposing differentiable gradient estimator and an outlier compensation mechanism, we effectively reduce the accuracy gap between FP4 and higherprecision baselines like FP8 or FP16, achieving comparable performance across diverse model scales. Our findings demonstrate the feasibility of FP4-based training, providing insights into improving quantization methods for ultralow-precision computing, and may also serve as call for next-generation hardware designs to enable efficient 4-bit computation kernels. 8 Optimizing Large Language Model Training Using FP4 Quantization"
        },
        {
            "title": "Impact Statement",
            "content": "This work demonstrates the feasibility of using ultra-low precision formats like FP4 for training large language models, offering pathway toward energy conservation and reduced carbon emissions in AI development. By significantly lowering computational and memory demands, FP4-based methods can democratize access to advanced AI systems while promoting environmental sustainability. Additionally, this research calls for next-generation AI accelerators optimized for 4-bit computations, potentially shaping future hardware innovations. However, broader societal implications must be considered, including the risks of misuse and the amplification of biases inherent in large-scale AI models. Addressing these challenges is essential to ensure responsible and equitable adoption of this technology."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Ashkboos, S., Mohtashami, A., Croci, M. L., Li, B., Cameron, P., Jaggi, M., Alistarh, D., Hoefler, T., and Hensman, J. Quarot: Outlier-free 4-bit inference in rotated llms. arXiv preprint arXiv:2404.00456, 2024. Banner, R., Hubara, I., Hoffer, E., and Soudry, D. Scalable methods for 8-bit training of neural networks. Advances in neural information processing systems, 31, 2018. Bengio, Y., Leonard, N., and Courville, A. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 74327439, 2020. Chen, S., Wang, W., and Pan, S. J. Metaquant: Learning to quantize by learning to penetrate non-differentiable quantization. Advances in Neural Information Processing Systems, 32, 2019. Cheng, W., Zhang, W., Shen, H., Cai, Y., He, X., Lv, K., and Liu, Y. Optimize weight rounding via signed gradient descent for the quantization of llms. arXiv preprint arXiv:2309.05516, 2023. Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Dettmers, T. and Zettlemoyer, L. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning, pp. 77507774. PMLR, 2023. Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems, 35:3031830332, 2022. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems, 36, 2024. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Fishman, M., Chmiel, B., Banner, R., and Soudry, D. Scaling fp8 training to trillion-token llms. arXiv preprint arXiv:2409.12517, 2024. Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq: Accurate post-training quantization for generative pretrained transformers. arXiv preprint arXiv:2210.17323, 2022. Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noach, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. framework for few-shot language model evaluation, 07 2024. URL https://zenodo.org/records/ 12608602. Gong, R., Liu, X., Jiang, S., Li, T., Hu, P., Lin, J., Yu, F., and Yan, J. Differentiable soft quantization: Bridging fullprecision and low-bit neural networks. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 48524861, 2019. Huang, X., Shen, Z., Li, S., Liu, Z., Xianghong, H., Wicaksana, J., Xing, E., and Cheng, K.-T. Sdq: Stochastic differentiable quantization with mixed precision. In International Conference on Machine Learning, pp. 9295 9309. PMLR, 2022. 9 Optimizing Large Language Model Training Using FP4 Quantization Kahan, W. Ieee standard 754 for binary floating-point arithmetic. Lecture Notes on the Status of IEEE, 754(947201776):11, 1996. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Outlieraware weight quantization for efficient fine-tuning and inference of large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1335513364, 2024. Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., et al. Datacomplm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794, 2024a. Li, M., Lin, Y., Zhang, Z., Cai, T., Li, X., Guo, J., Xie, E., Meng, C., Zhu, J.-Y., and Han, S. Svdqunat: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024b. Li, Z., Yang, T., Wang, P., and Cheng, J. Q-vit: Fully differentiable quantization for vision transformer. arXiv preprint arXiv:2201.07703, 2022. Lin, J., Tang, J., Tang, H., Yang, S., Chen, W.-M., Wang, W.-C., Xiao, G., Dang, X., Gan, C., and Han, S. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024a. Lin, Y., Tang, H., Yang, S., Zhang, Z., Xiao, G., Gan, C., and Han, S. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving. arXiv preprint arXiv:2405.04532, 2024b. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. Logiqa: challenge dataset for machine reading comprehension with logical reasoning. arXiv preprint arXiv:2007.08124, 2020. Liu, S.-y., Liu, Z., Huang, X., Dong, P., and Cheng, K.- T. Llm-fp4: 4-bit floating-point quantized transformers. arXiv preprint arXiv:2310.16836, 2023a. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888, 2023b. Liu, Z., Zhao, C., Fedorov, I., Soran, B., Choudhary, D., Krishnamoorthi, R., Chandra, V., Tian, Y., and Blankevoort, T. Spinquantllm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. Ma, S., Wang, H., Ma, L., Wang, L., Wang, W., Huang, S., Dong, L., Wang, R., Xue, J., and Wei, F. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764, 2024. Mellempudi, N., Srinivasan, S., Das, D., and Kaul, B. Mixed precision training with 8-bit floating point. arXiv preprint arXiv:1905.12334, 2019. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg, B., Houston, M., Kuchaiev, O., Venkatesh, G., et al. Mixed precision training. arXiv preprint arXiv:1710.03740, 2017. Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A. Can new dataset arXiv preprint suit of armor conduct electricity? for open book question answering. arXiv:1809.02789, 2018. fp8 with engine, Using Nvidia. URL https://docs.nvidia.com/ 2022. deeplearning/transformer-engine/userguide/examples/fp8_primer.html. transformer Nvidia. Nvidia h100 tensor core gpu architecture, 2023. URL https://resources.nvidia.com/ en-us-tensor-core. Nvidia. Nvidia blackwell architecture technical brief, 2024. URL https://resources.nvidia.com/ en-us-blackwell-architecture. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, Q. N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The lambada dataset: Word prediction arXiv preprint requiring broad discourse context. arXiv:1606.06031, 2016. Peng, H., Wu, K., Wei, Y., Zhao, G., Yang, Y., Liu, Z., Xiong, Y., Yang, Z., Ni, B., Hu, J., et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Rouhani, B. D., Garegrat, N., Savell, T., More, A., Han, K.-N., Zhao, R., Hall, M., Klar, J., Chung, E., Yu, Y., et al. Ocp microscaling formats (mx) specification, 2023a. URL https://www.opencompute.org/ documents/ocp-microscaling-formatsmx-v1-0-spec-final-pdf. Rouhani, B. D., Zhao, R., More, A., Hall, M., Khodamoradi, A., Deng, S., Choudhary, D., Cornea, M., Dellinger, E., Denolf, K., et al. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537, 2023b. Sun, X., Choi, J., Chen, C.-Y., Wang, N., Venkataramani, S., Srinivasan, V. V., Cui, X., Zhang, W., and Gopalakrishnan, K. Hybrid 8-bit floating point (hfp8) training and 10 Optimizing Large Language Model Training Using FP4 Quantization Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning, pp. 3808738099. PMLR, 2023. Yang, Y., Deng, L., Wu, S., Yan, T., Xie, Y., and Li, G. Training high-performance and large-scale deep neural networks with full 8-bit integers. Neural Networks, 125: 7082, 2020. Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y. Zeroquant: Efficient and affordable posttraining quantization for large-scale transformers. Advances in Neural Information Processing Systems, 35: 2716827183, 2022. Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., and Xin, J. Understanding straight-through estimator in training activation quantized neural nets. arXiv preprint arXiv:1903.05662, 2019. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. inference for deep neural networks. Advances in neural information processing systems, 32, 2019. Sun, X., Wang, N., Chen, C.-Y., Ni, J., Agrawal, A., Cui, X., Venkataramani, S., El Maghraoui, K., Srinivasan, V. V., and Gopalakrishnan, K. Ultra-low precision 4-bit training of deep neural networks. Advances in Neural Information Processing Systems, 33:17961807, 2020. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama 2: Open foundation and finetuned chat models. arXiv preprint arXiv:2307.09288, 2023. Uhlich, S., Mauch, L., Yoshiyama, K., Cardinaux, F., Garcia, J. A., Tiedemann, S., Kemp, T., and Nakamura, A. Differentiable quantization of deep neural networks. arXiv preprint arXiv:1905.11452, 2(8), 2019. Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L., Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. Wang, J., Liu, H., Feng, D., Ding, J., and Ding, B. Fp4quantization: Lossless 4bit quantization for large language models. In 2024 IEEE International Conference on Joint Cloud Computing (JCC), pp. 6167. IEEE, 2024. Wang, N., Choi, J., Brand, D., Chen, C.-Y., and Gopalakrishnan, K. Training deep neural networks with 8-bit floating point numbers. Advances in neural information processing systems, 31, 2018. Wei, X., Zhang, Y., Zhang, X., Gong, R., Zhang, S., Zhang, Q., Yu, F., and Liu, X. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:17402 17414, 2022. Welbl, J., Liu, N. F., and Gardner, M. Crowdsourcing multiple choice science questions. arXiv preprint arXiv:1707.06209, 2017. Wu, X., Li, C., Aminabadi, R. Y., Yao, Z., and He, Y. Understanding int4 quantization for language models: latency speedup, composability, and failure cases. In International Conference on Machine Learning, pp. 37524 37539. PMLR, 2023. Xi, H., Li, C., Chen, J., and Zhu, J. Training transformers with 4-bit integers. Advances in Neural Information Processing Systems, 36:4914649168, 2023. Xi, H., Chen, Y., Zhao, K., Teh, K. J., Chen, J., and Zhu, J. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. 11 Optimizing Large Language Model Training Using FP4 Quantization A. Implementation of FP4 quantizaiton Floating-point numbers in computer are represented using binary format defined by the IEEE 754 standard (Kahan, 1996). Each number is divided into three components: the sign bit (S), the exponent (E), and the mantissa (or significand, M). This is commonly represented as ExMy, where and denote the number of bits for the exponent and mantissa, respectively. The sign bit determines whether the number is positive (S = 0) or negative (S = 1). The exponent, stored in biased format, encodes the power of two that scales the number, enabling the representation of wide range of values. The mantissa contains the significant digits of the number, capturing its precision. normalized floating-point number is decoded as: Value = (1)S (1.M ) 2Ebias Where 1.M represents the normalized mantissa with an implicit leading 1, and the bias (e.g., 127 for single precision or 1023 for double precision) adjusts the exponent to account for its encoding. Subnormal numbers, where the exponent is all zeros, are handled separately with no implicit leading 1. This representation allows for efficient computation but introduces rounding errors due to the limited number of bits in the mantissa. The IEEE 754 standard does not define rules for floating-point formats with precision below 16 bits, such as FP8 and FP4. For 4-bit floating-point representation, we adopt the E2M1 format as defined in prior studies (Rouhani et al., 2023b;a). According to the IEEE definition, an exponent field (E) filled with ones does not correspond to valid numeric value; instead, it represents infinity (Inf) when the mantissa (M) is all zeros or an invalid number (NaN, Not Number) when the mantissa contains nonzero bits. However, this rule is often disregarded in FP8 and FP4 formats due to their limited bit width, as the priority is to maximize the representation of meaningful numerical values. For example, FP8-E4M3 format doesnt define Inf, FP6 and FP4 formats dont define both Inf and NaN. Based on the distribution of exponent and mantissa bits, all representable numbers in the FP4 format are listed in Table 3. Table 3. FP4 Quantization Table under different FP4 formats. FORMAT 1111 1110 1101 1100 1011 1101 0001 0010 0011 0100 0110 0111 BINARY SEQUENCE 1000/0000 E1M2 E2M1 E3M0 -3.5 -6 -16 -3 -4 - -2.5 -3 -4 -2 -2 -2 -1.5 -1.5 -1 -1 -1 -0.5 -0.5 -0.5 -0.25 0 0 0.5 0.5 0.25 1 1 0.5 1.5 1.5 1 2 2 2 2.5 3 4 3 4 3.5 6 16 Figure 7. Visualization of all representable numbers in different FP4 formats. The E0M3 format is not included here because it is equivalent to the INT4 format, as it doesnt have any exponent bits. From Table 3 and Figure 7, we observe that increasing the number of exponent bits expands the dynamic range, while increasing the number of mantissa bits improves the precision of quantization intervals. We select the E2M1 format in our main experiments as it offers balanced trade-off between dynamic range and quantization precision. Since the FP4 format supports only 24 = 16 distinct values, we implement look-up table for FP4 quantization in custom CUDA kernel. Quantization functions typically involve element-by-element operations on large amounts of data, which can 12 Optimizing Large Language Model Training Using FP4 Quantization be parallelized to take advantage of the highly parallel computing power of GPUs. The following code paragraph shows the implementation of the quantization kernel. ? -6.0f : closest = (value < -5.0f) float value = x[idx]; float closest; int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx < x_size) { (value < -3.5f) ? -4.0f : ? -3.0f : (value < -2.5f) (value < -1.75f) ? -2.0f : (value < -1.25f) ? -1.5f : (value < -0.75f) ? -1.0f : (value < -0.25f) ? -0.5f : (value < 0.25f) (value < 0.75f) (value < 1.25f) (value < 1.75f) (value < 2.5f) (value < 3.5f) (value < 5.0f) 1 __global__ void quantize_kernel(const float* x, float* output, int x_size) { 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 } 25 26 void quantize(at::Tensor input, at::Tensor output, int size) { const float* input_data = input.data_ptr<float>(); 27 float* output_data = output.data_ptr<float>(); 28 29 30 31 32 33 34 const int threadsPerBlock = 256; const int blocks = (size + threadsPerBlock - 1) / threadsPerBlock; cudaStream_t stream = at::cuda::getCurrentCUDAStream(); ? 0.0f : ? 0.5f : ? 1.0f : ? 1.5f : ? 2.0f : ? 3.0f : ? 4.0f : 6.0f; output[idx] = closest; } quantize_kernel<<<blocks, threadsPerBlock, 0, stream>>>(input_data, output_data, size) ; 35 } B. Supplementary Proof for Differentiable Quantization Estimator We present the complementary proof procedure for the Differentiable Gradient Estimator (DGE) method under actual quantization with vector-wise scaling factors. In the GeMM operation = AW , where is the activation tensor with dimensions (s ci, sequence length input channels) and is the weight tensor with dimensions (ci co, input channels output channels), quantization is applied along distinct dimensions to adhere to the mathematical logic of matrix multiplication. The quantization function is defined as: xfp4 = Q(xfp16 γ), γ = MAXfp4 max(xfp16) For the weight tensor with dimensions (ci co), channel-wise quantization is performed as follows: Wscaled = sf scaled = Q(Wscaled) Wq = scaled 1 sf 13 (10) (11) (12) (13) Optimizing Large Language Model Training Using FP4 Quantization Here, sf is the scaling factor, and represents the element-wise (Hadamard) product. In tensor-wise quantization, sf is scalar. For channel-wise quantization, sf is vector with dimensions (1 co). In this case, the operation involves broadcasting sf to each column of the matrix (ci co), followed by element-wise multiplication. For Equation (13), it is crucial to note that multiplying by 1/sf ensures mathematical correctness. Practically, however, this step is performed after the GeMM kernel execution. In other words, the quantized weight tensor provided to the GeMM kernel is the scaled quantized weight tensor scaled from Equation (12). Nevertheless, for mathematical analysis, the quantized weight tensor Wq must be re-scaled. In the backward computation, the loss gradient with respect to is derived from the forward operation = AWq. According to the matrix multiplication rules for differentiation, the gradient L/Wq is computed using the activation gradient L/Y from the subsequent layer. Fwd: = AWq Bwd: Wq = AT Y (14) By applying the chain rule and referring to Equations (11) to (13), the relationship between L/Wq and the actual weight gradient L/W is established. According to Equation (13), the gradient L/W scaled can be expressed in terms of L/Wq using the scaling factor sf : Subsequently, the differentiable gradient estimator correction term Q(x) is applied to compute L/Wscaled: W scaled = Wq 1 sf Wscaled = W scaled Q(Wscaled) (15) (16) Where Q(x) is the differentiable gradient estimator correction item introduced in Equation (8). Finally, the relationship between L/Wscaled and L/W is derived by incorporating sf : By combining all these steps, the formula for calculating the true weight gradient L/W is obtained: W = Wscaled sf W = = (cid:32) Wq 1 sf (cid:33) Q(Wscaled) sf Wq Q(Wscaled) (17) (18) (19) Importantly, the scaling and un-scaling steps cancel each other due to the element-wise nature of the operations, resulting in simplified expression. This final formula matches the previously demonstrated Equation (6) in the main body of the paper, with the only difference being that the variables within the DGE correction term must account for scaled weights. No changes are required for the and functions. C. Analyzing Quantization Difficulty Through Tensor Distribution Section 3 highlights the necessity of quantizing both weight and activation tensors to fully leverage the FP4 tensor core. It also points out that activation tensors are significantly more challenging to quantize compared to weight tensors. To further support this observation, we provide the actual distributions of weight and activation tensors during model training. 14 Optimizing Large Language Model Training Using FP4 Quantization Figure 8. Visualization of the weight tensors in the dense projection layers of the self-attention module. Figure 9. Visualization of the weight tensors in the up-projection linear layers of the MLP module. Figure 10. Visualization of the weight tensors in the down-projection linear layers of the MLP module. Optimizing Large Language Model Training Using FP4 Quantization Figure 11. Visualization of the activation tensors from the core attention output. Figure 12. Visualization of the activation tensors from the post-attention layer normalization output. Figure 13. Visualization of the activation tensors from the MLP down-projection layer output. 16 Optimizing Large Language Model Training Using FP4 Quantization Figures 8 to 10 illustrate the distribution of weight tensors, while Figures 11 to 13 show the distribution of activation tensors. These results are derived from training the LLaMA 1.3B model over 30,000 iterations. The y-axis is set to logarithmic scale to enhance visualization. From these figures, it is evident that weight tensors generally exhibit smaller dynamic range, while activation tensors have significantly larger dynamic range, making them more challenging to quantize. Regarding distribution characteristics, weight tensors typically follow normal distribution, with certain tensors exhibiting small outliers. In contrast, activation tensors vary widely in their distributions. For example, core attention outputs often follow regular distribution with minimal outliers. However, many activation tensors, such as layer-norm outputs and transformer layer outputs, display irregular distributions with numerous outliers, making them particularly difficult to quantize. Notably, the outliers in activation tensors during LLM training tend to appear in specific channels. This observation is further validated through heatmap analysis in Figure 14. The result is obtained through the activation function (GeLU) output from the first transformer layer. These analyses underscore the critical importance of effectively addressing activation tensors during quantization, especially their outliers. Future research could gain valuable insights by exploring the complex distributions and outlier behavior of activation tensor values. Figure 14. Heatmap visualization of the activation function (GeLU) output from the first transformer layer on the 30,000 training iteration of the LLaMA 1.3B model. The vertical light lines in the heatmap correspond to specific channel dimensions in the activation tensor, highlighting the channel-wise distribution of outliers."
        }
    ],
    "affiliations": [
        "Microsoft Research Asia",
        "Microsoft SIGMA Team",
        "University of Science and Technology of China"
    ]
}