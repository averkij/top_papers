{
    "paper_title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
    "authors": [
        "Rui Wang",
        "Ce Zhang",
        "Jun-Yu Ma",
        "Jianshu Zhang",
        "Hongru Wang",
        "Yi Chen",
        "Boyang Xue",
        "Tianqing Fang",
        "Zhisong Zhang",
        "Hongming Zhang",
        "Haitao Mi",
        "Dong Yu",
        "Kam-Fai Wong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 8 3 4 4 1 . 0 1 5 2 : r a"
        },
        {
            "title": "Technical Report",
            "content": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents Rui Wang, Ce Zhang, Jun-Yu Ma, Jianshu Zhang, Hongru Wang, Yi Chen, Boyang Xue, Tianqing Fang*, Zhisong Zhang, Hongming Zhang, Haitao Mi, Dong Yu, Kam-Fai Wong The Chinese University of Hong Kong, Tencent AI Lab (cid:135) https://github.com/Tencent/CognitiveKernel-Pro (cid:135) https://github.com/Tencent/WebAggregator Figure 1: The Pass@1 performance of our WebAggregator models, tuned on the automatically constructed training resource, WebAggregatorQA, is comparable to or even exceeds that of GPT-4.1 on both GAIA-text and the more challenging WebAggregatorQA test set. Abstract Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge in order to generate high-quality, insightful research. However, existing open-source deep research agent systems predominantly focus on enhancing information seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to generate coherent insights or support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. The process begins with proactive online exploration, where an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop series of foundation models, named WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches the performance of Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents information aggregation abilities, we construct human-annotated evaluation split of WebAggregatorQA as challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve individual references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations. Correspondence to Rui ruiwangnlp@outlook.com, Tianqing tianqfang@tencent.com, Kam-Fai kfwong@c uhk.edu.hk"
        },
        {
            "title": "Technical Report",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "DeepResearch agent systems (OpenAI, 2025; Monica.Im, 2025) are built upon foundational large language models (LLMs), aiming to perform complex, human-level tasks. Achieving this level of capability requires not only effective information seeking, using web-interactive tools to retrieve accurate and relevant knowledge, but more importantly, information aggregation, where retrieved materials are synthesized into coherent, novel insights in the spirit of expert human authors (Bereiter & Scardamalia, 1987; Flower & Hayes, 1981). Developing web agents with human-level task composition capabilities fundamentally requires large-scale training corpora that explicitly capture both information seeking and aggregation behaviors. Yet, such datasets remain scarce. Existing multi-hop QA datasets (Yang et al., 2018; Talmor et al., 2021; Trivedi et al., 2022) rarely involve authentic web interactions, and can often be solved from the models parametric knowledge alone. More recent web agent datasets (Shi et al., 2025a; Wu et al., 2025a; Tao et al., 2025) simulate multi-hop logics by linking offline static web pages into graphs and constructing questions along random paths, but their scope remains limited: Our analysis  (Table 1)  reveals two critical gaps in current resources. First, agent solutions in realworld contexts require accessing and synthesizing information from dynamic, heterogeneous webincluding diverse domains, file processing (e.g., parsing PDFs, CSVs) or active interactions with dynamic web elements (e.g., form submissions, JavaScript-rendered content)far beyond the static, pre-collected page sets most methods employ. Second, existing datasets prioritize pure information seeking, overlooking the need for complex aggregation: 30.79% of WebWalkerQA (Wu et al., 2025b) tasks are solved by simple text parsing, while cases demanding deeper and structured analysis are inherently constrained by their randomly sampled logic flows. As shown in Figure 2 and Figure 5, robust web agents should not only find relevant facts but also perform deep analysis by aggregating and reasoning with information, refining gold from sand. Thus, promoting and evaluating aggregation ability is crucial but underexplored challenge in web agent research. To bridge these gaps, we introduce an Explore to Evolve method (see Figure 2) for automatically constructing training data that requires both information seeking from diversified sources and sophisticated aggregation logics for generalist web agents. This approach adopts Explore: Proactive Online Web Exploring and Evolve: Automatic Aggregation Logic Synthesis, treating the entire task composition process as an agent-driven pipeline. The agent is equipped with advanced web tools enabling search, static parsing, dynamic interaction, file processing, and vision input, thereby supporting diverse user scenarios1. In the Proactive Online Exploration, the agent gathers corpus of resources by exploring the live web, with the scope and complexity governed by the initial anchor URL and traversal budget. This explored content then becomes the canvas for the Automatic Aggregation Logic Synthesis. Instead of injecting predefined logic, the agent utilizes taxonomy of high-level aggregation logics 1Details shown in Appendix Resource IS IA Train GAIA (Mialon et al., 2023) BrowseComp (Wei et al., 2025a) WebWalker (Wu et al., 2025b) TaskCraft (Shi et al., 2025a) WebShaper (Tao et al., 2025) WebAggregatorQA (Ours) Information Source Text, Figure, Audio, File, Dynamic Web Elements N/A N N/A Text Text Text, PDF, Figure Text Text, Figure, File, Dynamic Web Elements #Dom - 9 4 8 12 Table 1: Comparison between our WebAggregatorQA created by Explore to Evolve and previous data resources. IS: information-seeking, IA: information-aggregation. Our method could construct data that covers diverse aggregation needs  (Table 2)  compared with samples of previous work (Figure 5)."
        },
        {
            "title": "Technical Report",
            "content": "Figure 2: The Explore to Evolve data construction pipeline of WebAggregatorQA. (1) Proactive Online Web Exploring gathers comprehensive information by interacting with the web environment through tools (more details in Figure 8). (2) Task Construction via Automatic Aggregation Logic Synthesis constructs QA pairs grounded on the explored knowledge by instantiating and evolving the high-level aggregation guidance into concrete operations, e.g., Statistic Analysis standard deviation. (3) Quality Control ensures the data quality and diversity. (spanning Element, Set, Scientific Analysis, and Temporal Reasoning ) inspired by prior studies of multi-hop analysis (Chang et al., 2022; Yang et al., 2018; Talmor et al., 2021; Wu et al., 2025c) and logical reasoning (Ren et al., 2020; Fang et al., 2024; Venkatraman et al., 2025), and evolves them into unique, multi-step aggregation chain. Crucially, the structure of this evolving task is grounded in the specific information uncovered during exploration, ensuring that each generated QA pair is both unique and complex. Our analysis shows broad diversity and complexity of aggregation operations evolved and emerged in synthesized tasks (Figure 4). Following quality control, we compile the WebAggregatorQA dataset consisting of approximately 10K query-answer pairs, and challenging human-annotated test set. We further employ rejection sampling on GPT-4.1 sampled trajectories on the constructed dataset, and train our WebAggregator model family based on Qwen3 series and use SmolAgents (Roucher et al., 2025) as the agent scaffold. Extensive experiments demonstrate that WebAggregator outperforms strong baselines on GAIA-text and WebAggregatorQA, exemplifying the value of our data construction pipeline. The contribution of our work is as follows: We propose an automated and scalable Explore to Evolve workflow for web agent dataset construction, uniquely emphasizing aggregation complexity. The resulting WebAggregatorQA dataset covers broad range of domains, source types, tool uses, and especially aggregation logics. The trained foundation models for web agents, WebAggregator, show superior performance. The WebAggregator-8B surpasses GPT-4.1, and the 32B version surpasses current strong baselines. Our test set remains challenging, with even the Claude-3.7-sonnet achieving only 28.3%. Notably, accurate reference retrieval does not guarantee success in aggregation, highlighting the crucial need for progress in this capability."
        },
        {
            "title": "2 Explore to Evolve",
            "content": "Our objective is to automatically generate at scale diverse and challenging set of QA pairs grounded in real web resources, suitable for training web agents with few human involvement. To reflect"
        },
        {
            "title": "Technical Report",
            "content": "Figure 3: The distribution of domains in WebAggregatorQA, tasks categorized by the number of different tools involved during construction, and steps (an action-observation round) in data synthesis. realistic scenarios, our tasks require complex information retrieval, deep aggregation, and structured reasoning rather than simple fact lookup. To achieve this, we propose an automatically verifiable training data construction method illustrated in Figure 2. We frame data synthesis as an Explore to Evolve procedure for web agents: starting from an anchor URL, an agent performs Proactive Online Web Exploring to collect relevant information across heterogeneous sites and file types, then generates QA pairs requiring complex aggregation and reasoning through Automatic Aggregation Logic Synthesis. rigorous automated quality control stage filters out low-quality samples. The process requires only anchor URLs as input, and no human annotation is needed. The agent we used is depicted in Appendix A, including action and observation space."
        },
        {
            "title": "2.1 Anchor URL Collection",
            "content": "Diversity of anchor URLs is crucial for broad domain coverage. By selecting seed URLs across multiple domains, we can effectively regulate the datasets domain distribution, thereby enabling precise control during data construction. We sampled 5,000 topic-diverse queries from QA and scientific datasets (Yang et al., 2018; Jin et al., 2019; Trivedi et al., 2022) and retrieved URLs via Google Search, resulting in over 160,000 anchor links from 11+ domains (see Figure 3)."
        },
        {
            "title": "2.2 Data Synthesis as an Agent Task in Real Internet",
            "content": "In this section, we introduce our approach to synthesizing target QA pairs by framing task synthesis as specialized form of web-agent tasks. The agent is given task prompt consisting of two components: Proactive Web Exploration and Automatic Aggregation Logic Synthesis, enabling it to complete the task construction in an end-to-end manner."
        },
        {
            "title": "2.2.1 Explore for Information Collection: Proactive Online Web Exploring",
            "content": "The first step, Proactive Online Web Exploring, aims to collect diverse information as the foundation for task construction. During this phase (Figure 2), the agent is prompted to start from single anchor URL and employ various tools to navigate across web pages just like human browsing, to discover unknown but relevant information that serves as the basis for generating QA pairs. Interactions include navigating heterogeneous content types such as text, files, and images, as well as dynamic element interactions. To control task difficulty and ensure the comprehensiveness of the knowledge scope, minimum number of web page visits (e.g., at least = 7) is enforced (see Appendix B.3). We found that this exploration step could incorporate diversified information from multiple sources. By analyzing the tool calling statistics of 5,296 web exploring trajectories in Figure 3, we found that the proactive web exploration of these tasks involves at least three tools: the Search, Visit, and"
        },
        {
            "title": "Technical Report",
            "content": "Figure 4: Word cloud of aggregation operations extracted from the constructed tasks. In the Automatic Aggregation Logic Synthesis stage, the agent converts high-level guidance into concrete low-level operations to combine knowledge snippets into new conclusions. The illustrated task requires seeking knowledge by Search, Visit, Click, FileRead, and aggregations to derive the final answer. the compulsory tool ScreenShot. Moreover, 48.36%, 28.55%, and 13.41% of the samples involve the use of 3, 4, and 5 different tools, respectively. The broad interactions here promote greater knowledge diversity and introduce additional challengesfor example, questions derived from file-based information also evaluate the file-processing capabilities of the responding agents."
        },
        {
            "title": "2.2.2 Evolve for Data Synthesis: Automatic Aggregation Logic Synthesis",
            "content": "The Automatic Aggregation Logic Synthesis procedure is designed to evolve informationaggregation behaviors, guided by predefined instructions and exploration results, to synthesize QA pairs. Consequently, the generated training resource aims to strengthen the agents reasoning ability, enabling it to produce concise, meaningful insights derived from retrieved knowledge rather than merely returning lists of entities or numbers. To broaden the range of aggregation strategies, we first define set of high-level logical operations, compiled from human annotations and prior work (Sen et al., 2022; Talmor et al., 2021; Wu et al., 2025c; Fang et al., 2024; Krishna et al., 2025), which agents use to develop concrete aggregation logic chains. As shown in Figure 2, aggregation operations are categorized into four major types, Element, Set, Scientific Analysis, and Temporal Reasoning, with total of 12 subtypes. Element and Set operations are the basis of regular aggregation behaviors among knowledge snippets, such as mathematical calculation among elements and set merging among sets. While Scientific Analysis and Temporal Reasoning are advanced applications of them, which are expected to reflect user cases and increase the task complexity. These subtypes represent high-level guidance that appears in the prompt to instruct the agent to evolve them into concrete reasoning steps, rather than rigid constraints. vivid example is that math calculations between elements could be derived into addition, subtraction, etc., which could be observed in Figure 4. More detailed seed operations and corresponding prompts are provided in Appendix B.3."
        },
        {
            "title": "2.2.3 Quality Control",
            "content": "QA Alignment Checking: We implement two-stage refinement process. First, self-refinement tool for the agent with checklist verifies and revises questions before outputting the sample"
        },
        {
            "title": "Technical Report",
            "content": "Figure 5: Samples from TaskCraft (Shi et al., 2025a), WebDancer (Wu et al., 2025a), and WebShaper (Tao et al., 2025) primarily evaluate basic information-seeking skills, such as Element -> Retrieve and Set -> Sets Composition for entity filtering. In contrast, the selected WebAggregatorQA samples demand significantly more complex information aggregation to derive final answers. Crucially, these diverse aggregation strategies are automatically evolved by agents, guided by high-level logics and accumulated knowledge during data construction, resulting in rich variability that reflects task-specific intricacies. (Appendix B.5). Second, data checking agent thoroughly reviews the entire task by verifying reference URLs to ensure alignment among questions, answers, and sources (Appendix B.4). About 11.72% of the original data are filtered out in the second stage. Diversity Constraint: We ensure dataset diversity by balancing domain and aggregation operation distributions. First, we annotated anchor URL domains with GPT-4.1 and balanced data to achieve more balanced distribution (Figure 3). Second, we analyzed information aggregation types using GPT-4.1 to identify low-level operations. Although not perfectly reliable without solving the questions, operations like calculating average can be easily detected. We then adjusted prompts to emphasize rare aggregation types, increasing their sample frequency. The word cloud of the aggregation operations (Figure 4) exhibits that different high-level aggregation guidance will spawn diversified low-level, specific operations, e.g., intersection for Set, table processing for Scientific Analysis. Data Leakage Avoidance During proactive web exploration, agents may download and parse existing datasets. To prevent data contamination, we created website keyword blacklist. Pages matching the blacklist or containing identified datasets were excluded from retrieval and subsequent model evaluation to ensure the fairness of the evaluation."
        },
        {
            "title": "2.2.4 Trajectory Sampling",
            "content": "After the task synthesis, we collect the trajectory that completes these tasks. We utilize the agent based on GPT-4.1 with SmolAgents, equipped with almost the same tools exhibited in Table 6, except for the Screenshot and Scroll, because we only collect the plain text trajectories. To ensure the quality of the collected trajectories, we conduct further filtering procedure and finally collected 6,184 trajectories for the foundation model training: Correctness We employ rejection sampling to retain those trajectories that with correct answers according to the reference answers in the WebAggregatorQA. Format Data with output format errors (e.g., undefined tool name or parameters) is filtered out. Exception Handling Anomalies in observations (e.g., page failures) are kept to improve the models generalization, since similar situations would occur in real web environments."
        },
        {
            "title": "Technical Report",
            "content": "Operations Element Operations Retrieve (x) Inverse (x) Math (x, y) Set Operations Filter(Y) Existence(x, Y) Compose(Y, Z) Temporal Reasoning Change TempCalc Science Analysis CompIntensive(X) Predict (x1, ..., xn) Statistic (x1, ..., xn) Correlate (X, Y) Questions Aggregate elements/entities, e.g., numbers, times, names(x, y). In Amor: Recipe for Building Adaptable ... , what hourly pay (in USD) is for the hired NLP expert? Which American actor won the Academy Award for XXX in the 1990s released their first solo studio album the greatest number of years after their Oscar win? Among Benedict, Robert Downey, and ..., for the persons first appearance in Marvel Cinematic Universe film corresponded to the highest ROI for their debut Marvel movie, what is the ROI (three decimals)? Aggregate elements (x) and sets (Y, Z, ...). Among the countries that won at least 15 gold medals at the London 2012 Summer Olympics, what is the HDI of the country that had the third highest per-capita GDP (in USD) in 2012? For the college that had the most players selected overall in the 2023 NBA Draft, how many of its draftees were picked in the first round? According to the WorldPopulationReview, how many cities among the top 100 most populous cities in 2025 have experienced population decrease compared to 2024? Reasoning or calculation related with time. Between 1990 and 2022, which country had the third largest average annual percentage increase in nominal GDP? Among Robert De Niro, Al Pacino, Christopher Walken, and Jessica Lange, who has the longest interval between their first and most recent Academy Award nominations without winning, and what is the length of that span in years? Coding is must to improve efficiency or precision. What is the average closure price of Apple.inc from Jan. 2024 to Oct. 2024? KFF published an article on abortion in Womens Health Policy on Feb 27, 2025. Using single exponential smoothing and MSE, search for the optimal alpha (0.01-0.99, step=0.01) based on the historical data, the MSE loss, and use the alpha to estimate the next data point. Among all Cleveland Cavaliers head coaches who have won at least one playoff game with the team, what is the standard deviation of their playoff win percentages? Between the 2012 to 2022 NBA seasons, what is the Pearson correlation coefficient between Damian Lillards season average points per game and the Portland Trail Blazers regular season win percentage? Table 2: Several representative examples in WebAggregatorQA of information aggregation operations are presented. Note that the operations here are high-level guidance that could be derived into diversified, specific form, rather than low-level constraints. means an element or knowledge snippet, denotes list of knowledge snippets that fulfill certain condition."
        },
        {
            "title": "2.2.5 Statistics of WebAggregatorQA",
            "content": "WebAggregatorQA comprises 9,883 tasks (with 200 reserved for testing), covering 54,064 unique URLs across 12 domains. Figure 3 shows the distribution of domains and steps for both QA construction trajectories. Domains are labeled by GPT-4.1. Most QA pairs are constructed with around 15 steps, demonstrating that the generated data points are not hastily created from only few reasoning steps, thus avoiding overly simplistic questions."
        },
        {
            "title": "2.3 Curation of WebAggregatorQA Test Set",
            "content": "Evaluating web agents is vital for their improvement. Existing benchmarks (Wu et al., 2025b; Wei et al., 2025a) mainly focus on information-seeking tasks (Figure 5), like deducing answers from ambiguous clues and retrieving entities, often corresponding to Element-> Retrieve / Inverse Questions and Set->Filtering. 30.29% of WebWalkerQA tasks require only direct retrieval of single entity, with almost none involving large-scale computation or analysis for the answers."
        },
        {
            "title": "Technical Report",
            "content": "While this is important for evidence retrieval, the deeper analytical capabilities, such as generating clear and structured answers through reasoning and aggregation (Mialon et al., 2023; Krishna et al., 2025) of evidence, are inadequately evaluated. To bridge this gap, we developed the WebAggregatorQA test set to comprehensively measure both complex retrieval and aggregation skills. Annotation Details We uniformly split 200 tasks as seeds from WebAggregatorQA across different domains to ensure high task diversity. Since humans have inherent cognitive limits in creating highly complex tasks spanning multiple domains (Chen et al., 2025). > Step 1: Human annotators review the QA pairs and references to eliminate ambiguities and provide revised version of the original data. Our analysis, aligned with prior work (Wei et al., 2025a), shows that while questions are generally well-structured, they might lack unique ground truth due to the high uncertainty of the web. Thus, we ensure every question is unambiguous with exactly one correct answer by adding constraints, e.g., explicit reference sources (the World Bank in Figure 4). > Step 2 & 3: To further enhance sample reliability and reduce bias from the solvers perspective, this process is repeated twice: tasks are solved, ambiguities identified, and revisions made by annotators. > Step 4: In the final cross-validation stage, each question was answered by two annotators, yielding 155 consistently aligned samples. Additionally, there are 4 samples that annotators abandoned during the answering process due to difficulty, but whose references and questions were verified to ensure data quality and thus were retained. More details are shown in Appendix B.2. This yielded 159 samples, including those in text and multimodal, categorized by difficulty into Level 1 (24), Level 2 (99), and Level 3 (36). Each sample contains question, reference answer, solution, and supporting URLs. text example is shown in Figure 4 and multimodal one is in Figure 9."
        },
        {
            "title": "3.1 Experimental Setups",
            "content": "Models and Benchmarks We construct the WebAggregator models by SFT Qwen2.5-7B, Qwen2.532B (Yang et al., 2024), Qwen3-8B, and Qwen3-32B (Yang et al., 2025) on the training set of WebAggregatorQA. We evaluate the baselines and our methods on the subset of 103 text-only cases of GAIA (Mialon et al., 2023) following Li et al. (2025a;b); Wu et al. (2025b), and WebAggregatorQA. Training Configs We formalize the trajectory we sampled as (question, a1, o1, ..., an, on, answer). ai stands for the action code the agent generated to perform tool calling, and oi is the observation returned by the web environment. The question and observations are masked during training. Baselines and Metrics We mainly compare WebAggregator with three types of prior works. a. Non-agentic foundation models that answer questions using their internal knowledge. b. Zeroshot foundation models initialized as agents via the SmolAgents framework. c. Strong fine-tuned foundation models: WebThinker (Li et al., 2025b), WebDancer (Wu et al., 2025a), CognitiveKernelPro (Fang et al., 2025b), WebSailor (Li et al., 2025a) and WebShaper (Tao et al., 2025). We utilize the pass@1 for performance comparison. The correctness is evaluated by GPT-4.1 with the prompt following previous works (Wu et al., 2025a). Due to the inevitable network fluctuations and CAPTCHA, the agent will be allowed up to two additional attempts when encountering exceptions."
        },
        {
            "title": "3.2 Experiment Results",
            "content": "Effects of WebAggregatorQA Training Set The experiment results are shown in Table 3. For the zero-shot foundations, the closed-sourced models surpass the Qwen models on both the GAIA-text and WebAggregatorQA. However, after tuning on WebAggregatorQA, Qwen models exhibit clear"
        },
        {
            "title": "Technical Report",
            "content": "Methods GAIA-text WebAggregatorQA level-1 level-2 level-3 Avg. level-1 level-2 level-3 Avg. Non-Agentic GPT-4.1 Claude-3.7-sonnet Qwen2.5-7B Qwen2.5-32B Qwen3-8B Qwen3-32B Zero-shot Foundations GPT-4.1 Claude-3.7-sonnet Qwen2.5-7B Qwen2.5-32B Qwen3-8B Qwen3-32B Fine-tuned Foundations WebThinker Qwen2.5-32B WebDancer Qwen2.5-7B Qwen2.5-32B WebSailor Qwen2.5-7B Qwen2.5-32B WebShaper Qwen2.5-32B MiroThinker Qwen2.5-32B CogKernal-Pro Qwen3-8B WebAggregator Qwen2.5-7B - pass@3 Qwen2.5-32B - pass@3 Qwen3-8B - pass@3 Qwen3-32B - pass@3 10.3 35.9 12.8 20.5 12.8 17.9 51.3 74.4 23.1 46.1 33.3 48. 56.4 41.0 46.1 - - 13.5 17.3 3.8 9.6 3.8 3.8 44.2 55.8 15.4 21.2 11.5 40.4 50. 30.7 44.2 - - 8.3 0.0 0.0 8.3 0.0 0.0 16.7 33.3 0.0 0.0 0.0 16.7 11.7 22.3 6.8 13.6 6.8 8.7 43.7 60.2 16.5 28.2 18.4 40. 16.7 48.5 0.0 8.3 - - 31.0 40.7 37.9 53. 61.5 53.8 16.7 52.2 - 56. 53.8 74.4 66.7 79.5 61.5 82.1 69.2 79.5 - 42.3 30.8 63.5 44.2 67.3 34.6 53.8 55.8 67.3 - 8. 16.7 25.0 33.3 50.0 16.7 33.3 16.7 50.0 55.3 43.7 40.8 63.1 51.5 69.9 42.7 62.1 56.3 69.9 15.4 18.5 4.2 4.2 4.2 8.3 62.4 66.7 27.3 25.0 30.8 45. 4.0 5.1 1.0 1.0 1.0 1.0 22.2 25.3 3.4 10.1 5.1 10.1 2.8 2.8 0.0 0.0 2.8 0.0 11.1 11.1 2.8 5.6 5.6 5.6 5.6 6.8 1.3 1.3 1.9 1.9 25.8 28.3 6.3 11.3 9.4 14. 37.5 54.2 54.2 70.8 54.2 62.4 62.4 66.7 11.1 22.2 15.2 22.2 11.1 21.2 24.2 35.4 8.3 19.4 11.1 19.4 5.6 11.1 8.3 13.9 14.5 26.4 20.1 28.9 16.4 25.2 26.4 35.2 Table 3: The Pass@1 performance of agents on GAIA-text and WebAggregatorQA. The best performance of different settings is in bold. and steady improvements on GAIA-text and WebAggregatorQA and approach the performance of these strong baselines. Specifically, the WebAggregator based on Qwen2.5-32B and Qwen3-32B surpasses most of the strong baselines, including GPT-4.1 and WebShaper. The pass@3 performance of WebAggregator-32B achieves 69.9 on GAIA-text. These observations prove the quality of WebAggregatorQA and the effectiveness of our data construction paradigm. Difficulty of WebAggregatorQA Test Set WebAggregatorQA poses new challenge for current agent systems. GPT-4.1-powered SmolAgents attain 43.7% accuracy on GAIA-text but drop to 25.8% on WebAggregatorQA. Claude-3.7-sonnet shows similar decline. Furthermore, the performance gap between Claude and GPT-4.1 is smaller on WebAggregatorQA than on GAIA-text. This suggests that for the harder questions in WebAggregatorQA, neither model can solve them effectively, which leads to the reduced gap. These results highlight the substantial gap between current agent capabilities and the demands of information aggregation needed for multi-hop web tasks."
        },
        {
            "title": "Technical Report",
            "content": "Figure 6: Distributions of tasks required different numbers of tools (a) and aggregation operations (d). Proportion of information source (b) and aggregation operations (c) that are needed across tasks. WWQA XBench Model WebDancer-7B WebSailor-7B WebAggregator-7B WebAggregator-8B Transferability of WebAggregator Models Considering the response latency and efficiency of small foundation models, it is crucial to further enhance these smaller foundations to offer society more affordable yet powerful alternative. To explore their potential, we evaluate these models on two additional benchmarks: WebWalkerQA (Wu et al., 2025b) and XBench (Chen et al., 2025), as summarized in Table 4. In Table 3, WebAggregator-8B achieves performance comparable to GPT-4.1 on GAIA-text, demonstrating strong capabilities despite its relatively smaller size. Furthermore, both WebAggregator-8B and 7B significantly outperform previous strong baselines on WebWalkerQA and XBench. Although these results confirm that smaller WebAggregators excel on these benchmarks, they still face challenges with the more difficult tasks in WebAggregatorQA, where WebAggregator-8B notably trails behind the 32B counterpart and GPTA-4.1. Consequently, breaking through the performance bottleneck of small foundation models on hard tasks remains vital direction. Table 4: Performance on XBench and WWQA (WebWalkerQA). 36.0 - 44.7 41. - 34.3 37.0 40."
        },
        {
            "title": "4.1 How to Solve WebAggregatorQA",
            "content": "We present the distribution of the information source and aggregation operations needed to solve WebAggregatorQA, as shown in Figure 6. Diversified Information Source Reliance We observe that all of the tasks of WebAggregatorQA need information from Search and Web Text. Moreover, the tasks also require information from Files and do not rely solely on one source. Solving WebAggregatorQA requires advanced web-browsing capabilities to retrieve knowledge. The task is highly challenging for models that rely only on their internal knowledge: even strong base models such as Claude-3.7 and GPT-4.1 correctly solve fewer than 7% of the questions. The advantage of GPT-4.1 and Claude over the Qwen series is largely attributable to their multimodal (image) processing capabilities. Without access to tools to fulfill multimodal understanding, Qwen models can only answer small fraction of questions. Diversified Information Aggregation Requirements We observe that the information aggregation requirements of WebAggregatorQA challenge the agent systems. Figure 6 illustrates that all of the tasks possess these operations, and many of them contain multiple operations, further increasing the task difficulty. We then further examine the impact of information aggregation. We analyze the agents trajectories to identify the frequency of specific failure mode: successfully retrieving all of the reference URLs but fails the task. The occurrence of this failure mode indicates that the foundational agent models still struggle with information aggregation for certain Model GPT-4.1 Claude WebAgg-32B Counts Acc. 30 38 33.3 42.1 35.7 Table 5: Counts and accuracy of trajectories that accessed all of the reference URLs."
        },
        {
            "title": "Technical Report",
            "content": "reasons. From Table 5, we observe that these tasks that all of the reference URLs are visited exhibit higher accuracy compared with the overall accuracy. However, the agents still could not achieve perfect score due to the complex information aggregation logic in the tasks."
        },
        {
            "title": "4.2 Tool Usage Analysis",
            "content": "We evaluate the impact of information aggregation on agents by analyzing their tool usage patterns across WebAggregatorQA and WebWalkerQA, as shown in Figure 7. We define tool call density as the percentage of steps that involve tool usage. We observe that while tasks in WebAggregatorQA require more total steps to complete, the tool call density is notably lower. This pattern suggests that in WebAggregatorQA, models rely more heavily on reasoning steps to execute information aggregationenabling deeper synthesis and analysis of existing informationrather than predominantly invoking tools to acquire new external knowledge."
        },
        {
            "title": "4.3 Training Efficiency",
            "content": "The construction of datasets and the training of web agent models are typically resource-intensive processes. If satisfactory performance can be achieved with smaller trajectory size, the approach would become more accessible and cost-effective. Motivated by this, we train the Qwen3-8B on small subsets of WebAggregatorQA, using 500 and 1,200 samples, respectively. The model trained on 500 samples attained 36.9% accuracy on GAIA-text, while the one trained on 1,200 samples achieved 38.83%. These results further demonstrate the high quality of WebAggregatorQA; even small-scale subset can deliver significant performance gains for foundation models. Figure 7: Steps and tool use density of two models across test sets."
        },
        {
            "title": "5 Related Work",
            "content": "Resources for Web Agent Foundation Models Multi-hop questions for training web agent foundation models (Tongyi, 2025; Qiao et al., 2025; He et al., 2024b) require advanced tool use, complex reasoning (Hu et al., 2025; Wei et al., 2025b), and grounding in real-world web environments (Fang et al., 2025a), making manual dataset construction challenging. Existing QA datasets, such as HotpotQA (Yang et al., 2018) and Musique (Trivedi et al., 2022), do not capture the intricacy of authentic web interactions. While some works generate request-action pairs (Xu et al., 2025; Chen et al., 2024; He et al., 2024a; Zhang et al., 2025), these are not applicable for goal-oriented web tasks. Recent methods first construct logical flows over knowledge snippets and then synthesize tasks accordingly (Wu et al., 2025a; Li et al., 2025a; Shi et al., 2025a; Tao et al., 2025; Xia et al., 2025; Shi et al., 2025b). For instance, knowledge graphs built from offline pages are used for task generation (Shi et al., 2025a; Wu et al., 2025a), and entity expansion or formalization helps model logic flows (Xia et al., 2025; Tao et al., 2025). However, these approaches are restricted by their dependence on static pages and often neglect the aggregation of information from diverse sources (Figure 2). Moreover, their complexity mainly comes from entity tracing rather than synthesizing information across multiple sources. Benchmarking Web Agents Most existing benchmarks focus on information-seeking, requiring agents to use tools and perform multi-hop reasoning in realistic web scenarios, as in WebWalker (Wu et al., 2025b) and BrowseComp (Wei et al., 2025a). Few research (Li et al., 2025c) assess information aggregation. FRAMES (Krishna et al., 2025) aim to evaluate the factuality, retrieval, and aggregation abilities of LLMs, but their knowledge scope is limited to Wikipedia. WideSearch (Wong et al., 2025) addresses aggregation by constructing tasks involving many simple actions. GAIA (Mialon et al., 2023), which is most relevant to our work, evaluates general capabilities with human-constructed"
        },
        {
            "title": "Technical Report",
            "content": "tasks. However, recent agents (Fang et al., 2025b; Qiu et al., 2025) perform well on GAIA, indicating crucial need for more challenging benchmarks that jointly evaluate information-seeking and aggregation."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we identify the critical limitation of existing web research agents, their inadequate focus on information aggregation, which restricts their capacity for generating insightful and coherent research outputs. To address this, we propose an automated, agent-driven data construction paradigm, Explore to Evolve, that enables the synthesis of diverse and verifiable tasks demanding both information seeking and complex aggregation across real-world web environments. Our resulting WebAggregatorQA dataset and the foundation model family, WebAggregator, demonstrate substantial improvements over current baselines on GAIA-text and WebAggregatorQA. Notably, even advanced commercial models like GPT-4.1 and Claude-3.7-sonnet struggle on these tasks. Even after retrieving all of the references, the agents still struggle on WebAggregatorQA, reflecting the importance and difficulty of effective information aggregation for web agents."
        },
        {
            "title": "References",
            "content": "Carl Bereiter and Marlene Scardamalia. The psychology of written composition. 1987. URL https://api.semanticscholar.org/CorpusID:143781031. Yingshan Chang, Guihong Cao, Mridu Narang, Jianfeng Gao, Hisami Suzuki, and Yonatan Bisk. Webqa: Multihop and multimodal QA. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 1647416483. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01600. URL https://doi.org/10.1109/CVPR52688.2022.01600. Kaiyuan Chen, Yixin Ren, Yang Liu, Xiaobo Hu, Haotong Tian, Tianbao Xie, Fangfu Liu, Haoye Zhang, Hongzhang Liu, Yuan Gong, Chen Sun, Han Hou, Hui Yang, James Pan, Jianan Lou, Jiayi Mao, Jizheng Liu, Jinpeng Li, Kangyi Liu, Kenkun Liu, Rui Wang, Run Li, Tong Niu, Wenlong Zhang, Wenqi Yan, Xuanzheng Wang, Yuchen Zhang, Yi-Hsin Hung, Yuan Jiang, Zexuan Liu, Zihan Yin, Zijian Ma, and Zhiwen Mo. xbench: Tracking agents productivity scaling with profession-aligned real-world evaluations, 2025. URL https://arxiv.org/abs/2506.13651. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-FLAN: Designing data and methods of effective agent tuning for large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics: ACL 2024, pp. 93549366, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.557. URL https://aclanthology.org/2024.findings-acl.557/. Tianqing Fang, Zeming Chen, Yangqiu Song, and Antoine Bosselut. Complex reasoning over logical queries on commonsense knowledge graphs. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pp. 1136511384. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024.ACL-LONG.613. URL https: //doi.org/10.18653/v1/2024.acl-long.613. Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, and Dong Yu. Webevolver: Enhancing web agent self-improvement with coevolving world model. arXiv preprint arXiv:2504.21024, 2025a. Tianqing Fang, Zhisong Zhang, Xiaoyang Wang, Rui Wang, Can Qin, Yuxuan Wan, Jun-Yu Ma, Ce Zhang, Jiaqi Chen, Xiyun Li, Hongming Zhang, Haitao Mi, and Dong Yu. Cognitive kernelpro: framework for deep research agents and agent foundation models training, 2025b. URL https://arxiv.org/abs/2508.00414."
        },
        {
            "title": "Technical Report",
            "content": "Linda S. Flower and J. R. Hayes. cognitive process theory of writing. College Composition & Communication, 1981. URL https://api.semanticscholar.org/CorpusID:18484126. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. WebVoyager: Building an end-to-end web agent with large multimodal models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 68646890, Bangkok, Thailand, August 2024a. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-l ong.371. URL https://aclanthology.org/2024.acl-long.371/. Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Hongming Zhang, Tianqing Fang, Zhenzhong Lan, and Dong Yu. Openwebvoyager: Building multimodal web agents via iterative real-world exploration, feedback and optimization. CoRR, abs/2410.19609, 2024b. doi: 10.48550/ARXIV.241 0.19609. URL https://doi.org/10.48550/arXiv.2410.19609. Minda Hu, Tianqing Fang, Jianshu Zhang, Junyu Ma, Zhisong Zhang, Jingyan Zhou, Hongming Zhang, Haitao Mi, Dong Yu, and Irwin King. Webcot: Enhancing web agent reasoning by reconstructing chain-of-thought in reflection, branching, and rollback. arXiv preprint arXiv:2505.20013, 2025. Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu. Pubmedqa: dataset for biomedical research question answering, 2019. URL https://arxiv.org/abs/1909.0 6146. Satyapriya Krishna, Kalpesh Krishna, Anhad Mohananey, Steven Schwarcz, Adam Stambler, Shyam Upadhyay, and Manaal Faruqui. Fact, fetch, and reason: unified evaluation of retrievalaugmented generation. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 47454759, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. doi: 10.18653/v1/ 2025.naacl-long.243. URL https://aclanthology.org/2025.naacl-long.243/. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, and Jingren Zhou. WebSailor: Navigating Super-human Reasoning for Web Agent, July 2025a. URL http://arxiv.org/abs/2507.02592. arXiv:2507.02592 [cs]. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. Webthinker: Empowering large reasoning models with deep research capability, 2025b. URL https://arxiv.org/abs/2504.21776. Zijian Li, Xin Guan, Bo Zhang, Shen Huang, Houquan Zhou, Shaopeng Lai, Ming Yan, Yong Jiang, Pengjun Xie, Fei Huang, Jun Zhang, and Jingren Zhou. Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research, 2025c. URL https://arxiv.org/ abs/2509.13312. Grégoire Mialon, Clémentine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: benchmark for general ai assistants, 2023. URL https://arxiv.org/abs/2311.12983. Monica.Im. Manus ai. Technical report, Monica.Im, 2025. URL https://manus.im/. OpenAI. Introducing deep research OpenAI, 2025. URL https://openai.com/index/introduci ng-deep-research/. Zile Qiao, Guoxin Chen, Xuanzhong Chen, Donglei Yu, Wenbiao Yin, Xinyu Wang, Zhen Zhang, Baixuan Li, Huifeng Yin, Kuan Li, Rui Min, Minpeng Liao, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webresearcher: Unleashing unbounded reasoning capability in long-horizon agents, 2025. URL https://arxiv.org/abs/2509.13309."
        },
        {
            "title": "Technical Report",
            "content": "Jiahao Qiu, Xuan Qi, Tongcheng Zhang, Xinzhe Juan, Jiacheng Guo, Yifu Lu, Yimin Wang, Zixin Yao, Qihan Ren, Xun Jiang, Xing Zhou, Dongrui Liu, Ling Yang, Yue Wu, Kaixuan Huang, Shilong Liu, Hongru Wang, and Mengdi Wang. Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution, 2025. URL https://arxiv.org/abs/25 05.20286. Hongyu Ren, Weihua Hu, and Jure Leskovec. Query2box: Reasoning over knowledge graphs in vector space using box embeddings. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openre view.net/forum?id=BJgr4kSFDS. Aymeric Roucher, Albert Villanova del Moral, Thomas Wolf, Leandro von Werra, and Erik Kaunismäki. smolagents: smol library to build great agentic systems. https://github.com/hugging face/smolagents, 2025. Priyanka Sen, Alham Fikri Aji, and Amir Saffari. Mintaka: complex, natural, and multilingual dataset for end-to-end question answering. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (eds.), Proceedings of the 29th International Conference on Computational Linguistics, pp. 16041619, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https: //aclanthology.org/2022.coling-1.138/. Dingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King Zhu, Minghao Yang, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and Wangchunshu Zhou. TaskCraft: Automated Generation of Agentic Tasks, June 2025a. URL http://arxiv.org/abs/2506.10055. arXiv:2506.10055 [cs]. Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment, 2025b. URL https://arxiv.org/abs/2507.05720. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. MULTIMODALQA: COMPLEX QUESTION ANSWERING OVER TEXT, TABLES AND IMAGES. 2021. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. Webshaper: Agentically data synthesizing via information-seeking formalization, 2025. URL https://arxiv. org/abs/2507.15061. Tongyi. Tongyi-deepresearch. https://github.com/Alibaba-NLP/DeepResearch, 2025. Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. Transactions of the Association for Computational Linguistics, 2022. Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, and Moksh Jain. Recursive self-aggregation unlocks deep thinking in large language models, 2025. URL https://arxiv.org/abs/2509.26626. Jason Wei, Zhiqing Sun, Spencer Papay, Scott McKinney, Jeffrey Han, Isa Fulford, Hyung Won Chung, Alex Tachard Passos, William Fedus, and Amelia Glaese. Browsecomp: simple yet challenging benchmark for browsing agents, 2025a. URL https://arxiv.org/abs/2504.12516. Zhepei Wei, Wenlin Yao, Yao Liu, Weizhi Zhang, Qin Lu, Liang Qiu, Changlong Yu, Puyang Xu, Chao Zhang, Bing Yin, Hyokun Yun, and Lihong Li. Webagent-r1: Training web agents via end-to-end multi-turn reinforcement learning, 2025b. URL https://arxiv.org/abs/2505.16421."
        },
        {
            "title": "Technical Report",
            "content": "Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, and Ke Wang. Widesearch: Benchmarking agentic broad info-seeking, 2025. URL https://arxiv.org/abs/2508.07999. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Yong Jiang, Pengjun Xie, Fei Huang, and Jingren Zhou. WebDancer: Towards Autonomous Information Seeking Agency, May 2025a. URL http://arxiv.org/abs/2505.22648. arXiv:2505.22648 [cs]. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, 2025b. URL https://arxiv.org/abs/2501.07572. Xianjie Wu, Jian Yang, Linzheng Chai, Ge Zhang, Jiaheng Liu, Xeron Du, Di Liang, Daixin Shu, Xianfu Cheng, Tianzhen Sun, et al. Tablebench: comprehensive and complex benchmark for table question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2549725506, 2025c. Ziyi Xia, Kun Luo, Hongjin Qian, and Zheng Liu. Open data synthesis for deep research, 2025. URL https://arxiv.org/abs/2509.00375. Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: Agent trajectory synthesis via guiding replay with web tutorials, 2025. URL https://arxiv.org/abs/2412.09605. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: dataset for diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 23692380, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/ D18-1259. URL https://aclanthology.org/D18-1259/. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/ab s/2210.03629. Zhisong Zhang, Tianqing Fang, Kaixin Ma, Wenhao Yu, Hongming Zhang, Haitao Mi, and Dong Yu. Enhancing web agents with explicit rollback mechanisms. arXiv preprint arXiv:2504.11788, 2025."
        },
        {
            "title": "Technical Report",
            "content": "Figure 8: running example of Proactive Web Exploring: greater variety of interactions fosters richer diversity of knowledge and introduces more challenges throughout the process, e.g., questions built from file knowledge also test the file-processing abilities of responding agents."
        },
        {
            "title": "A Agent Structure",
            "content": "First, we introduce our agent framework. User instructions often require accessing diverse information on the webplain text, images, or filessome needing specific interactions like button clicks. Thus, the agent must go beyond extracting static page text. We categorize tools by information source: Search (Search), Static Page Parsing (Visit, StrFind), Dynamic Interaction (Input, Click, Scroll, Goback), File Processing (FileRead), and Image Captioning (ImageCaption). Observation Action Search(Query) Search results Visit(URL) Web Text & DOM StrFind(Query) Matched str in text Input(str, tbox_id) Web Text & DOM Click(button_id) Web Text & DOM Scroll(Pixels) Web Text & DOM Goback() Web Text & DOM FileRead(Path) File content Screenshot(Path) Capture screen ImageCaption(Path) Image description Our implementation utilizes code-based ReAct (Yao et al., 2023) agent, built on SmolAgents (Roucher et al., 2025), which outputs natural language thoughts, Python-coded actions, and receives code log as environment observations. Each task runs within fixed 30-step budget, where step includes an agent action and its observation. We extend the opendeepresearch SmolAgents instance with DOM parsing for finer web interactions (see Table 6). This web agent effectively handles tasks requiring information from multiple sources, as summarized in Table 1. Table 6: Action and Observation space."
        },
        {
            "title": "B More Details for WebAggregatorQA",
            "content": "B.1 More Explanations of Quality Control QA Alignment-based Filtering To improve data quality, we implemented two-stage refinement process for task construction. The first stage uses self-refinement tool with checklist (Appendix B.5) to quickly verify and revise questions during creation. Items meeting all criteria are accepted and outputted; those that dont are revised based on feedback until they comply. The second stage involves data checking agent that thoroughly reviews all reference URLs to ensure alignment of question, answer, and sources (Appendix B.4). About 11.72% of the data were filtered out for failing to meet these standards."
        },
        {
            "title": "Technical Report",
            "content": "Diversity Constraint We ensure dataset diversity by balancing domain and aggregation operation distributions. First, we annotated anchor URL domains with GPT-4.1 and sampled data to achieve more balanced domain distribution, shown in Figure 3. Second, we analyzed information aggregation types using GPT-4.1 to identify highand low-level operations (e.g., math subtraction). Although not perfectly reliable without solving the questions, common operations like calculating average GDP can be accurately detected. We then adjusted prompts to emphasize rare aggregation types, increasing their sample frequency. Figure 4 shows word cloud illustrating how different high-level aggregation tasks yield diverse specific operations, such as intersection for set operations and table processing for Science Analysis. Data Leakage Avoidance During proactive web exploration, agents may download and parse existing datasets. To prevent data contamination, we created website keyword blacklist. Pages matching the blacklist or containing identified datasets were excluded from retrieval and subsequent model evaluation to ensure the fairness of the evaluation. B.2 Testset Annotation B.2.1 Data Collection and Verification The construction of the test set includes the seed tasks collection, several rounds of revision, and cross-validation procedures. Three human annotators are involved, all of them with at least bachelors degree. Each sample requires an average of 3 hours of annotation work, and the whole procedure lasts for more than 4 weeks of part-time work (4 hours day). Seed Tasks Collection single human annotator, even if highly specialized in one domain, faces inherent limitations in generating diverse and comprehensive samples across multiple fields. To address this, we engage multiple annotators to revise 200 topic-diverse tasks, uniformly selected from WebAggregatorQA based on their domain labels. These carefully revised seed examples help ensure that the test set attains the desired diversity. Task Revision Principles Our initial analysis, consistent with prior work (Wei et al., 2025a), shows that due to high uncertainty in the web environment and an answer-to-question task design, questions are generally well-structured but often lack unique ground truths. While this ambiguity may be tolerable for training, it is unacceptable for testing. Thus, our key revision principle is to ensure each question is unambiguous and has exactly one correct answer. QA (1) Ensure clarity in the question statements. All claims must be explicitly stated, and if multiple sources of evidence exist, additional constraints should be provided in the question to avoid ambiguity. (2) The reference answer must be the sole feasible and correct one to the question. Reference Reference information, including URLs and solutions, is vital to the revision process. When these reference URLs and solutions are properly validated, the quality and reliability of the questions and answers are assured. Accordingly, annotators are required to: (1) verify the reliability of URLs, ensuring they originate from authoritative and reputable sources; (2) ensure consistency: the evidence remains stable and not prone to variation across different websites, contexts, or over time; (3) confirm the fidelity of URLs: each provided reference URL directly and substantively supports the question. Those pages that have strict CAPTCHA will be replaced with more stable ones. Then the questions and answers are revised accordingly. The second principle is to increase task complexity from the same two perspectives: complex information aggregation and diversified information sources. We provide annotators with the information aggregation guidance and encourage them to incorporate more reasoning steps into the questions to enhance their difficulty. They are also advised to leverage various information forms beyond plain webpage text. The answer should not be directly found on the web page. Verification We utilize agents to assist the human validation procedure. Initially, GPT-4.1powered agent attempts to solve the questions, facilitating identification of potential ambiguities from the solvers perspective within realistic web environment. Subsequently, human annotators"
        },
        {
            "title": "Technical Report",
            "content": "Figure 9: multimodal sample from the test set of WebAggregatorQA. To solve this task, the agent must extract information from the image to obtain clues for the next step. Since the image is not provided with the question, the agent is required to locate the relevant picture independently. review the agents responses, detect any misunderstandings, and revise the questions, solutions, and answers accordingly. This cycle is repeated once more to ensure comprehensive disambiguation. In the final stage, human annotators independently solve the tasks for cross-validation purposes. Samples that fail to achieve consensus with previously annotated answers are excluded. The independent annotation achieved agreement with 155 out of 159 references, with 4 tasks omitted due to excessive complexity, thereby validating the quality of the references. B.3 Data Construction Prompt Part-1: Proactive Online Web Exploring URL: {URL} - Task Overview Create challenging multi-hop question based on the given URL and related information. Please Information Gathering Ensure the quality of the answer when providing reference answer! calculate and verify the reference answer before giving the final data. The question should be written in the SAME language as the website content. - 1. Start by thoroughly exploring the given URL and its description. Visit and browse at least **{least_visits} different websites** to collect diverse and relevant information. Avoid relying solely on simple search engine queries or Wikipedia. actively browse, jump between pages, and record your navigation steps and key findings. After each browsing action, briefly document what you did and the important information you discovered. - 2. Formulate **multi-hop question** that MUST requires reasoning across multiple sources. page. The answer should **not** be obtainable by simple search or from single Question Design Instead, The question should be:"
        },
        {
            "title": "Technical Report",
            "content": "Challenging but natural and concise, as if real user is seeking to learn or solve puzzle. Avoid unnatural or arbitrary questions such as summing unrelated numbers. - e.g., year * (number of countries of china) is unacceptable! Self-contained. - Illustrated with essential clues that guide the respondent to locate the information without explicitly naming the sources or searching queries. must be necessary but precise, avoiding overly broad candidates. The clues - BAD EXAMPLES: Some China city has,... by specifying the name or providing clues.) Based on specific details from at least 5 to 8 different web pages. Reflective of the domains characteristics (e.g., medical: functions, gaming: guidance, players, chemistry, math, puzzles). (NOT self-contained! Specify the city"
        },
        {
            "title": "Technical Report",
            "content": "Part-2: Automatic Aggregation Logic Synthesis Composition Reasoning Operations (Mandatory) 3. Incorporate at least one of the following reasoning operations in your question: Scientific Analysis > Statistical Analysis - Analyze data from web pages, you may use, but not limited to: calculating the mean, variance, or standard deviation within specified time period. examples: 1. 2. What is the median winnings for drivers who have driven Chevrolet car? Which category exhibits the most consistent growth rate across the 5-year Some good period, and what is the average annual percentage increase for that category? 3. Can you calculate the standard deviation of the average comprehension scores across A, B, and C? > Correlation Analysis 1. What is the Pearson correlation coefficient (to two decimal places) between Chinas average annual temperature and its CO2 emissions per capita over the same period? > Trend Forecasting - Based on historical data, predict future data points. Any algorithm can be used, such as linear regression, polynomial regression, logistic regression, EMA, etc. REMEMBER: Clearly specify the basis for prediction to ensure unique answer. Some good examples: 1. Considering the historical data from 1961 to 1967, what could be the forecasted points of Suzuki in the 50cc and 125cc classes for the upcoming years? Use the average growth rate or the most recent 5-year growth rate for prediction. 2. KFF published an article on abortion in Womens Health Policy on Feb 27, 2025. Using single exponential smoothing and MSE, search for the optimal alpha (0.01-0.99) based on the historical data, the MSE loss, and use the alpha to estimate the next data point. > General Computation Intensive Tasks - Batch Data Analysis Requires Intensive Computation. The need to retrieve and process large lists of numbers makes coding ESSENTIAL. 1. 2. What is the average closure price of Apple.inc from Sep. Across all NBA seasons where Manu Ginobilis Player Efficiency Rating (PER) 2024 to Oct. 2024? exceeded 20 in the regular season, what was the average number of regular season wins by his team? > Other Tasks - Complex Algorithm with high Complexity: Try to design problems that require coding to reduce time complexity. Element-wise operations > Calculation - Selecting specific elements, performing mathematical operations between elements, e.g., probability, calculation. - Examples: 1. 2. 3. What is the sum of As speed and Bs speed? By how much does Cs value exceed Ds value? What is the difference between the population of city and city Y? > Inverse Question - Formalized as an inverse question about certain information. Avoid direct listing; use indirect clues framed as questions. identifies the subject without ambiguity. Ensure your phrasing uniquely - Examples: 1. Instead of \"Tom is singer from New York, who was born on 11 Nov 2024, he...\", you can use \"for the single from New York, who was born on 11 Nov 2024, he...\". 2. In June 2022, researchers from Huddersfield University published paper on the application of YOLO in agriculture. My research primarily focuses on ..."
        },
        {
            "title": "Technical Report",
            "content": "List/Set-wise operations > sorting (alphabetical, numerical, top-K), sum, average, counting, intersection, subtraction, merging. Examples: Which is the shortest among XXX? What is the average length of YYY? How many items appear in both set and set B? What is the total number of across all categories? 1. 2. 3. 4. Element-Set operations > checking membership or counting occurrences. Examples: 1. 2. 3. Is element part of the top 10 ranked items? Exclude all names that were born in 1984 from ... Between 2012 and 2021, was the rate of increase in Chinas average annual temperature higher or lower than the global average? 4. On the same day that landmark house on South Main Street in Coeymans Answer Requirements The numbers or elements used in these operations should be discoverable by Landing, New York, rich with local history, built in the late 1830s, officially entered the National Register of Historic Places listing, how many places entered the list total? Note: reading the web content, not directly provided in the question. 4. retrieved text and MUST be derived through reasoning. to verify. > Stable over time (avoid dynamic or real-time data). > Of clear entity type (e.g., person, number, date, place). 5. Output your final result in the following JSON format: { > The answer MUST not be obtained directly from the > Short, Concise and easy Output Format \"topic\": \"Brief description of the questions domain or topic\", \"question\": \"The constructed multi-hop question\", \"answer\": \"The answer X\", \"context\": { \"URLs\": } [ \"url_1\", \"url_2\", \"url_3\", \"url_4\", \"url_5\", ... ] }"
        },
        {
            "title": "Technical Report",
            "content": "B.4 Prompt of Data Quality Checking Agent"
        },
        {
            "title": "TASK DESCRIPTION OF DATA QUALITY CHECKING AGENT",
            "content": "{Composition Reasoning Operations Prompt} Evidence Checking URL Validity: Verification that all URLs conform to proper syntax and resolve correctly without errors. Information Relevance: Assessment of whether each URL contains information that is necessary and sufficient to address the research question. Question Checking Self-Containment: The extent to which the question is fully specified and comprehensible without requiring additional external context. Retrieval Necessity: The degree to which answering the question necessitates consulting external sources, while avoiding excessive disclosure of information within the question itself. Aggregation Necessity: The question must include at least three different aggregation operations, ensuring that the answer cannot be obtained through direct retrieval. Clarity: The precision and unambiguity of the cues or references embedded in the question that facilitate accurate data retrieval. The clues will not lead to multiple feasible answers. Temporal Stability: The property that the correct answer to the question remains consistent over time, unaffected by temporal changes (e.g., Who was the immediate past president of the United States?). Answer Quality Assessment Information Fidelity: The extent to which all information presented in the reference answer is fully consistent with the URLs or other provided external information sources. Example of inconsistency : The temperature retrieved from the reference URL is 37C, whereas the solution states 35C, resulting in an erroneous calculation of the average temperature. Ground Truth Validity: The reference answer must accurately and unambiguously reflect the requirements of the question, conforming to information obtained from authoritative and reliable data sources. The answer should be derived from recognized authoritative channels or verified databases. Ensuring verifiability through reliable sources is especially important for questions involving numerical data, statistics, or other factual information. Example of invalid answer : The moons distance from Earth is 100,000 km. This contradicts scientific consensus, which states the distance is approximately 384,400 km. Uniqueness and Unambiguity: The reference answer should be uniquely correct, avoiding ambiguity or multiple plausible solutions. Are there conflicting data from multiple sources that lead to multiple possible answers? Are there precision conflicts between different data sources (e.g., 33. vs. 33.20987)? Based on the above criteria, analyze the following data: Question: {}"
        },
        {
            "title": "Technical Report",
            "content": "Answer: {} Evidence_URLs: {} Please verify whether each item meets the standards. Output Format Return your analysis in the following JSON format: json { \"Evidence Passed\": \"Question Passed\": \"Answer Passed\": \"Domain\": \"[USE ONLY ONE WORD OF THE FOLLOWING!] Gaming, Sport, TV shows & movies, 1 or 0, 1 or 0, 1 or 0, Computer Science, Art, History, Music, Geography, Politics, Finance, Medical, Law\", \"Aggregation_Operation\": { \"type\": [\"Science Analysis Operations->Informations search->XLSX Processing of ...\", \"Element-wise->Math->Addition\", \"Science Analysis Operations->Batch Data Processing->\", ...] } }"
        },
        {
            "title": "Technical Report",
            "content": "B.5 Prompt of Intergrated Data Quality Checking Tool"
        },
        {
            "title": "PROMPT OF EFFICIENT QUESTION CHECKING TOOL",
            "content": "{Composition Reasoning Operations Prompt} Question Checking Self-Containment: The extent to which the question is fully specified and comprehensible without requiring additional external context. Retrieval Necessity: The degree to which answering the question necessitates consulting external sources, while avoiding excessive disclosure of information within the question itself. Aggregation Necessity: The question must include at least three different aggregation operations, ensuring that the answer cannot be obtained through direct retrieval. Clarity: The precision and unambiguity of the cues or references embedded in the question that facilitate accurate data retrieval. The clues will not lead to multiple feasible answers. Temporal Stability: The property that the correct answer to the question remains consistent over time, unaffected by temporal changes (e.g., Who was the immediate past president of the United States?). {} Based on the above criteria, analyze the following data: Question: Answer: {} Evidence_URLs: {} Please verify whether each item meets the standards and provide advice for improvements."
        }
    ],
    "affiliations": [
        "Tencent AI Lab",
        "The Chinese University of Hong Kong"
    ]
}