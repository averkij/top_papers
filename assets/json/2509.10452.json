{
    "paper_title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers",
    "authors": [
        "Akshat Pandey",
        "Karun Kumar",
        "Raphael Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios."
        },
        {
            "title": "Start",
            "content": "WHISTLE: DEEPLY SUPERVISED, TEXT-ONLY DOMAIN ADAPTATION FOR PRETRAINED SPEECH RECOGNITION TRANSFORMERS Akshat Pandey,1 Karun Kumar,1 Raphael Tang2 1Comcast Applied AI 2University College London 5 2 0 2 2 ] . [ 1 2 5 4 0 1 . 9 0 5 2 : r ABSTRACT Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, deeply supervised, text-only adaptation method for pretrained encoder decoder ASR models. WhisTLE trains variational autoencoder (VAE) to model encoder outputs from text and finetunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios. 1. INTRODUCTION Although state-of-the-art automatic speech recognition (ASR) models such as Whisper [1] are trained on hundreds of thousands of hours of speechtext pairs, they still benefit from domain adaptation, especially if the target domain contains words unseen in the source domain. Unfortunately, gathering speech in the target domain may be infeasible, either financially or otherwise, limiting us to text-only adaptation. Furthermore, using pretrained ASR models precludes us from applying model architectures that afford text-only training [25], as those require training from scratch. This problem is not merely an academic artifact: in real-world deployment, users continuously evolve and speak new words and parlance, reducing ASR effectiveness. standard text-only adaptation approach called shallow fusion is to train an auxiliary language model (LM) over the target domain text and linearly combine the log probabilities of the ASR model and the LM during decoding. However, this can still be ineffective for unseen words since end-to-end ASR models do not generalize well to novel grapheme sequences. More promisingly, recent approaches propose to synthesize speech using text-to-speech (TTS) models, then fine-tune the ASR model end-to-end [6]. Nevertheless, TTS provides only inputoutput supervision: it does not explicitly guide how an end-to-end models internal state should adapt Fig. 1. Our WhisTLE adaptation process. Speechtext pairs are treated as usual (see left), while text data with no paired audio is passed through our frozen text-to-latent encoder (TLE), trained variational auto-encoder (VAE) instead of the Whisper encoder, before being fed to the decoder (see right). to target domain. Prior work has shown deep supervision to benefit knowledge distillation [7] and image classification [8], to name few examples. In this work, we propose WhisTLE (adapting Whisper with Text-to-Latent Encodings), deeply supervised adaptation approach for encoderdecoder ASR models, such as OpenAIs Whisper [1] and Canary [9]. WhisTLE runs complementary to TTS adaptation, which supervises only the input speech and the output text, by also deeply supervising the ASR models latent state. Specifically, we train variational autoencoder (VAE) [10] to directly model ASR encoder output using text rather than audio. Then, we fine-tune the ASR decoder using our text-only encoder as drop-in replacement for the ASR encoder, optionally mixing in TTS adaptation. Our main contributions are as follows: First, to the best of our knowledge, we propose the first deeply supervised, text-only domain adaptation method for pretrained encoder decoder ASR transformers. Unlike prior work on text-only training with novel architectures, our approach applies in the pretrained setting. Second, we show that WhisTLE improves ASR quality for four state-of-the-art models on outof-distribution domains, particularly when combined with TTS adaptation, likely due to its deep latent supervision in addition to TTSs inputoutput supervision. Across four out-of-domain datasets, two in-domain datasets, and four encoderdecoder models, WhisTLE with TTS yields 12.3% average relative WER reduction over TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 out of 32 experimental scenarios. 2. METHODOLOGY Recently, transformer-based models [11] have been applied to ASR with great success [1214]. Whisper [1] and Canary [9] are such state-of-the-art encoderdecoder transformers; we thus focus on them in this paper. In the context of speech recognition, text-only domain adaptation refers to training speech recognition model without any ground-truth audio, only text. Current approaches can be broadly split into two categories: those that modify and extend existing ASR models to handle new vocabulary [1520], and those that detail novel ASR architectures capable of textonly training [25]. We are solely concerned with the former since the latter additionally requires copious amounts of speech recognition training from scratch; we wish to work exclusively with existing ASR models, as that enables us to leverage large-scale pretrained models and greatly reduce resource consumption. Literature regarding text-only domain adaptation for existing ASR models can be further split into approaches that generatively model ASR input when audio is not available [1517], and approaches that integrate the use of an external language model at inference time [1820]. 2.1. Our WhisTLE Approach The goals in our approach to text-only adaptation are as follows: (1) maximize our leveraging of the current state of the art in automatic speech recognition, (2) minimize training time and resource efforts, and (3) require no additional resources at inference time. Our WhisTLE approach differs from prior approaches making use of an external language model because it requires no additional resources at inference time. It differs from prior generative approaches in that we train model to synthesize transformer encoder output given text, rather than modeling audio or Mel spectrograms directly. Our approach rests on two hypotheses: that deep supervision of latent states helps for domain adaptation, and that modeling transformer models encoder output is simpler (i.e., more training efficient, as we show in Section 4) than modeling speech in general. In line with the information bottleneck hypothesis [21], this is likely due to encoder representations capturing less information than speech, as they focus on what is necessary for speech recognition. Concretely, we train variational autoencoder (VAE) [10] to precisely model Whisper [1] encoder outputs given text input. After the text adaptation VAE is trained, the Whisper decoder can be fine-tuned on text without corresponding audio by using the VAE output rather than the Whisper encoder output during training. At inference time, Whisper is run exactly the same as it was prior to text-only adaptation, without the use of any additional runtime or memory. Figure 1 displays the pipeline for training Whisper using the text-only encoder. Formally, let Rn be speech audio waveform in the target domain and the string Σℓ be the tokenized transcript (of ℓ Fig. 2. Model architecture of our text-to-latent encoding VAE (purple), which also follows an encoderdecoder paradigm. length) of x, where Σ is the vocabulary. During full Whisper training, audio input is encoded using the Whisper encoder fθ : Rn (cid:55) Rn/kh, where is the downsampling factor and is the number of hidden dimensions. The encodings are then fed to the decoder gθ : Rn/kh (cid:55) P(Σℓ), which outputs differentiable probability distribution over strings of length ℓ. We optimize the networks parameters θ end-to-end using gradient descent on backpropagating the negative loglikelihood loss, e.g., θ := argmin θ LNLL, LNLL := ℓ (cid:88) i=1 log gθ(fθ(x))[yi], where the indexing operation [yi] denotes the output probability associated with token yi at timestep i. During text-only adaptation with WhisTLE, we instead : Σℓ (cid:55) replace fθ with frozen text-based encoder TLE Rn/kh. The Whisper encoder is set aside, and we pass only the tokens to TLE to produce an approximation of the Whisper encoder output fθ(x). We can then feed this approximation to the decoder for training and proceed with log-likelihood optimization as usual: ϕ ϕ θ TLE := argmin θ LNLL, LNLL := ℓ (cid:88) i=1 log gθ(f TLE ϕ (y))[yi]. Note the lack of dependence on x. After text-only training, we abandon TLE and use Whisper in the standard fashion. ϕ Figure 2 displays the architecture of our proposed text-tolatent encoder (TLE). The model is convolutional VAE with three convolutional encoder layers and four convolutional decoder layers, with residual connections between the respective encoder and decoder layers. Text is embedded and upsampled using transposed convolutional layer prior to entering convolutional encoder layers. Table 1. Text-only adaptation results for Whisper-large Table 2. Text-only adaptation results for Whisper-medium Method None TLE TTS SF TTS+SF TLE+TTS TLE+TTS+SF None TLE TTS SF TTS+SF TLE+TTS TLE+TTS+SF a n o - c n o e i L Out-of-Domain Dataset EMNS EmoV-DB ST-AEDS EABI 14.6 9.5 11.8 10.1 8.1 7.2 6.6 11.8 11.3 10.8 84.5 41.6 8.7 26.3 11.6 11.8 7.9 10.5 9.0 6.9 8.2 26.4 20.2 17.5 69.8 61.4 10.1 84.7 6.4 3.8 2.8 6.1 3.2 2.5 3.4 7.1 6.3 4.1 52.1 52.1 4.2 72.7 9.5 8.5 7.1 11.3 4.7 6.1 3.3 9.4 8.6 7.8 32.9 53.8 7.1 22.0 Method None TLE TTS SF TTS+SF TLE+TTS TLE+TTS+SF None TLE TTS SF TTS+SF TLE+TTS TLE+TTS+SF s D m - c n o e i L Out-of-Domain Dataset EMNS EmoV-DB ST-AEDS EABI 12.5 7.5 4.8 17.9 5.9 4.0 3.9 6.4 5.9 4.2 4.9 2.5 3.9 2.2 16.7 9.3 5.6 15.2 8.8 6.1 6.1 8.7 8.3 6.0 17.6 7.4 5.3 6. 13.7 6.2 4.3 17.3 10.9 2.3 6.2 3.1 3.0 2.1 9.6 3.6 2.0 5.5 23.1 13.7 13.5 23.7 11.8 8.4 6.4 9.6 7.5 5.5 18.4 9.7 4.8 8.5 We train the VAE itself on speech and text in the source domain, e.g., xs and ys. We use Whisper encoder outputs as the ground truth and optimize the standard VAE loss function with beta regularization: LVAE := Efθ(x) TLE ϕ (y)2 2 + βKL (Pϕ(z) (0, I)) , where (µ, Σ) represents the multivariate normal distribution with mean µ and covariance matrix Σ, the first term is the mean-squared reconstruction loss, and the second term controls how far the posterior deviates from the isotropic unit Gaussian prior, scaling with β. Our TLE VAE TLE is trained to minimize this objective, e.g., argminϕ LVAE. ϕ 3. EXPERIMENTS 3.1. Data and Setup We use six total different datasets in our testing, two of which we consider in domain and four which we consider out of domain. For the in-domain datasets, we use CommonVoice [22] and LibriSpeech [23]. For the out-of-domain datasets, we use EMNS [24], EmoV-DB [25], the Free ST American English corpus (ST-AEDS) [25], and the Opensource Multi-speaker Corpora of the English Accents in the British Isles (EABI) [26]. These choices reflect the practical setting where an end-to-end ASR model pretrained in domain with large-scale data (e.g., CommonVoice) is adapted to target domain with smaller resources. We assess our WhisTLE approach following these steps: (1) fine-tuning Whisper on an in-domain dataset using the standard approach; (2) training our VAE on the same indomain dataset; (3) training Whisper using text-only training on an out-of-domain dataset; then (4) testing Whisper on both the audio and text pairs of the out-of-domain dataset. As our baselines, we compare WhisTLE to training on TTSgenerated data, as well as the use of shallow fusion at inference time. We also run experiments combining multiple adaptation approaches, as they are not mutually exclusive. For fine-tuning on in-domain datasets, we train Whisper for 100K steps with batch size of 8. For fine-tuning with text-only training on out-of-domain datasets (e.g., WhisTLE and TTS adaptation), we train for 50K steps with batch size of 8. During text-only training, for every step of text-only training, we also train on the in-domain dataset for two steps using the audio and text, in order to prevent the model from catastrophically forgetting in-domain data. Shallow fusion (SF) evaluation is done using trigram language model trained on the out-of-domain datasets and linearly combined (scaled by weight γ) with the outputs of Whisper model fine-tuned on an in-domain dataset. For all SF experiments, we search over γ values of 0.10, 0.25, 0.50, and 0.75, and report the best result of the set. We use the FastSpeech2 [27] model trained on CommonVoice and the SpeechT5 [28] model trained on LibriSpeech for all TTS experiments with CommonVoice and LibriSpeech as the indomain datasets, respectively. For all TTS experiments, we generate audio for out-of-domain datasets using randomly selected audio from the in-domain dataset for speaker simulation during generation. We then fine-tune the base model on the generated audio after being fine-tuning on the in-domain audio. When combining TTS and TLE, TLE training is also performed on the same data as TTS training. 3.2. Main Whisper Results and Discussion Tables 1 and 2 display the changes in word error rates (WER) on out-of-domain datasets for our WhisTLE approach (TLE for short), training using TTS-generated data (TTS), and shallow fusion (SF), as well as combining multiple approaches. Overall, combining TLE and TTS attains the best average WER of 5.6 across the treatmentssupporting our hypothesis that the model benefits from both inputoutput and latent supervisionfollowed by TTS at 7.2, TLE at 8.8, no adaptation at 11.9, and the rest higher than 15. Although TTS outperforms TLE, combining TLE and TTS yields an average WER improvement (1.64 points) roughly equal to the origTable 3. Text-only adaptation results for Canary-1B Table 4. Text-only adaptation results: Canary-180M-flash Model Out-of-Domain Dataset EMNS EmoV-DB ST-AEDS UIED m - Standard TOA Standard TOA 7.8 7.4 12.2 11. 18.4 16.8 31.0 24.5 3.7 4.5 5.6 5.5 8.6 8.1 7.8 6.4 Model Out-of-Domain Dataset EMNS EmoV-DB ST-AEDS UIED a - Standard TOA Standard TOA 10.4 10.6 20.5 9.4 10.3 10.6 35.9 9.3 4.9 4.7 23.5 3.5 9.4 7.0 30.4 9. inal performance gap between TTS and TLE alone (1.60), showing strong compounding effect. TLE outperforms shallow fusion in 14 out of 16 occurrences, largely due to the increased occurrence of repetition hallucinations when using shallow fusion with Whisper. Adding TLE in any combination (e.g., TLE+TTS vs. TTS) helps in 41 out of 48 (85%) scenarios, resulting in an average relative WER drop of 17%. Dataset-wise, the largest absolute gains from TLE+TTS appear on ST-AEDS and EMNS, with average drops of 34 WER compared to either TTS or TLE alone. Improvements are somewhat smaller on EABI (2 points) and EmoV-DB (1.5 points) but remain consistent in direction. Model-wise, Whisper-medium consistently benefits more from TLE+TTS, with average WERs below 6.0 on all out-of-domain sets and showing relative drops exceeding 25% compared to no adaptation. Whisper-large shows strong gains from TLE+TTS on CommonVoice (50% relative drop vs. None), but its LibriSpeech results are noisier, partly due to SF catastrophically raising WER. Across both models, the direction of improvement is robust, confirming that the compounding benefit of TLE and TTS is not model specific. 3.3. Auxiliary Canary Experiments and Results To ensure generalization to other encoderdecoder speech recognition models, we run experiments for Canary-1B [9] and Canary-180M-flash [29]. Canary-1B uses Conformerbased encoder [14] and standard transformer decoder, while Canary-180M-flash is smaller variant of Canary-1B with faster training and inference speeds. The only adjustment we make for WhisTLE is adding an additional linear layer at the end of our VAE, as Canary, unlike Whisper, also requires encoding lengths to be input into its decoder, so our VAE must also produce encoding lengths. Tables 3 and 4 report WhisTLE results for Canary-1B and Canary-180M-flash on four out-of-domain datasets. With LibriSpeech as the in-domain corpus, Canary-1B achieves 16.3% relative WER reduction and Canary-180M-flash substantial 71%, improving on all datasets. These trends align with our Whisper results, where latent supervision consistently lowered WER. Using CommonVoice yields smaller gains, with 4.4% for Canary-1B and 6.0% for Canary-180Mflash, like Whispers more modest improvements on challenging datasets such as EmoV-DB. Overall, these findings confirm that text-only latent supervision generalizes beyond Whisper to other encoderdecoder ASR models. 4. RELATED WORK Past generative approaches include using synthetic text-tospeech (TTS) systems to generate out-of-distribution audio [17] and generative adversarial networks to synthesize Mel spectrograms [16]. Zheng et al. [17] rely on closedsource paid TTS system, so model details are unavailable. Bataev et al. [16] uses 130M parameters for spectrogram generation and audio synthesis. Our FastSpeech2 [27] implementation has 46M parameters and SpeechT5 [28] has 144M, both using the HiFi-GAN vocoder. Our TLE approach uses 91M parameters for Whisper-medium and 104M for Whisper-large, similar in size to these TTS systems but much faster to train. FastSpeech2 trains on batches of 48 sentences for 160K steps, while SpeechT5 jointly pretrains on text and audio with 12K-token batches before TTS fine-tuning. Both TTS papers above also do not include the additional training required for HiFi-GAN. In contrast, our TLE trains for only 100K steps on batches of 4 sentences. Using separate language models in ASR dates back to hidden Markov models and remains widely used. In end-toend ASR, shallow fusion [18] combines model output scores with an external LM during inference. Meng et al. [19] approximates internal LM scores in end-to-end ASR for better external LM integration; Li et al. [20] use reranking or deep fusion with LLaMA to handle out-of-domain vocabulary. On the other hand, architecture-based approaches introduce new models to jointly learn from audio and text. Deng et al. [30] modifies Wav2vec2 [12] and use large language model-based decoder to reduce reliance on acoustic representations. Hybrid Autoregressive Transducer [31] proposes modular model with an internal LM trainable on text only. 5. CONCLUSIONS AND FUTURE WORK We introduced WhisTLE, deeply supervised, text-only adaptation method for pretrained encoderdecoder ASR. By modeling encoder outputs from text and fine-tuning the decoder, WhisTLE complements the inputoutput supervision of TTS adaptation. Across our experiments, WhisTLE with TTS reduced WER by 12.3% on average and outperformed all non-WhisTLE baselines in most scenarios. These results demonstrate the efficacy of deep supervision for text-only adaptation. In the future, we plan to generalize this adaptation paradigm beyond ASR. 6. REFERENCES [1] A. Radford, J.W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, Robust speech recognition via large-scale weak supervision, 2022. [2] T. Vuong, K. Mundnich, D. Bekal, V. Elluru, et al., AdaBERT-CTC: Leveraging BERT-CTC for text-only domain adaptation in ASR, in EMNLP, 2023. [3] S. Ling, G. Ye, R. Zhao, and Y. Gong, Hybrid attention-based encoderdecoder model for efficient language model adaptation, 2023. [4] K. Deng and P.C. Woodland, Decoupled structure for improved adaptability of end-to-end models, 2023. [5] K. Deng, S. Cao, Y. Zhang, and L. Ma, Improving hybrid CTC/attention end-to-end speech recognition with pretrained acoustic and language model, 2021. [6] Y. Perrin and G. Boulianne, Towards improved speech recognition through optimized synthetic data generation, 2025. [7] S. Sun, Y. Cheng, Z. Gan, and J. Liu, Patient knowledge distillation for BERT model compression, 2019. [8] C. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu, Deeply-supervised nets, in AISTATS, 2015. [9] K.C. Puvvada, P. Zelasko, H., O. Hrinchuk, N. Rao Koluguri, et al., Less is more: Accurate speech recognition and translation without web-scale data, 2024. [10] D.P. Kingma and M. Welling, Auto-encoding variational bayes, 2022. [11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, et al., Attention is all you need, 2023. [12] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, wav2vec 2.0: framework for self-supervised learning of speech representations, 2020. [13] W. Hsu, B. Bolte, Y.H. Tsai, K. Lakhotia, et al., HuBERT: Self-supervised speech representation learning by masked prediction of hidden units, 2021. [14] A. Gulati, J. Qin, C. Chiu, N. Parmar, Y., J. Yu, W. Han, et al., Conformer: Convolution-augmented transformer for speech recognition, 2020. [15] R. Joshi and A. Singh, simple baseline for domain adaptation in end to end ASR systems using synthetic data, in ECNLP, 2022. [16] V. Bataev, R. Korostik, E. Shabalin, et al., Text-only domain adaptation for end-to-end ASR using integrated text-to-Mel-spectrogram generator, 2023. [17] X. Zheng, Y. Liu, D. Gunceler, , and D. Willett, Using synthetic audio to improve the recognition of out-ofvocabulary words in end-to-end ASR systems, 2021. [18] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H. Lin, et al., On using monolingual corpora in neural machine translation, 2015. [19] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong, Internal language model adaptation with text-only data for end-to-end speech recognition, 2022. [20] Y. Li, Y. Wu, J. Li, and S. Liu, Prompting large language models for zero-shot domain adaptation in speech recognition, 2023. [21] N. Tishby, F.C. Pereira, and W. Bialek, The information bottleneck method, 2000. [22] R. Ardila, M. Branson, K. Davis, M. Henretty, Common Voice: massivelyM. Kohler, et al., multilingual speech corpus, 2020. [23] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, Librispeech: An ASR corpus based on public domain audio books, in ICASSP, 2015. [24] K.A. Noriy, X. Yang, and J.J. Zhang, EMNS /imz/ corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels, 2023. [25] A. Adigwe, N. Tits, K. El Haddad, et al., The emotional voices database: Towards controlling the emotion dimension in voice generation systems, 2018. [26] I. Demirsahin, O. Kjartansson, A. Gutkin, and C. Rivera, Open-source Multi-speaker Corpora of the English Accents in the British Isles, in LREC, 2020. [27] Y. Ren, C. Hu, X., T. Qin, S. Zhao, Z. Zhao, and T. Liu, FastSpeech 2: Fast and high-quality end-to-end text to speech, 2022. [28] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, et al., SpeechT5: Unified-modal encoderdecoder pretraining for spoken language processing, 2022. [29] P. Zelasko, K. Dhawan, D. Galvez, K.C. Puvvada, A. Pasad, et al., Training and inference efficiency of encoderdecoder speech models, 2025. [30] K. Deng and P.C. Woodland, Adaptable end-to-end ASR models using replaceable internal LMs and residual softmax, 2023. [31] Z. Meng, T. Chen, R. Prabhavalkar, Y. Zhang, G. Wang, K. Audhkhasi, J. Emond, T. Strohman, B. Ramabhadran, W.R. Huang, E. Variani, Y. Huang, , and P.J. Moreno, Modular hybrid autoregressive transducer, 2023."
        }
    ],
    "affiliations": [
        "Comcast Applied AI",
        "University College London"
    ]
}