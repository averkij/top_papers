{
    "paper_title": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking",
    "authors": [
        "Hongzhan Lin",
        "Zixin Chen",
        "Zhiqi Shen",
        "Ziyang Luo",
        "Zhen Ye",
        "Jing Ma",
        "Tat-Seng Chua",
        "Guandong Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, a fully automated arena-style evaluation framework that conducts comprehensive, stage-wise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claim-evolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs' factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and end-to-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers a scalable and trustworthy paradigm for diagnosing LLMs' factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications."
        },
        {
            "title": "Start",
            "content": "Towards Comprehensive Stage-wise Benchmarking of Large Language Models in Fact-Checking Hongzhan Lin, Zixin Chen, Zhiqi Shen, Ziyang Luo, Zhen Ye, Jing Ma, Tat-Seng Chua and Guandong Xu 1 6 2 0 2 ] . [ 1 9 6 6 2 0 . 1 0 6 2 : r AbstractLarge Language Models (LLMs) are increasingly deployed in real-world fact-checking systems, yet existing evaluations focus predominantly on claim verification and overlook the broader fact-checking workflow, including claim extraction and evidence retrieval. This narrow focus prevents current benchmarks from revealing systematic reasoning failures, factual blind spots, and robustness limitations of modern LLMs. To bridge this gap, we present FactArena, fully automated arenastyle evaluation framework that conducts comprehensive, stagewise benchmarking of LLMs across the complete fact-checking pipeline. FactArena integrates three key components: (i) an LLM-driven fact-checking process that standardizes claim decomposition, evidence retrieval via tool-augmented interactions, and justification-based verdict prediction; (ii) an arena-styled judgment mechanism guided by consolidated reference guidelines to ensure unbiased and consistent pairwise comparisons across heterogeneous judge agents; and (iii) an arena-driven claimevolution module that adaptively generates more challenging and semantically controlled claims to probe LLMs factual robustness beyond fixed seed data. Across 16 state-of-the-art LLMs spanning seven model families, FactArena produces stable and interpretable rankings. Our analyses further reveal significant discrepancies between static claim-verification accuracy and endto-end fact-checking competence, highlighting the necessity of holistic evaluation. The proposed framework offers scalable and trustworthy paradigm for diagnosing LLMs factual reasoning, guiding future model development, and advancing the reliable deployment of LLMs in safety-critical fact-checking applications. Index TermsFact-checking, large language models, benchmark, trustworthy auditing. I. INTRODUCTION ARGE language models (LLMs) have significantly advanced the field of natural language processing (NLP), demonstrating remarkable improvements across numerous tasks [1], [2]. Previous studies have particularly highlighted the capability of LLMs to perform different tasks in the factchecking pipeline [3][5]. Despite these advancements, LLMs remain vulnerable to factual inaccuracies and are prone to errors in reasoning [6], [7]. Mistakes arising from flawed memorized knowledge or inadequate factual reasoning, can undermine their reliability and effectiveness in fact-checking applications [8][10]. Consequently, it is critical to systematically identify and understand the capabilities of LLMs in factHongzhan Lin, Zixin Chen, Jing Ma are with the Department of Computer Science, Hong Kong Baptist University. (E-mail: cshzlin@comp.hkbu.edu.hk) Zhiqi Shen, Ziyang Luo are with Salesforce AI Research. Zhen Ye is with The Hong Kong University of Science and Technology. Tat-Seng Chua is with National University of Singapore. Guandong Xu is with The Education University of Hong Kong. Fig. 1. The comparison of different focuses between our proposed FactArena and traditional solutions in fact-checking evaluation. Different from the traditional solutions that only focus on claim verification, FactArena aims to scrutinize the full stages of the fact-checking pipeline. checking to ensure their robustness and enhance their overall trustworthiness. As illustrated in Figure 1, the fact-checking process [11] typically consists of three critical stages: (i) claim extraction, where claims requiring verification are identified; (ii) evidence retrieval, where relevant sources supporting or refuting these claims are gathered; and (iii) claim verification, which involves assessing the veracity of the claims based on the retrieved evidence and providing justifications for the resulting verdict. Traditional approaches [12], [13] for evaluating LLM performance in fact-checking have primarily focused on the last stage: claim verification, while largely neglecting the equally critical earlier stages. Benchmarking LLM performance comprehensively across all fact-checking stages is essential, not solely claim verification. LLMs are increasingly deployed throughout the entire fact-checking pipeline, especially when addressing complex real-world claims. Overlooking any stage, especially the two preliminary stages, could severely compromise the integrity and effectiveness of the overall fact-checking systems. Thus, thorough and systematic assessment of LLM capabilities at every stage is indispensable. comprehensive auditing can yield deeper insights into the model limitations, thereby significantly advancing the reliable deployment of LLMs in practical fact-checking scenarios. Designing such an evaluation is inherently challenging, as the different tasks of the fact-checking pipeline may involve open-ended judgments that lack clear-cut ground truth. potential remedy lies in leveraging platforms such as Chatbot Arena [14], which crowdsource annotations from diverse users and employ pairwise comparisons to yield more nuanced and objective assessments of LLM capabilities. However, this strategy requires considerable data preparation and annotation resources [15], rendering it prohibitively costly when applied at scale. To address these challenges, we propose systematic, automated arena-style framework that evaluates LLMs across the entire fact-checking pipeline using unbiased collective judgments. The goal is to facilitate comprehensive and reliable assessments of LLMs fact-checking capacities. Our framework is designed around three key principles: 1) The framework scrutinizes all phases of the fact-checking process rather than limiting evaluation to claim verification, thereby capturing holistic view of model capabilities. 2) The framework must integrate diverse evaluator perspectives to ensure fair assessments of LLM performance. To ensure reliability as well as minimizing subjectivity in single judgment, we draw on the notion of collective intelligence [16], aligning judgments across heterogeneous evaluators by establishing consensus on factuality. 3) The framework automates the evaluation process to enable efficient and large-scale assessments. Inspired by agentic evaluation approaches [17], [18], we modularize the auditing of LLMs, thereby reducing reliance on costly manual annotation. In this work, we introduce multi-agent evaluation framework, FactArena, to systematically benchmark the factchecking capabilities of LLMs across all stages of the pipeline in an arena-style fashion. Specifically, we first design an automatic fact-checking pipeline deployed with LLMs. Then, aligned with the fact-checking process, our framework is designed with stage-wise peer-battle arenas in three steps: (i) assessing the ability of LLMs to decompose complex claims and plan verification tasks; (ii) evaluating their capacity to retrieve and integrate auxiliary evidence through tool-augmented interactions; and (iii) auditing the quality of their explanations and the soundness of their final verdicts. Besides, we incorporate an arena-driven claim evolution mechanism that generates progressively more challenging check-worthy claims based on model performance, thereby enabling more adaptive and model-centric evaluation. By unifying these stages under scalable, automated evaluation paradigm, FactArena offers holistic and unbiased lens for understanding LLMs strengths and limitations. We believe this framework provides critical step toward advancing rigorous, reliable, and trustworthy deployment of LLMs in real-world fact-checking applications. Our contributions can be summarized as follows: To the best of our knowledge, this is the first work to fully automate the trustworthy benchmarking of LLMs across the entire fact-checking pipeline. Our framework provides an analytical lens for assessing stage-wise factual reasoning under diverse, open-form evaluation scenarios. We propose FactArena, an arena-style, agent-based evaluation framework that enables fair and comprehensive evaluations that address the inherently open-ended and multi-stage nature of fact-checking. We introduce an arena-driven claim evolution module that adaptively generates increasingly challenging and unseen claims. This mechanism systematically probes the factual robustness and knowledge boundaries of LLMs, offering dynamic complement to fixed test sets. Extensive experiments demonstrate that FactArena substantially improves the reliability and wholeness of LLM evaluation in the fact-checking workflow. Judgments from 2 diverse agent evaluators exhibit strong consistency and align closely with human expert assessments, providing actionable insights for trustworthy LLM auditing. II. RELATED WORK a) Fact-Checking Evaluation: Automated fact-checking has received growing attention in the NLP community as strategy to combat misinformation and disinformation. Over the past few years, variety of datasets have been introduced to support the development and evaluation of fact-checking systems. These include collections based on: human-curated claims from Wikipedia [19][21]; fabricated claims from news outlets [22][25]; rumorous claims on social media [26] [28]; complex claims requiring multi-hop reasoning [29], [30]; naturally occurring claims in specialized domains [31][34]; and, more recently, misinformation generated by LLMs [35]. To assess the factual knowledge of LLMs, [12] consolidated several representative datasets into unified benchmark, aiming to reveal weaknesses in LLM-based fact verification. However, in addition to the inevitable risk of test set leakage, this static evaluation paradigm largely depends on expertdesigned datasets and specialized tasks. Such reliance limits adaptability to emerging forms of misinformation, particularly LLM-generated content, and falls short in capturing the openended complexity of real-world applications. Distinct from prior static accuracy evaluations, our work leverages the justifications produced by LLMs [11], [36] to enable dynamic auditing. Rather than focusing solely on veracity prediction, we seek to actively elicit and analyze the limitations of LLMs in fact-checking, thereby offering more adaptive and finegrained understanding of their capabilities. b) Multi-agent System: Deliberation among multiple agents can enhance factual accuracy and robustness through debate and consensus. To this end, multi-agent frameworks have been developed to address complex tasks through coordinated interactions. For instance, MetaGPT [37], ChatDev [38], and AgentVerse [39] assign specialized roles and enable information sharing and cross-checking via natural language dialogue, often surpassing single-agent approaches [40]. Roleplaying paradigms such as CAMEL [41] further illustrate how inception prompting supports autonomous collaboration while reducing reliance on human supervision. Beyond task-specific applications, generative multi-agent environments [17], [42] demonstrate that agents equipped with memory and reflection can simulate complex social interactions and long-term behavioral dynamics. While LLM-based multi-agent evaluation has been applied to audit fact-checking performance in the previous work [13], it largely overlooks the equally critical stages of claim extraction and evidence retrieval, primarily focusing on claim verification. This gap underscores the need for comprehensive evaluation [43] across the full stage-wise fact-checking pipeline, particularly for assessing LLM capacities in claim extraction, evidence retrieval, and justification production, where no closed-form ground truth is available for direct benchmarking. 3 Fig. 2. An overview of our proposed FactArena framework. We automatically conduct comprehensive benchmarking process for large language models in complete fact-checking stages (e.g., claim extraction, evidence retrieval, and claim verification), distinct from previous traditional audits only focused on the claim verification stage, which is only part of the full fact-checking pipeline. III. METHODOLOGY where is the number of decomposed sub-claims. A. Overview Problem Statement. Given complex claim C, factchecking aims to predict the factuality and provide convincing justifications, to evaluate the claim as supportive (i.e., true) or refuted (i.e., false), based on evidence as auxiliary information. Our objective is to develop an agent-driven evaluation framework, which aims to conduct comprehensive and unbiased arena-style evaluation that automatically assesses LLMs stage-wise abilities to fact-check complex claims. To thoroughly scrutinize the fact-checking capacities of LLMs in different fact-checking stages, the FactArena framework consists of the following three major components: 1) Construct the completed fact-checking process driven by LLMs (III-B); 2) Conduct peer-battle and automatic judgment by each fact-checking stage in an arena-like manner (III-C); 3) Generate progressively challenging check-worthy claims based on model performance to further assess the models capabilities (III-D). An overview of our FactArena framework is shown in Figure 2. B. LLM-based Fact-Checking Task Formulation To comprehensively benchmark the fact-checking ability of LLMs, we need to construct completed fact-checking pipeline by deploying the target model as the core controller. The pipeline consists of three sequential stages: 1) Claim Extraction; 2) Evidence Retrieval; and 3) Justification Production & Verdict Prediction (i.e., Claim Verification). a) Claim Extraction: It refers to decomposing complex claim into verifiable sub-claims, which then serve as the basis for planning the subsequent fact-checking process. This formulation aligns claim extraction with task decomposition, ensuring that each sub-claim can be systematically verified through evidence retrieval and justification generation. Specifically, for the complex claim in our designed fact-checking pipeline, we first employ the target LLM to decompose it as set of sub-claims: {c1, c2, ..., ck} LLM(C), (1) b) Evidence Retrieval: It aims to find and summarize factual information beyond the claim from external knowledge sources to indicate veracity. Specifically, the target LLM would integrate the external tool (i.e., Google Search) to retrieve potentially relevant information. The claim is used as the query, and we collect the title and snippet) of the top-ranked result as the initial contextual source, as follows: the web information (i.e., Search(C), (2) where is the retrieved web information for the claim C, Search() denotes the function of Google Search. Subsequently, the target LLM processes the claim together with its decomposed sub-claims and the retrieved web information. Based on this input, the model extracts and summarizes the key factual information that can serve as evidence for claim verification: LLM(C, {c1, ..., ck}, w), (3) where means the output evidence. This step ensures that the evaluation leverages both the target models reasoning over decomposed sub-claims and its ability to identify salient facts from external sources, thereby constructing evidence to support or refute the original claim. c) Justification Production & Verdict Prediction: It targets yielding reasonable justification for fact-checking the original claim, ultimately leading to the predicted veracity label. Specifically, the target LLM executes the claim verification stage by processing the original claim, the sub-claims, and the retrieved evidence obtained in previous fact-checking stages, formulated as: {r, y} LLM(C, {c1, ..., ck}, e), (4) where and denote the generated justification and the predicted verdict, respectively, forming the final verification output. 4 Fig. 3. An illustration of arena-styled stage-wise judgment and arena-driven claim evolution. C. Arena-styled Stage-wise Judgment By deploying the target LLM to strictly follow the complete fact-checking workflow, we have first provided transparent platform for evaluating its capabilities at each stage of the process. key challenge is to establish comprehensive benchmark system that assesses model performance across all fact-checking stages, accommodates their open-form outputs, and seamlessly integrates stage-wise abilities into an overall evaluation of fact-checking competence. Therefore, we propose the arena-styled stage-wise judgment approach: We initiate peer battles between two anonymous models, following the Chatbot Arena style, where two models are randomly chosen from the model pool, to generate the stage-wise response (e.g., sub-claims, evidence, or verification output). Our goal is to fairly compare the overall quality of stage-wise responses, thereby providing transparent basis for determining which target model demonstrates superior performance across the full fact-checking pipeline. Specifically, on set of the complex claim data, we evaluate group of target LLMs {T1, ..., Tn}, with an evaluator panel consisting of judge agents as follows: FactArena(T1, ..., TnJ, C), (5) where the evaluator panel = {J1, . . . , Jm} consists of judge agents, and denotes the stage-wise evaluation outcomes in the form of ranking that reflects the relative capabilities of the (n > m) target LLMs in fact-checking complex claims. Any competitive evaluation requires clear reference guidelines to minimize biases arising from differences among committee members [44], [45]. To this end, we first establish evaluation guidelines for the open-form responses produced in the three stages of fact-checking: claim extraction, evidence retrieval, and claim verification. For claim extraction, fair evaluation of decomposed subclaims requires evaluators to reach holistic consensus on what constitutes an appropriate breakdown. Thus, the judge agents begin by consolidating the diverse sub-claims generated by different target models, synthesizing their strengths to construct the most impartial guideline. This unified reference provides consistent basis for judges to assess model performance at this stage. For each claim with sub-claim sets from target models, an evaluator panel integrates these different ways of claim decomposition to refine the evaluation guideline through multiple rounds. To mitigate bias, the panel is diversified by selecting strong judge agents from different model families, and judges are not allowed to evaluate their own responses. In the current rth round, judge examines the current guideline g(r) and randomly sampled set of sub-claims, then proposes an updated version: g(r+1) J(g(r) {c1, ..., ck}). (6) Judges and sub-claim sets are sampled in rotation until all candidate sub-claim sets are incorporated, ensuring fairness and coverage. The process initializes with randomly selected judge response and terminates once all viewpoints are integrated into consolidated guideline. For evidence retrieval, to define reliable reference information source, judge agents search Wikipedia using the entities mentioned in the complex claim, with the gold evidence from Wiki knowledge as the reference guideline for judgment. Judge agents then evaluate the evidence generated by target models against this guideline. In this way, model outputs are assessed by their ability to capture the essential factual content needed to support or refute the claim. Note that here the evidence guideline is not used as the golden label, but instead as the factual basis to further mitigate the potential factual errors inherent in the judge committee. For claim verification, verdict prediction can be directly evaluated as classification task; however, justification production requires additional guidelines to support arena-style the reasoning evaluation. Justifications focus on whether throughout the fact-checking pipeline is coherent, e.g., whether the model effectively leverages its extracted key information and provides sound explanation for its final verdict. Therefore, rather than attempting to consolidate all modelgenerated justifications into single comprehensive reference, inspired by previous automated judging benchmarks [15], [46], we establish evaluation guidelines [47] from the following perspectives: Helpfulness: The justification should align with the final verdict of the claim, ensuring consistency and avoiding misleading or contradictory reasoning. Informativeness: The justification should identify and include all salient facts and key points from the evidence that are essential for verifying the claim. Soundness: The justification should demonstrate logical is wellvalidity and coherence, with reasoning that structured and adequately supported by evidence. Readability: The justification should be clearly articulated, grammatically correct, and easy to follow, allowing evaluators to understand the reasoning without difficulty. Based on these guidelines, judge agents from the evaluator panel are employed in manner similar to the LLM-as-a-Judge paradigm [15], to determine and explain which target LLM produces the superior performance for the full fact-checking pipeline by scrutinizing their stage-wise outputs. After collecting pairwise judgments from the evaluator agents, we compute model rankings using the Elo rating system [48], which iteratively updates scores based on head-tohead outcomes. The Elo framework estimates the probability that an LLM will outperform another LLM b, given their current ratings Ra and Rb, where a, N. For each comparison, we define binary outcome variable Yab, which equals 1 if model is judged the winner and 0 otherwise. The predicted probability is then expressed as: (Yab = 1) = 1 1 + 10(RbRa)/α , (7) where α = 400 serves as the scaling constant in the Elo formula. In the standard Elo algorithm, model ratings are updated according to: = Ra + (S(a, b) (Yab = 1)), where is scaling constant and S(a, b) represents the observed outcome for LLM in its comparison with LLM (taking values 1 for win, 0.5 for draw, and 0 for loss). Although the Elo framework effectively models pairwise win probabilities, it performs sequential updates that make the resulting scores sensitive to the order of match-ups. To mitigate this limitation and achieve more stable rankings, we additionally employ the BradleyTerry model, which estimates relative strengths without relying on update order. The BradleyTerry algorithm [49] provides probabilistic framework for ranking, extending Elo-based evaluations by treating pairwise outcomes as logistic comparisons and estimating model strengths via maximum likelihood. For set of models with observed pairwise results, let Wab denote the number of times LLM defeats LLM b. The overall loglikelihood of the comparisons is then formulated as: L(R) = (cid:88) a=b Wab log (Yab = 1), (8) where = {R1, R2, . . . , Rn} are the model ratings. Because the BradleyTerry algorithm does not natively account for ties, we handle tie votes by splitting them evenly: each tie is recorded as half win for both models, incrementing both Wab and Wba by 0.5. This adjustment ensures fair and balanced estimation of model rankings across all pairwise comparisons. Finally, by sorting the estimated model ratings R, we obtain the ranking of the target LLMs, which serves as the ultimate evaluation outcome for the fact-checking pipeline. 5 TABLE THE STATISTICS OF CLAIMS AND BATTLE COUNT IN DIFFERENT STAGES OF FACTARENA FRAMEWORK, WHERE OC MEANS Original Claim AND EC MEANS Evolved Claim."
        },
        {
            "title": "Models",
            "content": "Claude Opus 4 Claude Sonnet 4 DeepSeek-R1 DeepSeek-V3 GPT-4.1 GPT-4.5 GPT-4o GPT-o3 GPT-o4 mini Gemini 2.5 Flash Gemini 2.5 Pro Grok-3 Grok-3 mini Llama 4 Maverick Qwen3 (235B) Qwen3 (32B) Claim Battle (OC) Battle (EC) Battle (Total) 213 216 214 208 241 240 210 224 237 245 231 215 219 221 234 200 1148 1006 1157 1136 1326 1315 1159 1269 1289 1325 1259 1196 1180 1221 1299 358 299 427 414 466 469 434 357 450 476 457 361 459 416 438 259 1506 1305 1584 1550 1792 1784 1593 1626 1739 1801 1716 1557 1639 1637 1737 1396 target model underperforms. While existing fact-checking datasets provide an intuitive yet superficial assessment of model performance, we argue that they are insufficient for uncovering the deeper limitations and knowledge boundaries of LLMs. This limitation stems from the inherent constraints of fixed test sets, which may also introduce risks of test leakage. To craft diverse and harder claims when the target models all predict the correct verdict, we propose iteratively evolving for more comprehensive fact-checking evaluation by drawing insights from the model behaviors of the arena-like auditing records as environmental feedback. Specifically, the target models predict the correct factuality of the claim C, we first employ the evolver agent to reverse the semantics of the original claim C, thereby generating contrastive claim ˆC with the opposite verdict. If the factuality of the contrastive claim is still correctly predicted by all target models, we then initiate an iterative targeted evolution process driven by the arena battle judgments, generating semantically equivalent but increasingly challenging variations of the claim. if all Note that each evolved claim is evaluated by judge agents against the target LLMs, with the results stored in judgment record pool. By iteratively evolving claims in this manner, we can identify test instances where models perform poorly under specific fact-checking scenarios, thereby yielding comprehensive insights into their fact-checking capacities across diverse complex claims. IV. EXPERIMENTS A. Experimental Settings D. Arena-driven Claim Evolution Analogous to the use of hard prompts in Chatbot Arena [46], introducing more challenging claims can further advance the evaluation of LLMs fact-checking abilities. central challenge, however, lies in identifying the specific areas where a) Dataset: We conduct experiments on complex claims selected from two publicly available fact-checking datasets, HOVER [29] and FEVEROUS [30], that require multi-hop reasoning beyond simple factual matching to verify in factchecking [3]. The detailed data statistics are shown in Table I. TABLE II MODEL PERFORMANCE RANKINGS ACROSS MULTIPLE EVALUATION DIMENSIONS. BATTLE COUNT INDICATES THE NUMBER OF PAIRWISE COMPARISONS EACH MODEL PARTICIPATED IN. ACC. (%) DENOTES THE ACCURACY PERCENTAGE. THE RANKING IS SORTED BASED ON OVERALL JUDGE."
        },
        {
            "title": "Target mLLMs",
            "content": "GPT-o3 (-) DeepSeek-R1 (671B) GPT-o4 mini (-) Gemini 2.5 Pro (-) GPT-4.5 (-) GPT-4.1 (-) Grok-3 (-) Grok-3 mini (-) Gemini 2.5 Flash (-) Claude Opus 4 (-) DeepSeek-V3 (671B) Llama 4 Maverick (400B) Claude Sonnet 4 (-) Qwen3 (235B) GPT-4o (-) Qwen3 (32B)"
        },
        {
            "title": "Claim",
            "content": "Justification -Helpfulness Justification -Informativeness Justification -Soundness Justification Overall Judge -Readability Acc. (%) 1626 1584 1739 1716 1784 1792 1557 1639 1801 1506 1550 1637 1305 1737 1593 1188.11 1096.22 1081.00 1046.19 1050.66 1018.24 995.75 979.61 969.48 973.20 968.84 919.58 940.14 936.05 937.60 899.33 1351.95 1129.69 1129.67 1066.15 1041.89 1033.21 1007.77 960.10 955.35 949.44 924.27 916.68 908.29 892.18 888.55 844.80 1294.67 1141.88 1094.46 1085.29 1052.08 1044.00 1027.00 965.29 962.62 953.03 937.19 922.33 905.29 893.44 888.45 832.96 1331.77 1160.02 1101.49 1078.02 1039.95 1032.77 1026.11 966.40 945.27 961.30 923.45 926.36 930.52 882.68 872.55 821.33 1292.55 1131.83 1097.33 1081.98 1053.24 1044.03 1026.05 958.97 956.76 958.32 938.83 916.31 919.29 891.82 891.96 840.73 1268.83 1117.53 1097.51 1077.44 1041.03 1029.11 1019.52 974.06 980.50 933.39 951.61 909.13 892.76 922.65 913.03 871. 1320.14 65.02 1127.77 55.14 1108.47 64.14 1084.19 66.52 1047.15 59.41 1041.71 64.32 1022.09 55.14 52.75 960.74 55.51 955.60 44.34 949.27 55.77 934.38 60.91 919.57 34.72 905.28 55.13 890.89 50.95 890.75 51.76 841.99 b) Models: We comprehensively evaluate total of 16 LLMs across 7 distinct model families, covering broad range of architectures and scales. Specifically, the models include: 1) Claude 4 Opus, Claude 4 Sonnet; 2) DeepSeek-R1, DeepSeekV3; 3) GPT-4.5, GPT-4.1, GPT-o4 mini, GPT-o3, GPT-4o; 4) Gemini 2.5 Flash, Gemini 2.5 Pro; 5) Grok-3, Grok-3 mini; 6) Llama 4 Maverick; 7) Qwen3 (235B, 32B), from which the strong judge agents, DeepSeek-V3, GPT-o4 mini, Gemini 2.5 Flash, and Qwen3 (235B), are selected after balancing their performance and computational cost for the evaluator panel. B. Implementation Details a) Model Details: In FactArena, we conduct evaluation on total of 16 LLMs across 7 distinct model families with the following representative versions: 1) Claude Opus 4: claude-opus-4-20250514; 2) Claude Sonnet 4: claude-sonnet4-20250514; 3) DeepSeek-R1: deepseek-r1; 4) DeepSeek-V3: deepseek-v3; 5) Gemini 2.5 Flash: gemini-2.5-flash; 6) Gemini 2.5 Pro: gemini-2.5-pro-preview-06-05; 7) GPT-4.1: gpt-4.12025-04-14; 8) GPT-4.5: gpt-4.5-preview-2025-02-27; 9) GPT4o: gpt4o; 10) GPT-o3: o3-2025-04-16; 11) GPT-o4 mini: o4mini-2025-04-16; 12) Grok-3: grok-3; 13) Grok-3 mini: xai/grok-3-mini-beta; 14) Llama 4 Maverick: llama-4-maverick17b-128e-instruct; 15) Qwen3 (235B): qwen3-235b-a22b; 16) Qwen3 (32B): qwen3-32b; In arena-driven claim evolution, we utilize GPT-o4 mini, one of the dominant mLLMs, as the agent controller to reverse the semantics and create harder claims. We set the temperature parameter of all the target models and judges as 0.0 to guarantee reproducibility of our evaluation results as much as possible. b) Data Statistics: We randomly selected total of 400 complex claims, each 200 from the datasets mentioned in IV-A. For each claim we sampled 8 target model responses per task for pairwise comparisons to ensure diversity of samples in model comparisons as well as to maintain controllable number of total battles, following the combinatorial coverage theory [50]. In the selected 400 claims, 85 are correctly predicted by all the selected target models, which are then semantically reversed and evolved. The final arena-styled judgment results in about 13,000 valid judgments, with each target LLM participating in approximately 1,600 comparisons, roughly 104 times for each model pair on average. Compared results are averaged over three random 3 runs. The cost of API for evaluating one target model is about 15 dollars and 3 hours. C. Main Results Table II presents the main experimental results across the three stages of the full fact-checking pipeline, as well as the overall performance determined jointly by the votes of judge agents in the evaluator panel. From the results, we observe that: 1) GPT-o3 and DeepSeek-R1 achieve the best overall performance, ranking first and second, respectively, in terms of Elo scores, consistently outperforming other models across all stages. 2) The target LLMs generally show consistent performance across the three stages of the full fact-checking pipeline, except for Llama 4 Maverick and Gemini 2.5 Flash, whose results exhibit slight instability in the claim extraction stage. This indicates that despite variations in reasoning paths, these models demonstrate degree of robustness, eventually converging into reasonable verifications through the complete reasoning process. 3) The stage-wise benchmarking for factchecking can reveal distinct strengths and weaknesses among the models. For example, Gemini 2.5 Pro is better than GPT4.5 in both claim extraction and claim verification, but weaker in evidence retrieval. 4) The accuracy metric reflects the performance as captured by traditional benchmarking methods. However, our results beyond the accuracy-only performance show that relying solely on such metric provides an incomplete view of fact-checking capability. In contrast, our proposed benchmarking framework offers more diverse and nuanced view by evaluating LLMs across open-form stages of 7 Fig. 4. An illustration of the Elo rankings under FactArena and w/o guideline settings. The order of target mLLMs is the ranking of our main result in Table II. TABLE III THE QUANTITATIVE RESULT OF THE JUDGES IN FactArena AND w/o guideline SETTINGS. THE INTER-JUDGE CONSISTENCY SCORES ARE INDICATED BY THE ACCURACY(%) OF DIFFERENT JUDGE RESULTS COMPARED TO MAJORITY VOTE RESULTS OF THE JUDGE COMMITTEE."
        },
        {
            "title": "FactArena",
            "content": "w/o guideline DeepSeek GPT-o4 Gemini 2.5 Qwen3 (235B)"
        },
        {
            "title": "Flash",
            "content": "mini -V"
        },
        {
            "title": "Claim Extraction\nEvidence Retrieval\nJustification Production\nOverall Pipeline",
            "content": "92.97 93.23 94.12 93.60 94.54 91.65 92.12 93.66 91.14 83.86 85.45 87.03 91.92 94.08 95.47 95.73 Avg. 92.64 90.71 91.78 92. DeepSeek GPT-o4 Gemini 2.5 Qwen3 (235B)"
        },
        {
            "title": "Flash",
            "content": "mini -V3 93.33 94.00 94.06 95.25 89.42 91.81 92.23 92.36 81.96 85.60 86.27 87.44 93.18 92.13 91.56 92. Avg. 89.47 90.88 91.03 91.98 the classical fact-checking pipeline, thereby enabling deeper scrutiny of their reasoning processes and robustness. 5) It can be further reflected in model-specific behavior. For example, although Gemini 2.5 Pro achieves the highest accuracy among all models, its advantage diminishes when evaluated across the full fact-checking pipeline. Similar inconsistencies are observed for Llama 4 Maverick and GPT-4.1, suggesting that accuracy on claim verification alone is insufficient to reflect models overall fact-checking capability. Our proposed comprehensive and trustworthy auditing of the entire fact-checking workflow provides more informative benchmarking perspective to supplement the evaluation, revealing strengths and weaknesses that static verification accuracy fails to capture. D. Analysis of Judgment Reliability with Guideline To qualitatively analyze the judgment reliability, we first conduct the ablative study to compare the FactArena, and its w/o guideline setting where the judge agents directly compare the analyses of target LLMs without using any reference guidelines. Figure 4 illustrates the Elo rankings under FactArena and w/o guideline settings. We can observe that: 1) Overall, FactArena yields more robust and well-stratified ranking structure, indicating that the use of reference guidelines helps unify judge decisions and produces more unbiased assessment outcomes. 2) Gemini 2.5 Flash deviates substantially from other judges, especially when evaluating lower-ranked models, suggesting systematic differences in judgment criteria. 3) Furthermore, Gemini 2.5 Flash and DeepSeek-V3 display notable preference patterns under the w/o guideline setting, tending to assign higher rankings to models from the same family. This bias diminishes when guidelines are applied, reinforcing the necessity of standardized evaluation criteria for fair crossmodel comparisons. To further quantify inter-judge consistency for the analysis of the judgment reliability, we compute the accuracy of each judges decision relative to the majority-vote outcome, and report the average accuracy across judge agents. higher average accuracy indicates stronger agreement among judges, and thus less idiosyncratic bias among LLM judges in evaluation. As shown in Table III, incorporating our evaluation guidelines generally leads to more aligned judgments across different fact-checking stages. In particular, the claim extraction stage and the justification production stage exhibit notable improvements in average accuracy when guidelines are applied, suggesting that guidelines effectively align the judge committee in assessing model responses of different fact-checking stages. Although the average accuracy for the judgment in the evidence retrieval stage remains relatively unchanged when with the reference guideline, the designed reference guidelines generally help reduce individual subjective bias in evaluators decisions across the overall pipeline. These results demonstrate that the proposed guidelines contribute to more stable and coherent judgments, reinforcing the reliability of the evaluation framework. To further assess the reliability of our guideline-based evaluation, we conduct human subject study in which human annotators replace the judge agents. Five human experts (aged 2428) independently annotate randomly sampled set of 100 battle pairs, following the same FactArena evaluation protocol as the judge agents. TABLE IV THE CONSISTENCY BETWEEN JUDGE AGENTS AND HUMAN EVALUATORS UNDER DIFFERENT SETTINGS. 8 FactArena w/o guideline LLM-as-a-judge DeepSeek GPT-o4 Gemini 2.5 Qwen3 (235B) 0.69 0.66 0.63 Flash 0.72 0.67 0.69 mini 0.85 0.74 0.69 -V3 0.67 0.74 0.70 Joint Voting 0.75 0.69 0.66 TABLE THE INTER-JUDGE CONSISTENCY(%) UNDER DIFFERENT SETTINGS. FactArena w/o guideline LLM-as-a-judge DeepSeek GPT-o4 Gemini 2.5 Qwen3 (235B) 95.73 92.87 93.73 Flash 87.03 87.44 91.88 -V3 93.60 95.25 88.35 mini 93.66 92.36 95.61 Avg. 92.51 91.98 92. Fig. 5. Elo ratings of target LLMs before and after claim evolution. Specifically, as shown in Table IV, the annotators, with an inter-annotator agreement of 0.632, and an intra-annotator agreement of 0.726, need to evaluate the fact-checking process as the judge agents. The consistency is indicated by the accuracy of the judge agents using human annotations as the gold standard. We compare the FactArena with the following two settings: 1) w/o guideline: The judge agents directly compare the analyses of target mLLMs without using any references; 2) LLM-as-a-judge: The judge agents use the analyses generated by themselves as references for judgments. We can see that the joint voting of the FactArena judge committee has the best consistency with the human annotations. The inter-judge consistency under the three ablative settings in Table also validates the advantage of our proposed FactArena. The resulting alignment indicates that the guideline-driven judging procedure provides more consistent and human-aligned evaluations of the fact-checking workflow. E. Analysis of Arena-driven Claim Evolution To verify the effectiveness of the arena-driven claim evolution, we conduct ablative studies in the following settings: a) FactArena: the benchmark framework with arena-driven claim evolution; b) w/o evolution: simply reverse the trivial claim whose factuality is correctly predicted by all target models, but without further data evolution; c) w/o reverse: directly removing the first reverse step and no further evolution step. Figure 5 provides an intuitive illustration of the Elo ratings of target models under our ablation settings. We observe that the ratings of DeepSeek-R1, Gemini 2.5 Flash, Qwen 3 (235B), and Qwen 3 (32B) increase noticeably after claim evolution, indicating that these models are more robust and Fig. 6. Effect of arena-driven claim evolution rounds. generalizable when confronted with adaptively generated challenging claims. Conversely, models whose ratings drop under evolution exhibit stronger vulnerability. In contrast, decrease under the FactArena setting may also suggest that there could be potential data leakage under the w/o evolution and w/o reverse settings, models perform well on static benchmarks but fail once claims are transformed into more novel and unseen forms. Since our claim-evolution procedure continuously adapts claims toward harder and out-of-distribution variations, models that sustain or improve their scores under this setting are likely to possess more robust and broadly generalizable fact-checking capabilities. those correctly predicted by all We further analyze the effect of the arena-driven claim evolution rounds, as illustrated in Figure 6, where the Reverse means the Round 0 of the claim evolution. It can be observed that simple claims, target models (100% accuracy), experience substantial drop in average prediction accuracy to 68% after semantic reversal, indicating that the reversed claims become significantly more challenging. As the evolution rounds progress, the average accuracy continues to decrease, though the rate of decline gradually slows. The battle-count statistics further corroborate this trend: the number of simple claims decreases steadily across rounds and the battle count stabilizes at around 220 by rounds 23, suggesting that the evolution process converges as fewer claims remain trivially solvable by all the target models. F. Qualitative Comparison with Traditional Benchmark To guarantee the reliability of the benchmarking results from FactArena, we design blind test that compares FactArena against traditional benchmarking like [13] that focuses only on the claim verification stage. We randomly sample 100 battles, and five human experts are asked to select the better judgment of the same judge agent under the two evaluation designs based on the following criteria: 1) Helpfulness: whether the judges explanation about the battle results aligns with the true veracity label of the claim. Human experts are asked to select the response that is more convincing and less misleading; 2) Informativeness: whether the judges explanation provides additional useful information, such as background details or contextual knowledge; 3) Soundness: whether the judges explanation appears valid, well-reasoned, and logically coherent; 4) Readability: whether the judges explanation follows proper grammar and structure, and whether the sentences are clear and easy to follow. Note that here we use the same guidelines for the human experts as those for the judge agents in the claim verification stage, but the difference lies in that here we need to evaluate the judge agents themselves. As shown in Figure 7, judge 9 TABLE VII THE EFFECT OF DIFFERENT NUMBERS OF JUDGE AGENTS. 1 Judge 2 Judges 3 Judges 4 Judges Accuracy 0.67 0.65 0.69 0.75 Fig. 7. Human preference of judgments under FactArena and the traditional benchmarking paradigm. TABLE VI THE CONSISTENCY BETWEEN JUDGES AND THEIR STRONGER VERSIONS FROM THE SAME MODEL FAMILY. Model Pair DeepSeek-V3 DeepSeek-R1 GPT-o4 mini GPT-o3 Gemini 2.5 Flash Gemini 2.5 Pro Avg. Consistency 0.92 0.74 0.72 0.79 responses produced under the FactArena framework exhibit significant advantage in Informativeness, indicating that fullpipeline fact-checking naturally incorporates richer information and enables more comprehensive assessment of model behavior. Moreover, responses from FactArena judges are not only more sound but also more convincing and less misleading compared to those from the traditional benchmarking paradigm. G. Discussion of Judge Model Version In deploying the judge agents for FactArena, we deliberately selected models that balance evaluation quality and computational cost, rather than exclusively relying on the strongest available LLMs. To assess whether this choice affects evaluation reliability, we compare each judge agent against stronger model from the same family. Table VI presents the consistency between our chosen judges and their more capable counterparts. The results show that the judgments produced by the selected judge agents remain highly consistent with those of stronger models, with average agreement approaching 80%, and in some cases, such as DeepSeek-V3, exceeding 90%. These findings indicate that the evaluation quality is not significantly compromised by using computationally lighter judge agents, and that their decisions remain well aligned with both human assessments and stronger LLMs within the same model family. Fig. 8. The Completeness and Correctness scores of the evaluation guideline for claim extraction during multiple rounds iterations. as follows: 1) 1 Judge: Deepseek-V3; 2) 2 Judges: DeepseekV3 and GPT-4o; 3) 3 Judges: Deepseek-V3, GPT-4o and Gemini 2.5 Flash; 4) 4 Judges: the 4 judge agents in our main experiment. Compared with configurations using fewer judges, the 4judge setting in FactArena achieves the highest accuracy. We also observe that Deepseek-V3 alone reaches an accuracy of 0.67 as single judge, but when GPT-4o is added to the evaluator panel, the combined accuracy slightly decreases, indicating divergence in judge opinions. However, as the number of judges continues to grow, the panels overall accuracy increases, suggesting that aggregating more judge agents leads to decisions that better reflect human preferences. While involving larger set of judges improves alignment, greater computational and resource costs are inevitable. From cost effective standpoint, and consistent with the principle of Occams razor, the 4-judge configuration offers strong balance between evaluation quality and efficiency, making it practical choice. I. Analysis of Claim Extraction To examine how the quality of the claim extraction guideline evolves across different rounds of integration, we conduct detailed human evaluation. We recruit five human experts aged 2428 and randomly sample 20 claims for assessment at each round (the average intra-annotator agreement is 0.618). Following [51], we evaluate guideline quality using two task-decomposition metrics: (1) Completeness: whether the sub-claims collectively cover all essential information in the original claim; (2) Correctness: whether each sub-claim faithfully represents specific part of the original claim without distortion or unnecessary additions. As shown in Figure 8, we can see that the results show clear trend of an increase in guideline quality from rounds 0 to 3. After the third round, the improvement rate slows, and by rounds 56, the scores converge, indicating that additional refinement brings diminishing returns. H. Discussion of Judge Agent Numbers To further explore the effect of the judge agents in the evaluator panel, we conduct evaluations on the number of judges. As shown in Table VII, we compare the consistency of joint agent judgments with human evaluators. The settings are J. Case Study To probe deeper into the knowledge boundaries of target models, FactArena employs an arena-driven claim evolution procedure that progressively transforms simple claims into Fig. 9. An example of arena-driven claim evolution. Both Model and Model correctly predict the original and reversed claims. After model-specific weaknesses are analyzed and more challenging evolved claim is derived, clear performance distinctions emerge between the target models. more challenging ones. As shown in Figure 9, the original claim is straightforward fact-verification question that is correctly answered by all tested target models. After reversing the claims factuality, both Model and Model still respond correctly. The judge agent, therefore, considers both models to be strong and makes tie judgment. However, during the analysis of model weaknesses, the evolver agent observes that Model As reasoning may lack structured reasoning, while Model tends to include unnecessary details and might potentially leading to information overload. Motivated by these observations, the evolved claim rewrites and expands the original statement, reformulating the claim from simple knowledge verification problem into more logically complex statement. When evaluated on the evolved claim, Model correctly identifies that the seventh sub-claim is unsupported, whereas Model Bs analysis contains substantial redundant information and misses the false relation in the claim. This example demonstrates that in arena-driven evolution, the claim difficulty is selectively intensified, which effectively amplifies performance distinctions between models and thus enables more precise assessment of target models true understanding of factual knowledge. V. CONCLUSION AND FUTURE WORK In this work, we introduced FactArena, fully automated, arena-style benchmarking framework that systematically evaluates LLMs across the entire fact-checking pipeline. By integrating stage-wise auditing, guideline-driven judgment, and arena-based claim evolution, FactArena provides more comprehensive and trustworthy assessment of LLMs factual reasoning capabilities than traditional verification-only evaluation. Extensive experiments over 16 state-of-the-art models show that FactArena produces robust and fair rankings, substantially improves inter-judge consistency, and reveals performance discrepancies that static accuracy metrics fail to capture. Moreover, the adaptive claim-evolution mechanism effectively probes model robustness and uncovers deeper reasoning limitations that remain hidden under fixed test sets. Future work can further extend FactArena along several directions. First, although our framework focuses on textual fact-checking, incorporating multimodal claims [52], [53] and richer retrieval sources (e.g., structured databases, temporal information) may broaden its applicability. Second, enhancing judge-agent diversity and integrating uncertainty-aware evaluation may provide finer-grained diagnostic signals for model reliability. Finally, deploying FactArena in real-world factchecking scenarios, where complex claims are noisier, adversarial, or rapidly evolving, opens opportunities for studying model behavior under more realistic and dynamic conditions. We hope this framework serves as foundation for building rigorous, scalable, and human-aligned evaluations of future LLMs in safety-critical factual reasoning tasks."
        },
        {
            "title": "REFERENCES",
            "content": "[1] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al., Llama: Open and efficient foundation language models, arXiv preprint arXiv:2302.13971, 2023. [2] OpenAI, 2023. 257532815 Gpt-4 technical abs/2303.08774, [Online]. Available: https://api.semanticscholar.org/CorpusID: report, ArXiv, vol. [3] L. Pan, X. Wu, X. Lu, A. T. Luu, W. Y. Wang, M.-Y. Kan, and P. Nakov, Fact-checking complex claims with program-guided reasoning, in Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2023, pp. 6981 7004. [4] H. Wang and K. Shu, Explainable claim verification via knowledgegrounded reasoning with large language models, in Findings of the Association for Computational Linguistics: EMNLP 2023, 2023, pp. 62886304. [5] X. Zhao, L. Wang, Z. Wang, H. Cheng, R. Zhang, and K.-F. Wong, Pacar: Automated fact-checking with planning and customized action reasoning using large language models, in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 12 564 12 573. [6] S. Lin, J. Hilton, and O. Evans, Teaching models to express their uncertainty in words, arXiv preprint arXiv:2205.14334, 2022. [7] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., Sparks of artificial intelligence: Early experiments with gpt-4, arXiv preprint general arXiv:2303.12712, 2023. [8] Y. Elazar, N. Kassner, S. Ravfogel, A. Ravichander, E. Hovy, H. Schutze, and Y. Goldberg, Measuring and improving consistency in pretrained language models, Transactions of the Association for Computational Linguistics, vol. 9, pp. 10121031, 2021. [9] B. Cao, H. Lin, X. Han, L. Sun, L. Yan, M. Liao, T. Xue, and J. Xu, Knowledgeable or educated guess? revisiting language models as knowledge bases, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 2021, pp. 18601874. [10] H. Lin, L. Chen, J. Ma, Z. Yang, and G. Chen, Amif: hybrid model for improving fact checking in product question answering, in 2022 International Joint Conference on Neural Networks (IJCNN). IEEE, 2022, pp. 18. [11] Z. Guo, M. Schlichtkrull, and A. Vlachos, survey on automated fact-checking, Transactions of the Association for Computational Linguistics, vol. 10, pp. 178206, 2022. [12] X. Hu, J. Chen, X. Li, Y. Guo, L. Wen, S. Y. Philip, and Z. Guo, Do large language models know about facts? in The Twelfth International Conference on Learning Representations, 2024. [13] H. Lin, Y. Deng, Y. Gu, W. Zhang, J. Ma, S.-K. Ng, and T.-S. Chua, Fact-audit: An adaptive multi-agent framework for dynamic factchecking evaluation of large language models, in Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics, 2025. [14] W.-L. Chiang, L. Zheng, Y. Sheng, A. N. Angelopoulos, T. Li, D. Li, B. Zhu, H. Zhang, M. Jordan, J. E. Gonzalez et al., Chatbot arena: An open platform for evaluating llms by human preference, in Forty-first International Conference on Machine Learning, 2024. [15] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing et al., Judging llm-as-a-judge with mt-bench and chatbot arena, in Proceedings of the 37th International Conference on Neural Information Processing Systems, 2023, pp. 46 59546 623. [16] J. M. Leimeister, Collective intelligence, Business & Information Systems Engineering, vol. 2, pp. 245248, 2010. [17] J. S. Park, J. OBrien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein, Generative agents: Interactive simulacra of human behavior, in Proceedings of the 36th annual acm symposium on user interface software and technology, 2023, pp. 122. [18] J. Gu, X. Jiang, Z. Shi, H. Tan, X. Zhai, C. Xu, W. Li, Y. Shen, S. Ma, H. Liu et al., survey on llm-as-a-judge, arXiv preprint arXiv:2411.15594, 2024. [19] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, Fever: large-scale dataset for fact extraction and verification, in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 809819. [20] A. Sathe, S. Ather, T. M. Le, N. Perry, and J. Park, Automated factchecking of claims from wikipedia, in Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020, pp. 68746882. [21] T. Schuster, A. Fisch, and R. Barzilay, Get your vitamin c! robust fact verification with contrastive evidence, in Proceedings of the 2021 Conference of the Association for Computational Linguistics: Human Language Technologies, 2021, pp. 624643. the North American Chapter of [22] A. Tariq, A. Karim, and H. Foroosh, Nelasso: group-sparse modeling for characterizing relations among named entities in news articles, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 39, no. 10, pp. 20002014, 2016. [23] C. Buntain and J. Golbeck, Automatically identifying fake news in popular twitter threads, in 2017 IEEE international conference on smart cloud (smartCloud). IEEE, 2017, pp. 208215. [24] K. Shu, D. Mahudeswaran, S. Wang, D. Lee, and H. Liu, Fakenewsnet: data repository with news content, social context, and spatiotemporal information for studying fake news on social media, Big data, vol. 8, no. 3, pp. 171188, 2020. [25] P. Nakov, A. Barron-Cedeno, G. Da San Martino, F. Alam, J. M. Struß, T. Mandl, R. Mıguez, T. Caselli, M. Kutlu, W. Zaghouani et al., The 11 clef-2022 checkthat! lab on fighting the covid-19 infodemic and fake news detection, in European Conference on Information Retrieval. Springer, 2022, pp. 416428. [26] J. Ma, W. Gao, Z. Wei, Y. Lu, and K.-F. Wong, Detect rumors using time series of social context information on microblogging websites, in Proceedings of the 24th ACM international on conference on information and knowledge management, 2015, pp. 17511754. [27] J. Ma, W. Gao, and K.-F. Wong, Detect rumors in microblog posts using propagation structure via kernel learning, in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017, pp. 708717. [28] H. Lin, J. Ma, M. Cheng, Z. Yang, L. Chen, and G. Chen, Rumor detection on twitter with claim-guided hierarchical graph attention networks, in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 2021, pp. 10 03510 047. [29] Y. Jiang, S. Bordia, Z. Zhong, C. Dognin, M. Singh, and M. Bansal, Hover: dataset for many-hop fact extraction and claim verification, in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020, pp. 34413460. [30] R. Aly, Z. Guo, M. S. Schlichtkrull, J. Thorne, A. Vlachos, C. Christodoulopoulos, O. Cocarascu, and A. Mittal, Feverous: Fact extraction and verification over unstructured and structured information, in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. [31] A. Gupta and V. Srikumar, X-fact: new benchmark dataset for multilingual fact checking, in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), 2021, pp. 675682. [32] D. Wadden, K. Lo, B. Kuehl, A. Cohan, I. Beltagy, L. L. Wang, and H. Hajishirzi, Scifact-open: Towards open-domain scientific claim verification, in Findings of the Association for Computational Linguistics: EMNLP 2022, 2022, pp. 47194734. [33] H. Lin, J. Ma, L. Chen, Z. Yang, M. Cheng, and C. Guang, Detect rumors in microblog posts for low-resource domains via adversarial contrastive learning, in Findings of the Association for Computational Linguistics: NAACL 2022, 2022, pp. 25432556. [34] H. Lin, P. Yi, J. Ma, H. Jiang, Z. Luo, S. Shi, and R. Liu, Zeroshot rumor detection with propagation structure via prompt learning, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 4, 2023, pp. 52135221. [35] C. Chen and K. Shu, Can llm-generated misinformation be detected? in The Twelfth International Conference on Learning Representations, 2024. [36] P. Atanasova, J. G. Simonsen, C. Lioma, and I. Augenstein, Generating fact checking explanations, in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 2020, pp. 73527364. [37] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang, J. Wang, Z. Wang, S. K. S. Yau, Z. Lin et al., Metagpt: Meta programming for multi-agent collaborative framework, in ICLR, 2024. [38] C. Qian, W. Liu, H. Liu, N. Chen et al., Chatdev: Communicative agents for software development, in ACL, 2024, pp. 15 17415 186. [39] W. Chen, Y. Su, J. Zuo et al., Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors, in ICLR, 2024. [40] K.-T. Tran, D. Dao, M.-D. Nguyen, Q.-V. Pham et al., Multi-agent collaboration mechanisms: survey of llms, CoRR, vol. abs/2501.06322, 2025. [41] G. Li, H. A. Al Kader Hammoud, H. Itani, D. Khizbullin, and B. Ghanem, Camel: communicative agents for mind exploration of large language model society, in NeurIPS, 2023, pp. 51 99152 008. [42] S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, Gorilla: large language model connected with massive apis, in NeurIPS, 2024, pp. 126 544126 565. [43] P. Xu, W. Shao, K. Zhang, P. Gao, S. Liu, M. Lei, F. Meng, S. Huang, Y. Qiao, and P. Luo, Lvlm-ehub: comprehensive evaluation benchmark for large vision-language models, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [44] X. F. Zhang and M.-C. de Marneffe, Identifying inherent disagreement in natural language inference, in NAACL-HLT, 2021, pp. 49084915. [45] S. Gehrmann, A. Bhattacharjee, A. Mahendiran, A. Wang, A. Papangelis, A. Madaan, A. McMillan-Major, A. Shvets, A. Upadhyay, B. Bohnet et al., Gemv2: Multilingual nlg benchmarking in single line of code, in EMNLP: System Demonstrations, 2022, pp. 266281. [46] T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica, From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline, arXiv preprint arXiv:2406.11939, 2024. 12 [47] B. Wang, J. Ma, H. Lin, Z. Yang, R. Yang, Y. Tian, and Y. Chang, Explainable fake news detection with large language model via defense among competing wisdom, in Proceedings of the ACM on Web Conference 2024, 2024, pp. 24522463. [48] A. E. Elo, The USCF rating system: Its development, applications. United States Chess Federation, 1966. theory, and [49] R. A. Bradley and M. E. Terry, Rank analysis of incomplete block designs: I. the method of paired comparisons, Biometrika, 1952. [50] D. R. Kuhn, I. D. Mendoza, R. N. Kacker, and Y. Lei, Combinatorial coverage measurement concepts and applications, in 2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops. IEEE, 2013, pp. 352361. [51] R. Kamoi, T. Goyal, J. D. Rodriguez, and G. Durrett, Wice: Realworld entailment for claims in wikipedia, 2023. [Online]. Available: https://arxiv.org/abs/2303.01432 [52] R. Shao, T. Wu, J. Wu, L. Nie, and Z. Liu, Detecting and grounding multi-modal media manipulation and beyond, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 46, no. 8, pp. 5556 5574, 2024. [53] S. Wang, H. Lin, Z. Luo, Z. Ye, G. Chen, and J. Ma, Mfc-bench: Benchmarking multimodal fact-checking with large vision-language models, in ICLR 2025 Workshop on Building Trust in Language Models and Applications, 2025. 13 Fig. 12. The prompt for justification production and verdict prediction. d) Evolution Prompt: Figure 13 and Figure 14 show the prompts for reversing the semantics and generating more challenging claims. When reversing the claims semantics, the evolver agent reformulates the claim into logical and verifiable alternative that has an opposite veracity label, avoiding making superficial changes or trivial modifications. In the arena-driven claim evolution process, the evolver agent analyzes the specific weaknesses of the target models and subsequently rephrases the claim into more challenging version designed to probe these weaknesses. e) Claim Extraction Guideline Consolidation Prompt: The prompt for consolidating the claim extraction guideline is shown in Figure 15. To mitigate potential bias, the judge agents are blinded to the sources of the sub-claims, and their order of model answers is randomly shuffled. judgment f) Arena-styled Stage-wise Judgment Prompt: The prompt for agent is provided in Figure 16. To avoid potential positional bias, the order of model answers is randomly given. The judge gives its judgment by comparing the performance of the target models at each stage of claim verification. Fig. 10. The prompt for conducting claim extraction. Fig. 11. The prompt for constructing evidence for claim verification."
        },
        {
            "title": "We provide the curated prompt templates in our FactArena",
            "content": "framework here. a) Claim Extraction Prompt: The prompt used for claim extraction is provided in Figure 10. The target models are instructed to break down the claims into sub-claims of proper granularity. b) Evidence Retrieval Prompt: The prompt used for evidence retrieval is provided in Figure 11. In evidence retrieval, the target models generate list of factual evidence based on the claim, the sub-claims, and the web information retrieved using the external tool. c) Justification Production & Verdict Prediction Prompt: The prompt used to generate justification and verdict prediction is shown in Figure 12. Based on the previously generated sub-claims and evidence, the target model is instructed to justify and give final verdict of the claim. APPENDIX DETAILED CASES In this section, we provide more case studies regarding each specific procedure in the FactArena framework. a) Case of Fact-checking Pipeline: The three-stage fact-checking pipeline of FactArena is illustrated in Figure 17, consisting of claim extraction, evidence retrieval, and claim verification. In the claim extraction stage, the target model decomposes the original claim into set of sub-claims. Based on these sub-claims, the model then retrieves relevant web information via external tools and organizes it into supporting evidence. Finally, during claim verification, the model integrates the original claim with the extracted sub-claims and retrieved evidence to produce final veracity verdict. 14 Fig. 14. The prompt for arena-driven claim evolution. Fig. 13. The prompt for reversing the semantics of claims. b) Case of Guideline Consolidation: Figure 18 demonstrates an example of synthesizing the claim extraction evaluation guideline. As shown in the case, the refined guideline preserves the decomposition strategy of the current reference guideline, while incorporating the expression style in the model answers. The synthesized decomposition results in more coherent and practically verifiable reference guideline for claim extraction. c) More Cases of Claim Evolution: We further present additional cases of claim evolution in Figure 19, illustrating the original claim, the semantically reversed claim, and the claims in two rounds of arena-driven evolution. As the evolution progresses, the claim semantics become increasingly complex, making the verification task more challenging. d) Detailed Case of Arena-driven Claim Evolution: We show the more detailed case of the arena-driven claim evolution in our benchmarking process in Figure 20 and Figure 21. Fig. 15. The prompt for consolidating the sub-claims into the evaluation guideline for claim extraction. 15 Fig. 17. case of FactArena fact-checking pipeline. Fig. 16. The prompt for arena-styled stage-wise judgment. 16 Fig. 18. case of guideline consolidation in constructing the evaluation guideline of claim extraction. Fig. 19. More cases of claim evolution. Fig. 20. detailed example of arena-driven claim evolution in reversing the claims semantics. 17 Fig. 21. detailed example of arena-driven claim evolution in generating more challenging claim."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Hong Kong Baptist University",
        "National University of Singapore",
        "Salesforce AI Research",
        "The Education University of Hong Kong",
        "The Hong Kong University of Science and Technology"
    ]
}