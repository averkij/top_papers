{
    "paper_title": "LLM2CLIP: Powerful Language Model Unlocks Richer Visual Representation",
    "authors": [
        "Weiquan Huang",
        "Aoqi Wu",
        "Yifan Yang",
        "Xufang Luo",
        "Yuqing Yang",
        "Liang Hu",
        "Qi Dai",
        "Xiyang Dai",
        "Dongdong Chen",
        "Chong Luo",
        "Lili Qiu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "CLIP is one of the most important multimodal foundational models today. What powers CLIP's capabilities? The rich supervision signals provided by natural language, the carrier of human knowledge, shape a powerful cross-modal representation space. However, with the rapid advancements in large language models LLMs like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs' strong textual understanding can fundamentally improve CLIP's ability to handle image captions, drastically enhancing its ability to process long and complex texts, a well-known limitation of vanilla CLIP. Moreover, LLMs are trained on a vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this paper, we propose LLM2CLIP, a novel approach that embraces the power of LLMs to unlock CLIP's potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layer's textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as a powerful teacher for CLIP's visual encoder. Thanks to the LLM's presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP's text encoder's context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks."
        },
        {
            "title": "Start",
            "content": "Preprint LLM2CLIP: POWERFUL LANGUAGE MODEL UNLOCKS RICHER VISUAL REPRESENTATION Weiquan Huang1, Aoqi Wu1, Yifan Yang2, Xufang Luo2, Yuqing Yang2, Liang Hu1, Qi Dai2, Xiyang Dai2, Dongdong Chen2, Chong Luo2, Lili Qiu2 1Tongji University Code & Models: https://aka.ms/llm2clip 2Microsoft Corporation"
        },
        {
            "title": "ABSTRACT",
            "content": "CLIP is one of the most important multimodal foundational models today, aligning visual and textual signals into shared feature space using simple contrastive learning loss on large-scale image-text pairs. What powers CLIPs capabilities? The rich supervision signals provided by natural language the carrier of human knowledge shape powerful cross-modal representation space. As result, CLIP supports variety of tasks, including zero-shot classification, detection, segmentation, and cross-modal retrieval, significantly influencing the entire multimodal domain. However, with the rapid advancements in large language models (LLMs) like GPT-4 and LLaMA, the boundaries of language comprehension and generation are continually being pushed. This raises an intriguing question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs strong textual understanding can fundamentally improve CLIPs ability to handle image captions, drastically enhancing its ability to process long and complex texts well-known limitation of vanilla CLIP. Moreover, LLMs are trained on vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. However, realizing this potential is challenging. Despite LLMs powerful internal comprehension, their autoregressive nature hides this capability within the model, leading to output features with poor discriminability. Our experiments show that directly integrating LLMs into CLIP results in catastrophic performance drops. In this paper, we propose LLM2CLIP, novel approach that embraces the power of LLMs to unlock CLIPs potential. By fine-tuning the LLM in the caption space with contrastive learning, we extract its textual capabilities into the output embeddings, significantly improving the output layers textual discriminability. We then design an efficient training process where the fine-tuned LLM acts as powerful teacher for CLIPs visual encoder. Thanks to the LLMs presence, we can now incorporate longer and more complex captions without being restricted by vanilla CLIP text encoders context window and ability limitations. Our experiments demonstrate that this approach brings substantial improvements in cross-modal tasks. Our method directly boosted the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming CLIP model trained solely on English data into state-of-the-art cross-lingual model. Moreover, when integrated into multimodal training with models like Llava 1.5, it consistently outperformed CLIP across nearly all benchmarks, demonstrating comprehensive performance improvements."
        },
        {
            "title": "INTRODUCTION",
            "content": "CLIP (Radford et al., 2021) is one of the most important multimodal foundational models today. It aligns vision and language signals into shared feature space by employing simple contrastive learning loss on large-scale image-text pairs. As retriever, CLIP supports wide range of tasks, including zero-shot classification (Qian & Hu, 2024), detection (Lin & Gong, 2023), segmentation (Zhou et al., 2023), and image-text retrieval (Lulf et al., 2024; Koukounas et al., 2024). As feature extractor, it has become dominant in virtually all cross-modal representation tasks, such as image understanding, video understanding, and text-to-image/video generation. For instance, works like LLaVA (Liu et al., 2023) and Qwen-VL (Bai et al., 2023) leverage CLIP as feature extractor to obtain visual features for text models, while models like Stable Diffusion (Rombach et al., 2021) and DALLE 2 (Ramesh et al., 2022) use CLIPs text encoder to extract textual features for visual models. What makes CLIP so powerful, particularly as vision encoder? The core of its strength lies in its unprecedented ability to align visual pretraining with natural language the carrier of human knowledge. Unlike earlier vision Equal contribution. Work done during internship at Microsoft Research Asia. Corresponding author. Please contact: yifanyang@microsoft.com 1 4 2 0 2 4 ] . [ 2 7 9 9 4 0 . 1 1 4 2 : r Preprint Figure 1: LLM2CLIP Overview. After applying caption contrastive fine-tuning to the LLM, the increased textual discriminability enables more effective CLIP training. We leverage the open-world knowledge and general capabilities of the LLM to better process dense captions, addressing the previous limitations of the pretrained CLIP visual encoder and providing richer, higher-dimensional textual supervision. Experimental results demonstrate that LLM2CLIP can make any SOTA CLIP model even more SOTA ever. encoders such as VGG and ResNet, which relied on the limited ImageNet dataset and simple image categories with just few words, CLIP is trained on web-scale data using rich descriptive text. This alignment with language is what sets CLIP apart and unlocks its vast potential. However, since CLIPs introduction, large language models (LLMs) have advanced significantly. Models like GPT-4 (Achiam et al., 2023) and Llama (Dubey et al., 2024) now demonstrate remarkable language capabilities, yet these advancements have not translated to corresponding improvements in visual representation learning. This prompts the question: can the capabilities of LLMs be harnessed to further improve multimodal representation learning? The potential benefits of incorporating LLMs into CLIP are clear. LLMs strong textual understanding can fundamentally improve CLIPs ability to handle image captions, drastically enhancing its ability to process long and complex texts well-known limitation of vanilla CLIP. Moreover, LLMs are trained on vast corpus of text, possessing open-world knowledge. This allows them to expand on caption information during training, increasing the efficiency of the learning process. In this work, we aim to leverage large language models (LLMs) to enable CLIP to learn more powerful, finegrained, and rich visual representations. Currently, CLIP is often criticized for its bag-of-words-like perception and the limitations of its text encoder, which suffers from constrained model size, limited context length, and is trained predominantly on image captioning data, lacking exposure to diverse world corpora. natural approach would be to replace CLIPs text encoder with an LLM that embeds rich human knowledge. However, this presents significant challenges. In the cross-modal contrastive learning framework employed by CLIP, the text encoder functions as set of knowledge anchors in the shared latent space, guiding the alignment of the vision encoder with human knowledge of the physical world. The structure, richness, and discriminability of these knowledge anchors are critical to the visual models effectiveness. In contrast, LLMs are primarily designed to predict the next word rather than generate explicit representations of the knowledge they contain. Their textual comprehension abilities and open-world knowledge are latent within the model, rather than present in the output embeddings, making them difficult to utilize in the same explicit manner as CLIPs text encoder. As result, using LLMs as text encoder may not produce linearly separable features, which are crucial for effective feature alignment. To validate our hypothesis, we designed caption-to-caption retrieval experiment, as shown in Table 1 and Figure 2. Each image in the MS-COCO dataset has five human-annotated captions. We selected the first two captions as positive samples and performed retrieval across the entire validation set. Using the caption retrieval accuracy (CRA), we evaluated the text models ability to differentiate between captions, helping us determine which language model is better suited for CLIP. We found that Llama-3 8B achieved only 18.4% top-1 accuracy, while the standard CLIP-ViT-L reached 66.0% top-1 accuracy. As illustrated in Figure 2, the top-1 caption retrieved by original Llama-3 can be entirely unrelated to the query caption, clearly obstructing effective CLIP learning. Therefore, directly using an LLM to guide CLIPs visual encoder training is highly constrained. 2 Preprint Table 1: Comparison of top1 Caption Retrieval Accuracy (CRA) for various language models in MS COCO 5K testing set. Language Model CRA 25.3 27.1 5.2 5.6 29.2 28.0 CLIP-L/14 EVA02-L/14 Llama3-8B Llama3.2-1B Llama3-8B-CC Llama3.2-1B-CC Figure 2: Real examples of top-1 results from the caption-to-caption retrieval experiment. Before fine-tuning, Llama3s results were often completely unrelated. We believe that enhancing the discriminative power of LLM output tokens through fine-tuning is vital for the success of our proposed approach, allowing the latent capabilities of LLMs to surface. Encouragingly, we found that this can be achieved very efficiently. Specifically, we designed caption contractive (CC) fine-tuning strategy, applying lightweight fine-tuning to the output tokens of Llama-3 8B using LoRA on the CC3M (Sharma et al., 2018) image captioning dataset. The primary goal of this training task was to adjust the output space, improving the models ability to distinguish between different captions. We utilized supervised SimCSE (Gao et al., 2021; BehnamGhader et al., 2024) contrastive learning loss, where the original captions and re-annotated captions generated by ShareCaptioner (Chen et al., 2023) were treated as positive pairs, pulling them closer. In contrast, all other captions formed negative sample set that the model learned to push away. Remarkably, after this CC fine-tuning, the caption retrieval accuracy, as shown in Table 1, rose from 18.4% to 73%, which is 7% improvement over the original CLIP-ViT-L text encoder. This successful fine-tuning process enables us to more effectively harness the open-world capabilities of LLMs for CLIP training. In nutshell, we present LLM2CLIP, novel approach for enhancing visual representation learning through the integration of large language models (LLMs) as shown in Figure 1. This method takes straightforward yet audacious step by replacing the original CLIP text encoder and augmenting the CLIP visual encoder with the vast knowledge embedded in LLMs. We have identified key obstacles associated with this innovative idea and proposed cost-effective fine-tuning strategy to overcome them. Our experiments demonstrate that leveraging LLMs as teachers for CLIP training yields substantial improvements, with LLM2CLIP significantly outperforming state-of-the-art pre-trained CLIP models. Our method increased the performance of the previously SOTA EVA02 model by 16.5% on both long-text and short-text retrieval tasks, transforming CLIP model trained solely on English data into state-of-the-art cross-lingual model. Furthermore, when incorporated into multimodal model training, such as with Llava 1.5, it consistently achieved comprehensive improvements over EVA02 across nearly all benchmarks. Additionally, the efficient training method proposed by LLM2CLIP ensures that the training cost is nearly identical to fine-tuning the original CLIP. We have also demonstrated that using more powerful language models and larger training datasets can further boost LLM2CLIPs performance, showcasing the immense potential of our approach. These promising outcomes affirm that we have successfully transformed CLIP into more general-purpose foundational model. The enhanced LLM2CLIP model possesses richer knowledge and exhibits remarkable capacity for distinguishing fine-grained and complex long-text semantics. This advancement not only broadens the range of supported downstream tasks but also propels progress across the entire vision domain."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Contrastive Language-Image Pre-training (CLIP). CLIP (Radford et al., 2021) is one of the most important multimodal models today. By training on web-scale data, image captions serve as rich, fine-grained supervision signals for learning representations that align closely with image, freeing CLIP from the limitations of manually defined labels. The richness of the captions endows CLIP with many remarkable capabilities; besides zero-shot image-text retrieval, CLIP features can even achieve region-level correspondence, supporting zero-shot detection and segmentation tasks. Furthermore, CLIP is the most widely used encoder in current image and video understanding tasks. In Tong et al. (2024)s analysis, it stands out among various visual encoders in MLLMs training as the most significant model, demonstrating its strong capabilities. For generative tasks, CLIP is equally foundational. Its text and image encoders are used to provide text control (Rombach et al., 2021; Ramesh et al., 2022) and image control signals (Blattmann et al., 2023; Zhang et al., 2023) for conditioning generative models. Additionally, the CLIP score (Hessel et al., 2021) has become crucial metric for assessing the relevance of generated 3 Preprint images to text. While the original CLIP only utilized image and text modalities, there have been massive extensions of CLIP to incorporate various other modalities (Dai et al., 2024; He et al., 2023; Moon et al., 2022; Zhang et al., 2022). CLIP definitely has significant impact on the field of vision and multimodal, and its influence is still growing. CLIP meets Stronger Language Models. Several works have explored the integration of LLMs into CLIP. JinaCLIP (Koukounas et al., 2024) employed Jina-embeddings-v2 (Gunther et al., 2023) as the text encoder, which is BERT variant with 137M parameters, supporting longer texts. Though achieving similar visual performance to EVA-CLIP (Sun et al., 2023), its text encoder is far behind our used Llama3-8B, limiting the potential benefits from LLMs. T5-V (Jiang et al., 2024) took different approach by aggregating LLM features at the MLLM layer of LLAVA-next-8B (Li et al., 2024a) while freezing the ViT gradient to focus on adjusting the MLLM output. However, freezing visual encoder does not address the problem that ViT inherently lack complex visual feature extraction capabilities, leading to much worse performance than LLM2CLIP. MATE (Jang et al., 2024) designed learnable adaptor to bridge the gap between CLIPs text encoder and LLMs. They trained the CLIP visual encoder using LoRA on small dataset focused on long-text image retrieval tasks. However, they did not recognize the critical issue we propose in this paper: the poor separability of LLM feature space, which is insufficient for direct support of CLIP training. Our work aims to thoroughly explore the impact of LLMs on CLIP as foundational model, going beyond the influence on long-text retrieval. CLIP meets Longer Captions. It has been widely recognized that the quality of CLIPs text embedding is coarse and limited to only 77 tokens. Many works have attempted to extend the length of CLIP captions and retrain CLIP accordingly. For instance, DCI (Urbanek et al., 2024) hired annotators to expand captions based on SAM (Kirillov et al., 2023)s target hints. LaCLIP (Fan et al., 2024) adopted ChatGPT, Bard, and human rewriting. DreamLIP (Zheng et al., 2024) leveraged ShareCaptioner (Chen et al., 2023) and InstructBLIP to augment 30M captions. Recap-DataComp-1B (Li et al., 2024b) used Llama3-trained LLAVA1.5 to extend 1B captions. In order to handle captions longer than 77 tokens, these methods compromised by summarizing long captions into shorter ones (Urbanek et al., 2024), splitting captions into multiple segments (Fan et al., 2024; Zheng et al., 2024), or finetuning models positional encoding to support longer token inputs (Zhang et al., 2024). LLM2CLIP, in comparison, leverages Llama3 as the text encoder, which enables comprehensive understanding of long and dense captions. It not only resolves the limition of token length but also allows for better understanding using LLMs open-world knowledge. As result, our model achieves breakthrough in performance."
        },
        {
            "title": "3 METHODS",
            "content": "The contributions of our methodology are threefold: First, we designed experiments to analyze the key reason preventing LLMs from directly participating in multimodal representation learning the weak discriminability of their output features. Second, we introduced the caption contrastive fine-tuning method, which significantly improves feature discriminability. Third, we developed the LLM2CLIP training framework, which has been proven to be an efficient and effective method for leveraging LLMs to deliver substantial performance improvements to pretrained CLIP models. 3.1 NATIVE LLMS ARE INEFFECTIVE TEXT ENCODERS FOR CLIP Large Language Models (LLMs), such as Llama-3, are trained on massive world corpora through auto-regression, equipping them with open-world knowledge and enabling them to perform various tasks. We initially hoped to leverage the capabilities of LLMs to directly retrain CLIP model. Although LLMs exhibit strong text comprehension, they are difficult to use directly as text embedding models. This is because their knowledge is encapsulated within the model, and their output features are heavily skewed towards individual word predictions. As generative models, they are not trained to ensure good linear separability of output features, making them less effective when used to interpret captions for CLIP. As highlighted by Chen et al. (2022), cross-modal contrastive learning in CLIP requires that each modality possesses strong internal discriminability. To evaluate the effectiveness of various language models in terms of text discriminability, and to test whether the native output features of LLMs, as hypothesized, struggle to distinguish image captions, we introduce new metric: the MS COCO Caption Retrieve Accracy (CRA). MS COCO is widely used multimodal dataset, containing over 330K images, each with five captions. These captions are written by different human annotators and provide diverse descriptions for each image. In our evaluation on the MS COCO validation set, we use only the first two captions of each image and treat the captions of the same image as positive pairs, while all other captions serve as negative samples. We then perform caption-to-caption retrieval and assess Top-1 accuracy using different language models, defining the result as their CRA score. Higher CRA scores indicate better discriminability of the language models on image captions. Preprint As shown in Table 1, using pure LLM results in CRA score of only 18.4%, indicating that the majority of captions cannot be well-separated in the output space. In fact, captions with similar distances may be entirely unrelated, as illustrated in Figure 2. However, the text encoder from the original state-of-the-art CLIP model achieves CRA score of 66%, proving the inadequacy of native LLM output features in caption discriminability. Consequently, it is challenging to apply LLMs directly in CLIP model training. Subsequent experiments in Table 6 also confirm that replacing CLIPs text encoder and the corresponding ViT with Llama-3 8B for contrastive learning significantly underperforms the original CLIP. 3.2 CRUCIAL LESSON: LLM LEARNING FOR IMAGE CAPTION DISCRIMINATION In this section, we aim to fine-tune the LLMs token outputs to better capture features that can distinguish between image captions. The process of improving the discriminability of LLM output features on caption text is quite straightforward: we need the distances between captions of the same image to be closer, and those of different images to be further apart. Therefore, we apply caption contrastive (CC) fine-tuning to the LLMs output features, treating different captions of the same image as positive samples and the rest of the captions as negative samples. To obtain enough varied descriptions for the same image, we use the ShareCaptioner (Zheng et al., 2024; Chen et al., 2023) modified CC-3M (Sharma et al., 2018) dataset, which provides both original captions and augmented dense captions for each image. These can be treated as positive pairs. We followed the training methodology of LLM2Vec (BehnamGhader et al., 2024), first expanding the LLMs attention mechanism to bidirectional attention, and employing Masked Next Token Prediction (MNTP) for initialization to achieve better results. Specifically: First, we transform the LLMs causal attention mechanism into bidirectional attention, as we no longer need it to retain generative capabilities but rather function as an encoder. Since autoregressive training is no longer required, switching to bidirectional attention improves its ability to capture contextual information. Second, we employ MNTP to train the newly added bidirectional attention mechanism, providing strong initialization. For given sequence of tokens, we mask subset and predict their values, similar to BERT (Devlin et al., 2018). However, unlike BERT, we adapt this process to fit the nature of LLMs by predicting tokens just before the masked token. We train on both image captions and pure text with equal weighting. In addition to CC-3M, we also use the Wikitext-103 (Merity et al., 2016) dataset to preserve the LLMs text capabilities and prevent divergence from its original strengths. Finally, we perform the actual caption contrastive fine-tuning, using supervised SimCSE loss to bring captions of the same image closer together and push apart captions of different images. We use two prompt templates: Given caption, retrieve detailed relevant caption and Given detailed caption, retrieve short relevant caption, which are prepended to the query (either original or dense caption) to retrieve the corresponding dense or original caption. Similarly, we use 1.5M-paired general text dataset curated from Springer et al. (2024) to maintain strong performance in pure language tasks. All training is efficiently conducted using LoRA and is completed in just one epoch, ensuring low computational cost. remarkable result followed: after fine-tuning the LLM as caption encoder for only one epoch on CC-3M, the CRA score for Llama-3 8B jumped from 18.4% to 73.0%, surpassing the previous state-of-the-art CLIP and EVA models text encoders trained on the massive Laion-2B (Schuhmann et al., 2022) and COYO-700M (Byeon et al., 2022) image-text datasets. As shown in Table 6, subsequent experiments demonstrated that after CC fine-tuning, the LLM finally unleashed its powerful capabilities, significantly boosting the performance of the previously state-of-the-art CLIP model, in stark contrast to the results without CC fine-tuning. This breakthrough uncovers the potential of LLMs in CLIP training and removes major obstacle in leveraging LLMs to advance vision foundation models. 3.3 LLM2CLIP: MAKES SOTA CLIP MORE SOTA EVER With the modifications to the LLM discussed earlier, we have now obtained super text encoder that is well-suited for CLIP training. The next step is to use this LLM in conjunction with the pretrained state-of-the-art CLIP visual encoder to reconstruct more powerful cross-modal feature space. As shown in Figure 1, in the LLM2CLIP training phase, we freeze the gradients of the LLM to preserve its inherent capabilities, for two primary reasons. First, this significantly reduces the computational cost and memory footprint of fine-tuning. CLIP training requires very large batch size to maintain the effectiveness of negative samples. Allocating memory to the LLM could compromise CLIPs performance. Second, by freezing the LLM, we ensure that the open-world knowledge it has acquired from large-scale corpora remains intact during the multimodal alignment process. To compensate for the frozen LLM, and inspired by methods like FuseMix (Vouitsis et al., 2023) and APE (Rosenfeld et al., 2022), we introduce several new linear layers after the LLM as adapters. These layers serve as learnable parameters to improve the alignment between the LLM and CLIP visual encoder. Following the original design of 5 Preprint CLIP, we also employ projector layer to align the dimensions of the two encoders, facilitating the use of CLIP loss for training. With this powerful LLM-based super text encoder, we achieve substantial qualitative leap in CLIPs language comprehension capabilities. The LLMs open-world knowledge allows the CLIP visual encoder to learn more structured and globally informed visual representations that are aligned with human knowledge. Moreover, this approach enables us to fully utilize high-quality, long, and dense caption datasets without requiring any special architectural adjustments, which previous works like DCI, DreamLip, and Recaption struggled to effectively leverage. As shown in Table 2, LLM2CLIP makes any existing SOTA CLIP model even more SOTA, significantly surpassing the performance of previous one. 3.4 OVERVIEW AND EFFICIENCY DISCUSSION We propose LLM2CLIP as method that efficiently incorporates large language models (LLMs) into CLIP training, leveraging the capabilities of LLMs to make cross-modal representation learning significantly more powerful. In our experiments, We evaluated large language models, including Llama with 1B and 8B parameters, as well as Mistral-Nemo with 12B parameters.It might seem that incorporating such large LLMs would greatly increase the computational burden of training CLIP, especially since CLIP itself is computationally expensive, requiring large batch size. However, our proposed LLM2CLIP is remarkably lightweight. The training overhead is nearly identical to fine-tuning the original CLIP model, with minimal additional cost, yet the LLM provides much stronger supervision. Here, we highlight some of the design details that significantly improve training efficiency: 1). During the caption contrastive fine-tuning stage, we employ LoRA training for the LLM. Even for 12B LLM, training with batch size of 512 only requires around 70GB of GPU memory, making it feasible to run on single 80GB 8 A100 GPU node. 2). In the LLM2CLIP stage, we freeze the LLMs gradients and only train the learnable adapter, CLIPs original Vision encoder, and two projectors. The additional trainable parameters are roughly equivalent to those in the original CLIP, minimizing the overhead. To further reduce the inference cost of using LLMs, we pre-extract all the text features from the training data and store them in memory. This ensures that, even though LLMs provide powerful textual supervision, the memory and computational costs during training remain nearly identical to those of standard CLIP training. For instance, when we trained LLM2CLIP using Mistral-Nemo* 12B model integrated with the commonly used EVA ViT-L/14-224, with batch size of 4096 on 8 H100 GPUs, the memory usage per GPU was only 30GB, and the entire training process took just 9 hours. Despite this efficient training cost, LLM2CLIP brought transformative improvements in downstream tasks such as long and short text retrieval, cross-language retrieval, and LLAVA training."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENT SETTING 4.1.1 CAPTION CONTRASTIVE FINE-TUNING Training Dataset. We utilized the ShareCaptioner-modified CC-3M dataset (Zheng et al., 2024; Chen et al., 2023), which provides both original captions and augmented dense captions for each image, for contrastive learning. For the Masked Next Token Prediction and caption contrastive fine-tuning stages, we employed the Wikitext103 dataset (Merity et al., 2016) and the E5 dataset from Springer et al. (2024) to ensure that the general text domain was not excessively biased. Training Setting. We trained the language model using LoRA, applying lightweight training with 1 epoch on all datasets. We adopted the average of all LLM output tokens as the text global embedding for caption. All training parameters follow the design of BehnamGhader et al. (2024). 4.1.2 LLM2CLIP FINE-TUNING Training Dataset. To compare different configurations, we designed three experimental settings based on dataset size: LLM2CLIP-3M: This configuration only uses the CC-3M dataset for training, representing our lightweight setting. LLM2CLIP-15M: This configuration uses both CC-3M and CC-12M datasets, which is our default setting. All LLM2CLIP models without specific dataset size label are trained with this 15M data. LLM2CLIP-60M: This configuration scales up the dataset, combining CC-3M, CC-12M, YFCC-15M, and randomly selected 30M subset from Recaption-1B (Li et al., 2024b). All data used are dense captions rewritten by multimodal large language models (MLLMs). The CC-3M, CC-12M, and YFCC datasets are sourced from *https://mistral.ai/news/mistral-nemo/ 6 Preprint Zheng et al. (2024) using the ShareCaptioner for rewriting, while Recaption data was rewritten using Llama3LLAVA1.5. We primarily used mix of original captions and dense captions, with the default mixing ratio being 1:1. LLM2CLIP-S represents setting where only original data is used for training, matching the original CLIP pretraining distribution to analyze the models benefits separately. Training Setting. All experiments with different datasets were conducted for 4 epochs. We froze the LLM gradients by default. During training, we pre-extracted the text features from the captions using the LLM and stored them in memory to avoid repeated LLM inference, reducing additional computational overhead. We trained the original CLIP vision encoder, the LLMs learnable adapter, and the projectors for both encoders. Training on the 3M, 15M, and 60M datasets for CLIP ViT-L/14-224 required only 30G per GPU memory and 2, 9, and 45 hours on 8 H100 GPUs, respectively. 4.1.3 EVALUATION Evaluation Dataset. For short-text datasets, we used the commonly available COCO 2014 5k test set and the Flickr 1k test set as our test datasets. For long-text datasets, we used the 1K subset of the ShareGPT4V (Chen et al., 2023) dataset and the Urban1k (Zhang et al., 2024) dataset, both provided by LongCLIP, along with the DOCCI (Onoe et al., 2024) dataset. The ShareGPT4V-1M dataset was generated using annotations from GPT-4V and ShareCaptioner, with images from Laion, CC, SBU (Ordonez et al., 2011) and MS COCO. We used randomly selected 1K subset of this dataset. Urban1k consists of captions generated by GPT-4V for 1,000 busy urban view images from Visual Genome. Each caption is long and complete sentence that describes the image, including types, colors, and relative locations of various attributes. The model can only successfully match the images with the correct captions if it accurately understands and models the detailed attributes in both modalities. The DOCCI dataset contains approximately 1.5K high-resolution images with detailed human-annotated descriptive captions. DOCCI is divided into training set of 9.6K pairs and test set of 5.1K pairs. We used the test set for image-lengthy caption retrieval experiments. For the Chinese retrieval tasks, we used the FlickrCN (Lan et al., 2017) and CNCOCO (Li et al., 2018) datasets, which are translated versions of Flickr30K and MS COCO-1K, respectively. These datasets were tested in Chinese. For Llava (Liu et al., 2023) training, we used the standard training and test sets from LLAVA 1.5. Evaluation Setting. The experiments in this paper primarily focus on how LLM2CLIP enhances the performance of widely used vanilla CLIP across various dimensions. We mainly compare our approach with the EVA (Fang et al., 2023) and OpenAI CLIP models as baselines, as they are the most widely used SOTA vision encoders in the open-source community. The CLIP models we use for comparison are ViT-B/16, ViT-L/14, and ViT-L/14-336. For EVA02 models, we use EVA02 ViT-B/16, EVA02 ViT-L/14, and EVA02 ViT-L/14336**, making up total of six models. For language models, we experimented with four different models: Jina-Embeddings-V2, and three popular LLMs from the Llama3 (Dubey et al., 2024) family, namely Llama 3.2 1B, Llama 3 8B, and Mistral-Nemo 12B. To avoid any misunderstanding, the experiments in this paper, unless otherwise noted, use EVA02 ViT-L/14 as the default baseline for comparison. The default language model used is LLaMA 3 8B, trained on dataset of 15M setting mentioned above. 4.2 MAIN RESULTS Directly Replacing CLIPs Text Encoder with Vanilla LLM is Harmful. As shown in the experiments in Table 4, directly replacing the text encoder of EVA02 ViT-L/14 with Llama3-8B significantly degrades retrieval performance. For example, performance on the DOCCI benchmark nearly halves, dropping from 75.0/73.4 to 51.7/50.6. Additionally, the CRA score of the vanilla Llama3-8B is particularly low, indicating that its output features exhibit very poor discriminability for captions. These results clearly show that such an approach imposes significant burden on CLIPs learning process. Improving the Discriminability of LLM Output Features is Key to Integrating LLMs with CLIP. To enhance the discriminability of LLM output features, we applied Caption Contrastive fine-tuning. Llama3-8B-TC and Llama3-8B-CC represent the results of using pure text corpora and mix of text and augmented CC3M corpora from ShareCaptioner, respectively, both trained with supervised SimCSE loss. As shown in the Table 6, the contrastive learning on mixed caption corpora yields higher CRA scores compared to training on generic text, with noticeable improvements across almost all benchmarks. This difference stems from the fact that LLMs trained on https://huggingface.co/openai/clip-vit-base-patch16 https://huggingface.co/openai/clip-vit-large-patch14 https://huggingface.co/openai/clip-vit-large-patch14-336 https://huggingface.co/QuanSun/EVA-CLIP/blob/main/EVA02_CLIP_B_psz16_s8B.pt https://huggingface.co/QuanSun/EVA-CLIP/blob/main/EVA02_CLIP_L_psz14_s4B.pt **https://huggingface.co/QuanSun/EVA-CLIP/blob/main/EVA02_CLIP_L_336_psz14_s6B.pt https://huggingface.co/jinaai/jina-embeddings-v2-base-en 7 Preprint Table 2: Systematic Comparison Experiment Demonstrating the Performance Improvements of LLM2CLIP. Flickr30k T2I I2T COCO I2T T2I ShareGPT4V I2T T2I Urban-1k T2I I2T DOCCI I2T T2I Methods ViT-B/16 ALIGN BLIP Jina-CLIP Long-CLIP CLIP +LLM2CLIP EVA02 +LLM2CLIP 80.6 80.6 80.6 85.8 82.3 89.2 86.2 88.5 ViT-L/14 Long-CLIP CLIP EVA02 +LLM2CLIP 90.0 85.2 92.6 89.7 +LLM2CLIP-3M 89.6 +LLM2CLIP 92.0 +LLM2CLIP-30M 92.0 +LLM2CLIP-60M 94.4 ViT-L/14-336 CLIP 87.7 +LLM2CLIP 91.2 +LLM2CLIP-60M 93.9 89.6 93.9 +LLM2CLIP EVA02 62.2 74.1 67.4 70.6 62.2 78.1 71.5 78.0 76.2 65.0 81.7 77.3 77.3 82.8 83.5 83.2 67.0 82.1 82.3 78.0 83. 52.0 61.7 55.6 56.9 52.4 62.2 58.7 63.6 62.8 56.3 64.9 63.7 59.7 68.5 69.0 70.4 58.0 65.5 68.5 64.2 68.7 43.2 48.5 41.1 40.9 33.1 48.7 42.1 49.8 46.3 36.5 52.5 47.5 48.0 54.8 55.3 55.7 37.1 53.6 54.8 47.9 55. 75.9 65.8 - 94.8 84.5 98.1 90.5 98.0 97.2 84.2 98.4 91.9 98.3 98.6 98.9 99.2 86.2 98.1 98.9 91.5 98.8 80.6 74.3 - 93.5 79.8 97.4 85.5 98.1 97.3 83.6 98.4 89.3 98.6 99.0 98.8 99.4 84.0 98.4 99.1 89.4 99. 62.2 45.5 87.7 79.1 67.5 86.1 67.0 84.7 82.5 68.3 87.6 73.3 87.1 88.1 93.1 94.1 72.8 90.3 94.6 76.6 89.5 59.1 48.5 88.0 79.1 53.1 90.0 60.8 89.7 86.1 55.6 92.0 68.5 91.1 94.0 95.0 95.2 57.0 93.2 95.9 70.0 94. 59.7 50.5 78.7 63.1 60.7 84.1 67.7 85.5 66.5 63.1 87.6 73.5 84.9 88.2 89.3 90.2 67.4 87.7 89.6 74.7 89.2 62.1 53.5 80.0 71.4 57.1 85.0 68.0 86.8 78.6 65.8 88.7 75.0 87.8 90.4 91.2 92.0 65.7 89.0 90.6 76.4 91. Table 3: Retrieval Performance across Flickr30K-CN and COCO-CN. Methods ViT-L/14-336 Wukong CN-CLIP JinaCLIP EVA02 +LLM2CLIP I2T@1 I2T@ I2T@10 T2I@1 T2I@5 T2I@10 I2T@1 I2T@5 I2T@10 T2I@1 T2I@5 T2I@10 Flickr-CN COCO-CN 76.1 80.2 3.30 4.40 86.9 94.8 96.6 9.90 11.8 98.1 97.5 98.2 15.1 16.7 99.3 51.7 68.0 0.7 0.94 75.1 78.9 90.7 3.5 2.9 92.9 86.3 95.4 6.0 4.8 96. 53.4 63.4 2.9 2.7 69.1 80.2 84.2 8.9 9.8 92.5 90.1 92.9 13.7 15.2 97.2 55.2 64.0 1.0 1.0 70.0 81.0 89.2 4.9 3.7 92.6 90.6 94.4 8.2 7.3 96. caption-distributed data have stronger discriminability for caption features. These findings highlight the importance of text model discriminability for CLIP training and underscore the rationale behind the caption contrastive fine-tuning in LLM2CLIP. LLM2CLIP Makes Pretrained SOTA CLIP Even More SOTA. We applied the LLM2CLIP fine-tuning method to both EVA02 and CLIP models and observed significant performance improvements in Table 2. Even with lightweight fine-tuning, the results substantially surpassed those of the original models pretrained on datasets like Laion2B. Compared to other methods such as LongCLIP and JinaCLIP, which also attempt to fine-tune pretrained CLIP models, our performance gains were transformative. This demonstrates the effectiveness of LLM2CLIP as method for introducing LLMs to enhance CLIPs performance. Table 4: Ablation Study of LLM2CLIP. Here LLM2CLIP-S refers to the results trained on the original short caption dataset. Methods Flickr30k COCO ShareGPT4v Urban-1k DOCCI I2T T2I I2T T2I I2T EVA02 Vit-L/14 + Jina-Bert ++ Dense Caption 89.7 77.3 63.7 88.1 77.7 60.5 87.9 77.9 60.9 87.9 75.6 56.7 92.4 82.9 67.6 +++ Dense Caption 92.0 82.8 68.5 ++ CC Finetuning + Llama3-8B-S 91.9 47.5 83.3 51.1 95.3 50.3 55.1 41.8 54.5 97.7 54.8 98.6 T2I 89.3 81.0 95.1 46.1 94.9 99.0 I2T T2I I2T T2I 73.3 68.5 73.5 75.0 66.9 68.5 68.9 71.2 79.4 83.8 73.8 77.9 37.2 35.1 39.3 32.3 75.8 83.4 83.7 85.6 88.1 94.0 88.2 90.4 LLMs Built-in Knowledge Directly Benefits CLIP. In Table 4s ablation study, we examine the impact of different steps in LLM2CLIP on the EVA02 model. When using only the original captions, there was minimal improvement, as these captions are from the same distribution as those used in EVA02 CLIPs pretraining. However, we observed that the LLM, through its open-world knowledge, is able to provide deeper understanding of Preprint the original captions, empowering the CLIP model with enhanced capabilities and further boosting performance. Naturally, when dense captions are introduced, the performance improvement becomes even more pronounced. LLM Improves CLIPs Ability to Handle Long and Dense Captions. Thanks to the LLMs inherent long context windows and strong text understanding capabilities, CLIP can process long caption datasets more effectively without requiring any architectural modifications. This stands in contrast to methods like LongCaption, which require fine-tuning of positional encodings, or Dreamlip, which splits captions into sub-captions. Jina CLIP, for example, enables long-text input by replacing its text encoder with Jina BERT. To verify the LLMs natural advantage with long texts, we compared our method to Jina CLIP to assess whether the LLM provides better understanding of long captions. As shown in Table 4, when dense captions are added during training, LLM2CLIP demonstrates significantly greater performance improvements than Jina CLIP, proving that LLMs are better suited to processing long and dense captions. Larger LLMs Lead to Better Performance. LLM2CLIP explores the method of replacing CLIPs text encoder with an LLM and continuing training. In Table X, we thoroughly investigate the performance differences that various versions of LLMs bring to CLIP. In the experiments shown in Table 6, we swapped the text encoder in CLIP with different language models to explore their impact. We maintained the original LLM2CLIP design, with the only change being the initialization of the text encoder. As expected, within the same Llama family, the 8B model significantly outperforms the 1B model. Mistral Namo also shows slight improvement over the 8B model. Its important to note that during CLIP training, the LLMs gradients remain frozen, further proving that larger LLMs, with their richer knowledge, can deliver substantial performance improvements to CLIP. Even so, the 1B LLaMA3.2 model still achieved an impressive performance boost for EVA02 CLIP, with an average improvement of 11.1% an impressive feat considering that EVA02 is already one of the SOTA open-source CLIP models. Table 5: Comparison Experiment of Different Ratios of Dense Captions in the LLM2CLIP Training Process. Ratio Flickr30k COCO ShareGPT4v Urban-1k DOCCI I2T 100% 85.5 75% 92.4 50% 92.0 25% 93.0 0% 92.4 T2I 72.7 82.6 82.8 82.8 82.9 I2T 60.1 68.5 68.5 68.1 67.6 T2I 46.9 54.2 54.8 54.8 54.5 I2T 98.7 98.7 98.6 98.4 97.7 T2I 99.0 99.3 99.0 98.7 94.9 I2T 88.7 89.0 88.1 87.7 75.8 T2I 93.9 94.3 94.0 92.9 83.4 I2T 88.0 88.1 88.2 87.9 83.7 T2I 90.5 90.2 90.4 90.0 85.6 Table 6: Comparison of various text encoders. Methods Flickr30k COCO ShareGPT4v Urban-1k DOCCI I2T T2I I2T T2I I2T EVA02 Vit-L/14 89.8 73.3 63.8 63.8 89.3 +Jina Bert 87.9 77.9 60.9 50.3 95.3 +Llama3-8B 87.1 75.3 56.4 41.6 89.3 +Llama3-8B-TC 92.7 82.1 68.1 54.6 97.7 92.0 82.8 68.5 54.8 98.6 +Llama3-8B-CC +Llama3.2-1B-CC 91.6 81.3 65.8 52.5 98.3 +Mistral-Nemo-12B-CC 93.5 83.7 68.5 54.7 98.6 T2I 91.9 95.1 91.4 98.2 99.0 98.2 98.9 I2T T2I I2T T2I 68.5 73.3 75.0 73.4 79.4 83.8 73.8 77.9 58.6 60.9 51.7 50.6 88.9 93.8 85.0 87.8 88.1 94.0 88.2 90.4 84.5 91.9 83.4 86.4 90.4 94.3 88.0 89.7 Average CRA 76.2 78.2 66.3 84.8 85.6 83.4 86. 69.8 74.2 18.4 71.3 73.0 72.8 73.3 Table 7: Performance of Llava 1.5. The best results are highlighted in bold.We explored whether LLM2CLIP could enhance complex image understanding tasks by modifying Llavas visual encoder. MODEL Llava (Paper) Llava (Rep.) +LLM2CLIP VQA Datasets Pope Metrics MM Benchmarks VQAv2 GQA VizWiz 62.0 62.86 63.15 50.0 50.57 52.37 78.5 79.04 79.80 SQA-IMG TextVQA Random Adv. 86.1 84.85 82. 58.2 57.48 58.35 66.8 67.97 69.92 87.3 87.7 88.55 Popular MME MMBench MMBench-CN LlavaBench MMVet 84.2 86.3 87.75 1510.7 1476.69 1505. 64.3 66.66 68.29 58.3 60.39 60.40 65.4 58.0 62.7 31.1 34.3 34.8 Seed Benchmarks All 58.6 59.86 60. IMG Video 37.3 66.1 39.71 66.95 68.80 38.96 Like miracle: LLM enables English CLIP to learn Chinese without exposure to Chinese data. To validate the knowledge transfer capabilities of the LLM, we designed an out-of-distribution (OOD) experiment by conducting an image-text retrieval task in completely unfamiliar language. This is an especially challenging experiment because all of our training was performed on purely English text, yet now we are testing the model on Chinese data. As shown in Table 3, models like EVA02 CLIP and Jina CLIP, which performed well on English datasets, achieved near-zero accuracy on this task. However, the magical power of LLM2CLIP became evident: the LLMs capabilities allowed the model to achieve impressive performance on Chinese retrieval tasks, even surpassing models that were originally trained on hundreds millions of Chinese data, such as Wukong (Gu et al., 9 Preprint 2022) and CN-CLIP (Yang et al., 2022). This result once again demonstrates that LLM2CLIP can effectively integrate the inherent abilities of the LLM into CLIP, enabling it to handle tasks far beyond its original scope. LLM2CLIP Can Improve the Performance of VLLM. As shown in Table 7, we conducted experiments using the Llava 1.5 Liu et al. (2023) VLLM (Vision-Language Large Model) training framework. Llava incorporates CLIP visual encoder into the LLM for multimodal instruction learning, meaning the quality of the visual encoder can significantly impact Llavas performance. We compared the original CLIP with version enhanced by our LLM2CLIP fine-tuning, running two versions of the experiment according to Llavas official implementation for fair comparison. The results showed that in over 87.5% of the benchmarks, we achieved substantial performance improvements, with the remaining benchmarks showing results that were very close. This demonstrates the potential of LLM2CLIP for complex image reasoning and related tasks. 4.3 ABLATION Impact of Different Data Ratios. With the integration of an LLM, LLM2CLIP enhances our ability to understand dense captions. In Table 5, we present an ablation study that examines the impact of different ratios of dense captions versus original captions on CLIPs training performance. The Ratio column represents the proportion of dense captions used during training. Overall, dense captions contribute to noticeable improvement in performance, but more is not always better. For example, when using 0% dense captions, the performance on datasets like COCO and Flickr30K remains decent, but long-text benchmarks such as ShareGPT4V, Urban-1K, and DOCCI show poor results. Conversely, with 100% dense captions, performance on short-text retrieval benchmarks is the worst. Its important to note that the dense captions we used were generated by the ShareCaptioner model, and there may be some distribution differences and noise compared to real data captions, which could somehow explain why using 100% dense data is suboptimal. Our findings indicate that the best performance is achieved when dense captions make up 50% to 75% of the training data, striking balance between both short-text and long-text retrieval tasks. Impact of Different Data Size. As shown in Table 2, larger training datasets consistently yield positive results for LLM2CLIP, with clear improvements in both long-text and short-text retrieval tasks. Our 60M dataset version has already pushed the limits of what CLIP could previously achieve. Even the 3M version, though lightweight, still delivers significant performance gains, demonstrating the efficiency of the LLM2CLIP approach. Its worth noting that training on the 3M dataset takes only about 3 hours on an 8 H100 GPUs machine, yet results in transformative leap in performance for well-pretrained CLIP ViT model."
        },
        {
            "title": "5 LIMITATIONS AND FUTURE WORK",
            "content": "While LLMs possess open-world knowledge, aligning them with CLIP may require specific data to fully unlock their potential. LLM2CLIP is method that requires only small amount of data for fine-tuning in conjunction with pretrained CLIP visual encoder, and we have already demonstrated significant performance gains. However, we did not intentionally select fine-tuning data based on specific characteristics. By focusing on aspects such as data distribution, length, or categories, we could further tailor the LLM to address CLIPs limitations, allowing the LLM to act as more comprehensive teacher for various tasks. Moreover, in this work, we froze the gradients of the LLM during fine-tuning to maintain large batch size for CLIP training. However, it would be worth exploring approaches that allow for updating the LLMs gradients, as there may be better engineering practices that can balance the two objectives. Re-training LLM2CLIP from scratch on datasets like Laion-2B (Schuhmann et al., 2022) and Recaption-1B (Li et al., 2024b) is also promising direction that we did not pursue due to time constraints."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper presents method that enables LLMs to assist in CLIP training. Given the powerful text understanding capabilities of LLMs, we aim to harness their potential to enhance cross-modal representation learning, allowing language the carrier of human knowledge to play pivotal role in spreading the intelligence of LLMs across multiple modalities and applications. The core contributions of this work are threefold: First, we identified the primary reason why LLMs struggle to directly participate in multimodal representation learning the lack of discriminability in their output features. Second, we introduced the caption contrastive fine-tuning method, which significantly improves feature discriminability, removing the biggest obstacle to utilizing LLMs in CLIP training. Third, we designed the LLM2CLIP training framework, which has proven to be an efficient and effective approach for significantly enhancing the performance of pretrained CLIP models. We experimented with LLM2CLIP on both CLIP and EVA models and validated it across different Llama3 models, including 1B, 8B, and 13B, all of which yielded transformative improvements in CLIP performance. LLM2CLIP offers several advantages, such as seamless compatibility with long and complex text inputs, and the ability to incorporate the open-world knowledge 10 Preprint of LLMs into CLIP training as demonstrated by its ability to match Chinese text even when trained solely on English data. We firmly believe that the potential of LLMs extends far beyond what has been shown, and we hope this work serves as starting point for using LLMs to enhance various vision foundation models, ultimately spreading their general capabilities and open-world knowledge across wide range of modalities and applications."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. LLM2Vec: Large language models are secretly powerful text encoders. arXiv preprint, 2024. URL https://arxiv.org/abs/2404.05961. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022. Delong Chen, Zhao Wu, Fan Liu, Zaiquan Yang, Huaxi Huang, Ying Tan, and Erjin Zhou. ProtoCLIP: Prototypical Contrastive Language Image Pretraining. arXiv e-prints, art. arXiv:2206.10996, June 2022. doi: 10.48550/ arXiv.2206.10996. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. Shenghong Dai, Shiqi Jiang, Yifan Yang, Ting Cao, Mo Li, Suman Banerjee, and Lili Qiu. Advancing MultiModal Sensing Through Expandable Modality Alignment. arXiv e-prints, art. arXiv:2407.17777, July 2024. doi: 10.48550/arXiv.2407.17777. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv e-prints, art. arXiv:1810.04805, October 2018. doi: 10. 48550/arXiv.1810.04805. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36, 2024. Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: visual representation for neon genesis. arXiv preprint arXiv:2303.11331, 2023. Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. arXiv preprint arXiv:2104.08821, 2021. Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao, Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: 100 million large-scale chinese cross-modal pre-training benchmark. Advances in Neural Information Processing Systems, 35:2641826431, 2022. Michael Gunther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, et al. Jina embeddings 2: 8192-token generalpurpose text embeddings for long documents. arXiv preprint arXiv:2310.19923, 2023. Xiaoxuan He, Yifan Yang, Xinyang Jiang, Xufang Luo, Haoji Hu, Siyun Zhao, Dongsheng Li, Yuqing Yang, and Lili Qiu. Unified Medical Image Pre-training in Language-Guided Common Semantic Space. arXiv e-prints, art. arXiv:2311.14851, November 2023. doi: 10.48550/arXiv.2311.14851. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: Reference-free Evaluation Metric for Image Captioning. arXiv e-prints, art. arXiv:2104.08718, April 2021. doi: 10.48550/ arXiv.2104.08718. 11 Preprint Young Kyun Jang, Junmo Kang, Yong Jae Lee, and Donghyun Kim. Mate: Meet at the embeddingconnecting images with long texts. arXiv preprint arXiv:2407.09541, 2024. Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, and Fuzhen Zhuang. E5-v: Universal embeddings with multimodal large language models. ArXiv, abs/2407.12580, 2024. URL https://api.semanticscholar.org/CorpusID:271245054. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything. arXiv:2304.02643, 2023. Andreas Koukounas, Georgios Mastrapas, Michael Gunther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martınez, Saahil Ognawala, et al. Jina clip: Your clip model is also your text retriever. arXiv preprint arXiv:2405.20204, 2024. Weiyu Lan, Xirong Li, and Jianfeng Dong. Fluency-guided cross-lingual image captioning. In Proceedings of the 25th ACM international conference on Multimedia, pp. 15491557, 2017. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024a. URL https: //llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing arXiv preprint Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv:2406.08478, 2024b. Xirong Li, Chaoxi Xu, Xiaoxu Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jieping Xu. Coco-cn for cross-lingual image tagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21:23472360, 2018. URL https://api.semanticscholar.org/CorpusID:46897324. Jiayi Lin and Shaogang Gong. Gridclip: One-stage object detection by grid-level clip representation learning. arXiv preprint arXiv:2303.09252, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. Christian Lulf, Denis Mayr Lima Martins, Marcos Antonio Vaz Salles, Yongluan Zhou, and Fabian Gieseke. Clipbranches: Interactive fine-tuning for text-image retrieval. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 27192723, 2024. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv: Computation and Language,arXiv: Computation and Language, Sep 2016. Seungwhan Moon, Andrea Madotto, Zhaojiang Lin, Alireza Dirafzoon, Aparajita Saraf, Amy Bearman, and Babak Damavandi. Imu2clip: Multimodal contrastive learning for imu motion sensors from egocentric videos and text. arXiv preprint arXiv:2210.14395, 2022. Yasumasa Onoe, Sunayana Rane, Zachary Berger, Yonatan Bitton, Jaemin Cho, Roopal Garg, Alexander Ku, Zarana Parekh, Jordi Pont-Tuset, Garrett Tanzer, Su Wang, and Jason Baldridge. Docci: Descriptions of connected and contrasting images, 2024. URL https://arxiv.org/abs/2404.19753. Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. URL https://proceedings. neurips.cc/paper/2011/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf. Qi Qian and Juhua Hu. Online zero-shot classification with clip. arXiv preprint arXiv:2408.13320, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-Resolution Image Synthesis with Latent Diffusion Models. arXiv e-prints, art. arXiv:2112.10752, December 2021. doi: 10.48550/ arXiv.2112.10752. 12 Preprint Elan Rosenfeld, Preetum Nakkiran, Hadi Pouransari, Oncel Tuzel, and Fartash Faghri. Ape: Aligning pretrained encoders to quickly learn aligned multimodal representations. ArXiv, abs/2210.03927, 2022. URL https: //api.semanticscholar.org/CorpusID:263792597. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35:25278 25294, 2022. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, and Aditi Raghunathan. Repetition improves language model embeddings. arXiv preprint arXiv:2402.15449, 2024. Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. EVA-CLIP: Improved Training Techniques for CLIP at Scale. arXiv e-prints, art. arXiv:2303.15389, March 2023. doi: 10.48550/arXiv.2303.15389. Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Jack Urbanek, Florian Bordes, Pietro Astolfi, Mary Williamson, Vasu Sharma, and Adriana Romero-Soriano. picture is worth more than 77 text tokens: Evaluating clip-style models on dense captions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2670026709, 2024. Noel Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, and Maksims Volkovs. Data-efficient multimodal fusion on single gpu. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2722927241, 2023. URL https: //api.semanticscholar.org/CorpusID:266348670. An Yang, Junshu Pan, Junyang Lin, Rui Men, Yichang Zhang, Jingren Zhou, and Chang Zhou. Chinese clip: Contrastive vision-language pretraining in chinese. arXiv preprint arXiv:2211.01335, 2022. Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the long-text capability of clip. arXiv preprint arXiv:2403.15378, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 85528562, 2022. Kecheng Zheng, Yifei Zhang, Wei Wu, Fan Lu, Shuailei Ma, Xin Jin, Wei Chen, and Yujun Shen. Dreamlip: Language-image pre-training with long captions. ArXiv, abs/2403.17007, 2024. URL https://api. semanticscholar.org/CorpusID:268681239. Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for zeroIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern shot semantic segmentation. Recognition, pp. 1117511185, 2023."
        }
    ],
    "affiliations": [
        "Tongji University",
        "Microsoft Corporation"
    ]
}