{
    "paper_title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
    "authors": [
        "Eric Onyame",
        "Akash Ghosh",
        "Subhadip Baidya",
        "Sriparna Saha",
        "Xiuying Chen",
        "Chirag Agarwal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/"
        },
        {
            "title": "Start",
            "content": "6 2 0 2 9 1 ] . [ 1 2 6 2 3 1 . 1 0 6 2 : r CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Eric Onyame University of Virginia Akash Ghosh IIT-Patna Subhadip Baidya IIT-Patna Sriparna Saha IIT-Patna Xiuying Chen MBZUAI Chirag Agarwal University of Virginia"
        },
        {
            "title": "Abstract",
            "content": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, high-quality multilingual medical reasoning dataset with openended reasoning queries with single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at cure_med."
        },
        {
            "title": "Introduction",
            "content": "Recent progress in large language models (LLMs) and reasoning-oriented systems has produced strong performance in mathematical reasoning and code generation [14]. While these advances suggest LLMs can learn structured solution strategies beyond pattern completion, medical reasoning remains challenging [5, 6] because it requires domain knowledge, careful use of context, and reasoning that clinicians can inspect [7, 8]. Prior work shows promising medical QA results, yet reliable medical reasoning still depends on reasoning-centric data and evaluations that test reasoning behavior rather than answer plausibility [911]. Without such resources, models may generate fluent, credible-sounding outputs without dependable reasoning. The problem is amplified in multilingual settings: progress remains English-centered, leaving midand low-resource languages underrepresented and reliability uneven across communities. Despite cross-lingual transfer, open-ended medical reasoning often exhibits two recurring failures: reduced logical accuracy and unstable language behavior [12, 13]. For clinical use, these failures erode interpretability and trust, since clinicians and patients must understand not only what system concludes, but how it arrives there [14]. While recent efforts attempt to strengthen medical capability through domain-specific supervision [15, 16], benchmarks primarily remain monolingual and rely on closed-form settings, providing limited visibility into multilingual reasoning quality and language fidelity [17]. As LLMs increasingly support clinical education and decision-making, systematic evaluation of multilingual reasoning and language consistency becomes essential for fairness, reliability, and generalization [9, 12]. In this work, We study multilingual medical reasoning across 13 high-, mid-, and low-resource languages. We introduce CUREMED-BENCH, an open-ended benchmark where each query has single verifiable answer, enabling independent evaluation of logical accuracy and language consistency and analysis of cross-lingual generalization under clinically grounded constraints. Next, we propose CUREMED, two-stage training framework (see Figure 1) for multilingual medical reasoning. We apply code-switching-aware supervised fine-tuning (SFT) to stabilize language usage during Equal Contribution. Correspondence Authors: Eric Onyame and Akash Ghosh CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Figure 1: The CURE-MED pipeline for multilingual medical reasoning. The framework progresses through three stages: (A) curation of clinically validated multilingual data from sources like MedlinePlus to enable cross-lingual reasoning; (B) supervised fine-tuning of the Qwen2.5-Instruct backbone on code-switched reasoning traces; and (C) GRPO-guided curriculum reinforcement learning, progressively training from highto midand low-resource languages to enhance logical correctness and language consistency. intermediate reasoning steps and perform curriculum-informed GRPO to improve logical correctness and language fidelity. Our contributions are: 1) We present systematic evaluation of multilingual medical reasoning of LLMs using verifiable medical queries, enabling reliable measurement of logical accuracy and language consistency across languages; 2) We introduce CUREMED-BENCH, large-scale multilingual medical reasoning dataset spanning 13 languages across high-, mid-, and low-resource settings; 3) We propose CURE-MED, two-stage training framework for multilingual medical reasoning that combines code-switching-aware SFT with curriculum-informed reinforcement learning (RL) to jointly optimize logical correctness and linguistic fidelity; and 4) Through extensive automatic and human evaluations, we show that CURE-MED achieve state-of-the-art performance on CUREMED-BENCH and demonstrate improved out-of-distribution generalization, including improved robustness in low-resource languages and stronger performance on unseen medical questions and languages."
        },
        {
            "title": "2 Related Work",
            "content": "This work lies at the intersection of medical reasoning with LLMs and multilingual reasoning. We summarize key gaps in prior work and position CURE-MED as unified response. Large Medical Reasoning Models. LLMs have been widely studied for medical QA, clinical retrieval, and diagnostic tasks [10, 15, 18, 19]. Domain-specific pretraining and instruction tuning can improve factuality, yet benchmark gains often do not translate to reliable medical reasoning [11, 20], with models producing fluent but clinically unsound explanations [14]. core issue is evaluation: many medical benchmarks are closed-form (e.g., multiple-choice), which hides intermediate reasoning and limits verification of logical validity [17, 20]. Recent open-ended evaluations exist, but are largely monolingual or limited to few high-resource languages, leaving multilingual medical reasoning underexplored [17, 21]. We address these gaps by introducing open-ended medical queries with single verifiable answers across 13 diverse languages, enabling independent assessment of reasoning correctness. Multilingual Reasoning and Language Fidelity. Prior work shows CoT prompting can enable cross-lingual inference transfer [3, 22, 23], but evaluations mostly target general-domain math/symbolic tasks and skew toward high-resource languages [4, 12, 13, 24, 25]. In medical settings, models often exhibit degraded accuracy, language drift, and weak cross-lingual generalization [17, 21]. Methods such as language mixing and supervised reasoning distillation can improve fluency, but are typically studied in limited bilingual settings or overfit high-resource languages [2630]. RL has also been used to promote structured reasoning, but remains largely English-centric and general-domain [3135]. 2 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning CURE-MED differs from prior work by optimizing language fidelity and reasoning correctness jointly. We evaluate across high-, mid-, and low-resource languages, and integrate code-switching-aware supervision with curriculuminformed RL for robust multilingual medical reasoning."
        },
        {
            "title": "3 Methodology",
            "content": "Here, we describe the construction of CUREMED-BENCH (Sec. 3.1), including dataset collection and human verification. Next, we present CURE-MED: cold-start initialization (Sec. 3.2), reward design (Sec. 3.3), and GRPO-guided curriculum reinforcement learning (Sec. 3.4). 3.1 Dataset Collection We construct CUREMED-BENCH, multilingual medical reasoning dataset of 15,774 open-ended QA instances across 13 languages spanning Africa, Asia, and Europe, enabling evaluation under diverse linguistic conditions (including African languages such as Hausa, Yoruba, and Swahili). breakdown by language and language family is provided in Appendix C. Source Material and Question Generation. CUREMED-BENCH is grounded in MedlinePlus, clinically validated medical resource curated by U.S. federal health agencies. Following tool-assisted synthetic data generation [3641], we use GPT-4o to retrieve MedlinePlus content and draft closed-ended multiple-choice questions in each target language. Each item is anchored to the source, includes four options with exactly one correct answer, and provides clinically grounded supervision prior to conversion to open-ended prompts. Filtering for Reasoning Difficulty. Following Chen et al. [42], we apply multi-stage filtering to retain questions requiring substantive medical reasoning. We remove trivial items by discarding questions answered correctly by all three compact LLMs: Qwen2.5-3B/7B [43] and LLaMA-3.1-8B [44]. We further exclude under-specified or ambiguous questions, retaining samples with single, unambiguous correct answer and consistent cross-lingual interpretation; GPT-4o is used to identify cases with multiple valid answers or cross-lingual inconsistency. Conversion to Open-Ended Problems. We convert each remaining item into an open-ended prompt using GPT-4o, and generate an explicit reasoning chain with free-form ground-truth answer y. This removes multiple-choice cues and yields open-ended instances with supervised reasoning, enabling direct evaluation of reasoning quality and answer correctness. We define the dataset as = {(x, r, y)}, where each instance has single clinically grounded solution supported by an explicit reasoning trace. As summarized in Table 1, CUREMED-BENCH contains 15,774 instances across 13 languages, including low-resource languages, extending prior benchmarks that are largely multiple-choice and/or linguistically limited. Human Verification and Ethical Review. All samples are verified by native speakers and medical experts (physicians, advanced medical students, and nursing PhD candidates). Reviewers assess clinical correctness, linguistic fidelity, and cultural appropriateness, revising culture-specific terminology and removing translation artifacts or medically inappropriate content. Across 13 languages, user studies report an average rating of 4.89/5, supporting clinical validity (Appendix Table 5). All procedures were approved by an Institutional Review Board for social and behavioral sciences and followed established ethical research standards. Additional details are provided in Appendix D. Dataset Lang. Size Open-ended? Reasoning-Supervision Low-resource? MMedBench MedQA MedExpQA PubMedQA MedQAUSMLE MedMCQA OphthaLingua MCMLE XMedBench WorldMedQA HealthSearchQA CURE-MEDBench 6 3 4 1 1 1 7 1 4 4 1 8.5k 13k 2,488 211k 11.4k 193k 1,184 270k 8,280 568 3,173 13 15,774 Table 1: Comparison of medical domain benchmarks. 3 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Example: Cold-Start Code-Switched Reasoning (French) Question. Une femme de 34 ans se présente aux urgences avec des douleurs abdominales sévères localisées dans lhypochondre droit, associées à des nausées et des vomissements. Étant donné ses antécédents de calculs biliaires, quel est le diagnostic le plus probable pour son état? Reasoning (code-switched CoT). <thinking> <step1> The question describes une femme de 34 ans presenting with severe abdominal pain in the région de lhypochondre droit, along with des nausées et des vomissements. These are classic upper abdominal symptoms. </step1> <step2> Ses symptômes suggèrent une atteinte possible de la vésicule biliaire, especially given her history of calculs biliaires. This could indicate gallbladder-related condition. </step2> <step3> The presence of fièvre à 38,5C and tenderness upon palpation of the quadrant supérieur droit indicates an inflammatory process likely une infection. </step3> <step4> In this context, on peut fortement envisager la cholécystite aiguë (acute cholecystitis) as the most probable diagnosis given the presentation and risk factors. </step4> </thinking> Answer. <answer> Le diagnostic le plus probable est la cholécystite aiguë </answer> Language: French Type: Cold-start code-switched CoT sample Figure 2: An example from the cold-start multilingual dataset showing CoT reasoning in French. The reasoning combines English-based clinical terms and local-language expressions, reflecting code-switching in medical contexts. 3.2 Cold-Start Initialization via Supervised Fine-Tuning (SFT) We initialize multilingual reasoning with cold-start SFT stage on code-switched long CoT trajectories. This stage stabilizes multi-step reasoning in the base model before we introduce stricter language-consistency constraints in later training. Given an input query in the target language ℓ, we construct multi-step reasoning trajectory that allows controlled code-switching in intermediate steps (see Figure 2 for French subset example). Each trajectory contains reasoning steps = {r1, . . . , rT }, where step rt may be written in language ℓt L, followed by final answer written in the target language ℓ. We fine-tune the model by maximizing the likelihood of the reasoning trajectory and final answer conditioned on the input: LSFT = log pθ(r, x), training the model to produce multi-step reasoning before generating the final response. Code-switching in allows the LLM use the most effective language for intermediate inference while keeping the final answer in ℓ. The resulting language-adaptive reasoning behavior provides strong initialization for RL stages that enforce language consistency without degrading logical accuracy. 3.3 Reward Design We train CURE-MED with weighted reward that promotes clinical correctness, language fidelity, and adherence to structured output format. We use closed-source multilingual reward model that performs competitively on RewardBench [45]. To mitigate same-model judge bias, we use separate model for LLM-as-a-judge verification [46, 47]. Correctness Reward. Following Zheng et al. [48], we use GPT-4.1 as verifier to score semantic and clinical equivalence between the model output (y), and reference answer (y). The verifier returns continuous score in [0, 1]: Racc(y x, y) = vacc(x, y, y) [0, 1]. (1) We use exact-match scoring for closed-ended questions. For open-ended questions, the verifier assigns partial credit when the response reaches the correct conclusion via clinically valid reasoning, even under paraphrase [49], providing smoother learning signals. Language Consistency Reward. We enforce strict output-language fidelity by scoring whether is written entirely in the query language ℓ: Rlang(y ℓ) = (cid:26)1 0 if the language of matches ℓ otherwise. (2) Format Reward. parser checks compliance with the required structure (<thinking>, numbered <step n>, and <answer> tags): Rfmt(y) = The final composite reward is defined as: (cid:26)1 0 if the required format is followed otherwise. (3) (4) R(y x, y, ℓ) = λaccRacc(y x, y) + λlangRlang(y ℓ) + λfmtRfmt(y) 4 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Example: Baseline vs. CURE-Med (Spanish) Question. Un paciente presenta congestión nasal tos leve desde hace dos días. No tiene fiebre ni dificultad para respirar. Cuál es la causa más probable? Baseline model (incorrect) Reasoning (flawed). El cuadro parece un resfriado común, pero la ausencia de fiebre podría indicar que no es viral la tos podría ser señal de algo más serio como una infección pulmonar temprana. La congestión nasal podría ser un síntoma inicial de una patología más grave. Answer. Podría tratarse de una infección pulmonar temprana. CURE-Med (correct) Reasoning (code-switched CoT). <step1> The symptoms are mild, lo que coincide con un resfriado leve. </step1> <step2> No fever, lo que reduce la probabilidad de neumonía. </step2> <step3> Lo más probable es un resfriado viral leve. </step3> Answer. Lo más probable es un resfriado viral leve. Figure 3: Qualitative Spanish medical-reasoning example comparing baseline Qwen2.5-7B-Instruct model and CURE-MED-7B. The baseline model produces fluent but clinically flawed reasoning (red) and an incorrect diagnosis, whereas CURE-MED generates structured, code-switched CoT (blue) and arrives at the correct diagnosis (green). 3.4 GRPO-guided curriculum reinforcement learning After SFT, we fine-tune the model with curriculum-guided GRPO [34, 50] for optimizing the reasoning policy under the multilingual verifier-driven reward described in Sec. 3.3. Curriculum Design. We design the curriculum around language resource availability rather than problem complexity. This is motivated by the observation that models achieve higher reasoning accuracy in high-resource languages, providing more stable reward signals early in reinforcement learning. We therefore treat languages as tasks of increasing difficulty and progress from highmediumlow-resource tiers. Based on baseline performance, we define three tiers: high- (French, Japanese, Spanish, Vietnamese), medium- (Korean, Thai, Turkish, Bengali), and low-resource (Amharic, Yoruba, Hausa, Hindi, Swahili). We start GRPO on the high-resource and progressively expand training to lowerresource tiers. To reduce catastrophic forgetting, we retain fixed fraction of samples from the previous phase when introducing new tier. Formally, curriculum phase Ci draws samples from languages in tier Li {high, medium, low}. Training Procedure. While following prior works [34, 50, 51], we apply GRPO without modifying the optimization rule, the training was designed in curriculum phases. When reward improvements plateau within tier, we expand sampling to include the next tier while mixing in data from the previous phase to preserve earlier capabilities. At phase i, we sample batches from: Di = α Di1 + (1 α) DLi, where DLi denotes data from tier Li, Di1 is the retained data from phase 1, and α=0.85 controls the retention ratio. This retention-aware curriculum supports incremental transfer to low-resource languages while maintaining performance."
        },
        {
            "title": "4 Experiments",
            "content": "Next, we outline the experimental setup, baseline models, training and evaluation procedures used to address key research questions: RQ1) Does CURE-MED improve multilingual medical reasoning over instruction-tuned baselines and their vanilla variants? RQ2) What is the performance trade-off between language fidelity and medical reasoning accuracy? RQ3) How does curriculum-guided learning affect performance across model scales? RQ4) Does CURE-MED generalize to unseen medical questions and languages under out-of-distribution evaluation? 4.1 Experimental Setup Dataset and Splits. All experiments are conducted on CUREMED-BENCH, where the dataset is partitioned into 80% train and 20% held-out test set. The train set is further divided into 80% for supervised fine-tuning and 20% for reinforcement fine-tuning. Dataset construction and filtering procedures are described in Sec. 3. Baselines. We benchmark CURE-MED against 28 baseline models comprising i) general-purpose, including Qwen 2.5-Instruct [52], LLaMA [53], Gemma [54], Mistral [55], Apollo2 [56], and Ministral [57]; and ii) medical-specific, including MedAlpaca [58], Meditron [59], UltraMedical [60], HuatuoGPT [61], OpenBioLLM [62], BioMistral [63], and MMed-LLaMA [17]. All models are evaluated in zero-shot setting across three independent runs. Model Training and Evaluation. We use Qwen-2.5-{1.5B,3B,7B,14B,32B} instruction-tuned models as backbones. Training is performed on eight NVIDIA A100 GPUs in two stages: i) SFT on the multi-step cold-switched dataset for three epochs and ii) language-resource-aware curriculum fine-tuning with GRPO. Reinforcement progresses from highto low-resource languages, retaining 85% of data from earlier stages to mitigate catastrophic forgetting. See Appendix C.1 for additional details on our high-/low-resource language definition and the criteria used to assign languages to each group. CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Model Consistency () Accuracy () Small Models ( 3B) LLaMA-3.2-3B Qwen2.5-Instruct-1.5B Qwen2.5-Instruct-3B CURE-MED-Qwen2.5-1.5B CURE-MED-Qwen2.5-3B Medium Models (79B) BioMistral-7B Gemma-7B MedAlpaca-7B Meditron-7B Mistral-7B Apollo2-7B Qwen2.5-Instruct-7B LLaMA-3.1-Instruct-8B HuatuoGPT-o1-8B OpenBioLLM-Llama3-8B MMed-Llama-3-8B UltraMedical LLaMA-3-8B Ministral-8B LLaMA-3-8B Gemma-9B CURE-MED-Qwen2.5-7B Large Models ( 14B) MedAlpaca-13B Qwen2.5-Instruct-14B Qwen2.5-Instruct-32B Qwen2.5-Instruct-72B LLaMA-3.1-70B LLaMA-3.3-Instruct-70B HuatuoGPT-o1-70B OpenBioLLM-Llama3-70B Meditron-70B MMed-LLaMA-3.1-70B CURE-MED-Qwen2.5-14B CURE-MED-Qwen2.5-32B 23.690.36 3.840.25 8.390.42 57.600.65 74.280.60 7.100.90 0.370.25 3.500.90 0.430.40 18.701.30 25.631.35 25.440.36 36.560.31 67.300.14 1.470.45 21.380.56 47.031.03 46.930.45 31.580.12 23.221.14 85.210.63 0.100.17 35.570.38 41.510.38 70.731.10 75.681.01 79.660.32 86.790.44 70.300.43 0.210.55 26.490.36 90.270.31 94.960. 10.410.38 6.200.24 10.830.60 28.320.35 42.930.60 4.800.95 1.230.80 2.470.95 2.501.10 15.231.20 15.931.35 29.560.42 18.910.18 46.860.09 36.620.72 28.090.62 35.291.10 42.870.21 28.930.42 36.971.03 54.350.50 0.070.12 41.790.39 49.690.40 58.801.20 54.650.31 60.800.72 66.670.24 51.220.41 4.540.59 37.850.76 63.740.43 70.040.04 Table 2: Mean results across 13 languages on 28 baseline models and CURE-MED. We observe that CURE-MED models outperform models across all parameter scales. Consistency denotes language consistency and Accuracy denotes logical accuracy. Best overall results are bold, best baselines are underlined. Following Chen et al. [42], we evaluate on the held-out test set using an LLM-as-a-judge framework, with GPT-4o used to match each model output to the known ground-truth answer. We assess logical accuracy (LA), defined as the clinical accuracy of the final answer, and language consistency (LC), defined as whether the final answer is produced in the questions corresponding target language. Figure 3 provides representative Spanish example, illustrating how curriculum-guided reinforcement improves accuracy while maintaining language consistency compared to fluent but incorrect baseline. See Appendix for Additional implementation details."
        },
        {
            "title": "5 Results",
            "content": "Here, we report results that answer RQ1RQ4 from Sec. 4. We compare CURE-MED to instruction-tuned baselines and analyze language-reasoning trade-offs, scaling under curriculum-guided reinforcement, and out-of-distribution generalization. RQ1) CURE-MED outperforms baselines. Table 2 compares CURE-MED to three baseline families: general-purpose instruction-tuned LLMs, medical-domain instruction-tuned models, and medical-specialized LLMs. Across scales, CURE-MED improves both logical accuracy and target-language consistency. At 3B, baselines show low correctness and frequent language violations, while CURE-MED reaches 42.93% logical correctness and 74.28% consistency (3B). At 79B, CURE-MED improves over the best baseline in logical correctness (54.35% vs. 46.86%) while maintaining 85.21% consistency. At 14B, CURE-MED remains best, reaching 70.04% logical correctness and 94.96% consistency. 6 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Figure 4: Trade-off performance between logical of multilingual medical reasoning models, where each point represents model instance with bubble size reflecting model scale. Baseline and CURE-MED models are shown as and , respectively. CURE-MED shifts performance toward the upper-right, indicating consistent gains in language consistency and logical accuracy. Figure 5: Scaling performance of CURE-MED vs. base across Qwen2.5-Instruct variants on language consistency (left) and logical accuracy (right). Our method (solid red line) consistently outperforms the base model (dashed blue line), with performance gaps widening at larger model scales, highlighting the effectiveness of CURE-MED for multilingual medical reasoning. Notably, our 32B model is competitive with closed-source systems and outperforms several proprietary models on CUREMED-BENCH (See Appendix E.3, E.4; Tables 8, 9, 10). RQ2) CURE-MED achieves better language and reasoning trade-offs. Figure. 4 shows that while baselines exhibit weak trade-off between language consistency and logical correctness, CURE-MED shifts this in the upper-right corner, highlighting that CURE-MED improves medical reasoning without sacrificing target-language fidelity, addressing key failure mode of prior multilingual medical systems. We observe that CURE-MED-1.5B outperform several baselines ranging from 7B to 70B and our CURE-MED-32B model outperform all 28 baseline models. RQ3) Scaling Trends of CURE-MED. Fig. 5 shows that CURE-MED smoothly scale language consistency (57.6%@1.5B 95.0%@32B) and logical correctness (28.3%70.0%). By comparison, instruction-tuned baselines exhibit only modest gains in language consistency as scale increases, remaining unreliable even at larger scale. Tables 6-7 in App. report per-language results, showing that CURE-MED consistently improves performance across languages and scales effectively. These trends indicate that curriculum-guided reinforcement fundamentally alters scaling behavior by coupling reasoning optimization with language fidelity. 7 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning RQ4) Out-of-distribution cross-lingual generalization. We evaluate transfer to held-out medical benchmarks: MMedBench [17], MedExpQA [64], and MedQA [65]. Across all three benchmarks, CURE-MED improves accuracy over the Qwen2.5 backbones in the majority of languagescale settings, with the clearest gains for smaller models. On MMedBench  (Table 4)  , the 1.5B backbone increases from 6.0024.00 and from 20.0057.50 on representative languages, demonstrating strong transfer under limited capacity. MedExpQA  (Table 11)  shows similar large jump at 1.5B, rising from 1.4044.80, while MedQA  (Table 12)  improves from 21.0059.50 at 1.5B on Chinese variants. These gains remain at larger scales, indicating that curriculum-guided RL transfers beyond in-domain training to unseen questions and language variants. Model size Base Naïve SFT CURE-MED (w/o RL) Naïve RFT CURE-MED (w/ RL) Qwen2.5-Instruct Language consistency () 53.670.38 (+45.07) 3.840.25 8.601.23 8.810.34 57.600.65 (+48.79) 8.390.42 13.070.33 72.680.38 (+59.61) 13.280.57 74.280.60 (+61.00) 25.440.36 37.110.44 83.460.36 (+46.35) 38.990.68 85.210.63 (+46.22) 35.570.38 37.200.33 84.280.35 (+47.08) 39.101.05 90.270.31 (+51.17) 35.570.38 43.000.27 90.290.21 (+47.29) 45.101.12 94.960.40 (+49.86) Qwen2.5-Instruct Logic accuracy () 22.970.57 (+18.36) 8.800.47 28.320.35 (+19.52) 6.200.24 4.610.36 10.830.60 9.500.38 39.130.53 (+29.63) 10.060.45 42.930.60 (+32.87) 29.560.42 30.051.10 50.030.48 (+19.98) 38.500.38 54.350.50 (+15.85) 41.790.39 43.100.13 61.910.45 (+18.81) 45.200.55 63.740.43 (+18.54) 49.690.40 51.210.15 66.340.43 (+15.13) 53.400.49 70.040.04 (+16.64) 1.5B 3B 7B 14B 32B 1.5B 3B 7B 14B 32B Table 3: Ablation study of CURE-MED. Results are averaged over three runs and reported as mean standard deviation and green columns denote CURE-MED variants. French Model Qwen2.5-1.5B 6.00 CURE-MED 24.00 6.50 Qwen2.5-3B CURE-MED 42.00 Qwen2.5-7B 42.00 CURE-MED 50.00 61.00 Qwen2.5-14B CURE-MED 64.00 Qwen2.5-32B 69.50 CURE-MED 78. Japanese 11.06 35.18 24.62 37.69 51.76 46.73 57.29 65.83 67.84 77.29 Russian 20.00 57.50 22.50 60.50 53.50 66.00 63.00 75.50 72.00 80.00 Spanish 20.00 44.50 23.00 56.00 63.00 64.00 71.50 78.00 29.50 82.50 Table 4: OOD accuracy on MMedBench. CURE-MED improves reasoning performance across all model sizes, showing strong cross-lingual generalization to unseen medical questions and languages. See Tables 11-12 for results on MedExpQA and MedQA datasets."
        },
        {
            "title": "6 Ablation Study",
            "content": "Here, we ablate CURE-MEDs key components and measure their impact on logical accuracy. We also assess robustness by evaluating CURE-MED across multiple multilingual medical QA benchmarks and strong medical-domain LLM baselines. Effect of Codeswitched Supervised Fine-Tuning. We isolate the effect of code-switched supervision during SFT by contrasting the base model, naïve SFT trained on multilingual long-CoT data, and CURE-MED SFT without reinforcement learning. Naïve SFT yields small and sometimes unstable improvements: language consistency rises from 8.39%13.07% at 3B, yet logic accuracy decreases from 10.83%9.50%, indicating that multilingual instruction tuning does not consistently strengthen medical reasoning as shown in Table 3. In contrast, code-switched SFT in CURE-MED produces large, consistent gains across model scales. At 1.5B, language consistency increases from 3.84%53.67% and logic accuracy from 6.20%22.97%. These improvements persist as scale increases, reaching 90.29% language consistency and 66.34% logic accuracy at 32B. In summary, the results show that structured 8 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning code-switching during SFT drives the strongest gains, while naïve multilingual SFT remains insufficient for reliable multilingual medical reasoning. Effect of GRPO-guided curriculum reinforcement learning. We assess whether RL adds value beyond SFT by comparing naïve single stage GRPO based RFT against the curriculum and language resource-aware RL used in CURE-MED, with results summarized in Table 3. Naïve RFT yields limited and uneven gains, especially at smaller scales, suggesting that uniform reinforcement signals do not consistently shape multilingual behavior. In contrast, CURE-MED applies RL after code switched SFT and delivers reliable improvements in both language consistency and logical accuracy across all model sizes. These results show that curriculum and resource-aware RL stabilizes optimization and strengthens multilingual medical reasoning beyond naïve GRPO. CURE-MED vs. Medical LLM baselines across Benchmarks. We evaluate CURE-MED against strong medicaldomain LLM baselines across four multilingual medical benchmarks (see Fig. 6). CURE-MED remains consistent, with CURE-MED-32B achieving the best performance on CUREMED-BENCH (70.04%) and MMed-Bench (79.57%), and remains competitive on MedQA and MedExpQA, where HuatuoGPT-70B leads narrowly. CURE-MED-14B also provides strong results across all benchmarks, while other medical baselines lag behind more substantially, highlighting CURE-MEDs robustness across diverse evaluation settings. Figure 6: CURE-MED vs. medical LLM baselines across four multilingual medical QA benchmarks. Results show logi cal accuracy, highlighting CURE-MEDs consistent across diverse evaluation settings."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduce CUREMED-BENCH, multilingual medical reasoning benchmark of open-ended questions with explicit reasoning traces and single verifiable answer across 13 languages, including low-resource settings. Using CUREMEDBENCH, we propose CURE-MED, which combines cold-start code-switched initialization, structured supervised fine-tuning, and language-resource-aware curriculum-RL to improve reasoning while preserving target-language fidelity. Across languages, datasets, and model scales, CURE-MED improves logical correctness and language consistency over strong baselines; ablations show supervised and RL stages provide complementary gains for stable multilingual reasoning."
        },
        {
            "title": "8 Limitations",
            "content": "CUREMED-BENCH is constrained by the availability of clinically reliable source material across languages, which limits coverage and can create uneven difficulty between highand low-resource settings. Our benchmark targets open-ended questions with single verifiable answer and thus does not capture longitudinal care trajectories, multi-visit decision-making, or multimodal clinical evidence. In addition, parts of our pipeline rely on API-based models (e.g., for generation and/or verification), which can be costly and may hinder reproducibility for some researchers; practical direction is to replace these components with smaller open-source models trained for the same roles and to release prompts, code, and verifier alternatives to reduce dependence on paid APIs. Future work will expand language coverage, broaden clinical settings and modalities, and further reduce reliance on proprietary APIs. 9 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning"
        },
        {
            "title": "9 Ethical Considerations",
            "content": "This work supports the evaluation and training of multilingual medical reasoning systems by measuring reasoning correctness and target-language fidelity across diverse languages. CUREMED-BENCH is derived from publicly available, clinically curated sources and contains no patient records or personally identifiable information. Native speakers and medical experts reviewed all samples for clinical correctness, linguistic fidelity, and cultural appropriateness under IRB-approved procedures, and we report per-language results to surface reliability differences across resource levels."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank all members of the Aikyam Lab for their insightful discussions and valuable feedback. We also thank the native speakers and medical experts across the 13 languages studied in this work for their support with the data verification procedures. C.A. is supported, in part, by grants from Capital One, LaCross Institute for Ethical AI in Business, the UVA Environmental Institute, OpenAI Researcher Program, Thinking Machines Tinker Research Grant, and Cohere. The views expressed are those of the authors and do not reflect the official policy or the position of the funding agencies."
        },
        {
            "title": "References",
            "content": "[1] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):10921097, 2022. 1 [2] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [3] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 2 [4] Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403, 2022. 1, 2 [5] Farah Magrabi, Elske Ammenwerth, Jytte Brender McNair, Nicolet De Keizer, Hannele Hyppönen, Pirkko Nykänen, Michael Rigby, Philip Scott, Tuulikki Vehko, Zoie Shui-Yee Wong, et al. Artificial intelligence in clinical decision support: challenges for evaluating ai and practical implications. Yearbook of medical informatics, 28(01):128134, 2019. 1 [6] William Stead. Clinical implications and challenges of artificial intelligence and deep learning. Jama, 320(11): 11071108, 2018. 1 [7] Vimla Patel, José Arocha, and Jiajie Zhang. Thinking and reasoning in medicine. The Cambridge handbook of thinking and reasoning, 14:727750, 2005. 1 [8] Jose Arocha, Dongwen Wang, and Vimla Patel. Identifying reasoning strategies in medical decision making: methodological guide. Journal of biomedical informatics, 38(2):154171, 2005. 1 [9] Valentin Liévin, Christoffer Egeberg Hother, Andreas Geert Motzfeldt, and Ole Winther. Can large language models reason about medical questions? Patterns, 5(3), 2024. 1 [10] Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, et al. Toward expert-level medical question answering with large language models. Nature Medicine, 31(3):943950, 2025. 2 [11] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. 1, 2 [12] Samuel Cahyawijaya, Holy Lovenia, and Pascale Fung. Llms are few-shot in-context low-resource language learners. arXiv preprint arXiv:2403.16512, 2024. 1, [13] Xuan-Phi Nguyen, Sharifah Mahani Aljunied, Shafiq Joty, and Lidong Bing. Democratizing llms for low-resource languages by leveraging their english dominant abilities with linguistically-diverse prompts. arXiv preprint arXiv:2306.11372, 2023. 1, 2 [14] Julia Amann, Alessandro Blasimme, Effy Vayena, Dietmar Frey, Vince Madai, and Precise4Q Consortium. Explainability for artificial intelligence in healthcare: multidisciplinary perspective. BMC medical informatics and decision making, 20(1):310, 2020. 1, 2 10 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning [15] Lei Liu, Xiaoyan Yang, Junchi Lei, Yue Shen, Jian Wang, Peng Wei, Zhixuan Chu, Zhan Qin, and Kui Ren. survey on medical large language models: Technology, application, trustworthiness, and future directions. arXiv preprint arXiv:2406.03712, 2024. 1, 2 [16] Zhang Shengyu, Dong Linfeng, Li Xiaoya, Zhang Sen, Sun Xiaofei, Wang Shuhe, Li Jiwei, Runyi Hu, Zhang Tianwei, Fei Wu, et al. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792, 2023. [17] Pengcheng Qiu, Chaoyi Wu, Xiaoman Zhang, Weixiong Lin, Haicheng Wang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards building multilingual language model for medicine. Nature Communications, 15(1):8384, 2024. 1, 2, 5, 8 [18] Quan Guo, Shuai Cao, and Zhang Yi. medical question answering system using large language models and knowledge graphs. International Journal of Intelligent Systems, 37(11):85488564, 2022. 2 [19] Akash Ghosh, Debayan Dutta, Sriparna Saha, and Chirag Agarwal. survey of multilingual reasoning in language models. Findings of the Association for Computational Linguistics: EMNLP, 2025:89208936, 2025. 2 [20] Hanjie Chen, Zhouxiang Fang, Yash Singla, and Mark Dredze. Benchmarking large language models on answering and explaining challenging medical questions. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35633599, 2025. [21] Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason Eshraghian, Peter Abadir, and Rama Chellappa. Addressing cognitive bias in medical language models. arXiv preprint arXiv:2402.08113, 2024. 2 [22] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. 2 [23] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. 2 [24] Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking language barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint arXiv:2310.20246, 2023. 2 [25] Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo: Advancing multilingual reasoning through multilingual alignment-as-preference optimization. arXiv preprint arXiv:2401.06838, 2024. 2 [26] Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindˇrich Libovick`y, Constantin Rothkopf, Alexander Fraser, and Kristian Kersting. Speaking multiple languages affects the moral bias of language models. arXiv preprint arXiv:2211.07733, 2022. [27] Haneul Yoo, Cheonbok Park, Sangdoo Yun, Alice Oh, and Hwaran Lee. Code-switching curriculum learning for multilingual transfer in llms. arXiv preprint arXiv:2411.02460, 2024. [28] Yubin Ge, Devamanyu Hazarika, Yang Liu, and Mahdi Namazifar. Supervised fine-tuning of large language models on human demonstrations through the lens of memorization. 2023. [29] Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern, Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu. O1 replication journeypart 2: Surpassing o1-preview through simple distillation, big progress or bitter lesson? arXiv preprint arXiv:2411.16489, 2024. [30] Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. Limo: Less is more for reasoning. arXiv preprint arXiv:2502.03387, 2025. 2 [31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. 2 [32] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [33] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [34] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 5 11 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning [35] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 2024. 2 [36] Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022. 3 [37] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Stanford alpaca: An instruction-following llama model, 2023. [38] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36: 5500655021, 2023. [39] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705, 2022. [40] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. [41] Akash Ghosh, Srivarshinee Sridhar, Raghav Kaushik Ravi, Muhsin Muhsin, Sriparna Saha, and Chirag Agarwal. Clinic: Evaluating multilingual trustworthiness in language models for healthcare. arXiv, 2025. [42] Junying Chen, Zhenyang Cai, Ke Ji, Xidong Wang, Wanlong Liu, Rongsheng Wang, Jianye Hou, and Benyou Wang. Huatuogpt-o1, towards medical complex reasoning with llms. arXiv preprint arXiv:2412.18925, 2024. 3, 6, 13 [43] Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, et al. Qwen2. 5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 3 [44] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 3 [45] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, Lester James Validad Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 17551797, 2025. 4 [46] Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. Replacing judges with juries: Evaluating llm generations with panel of diverse models. arXiv preprint arXiv:2404.18796, 2024. [47] Hritik Bansal, John Dang, and Aditya Grover. Peering through preferences: Unraveling feedback acquisition for aligning large language models. arXiv preprint arXiv:2308.15812, 2023. 4 [48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623, 2023. 4 [49] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains. arXiv preprint arXiv:2503.23829, 2025. 4 [50] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 5 [51] Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, and Paul Pu Liang. Learn globally, speak locally: Bridging the gaps in multilingual reasoning. arXiv preprint arXiv:2507.05418, 2025. 5, 16 [52] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. [53] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pages arXiv2407, 2024. 5 12 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning [54] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118, 2024. 5 [55] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mistral 7b, 2023. URL https://arxiv.org/abs/2310.06825. 5 [56] Guorui Zheng, Xidong Wang, Juhao Liang, Nuo Chen, Yuping Zheng, and Benyou Wang. Efficiently democratizing medical llms for 50 languages via mixture of language family experts, 2024. URL https://arxiv.org/abs/ 2410.10626. [57] Mistral AI Team. Un ministral, des ministraux, October 2024. URL https://mistral.ai/news/ministraux. Accessed: 2025-12-24. 5 [58] Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexei Figueroa, Alexander Löser, Daniel Truhn, and Keno K. Bressem. Medalpaca an open-source collection of medical conversational ai models and training data, 2025. URL https://arxiv.org/abs/2304.08247. 5 [59] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079, 2023. 5 [60] Kaiyan Zhang, Sihang Zeng, Ermo Hua, Ning Ding, Zhang-Ren Chen, Zhiyuan Ma, Haoxin Li, Ganqu Cui, Biqing Qi, Xuekai Zhu, et al. Ultramedical: Building specialized generalists in biomedicine. Advances in Neural Information Processing Systems, 37:2604526081, 2024. 5 [61] Zhang, Chen, Jiang, Yu, Chen, Li, Chen, Wu, Zhang, Xiao, et al. Huatuogpt, towards taming language model to be doctor. arxiv (2023). arXiv preprint arXiv:2305.15075. 5, 16 [62] Saama AI Labs. Openbiollm: Llama3-based biomedical large language model. https://huggingface.co/ aaditya/Llama3-OpenBioLLM-70B, 2024. Model card. Paper in preparation. 5 [63] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. Biomistral: collection of open-source pretrained large language models for medical domains, 2024. URL https://arxiv.org/abs/2402.10373. 5 [64] Iñigo Alonso, Maite Oronoz, and Rodrigo Agerri. Medexpqa: Multilingual benchmarking of large language models for medical question answering. Artificial intelligence in medicine, 155:102938, 2024. [65] Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11 (14):6421, 2021."
        },
        {
            "title": "Appendix",
            "content": "A LLM-as-a-Judge Verification Protocol Inspired by [42], We employ an LLM-as-a-judge framework to automatically evaluate the correctness of modelgenerated responses. In this setup, GPT-4o acts as verifier that compares models prediction against reference answer and determines whether the response is logically correct and linguistically valid. The verifier outputs binary decision, returning True when the response aligns with the reference and False otherwise. Fig. 7 shows the exact prompt used for verification."
        },
        {
            "title": "B Training and Verification Protocols",
            "content": "This section documents the prompts, reward verification procedures, and training hyperparameters used for supervised and reinforcement fine-tuning. Together, these components define the optimization signals and structured supervision underlying the proposed framework. 13 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Prompt for the LLM-as-a-Judge Evaluator <Model Response> {Model Response} </Model Response> <Reference Answer> {Ground-truth Answer} </Reference Answer> You are given model-generated response and reference answer. Determine whether the model response is correct with respect to the reference. Output \"True\" if the response is correct and \"False\" otherwise. Figure 7: Prompt used for LLM-as-a-judge verification. B.1 Reward Verification and Weighting We design composite reward that jointly enforces clinical correctness, language fidelity, and output format compliance. The final reward is defined as This weighting prioritizes medical correctness while explicitly penalizing language drift and format violations. = 0.65 Raccuracy + 0.30 Rlanguage + 0.05 Rformat. B.2 Verifier Models and Prompts Both correctness and language rewards are scored using gpt-4.1 with temperature=0.0 and max_tokens=10. For each prompt, we generate 16 candidate responses to estimate stable reward signals. B.3 Accuracy Verifier. You are an expert multilingual medical evaluator. Score the generated response for correctness and medical validity on continuous scale from 0.0 to 1.0. Give 1.0 if the reasoning is clinically sound and semantically correct, even if phrased differently from the reference. Focus on factual and clinical accuracy rather than wording. Question: {question} Ground truth answer: {ground_truth} Generated response: {generated} Output only float between 0.0 and 1.0. B.3.1 Language Consistency Verifier Prompt You are an expert multilingual medical evaluator. Determine whether the model response is written entirely in the same language as the question. Question language: {language} Generated response: {generated} Output 1.0 if the language matches exactly; otherwise output 0.0. B.3.2 Format Reward We apply deterministic rule-based check requiring exactly one <thinking> block and one <answer> block, implemented using regular expressions with re.DOTALL. This constraint ensures consistent structure during reinforcement learning. B.4 Training Hyperparameters B.4.1 Supervised Fine-Tuning. Optimizer: AdamW (β1 = 0.9, β2 = 0.999) Learning rate: 1 105 (cosine scheduler, 10% warmup) CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Language Family Languages Afroasiatic NigerCongo Indo-European Turkic Austroasiatic TaiKadai Japonic Koreanic Amharic (Am), Hausa (Ha) Swahili (Sw), Yoruba (Yo) Bengali (Bn), Hindi (Hi), French (Fr), Spanish (Es) Turkish (Tr) Vietnamese (Vi) Thai (Th) Japanese (Ja) Korean (Ko) Figure 8: Language and family composition of CUREMED-BENCH. Left: Number of dataset instances per language across the 13 languages. Right: Assignment of languages to eight language families with standard abbreviations. Epochs: 3 Effective batch size: 32 Max sequence length: 4096 Precision: bf16 Optimization: DeepSpeed ZeRO-3 with gradient checkpointing B.4.2 Reinforcement Fine-Tuning. Algorithm: GRPO Learning rate: 1 106 (cosine scheduler, warmup ratio 0.1) Weight decay: 0.1 Effective batch size: 16 Generations per prompt: 16 Max training steps: 500 Max prompt / completion length: 1024 /"
        },
        {
            "title": "C Dataset Details",
            "content": "This appendix characterizes the linguistic composition of CUREMED-BENCH. Figure 8 shows the per-language instance distribution, with French contributing the largest share (13.5%) and Bengali the smallest (2.9%), and most languages occupying mid-range band of roughly 710% of the data. The figure also groups the 13 languages into eight language families, spanning Afroasiatic and NigerCongo as well as IndoEuropean, Turkic, Austroasiatic, TaiKadai, Japonic, and Koreanic. Together, these statistics highlight both the datasets uneven language coverage and its broad typological diversity. C.1 Language-based Curriculum Tiers We construct our curriculum by defining difficulty along the linguistic axis rather than by question complexity. To operationalize this design, we use Qwen2.5-14B-Instruct as reference model and estimate baseline reasoning accuracy separately for each language. The model performs best on high-resource languages and degrades as linguistic resources and model familiarity decrease, so we treat high-resource languages as easier tasks and progressively introduce more challenging languages during training. This curriculum aims to transfer reasoning competence learned in high-resource settings to underrepresented languages while maintaining language fidelity. Based on the baseline accuracy ranking, we partition languages into three tiers. The high-resource tier includes French, Japanese, Spanish, and Vietnamese. The medium-resource tier includes Korean, Thai, Turkish, and Bengali. The low-resource tier includes Amharic, Yoruba, Hausa, Hindi, and Swahili. This tiering reflects the reference models initial proficiency distribution and provides structured progression from easier to harder multilingual reasoning conditions. 15 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning"
        },
        {
            "title": "D Data Curation",
            "content": "The following prompt was used to generate the initial pool of medically grounded multiple-choice questions across 13 languages. Inspired by the approach of Hwang et al. [51] and Zhang et al. [61], we adapted their template and instructed GPT-4o to query MedlinePlus directly and independently construct questions in each target language rather than translating from shared source. This ensures linguistic naturalness, cultural appropriateness, and strong domain grounding across all languages. Prompt for Generating Multilingual Medical Multiple-Choice Questions Task: You are an expert medical content generator. Generate {num_questions} high-quality, medically accurate multiple-choice questions (MCQs) based strictly on content from MedlinePlus by searching and curating from the website. You must independently compose each question in ALL of the following languages: Amharic, Bengali, French, Hausa, Hindi, Japanese, Korean, Spanish, Swahili, Thai, Turkish, Vietnamese, Yoruba. Requirements: 1. Medical Grounding: All information must be sourced from MedlinePlus, covering symptoms, causes, risk factors, diagnostics, treatments, or prevention strategies. 2. Independent Composition: Each language version must be originally written (not translated) using natural phrasing and medically appropriate terminology for that language. 3. Clinical Reasoning Depth: Questions must require genuine clinical reasoning beyond trivial fact recall. Each question should have exactly one unambiguous correct answer. 4. Format: 4-option MCQ (A/B/C/D) with one correct answer. Output Format: Return valid JSON array: [ {\"question_id\": \"<id>\", \"source_concept\": \"<MedlinePlus_topic>\", \"mcq_items\": [{\"language_code\": \"<lang>\", \"question\": \"<text>\", \"option_A\": \"<text>\", \"option_B\": \"<text>\", \"option_C\": \"<text>\", \"option_D\": \"<text>\", \"correct_answer\": \"<ABCD>\"}, ...]} ] IMPORTANT: Return ONLY valid JSON without explanations, formatting, or additional text. Ensure all special characters are properly escaped. Figure 9: Prompt for Stage 1 multilingual MCQ generation. Here, {num_questions} specifies the number of questions to generate, and GPT-4o queries MedlinePlus directly to construct clinically grounded questions independently in each of the 13 target languages. D.1 Human Verification Protocol and Rater Instructions This section documents the human verification procedures used to validate the quality of our synthetic data. We provide the exact instructions used by medical professionals who assessed the clinical correctness of question-answer pairs and by native speakers who evaluated the languages correctness and fidelity in the target language, as shown in Figures 10 and 11. These materials specify the task setup, scoring rubric, and optional comment guidelines used throughout our verification pipeline. D.2 Human Verification Scores by Language We report per-language human verification scores from two rater groups. Medical professionals score clinical correctness of each questionanswer pair, while native speakers score target-language quality and fidelity. Table 5 summarizes both scores on 15 scale, where higher values indicate better quality. Per-Language Model Performance This section provides fine-grained analysis of multilingual medical reasoning performance broken down by language. We compare CURE-MED with instruction-tuned baselines across all 13 languages in CUREMED-BENCH, enabling detailed examination of logical correctness and language consistency under diverse linguistic and resource conditions. This per-language view complements aggregate results by revealing where gains are most pronounced and where challenges remain. CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Participant Instructions: Verification Task Task Overview You will review synthetically generated medical questionanswer pairs based on public sources such as MedlinePlus. These pairs are generated synthetically and do not involve real patient data. Your role is to assess medical correctness and accuracy. What You Will Do For each questionanswer pair: Read the question and the provided answer. Check for medical correctness: ensure the information is accurate, logically sound, and aligned with standard medical knowledge. Assign score from 1 to 5: 1: Completely inaccurate or misleading. 2: Mostly inaccurate with major errors. 3: Partially accurate but with notable issues. 4: Mostly accurate with minor issues. 5: Fully accurate and reliable. (Optional) Provide brief comment if necessary (e.g., explain errors, suggest corrections, or note cultural/language specifics). Comments are optional but helpful. You will receive batches of 50100 pairs via an online survey. The task takes approximately 12 hours and can be completed remotely at your convenience. You may skip any pair or stop at any time. Figure 10: Instructions provided to medical professional annotators for verifying clinical correctness of synthetic questionanswer pairs. Participant Instructions: Language Verification Task Task Overview You will review synthetically generated medical questionanswer pairs written in one of the following target languages: Amharic, Bengali, French, Hausa, Hindi, Japanese, Korean, Spanish, Swahili, Thai, Turkish, Vietnamese, and Yoruba. These pairs are generated synthetically and do not include real patient data. Your role is to verify whether the question and answer are written correctly and naturally in the target language. What You Will Do For each questionanswer pair: Read the question and the provided answer. Verify language correctness and fidelity: The text is in the requested target language (no switching to another language). The wording is grammatical and understandable for native speaker. The phrasing is natural and appropriate for medical communication. Medical terms are expressed in an acceptable way for the target language (including common loanwords, when appropriate). Assign score from 1 to 5: 1: Not in the target language or largely unintelligible. 2: Major language errors; difficult to understand. 3: Understandable but with noticeable errors or unnatural phrasing. 4: Mostly correct and natural with minor issues. 5: Fully correct, natural, and clearly in the target language. (Optional) Provide brief comment to note issues (e.g., incorrect language, grammar problems, unnatural phrasing, or better word choices). You will receive batches of 50100 pairs via an online survey. The task takes approximately 12 hours and can be completed remotely at your convenience. You may skip any pair or stop at any time. Figure 11: Instructions provided to native-speaker annotators for verifying language correctness and target-language fidelity of synthetic questionanswer pairs. CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Language Medical correctness Language quality Amharic Bengali French Hausa Hindi Japanese Korean Spanish Swahili Thai Turkish Vietnamese Yoruba 4.45 4.92 5.00 4.96 5.00 4.96 5.00 5.00 5.00 4.70 4.60 4.95 5.00 4.45 4.96 5.00 5.00 4.92 4.96 5.00 5.00 4.96 4.70 4.60 4.95 5.00 Table 5: Per-language human verification scores (15) from medical professionals (clinical correctness) and native speakers (language quality). Higher is better. Language Logic (Base) Logic (CURE-MED) Lang. (Base) Lang. (CURE-MED) Amharic Bengali French Hausa Hindi Japanese Korean Spanish Swahili Thai Turkish Vietnamese Yoruba 0.95 10.00 67.86 5.06 4.48 68.57 41.33 62.86 0.00 51.02 12.50 66.67 0.00 17.14 60.00 77.86 43.04 48.51 77.14 52.00 72.38 35.71 59.18 43.75 70.48 40.86 +16.19 +50.00 +10.00 +37.98 +44.03 +8.57 +10.67 +9.52 +35.71 +8.16 +31.25 +3.81 +40.86 0.00 2.14 71.43 0.00 5.97 60.00 26.67 60.95 0.00 37.76 3.57 61.90 0. 64.76 91.43 96.43 77.22 90.30 94.29 84.00 96.19 67.14 86.73 75.89 94.29 77.42 +64.76 +89.29 +25.00 +77.22 +84.33 +34.29 +57.33 +35.24 +67.14 +48.97 +72.32 +32.39 +77.42 Table 6: Per-language performance of Qwen2.5-7B-Instruct (Base) and the CURE-MED 7B variant on CUREMEDBENCH. We report logical correctness and language accuracy, along with absolute gains (CURE-MED Base). E.1 Per-Language Results for Qwen2.5-7B Table 6 reports per-language performance for the Qwen2.5-7B-Instruct baseline and its CURE-MED variant. Across all 13 languages, CURE-MED substantially improves both logical accuracy and language consistency. Gains are especially large in low-resource languages such as Amharic, Hausa, Swahili, and Yoruba, where the baseline frequently fails to produce correct or language-faithful responses. In higher-resource languages such as French, Japanese, and Spanish, CURE-MED yields more moderate but consistent improvements, indicating that GRPO-guided curriculum RL enhances reasoning robustness without degrading performance in well-resourced settings. Overall, these results show that CURE-MED improves multilingual medical reasoning uniformly while significantly narrowing performance disparities across languages. E.2 Per-Language Results for Qwen2.5-3B Table 7 shows that CURE-MED consistently improves the 3B model across all evaluated languages in both logical correctness and language accuracy. The baseline 3B model exhibits extremely low performance for several languages, including Amharic, Hausa, Swahili, and Turkish, whereas the CURE-MED variant achieves large absolute gains, often exceeding 4080 percentage points. Even in languages where the base model is already relatively stronger, such as French, Japanese, Spanish, and Vietnamese, CURE-MED delivers clear and reliable improvements. These results demonstrate that curriculum-guided reinforcement is particularly effective for small models, enabling robust multilingual medical reasoning despite limited model capacity. 18 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Language Logic (Base) Logic (CURE-MED) Lang. (Base) Lang. (CURE-MED) Amharic Bengali French Hausa Hindi Japanese Korean Spanish Swahili Thai Turkish Vietnamese Yoruba 0.95 2.86 12.14 2.53 5.97 23.81 8.00 17.14 0.00 10.20 1.79 44.76 6. 14.29 55.71 70.71 27.85 28.36 62.86 36.00 62.86 17.86 58.16 28.57 69.52 17.20 +13.34 +52.85 +58.57 +25.32 +22.39 +39.05 +28.00 +45.72 +17.86 +47.96 +26.78 +24.76 +10.75 0.00 0.00 22.14 0.00 0.00 26.67 2.67 23.81 0.00 0.00 0.00 79.05 0.00 40.95 85.00 95.71 64.56 83.58 89.52 76.00 94.29 51.43 73.47 53.57 80.00 69.89 +40.95 +85.00 +73.57 +64.56 +83.58 +62.85 +73.33 +70.48 +51.43 +73.47 +53.57 +0.95 +69.89 Table 7: Per-language performance of the 3B Base model and its CURE-MED variant on CUREMED-BENCH. We report logical correctness and language accuracy, along with absolute gains (CURE-MED Base). Model Lang. Consistency () Logical Acc. () GPT-5-nano GPT-5-mini Gemini 2.5 Flash Gemini 2.5 Pro Claude 3 Haiku 69.11 75.33 48.01 4.33 93.43 73.24 80.57 54.79 10.62 73.31 Table 8: Inference-only performance of proprietary models on CUREMED-BENCH (averaged across 13 languages). E.3 Proprietary Model Performance on CUREMED-BENCH Table 8 summarizes inference-only performance of frontier models on CUREMED-BENCH, reporting language consistency and logical accuracy averaged over 13 languages. While some models maintain strong target-language adherence (e.g., Claude 3 Haiku), results reveal substantial brittleness: GPT-5-nano exhibits notably weaker language consistency, and the Gemini 2.5 family degrades sharply in both language control and reasoning quality (with Gemini 2.5 Pro nearly collapsing). These averages also conceal larger failures in low-resource languages, where models more frequently drift from the target language and show steeper drops in logical accuracy (see Appendix E.4). Overall, CUREMED-BENCH exposes reliability gap for proprietary LLMs: strong performance in some settings does not ensure robust multilingual reasoning or consistent target-language adherence. E.4 Per-language Performance of Closed-source Models We analyze proprietary models on CUREMED-BENCH at the per-language level using logical accuracy  (Table 9)  and language consistency  (Table 10)  . Even among the five evaluated systems (GPT-5-nano, GPT-5-mini, Gemini 2.5 Flash/Pro, and Claude 3 Haiku), strong aggregate scores mask substantial cross-lingual brittleness. Across higherresource languages, performance is comparatively stable: French and Spanish achieve high logical accuracy (often 90%) and strong language adherence, and we observe similarly consistent behavior in Japanese, Korean, Thai, Turkish, and Vietnamese, where language consistency typically remains high alongside solid reasoning performance. In contrast, low-resource languages expose clear failure modes. Amharic exhibits severe target-language breakdown for several models (e.g., GPT-5-nano and Gemini 2.5 Flash/Pro), where language consistency collapses despite non-trivial logical accuracy for some settings; Claude 3 Haiku is more robust, maintaining high language adherence and stronger accuracy. Hausa shows different dissociation: multiple models drift from the target language even when logical accuracy remains moderate to high, indicating that medical reasoning does not imply reliable language control under inference-only prompting. Yoruba is the most challenging overall: language adherence is often low (notably for GPT-5 and Gemini 2.5 variants), and logical accuracy drops sharply across models, revealing compounding failures in both reasoning and language control. Overall, these results underscore persistent reliability gap in proprietary LLMs and motivate evaluating multilingual medical reasoning with joint measures of correctness and target-language fidelity. 19 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Language GPT-5-nano GPT-5-mini Gemini 2.5 Flash Gemini 2.5 Pro Claude 3 Haiku Amharic Bengali French Hausa Hindi Japanese Korean Spanish Swahili Thai Turkish Vietnamese Yoruba 5.71 65.00 89.29 78.48 78.36 84.76 78.67 89.52 84.29 85.71 79.46 88.57 35.48 41.90 73.57 93.57 89.87 78.36 84.76 80.00 94.29 86.43 90.82 84.82 88.57 56.99 24.76 38.57 75.71 43.04 62.69 69.52 62.67 74.29 48.57 64.29 53.57 63.81 25.81 0.95 9.29 23.57 3.80 14.93 13.33 6.67 18.10 7.14 6.12 6.25 18.10 2.15 70.48 62.86 90.71 55.70 76.12 87.62 73.33 88.57 77.14 75.51 70.54 84.76 25. Table 9: Logical accuracy (%) of proprietary models on CUREMED-BENCH across 13 languages under inference-only prompting. We report accuracy against the single ground-truth answer. Language GPT-5-nano GPT-5-mini Gemini 2.5 Flash Gemini 2.5 Pro Claude 3 Haiku Amharic Bengali French Hausa Hindi Japanese Korean Spanish Swahili Thai Turkish Vietnamese Yoruba 1.90 39.29 92.86 56.96 53.73 80.00 92.00 97.14 82.86 94.90 90.18 89.52 27.96 24.76 65.71 98.57 43.04 73.88 88.57 88.00 98.10 77.86 88.78 93.75 91.43 32.26 12.38 34.29 67.86 35.44 58.96 56.19 56.00 71.43 32.86 59.18 52.68 60.00 23.66 1.90 1.43 5.00 1.27 8.21 5.71 4.00 5.71 2.86 9.18 7.14 0.95 2. 92.38 95.71 98.57 73.42 97.76 96.19 97.33 91.43 98.57 97.96 96.43 99.05 67.74 Table 10: Language consistency (%) of proprietary models on CUREMED-BENCH across 13 languages under inferenceonly prompting. We report the fraction of outputs that adhere to the requested target language. Model Qwen2.5-1.5B CURE-MED Qwen2.5-3B CURE-MED Qwen2.5-7B CURE-MED Qwen2.5-14B CURE-MED Qwen2.5-32B CURE-MED English 1.40 44.80 24.8 48.00 54.40 53.60 61.60 66.40 72.80 72.20 French 6.40 47.20 12.00 50.60 44.00 56.80 54.40 64.40 73.60 73.00 Italian 4.80 24.00 13.60 36.80 34.40 47.20 46.40 64.80 64.80 72. Spanish 6.80 32.80 13.60 48.80 48.00 57.60 60.00 68.00 70.40 76.20 Table 11: OOD accuracy on MedExpQA across four languages. CURE-MED improves reasoning performance across model sizes, showing cross-lingual generalization to unseen medical questions and languages. 20 CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning Model Qwen2.5-1.5B CURE-MED Qwen2.5-3B CURE-MED Qwen2.5-7B CURE-MED Qwen2.5-14B CURE-MED Qwen2.5-32B CURE-MED English 18.50 37.80 32.50 41.00 50.50 51.50 56.00 59.50 63.00 64. Simplified Chinese 21.00 59.50 55.00 68.00 73.00 70.00 80.50 75.00 84.00 81.00 Traditional Chinese 16.00 47.50 36.00 54.00 60.00 57.00 69.50 70.00 71.00 76.00 Table 12: OOD accuracy on MedQA across English and Chinese. CURE-MED improves reasoning performance across model sizes, demonstrating robustness across unseen languages."
        }
    ],
    "affiliations": [
        "IIT-Patna",
        "MBZUAI",
        "University of Virginia"
    ]
}