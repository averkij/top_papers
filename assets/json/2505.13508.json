{
    "paper_title": "Time-R1: Towards Comprehensive Temporal Reasoning in LLMs",
    "authors": [
        "Zijia Liu",
        "Peixuan Han",
        "Haofei Yu",
        "Haoru Li",
        "Jiaxuan You"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \\textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \\textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \\textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \\textit{Time-R1} checkpoints."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 8 0 5 3 1 . 5 0 5 2 : r Time-R1: Towards Comprehensive Temporal Reasoning in LLMs Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign {zliu331,jiaxuan}@illinois.edu"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce Time-R1, the first framework to endow moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features novel three-stage development path; the first two constitute reinforcement learning (RL) curriculum driven by meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering practical and scalable path towards truly time-aware AI. To foster further research, we also release Time-Bench, large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of Time-R1 checkpoints."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) have achieved remarkable success across spectrum of language understanding, generation, and even some complex reasoning tasks[13]. However, persistent shortcoming in even the most advanced LLMs is their temporal reasoning ability[4, 5]. This encompasses several key capacities[68]: accurately interpreting temporal relationships within their existing knowledge base (such as inferring event times, time differences, event order, and completing temporal entities), predicting the timing of future events based on learned patterns, and creatively generating plausible future events anchored in time. Studies have shown that most LLMs indeed struggle to update or contextualize knowledge under time constraints [9]; even frontier models have been observed to perform worse than some smaller models in tasks that require integrating new temporal information [10]. This suggests systemic weakness in how current LLMs grasp time. This weakness stems from multiple factors: architectural limitations [11], such as the lack of explicit 1Our code, the Time-Bench dataset, and Time-R1 model checkpoints are available at the project repository: https://github.com/ulab-uiuc/Time-R1 and via our Hugging Face Collection: https://huggingface. co/collections/ulab-ai/time-r1-682626aea47cb2b876285a16. Preprint. Under review. module representation of time; the static nature of their training corpora [12], which inevitably become outdated; and the non-chronological training process [13], where temporal information across different periods is processed concurrently rather than sequentially, hindering the development of robust logical mappings between events and their corresponding times. While existing research aims to enhance temporal reasoningfor instance, Zhao et al. [13] aligned LLM knowledge to target times, Kim et al. [9] improved temporal consistency, and Yuan et al. [5] focused on future event prediction, with other works exploring representation methods [14, 15]these efforts often target isolated skills. They typically fall short of endowing LLMs with unified, comprehensive temporal intelligence that spans past understanding, future prediction, and creative, time-anchored generation, especially for events beyond their knowledge cutoffs [13, 5]. Figure 1: Generated outputs from Time-R1 showcasing its capabilities. (Left) Future Event Time Prediction (Stage 2). (Right) Creative Scenario Generation (Stage 3), with output compared to real-world headline. In this paper, we aim to bridge this gap by equipping single 3B-parameter model with comprehensive temporal reasoning capabilities through multi-stage Reinforcement Learning (RL), which has become powerful framework for improving LLM reasoning. Recent frontior models such as OpenAI-o1 [16] and DeepSeek-R1 [17] utilize RL methods like PPO [18] and GRPO [19], proving effectiveness to learn complex reasoning capabilities, such as mathematical problem solving and multi-step logical deduction. We build upon Qwen2.5-3B-Instruct, moderate-sized LLM, and demonstrate that through specialized training it can surpass models over 200 larger (for instance, DeepSeek-R1, 671B-parameter model) on highly challenging temporal prediction and generation tasks. We propose three-stage framework with RL and dynamic rewards to progressively establish the models unified temporal capabilities, spanning temporal logic, future prediction, and time-anchored scenario generation: (1) Stage 1 - Comprehension: RL fine-tune the model using pre-cutoff data from cold start on four fundamental temporal tasks timestamp inference, timedifference estimation, events ordering, and masked time entity completion to develop powerful logical mappings between events and their corresponding times. (2) Stage 2 - Prediction: Further train the model to predict events occurring after knowledge cutoff, thereby teaching it to utilize general reasoning ability built in Stage 1 to extrapolate trends and anticipate future outcomes. (3) Stage 3 - Generation: Directly have the model generate logical future scenario without fine-tuning, leveraging the capabilities obtained from the first two stages. Through this staged curriculum, the LLM thus progresses from comprehending known temporal facts to skillfully navigating the complexities of the future. This advanced training culminates in robust capabilities for both predicting future event timelines and creatively generating plausible scenarios for unseen future contextsaddressing significant limitations in how current AI handles such challenging forward-looking tasks. Illustrative examples of these advanced future-oriented skills, such as Time-R1s proficiency in forecasting event dates and generating contextually appropriate news headlines for future dates (as depicted in Figure 1), highlight the practical efficacy of our approach. In summary, the key contributions of our work are as follows: (1) Unified Temporal Reasoning in One Model: We introduce the first LLM that exhibits holistic temporal reasoning ability encompassing logic, prediction, and generation. (2) Small Model, Big Performance: We show that relatively small 3B model, when fine-tuned with our meticulously designed multi-stage dynamicreward RL strategy, can match or even exceed the performance of models with hundreds of billions of parameters (e.g., the 671B-parameter R1 model) on temporal prediction and generation tasks. (3) Fast Adaptability and Cost Efficiency: Our approach demonstrates that temporal knowledge can be continuously refreshed in cost-effective manner. 3B model can be quickly fine-tuned on new data as time progresses, which is infeasible for hundreds of billion model that would require enormous computational resources (on the order of millions of dollars for fine-tuning). (4) Resources for the Community: To encourage further research in temporal-aware AI, we release Time-Bench, dataset of over 200,000 examples with explicit temporal annotations covering diverse tasks including timestamp inference, time-gap estimation, event ordering, and temporal entity completion. We also 2 release Time-R1, series of high-performing and continuously updatable temporal reasoning model checkpoints, offering strong foundation for future time-aware LLM development and iterative refinement."
        },
        {
            "title": "2 Related Work",
            "content": "Temporal Reasoning in LLMs. While adept at many complex tasks [17, 20], LLMs struggle significantly with temporal reasoningunderstanding time and event interrelationsa faculty crucial for comprehensive world understanding and interaction [4, 21, 6]. Recent studies increasingly target these deficiencies, often focusing on specific temporal facets. For example, some efforts aim to improve temporal accuracy by aligning LLM knowledge with target time for time-sensitive questions [13]. Meantime, some investigate methods for better integrating temporal information into model representations [14], while others explore leveraging external knowledge sources or structured representations like temporal graphs to augment LLM capabilities [15]. However, LLMs exhibit particularly poor generalization when reasoning about the future, especially for events beyond their knowledge cutoff or tasks requiring creative foresight. Consequently, robust methods for direct, challenging future event prediction or creative scenario generation remain scarce in the literature. While some initiatives explore future event prediction and forecasting (e.g., Yuan et al. [5] employed instruction tuning to predict event occurrences from past contexts), comprehensive approaches addressing the full spectrum of complex and creative future-oriented reasoning are largely underdeveloped. Reinforcement Learning in LLMs. Reinforcement learning (RL) has recently attracted attention due to its scalability and enhanced generalization capabilities. Building on policy optimization algorithms like PPO [18], reinforcement learning from human feedback (RLHF) the first application of RL to large language models has become standard paradigm for aligning LLMs with desired behaviors [22, 23]. Recent advances aim to simplify or improve this process: Direct Preference Optimization (DPO) [24] and Simple Preference Optimization (SimPO) [25] replace the conventional RL loop with more direct optimization of preference-based rewards, eliminating the need for separate reward model or reference policy. Other methods are tailored specifically for LLMs; for instance, Group Regularized Policy Optimization (GRPO) [19] introduces group-based reward formulation in place of single critic, achieving more stable training and better generalization. Likewise, Ahmadian et al. [26] revisit classic policy gradient techniques [27] to propose RLOO (REINFORCE-LeaveOne-Out), an online RL algorithm that refines LLM policies with reduced variance and cost. These RL-driven approaches have demonstrated notable gains in LLM reasoning capabilities. In particular, GRPO and related strategies have yielded state-of-the-art performance on complex reasoning tasks including mathematical problem solving [19, 28], search engine interaction and knowledge retrieval [29, 30], code generation tasks [31] and others [3234]. Despite these successes, the application of reinforcement learning to temporally-grounded reasoning remains underexplored. This gap suggests an opportunity to leverage RL methods to develop unified, time-sensitive reasoning abilities in future LLMs."
        },
        {
            "title": "3 Method",
            "content": "This section details the Time-R1 methodology for enhancing LLM temporal capabilities via Reinforcement Learning (RL) fine-tuning. We introduce novel three-stage training framework (Section 3.2) guided by dynamic, rule-based reward system (Section 3.3). We first outline the underlying RL optimization setup using Group Relative Policy Optimization (GRPO) (Section 3.1) before detailing these core framework and reward components. 3.1 Reinforcement Learning Fine-tuning for Temporal Reasoning Our approach employs reinforcement learning (RL) to fine-tune Large Language Model (LLM) for complex temporal reasoning tasks. The core process involves interaction between the LLM policy and rule-based environment. Given prompt detailing specific temporal task, the LLM, parameterized by θ, generates an output sequence autoregressively according to its current policy πθ(y x) = (cid:81)y t=1 πθ(yt x, y<t). 3 Figure 2: Overview of the Time-R1 framework. The process consists of three stages: (a) Stage 1 establishes foundational understanding by fine-tuning base LLM on historical data across four temporal subtasks, driven by reinforcement learning (GRPO) and dynamic reward system, resulting in model θ1. (b) Stage 2 trains θ1 for future event time prediction using post-cutoff data and rule-based reward, producing θ2. (c) Stage 3 leverages θ2 for inference-based creative future scenario generation, followed by evaluation, without further RL. Structured Generation Process. To facilitate complex reasoning, interpretability and structured output, we guide the model generation process. For all tasks, the LLM is prompted using specific templates incorporating system instructions (i.e., instructing the model to reason first: You are helpful assistant. You first think about the reasoning process in your mind and then provide the user with the answer.) to generate its reasoning within <think>...</think> tags, followed by the final answer within <answer>...</answer> tags. The entire generated sequence y, encompassing both thought and answer components, constitutes the output evaluated by the environment. Policy Optimization using GRPO. The environment evaluates the output using task-specific dynamic reward function R(x, y) (detailed in Section 3.3). To optimize the policy parameters θ, we utilize Group Relative Policy Optimization (GRPO) [19]. key challenge in RL fine-tuning of LLMs is the high variance often associated with policy gradient estimates[35]. GRPO addresses this by calculating the advantage of generated response relative to other responses sampled for the same input prompt, thereby providing more stable learning signal without requiring an auxiliary value function. Specifically, for given prompt x, we first sample batch of responses {yk}K k=1 using reference policy πref (typically the policy before the update step). After computing the reward R(x, yk) for each response, the group-normalized advantage ˆA(x, yk) for response yk is calculated as: ˆA(x, yk) = R(x, yk) b(x), where b(x) = 1 (cid:88) j= R(x, yj). (1) This advantage estimate ˆA(x, yk) reflects the relative quality of response yk compared to the average performance within its group. To update the policy πθ stably using this advantage, we employ clipped surrogate objective function, similar in structure to that used in PPO [18], which helps prevent large, detrimental policy updates. Let the probability ratio be rk(θ) = πθ(ykx) πref(ykx) . The per-sample clipped objective term is: LCLIP (θ) = min (cid:16) rk(θ) ˆA(x, yk), clip (rk(θ), 1 ϵ, 1 + ϵ) ˆA(x, yk) (cid:17) (2) where ϵ is the clipping hyperparameter. The overall objective function JGRPO(θ) maximized during training balances the expected clipped advantage with KL-divergence penalty against the reference policy πref: max θ JGRPO(θ) = ExD,{yk}πref [ 1 (cid:88) k=1 LCLIP (θ)] β ExDDKL[πθ( x) πref( x)], (3) where is the training dataset union, β controls the KL penalty strength, DKL is the KullbackLeibler divergence, and πref is the stage-specific frozen reference policy (initialized from Qwen2.5-3B-Instruct for Stage 1) used for both advantage calculation reference and KL regularization. This objective guides the policy towards higher rewards, leveraging the stable GRPO advantage estimates within constrained optimization framework. 4 3.2 Time-R1: Three-Stage Temporal Learning Framework To empirically evaluate the effectiveness of our proposed methodology (outlined in Section 3.1), we designed comprehensive three-stage experimental procedure to train Time-R1, as shown in Figure 2. This staged approach aims to progressively cultivate sophisticated temporal logic, prediction, and generation capabilities within the Large Language Model (LLM). We detail each stage below. 3.2.1 Stage 1 - Comprehension: Foundational Temporal Understanding via RL Fine-tuning Objective. The primary goal of this initial stage is to establish robust foundation for temporal comprehension within the LLM. We aim to instill the ability to interpret fundamental temporal relationships between events and their corresponding times by fine-tuning the model using historical news data from before its knowledge cutoff date. Dataset. We construct specialized dataset derived from large corpus of New York Times (NYT) news articles [36] (over 200,000) spanning eight years, from January 2016 to December 2023. We extract the headline and abstract of the news article to represent each event E, i.e., = (h, a). Details can be found in Appendix B.1. Subtasks. From this corpus, we curate data instances tailored to four specific and fundamental temporally-focused and logic-based subtasks [37, 38]: (1) Timestamp Inference: Infer the specific date (e.g., 2023-12) associated with described event E. (2) Time-Difference Estimation: Estimate the temporal gap (e.g., 14 months) between two described events, E1 and E2. (3) Event Ordering: Determine the correct chronological sequence (e.g., Event order: 2-1-3) of three events E1, E2 and E3 presented out of order. (4) Masked Time Entity Completion: Fill in masked temporal expression Me (i.e., <Year> and <Month>) within given event description E. In order to help the model develop general logic and indeed acquire the skill to accurately map events to their respective times from textual clues, we force the model to infer each events date first and then give task-specific answer for every subtask except the first. Both would be judged score that would then serve as part of the reward (see Section 3.3). Consequently, this prevents the model from merely guessing the final answer implicitly. For instance, for the Masked Time Entity Completion task, success hinges on the models ability to discern detailed semantics from the surrounding text. This is crucial because the specific temporal entity to be completed often refers to time distinct from the primary date of the event itself, thus pushing the model beyond simple date extraction towards deeper contextual understanding to answer both correctly. By mastering these diverse subtasks, the LLM (i.e., model checkpoint, denoted θ1) builds robust foundational temporal understanding. 3.2.2 Stage 2 - Prediction: Future Event Time Prediction via RL Fine-tuning Objective. After obtaining the foundational capabilities developed in Stage 1, the objective of Stage 2 is to further train the model to predict the timing of future events occurring after its initial knowledge cutoff (2023). This involves teaching the model to recall relevant and similar events in the past and their occurrence dates, extrapolate learned temporal development patterns and anticipate future event occurrences based on emerging, post-cutoff information. Dataset. For Stage 2, the training dataset, denoted D(2) train, is meticulously constructed to facilitate fair evaluation and strictly prevent data leakage from the test period. To ensure level playing field and align with the knowledge cutoff of the latest baseline models (e.g., DeepSeek-V3-0324-671B with knowledge cutoff in July 2024), we first incorporate real news data. Specifically, we include corpus of 7,000 real news articles from January 2024 to July 2024. To train for predicting events beyond this cutoff (August 2024 - February 2025) without using real data from this period, we employ data synthesis strategy. The synthetic dataset, created using the DeepSeek-V3 model informed by news from May to July 2024, constitutes approximately only half the volume of the real news data used for the earlier months. This approach of using exclusively synthetic data for the future period is deliberate measure to strictly avoid any potential data leakage, as the test dataset D(2) test is real news events from this period (August 2024 - February 2025). Further details about the datasets can be found in Appendix B.2. Task. In this stage, the model predicts the specific date for news event based on its extracted headline and abstract a. 5 Initializing the model with the checkpoint θ1 obtained from Stage 1, we continue the fine-tuning process using GRPO on post-cutoff news while carefully controlling the information availability to simulate true future prediction scenario. After training, this stage addresses the challenge that LLMs normally cannot generalize to events post-training [39] and results in another model checkpoint, θ2, specialized in future event time prediction. 3.2.3 Stage 3 - Generation: Creative Future Scenario Generation and Evaluation Objective. In the final stage, we pivot from training to application aiming to leverage the logical and predictive capabilities instilled in Stages 1 and 2 to enable the fine-tuned model to directly generate plausible, diverse, and temporally coherent future scenarios. This moves beyond predicting specific event times to creatively generating descriptions of hypothetical events given specific future date. Methodology. This stage utilizes the model checkpoint θ2, obtained from Stage 2, exclusively for inference without any further RL fine-tuning. The process involves three sequential steps: future news generation, diversity-based filtering, and plausibility evaluation against real news. First, the model generates hypothesized news events for specified future months (i.e., July 2024 onwards). To ensure comprehensive topical coverage, generation is conditioned on = 8 common and distinct themes τ (e.g., Foreign Affairs, Business, Technology, Politics). To enhance the richness of the output pool, each prompt asks the model to create multiple unique news (i.e., 3). This process results in raw set of generated news items Graw including each month and theme τ . Second, to curate varied and non-redundant set of scenarios for evaluation, diversity filtering process is applied to the raw generated articles Graw. We compute semantic embeddings R384 for each generated item using all-MiniLM-L6-v2 encoder [40], which retains excellent semantic capture capabilities through knowledge distillation from larger models [41]. Within each theme τ and month m, greedy selection algorithm iteratively constructs diverse subset. This filtering yields curated set Gfilt,m containing Ndiv = 5 high-diversity news items per theme per month, totaling Ng = Ndiv = 40 representative generated scenarios for each month m. Finally, the realism and plausibility of the generated future scenarios are quantified through comparison with actual news events from the corresponding future months. The ground truth consists of real news events from the held-out test dataset D(2) test , partitioned by month into sets Dreal,m. We compute semantic embeddings Ag for the filtered generated news items Gfilt,m and Br for the real news items Dreal,m, using the same all-MiniLM-L6-v2 model. The semantic relatedness between generated item Ag and real item Br is measured using cosine similarity: sim(Ag, Br) = cos(ϕ) = AgBr AgBr , where ϕ represents the angle between the 384-dimensional embedding vectors. To assess overall plausibility for given month m, we calculate the Average Maximum Similarity (AvgMaxSim) score. For each generated news item Ag,i (i = 1, . . . , Ng), we find its maximum similarity to any real news item in that month, maxBrDreal,m sim(Ag,i, Br). The AvgMaxSim score is the average of these maximum similarity values across all Ng generated items: AvgMaxSimm = 1 Ng Ng (cid:88) (cid:18) i=1 max BrDreal,m sim(Ag,i, Br) (cid:19) (4) This metric quantifies, on average, how closely the generated plausible future events align semantically with events that actually transpired during that month. The process culminates in generating monthly AvgMaxSim reports and visualizations, facilitating quantitative comparisons against baseline generative models or ablations of our framework. In summary, Stage 3 serves as evidence for the generalization fostered by our first two stages RL framework. It reveals that the strong temporal grounding comprehension and predictive skills learned previously, combined with the LLMs innate linguistic abilities, readily and effectively generalize, allowing the model to anticipate future event dynamics and generate plausible, creative scenarios accordingly, without task-specific fine-tuning for this generative capability. 3.3 Reward Design meticulously engineered reward function, R(x, y), underpins the success of our Time-R1 framework. Its comprehensive and rigorous design, refined through iterative experimentation, has proven 6 critical for developing the nuanced temporal reasoning abilities observed in our model (see experimental validation in Section 4, detailed analysis in Section 5, and more illustration in Appendix). The reward function R(x, y) serves as the primary training signal guiding the policy optimization process outlined in Equation (3). We adopt rule-based dynamic reward system that assesses the correctness and quality of the models generated output given the prompt x. The final scalar reward R(x, y) [0.8, 1.1] incorporates several components: task-specific accuracy (Racc), format rewards (Rformat), and penalties (Ppenalty) for undesirable outputs, i.e., R(x, y) = Racc + Rformat Ppenalty (5) 3.3.1 Universal Bonuses and Penalties Design Output Parsing and Format. We first parse the content yans within the <answer>...</answer> tags. If yans is missing or contains explicit refusal terms like no event or none, penalty Pno_event is applied (i.e., Pno_event {0.1, 0.2} for Stage 1 tasks, and {0.2, 0.3} for Stage 2 prediction, depending on severity). Common Bonuses and Penalties. set of bonuses and penalties apply across tasks to encourage well-formed and concise outputs: Format Adherence Bonus (Rans_fmt): small bonus bf mt = 0.05 is awarded if the content yans adheres to the expected format for the specific task (e.g., YYYY-MM format for date inference, and specific structures for multi-part answers). Valid format is also prerequisite for accuracy scoring. Range: {0, 0.05}. Tag Structure Bonus (Rtags): Minor bonuses (btag = 0.025) are given for both the correct presence and count of structural tags (e.g., <think>, </answer>), incentivizing the chain-of-thought structure. Range: [0, 0.05]. Length and Repetition Penalty (Plen_rep): penalty is subtracted to discourage overly verbose or repetitive outputs; this mechanism has proven particularly effective in our empirical experiments (see cases in Appendices and F). Range: [0, 0.5]. Plen_rep = max( Plength, Prepetition) (6) where Plength penalizes responses (of tokens) exceeding length threshold Lthresh (i.e., 900 tokens) to prevent them from approaching the maximum allowed length Lmax (i.e., 1024 tokens). This is calculated as: Plength = min(1.0, Lthresh Lmax Lthresh ) 0.3, if > Lthresh Prepetition is the maximum of three distinct repetition penalties: Prepetition = max(Pword_repeat, Pphrase_repeat, Pngram_diversity) (7) (8) where Pword_repeat penalizes sequences of more than 5 identical consecutive words, Pphrase_repeat penalizes recurring phrases, and Pngram_diversity penalizes insufficient global n-gram diversity. The combined penalty Prepetition [0, 0.5]. 3.3.2 Task-Specific Accuracy Score. Accuracy score (Racc [0, 1]) is the core component of our reward mechanism, varying by task: Timestamp Inference: The task is to infer the date tp for given event E. Let tgt be the ground truth date. The accuracy score is based on the temporal distance m(tp, tgt) (in months) between the inference and target: Racc = Rdate(tp, tgt, α) = e(αm(tp,tgt)) (9) where α is decay coefficient. For Stage 1 inference, α is dynamically adjusted based on sample difficulty and training step (ranging between 0.07 and 0.1). This exponential reward structure, particularly when coupled with the dynamic α, ensures that the reward signal clearly reflects the proximity of the inferred date to the ground truth, effectively allowing the model to perceive the magnitude of its temporal error m(tp, tgt). See Section 3.3.3 and Section 5.1 for more discussion. 7 Time-Difference Estimation: The task is to infer the dates tp1, tp2 of two events and their difference tp (in months). Let ground truths be tgt1, tgt2, tgt. The reward combines accuracy on dates and the difference, weighted (wd = 0.25, wt = 0.5), and includes an inconsistency penalty: Racc = (wdRd1 + wdRd2 + wtRt) Pincon (10) where Rd1 = Rdate(tp1, tgt1, α1) and Rd2 = Rdate(tp2, tgt2, α2) are date accuracy, using dynamic α1, α2. Rt = e(αttptgt) denotes difference accuracy, where αt = 0.05 if tp 25, otherwise αt = 0.1 or (α1 + α2)/2 depending on the dynamic strategy process, to balance the reward and to encourage more robust estimation even when the model is dealing with events separated by large time differences. The inconsistency penalty factor (Pincon (0, 1]) penalizes discrepancies between the explicitly inferred difference tp and the difference implied by the inferred dates tp2 tp1; this penalty is designed to ensure the internal logical consistency of the models output. Let the error be incon = tp2 tp1 tp. Then Pincon = e(αinconincon), where the decay αincon is smaller for larger tp (base αincon = 0.1, scaled down if tp 25). The learning dynamics of Pincon, illustrating the models progressive adherence to this logical constraint, are presented in Appendix C. Event Ordering: The task involves inferring dates tp1, tp2, tp3 and the correct chronological order Cp (permutation) for three events E1, E2, E3. Let ground truths be tgt1, tgt2, tgt3, Cgt. The reward combines accuracy on dates and the order, weighted (wd = 0.2, word = 0.4), and includes both an inconsistency penalty and diversity penalty: Racc = (wd 3 (cid:88) i=1 Rdi + wordRorder) Pincon Pdiv (11) where Rdi = Rdate(tpi, tgti, αi) for = 1, 2, 3 is date accuracy, using dynamic αi. Rorder represents order accuracy, calculated based on the number of correctly ordered pairs in Cp compared to Cgt (i.e., Rorder = Ncorrect_pair/Ntotal_pair, where Ntotal_pair = 3). The inconsistency penalty factor (Pincon {0.2, 0.4, 0.7, 1.0}) penalizes if the inferred order Cp contradicts the order implied by the inferred dates tp1, tp2, tp3 (based on pairwise similarity), thereby ensuring the models explicit ordering aligns with the chronology of its inferred event dates. The diversity penalty factor (Pdiv {0.2, 1.0}) penalizes trivial solutions where all inferred dates tpi are identical, or where dates are sequential (e.g., tp3 tp2 = tp2 tp1 = 1) and the order is trivial (e.g., 1-2-3); this encourages the model to infer more varied and realistic event date distributions rather than collapsing to overly simplistic patterns. Pincon and Pdiv are both proven effective in empirical experiments (see Appendix C). Masked Time Entity Completion: The task is to infer the date tp of an event and masked entity Me_p (either Year or Month). Let ground truths be tgt, Me_gt. The reward combines accuracy on the date and the entity, weighted (wd = we = 0.5): Racc = wdRdate + weRentity (12) where Rentity = e(3αmc) denotes entity accuracy, using dynamic α. When the masked entity is Month, mc represents the circular difference of exact or variant month name to better capture the proximity, i.e., mc = min(Me_p Me_gt, 12 Me_p Me_gt). Future Event Prediction: Similar to the Timestamp Inference task but for future events, however, this task employs stricter evaluation standard as the model already has foundational temporal comprehension. Thus, the decay coefficient is fixed larger value (i.e., α = 0.1) in Equation (9), resulting in more severe penalties for prediction errors. 3.3.3 Dynamic Reward Mechanism To address the cold-start challenge inherent in fine-tuning LLMs for specialized temporal tasks and to foster robust performance[28], particularly on more difficult examples, we employ dynamic reward mechanism specifically during the Stage 1 RL fine-tuning process ( more discussion can be found at Section 5.1). This mechanism utilizes curriculum learning principles by adaptively adjusting the decay coefficient α used in the date accuracy reward component (Equation (9)) based on data difficulty and training progression. This dynamic adjustment applies whenever Rdate is calculated for any Stage 1 subtask involving date inference (i.e., all four subtasks). 8 First, we stratify the Stage 1 training dataset based on difficulty. Using an initial model checkpoint (i.e., Qwen2.5-3B-Instruct), we perform Timestamp Inference task for all training samples. Samples where the absolute error in months (m) is less than or equal to 3 (m 3) are classified as easy level, while the remainder are classified as normal/hard. The curriculum then proceeds in three sequential training steps, each building upon the model checkpoint from the previous step: Phase 1: Foundational Logic and Format Learning. Initially, fine-tuning focuses exclusively on the Timestamp Inference task using only the samples classified as easy. During this step, we employ fixed, relatively strict decay coefficient α = αtarget = 0.1 in Equation (9). The primary goal is to enable the model to rapidly learn the fundamental task logic, establish correct response formatting, and build solid foundation before encountering more complex tasks or difficult samples. Phase 2: Exploration on Full Task Suite. Next, training expands to encompass all four Stage 1 subtasks and utilizes the full dataset (easy, normal, hard samples). For samples classified as normal/hard, we apply lower, fixed decay coefficient α = αstart = 0.07. This more lenient penalty function encourages the model to explore diverse reasoning pathways for challenging instances across all tasks without being excessively penalized for initial inaccuracies. Easy samples continue to be evaluated using the stricter α = 0.1. Phase 3: Transition to Strict Evaluation. Finally, while continuing to train on all tasks and difficulty levels, we progressively increase the evaluation strictness for the normal/hard samples. The decay coefficient α for these samples transitions linearly from αstart = 0.07 up to αtarget = 0.1 over stransition = 50 steps within this training phase, after which it remains fixed at αtarget = 0.1 for any subsequent steps. Let be the current training step within this phase. The adaptive alpha αtransition(s) for normal/hard samples, is calculated as: αtransition(s) = αstart + (αtarget αstart) min(1.0, s/stransition) (13) This gradual tightening of the reward function encourages the model to refine its precision on more difficult examples, adapting it towards the stricter evaluation standard (α = 0.1). This step aims to cultivate high accuracy across the entire data distribution by the end of Stage 1. Importantly, this dynamic α adjustment schedule is employed strictly during the Stage 1 training process. For all evaluations performed on the test datasets (across all stages where applicable), we consistently use fixed decay coefficient α = 0.1 for all samples to ensure stable and comparable assessment of model performance. 3.3.4 Final Reward Calculation. In summary, the total score R(x, y) for given task is computed by summing the relevant accuracy score and bonuses, then subtracting penalties introduced above. Thus, Equation (5) can be further expressed as: R(x, y) = Racc + Rans_fmt + Rtags Pno_event Plen_rep (14) Aggregating the potential minimum and maximum values of these components yields range of [0.8, 1.1] for the total score R(x, y)."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Datasets. We utilize the datasets constructed from the New York Times (NYT) as described in Section 3.2. 4.2 Baselines To rigorously evaluate the performance of Time-R1, we compare it against two categories of six baseline models: (1) Instruction-Tuned LLMs of Varying Scales: Qwen2.5-3B-Instruct (the base model for Time-R1), Qwen2.5-7B-Instruct [42] and Llama-3.1-8B-Instruct [43] (mediumscale models), and DeepSeek-V3-0324-671B [44] (an extra-large generalist foundation model). (1) Specialized Reasoning LLMs: DeepSeek-Distill-Qwen-32B (a larger model with strong emphasis on reasoning), and DeepSeek-R1-671B [17] (recognized for its state-of-the-art performance on 9 wide array of complex reasoning benchmarks). This comparison helps determine whether advanced, broad reasoning skills on well-trained models even with exceptionally large-scale can inherently address complex temporal tasks. 4.3 Experimental Setup Implementation. All our experiments build upon Qwen2.5-3B-Instruct [42], moderate size for fast adaptability and cost efficiency. We implement our three-stage RL fine-tuning framework using veRL framework [45], adopting the GRPO algorithm detailed in Equation (3). All RL fine-tuning experiments were conducted on four NVIDIA A6000 GPUs. Hyperparameters. Key hyperparameters for the GRPO optimization include KL coefficient β = 0.001, and = 5 rollout responses per prompt for group-normalized advantage estimation. The full configuration details can be found at Appendix A. 4.4 Main Results We now present the core experimental results, evaluating the performance of Time-R1 across its training stages against the established baselines. We specifically report on the performance of the model checkpoint after Stage 1 (θ1) for foundational tasks and the checkpoint after Stage 2 (θ2) for future prediction and scenario generation. 4.4.1 Stage 1: Foundational Temporal Reasoning Performance Stage 1 fine-tuning effectiveness on core temporal understanding across four subtasks is presented in Table 1, showing the average total score achieved by Time-R1 (θ1) against baselines (see appendix for details of fine-tuning curves for all subtasks and phases.). The results highlight the substantial benefits of our Stage 1 RL fine-tuning. Time-R1 (θ1) demonstrates remarkable improvement in its overall average score, with an increase of approximately 153.0% over its base Qwen2.5-3B-Instruct model, even highly competitive to the 671B DeepSeek-R1 model with significantly smaller size, securing the second-best performance on the challenging Event Ordering and Completion tasks. This strong performance, rivaling or exceeding much larger baselines, is largely attributed to our meticulously designed task-specific rewards. For instance, the inconsistency and diversity penalties for Event Ordering (detailed in Section 3.3.2) are pivotal. The learning curves in Appendix also illustrate that the models adherence to response consistency and diversity for this task steadily improves, reflecting enhanced logical reasoning. Such effective instillation of logical mapping allows Time-R1 to compete effectively with much larger models on these complex temporal logic challenges. Performance on the Inference task also shows considerable gains, with Time-R1 (θ1) clearly outperforming the ten times larger, specifically fine-tuned DeepSeek-Qwen-32B model. However, for Time-Difference estimation, despite significant improvement of around 76.4% over the Llama3.1-8B-Instruct model, gap persists compared to top-tier baselines. This may be partly due to our base models lack of specialized mathematical reasoning pre-training, common strength in larger, math-focused models [46, 47] (further discussed in Appendix G). Table 1: Stage 1 Foundational Temporal Reasoning Performance. Average Total Score (R(x, y)) on the four subtasks and overall. Higher scores indicate better performance. Best score in each column is bold, second best is underlined. Model Overall Avg. Ordering Completion Inference Difference Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct DeepSeek-Distill-Qwen-32B DeepSeek-V3-0324-671B DeepSeek-R1-671B Time-R1 (θ1, 3B) 0.2384 0.3092 0.2492 0.4702 0.6471 0.6916 0.6031 0.1583 0.2775 0.2239 0.5026 0.6409 0.6848 0. 0.2217 0.2953 0.2008 0.3943 0.6777 0.7493 0.7291 0.3372 0.3366 0.3339 0.5264 0.6796 0.7145 0.5857 0.2363 0.3275 0.2383 0.4576 0.5901 0.6172 0. 4.4.2 Stage 2: Future Event Time Prediction Stage 2 equips models to predict event timing post-knowledge cutoff (2023). We assess our full pipeline and Stage 1s impact by evaluating two variants: Time-R1 (θ2, 10 from base Qwen2.5-3B-Instruct, omitting Stage 1). 3B) (full curriculum, Section 3.2) and an ablation model, Time-R1-S2-Direct (θ 2, 3B) (Stage 2 fine-tuning only, Performance is compared against baselines for August 2024 - February 2025 predictions. The overall Stage 2 performance, measured by Average Total Score R(x, y) with strict evaluation (α = 0.1), is presented in Table 2. While models show clear improvement over Stage 1 Inference tasks, likely aided by narrower prediction time span, further significant gains prove challenging. For instance, the DS-Qwen-32B model, despite its scale and specialized complex reasoning training, scores lower than some 3B models lacking such enhancements (e.g., the base Qwen2.5-3B-Instruct), underscoring the inherent difficulty of learning extrapolation and handling post-cutoff data.Our primary model, Time-R1 (θ2, 3B), achieves the highest score. This strong performance, consistent across the prediction horizon (Figure 3), shows it generally outperforming most baselines, including the much larger DeepSeek-R1-671B and DeepSeekV3-671B models. This robust result strongly supports our hypothesis that specialized, staged temporal fine-tuning enables smaller models to achieve superior performance on challenging future prediction tasks. Furthermore, these findings highlight general LLM weaknesses in temporal reasoning and underscore the efficacy and necessity of our structured training framework. The foundational understanding from Stage 1, combined with Stage 2s predictive skill development, underpins this strong near-future temporal reasoning (see Section 5.3 for challenges facing standard LLMs). The ablation model, Time-R1-S2-Direct (θ 2, 3B), also demonstrates solid performance, outperforming several baselines and indicating Stage 2 RL fine-tunings standalone effectiveness. See more discussion on Section 4.5. Figure 3: Monthly Avg. Total Score R(x, y) for Stage 2 Future Event Prediction (August 2024 - Feb 2025). Compares Time-R1 variants (θ2 and θ 2) against baselines. Evaluated with α = 0.1. Table 2: Stage 2 Future Event Prediction Performance (Overall). Average Total Score R(x, y) evaluated with α = 0.1. Higher scores are better. Best score is bold, second best is underlined. θ2 checkpoint of Time-R1 is used. Metric Qwen2.5 Qwen2.5 Llama-3.1 DS-Qwen DS-V3 DS-R1 Time-R1 Time-R1 (θ2, 3B) 0.7697 Avg. Total Score 2, 3B) 0.7234 0.6015 0. 0.7036 0.7503 0.6226 0.6036 -671B -671B -32B -7B -3B -8B (θ 4.4.3 Stage 3: Creative Scenario Generation Quality Finally, we evaluate model generalization to generating plausible future scenariosa task without explicit fine-tuning. Table 3 presents AvgMaxSim scores, quantifying the semantic plausibility of generated news scenarios against real news events (August 2024 - February 2025). Results demonstrate Time-R1 (θ2, 3B)s strong generalization capability. It achieves the highest overall AvgMaxSim score, surpassing all baseline models, including the very large DeepSeek-V3-0324671B and DeepSeek-R1-671B. Monthly scores for Time-R1 (θ2, 3B) also reveal consistently strong performance. This Stage 3 success, achieved without direct training on generation, underscores the S1+S2 curriculums effectiveness in building robust, transferable temporal reasoning. These capabilities are significant for addressing research gaps in challenging future prediction and generation tasks and demonstrate practical application value. Our ablation model, Time-R1-S2-Direct (θ 2, 3B), also performs commendably, outperforming some baselines (further discussion in Section 4.5). 4.5 Ablation Study on Staged Curriculum Learning To quantify the impact of our staged curriculum, particularly the foundational comprehension from Stage 1, we compared our full model, Time-R1 (θ2, 3B) (S1+S2 training), against Time-R1-S2-Direct (θ 2, 3B) (S2 training only). 11 Table 3: Stage 3 Creative Scenario Generation Plausibility (AvgMaxSim Scores (%)). Compares semantic similarity of generated scenarios to real news events (August 2024 - Feb 2025). Higher scores indicate better plausibility. Best overall average is bold, second best is underlined. Model Avg. (%) Monthly AvgMaxSim Scores (%) Qwen2.5-3B-Instruct Qwen2.5-7B-Instruct Llama-3.1-8B-Instruct DeepSeek-Distill-Qwen-32B DeepSeek-V3-0324-671B DeepSeek-R1-671B Time-R1-S2-Direct (θ Time-R1 (θ2, 3B) 2, 3B) 47.66 47.59 47.96 47.12 48.81 47.46 47.89 49.22 24-08 24-09 24-10 2424-12 25-01 25-02 47.27 46.99 48.99 46.58 50.73 47.55 46.79 48.10 46.89 49.78 50.03 46.78 51.77 49. 49.16 51.19 47.39 46.18 47.42 47.94 48.60 47.29 47.64 49.97 48.57 48.53 46.21 47.04 48.46 45.29 48.71 48.86 48.77 46.91 47.06 48.40 47.52 47. 48.25 50.16 47.76 48.88 48.01 47.30 47.71 47.30 48.01 49.29 46.94 45.83 48.03 45.81 46.85 47.31 46.65 46.95 Figure 4: Impact of the Dynamic Reward Curriculum. (a) Total Score R(x, y) on the Stage 1 Time-Difference Estimation task, comparing training \"With Dynamic Reward\" (blue, solid line) versus \"Without Dynamic Reward\" (red, dashed line, fixed α = 0.1). (b) Corresponding Average Response Length across all Stage 1 tasks for the same two training strategies. The y-axis for response length is broken to accommodate different scales. The dynamic reward approach not only achieves higher task scores but also promotes significantly more concise model outputs. The results unequivocally highlight the benefits of the full curriculum. In Future Event Time Prediction (Stage 2, Table 2, Figure 3), Time-R1 (θ2, 3B) (0.7697) significantly outperformed TimeR1-S2-Direct (θ 2, 3B) (0.7234). This advantage persisted in Stage 3 Creative Scenario Generation  (Table 3)  , with scores of 49.22% and 47.89% respectively. These consistent gains demonstrate that the temporal logic and event-time mapping skills instilled by Stage 1 are crucial for achieving superior predictive accuracy and generative plausibility, validating our progressive learning approach. Notably, Time-R1-S2-Direct (θ 2, 3B) still demonstrated commendable performance, surpassing several baselines and even the larger DeepSeek-V3-671B in Stage 2. This underscores the inherent effectiveness of our Stage 2 RL fine-tuning for enhancing temporal reasoning. However, the superior performance of Time-R1 (θ2, 3B) across both tasks confirms that the initial foundational stage is key to unlocking the models full potential, enabling more comprehensive development of temporal intelligence from fundamental understanding to advanced prediction and generalization."
        },
        {
            "title": "5 Discussion",
            "content": "This section delves into detailed analysis of our proposed methodology, focusing on the efficacy of the dynamic reward curriculum learning approach, the impact of our reasoning process on response length, and the challenges standard LLMs face in advanced temporal tasks. Our findings provide empirical evidence supporting the benefits of specialized training regimes for comprehensive temporal intelligence in LLMs. Additional discussion on implementation settings (e.g., KL loss coefficients), as well as more generated examples like those shown in Figure 1, is available in Appendices and E. 12 5.1 Dynamic Reward Curriculum Learning Our methodology (Section 3.3.3) employs dynamic reward mechanism during Stage 1 fine-tuning. This curriculum learning approach, with its phased adjustment of reward strictness (from lenient αstart = 0.07 to strict αtarget = 0.1), is designed to mitigate cold-start challenges and guide the model towards robust performance on complex temporal tasks. We hypothesized this would lead to superior learning compared to static, strict reward function. Figure 4(a) empirically validates this for the Stage 1 Time-Difference Estimation task. It compares the Total Score R(x, y) of our model trained with the full dynamic reward curriculum (\"With Dynamic Reward\") against baseline trained with fixed, stringent α = 0.1 from the outset (\"Without Dynamic Reward\"). The results clearly demonstrate the curriculums advantage: while the baseline models progress slows and converges to lower, unstable ceiling (score 0.395), the curriculum-trained model consistently surpasses it (around training step 70-80) and achieves significantly higher and more stable final score (approximately 0.420). This suggests that the curriculums initial leniency and gradual transition to stricter evaluation criteria enable more effective exploration and learning, leading to better mastery of the task. 5.2 Reasoning Process Matters, Not Just Response Length Developing effective LLMs requires not only accuracy but also efficient and concise responses. Unnecessarily long outputs can signify less refined reasoning process and increase computational overhead. Our investigation reveals that the dynamic reward curriculum, coupled with phased learning approach (Section 3.3.3), positively influences this aspect. We employ curriculum that progresses from simpler to more complex tasks and from lenient to stricter penalties, enabling the model to explore and establish more efficient and rational reasoning pathways. Figure 4 offers compelling comparison. While Figure 4(a) establishes that our dynamic reward strategy enhances task performance (higher Total Score R(x, y)), Figure 4(b) highlights the impact on average response length for Stage 1 tasks. striking difference emerges: the model trained \"With Dynamic Reward\" (blue line, bottom panel) consistently generates significantly shorter responses throughout training compared to the model \"Without Dynamic Reward\" (red line, top panel, note the broken y-axis). Specifically, the \"Without Dynamic Reward\" models responses average between approximately 240 to 320 tokens. In contrast, the \"With Dynamic Reward\" models outputs are far more concise, averaging between roughly 60 to 140 tokens, mostly remaining below 120 tokens post-initialization. This substantial reduction in length, alongside superior task performance, strongly suggests that our curriculum fosters more efficient and focused reasoning process. The model achieves better outcomes without verbose outputs, implying clearer, more direct approach to solving temporal tasks. Such conciseness is desirable, indicating refined understanding and potentially leading to more interpretable and computationally efficient inferences. 5.3 Challenges for Standard LLMs in Advanced Temporal Tasks Standard Large Language Models (LLMs), including state-of-the-art reasoning-focused variants, exhibit commendable performance on foundational temporal tasks within their knowledge cutoff (Stage 1, Table 1). This is often attributable to their large scale and extensive pre-training, which can include significant mathematical and logical reasoning data. However, their capabilities are substantially challenged when faced with more advanced temporal tasks requiring extrapolation and nuanced future-oriented generalization. Specifically, in Stage 2 Future Event Time Prediction (Table 2, Figure 3) and Stage 3 Creative Scenario Generation  (Table 3)  , even powerful baselines like DeepSeek-R1-671B are outperformed by our significantly smaller Time-R1 (θ2, 3B). For instance, Time-R1 (θ2, 3B) achieved leading score of 0.7697 in Stage 2 prediction (vs. DeepSeek-R1s 0.7503) and 49.22% in Stage 3 generation (vs. DeepSeek-V3s 48.81%). This disparity suggests that vast knowledge, large scale, or general reasoning prowess alone do not readily translate to proficiency in predicting future event timings or creatively generating plausible future scenarios. The relatively uniform and modest performance of baselines in Stage 3, in particular, highlights general weakness in current LLM training methodologies to effectively generalize to future-oriented generation tasks. 13 In contrast, the success of our three-stage RL framework with Time-R1 (θ2, 3B) is notable. It not only excels in prediction but also demonstrates remarkable generalization to creative future scenario generation without any explicit fine-tuning on this generative task itself. This underscores the efficacy and robustness of our method in instilling deeper, more transferable temporal understanding. These findings highlight the necessity for specialized training regimes like ours to cultivate comprehensive and practically useful temporal intelligence in LLMs."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduced Time-R1, 3B-parameter language model achieving comprehensive temporal reasoningspanning understanding, prediction, and creative generationthrough novel, meticulously engineered three-stage reinforcement learning curriculum with dynamic reward system. Strikingly, Time-R1 outperforms models over 200 times its size on challenging future event prediction and creative scenario generation tasks, exhibiting robust generalization to the latter even without task-specific fine-tuning. This success directly addresses critical research gap concerning complex future-oriented tasks and demonstrates that our sophisticated, progressive RL approach enables smaller, efficient models to achieve superior temporal performance, offering practical, scalable path towards truly time-aware AI with substantial application potential. To foster further research and development, we release our Time-Bench dataset and Time-R1 model checkpoints, envisioning future work on scalability and enhanced reasoning integration."
        },
        {
            "title": "References",
            "content": "[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [3] Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip HS Torr, Fahad Shahbaz Khan, and Salman Khan. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321, 2025. [4] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, and Bing Qin. Timebench: comprehensive evaluation of temporal reasoning abilities in large language models. arXiv preprint arXiv:2311.17667, 2023. [5] Chenhan Yuan, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. Back to the future: Towards explainable temporal reasoning with large language models. In Proceedings of the ACM Web Conference 2024, pages 19631974, 2024. [6] Ashutosh Bajpai, Aaryan Goyal, Atif Anwer, and Tanmoy Chakraborty. Temporally consistent factuality probing for large language models. arXiv preprint arXiv:2409.14065, 2024. [7] Ben Zhou, Kyle Richardson, Qiang Ning, Tushar Khot, Ashish Sabharwal, and Dan Roth. Temporal reasoning on implicit events from distant supervision. arXiv preprint arXiv:2010.12753, 2020. [8] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Zefang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li, Nicholas Sukiennik, et al. Understanding world or predicting future? comprehensive survey of world models. arXiv preprint arXiv:2411.14499, 2024. [9] Jongho Kim and Seung-won Hwang. Counterfactual-consistency prompting for relative temporal understanding in large language models. arXiv preprint arXiv:2502.11425, 2025. [10] Xin Wu, Yuqi Bu, Yi Cai, and Tao Wang. Updating large language models memories with time constraints. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 1369313702, 2024. 14 [11] Kai Nylund, Suchin Gururangan, and Noah Smith. Time is encoded in the weights of finetuned language models. arXiv preprint arXiv:2312.13401, 2023. [12] Jack Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. [13] Bowen Zhao, Zander Brumbaugh, Yizhong Wang, Hannaneh Hajishirzi, and Noah Smith. Set the clock: Temporal alignment of pretrained language models. arXiv preprint arXiv:2402.16797, 2024. [14] Zhaochen Su, Jun Zhang, Tong Zhu, Xiaoye Qu, Juntao Li, Min Zhang, and Yu Cheng. Timo: Towards better temporal reasoning for language models. arXiv preprint arXiv:2406.14192, 2024. [15] Siheng Xiong, Ali Payani, Ramana Kompella, and Faramarz Fekri. Large language models can learn temporal reasoning. arXiv preprint arXiv:2401.06853, 2024. [16] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [18] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [19] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [20] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. Deepseek-coder: When the large language model meets programmingthe rise of code intelligence. arXiv preprint arXiv:2401.14196, 2024. [21] Aniket Deroy and Subhankar Maity. short case study on understanding the capabilities of gpt for temporal reasoning tasks. Authorea Preprints, 2024. [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [23] Timo Kaufmann, Paul Weng, Viktor Bengs, and Eyke Hüllermeier. survey of reinforcement learning from human feedback. arXiv preprint arXiv:2312.14925, 10, 2023. [24] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. [25] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. [26] Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet Üstün, and Sara Hooker. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740, 2024. [27] Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8:229256, 1992. 15 [28] Tian Xie, Zitian Gao, Qingnan Ren, Haoming Luo, Yuqian Hong, Bryan Dai, Joey Zhou, Kai Qiu, Zhirong Wu, and Chong Luo. Logic-rl: Unleashing llm reasoning with rule-based reinforcement learning. arXiv preprint arXiv:2502.14768, 2025. [29] Bowen Jin, Hansi Zeng, Zhenrui Yue, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516, 2025. [30] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [31] Xuefeng Li, Haoyang Zou, and Pengfei Liu. Torl: Scaling tool-integrated rl. arXiv preprint arXiv:2503.23383, 2025. [32] Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. [33] Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji. Otc: Optimal tool calls via reinforcement learning. arXiv preprint arXiv:2504.14870, 2025. [34] Xiusi Chen, Gaotang Li, Ziqi Wang, Bowen Jin, Cheng Qian, Yu Wang, Hongru Wang, Yu Zhang, Denghui Zhang, Tong Zhang, et al. Rm-r1: Reward modeling as reasoning. arXiv preprint arXiv:2505.02387, 2025. [35] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [36] The New York Times. Archive api. https://developer.nytimes.com/docs/ archive-product/1/overview. Accessed on March 6, 2024. [37] James Pustejovsky, José Castano, Robert Ingria, Roser Sauri, Robert Gaizauskas, Andrea Setzer, Graham Katz, and Dragomir Radev. Timeml: Robust specification of event and temporal expressions in text. New directions in question answering, 3:2834, 2003. [38] Volker Gast, Lennart Bierkandt, Stephan Druskat, and Christoph Rzymski. Enriching timebank: Towards more precise annotation of temporal relations in text. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 38443850, 2016. [39] Dong-Ho Lee, Kian Ahrabian, Woojeong Jin, Fred Morstatter, and Jay Pujara. Temporal knowledge graph forecasting without knowledge using in-context learning. arXiv preprint arXiv:2305.10613, 2023. [40] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in neural information processing systems, 33:57765788, 2020. [41] Carlo Galli, Nikolaos Donos, and Elena Calciolari. Performance of 4 pre-trained sentence transformer models in the semantic query of systematic review dataset on peri-implantitis. Information, 15(2):68, 2024. [42] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [43] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [44] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [45] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. [46] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157, 2024. [47] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023."
        },
        {
            "title": "A Experimental Configuration Details",
            "content": "This appendix provides further details on the experimental setup and hyperparameter configurations used for the Reinforcement Learning (RL) fine-tuning of Time-R1, complementing the summary in Section 4.3 of the main paper. Our experiments were conducted using the veRL framework [45]. A.1 General Setup and Key Hyperparameters The base Large Language Model (LLM) for all our experiments is Qwen2.5-3B-Instruct. The RL fine-tuning was performed using 4 NVIDIA A6000 GPUs. Key hyperparameters for the Group Relative Policy Optimization (GRPO) algorithm and the overall training process are summarized in Table 4. Table 4: Key hyperparameters for RL fine-tuning Time-R1. Parameter General & Model Base Model Name Number of GPUs Data & Batching train batch size (Global) GRPO mini batch size GRPO micro batch size max prompt length max response length Optimizer (Actor Model) learning rate warmup style warmup steps GRPO Algorithm & Rollout kl loss coef (β) rollout.n (K) Value Qwen2.5-3B-Instruct 4 128 64 16 1024 tokens 1024 tokens 2 106 cosine 20 0.001 5 A.2 Stage-Specific Training Configurations The multi-stage training of Time-R1 involved specific durations and checkpointing strategies for each stage, as outlined below. For both stages, checkpoints were selected based on the highest achieved score on the respective test set. Stage 1 (Comprehension): This stage implemented our dynamic reward curriculum (detailed in Section 3.3.3) and was divided into three phases: Phase 1 (Foundational Logic; Easy Timestamp Inference): Trained for 100 steps. Phase 2 (Exploration; Full Task Suite, Mixed Difficulty): Trained for 400 steps. Phase 3 (Transition to Strict Evaluation; Full Task Suite): Trained for 800 steps. Throughout Stage 1, evaluations on the test set were performed every 10 training steps, and model checkpoints were saved every 20 training steps. The best-performing checkpoint on the test set from each phase was used to initialize the subsequent phase or, for Phase 3, served as the final Stage 1 model (θ1). Stage 2 (Prediction): This stage focused on future event time prediction: Trained for 100 steps. During Stage 2, both model checkpointing and test set evaluations occurred every 10 training steps. The checkpoint yielding the highest test score was selected as the final Stage 2 model (θ2). These tailored configurations allowed for progressive and adaptive learning, ensuring that Time-R1 developed foundational understanding before advancing to more complex predictive tasks."
        },
        {
            "title": "B Dataset Construction and Details",
            "content": "This appendix provides further details on the datasets used for training and evaluating Time-R1, supplementing the descriptions in Sections 3.2.1 and 3.2.2. B.1 New York Times (NYT) Corpus Curation The primary data source for our research is corpus constructed from New York Times articles, utilizing publicly available information accessed via the NYT Archive API2. For each article, we extracted key fields including the headline, abstract, publication date, and the news desk (thematic section). We collected over 200,000 English-language NYT articles, with publication dates spanning from January 2016 to February 2025. To ensure the relevance of the articles to common temporal reasoning scenarios and current events, we selectively curated content from the following news desks: Politics, National, Washington, U.S., Business, SundayBusiness, RealEstate, Foreign, World, Metro, Science, Health, Climate, Opinion, and OpEd. Other news desks were excluded as they were found to reference current events less frequently. This extensive NYT corpus was utilized for several distinct purposes within our framework: Stage 1 (Comprehension) Training Data: Articles published from January 2016 to December 2023 were used to train the foundational temporal understanding capabilities of Time-R1 (see Section 3.2.1 for Stage 1 details). Stage 2 (Prediction) Real News Training Data: subset of articles from January 2024 to July 2024 served as real-world news data for the initial phase of Stage 2 training. Stage 2 (Prediction) Real News Test Data: Articles from August 2024 to February 2025 test ) for evaluating future event prediction were held out and used as the real-news test set (D(2) performance. In our task formulations, an event is typically represented by its headline and abstract a, i.e., = (h, a). B.2 Synthetic Data Generation for Future Event Prediction Training To train Time-R1 for predicting events in future months (specifically, August 2024 to February 2025) without encountering data leakage from the real-news test period, we employed data synthesis strategy as detailed in Section 3.2.2. This process utilized the DeepSeek-V3 model with knowledge cutoff in July 2024. The methodology for generating synthetic news articles was as follows: Targeted News Desk Distribution: The generation aimed to reflect historical distribution of articles across various news desks, based on NYT data prior to 2024. The primary target desk distribution used to guide generation proportions was: Foreign: 20.8%; Business: 16.5%; OpEd: 14.2%; National: 10.9%; Washington: 9.6%; Metro: 8.6%; Politics: 5.5%; Science: 4.6%. Few-Shot Prompting Strategy: To generate content for specific target future month (between August 2024 and February 2025) and designated news desk, the DeepSeek-V3 model was prompted using few-shot learning approach. Each prompt contained three real news headlines and abstracts from the same news desk, randomly sampled from articles published between May 2024 and July 2024. 2https://developer.nytimes.com/docs/archive-product/1/overview 19 Generation Task: For each such prompt, DeepSeek-V3 was instructed to generate six distinct synthetic news items (each comprising headline and an abstract) relevant to the specified future month and news desk, learning from the style and content of the provided examples. Output Distribution: The selection and aggregation of these generated articles were managed so that the overall proportion of news items per desk for each future month in the synthetic training set (D(2) train) approximately mirrored the historical desk distribution detailed above. This synthetic dataset provided the necessary training signals for the model to learn to predict events beyond its real-data cutoff while strictly ensuring no overlap with the real-news test data from the same period. The volume of this synthetic data for August 2024 - February 2025 was about half that of the real news data used for January 2024 - July 2024 in the Stage 2 training. Detailed Stage 1 Learning Curves and Analysis This section provides more detailed look at the learning dynamics during Stage 1 (Comprehension), complementing the summarized performance presented in Table 1 of Section 4.4.1. We present the training curves for all four fundamental temporal subtasksTimestamp Inference, Time-Difference Estimation, Event Ordering, and Masked Time Entity Completionspecifically focusing on their progression throughout Phase 2 and Phase 3 of our dynamic reward curriculum (see Section 3.3.3 for details on the curriculum phases). Additionally, we illustrate the evolution of the inconsistency penalty factor (Pincon) for the Time-Difference Estimation and Event Ordering tasks during Phase 2, highlighting the models improving adherence to logical and mathematical consistency. Figure 5: Learning curves for Stage 1 subtasks during (Left) Phase 2 and (Right) Phase 3 of the dynamic reward curriculum. The left plot also shows the Inconsistency Penalty Factor (Pincon) for Time-Difference Estimation and Event Ordering tasks on the right y-axis during Phase 2. The learning curves depicted in Figure 5 offer several key insights into the effectiveness of our methodology. Firstly, the steady increase and eventual convergence of the accuracy scores (Racc) across all subtasks in both Phase 2 and Phase 3 underscore the benefits of our dynamic reward design and curriculum learning strategy. This carefully structured approach enables the model to progressively master complex temporal logic, gradually adapting from more lenient to stricter evaluation criteria. As noted in Section 4.4.1, this robust Stage 1 performance allows our 3B Time-R1 20 model to surpass numerous baseline models, many of which are ten to over two hundred times larger in parameter count  (Table 1)  . Such strong foundational capabilities in temporal comprehension are crucial and deliberately engineered to provide solid grounding for the subsequent, more demanding future-oriented tasks in Stage 2 (Prediction) and Stage 3 (Generation). Secondly, the trends observed for the inconsistency penalty factors (Pincon) for the Time-Difference Estimation and Event Ordering tasks during Phase 2 (left plot of Figure 5, dashed lines) are particularly revealing. The increasing values of Pincon (approaching 1.0) indicate that the model is effectively learning to minimize inconsistencies in its responses. For instance, in Time-Difference Estimation, it learns to ensure that the explicitly stated time difference aligns with the difference calculated from its inferred dates for the two events. Similarly, for Event Ordering, the model becomes better at ensuring the stated order of events is consistent with the chronological sequence implied by its inferred dates for those events. This demonstrates that the penalty mechanisms detailed in Section 3.3.2 successfully guide the model not just towards task-specific accuracy but also towards generating responses that are logically coherent and mathematically sound, critical aspect of true temporal understanding. By the commencement of Phase 3, these consistency factors are generally high, allowing the training to focus further on refining accuracy under strict evaluation. Overall, these detailed learning dynamics from Stage 1 highlight the efficacy of our curriculum in building both accurate and logically consistent temporal reasoning, providing the essential groundwork for Time-R1s advanced capabilities in navigating future temporal challenges."
        },
        {
            "title": "D Further Discussion on Implementation Settings",
            "content": "This appendix elaborates on specific implementation settings, focusing on the impact of the KL loss coefficient on model response length and the overall stability of our training framework with respect to various hyperparameter changes. These details supplement the primary configurations presented in Table 4. D.1 Impact of KL Loss Coefficient on Response Length The Group Relative Policy Optimization (GRPO) objective function, as defined in Equation (3), incorporates KL divergence term DKL[πθ(x)πref (x)] scaled by coefficient β. This term penalizes deviations of the current policy πθ from reference policy πref , encouraging smoother and more stable policy updates. The magnitude of β directly influences the strength of this regularization. Figure 6: Impact of different KL loss coefficients (β) on the average response length during training. lower coefficient (0.0001) leads to longer average responses compared to the default setting (0.001). As illustrated in Figure 6, lower KL coefficient (e.g., β = 0.0001 compared to our default β = 0.001) reduces the penalty for deviating from the reference policy. This allows the model greater freedom to explore diverse generation strategies during training. noticeable consequence of this increased exploration with lower β is an increase in the average length of the generated responses. However, our experiments indicated that while the response lengths varied, the overall performance 21 scores on the test sets remained largely comparable across these KL coefficient settings. This suggests that while the KL coefficient can influence stylistic aspects of the generation, such as verbosity, the core temporal reasoning capabilities learned by the model are robust within this range of β values. D.2 Framework Stability under Hyperparameter Variations Beyond the KL coefficient, we investigated the sensitivity of Time-R1s performance to variations in other key hyperparameters relative to our main configuration detailed in Table 4. These variations included: Increasing the number of rollout responses (K) from 5 to 8 and 11. Adjusting the sampling temperature from 1.0 to 1.2. Modifying the learning rate from 2 106 to 5 106 (increase) and 1 106 (decrease). Increasing the GRPO micro batch size from 16 to 32. Varying the GRPO mini batch size from 64 to 128 (increase) and 32 (decrease). Across these diverse hyperparameter modifications, we observed that the performance of Time-R1 on our test sets remained largely consistent, with no significant degradation in scores. This robustness to moderate changes in key training parameters underscores the overall stability and reliability of our proposed three-stage RL framework and GRPO optimization setup. Such stability is advantageous, suggesting that the framework is not overly sensitive to precise hyperparameter tuning, which can be beneficial for practical application and further development. Additional Generated Examples of Time-R1 This appendix presents additional generated examples from our Time-R1 model, supplementing Figure 1 and further illustrating its capabilities across Stage 1 (Comprehension), Stage 2 (Prediction), and Stage 3 (Generation). These examples showcase the models structured reasoning process (within <think>...</think> tags) and its final outputs (within <answer>...</answer> tags), alongside ground truth information and achieved scores. The detailed prompts used to elicit these responses are available in Appendix I. Our analysis highlights how Time-R1 demonstrates comprehensive temporal reasoning by effectively understanding context, making logical inferences, and generating plausible future-oriented content. E.1 Example: Stage 1 - Timestamp Inference This example demonstrates Time-R1s ability to infer the publication date of news article by reasoning about the real-world events mentioned. Analysis In this instance, Time-R1 correctly associates the discussion about child care systems with the coronavirus pandemic. It leverages its knowledge that the outbreak began in early 2020 and reasons that related articles discussing systemic responses would likely appear in the subsequent months. The inferred date 2020-04 is very close to the ground truth 2020-05, showcasing accurate temporal localization based on contextual understanding of significant world events. The high score reflects this accuracy and proper formatting. E.2 Example: Stage 1 - Masked Time Entity Completion This example illustrates the models capability to not only infer an events primary date but also to fill in masked temporal entity within the text, requiring deeper semantic understanding. Analysis Time-R1 successfully identifies the masked year as 2016 by connecting the context of \"Hillary Clintons presidential campaign\" to the correct election cycle. Simultaneously, it infers the main events date (the articles publication discussing these past activities) as 2018-06, which is very close to the ground truth 2018-07. This demonstrates its ability to distinguish between the time of the events discussed within the text (the 2016 campaign) and the time of the news reporting itself, showcasing nuanced understanding of temporal references and context. 22 Aspect"
        },
        {
            "title": "Task\nInput Headline\nInput Abstract",
            "content": "Model Reasoning <think>...</think> Table 5: Example of Timestamp Inference by Time-R1 θ1. Details Stage 1: Timestamp Inference How to Build Better Child Care System The coronavirus has highlighted how necessary good child care is. Given that the coronavirus outbreak The article discusses how the coronavirus has highlighted the necessity of better child care system. began in January 2020 and has had significant impact on child care systems around the world, it is reasonable to infer that the article is describing efforts to improve the child care system in response to the coronavirus pandemic. The article likely covers the events happening during the spring of 2020, when many countries were implementing policies to support child care during the pandemic. Model Answer <answer>...</answer> 2020-04 Ground Truth (Event Pub Date) Total Score 2020-05 1.005 Table 6: Example of Masked Time Entity Completion by Time-R1 θ1. Aspect Task Input Headline Input Abstract Model Reasoning <think>...</think> Model Answer <answer>...</answer> Details Stage 1: Masked Time Entity Completion How Russian Spies Hid Behind Bitcoin in Hacking Campaign Operatives used cryptocurrency to obscure their identities and pay for the infrastructure needed to steal emails from Hillary Clintons <YEAR> presidential campaign. The article is about the hacking campaign that took place during the 2016 presidential election. Given that the hacking campaign involved stealing emails from the Hillary Clinton presidential campaign, it is reasonable to infer that the hacking campaign took place during the 2016 election year. The hacking campaign would have needed time to happen in order to steal the emails, and the use of cryptocurrency would have also needed time to develop and be implemented. Event: 2018-06. Missing entity: 2016. Ground Truth Total Score Event Pub Date: 2018-07, Masked Entity: 2016 1.052 E.3 Example: Stage 2 - Future Event Time Prediction This example showcases Time-R1s ability to predict the timing of future events by extrapolating from patterns and general knowledge. Table 7: Example of Future Event Time Prediction by Time-R1 θ2. Details Aspect"
        },
        {
            "title": "Input Abstract",
            "content": "Model Reasoning <think>...</think> Stage 2: Future Event Time Prediction The Olympics, Triumph of Ambition, Lift France From Its Gloom All sectors of society came together to achieve the nations dream for the Games. Success has shown that unity is possible, if often elusive. The Olympics are known to happen every four years, and the most recent Olympics were held in 2020. Given that the Olympics typically take at least two years to prepare for and the COVID-19 pandemic would have taken at least two years to resolve, it is reasonable to infer that the Olympics would happen relatively quickly after the end of the 2020 Olympics, which were delayed by one year due to the COVID-19 pandemic. Model Answer <answer>...</answer> 2024-08 Ground Truth (Event Pub Date) Total Score 2024-08 1.100 Analysis In this challenging future prediction task, Time-R1 correctly predicts the 2024-08 date for the Paris Olympics. Its reasoning demonstrates an understanding of typical event cycles (\"every four years\"), knowledge of recent past events (2020 Olympics and their delay), and the ability to synthesize this information to make an accurate future projection. This highlights its capacity for temporal extrapolation, key component of comprehensive temporal intelligence. The perfect score reflects this accurate prediction. E.4 Example: Stage 3 - Creative Future Scenario Generation This example illustrates Time-R1s capability for creative scenario generation, where it generates plausible future news item for given future date (January 2025 in this case), without explicit fine-tuning on this generative task. The quality is assessed by semantic similarity to actual news from that period. Analysis For the future date of January 2025, Time-R1 generated plausible news scenario about AIs impact on tech stocks. This generated content is thematically coherent and discusses relevant potential development in the technology and market sectors. When compared to real news headline from similar period that also discusses AI and tech investors, it achieves notable semantic similarity score (0.6731). This demonstrates Time-R1s ability to not just predict dates, but to creatively generate contextually relevant and plausible future narratives, showcasing strong generalization of its learned temporal understanding and reasoning skills. This ability to generate novel, coherent future content is hallmark of advanced temporal intelligence."
        },
        {
            "title": "F Illustration of Length and Repetition Penalty Efficacy",
            "content": "In Section 3.3.1 (Common Bonuses and Penalties), we introduced the Length and Repetition Penalty (Plen_rep), designed to discourage overly verbose or repetitive model outputs. We noted that this mechanism has proven particularly effective. This section provides an illustrative example of the type 24 Table 8: Example of Creative Future Scenario Generation by Time-R1 θ2 (Target: January 2025). Aspect Details"
        },
        {
            "title": "Task\nTarget",
            "content": "Stage 3: Creative Future Scenario Generation News for January 2025 (Technology/Market theme implied by output) Time-R1 Generated News Headline: Abstract: Comparable Real News Headline: Abstract: Global Tech Stocks Face Pressure as AI Innovations Begin to Shape the Market The introduction of new artificial intelligence (AI) technologies is putting pressure on the tech stock market. Analysts predict that companies that can effectively integrate AI into their operations will see significant gains, while others may struggle to keep up. Chinas A.I. Advances Spook Big Tech Investors on Wall Street Stiffer competition for the tech giants at the forefront of the artificial intelligence boom prompted investors to reassess the companies skyhigh valuations. Semantic Similarity Score 0.6731 of repetitive reasoning that the Prepetition component of this penalty targets, thereby guiding the model towards more efficient and varied responses. Table 9: Example Illustrating Repetitive Reasoning Targeted by the Prepetition Penalty. Aspect Task Input Headline Input Abstract Model Reasoning <think>...</think> Details Time Prediction (Stage 2 Example) After Thursdays Debate, Conservative Media Finds It Hard Not to Gloat Many conservative commentators have said President Bidens performance during the debate was sweeping validation of the alarm bells theyd been ringing for years about his age. Given that elections usually The article is about the debate between President Biden and other candidates in the Republican primary election. take several weeks to several months to be resolved, it is reasonable to infer that the article is about describing preparations for the debate. Given that elections usually take several weeks to several months to be resolved, it is likely that the article is about describing preparations for the debate. Model Answer <answer>...</answer> 2024-06 Ground Truth (Event Pub Date) 2024-07 Analysis and Impact of Penalties The models reasoning process shown in Table 9 exhibits clear instance of repetition: the core phrase Given that elections usually take several weeks to several months to be resolved, it is reasonable/likely to infer that the article is about describing preparations for the debate appears twice with only minor variation. This form of redundancy would be directly addressed by the Pphrase_repeat component within our Prepetition penalty (as defined in Section 3.3.1). By applying such penalties, the Plen_rep mechanism actively discourages the model from generating verbose or repetitive content. This not only improves the conciseness of the output but also pushes the model to explore more diverse and efficient reasoning pathways. The consistent application of these universal bonuses and penalties, including those for length and various forms of repetition (word, phrase, n-gram diversity), is therefore instrumental in achieving the well-formed, succinct, and accurate responses demonstrated in the examples throughout Appendix E. It ensures that Time-R1s advanced temporal reasoning is communicated clearly and effectively, without being undermined by tendency towards unnecessary verbosity or redundancy."
        },
        {
            "title": "G Limitations",
            "content": "Firstly, the base model for Time-R1 was not pre-trained on dedicated mathematical or logical reasoning datasets. This lack of strong mathematical foundation may have impacted its performance on Stage 1 subtasks that involve numerical calculations, such as Time-Difference Estimation. Without this foundation, the model might be more susceptible to inconsistency penalties when predicting large time differences, potentially leading to more conservative or less accurate estimations in such scenarios. Secondly, while our results demonstrate that smaller models can achieve strong performance on temporal tasks with specialized RL training, evidence from baseline comparisons also suggests that larger models generally exhibit higher capabilities. Due to resource constraints, we focused on demonstrating the efficacy of our approach on 3B model to highlight cost-effective and rapid iteration potential. However, applying our three-stage RL framework to larger foundation models could likely yield even more significant performance gains, leveraging their inherently greater knowledge capacity. Our work primarily showcases the potential of the RL methodology, which we believe would scale positively with model size."
        },
        {
            "title": "H Ethical Statement",
            "content": "The development of Time-R1 and the Time-Bench dataset aims to advance research in temporal reasoning for AI. The dataset constructed from New York Times articles uses publicly available information through Archive api. While endowing models with future prediction and scenario generation capabilities has many beneficial applications, such as in planning and risk assessment, we acknowledge the potential for misuse, such as generating misleading future-oriented content. To address this, we believe that fostering an environment of transparency and critical use is paramount; users should be aware when content is AI-generated, particularly for probabilistic future scenarios, allowing for informed interpretation rather than uncritical acceptance. This approach, emphasizing clear attribution and critical engagement, combined with ongoing research into robust safeguards, is crucial for responsibly harnessing such powerful capabilities. Our model development did not involve human-derived private data beyond publicly archived news. The research was conducted with the intention of fostering better understanding of AIs temporal intelligence, and we encourage responsible use and further investigation into safeguards for generative temporal models. The datasets and models will be released to the research community to promote transparency and further beneficial advancements in this domain."
        },
        {
            "title": "I Prompts",
            "content": "This appendix provides the detailed structure and content of the prompts used to guide our Large Language Model for each of the six temporal reasoning tasks evaluated in this work. Consistent with the methodology described in Section 3.1 (Structured Generation Process), all prompts employ specific template designed to elicit chain-of-thought reasoning. This template includes system instructions directing the model to first articulate its reasoning process within <think>...</think> tags, followed by the final answer encapsulated in <answer>...</answer> tags. This structured approach aims to enhance the robustness of the models reasoning and the interpretability of its outputs. The specific prompts for each task are detailed in the following subsections. I.1 Prompt for Timestamp Inference The Timestamp Inference task is one of the four fundamental temporal tasks in Stage 1 (Comprehension). It requires the model to infer the specific month and year (formatted as YYYY-MM) of an event based on its provided news headline and abstract. The detailed prompt given to the model for 26 this task, including system messages, user input structure with placeholders for event details, and specific output formatting requirements, is shown in Figure 7. Figure 7: Prompt for the Timestamp Inference task. I.2 Prompt for Time-Difference Estimation The Time-Difference Estimation task is part of Stage 1 (Comprehension). It requires the model to first infer the specific dates of two separate events (E1 and E2) described by their news headlines and abstracts, and then to estimate the temporal gap (i.e., in months) between these two events. The detailed prompt guiding the model through this multi-step reasoning process is shown in Figure 8. I.3 Prompt for Event Ordering The Event Ordering task, also component of Stage 1 (Comprehension), challenges the model to determine the correct chronological sequence of three distinct events (E1, E2, E3) presented out of order. Similar to other Stage 1 tasks, the model is prompted to first infer the date of each event before determining their order. The prompt structure for this task is presented in Figure 9. I.4 Prompt for Masked Time Entity Completion The Masked Time Entity Completion task is the fourth fundamental task in Stage 1 (Comprehension). In this task, the model is given an event description (E) containing masked temporal expression (such as <Year> or <Month>) and is required to fill in the correct missing time entity, after first inferring the events overall date. The specific prompt used to guide this completion process is shown in Figure 10. I.5 Prompt for Future Event Time Prediction The Future Event Time Prediction task constitutes Stage 2 (Prediction) of our framework. Here, the model is tasked with predicting the specific future date (YYYY-MM) of news event based on its 27 Figure 8: Prompt for the Time-Difference Estimation task. extracted headline and abstract, focusing on events occurring after the models initial knowledge cutoff. The prompt designed to elicit these future predictions is displayed in Figure 11. I.6 Prompt for Creative Future Scenario Generation The Creative Future Scenario Generation task is the focus of Stage 3 (Generation). In this stage, the model leverages capabilities developed previously to generate plausible, hypothetical news event descriptions or headlines for specified future date and thematic category (e.g., Business, Technology). This task evaluates the models ability to creatively imagine coherent future events. The prompt used to guide this generative process is presented in Figure 12. 28 Figure 9: Prompt for the Event Ordering task. 29 Figure 10: Prompt for the Masked Time Entity Completion task. Figure 11: Prompt for the Future Event Time Prediction task. 31 Figure 12: Prompt for the Creative Future Scenario Generation task."
        }
    ],
    "affiliations": [
        "Siebel School of Computing and Data Science, University of Illinois at Urbana-Champaign"
    ]
}