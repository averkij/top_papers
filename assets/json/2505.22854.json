{
    "paper_title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting",
    "authors": [
        "Kornel Howil",
        "Joanna Waczyńska",
        "Piotr Borycki",
        "Tadeusz Dziarmaga",
        "Marcin Mazur",
        "Przemysław Spurek"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussians, the first unified style transfer framework that supports text- and image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as a plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussians approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving a model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussians as a universal and efficient solution for multimodal style transfer."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 4 5 8 2 2 . 5 0 5 2 : r CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting Kornel Howil Jagiellonian University Joanna Waczy nska* Jagiellonian University Piotr Borycki Jagiellonian University Tadeusz Dziarmaga Jagiellonian University Marcin Mazur Jagiellonian University Przemysław Spurek Jagiellonian University Figure 1: We present CLIPGaussian, universal model for style transfer that supports wide range of data modalities, including images, videos, 3D objects, and 4D dynamic scenes. Style transfer in CLIPGaussian can be guided using an image or text prompt. Our method leverages Gaussian Splatting representation to model both color and geometric aspects of style transfer."
        },
        {
            "title": "Abstract",
            "content": "Gaussian Splatting (GS) has recently emerged as an efficient representation for rendering 3D scenes from 2D images and has been extended to images, videos, and dynamic 4D content. However, applying style transfer to GS-based representations, especially beyond simple color changes, remains challenging. In this work, we introduce CLIPGaussian, the first unified style transfer framework that supports textand image-guided stylization across multiple modalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates directly on Gaussian primitives and integrates into existing GS pipelines as plug-in module, without requiring large generative models or retraining from scratch. CLIPGaussian approach enables joint optimization of color and geometry in 3D and 4D settings, and achieves temporal coherence in videos, while preserving model size. We demonstrate superior style fidelity and consistency across all tasks, validating CLIPGaussian as universal and efficient solution for multimodal style transfer. Equal contribution kornel.howil@student.uj.edu.pl GitHub: https://github.com/kornelhowil/CLIPGaussian Project Page: https://kornelhowil.github.io/CLIPGaussian/ Preprint. Figure 2: CLIPGaussian architecture in the case of 4D dynamic scene. The method operates in two main stages. In the first stage, we train Gaussian Splatting model tailored to specific data modality. In the second stage, during training, we leverage training images, randomly sampled patches, and conditioning inputs (either an image or text) in the feature spaces of VGG-19 and CLIP models. We optimize the Gaussian parameters using composite loss function with four terms: content preservation, background preservation, local style transfer, and global style transfer. Notably, CLIPGaussian integrates with GS-based systems as plug-in module."
        },
        {
            "title": "Introduction",
            "content": "The past year has seen an explosive rise in user-driven content generation, particularly in the visual domain. Following the launch of OpenAIs GPT-4o [1], users created more than 700 million images within single week, highlighting the massive demand for generative tools that allow intuitive, direct manipulation of visual content. Although 2D image generation and edition are rapidly becoming mainstream tasks, editing in higher dimensions, such as video, 3D, and 4D content, is significantly more complex [2]. These domains bring challenges in consistency, geometry, temporal coherence, and user control that existing systems are not yet fully equipped to handle. Gaussian Splatting (GS) [3] is major advancement in computer graphics, representing 3D scenes as sets of Gaussian components with color and opacity. Its training and rendering are highly efficient and produce realistic images. GS has also been adapted for 4D dynamic scenes [4, 5, 6], 2D images [7, 8], and videos [9, 10], using slightly modified Gaussian components to model 2D content. Editing objects represented by 3D Gaussian primitives falls into four categories: texturing, local editing, insertion editing, and style transfer [11]. Texturing applies textures to mesh regions, local editing modifies selected areas while preserving others, and insertion editing adds new elements for coherent scene composition. Style transfer alters the global appearance of objects and scenes. Each task involves distinct methodologies and evaluation protocols. possible approach to address this issue is the use of plug-in type models. This refers to family of components that can be inserted into existing architectures without requiring complete retraining [12], allowing them to adapt to new tasks such as style transfer. In this paper, we introduce CLIPGaussian, plug-in type model suitable for Gaussian Splatting-based architectures. Our model stylizes content represented by Gaussian primitives, conditioned on either of reference image or text prompt, see Fig. 1. As plug-in component, CLIPGaussian integrates into existing pipelines without requiring retraining of the base model. Figure 3: Results of text-base 4D style transfer. Our model modifies both the color and geometry of Gaussian primitives. When conditioned on text prompt, models based on Gaussian Splatting primarily focus on editing rather than style transfer [13, 14, 15]. In the context of conditioning on reference image, models typically concentrate on modifying only color and opacity. Examples of this include StyleGaussian [16], 2 GT & Style I-GS2GS [13] DGE [14] CLIPGaussian Figure 4: We conduct user study, comparing our model against baseline methods. CLIPGaussian achieves scores comparable to G-Style with image conditioning and outperforms all models when using text prompts. Figure 5: Comparison of various 3D style transfer methods involving text conditioning. CLIPGaussian applies style transfer with more significant shape changes. Our model captures details by attending to local regions through patches processing. ReGS [17], InstantStyleGaussian [18], Style3D [19], and StyleSplat [20]. G-Style [21] employs two-phase process, consisting of stylization followed by refining the scenes geometry, but increasing the size of the model significantly. Since our CLIPGaussian uses the architecture of the base model, we can optimize all parameters, without changing the model size. By operating directly within the architecture of the Gaussian Splatting-based model, our method enables end-to-end optimization of the full set of Gaussian parameters including position and scale, rather than being restricted to color based edits, see Fig. 2. Crucially, we retain the original number of Gaussian primitives, thereby preserving the memory and computational characteristics of the objects reconstruction. In this work, we make the following primary contributions: Universal Multimodal Stylization: We propose CLIPGaussian, the first plug-in style transfer model for Gaussian Splatting, enabling imageand text-guided stylization across 2D, video, 3D, and 4D data without retraining the base model. Joint Appearance and Geometry Optimization: Our method allows end-to-end optimization of all Gaussian parameters, enabling geometric transformations and temporally consistent results, while preserving the original model size. Generalizable GS-based Stylization Framework: We demonstrate that Gaussian Splatting is versatile substrate for style transfer, achieving strong performance across tasks and modalities with unified architecture."
        },
        {
            "title": "2 Related Works",
            "content": "CLIPGaussian operates across diverse data modalities, including images, videos, and 3D or 4D scenes. To our knowledge, it is the first method capable of style transfer across such wide range. As result, direct comparison with existing work is challenging, so we evaluate our model separately for each modality. 3D Our model uses Gaussian Splatting to represent various data modalities. Therefore, the closest solutions are dedicated to 3D scenes. In the case of classical Gaussian Splating, style transfer methods usually work only on colors [16, 17, 18, 19, 20]. It means that such algorithms do not change the geometry of objects. Therefore, the only modification is to the colors and opacities of Gaussian primitives. StyleGaussian [16] embeds 2D VGG features into 3D Gaussians, transforms them based on style image, and decodes them with novel KNN-based 3D CNN. Such model can be seen as an adaptation of AdaIN [22] to 3D Gaussian Splatting. StyleSplat [20] segments individual objects, then fine-tunes their appearance using feature matching with style image, allowing customizable, multi-object stylization with strong visual consistency and efficiency. ReGS [17] introduces textureguided control mechanism for fine-grained appearance editing. It enhances style detail by replacing selected Gaussians with denser ones, guided by texture cues and regularized by depth to preserve geometry. G-Style [21] is 3D scene stylization algorithm that enhances Gaussian Splatting by 3 optimizing both appearance and geometry to match reference style image. It improves visual quality by preprocessing out problematic Gaussians, applying multi-scale style losses, and refining geometry through gradient-based Gaussian Splatting. Alternatively, we can use large diffusion model for the style transfer of 3D models. InstantStyleGaussian [18] uses diffusion models and an iterative dataset update strategy. It stylizes pre-reconstructed scenes by generating target-style images, updating the training data, and optimizing the scene efficiently, achieving high-quality results with improved speed and consistency. Style3D [19] introduces MultiFusion Attention to align structural and stylistic features across views, ensuring spatial coherence and visual fidelity, and uses large 3D reconstruction model for high-quality, efficient stylization. Morpheus [23] introduces novel autoregressive 3D Gaussian Splatting stylization method that enables controllable stylization of both appearance and geometry in 3D scenes. The key contribution is an RGBD diffusion model allowing users to adjust stylization strength over shape and look. Such task is closer to 3D scene editing than style transfer like in DGE [14], GaussCltr [15], ProGDF [24] or EditSplat [25]. GT & Style StyleGaussian [16] G-Style [21] CLIPGaussian 4D One of the most underexplored modalities is the 4D scene stylization which aims to modify the appearance of the 3D scene over time. Existing approaches remain limited. Currently, there are only two 4D style transfer models. 4DStyleGaussian [26] uses reversible neural network to train 4D embedded Gaussians, preserving content fidelity while reducing feature distillation artifacts. learned 4D style transformation matrix enables consistent stylization across views and time. Instruct 4Dto-4D [27] is NeRF-based method for instruction-guided editing of dynamic scenes. It enhances 2D diffusion models with 4D awareness and spatial-temporal consistency. Treating 4D scenes as pseudo-3D combines anchor-aware attention, optical flow-guided appearance propagation, and depth-based projection to enable consistent, high-quality edits across time and viewpoints. While both methods focus on scenes, we show that our method also works very well on objects without backgrounds, see Fig. 3 Figure 6: Comparison of 3D style transfer obtained by image conditioning. Results generated by CLIPGaussian are more detailed. Images In contrast to 4D scenes, 2D image style transfer is very popular task. The foundational work in neural style transfer [28] introduced an optimization-based method to match the content features of one image and the style captured using Gram matrices of another, using pre-trained convolutional neural model. This demonstrated that deep features could disentangle content and style, but the approach was computationally expensive due to its iterative nature. To address speed limitations, feed-forward networks were introduced to enable real-time stylization by applying fixed style in single pass [29, 30]. Arbitrary style transfer was later made possible by aligning feature statistics using Adaptive Instance Normalization [31], further refined through Whitening and Coloring Transform [32]. More recent methods have explored more flexible conditioning mechanisms. For image-guided style, transformer-based architectures like StyTr2 leverage both local and global context to improve stylization quality and content preservation [33]. Alternatively, text prompts have emerged as an intuitive way to specify style. Style transfer guided by CLIP embeddings enables stylization from textual description [34], while more efficient approaches learn lightweight style representations for feed-forward transfer [35]. Videos The majority of image-style transfer methods [36, 37, 38] are employed for video-style transfer. Linear [36] proposes fast and flexible style transfer method using learned transformation matrix. It replaces costly or handcrafted operations with feed-forward model. CCPL [37] uses novel Contrastive Coherence Preserving Loss to reduce local inconsistencies, maintaining temporal coherence without harming style quality. UniVST [38] is unified, training-free video style transfer 4 Table 1: Quantitative comparison of style transfer under text-conditioned guidance, compared against baseline methods. Model CLIP-S CLIP-SIM CLIP-F CLIP-CONS Memory size I-GS2GS [13] DGE [14] CLIPGaussian 16.80 17.59 26.86 12.03 12.27 26.31 99.19 99.31 98.80 13.53 12.46 2.34 -36% -5% +0% framework based on diffusion models that focuses on localized stylization. It uses point-matching mask propagation strategy to avoid the need for tracking models. Alternatively, we can use large generative models for style transfer on videos [39, 40, 41]. In [42], the authors adapt large text-to-image diffusion models for video generation, addressing the challenge of temporal consistency. The framework consists of two parts: key frame translation and full video translation. Style-a-video [43] is zero-shot video stylization method that uses pre-trained image latent diffusion model and generative transformer for text-guided style transfer. UniST [44] introduces the Domain Interaction Transformer (DIT), which enables cross-domain learning by sharing contextual information between images and videos. Each of the above approaches has issues with optical flow, so dedicated mechanism is necessary to create smooth style transitions between frames. Our method works directly on Gaussian components, which inherently addresses these problems."
        },
        {
            "title": "Gaussian Splatting",
            "content": "This section introduces our CLIPGaussian model, which is designed for style transfer across different data modalities: images, videos, and 3D or 4D scenes. CLIPGaussian is universal method that works as plug-in for Gaussian Splatting-based representations. The core idea is to fine-tune the parameters of the Gaussian components using the CLIP model [45], which evaluates the similarity between natural language and images. As result, our model is agnostic not only to the modality of data, but also to the conditioning mechanism, allowing the style to be transferred from either an image or text. The core of our approach is to represent all data modalities using Gaussian components. This unified representation facilitates style transfer across images, videos, and 3D or 4D scenes by modifying the corresponding Gaussian primitives. Although each modality introduces specific variationssuch as temporal embeddings for videos and 4D scenesour framework supports single, general architecture for style transfer. Figure 7: Style interpolation between \"Black marble\" and \"Green crystal\" styles on the lego object. Training Dataset The input data for CLIPGaussian are similar to those of typical style transfer models, but they vary slightly depending on the specific task. When applying style to an image, the input is simply single image. For videos, the input consists of all frames, which can be treated as sequence of 2D images. For 3D scenes, views from different camera positions are used. Finally, for 4D objects, temporal indices for these views are also included. Despite the mentioned differences, we can unify our framework by assuming that the training dataset consists of set of images = {I1, . . . , Ik}. Additionally, we have style given by an image or text prompt, which can be seen as conditioning factor S. Base Model: Gaussian Splatting Representation of Various Data Modalities Our model can be seen as plug-in for Gaussian Splatting representations. For this purpose, we work on GS-based general representation consisting of set of Gaussians: = Gmi,Σi,σi,ci,θi = {(N (mi, Σi), σi, ci, θi)}n i=1, (1) 5 Table 2: Quantitative comparison of style transfer using referenced style image, compared against baseline methods. Model CLIP-S CLIP-SIM CLIP-F CLIP-CONS Memory size StyleGaussian [16] G-Style [21] CLIPGaussian 63.69 76.94 72.65 13.07 24,94 20.72 98.87 98.94 98.78 1.36 1.31 1.77 +0% +126% +0% where mi are the mean positions, Σi are the covariance matrices, σi are the opacities, and ci are the Spherical Harmonics (SH) colors of the Gaussian componentsthese are standard GS parameters. On the other hand, θi are additional parameters dedicated to specific data modalities related to an applied base model, which can be any of the available GS-based architectures. In this paper, we use the classical 3DGS [3] for 3D scenes, D-MiSo [6] for 4D scenes, MiRaGe [6] for 2D images, and VeGaS [10] for videos. detailed description of each case can be found in Appendix A. Style 4DStyleGaussian [26] CLIPGaussian The first stage of CLIPGaussian is to train an established base model on the set of input images I, which boils down to optimizing all the parameters occurring in Eq. (1). Note that this varies depending on the chosen data modality. Once training is complete, the resulting pre-trained model becomes the input for the second stage of our algorithm, in which final style transfer procedure is performed. Figure 8: Comparative analysis of style transfer using referenced style image of novel views across various time frames and styles on the Neural 3D Video dataset (DyNeRF) [46]. CLIPGaussian: Final Style Transfer In the subsequent stage, we implement style transfer by further fine-tuning the parameters of the base model (see Eq. (1)). The key idea behind CLIPGaussian is to work on whole objects and small patches at the same time. The training procedure begins by selecting single image Il from the training dataset I, which is then used along with its reconstruction RG(Il) produced by the base model G, Then, collection of random patches = {p1(RG(Il)), . . . , pm(RG(Il)} from the rendered output RG(Il) is extracted, to which random perspective augmentations are further applied (these cropped patches maintain fixed dimensions). Eventually, the parameters of are updated according to the multi-component loss function, which is elaborated in the following paragraph, and the procedure is repeated from the beginning. It is important to note that CLIPGaussian does not perform densification or alter the number of Gaussian components. As result, the stylized objects retain the same size as the original. Moreover, this property enables style interpolation by linearly interpolating the parameters of each Gaussian component. CLIPGaussian: Loss Function Our loss function contains several components, which are detailed below. However, it should be noted that our approach relies on two pre-trained architectures: CLIP and VGG-19, which we further denote as ΦCLIP and ΦVGG, respectively. We use the CLIP model for style transfer and the VGG-19 model to ensure that the stylized output resembles the original object. Content loss (Lc) is the response for similarity between input views and elements after style transfer. To maintain the content information of the input object, following [28], we calculate Lc as the mean squared error (MSE) between the conv4_2 and conv5_2 features of the original image Il and rendered image RG(Il), extracted from pre-trained VGG-19 model, i.e., Lc(RG(Il), Il) = SE(ΦV GG (RG(Il)) , ΦV GG (Il)). (2) Directional CLIP loss (Ld) is responsible for global style transfer. Similar to [47], we define it as follows: Ld(RG(Il), Il) = 1 cos(ΦCLIP (RG(Il)) ΦCLIP (Il), ΦCLIP (S) ΦCLIP (Photo)). (3) It should be noted that in this case, CLIP embeddings of the original image Il and the reconstructed image RG(Il), in conjunction with outputs produced by the CLIP model for given style factor (either style image or style prompt) and simple universal negative prompt Photo, are utilized. Both negative and positive text prompts are combined with an ImageNet prompt template [48]. 6 Patch CLIP loss (Lp) is responsible for local style transfer. It is defined as follows: Lp(RG(Il), Il) = 1 n (cid:88) i=1 Ld(pi(RG(Il)), Il). (4) However, we emphasize that, as proposed in [34], for all patches pi(RG(Il)) we apply random perspective augmentations before calculating the CLIP directional loss. Instruct 4D-to-4D [27] CLIPGaussian Background loss (Lb) is assigned to fixed background elements. For 3D objects and videos, mask is typically used to distinguish between the foreground and background. We incorporate background loss to prevent the style transfer process from introducing unwanted background artifacts. Therefore, Lb is defined as the mean L1 distance between the designated background color and the pixel values corresponding to background regions in the rendered image. Total loss (Ltotal), which is used to fine-tune the parameters of the base model, is defined as weighted sum of the multiple loss components described above. This leads to the following formula: Ltotal = λdLd + λpLp + λcLc + λbLb, (5) where λd, λp, λc, and λb are empirically chosen weighting parameters. Details of the ablation study are provided in Appendix B. For Lp and Ld we use ViT-B/32 CLIP model [48]."
        },
        {
            "title": "4 Experiments",
            "content": "The experiments section is divided into four parts, each corresponding to specific modality for which the model has been applied. We note that because CLIPGaussian operates across multiple data modalities, baselines differ by task. Within each task, we standardize the setup for fair comparisons. Additional experiments using other datasets and ablation studies are presented in Appendix B. For all experiments, we used the NVIDIA RTX 4090 GPU. The source code will be available on GitHub after review. Figure 9: Comparative analysis of style transfer using text condition on the coffee_martini dataset [46]. 3D As our approach is designed as plug-in model compatible with Gaussian Splatting-based models, we begin our evaluation using the standard, vanilla Gaussian Splatting framework. The experiments evaluate both 3D object-level and scene-level performance. The NeRF-Synthetic dataset [49], comprising object-centric synthetic scenes with clean geometry and no background, enables controlled assessment of reconstruction and style transfer on isolated objects. Mip-NeRF 360 [50] provides photorealistic 360-degree real-world scenes with wide baselines, occlusions, and varying depth scales. To assess performance on both imageand text-based stylization, we employ CLIP-based metrics. CLIP Directional Similarity [47] (CLIP-SIM) and CLIP-S [51] evaluate transfer quality, while CLIP Directional Consistency [52] (CLIP-CONS) and CLIP-F [53] assess temporal consistency and content similarity. We also report changes in model size (number of Gaussians) after stylization. Metric definitions and detailed evaluation settings are provided in Appendices B.1 and B.2. Tab. 1 compares our method with Instruct-GS2GS [13] and DGE [14] for text-conditioned style transfer. Tab. 2 shows quantitative comparison with StyleGaussian [16] and G-Style [21] imageconditioned style transfer. Experiments use two objects (lego, hotdog) and two scenes (garden, bonsai), each evaluated under both imageand text-driven conditions. CLIPGaussian method, achieves state-of-the-art results under text-guided conditions. It significantly outperforms all baselines Style & GT AdaIN [22] StyTr2 [54] CLIPGaussian Style & GT InstructPix2Pix [22] ChatGPT [1] CLIPGaussian Figure 10: Comparison of 2D style transfer using image condition. Figure 11: Comparison of 2D style transfer using text condition. with respect to the CLIP-S and CLIP-SIM metrics. We also achieve high-quality results under imageguided conditions. Compared to G-Style, our method offers balance between performance and efficiency. This makes CLIPGaussian practical and scalable alternative for style transfer tasks. We conducted formal online user study on the same dataset as in the quantitative comparison. It evaluated the quality of the style transfer and determined which method best conveyed the style. We conducted four surveys to evaluate different methods of 3D content stylization: (1) 3D objects stylization with images, (2) 3D scenes stylization with images, (3) 3D objects stylization with text, and (4) 3D scenes stylization with text. Each survey had 30 randomly selected participants, one user could participate in multiple surveys. The surveys were conducted formally using the the CLICKworker platform3. detailed description of the questions is provided in Appendix B.2.2. Fig. 4 shows the results from four evaluated scenarios. In case (1), our method was most frequently selected for producing objects most similar to the reference image. In case (2), G-Style performed better on larger scenes, but our method was rated as the least similar to the reference image less often than StyleGaussian. Considering that G-Style uses over twice as many Gaussians, our method remains competitive alternative (see Table 1). In case (3), users most often rated our approach as the best for text-based object stylization. Case (4), involving 3D scene stylization from text, yielded mixed results. Users gave high and low ratings, likely due to the subjective nature of prompt interpretation. post hoc Conover-Friedman test showed significant differences in perceived similarity rankings across methods, except between our method and G-Style for object stylization. Fig. 5 and Fig. 6 show qualitative comparison with baseline methods, on subset of data used for quantitative evaluation and user study. Full comparison is provided in Appendix B.2. Unlike other methods, CLIPGaussian does not change the number of Gaussians, which enables style interpolation (see Fig. 7). Additional results and train time can be found in Appendix B.2. 4D To evaluate performance on dynamic 4D scenes, we used D-MiSo [6], which integrates the Multi-Gaussian Splatting representation with deformation network. All components of the pipeline actively contribute to the learning of style transfer. Empirical evaluations were performed on well-established benchmark datasets. The neural 3D video dataset coffee_martini (DyNeRF) [46] provides time-synchronized and calibrated multiview video sequences capturing complex 4D dynamic scenes. The D-NeRF dataset [55] consists of seven moving objects, with the constraint that only one camera view is accessible at any given time step. For comparative evaluation, we examined CLIPGaussian against 4DStyleGaussian [56]. However, at the time of submission, the official implementation of this method was not publicly available. Consequently, visual comparisons were made using qualitative results extracted directly from the paper. Fig. 8 shows comparison of style edition results produced by both methods, using the referenced image as the style source. Our method captures much more details in the style, especially in patterns and textures. Fig. 9 presents comparison with Instruct 4D-to-4D [27], first instructionguided 4D scene editing method based on diffusion models and NeRFs, which imposes significant hardware requirements. In this comparison, we evaluate style transfer using text prompt instead of reference image. Fig. 3 shows objects from D-NeRF stylization using text prompts, demonstrating that our model performs well not only on 4D scenes. Additional quantitative results and train time comparison can be found in Appendix B.3.1. 3https://www.clickworker.com 8 Style & GT CCPL [37] UniST [44] CLIPGaussian Style & GT Text2Video [39] Rerender [42] CLIPGaussian Figure 12: Comparison of video style transfer using image condition on DAVIS dataset [58]. CLIPGaussian produces more detailed results. Figure 13: Comparison of video style transfer using text condition on DAVIS dataset [58]. CLIPGaussian shows better temporal consistency than baseline methods. 2D We evaluate performance of CLIPGaussian on 2D images using MiRaGe [8] on subset of MS-COCO [57], which is commonly used for evaluating style transfer [28, 22]. Fig. 10 shows qualitative comparison with AdaIN [22] and StyTr2 [54], demonstrating image-guided style transfer. Fig. 11 provides visual comparison with InstructPix2Pix [22] and ChatGPT [1], using text-based stylization. While we acknowledge that our method may not achieve the same level of visual quality as large-scale or diffusion-based models, these models often alter the identity or structure of the stylized subject. In contrast, CLIPGaussian preserves the original content of the image. Additionally, GS-based 2D style transfer benefits from properties such as realistic and localized editing in 3D space, as demonstrated in [8]. Additional comparisons are provided in Appendix B.4. Video To evaluate our algorithm on the video style transfer task, we employed the VeGaS model [10], which represents videos using 3D Gaussian Splatting. The evaluation was conducted on the DAVIS dataset [58], high-quality, high-resolution video collection commonly used for video object segmentation. The dataset comprises numerous videos, each containing fewer than 100 frames. Fig. 12 presents qualitative comparison with existing baseline models CCPL [37] and UniST [44], showcasing an example of image-based style transfer. The figure displays the first and last frames of the respective videos to illustrate the style transformation over time. Our method produces visually more coherent and aesthetically pleasing results, better preserving the style details while maintaining high fidelity to the video content. Fig. 13 provides visual comparison with Text2Video [39] and RerenderAVideo [42], illustrating an example of text-conditioned style transfer; here too, the first and last frames are shown. Compared to RerenderAVideo, our method demonstrates significantly better temporal consistency, reducing flickering and preserving motion coherence. Compared to Text2Video, we achieve higher visual quality with more faithful adherence to the intended style prompt. All comparisons were conducted using the default settings for each method. Additional comparisons and quantitative evaluations are provided in Appendix B.5. Our experiments show that CLIPGaussian achieves comparable or superior style transfer. Moreover, CLIPGaussian maintains significantly better temporal consistency than prior methods. This is because video content is modeled as set of Gaussian primitives, where each Gaussian spans multiple frames. As result, style modifications applied to the Gaussians naturally propagate coherently over time."
        },
        {
            "title": "5 Conclusions",
            "content": "CLIPGaussian is designed as plug-in model, compatible with Gaussian Splatting-based architecture. Our method enables effective style transfer from both text prompt or image to reconstructed objects. This flexibility allows CLIPGaussian to operate seamlessly across various data modalities, including 2D images, videos, 3D objects, and 4D dynamic scenes. In the 3D domain, user study experiments demonstrate strong preference for the stylized outputs produced by CLIPGaussian. This suggests that our approach offers not only technical effectiveness but also practical appeal for creative applications. Quantitative results indicate that our method achieves comparable or superior visual quality relative to existing baselines. 9 Limitations Given the highly developed methods in 2D image stylization, we acknowledge that our results in 2D may not match the visual quality achieved by large models or diffusion-based methods. Although CLIPGaussian is designed for universal applicability, its performance is contingent upon the quality of the underlying base models. Moreover, when the selected base fails to reconstruct the scene with high fidelity, style transfer may become intractable or produce perceptually implausible results."
        },
        {
            "title": "References",
            "content": "[1] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [2] Yingshu Chen, Guocheng Shao, Ka Chun Shum, Binh-Son Hua, and Sai-Kit Yeung. Advances in 3d neural stylization: survey. International Journal of Computer Vision, pages 136, 2025. [3] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. [4] Yi-Hua Huang, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Sc-gs: Sparse-controlled gaussian splatting for editable dynamic scenes. arXiv preprint arXiv:2312.14937, 2023. [5] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. arXiv preprint arXiv:2309.13101, 2023. [6] Joanna Waczynska, Piotr Borycki, Joanna Kaleta, Sławomir Tadeja, and Przemysław Spurek. D-miso: Editing dynamic 3d scenes using multi-gaussians soup, 2024. [7] Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, and Jun Zhang. Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting. In European Conference on Computer Vision, 2024. [8] Joanna Waczynska, Tomasz Szczepanik, Piotr Borycki, Sławomir Tadeja, Thomas Bohné, and Przemysław Spurek. Mirage: Editable 2d images using gaussian splatting. arXiv preprint arXiv:2410.01521, 2024. [9] Yang-Tian Sun, Yihua Huang, Lin Ma, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Splatter video: Video gaussian representation for versatile processing. Advances in Neural Information Processing Systems, 37:5040150425, 2024. [10] Weronika Smolak-Dyzewska, Dawid Malarz, Kornel Howil, Jan Kaczmarczyk, Marcin Mazur, Przemysław Spurek, et al. Vegas: Video gaussian splatting. arXiv preprint arXiv:2411.11024, 2024. [11] Lihua Lu, Ruyang Li, Xiaohui Zhang, Hui Wei, Guoguang Du, and Binqiang Wang. Advances in text-guided 3d editing: survey. Artificial Intelligence Review, 57(12):161, 2024. [12] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. Small models are valuable plug-ins for large language models, 2023. [13] Cyrus Vachha and Ayaan Haque. Instruct-gs2gs: Editing 3d gaussian splats with instructions, 2024. [14] Minghao Chen, Iro Laina, and Andrea Vedaldi. Dge: Direct gaussian 3d editing by consistent multi-view editing. In European Conference on Computer Vision, pages 7492. Springer, 2024. [15] Jing Wu, Jia-Wang Bian, Xinghui Li, Guangrun Wang, Ian Reid, Philip Torr, and Victor Adrian Prisacariu. Gaussctrl: Multi-view consistent text-driven 3d gaussian splatting editing. In European Conference on Computer Vision, pages 5571. Springer, 2024. [16] Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, and Shijian Lu. Stylegaussian: Instant 3d style transfer with gaussian splatting. In SIGGRAPH Asia 2024 Technical Communications, pages 14. 2024. 10 [17] Yiqun Mei, Jiacong Xu, and Vishal Patel. Regs: Reference-based controllable scene stylization with gaussian splatting. Advances in Neural Information Processing Systems, 37:40354061, 2024. [18] Xin-Yi Yu, Jun-Xin Yu, Li-Bo Zhou, Yan Wei, and Lin-Lin Ou. Instantstylegaussian: Efficient art style transfer with 3d gaussian splatting. arXiv preprint arXiv:2408.04249, 2024. [19] Bingjie Song, Xin Huang, Ruting Xie, Xue Wang, and Qing Wang. Style3d: Attention-guided multi-view style transfer for 3d object generation. arXiv preprint arXiv:2412.03571, 2024. [20] Sahil Jain, Avik Kuthiala, Prabhdeep Singh Sethi, and Prakanshul Saxena. Stylesplat: 3d object style transfer with gaussian splatting. arXiv preprint arXiv:2407.09473, 2024. [21] Áron Samuel Kovács, Pedro Hermosilla, and Renata Raidou. G-style: Stylized gaussian splatting. In Computer Graphics Forum, volume 43, page e15259. Wiley Online Library, 2024. [22] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 15011510, 2017. [23] Jamie Wynn, Zawar Qureshi, Jakub Powierza, Jamie Watson, and Mohamed Sayed. Morpheus: Text-driven 3d gaussian splat shape and color stylization. arXiv preprint arXiv:2503.02009, 2025. [24] Yian Zhao, Wanshi Xu, Yang Wu, Weiheng Huang, Zhongqian Sun, and Wei Yang. Progdf: Progressive gaussian differential field for controllable and flexible 3d editing. arXiv preprint arXiv:2412.08152, 2024. [25] Dong In Lee, Hyeongcheol Park, Jiyoung Seo, Eunbyung Park, Hyunje Park, Ha Dam Baek, Shin Sangheon, Sangpil Kim, et al. Editsplat: Multi-view fusion and attention-guided optimization for view-consistent 3d scene editing with 3d gaussian splatting. arXiv e-prints, pages arXiv2412, 2024. [26] Wanlin Liang, Hongbin Xu, Weitao Chen, Feng Xiao, and Wenxiong Kang. 4dstylegaussian: Zero-shot 4d style transfer with gaussian splatting. arXiv preprint arXiv:2410.10412, 2024. [27] Linzhan Mou, Jun-Kun Chen, and Yu-Xiong Wang. Instruct 4D-to-4D: Editing 4d scenes as pseudo-3d scenes using 2d diffusion. In CVPR, 2024. [28] Leon Gatys, Alexander Ecker, and Matthias Bethge. Image style transfer using convolutional In Proceedings of the IEEE conference on computer vision and pattern neural networks. recognition, pages 24142423, 2016. [29] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694711. Springer, 2016. [30] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. arXiv preprint arXiv:1603.03417, 2016. [31] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance In Proceedings of the IEEE International Conference on Computer Vision normalization. (ICCV), Oct 2017. [32] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Universal style transfer via feature transforms. Advances in neural information processing systems, 30, 2017. [33] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1132611336, June 2022. [34] Gihyun Kwon and Jong Chul Ye. Clipstyler: Image style transfer with single text condition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1806218071, 2022. [35] Ananda Padhmanabhan Suresh, Sanjana Jain, Pavit Noinongyao, Ankush Ganguly, Ukrit Watchareeruetai, and Aubin Samacoits. Fastclipstyler: Optimisation-free text-based image style transfer using style representations. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 73167325, January 2024. 11 [36] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang. Learning linear transformations for fast image and video style transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 38093817, 2019. [37] Zijie Wu, Zhen Zhu, Junping Du, and Xiang Bai. Ccpl: Contrastive coherence preserving loss for versatile style transfer. In European conference on computer vision, pages 189206. Springer, 2022. [38] Quanjian Song, Mingbao Lin, Wengyi Zhan, Shuicheng Yan, Liujuan Cao, and Rongrong Ji. Univst: unified framework for training-free localized video style transfer. arXiv preprint arXiv:2410.20084, 2024. [39] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595415964, 2023. [40] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2320623217, 2023. [41] Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, and Limin Wang. Bivdiff: training-free framework for general-purpose video synthesis via bridging image and video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73937402, 2024. [42] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Rerender video: Zero-shot text-guided video-to-video translation. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. [43] Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-a-video: Agile diffusion for arbitrary text-based video style transfer. IEEE Signal Processing Letters, 2024. [44] Bohai Gu, Heng Fan, and Libo Zhang. Two birds, one stone: unified framework for joint learning of image and video style transfers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2354523554, 2023. [45] Joanna Materzynska, Antonio Torralba, and David Bau. Disentangling visual and written concepts in clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1641016419, 2022. [46] Tianye Li, Mira Slavcheva, Michael Zollhöfer, Simon Green, Christoph Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele, Richard Newcombe, and Zhaoyang Lv. Neural 3d video synthesis from multi-view video. pages 55115521, 06 2022. [47] Rinon Gal, Or Patashnik, Haggai Maron, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-guided domain adaptation of image generators. ACM Transactions on Graphics (TOG), 41(4):113, 2022. [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [49] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020. [50] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. [51] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. [52] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa. In Proceedings of the IEEE/CVF Instruct-nerf2nerf: Editing 3d scenes with instructions. International Conference on Computer Vision, pages 1974019750, 2023. [53] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image 12 diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [54] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1132611336, 2022. [55] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-nerf: Neural radiance fields for dynamic scenes. arXiv preprint arXiv:2011.13961, 2020. [56] Wanlin Liang, Hongbin Xu, Weitao Chen, Feng Xiao, and Wenxiong Kang. 4dstylegaussian: Zero-shot 4d style transfer with gaussian splatting, 2024. [57] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. [58] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, 2016. [59] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2031020320, 2024. [60] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. arXiv preprint arXiv:2310.10642, 2023. [61] Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei Qin, Guo Lu, Jing Geng, and Jun Zhang. Gaussianimage: 1000 fps image representation and compression by 2d gaussian splatting, 2024. [62] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In 3DV, 2024. 13 Base Model Choice: Implementation Details 3D The vanilla Gaussian Splatting [3] framework represents scene using dense set of 3D Gaussians, denoted as = {(N (mi, Σi), σi, ci)}n i=1,, where specifies the mean position, Σ the covariance matrix, σ the opacity and the colors of the Gaussians spherical harmonics. The optimization procedure iteratively renders images from these Gaussians and refines their parameters by minimizing the discrepancy between the rendered outputs and the corresponding ground-truth training views. Our method refines Gaussian primitives by optimizing their position, color, scale, rotation, and opacity parameters, allowing for both appearance and geometry modifications. Our model is trained for 5000 steps without densification or pruning using loss function described in the section 3. If not stated otherwise we set λb = 1000, λp = 90, λd = 5, λc = 0.8, patch_size = 128, num_patch = 64 and feature_lr = 0.01. Rest of hyperparameters follow default values of original GS implementation. 4D Many models in the literature are dedicated to 4D (3D dynamic) scenes [4, 59, 60]. Our approach can be adapted to all models since they all use General Gaussian components and an additional model for modeling time dependence. To present our concept for such models, we choose the D-MiSo model [6], which extends the Gaussian Splatting framework through Multi-Gaussian structure and deform networks. multi-Gaussian structure uses hierarchy based on the relationship between the Core and sub-Gaussian. Core Gaussians are designated to capture and model motion dynamics: Gcore = {(Ncore(mi, Ri, Si), σi, ci)}p i=1, (6) where specifies the mean position, the rotation matrix, the scaling parameters, σ the opacity and the colors of the Gaussians. Sub-Gaussians serve to enhance rendering quality, and is defined as Gsub = {(Nsub(m + RαiαiαiT , Ri, Si), σi, ci)}k i=1, (7) where m, is Core-Gaussian position and rotation; and αiαiαi are trainable parameters used to define the positions of the Sub-Gaussian relative to the Core-Gaussian. CLIPGaussian style transfer jointly optimizes the Gaussian primitive parameters and the deformation networks. Our first stage is classical D-MiSo framework, which gives us scenes represented by trainable parameters of D-MiSo, see Fig. 2. The second phase is dedicated to style transfer. Similar to 3D as default we set iterations = 5000, λp = 90, λd = 5, λc = 0.8, patch_size = 128, num_patch = 64. We use feature_lr = 0.025. Rest of hyperparameters follow default values of original D-MiSo implementation. The background loss weighting parameter λb was set to default value of 0, which is suitable for most styling experiments with text prompts. In special cases, it was adjusted to 500. 2D Adapting 3D Gaussian Splatting frameworks, originally designed for 3D scene representation, to the domain of single-image 2D reconstruction presents significant challenges [61]. MiraGe [8] models 2D image by embedding flat, parametric Gaussian primitives within perceptually motivated 3D latent space, aligning with human perception. Each Gaussian is defined as = {(N (mi, Ri, Si), σi, ci)}n i=1, where is rotation matrix and = diag(s1, s2, s3) is diagonal matrix containing the scaling parameters, where s1 = ϵ. To improve generalization and capture symmetries inherent in natural 2D images, MiraGe employs mirrored input image M(I) during training as form of data augmentation. While the standard 3D Gaussian Splatting framework employs loss function of the form = (1 λL1(I, GS(I)) + λLDSSIM (I, GS(I)), the MiraGe model extends this by incorporating both the original image and its mirrored utilizes cost function L(I) + L(M(I)). We optimize position, color, scale, rotation, and opacity parameters of the Gaussian primitives. We train our model for 5000 steps without densification or pruning, using loss function described in the section 3 (modified by using mirror camera as in MiraGe). If not stated otherwise we set λb = 0, λp = 90, λd = 5, λc = 0.8, patch_size = 128, num_patch = 64 and feature_lr = 0.0025. Rest of hyperparameters follow default values of original MiraGe implementation. Video For video representation, we use the VeGaS model [10], which models video as collection of 3D Folded Gaussians, denoted as GVeGaS = (FN (m, Σ, a, ), ρ, c) (8) 14 Here, and are temporal folding functions that modulate the Gaussian shape over time. Each video frame is rendered as 2D projection of these Gaussians, positioned and shaped according to their trained spatial-temporal parameters. CLIPGaussian maintains temporal consistency. This is because video content is modeled as set of Gaussian primitives, where each Gaussian spans multiple frames. As result, style modifications applied to the underlying Gaussians naturally propagate coherently over time. To preserve it, our framework optimizes only colors of the Gaussians spherical harmonics. Our model is trained for 5000 steps without densification or pruning using loss function described in the section 3. If not stated otherwise we set λp = 90, λd = 5, λc = 0.5, patch_size = 128, num_patch = 64 and feature_lr = 0.02. Rest of hyperparameters follow default values of original VeGaS implementation."
        },
        {
            "title": "B Extended Experimental Results and Evaluation",
            "content": "This section presents additional experimental results and analyses. We begin with detailed description of the evaluation metrics used to assess the effectiveness of our approach. Subsequently, we provide extended results across various data modalities, including images, videos, 3D objects, and 4D dynamic scenes. B.1 Evaluation Metrics In the quantitative evaluation, we report four metrics. CLIP Directional Similarity [47] and CLIP-S [51] measure the quality of style transfer while CLIP Directional Consistency [52] and CLIP-F [53] measure the consistency and similarity to the original content. For all metrics, we use ViT-L/14 CLIP model [48]. Let Epos denote the CLIP embedding of style (either style image or style prompt), and Eneg denote the embedding of negative prompt. In the case of the NeRF Synthetic dataset, we use object names (e.g., \"a lego\" for lego object) with ImageNet prompt templates [48]. Let Erender(i) and Egt(i) represent the CLIP embeddings of the stylized render and the ground truth of the i-th test image, respectively. is the size of test set. We define the CLIP Directional Similarity and CLIP-S as follows: CLIP-SIM = 1 N (cid:88) i=1 cos(Erender(i) Egt(i), Epos Eneg), CLIP-S = 1 (cid:88) i=1 cos(Erender(i), Epos) (9) (10) Assuming that testing frames come from video, we can also define CLIP Directional Consistency and CLIP-F as follows: CLIP-CONS = 1 1 1 (cid:88) i=1 cos(Erender(i + 1) Erender(i), Egt(i + 1) Egt(i)) (11) CLIP-F = (cid:80)N 1 i=1 cos(Erender(i + 1), Erender(i)) i=1 cos(Egt(i + 1), Egt(i)) (cid:80)N 1 (12) 16 B.2 3D: Additional Results and Explanation This section provides detailed description of the quantitative evaluation setup. During experiments we used two objects (lego and hotdog) from the NeRF-Synthetic dataset [49], and two scenes (garden and bonsai) from the Mip-NeRF 360 dataset [50]. Each was stylized using four text prompts (\"Fire\", \"Mosaic\", \"Starry Night by Vincent van Gogh\", and \"Scream by Edvard Munch\") and four style images (as shown in Fig. 6). We evaluated CLIPGaussian (our method), DGE [14], and G-Style [21] using the same base models trained with the original Gaussian Splatting codebase4. Since Instruct-GS2GS [13] is incompatible with this codebase, it was evaluated on models trained using Nerfstudio5. Due to the high VRAM requirements of StyleGaussian [16], it was evaluated on models trained with its own training script, which limits the number of Gaussians to 105. As Instruct-GS2GS [13] and DGE [14] employ different prompt templates, we prepended our text prompts with the prefixes used in their original papers: \"Turn it into \"for Instruct-GS2GS and \"Make it look like a\" for DGE. Baseline methods were trained using their default configurations. CLIPGaussian models were trained according to the settings described in Appendix A, except for the \"Fire\" condition, which used higher feature learning rate (0.02). Additionally, for the lego object with the \"Fire\" condition, we set λbg = 100. B.2.1 Training Time We evaluate training time using the same dataset as in the user study and quantitative analysis. For each scene, we report the average training time across all styles. Table 3 shows how the number of Gaussian primitives affects stylization time. Table 3: Number of Gaussian primitives impact on stylization time. Scene Number of Gaussians Avg. stylization time hotdog lego bonsai garden 0.14M 0.31M 1.35M 4.48M 11m29s 11m36s 11m37s 21m03s B.2.2 User Study: Details We conducted formal survey using the CLICKworker platform 6. This allowed us to recruit demographically balanced participant pool in terms of gender, age. Each participant was presented with questionnaire in which they evaluated stylized objects or scenes stylized using text prompt or reference image. We wanted the survey for both conditional text and image to be comparable, so we chose prompts corresponding to images. Table 4: Friedman statistic test Fr and values for user study Question 1. In case of image condition we consider StyleGaussian G-Style, CLIPGaussian and CLIPGaussian, in case text condition we consider I-GS2GS, DGE, CLIPGaussian object scene object scene condition image image text text Fr 139.3 1178.8 54.70 8.40 5.62e-31 1.44e-39 1.32e-12 0. The survey was divided into two parts: main evaluation section and additional question in the end. The main section included eight comparative examples, each involving stylized outputs from three different methods (see Fig. 5, 6). Fig. 18 illustrates typical evaluation interface. In each example, participants were first shown the reference image or text prompt used for styling. This was followed by GIF animation showing the original object or scene, and then the stylized results from each method. For each evaluation we also asked about stylized factors like Color Palette, Lighting, Detail Level, Mood or Atmosphere, Brushstrokes\". Main evaluation consisted of four questions: 4https://github.com/graphdeco-inria/gaussian-splatting 5https://github.com/nerfstudio-project/nerfstudio 6https://www.clickworker.com [access: 12.05.2025] 17 Figure 14: User study: Comparison of 3D style transfer using referenced image. Figure 15: User study: Comparison of 3D style transfer using text condition. Figure 16: User study: Comparison of 3D style transfer using text condition.User ratings of factors influencing their judgment of styled 3D objects/scenes by text. Figure 17: User study: Comparison of 3D style transfer using text condition.User ratings of factors influencing their judgment of styled 3D objects/scenes by image. Question 1: \"Please rank the generated results according to how closely they match the style from most similar(1) to least similar(3)\" focused on style transfer quality. Participants ranked the three methods based on how well they reflected the reference style. Each method could be selected only once per ranking. For example, Method might be ranked 3rd, Method 1st, and Method 2nd. Question 2: \"On scale from \"Very Low\" to \"Very High\", how would you rate the visual appeal of each generated result?\" assessed visual appeal. For each method, participants selected one of several options (\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\" ). This allowed users to express their perception of the aesthetic quality of each result independently. Question 3: \"On scale from \"Very difficult\" to \"Very easy\", How well can you recognize the original scene content in each generated result\" focused on recognizability of content. The users evaluated how clearly the original object or scene was preserved after stylization. Options (\"Very Difficult\", \"Difficult\", \"Medium\", \"Easy\", \"Very Easy\") independently rated for each method. Question 4: \"On scale from \"Not important at all\" to \"Extremely important\" how much does this factor influence your judgment of the styled 3D object?\", asked about the factors influencing the users decision. Participants chose from predefined options (\"Not important at all\", \"Not important\", \"Important\", \"Extremely important\"). Each factor was rated separately, following the same format as questions 2 and 3. Additional question: 18 In the context of \"Starry Night by Vincent van Gogh\" and \"Scream by Edvard Munch\". We asked an additional question: \"Were you familiar with the following images before?\". Users could answer \"Yes\" or \"No\". The poll format was consistent across all object/scene and text/image conditions. For evaluations based on text prompts, we wanted to avoid introducing visual bias, and we didnt attach any referenced image. Therefore, we included only those participants who were already familiar with the references, such as \"Scream by Edvard Munch\" and \"Starry Night by Vincent van Gogh\". Tab. 4 shows Friedman statistic test Fr and values for Question 1 user study. For the image condition, we evaluated three methods: StyleGaussian, G-Style, and CLIPGaussian. For the text condition, the evaluated methods were IGS2GS, DGE, and CLIPGaussian. In all scenarios (1-4) we observe significant differences between methods (reject H0). Additional results for image-based style transfer are presented in Fig. 14, while results for text-based style transfer are shown in Fig. 15. The post hoc Conover Friedman test for user study on stylized objects/scenes by image/text revealed statistically significant differences between methods (p < 0.05), indicating if the null hypothesis can be rejected is shown in the Tabs. 5, 6, 7 ,8. In almost every case we can reject H0, the exception is the comparison of our model with G-Style in the case of evaluation on objects stylized with reference image. Fig 16 and Fig. 17 show that the majority of participants identified color and lighting as highly influential factors in their assessment of styled 3D objects/scenes. In comparison, brushstroke details were generally regarded as less important to their judgments. high-resolution comparison across object/scenes styled by text-based methods is shown in Fig. 19 and Fig. 20. corresponding comparison for image-based methods is presented in Fig. 21 and Fig. 22. Figure 18: Survey interface view from the ClickWorker website, users evaluate stylization of the 3D object hotdog conditioned on the painting Starry Night by Vincent van Gogh. Table 5: Calculated p-value Posthoc Conover Friedman test for user study in case of objects stylized by image. StyleGaussian G-Style CLIPGaussian StyleGaussian 1.00e+00 1.45e-25 5.71e-33 G-Style 1.45e-25 1.00e+00 6.67e-02 CLIPGaussian 5.71e-33 6.66e-02 1.00e+00 Table 7: Calculated p-value Posthoc Conover Friedman test for user study in case of objects stylized by text. I-GS2GS G-Style CLIPGaussian I-GS2GS 1.000e+00 6.67e-05 2.39e-14 DGE 0.000067 1.000000 0.000125 CLIPGaussian 2.39e-14 1.24e-04 1.00e+00 Table 6: Calculated p-value Posthoc Conover Friedman test for user study in case of scene stylized by image. StyleGaussian G-Style CLIPGaussian StyleGaussian 1.00e+00 8.90e-51 1.68e-17 G-Style 8.90e-51 1.00e+00 3.32e-15 CLIPGaussian 1.68e-17 3.32e-15 1.00e+00 Table 8: Calculated p-value Posthoc Conover Friedman test for user study in case of scene stylized by text. I-GS2GS DGE CLIPGaussian I-GS2GS 1.00e+00 6.67e-05 2.39eDGE 0.000067 1.00e+00 0.000125 CLIPGaussian 2.39e-14 1.24e-04 1.00e+00 19 B.2.3 Hyperparameters Analysis We evaluate the visual quality of 3D style transfer with respect to the parameters feature_lr and patch_size on three objects from the NeRF-Synthetic dataset: lego, hotdog, and mic. These objects are stylized using prompts such as \"Starry Night by Vincent van Gogh\", \"Fire\", and \"The Great Wave off Kanagawa by Katsushika Hokusai\". The quantitative impact of feature_lr is shown in Tab. 9, while the influence of patch_size is reported in Tab. 10. Our observations indicate that both parameters contribute to enhancing the stylistic expressiveness of the output. However, excessively large values introduce greater flexibility in the spatial distribution of the Gaussians, resulting in noticeable loss of content detail and reduced spatial consistency (see Fig. 23). Additionally, we evaluate the effect of the weighting factors of the loss components, λp and λd. The quantitative impact of λp is shown in Tab. 11, while the influence of λd is reported in Tab. 12. Qualitative results are presented in Fig. 24. higher λp increases the expressiveness of local style transfer; however, excessively high values may lead to overstylization of the object. Similarly, increasing λd enhances the global style characteristics. Quantitative evaluations suggest that λd parameter has limited impact on spatial consistency and content similarity. Table 9: Effect of feature learning rate on the performance of CLIPGaussian in terms of the CLIP metrics. patch_size CLIP-S CLIP-SIM CLIP-F CLIP-CONS 32 64 128 256 18.53 21.73 25.60 27.45 19.51 19.31 29.78 32.65 98.08 97.70 97.87 97. 15.02 11.06 6.93 4.42 Table 10: Effect of patch size on the performance of CLIPGaussian in terms of the CLIP metrics. feature_lr CLIP-S CLIP-SIM CLIP-F CLIP-CONS 0.0025 0.005 0.01 0.02 25.24 25.60 25.61 26.27 29.31 29.77 29.93 30. 97.68 97.99 97.61 97.54 7.91 6.93 6.26 6.35 Table 11: Effect of λp parameter on the performance of CLIPGaussian in terms of the CLIP metrics. λp 0 90 180 CLIP-S CLIP-SIM CLIP-F CLIP-CONS 16.00 23.67 24.36 14.12 25.07 25.27 98.74 97.95 97.78 11.03 2.36 1.48 Table 12: Effect of λd parameter on the performance of CLIPGaussian in terms of the CLIP metrics. λd 0 5 10 CLIP-S CLIP-SIM CLIP-F CLIP-CONS 22.20 23.67 23.55 22.76 25.07 25.15 97.71 97.95 98.04 2.52 2.32 2."
        },
        {
            "title": "Style",
            "content": "I-GS2GS [13] DGE [14] CLIPGaussian"
        },
        {
            "title": "Starry Night\nby Vincent\nvan Gogh",
            "content": "Mosaic Scream by Edvard Munch Fire Starry Night by Vincent van Gogh Mosaic Scream by Edvard Munch Figure 19: Full comparison of CLIPGaussian (our) and baseline models in 3D style transfer, conditioned by text, on hotdog and lego objects from NeRF-Synthetic dataset [49]."
        },
        {
            "title": "Style",
            "content": "I-GS2GS [13] DGE [14] CLIPGaussian"
        },
        {
            "title": "Mosaic",
            "content": "Scream by Edvard Munch Fire Starry Night by Vincent van Gogh Mosaic Scream by Edvard Munch Figure 20: Full comparison of CLIPGaussian (our) and baseline models in 3D style transfer, conditioned by text, on garden and bonsai objects from Mip-NeRF 360 dataset [50]."
        },
        {
            "title": "Style",
            "content": "StyleGaussian [16] G-Style [21] CLIPGaussian Figure 21: Full comparison of CLIPGaussian (our) and baseline models in 3D style transfer, conditioned by image, on hotdog and lego objects from NeRF-Synthetic dataset [49]."
        },
        {
            "title": "Style",
            "content": "StyleGaussian [16] G-Style [21] CLIPGaussian Figure 22: Full comparison of CLIPGaussian (our) and baseline models in 3D style transfer, conditioned by image, on garden and bonsai objects from Mip-NeRF 360 dataset [50]. 24 patch_size 128 256 4 0 0 . 2 0 0 . 1 0 . 0 4 0 . 0 2 0 0 . 0 1 0 0 . _ t 4 0 . 0 2 0 . 0 1 0 0 . Figure 23: Effect of patch size and feature learning rate on the performance of CLIPGaussian on lego, hotdog and mic objects from NeRF-Synthetic dataset [49]. Objects are stylized with \"Starry Night by Vincent van Gogh\", \"Fire\" and \"The Great Wave off Kanagawa by Katsushika Hokusai\" prompts. 25 λp 90 180 0 5 0 0 5 0 1 λ 0 0 1 Figure 24: Effect of λp and λd rate on the performance of CLIPGaussian on lego, hotdog and mic objects from NeRF-Synthetic dataset [49]. Objects are stylized with \"Starry Night by Vincent van Gogh\", \"Fire\" and \"The Great Wave off Kanagawa by Katsushika Hokusai\" prompts. 26 B.3 4D: Additional Results and Explanation This section delves deeper into our experimental analysis of 4D scenes. We begin with detailed exploration of stage 2 hyperparameters, followed by demonstrating the adaptability of our method on an additional multi-camera dataset. Fig. 25 presents style transfer on selected objects from the D-NeRF dataset. These results illustrate that CLIPGaussian enables style conditioning via both text prompts and reference images, highlighting its versatility across multiple guidance modalities."
        },
        {
            "title": "Starry Night\nby Vincent\nvan Gogh",
            "content": "Scream by Edvard Munch Figure 25: Style transfer results on samples from the D-NeRF dataset [55]. Our CLIPGaussian model accommodates both text-based and image-based style inputs. B.3.1 Hyperparameters Analysis For dynamic 3D scenes using the D-MiSo model, the background parameter was set to the default value of 0 in most cases. However, using the prompt \"Summer\" we could observe the background stylization as well. Fig. 26 shows the effect of background loss on the Jumpingjacks object: without background loss (λbg = 0) and with background loss (λbg = 500). More precisely, we used the alpha channel from the original images available in D-NeRF dataset to create background mask for each view. In this case Lb = λb (m(Il) m(RG(Il))). λbg = 0 λbg = We evaluate the visual quality of style transfer in 4D with respect to feature_lr and the number of patches n. Fig. 27 shows two representative objects from the D-NeRF dataset, each conditioned on distinct text prompt (Hellwarrior - \"The Great Wave of Kanagawa by Katsushika Hokusai\", Jumpingjacks - \"Spring\"). Features_lr is influences primarily the color saturation of Gaussians. We observe that increasing this parameter enhances the stylistic expressiveness of the output. higher generally improves styles details. In Hellwarrior example for the larger we see more water swirls appropriate to the style. However, excessively large values (e.g., n=128) introduce increased freedom in the Gaussians spatial distribution, leading to notable loss of content detail. This effect is particularly pronounced in the Jumpingjacks example, where reconstruction of the hand visibly deteriorates. Figure 26: Effect of background loss λbg on stylization Jumpingjacks object with the prompt \"Summer\". Fig. 28 shows that increasing batch size and the number of patches leads to noticeably improved visual fidelity in style transfer. Increasing the batch size enables the preservation of finer visual details. We check the influence of batch size hyperparameter from the chosen base model (D-MiSo). larger 27 32 Patch size 128 5 2 0 0 . . 5 2 0 0 0 _ t 5 2 0 . 5 2 0 0 . 0 Figure 27: Visual comparison of style transfer results on D-NeRF objects under two text prompts \"The Great Wave of Kanagawa by Katsushika Hokusai\" and \"Spring\", showing the impact of features_lr and number of patches. a 1 2 4 CLIP-S 27.73 27.75 26. Hook CLIPSIM 22.68 23.11 23.42 Train time 10:12 20:59 39:31 Jumpingjacks CLIPSIM 18.32 18.68 17.64 Train time 10:35 20:32 42:25 CLIP-S 23.15 23.19 22. Trex CLIPSIM 23.57 22.94 22.13 CLIP-S 28.00 27.54 27.47 Train time 10:41 20:53 42:01 BouncingBalls CLIPSIM 22.81 24.36 25.37 Train time 11:43 21:49 43: CLIP-S 24.77 25.27 26.35 a 1 2 4 Hellwarrior CLIPSIM 23.52 24.53 25.85 Train time 10:17 20:56 39:37 CLIP-S 24.58 25.40 26.02 Mutant CLIPSIM 21.58 21.99 21.86 CLIP-S 29.76 29.76 29.43 Train time 10:43 20:08 39:57 CLIP-S 24.55 24.21 23.02 Standup CLIPSIM 21.43 21.42 20.70 Train time 10:28 21:18 39:17 Table 13: Numerical comparison of training time and CLIP score, CLIP-SIM metrics for the selected object under the text condition Starry Night by Vincent van Gogh and number of patch=32, checking influence of batch size hyperparameter from the chosen base model (D-MiSo) batch size prevents deformation of the subjects head. This highlights that our method faithfully inherits and reinforces the structural integrity imparted by the base architecture. Table 13 presents comparison of numerical training time and CLIP-S, CLIP-SIM metrics for the selected object under batch size, the text condition Starry Night by Vincent van Gogh and number of patch=32. The training time results reflect dependency on the base architecture. The CLIP-S suggests that models trained with smaller batch strategies achieve marginally higher. However, the differences in CLIP-S between models remain relatively minor. In particular, despite the small numerical variance, visual comparison  (Fig. 28)  reveals substantial qualitative differences in output fidelity, highlighting the limitations of current metrics in capturing perceptual quality. 28 2 Number of patches 32 64 1 3 s a Figure 28: Effect of Batch Sizes and Number of patches using D-MiSo base model. Mutant from D-NeRF is stylized with \"Starry Night by Vincent van Gogh\". Text condition Styled images Fire Starry Night by Vincent van Gogh The Great Wave of Kanagawa by Katsushika Hokusai Figure 29: Qualitative style transfer results on samples from the PanopticSports dataset [62]. B.3.2 Multi-Camera Setup Fig. 8 in main paper shows experiment on The Neural 3D Video dataset (DyNeRF) [46]. It consists of videos captured by 21 cameras for each scene. The multi-view inputs were time synchronized and the images were extracted at 30FPS. In our experiments we use the first 24 frames following the data loader provided in [5] to show the capabilities of CLIPGaussian in real 4D scenes. In this dataset only the frontal views of the scenes are shown. In contrast the PanopticSports datasets [62] is full 360 view dataset, which comprises dynamic scenes featuring significant object and actor movements. Each scene was recorded using 31 cameras over 150 timesteps. Fig. 29 shows the transferred style is prominently expressed on both the primary actor and the background. 29 B.4 2D: Additional Results and Explanation This section presents additional qualitative results for 2D style transfer. Fig. 30 shows qualitative comparison with similar text-based methods: CLIPstyler [34] and FastCLIPstyler [35]. Fig. 31 presents an additional comparison with AdaIN [31] and StyTr2 [33]. CLIPGaussian is competitive alternative to these methods. We see that our method mainly focuses on details."
        },
        {
            "title": "Original",
            "content": "CLIPstyler [34] FastCLIPstyler [35] CLIPGaussian"
        },
        {
            "title": "Starry Night\nby Vincent\nvan Gogh",
            "content": "Mosaic Scream by Edvard Munch Figure 30: Comparison of image style transfer using text condition. Style Original AdaIN [31] StyTr2 [33] CLIPGaussian Figure 31: Comparison of image style transfer using image condition. 30 B.5 Video: Additional Results and Explanation This section provides detailed description of the quantitative evaluation setup. During experiments we used four videos (camel, bear, train and blackswan) from the DAVIS dataset [58]. Similarly to evaluation of 3D style transfer, each was stylized using four text prompts (\"Fire\", \"Mosaic\", \"Starry Night by Vincent van Gogh\", and \"Scream by Edvard Munch\") and four style images (same as in Fig. 32). As Text2Video [39] and RerenderAVideo [42] employ different prompt templates, we use \"Make it {style} style\" for Text2Video and \"A { object} in {style} style\" for RerenderAVideo, where style is CLIPGaussian style prompt and object is video name i.e. bear, blackswan, camel or train. Baseline methods were trained using their default configurations. CLIPGaussian models were trained according to the settings described in Appendix A. On average CLIPGaussian took around 11 minutes to style video from DAVIS dataset. Fig. 32 and Fig. 33 show example comparison on subset of the test set on text and image conditioning respectively. Tables 14 and 15 presents quantitative evaluation results on dataset described above. Style & GT CCPL [37] UniST [44] CLIPGaussian Style & GT Text2Video [39] Rerender [42] CLIPGaussian Figure 32: Comparison of video style transfer using image condition on DAVIS dataset [58]. Figure 33: Comparison of video style transfer using text condition on DAVIS dataset [58]. Table 14: Quantitative comparison of video style transfer using referenced style prompt, compared against baseline methods. Model CLIP-S CLIP-SIM CLIP-F CLIP-CONS CCPL [37] UniST [44] CLIPGaussian 18.89 15.93 74. 8.20 3.85 17.60 97.92 99.36 99.18 -0.02 5.16 1.27 Table 15: Quantitative comparison of video style transfer using referenced style prompt, compared against baseline methods. Model CLIP-S CLIP-SIM CLIP-F CLIP-CONS Rerender [42] Text2Video [39] CLIPGaussian 19.40 26.05 26.25 98.23 93.63 99.00 -0.03 0.03 1.92 9.83 24.99 24."
        }
    ],
    "affiliations": [
        "Jagiellonian University"
    ]
}