{
    "paper_title": "PointArena: Probing Multimodal Grounding Through Language-Guided Pointing",
    "authors": [
        "Long Cheng",
        "Jiafei Duan",
        "Yi Ru Wang",
        "Haoquan Fang",
        "Boyang Li",
        "Yushan Huang",
        "Elvis Wang",
        "Ainaz Eftekhar",
        "Jason Lee",
        "Wentao Yuan",
        "Rose Hendrix",
        "Noah A. Smith",
        "Fei Xia",
        "Dieter Fox",
        "Ranjay Krishna"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pointing serves as a fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, a curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, a real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions. Project page: https://pointarena.github.io/"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 0 9 9 9 0 . 5 0 5 2 : r PointArena: Probing Multimodal Grounding Through Language-Guided Pointing Long Cheng1 Jiafei Duan1,2 Yi Ru Wang1 Haoquan Fang1 Boyang Li1 Yushan Huang1 Elvis Wang3 Ainaz Eftekhar1,2 Jason Lee1 Wentao Yuan1 Rose Hendrix2 Noah A. Smith1,2 Fei Xia1 Dieter Fox1 Ranjay Krishna1,2 1University of Washington 2Allen Institute for Artificial Intelligence 3Anderson Collegiate Vocational Institute https://pointarena.github.io"
        },
        {
            "title": "Abstract",
            "content": "Pointing serves as fundamental and intuitive mechanism for grounding language within visual contexts, with applications spanning robotics, assistive technologies, and interactive AI systems. While recent multimodal models have started to support pointing capabilities, existing benchmarks typically focus only on referential object localization tasks. We introduce PointArena, comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. PointArena comprises three components: (1) Point-Bench, curated dataset containing approximately 1,000 pointing tasks across five reasoning categories; (2) Point-Battle, an interactive, web-based arena facilitating blind, pairwise model comparisons, which has already gathered over 4,500 anonymized votes; and (3) Point-Act, real-world robotic manipulation system allowing users to directly evaluate multimodal model pointing capabilities in practical settings. We conducted extensive evaluations of both state-of-the-art open-source and proprietary multimodal models. Results indicate that Molmo-72B consistently outperforms other models, though proprietary models increasingly demonstrate comparable performance. Additionally, we find that supervised training specifically targeting pointing tasks significantly enhances model performance. Across our multi-stage evaluation pipeline, we also observe strong correlations, underscoring the critical role of precise pointing capabilities in enabling multimodal models to effectively bridge abstract reasoning with concrete, real-world actions."
        },
        {
            "title": "Introduction",
            "content": "Pointing focuses our attention. It is one of the earliest and most universal non-verbal methods we use to communicate intent; in fact, children learn to point as prelinguistic form of communication [30]. Precise spatial groundingpointingenables wide range of practical and high-impact applications across robotics, assistive technology, human-computer interaction, and vision-language interfaces. In robotics, pointing-capable model can interpret language commands like pick up the red cup next to the bowl and translate them into precise spatial actions [38], enabling fine-grained object manipulation in cluttered environments [13]. In assistive technologies, systems can help visually impaired users by answering spatial queries such as where is the handle on this door? [6] or which one is the garlic? In education or creative tools, pointing allows for interactive visual tutoring, such as identifying components in scientific diagrams or guiding learner through painting [16]. Co-first authors. Co-second authors. Preprint. Under review. Even in everyday virtual assistants or search engines, the ability to refer to specific image regions via pointing could make multimodal interactions more intuitive and expressive [10]. Across these domains, pointing provides low-bandwidth yet powerful spatial interface for grounding language in visionprecise enough for manipulation [27], intuitive enough for communication, and general enough to scale with modern multimodal models. Figure 1: Overview of PointArena. PointArena consists of three components: Point-Bench, curated dataset for evaluating grounded pointing across five reasoning types; Point-Battle, live platform for blind, pairwise model comparisons with user voting; and Point-Act, real-world task involving manipulation via pointing-based language commands. Recent advances in multimodal models have begun to incorporate more dynamic and spatially expressive forms of interaction. The Segment Anything Model (SAM) [19] enables segmentation from sparse visual prompts such as points or boxes, revealing the potential of fine-grained spatial control. Googles Gemini models [14] push the boundaries of long-context visual reasoning, incorporating multiple modalities over extended sequences. In parallel, new datasets have emerged to support explicit spatial referencing. Molmos PixMo dataset [10] introduces 2D pointing as form of multimodal alignment between images and instructions, while RoboPoint [38] focuses on spatial affordance prediction by linking instructions to interaction-relevant keypoints in robotic contexts. These setups bias evaluations toward pixel-level accuracy rather than conceptual reasoning and often lack diversity or scalability. There is need for holistic evaluation platform to make progress towards language-guided pointing. Although datasets for referring expressions exist (e.g.RefCOCO, RefCOCO+, and RefCOCOg [17, 35]), they are focused on subset of pointing tasks: object location. They lack the ambiguity and contextual variability that users expect from modern interactive models, limiting their utility for studying pragmatic or interactive applications. As result, they offer only partial insight into the full spectrum of grounding required for embodied or assistive agents. We, therefore, propose PointArena, platform to probe and evaluate grounded visual reasoning with pointing. PointArena presents suite of tasks where multimodal model must answer questions or resolve instructions by combining language and pointing gestures to identify specific image regions. These tasks go beyond traditional VQA by requiring spatial outputs (e.g., selecting location or region) rather than purely textual ones. PointArena allows for both unambiguous and ambiguous scenarios, supporting studies of disambiguation, spatial commonsense, and pragmatic inference. Unlike bounding boxes, segmentation masks, or free-form text responses, pointing offers high-precision signal that avoids reliance on object contours or dense annotations and is directly compatible with human evaluation. PointArena decomposes pointing into three stages of evaluation: 1) Point-Bench is curated dataset of 982 manually selected, annotated, and verified image-question pairs across five high-level categories (Spatial, Affordance, Counting, Steerable, and Reasoning), an interactive, online platform for blind, pairwise comparison between models based on user instructions. Users select from curated or custom-uploaded images. Voting is anonymized, and we have collected over 4, 500 votes from 2 more than 100 participants. 3) Point-Act is real-world benchmark that evaluates the utility of pointing in for downstream application. The system directs robotic arm to manipulate objects through pointing-based language commands. All three evaluation stages require minimal human effort; each is self-contained and can run live to evaluate any model. Through our evaluation of both open-source and proprietary models across the three stages of the PointArena benchmark, we find that Molmo-72B achieves the highest performance on Point-Bench, with proprietary models such as Gemini-2.5-Pro performing comparably. Models trained with explicit pointing supervision consistently outperform those without. We also observe strong correlation between static benchmark accuracy and human preference in Point-Battle. Notably, we find that adding language reasoning (e.g., Chain-of-Thought [32]) does not improve visual grounding for pointing tasks. Our study further reveals several other actionable insights into model behavior and evaluation design. We see PointArena as missing component necessary as we develop general-purpose vision-language models that can reason about and interact with the world."
        },
        {
            "title": "2 Related work",
            "content": "Grounding benchmarks. There are an abundance of benchmarks for visual grounding and spatial reasoning capabilities of multimodal large language models (MLLMs). The RefCOCO, RefCOCO+, and RefCOCOg datasets focus on 2D visual grounding, with RefCOCOg emphasizing long-form referring expressions and fine-grained object distinctions [36]. In 3D, ScanRefer provides 51,583 descriptions across 800 RGB-D scans, supporting joint language-geometry models for 3D object localization [7]. ReferIt3D and CityRefer extend this to fine-grained and outdoor settings, with CityRefer incorporating geographic features from OpenStreetMap [2, 23]. Interactive benchmarks such as GuessWhat?! evaluate multi-turn object grounding through binary dialog across 150K games [9]. Flickr30K Entities supports phrase-region grounding through 276K bounding boxes and cross-caption coreference annotations [26]. These datasets primarily focus on bounding box grounding, object retrieval, or dialog-based localization. They do not explicitly evaluate pointing behavior. Arena style evaluation. Arena-style evaluations have become common framework for comparing large language models (LLMs) via pairwise comparisons and user voting. Chatbot Arena established the core methodology, using anonymized head-to-head comparisons and Elo ratings, collecting over 240K human votes across 50+ models and 100+ languages [8]. The MT-Bench framework enhanced this approach by introducing specialized multi-turn dialogue evaluation system that assesses contextual understanding and reasoning through AI-powered scoring [40]. am-ELO addressed instability in Elo rankings by incorporating maximum likelihood estimation and annotator reliability modeling [21]. Auto-Arena fully automated the evaluation process using LLM-generated questions, peer model battles, and LLM-based voting, achieving 92.14% agreement with human votes [39]. BenchBuilder introduced automated benchmark construction from crowdsourced data, yielding ArenaHard-Auto with tighter confidence intervals than MT-Bench. Specialized variants such as Werewolf Arena evaluated models on social reasoning [5], while OpenArena enabled offline evaluations using LLMs as judges [28]. Other work has highlighted vulnerabilities, including susceptibility to vote manipulation [22], and proposed alternatives like Tournament Evaluation to improve robustness [18]. While arena-style methods are scalable and align with user preferences, they lack ground-truth supervision and are susceptible to adversarial influence. PointArena addresses these limitations by combining ground-truth evaluation (Point-Bench) with arena-style human evaluations (Point-Battle). Models that point or sketch. Multimodal large language models (MLLMs) have shown significant advances in reasoning capabilities, exemplified by GPT-4V, which demonstrates complex visionlanguage tasks such as image-based story generation and OCR-free mathematical reasoning [34]. Typical MLLM architectures employ encoder-decoder frameworks featuring early, intermediate, or joint visual-language fusion methods to enhance cross-modal integration. For instance, MiniGPT-4 leverages ViT-based visual features aligned with language models such as Vicuna via Q-Formers and learned projection layers [31]. Molmo notably improves 2D spatial grounding by predicting normalized coordinates directly from modality-specific embeddings, achieving up to 92% precision for tasks like desktop icon localization [11]. RoboPoint[37] uses instruction-tuned vision-language models trained on synthetic data to predict robotic affordances, significantly outperforming GPT-4o and PIVOT[24] by more than 20% in spatial accuracy, and effectively supports tasks in manipulation, navigation, and augmented reality without requiring real-world supervision. Other models, such as 3 VisCPM and Qwen-VL, incorporate region-conditioned controls and multilingual capabilities through multi-stage training approaches [34]. Additionally, newer systems like NExT-GPT extend capabilities further by integrating 3D point clouds, audio, and video modalities [34]. Despite these advancements, significant opportunities remain to further enhance the precision and accuracy of spatial localization within MLLMs. Key open questions include identifying factors that improve pointing capabilities and understanding their underlying mechanisms in multimodal models."
        },
        {
            "title": "3 PointArena",
            "content": "Evaluating the ability of MLLMs to localize language-referred entities in images requires benchmarks that are both precise and diagnostic. Existing benchmarks often emphasize classification or captioning, but fall short when it comes to assessing fine-grained spatial groundingthe ability to resolve natural language instructions into specific image coordinates. This capability is critical not only for understanding model alignment with human intent, but also for enabling downstream applications in robotics [37], augmented reality [12], and interactive web agents [15], and potentially contributing to explainability [25]. As both specialized pointing models and general-purpose MLLMs improve, standardized evaluation across open-source and proprietary systems becomes essential. We introduce PointArena, an evaluation suite for language-conditioned pointing. It consists of three phases of evaluation: (i) Point-Bench, curated dataset for controlled measurement of spatial localization accuracy; (ii) Point-Battle, live, blinded human-preference arena for pairwise model comparison; and (iii) Point-Act, real-world robotic task setting that evaluates pointing precision through real-world execution. Together, these components provide unified framework to quantify and analyze how well MLLMs ground language into visual space and even further, into the physical world. 3.1 Task formulation We formalize pointing as language-conditioned fine-grained localization task. The input consists of an RGB image RHW 3 and natural-language instruction prompt = {wt}T t=1. multimodal large language model (MLLM) Fθ takes (I, q) as input and predicts set of image-space coordinate points = {(xi, yi)}K i=1, where each point lies within the image bounds: xi [0, 1], yi [0, H1]. Ground-truth supervision is provided as set of binary masks {Mj}K j=1, with each mask Mj {0, 1}HW denoting the valid region for one of annotated targets. predicted point (xi, yi) is considered correct if it lies within the spatial support of some mask Mj, i.e., Mj[yi, xi] = 1. prediction is considered successful if: 1. The number of predicted points matches the number of target regions: = , 2. Each target region Mj is covered by at least one predicted point: (xi, yi) such that Mj[yi, xi] = 1. This formulation enables fully automated evaluation given access to the ground-truth masks, with no need for human annotators at test time. 3.2 Point-Bench Point-Bench is the largest benchmark for evaluating language-guided pointing, offering 982 textimage pairs with pixel-level target masks collected from public sources posted after 20 April 20, 2025. The data set is evenly divided into five task-driven categories, Spatial, Affordance, Counting, Steerable, and Reasoning, identified through survey of the question types most frequently tackled by open-source MLLMs [11, 37, 29]. Annotators recruited through crowd-sourcing were free to ask any question, but category-specific image selection naturally steers them toward prompts along the desired axis (e.g., scenes with repeated objects for counting). Together, these curated splits enable systematic evaluation of an MLLMs ability to recognize, reason, and precisely ground language in visual space. 4 Figure 2: Overview of the five Point-Bench categories and the annotation UI. Point-Bench includes 982 image-query pairs grouped into five categories: Spatial (positional references), Affordance (functional part identification), Counting (attribute-based grouping), Steerable (relative pointing), and Reasoning (open-ended visual inference). Each example shows representative query and the corresponding target. On the right, we show the Gradio-based annotation interface used to collect and refine segmentation masks. Initial masks are generated using SAM and refined by annotators, followed by manual verification. Spatial Scenes are selected for rich spatial relationships or repeated objects. Annotators craft purely positional queries; e.g., Point to the leftmost tree in the image. Affordance Scenes show tabletop objects or close-ups emphasizing functional parts. Annotators ask about actionable components; e.g., Point to the handle used for pouring. Counting Scenes feature multiple similar objects in varying quantities. Annotators pose queries that select subset by number, or attribute; e.g., Point to all the blue cars in the image. Steerable Images from the PixMo dataset [11] each include reference point. Annotators pose queries relative to that point, avoiding explicit object names; e.g., Point to the item closest to the marked point. Reasoning Generic, event-rich scenes invite open-ended queries that require visual reasoning, with answers conveyed by pointing; e.g., Point to the tallest man-made object in the image. To construct Point-Bench, we developed an intuitive Gradio-based annotation interface. Annotators were shown images sampled from each category and asked to write natural language queries aligned with the category theme. These queries were then evaluated using predictions from three anonymized MLLMs. If one or fewer models produced correct prediction as judged by human evaluators, the query was considered sufficiently challenging and accepted for inclusion in the dataset. Following this, the annotators used the same interface to annotate the target points directly on the image. SAM model was used to generate initial masks based on the selected point, and users could refine these masks by editing or removing portions before submission. Finally, separate group of annotators manually verified the masks to ensure they accurately reflected the user-generated queries. 3.3 Point-Battle As MLLMs increasingly incorporate visually grounded reasoning and pointing capabilities, static benchmarks become inadequate for evaluating performance in open-ended, real-world scenariosparticularly with respect to human preferences. To address this limitation, we introduce Point-Battle, dynamic platform for pairwise evaluation of MLLMs pointing abilities based on user-provided language instructions. Point-Battle adopts head-to-head evaluation format inspired by Chatbot Arena [8], implemented through Gradio-based web interface. In each round, two anonymized models are randomly sampled from the top performers in Point-Benchincluding GPT-4o, Gemini 2.5 Flash, Molmo-7B-D, Qwen2.5-VL-7B, and Grok-2 Vision. Users submit natural language instruction and select an image from curated dataset (post-April 20, 2025) or upload one of their own. The two models return point predictions, which are displayed side-by-side. Participants vote for the better output or select both good or both bad if applicable. No preset prompts are provided, encouraging diverse and unbiased instructions. Model identities were kept anonymous to prevent bias. Since its launch, Point-Battle has collected over 4,500 votes from approximately 100 participants worldwide. Unlike the static Point-Bench, which may be subject to overfitting if used during model development, Point-Battle serves as continuously updated benchmark that captures real-time human preferences and tracks progress in visually grounded reasoning across MLLMs. Furthermore, as Point-Battle scales up, this would also be platform for collecting pointing data. 3.4 Point-Act The first two stages of Point Arena evaluate MLLMs pointing capabilities through quantitative metrics and human preference assessments. However, pointing is only meaningful insofar as it enables real-world utility. To evaluate such support, we introduce Point-Actan interactive system where users issue natural-language instructions via GUI to double-blind MLLM. The model generates one or more predicted points, which are translated into actionable commands for an xArm 6 Lite robot. The robot executes pick-or-place action at the indicated location using depth sensing for spatial reasoning. This setup operationalizes pointing into end-to-end physical manipulation, bridging language grounding with robotic control. Point-Act highlights the downstream consequences of grounding precision: even small localization errors can cause execution failures, whereas accurate predictions enable consistent real-world success."
        },
        {
            "title": "4 Experiments",
            "content": "Figure 3: Success rates of MLLMs on Point-Bench across six task categories: Spatial, Affordance, Counting, Steerable, Reasoning, and Average. Each bar represents the mean success rate (%) for given model, with error bars indicating standard deviation across three evaluation runs. The Human bar serves as an upper-bound reference. The results demonstrate substantial performance disparities, with top models (e.g., GPT-4o, Gemini-2.5-Pro, Molmo-72B) achieving near-human accuracy in select categories, while others (e.g., LLaVA, Grok, and Claude) consistently underperform. We evaluate range of multimodal large language models (MLLMs)both proprietary and opensourceusing three components: Point-Bench (static benchmark evaluation), Point-Battle (human preference comparison), and Point-Act (real-world robotic execution). Section 4.1 describes the evaluation protocols, including model selection, prompting, and success metrics. Section 4.2 presents results on model performance and the impact of pointing supervision. Section 4.3 presents results demonstrating the correlation between benchmark accuracy, human preference judgments, and realworld task performance in pointing tasks. Section 4.4 includes ablations on prompt structure and output formats using GPT-4o to analyze factors affecting pointing accuracy. 4.1 Evaluation Setup All evaluations were performed under zero-shot prompting conditions. To ensure consistent outputs across models with differing internal coordinate systemsparticularly proprietary oneswe adopted standardized output format: [x, y], where and denote horizontal and vertical pixel coordinates, 6 respectively. This format was used across all models, except for those like Molmo, Qwen2.5-VL, and Gemini, which provide explicit coordinate outputs or prompting instructions. Success was measured using binary metric: prediction was considered correct if the point lay within the target mask. For non-counting tasks, models were prompted to predict single point; if multiple were returned, only the first point was evaluated, assuming it reflected the highest-confidence prediction due to the autoregressive generation process. Point-Bench. We benchmarked 16 MLLMs (spanning open-source and proprietary models, including key variants). Each model was evaluated on the same 982 image-instruction pairs, three times independently, to compute means and standard deviations. Open-source models were executed locally on NVIDIA A100 GPUs, while proprietary models were accessed via public APIs. Point-Battle. To measure alignment with human preferences, we released live evaluation platform and promoted it via social media and mailing lists. Users voted on head-to-head comparisons between anonymous model outputs. Elo ratings were computed from pairwise comparisons excluding ambiguous votes (both good or both bad). Point-Act. We recruited 10 remote participants to interact with our real-world robot setup. For fixed scene, participants evaluated three agentsMolmo-7B-D, GPT-4o, and human referenceacross three trials. After each condition, they completed System Usability Scale (SUS) survey. Models. We evaluate variants from Molmo [10], Gemini [29], OpenAI [1], Claude [3], Grok [33], LLaVA [20], and Qwen [4]. See appendix for details. Figure 4: Qualitative predictions across Point-Bench categories. Example model predictions are shown for each of the five Point-Bench categories: Spatial, Affordance, Counting, Steerable, and Reasoning. Each colored dot corresponds to prediction from different MLLM, labeled by model name in the legend. These examples highlight the diversity of pointing behaviors and the variation in performance across models. 4.2 Main Results Open-source models perform comparably to proprietary models in pointing accuracy. PointBench results show that open-source MLLMs explicitly trained on pointing data often match or outperform proprietary models. For example, Molmo-72B outperformed Gemini-2.5-Pro by 0.43 percentage pointsa statistically insignificant margin (p 0.29). In affordance reasoning, open7 Figure 5: Insights drawn from Point-Battle and Point-Bench. (a) This figure shows Point-Bench performance (%) of MLLMs over time, grouped by model family. sharp performance increase is observed in models released after the PixMo dataset (dashed line, December 2024). Notably, GPT4.1 improves by 21.1 percentage points over GPT-4-Turbo, and Gemini-2.0-Flash improves by 45.9 points over Gemini-1.5-Flash. These trends suggest that newer proprietary models may incorporate pointing supervision, potentially derived from or inspired by PixMo. (b) Linear regression of the five models common to Point-Battle and Point-Bench reveals strong correlation (R2 = 0.85), confirming close agreement between the two evaluation frameworks. (c) Performance of open-source models as function of parameter count. While there is slight upward trend, the performance gains with increasing model size are marginal, suggesting diminishing returns and limited sensitivity to scale within this range. source models like Molmo-72B and Qwen2.5-VL consistently exceed proprietary baselines. Overall, Molmo-72B achieves the highest performance on the Point-Bench benchmark as shown in Table 4. Pointing supervision significantly boosts performance. Access to explicit pointing data is key driver of model accuracy as shown in Figure 5a. Within the Qwen family, incorporating the PixMo corpus into Qwen2.5-VL-7B increased performance to 52.3%, substantial gain over the 17.4% achieved by Qwen2-VL-7B, which did not use such data. In contrast, LLaVA variantsalso trained without explicit pointing supervisionachieved only 4.817.4% on average. Proprietary models likely benefit from open-source pointing datasets. While proprietary training data is opaque, we observe large performance jumps in models released shortly after the PixMo [11] and RoboPoint dataset [37]. For instance, GPT-o3 improved by 21.1 percentage points over GPT-4-Turbo, and Gemini-2.5-Flash improved by 45.9 points over Gemini-1.5-Flash (Figure 5a). These results suggest that recent proprietary models may have incorporated PixMo or similar corpus. Open-source models align more closely with human preferences. In Point-Battle, Molmo-7B-D outperformed Gemini-2.5-Flash by 196 Elo points. Their 95% confidence intervals do not overlap, and Molmo-7B-D won 79% of the 115 direct head-to-head comparisons, as shown in Figure 6. Both Qwen2.5-VL-7B and Molmo-7B-D surpass proprietary models in human preference evaluations and exceed the 1000-point baseline, indicating statistically significant advantage over random guessing. However, in terms of preference-aligned pointing performance, Molmo-7B-D remains clearly superior to Qwen2.5-VL-7B. Molmo excels on Point-Act evaluation. User study results shown in Figure 7 that Molmo-7B-D outperforms the proprietary GPT-4o model by substantial margin, achieving 65% higher performance and approaching human (oracle) baseline levels. This superiority is also reflected in user preference, with Molmo-7B-D scoring 60.3 points higher in SUS than GPT-4o. Model size does not impact pointing performance. As shown in Figure 5c, the performance of open-source models (LLaVA-OV, Molmo, and Qwen-VL) on Point-Bench remains largely unchanged with increased model size. For example, Qwen2.5-VL-7B performs within 3% of Qwen2.5-VL-72B, 8 and Molmo-7B-O differs by less than 1% from Molmo-72B. These results suggest that scaling model size does not significantly improve pointing accuracy. 4.3 Results between three evaluation frameworks. The three-stage evaluation of MLLMs pointing capabilities should not be viewed as isolated components, but as complementary steps in progressive pipeline. As MLLMs improve, they are expected to advance through these stages. Therefore, understanding the correlation and agreement between stages is crucial for assessing consistent performance gains. Human-preference and static dataset evaluations are highly consistent. Point-Benchs static dataset will inevitably plateau as MLLMs improve by training on ever-larger, real or synthetic pointing corpora (e.g., RoboPoint [37]). To stay ahead, we introduce Point-Battle, live arena that updates continuously and enables open-ended model comparison in real time. Validating this setup, we re-evaluated the models tested on Point-Bench and observed strong alignment: Point-Battle scores correlate with Point-Bench results at R2 = 0.85 (Figure 5b). Point-Bench accuracy predicts real-world task success. We validated Point-Bench as reliable proxy by testing three agentsMolmo-7B-D, GPT-4o, and human referenceon Point-Act. Success rates closely aligned with Point-Bench scores, yielding strong linear correlation (R2 = 0.92). This high correlation indicates that Point-Bench is reliable proxy for the pointing capability of multimodal LLMs in practical settings. 4.4 What Other Factors Drive Pointing Performance? To understand the design choices that impact pointing, we conducted ablations on GPT-4o using variations in prompt structure and output representation, as shown in Table 1. Targeted prompts outperform verbose reasoning. Incorporating Chain-of-Thought (CoT) reasoning reduced pointing accuracy by 2.9% for GPT-4o and by substantial 16% for Gemini-2.5-Flash. Using raw, unfiltered user queries led to an additional drop of 2.6% and 3.7% for GPT-4o and Gemini-2.5-Flash, respectively. These results suggest that clear, targeted prompts with welldefined coordinate systems are crucial for effective pointing, while additional reasoning through language does not enhance MLLMs pointing capabilities. Table 1: Performance (%) of different GPT-4o and Gemini-2.5-Flash variants across evaluation categories. Method Steerability Counting Average Spatial Reasoning Affordance GPT-4o + In-Context (2-shots) GPT-4o + Chain-of-Thought (CoT) [32] GPT-4o + Unparsed language instruction GPT-4o (Default) Gemini-2.5-Flash + In-Context (2-shots) Gemini-2.5-Flash + Chain-of-Thought (CoT) [32] Gemini-2.5-Flash + Unparsed language instruction Gemini-2.5-Flash (Default) 46.0 41.4 37.8 42.4 50.0 44.4 55.0 58.6 26.7 24.6 23.7 25.6 40.5 25.6 44.6 47.7 23.9 21.2 19.7 23. 35.8 24.9 43.5 43.2 22.5 13.5 21.9 24.5 32.0 26.0 25.0 35.0 33.2 32.1 31.1 31.1 40.3 33.7 47.5 49.7 30.4 26.6 26.9 29. 39.7 30.9 43.1 46."
        },
        {
            "title": "5 Limitations, Discussion, and Conclusions",
            "content": "Discussions. PointArena integrates static benchmarks through Point-Bench and human preferencebased comparisons via Point-Battle to evaluate spatial reasoning in multimodal models. To improve annotation fidelity, future pipelines may replace the current grid-based refinement tools with free-form contouring interfaces, allowing annotators to directly trace object boundaries with mouse or stylus. This could yield smoother, more precise masksparticularly around object edges, where coarse grids often fail. To address benchmark staleness, we augment Point-Bench with user-generated content from Point-Battle, where participants upload images and provide implicit supervision through interaction. Although these annotations are noisier than manually curated ones, they enable scalable, up-to-date evaluations. Finally, we plan to implement an adaptive sampling strategy that dynamically selects model pairs with similar performance, increasing the informativeness of each comparison. 9 Figure 6: Performance on Human Preference Evaluation with Point-Battle. We collected over 4,500 votes from more than 100 global participants. Based on the Elo ratings derived from these votes, we observed clear preference for outputs from open-source models such as Molmo-7B-D and Qwen2.5-VL-7B, which consistently outperformed proprietary models in terms of human preference. Figure 7: Overview of the Point-Act system. (a) The Point-Act manipulation setup enables remote control of real-world xArm 6 Lite robot via language instructions, allowing users to evaluate pointing MLLMs. (b) User-blind evaluations and SUS preference scores collected for each model. Limitations. The current annotation pipeline relies on the Segment Anything Model (SAM) [19] to generate initial masks, which annotators refine through grid-based interface. While efficient, this approach often results in coarse and imprecise boundaries, particularly for fine-grained or irregular shapes. This degrades segmentation quality and introduces noise into downstream evaluation. Additionally, as large multimodal models are frequently trained on publicly available datasets, static benchmarks such as Point-Bench are at increasing risk of becoming part of training data, reducing their effectiveness in evaluating generalization. Lastly, Point-Battle currently selects model pairs uniformly at random, which leads to uninformative comparisonsespecially between models with large performance gapslimiting the efficiency of the evaluation process. Conclusion. PointArena offers unified, extensible framework for benchmarking languageconditioned pointing in multimodal models, combining static evaluations with live user-driven comparisons. While current limitations include coarse annotation tools, dataset contamination risks, and inefficient evaluation strategies, our proposed improvements aim to enhance scalability, annotation fidelity, and comparative signal. As the field evolves, PointArena is designed to adapt, supporting rigorous, real-world evaluation of grounded spatial reasoning."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "We gratefully acknowledge the contributions of our volunteer annotators, listed in no particular order: Rachel Edwards and Kathryn Slusarczyk. We also thank Jieyu Zhang for valuable discussions on the paper. Jiafei Duan is supported by the A*STAR National Science PhD Fellowship."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 10 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas J. Guibas. ReferIt3D: Neural listeners for fine-grained 3d object identification in real-world scenes. In 16th European Conference on Computer Vision (ECCV), 2020. [3] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com/ news/claude-3-family, 2024. Claude-3 Model Card. [4] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [5] Suma Bailis, Jane Friedhoff, and Feiyang Chen. Werewolf arena: case study in llm evaluation via social deduction, 2024. [6] Jeffrey Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al. Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333342, 2010. [7] Dave Zhenyu Chen, Angel X. Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language, 2020. [8] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms by human preference, 2024. [9] Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. Guesswhat?! visual object discovery through multi-modal dialogue, 2017. [10] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [11] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, Kuo-Hao Zeng, Jon Borchardt, Dirk Groeneveld, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah A. Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for state-of-the-art vision-language models, 2024. [12] Jiafei Duan, Yi Ru Wang, Mohit Shridhar, Dieter Fox, and Ranjay Krishna. Ar2-d2: Training robot without robot. arXiv preprint arXiv:2306.13818, 2023. [13] Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, and Ranjay Krishna. Manipulate-anything: Automating real-world robots using vision-language models. arXiv preprint arXiv:2406.18915, 2024. [14] Petko Georgiev et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 11 [15] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023. [16] Yushi Hu, Weijia Shi, Xingyu Fu, Dan Roth, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, and Ranjay Krishna. Visual sketchpad: Sketching as visual chain of thought for multimodal language models. arXiv preprint arXiv:2406.09403, 2024. [17] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. [18] Richard Kelley and Duncan Wilson. Tournament evaluation of large language models, 2025. [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollár, and Ross Girshick. Segment anything, 2023. [20] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [21] Zirui Liu, Jiatong Li, Yan Zhuang, Qi Liu, Shuanghong Shen, Jie Ouyang, Mingyue Cheng, and Shijin Wang. am-elo: stable framework for arena-based llm evaluation, 2025. [22] Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, and Min Lin. Improving your model ranking on chatbot arena by vote rigging, 2025. [23] Taiki Miyanishi, Fumiya Kitamori, Shuhei Kurita, Jungdae Lee, Motoaki Kawanabe, and Nakamasa Inoue. Cityrefer: Geography-aware 3d visual grounding dataset on city-scale point cloud data, 2023. [24] Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, Quan Vuong, Tingnan Zhang, Tsang-Wei Edward Lee, Kuang-Huei Lee, Peng Xu, Sean Kirmani, Yuke Zhu, Andy Zeng, Karol Hausman, Nicolas Heess, Chelsea Finn, Sergey Levine, and Brian Ichter. Pivot: Iterative visual prompting elicits actionable knowledge for vlms, 2024. [25] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence, 2018. [26] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016. [27] Arijit Ray, Jiafei Duan, Reuben Tan, Dina Bashkirova, Rose Hendrix, Kiana Ehsani, Aniruddha Kembhavi, Bryan Plummer, Ranjay Krishna, Kuo-Hao Zeng, et al. Sat: Spatial aptitude training for multimodal language models. arXiv preprint arXiv:2412.07755, 2024. [28] SYV-AI. Openarena: An open platform for llm-as-a-judge evaluation. https://github.com/ syv-ai/OpenArena, 2024. Accessed: 2025-05-09. [29] Gemini Robotics Team, Saminda Abeyruwan, Joshua Ainslie, Jean-Baptiste Alayrac, Montserrat Gonzalez Arenas, Travis Armstrong, Ashwin Balakrishna, Robert Baruch, Maria Bauza, Michiel Blokzijl, Steven Bohez, Konstantinos Bousmalis, Anthony Brohan, Thomas Buschmann, Arunkumar Byravan, Serkan Cabi, Ken Caluwaerts, Federico Casarini, Oscar Chang, Jose Enrique Chen, Xi Chen, Hao-Tien Lewis Chiang, Krzysztof Choromanski, David DAmbrosio, Sudeep Dasari, Todor Davchev, Coline Devin, Norman Di Palo, Tianli Ding, Adil Dostmohamed, Danny Driess, Yilun Du, Debidatta Dwibedi, Michael Elabd, Claudio Fantacci, Cody Fong, Erik Frey, Chuyuan Fu, Marissa Giustina, Keerthana Gopalakrishnan, Laura Graesser, Leonard Hasenclever, Nicolas Heess, Brandon Hernaez, Alexander Herzog, R. Alex Hofer, Jan Humplik, Atil Iscen, Mithun George Jacob, Deepali Jain, Ryan Julian, Dmitry Kalashnikov, M. Emre Karagozler, Stefani Karp, Chase Kew, Jerad Kirkland, Sean Kirmani, Yuheng Kuang, Thomas Lampe, Antoine Laurens, Isabel Leal, Alex X. Lee, Tsang-Wei Edward Lee, Jacky Liang, Yixin Lin, Sharath Maddineni, Anirudha Majumdar, Assaf Hurwitz Michaely, Robert Moreno, Michael Neunert, Francesco Nori, Carolina Parada, Emilio Parisotto, Peter Pastor, Acorn Pooley, Kanishka Rao, Krista Reymann, Dorsa Sadigh, Stefano Saliceti, Pannag Sanketi, Pierre Sermanet, Dhruv Shah, Mohit Sharma, Kathryn Shea, Charles Shu, Vikas Sindhwani, Sumeet Singh, Radu Soricut, Jost Tobias Springenberg, Rachel Sterneck, Razvan Surdulescu, Jie Tan, Jonathan Tompson, Vincent Vanhoucke, Jake Varley, Grace Vesom, Giulia Vezzani, Oriol Vinyals, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Fei Xia, Ted Xiao, Annie Xie, Jinyu Xie, Peng Xu, Sichun Xu, Ying Xu, Zhuo Xu, Yuxiang Yang, Rui Yao, Sergey Yaroshenko, Wenhao Yu, Wentao Yuan, Jingwei Zhang, Tingnan Zhang, Allan Zhou, and Yuxiang Zhou. Gemini robotics: Bringing ai into the physical world, 2025. [30] Michael Tomasello, Malinda Carpenter, and Ulf Liszkowski. new look at infant pointing. Child development, 78(3):705722, 2007. [31] Jiaqi Wang, Hanqi Jiang, Yiheng Liu, Chong Ma, Xu Zhang, Yi Pan, Mengyuan Liu, Peiran Gu, Sichen Xia, Wenjun Li, Yutong Zhang, Zihao Wu, Zhengliang Liu, Tianyang Zhong, Bao Ge, Tuo Zhang, Ning Qiang, Xintao Hu, Xi Jiang, Xin Zhang, Wei Zhang, Dinggang Shen, Tianming Liu, and Shu Zhang. comprehensive review of multimodal large language models: Performance and challenges across different tasks, 2024. [32] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [33] xAI. Grok-2 model card. https://x.ai/news/grok-2, 2024. Large language model. [34] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12):nwae403, 11 2024. [35] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions. arXiv preprint arXiv:1608.00272, 2016. [36] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg, and Tamara L. Berg. Modeling context in referring expressions, 2016. [37] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. [38] Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, and Dieter Fox. Robopoint: vision-language model for spatial affordance prediction in robotics. In Proc. Conference on Robot Learning (CoRL), volume 270, pages 40054020, 2025. [39] Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Weiwen Xu, Deli Zhao, and Lidong Bing. Auto-arena: Automating llm evaluations with agent peer battles and committee discussions, 2024. [40] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023."
        }
    ],
    "affiliations": [
        "Allen Institute for Artificial Intelligence",
        "Anderson Collegiate Vocational Institute",
        "University of Washington"
    ]
}