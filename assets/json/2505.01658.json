{
    "paper_title": "A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency",
    "authors": [
        "Sihyeong Park",
        "Sungryeol Jeon",
        "Chaelyn Lee",
        "Seokhun Jeon",
        "Byung-Soo Kim",
        "Jemin Lee"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workloads such as chain-of-thought, complex reasoning, and agent services significantly increase the inference cost by invoking the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as a key component for integrating the optimization methods into service-oriented infrastructures. However, a systematic study on inference engines is still lacking. This paper provides a comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughput- and latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions. We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide a public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 8 5 6 1 0 . 5 0 5 2 : r Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency SIHYEONG PARK, Korea Electronics Technology Institute, South Korea SUNGRYEOL JEON, Korea Electronics Technology Institute, South Korea CHAELYN LEE, Korea Electronics Technology Institute, South Korea SEOKHUN JEON, Korea Electronics Technology Institute, South Korea BYUNG-SOO KIM, Korea Electronics Technology Institute, South Korea JEMIN LEE, Electronics and Telecommunications Research Institute, South Korea Large language models (LLMs) are widely applied in chatbots, code generators, and search engines. Workload such as chain-of-through, complex reasoning, agent services significantly increase the inference cost by invoke the model repeatedly. Optimization methods such as parallelism, compression, and caching have been adopted to reduce costs, but the diverse service requirements make it hard to select the right method. Recently, specialized LLM inference engines have emerged as key component for integrating the optimization methods into service-oriented infrastructures. However, systematic study on inference engines is still lacking.This paper provides comprehensive evaluation of 25 open-source and commercial inference engines. We examine each inference engine in terms of ease-of-use, ease-of-deployment, general-purpose support, scalability, and suitability for throughputand latency-aware computation. Furthermore, we explore the design goals of each inference engine by investigating the optimization techniques it supports. In addition, we assess the ecosystem maturity of open source inference engines and handle the performance and cost policy of commercial solutions.We outline future research directions that include support for complex LLM-based services, support of various hardware, and enhanced security, offering practical guidance to researchers and developers in selecting and designing optimized LLM inference engines. We also provide public repository to continually track developments in this fast-evolving field: https://github.com/sihyeong/Awesome-LLM-Inference-Engine. CCS Concepts: General and reference Surveys and overviews; Software and its engineering Development frameworks and environments; Computing methodologies Artificial intelligence. Additional Key Words and Phrases: Large Language Model, Transformer, Inference Engine, Framework, Optimization ACM Reference Format: Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, and Jemin Lee. 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency. ACM Comput. Surv. 37, 4, Article 111 (August 2018), 65 pages. https://doi.org/XXXXXXX.XXXXXXX Corresponding Author Authors addresses: Sihyeong Park, sihyeong@keti.re.kr, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, South Korea; Sungryeol Jeon, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, South Korea, wjstjdfuf98@ keti.re.kr; Chaelyn Lee, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, South Korea, mylynchae@keti.re. kr; Seokhun Jeon, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, South Korea, seokhun.jeon@keti.re.kr; Byung-Soo Kim, Korea Electronics Technology Institute, Seongnam-si, Gyeonggi-do, South Korea, bskim4k@keti.re.kr; Jemin Lee, Electronics and Telecommunications Research Institute, Daejeon, South Korea, leejaymin@etri.re.kr. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0360-0300/2018/8-ART111 $15.00 https://doi.org/XXXXXXX.XXXXXXX 111 ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:2 Park et al."
        },
        {
            "title": "1 INTRODUCTION\nLarge Language Models (LLMs) are being utilized in a wide range of services, such as chatbots,\ncode generation, and search engines, with remarkable examples including OpenAI’s ChatGPT [5],\nGitHub Copilot [84], and Google Gemini [86]. Building on these successes, numerous new models\nand services have rapidly emerged; however, this expansion introduces new challenges in deploying\nand serving LLMs on a scale.",
            "content": "Recent trends like reasoning-centric test-time scaling [124, 226] and LLM-based AI agents [92, 134] have significantly increased both computational demands and the number of inference calls in LLM-based applications. Reasoning-centric test-time scaling replaces single-pass answer generation with multi-step reasoning or iterative self-verification to improve output quality. Also known as chain-of-thought (CoT) [259], self-consistency [45], and test-time reasoning [98], these methods increase accuracy by invoking the model multiple times per query, thereby raising latency and computing costs. Meanwhile, LLM-based AI agents such as AutoGPT [26] and LangChain [126] autonomously plan sequence of tasks to fulfill single user request, repeatedly calling the model within single session. Consequently, inference efficiency has become essential for deploying both reasoning-oriented LLMs and AI agents in practice. To manage the growing inference costs of LLMs, various optimization techniquessuch as quantization [61], lightweight architectures [268], and knowledge distillation (KD) [271]have been adopted. In large-scale services, however, the diversity of prompt lengths, query types, and output formats often means that single optimization method cannot cover every scenario. As result, LLM inference engines, which offer multiple optimization strategies and handle the inference process, have become crucial infrastructure components that directly affect both service quality and cost. Although general-purpose deep learning frameworks like PyTorch [201] and TensorFlow [1] originally designed to support wide range of models, from convolutional neural networks (CNNs) to recurrent neural networks (RNNs)are widely used for LLM inference, they prioritize broad hardware and architecture compatibility. Consequently, they do not include various specialized optimizations for LLMs or for sequential decoding. Running large-scale models on these frameworks can lead to slower performance and higher resource usage, underscoring the need for dedicated inference solutions. Reflecting this need, growing number of specialized LLM inference engines have emerged. They provide capabilities such as batching, streaming, and attention optimizations that are not typically found in general-purpose frameworks. However, Each engine targets different hardware such as graphics processing units (GPUs) and LLM accelerators, optimization scopes ranging from model compression to memory offloading, and intended use cases varying from real-time conversational systems to large-scale text generation. As result, the ecosystem has become both rapidly evolving and fragmented, making it difficult to determine which optimization methods are supported by each engine and how effectively they perform under various conditions. Consequently, there is pressing need for comprehensive review and comparison of LLM inference engines and the optimization techniques they offer. Most existing surveys on LLM optimization  (Table 1)  have focused on specific methods, such as model compression or hardware acceleration, and therefore have not fully explored which optimization techniques are supported by individual inference engines. In addition, many of these surveys omit recently released commercial engines. For instance, Chitty-Venkata et al. [48] and Yuan et al. [281] focus on transformer-based model compression, while Park et al. [200] and Zhu et al. [301] examine compression methods in detail. Similarly, works such as Xu et al. [268], Xu et al. [267] and Wang et al. [254] discuss optimization strategies for LLM inference or serving ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:3 Table 1. Comparison of Representative Surveys on Efficient LLM Inference Survey (Author, Year) Publication Venue Scope Taxonomy Approach Chitty-Venkata et al., 2023 [48] JSA Efficient inference - architecture design, knowledge distillation, pruning, quantization Optimization-based, Performance-oriented Miao et al., 2023 [173] ArXiv Bai et al., 2024 [27] ArXiv Xu et al., 2024 [268] ArXiv Park et al., 2024 [200] ArXiv Yuan et al., 2024 [281] ArXiv Zhu et al., 2024 [301] TACL Wang et al., 2024 [254] ArXiv Zhou et al., 2024 [298] ArXiv Wan et al., 2024 [249] TMLR Li et al., 2024 [132] ArXiv Xu et al., 2025 [267] CSUR Efficient model serving - decoding algorithms, architecture design, model compression, quantization, parallel computation, memory management, request scheduling, kernel optimizations Resource-efficient model - architecture design, pretraining, fine-tuning, inference optimization, system design Resource-efficient foundation models - foundation model, architecture design, resource-efficient algorithms, resource-efficient systems Model compression - pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, architecture design Efficient inference - model compression, fast decoding algorithm, compiler/system optimization, hardware optimization Model compression - quantization, pruning, knowledge distillation, low-rank factorization Optimization-based Optimization-based, Performance-oriented Architecture-, Optimization-based Optimization-based, Performance-oriented Optimization-based, Performance-oriented Optimization-based Model compression and efficient inference - quantization, pruning, knowledge distillation, architecture design, framework Efficient inference - data-level optimization, architecture design, model compression, inference engine, serving system Efficient models - model optimization schemes, data selection/engineering, framework Hardware perspective inference optimization - hardware architecture (CPU, GPU, FPGA, ASIC, PIM/NDP), quantization, sparsity, speculative decoding, homogeneous/heterogeneous cooperation Resource-efficient algorithms - attention optimization, architecture design, pre-training, fine-tuning, inference algorithm, model compression, distributed training, serving Optimization-based Optimization-based Optimization-based Hardware-, Optimization-based Optimization-based Zheng et al., 2025 [296] CSUR Efficient models - model compression, runtime optimization, on-device applications Optimization-based Discussion of Inference Engine Open Issues and Future Directions (Parallel computation) (Cloud/Edge Training, Inference) (Multi-GPU, Heterogeneous Inference) (Inference, Serving Optimization) (Training, Inference) (Cloud/Edge Training, Inference) (Edge/Mobile support) systems in cloud or edge environments, but they lack detailed examination of the design and implementation of each engine. Consequently, there remains gap in the literature for survey that not only presents the current landscape of LLM inference engines, but also systematically links their specialized features to the optimization techniques they implement. To fill this gap, this paper adopts framework-centric perspective, thoroughly examining range of LLM inference engines and categorizing the optimization techniques each one implements. In particular, it maps how these engines handle methods like quantization, KD, caching, and parallelization, enabling readers to quickly identify engines that align with specific requirements. This paper also includes recently released commercial engines that are not covered in older surveys, comparing their architectural goals, hardware targets, and significant features. The goal is to offer practical insights for researchers and engineers who need to build or operate high-performance, cost-efficient LLM services. As shown in Fig. 1, this paper systematically organizes the major LLM inference engines and their respective optimization methods. Section 2 outlines the core aspects of decoder-based transformer architectures, attention mechanisms, and the standard LLM inference process. Section 3 presents comprehensive review of the leading LLM inference engines, including ecosystem, hardware and operating system (OS) support. In particular, commercial offerings are discussed to help readers find suitable solutions for their own service environments and deployment objectives. To this end, we analyzed various aspects of inference engines, including their ecosystem, usability, as well as their support for hardware and platforms across both edge and server environments. Section 4 offers ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:4 Park et al. Introduction (1) Backgrounds (2) Practical Guides to Inference Engines (3) Detailed Review of Inference Engines (4) Structure and Optimizations of LLMs (2.1) LLM Inference Process and Optimization (2.2) Inference-Aware LLM Serving Workflow (2.3) Ecosystem Maturity and Sustainability Signals (3.1) Hardware Compatibility and Platform Support (3.2) Design and Pricing Strategies of Commercial Inference Engines (3.3) Single-Node & Heterogeneous Devices (4) Ollama [194] (4.1), llama.cpp [82] (4.2), MAX [179] (4.6), MLC LLM [177] (4.7), PowerInfer [227, 270] (4.15), TGI [110] (4.25) Single-Node & Homogeneous Devices (4) Unsloth [244] (4.5), llama2.c [21] (4.8), bitnet.cpp [251] (4.9), OpenLLM [30] (4.12), LightLLM [142] (4.17), NanoFlow [300] (4.18), vAttention [206] (4.20), Sarathi-Serve [8] (4.21), Friendli Inference [71] (4.22) vLLM [125] (4.3), DeepSpeed-FastGen [102] (4.4), SGLang [295] (4.10), LitGPT [145] (??), LMDeploy [162] (4.16), Fireworks AI [67] (4.23), Together Inference [239] (4.25) TensorRT-LLM [191] (4.13), DistServe [297] (4.19), GroqCloud [89] (4.24) Multi-Node & Heterogeneous Devices (4) Multi-Node & Homogeneour Devices (4) Batch Optimization (5.1) Parallelism (5.2) Dynamic Batching [13, 49] (5.1.1) Continuous Batching [99, 280] (5.1.2) Nano-batching [300] (5.1.3) Chunked-prefills [9] (5.1.4) Data Parallelism [212] (5.2.1) Fully Shared Data Parallelism [292] (5.2.2) Tensor Parallelism [205, 230] (5.2.3) Pipeline Parallelism [9, 106, 164, 279] (5.2.4) Quantization (5.3.1) e ff a t m p s t s : d g n g r e n n f o r Compression (5.3) Pruning (5.3.2) LLM Inference Optimization (5) Future Directions and Open Challenges (6) Conclusion (7) Fine-Tuning (5.4) Sparsity Optimization (5.3.3) Full-Parameter Fine-Tuning [163] (5.4) Parameter-Efficient FineTuning (PEFT) (5.4) Prompt Caching [299] (5.5.1) Caching (5.5) Prefix Caching [152, 199] (5.5.2) PTQ [133], QAT [42, 159], AQLM [62], SmoothQuant [265], KV Cache Quantization [104, 161], EXL2 [242], EETQ [182], LLM Compressor [247], GPTQ [69], Marlin [70], Microscaling Format [213] cuSPARSE [190], Wanda [233], Mini-GPTs [245], Token pruning [72], Post-Training Pruning [291] Structured Sparsity [56, 294], Dynamic Sparsity [288], Kernel-level Sparsity [34, 172, 263, 264], Block Sparsity [74], N:M Sparsity [287], MoE [38], Sparse MoE [60, 66], Dynamic Token Sparsity [72, 272], Contextual Sparsity [12, 160] LoRA [105, 219], QLoRA [53, 286] KV Caching [204] (5.5.3) KV Cache Optimization (5.6.1) PagedAttention [125], TokenAttention [142], ChunkedAttention [274] I/O Optimization (5.6.2) FlashAttention [50, 51, 215] Attention Optimization (5.6) KV Cache Reuse (5.6.3) RadixAttention [295] Attention Programming Model (5.6.4) MQA Optimization (5.6.5) FlexAttention [57] FireAttention [67] Sampling Optimization (5.7) Speculative Decoding (5.7) EAGLE [136138], Medusa [37], ReDrafter [47] Structured Outputs (5.8) Constrained Decoding (5.8) FSM [260], CFG [28, 77], Outlines [59], XGrammar [58], LM Format Enforcer [184], llguidance [93], GBNF [81], OpenAI Structured Outputs [197], JSONSchemaBench [76], StructTest [41], SoEval [156] Fig. 1. Taxonomy of LLM Inference Engines and Optimizations detailed discussion of the architectures of various LLM inference engines and the inference-specific optimization features offered by each engine. Section 5 classifies fundamental inference optimization techniques found in current inference enginescovering batch optimization ( 5.1), parallelization ( 5.2), model compression ( 5.3), fine-tuning ( 5.4), caching ( 5.5), attention optimization ( 5.6), sampling optimization ( 5.7) and structured outputs ( 5.8)while also examining emerging trends. By synthesizing these techniques, the chapter helps readers choose the inference engine that best matches their service requirements. Based on these discussions, Section 6 explores future directions and major challenges in the development of LLM inference engines. Specifically, we examine the ongoing evolution of LLMs and how inference engines accommodate these changes, with particular ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111: Fig. 2. Overview of Decoder-only Transformer Architecture Fig. 3. Attention Mechanism attention to security and compatibility across diverse hardware platforms. We present perspectives on multiple aspects, including inference engine optimization strategies, security for inference and support for diverse hardware platforms and architectures. Finally, section 7 concludes the paper."
        },
        {
            "title": "2 BACKGROUNDS\nTo enhance the efficiency of LLM inference, it is crucial not only to select a model suited to the\ndomain but also to choose and optimize an appropriate inference engine while taking a diverse\napproach to overall development. This section examines LLMs from the perspective of inference,\ndemonstrating how tasks such as model compression and deployment strategies can be seamlessly\nintegrated with inference engines to achieve fast and cost-effective services.",
            "content": "First, we review the decoder-only transformer architecture, along with various attention mechanisms and considerations related to efficient inference. Second, we explain the inference process, focusing on the prefill and decode phases, and highlight corresponding optimization techniques from the inference engine perspective. Finally, we combine these elements to provide comprehensive overview of the entire pipeline for inference and service deployment."
        },
        {
            "title": "2.1 Structure and Optimizations of LLMs\nLLM Architecture Types. LLMs can be broadly categorized into three types based on the Trans-\nformer architecture [246]: decoder-only, encoder-decoder, and encoder-only models. The encoder-\ndecoder model first encodes the entire input and then uses the decoder with cross-attention at each\nstep, which leads to higher memory usage and more complex procedures during inference. Encoder-\nonly models are suitable for tasks like classification or retrieval, but since they are optimized for\none-time inference, they are not ideal for token-by-token generation.",
            "content": "In contrast, decoder-only models have simpler structure and are widely adopted in recent LLMs due to their strong zero-shot performance through autoregressive training [253, 258]. Therefore, this paper mainly focuses on the decoder-only architecture. Standard Decoder-Only Architecture. Fig. 2 shows the architecture of decoder-only transformer. When text input is received, it is first tokenized and then converted to high-dimensional vectors by an embedding layer. At this stage, positional encoding is added to incorporate token order. The resulting embeddings pass through several transformer blocks, each comprising Multi-Head Attention (MHA), Feed-Forward Network (FFN), and residual connections. The MHA layer splits the input into Query (Q), Key (K), and Value (V) vectors and performs scaled dot-product attention in parallel across multiple heads. In each head, similarity scores are computed and applied to V, aggregating the results. Causal masking ensures that only previously generated tokens are attended to, enabling autoregressive context learning. Next, the FFN layer refines the attention ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:6 Park et al. output by applying linear transformation, expanding it to higher-dimensional space, and using an activation function (e.g., ReLU [6], GELU [100], or SiLU [63]) before reducing it back to the original dimension. This sequence of operations increases the models representational capacity. Both the MHA and FFN layers employ residual connections and layer normalization. Residual connections mitigate the vanishing gradients in deep networks [282], and layer normalization keeps the output distributions stable, facilitating smoother training. After the transformer block operations are finished, each input token produces hidden state which is normalized and used to predict the next token in text generation. The hidden state is then passed through linear layer, resulting in logit vector over the vocabulary. Applying the softmax function converts these logits into probability distribution, and the token with the highest probability is chosen as the next token. This procedure is repeated iteratively to generate the final text. Attention Structure Variants. In standard transformer, MHA is employed, but recent modificationslike the one shown in Fig. 3have been introduced to improve inference efficiency. In MHA, each of the 𝑁ℎ heads uses its own Q, K, matrices, enabling the model to learn distinct subspace representations. However, increasing the number of heads also expands the size of the KV cache during inference, because all and values must be stored. To address this, Multi-Query Attention (MQA) [218] was proposed. MQA retains multiple query heads while sharing single set of and across all heads, thereby reducing the KV cache to roughly 1/𝑁ℎ of what MHA requires. Although this approach may slightly reduce expressiveness, it significantly decreases memory usage. Grouped-Query Attention (GQA) [11] takes middle ground by sharing and among head groups rather than across all heads. By tuning the number of groups (𝑁𝑔), developers can strike balance between memory efficiency and model performance. More recently, DeepSeek-v2 [147] introduced Multi-Head Latent Attention (MLA), which compresses and from multiple heads into shared latent vector. This design further minimizes cache size while preserving accuracy. Because these alternative attention mechanisms alter the size and structure of the KV cache, inference engines must adapt accordingly. For instance, MQA and GQA require cache management that reflects shared and V, whereas MLA involves reconstructing compressed and V. As result, the compatibility of an inference engine can vary depending on the attention structure of the model. Variants in Positional Embedding, Tokenization and Normalization. In LLMs, key architectural variants include the type of positional embedding, tokenizer choice, and the placement of normalization layers. Even well-known LLMs adopt different configurations: BLOOM [261] uses Attention with Linear Biases (ALiBi) [207], while Llama [88, 240, 241] and Mistral [114] employ Rotary Position Embedding (RoPE) [232]. The selection of tokenizer also varies across models-GPT variants typically use Byte-Pair Encoding (BPE) tokenizers [302], whereas Llama [88, 240, 241] and T5 [209] rely on SentencePiece-based unigram tokenizers [123]. Other architectural differences include the placement of normalization layers [210]."
        },
        {
            "title": "2.2 LLM Inference Process and Optimization\nLLM inference proceeds by tokenizing the user’s input text and generating subsequent tokens until\na stopping criterion (e.g., token limit or end-of-sequence (EOS) command) is met. As shown in\nFig. 4, this process comprises two main phases: the prefill phase, which generates the first token\nbased on the input, and the decode phase, which sequentially produces the remaining tokens.",
            "content": "Prefill phase. This phase processes the input text to compute the hidden state of the last token for each sample, thereby capturing the contextual and semantic meaning of the input. In decoder-only transformer models, this phase involves tokenization, embedding, and transformer block computations. Attention and FFN operations are performed on all input tokens. In this step, ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:7 Fig. 4. LLM Inference Process Fig. 5. Inference and Serving Process of LLM Table 2. Key metrics of LLM performance Metric Definition User Perspective Optimization Technique Time-to-first-token (TTFT) Time-between-tokens (TBT) End-to-end latency Time taken for the model to generate the first token Time interval between each token Total time from client request to complete response Most directly impacts the users perception of response speed Reflects the speed at which subsequent tokens are generated Reflects overall response time and user experience Throughput Number of tokens processed per unit time Represents the systems processing capacity Batching (5), Kernel fusion, Prompt caching (5.5.1), Speculative decoding (5.7) KV caching (5.5.3), Kernel fusion, Attention optimization (5.6), Quantization (5.3.1) Batching (5), KV caching (5.5.3), Pruning (5.3.2), Speculative decoding (5.7), FlashAttention (5.6.2) Batching (5), Prefill optimization (5.1.4), Parallelism (TP/PP) (5.2), Quantization (5.3.1) attention scales approximately with the square of the sequence length (O (𝑛2)), and the complexity of FFN increases with the size of the intermediate layer, resulting in large-scale array-to-array computations. Q, K, and Vused to capture relationships among all input tokensare generated immediately, loading substantial data into memory for intensive computation. For example, as shown in Fig. 4, if the user input is Is Seoul in Korea?, it is tokenized into [Is, Seoul, in, Korea, ?], mapped to unique token value such as [101, 4523, 1102, 2342, 63]. Position embeddings are then applied to these token IDs, converting them into high-dimensional vectors (e.g., [[0.1, 0.2, ...], [0.3, 0.5, ...], ...]). These vectors undergo attention and FFN computations, allowing the model to learn contextual relationships among tokens and refine their representations. Finally, the hidden state of the last token (?) is stored for use in the decode phase, where it guides the generation of subsequent tokens. Decode phase. This phase iteratively generates new tokens based on the hidden state computed during the prefill phase, following an autoregressive process in which only one token is produced at time. In this phase, the final hidden state of the transformer block is passed through linear transformation and softmax function, which yields probability distribution over the vocabulary. The token with the highest probability is selected and appended to the input sequence. Throughout this process, and V, as well as the input and output tokens, are stored on the GPU, system memory, or cache. Because and must be accessed and updated repeatedly, the decode phase often becomes limitation of the memory bandwidth. Although the attention computation resembles that of the prefill phase, frequent reference to previously generated tokens increases latency, and the data to be accessed grows linearly with the sequence length. Specifically, during the decode phase, the hidden state of the last token (?) saved in the prefill phase is used to predict the next token. For example, when the attention mechanism processes the newly generated token, the transformer block produces final hidden state that is then linearly projected into logit vector over the vocabulary. After softmax is applied, the most probable wordsay, Yesis chosen and appended to the existing sequence. Repeating this procedure can generate full response, such as Yes it is, as shown in Fig. 4. System terms. The performance terms of an LLM system are illustrated in Fig. 4 and the accompanying Table 2. Time-To-First-Token (TTFT) measures the time it takes to receive user ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:8 Park et al. request to generate the first token. It is especially important for how fast the system feels to the user. Time-Between-Tokens (TBT) refers to the time it takes to generate each following token. It is often described as Time Per Output Token (TPOT), which is the average token generation speed during decoding. In addition, end-to-end latency represents the total response time for user query and can be calculated as: Latency = 𝑇𝑇 𝐹𝑇 + (𝑇 𝐵𝑇 𝑛𝑢𝑚𝑏𝑒𝑟 𝑜 𝑓 𝑡𝑜𝑘𝑒𝑛𝑠)) While latency gives an overall measure of responsiveness, throughput shows how many user requests the system can handle at the same time. From phase-wise perspective, the prefill phase affects TTFT and the decode phase impacts TBT. The latency of the prefill phase increases with input length, but can be reduced using parallel computation. On the other hand, latency in the decode phase grows with the number of generated tokens and has more direct impact on the user experience. Optimization. Taking these performance metrics into account, LLM inference engines employ various customized optimization techniques for the prefill and decode phase. Most engines use KV caching to avoid redundant computation during decoding by reusing cached context and computing new operations only for the latest token. Recently, techniques such as continuous batching [280] and hybrid batching [117] have been introduced to further improve decode phase efficiency, group prefill, and decode operations from multiple requests to better use GPU resources. In addition, many inference engines reduce per-token overhead during decoding through kernel fusion and hardware-specific computation kernels. Kernel fusion consolidates operationssuch as LayerNorm, matrix multiplication, and activation functionsinto single GPU kernel, which decreases memory access and kernel launch overhead. Quantization [61] is another key optimization. By representing model parameters in 8-bit or 4-bit integers instead of 16-bit or 32-bit floating-point formats, memory usage and bandwidth demands drop, especially during decoding. Quantized models can cache more tokens and handle more concurrent requests on the same hardware, often boosting the computation speed. In general, caching, batching, kernel optimization, and quantization are fundamental to optimizing token throughput and minimizing latency in LLM inference services. Providing robust support for these techniques within an inference engine is crucial for delivering high-quality, scalable LLM solutions."
        },
        {
            "title": "2 Prompt engineering. This step involves optimizing how the model is prompted and deployed.\nPrompt design can significantly influence model performance, as developers carefully craft system\nmessages and user prompts to ensure consistent, high-quality outputs. This practice is known as\nprompt engineering [275], which directs the model to produce desired responses without requiring\nadditional computation. For example, a well-structured system prompt can adjust the model tone",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:9 or decrease inappropriate responses, reducing trial-and-error during inference and contributing to more stable operation. During development, prompt templates undergo iterative testing and revision so that, in production, the model achieves the intended output with minimal further tuning. 3 Evaluation and fine-tuning. When prompt design is completed, the model must be evaluated to verify if it achieves the required level of performance. If not, fine-tuning can be applied to enhance accuracy or domain-specific capabilities. For example, instruction tuning [284] can train the model with instruction response datasets to increase accuracy or domain-related responses. Other techniques include prompt tuning [130], which adds task-optimized vectors to input embeddings, and prefix tuning [248], which modifies the model by inserting trainable parameters into hidden states at all layers. If the model size exceeds the available hardware, quantization can be used to compress activations or weights. Post-training quantization [69, 133, 265] is based on calibration dataset to calculate scaling parameters, converting weights or activations to lower precision. Alternatively, quantization-aware training [42, 159] simulates quantized conditions during training, ensuring that the model retains accuracy despite low-precision weights."
        },
        {
            "title": "3 PRACTICAL GUIDES TO INFERENCE ENGINES\nThis section offers practical guidance on choosing an LLM inference engine by examining several\nkey aspects. First, we look at ecosystem maturity and sustainability signals, such as how the engine\nis developed, licensed, and supported by its community. Next, we discuss hardware compatibility\nand platform support, focusing on whether the engine targets edge devices or server environments.\nWe then explore the design and pricing strategies of commercial inference engines, including cost\nconsiderations and memory usage. Finally, we present a hardware-aware categorization of LLM\ninference engines, comparing engines based on their target use (edge or server), device types, and\nperformance goals.",
            "content": "Table 3 provides summary of the LLM inference engines examined in this paper, and Fig. 6 offers visual representation of the characteristics of each engine. General-Purpose is composite metric derived from the number of supported models in Table 3 and the range of hardware platforms in Table 4. higher score indicates broader compatibility with diverse models and hardware. Ease-of-Deploy measures how easily an engine can be installed via the Python package installer (pip), Debian Advanced Package Tool (APT), Homebrew [170], customized through source builds, Docker [55] or Conda [19] environments or prebuilt binaries. higher rating suggests simpler, faster installation and deployment. Ease-of-Use evaluates both documentation quality and user community activity level (as shown in Table 3). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111: Park et al. GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy l l S e e - - l l U a - - l l S e e - - l l U a - - l l S e e - - ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware (a) Ollama (b) llama.cpp (c) vLLM (d) DeepSpeed-FastGen (e) Unsloth GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy l l U a - - i a S E - - l l U a - - i a S E - - l l U a - - ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware (f) MAX (g) MLC LLM (h) llama2.c (i) bitnet.cpp (j) SGLang GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy l l U a - - l l U E - - l l U a - - l l U E - - l l U a - - ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware (k) LitGPT (l) OpenLLM (m) TensorRT-LLM (n) TGI (o) PowerInfer GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy l l S e e - - l l U a - - l l S e e - - l l U a - - l l S e e - - ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware (p) LMDeploy (q) LightLLM (r) NanoFlow (s) DistServe (t) vAttention GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy GeneralPurpose Ease-ofDeploy l l U a - - i a S E - - l l U a - - i a S E - - l l U a - - ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware ThroughputAware LatencyAware (u) Sarathi-Serve (v) Friendli Inference (w) Fireworks AI (x) GroqCloud (y) Together Inference Fig. 6. Representative characteristics comparison of LLM inference engines across six dimensions: model generality, ease of deployment and use, latency and throughput optimization, and scalability ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:11 Table 3. Comparison of LLM Inference Engines Frameworks Organization Release Date Open-Source Support GitHub # Stars (Rate) Star Commit Supported Models Community (Ollama) Community (gml.ai) Academic (vLLM Team) Ollama [194] llama.cpp [82] vLLM [125] DeepSpeed-FastGen [102] Big Tech (Microsoft) Startup (unsloth AI) Unsloth [244] Startup (Modular Inc.) MAX [179] Community (MLC-AI) MLC LLM [177] Community (Andrej Karpathy) llama2.c [21] Big Tech (Microsoft) bitnet.cpp [251] Academic (SGLang Team) SGLang [295] Startup (Lightning AI) LitGPT [145] Startup (BentoML) OpenLLM [30] Big Tech (NVIDIA) TensorRT-LLM [191] Startup (Hugging Face) TGI [110] Academic (SJTU-IPADS) PowerInfer [227] Startup (MMRazor/MMDeploy) LMDeploy [162] Academic (Lightllm Team) LightLLM [142] Academic (UW Efeslab) NanoFlow [300] Academic (PKU) DistServe [297] Big Tech (Microsoft) vAttention [206] Big Tech (Microsoft) Sarathi-Serve [9] Friendli Inference [71] Fireworks AI [67] GroqCloud [89] Together Inference [239] Startup (FriendliAI Inc.) Startup (Fireworks AI, Inc.) Startup (Groq Inc.) Startup (together.ai) Jun. 2023 Mar. 2023 Feb. 2023 Nov. 2023 Nov. 2023 Apr. 2023 Apr. 2023 Jul. 2023 Oct. 2024 Jan. 2024 Jun. 2024 Apr. 2023 Aug. 2023 Oct. 2022 Dec. 2023 Jun. 2023 Jul. 2023 Aug. 2024 Jan. 2024 May. 2024 Nov. 2023 Nov. 2023 Jul. 2023 Feb. 2024 Nov. 2023 136K (209.6) 77.6K (102.6) 43.4K (55.2) 37.7K (72.6) 36.5K (74.1) 23.8K (33.7) 20.3K (28.8) 18.3K (29.1) 13.6K (53.0) 12.8K (28.4) 12.0K (16.6) 11.1K (15.5) 10.1K (16.1) 10.0K (11.0) 8.2K (17.2) 6.0K (9.1) 3.1K (5.0) 0.7K (3.5) 0.5K (1.2) 0.3K (1.0) 0.3K (0.6) Docs User Forum M indicates partial open-source support, Each square represents 50 models (Mar. 2025) Indicates the level of detail of the document (: Simple, : Moderate, : Detail), refers for social networking services (Discord/Slack), refers for discussion forums (private forums/reddit), and refers for meetups Table 4. Hardware Features of LLM Inference Engines Engines Ollama [194] llama.cpp [82] vLLM [125] DeepSpeed-FastGen [102] Unsloth [244] MAX [179] MLC LLM [177] llama2.c [21] bitnet.cpp [251] SGLang [295] LitGPT [145] OpenLLM [30] TensorRT-LLM [191] TGI [110] PowerInfer [227, 270] LMDeploy [162] LightLLM [142] NanoFlow [300] DistServe [297] vAttention [206] Sarathi-Serve [9] Friendli Inference [71] Fireworks AI [67] GroqCloud [89] Together Inference [239] Supported Platform Linux Windows macOS Web/ API x86-64 CPU ARM/ Apple Silicon (Vulkan, Metal) GPU AI Accelerators NVIDIA (CUDA) AMD (ROCm, HIP) Intel (SYCL) Google TPU AMD Instinct Intel Gaudi Huawei Ascend AWS Inferentia Mobile/ Edge ETC. (NVIDIA Jetson) (Qualcomm Adreno) (NVIDIA Jetson) (Qualcomm Adreno, ARM Mali) (NVIDIA Jetson) (NVIDIA Jetson) (Qualcomm Snapdragon 8) (NVIDIA Jetson) Moore Thread MTT Tecorigin SDAA Groq LPU Latency-Aware and Throughput-Aware represent the each engines support for latencyand throughput-specific optimization techniques, respectively, based on the metrics in Table 2 (2) and the optimization features in Table 7 (5). Higher values imply more robust capabilities to optimize in those areas. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:12 Park et al. Lastly, Scalability indicates how effectively an engine accommodates edge, server, and multi-node environments. Higher scores indicate suitability for large-scale LLM workloads. For commercial inference engines, some metric scores may be lower because they rely on publicly available information. By referring to Fig. 6, users can determine which LLM inference engine best matches their service needs and deployment settings."
        },
        {
            "title": "3.1 Ecosystem Maturity and Sustainability Signals\nThis section discusses non-technical indicators related to the current status of LLM inference\nengines. As shown in Table 3, LLM inference engines can be categorized into open-source and\nclosed-source commercial tools. For open-source tools, we analyze sustainability based on the types\nof development and maintenance organization, open software licenses, and the maturity of user\nsupport.",
            "content": "Development and Maintenance Organizations. Open-source inference engines are mainly developed and maintained by big tech companies, startups, communities or academic institutions. Among the 21 inference engines surveyed, the number of inference engines by organization type is: Academic (6), Startup (6), Big Tech (5), and Community (4). While the difference is not large, this shows that LLM inference engines are being developed and maintained by variety of organizations. Regardless of the organization, most open-source projects use permissive licenses such as MIT or Apache 2.0, making them easy to adopt and use. Projects maintained by community groups may face challenges in long-term maintenance, which could limit the integration of new technologies. Some projects like Unsloth [244], MAX [179], OpenLLM [30], and TensorRT-LLM [191], which are led by big tech or startups, only release parts of their source code. While open-source engines are developed by diverse groups such as big tech, startups, communities, and academia, most commercial LLM inference engines are developed and run by startups. This is because startups can move quickly and develop specialized technologies to enter the market faster with differentiated, high-performance services. User Preference. We measured user preference for open-source LLM inference engines using GitHub statistics such as total stars, daily average growth rate, and star growth trends over time. In this study, we considered project to be highly popular if it gained more than 25 stars per day on average. Projects like Ollama [194] (209.6), llama.cpp [82] (102.6), DeepSpeed-FastGen [102] (72.6) and Unsloth [244] (74.1), bitnet.cpp [251] (53.0), vLLM [125] (55.2) meet this criterion and have tens of thousands of total stars, indicating high interest and rapid adoption by the community. On the other hand, some projects have large number of total stars but show slower recent growth. For example, TGI [110], TensorRT-LLM [191], and OpenLLM [30] each have more than 10K stars, but their daily growth is below 25, and their growth curves are flat after an initial spike. This may suggest that they received attention early, but are now facing difficulties in maintaining community interest. Possible reasons include limited usability or closed ecosystems. This kind of analysis helps estimate the future growth potential of projects, providing long-term perspective when choosing engines for practical or research use. Ease of Use. We evaluate the user-friendliness of LLM inference engines based on the quality of documentation and the availability of user forums. Our analysis shows that top projects like vLLM [125], and DeepSpeed-FastGen [102], TensorRT-LLM [191] provide well-written documentation, and vLLM [125] and MAX [179], LitGPT [145] have active community channels (e.g., Discord, forums), making onboarding and troubleshooting easier. This is closely related to their high star counts and rapid user adoption. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:13 In contrast, projects such as bitnet.cpp [251], OpenLLM [30], PowerInfer [270], NanoFlow [300], DistServe [297], etc. have limited documentation or lack community channels. This is also reflected in their slow star growth, indicating higher entry barrier for users. Projects with poor documentation and no forums tend to have lower popularity and slower growth. These results suggest that beyond technical performance, user support systems are important factors in engine selection and community growth. Development Activity. We evaluated the development activity of LLM inference engines based on GitHub commit trends and the number of supported models. By considering both indicators together, we achieved more reliable results than simply counting commits alone. Projects such as llama.cpp [82], vLLM [125], and DeepSpeed-FastGen [102] show consistent and frequent updates in their commit histories, while also supporting wide range of LLM models. On the other hand, engines like TGI [110] and TensorRT-LLM [191], which gained many stars early on, show relatively stagnant commit activity and limited model support. This may indicate lower flexibility for future feature extensions. In particular, projects such as OpenLLM [30] and PowerInfer [270], which have narrow range of supported models or only short-term commit activity, show signs of limited technical adaptability, which can be constraint for real-world applications. Overall, the number of GitHub stars and commit activity show similar patterns, suggesting that user interest and active development often go hand-in-hand. Inference engines that are frequently updated and support diverse models are more likely to be well-maintained over the long term."
        },
        {
            "title": "3.2 Hardware Compatibility and Platform Support\nHardware and OS Support. As shown in Table 4, each inference engine is designed with different\ngoals and target systems. Some inference engines support various hardware types, while others\nare optimized for a single platform. These hardware compatibility differences affect performance-\nrelated features such as quantization data formats, kernel fusion, and support for multi-node or\nmulti-GPU configurations. Therefore, for optimal service performance, inference engines should\nbe selected based on their compatibility with the intended hardware setup. In addition, Table 4\nsummarizes the OS and hardware support status for each inference engine.",
            "content": "Most inference engines operate in Linux environments, with some additionally supporting Windows or macOS. Commercial engines often provide web-based inference services, but they also enable on-premise deployments. These platform differences can affect both development complexity and inference performance, depending on the range of software capabilities. CPU-Based Inference. While many engines include CPU-based inference, non-edge-focused solutions typically employ the CPU for specific taskssuch as offloading operations or handling model weightsrather than as the primary compute resource. Edge and Server Environments. On edge devices (e.g., mobiles and Internet of Things (IoTs) systems), limited compute and memory resources require inference engines to focus on lightweight design. These engines reduce model size and apply techniques like quantization to minimize memory usage and enable execution on low-power hardware. Mobile and edge-oriented engines may need to run entirely on CPUs or leverage AI accelerators embedded in system-on-chip (SoC) platforms, such as Neural Processing Units (NPUs) or Digital Signal Processors (DSPs). For example, Apple Core ML [24] and Google AI Edge SDK [87] allow deployment of transformer operations to dedicated hardware on consumer devices. Edge inference engines include Ollama [194], llama.cpp [82], and MLC LLM [177], and in particular, MLC LLM provides compiler technology for various edge hardware. Conversely, server-side inference engines are optimized for multi-GPU environments to handle high volumes of requests. They rely on distributed computing techniques such as model and pipeline parallelism to spread large models across devices, and they use large batch sizes and dynamic ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111: Park et al. Heterogeneous Devices Homogeneous Devices Device Support llama.cpp [82], MAX [179], MLC LLM [177], Ollama [194], PowerInfer [270], TGI [110] bitnet.cpp [251], LightLLM [142], llama2.c [21], NanoFlow [300], OpenLLM [142], Sarathi-Serve [8], Unsloth [244], vAttention [206], Friendli Inference [71] DeepSpeed-FastGen [102], LitGPT [145], LMDeploy [162], SGLang [295], vLLM [125], Fireworks AI [67], Together Inference [239] DistServe [297], TensorRT-LLM [191], GroqCloud [89] N - n a e - u Fig. 7. taxonomy of LLM inference engines categorized by scalability and hardware support scheduling to maximize hardware utilization.As AI accelerators such as Intel Max [112], Google TPU [116], AMD Instinct [225], and Intel Gaudi [118] are adopted as replacements for NVIDIA GPUs in inference servers, more and more engines are offering heterogeneous hardware backends. Server inference engines include TensorRT-LLM [191], vLLM [125], DeepSpeed-FastGen [102], etc., and provide optimization techniques for throughput or latency. Scalability and Device Types. Fig. 7 groups the inference engines from Table 3 according to their hardware characteristics. The -axis distinguishes between support for single device type versus multiple types, while the -axis shows whether each engine supports single-node or multi-node configurations. single node generally includes one to eight GPUs, whereas multi-node systems connect multiple such nodes. Single-node inference engines emphasize intra-node optimization for CPUs, consumer-level GPUs, or edge/IoT devices. Ollama [194] and llama.cpp [82] focus on consumer-level hardware (e.g., laptops and PCs), and MLC LLM [177] targets efficient inference on various edge platforms. By contrast, multi-node inference engines handle both inter-node and intra-node computations, optimizing scalability and performance for multi-user workloads. Representative inference engines belonging to this category include vLLM [125], TensorRT-LLM [191], and SGLang [295]. Inference engines supporting heterogeneous devices can operate with multiple hardware types beyond GPUs, allowing developers to choose hardware based on application requirements. In contrast, engines that support only homogeneous devicessuch as those specialized for NVIDIA GPUs or Groq LPU [4]can deliver high performance through custom kernels and low-level optimizations, though their narrower hardware support may limit portability."
        },
        {
            "title": "3.3 Design and Pricing Strategies of Commercial Inference Engines\nCloud Services and Model Coverage. Commercial inference engines offer cloud-based ser-\nvices that simplify the setup of LLM applications and underlying hardware, compared to many\nopen-source solutions. In particular, Friendli Inference [71], Fireworks AI [67], and Together In-\nference [239] support a broader model range than most open-source inference engines, covering\nnot only LLMs but also image, audio, and multimodal models, and they facilitate rapid adoption of\nnewly released models.",
            "content": "A key advantage of commercial inference engines is that they can provide various model and hardware support customized to the scale of the service, reducing the cost and complexity of server deployment and maintenance. Unlike some open-source engineswhose maintenance may be inconsistent or whose licensing might shift to paid models if resources become constrained commercial services generally guarantee updates and enhancements over specified duration, ensuring reliable long-term operation. Hardware Variety and Specialization. Among these services, Friendli Inference [71] and Together Inference [239] focus on optimizing inference for NVIDIA GPUs, whereas GroqCloud [89] ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:15 Friendli Inference Fireworks AI GroqCloud Together Inference / k u O 500 400 300 200 100 9 . 6 8 4 6 . 2 5 4 5 . 6 9 1 6 . 8 5 2 2 1 2 2 8 . 4 . 8 3 5 1 0 . 1 0 7 3 . 3 2 3 9 1 1 . 4 1 3 7 . 1 6 5 8 6 . 4 6 1 . 5 6 ) ( e L 1 . 5 7 2 2 . 7 0 1 9 . 3 9 9 . 4 6 3 . 5 7 2 7 . 4 5 1 . 6 7 1 5 . 3 2 3 1 4 . 2 . 5 6 3 0 0 0 0 0 0 4 8 . 2 0 7 . 0 3 7 . 0 3 5 . 0 1 4 . 0 3 4 . 2 3 . 0 7 7 . 0 5 6 . 0 5 5 . 0 2 7 . 0 5 5 . 9 4 . 0 3 3 . 0 6 5 . 0 6 . 9 0 4 3 . 4 0 . 0 4 3 . 0 5 3 . 9 3 . 0 4 3 . 0 1 3 . 0 8 2 . 0 2 . 0 0 0 0 0 0 0 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 eepSeek-R eepSeek-V3 Lla 70B 3.3 Lla 3.1 405B Lla 3.1 70B Lla 3.1 2.5 8B 32B 32B en en oder Previe eepSeek-R1 eepSeek-V3 Lla 70B 3.3 Lla 3. 405B Lla 3.1 70B Lla 3.1 2.5 8B 32B 32B en en oder Previe (a) Inference Throughput (Output Tokens per Second) (b) Initial Response Latency (Time to First Token in Seconds) Fig. 8. Comparison of Inference Performance across commercial LLM inference engines Friendli AI Fireworks AI GroqCloud Together AI Input Output Hardwares Friendli AI Fireworks AI GroqCloud Together AI Models Input Output Input Output Input Output 0.99 0.75 0.79 0.59 0.08 0.05 0.79 0.79 0.39 0.29 7.00 0.60 0.60 0.10 3.00 0.90 3.00 8.00 0.90 3.00 DeepSeek-R1 3.00 DeepSeek-V3 Llama 3.3 70B 0.60 Llama 3.1 405B Llama 3.1 70B 0.60 Llama 3.1 8B 0.10 Qwen 2.5 Coder 32B Qwen QwQ Preview 32B Llama is Instruct model, Turbo mode price DeepSeek-R1 Distill Llama 70B Table 5. Pricing by Model in Commercial LLM Engines ($/1M tokens) 3.00 1.25 0.88 3.50 0.88 0.18 0.80 1.20 7.00 1.25 0.88 3.50 0.88 0.18 0.80 1.20 2.9 5.6 NVIDIA A100 80GB NVIDIA H100 80GB NVIDIA H200 141GB AMD MI300X Groq LPU Table 6. Pricing by Hardware Type in Commercial LLM Engines ($/hour for 1 device) 2.56 3.36 4.99 2.9 5.8 9.99 4.99 leverages the proprietary Groq LPU AI accelerator [4]. Fireworks AI supports broader range of hardware, including AMD Instinct MI300X [225], and meets privacy and reliability standards through relevant certifications. Performance and Cost Trade-offs. When selecting commercial engine, it is also necessary to consider hardware support and cost. Commercial inference engines typically aim for low latency and high throughput by implementing batch optimization, request pipelining, and other techniques that offer faster and more streamlined deployment compared to open-source alternatives. Fig. 8 and Table 5 show the inference performance and costs for various models (e.g., reasoning (DeepSeekR1 [96]), Mixture-of-Experts (MoE) (DeepSeek-V1 [31]), large-scale (Llama 3 [88]), code generation (Qwen 2.5 Coder [111]), multimodal (Qwen QWQ [236])) using different commercial engines [25]. Additionally, Table 6 summarizes the hardware costs provided by each commercial engine. Even when using the same hardware, the cost may vary depending on the degree of kernel and compute optimization in each engine."
        },
        {
            "title": "4.1 Ollama\nOllama [194] is a Go programming language [85]-based inference engine designed to run LLMs in\nlocal environments, enabling users without technical background to easily test and deploy models.\nConsequently, it primarily targets single-GPU setups rather than multi-GPU systems, relying on\nllama.cpp as its core backend.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:16 Park et al. Ollama is composed of two primary components: client and server. The client sends requests to the server via command-line interface (CLI), while the server includes an HTTP server and llama.cpp [82] backend. The HTTP server manages client-server communication, and the llama.cpp backend loads the model and processes inference requests. The inference engine supports variety of modelssuch as Llama [88], Falcon [14], Mistral [114], and DeepSeek-R1 [96]and is important to quickly adapt to newly released models. It uses both GGUF [80] and Safetensors [108] formats for model inference and provides model customization via Modelfile. In addition, Ollama offers REST API that allows users to manage and execute models through HTTP requests, making it suitable for chat, text generation, and other applications. Integration options include Open WebUI [195], SwiftChat [17], Google Cloud, and oterm [278], extending its deployment capabilities in mobile, cloud, and local environments. However, Ollama prioritizes user accessibility over advanced inference optimizations, meaning it lacks features such as memory optimization, multi-GPU functionality, and multi-node support. In return, it delivers broad compatibility by supporting not only NVIDIA GPUs but also AMD GPUs and ARM platforms. Representative Characteristics Summary General-Purpose [Medium]: Supports popular community models and both NVIDIA and AMD GPUs, but lacks multi-GPU or edge specialization. Ease-of-Deploy [High]: One-line installation via Homebrew, pip, or Docker makes setup extremely simple. Ease-of-Use [Medium]: concise CLI and REST API, plus GUI integrations such as Open WebUI, lower the entry barrier for non-experts. Latency-Aware [Medium]: The engine provides no Flashor KV-cache optimizations, so single-token latency remains higher. Throughput-Aware [Medium]: Single-GPU operation without batching strategies limits sustained throughput. Scalability [Medium]: Designed for local single-GPU use and cannot extend to multi-node deployments. 4.2 llama.cpp llama.cpp [82] is C++ library for LLM inference that runs models on CPUs without GPU. Consequently, it depends on minimal external software and operates efficiently on diverse hardware architectures. It supports quantization for multiple data types (e.g., 1.5-bit, 4-bit, 8-bit), reducing memory usage, and boosting efficiency. llama.cpp also introduces the Georgi Gerganov Unified Format (GGUF) [80] for streamlined LLM storage and deployment. GGUF consolidates model parameters, structure, and metadata into single file, improving the Georgi Gerganov Machine Learning (GGML) [79] format by providing better flexibility and compatibility. This approach standardizes model storage and simplifies deployment. llama.cpp supports range of hardware platformsincluding x86, ARM, and NVIDIA GPUsand uses the GGML context to configure these backends. It provides hardware-specific kernel and graph optimizations that facilitate efficient inference. Additionally, llama.cpp extends usability with subprojects such as llama-cli for command-line execution, llama-server for OpenAI API compatible HTTP-serving, and lightweight runners like llama-run and llama-simple. Representative Characteristics Summary General-Purpose [Medium]: Runs on x86, ARM CPUs and NVIDIA GPUs with several quantization formats for broad hardware reach. Ease-of-Deploy [High]: single static binary or minimal CMake build keeps external dependencies near zero. Ease-of-Use [Low]: CLI helpers and an OpenAI-style server exist, but documentation is concise and communitydriven. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:17 Latency-Aware [Medium]: Optional FlashAttention kernels and GPU offload reduce token delay on capable devices. Throughput-Aware [Medium]: Multithreading and continuous batching boost CPU throughput, although distributed support is minimal. Scalability [Low]: Optimized for single-node execution and lacks native cluster features. 4.3 vLLM vLLM [125] is high-performance LLMs serving library, focusing on fast token generation and low latency. Its PagedAttention mechanism enhances memory efficiency by storing KV cache in non-contiguous memory blocks, preventing the fragmentation issues associated with contiguous storage. vLLM is built around AsyncLLM for asynchronous request handling, an OpenAI-compatible API server, and an EngineCore that conducts inference. ZeroMQ [237]-based multiprocessing API server overlaps operations between AsyncLLM and the API layer. EngineCore features modules for scheduling and model execution, enabling concurrent handling of CPU-heavy tasks (e.g., tokenization, multimodal input management, and token detokenization) alongside the main execution loop for improved throughput. Its symmetric architecture reduces inter-process overhead and supports optimized tensor parallelism. Additionally, vLLM supports FlashAttention-3 [215] to further reduce inference latency. It employs distributed system architecture for multi-GPU workload distribution, leveraging MegatronLMs tensor parallelism [224]. Beyond CPU and GPU support, vLLM is compatible with AWS Inferentia [15] and Google TPU [116], extending its capabilities to multimodal inference. Representative Characteristics Summary General-Purpose [High]: Serves wide range of LLMs across GPUs, TPUs, and AWS Inferentia accelerators. Ease-of-Deploy [High]: Docker images and pip package simplify setup, but distributed configuration still requires manual steps. Ease-of-Use [High]: OpenAI-compatible endpoints and an active community streamline application integration. Latency-Aware [Medium]: FlashAttention-3 and PagedAttention aggressively cut attention-time latency. Throughput-Aware [High]: AsyncLLM scheduling and ZeroMQ multiprocessing maintain high token-per-second rates. Scalability [High]: Built-in tensor parallelism enables multi-GPU and multi-node clustering."
        },
        {
            "title": "4.4 DeepSpeed-FastGen\nDeepSpeed-FastGen [102] is an LLM inference engine integrating Microsoft DeepSpeed Inference\n[18] and DeepSpeed Model Implementations for Inference (MII) [176]. It optimizes memory usage\nto enable efficient model inference.",
            "content": "DeepSpeed-FastGen deploys DeepSpeed MII for its frontend and backend, handling requests through features like dedicated query/response APIs, continuous batching, and model pipeline. Internally, it leverages DeepSpeed Inference to support hardware-optimized kernels (e.g., NVIDIA CUDA), as well as Blocked KV-Cache and tensor parallelism. major feature is the Dynamic SplitFuse technique, which splits long prompts into smaller segments and processes them in multiple forward passes, improving throughput and reducing latency. By maintaining consistent forward pass size, system processing efficiency increases. DeepSpeed-FastGen also offers replica-level load balancing, distributing inference workloads across multiple nodes. Compared to single-node inference, multi-node deployments can deliver significant speedups in query processing. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:18 Park et al. Representative Characteristics Summary General-Purpose [Medium]: DeepSpeed-MII front end supports numerous HuggingFace checkpoints and custom models. Ease-of-Deploy [High]: containerized launcher is available, though model conversion and registry are still required. Ease-of-Use [High]: MII-style APIs are clear, but some DeepSpeed configuration know-how is assumed. Latency-Aware [Medium]: Dynamic SplitFuse splits long prompts to cap worst-case latency. Throughput-Aware [High]: Continuous batching, blocked KV-cache, and tensor parallelism keep GPUs saturated. Scalability [High]: Replica-level load balancing supports efficient multi-node service."
        },
        {
            "title": "4.5 Unsloth\nUnsloth [244] is a engine focused on efficient fine-tuning and inference for LLMs. It achieves\nrapid fine-tuning and reduced memory usage through techniques such as Low-rank adaptation\n(LoRA) [105] and Quantized-LoRA (QLoRA) [53] while preserving model accuracy. All kernels are\nimplemented in OpenAI Triton [238], further enhancing the execution speed of LLM. Although\nUnsloth integrates modules such as xFormers [172] to accelerate transformer operations. This\napproach allows for flexible customization of attention blocks and other modules, providing greater\nadaptability for diverse use cases.",
            "content": "For compatibility, Unsloth supports both GGUF [80] and vLLM [125] formats and offers straightforward API for creating inference services. However, it currently runs only on NVIDIA GPUs, and advanced optimization features such as multi-GPU and multi-node support are exclusive to the paid version. The open-source release is restricted to single-GPU setups and supports limited number of models. Representative Characteristics Summary General-Purpose [Low]: Provides GGUF and vLLM model formats but currently restricts execution to NVIDIA GPUs. Ease-of-Deploy [Medium]: single pip install delivers both fine-tuning and inference capabilities. Ease-of-Use [Medium]: High-level Python APIs are simple, though advanced documentation is still limited. Latency-Aware [Medium]: Triton-fused kernels shorten attention steps, trimming token latency moderately. Throughput-Aware [Low]: xFormers integration helps single-GPU throughput; distributed execution is paywalled. Scalability [Low]: The open-source edition runs on single GPU and omits multi-node features."
        },
        {
            "title": "4.6 MAX\nModular Accelerated Xecution (MAX) [179] is an integrated platform aimed at simplifying the\ncreation and deployment of high-performance AI endpoints, while maintaining flexibility across\ndiverse hardware setups. It offers a graph compiler and runtime capable of accelerating generative\nAI models through hardware-agnostic libraries. By compiling models into optimized computation\ngraphs, MAX enhances execution efficiency and reduces latency for better performance.",
            "content": "MAX is built on the Mojo programming language [178]. Mojo extends Python with system programming features from C, C++, and CUDA via Multi-Level Intermediate Representation (MLIR) [128], enabling high performance on CPUs, GPUs, and specialized AI accelerators. MAX comprises two main components: the MAX Engine (an inference library and runtime) and MAX Serve, serving utility for model deployment. MAX Serve hosts LLMs and provides OpenAI API compatible REST endpoints in both local and cloud environments. It applies continuous heterogeneous batching and multi-step scheduling to maximize GPU utilization and ensure stable performance, particularly for large-scale workloads. Internally, MAX Serve integrates the MAX Engine, which utilizes its graph compiler and runtime to accelerate models on CPUs and GPUs. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:19 Currently, MAX supports inference workloads across both local and cloud environments and operates on CPUs and NVIDIA GPUs. Representative Characteristics Summary General-Purpose [Medium]: Mojos MLIR compiler targets CPUs, GPUs, and future accelerators from one model graph. Ease-of-Deploy [High]: Docker images and CLI exist, but users still package models into MAX Serve. Ease-of-Use [High]: REST endpoints are easy to consume, yet Mojo tooling is early-stage for newcomers. Latency-Aware [Medium]: Ahead-of-time graph compilation fuses kernels and shortens critical paths. Throughput-Aware [Medium]: Continuous heterogeneous batching and multi-step scheduling keep devices busy. Scalability [High]: Operates on local and cloud machines with experimental multi-GPU support."
        },
        {
            "title": "4.7 MLC LLM\nMLC LLM [177] is a compiler and high-performance deployment engine for LLMs, designed to\nenable model development, optimization, and deployment across multiple platforms. It supports\ninference not only on NVIDIA and AMD GPUs but also on mobile and edge devices such as iOS and\nAndroid, unifying server and edge environments into a single LLM engine. The provided engine,\nMLCEngine, delivers high throughput and low latency in server environments and also supports\nlightweight local deployment.",
            "content": "Achieving platform-wide LLM acceleration requires extensive GPU programming and runtime compatibility. To address this, MLC LLM builds on Apache TVM [43], generating GPU libraries automatically for each hardware and platform. It integrates LLM-specific optimizations such as continuous batching [280] and speculative decoding [131, 262], and employs FlashInfer [276] to accelerate NVIDIA GPUs. MLC LLM either converts and quantizes foundation model weights or loads pre-converted weights, using the model-weights-mlc module for operator fusion, memory allocation, and hardware-specific optimizations; the model-lib component then constructs platformnative runtimes for each device. MLC LLM offers range of deployment modesPython APIs, OpenAI-compatible APIs, REST servers, and WebLLM [214]ensuring broad portability across cloud and local platforms. Representative Characteristics Summary General-Purpose [Medium]: single engine serves desktop, mobile, and WebLLM runtimes across NVIDIA and AMD GPUs. Ease-of-Deploy [High]: The installer script compiles TVM kernels for each target automatically. Ease-of-Use [Medium]: Python and REST APIs plus web demo provide moderate integration effort. Latency-Aware [Medium]: FlashInfer kernels and continuous batching enable low-latency generation. Throughput-Aware [High]: Speculative decoding and operator fusion lift tokens-per-second on GPUs. Scalability [Medium]: Generates native runtimes for edge devices through to cloud servers. 4.8 llama2.c llama2.c [21] is an inference engine designed to run small Llama2 [241]-based models in single file. It comprises approximately 700 lines of code and can load models trained with PyTorch [201] for inference. The inference engine focuses on small-scale domains and is intended for educational use and features simple structure. Rather than implementing advanced optimization techniques, it only includes the essential code needed for LLM inference. Parallel processing is limited to OpenMP-based multithreading and runs exclusively on CPUs, without support for GPU execution or distributed environments. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:20 Park et al. Representative Characteristics Summary General-Purpose [Low]: Runs only small Llama-2 checkpoints on CPUs for education and demonstration. Ease-of-Deploy [Low]: Compiles in seconds with no external libraries for rapid experimentation. Ease-of-Use [Low]: Approximately 700 lines of readable code make learning and modification easy. Latency-Aware [Low]: Only basic OpenMP threading is present, leaving high per-token latency. Throughput-Aware [Low]: No batching, GPU support, or cache management reduces sustained throughput. Scalability [Low]: Designed for single CPU host with no distributed or GPU pathway. 4.9 bitnet.cpp bitnet.cpp [251] is CPU-only inference engine developed in the context of one-bit LLM research. Built based on llama.cpp [82], it focuses on fast, lossless inference of ternary models (BitNet b1.58 [165]) while minimizing power consumption. The project offers three kernel typesI2_S, TL1, and TL2optimized for both x86 and ARM processors. The I2_S kernel converts full-precision weights to two-bit format offline, then restores the original values during inference to accelerate general matrix-vector multiply (GEMV) operations. This approach reduces memory and bandwidth and also improves performance in multithreading systems. The TL1 kernel compresses every two weights into four-bit index and employs lookup table (LUT) with nine precomputed activation values based on the T-MAC [257] method, allowing large models to run efficiently even with limited thread environments. TL2 compresses every three weights into five-bit index, shrinking the model size to one-sixth of the TL1 footprint and making it suitable for environments with tight memory or bandwidth constraints. bitnet.cpp supports only local CPU execution and relies on multithreading rather than distributed parallelism for acceleration. In addition to BitNet b1.58 [165], it can run the Llama 3 8B [88] and Falcon 3 [64] family models, but it does not yet support broader hardware platforms or large-scale distributed deployments. Representative Characteristics Summary General-Purpose [Low]: Runs only on local CPUs and supports narrow model set (BitNet b1.58 plus few Llama 3 and Falcon variants), so overall hardware and model diversity is limited. Ease-of-Deploy [Medium]: Ships as self-contained C++ binary that builds with minimal dependencies and requires no GPU drivers, enabling rapid installation on almost any x86 or ARM host. Ease-of-Use [Low]: While the CLI closely mirrors llama.cpp, documentation and community examples are still sparse, which raises the learning curve for first-time users. Latency-Aware [Low]: The engine focuses on memory-bandwidth reduction rather than dedicated latency techniques; single-token delay remains governed by CPU core speed. Throughput-Aware [Low]: Multithreaded I2_S, TL1, and TL2 kernels use 2to 5-bit weight compression to boost GEMV throughput compared with full-precision CPU baselines. Scalability [Low]: All acceleration is confined to one multicore server; there is no support for multi-socket or distributed execution across nodes."
        },
        {
            "title": "4.10 SGLang\nStructured Generation Language for LLMs (SGLang) [295] is a system designed to execute LLMs\nefficiently by overcoming limitations found in existing inference engines, including multimodal\ninput handling, parallel processing, and KV cache reuse. To achieve this, SGLang uses multi-call\nstructures and introduces Language Model Programs (LM Programs), which support various model\ntypes (vision, embedding, reward models) as well as multi-node operation.",
            "content": "The inference engine comprises frontend and backend (runtime) and provides an OpenAIcompatible API. SGLangs frontend, written in Python, enables flexible authoring of LM Programs using conventional control flow and libraries, enhancing developer ease. Meanwhile, the backend ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:21 applies execution optimizations that include RadixAttention-based KV cache management and structured decoding with compressed finite state machines, enabling rapid inference. These methods allow SGLang to outperform existing inference engines in throughput and excel in tasks such as agent control and logical reasoning. SGLang provides both an interpreter and compiler. The interpreter manages prompt states as streams and asynchronously handles fundamental operations to improve synchronization and parallelism. It also tracks program execution paths, enabling further compiler optimizations. After compiling these programs into computation graphs, the SGLang graph executor rewrites the graph or establishes static execution plans. For further optimization, SGLang employs Zero-Overhead Batch Scheduler, similar to NanoFlows Nano-batching strategy [300], to increase parallelism in model inference. It also features cacheaware load balancer that improves prefix cache hit rates, thus boosting overall throughput. Representative Characteristics Summary General-Purpose [Medium]: Language-Model Programs manage multimodal models and support multi-node execution. Ease-of-Deploy [High]: Requires source compilation and CUDA toolchain configuration before use. Ease-of-Use [Medium]: Python DSL is flexible but introduces learning curve with stream-based semantics. Latency-Aware [Medium]: RadixAttention and compressed finite-state decoding reduce tail latency. Throughput-Aware [Medium]: The Zero-Overhead Batch Scheduler maximizes overlap, achieving extreme throughput. Scalability [High]: Cache-aware load balancing enables cluster execution, though tooling is still maturing."
        },
        {
            "title": "4.11 LitGPT\nLitGPT [145] is an end-to-end framework that covers fine-tuning, inference, testing and deployment.\nBuilt on nanoGPT [20], Lit-LLaMA [144], and Lightning Fabric [143], it supports pretrained models\nfor rapid prototyping.",
            "content": "LitGPT scales from single GPU to multi-GPU and multi-node environments, offering distributed parallelism through Fully Sharded Data Parallelism (FSDP) [292] and faster computation with FlashAttention-2 [50]. This framework also includes memory and speed optimizations via quantization [61] and LoRA [105], and it can run LLMs on Google TPUs through the PyTorch/XLA compiler [208]. Representative Characteristics Summary General-Purpose [Low]: Supports NVIDIA GPUs, AMD Instinct, and Google TPU, but is primarily optimized for NVIDIA GPUs. Ease-of-Deploy [Medium]: Offers easy installation via pip and provides prebuilt packages. Ease-of-Use [Medium]: Provides brief manuals and maintains community through forums and meet-ups. Latency-Aware [Medium]: Reduces response time with FlashAttention-2, speculative decoding, and KV caching. Throughput-Aware [Medium]: Increases overall throughput with FSDP and batching optimizations. Scalability [High]: Extends from single-GPU setup to multi-GPU and multi-node deployments."
        },
        {
            "title": "4.12 OpenLLM\nOpenLLM [30] is a platform for the straightforward execution and deployment of open-source\nLLMs and custom models through simple commands. Designed as a cloud-based solution that\novercomes the scalability and high-load issues of existing platforms like Ollama [194], OpenLLM\ntargets multi-user support, high throughput, and low latency. This makes it well suited for deploying\nLLMs on cloud or on-premise servers and for building LLM-based applications. A key advantage is\ndata security, achieved via a Bring Your Own Cloud (BYOC) model.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:22 Park et al. OpenLLM provides an OpenAI-compatible API server that simplifies LLM execution and employs vLLM [125] and BentoML [29] as backends to maintain high throughput in large-scale environments. It uses Bento, custom file format developed by BentoML, which packages source code, models, data files, and dependencies into single entity. These Bento objects can be transformed into container images for convenient deployment. Representative Characteristics Summary General-Purpose [Low]: Combines vLLM and BentoML back ends to run varied open-source models in the cloud. Ease-of-Deploy [Medium]: One command converts model into Bento image deployable in any BYOC environment. Ease-of-Use [Low]: CLI, web UI, and OpenAI-style endpoints cut application integration time sharply. Latency-Aware [Low]: FlashAttention from vLLM lowers core latency; additional cloud overhead may remain. Throughput-Aware [Medium]: Bento containers batch requests continuously and scale horizontally. Scalability [Medium]: Multi-tenant support is built-in, while multi-node GPU pods require custom orchestration."
        },
        {
            "title": "4.13 TensorRT-LLM\nTensorRT-LLM [191] is a inference engine to optimize inference on NVIDIA GPUs and is part of\nNVIDIA’s NeMO [122] end-to-end generative AI development ecosystem. It includes compilation\nand optimization libraries to boost model inference performance. During compilation, the Ten-\nsorRT [189] compiler analyzes the computation graph to select optimal kernels, fusing them to\nminimize memory overhead. This allows maximal exploitation of CUDA kernels and Tensor Cores,\nand supports various low-precision operations for faster inference.",
            "content": "Models for inference can be trained using NVIDIA NeMo or PyTorch [201], or sourced from pretrained weights on platforms like Hugging Face, and must be converted to TensorRT-compatible format using the Model Definition API. Although TensorRT-LLM primarily uses TensorRT as its backend, it also includes Python and C++ backends for NVIDIA Triton Inference Server [187], providing an end-to-end solution for online LLM deployment. PyTorch backend is available experimentally. With support from NVIDIA Collective Communication Library (NCCL) [186], TensorRT-LLM offers distributed inference via tensor parallelism and pipeline parallelism in multiGPU environments. For optimized serving, in-flight batching groups incoming requests dynamically. To overcome performance constraints of ring-based All-Reduce topologies in multi-node environments, TensorRT-LLM introduces multishot approach that harnesses NVSwitchs multicast capabilities, reducing latency by up to 3. However, TensorRT-LLM is limited to NVIDIA GPUs, restricting hardware scalability. Representative Characteristics Summary General-Purpose [Low]: Targets NVIDIA GPUs exclusively, limiting hardware diversity. Ease-of-Deploy [High]: Model conversion and Triton back-end registration add setup steps despite helper scripts. Ease-of-Use [High]: Sample Python and C++ code exist, but NeMo and Triton familiarity helps. Latency-Aware [Medium]: Kernel fusion on Tensor Cores delivers very low single-token latency. Throughput-Aware [High]: In-flight batching and pipeline parallelism maintain high throughput on large models. Scalability [High]: NVSwitch multicast and NCCL enable efficient multi-GPU and multi-node deployment."
        },
        {
            "title": "4.14 Hugging Face TGI\nHugging Face Text Generation Inference (TGI) [110] is a toolkit for deploying and serving LLMs, sup-\nporting diverse inference workloads and integrating with backends like vLLM [125] and TensorRT-\nLLM [191]. It accommodates various hardware platforms, including NVIDIA GPUs, AWS Inferen-\ntia [15], and Intel Gaudi [118] letting users choose suitable backends for their hardware. Built in\nRust, TGI’s backend supports streaming and concurrency, efficiently handling high LLM traffic.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:23 TGI comprises three key components: router, launcher and model server. The router is an HTTP server that manages client requests (supporting Hugging Faces custom APIs and the OpenAI Message API), batching incoming requests with queue, scheduler and memory block allocator. The launcher spins up one or more model server and shards models based on parameters from the router. The model serverimplemented in Pythonreceives Google Remote Procedure Call (gRPC) [90]-based requests for model loading and inference. To optimize inference, TGI employs quantization, RoPE scaling [155], Safetensors [108], and Zero Config for automatic configuration depending on hardware and model. It also leverages Flashinfer [276] and Flashdecoding [103] to deliver fast performance on long prompts. For observability, it connects with tools like Prometheus [243] and Grafana [39]. When running models on multiple devices, TGI synchronizes using NVIDIA NCCL [186] Although it supports tensor parallelism for multi-device inference, only certain LLM models are currently compatible. Representative Characteristics Summary General-Purpose [Medium]: Swappable vLLM or TensorRT-LLM back ends cover NVIDIA, Inferentia, and Gaudi hardware. Ease-of-Deploy [High]: single launcher auto-configures hardware and downloads model weights. Ease-of-Use [Medium]: Supports custom HF APIs and OpenAI messages with built-in monitoring hooks. Latency-Aware [Medium]: FlashInfer and Flashdecoding accelerate long-sequence generation. Throughput-Aware [Medium]: Router and scheduler batch inputs continuously for high request volume. Scalability [High]: Model sharding and NCCL permit multi-GPU serving across nodes."
        },
        {
            "title": "4.15 PowerInfer\nPowerInfer [227] is an LLM inference system built by extending llama.cpp [82], designed to run\nLLMs on a single consumer-grade GPU. Running LLMs without model compression techniques\noften leads to accuracy loss and memory limitations. CPU-GPU offloading methods suffer from\nhigh PCIe latency, which slows down the inference. Additionally, speculative decoding becomes\ninefficient with small batch sizes and can degrade model performance.",
            "content": "To address these limitations, PowerInfer leverages the observation that neuron activations in LLMs follow power-law distribution. It separates frequently activated neurons (hot neurons) from less active ones (cold neurons). Hot neurons are loaded onto the GPU for fast computation, while cold neurons are handled on the CPU. This design reduces GPU memory usage and minimizes CPU-GPU data transfer. PowerInfer uses an offline profiling step to identify hot and cold neurons based on their activation frequency, and an online predictor to determine which neurons are active for each input. PowerInfer uses hybrid approach for inference, comprising offline and online components. In the offline phase, it analyzes neuron activation patterns (Insight-1) and classifies neurons into hot and cold categories using the activation data. It then performs neuron assignment optimization through Integer Linear Programming (ILP) to maximize memory utilization. In the online component, neurons are assigned to the GPU or CPU based on predefined policies, and distributed computations are performed via GPU and CPU executors. PowerInfer also introduces neuron-aware sparse operators to overcome the limitations of existing sparse computation libraries. These operators can directly handle irregular tensors at the neuron level without format conversion, and are optimized for both GPU and CPU execution. As result, PowerInfer enables efficient LLM inference without fully loading the model into GPU memory, making it practical solution for memory-constrained local environments. Recently, PowerInfer-2 [270] has been proposed to further extend this approach to mobile devices such as smartphones. PowerInfer-2 extends PowerInfers capabilities to scenarios involving ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:24 Park et al. memory-constrained mobile devices. Relying on the same hot-cold neuron algorithm, it partitions matrix operations by neuron clusters and allocates them efficiently between the CPU and NPU, implementing I/O pipeline optimizations for faster inference. During the offline phase, PowerInfer-2 generates an execution plan adapted to neuron activation patterns, hardware constraints, and batch sizes. In the online inference phase, it uses neuron caching along with an NPU-based prefill stage and CPU-NPU hybrid decoding phase, thus boosting overall performance. Representative Characteristics Summary General-Purpose [Low]: Extends llama.cpp for single consumer GPUs and desktop scenarios. Ease-of-Deploy [Low]: Pre-built Docker images simplify setup on one GPU. Ease-of-Use [Low]: Basic scripts are provided, though neuron-level tuning remains manual. Latency-Aware [Medium]: Hot-cold neuron separation removes some transfers but PCIe overhead persists. Throughput-Aware [Medium]: Neuron-aware sparse operators moderately raise tokens per second. Scalability [Low]: Designed for single GPU with CPU assist and no cluster capability."
        },
        {
            "title": "4.16 LMDeploy\nLMDeploy [162] is an inference and serving engine that incorporates several optimization tech-\nniques, including continuous batching [280], dynamic split and fuse, and high-performance CUDA\nkernels. In addition to facilitating efficient inference, it provides features such as quantization,\nfine-tuning, and multi-model services across multiple machines and cards, enabling straightforward\nand effective service deployment in various contexts.",
            "content": "To support high throughput in interactive LLM inference, LMDeploy offers an engine called TurboMind, which is built on NVIDIA FasterTransformer [188]. TurboMind includes efficient LLM implementations, Persistent Batch module, and KV Cache Manager, all accessible through simple API. The Persistent Batch module manages continuous batching with fixed number of batch slots. When request arrives, it occupies one of these slots, and upon completion, the slot is freed. Meanwhile, the KV Cache Manager functions as memory pool, applying Least Recently Used (LRU) policy to decide which sequence cache to evict when additional memory is required. In addition to TurboMind, LMDeploy provides developer-friendly engine named lmdeploy.pytorch, which offers PyTorch-like environment while sharing the same service interface as TurboMind. It performs model loading, adapter integration, cache management, and parallel processing through an Engine object composed of three components. ModelAgent encapsulates the model, Scheduler handles resource allocation and sequence tracking, and RequestManager manages input and output for requests. In particular, the Scheduler uses mechanism similar to vLLMs PagedAttention [125] to allocate and release blocks based on the sequence length and supports S-LoRA [219], enabling multiple LoRA adapters to operate within limited memory. Although LMDeploy features both TurboMind for high-performance inference and lmdeploy.pytorch for easier development, it currently supports only NVIDIA GPU environments. Representative Characteristics Summary General-Purpose [Low]: Includes TurboMind and PyTorch engines but remains NVIDIA-only. Ease-of-Deploy [High]: Docker images and serve script ease installation, though driver matching is needed. Ease-of-Use [Medium]: unified API toggles between high-performance and development modes. Latency-Aware [Medium]: KV-cache LRU and dynamic split-and-fuse significantly reduce prompt latency. Throughput-Aware [Medium]: Persistent batching and continuous scheduling keep GPUs fully occupied. Scalability [High]: Supports multiple GPUs per node; multi-node orchestration is still experimental. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:"
        },
        {
            "title": "4.17 LightLLM\nLightLLM [142] is a Python-based, lightweight, and highly scalable LLM inference engine that\naddresses performance, scheduling, and memory inefficiencies in existing solutions. Using a three-\nprocess asynchronous collaboration approach, it separates tokenization, model inference, and\ndetokenization to boost GPU utilization.",
            "content": "LightLLM replaces PagedAttention [125] with TokenAttention and introduces Efficient Router Scheduling. LightLLM uses an Efficient Router to manage GPU memory at fine-grained, tokenlevel granularity depending on whether it is in the prefill or decode phase. This router employs custom algorithm to batch tokens appropriately. Additionally, the scheduling and model inference stages are merged, removing the communication overhead between the scheduler and the modelRPC. LightLLM also integrates OpenAI Triton [238] to optimize service scheduling kernels. The inference engine consists of multiple modules, each running as separate process (e.g., Metric Server, Health Server, HTTP Server, Router). These modules communicate via ZeroMQ [237] or RPC. The Cache Manager stores multimodal inference results, while the Visual Server handles multimodal requests. LightLLM also features CacheTensorManager class to handle the allocation and deallocation of Torch tensors. By maximizing inter-layer tensor sharing during runtime and permitting memory sharing across distinct CUDA graphs, it reduces overall memory usage. ModelBackend defines the mechanism and operations needed for prefill or decode requests from the router. Each backend maintains its own model object, supporting parallel existence of multiple backends. The model class performs computations on the device and includes tensor parallelism support. Representative Characteristics Summary General-Purpose [Low]: TokenAttention backend offers lightweight footprint for NVIDIA GPUs. Ease-of-Deploy [High]: Manual source builds and custom dependencies increase setup complexity. Ease-of-Use [Medium]: Multi-process ZeroMQ architecture and minimal docs raise the learning barrier. Latency-Aware [Medium]: Triton-optimized kernels and router fusion shorten critical-path latency. Throughput-Aware [Medium]: Efficient router scheduling and memory sharing maintain high TPS. Scalability [Medium]: Multiple back ends can run concurrently; cluster scaling is manual."
        },
        {
            "title": "4.18 NanoFlow\nNanoFlow [300] is a high-performance inference engine that improves LLM throughput by in-\ntroducing Nano-batching and supporting co-scheduling of operations for intra-device parallelism.\nTraditional systems process pipelines sequentially, often underutilizing hardware resources.",
            "content": "By dividing batches into smaller nano-batches, NanoFlow boosts optimization flexibility. It can also estimate GPU memory usage to check whether additional requests fit. If necessary, it offloads KV cache data to lower memory tierslike system memory or diskmaximizing overall resource usage. To implement Nano-batching, NanoFlow classifies LLM service operations into three types: memory-bound operations like self-attention computations, compute-bound operations such as General Matrix Multiplication (GEMM), and network-bound operations such as AllReduce. Then analyzes the resource requirements of each operation and the corresponding iterations or latencies to pinpoint performance characteristics and bottlenecks. Based on these findings, NanoFlow maximizes hardware parallelism to achieve higher throughput. NanoFlow consists of three primary components. The global batch scheduler collects all incoming requests, creates dense batches in high-performance sizes (determined by offline profiling), and uses continuous batching [280] technique to fill these batches dynamically. It also applies chunked prefill [9] operations and discrete batching approach, selecting only the batch sizes ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:26 Park et al. that were identified as optimal rather than arbitrary ones. By prioritizing throughput rather than focusing solely on latency, this method exploits available memory to process more requests in parallel. Next, the intra-device parallelism engine enables fine-grained parallel operations for Nano-batching, along with execution unit scheduling to reduce interference among tasks. Lastly, the KV cache manager oversees the decoding status of every request, estimates future memory usage (assuming an average decode length), and manages GPU memory to prevent out-of-memory issues. If predicted usage does not exceed the GPU limits, the request is accepted; otherwise, it is deferred. However, NanoFlows Nano-batching mechanism requires additional setupsuch as per-model schedule optimizationand may need pipeline adjustments or kernel re-implementation for new models. It also introduces overhead, potentially lowers efficiency for individual operations due to smaller batch sizes, and remains dependent on NVIDIA GPUs. Representative Characteristics Summary General-Purpose [Low]: Operates solely on NVIDIA GPUs and demands per-model nano-schedule tuning. Ease-of-Deploy [Low]: Research-grade code requires custom schedule files and environment tweaks. Ease-of-Use [Low]: Sparse documentation and pipeline modifications limit accessibility. Latency-Aware [Medium]: Memory forecasting and KV offload avoid OOM stalls, indirectly cutting latency. Throughput-Aware [Medium]: Nano-batching plus intra-device parallelism greatly boost throughput. Scalability [Medium]: Confined to single node without distributed scheduling."
        },
        {
            "title": "4.19 DistServe\nDistServe [297] is a serving system designed to efficiently run LLM inference across multiple GPU\nclusters while keeping latency low. It breaks down LLM inference requests at a granular level\nto enable parallel execution, thereby boosting throughput and resource utilization. Traditional\ninference engines handle prefill and decode on a single device, causing resource interference and\npipeline inefficiencies. By decoupling them and applying both intra-operation and inter-operation\nparallelization via SwiftTransformer [222], DistServe reduces overhead.",
            "content": "DistServe also addresses large model sizes, such as 175B-parameter model that can require 350GB of memory. It uses low node-affinity placement algorithm for batch allocation, relying on NVLink when computations for given stage remain on the same node. Online scheduling further manages workloads in real time to meet latency SLO requirements. DistServe consists of batching algorithm module, RESTful API frontend, an orchestration layer, and parallel execution engine. The batching module provides simulator and algorithms to optimally distribute requests based on particular models and cluster setups. The RESTful API frontend supports an OpenAI-compatible interface and accepts user inputs such as maximum output length and temperature. The orchestration layer manages prefill and decode instances, handles request dispatching, and coordinates KV cache transfers. For inter-node GPU communication, DistServe uses NCCL [186], while intra-node transfers rely on asynchronous memory copy. Individual instances run as GPU workers through Ray [181], driven by parallel execution engine. Because DistServe is intended for large GPU clusters, its parallel strategies and resource allocations can be difficult to adapt to smaller-scale or resource-constrained settings (e.g., single or few-GPU systems), potentially limiting performance in those scenarios. Representative Characteristics Summary General-Purpose [Low]: Aims at very large models across multi-GPU clusters. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:27 Ease-of-Deploy [Low]: Requires Ray cluster setup and NVLink topology awareness. Ease-of-Use [Low]: Placement-algorithm tuning and orchestration add complexity for operators. Latency-Aware [Medium]: Decoupled prefill and decode stages reduce tail latency under load. Throughput-Aware [High]: Intraand inter-operation parallelization plus low node-affinity batching maximize throughput. Scalability [High]: Designed for multi-node clusters, scaling to hundreds of GPUs. 4.20 vAttention vAttention [206] is inference engine for dynamically managing KV cache memory during LLM inference. Built on Sarathi-Serve [9], it includes components such as sarathi-lean, vattention memory allocator, and custom Unified Virtual Memory (UVM) driver. These elements support both PagedAttention [125] and vAttention-style memory management. vAttention addresses the complexity and performance limitations linked to virtual contiguity in PagedAttentioncommonly used in transformer-based LLMs. It enhances performance (especially in prefill-bound workloads) while staying compatible with existing kernels. To achieve this, vAttention modifies PyTorch [201] caching allocator to introduce virtual tensors, reserving virtual memory buffers without allocating physical memory from the start. Unlike PagedAttention, where LLM serving systems must manually handle mappings between KV cache and dynamic memory blocks, vAttention integrates memory allocation and computation and enables predictive page allocation. It separates virtual and physical memory usage via low-level CUDA APIs (rather than cudaMalloc), and supports optimizations that target NVIDIAs Hopper architecture through FlashAttention-3 [215], restricting it to NVIDIA GPUs. vAttention is implemented as Python library that wraps CUDA/C++ extension libraries that interfacing with the CUDA driver. During model serving, each worker sets up vAttention based on model parameters and page group sizes, allocating virtual tensors as needed. It checks whether the KV cache is mapped to physical memory before launching kernels, tracking page allocations during both prefill and decode. Only when all current pages are used does it allocate new pages and it frees or reclaims pages once request ends. Representative Characteristics Summary General-Purpose [Low]: Tailored for NVIDIA Hopper GPUs, limiting portability. Ease-of-Deploy [Low]: CUDA driver patches and custom UVM setup complicate installation. Ease-of-Use [Low]: Experimental wrapper and minimal docs hamper quick adoption. Latency-Aware [Medium]: Predictive page allocation hides memory-map costs and speeds prefill. Throughput-Aware [Medium]: Integrated KV memory and compute paths provide moderate gains. Scalability [Medium]: Currently supports only single-node, single-GPU execution."
        },
        {
            "title": "4.21 Sarathi-Serve\nSarathi-Serve [8] is a high-performance inference scheduler built on vLLM [125] to address the\ntrade-off between throughput and latency in LLM inference. It relies on FlashAttention-2 [50] and\nFlashInfer [276] as backends to enhance decode-stage throughput in multi-GPU and multi-node\nenvironments.",
            "content": "Previous systems, such as Orca [280] and vLLM [125], faced generation stallswhere decode requests wait because of prolonged prefilland pipeline inefficiencieswhere insufficient parallelism at the request level left GPU resources underused. Sarathi-Serve tackles these problems via chunked prefill and stall-free scheduling, cutting down TBT while offering high throughput and minimal TBT latency. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:28 Park et al. Sarathi-Serve decides the maximum number of tokens (token budget) in each batch based on TBT SLOs and chunked prefill overhead. Under strict latency requirements, it sets smaller token budget and splits prompts into smaller chunks, lowering tail latency at the cost of some overall system efficiency. Under looser latency constraints, it raises the token budget to improve prefill efficiency. With token budgets like 2,048 or 512, Sarathi-Serve provides efficient inference for varying SLO conditions. Representative Characteristics Summary General-Purpose [Low]: Extends vLLM scheduling to multiple model categories. Ease-of-Deploy [Low]: simple CLI launches servers, yet CUDA and NCCL versions must align. Ease-of-Use [Low]: Interactive SLO slider lets users trade latency for throughput with ease. Latency-Aware [Medium]: Chunked prefill and stall-free scheduling keep TBT consistently low. Throughput-Aware [Medium]: Token-budget batching adapts to workload for maximum throughput. Scalability [Medium]: Multi-GPU and multi-node deployments are supported via FlashAttention-2."
        },
        {
            "title": "4.22 Friendli Inference\nFriendli Inference [71] is a commercial LLM inference engine built on top of Orca [280] and designed\nto enhance inference through features such as iteration batching. It supports both web- and API-\nbased serving via Friendli Container and Friendli Serverless/Dedicated Endpoints, with the\nlatter focusing on stable service by managing traffic and adhering to service-level agreements\n(SLAs). Users can integrate Friendli AI solutions with Amazon SageMaker [16] and gRPC [90]\ninference servers, while monitoring is facilitated through tools like Grafana [39].",
            "content": "For optimization, Friendli Inference allows serving multiple LoRA [105] models on single GPU, maximizing utilization of different user-defined models. It introduces TCache, GPU load reduction technique that caches frequently accessed results to maintain high TTFT compared to conventional frameworks. Quantization techniques are also used to further improve the inference performance. However, Friendli Inference primarily targets NVIDIA GPUs, limiting its support on other hardware platforms. Representative Characteristics Summary General-Purpose [Low]: Focuses on hosting multiple LoRA models on NVIDIA GPUs. Ease-of-Deploy [High]: Container and serverless options plus SageMaker integration simplify rollout. Ease-of-Use [Medium]: Web console with Grafana metrics streamlines monitoring and management. Latency-Aware [Medium]: TCache lowers time-to-first-token, though deeper latency tooling is not described. Throughput-Aware [Medium]: Iteration batching and quantization deliver strong tokens-per-second performance. Scalability [Medium]: Dedicated and serverless GPU instances scale horizontally, but non-GPU back ends are absent."
        },
        {
            "title": "4.23 Fireworks AI\nFireworks AI [67] is a inference platform for rapid and efficient serving of both LLMs and image\nmodels, supporting inference, fine-tuning, and deployment. It offers a simple interface and APIs—\ncompatible with services like LangChain [126] and the OpenAI API—and can operate in serverless,\non-demand, or enterprise environments. Beyond NVIDIA GPUs, Fireworks AI also supports LLM\ninference on AMD Instinct MI300X [225], broadening its hardware compatibility.",
            "content": "To meet diverse throughput and latency demands, Fireworks AI uses multiple parallelization and optimization techniques, including multi/group query attention optimization, sharding, quantization, and continuous batching. It provides deployment configurations specifically tailored for low latency, high throughput, or long input/output sequences. In particular, Fireworks AI employs its own MQA [218] model and custom CUDA kernel (FireAttention) to further accelerate inference. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:29 To ensure service reliability, Fireworks AI is SOC 2 Type II [10] and HIPAA [169] certified, ensuring privacy, security, availability, processing integrity and confidentiality. Representative Characteristics Summary General-Purpose [Low]: Serves LLMs and vision models on NVIDIA GPUs and AMD MI300X accelerators. Ease-of-Deploy [High]: Cloud console offers API keys and quick templates for inference or fine-tuning. Ease-of-Use [Medium]: OpenAI-compatible endpoints and LangChain adapters reduce integration effort. Latency-Aware [Medium]: FireAttention kernels and low-latency deployment profiles minimize response time. Throughput-Aware [Medium]: Multi-query attention, sharding, and continuous batching provide high TPS. Scalability [High]: Serverless and enterprise clusters scale elastically with SOC 2 and HIPAA compliance."
        },
        {
            "title": "4.24 GroqCloud\nGroqCloud [89] is an AI infrastructure platform focused on delivering high-performance, low-\nlatency inference for LLMs through a specialized hardware architecture and software stack. It aims\nto resolve bottlenecks and non-deterministic behaviors commonly found in conventional GPU\nsystems by offering inference services powered by the Groq Language Processing Unit (LPU) [3, 4].\nBuilt specifically for AI inference, the Groq LPU achieves lower latency and higher throughput\nthan traditional GPUs. Its Tensor Streaming Processor (TSP) architecture [4] statically schedules\nand locks the model execution path at compile time, eliminating runtime variability and enabling\npredictable response times.",
            "content": "A key advantage of Groq LPU is its ability to maintain optimal performance even with batch size of one, making it well suited for latency-sensitive applications like financial trading and autonomous driving. Through its TruePoint technology, the Groq LPU delivers near-FP32 precision when using FP16 or INT8 computations. For high-throughput workloads, GroqCloud provides an asynchronous batching API, Flex Processing for scalable throughput, and deterministic quality of service (QoS) scheduling to meet range of SLAs. Additionally, its kernel-less compiler approach removes the need for manual kernel optimization, simplifying development, and lowering maintenance overhead. However, because GroqCloud relies on static compilation, it may have limited flexibility in dynamically adjusting batch sizes or handling complex runtime branching. Representative Characteristics Summary General-Purpose [Low]: Runs exclusively on Groq LPUs, limiting hardware flexibility. Ease-of-Deploy [High]: Fully managed cloud API hides all compilation and runtime details. Ease-of-Use [Low]: Deterministic QoS and simple REST calls facilitate rapid integration. Latency-Aware [Low]: TSP architecture yields sub-millisecond latency even at batch size 1. Throughput-Aware [Medium]: Flex Processing sustains high throughput without hurting single-request latency. Scalability [High]: LPU pods scale horizontally under the same deterministic schedule."
        },
        {
            "title": "4.25 Together Inference\nTogether Inference [239] is part of the Together AI platform, offering high-performance LLM\ninference with an emphasis on speed, cost, and accuracy. To enhance LLM serving, it implements\ntransformer-optimized CUDA kernels, quantization [61], and speculative decoding [131]. Together\nInference provides different model configurations to meet diverse needs, from maximum perfor-\nmance and full-precision accuracy to lower cost and higher throughput. It supports dedicated\ninstances, serverless deployments, and multi-GPU environments; however, it is optimized exclu-\nsively for NVIDIA GPU-based services.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:30 Engines Table 7. Optimizations of LLM Inference Engines Batch Optimization Parallelism Compression Finetuning Caching Attention Optimization Sampling Optimization Output Optimization Park et al. h B u n n t i y l fi - n m l a a g c o d a l l a a Ollama [194] llama.cpp [82] vLLM [125] DeepSpeed-FastGen [102] Unsloth [244] MAX [179] MLC-LLM [177] llama2.c [21] bitnet.cpp [251] SGLang [295] LitGPT [145] OpenLLM [30] TensorRT-LLM [191] TGI [110] PowerInfer [227, 270] LMDeploy [162] LightLLM [142] NanoFlow [300] DistServe [297] vAttention [206] Sarathi-Serve [8] Friendli Inference [71] Fireworks AI [67] Groq Cloud [89] l a r e i l P l P / t t Q o z a r u (A, A, B, D, G, L, M) (B) (A) (A, G) (A, B, G, L, M) (B) (B, G) (A, G, S, W) (A, E, E, G, M) (A, G, S) i P (S, U) s S (MoE, SL) (MoE, SL) (BS, MoE, N:M) (N:M, NxM, MoE, SA) (MoE) (MoE) (MoE) (DSA, N:M, MoE) (MoE) (BSA, MoE) (N:M, MoE) (MoE) (MoE) (N:M) (MoE) (MoE) (MoE) (MoE) (MoE) h t r g c fi g c K t t g o e h F i t i n n A F t t i n n A (v3) (v2) (v3) (v2) (v2) (v1) (v1) (v2) (v2) L d D t c s t e c S (GG) (GG) (LM, OA, OL, XG) (XG) (XG) (LL, OL, XG) (XG) (OL) (GG) (PT) (OL, XG) (OA) (OA) Together Inference [239] A: AWQ, A: AQLM, B: bitsandbytes, D: DeepSpeedFP, E: EXL2, E: EETQ, G: GPTQ, L: LLM Compressor, M: Marlin, S: SmoothQuant, W: Weight-only BS: Block Sparse, BSA: Block Sparse Attention, DSA: Double Sparse Attention, SL: SparseLLM GG: GBNF, LL: llguidance, LM: lm-formet-enforcer, OA: OpenAI API, OL: outlines, PT: PyTorch, XG: XGrammar (v3) Representative Characteristics Summary General-Purpose [Low]: Offers multiple accuracy-versus-cost tiers, but only on NVIDIA GPUs. Ease-of-Deploy [High]: Provides serverless endpoints and dedicated instances through web UI. Ease-of-Use [Medium]: OpenAI-style API eases migration for existing clients. Latency-Aware [Medium]: Speculative decoding and custom CUDA kernels reduce median and tail latencies. Throughput-Aware [Low]: Quantization, optimized attention, and continuous batching significantly raise TPS. Scalability [High]: Multi-GPU instances scale vertically; adding endpoints enables horizontal growth."
        },
        {
            "title": "5 LLM INFERENCE OPTIMIZATION\nLLM inference performance depends not only on model size, and hardware environment but also on\ndiverse inference optimization techniques. The specialized LLM inference engines introduced earlier\nseek low latency and high throughput by employing parallelism, efficient memory management,\nkernel optimization, and quantization.",
            "content": "This section explains key inference optimization strategies, including parallel processing, memory optimization, latency and throughput optimization. In addition to the methods provided by existing inference engines, we also examine recent research findings on inference optimization. Table 7 summarizes the optimization techniques supported by each LLM inference engine. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:31 (a) Static Batching (b) Dynamic Batching (c) Continuous Batching Fig. 9. Comparison of Batching Strategies"
        },
        {
            "title": "5.1 Batch Optimization\nIn LLM inference, batching groups multiple input requests for simultaneous processing, boosting\nhardware utilization and throughput. Efficient batch processing is essential to maximize computa-\ntional parallelism and latency.",
            "content": "Hence, finding the optimal batch size is essential. Smaller batches reduce response time but may underuse hardware resources, while larger batches yield higher throughput but risk longer response times. Various methods have been proposed to select optimal batch sizes [95, 202, 220], and inference engines typically provide mechanisms to explore the best batch size based on workload and SLOs. Beyond batch size, the scheduling method also significantly influences inference performance. As shown in Fig. 9 (a), static batching processes fixed number of requests, potentially increasing latency as new requests must wait until batch completes. Dynamic batching [13, 49] and continuous batching [99, 280], in contrast, adapt the batch in real time, often reducing latency and increasing overall efficiency."
        },
        {
            "title": "5.1.1 Dynamic Batching. Dynamic batching [13, 49] alleviates the latency and hardware underuti-\nlization issues of static batching. As shown in Fig. 9 (b) new requests are immediately added to an\nongoing batch, enabling more flexible and efficient inference.",
            "content": "Unlike static batching, dynamic batching reconstructs batches based on incoming requests and available hardware resources, adaptively determining batch sizes. When new request arrives, it can be merged with an existing batch or appended to an ongoing process to optimize resource usage. Several parameters must be tuned to implement dynamic batching effectively, including the maximum batch wait time, minimum batch size, and batch size limits. Although dynamic batching can minimize latency by reducing batch size in real time, new requests can only be added after the current batch finishes. It may also introduce overhead for dynamically resizing batches and degrade performance if requests have widely varying prompt or output token lengths."
        },
        {
            "title": "5.1.2 Continuous Batching. Continuous batching [99, 280] is similar to dynamic batching [13, 49]\nbut allows new requests to join an ongoing batch without interruption, minimizing latency. Fig. 9 (c)\nshows how requests are continuously inserted to maximize GPU and memory efficiency.",
            "content": "Orca [280] implements continuous batching via Iteration-Level Scheduling and Selective Batching. Iteration-Level Scheduling forms batches each iteration while accommodating new requests on the fly. Selective Batching focuses only on batchable transformer operations, enabling immediate results for completed requests and lowering both the average response time and the waiting time. However, continuous batching requires sophisticated scheduling that new requests can be integrated without disrupting active processing. Efficient KV cache management is crucial, often involving methods such as PagedAttention [125]. Inference engines such as llama.cpp [82], DeepSpeed-FastGen [102], and vLLM [125] use continuous batching techniques derived from Orca. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:32 Park et al. Fig. 10. Nano-batching Fig. 11. Chunked-prefills"
        },
        {
            "title": "5.1.3 Nano-batching. Nano-batching, introduced by NanoFlow [300], maximizes resource utiliza-\ntion and throughput by running computation, memory, and network-bound operations in parallel\non a single device. Traditional inference engines batch tasks at the request level, but NanoFlow\ndivides them at the operation level, as illustrated in Fig. 10.",
            "content": "Operation units include attention and KV generation, GEMM, and collective communication for multi-GPU synchronization. NanoFlow dynamically adjusts nano-batch sizes to optimize each resource type, employing scheduling approach that merges topological sorting and greedy search based on hardware resources and kernel optimizations. By breaking operations into smaller nano batches, tasks can overlap and run concurrently with user-server network operations, boosting resource utilization and increasing throughput. However, nano-batching demands complex scheduling and can incur additional communication overhead when operations are spread across multiple GPUs."
        },
        {
            "title": "5.1.4 Chunked-prefills. Chunked prefills [9] addresses pipeline inefficiencies that arise in dynamic\nor continuous batching, especially in multi-GPU environments where the memory-bound decode\nphase might sit idle if a batch is empty. Processing long prompts in one step can also increase\nlatency and hinder shorter requests.",
            "content": "Fig. 11 illustrates chunked prefills, which splits long prompts into multiple segments and processes them incrementally. The decoding for the first segment can begin immediately while subsequent segments undergo prefill, allowing these phases to run concurrently and improving resource usage. However, chunked prefills adds scheduling complexity by requiring more granular batch management. KV cache usage can also surge due to simultaneous prefill and decode execution. For example, DeepSpeed-FastGen [102]s Dynamic SplitFuse splits prompts to generate tokens earlier, while Sarathi-Serve [8] stall-free scheduling immediately admits new requests, eliminating wait time and boosting efficiency."
        },
        {
            "title": "5.2 Parallelism\nBecause LLMs may contain billions or even trillions of parameters, relying on a single GPU or\nsimilar hardware for inference has become increasingly challenging. As a result, distributed and\nparallel processing across multiple devices or nodes is vital for reducing latency and maximizing\nhardware utilization.",
            "content": "Parallelism strategies in LLM inference differ in their implementation and performance based on server architecture and hardware configuration. Factors such as the number of available GPUs or accelerators, interconnect bandwidth, memory hierarchy, and computational capacity influence how effectively tensor parallelism (TP) [205, 230], data parallelism (DP) [212], FSDP [292], and pipeline parallelism (PP) [9, 106] can be employed. Additionally, hybrid approaches can combine multiple strategies to further improve performance [46, 269]. For example, in multi-node clusters, inter-node communication latency may become bottleneck, necessitating techniques like communication compression or asynchronous scheduling to maintain high performance. Conversely, in single-node, multi-GPU setups, shared memory, and ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:33 (a) No Parallelism (b) Data Parallelism (DP) (c) Fully Sharded Data Parallel (FSDP) (d) Tensor Parallelism (TP) (e) Pipeline Paralleism (PP) Fig. 12. Comparison of Parallelism Strategies high-speed interconnects (e.g., NVLink, NVSwitch) allow for more efficient synchronization and workload distribution. Ultimately, the success of any parallelization strategy depends on balancing computation and communication overhead, emphasizing the need to tailor the approach to each unique hardware environment and model architecture. To address these challenges, researchers are actively developing methods to automatically explore and identify optimal parallelism strategies for each system [139, 175]. Various parallelism mechanisms are shown in Fig. 12."
        },
        {
            "title": "5.2.1 Data Parallelism. As shown in Fig. 12 (b), DP [212] replicates the same model across multiple\nGPUs or nodes. A mini-batch is split among available hardware devices, each performing inference\nindependently on its share of the data. Once the computations are completed, the outputs (or\nweights) are collected into a single device to produce the final results.",
            "content": "Although this method is straightforward to implement and features relatively low communication overheadsince synchronization happens only after inferenceDP can become impractical if the entire model must reside on each device, especially for massive LLMs. Furthermore, if hardware devices differ significantly in performance, the overall system may experience bottlenecks. Fully Sharded Data Parallelism. FSDP [292] is parallelism technique designed to reduce 5.2.2 memory usage and improve training efficiency when working with large language models. Unlike traditional data parallelism, where each device holds full copy of the model parameters and optimizer states, FSDP shards the models parameters, gradients, and optimizer states across multiple devices. This removes duplicated memory usage and allows larger models to be trained on the same hardware resources. As shown in Fig. 12 (c), FSDP works by gathering all the parameters of layer on each GPU right before that layer is executed. This allows full computation to happen on each GPU. The full parameters are temporarily loaded into memory only during this operation and are removed right after the layer finishes. This approach does not split the operation itself, making it simple to implement and compatible with most models. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:34 Park et al. However, since parameters must be all-gathered at every layer, there is communication overhead. This can lead to performance issues during inference, especially for workloads with small batch sizes or where low latency is important. Also, if layer requires more memory than what single GPU can handle during the all-gather phase, it cannot be run. During training, FSDP brings large memory savings by sharding activations and parameters. But during inference, there is no gradient or activation recomputation, thus memory savings are smaller. Therefore, the use of FSDP during inference should be decided based on model size. FSDP is natively supported in PyTorch and works well with its autograd engine, checkpointing, and mixed precision training. It can also be combined flexibly with other parallel strategies such as hybrid parallelism. Among the LLM inference engines we studied, vLLM[125], DeepSpeedFastGen[102], and SGLang [295], LitGPT [145] support FSDP."
        },
        {
            "title": "5.2.3 Tensor Parallelism. TP [205, 230], also known as model parallelism or sharding, divides\nspecific LLM operations (e.g., matrix multiplication, attention, fully connected (FC) layers) across\nmultiple hardware devices. Each device processes a slice of the operation, and intermediate results\nare merged afterward.",
            "content": "For example, Fig. 12 (d) shows situation in which four GPUs handle the matrix operation = Y. The matrix is partitioned among the GPUs, either row-wise or column-wise, and the computations are reconciled via collective communication (e.g., All-Reduce or All-Gather). By distributing large computations, tensor parallelism speeds up inference and reduces the memory footprint of each device, since individual GPUs do not need to store all weights. However, frequent inter-device communication can increase overhead, and suboptimal partitioning may reduce efficiency. Inference engines such as vLLM [125], DeepSpeed-FastGen [102], and TensorRTLLM [191] often integrate techniques to address these challenges [193, 224, 269]. To mitigate communication bottlenecks in tensor parallelismparticularly the performance degradation observed in TTFTrecent research has proposed communication compression techniques that reduce overhead and enhance inference speed [97]."
        },
        {
            "title": "5.2.4 Pipeline Parallelism. PP [9, 106] assigns different parts (layers) of an LLM to different GPUs.\nThe input data are split into micro-batches which traverse this pipeline of layers sequentially. As\nillustrated in Fig. 12 (e), if a transformer model has four layers and there are four GPUs available,\neach GPU is responsible for one layer.",
            "content": "This arrangement can reduce memory usage by distributing layers across devices and can also accelerate inference by overlapping operations. However, communication overhead occurs when intermediate results move between devices, and the initial pipeline stages remain underutilized until the pipeline is warmed up. Various pipeline optimization techniques have been proposed to mitigate these concerns [164, 279]. Various inference frameworks support PP, including Ollama [194], llama.cpp [82], vLLM [125], and Friendli Inference [71]."
        },
        {
            "title": "5.3 Compression\nAs LLMs grow larger, conducting inference on a single GPU or server node becomes increasingly\ndifficult. To mitigate this issue, model compression techniques—such as quantization [61], KD [271],\nand pruning [120, 301], sparsity optimization [65, 223]—have emerged. Among these, quantization is\nparticularly important for saving memory and increasing inference speed, thereby reducing power\nconsumption and cost. Pruning and sparsity optimization can enhance computational efficiency\nand inference speed, and several inference engines offer support for these techniques. Although\nthey are closely tied to training or fine-tuning, inference engines must still ensure proper kernel\nselection and execution when running quantized models.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:35 (a) Quantization Scheme (b) Post Training Quantization (PTQ) (c) Quantization-Aware Training (QAT) Fig. 13. Quantization Methods"
        },
        {
            "title": "5.3.1 Quantization. Quantization Algorithm. Quantization converts pretrained FP32 or FP16\nmodels into lower-precision floating-point formats (e.g., FP4, FP8) or integer formats (e.g., INT4,\nINT8), as illustrated in Fig. 13 (a). By representing fewer distinct numerical values, quantization\ncan substantially accelerate matrix multiplications and reduce memory requirements with only\nminor performance trade-offs.",
            "content": "To convert high-precision models into lower-bit representations, methods like Absolute Max Quantization [52] calculate scale factor from the tensors absolute maximum value. During dequantization, the models values are approximated by applying the scale factor to the quantized data. Although this method is straightforward and hardware-efficient, it can be sensitive to outliers. Approaches like Affine Quantization [167] address this issue by adjusting the distribution more flexibly. Depending on the workflow, several methods can be applied, such as post-training quantization (PTQ) [133, 265], which applies quantization after model training (Fig. 13 (b), or quantization-aware training (QAT) [42, 159], which integrates quantization into the training process (Fig. 13 (c) PTQ can be used with pretrained models, making it straightforward and quick to implement. When applying PTQ, small calibration dataset of representative sample data is used to refine the quantization parameters in the pretrained model. This dataset helps determine the activation distributions within each layer, which are then used to define the quantization parameters, such as clipping ranges and scale factors. However, it may result in accuracy degradation due to quantization effects. In contrast, QAT incorporates quantization operations into the training process, allowing gradient information to be considered and thereby preserving accuracy more effectively. However, QAT involves additional stepssuch as fine-tuning quantization parameters, retraining, and adjusting training strategieswhich increase overall training costs [42]. Consequently, even in services where high accuracy is required, PTQ is used more frequently in practice, given these realistic constraints. range of quantization methods can be used for LLMs and are supported by various inference engines. Generalized Post-Training Quantization (GPTQ) [69] offers weight-only quantization, optimizing per-layer scales with error compensation to minimize accuracy degradation. ActivationAware Weight Quantization (AWQ) [146] improves weight quantization by grouping weights according to activation distributions, increasing accuracy. Additive Quantization of Language Models (AQLM) [62] quantizes both weights and activations using PTQ only, avoiding QAT and achieving high performance with lower overhead than GPTQ. SmoothQuant [265] normalizes activation and weight distributions to reduce clipping during quantization, leading to stable PTQ-based activation quantization while reducing latency and memory usage. Additionally, KV cache quantization [104, 161] has been proposed to minimize memory usage in long-context scenarios, allowing balance between memory efficiency and generation speed while minimizing the impact on model quality. KVQuant [104] applies ultra-low precision quantization with minimal accuracy drop using Pre-Channel and Pre-RoPE key quantization, non-uniform ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111: Park et al. Table 8. Data Type Support in LLM Inference Engines Engines FP32 Ollama [194] llama.cpp [82] vLLM [? ] DeepSpeed-FastGen [102] Unsloth [244] MAX [179] MLC LLM [177] llama2.c [21] bitnet.cpp [251] SGLang [295] LitGPT [145] OpenLLM [30] TensorRT-LLM [191] TGI [110] PowerInfer [227] LMDeploy [162] LightLLM [142] NanoFlow [300] DistServe [297] vAttention [206] Sarathi-Serve [8] Friendli Inference [71] Fireworks AI [67] GroqCloud [89] Together Inference [239] Data Type FP16 FP4 NF4 BF16 FP8 INT8 INT4 MXFP8 MXFP6 MXFP4 MXINT8 KV cache quantization, and Per-Vector Dense-and-Sparse Quantization. KIVI [161] quantizes key caches per channel and value caches per token, achieving 2-bit quantization. Kernel Code and Hardware Support. Many inference engines integrate external quantization tools. bitsandbytes [33] is CUDA-based Python library that supports 8-bit and 4-bit quantization; it supported in engines such as vLLM [125] and SGLang [295], LitGPT [145]. DeepSpeed FP is DeepSpeeds library for 6-bit and 8-bit weight-only quantization and is partially supported in vLLM [125] on NVIDIA GPUs. ExLlamaV2 (EXL2) [242] is an inference library for consumer GPUs, offering flexible 2-bit to 8-bit quantization similar to GPTQ, along with the option to mix quantization at different bit-levels per layer. EETQ [182] provides straightforward and efficient approach to INT8 weight-only PTQ for transformer models. Both EXL2 and EETQ are supported in TGI [110]. LLM Compressor [247] is quantization library designed for the vLLM environment, allowing both weight-only and activation quantization. It supports mixed-precision modes (e.g., W4A16, W8A16) and integrates techniques such as simple PTQ, GPTQ [69], and SmoothQuant [265]. Inference engines such as vLLM [125] and SGLang [295] can employ LLM Compressor for quantization. Mixed Auto-Regressive Linear Kernel (Marlin) [70] is highly optimized kernel for FP16INT4 matrix multiplication. Designed to maximize inference speed, it can theoretically deliver up to four times the performance of FP16 by fully utilizing GPU global memory, cache, shared memory, and tensor cores. Implemented at the NVIDIA Parallel Thread Execution (PTX) [185] assembly level, Marlin depends on NVIDIA GPUs and is supported by vLLM [125], SGLang [295], and TGI [110]. These quantization techniques are closely connected to the hardware supported by each inference engine. As indicated in Table 8, every engine accommodates specific data types, which in turn govern how quantized models are executed. In particular, data types based on block-level scaling in the 4-8-bit rangesuch as the microscaling (MX) format [213]have been proposed to balance training and inference performance, accuracy, and framework compatibility. Examples include MXFP8 and MXINT8. Each MX block consists of single scale value and set of compressed values (𝑃1, 𝑃2, . . . , 𝑃𝑘 ). The scale format (e.g., E8M0) and element format (e.g., FP4, FP6, FP8, INT8) can be independently configured. Unlike traditional FP8 or INT8 formats, which require single tensor-level scaling factor to match the dynamic range of the entire tensor, MX formats split the tensor into smaller sub-blocks ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111: Fig. 14. Pruning in Transformer-based Model Fig. 15. Three Types of Pruning: Structured, Unstructured, and Contextual and assign separate scale values to each, thereby circumventing the limitations of sub-8-bit formats. Hardware platforms such as the Qualcomm Cloud AI 100 [40] and NVIDIA GPUs based on the Hopper architecture support MX formats, and some inference engines (e.g., TensorRT-LLM [191]) offer support for them."
        },
        {
            "title": "5.3.2 Pruning. Overview and Inference Benefits of Pruning. Pruning [120, 301] is a model\ncompression technique that removes less important parameters to reduce model size, often targeting\nweights or attention heads, as shown in Fig. 14 in Transformer-based LLMs. It can be applied after\ntraining by zeroing out and removing certain weights, applied dynamically during training/fine-\ntuning [158], or performed through one-shot pruning [68, 216] to rapidly shrink large models.",
            "content": "From an inference perspective, pruning reduces the number of parameters, thereby improving memory utilization, bandwidth efficiency, and cache usage. As more weights become zero, sparse computation can reduce actual computation costs. However, to fully exploit sparse computation, the inference engine or compute library must support kernels capable of skipping or efficiently handling zero weights. Three Types of Pruning. Pruning methods in LLMs generally fall into three categories: structured pruning, unstructured pruning, and contextual pruning [27], and each pruning method is illustrated in Fig. 15. Structured pruning eliminates groups of parameters with fixed structures, such as convolutional filters or neuron channels. Since the matrix dimensions are physically reduced, the inference speed can be improved even without specialized sparse computation kernels. However, inference graphs or kernels must be adjusted to match the pruned architecture. One of the structured pruning methods is LLM-Pruner [166], which prunes low-importance structures based on gradient information. Unstructured pruning removes individual weights according to their importance scores. Although this reduces model size and FLOPS, the use of random sparsity patterns can limit performance gains on dense matrix multiplication kernels, requiring optimized sparse kernels for effective acceleration. For example, NVIDIA CUDA provides sparse operations via cuSPARSE [190]. Among the unstructured pruning methods, Wanda [233] prunes weights based on the product of weight magnitude and input activation. Contextual pruning dynamically assesses the importance of weights depending on the input context or domain, selectively removing or retaining weights. This approach adapts the model to specific inputs, skips unnecessary computation paths, and improving inference efficiency. Although sparse computation may yield smaller performance gains compared to other methods, it can enhance domain-specific accuracy. Implementing contextual pruning requires inference engines to support conditional branching or precompiled kernels with logic for bypassing certain layers. For example, Mini-GPTs [245] applied contextual pruning to Phi-1.5 [135] and Opt-1.3 [285] using legal and medical QA datasets to prune linear, activation, and embedding layers. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111: Park et al. (a) Structured Sparsity - N:M sparsity (b) Dynamic Sparsity - MoE Fig. 17. Sparsity Optimizations Fig. 18. LoRA Recent Advances: Post-Training and Token Pruning. Like quantization, post-training pruning has been research focus for LLMs, given the high cost of training or fine-tuning. Recent efforts involve unstructured and semi-structured post-training pruning algorithms to address the Multiple Removal Problem (MRP) by pruning large quantities of weights at the LLM layer level [291]. In LLM inference, as the input token length increases, the TTFT generally increases as well. One proposed solution is token pruning [72], which selectively computes KV representations solely for tokens judged important for next-token predictionwithout requiring extra training or fine-tuning. The remaining tokens are deferred and only computed if needed, reducing initial computation costs and improving TTFT. Engine-Level Support for Pruned Models. Among the inference engines discussed in this paper, fewer than half directly support pruning. Most rely on NVIDIA pruning libraries, with DeepSpeed-FastGen [102] explicitly supporting row, head, sparse, and structured/unstructured pruning through the DeepSpeed backend. Other engines generally only support running pre-pruned models. Sparsity Optimization. Sparsity Optimization Overview. Sparsity optimization [65, 223] 5.3.3 is technique that reduces computational costs and speeds up inference by increasing the number of zero values in model weights or activations. This approach can reduce memory usage and improve compute performance when supported by hardware or inference engines that allow sparse operations. Although it shares the same goal as pruning, sparsity optimization focuses on designs sparse model structures or applies predefined sparse patterns to achieve computational efficiency. Sparsity can be applied to the attention mechanism (e.g., sparse attention patterns) or to the weights of individual heads. Pruning can induce sparsity and models that are already sparse can be further refined through pruning. Sparsity optimization techniques include structured sparsity [56, 294], dynamic sparsity [288], and kernel-level sparsity [34, 263]. Structured Sparsity. Structured sparsity [56, 294] imposes sparsity on weights or tensor values in fixed patterns, simplifying hardware-level optimizations. Typical examples are N:M sparsity [287] as shown in Fig. 17 (a), where values within an m-sized block remain active; and block sparsity [74] which divides weight matrices into blocks and removes values to create sparsity. On NVIDIA GPUs and similar hardware, these static patterns can be optimized at the time of model compilation. However, rigid patterns may negatively affect model performance. Dynamic Sparsity and Mixture-of-Experts. Dynamic sparsity [288] activates only the computations required at run time based on input tokens, skipping unnecessary operations to improve efficiency. prominent example is MoE [38], which replaces the MLP with multiple FFNs (experts) but activates only subset of them, depending on the input tokens, as shown in Figure 17 (b). This reduces the number of computations per token and enables large models to run more efficiently. To support MoE, the inference engine must offer gating mechanisms and flexible architectures for dynamic routing. Example MoE models include Mixtral 8x7B [115], DeepSpeed-MoE [211], ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:39 and DeepSeek-R1 [96], which allow more tokens to be processed or trained with limited time or resources. Most inference engines provide MoE support and related optimizations. Enhancements to Sparse MoE Training. In dense expert models, all experts are activated for every input, leading to more complicated computation. Sparse MoE [60, 66] complements this by using only some of them, such as selecting only top-k experts. To address reduced specialization or unstable training due to sparse-gating, SMoE-Dropout [44] employs randomized router networks that gradually increase the number of active experts throughout training to refine the model. Token and Contextual Sparsity. Another example of dynamic sparsity is dynamic token sparsity [72, 272] which avoids computing attention for all tokens by focusing on subset. In addition to contextual pruning [245], contextual sparsity [12, 160] has also been proposed, which selectively activates only subset of attention heads or MLP parameters depending on the input. In contextual sparsity research such as Deja Vu [160], lightweight sparsity method was employed to dynamically skip computations based on the input context, addressing the high cost of verifying whether contextual sparsity truly exists for each input. Kernel-Level Sparsity. Kernel-level sparsity checks for zero values in the computation kernels and skips them. For example, sparse matrix-dense matrix multiplication (SpMM) [34] kernels or rely on libraries like cuTeSpMM [264] to utilize NVIDIA GPU Tensor Cores. The xFormers [172] library also includes CUDA kernels for memory-efficient attention, sparse attention, and block-sparse attention. Support in Inference Engines. Several engines already integrate such methods. vLLM [125], SGLang [295] and TGI [110] support N:M sparsity, with vLLM also offering block sparsity. SGLang applies Double Sparse Attention [272], post-training sparse attention approach that prioritizes essential tokens (token sparsity) in self-attention while determining important feature channels offline (channel sparsity). DeepSpeed-FastGen [102] adopts the Sparse Attention technique to introduce block-level sparsity in self-attention, reducing compute and memory usage through variety of patterns and custom modifications. TensorRT-LLM [191] employs Block Sparse Attention to accelerate SpMM through block-structured sparsity, relying on the Sparse Tensor Cores featured in NVIDIA GPUs from the Ampere architecture onward."
        },
        {
            "title": "5.4 Fine-tuning\nLLMs typically rely on foundation models pretrained on large-scale datasets to perform diverse infer-\nence tasks. However, for domain-specific or task-specific optimization, fine-tuning can significantly\nboost model performance.",
            "content": "Parameter-Efficient Fine-Tuning. Fine-tuning is technique originally applied in CNN that modifies the parameters of pre-trained model. It can be divided into full-parameter finetuning [163], where every parameter is updated, and parameter-efficient fine-tuning (PEFT) [54], where only subset of parameters is adjusted. Because full fine-tuning requires substantial hardware resources, LLMs generally favor PEFT methods that update only part of the model. Although fine-tuning mainly targets model training, it directly affects LLM inference performance as well. Fine-tuning can be implemented by inserting adapter networks (additional layers) to recalibrate parameters [107] or by supplying domain-specific data through prompt engineering [275]. Low-rank adaptation. LoRA [105, 219] is representative PEFT approach. Instead of updating the entire model, LoRA keeps the original weights frozen and trains additional low-rank matrices to adjust the model parameters. Research indicates that despite the large dimensions of the FC layers in LLMs, the effective dimensionality required for adaptation is relatively low. LoRA exploits this by approximating weight updates with low-rank matrices, greatly reducing the training cost. As shown in Fig. 18, LoRA retains the pre-trained weight matrix and trains only two small matrices, 𝐴 and 𝐵. Rather than directly computing updates for the full weight matrix (𝑑 𝑘, 𝑑 : input dimension, ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:40 Park et al. Fig. 19. Prompt Caching Fig. 20. Prefix Caching 𝑘 : output dimension), LoRA approximates 𝐴 𝐵 by low rank (𝑟 ) matrix multiplication (𝑑 𝑟 and 𝑟 𝑘). major advantage of LoRA is the ability to swap in different LoRA modules for varying tasks, facilitating quick adaptation without retraining the entire model. Moreover, since LoRA only updates the small low-rank matrices, training is faster and demands less memory. Merging these trained modules with the original weights generally does not degrade inference speed, although merging may limit the capacity for multi-task processing in real time. Quantized Low-rank adaptation. As LLM sizes increase, additional methods like Quantized LoRA (QLoRA) [53, 286] have been introduced, combining 4-bit quantization with LoRA fine-tuning. QLoRA backpropagates through 4-bit quantized model, preserving LoRAs benefits while reducing memory usage. This enables running large models on single device, offering efficient options for both training and inference. Support in Inference Engines. Ollama [194], llama.cpp [82], Friendli Inference [71], and Fireworks AI [67] support LoRA, morevoer DeepSpeed-FastGen [102], SGLang [295], TensorRTLLM [191], Unsloth [244], and vLLM [125] support QLoRA. Especially, vLLM [125], TensorRTLLM [191], TGI [110], LMDeploy [162], Friendli Inference [71], and Together Inference [239] also provide Multi-LoRA functionality, enabling simultaneous serving of multiple user-customized models."
        },
        {
            "title": "5.5.1 Prompt Caching. A large portion of LLM prompts may include frequently reused text. Iden-\ntical content, such as system messages or common instructions, often appears multiple times—\nparticularly in conversational agents, coding assistants, or extensive document processing. To\noptimize this, a technique known as Prompt Cache [83, 299] has been introduced. The Prompt\nCache stores attention states for frequently used text segments in advance, and when the same\nsegment reappears in a prompt, it reuses those stored attention results and only computes the new\nsegments, thereby expediting inference. However, because the transformer architecture applies\npositional encoding, the attention states depend on the position of each segment, meaning that reuse\nis only possible if the segment appears in the same position. To overcome this limitation, Prompt\nMarkup Language (PML) was proposed. It explicitly defines a prompt’s structure, identifies reusable\nsegments (prompt modules), and assigns unique position IDs to each module. PML functions as",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:41 schema for module positions and hierarchy, offering an interface for generating and reusing attention states at the module level. An example of prompt caching is shown in Fig. 19. Prompt Caching in Commercial AI Services. Commercial services such as ChatGPT [196] and Claude [23] also employ prompt caching. ChatGPT routes API requests with the same prompt to the same server that previously handled them, allowing reuse of cached results instead of recomputing from the first token. This approach has reduced latency by as much as 80% for long prompts. To increase cache hit rates, ChatGPT places static components (e.g., instructions, examples) at the beginning of the prompt, while dynamic user content goes at the end. ChatGPT applies caching to prompts exceeding 1,024 tokens and triggers cache hits in 128-token segments. It retains cached prompts for five to sixty minutes, depending on system load, automatically evicting any prompts that remain unused. Claude employs similar format, providing up to four cache breakpoints that split prompts into multiple cacheable segments. Cache Hit Prediction. method has also been proposed to improve the accuracy of prompt caching [299]. This study recommends predicting the effectiveness of caching based on embedding similarity. In single-turn question-answer scenarios, the study uses embeddings refined through knowledge distillation to determine whether cached responses can be reused. Cosine similarity is computed between prompt embeddings, and model is trained to decide if the same response can be reapplied. This research also presents finite sample guarantees for loss functions like Binary Cross Entropy (BCE) and Squared Log Difference (SLD). Support in Inference Engines. Several inference engines, including Ollama [194], llama.cpp [82] and TensorRT-LLM [191], support prompt caching. TensorRT-LLM [191] provides system prompt caching, while Ollama [194] offers optimized prompt caching even in multi-user environments."
        },
        {
            "title": "5.5.2 Prefix Caching. Prefix Caching [152, 199] is conceptually similar to prompt caching [83, 299]\nbut focuses on caching only the common prefix segments that reappear across multiple requests,\nrather than caching the entire prompt, as shown in Fig. 20. During batched inference, when multiple\nprompts share the same prefix, the computation for that shared segment can be reused to improve\nefficiency. For instance, in question-answering tasks where the same system prompt or few-shot\nexamples are repeatedly used, caching these portions of the prompt during the prefill phase can\nreduce overall inference time.",
            "content": "However, prefix caching typically accelerates only the prefill phase and does not affect the decode phase. Consequently, if the primary bottleneck stems from extended decoding for very long responses, the performance gains from prefix caching may be limited. In addition, if new request does not share prefix with any existing request, the caching advantage diminishes. Support in Inference Engines. vLLM [125] provides Automatic Prefix Caching (APC), which stores the KV cache from previous requests and reuses it whenever new request shares prefix with an existing one, thus skipping attention computations for the shared segments. TGI [110] employs high-performance data structures instead of basic string matching to speed up prefix lookups and applies chunking code to optimize memory usage; it also integrates prefix caching with Flashdecoding [103] kernels to support rapid inference for lengthy sequences. MAX [179] employs PagedAttention [125]-based mechanism to apply prefix caching and improve inference efficiency, and other engines, such as LMDeploy [162], also include prefix caching features."
        },
        {
            "title": "5.5.3 KV Caching. In self-attention of the Transformer, each token attends to all preceding tokens,\nresulting in a time complexity of O (𝑛2). To reduce this overhead, KV Caching [75, 204, 256] was\nproposed. By storing the K and V matrices produced in each token step and reusing them for\nsubsequent tokens, inference can run in O (𝑛) time. The KV Cache operation in the prefill and\ndecode phases is as shown in Fig. 21. This provides significant efficiency gains when large batch",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:42 Park et al. Fig. 21. KV Caching size processing or multi-turn conversation scenarios. KV caching also works well alongside other optimizations such as Prefix Caching [152, 199] and Speculative Decoding [131, 154, 228]. Increasing KV Cache Size. However, the memory needed for KV caching increases significantly with longer contexts because both and must be stored in memory. The memory needed for LLM inference can be expressed as 𝑆𝐾𝑉 /token = 2 𝑛layers (𝑛heads 𝑑head) 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 and the total size of KV Cache can be expressed as Σ 𝑆𝐾𝑉 = 𝑠batch 𝑙seq 2 𝑛layers 𝑠hidden 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛. In the equation, 𝑛𝑙𝑎𝑦𝑒𝑟𝑠 represents the number of layers, 𝑛ℎ𝑒𝑎𝑑𝑠 represents the number of attention heads, and 𝑑ℎ𝑒𝑎𝑑 represents the dimension of the heads. 𝑠𝑏𝑎𝑡𝑐ℎ is the batch size, 𝑙𝑠𝑒𝑞 is the sequence length, 𝑠ℎ𝑖𝑑𝑑𝑒𝑛 is the hidden size, 𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 is precision bytes such as FP16, and each KV cache size (𝑆𝐾𝑉 , Σ 𝑆𝐾𝑉 is in bytes. Multiplication by 2 in the equation accounts for both the and components. Reducing batch size to decrease KV cache memory lowers memory usage but also reduces hardware utilization and throughput. Limiting the sequence length forces the recomputation of some KV caches, causing inefficiency. Reducing the depth of the model may compromise performance; hence, current research focuses on attention mechanisms, cache compression, and quantization techniques. KV Cache Optimizations. As shown in Fig. 3, attention mechanisms like GQA [11] and MQA [218] reduce the number of heads or reuse KV heads, naturally shrinking KV cache size. Research also focuses on compressing KV caches. MiniCache [149] uses depth-wise compression, observing that middle-to-deep layer KV caches are highly similar. H2O [289] leverages sparsity of the attention matrix to reuse only essential parts while retaining the same output tokens. FlexGen [221] profiles the structural properties of the attention heads and proposes adaptive KV cache compression. As explained in Section 5.3.1 above, quantization can also be applied to reduce the KV cache size [104, 161]. As context windows and model sizes grow, GPU memory alone can become insufficient to store KV caches, leading to offloading solutions. InfiniGen [129] prefetches necessary KV segments into CPU memory speculatively, while CacheGen [157] encodes and streams KV caches as compressed bitstreams in distributed systems, reducing network transfer latency. Support in Inference Engines. Many inference engines implement KV caching. LMDeploy [162] and Ollama [194] use INT8 or FP16 quantization to reduce the cache size. DeepSpeed-FastGen [102] applies ZeRO-Inference [18]-based offloading and 4-bit quantization to increase throughput in resource-constrained settings. TensorRT-LLM [191] offers various optimizations, including paged KV cache, KV cache quantization, circular KV cache, and KV cache reuse. It saves memory using an LRU-based eviction strategy and KV-aware routing or scheduling that forwards requests to instances with the required KV cache already in place. vLLM [125] implements KV Cache Preemption, allowing some requests to be preempted to free up space. Although preempted requests ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:43 Fig. 22. PagedAttention Fig. 23. TokenAttention required recomputation and may increase end-to-end latency, this approach can enhance overall system stability."
        },
        {
            "title": "5.6 Attention Optimization\nAttention lies at the core of transformer-based LLMs, but its high memory and computational costs\nmake optimization essential for efficient service deployment. As the sequence length grows, the\ntime required for attention operations rises quadratically, and calculating and storing Q, K and V\nmatrices consumes substantial memory. Increasing memory efficiency and batch size is therefore,\ncritical for managing multiple requests and boosting overall throughput.",
            "content": "To improve inference performance, most LLM inference engines utilize KV Cache mechanism [157, 231] that stores the and vectors from previously generated tokens. This approach prevents redundant computations when producing subsequent tokens, thereby reducing both inference latency and computational overhead. LLM inference engines offer range of optimization techniques for efficiently storing and retrieving the KV cache, as well as methods to improve the use of Q, and vectors are utilized during inference."
        },
        {
            "title": "5.6.1 KV Cache Optimization: PagedAttention. Effective KV cache management is crucial for en-\nhancing LLM inference performance. Conventional inference engines often allocate a contiguous\nKV cache based on the maximum sequence length, which can lead to internal and external frag-\nmentation as sequence lengths vary. These fragmentation issues reduce memory efficiency and\nlimit parallelism.",
            "content": "PagedAttention [125] addresses this problem by adopting Linux-style paging mechanism, as shown in Fig. 22. It partitions the KV cache into smaller pages and uses page table to map logical blocks to physical blocks. Newly needed blocks are allocated on demand, and memory from completed requests is quickly reclaimed for new requests. Requests with identical prompts share the same KV cache block, further saving memory. Various inference engines, such as DeepSpeedFastGen [102] and vLLM [125], MAX [179], SGLang [295] integrate PagedAttention to improve inference efficiency. LightLLM [142] introduces TokenAttention, which manages KV cache at the token level  (Fig. 23)  . Instead of page table, TokenAttention uses token table to track each tokens actual storage location. It preallocates KV cache based on user-defined maximum token limit and assigns continuous memory regions to new requests. This strategy minimizes fragmentation and improves resource utilization through fine-grained memory management. Other approaches, such as ChunkedAttention [274], reduce KV cache duplication by recognizing that system prompts often repeat. ChunkedAttention splits the KV cache into smaller chunks and organizes them with Prefix-Aware KV Cache (PAKV) structure, enabling requests with the same prefix to share cache blocks. This further enhances memory efficiency. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:44 Park et al. Algorithm 1 Simple FlashAttention for 𝑘 1 to 𝑁 do for 𝑖 1 to #tiles do 𝑖 𝑥𝑖 𝑄 [𝑘, :]𝐾 [:, (𝑖 1)𝑏 : 𝑖𝑏] 𝑚 (partial) (cid:0)𝑥𝑖 [ 𝑗 ](cid:1) max𝑏 𝑗 =1 𝑚𝑖 max (cid:0)𝑚𝑖 1, 𝑚 (partial) 𝑒𝑚𝑖 1 𝑚𝑖 + (cid:205)𝑏 𝑖 𝑑 𝑑 𝑗 =1 𝑑 𝑖 1 𝑖 𝑜 𝑜 𝑑 𝑖 Copy to SRAM Compute on SRAM (cid:1) Compute on SRAM 𝑒𝑥𝑖 [ 𝑗 ]𝑚𝑖 Compute on SRAM 𝑉 [ 𝑗 + (𝑖 1)𝑏, :] Compute on SRAM 𝑒𝑚𝑖 1 𝑚𝑖 + (cid:205)𝑏 𝑒𝑥𝑖 [ 𝑗 ]𝑚𝑖 𝑑 𝑖 𝑖 1 𝑖 1 𝑗 =1 𝑖 Fig. 24. FlashAttention end for 𝑂 [𝑘, :] 𝑜 𝑖 end for Copy to HBM I/O Optimization: FlashAttention. During LLM inference, attention requires (𝑛2) computa5.6.2 tions for sequence of length 𝑛. This involves forming score matrix through the dot product of Q, K, and V, which is highly memory-intensive due to frequent data transfers between memory hierarchies on GPUs. FlashAttention [51] reduces unnecessary data transfers by splitting Q, K, and into smaller blocks. Unlike approaches that compute the entire attention matrix before applying softmax, FlashAttention applies an online softmax per tile, avoiding redundant writes to memory. It also fuses matrix multiplication and softmax into single pass, thereby decreasing kernel invocation overhead. In order to illustrate the core concept of attention operation fusion in FlashAttention, we present the essential idea in simplified manner, as shown in Algorithm 1 and Fig. 24. In fused attention, only small subset of intermediate results is maintained on-chip at each step, which enables memory-efficient self-attention. This design scales linearly with the sequence length 𝑁 while respecting GPU shared memory limits. We define 𝑏 as the block size (also referred to as the tile width). The total number of tiles along (cid:109) . The term 𝑥𝑖 R𝑏 denotes the pre-softmax logits the sequence dimension is given by #tiles = for tile 𝑖. We let 𝑚𝑖 be the global maximum value over all tiles from 1 to 𝑖, and 𝑚(partial) represent the maximum value within the partial tile 𝑖. The variable 𝑑𝑖 is the cumulative denominator for the softmax computation up to tile 𝑖. Finally, 𝑜𝑖 is the partial output vector (corresponding to 𝑂 [𝑘, :]) that accumulates results up to tile 𝑖. Each iteration computes the local logits 𝑥𝑖 for the current tile and updates the softmax scaling factor 𝑚𝑖 , cumulative denominator 𝑑𝑖 , and partial output 𝑜𝑖 . (cid:108) 𝑁 𝑏 Because 𝑥𝑖 , 𝑚𝑖 , 𝑑𝑖 , and 𝑜𝑖 have small and fixed sizes (O (𝑏) or (𝐷)), they can reside in shared memory during kernel execution. This formulation ensures that the computation is both numerically stable and compatible with parallel tiling, making it ideal for long-sequence Transformer inference or training. 𝑖 FlashAttention-2 [50] further optimizes GEMM and related non-matrix operations. It merges certain scaling steps and allows parallelization along the sequence length dimension, which is beneficial when the batch size and number of attention heads are small. This strategy maximizes GPU utilization for long sequences, but initially suffered from low GPU usage in GEMM on NVIDIA H100 GPUs. To address that, FlashAttention-3 [215] introduces asynchronous computation and low-precision arithmetic. By splitting data transfer and computation into separate GPU warps and using producer-consumer model, it overlaps softmax operations with Warp Group Matrix MultiplyAccumulate (WGMMA), reducing latency. Many LLM inference engines now include support for FlashAttention variants, with vLLM [125] compatible up to FlashAttention-3 [215]. Because these optimizations are closely related to NVIDIA ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111: GPU architectures, researchers are exploring ways to adapt similar principles for other hardware [35, 103], aiming to generalize attention acceleration across diverse platforms."
        },
        {
            "title": "5.6.3 KV Cache Reuse: RadixAttention. RadixAttention is an optimization technique proposed\nby SGLang [295] that enables the automatic reuse of the KV cache across multiple operations.\nTraditional inference engines flush all related KV caches once a request finishes, which prevents\nreuse between requests and slows performance. To address this limitation, SGLang manages KV\ncaches with a radix tree-based LRU mechanism that allows fast matching, insertion, and deletion,\nand it applies cache-aware scheduling to handle diverse reuse patterns efficiently. The approach is\ncompatible with continuous batching [280], PagedAttention [125], and tensor parallelism, and it\nadds only minimal time and memory overhead, even for cache misses.",
            "content": "RadixAttention maps token sequences to their corresponding KV cache tensors through radix tree, while the cache itself is stored in non-contiguous, page-based memory layout. The tree resides in CPU memory and incurs little maintenance cost. When continuous batching is enabled, the nodes referenced by active batches cannot be deleted. Each node maintains reference counter that tracks how many active requests use it, and nodes are removed only when this counter reaches zero. RadixAttention also applies in multi-GPU environments. Under tensor parallelism, each GPU keeps its own shared KV cache and sub-tree without inter-GPU synchronization. The SGLang Router builds and manages meta-tree by combining all GPU sub-trees. When new batch arrives, the Router performs prefix matching on the meta-tree and selects dispatch policies based on request affinity. After processing, the router and the workers update their local trees independently, and the Router updates the meta-tree during periods of low system load to maintain consistency."
        },
        {
            "title": "5.6.4 Attention Programming Model: FlexAttention. Numerous attention optimizations have been\nproposed to accelerate attention operations, but most rely on manually writing hardware-specific\nkernels, complicating implementation and testing. To improve applicability and flexibility, FlexAt-\ntention [57] was introduced.",
            "content": "FlexAttention is general-purpose, flexible programming model that allows developers to implement diverse attention optimizations with only minimal additional code in PyTorch [201]. Observing that most attention variants can be expressed by modifying the intermediate score matrix before the softmax stage, FlexAttention accepts two callable functionsscore_mod and mask_mod together with the tensor input. During compilation, PyTorch automatically converts these functions into template-based handwritten attention kernels, and both forward and backward graphs are generated through the PyTorch autograd machinery. Operator fusion occurs automatically in this process, producing optimized kernel code without any low-level kernel development. FlexAttention also supports sparsity optimization through BlockMask, which records blocklevel sparsity in the mask matrix to reduce computational load and memory usage while enabling flexible composition of multiple attention variants. The framework allows for independent or combined implementation of techniques such as Relative Positional Embedding [217], ALiBi [207], and FlashAttention [51]. FlexAttention is supported in PyTorch 2.5 and later, and Unsloth [244] provides application of various kernels based on FlexAttention."
        },
        {
            "title": "5.6.5 MQA Optimization: FireAttention. FireAttention is an FP16- and FP8-based optimization\ntechnique developed by Fireworks AI [67] to improve the performance of MoE models. Implemented\nas a custom CUDA kernel for MQA, it efficiently leverages memory bandwidth across a wide range\nof batch sizes and sequence lengths on NVIDIA H100 GPUs. Designed for multi-GPU environments,",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:46 Park et al. Fig. 25. Speculative Decoding FireAttention achieves higher requests per second (RPS) and lower token-generation latency than traditional LLM inference engines. FireAttention V2 adds FP16 and FP8 prefill-kernel support for the NVIDIA Hopper architecture and introduces multi-host deployment optimizations. In long-context, online inference workloads, it delivers up to an 8 increase in throughput and up to 12 reduction in latency relative to existing engines. FireAttention V3 further refines key matrix-multiplication and attention operations by providing dedicated kernels that extend support beyond NVIDIA GPUs to AMD MI300 hardware."
        },
        {
            "title": "5.7 Sampling Optimization\nBecause LLMs generate text autoregressively, longer input sequences increase the amount of com-\nputation and prolong user wait times. Moreover, memory I/O latency rather than raw computation\noften becomes the main bottleneck, affecting performance in interactive AI systems or real-time\ntranslation scenarios, where quick responses are critical.",
            "content": "Speculative decoding [131, 262] accelerates token generation by leveraging an optimization concept inspired by speculative execution in computer processors. In speculative execution, operations are performed in parallel before it is confirmed whether they are neededmuch like branch prediction. Fig. 25 illustrates this concept in LLMs through two models: high-accuracy target model (the original LLM) and lighter and faster draft model. The draft model generates candidate tokens that the target model later validates. This mechanism, known as speculative sampling, allows the target model to check up to 𝐾 tokens at once and either accept or reject them. If certain tokens are rejected, additional tokens are sampled from an adjusted probability distribution. Parallel validation of these candidate tokens constitutes speculative decoding, speeding up generation without altering the architecture of the original model. Speculative Decoding Model. Several inference engines can employ multiple draft models for speculative decoding. Although lightweight LLMs are commonly used as drafts, some systems adopt models built on the Extrapolation Algorithm for Greater Language-model Efficiency (EAGLE) [137]. EAGLE is speculative sampling framework that reduces feature-level uncertainty by using one-step-ahead token sequence and performs accurate feature prediction with minimal overhead. It achieves speculative sampling through tree-structured draft that employs tree attention, and it is lightweight enough for real-world deployment by adding only one transformer-decoder layer as plug-in module. Because the acceptance rate of draft tokens depends on both position and context, EAGLE2 [136] estimates this rate with confidence score from the draft model and dynamically adjusts the draft-tree structure to increase the number of accepted tokens. Following recent test-time scaling trends, EAGLE-3 [138] was introduced to overcome the featureprediction constraints in the original EAGLE [137], which limited token-prediction flexibility and reduced the benefits of data augmentation. EAGLE-3 removes these constraints and directly predicts ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:47 Fig. 26. Unstructured Outputs and Structured Outputs Fig. 27. Constrained Decoding in Decoding Phase tokens. By simulating multi-stage generation during training, it maximizes input flexibility for draft models and achieves higher speed-up ratios as training data scales. Optimization for Speculative Decoding. Research continues to refine speculative execution. Tree-based Speculative Inference [174] simultaneously generates multiple candidate sequences, increasing the likelihood that the target model will approve tokens. To address difficulties in adapting draft models to changing input distributions, online speculative decoding [154] gradually aligns draft model with the target model using knowledge distillation. This allows training and deployment of different draft models for various input patterns, and routes queries to the most suitable draft model, increasing token acceptance rates. One study proposes accelerating draft model inference by applying MXFP4 weight-only quantization [78]. This research introduces MultiLevel Speculative Decoding with Quantized Draft Models (ML-SpecQD), method that combines quantization with staged speculative decoding by delegating token generation in the draft model to an even smaller, quantized draft model. Google also applied speculative decoding to its AI search function, which improved the inference speed more than two times. Support in Inference Engines. Several LLM inference frameworks already incorporate speculative decoding. Ollama [194], PowerInfer [227], MAX [179] and llama.cpp [82] each expose this capability directly. vLLM [125] performs offline speculative decoding, generating up to five tokens at time and-including an n-gram-based suggestion module-splits the input string into n-grams to compute similarity scores. MLC LLM [177] supports speculative decoding with lightweight draft models, as well as Medusa [37] and EAGLE [137]-family drafts, and TGI [110] also offers Medusa [37] and n-gram methods. SGLang [295] implements speculative decoding with EAGLE [137], EAGLE-2 [136], and EAGLE-3 [138] and integrates smoothly with Radix Cache and chunked-prefill. TensorRT-LLM [191] accepts draft models such as EAGLE [137], EAGLE-2 [136], and Medusa [37] and additionally supports Recurrent Drafter (ReDrafter) [47], which recursively predicts drafts, and look-ahead decoding, which performs parallel n-gram prediction and verification. Commercial engines, including Friendli Inference [71] and Fireworks AI [67], also leverage speculative decoding to accelerate inference."
        },
        {
            "title": "5.8 Structured Outputs\nIn autoregressive LLMs, the generated tokens function as both the model’s inputs and its outputs.\nDuring tokenization, however, meaningful units can be divided or Unicode characters fragmented.\nThis limitation becomes especially problematic in applications that require problem solving or\nplanning, such as reasoning tasks or AI agents, where structured outputs like JSON, function calls,\nor code blocks are essential.",
            "content": "As shown in Figure 26, structured output [113, 151] refers to generating text that follows predefined formatJSON, XML, or another structured schema. Unlike typical free-form LLM output, structured generation ensures that the produced content conforms to constraints expected ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:48 Park et al. by downstream systems. For example, if database entry or an API call requires JSON, structured output enables the LLM to return valid JSON. Modern LLMs have evolved beyond basic text generation to support tasks such as code creation, function invocation, and autonomous decision making. To enable these applications, inference engines must provide machine-readable structured outputs that integrate smoothly with other systems. Structured generation produces output that complies with explicit schemassuch as JSON or SQLthus improving correctness and consistency, simplifying interpretation and integration, and reducing hallucinations by eliminating unnecessary or invalid information. Consequently, domain-specific output requirements are often satisfied without additional fine-tuning [250]. Constrained decoding [250, 260] is commonly used to generate structured outputs. As Figure 27 illustrates, the entire vocabulary is evaluated at each decoding phase and tokens that violate the output schema are masked out. After the logits are computed, token-level mask invalidates any token that would break the structure; those logits are then set to zero before the softmax operation. This masking procedure directly influences both the performance and the speed of structured generation. In structured generation with finite-state machine (FSM) [260], the LLM generates tokens sequentially while moving through the states defined by the machine. The FSM records the current state and assigns probability of zero to any token that violates the required format, thereby filtering it out. Structures such as JSON schemas can be modeled as directed-graph FSMs, where each node represents valid partial sequence and each edge represents an allowed next token. FSM decoding is fast, but it handles only simple patterns and cannot easily express recursive structures such as nested JSON. Using context-free grammar (CFG) [28, 77] supports more complex structured generation. CFG defines language structure through production rules and can capture formats that an FSM cannot. Runtime guidance with CFG, however, requires recursively checking these rules against the entire vocabulary and maintaining multiple parser stacks, which adds significant computational overhead. Library/Frameworks for Structured Outputs. The Outlines [59] library enables guided generation through FSM based regular expression parser that can be started or stopped at any point in the decoding process. To efficiently identify valid tokens, it builds an index that lets each step run in amortized (1) time. The library expands sequences with multinomial sampling until an EOS token appears, then applies Boolean mask to produce an unnormalized conditional distribution; the same mask is reused in subsequent steps. In addition to FSM guidance, Outlines supports multiple-choice prompts and structured generation from CFGs written in Extended Backus-Naur Form (EBNF). XGrammar [58] is structured generation library designed for efficiency, flexibility, and portability. It supports CFGs for complex formats and includes system-level optimizations for high-speed execution. C++ backend simplifies integration into diverse runtimes. To reduce recursion overhead, XGrammar converts each CFG into byte-level pushdown automaton (PDA). The PDA separates context-independent tokensvalidated by position alonefrom context-dependent tokens that need entire stack inspection. Context-independent tokens are prevalidated and cached for reuse, while context-dependent tokens are dynamically checked by the PDA. XGrammar also maintains persistent execution stack for rapid branching and rollback and expands context windows to decrease context-dependent tokens, achieving up to 100 lower per-token latency. The LM Format Enforcer [184] guarantees output formatting through token-level filtering while preserving the expressive freedom of the model. It works independently of the base model and tokenizer, supports batch decoding and beam search, and enforces formats such as JSON Schema, JSON, or regular expressions. Internally, it merges Character-Level Parser, which used to read ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:49 character sets, with Tokenizer Prefix Tree that stores every token the tokenizer can be generated. token is accepted only if both structures allow it. After each token is generated, the parser and tree advance together, preparing the constraints for the next step. This approach maintains strict adherence to the target format, while allowing the model to manage details such as spacing and word order. Low-level Guidance (llguidance) [93] is library for fast structured output generation using CFGs. It accepts CFG, tokenizer, and token prefix, then computes the set of valid next tokens (a token mask) that can follow the prefix while still yielding strings valid under the grammar. The library is highly efficient, taking only about 50 µs of CPU overhead per token for 128 𝑘-entry tokenizer. This speed comes from CFG parser based on the Earley algorithm on top of regular expression derivatives [94] and token prefix trie for mask computation. Supported grammar formats include the JSON Schema, regular expressions, CFGs derived from Lark [127], and the llguidances own grammar syntax. GGML Backus-Naur Form (GBNF) [81] is formal grammar format introduced in llama.cpp [82] to constrain the output. It merges traditional BNF notation with modern regex features, specifying grammars as production rules that link non-terminal and terminal symbols. GBNF is compatible with JSON Schema and can be used via CLI or API, allowing straightforward control of structured generation. OpenAI Structured Outputs [197] enables structured generation through function calling and response-format specifications. Function calling permits model to invoke external system functions safely, while response-format specification ensures that the output matches the desired JSON schema. Both features provide type safety, eliminating post-processing or retries due to format errors, and maintaining consistent formatting without extensive prompt engineering. Built-in safety policies allow the system to reject unsafe or inappropriate requests. Performance Evaluation of Structured Output. Benchmarks such as The Beyond the Imitation Game Benchmark (BIG-Bench) [229] and Massive Multitask Language Understanding (MMLU)-Pro [255] were used to evaluate multilingual and multitask performance of the LLM. As the quality of structured output has become critical for real-world LLM deployment, newer benchmarks target this capability more directly. JSONSchemaBench [76] provides 10,000 real-world JSON schemas of varying complexity and constraints, enabling the evaluation of constraineddecoding techniques on metrics such as schema adherence, output efficiency, and generalization. Using the official JSON Schema test suite, JSONSchemaBench evaluates llguidance [93], GBNF [81], Outlines [59], XGrammar [58], OpenAI [197], and Gemini [86], providing detailed functional and accuracy analyses. StructTest [41] is rule-based evaluator that measures the ability of LLM to follow complex instructions while producing structured output. Emphasizing cost efficiency, ease of use, bias reduction, and robustness to data contamination, StructTest reports both accuracy and consistency and indirectly gauges instruction decomposition and reasoning capabilities. It has been applied to models such as GPT-3.5 and GPT-4 [5], the Claude 3 family [22], and DeepSeek-v3 [148] for tasks including summarization, code generation, HTML creation, and mathematical reasoning. To further study structured generation, SoEval [156] offers structured output benchmark dataset covering 13 output typessuch as JSON and XMLacross more than 20 domains, including science, technology, literature, and healthcare. Support in Inference Engines. From the inference engine perspective, vLLM [125] supports guided decoding with Outlines [59], LM Format Enforcer [184] and XGrammar [58] in both online such as OpenAI Completions and Chat APIs [197] and offline modes. SGLang [295] integrates Outlines [59], XGrammar [58], and llguidance [93], while LightLLM [142] supports Outlines [59] and XGrammar [58]. MAX [179], MLC LLM [177], and TensorRT-LLM [191] adopt XGrammar [58], ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:50 Park et al. whereas Ollama [194], llama.cpp [82], and PowerInfer [227] rely on GBNF [81] for format enforcement. LMDeploy [162] provides structured output through its PyTorch [201] interface, and major commercial engines offer equivalent capabilities via proprietary solutions or OpenAI-compatible APIs [197]."
        },
        {
            "title": "6.1 Requirements for Multimodal LLMs\nMost current LLM inference engines are optimized for text-based models. However, relying solely on\ntext has limitations when processing information. To achieve human-level intelligence, it is essential\nto support multiple data types such as images, audio, and video. In response to this, multimodal\nmodels like Qwen2-VL [252] and LLaVA-1.5 [150] have been developed. To support such models\neffectively, inference engines must be designed to handle multimodal data preprocessing and\nmulti-stream parallel execution efficiently.",
            "content": "In this context, existing model compression techniques, such as quantization must be adapted to preserve modality-specific information while still reducing model size. Software-level methods like hybrid parallelization are not sufficient on their own. Therefore, new hardware-accelerated kernels and decoding methods, such as speculative decoding tailored for multimodal inputs, need to be considered. good example of adapting model architecture to hardware for multimodal tasks is Multimodal Rotary Position Embedding (M-RoPE), introduced in Qwen2-VL[252] and LLaVA-1.5 [150]. M-RoPE extends the traditional positional embedding used in Transformer models to more effectively capture the positional relationships in various multimodal inputs."
        },
        {
            "title": "6.2 Alternative Architectures Beyond Transformers\nWhile Transformer-based models still dominate LLM inference, new architectures tailored for\nmultimodal LLM are emerging. Models like RetNet [234] and RWKV [203] propose alternatives to\nthe standard Transformer design. Another notable direction is Mamba [91], a sequence modeling\narchitecture developed to overcome the limitations of Transformers. Mamba uses a Selective State\nSpace Model (SSM) to process long sequences more efficiently, achieving linear time complexity\nwithout relying on the standard attention mechanism.",
            "content": "Jamba [141] is hybrid model that combines the Mamba [91] and Transformer architectures, aiming to take advantage of both. It also integrates MoE strategy that increases model capacity while keeping the number of active parameters manageable during inference. These trends highlight the growing need for general-purpose inference engines that can support diverse architectures. Future inference systems must not only be optimized for internal operations of Transformer models but also be scalable and flexible enough to support new and evolving model structures."
        },
        {
            "title": "6.3 Hardware-Aware Fusion and Mixed-Precision Kernels for Efficiency\nIn traditional AI workloads that primarily relied on convolution operations, optimization was often\nachieved through simple operator fusions, such as with ReLU. However, in the era of generative\nAI based on Transformers and diffusion models, more sophisticated fusion strategies tailored to\nspecific hardware architectures are required. A representative example is FlashAttention-3 [215],\nwhich is highly optimized for NVIDIA’s H100 hardware. These complex fusion techniques involve",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:51 advanced tiling strategies, carefully designed by expert engineers to align with the hardwares shared memory and cache size constraints. Moreover, as shown in Table 8, generative AI models demand support for wide range of data type precisions to reduce model size while preserving accuracy. Therefore, it is essential to develop operator kernels that can flexibly and efficiently handle mixed-precision computation."
        },
        {
            "title": "6.4 Extended Context Windows and Memory Management\nThere is a growing trend in LLMs toward handling extremely long context windows, ranging from\ntens of thousands to millions of tokens. For example, ChatGPT o1 [196] supports up to 128K tokens,\nGoogle Gemini 2.0 Flash [86] supports 1M tokens, and the recently introduced Llama 4 Scout [171]\nclaims to handle up to 10M tokens.",
            "content": "This expansion leads to dramatic increase in the KV cache size, posing significant challenges for memory management. To address this, techniques such as hierarchical cache management [293], partial offloading to CPU memory [129], and memory-efficient attention mechanisms [125] have been proposed. However, these methods are not yet sufficient to fully cope with the increasing context length, and further research is required."
        },
        {
            "title": "6.6 Inference Engine Selection Based on Application Needs\nThe selection and design of LLM inference engines should be based on a balance between application\nrequirements and system constraints. For applications like translation services or conversational\nagents where real-time interaction is critical, latency optimization is the top priority. On the other\nhand, server-side applications that must handle high-volume traffic will prioritize throughput\nmaximization.",
            "content": "Looking ahead, it will be increasingly important to develop inference engines that support both hardware acceleration for multimodal data and general-purpose compatibility with diverse model architectures. Persistent challenges include optimizing memory for extended context windows, designing architectures that can flexibly handle complex reasoning, and developing strategies that strike the right balance between latency and throughput."
        },
        {
            "title": "6.7 Security Support in Inference\nDuring LLM inference, vulnerabilities such as prompt injection attacks, jailbreak attacks, and data\nleaks have emerged [273]. Prompt injection attacks occur when an attacker manipulates inputs to\noverride the model’s system prompts or objectives. In environments handling sensitive data, such\nas finance and healthcare, personal data exposure risks are significant. Additionally, if malicious\nattacks generate abnormal or harmful data, it can severely impact user experience and system\nstability.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:52 Park et al. To mitigate these risks, robustness techniques such as adversarial training [153] can be applied during the model training phase. During inference, tools like OpenAI Moderation [168], instruction manipulation prevention, and input sanitization methods [180, 183] can be used to block harmful or malicious inputs. From service security perspective, role-based access control (RBAC) and multi-factor authentication (MFA) can be implemented to prevent unauthorized access. Additionally, access tokens can be set to expire after certain period to enhance security policies. Currently, most LLM inference engines focus primarily on performance and do not include dedicated security features. However, they aim to reduce risks through methods such as data filtering and strengthened ethical policies."
        },
        {
            "title": "6.8 Support for On-Device Inference\nMost LLM services perform inference using large-scale resources in cloud or data center envi-\nronments, delivering the results to users. While this approach enables fast computation, it is\nnetwork-dependent and requires transmitting user data to servers, raising privacy concerns. As a\nresult, demand for on-device (or on-premise) LLM inference on edge and mobile devices has been\nincreasing.",
            "content": "Traditionally, LLM models were too large to run on single device, but the emergence of small language models (SLMs) such as Llama 3.2 [88], Gemma [235], Phi-3 [2], and Pythia [32] has enabled LLM execution on embedded systems, mobile devices, and IoT systems, as well as single-GPU environments. Since edge environments (e.g., embedded and mobile systems) have lower hardware specifications than servers, both model compression and hardware-specific parallelization and memory optimizations are essential. For example, mobile inference optimizations include tolerance-aware compression, I/O recomputation pipeline loading, and chunk life cycle management [277]. Research has also explored collaborative inference, where multiple edge devices share computational workloads [283]. For single-GPU environments, 4-bit quantization and memory offloading techniques that distribute weights, activations, and KV caches across CPU, disk, and other memory resources [221] are being investigated. These advancements help reduce server power consumption and enable personalized models and inference in network-limited environments. However, edge devices vary significantly by manufacturer and environment, making generalized optimization difficult. Additionally, developing compiler and hardware-dependent transformation tools incurs extra development costs. Among existing LLM inference engines, llama.cpp [82], MLC LLM [177], and TensorRT-LLM [191] offer partial support for edge environments. llama.cpp [82], implemented in C/C++, is highly portable across different platforms. MLC LLM [177] uses the TVM [43] compiler to support GPU, mobile, and web environments, although its hardware compatibility is limited. TensorRT-LLM [191] supports only specific edge devices, such as NVIDIA Jetson series."
        },
        {
            "title": "6.9 Support Diverse Hardware for Inference Optimization\nLLM inference, which was traditionally centered around NVIDIA GPUs, is now expanding to\nheterogeneous hardware with the emergence of TPU [116], Neural Processing Units (NPUs), and\nvarious LLM accelerators. In addition to widely used AWS Inferentia [15] and Google TPU [116],\nnew accelerators such as AMD Instinct MI300X [225], Furiosa AI RNGD (Tensor Contraction Pro-\ncessor) [121], and Cerebras CS-2 (WSE-2) [140] are being developed. Furthermore, next-generation\nmemory technologies such as Processing-in-Memory (PIM) [119, 198] are also under development.",
            "content": "ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:53 To support heterogeneous hardware, engines must incorporate pipeline execution, batch optimization, and load balancing. However, differences in performance, synchronization, and communication overhead across hardware types can pose challenges. Research has explored LLM inference optimizations in heterogeneous GPU clusters using Phase-Aware Partitioning and Adaptive Quantization [290] and hardware-aware allocation of prefill and decode processes for optimized inference [202]. Furthermore, techniques such as sub-batch interleaving [101] have been proposed to optimize inference across systems with multiple NPUs and PIM devices."
        },
        {
            "title": "6.10 Support for Cloud Orchestration and Serving Platforms\nCloud orchestration and serving strategies are critical for LLM inference services. When deploying\nlarge-scale inference services in the cloud, orchestration platforms like Kubernetes [36] enable\nautoscaling, hardware resource monitoring (Prometheus [243], Grafana [39]), and failover recov-\nery. To facilitate this, inference engines should provide containerized environments, multi-node\ndeployment, and load balancing tools that allow easy configuration based on service requirements\nand SLOs.",
            "content": "Most LLM inference engines offer built-in serving functionalities, but large-scale serving systems require additional workload distribution, scheduling, and autoscaling optimizations. vLLM [125], TensorRT-LLM [191], DistServe [297], and Sarathi-Serve [297] utilize Ray [181] to support distributed runtime and serving. Additionally, TensorRT-LLM [191] integrates with NVIDIA Triton Inference Server [187] and NVIDIA Dynamo [192] for model deployment and execution, while TGI [110] enables model deployment via Hugging Face Spaces [109]."
        },
        {
            "title": "7 CONCLUSION\nThis paper systematically analyzed the optimization methods and hardware adaptation strategies\nof LLM inference engines. First, we identified the memory and computation bottlenecks of decoder-\nonly transformers and summarized mitigation methods such as batching, parallelism, caching, and\ncompression. Second, we classified 25 open source and commercial inference engines along two\naxes, single-node versus multi-node, and homogeneous versus heterogeneous device support, and\ncompared their architectural goals and supported hardware. In particular, we analyzed the inference\nengine with a focus on ease-of-use, ease-of-deployment, general-purpose support, scalability,\nthroughput-aware, and latency-aware. Our analysis showed that selecting an inference engine\nrequired balancing multiple factors, including latency-throughput trade-offs, hardware diversity,\ninference engine-level optimization support, and SLO. Additionally, we outlined future directions\nthat included multimodal inference support, alternative transformer architectures, longer context\nwindows, improved logical reasoning, application-specific design, stronger security, on-device\nexecution, heterogeneous acceleration, and cloud orchestration. In general, this study provided a\npractical foundation for designing and operating next-generation inference infrastructure.",
            "content": "ACKNOWLEDGMENTS This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No. RS-2024-00402898, Simulationbased High-speed/High-Accuracy Data Center Workload/System Analysis Platform) and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government(MSIT) (No.RS-2023-00277060, Development of open edge AI SoC hardware and software platform). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:"
        },
        {
            "title": "REFERENCES",
            "content": "Park et al. [1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. 2016. {TensorFlow}: system for {Large-Scale} machine learning. In 12th USENIX symposium on operating systems design and implementation (OSDI 16). 265283. [2] Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, et al. 2024. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 (2024). [3] Dennis Abts, Garrin Kimmell, Andrew Ling, John Kim, Matt Boyd, Andrew Bitar, Sahil Parmar, Ibrahim Ahmed, Roberto DiCecco, David Han, et al. 2022. software-defined tensor streaming multiprocessor for large-scale machine learning. In Proceedings of the 49th Annual International Symposium on Computer Architecture. 567580. [4] Dennis Abts, Jonathan Ross, Jonathan Sparling, Mark Wong-VanHaren, Max Baker, Tom Hawkins, Andrew Bell, John Thompson, Temesghen Kahsai, Garrin Kimmell, et al. 2020. Think fast: tensor streaming processor (TSP) for accelerating deep learning workloads. In 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA). IEEE, 145158. [5] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [6] Abien Fred Agarap. 2018. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375 (2018). [7] Saurabh Agarwal, Anyong Mao, Aditya Akella, and Shivaram Venkataraman. 2024. SYMPHONY: Improving Memory Management for LLM Inference Workloads. arXiv preprint arXiv:2412.16434 (2024). [8] Amey Agrawal, Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, Alexey Tumanov, and Ramachandran Ramjee. 2024. Taming {Throughput-Latency} tradeoff in {LLM} inference with {Sarathi-Serve}. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 117134. [9] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav Gulavani, and Ramachandran Ramjee. 2023. Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills. arXiv preprint arXiv:2308.16369 (2023). [10] AICPA & CIMA. 2022. SOC 2 Reporting on an Examination of Controls at Service Organization Relevant to Security, Availability, Processing Integrity, Confidentiality, or Privacy. E-book. [11] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 48954901. [12] Yash Akhauri, Ahmed AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander Rush, Safeen Huda, and Mohamed Abdelfattah. 2024. Shadowllm: Predictor-based contextual sparsity for large language models. arXiv preprint arXiv:2406.16635 (2024). [13] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni. 2020. Batch: Machine learning inference serving on serverless platforms with adaptive batching. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 115. [14] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023). [15] Amazon Web Services. 2018. AWS Inferentia. https://aws.amazon.com/ai/machine-learning/inferentia/ [16] Amazon Web Services. 2024. Amazon SageMaker AI. https://aws.amazon.com/sagemaker-ai/ [17] Amazon Web Services. 2024. SwiftChat - Cross-platform AI Chat App. https://github.com/aws-samples/swift-chat [18] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 115. [19] Anaconda, Inc. 2012. Conda: OS-agnostic, system-level binary package and environment manager. https://anaconda. org/anaconda/conda [20] Andrej Karpathy. 2022. nanoGPT. https://github.com/karpathy/nanoGPT [21] Andrej Karpathy. 2023. llama2.c: Inference Llama 2 in one file of pure C. https://github.com/karpathy/llama2.c [22] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1 (2024), 1. [23] Anthropic PBC. 2023. Claude. https://claude.ai/ March 12, 2025. [24] Apple Inc. 2017. Core ML. https://developer.apple.com/documentation/coreml/ [25] Artificial Analysis. 2024. Artificial Analysis: AI Model & API Providers Analysis. https://artificialanalysis.ai/ March 12, 2025. [26] AutoGPT. 2023. AutoGPT: Build, Deploy, and Run AI Agents. https://agpt.co/ ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:55 [27] Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan Zhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. 2024. Beyond efficiency: systematic survey of resource-efficient large language models. arXiv preprint arXiv:2401.00625 (2024). [28] Shraddha Barke, Emmanuel Anaya Gonzalez, Saketh Ram Kasibatla, Taylor Berg-Kirkpatrick, and Nadia Polikarpova. 2024. Hysynth: Context-free llm approximation for guiding program synthesis. Advances in Neural Information Processing Systems 37 (2024), 1561215645. [29] BentoML. 2019. BentoML: Unified Inference Platform for any model, on any cloud. https://www.bentoml.com/ [30] BentoML. 2023. OpenLLM: Self-Hosting LLMs Made Easy. https://github.com/bentoml/OpenLLM [31] Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. 2024. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954 (2024). [32] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: suite for analyzing large language models across training and scaling. In International Conference on Machine Learning. PMLR, 23972430. [33] bitsandbytes-foundation. 2021. bitsandbytes. https://github.com/bitsandbytes-foundation/bitsandbytes [34] Urban Borštnik, Joost VandeVondele, Valéry Weber, and Jürg Hutter. 2014. Sparse matrix multiplication: The distributed block-compressed sparse row library. Parallel Comput. 40, 5-6 (2014), 4758. [35] William Brandon, Aniruddha Nrusimha, Kevin Qian, Zachary Ankner, Tian Jin, Zhiye Song, and Jonathan RaganKelley. 2023. Striped attention: Faster ring attention for causal transformers. arXiv preprint arXiv:2311.09431 (2023). [36] Brendan Burns, Brian Grant, David Oppenheimer, Eric Brewer, and John Wilkes. 2016. Borg, omega, and kubernetes. Commun. ACM 59, 5 (2016), 5057. [37] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774 (2024). [38] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. survey on mixture of experts. arXiv preprint arXiv:2407.06204 (2024). [39] Mainak Chakraborty and Ajit Pratap Kundan. 2021. Grafana. In Monitoring cloud-native applications: Lead agile operations confidently using open source software. Springer, 187240. [40] Karam Chatha. 2021. Qualcomm Cloud Al 100: 12TOPS/W scalable, high performance and low latency deep learning inference accelerator. In 2021 IEEE Hot Chips 33 Symposium (HCS). IEEE, 119. [41] Hailin Chen, Fangkai Jiao, Mathieu Ravaut, Nawshad Farruque, Xuan Phi Nguyen, Chengwei Qin, Manan Dey, Bosheng Ding, Caiming Xiong, Shafiq Joty, et al. 2024. StructTest: Benchmarking LLMs Reasoning through Compositional Structured Outputs. arXiv preprint arXiv:2412.18011 (2024). [42] Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, and Ping Luo. 2024. Efficientqat: Efficient quantization-aware training for large language models. arXiv preprint arXiv:2407.11062 (2024). [43] Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan, Haichen Shen, Meghan Cowan, Leyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018. {TVM}: An automated {End-to-End} optimizing compiler for deep learning. In 13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18). 578594. [44] Tianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei Liu, and Zhangyang Wang. 2023. Sparse moe as the new dropout: Scaling dense and self-slimmable transformers. arXiv preprint arXiv:2303.01610 (2023). [45] Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023. Universal self-consistency for large language model generation. arXiv preprint arXiv:2311.17311 (2023). [46] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. 2023. Ee-llm: Large-scale training and inference of early-exit large language models with 3d parallelism. arXiv preprint arXiv:2312.04916 (2023). [47] Yunfei Cheng, Aonan Zhang, Xuanyu Zhang, Chong Wang, and Yi Wang. 2024. Recurrent drafter for fast speculative decoding in large language models. arXiv preprint arXiv:2403.09919 (2024). [48] Krishna Teja Chitty-Venkata, Sparsh Mittal, Murali Emani, Venkatram Vishwanath, and Arun Somani. 2023. survey of techniques for optimizing transformer inference. Journal of Systems Architecture 144 (2023), 102990. [49] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael Franklin, Joseph Gonzalez, and Ion Stoica. 2017. Clipper: {Low-Latency} online prediction serving system. In 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 613627. [50] Tri Dao. 2023. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691 (2023). [51] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems 35 (2022), 1634416359. [52] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in neural information processing systems 35 (2022), 3031830332. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:56 Park et al. [53] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems 36 (2023), 1008810115. [54] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence 5, 3 (2023), 220235. [55] Docker Inc. 2013. Docker. https://www.docker.com/ [56] Harry Dong, Beidi Chen, and Yuejie Chi. 2023. Towards structured sparsity in transformers for efficient inference. In Workshop on Efficient Systems for Foundation Models@ ICML2023. 112. [57] Juechu Dong, Boyuan Feng, Driss Guessous, Yanbo Liang, and Horace He. 2024. Flex Attention: Programming Model for Generating Optimized Attention Kernels. arXiv preprint arXiv:2412.05496 (2024). [58] Yixin Dong, Charlie Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, and Tianqi Chen. 2024. Xgrammar: Flexible and efficient structured generation engine for large language models. arXiv preprint arXiv:2411.15100 (2024). [59] dottxt-ai. 2023. outlines: Make LLMs speak the language of every application. https://github.com/dottxt-ai/outlines [60] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International conference on machine learning. PMLR, 55475569. [61] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2025. Exploiting llm quantization. Advances in Neural Information Processing Systems 37 (2025), 4170941732. [62] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem Babenko, and Dan Alistarh. 2024. Extreme compression of large language models via additive quantization. In Proceedings of the 41st International Conference on Machine Learning. 1228412303. [63] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks 107 (2018), 311. [64] Falcon-LLM Team. 2024. The Falcon 3 Family of Open Models. https://huggingface.co/blog/falcon3 [65] Ruibo Fan, Xiangrui Yu, Peijie Dong, Zeyu Li, Gu Gong, Qiang Wang, Wei Wang, and Xiaowen Chu. 2025. SpInfer: Leveraging Low-Level Sparsity for Efficient Large Language Model Inference on GPUs. In Proceedings of the Twentieth European Conference on Computer Systems. 243260. [66] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 139. [67] Fireworks AI, Inc. 2023. Fireworks AI. https://fireworks.ai/ [68] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning. PMLR, 1032310337. [69] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323 (2022). [70] Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. 2025. Marlin: Mixed-precision autoregressive parallel inference on large language models. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming. 239251. [71] FriendliAI Inc. 2023. Friendli Inference: The fastest LLM inference engine on the market. https://friendli.ai/solutions/ inference [72] Qichen Fu, Minsik Cho, Thomas Merth, Sachin Mehta, Mohammad Rastegari, and Mahyar Najibi. 2024. Lazyllm: Dynamic token pruning for efficient long context llm inference. arXiv preprint arXiv:2407.14057 (2024). [73] Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, and Pengfei Zuo. 2024. {Cost-Efficient} large language model serving for multi-turn conversations with {CachedAttention}. In 2024 USENIX Annual Technical Conference (USENIX ATC 24). 111126. [74] Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. 2024. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276 (2024). [75] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2023. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801 (2023). [76] Saibo Geng, Hudson Cooper, Michał Moskal, Samuel Jenkins, Julian Berman, Nathan Ranchin, Robert West, Eric Horvitz, and Harsha Nori. 2025. Generating Structured Outputs from Language Models: Benchmark and Studies. arXiv preprint arXiv:2501.10868 (2025). [77] Saibo Geng, Martin Josifoski, Maxime Peyrard, and Robert West. 2023. Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 1093210952. [78] Evangelos Georganas, Dhiraj Kalamkar, Alexander Kozlov, and Alexander Heinecke. 2025. ML-SpecQD: Multi-Level Speculative Decoding with Quantized Drafts. arXiv preprint arXiv:2503.13565 (2025). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:57 [79] Georgi Gerganov. 2022. GGML: Tensor library for machine learning. https://github.com/ggml-org/ggml/blob/master/ docs/gguf.md [80] Georgi Gerganov. 2023. GGUF. https://github.com/ggml-org/ggml/blob/master/docs/gguf.md [81] ggml-org. 2023. GBNF: Format for defining formal grammars to constrain model outputs in llama.cpp. https: //github.com/ggml-org/llama.cpp/blob/master/grammars/README.md [82] ggml.ai. 2023. llama.cpp. https://github.com/ggml-org/llama.cpp [83] In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin Zhong. 2024. Prompt cache: Modular attention reuse for low-latency inference. Proceedings of Machine Learning and Systems 6 (2024), 325338. [84] GitHub, Inc. 2021. GitHub Copilot: The AI editor for everyone. https://github.com/features/copilot [85] Google. 2009. Go: Build simple, secure, scalable systems with Go. https://go.dev/ [86] Google. 2023. Gemini. https://gemini.google.com/app [87] Google. 2024. Google AI Edge SDK. https://developer.android.com/ai/gemini-nano [88] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [89] Groq, Inc. 2024. GroqCloud: Easy Access to Fast AI Inference. https://groq.com/groqcloud/ [90] gRPC. 2016. gRPC: high performance, open source universal RPC framework. https://grpc.io/ [91] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023). [92] Yanchu Guan, Dong Wang, Zhixuan Chu, Shiyu Wang, Feiyue Ni, Ruihua Song, and Chenyi Zhuang. 2024. Intelligent agents with llm-based process automation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 50185027. [93] guidance-ai. 2024. Low-level Guidance (llguidance). https://github.com/guidance-ai/llguidance [94] guidance-ai. 2025. Derivative based regex matcher. https://github.com/guidance-ai/derivre [95] Ozgur Guldogan, Jackson Kunde, Kangwook Lee, and Ramtin Pedarsani. 2024. Multi-bin batching for increasing LLM inference throughput. arXiv preprint arXiv:2412.04504 (2024). [96] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948 (2025). [97] Jan Hansen-Palmus, Michael Truong Le, Oliver Hausdörfer, and Alok Verma. 2024. Communication Compression for Tensor Parallel LLM Inference. arXiv preprint arXiv:2411.09510 (2024). [98] Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu, Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma, Adithya Samavedhi, Qiyue Gao, et al. 2024. Llm reasoners: New evaluation, library, and analysis of step-by-step reasoning with large language models. arXiv preprint arXiv:2404.05221 (2024). [99] Yongjun He, Yao Lu, and Gustavo Alonso. 2024. Deferred continuous batching in resource-efficient large language model serving. In Proceedings of the 4th Workshop on Machine Learning and Systems. 98106. [100] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016). [101] Guseul Heo, Sangyeop Lee, Jaehong Cho, Hyunmin Choi, Sanghyeon Lee, Hyungkyu Ham, Gwangsun Kim, Divya Mahajan, and Jongse Park. 2024. Neupims: Npu-pim heterogeneous acceleration for batched llm inferencing. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 722737. [102] Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash Bakhtiari, Lev Kurilenko, et al. 2024. Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference. arXiv preprint arXiv:2401.08671 (2024). [103] Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Yuhan Dong, Yu Wang, et al. 2024. Flashdecoding++: Faster large language model inference with asynchronization, flat gemm optimization, and heuristics. Proceedings of Machine Learning and Systems 6 (2024), 148161. [104] Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael Mahoney, Sophia Shao, Kurt Keutzer, and Amir Gholami. 2024. Kvquant: Towards 10 million context length llm inference with kv cache quantization. Advances in Neural Information Processing Systems 37 (2024), 12701303. [105] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2022. Lora: Low-rank adaptation of large language models. ICLR 1, 2 (2022), 3. [106] Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter Beerel, Stephen Crago, and John Paul Walters. 2021. Pipeline parallelism for inference on heterogeneous edge computing. arXiv preprint arXiv:2110.14895 (2021). [107] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. arXiv preprint arXiv:2304.01933 (2023). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:58 Park et al. [108] Hugging Face. 2022. Safetensors: ML Safer For All. https://huggingface.co/docs/safetensors/index [109] Hugging Face. 2022. Spaces: The AI App Directory. https://huggingface.co/spaces [110] Hugging Face. 2023. Text Generation Inference. https://huggingface.co/docs/text-generation-inference/index [111] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. 2024. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186 (2024). [112] Intel technologies. 2023. Intel Data Center GPU Max Series. https://www.intel.com/content/www/us/en/products/ details/discrete-gpus/data-center-gpu/max-series.html [113] Chandra Irugalbandara. 2024. Meaning Typed Prompting: Technique for Efficient, Reliable Structured Output Generation. arXiv preprint arXiv:2410.18146 (2024). [114] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7B. CoRR abs/2310.06825 (2023). doi:10.48550/ARXIV.2310.06825 arXiv:2310.06825 [115] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024). [116] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. 2023. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings. In Proceedings of the 50th annual international symposium on computer architecture. 114. [117] Aditya Kamath, Ramya Prabhu, Jayashree Mohan, Simon Peter, Ramachandran Ramjee, and Ashish Panwar. 2024. Pod-attention: Unlocking full prefill-decode overlap for faster llm inference. arXiv preprint arXiv:2410.18038 (2024). [118] Roman Kaplan. 2024. Intel gaudi 3 ai accelerator: Architected for gen ai training and inference. In 2024 IEEE Hot Chips 36 Symposium (HCS). IEEE, 116. [119] Byeongho Kim, Sanghoon Cha, Sangsoo Park, Jieun Lee, Sukhan Lee, Shin-haeng Kang, Jinin So, Kyungsoo Kim, Jin Jung, Jong-Geon Lee, et al. 2024. The breakthrough memory solutions for improved performance on llm inference. IEEE Micro (2024). [120] Gun Il Kim, Sunga Hwang, and Beakcheol Jang. 2024. Efficient Compressing and Tuning Methods for Large Language Models: Systematic Literature Review. Comput. Surveys (2024). [121] Hanjoon Kim, Younggeun Choi, Junyoung Park, Byeongwook Bae, Hyunmin Jeong, Sang Min Lee, Jeseung Yeon, Minho Kim, Changjae Park, Boncheol Gu, et al. 2024. TCP: Tensor Contraction Processor for AI Workloads Industrial Product. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 890902. [122] Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. 2019. Nemo: toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577 (2019). [123] Taku Kudo and John Richardson. 2018. Sentencepiece: simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226 (2018). [124] Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, MingHsuan Yang, Phillip HS Torr, Salman Khan, and Fahad Shahbaz Khan. 2025. Llm post-training: deep dive into reasoning large language models. arXiv preprint arXiv:2502.21321 (2025). [125] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles. 611626. [126] LangChain. 2022. LangChain: The largest community building the future of LLM apps. https://www.langchain.com/ langchain [127] Lark-Parser. 2018. Lark - parsing toolkit for Python. https://github.com/lark-parser/lark [128] Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. 2021. MLIR: Scaling compiler infrastructure for domain specific computation. In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO). IEEE, 214. [129] Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim. 2024. {InfiniGen}: Efficient generative inference of large language models with dynamic {KV} cache management. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 155172. [130] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691 (2021). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:59 [131] Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning. PMLR, 1927419286. [132] Jinhao Li, Jiaming Xu, Shan Huang, Yonghua Chen, Wen Li, Jun Liu, Yaoxiu Lian, Jiayi Pan, Li Ding, Hao Zhou, et al. 2024. Large language model inference acceleration: comprehensive hardware perspective. arXiv preprint arXiv:2410.04466 (2024). [133] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang Chu, Yerui Sun, Li Du, and Yuchen Xie. 2023. Fptq: Fine-grained post-training quantization for large language models. arXiv preprint arXiv:2308.15987 (2023). [134] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. 2024. survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth 1, 1 (2024), 9. [135] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023). [136] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858 (2024). [137] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077 (2024). [138] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2025. EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test. arXiv preprint arXiv:2503.01840 (2025). [139] Zongbiao Li, Xiezhao Li, Yinghao Cui, Yijun Chen, Zhixuan Gu, Yuxuan Liu, Wenbo Zhu, Fei Jia, Ke Liu, Qifeng Li, et al. 2024. Automatically Planning Optimal Parallel Strategy for Large Language Models. arXiv preprint arXiv:2501.00254 (2024). [140] Sean Lie. 2024. Inside the cerebras wafer-scale cluster. IEEE Micro (2024). [141] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. 2024. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887 (2024). [142] Lightllm Team. 2023. LightLLM: Light and Fast Inference Service for LLM. https://github.com/ModelTC/lightllm [143] Lightning AI. 2023. Lightning Fabric. https://lightning.ai/docs/fabric/stable/ [144] Lightning AI. 2023. Lit-LLaMA. https://github.com/Lightning-AI/lit-llama [145] Lightning AI. 2023. LitGPT: Use, finetune, pretrain, and deploy LLMs Lightning fast. https://github.com/LightningAI/litgpt [146] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems 6 (2024), 87100. [147] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 (2024). [148] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [149] Akide Liu, Jing Liu, Zizheng Pan, Yefei He, Reza Haffari, and Bohan Zhuang. 2024. Minicache: Kv cache compression in depth dimension for large language models. Advances in Neural Information Processing Systems 37 (2024), 139997 140031. [150] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2629626306. [151] Michael Xieyang Liu, Frederick Liu, Alexander Fiannaca, Terry Koo, Lucas Dixon, Michael Terry, and Carrie Cai. 2024. \" We Need Structured Output\": Towards User-centered Constraints on Large Language Model Output. In Extended Abstracts of the CHI Conference on Human Factors in Computing Systems. 19. [152] Shu Liu, Asim Biswal, Audrey Cheng, Xiangxi Mo, Shiyi Cao, Joseph Gonzalez, Ion Stoica, and Matei Zaharia. 2024. Optimizing llm queries in relational workloads. arXiv preprint arXiv:2403.05821 (2024). [153] Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng Gao. 2020. Adversarial training for large neural language models. arXiv preprint arXiv:2004.08994 (2020). [154] Xiaoxuan Liu, Lanxiang Hu, Peter Bailis, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. 2024. Online speculative decoding. In Proceedings of the 41st International Conference on Machine Learning. 3113131146. [155] Xiaoran Liu, Hang Yan, Shuo Zhang, Chenxin An, Xipeng Qiu, and Dahua Lin. 2023. Scaling laws of rope-based extrapolation. arXiv preprint arXiv:2310.05209 (2023). [156] Yu Liu, Duantengchuan Li, Kaili Wang, Zhuoran Xiong, Fobo Shi, Jian Wang, Bing Li, and Bo Hang. 2024. Are LLMs good at structured outputs? benchmark for evaluating structured output capabilities in LLMs. Information Processing & Management 61, 5 (2024), 103809. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:60 Park et al. [157] Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, et al. 2024. Cachegen: Kv cache compression and streaming for fast large language model serving. In Proceedings of the ACM SIGCOMM 2024 Conference. 3856. [158] Yijiang Liu, Huanrui Yang, Youxin Chen, Rongyu Zhang, Miao Wang, Yuan Du, and Li Du. 2025. PAT: PruningAware Tuning for Large Language Models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 39. 2468624695. [159] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 (2023). [160] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. 2023. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning. PMLR, 2213722176. [161] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. 2024. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750 (2024). [162] LMDeploy Contributors. 2023. LMDeploy: Toolkit for Compressing, Deploying, and Serving LLM. https://github. com/InternLM/lmdeploy [163] Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2023. Full parameter fine-tuning for large language models with limited resources. arXiv preprint arXiv:2306.09782 (2023). [164] Ruilong Ma, Xiang Yang, Jingyu Wang, Qi Qi, Haifeng Sun, Jing Wang, Zirui Zhuang, and Jianxin Liao. 2024. HPipe: Large Language Model Pipeline Parallelism for Long Context on Heterogeneous Cost-effective Devices. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track). 19. [165] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Lifeng Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764 1 (2024). [166] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36 (2023), 2170221720. [167] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang, Shilei Wen, Fei Chao, and Rongrong Ji. 2024. Affinequant: Affine transformation quantization for large language models. arXiv preprint arXiv:2403.12544 (2024). [168] Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023. holistic approach to undesired content detection in the real world. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 1500915018. [169] Jeffrey Marron. 2024. Implementing the Health Insurance Portability and Accountability Act (HIPAA) Security Rule Cybersecurity Resource Guide. [170] Max Howell. 2016. Homebrew: The Missing Package Manager for macOS (or Linux). https://brew.sh/ [171] meta. 2025. The Llama 4 herd: The beginning of new era of natively multimodal AI innovation. https://ai.meta. com/blog/llama-4-multimodal-intelligence/ [172] Meta Platforms, Inc. 2022. xFormers: modular and hackable Transformer modelling library. https://github.com/ facebookresearch/xformers [173] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Hongyi Jin, Tianqi Chen, and Zhihao Jia. 2023. Towards efficient generative large language model serving: survey from algorithms to systems. arXiv preprint arXiv:2312.15234 (2023). [174] Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, et al. 2024. Specinfer: Accelerating large language model serving with tree-based speculative inference and verification. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3. 932949. [175] Xupeng Miao, Yujie Wang, Youhe Jiang, Chunan Shi, Xiaonan Nie, Hailin Zhang, and Bin Cui. 2022. Galvatron: Efficient transformer training over multiple gpus using automatic parallelism. arXiv preprint arXiv:2211.13878 (2022). [176] Microsoft. 2022. DeepSpeed Model Implementations for Inference (MII). https://github.com/deepspeedai/DeepSpeedMII [177] MLC-AI. 2023. MLC LLM: Universal LLM Deployment Engine With ML Compilation. https://llm.mlc.ai/ [178] Modular Inc. 2023. Mojo: Powerful CPU+GPU Programming. https://www.modular.com/mojo [179] Modular Inc. 2024. MAX. https://www.modular.com/max [180] Raha Moraffah, Shubh Khandelwal, Amrita Bhattacharjee, and Huan Liu. 2024. Adversarial text purification: large language model approach for defense. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 6577. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:61 [181] Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael Jordan, et al. 2018. Ray: distributed framework for emerging {AI} applications. In 13th USENIX symposium on operating systems design and implementation (OSDI 18). 561577. [182] NetEase-FuXi. 2024. EETQ: Easy & Efficient Quantization for Transformers. https://github.com/NetEase-FuXi/EETQ [183] Liang-bo Ning, Shijie Wang, Wenqi Fan, Qing Li, Xin Xu, Hao Chen, and Feiran Huang. 2024. Cheatagent: Attacking llm-empowered recommender systems via llm agent. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 22842295. [184] Noam Gat. 2024. lm-format-enforce: Enforce the output format (JSON Schema, Regex etc) of language model. https://github.com/noamgat/lm-format-enforcer [185] NVIDIA. 2007. Parallel Thread Execution ISA. https://docs.nvidia.com/cuda/parallel-thread-execution/ [186] NVIDIA. 2016. NVIDIA Collective Communications Library (NCCL). https://developer.nvidia.com/nccl [187] NVIDIA. 2018. NVIDIA Triton Inference Server. https://developer.nvidia.com/triton-inference-server [188] NVIDIA. 2019. FasterTransformer. https://github.com/NVIDIA/FasterTransformer [189] NVIDIA. 2019. NVIDIA TensorRT. https://github.com/NVIDIA/TensorRT [190] NVIDIA. 2022. cuSPARSE: GPU library APIs for sparse computation. https://developer.nvidia.com/cusparse [191] NVIDIA. 2023. NVIDIA TensorRT-LLM: TensorRT Toolbox for Optimized Large Language Model Inference. https://github.com/NVIDIA/TensorRT-LLM [192] NVIDIA. 2025. NVIDIA Dynamo. https://github.com/ai-dynamo/dynamo [193] Hyungjun Oh, Kihong Kim, Jaemin Kim, Sungkyun Kim, Junyeol Lee, Du-seong Chang, and Jiwon Seo. 2024. Exegpt: Constraint-aware resource scheduling for llm inference. In Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2. 369384. [194] Ollama. 2023. Ollama. https://ollama.com/ [195] Open WebUI. 2024. Open WebUI. https://openwebui.com/ [196] OpenAI. 2022. ChatGPT. https://chatgpt.com/ March 12, 2025. [197] OpenAI. 2024. Structured Outputs. https://platform.openai.com/docs/guides/structured-outputs [198] Cristobal Ortega, Yann Falevoz, and Renaud Ayrignac. 2024. PIM-AI: Novel Architecture for High-Efficiency LLM Inference. arXiv preprint arXiv:2411.17309 (2024). [199] Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, and Ravi Netravali. 2024. Marconi: Prefix caching for the era of hybrid llms. arXiv preprint arXiv:2411.19379 (2024). [200] Seungcheol Park, Jaehyeon Choi, Sojin Lee, and Kang. 2024. comprehensive survey of compression algorithms for language models. arXiv preprint arXiv:2401.15347 (2024). [201] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [202] Pratyush Patel, Esha Choukse, Chaojie Zhang, Aashaka Shah, Íñigo Goiri, Saeed Maleki, and Ricardo Bianchini. 2024. Splitwise: Efficient generative llm inference using phase splitting. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 118132. [203] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. 2023. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048 (2023). [204] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023), 606624. [205] Rohan Baskar Prabhakar, Hengrui Zhang, and David Wentzlaff. 2024. Kraken: Inherently Parallel Transformers For Efficient Multi-Device Inference. Advances in Neural Information Processing Systems 37 (2024), 79577980. [206] Ramya Prabhu, Ajay Nayak, Jayashree Mohan, Ramachandran Ramjee, and Ashish Panwar. 2025. vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention. In Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1. 11331150. [207] Ofir Press, Noah Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409 (2021). [208] PyTorch. 2020. PyTorch/XLA. https://github.com/pytorch/xla [209] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 167. [210] Nived Rajaraman, Jiantao Jiao, and Kannan Ramchandran. 2024. Toward theory of tokenization in llms. arXiv preprint arXiv:2404.08335 (2024). ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:62 Park et al. [211] Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, and Yuxiong He. 2022. Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale. In International conference on machine learning. PMLR, 1833218346. [212] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 116. [213] Bita Darvish Rouhani, Ritchie Zhao, Ankit More, Mathew Hall, Alireza Khodamoradi, Summer Deng, Dhruv Choudhary, Marius Cornea, Eric Dellinger, Kristof Denolf, et al. 2023. Microscaling data formats for deep learning. arXiv preprint arXiv:2310.10537 (2023). [214] Charlie Ruan, Yucheng Qin, Xun Zhou, Ruihang Lai, Hongyi Jin, Yixin Dong, Bohan Hou, Meng-Shiun Yu, Yiyan Zhai, Sudeep Agarwal, et al. 2024. WebLLM: High-Performance In-Browser LLM Inference Engine. arXiv preprint arXiv:2412.15803 (2024). [215] Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. 2024. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. Advances in Neural Information Processing Systems 37 (2024), 6865868685. [216] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1129611300. [217] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-Attention with Relative Position Representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 464468. [218] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019). [219] Ying Sheng, Shiyi Cao, Dacheng Li, Coleman Hooper, Nicholas Lee, Shuo Yang, Christopher Chou, Banghua Zhu, Lianmin Zheng, Kurt Keutzer, et al. 2023. S-LoRA: Serving Thousands of Concurrent LoRA Adapters. CoRR (2023). [220] Ying Sheng, Shiyi Cao, Dacheng Li, Banghua Zhu, Zhuohan Li, Danyang Zhuo, Joseph Gonzalez, and Ion Stoica. 2024. Fairness in serving large language models. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 965988. [221] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with single gpu. In International Conference on Machine Learning. PMLR, 3109431116. [222] Shengyu Liu. 2024. SwiftTransformer. https://github.com/LLMServe/SwiftTransformer [223] Jiho Shin, Hoeseok Yang, and Youngmin Yi. 2024. SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference. arXiv preprint arXiv:2411.12692 (2024). [224] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. arXiv preprint Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv:1909.08053 (2019). [225] Alan Smith, Gabriel Loh, John Wuu, Samuel Naffziger, Tyrone Huang, Hugh McIntyre, Ramon Mangaser, Wonjun Jung, and Raja Swaminathan. 2024. AMD Instinct MI300X Accelerator: Packaging and Architecture Co-Optimization. In 2024 IEEE Symposium on VLSI Technology and Circuits (VLSI Technology and Circuits). IEEE, 12. [226] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 (2024). [227] Yixin Song, Zeyu Mi, Haotong Xie, and Haibo Chen. 2024. Powerinfer: Fast large language model serving with consumer-grade gpu. In Proceedings of the ACM SIGOPS 30th Symposium on Operating Systems Principles. 590606. [228] Benjamin Spector and Chris Re. 2023. Accelerating llm inference with staged speculative decoding. arXiv preprint arXiv:2308.04623 (2023). [229] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615 (2022). [230] Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, and Josep Torrellas. 2024. Towards greener llms: Bringing energy-efficiency to the forefront of llm inference. arXiv preprint arXiv:2403.20306 (2024). [231] Foteini Strati, Sara McAllister, Amar Phanishayee, Jakub Tarnawski, and Ana Klimovic. 2024. DéjàVu: KV-cache streaming for fast, fault-tolerant generative LLM serving. In Proceedings of the 41st International Conference on Machine Learning. 4674546771. [232] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:63 [233] Mingjie Sun, Zhuang Liu, Anna Bair, and Zico Kolter. 2023. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695 (2023). [234] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. Retentive network: successor to transformer for large language models. arXiv preprint arXiv:2307.08621 (2023). [235] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024). [236] Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face (2024). [237] The ZeroMQ authors. 2023. ZeroMQ: An open-source universal messaging library. https://zeromq.org/ [238] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. 2019. Triton: an intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages. 1019. [239] Together AI. 2023. Together Inference. https://www.together.ai/products#inference [240] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [241] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023). [242] Turboderp. 2023. ExLlamaV2: Inference library for running local LLMs on modern consumer GPUs. https://github. com/turboderp-org/exllamav2 [243] James Turnbull. 2018. Monitoring with Prometheus. Turnbull Press. [244] unsloth. 2023. unsloth. https://unsloth.ai/ [245] Tim Valicenti, Justice Vidal, and Ritik Patnaik. 2023. Mini-gpts: Efficient large language models through contextual pruning. arXiv preprint arXiv:2312.12682 (2023). [246] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [247] vLLM Team. 2024. LLM Compressor: An easy-to-use library for optimizing models for deployment with vllm. https://github.com/vllm-project/llm-compressor [248] David Vos, Till Döhmen, and Sebastian Schelter. 2022. Towards parameter-efficient automation of data wrangling tasks with prefix-tuning. In NeurIPS 2022 First Table Representation Workshop. 19. [249] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. 2024. Efficient Large Language Models: Survey. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=bsCCJHbO8A Survey Certification. [250] Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif Saurous, and Yoon Kim. 2023. Grammar prompting for domain-specific language generation with large language models. Advances in Neural Information Processing Systems 36 (2023), 6503065055. [251] Jinheng Wang, Hansong Zhou, Ting Song, Shaoguang Mao, Shuming Ma, Hongyu Wang, Yan Xia, and Furu Wei. 2024. 1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1. 58 Inference on CPUs. arXiv preprint arXiv:2410.16144 (2024). [252] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). [253] Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao, Hyung Won Chung, Iz Beltagy, Julien Launay, and Colin Raffel. 2022. What language model architecture and pretraining objective works best for zero-shot generalization?. In International Conference on Machine Learning. PMLR, 2296422984. [254] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. 2024. Model compression and efficient inference for large language models: survey. arXiv preprint arXiv:2402.09748 (2024). [255] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. 2024. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. [256] Zheng Wang, Boxiao Jin, Zhongzhi Yu, and Minjia Zhang. 2024. Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks. arXiv preprint arXiv:2407.08454 (2024). [257] Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, and Mao Yang. 2025. T-mac: Cpu renaissance via table lookup for low-bit llm deployment on edge. In Proceedings of the Twentieth European Conference on Computer Systems. 278292. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. 111:64 Park et al. [258] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022). [259] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [260] Brandon Willard and Rémi Louf. 2023. Efficient guided generation for large language models. arXiv preprint arXiv:2307.09702 (2023). [261] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100 (2022). [262] Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. 2024. Unlocking Efficiency in Large Language Model Inference: Comprehensive Survey of Speculative Decoding. In Findings of the Association for Computational Linguistics ACL 2024. 76557671. [263] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. 2023. Flash-llm: Enabling cost-effective and highly-efficient large generative model inference with unstructured sparsity. arXiv preprint arXiv:2309.10285 (2023). [264] Lizhi Xiang, Omid Asudeh, Gerald Sabin, Aravind Sukumaran-Rajam, and Sadayappan. 2025. cuTeSpMM: Accelerating Sparse-Dense Matrix Multiplication using GPU Tensor Cores. arXiv preprint arXiv:2504.06443 (2025). [265] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 3808738099. [266] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453 (2023). [267] Mengwei Xu, Dongqi Cai, Wangsong Yin, Shangguang Wang, Xin Jin, and Xuanzhe Liu. 2025. Resource-efficient algorithms and systems of foundation models: survey. Comput. Surveys 57, 5 (2025), 139. [268] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. 2024. survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024). [269] ZHAO XUANLEI, Bin Jia, Haotian Zhou, Ziming Liu, Shenggan Cheng, and Yang You. 2024. Hetegen: Efficient heterogeneous parallel inference for large language models on resource-constrained devices. Proceedings of Machine Learning and Systems 6 (2024), 162172. [270] Zhenliang Xue, Yixin Song, Zeyu Mi, Xinrui Zheng, Yubin Xia, and Haibo Chen. 2024. Powerinfer-2: Fast large language model inference on smartphone. arXiv preprint arXiv:2406.06282 (2024). [271] Chuanpeng Yang, Yao Zhu, Wang Lu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024. Survey on knowledge distillation for large language models: methods, evaluation, and application. ACM Transactions on Intelligent Systems and Technology (2024). [272] Shuo Yang, Ying Sheng, Joseph Gonzalez, Ion Stoica, and Lianmin Zheng. 2024. Post-training sparse attention with double sparsity. arXiv preprint arXiv:2408.07092 (2024). [273] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024. survey on large language model (llm) security and privacy: The good, the bad, and the ugly. High-Confidence Computing (2024), 100211. [274] Lu Ye, Ze Tao, Yong Huang, and Yang Li. 2024. Chunkattention: Efficient self-attention with prefix-aware kv cache and two-phase partition. arXiv preprint arXiv:2402.15220 (2024). [275] Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. 2023. Prompt engineering prompt engineer. arXiv preprint arXiv:2311.05661 (2023). [276] Zihao Ye, Lequn Chen, Ruihang Lai, Wuwei Lin, Yineng Zhang, Stephanie Wang, Tianqi Chen, Baris Kasikci, Vinod Grover, Arvind Krishnamurthy, et al. 2025. Flashinfer: Efficient and customizable attention engine for llm inference serving. arXiv preprint arXiv:2501.01005 (2025). [277] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. Llm as system service on mobile devices. arXiv preprint arXiv:2403.11805 (2024). [278] Yiorgis Gozadinos. 2023. oterm: the text-based terminal client for Ollama. https://github.com/ggozad/oterm [279] Chengye Yu, Tianyu Wang, Zili Shao, Linjie Zhu, Xu Zhou, and Song Jiang. 2024. Twinpilots: new computing paradigm for gpu-cpu parallel llm inference. In Proceedings of the 17th ACM International Systems and Storage Conference. 91103. [280] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon Chun. 2022. Orca: distributed serving system for {Transformer-Based} generative models. In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22). 521538. ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018. Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency 111:65 [281] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. 2024. Llm inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363 (2024). [282] Biao Zhang, Ivan Titov, and Rico Sennrich. 2019. Improving deep transformer with depth-scaled initialization and merged attention. arXiv preprint arXiv:1908.11365 (2019). [283] Mingjin Zhang, Xiaoming Shen, Jiannong Cao, Zeyang Cui, and Shan Jiang. 2024. Edgeshard: Efficient llm inference via collaborative edge computing. IEEE Internet of Things Journal (2024). [284] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. 2023. Instruction tuning for large language models: survey. arXiv preprint arXiv:2308.10792 (2023). [285] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022). [286] Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn. 2023. Machine translation with large language models: Prompting, few-shot learning, and fine-tuning with QLoRA. In Proceedings of the Eighth Conference on Machine Translation. 468481. [287] Yuxin Zhang, Mingbao Lin, Zhihang Lin, Yiting Luo, Ke Li, Fei Chao, Yongjian Wu, and Rongrong Ji. 2022. Learning best combination for efficient n: sparsity. Advances in Neural Information Processing Systems 35 (2022), 941953. [288] Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, and Rongrong Ji. 2023. Dynamic sparse no training: Training-free fine-tuning for sparse llms. arXiv preprint arXiv:2310.08915 (2023). [289] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2023. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems 36 (2023), 3466134710. [290] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. Llm-pq: Serving llm on heterogeneous clusters with phase-aware partition and adaptive quantization. arXiv preprint arXiv:2403.01136 (2024). [291] Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, and Xue Lin. 2024. Pruning Foundation Models for High Accuracy without Retraining. In Findings of the Association for Computational Linguistics: EMNLP 2024. 96819694. [292] Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. 2023. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277 (2023). [293] Youpeng Zhao, Di Wu, and Jun Wang. 2024. Alisa: Accelerating large language model inference via sparsity-aware kv caching. In 2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA). IEEE, 10051017. [294] Haizhong Zheng, Xiaoyan Bai, Xueshen Liu, Zhuoqing Morley Mao, Beidi Chen, Fan Lai, and Atul Prakash. 2024. Learn to be efficient: Build structured sparsity in large language models. Advances in Neural Information Processing Systems 37 (2024), 101969101991. [295] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. 2024. Sglang: Efficient execution of structured language model programs. Advances in Neural Information Processing Systems 37 (2024), 6255762583. [296] Yue Zheng, Yuhao Chen, Bin Qian, Xiufang Shi, Yuanchao Shu, and Jiming Chen. 2024. review on edge large language models: Design, execution, and applications. Comput. Surveys (2024). [297] Yinmin Zhong, Shengyu Liu, Junda Chen, Jianbo Hu, Yibo Zhu, Xuanzhe Liu, Xin Jin, and Hao Zhang. 2024. {DistServe}: Disaggregating prefill and decoding for goodput-optimized large language model serving. In 18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24). 193210. [298] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. 2024. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294 (2024). [299] Hanlin Zhu, Banghua Zhu, and Jiantao Jiao. 2024. Efficient prompt caching via embedding similarity. arXiv preprint arXiv:2402.01173 (2024). [300] Kan Zhu, Yilong Zhao, Liangyu Zhao, Gefei Zuo, Yile Gu, Dedong Xie, Yufei Gao, Qinyu Xu, Tian Tang, Zihao Ye, et al. 2024. Nanoflow: Towards optimal large language model serving throughput. arXiv preprint arXiv:2408.12757 (2024). [301] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2024. survey on model compression for large language models. Transactions of the Association for Computational Linguistics 12 (2024), 15561577. [302] Vilém Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du, Tim Vieira, Mrinmaya Sachan, and Ryan Cotterell. 2023. formal perspective on byte-pair encoding. arXiv preprint arXiv:2306.16837 (2023). Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009 ACM Comput. Surv., Vol. 37, No. 4, Article 111. Publication date: August 2018."
        }
    ],
    "affiliations": [
        "Electronics and Telecommunications Research Institute, South Korea",
        "Korea Electronics Technology Institute, South Korea"
    ]
}