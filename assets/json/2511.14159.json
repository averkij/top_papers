{
    "paper_title": "MVI-Bench: A Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs",
    "authors": [
        "Huiyi Chen",
        "Jiawei Peng",
        "Dehai Min",
        "Changchang Sun",
        "Kaijie Chen",
        "Yan Yan",
        "Xu Yang",
        "Lu Cheng"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, a novel metric that characterizes LVLM robustness at a granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/MVI-Bench."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 9 5 1 4 1 . 1 1 5 2 : r MVI-Bench: Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs Huiyi Chen1 Jiawei Peng1 Dehai Min3 Changchang Sun3 Kaijie Chen2 Yan Yan3 Xu Yang1 Lu Cheng3 1 School of Computer Science & Engineering, Southeast University 2 Guohao School, Tongji University 3 Department of Computer Science, University of Illinois at Chicago huiyichen@seu.edu.cn, pengjiawei@seu.edu.cn, dmin10@uic.edu, sun47@uic.edu, 2252538@tongji.edu.cn, yyan55@uic.edu, xuyang palm@seu.edu.cn, lucheng@uic.edu"
        },
        {
            "title": "Abstract",
            "content": "Evaluating the robustness of Large Vision-Language Models (LVLMs) is essential for their continued development and responsible deployment in real-world applications. However, existing robustness benchmarks typically focus on hallucination or misleading textual inputs, while largely overlooking the equally critical challenge posed by misleading visual inputs in assessing visual understanding. To fill this important gap, we introduce MVI-Bench, the first comprehensive benchmark specially designed for evaluating how Misleading Visual Inputs undermine the robustness of LVLMs. Grounded in fundamental visual primitives, the design of MVI-Bench centers on three hierarchical levels of misleading visual inputs: Visual Concept, Visual Attribute, and Visual Relationship. Using this taxonomy, we curate six representative categories and compile 1,248 expertly annotated VQA instances. To facilitate fine-grained robustness evaluation, we further introduce MVI-Sensitivity, novel metric that characterizes LVLM robustness at granular level. Empirical results across 18 state-of-the-art LVLMs uncover pronounced vulnerabilities to misleading visual inputs, and our in-depth analyses on MVI-Bench provide actionable insights that can guide the development of more reliable and robust LVLMs. The benchmark and codebase can be accessed at https://github.com/chenyil6/ MVI-Bench. 1. Introduction Recent advances in Large Vision-Language Models (LVLMs) [60] have driven remarkable progress across wide range of multimodal tasks, including image understanding [12, 53], visual question answering [5, 28], and Figure 1. (a) Misleading Textual Input: misleading questions are created by injecting inaccurate or irrelevant information into otherwise normal queries. (b) Misleading Visual Input: misleading visual cues arise from real-world scenes, causing models to misinterpret the image content (e.g., stools mistaken for mushrooms). complex visual reasoning [56, 63]. With these rapid developments comes an urgent need for rigorous evaluation of LVLM robustness, motivating the creation of numerous robustness benchmarks [3, 19, 25]. While existing robustness benchmarks have provided valuable insights, they largely focus on specific challenges, such as hallucination [9, 54] or adversarial robustness [3, 25], and thus offer only partial view of LVLM robustness. To broaden this scope, recent efforts [15, 17, 35, 40] have begun exploring LVLM behavior under misleading scenarios, typically by introducing misleading textual inputs. For instance, MAD-Bench [40] constructs misleading questions by injecting inaccurate information into textual prompts (Fig. 1(a)). However, misleading information is not confined to the language modality. In real-world settings, misleading visual inputs naturally arise and can deceive both machines and humans. This phenomenon has long been examined in philosophy under the notion of the Problem of Perception [44], which emphasizes that human visual cognition is prone to systematic errors induced by various factors such as perceptual biases. As illustrated in Fig. 1(b), humans may initially misinterpret visually ambiguous scenes, such as perceiving stools as mushrooms, but can often correct these misconceptions through contextual reasoning and prior world knowledge. This observation reveals fundamental challenge for LVLMs: unlike humans, models often rely on superficial visual patterns and struggle to disambiguate misleading cues. Yet, this critical dimension of LVLM robustness has received limited attention in existing evaluations. Moreover, prior works on misleading textual inputs often introduce deliberately misleading information into the question. However, such intentional deception rarely occurs in real-world user interactions and differs fundamentally from the visual domain, where misleading cues arise naturally rather than through explicit manipulation. Consequently, these benchmarks exploring misleading scenarios provide only narrow approximation of the robustness challenges that LVLMs face in realistic multimodal environments. To bridge these gaps, we introduce the Misleading Visual Input Benchmark (MVI-Bench), the first comprehensive benchmark specifically designed to evaluate the robustness of LVLMs against misleading visual inputs. Grounded in fundamental visual primitives [22], the design of MVI-Bench is organized around three hierarchical levels of misleading cues: Visual Concept, Visual Attribute, and Visual Relationship. Each level further comprises several representative misleading categories that closely mirror the types of perceptual errors humans encounter in real-world visual experiences, as detailed in Section 3.1. MVI-Bench includes total of 624 pairs of VQA instances (i.e., 1,248 instances in total), where each pair consists of normal image and its corresponding misleading counterpart. The two images within pair share nearly identical semantic content, differing only by the introduction of subtle misleading visual cues (see Fig. 2 for details). To complement this paired design, we further propose MVI-Sensitivity, novel evaluation metric that quantifies the relative performance degradation from normal to misleading visual inputs. This metric provides fine-grained measure of LVLM robustness, enabling deeper understanding of model sensitivity to misleading visual inputs. In summary, our main contributions are as follows: We establish the first comprehensive taxonomy of misleading visual inputs. Grounded in fundamental visual this taxonomy systematically characterizes primitives, Visual Concept, Visual Attribute, and Visual Relationship as the core dimensions for evaluating LVLM behavior under misleading visual conditions. We introduce MVI-Bench, carefully curated benchmark comprising six representative categories and 624 expertly annotated VQA instance pairs. Each pair is designed to isolate the effects of misleading visual cues while preserving the underlying semantic content. We propose MVI-Sensitivity, novel evaluation metric that enables fine-grained robustness assessment by quantitatively measuring the performance degradation of LVLMs when exposed to misleading visual inputs. Through extensive experiments and in-depth analyses on 18 openand closed-source LVLMs, we uncover three key observations: limited LVLMs robustness, visual perception as the primary bottleneck and the presence of spurious correlation. These findings offer actionable insights for developing more reliable and robust LVLMs. 2. Related Work Large Vision-Language Models (LVLMs). LVLMs unify visual perception and visual reasoning through an architecture comprising vision encoder, large language model (LLM), and connector module that aligns visual and textual feature spaces, enabling seamless multimodal understanding [1, 6, 7, 13, 14, 18, 34, 49, 68]. Leveraging this architecture, LVLMs have rapidly evolved from early multimodal frameworks such as Flamingo [6] and InstructBLIP [14] to new generation of high-performing models, such as LLaVA [34] and Qwen-VL [7]. When deployed in real-world settings, LVLMs exhibit remarkable includadvancements across multiple multimodal tasks, ing image understanding [12, 53], visual question answering [5, 28], and complex visual reasoning [56, 63]. This progress is reflected across both open-source and proprietary LVLM families. For example, open-source series such as InternVL [13] and SAIL-VL [61] have made substantial progress, while proprietary models like GPT-4 [2] and Gemini [48] further push the frontier through large-scale multimodal training and enhanced reasoning capabilities. Robustness Benchmarks for LVLMs. Evaluating the robustness of LVLMs is essential for their continued development and responsible deployment in real-world applications. Existing studies on LVLM robustness have primarily focused on two perspectives. On the one hand, prior works examine hallucination issues, which evaluates the discrepancy between generated responses and the visual content [9, 24, 54]. On the other hand, separate line of research investigates adversarial vulnerabilities, analyzing how models behave under carefully crafted perturbated inputs [3, 25, 66]. However, these evaluations illuminate only partial view of LVLM robustness. To broaden this scope, recent researches have begun to investigate LVLM behavior under misleading scenarios, most commonly through misleading textual inputs [15, 17, 35, 40]. However, robustFigure 2. Examples from six misleading categories defined in MVI-Bench. Each pair contains normal image (left) and misleading image (right) with the same MCQ and corresponding ground-truth answer. For the misleading image, an additional distractor option is shown alongside the correct answer. Answer choices are omitted for brevity (see Fig. 4 for full format). ness against misleading visual inputs, challenge that naturally arises in real-world environments, remains substantially underexplored. Although few recent efforts, such as IllusionVQA [43] and O-Bench [36], have begun to investigate this direction, these benchmarks are typically limited in scope, where only single type of misleading phenomenon is concerned and rely primarily on coarse metrics such as accuracy. Consequently, they fall short of providing comprehensive and fine-grained analysis of LVLM robustness under visually misleading conditions. 3. MVI-Bench To comprehensively evaluate LVLM robustness under misleading visual inputs, we propose MVI-Bench, paired VQA benchmark. Specifically, each instance pair shares the same multiple-choice question (MCQ) with four options (A-D) but differs in visual input: normal image and its misleading counterpart  (Fig. 2)  . The two images are visually similar in overall composition, but the misleading one introduces subtle cues designed to confuse the model. This paired design enables controlled analysis of how visual misleading affects LVLMs behavior: for each image, only one option is correct, and at least one distractor is intentionally crafted to appear plausible given the misleading visual cues. Overall, MVI-Bench comprises 624 such pairs (1,248 instances total) across six categories, enabling systematic evaluation under visually misleading conditions. Tab. 1 presents detailed comparison of MVI-Bench with existing related benchmarks. Table 1. Comparison between MVI-Bench and prior visually misleading benchmarks. MVI-Bench uniquely covers six well-defined misleading categories and includes diverse image sources with paired design, enabling comprehensive and controlled robustness evaluation. (Abbreviations: Res.-Visual resemblance; Rep.-Representation confusion; Mat.-Material confusion; Mir.-Mirror reflection; Occ.- Occlusion confusion; Ill.-Visual illusion.) Dataset Venue #Image Res. Rep. Mat. Mir. Occ. Ill. IllusionVQA [43] HallusionBench [20] IllusionBench+ [65] O-Bench [36] MVI-Bench (ours) COLM 2024 CVPR 2024 arXiv 2025.6 arXiv 2025.8 374 346 1,051 1,365 1,248 Natural Image Source Synthetic Edited Paired 3.1. Taxonomy of Visually Misleading Types We ground our taxonomy in the principles of fundamental visual primitives [22], namely, Visual Concept, Visual Attribute, and Visual Relationship1. These primitives form the core of visual cognition and offer structured lens through which misleading phenomena can be characterized. Accordingly, we define three corresponding levels of misleading visual inputs and their associated categories. 1. Visual Concept Misleading. Misleading effects at this level arise from coarse-grained visual similarity between semantically different entities (e.g., objects, people, scenes). Addressing such misleading inputs requires finer visual discrimination and modest reasoning that goes beyond superficial pattern matching. This level includes two categories: Visual Resemblance. Cases where models may confuse an object with visually similar but semantically different object or an object from different category. Representation Confusion. Cases where models can fail to distinguish between an actual object and its twodimensional representation (e.g., photograph or painting). 2. Visual Attribute Misleading. This level captures misleading cases from fine-grained ambiguity in visual attributes (e.g., texture, gloss, material). Correctly resolving these cases requires refined perceptual discrimination to differentiate materials that may look similar at coarse level but differ fundamentally. This level includes one category: Material confusion. Cases where models sometimes confuse items based on their texture, material, or other physical attributes. 3. Visual Relationship Misleading. This level encompasses misleading cases that arise from incorrect interpretation of visual relationships, which describe spatial arrangements, partwhole structures, or contextual dependencies among entities. Unlike conceptor attribute-level misleading, relationship-level misleading requires the model to reason about higher-order spatial arrangements and interactions between objects. This level includes three categories: 1We exclude visual combination, since our aim is to isolate and analyze the effect of each individual misleading level. Mirror Reflection. Cases where models can be misled by virtual images from mirror reflections, confusing them with actual, real-world objects. Occlusion Confusion. Cases where models struggle to recognize an object when part of it is visually occluded, sometimes leading to incorrect identification or counting. Visual Illusion. Cases where models are susceptible to visual illusions arising from complex spatial arrangements, lighting conditions, or deceptive perspectives. Above all, these three levels and six categories form comprehensive taxonomy for evaluating LVLMs susceptibility to misleading visual inputs. This taxonomy spans the full spectrum of visual understanding from low-level recognition and mid-level attribute discrimination to high-level spatial and contextual reasoning. Representative examples from each category are illustrated in Fig. 2. 3.2. Data Curation Data Collection. Following the taxonomy of visually misleading types described above, three trained human experts with at least bachelors degree manually curated an initial set of image pairs from three complementary sources to provide broad topical coverage: (1) Natural images, collected from multiple international social media platforms to ensure broad and diverse realworld content coverage; (2) Synthetic images, generated using the powerful generative model Seedream [42] to supplement misleading cases that are rare and low-quality in natural data; (3) Edited images, created by human experts using advanced image editing tools to modify natural or synthetic images, typically by removing misleading visual cues to serve as normal counterparts."
        },
        {
            "title": "Normal",
            "content": "images in our benchmark primarily originate from natural (23.56%) and edited images (76.44%), while misleading images are mainly sourced from natural (81.09%) and synthetic ones (18.91%). For each image, human experts craft MCQ based on its visual content, with exactly one correct answer and at least one distractor that plausibly leverages the misleading visual cues. AdditionFigure 3. Overview of MVI-Bench statistics. (a) Six balanced misleading visual categories. (b) Three diverse image sources: natural, synthetic, and edited. (d) High pairwise similarity ensures semantic consistency between normal and misleading image pairs. (c) Broad object coverage across multiple domains. ally, each annotated VQA pair including its label (misleading or normal) and the answer is independently reviewed by additional human experts, and any controversial cases are discarded to ensure quality. Data Filtering and Refinement. To further ensure the discriminative value of MVI-Bench, we evaluate the initially collected data using two strong LVLMs: InternVL3-2B [13] and Qwen2.5-VL-3B [8]. Based on the evaluation results, we then identify instances where both models correctly answer the questions for both normal and misleading images. Since these instances pose little challenge and provide limited ability to differentiate model robustness, we treat them as low-difficulty samples. To maintain dataset diversity while preventing such easy cases from dominating the benchmark, we randomly retain only portion of them. After this refinement process, the dataset is reduced from 1,578 to 1,248 high-quality, discriminative VQA instances. Data Statistics. Our final benchmark comprises 1,248 VQA instances. Fig. 3 illustrates the overall statistics of MVI-Bench from four complementary perspectives. Fig. 3 (a) shows the data distribution across our taxonomy, with roughly balanced instances per category for unbiased evaluation. Fig. 3 (b) depicts the composition of our three image sources: natural images (52.32%), synthetic images (9.62%), and edited images (38.06%). Fig. 3 (c) shows that the benchmark exhibits broad semantic coverage, encompassing wide range of categories such as food, furniture, nature, and decorations. Moreover, each normal and misleading image pair is meticulously designed to isolate the effect of visual misleading cues while preserving other semantic content, ensuring minimal differences between paired images. Fig. 3 (d) shows empirical evidence for this design by showing high cosine similarity (using features obtained using CLIP-Large [41]) between paired images across all six categories. 3.3. Evaluation Metrics We employ two evaluation metrics. First, we report Accuracy on normal and misleading examples to directly measure model performance under subtly different visual conditions. Second, we introduce MVI-Sensitivity, new metric designed to quantify the extent to which an LVLMs performance degrades when exposed to misleading visual inputs, compared to corresponding normal images: MVI-Sensitivity = Accn Accm Accn , (1) where Accn and Accm denote the accuracy on normal and misleading samples, respectively. lower MVI-sensitivity indicates that the model is less affected by misleading cues and exhibits stronger robustness. 4. Experiments We employ MVI-Bench to comprehensively study LVLMs behavior under misleading visual inputs. Our experimental design centers on the following research questions: RQ1: How do existing LVLMs perform on MVI-Bench across different categories of misleading visual inputs? RQ2: How do visual perception and reasoning abilities contribute respectively to LVLMs robustness in visually misleading scenarios? RQ3: What insights can be derived from counterintuitive cases where LVLMs behave unexpectedly under misleading visual inputs? 4.1. Models and Experimental Settings We evaluate 18 state-of-the-art LVLMs on MVI-Bench to systematically assess their robustness against misleading visual inputs. These models span diverse range of architectures and scales. Specifically, it includes six closedsource models: GPT-4o [23], GPT-4.1 [2], Claude-3.7Sonnet [4], Gemini-2.5-Flash [48], Gemini-2.5-Pro [48] and GPT-5-Chat [38]; and twelve open-source models, including Qwen2.5-VL series [8], the InternVL-3 series [69], the SAIL-VL2 series [61], LLaVA-OneVision [31] and Molmo [16]. For closed-source models, the temperature is fixed at 0.0, and for open-source models, the decoding is performed using greedy strategy. Meanwhile, modern LLMs are known to be vulnerable to option-position variations in MCQs due to their inherent selection bias [67], namely tendency to prefer specific option IDs as answers. To mitigate this bias, we randomly shuffle the answer options each time the LVLMs are queried. 4.2. Main Results (RQ1) We report the evaluation results of 18 LVLMs in Tab. 2 across 6 misleading categories in MVI-Bench. The main findings are summarized as follows. Existing LVLMs remain highly susceptible to misleading visual inputs, especially open-source ones. As shown in Tab. 2, all evaluated LVLMs exhibit substantial performance degradation when exposed to misleading visual cues, with consistently high MVI-Sensitivity across different models and misleading categories. On the closed-source side, even the latest state-of-theart models are not immune. For instance, GPT-5-Chats accuracy drops from 90.00% to 61.00% when visual illusion is introduced, and Gemini-2.5-Pros accuracy decreases from 86.54% to 54.81% under mirror reflection. All evaluated closed-source LVLMs yield overall MVISensitivity above 20%, indicating substantial vulnerability. Notably, Claude-3.7-Sonnet, despite its strength on reasoning-intensive tasks such as mathematics and coding, performs the worst among closed-source models, achieving only 42.13% accuracy on misleading images (Accm) with an MVI-Sensitivity of 42.10%. In contrast, open-source models, which play crucial role in advancing accessible research and real-world deployment, demonstrate greater susceptibility. The strongest open-source model, Qwen2-VL-72B, reaches an MVISensitivity of 32.38%, yet lags noticeably behind top closed-source counterparts. Meanwhile, Molmo-7B, representative fully-open model releasing both its training data and recipes, performs worst with an MVI-Sensitivity of 48.69%, meaning nearly half of its responses are affected by misleading cues. The performance gap between openand closed-source models may stem from several factors [13, 29, 30]: closed-source models typically benefit from largerscale, higher-quality proprietary training data [46, 59] and more extensive computational budgets enabling sophisticated post-training refinement techniques [30, 62]. LVLMs demonstrate stronger mastery of coarsegrained visual concepts than fine-grained visual attributes. Within the visual concept level, both visual resemblance and representation confusion involve coarse-grained semantic discrimination. LVLMs consistently show high and stable robustness on these two categories: most LVLMs models exceed or approach 50% Accm in visual resemblance, and all models except Molmo maintain over 40% Accm in representation confusion. These results indicates that current LVLMs handle high-level semantic discrimination more reliably than other types of misleading visual inputs. For example, Qwen2-VL-72B achieves highest 76.19% Accm and 15.79% MVI-sensitivity in visual resemblance, and GPT-4.1 reaches an 74.77% Accm with the lowest MVI-Sensitivity of 8.04% in representation confusion. However, performance drops considerably in material confusion, where the best model, Gemini-2.5-Pro, attains only 66.00% Accm and GPT-4o falls even further to 43.00%. Spatial reasoning remains critical weakness of current LVLMs. Mirror reflection is common in everyday life, yet it remains highly challenging for LVLMs. Even the strongest open-source model, Qwen2-VL-72B, achieves only 50.00% Accm, and GPT-4o reaches merely 52.88%. This weakness is even more pronounced among smaller models: InternVL3-2B, Qwen2.5-VL-3B, and LLaVAOneVision-7B all exhibit MVI-Sensitivity values exceeding 70%. These high sensitivity values indicate that while LVLMs perform well on images without mirror reflections (Accn often exceeds 80%), their performance drops sharply once virtual reflections appear, suggesting fundamental difficulties in distinguishing real objects from mirror images. Similarly, occlusion confusion reveals another major limitation in spatial reasoning. Only Gemini-2.5-Flash, Gemini-2.5-Pro and GPT-5-Chat surpass 50% Accm, while all other models fall below, highlighting substantial deficiency in occlusion perceptiona capability that underpins human spatial understanding. Notably, performance differences across models on visual illusion are less pronounced than in other categories. For example, InternVL3s Accm varies only between 48% and 52% as model size increases from 2B to 78B. We speculate that this may stem from the relative scarcity of visual illusion examples in multimodal pre-training datasets, suggesting that targeted data augmentation could be promising direction for future work. 4.3. Assessing the Impact of Visual Perception and Reasoning (RQ2) To understand what drives robustness against misleading visual inputs, we separately investigate the impact of visual perception and visual reasoning capabilities through controlled experiments, yielding following two key findings. Finding 1: Enhanced visual perception alone substantially improves robustness against misleading visual inputs. The vision encoder serves as the eye of an LVLM, establishing the upper bound of its perceptual capacity [11, Table 2. Performance comparison on MVI-Bench. Each category reports model performance on normal (Accn) and misleading (Accm) images, as well as MVI-Sensitivity (Sens). For each category and the overall results, the best score is in bold and the second best is underlined. Accm evaluates performance under misleading visual cues (: higher is better), while Sens measures the relative performance drop from normal to misleading inputs (: lower is better). Misleading Level Concept Misleading Type Resemblance Representation Attribute Material Mirror Relationship Occlusion Illusion Overall Model Accn Accm Sens Accn Accm Sens Accn Accm Sens Accn Accm Sens Accn Accm Sens Accn Accm Sens Accm Sens Open-source Models 89.52 66.67 25.53 82.24 50.47 38.63 77.67 45.63 41.25 88.46 23.08 73.91 70.48 29.52 58.12 87.00 47.00 45.98 43.75 46.99 LLaVA-OV (7B) 82.52 49.52 40.23 70.09 23.36 66.67 65.05 34.95 46.27 80.77 21.15 73.81 62.86 48.57 22.73 79.00 49.00 37.97 37.66 48.69 Molmo (7B) 91.43 53.33 41.67 79.44 65.42 17.65 73.79 46.60 36.85 93.27 36.54 60.85 68.57 39.05 43.06 85.00 48.00 43.53 48.23 41.10 SAIL-VL2 (2B) 90.48 57.14 36.85 81.31 63.55 21.84 72.82 45.63 37.34 92.31 39.42 57.29 72.38 38.10 47.36 85.00 51.00 40.00 49.19 40.28 SAIL-VL2 (8B) 83.81 40.00 52.27 70.09 45.79 34.67 65.05 42.72 34.33 86.54 25.96 70.00 64.76 32.38 50.00 84.00 51.00 39.29 39.58 47.67 InternVL3 (2B) 88.57 60.95 31.18 75.70 58.88 22.22 71.84 45.75 36.32 93.27 30.77 67.01 67.62 41.90 38.04 86.00 49.00 43.02 48.23 40.04 InternVL3 (8B) 89.52 66.67 25.53 83.18 64.49 22.47 78.64 62.14 20.98 89.42 47.12 47.30 78.12 48.57 37.83 86.00 52.00 39.53 56.89 32.38 InternVL3 (78B) 90.48 76.19 15.79 81.31 69.16 14.94 84.47 51.46 39.10 91.35 50.00 45.26 75.24 40.95 45.57 87.00 61.00 29.89 58.17 31.52 Qwen2-VL (72B) Qwen2.5-VL (3B) 87.62 62.86 28.26 71.96 49.53 31.17 68.93 38.83 43.67 84.62 23.08 72.73 60.95 31.43 48.43 79.00 43.00 45.57 41.51 45.01 Qwen2.5-VL (7B) 87.62 53.33 39.14 77.57 60.75 21.68 77.67 46.60 40.00 92.31 43.72 52.64 71.43 29.52 58.67 85.00 42.00 50.59 45.99 43.84 Qwen2.5-VL (32B) 91.43 60.00 34.38 79.44 59.81 24.72 73.79 43.69 40.78 89.42 37.50 58.08 74.29 30.48 58.97 87.00 54.00 37.93 47.59 42.35 Qwen2.5-VL (72B) 92.38 63.81 30.92 78.50 67.29 14.28 79.61 51.46 35.36 89.42 49.04 45.16 80.95 48.57 40.00 87.00 63.00 27.57 57.21 32.38 Closed-source Models Claude-3.7-Sonnet 90.48 54.29 40.00 72.90 43.93 39.74 59.22 38.83 34.43 73.08 29.81 59.21 57.14 39.05 31.66 86.00 48.00 44.19 42.13 42.10 90.48 74.29 17.89 71.96 54.21 24.67 58.00 43.00 25.86 77.88 52.88 32.10 59.05 42.86 27.42 85.00 54.00 36.47 53.37 27.28 8.04 68.00 55.00 19.12 86.54 67.31 22.22 67.62 49.52 26.77 88.00 58.00 34.09 62.82 20.80 86.67 73.33 15.39 81.31 74.77 86.67 61.90 28.58 72.90 49.50 32.13 74.76 55.34 25.98 72.12 36.54 49.33 60.95 55.23 9.38 80.00 54.00 32.50 52.08 30.11 90.48 67.62 25.26 79.44 64.49 18.82 81.00 66.00 18.52 86.54 54.81 36.67 78.10 65.71 15.86 91.00 58.00 36.26 62.50 22.00 83.81 70.48 15.91 84.11 70.09 16.67 84.47 59.22 29.89 85.58 70.19 17.98 69.52 51.43 26.02 90.00 61.00 32.22 63.78 23.02 GPT-4o GPT-4.1 Gemini-2.5-Flash Gemini-2.5-Pro GPT-5 Chat Table 3. Performance of Qwen2.5-VL-7B with caption-assisted inference. Baseline (caption model set to None) uses no caption. Caption Model Res. Rep. Mat. Mir. Occ. Ill. Overall Accm None Qwen2.5-VL GPT-4.1 53.33 54.29 58. 60.75 61.68 71.96 46.60 40.78 49.51 43.72 40.38 53.85 29.52 26.67 33.33 42.00 52.00 56.00 45.99 45.99 (+0.0) 53.85 (+7.86) Table 4. Comparison of model performance without and with the thinking process across misleading visual categories. Model Res. Rep. Mat. Mir. Occ. Ill. Overall Accm w/o thinking process SAIL-VL-2-8B Qwen2-VL-72B Gemini-2.5-Flash GPT-5-Chat 57.14 76.19 61.90 70.48 63.55 69.16 49.50 70.09 45.63 51.46 55.34 59. 39.42 50.00 36.54 70.19 w/ thinking process SAIL-VL2-8B 49.52 QVQ-72B-Preview 61.90 64.76 Gemini-2.5-Flash 67.62 GPT-5-thinking 55.14 66.36 72.90 73.81 54.37 60.19 65.05 66. 47.12 54.81 63.46 57.69 38.10 40.95 55.23 51.43 34.29 21.90 59.05 55.24 51.00 61.00 54.00 61.00 53.00 45.00 57.00 65.00 49.19 58.17 52.08 63. 48.08 (-1.11) 51.76 (-6.41) 63.14 (+11.06) 64.42 (+0.64) 37, 51, 64]. Therefore, enhancing the models ability to perceive and encode visual details may effectively improve its robustness. To test this hypothesis, we employ captionassisted inference approach [32, 52], where image captions (detailed descriptions in our setting) are used as proxy for enhanced perception. By supplying rich visual information in textual form, this method compensates for limitations of the vision encoder without modifying it. Specifically, we first use strong auxiliary LVLM (the caption model) to generate detailed image caption via prompt Please describe the image in detail. The caption is then concatenated with the image and question as input to the inference model. As shown in Tab. 3, when Qwen2.5-VL-7B answers directly without any additional captions, its Accm is only 45.99%. When the model itself serves as the caption model, performance remains nearly unchanged. However, replacing the caption model with stronger GPT-4.1 yields substantial gains, increasing overall Accm from 45.99% to 53.85% with improvements across all categories. Notably, this performance surpasses Qwen2.5-VL-32B (47.59%, as shown in Tab. 2) and approaches the results of Qwen2.5VL-72B (57.21%). These results suggest that the visual perceptual capacity may be key factor of LVLM robustness under visually misleading conditions. Finding 2: Enhanced visual reasoning improves robustness against misleading visual inputs, yet the improvement remains inconsistent and unstable. There are currently two mainstream approaches to strengthen visual reasoning in LVLMs: (1) scaling the language model [10, 57], and (2) enabling explicit reasoning via long-form chain-of-thought (CoT) [58] through reinforcement learning (RL) [21, 39]. We examine both approaches to understand their impacts on robustness. Scaling language models. As shown in Tab. 2, when scaling from Qwen2.5-VL-3B to Qwen2.5-VL-72B while keeping the vision encoder fixed, overall Accm increases Figure 4. Comparison between the non-think and think modes of SAIL-VL. In the non-think mode, the model answers directly based on visual evidence, while in the think mode, the model is guided by historical thoughts and tend to overemphasize fine details. and MVI-Sensitivity decreases. This trend suggests that stronger reasoning capacity can partially compensate for limited visual perceptual ability. However, the improvement is non-monotonic: within the mid-scale range (e.g., from 7B to 32B), performance on misleading images degrades in certain categories, including representation confusion and mirror reflection. Enabling explicit reasoning via long-form CoT. Next, we examine how enabling an explicit long-thinking process influences models performance under misleading visual inputs. While this paradigm has shown promise in reasoningheavy tasks, its effect on visually misleading conditions remains unclear. Unexpectedly, we observe that longthinking variants of open-source models (e.g., SAIL-VL2 and Qwen2-VL) exhibit lower Accm on MVI-Bench. This degradation is primarily attributed to substantial declines in categories such as visual resemblance and representation confusion, which depend heavily on perception capabilities. This observation aligns with prior works [33, 50], which demonstrate that multimodal reasoning models perceptual abilities weaken as reasoning processes. detailed analysis of paired instances (those answered correctly without thinking but incorrectly with thinking, as shown in Fig. 4) reveals two key failure patterns. First, the reasoning process becomes increasingly guided by historical thoughts rather than image content, phenomenon termed visual forgetting [50], where generated reasoning steps progressively dominate decision-making and lead the model to drift away from the visual evidence. (More cases are provided in Appendix E.1.). Second, models increasingly over-attend to fine-grained details (e.g., shape or color), which drives them away from correct answers. Interestingly, this behavior can be beneficial in categories such as material confusion and mirror reflection, where models need to carefully examine and analyze visual details. In contrast, proprietary models (like Gemini-2.5-Flash and GPT-5) show performance gains with explicit thinking, suggesting that closed-source models are better trained to effectively leverage extended reasoning without sacrificing perceptual ability. However, the thinking-enhanced Gemini-2.5-Flash still underperforms the non-reasoning GPT-5-Chat by 0.64% in Accm. These results indicate that visual perception remains the primary bottleneck in resisting misleading visual inputs and serves as fundamental prerequisite for robust extended reasoning in LVLMs. 4.4. Case Study: When LVLMs Outperform on Misleading Visual Inputs (RQ3) While most models perform worse on misleading images, we observe small but revealing set of counterintuitive instances (about 4% of total cases) where models answer misleading images correctly but fail on corresponding normal images. To investigate this phenomenon, we employ an attention-guided masking approach inspired by recent interpretability studies [26, 27, 45, 47, 70]. For each image pair, we (1) visualize attention maps for the first answer token, (2) identify regions with highest attention weights, (3) mask those regions, and (4) re-evaluate whether the models answer changes (see Appendix A.2 for details). Findings. Our analysis reveals that these counterintuitive cases arise from spurious correlations between visual artifacts and the target label. Fig. 5 shows representative case. In the misleading image (Fig. 5 (b)), two partially overlapping books and receipt appear on the table. The model (Qwen2.5-VL-7B) misidentifies the overlapping books as one and incorrectly treats the receipt as another book. This spurious correlation causes model to accidentally produce the correct answer (2 books). When we mask the receipt region (Fig. 5 (c)), the models answer immediately becomes incorrect, confirming its reliance on this false cue. This also explains its failure on the normal image (Fig. 5 (a)): the model predicts 2 despite only one book present. this phenomenon stems from the lack of fine-grained supervision in current VQA training"
        },
        {
            "title": "We speculate that",
            "content": "[3] Amit Agarwal, Srikant Panda, Angeline Charles, Hitesh Laxmichand Patel, Bhargava Kumar, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Hansa Meghwani, Karan Gupta, et al. Mvtamperbench: Evaluating In Findings of the robustness of vision-language models. Association for Computational Linguistics: ACL 2025, pages 1880418828, 2025. 1, 2 [4] Anthropic. Claude 3.7 sonnet system card, 2025. Accessed: 2025-02-04. 6, 3 [5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425 2433, 2015. 1, 2 [6] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An opensource framework for training large autoregressive visionlanguage models. arXiv preprint arXiv:2308.01390, 2023. [7] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei arXiv preprint Huang, et al. Qwen technical report. arXiv:2309.16609, 2023. 2 [8] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5, 6, 3 [9] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. Hallucination of multimodal large language models: survey. arXiv preprint arXiv:2404.18930, 2024. 1, 2 [10] Leonardo Berti, Flavio Giorgi, and Gjergji Kasneci. Emergent abilities in large language models: survey. arXiv preprint arXiv:2503.05788, 2025. 7 [11] Haoran Chen, Junyan Lin, Xinhao Chen, Yue Fan, Xin Jin, Hui Su, Jianfeng Dong, Jinlan Fu, and Xiaoyu Shen. Rethinking visual layer selection in multimodal llms. arXiv preprint arXiv:2504.21447, 2025. 6 [12] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 1, [13] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2418524198, 2024. 2, 5, 6 [14] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose visionlanguage models with instruction tuning. Advances in neural information processing systems, 36:4925049267, 2023. 2 [15] Yunkai Dang, Mengxi Gao, Yibo Yan, Xin Zou, Yanggan Gu, Jungang Li, Jingyu Wang, Peijie Jiang, Aiwei Liu, Jia Liu, et al. Exploring response uncertainty in mllms: An emFigure 5. Attention-guided masking for counterintuitive instance. Qwen2.5-VL-7B spuriously associates receipt with book. (a) On the normal image with one book, it answers incorrectly. (b) On the misleading image, it coincidentally answers 2 by counting the receipt as an extra book. (c) Masking the receipt flips the prediction, confirming the spurious correlation. paradigms, where models are supervised only on final answer labels without explicit guidance on the rationale. Under this weak supervision setting, where only single answer label is provided per image-question pair, models tend to minimize training loss by exploiting unexpected shortcut cues, forming spurious correlations between incidental image regions and labels rather than learning true causal associations. Moreover, this also identifies potential issue in current VQA evaluation: assessments should consider not only answer correctness but also the underlying reasoning process for more robust evaluation in the future work. 5. Conclusion This paper introduces MVI-Bench, the first comprehensive benchmark specifically designed to evaluate how misleading visual inputs undermine the robustness of LVLMs. In addition, we propose MVI-Sensitivity, metric that characterizes LVLM robustness at fine-grained level. Our evaluation reveals that existing LVLMs exhibit pronounced vulnerabilities to misleading visual inputs, and in-depth analysis on MVI-Bench provides actionable insights for developing more reliable and robust LVLMs."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Jyoti Aneja, Harkirat Behl, Sebastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell Hewett, Mojan Javaheripi, Piero Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905, 2024. 2 [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 2, 6, 3 pirical evaluation under misleading scenarios. arXiv preprint arXiv:2411.02708, 2024. 1, 2 Computer Vision and Pattern Recognition Conference, pages 2500425014, 2025. 8 [16] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art In Proceedings of the Computer vision-language models. Vision and Pattern Recognition Conference, pages 91104, 2025. 6, 3 [17] Ailin Deng, Tri Cao, Zhirui Chen, and Bryan Hooi. Words or vision: Do vision-language models have blind faith in text? In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 38673876, 2025. 1, 2 [18] Hongyuan Dong, Zijian Kang, Weijie Yin, Xiao Liang, Chao Feng, and Jiao Ran. Scalable vision language model arXiv preprint training via high quality data curation. arXiv:2501.05952, 2025. [19] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 1 [20] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual ilIn Proceedings of lusion in large vision-language models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1437514385, 2024. 4 [21] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 7 [22] Junlin Han, Shengbang Tong, David Fan, Yufan Ren, Koustuv Sinha, Philip Torr, and Filippos Kokkinos. Learning to see before seeing: Demystifying llm visual priors from language pre-training. arXiv preprint arXiv:2509.26625, 2025. 2, 4 [23] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 6, 3 [24] Chaoya Jiang, Hongrui Jia, Mengfan Dong, Wei Ye, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. Hal-eval: universal and fine-grained hallucination evaluation framework for large vision language models. In Proceedings of the 32nd ACM International Conference on Multimedia, pages 525 534, 2024. [25] Chengze Jiang, Zhuangzhuang Wang, Minjing Dong, and Jie Gui. Survey of adversarial robustness in multimodal large language models. arXiv preprint arXiv:2503.13962, 2025. 1, 2 [26] Zhangqi Jiang, Junkai Chen, Beier Zhu, Tingjin Luo, Yankun Shen, and Xu Yang. Devils in middle layers of large visionlanguage models: Interpreting, detecting and mitigating object hallucinations via attention lens. In Proceedings of the [27] Seil Kang, Jinyeong Kim, Junhyeok Kim, and Seong Jae Hwang. See what you are told: Visual attention sink in large multimodal models. arXiv preprint arXiv:2503.03321, 2025. 8 [28] Jiayi Kuang, Ying Shen, Jingyou Xie, Haohao Luo, Zhe Xu, Ronghao Li, Yinghui Li, Xianfeng Cheng, Xika Lin, and Yu Han. Natural language understanding and inference with mllm in visual question answering: survey. ACM Computing Surveys, 57(8):136, 2025. 1, 2 [29] Hugo Laurencon, Leo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building vision-language models? Advances in Neural Information Processing Systems, 37:8787487907, 2024. 6 [30] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [31] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 6, 3 [32] Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, and Bin Dong. Multimodal reasoning for science: Technical report and 1st place solution to the icml 2025 seephys challenge. arXiv preprint arXiv:2509.06079, 2025. 7 [33] Chengzhi Liu, Zhongxing Xu, Qingyue Wei, Juncheng Wu, James Zou, Xin Eric Wang, Yuyin Zhou, and Sheng Liu. assessing amplified halluciMore thinking, less seeing? arXiv preprint nation in multimodal reasoning models. arXiv:2505.21523, 2025. 8 [34] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. 2 [35] Yexin Liu, Zhengyang Liang, Yueze Wang, Xianfeng Wu, Feilong Tang, Muyang He, Jian Li, Zheng Liu, Harry Yang, Sernam Lim, et al. Unveiling the ignorance of mllms: SeeIn Proceedings of the ing clearly, answering incorrectly. Computer Vision and Pattern Recognition Conference, pages 90879097, 2025. 1, 2 [36] Zhaochen Liu, Kaiwen Gao, Shuyi Liang, Bin Xiao, Limeng Qiao, Lin Ma, and Tingting Jiang. Beyond the visible: Benchmarking occlusion perception in multimodal large language models. arXiv preprint arXiv:2508.04059, 2025. 3, 4 [37] Clement Neo, Luke Ong, Philip Torr, Mor Geva, David Krueger, and Fazl Barez. Towards interpreting visual information processing in vision-language models. arXiv preprint arXiv:2410.07149, 2024. [38] OpenAI. Gpt-5 system card, 2025. Accessed: 2025-08-13. 6, 3 [39] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. 7 [40] Yusu Qian, Haotian Zhang, Yinfei Yang, and Zhe Gan. How easy is it to fool your multimodal llms? an empirical analysis on deceptive prompts. arXiv preprint arXiv:2402.13220, 2 (7), 2024. 1, 2 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 5 [42] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. [43] Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, and Rifat Shahriyar. illuarXiv preprint sion dataset for vision language models. arXiv:2403.15952, 2024. 3, 4 Illusionvqa: challenging optical [44] Arthur David Smith. The problem of perception. Motilal Banarsidass Publishe, 2005. 2 [45] Gabriela Ben Melech Stan, Estelle Aflalo, Raanan Yehezkel Rohekar, Anahita Bhiwandiwalla, Shao-Yen Tseng, Matthew Lyle Olson, Yaniv Gurwicz, Chenfei Wu, Nan Duan, and Vasudev Lal. Lvlm-interpret: an interpretability arXiv preprint tool for large vision-language models. arXiv:2404.03118, 2024. 8 [46] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, YuXiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1308813110, 2024. [47] Feilong Tang, Chengzhi Liu, Zhongxing Xu, Ming Hu, Zile Huang, Haochen Xue, Ziyang Chen, Zelin Peng, Zhiwei Yang, Sijin Zhou, et al. Seeing far and clearly: Mitigating hallucinations in mllms with attention causal decoding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2614726159, 2025. 8 [48] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. 2, 6, 3 [49] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. Kimi-vl technical report. arXiv preprint arXiv:2504.07491, 2025. 2 [50] Xinyu Tian, Shu Zou, Zhaoyuan Yang, Mengqi He, Fabian Waschkowski, Lukas Wesemann, Peter Tu, and Jing Zhang. on the dual nature of More thought, arXiv preprint reasoning in vision-language models. arXiv:2509.25848, 2025. 8 less accuracy? [51] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the visual shortcomings of multimodal llms. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95689578, 2024. 7 [52] Qixin Wan, Zilong Wang, Jingwen Zhou, Wanting Wang, Ziheng Geng, Jiachen Liu, Ran Cao, Minghui Cheng, and Lu Cheng. Som-1k: thousand-problem benchmark dataset for strength of materials. arXiv preprint arXiv:2509.21079, 2025. 7 [53] Fei Wang, Xingyu Fu, James Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et al. Muirbench: comprehensive benchmark for robust multi-image understanding. arXiv preprint arXiv:2406.09411, 2024. 1, 2 [54] Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, et al. Amber: An llm-free multi-dimensional bencharXiv preprint mark for mllms hallucination evaluation. arXiv:2311.07397, 2023. 1, 2 [55] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 3 [56] Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao, Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, and Hongxia Yang. Exploring the reasoning abilities of multimodal large language models (mllms): comprehensive survey on emerging trends in multimodal reasoning. arXiv preprint arXiv:2401.06805, 2024. 1, [57] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. arXiv preprint arXiv:2206.07682, 2022. 7 [58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 7 [59] Mang Ye, Xuankun Rong, Wenke Huang, Bo Du, Nenghai Yu, and Dacheng Tao. survey of safety on large visionlanguage models: Attacks, defenses and evaluations. arXiv preprint arXiv:2502.14881, 2025. 6 [60] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12): nwae403, 2024. 1 [61] Weijie Yin, Yongjie Ye, Fangxun Shu, Yue Liao, Zijian Kang, Hongyuan Dong, Haiyang Yu, Dingkang Yang, Jiacong Wang, Han Wang, et al. Sail-vl2 technical report. arXiv preprint arXiv:2509.14033, 2025. 2, 6, 3 [62] Tianyu Yu, Haoye Zhang, Qiming Li, Qixin Xu, Yuan Yao, Da Chen, Xiaoman Lu, Ganqu Cui, Yunkai Dang, Taiwen He, et al. Rlaif-v: Open-source ai feedback leads to super gpt-4v trustworthiness. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 19985 19995, 2025. [63] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. 1, 2 [64] Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, and Filip Ilievski. Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422, 2025. 7 [65] Yiming Zhang, Zicheng Zhang, Xinyi Wei, Xiaohong Liu, Guangtao Zhai, and Xiongkuo Min. Illusionbench: large-scale and comprehensive benchmark for visual illusion understanding in vision-language models. arXiv preprint arXiv:2501.00848, 2025. 4 [66] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. Advances in Neural Information Processing Systems, 36:5411154138, 2023. 2 [67] Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. Large language models are not robust multiple choice selectors. arXiv preprint arXiv:2309.03882, 2023. [68] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023. 2 [69] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 6, 3 [70] Jingze Zhu, Yongliang Wu, Wenbo Zhu, Jiawang Cao, Yanqiang Zheng, Jiawei Chen, Xu Yang, Bernt Schiele, Jonas Fischer, and Xinting Hu. Layercake: Token-aware contrastive decoding within large language model layers. arXiv preprint arXiv:2507.04404, 2025. 8 MVI-Bench: Comprehensive Benchmark for Evaluating Robustness to Misleading Visual Inputs in LVLMs"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Limitations and Future Work C. More Details about MVI-Bench Although spurious-correlation cases constitute only very small fraction of our benchmark, their implications may be more substantial in real-world settings. Modern LVLMs are trained on extremely large-scale datasets, often containing millions to billions of imagetext pairs and even small proportions of these examples can accumulate into systematic biases. However, our analysis of spurious correlations relies fundamentally on the paired design of MVI-Bench, which enables counterfactual comparisons between normal and misleading images. As such, the methodology does not directly generalize to other datasets without further adaptations. Furthermore, our findings underscore broader challenges in current LVLM training and evaluation pipelines: shortcut learning remains difficult to mitigate under weak supervision that provides only answer labels without rationales, and existing VQA evaluations emphasize answer correctness without assessing whether predictions are causally grounded in visual evidence. Future work should explore training objectives and evaluation protocols that discourage shortcut exploitation and promote faithful, causally aligned visual reasoning. B. Ethics Statement During the collection of major instances, we strictly complied with the copyright and licensing regulations of each social media platform, ensuring that data was collected only from publicly accessible posts and that no images were downloaded from sources explicitly prohibiting data reuse or redistribution. portion of the dataset was additionally generated using the Seedream. While this model produces high-quality outputs, the generated content inevitably reflects the biases and limitations inherent in its training data. We recognize the ethical concerns associated with such models, including the potential to reinforce stereotypes or generate inappropriate content. To minimize these risks, we applied careful dataset curation and conducted manual reviews throughout the construction process. We encourage future research to explore stronger methods for identifying and mitigating such biases, ensuring that both collected and generated content align with ethical standards and societal norms. C.1. Data Generation Pipeline The data curation pipeline is illustrated in Fig. 6. C.2. More Examples Fig. 8 presents additional examples from each category of MVI-Bench. D. Experiment Details D.1. Prompt Format for Caption-Assisted Inference We describe the prompt format used for caption-assisted inference. After prompting strong auxiliary LVLM with the instruction Please describe the image, we obtain detailed caption that compensates for the limitations of the inference models vision encoder. We then concatenate this caption with the corresponding image and question as input to the inference model, as illustrated below. <image> Here are some additional information which are text descriptions based on the image to assist you for answering the later question:{Caption} {Question} Answer with the letter from the given choices directly. D.2. Model Details Table 5 presents the release time and model sources of LVLMs used in MVI-Bench. D.3. Implementation of Attention Visualization To visualize where the model attends when generating an answer, we implement the relative answer-to-image attention. Specifically, for given image and question q, we first feed the multimodal input sequence into the LVLM and extract the cross-attention weights from the first generated answer token to all image tokens between the <vision start> and <vision end> markers in each layer. The attention matrices are averaged across all attention heads, yielding the answer-to-image-token attention Ast(x, q). We then normalize this attention by its counterpart obtained from generic instruction =Write general deFigure 6. Benchmark Curation Pipeline. The pipeline starts with image collection, followed by VQA annotation, data filtering, and ultimately results in MVI-Bench. To ensure data quality, human verification is performed at each key stage to eliminate low-quality data, annotations, and ambiguous evaluation questions. Figure 7. Comparison between the non-think and think modes of SAIL-VL. In the non-think mode, the model answers directly based on visual evidence, while in the think mode, the model is guided by historical thoughts and tend to overemphasize fine details. scription of the image., resulting in the relative attention: E. More Cases Arel(x, q) = Ast(x, q) Ast(x, q) . This normalization removes the models default visual bias and highlights the image regions whose attention increases specifically in response to the question. E.1. Comparison of the Non-think and Think"
        },
        {
            "title": "Modes",
            "content": "More cases illustrating the comparison between the Nonthink and Think modes are presented in Fig. 7. Table 5. The Release Time and Model Source of LVLMs Evaluated in MVI-Bench."
        },
        {
            "title": "URL",
            "content": "LLaVA-OneVision-7B [31] 2024-"
        },
        {
            "title": "ByteDance",
            "content": "https://github.com/LLaVA-VL/LLaVA-NeXT Molmo-7B-D-0924 [16] 2025-04 allenai (Ai2) https://huggingface.co/allenai/Molmo-7B-D0924 SAIL-VL2-2B [61] 2025-"
        },
        {
            "title": "ByteDance",
            "content": "SAIL-VL2-8B [61] 2025-"
        },
        {
            "title": "ByteDance",
            "content": "InternVL3-2B [69] 2025-"
        },
        {
            "title": "OpenGVLab",
            "content": "InternVL3-8B [69] 2025-"
        },
        {
            "title": "OpenGVLab",
            "content": "InternVL3-78B [69] 2025-"
        },
        {
            "title": "OpenGVLab",
            "content": "Qwen2-VL-72B-Instruct [55] 2024-"
        },
        {
            "title": "Alibaba",
            "content": "Qwen2.5-VL-3B-Instruct [8] 2025-"
        },
        {
            "title": "Alibaba",
            "content": "Qwen2.5-VL-7B-Instruct [8] 2025-"
        },
        {
            "title": "Alibaba",
            "content": "Qwen2.5-VL-32B-Instruct [8] 2025-"
        },
        {
            "title": "Alibaba",
            "content": "Qwen2.5-VL-72B-Instruct [8] 2025-"
        },
        {
            "title": "Alibaba",
            "content": "Claude-3.7-Sonnet [4] 2025-"
        },
        {
            "title": "Anthropic",
            "content": "https://huggingface.co/ BytedanceDouyinContent/SAIL-VL2-2B https://huggingface.co/ BytedanceDouyinContent/SAIL-VL2-8B https://huggingface.co/OpenGVLab/InternVL32B https://huggingface.co/OpenGVLab/InternVL38B https://huggingface.co/OpenGVLab/InternVL378B https://huggingface.co/Qwen/Qwen2-VL-72BInstruct https://huggingface.co/Qwen/Qwen2.5-VL-3BInstruct https://huggingface.co/Qwen/Qwen2.5-VL-7BInstruct https://huggingface.co/Qwen/Qwen2.5-VL32B-Instruct https://huggingface.co/Qwen/Qwen2.5-VL72B-Instruct https://www.anthropic.com/news/claude-3-7sonnet GPT-4o [23] GPT-4.1 [2] Gemini-2.5-Flash [48] Gemini-2.5-Pro [48] GPT-5-Chat [38] 2024-05 20242025-06 2025-06 2025-"
        },
        {
            "title": "OpenAI",
            "content": "https://platform.openai.com/ https://platform.openai.com/ https://gemini.google.com/app https://gemini.google.com/app https://platform.openai.com/ Figure 8. More Examples from six misleading categories defined in MVI-Bench."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Illinois at Chicago",
        "Guohao School, Tongji University",
        "School of Computer Science & Engineering, Southeast University"
    ]
}