{
    "paper_title": "UniREditBench: A Unified Reasoning-based Image Editing Benchmark",
    "authors": [
        "Feng Han",
        "Yibin Wang",
        "Chenglin Li",
        "Zheming Liang",
        "Dianyi Wang",
        "Yang Jiao",
        "Zhipeng Wei",
        "Chao Gong",
        "Cheng Jin",
        "Jingjing Chen",
        "Jiaqi Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for a comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, a unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both real- and game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, a large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closed-source image editing models, we reveal their strengths and weaknesses across various aspects."
        },
        {
            "title": "Start",
            "content": "UniREditBench: Unified Reasoning-based Image Editing Benchmark Feng Han1,2*, Yibin Wang1,2*, Chenglin Li2,3, Zheming Liang2, Dianyi Wang1,2, Yang Jiao1, Zhipeng Wei4, Chao Gong1, Cheng Jin1,2, Jingjing Chen1, Jiaqi Wang2 1Fudan University, 2Shanghai Innovation Institute, 3Zhejiang University, 4UC Berkeley Project Page: maplebb.github.io/UniREditBench 5 2 0 2 2 2 ] . [ 2 5 9 2 1 0 . 1 1 5 2 : r Figure 1. UniREditBench covers both real-world and game-world reasoning scenarios across 8 primary dimensions and 18 sub-dimensions. We provide qualitative editing cases of (a) real-world multi-object interaction, and (b) game-world logical/strategy reasoning."
        },
        {
            "title": "Abstract",
            "content": "Recent advances in multi-modal generative models have driven substantial improvements in image editing. However, current generative models still struggle with handling diverse and complex image editing tasks that require implicit reasoning, underscoring the need for comprehensive benchmark to systematically assess their performance across various reasoning scenarios. Existing benchmarks primarily focus on single-object attribute transformation in realistic scenarios, which, while effective, encounter two key challenges: (1) they largely overlook multi-object interactions as well as game-world scenarios that involve human-defined rules, which are common in real-life applications; (2) they only rely on textual references to evaluate the generated images, potentially leading to systematic misjudgments, especially in complex reasoning scenarios. To this end, this work proposes UniREditBench, unified benchmark for reasoning-based image editing evaluation. It comprises 2,700 meticulously curated samples, covering both realand game-world scenarios across 8 primary dimensions and 18 sub-dimensions. To improve evaluation reliability, we introduce multimodal dual-reference evaluEqual contribution. Corresponding author. ation, providing both textual and ground-truth image references for each sample assessment. Furthermore, we design an automated multi-scenario data synthesis pipeline and construct UniREdit-Data-100K, large-scale synthetic dataset with high-quality chain-of-thought (CoT) reasoning annotations. We fine-tune Bagel on this dataset and develop UniREdit-Bagel, demonstrating substantial improvements in both in-domain and out-of-distribution settings. Through thorough benchmarking of both open-source and closedsource image editing models, we reveal their strengths and weaknesses across various aspects. 1. Introduction Recent advances in multimodal generative models have led to remarkable improvements in instruction-conditioned image editing. Generative models [3, 33, 38, 40, 44, 47, 49], including Step1X-Edit [21], FLUX-Kontext [2], Bagel [6], Nano Banana [7], and GPT-4o [13], have demonstrated powerful ability to understand diverse textual instructions and generate semantically consistent image edits. In parallel, reinforcement learning-based training strategies [19, 31, 36, 43] are continuously advancing, further enhancing the capabilities of image editing models. With Table 1. Reasoning-based image editing benchmark comparison. Our UniREditBench excels in broader scenario and evaluation dimension coverage. S-Obj indicates single-object while M-Obj indicates multi-object."
        },
        {
            "title": "Reference Images",
            "content": "SmartEdit [12] RISE [51] KRIS [41] UniREditBench 219 360 1,267 2,700 219 70 50 2,700 Attribute Temporal (S-Obj) (S-Obj) Pose (S-Obj) these rapid developments, the need for more comprehensive benchmark to evaluate model editing capabilities across different aspects has become increasingly essential. Early benchmarks [21, 46] focus on local details or global stylistic changes, e.g., style transfer, color alteration, and object removal. However, they fail to cover editing tasks that require models to perform implicit reasoning [9, 48], which are commonly used in real-life applications. As illustrated in Fig. 1, when editing instructions involving realworld or human-defined game rules, current models often generate results that lack physical plausibility. To this end, recent efforts have introduced reasoning-aware evaluation across temporal, spatial, and logical dimensions [51], and proposed knowledge-grounded taxonomy assessing factual, conceptual, and procedural knowledge types [41]. Despite their effectiveness, these benchmarks still face (1) they primarily focus on two significant challenges: single-object attribute changes in realistic scenarios, neglecting multi-object interactions and game-world scenarios involving human-defined rules (see Tab. 1). This narrow scope restricts their ability to evaluate how effectively models generalize across wider range of complex reasoning contexts; Additionally, (2) they mainly rely on textual reference to evaluate the generated images [41, 51], which may lead to systematic misjudgments, especially in complex reasoning-based editing scenarios (see Fig. 2). In this work, we posit that: (1) While current models exhibit proficiency in perceptual instruction following and simple reasoning editing settings (e.g., Transform an intact apple to bitten one), they still struggle with complex reasoning-based image editing that necessitates the comprehension of multi-object interaction characteristics (e.g., Draw the paddle backward through the water) as well as logical constraints of puzzle and game scenarios (e.g., Control the player and push the box to the target), as illustrated in Fig. 1. (2) Relying solely on textual references in evaluating complex reasoning-based image editing task often leads to unreliable judgments. As shown in Fig. 2 (a), the textreference-only evaluator assigns an inflated score even the edited image introduces an additional faulty path. Therefore, we intuitively believe that incorporating ground-truth (GT) image as an additional visual reference can enable more precise evaluation. To this end, this work proposes UniREditBench, (M-Obj) (M-Obj) Spatial Motion Mechanic Medium (M-Obj) (M-Obj) Logical Long-planing"
        },
        {
            "title": "Spatial",
            "content": "Figure 2. Image editing evaluation comparison. Current textreference-only evaluation potentially leads to misjudging, while our dual-reference evaluation results in more reliable assessments. unified benchmark for reasoning-based image editing assessment with broader evaluation dimension coverage and robust evaluation pipeline. Specifically, (1) we adopt scenario-to-category hierarchical dimension design, covering diverse reasoning types in both real-world and gameworld scenarios (shown in Fig. 1): it includes 2,700 carefully curated samples organized across 8 primary dimensions and 18 sub-categories, e.g., multi-object interaction in real world, and long-horizon game planning in game world. Meanwhile, (2) as illustrated in Fig. 2, in contrast to existing work that relies solely on textual references for evaluation, we introduce additional reference GT images to facilitate direct visual comparison with the generated image. By utilizing the visual cues provided by the reference image, the evaluator is able to more accurately and reliably assess the alignment of the generated image with the given instruction, as shown in Fig. 2 (b). Furthermore, to ensure the diversity and reliability of samples in this benchmark, we design multi-scenario data synthesis pipeline. Specifically, as shown in Fig. 3, (a) For real-world scenarios, we first handcraft few reference text prompts, including the original image description, the editing instruction, and the textual reference of edited effect. These prompts are then scaled up using the VLM. Finally, all resulted textual descriptions are directly used to generate pairs of original and edited image. (b) For game-world scenarios, we first design diverse game problems, and then use Python programs to generate image pairs, instructions, and textual reference of edited effects, ensuring both logical and visual correctness in these ruleintensive scenarios [17, 27]. Ultimately, all data samples in UniREditBench undergo VLM-based filtering and human inspection to ensure their reliability and accuracy. Based on our data synthesis pipeline, we also propose UniREdit-Data-100K, comprehensive reasoning-based image editing dataset with high-quality chain-of-thought instructions with accurate visual manipulation. Traditional methods perform editing by altering the diffusion trajectory without requiring additional training, including partial denoising from intermediate SDE steps [22], cross-attention control [11, 30], mask-guided blending [1, 33, 38], CLIPor diffusion-guided manipulation [15], and latent inversion for fidelity preservation [14, 32]. Besides, several studies employ visual-language models (VLMs) to provide prompts, spatial priors, or synthetic supervision to guide generative editing model [3, 8, 10, 49, 50]. Recent unified frameworks aim to use single model for both image understanding and editing in complementary direction [29, 40, 44]. For instance, Bagel [6] features think mode that produces reasoning text prior to editing to enhance instruction fidelity and consistency. While effective, current methods still face challenges with complex reasoning-based editing, underscoring the need for comprehensive benchmarks to assess their performance across various reasoning scenarios. Reasoning-based Benchmarks for Image Generation and Editing. In T2I generation, several benchmarks [18, 23, 25, 34] have been developed to assess the reasoning capabilities of models in generating images. For example, WISE [23] focuses on assessing models world knowledge, such as cultural and physical understanding, while UniGenBench++ [34] unifies semantic generation evaluation, covering 10 primary dimensions and 27 sub-dimensions, such as logic reasoning, relational understanding, supporting multilingual and varying-length assessments. In image editing evaluation, recent reasoning-based benchmarks like RISEBench [51] aim to examine temporal, spatial, and logical editing capabilities of editing models. Besides, KRISBench [41] introduces knowledge-grounded taxonomy covering factual, conceptual, and procedural types. However, these benchmarks primarily focus on single-object knowledge and attribute reasoning. We suppose that extending evaluation to multi-object interactions and scenarios governed by human-defined rules is crucial next step. As for image quality evaluation [16, 45], recent works like UnifiedReward [35, 37] adopt the VLM-as-a-judge paradigm, leveraging the powerful capabilities of VLMs to score and provide explanatory judgments. In image editing tasks, evaluation is more challenging because the evaluator needs to assess not only image quality but also understand complex editing instructions and final edited effects. Most studies like RISEBench and KRISBench utilize the property model [13], to rate instruction following, temporal consistency, and image quality. Despite effectiveness, their evaluation relies solely on textual references, which may lead to systematic misjudgments in complex reasoning tasks. To this end, this work proposes UniREditBench, unified reasoning-based image editing benchmark that spans broad range of evaluation dimensions across real-world and game-world scenarios with multimodal dual-reference evalFigure 3. Multi-scenario data synthesis pipeline. (a) Real-world data synthesis pipeline; (b) Game-world data synthesis pipeline; and (c) Case study of our synthesized data. (CoT) reasoning annotations, consisting of detailed, stepby-step reasoning traces generated using VLM, as shown in Fig. 3. To validate its reliability and effectiveness, we finetune the Bagel [6] on this dataset, resulting in UniREditBagel. Experimental results demonstrate that the fine-tuned model achieves substantial improvements on both UniREditBench and other out-of-distribution benchmarks [41, 51]. Additionally, through comprehensive evaluation of both openand closed-source editing models on our UniREditBench, we reveal their strengths and weaknesses across diverse reasoning-based scenarios. Contribution: (1) We introduce UniREditBench, unified benchmark for reasoning-based image editing that covers both real-world and game-world scenarios across 8 primary dimensions and 18 sub-dimensions, augmented with reference GT images to enable robust evaluation; (2) We design multi-scenario data synthesis pipeline and develop UniREdit-Data-100K, large-scale synthetic reasoningbased image editing dataset that includes high-quality CoT reasoning annotations. By fine-tuning the Bagel on this dataset, we develop UniREdit-Bagel and achieve substantial improvements, validating the effectiveness and reliability of our dataset; (3) Through comprehensive benchmarking of both openand closed-source models, we systematically identify their strengths and weaknesses across diverse reasoning-based editing scenarios, offering valuable insights for advancing future models. 2. Related Work Instruction-based Image Editing. Instruction-based image editing models aim to bridge semantic understanding of Figure 4. Qualitative cases of evaluation dimensions in UniREditBench. We present qualitative examples for each dimension across both real-world and game-world scenarios. uation for more reliable and accurate assessments. 3. UniREditBench 3.1. Overview With the rapid advancements in image editing models, existing benchmarks are gradually becoming less adequate to fully capture their comprehensive capabilities, particularly their reasoning-based editing abilities. Specifically, current benchmarks encounter two major challenges: (1) their evaluation primarily focuses on simple single-object attribute edits in real-world scenarios, neglecting complex multiobject interactions, as well as logical or strategic reasoning in game-world scenarios, where explicit human-defined rules govern the outcomes (Tab. 1); (2) their evaluation predominantly rely on clip-based metrics or VLM-based evaluators with text-only references, which may offer insufficient or inaccurate assessments, particularly in complex reasoning-intensive editing scenarios  (Fig. 2)  . To this end, this work proposes UniREditBench, unified reasoning-based image editing benchmark that covers broad spectrum of reasoning dimensions in different scenarios. Compared with previous studies, this benchmark exhibits several key superiorities: Broader scenario and reasoning dimension coverage. It contains 2,700 high-quality samples organized into 8 primary reasoning dimensions and 18 sub-dimensions, spanning both real-world and game-world image editing tasks (Sec. 3.2). Reliable dual-reference evaluation. For each sample assessment, we introduce both the textual reference and ground-truth (GT) image reference. This multi-modal reference enables vision-language model (VLM) evaluators to perform direct and fine-grained comparisons at both the textual and visual levels with the generated images, leading to more reliable evaluation (Sec. 3.3). Scalable multi-scenario data synthesis. We propose an automatic data synthesis pipeline with distinct generation strategies tailored for real-world and game-world scenarios (Sec. 3.4). 3.2. Evaluation Dimensions In real-life applications, image editing scenarios often involve diverse requirements spanning both real-world and game-world contexts, where complex contextual understanding and implicit reasoning capabilities are crucial for accurate image edits. Therefore, UniREditBench organizes reasoning-based image editing tasks into scenarioto-category hierarchy framework. As illustrated in Fig. 1, it covers both real-world and game-world scenarios across 8 primary dimensions and 18 sub-categories, each representing unique visual reasoning challenge with 150 humaninspected examples. We will elaborate on each dimension in the following. 3.2.1. Real-World Scenarios Real-world scenarios involve editing tasks that reflect the perceptual and interaction dynamics commonly observed in natural environments. These tasks may involve transformations of individual objects or complex interactions among multiple objects. To handle such tasks, models must capture the semantic, physical, and temporal characteristics of objects, as well as their relationships. 1. Single-Object Transformation targets variations intrinsic to an individual object, including viewpoint and attribute changes that do not disrupt spatial relationships within the scene: Viewpoint Transformation: Altering the perspective or viewing angle to exhibit alternative views of the same object (e.g., side, top-down, close-up). Pose Adjustment: Modifying the articulation or positioning of an objects parts, such as limb configurations or postural shifts. Temporal Evolution: Simulating natural progressions over time like aging, decay, or seasonal changes impacting the objects appearance. Material Modification: Changing inherent surface or material properties (e.g., color, texture) while preserving geometry and location. 2. Multi-Object Interaction involves mutual influences and state changes arising from the physical or spatial interactions among multiple objects: Structural Integrity Change: Physical deformations resulting from forces or collisions. Motion State Change: Dynamics induced by contact or force transmission leading to altered movement or posture. Mechanical Reaction: State transitions caused by device operation or functional interactions. Medium Interaction: Changes mediated by substances or environmental factors that affect appearance or state. Figure 5. Statistic visualization. We visualize (a) word clouds and (b) data distribition of our UniREdit-Data-100K. Spatial Arrangement: Reorganization or repositioning of multiple objects within the scene. 3.2.2. Game-World Scenarios Game-world scenarios consist of tasks within synthetic environments governed by human-defined rules, evaluating logical, strategic, spatial, and long-horizon reasoning capabilities. These tasks require models to plan, deduce, and act in accordance with the explicit rules that govern the environment. Long-Horizon Planning requires multi-step sequential reasoning to accomplish distant goals, exemplified by navigation or puzzle games such as Maze-solving and Sokoban. Logical Puzzle Solving involves constraint satisfaction and symbolic inference to produce valid solutions under formal rule sets, including Sudoku, Tic-Tac-Toe, and Word Search. Strategic Reasoning requires resource management, adversarial planning over time, modeled after games like Pacman, Jewel2, and Space Invader. Spatial Intelligence focuses on geometric and topological reasoning within 3D environments, such as reconstructing spatial layouts in gaming contexts. Representative examples are provided in Figs. 1 and 4 to illustrate the scope and diversity of evaluation dimensions, and highlight the complexity and variety of tasks in our benchmark. 3.3. Dual-Reference Evaluation Evaluating reasoning-based image editing is intrinsically challenging due to the need for the evaluator to accurately understand the implicit reasoning intentions within the instruction. To achieve reliable and comprehensive assessments, we introduce VLM-based multi-dimensional scorFigure 6. Qualitative editing result comparison. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models. ing schema, leveraging both textual and visual evaluation references. Specifically, for each sample, this pipeline evaluates three core dimensions: Instruction Following measures how accurately the generated image reflects the input instruction, focusing on whether explicit effect of the edit is properly manifested. Here, the VLM compares the output image against both the textual reference of edited effect Rt and the corresponding reference GT image Ri to verify compliance: SIF = VLM(O, I, G, Ri, Rt) where represents the original image, denotes the editing instruction. Visual Consistency assesses the preservation of image regions and attributes unrelated to the edit instruction, ensuring that changes are localized and do not inadvertently alter irrelevant scene elements. This criterion favors models capable of accurate, fine-grained editing rather than wholesale regeneration: SVC = VLM(O, I, G) Visual Quality evaluates the realism of the generated output, checking for artifacts, distortions, and physical or logical implausibility in the final image: SVQ = VLM(G) We choose GPT-4.1 [13] as VLM evaluator. Each score ranges from 1 to 5, following prior detailed scoring guidelines [41, 51]. Finally, the overall evaluation score aggregates these via weighted sum: SOverall = α1SIF + α2SVC + α3SVQ, where α1 = 0.5, α2 = 0.3, and α3 = 0.2. This setting prioritizes instruction following to emphasize the importance of accurately adhering to the instructions intent, and also incorporates visual consistency and quality, ensuring that areas unrelated to the instruction are preserved and that the overall image quality is maintained. 3.4. Multi-Scenario Data Synthesis Given the distinct characteristics of realand game-world contexts, we develop specialized data generation process for each scenario, as illustrated in Fig. 3. Detailed elaboration of each data synthesis process is provided below. For Real-World Scenario, we employ text-thenimage data generation strategy. Specifically, (1) this process begins with hand-crafted textual triples that describe the original image, the editing instruction, and the textual reference of edited effect (a reasoning-based narrative of the anticipated outcome). Next, we use the powerful VLM [5], to expand this initial set into large corpus of text triples. Subsequently, (2) these curated textual triples are input to GPT-4o [13] to synthesize the original and edited images in alignment with the described textual reference of edited effect. (3) Finally, in the quality filtering stage, VLM [5] is used to assess the generated images based on visual fidelity, instruction alignment, and potential hallucination risks. Additionally, it generates reasoning chain-of-thought (CoT) text for each qualified instance, ensuring the production of high-quality, reasoning-based image editing training data. In Game-World Scenario, game states are inherently well-suited to be represented as structured reasoning-based editing data, where instructions can naturally be solved usTable 2. In-domain quantitative comparisons on UniREditBench. GPT-4.1 is used as the evaluator. Best scores are in bold."
        },
        {
            "title": "Physical",
            "content": "Avg."
        },
        {
            "title": "Spatial",
            "content": "Strategic Long-Horizon Logic Puzzle"
        },
        {
            "title": "Solving",
            "content": "Avg. Overall FLUX-Kontext-Pro Seedream4.0 Wan2.5 Nano Banana GPT-4o MagicBrush Omnigen2 Lumina-DiMOO Step1X-Edit Bagel-Think DreamOmni2 UniWorld-V2 Qwen-Image-Edit UniREdit-Bagel (Ours) 47.35 67.98 74.66 77.10 82.67 43.97 55.47 52.84 59.69 61.45 63.52 72.96 75.68 76.73 47.16 72.93 69.74 78.88 84. 46.09 58.27 52.31 56.36 59.51 58.35 69.37 73.03 77.80 Closed-source Models 41.44 59.49 62.87 74.70 77.40 45.00 66.22 67.23 75.22 81.01 49.12 38.87 63.73 66.74 77.73 Open-source Models 46.64 50.70 50.83 54.84 55.68 54.02 61.69 64.67 71.44 44.69 53.69 51.44 55.93 56.80 56.64 66.55 70.95 75.74 63.58 70.28 61.23 65.68 66.29 72.42 49.27 56.73 84.90 44.37 65.07 63.51 71.86 79.81 42.93 51.44 50.31 53.85 52.65 52.68 63.65 70.59 76.57 48.58 42.16 47.13 56.11 51. 33.59 27.50 36.96 34.89 43.23 42.17 40.00 36.63 72.83 51.16 44.09 55.00 56.83 67.61 30.72 36.25 39.57 47.01 43.00 48.80 53.41 48.80 84.88 40.49 51.65 54.93 64.91 63.79 35.31 24.31 53.09 43.89 41.30 48.09 37.53 37.68 83.72 46.52 45.38 52.67 60.39 62. 45.77 55.77 61.36 68.26 73.39 36.85 33.14 45.61 44.00 45.10 48.98 43.19 41.92 80.48 40.77 43.41 48.54 50.15 50.96 52.81 54.87 56.52 78.15 ing Python code. Inspired by Game-RL [27], (1) we first design diverse collection of game-based problems and develop corresponding Python programs tailored to each cat- (2) Then, these programs automatically generate egory. paired original and edited images, along with instructions, textual reference effects, and programmatic CoT reasoning (3) To bridge the gap between programmatic and traces. natural language CoT reasoning formats, we use VLMs to convert these reasoning traces into explanations that align with human inference patterns. Finally, quality filtering are applied to ensure the integrity and reliability of the data. Overall, this multi-scenario data synthesis pipeline generates UniREditBench, unified reasoning-based image editing benchmark, and UniREdit-Data-100K, largescale dataset with high-quality CoT annotations. We detail the elaboration of this dataset in the next section. 4. UniREdit-Data-100K To enhance the capability of current generative models on reasoning-driven image editing, we propose UniREditData-100K, which contains 100,421 samples spanning 8 reasoning dimensions and 18 categories defined in Sec. 3.2. 4.1. Statistical Analysis UniREdit-Data-100K is designed with an emphasis on balance and diversity, ensuring that each reasoning category contains over 4,000 instances to effectively support model training across wide range of editing tasks. It is divided into two primary scenarios: (i) Real-World Scenario, which captures natural object attributes and complex multiobject interactions, and (ii) Game-World Scenario, presenting structured, rule-based editing challenges, such as puzzles and strategic planning games. We visualize the word cloud for both real-world and game-world subsets in Fig. 5 (a) and the detailed distribution of samples across different categories in Fig. 5 (b). These visualizations highlight the extensive vocabulary as well as the broad coverage across various categories of our dataset. 4.2. UniREdit-Bagel To further validate the effectiveness of our dataset, we use it to fine-tune Bagel [6], unified understanding and generative model. Specifically, each training sample consists of the input image O, an editing instruction I, stepwise CoT text that grounds the edit effects step by step, and the target edited image G. During training, the original image and instruction are first input into the model, which then generates textual reasoning trace and synthesizes the edited image. We supervise both the textual reasoning trace and the visual edit. Formally, for reasoning text supervision, we minimize the negative log-likelihood: Ltext = (cid:88) log pθ (cid:0)yt y<t, O, I(cid:1). For image generation, we supervise the latent flowmatching loss [20] between the VAE latents of and G, conditioned on (O, I, C): Limg = EtU (0,1) (cid:13) (cid:13)uθ(zt, ; O, I, C) u(zt, t)(cid:13) 2 2, (cid:13) where uθ is the learned time-conditioned velocity field on the latent path from zO to zG, and is the target velocity. Finally, the overall objective is = λtextLtext + λimgLimg. Under the influence of Ltext, the model enhances its reasoning ability through explicit CoT learning, which effectively guides the accurate image generation, while Limg improves both the correctness and fidelity of the edited image. 5. Experiment 5.1. Implementation Details [39], Step1X-Edit Baselines. We benchmark closed-source models including: GPT-4o [13], Nano Banana [7], Gemini-2.0 [5], Seedream4.0 [24], Wan 2.5 [28], and FLUX-Kontext-Pro [2], as well as open-source models including: Bagel [6], QwenImage-Edit [21], FLUX.1-Kontextdev [2], Emu2 [26], Omnigen2 [40], Omnigen [42], HiDream-Edit [4], MagicBrush [49], and AnyEdit [47]. Training and Evaluation. We train all Bagel [6] components, except the VAE, for 5,000 iterations on UniREditData-100K using the Adam optimizer and cosine learningrate schedule with 500 warm-up steps, peak learning rate of 2105, and minimum learning rate of 106. The loss weights are set to λtext = 2 and λimg = 1. During inference, we use the official inference settings provided by Bagel. To ensure fair comparisons with other baselines, we adopt the original inference configurations of these models. 5.2. Benchmarking Results on UniREditBench As shown in Tab. 2, among closed-source models, GPT-4o achieves the highest average performance across all scenarios, with Nano Banana performing comparably. Wan2.5 delivers balanced results on real-world tasks but lags on game scenarios that require strategic reasoning. Besides, Seedream4.0 is competitive on structure transform yet encounters challenges in game scenarios. Among opensource baselines, Qwen-Image-Edit performs strongly on real-world tasks such as attribute modification and structure transform. However, most models remain comparatively weak on game scenarios like Strategic Reasoning. Overall, compared with open-source methods, closed-source models, particularly GPT-4o, maintain clear advantage. While some open-source models are competitive on specific realworld tasks, they generally struggle with complex reasoning in game scenarios. Notably, only GPT-4o and Nano Banana achieve an average score greater than 60 on game scenarios, underscoring that this setting remains highly challenging and serves as useful test for current models. 5.3. Comparison Results of UniREdit-Bagel Quantitative. UniREdit-Bagel achieves the best overall performance among all closedand open-source models on UniREditBench, surpassing the second-place GPT-4o by substantial margin. The largest gains occur in game-world scenarios (+17.08), indicating exceptional capability of understanding and processing complex reasoning image editTable 3. Out-of-distribution quantitative performance comparison on RISEBench [51]. GPT-4.1 is used as the evaluator. Best scores are in bold."
        },
        {
            "title": "Overall",
            "content": "Closed-source Models Gemini-2.0-Flash-pre Seedream-4.0 Gemini-2.0-Flash-exp GPT-4o Nano Banana 10.6% 12.9% 8.2% 34.1% 25.9% 2.3% 11% 13.3% 7.1% 11.0% 12.2% 23.0% 4.7% 15.5% 37.0% 10.6% 32.2% 47.8% 37.0% 18.8% Open-source Models HiDream-Edit OmniGen Step1X-Edit Bagel FLUX.1-Kontext-Dev Qwen-Image-Edit Bagel-Think UniREdit-Bagel (Ours) 0.0% 1.2% 0.0% 3.5% 2.3% 4.7% 4.7% 22.4% 0.0% 0.0% 0.0% 1.2% 0.0% 1.0% 3.5% 2% 2.2% 5.9% 9.0% 4.4% 1.2% 13.0% 5.5% 2.4% 17.0% 10.0% 15.5% 1.2% 14.0% 18.9% 21.0% 10.6% 9.4% 10.8% 13.3% 28.9% 32.8% 0.0% 0.8% 1.9% 5.8% 5.8% 8.9% 9.2% 18.3% ing tasks. In out-of-distribution performance comparison, UniREdit-Bagel achieves the strongest open-source results across all four categories on RISEBench, shown in Tab. 3, improving upon the Bagel-Think baseline by 9.1 points and surpassing the closed-source Gemini-2.0-Flash-exp by 5.0 points. It also remains competitive with top closed-source models like Nano Banana and GPT-4o, narrowing the gap between openand closed-source models. Qualitative. The qualitative results presented in Fig. 6 highlight the strengths of our UniREdit-Bagel across various tasks. Specifically, in Fig. 6 (Row 4), most models fail to reliably reproduce the physical heat effect. Although several baselines, such as Nano Banana and QwenImage-Edit, successfully capture the heat-induced warping of plastic bottle under sustained heat gun exposure, they fail to preserve the heat trace. Notably, UniREdit-Bagel not only renders the deformation accurately but also preserves the heat trace, offering superior visual consistency. Besides, in the Sokoban and Maze game settings (rows 1 and 3), Seedream-4.0, Nano Banana, and Wan-2.5 generally preserve instruction-irrelevant content but struggle with instruction-specific objectives. In contrast, UniREdit-Bagel excels in both fulfilling the instruction and maintaining the coherence of unrelated content. 6. Conclusion This paper presents UniREditBench, unified reasoningbased benchmark for image editing with broad dimension coverage and robust dual-reference evaluation protocol. We further introduce multi-scenario synthesis pipeline and release UniREdit-Data-100K, large-scale dataset with high-quality CoT annotations. To demonstrate its effectiveness, we fine-tune Bagel on this dataset and yields UniREdit-Bagel, which achieves substantial quantitative and qualitative gains. Comprehensive benchmarking of open-source and closed-source image editing models reveals their strengths and weaknesses across various aspects."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In CVPR, pages 1820818218, 2022. 3 [2] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li, Dominik Lorenz, Jonas Muller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith. Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025. 1, 8 [3] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. 1, 3 [4] Qi Cai, Jingwen Chen, Yang Chen, Yehao Li, Fuchen Long, Yingwei Pan, Zhaofan Qiu, Yiheng Zhang, Fengbin Gao, Peihan Xu, et al. Hidream-i1: high-efficient image generative foundation model with sparse diffusion transformer. arXiv preprint arXiv:2505.22705, 2025. 8 [5] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 8 [6] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 1, 3, 7, [7] Google. Introducing gemini 2.5 flash image, our statehttps : / / developers . image model. of-the-art googleblog.com/en/introducinggemini25-flash-image/, 2025. 1, 8 [8] Feng Han, Yang Jiao, Shaoxiang Chen, Junhao Xu, Jingjing Chen, and Yu-Gang Jiang. Controlthinker: Unveiling latent semantics for controllable image generation through visual reasoning. arXiv preprint arXiv:2506.03596, 2025. 3 [9] Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, and Jiangning Zhang. Reasoning to edit: Hypothetical instruction-based image editing with visual reasoning. arXiv preprint arXiv:2507.01908, 2025. 2 [10] Runze He, Kai Ma, Linjiang Huang, Shaofei Huang, Jialin Gao, Xiaoming Wei, Jiao Dai, Jizhong Han, and Si Liu. Freeedit: Mask-free reference-based image editing with multi-modal instruction. arXiv preprint arXiv:2409.18071, 2024. 3 [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [12] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In CVPR, pages 83628371, 2024. 2 [13] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 1, 3, 6, 8 [14] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In CVPR, pages 60076017, 2023. 3 [15] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In CVPR, pages 24262435, 2022. 3 [16] Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS, 36:3665236663, 2023. 3 [17] Chenglin Li, Qianglong Chen, Zhi Li, Feng Tao, and Yin Zhang. Vcbench: controllable benchmark for symbolic and abstract challenges in video cognition. arXiv preprint arXiv:2411.09105, 2024. [18] Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Xiaojuan Qi, and Fuli Feng. Easier painting than thinking: Can text-to-image models set the stage, but not direct the play? arXiv preprint arXiv:2509.03516, 2025. 3 [19] Zongjian Li, Zheyuan Liu, Qihui Zhang, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Yang Ye, Wangbo Yu, Yuwei Niu, and Li Yuan. Uniworld-v2: Reinforce image editing with diffusion negative-aware finetuning and mllm implicit feedback. arXiv preprint arXiv:2510.16888, 2025. 1 [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. 7 [21] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 1, 2, 8 [22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 3 [23] Yuwei Niu, Munan Ning, Mengren Zheng, Weiyang Jin, Bin Lin, Peng Jin, Jiaqi Liao, Chaoran Feng, Kunpeng Ning, Bin Zhu, et al. Wise: world knowledge-informed semantic evaluation for text-to-image generation. arXiv preprint arXiv:2503.07265, 2025. [24] ByteDance Seed. Seedream 4.0. https : / / seed . bytedance.com/en/seedream4_0, 2025. 8 [25] Kaiyue Sun, Rongyao Fang, Chengqi Duan, Xian T2i-reasonbench: Benchmarking Liu, and Xihui Liu. reasoning-informed text-to-image generation. arXiv preprint arXiv:2508.17472, 2025. 3 [26] Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In CVPR, pages 1439814409, 2024. [41] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 2, 3, 6 [42] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In CVPR, pages 1329413304, 2025. 8 [43] Yicheng Xiao, Lin Song, Yukang Chen, Yingmin Luo, Yuxin Chen, Yukang Gan, Wei Huang, Xiu Li, Xiaojuan Qi, and Ying Shan. Mindomni: Unleashing reasoning generation in vision language models with rgpo. arXiv preprint arXiv:2505.13031, 2025. 1 [44] Yi Xin, Qi Qin, Siqi Luo, Kaiwen Zhu, Juncheng Yan, Yan Tai, Jiayi Lei, Yuewen Cao, Keqi Wang, Yibin Wang, et al. Lumina-dimoo: An omni diffusion large language model for multi-modal generation and understanding. arXiv preprint arXiv:2510.06308, 2025. 1, 3 [45] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for textto-image generation. NeurIPS, 36:1590315935, 2023. 3 [46] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. [47] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified In CVPR, pages high-quality image editing for any idea. 2612526135, 2025. 1, 8 [48] Dong Zhang, Lingfeng He, Rui Yan, Fei Shen, and Jinhui Tang. R-genie: Reasoning-guided generative image editing. arXiv preprint arXiv:2505.17768, 2025. 2 [49] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. NeurIPS, 36:3142831449, 2023. 1, 3, 8 [50] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In CVPR, pages 90269036, 2024. 3 [51] Xiangyu Zhao, Peiyuan Zhang, Kexian Tang, Xiaorong Zhu, Hao Li, Wenhao Chai, Zicheng Zhang, Renqiu Xia, Guangtao Zhai, Junchi Yan, et al. Envisioning beyond the pixels: Benchmarking reasoning-informed visual editing. arXiv preprint arXiv:2504.02826, 2025. 2, 3, 6, 8 [27] Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, et al. Code2logic: Game-code-driven data synthesis for enhancing vlms general reasoning. arXiv preprint arXiv:2505.13886, 2025. 2, [28] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 8 [29] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 3 [30] Yibin Wang, Changhai Zhou, and Honghui Xu. Enhancing object coherence in layout-to-image synthesis. arXiv preprint arXiv:2311.10522, 2023. 3 [31] Yibin Wang, Zhiyu Tan, Junyan Wang, Xiaomeng Yang, Cheng Jin, and Hao Li. Lift: Leveraging human feedarXiv preprint back for text-to-video model alignment. arXiv:2412.04814, 2024. 1 [32] Yibin Wang, Weizhong Zhang, and Cheng Jin. Magicface: Training-free universal-style human image customized synthesis. arXiv preprint arXiv:2408.07433, 2024. 3 [33] Yibin Wang, Weizhong Zhang, Jianwei Zheng, and Cheng Jin. Primecomposer: Faster progressively combined diffusion for image composition with attention steering. In ACM MM, pages 1082410832, 2024. 1, 3 [34] Yibin Wang, Zhimin Li, Yuhang Zang, Jiazi Bu, Yujie Zhou, Yi Xin, Junjun He, Chunyu Wang, Qinglin Lu, Cheng Jin, et al. Unigenbench++: unified semantic evaluation benchmark for text-to-image generation. arXiv preprint arXiv:2510.18701, 2025. [35] Yibin Wang, Zhimin Li, Yuhang Zang, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Unified multimodal chain-of-thought reward model through reinforcement finetuning. arXiv preprint arXiv:2505.03318, 2025. 3 [36] Yibin Wang, Zhimin Li, Yuhang Zang, Yujie Zhou, Jiazi Bu, Chunyu Wang, Qinglin Lu, Cheng Jin, and Jiaqi Wang. Pref-grpo: Pairwise preference reward-based grpo for staarXiv preprint ble text-to-image reinforcement learning. arXiv:2508.20751, 2025. 1 [37] Yibin Wang, Yuhang Zang, Hao Li, Cheng Jin, and Jiaqi Wang. Unified reward model for multimodal understanding and generation. arXiv preprint arXiv:2503.05236, 2025. 3 [38] Yibin Wang, Weizhong Zhang, Honghui Xu, and Cheng Jin. In CVPR, Dreamtext: High fidelity scene text synthesis. pages 2855528563, 2025. 1, 3 [39] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. [40] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 1, 3, 8 UniREditBench: Unified Reasoning-based Image Editing Benchmark"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Data Filtering We design comprehensive, multi-stage pipeline that performs data filtering, i.e., instruction de-duplication, quality filtering, and human inspection, to remove redundancy and low-quality data. Detailed elaboration of the filtering pipeline is provided below. A.1. Instruction De-duplication During the first stage of the real-world scenario, text prompt for image editing are sampled from the Gemini-2.5-Pro, which may potentially introduce repeated or near-duplicate entries. We remove redundancy along two aspects: exact matches and semantic similarity. Exact-Match Deduplication: We first normalize the original image description by converting it to lowercase and removing punctuation. Afterward, we extract the set of words from the normalized text. If two samples contain identical word sets, they are considered duplicates, as the descriptions are effectively the same. These duplicate samples are then filtered out to ensure data diversity. Semantic-Similarity Deduplication: We use sentencetransformers model to extract sentence embeddings for both the original image description and edit instruction. We then compute the pairwise similarity between these embeddings. If the similarity score exceeds threshold of 0.7 for either description and instruction, the samples are deemed semantically redundant and are filtered out to enhance dataset diversity. These complementary exact-match and semantic filters improve dataset diversity by eliminating both literal and paraphrastic duplicates. A.2. Quality Filtering To ensure the quality of both the generated text and images, we evaluate and filter them across six key dimensions: text hallucination, instruction adherence, content preservation, visual quality, image hallucination, and CoT quality. Scores for each dimension are assigned by the Gemini-2.5-Pro on 15 scale. Only samples that achieve the maximum score across all dimensions are retained. Text Hallucination: We evaluate the textual reference for hallucinated content, defined as entities or visual effects that are not mentioned in the Instruction or that cannot be plausibly induced by the given Instruction. Instruction Following: We compare the edited image with the textual reference to assess whether the generated visual changes accurately reflect the specified efFigure 7. Benchmarking result visualization. (a) Closed-source model comparison; (b) Open-source model comparison. Table 4. Quantitative comparisons on KRISBench. GPT-4.1 is used as the evaluator. Best scores are in bold while second-best is underlined."
        },
        {
            "title": "Spatial",
            "content": "Doubao Step 3ϕ vision Gemini-2.0 GPT-4o MagicBrush AnyEdit Emu2 Step1X-Edit Bagel-Think UniREdit-Bagel (Ours) 70.92 69.67 66.33 83.17 53.92 47.67 51.50 55.50 69.27 71.75 Closed-source Models 59.17 61.08 63.33 79. 39.58 45.17 48.83 51.75 67.58 71.00 40.58 63.25 63.92 68.25 65.50 66.88 68.19 85.50 Open-source Models - - 22.17 - - - 42.94 38.56 34.69 44.69 65.00 69. 61.19 60.88 56.94 80.06 38.06 42.94 38.44 49.06 62.11 65.99 47.75 49.06 54.13 71.56 30.00 36.56 24.81 40.88 47.33 59.91 60.58 54.92 71.67 85.08 23.08 26.92 45.00 22.75 49.22 51. 60.70 61.43 62.41 80.09 37.15 38.55 39.70 43.29 60.77 65.45 fects. Samples that demonstrate poor adherence to the instructions and text reference are discarded. Content Preservation: We assess whether regions unrelated to the edit instruction, such as the background, remain consistent between the original and edited images, ensuring stability in unaffected areas. Visual Quality: We assess whether the generated images meet fundamental quality standards, specifically by ensuring they are free from artifacts or degradation. Image Hallucination: We examine the edited images for any unintended additions or alterations beyond the specified textual reference, such as the appearance of additional objects. CoT Quality: We evaluate the correctness of the chainof-thought (CoT) reasoning text, focusing on whether the analysis of the original image and instruction is logical and sound. A.3. Human Inspection In addition to our automated filtering pipeline, we perform final manual check of each data instance. To facilitate this, we developed two web-based interfaces and enlisted eight expert annotators to carry out two-stage filtering process: Initial Filtering: Annotators remove samples with extremely erroneous textual references or substandard generated images. Table 5. Detailed in-domain quantitative comparisons on UniREditBench. GPT-4.1 is used as the evaluator. Best scores are in bold. Real World Scenario Game World Scenario Model Attribute Modification Structure Transform Physical Interaction Property Response Spatial Intelligence Strategic Reason Logic Puzzle Solving Long-Horizon Plan Viewpoint Transformation Material Modification Pose Adjustment Temporal Evolution Structural Integrity Change Motion State Change Spatial Arrangement Mechanical Reaction Medium Interaction 3D Reconstruction Space Invader Jewel2 Pacman Word Search Tictactoe Sudoku Maze Sokoban FLUX-Kontext-Pro Seedream4.0 Wan2.5 Nano Banana GPT-4o MagicBrush Omnigen2 Lumina-DiMOO Step-1X-Edit Bagel-Think DreamOmni2 UniWorld-V2 Qwen-Image-Edit UniREdit-Bagel (Ours) 37.55 64.18 73.78 75.82 83.83 36.74 51.25 54.22 52.57 59.05 61.22 71.45 74.60 81.08 57.15 71.78 75.54 78.39 81.52 51.20 59.68 51.47 66.82 63.84 65.82 74.47 76.76 72.39 56.79 79.98 79.95 86.12 92. 47.75 68.25 54.40 62.15 63.52 67.07 83.19 81.45 84.78 37.52 65.87 59.53 71.63 77.33 44.43 48.28 50.23 50.58 55.49 49.63 55.55 64.62 70.81 41.46 56.10 62.67 71.37 75.86 44.05 48.37 48.67 53.07 54.05 49.58 58.53 66.48 73.85 44.62 61.60 64.38 71.07 74. 47.06 54.48 52.62 58.68 55.62 54.83 65.47 68.78 72.73 Closed-source Models 47.03 77.52 63.49 73.14 88.60 40.56 59.15 60.83 77.58 78.98 Open-source Models 37.67 51.48 49.65 49.80 48.27 53.62 66.95 76.50 83. 45.87 50.77 51.63 54.23 55.03 54.11 62.20 64.65 71.27 42.32 59.83 64.91 71.82 75.81 47.42 50.64 50.03 55.45 56.33 53.92 61.17 64.69 71.62 49.12 38.87 63.73 66.74 77.73 63.58 70.28 61.23 65.68 66.29 72.42 49.27 56.73 84.90 53.18 45.47 43.44 59.97 58. 45.33 51.33 48.23 35.05 45.48 38.78 34.44 38.42 87.77 38.12 40.87 56.57 54.33 45.60 25.60 1.38 36.29 36.38 42.89 42.33 42.90 41.02 57.68 54.43 40.13 41.38 54.04 49.75 29.83 29.80 26.35 33.25 41.33 45.40 42.65 30.45 73.05 32.45 48.05 58.38 64.13 64. 33.92 22.87 41.73 45.49 48.42 45.86 49.67 48.52 86.80 61.23 38.17 58.57 39.80 58.97 31.45 44.97 65.07 46.97 42.20 46.98 34.73 33.60 71.16 27.80 68.72 47.86 90.81 67.78 48.45 55.27 64.03 62.50 83.73 40.55 5.10 52.47 39.20 33.27 51.42 28.20 30.93 93. 33.20 39.57 43.57 55.94 48.62 56.55 71.59 61.38 98.30 53.87 32.90 45.97 51.15 51.49 28.23 32.93 35.57 38.08 37.38 41.05 35.22 36.21 71.47 Overall 45.77 55.77 61.36 68.26 73.39 40.77 43.41 48.54 50.15 50.96 52.81 54.87 56.52 78. E. Effect of Training Set Size on Overall Score We provide overall performance comparison on UniREditBench of models trained on difference size of UniREditData, which is visualized in Fig. 8. F. Ethical statement In this work, we affirm our commitment to ethical research practices and responsible innovation. To the best of our knowledge, this study does not involve any data, methodologies, or applications that raise ethical concerns. All experiments and analyses were conducted in compliance with established ethical guidelines, ensuring the integrity and transparency of our research process. Figure 8. Overall performance on UniREditBench of models trained on different size of UniREdit-Data. Manual Correction: Annotators make refinements to the textual reference effect that are only slightly incorrect, ensuring alignment and accuracy. Two web interfaces are shown in Figs. 12 and 13. B. Detailed Benchmarking Results We provide detailed benchmarking results on our UniREditBench for each category in Tab. 5. C. More Quantitative Results We provide more quantitative out-of-distribution performance comparisons on KRISBench in Tab. 4. D. More Qualitative Comparison Results We provide additional qualitative comparisons on UniREditBench in Fig. 9 and 10, and comparisons on RISEBench in Fig. 11. Figure 9. Qualitative editing result comparison on UniREditBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models. Figure 10. Qualitative editing result comparison on UniREditBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models. Figure 11. Qualitative editing result comparison on RISEBench. Our UniREdit-Bagel demonstrates significant superiority in both instruction following and visual quality compared with state-of-the-art closed-sourced and open-sourced models. Figure 12. Web interface of the initial filtering stage. Figure 13. Web interface of the manual correction stage."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Innovation Institute",
        "UC Berkeley",
        "Zhejiang University"
    ]
}