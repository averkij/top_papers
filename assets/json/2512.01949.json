{
    "paper_title": "Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models",
    "authors": [
        "Zhongyu Yang",
        "Dannong Xu",
        "Wei Pang",
        "Yingfang Yuan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is a technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, a plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: a graph-structured pruning module that removes visually redundant tokens, and a query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT-7B, it achieves up to 6.8x prefill speedup and 10x FLOP reduction, while retaining 96.88% of the original performance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 9 4 9 1 0 . 2 1 5 2 : r Published in Transactions on Machine Learning Research (11/2025) Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models Zhongyu Yang BCML, Heriot-Watt University Dannong Xu BCML, Heriot-Watt University Wei Pang BCML, Heriot-Watt University Yingfang Yuan BCML, Heriot-Watt University zy4028@hw.ac.uk danielxu0208@gmail.com w.pang@hw.ac.uk y.yuan@hw.ac.uk Reviewed on OpenReview: https: // openreview. net/ forum? id= F6xKzbgcHq Figure 1: Comparison of different token pruning methods. Attention-based and similarity-based methods prune tokens using attention scores and similarity scores, respectively. In contrast, divergencebased methods detect changes in model performance and retain tokens that cause minimal impact. Script (Graph-Structured and QueRy-CondItioned Token Pruning) combines graph-structured reduction of visual redundancy and query-conditioned semantic token selection to enable efficient pruning in MLLMs. In this example, Script successfully preserves key visual cues, such as the silver pot on the stove, the pineapple beside the limes, and the flowers on the table. Other methods fail to retain consistently."
        },
        {
            "title": "Abstract",
            "content": "The rapid growth of visual tokens in multimodal large language models (MLLMs) leads to excessive memory consumption and inference latency, especially when handling high-resolution images and videos. Token pruning is technique used to mitigate this issue by removing redundancy, but existing methods often ignore relevance to the user query or suffer from the 1 Equal contribution 2 Correspond Author 1 Published in Transactions on Machine Learning Research (11/2025) limitations of attention mechanisms, reducing their adaptability and effectiveness. To address these challenges, we propose Script, plug-and-play pruning method that requires no retraining and generalizes across diverse MLLMs. Script comprises two modules: graphstructured pruning module that removes visually redundant tokens, and query-conditioned semantic pruning module that preserves query-relevant visual information. Together, they enhance performance on multimodal tasks. Experiments on fourteen benchmarks across image and video understanding tasks show that Script consistently achieves higher model efficiency and predictive accuracy compared to existing pruning methods. On LLaVA-NeXT7B, it achieves up to 6.8 prefill speedup and 10 FLOP reduction, while retaining 96.88% of the original performance."
        },
        {
            "title": "1 Introduction",
            "content": "Recent advances in large language models (LLMs) (Touvron et al., 2023; Bai et al., 2023; Yang et al., 2024; 2025a) have substantially enhanced language understanding and reasoning, forming the backbone of general-purpose multimodal systems, including vision-language models and embodied agents operating in real-world scenarios. Building on this progress, multimodal large language models (MLLMs) (Li et al., 2024a; Chen et al., 2024d;c; Zhang et al., 2025b;a; Yang et al., 2025e) extend LLMs by integrating vision encoders, enabling joint reasoning over both visual and textual modalities. This integration empowers MLLMs to excel at diverse vision-language tasks such as visual question answering and image captioning. However, deploying MLLMs in practical scenarios such as multi-agent systems or interactive assistants is constrained by the high computational cost of handling high-resolution or temporally extended visual inputs (Yang et al., 2025c;d; Zhao et al., 2024). Patch-based vision encoders typically tokenize each visual input into hundreds or even thousands of tokens (Luo et al., 2024; Guo et al., 2024; Zhang et al., 2023; Chen et al., 2024b; Li et al., 2025c; Liu et al., 2025). In contrast to compact textual inputs, visual representations often contain hundreds or thousands of tokens, which increases memory usage and inference latency. The explosion of vision tokens, compounded by the quadratic complexity of attention mechanisms (Vaswani et al., 2017), creates significant bottleneck for latencyand memory-sensitive scenarios such as mobile deployment, online inference, and edge-based vision-language applications. To mitigate this inefficiency, visual token pruning has emerged as promising strategy Li et al. (2025a). An effective pruning method should minimize computational cost while preserving task performance within resource-constrained environments. Existing visual token pruning methods can be broadly categorized into three main paradigms: (1) attentionbased methods, which retain tokens with high model-assigned importance, commonly referred to as attention scores (Chen et al., 2024a; Xing et al., 2025; Zhang et al., 2024c); (2) similarity-based methods, which identify and eliminate redundant visual tokens based on feature similarity (Bolya et al., 2023; Zhang et al., 2024b; Wen et al., 2025b; Jeddi et al., 2025); and (3) divergence-minimization methods, which prune tokens by minimizing the change in the models output (Alvar et al., 2025; Ye et al., 2025). However, existing pruning methods face two fundamental challenges in query-conditioned scenarios. First, attention-based approaches depend on raw attention scores, which are susceptible to the attention sink issue (Barbero et al., 2025), often overlooking critical tokens. Moreover, assigning similar scores to adjacent or semantically similar tokens can reduce pruning efficiency (Yang et al., 2025b). As shown in Figure 1, such methods fail to preserve the token representing the flower, despite its clear relevance to the query. Second, similarityand divergence-based methods lack explicit query conditioning (Alvar et al., 2025; Li et al., 2025b). Although they address visual Figure 2: (a) Efficiency Analysis on LLaVA-NeXT-7B under 88.9% reduction. (b) Comparison with other baselines on LLaVA-1.5-7B under 94.4% reduction. Published in Transactions on Machine Learning Research (11/2025) redundancy or output stability, they generate fixed token subsets regardless of the input query, which can lead to the omission of query-referenced objects, such as the pineapple and lime shown in Figure 1. To address these limitations, we propose Script, training-free and architecture-agnostic token pruning approach that combines graph-structured pruning (GSP) and query-conditioned semantic pruning (QCSP). On one hand, we build bipartite graph to structure visual token redundancy, effectively identifying redundant tokens with lower computational cost. On the other hand, to eliminate reliance on attention scores and avoid issues such as attention sink, Script explicitly models interactions between the query and visual tokens, identifying query-relevant tokens using Determinantal Point Processes (DPP), probabilistic model that favors diverse and semantically meaningful subsets. Together, these two modules enable effective token pruning by jointly considering visual redundancy and input query relevance. As illustrated in Figure 2, Script trims 88.9% tokens yet preserves 99.88% accuracy on POPE, and consistently outperforms other baselines on 10 benchmarks. In summary, our contributions are as follows: We identify the core limitations of existing pruning paradigms in query-conditioned scenarios, highlighting their inability to adaptively preserve task-relevant content. We propose Script, training-free pruning method that combines graph-based redundancy reduction with query-aware token selection via DPP. Experiments across fourteen benchmarks demonstrate that Script consistently overperforms other baselines, and achieves up to 10 speedup while retaining 96.88% of original performance."
        },
        {
            "title": "2 Related Work",
            "content": "Scalability Challenges in MLLMs. MLLMs combine visual and textual modalities to support generalpurpose perception and reasoning (Alayrac et al., 2022; Zhu et al., 2024; Chen et al., 2023; Yu et al., 2024; Liu et al., 2024b; Cai et al., 2024). However, visual inputs introduce substantial token overhead due to spatial redundancy and high dimensionality, exacerbating the quadratic complexity of Transformer attention. For example, LLaVA-1.5 (Liu et al., 2023) generates 576 tokens from 336 336 image, which is already 45 longer than typical text-only prompts. High-resolution models like LLaVA-NeXT (Liu et al., 2024a) and video models like LongVA (Zhang et al., 2024a) push this further, producing tens or even hundreds of thousands of tokens per input, leading to untenable memory and compute demands for many downstream applications. Consequently, such models often encounter memory bottlenecks, latency spikes, or even inference failures, especially under real-time or edge deployment scenarios (Papa et al., 2023; Li et al., 2025d), where computational resources are inherently limited. These challenges have motivated growing body of work on token pruning, aiming to reduce input size while preserving task-relevant semantics and ideally without retraining or architectural changes. Token Pruning Strategies. Efforts to mitigate visual token overhead in MLLMs can be broadly categorized into four approaches, each with distinct assumptions and limitations. (1) Pre-fusion compression, which downsamples or selects tokens before vision-language fusion (Li et al., 2024b; Hu et al., 2024), often requires model retraining or structural modifications, hindering plug-and-play usage. (2) Attention-based pruning, which selects tokens using attention scores (Chen et al., 2024a; Ye et al., 2025), suffers from attention drift (Wen et al., 2025a), misguiding token importance, and limiting compatibility with optimized backends such as FlashAttention (Dao, 2024). (3) Pre-language fusion pruning, which drops tokens before language-level alignment (Shang et al., 2024; Song et al., 2025), is often tightly coupled to specific vision backbones, limiting transferability across architectures. (4) Similarity-based pruning, which eliminates redundant tokens using intra-image feature similarity (Wen et al., 2025b; Alvar et al., 2025), is generally efficient and model-agnostic, but often ignores query semantics, leading to the loss of task-relevant content. While existing approaches partially alleviate visual redundancy, few simultaneously achieve the trifecta of query relevance, architectural generality, and training-free deployment. In contrast, our proposed Script is explicitly query-aware, visual diversity, entirely model-agnostic, and operates without any additional training. 3 Published in Transactions on Machine Learning Research (11/2025) Figure 3: Token redundancy visualized via similarity and entropy on 10,000 COCO images."
        },
        {
            "title": "3 Preliminary",
            "content": "In this section, within the context of the task Text Visual Question Answering (TextVQA), we empirically investigate two key aspects of visual tokens in MLLMs: visual redundancy and query-conditioned relevance. In addition, we present theoretical formulation of efficient query-conditioned token selection. The insights from both empirical and theoretical analysis inform the design of our Script. 3.1 Research Objective Existing MLLMs (Liu et al., 2023; 2025; Xu et al., 2025) typically comprise three components: vision encoder fv, multimodal projector g, and large language model fϕ. After fv processes an input image Xv, the projector maps the features into visual-token sequence Hv Rnd, where is the token count and is the token dimension. The language model fϕ processes the visual tokens Hv and the tokenized query Hq to generate an answer grounded in the image. Typically, Hq, i.e., the visual sequence is much longer than the textual query. Equation 1 defines vision-token pruning by seeking the subset that preserves model performance while using the fewest tokens. v = arg min HvHv, Hv=m (cid:0)fϕ([ Hv; Hq]), fϕ([Hv; Hq])(cid:1) . (1) Here, measures the discrepancy between model outputs before and after pruning, serving as proxy for performance loss. The candidate subset Hv Hv contains tokens, with < n. To ensure query-aware pruning, it is essential to explicitly capture the semantic interactions between visual tokens Hv and their corresponding query tokens Hq. These query-specific interactions are crucial for identifying the most queryrelevant subset Hv. At the same time, assessing the visual redundancy between Hv and the full set Hv is vital for promoting diversity in token selection, which is important for generalizing across wide range of tasks. Together, these two aspects contribute to forming an informative and compact token subset. 3.2 Rethinking Visual Redundancy In natural images, spatially adjacent tokens often encode highly overlapping content, resulting in what we refer to as local redundancy. Carefully identifying and removing such redundancy can significantly reduce computation without compromising accuracy. To investigate local redundancy, we adopt the approach from (Wang et al., 2024) and compute Manhattan distances and cosine similarities between vision-token pairs. We group similarity values by distance and calculate the average for each group. As shown in Figure 4(a), cosine similarity is highest for nearby tokens and decreases until Table 1: Comparison of redundancy estimation methods with 90% pruning ratio on the POPE Benchmark. Sim. (%): Percentage of similarity pairs considered. Time: Inference time in milliseconds. IOU@K: Intersectionover-Union with top-K selections. Acc.: Classification accuracy (%). Speedup: Relative time speedup compared to Full Graph. Method Sim. (%) Time IOU@K Acc. Speedup Exhaustive Bipartite Random 100 50 0 100 35 34 1.00 0.93 0.52 85.14 84.49 78.63 1.00x 2.81x 2.82x Published in Transactions on Machine Learning Research (11/2025) distance of roughly 15. Beyond this, we observe that similarity increases again at longer distances, indicating that long-range token redundancy also exists and deserves attention. Furthermore, we visualize the similarity distributions for neighboring tokens (hop = 1) and randomly sampled neighbor token pairs (hop > 1). As shown in Figure 4(b), the number of neighboring tokens with similarity greater than 0.5 is consistently higher than that of random token pairs. These results indicate that neighboring (i.e., local) tokens encode more similar visual features, but long-range redundancy should also be taken into account, as it clearly exists. Insight 1: Redundancy Exists Beyond Local Neighborhoods Spatially adjacent tokens show high similarity (local redundancy), yet long-range pairs still share appreciable similarity, revealing long-range redundancy. Effective pruning must therefore model both local and long-range redundancy to avoid information loss. To further validate local redundancy and its estimation via cosine similarity, we introduce local information entropy as distribution-level measure of neighborhood compactness and compare it with cosine similarity. For each token, we extract its 33 Moore neighbourhood (v) = {hi pos(i)pos(j)1 1} (hop=1). We then project all vectors in the neighborhood onto their first principal component and discretize the resulting scalar values into 20 equal-width bins to estimate the probability distribution {pi}. The local information entropy is defined as: Hv = i=1 pi log pi, (2) where pi = pi + ϵ and ϵ = 108. compact neighborhood typically exhibits low Hv, as its token representations are similar and concentrated in few bins. In contrast, diverse neighborhood naturally yields higher Hv, reflecting greater variation among neighboring tokens. As shown in Figures 3(c) and (d), regions with low local similarity typically exhibit high entropy, confirming local redundancy and validating cosine similarity as an effective proxy. Insight 2: Cosine Similarity is Valid Redundancy Proxy Local information entropy inversely correlates with cosine similarity. Low-entropy (predictable) regions coincide with high neighbor similarity. Motivated by the validated role of cosine similarity in capturing visual redundancy and the need for computational efficiency, we design lightweight bipartite graph-based estimator that approximates both local and long-range redundancy using cosine similarity alone. This estimator forms the core of the graph-structured pruning module detailed in Section 4.1. Insight 3: Lightweight Bipartite Graph Yields Scalable Redundancy Estimation By weighting edges with cosine similarity, the evenodd bipartite graph achieves redundancy estimation comparable to exhaustive computation, but with significantly lower computational cost. Tokens are represented as nodes, divided into evenand odd-indexed sets; each node in one set connects to all nodes in the other, with edge weights defined by cosine similarity. These weights enable efficient redundancy estimation and guide token removal. As shown in Table 1, empirical results confirm the effectiveness of the bipartite graph-based approach. On the POPE Benchmark with 90% pruning ratio, it achieves an accuracy of 84.49%, which is close to that of the exhaustive approach (85.14%), while being nearly three times faster. Meanwhile, it retains 93% IOU agreement with the exhaustive approach, indicating that most key tokens are preserved. In contrast, the random approach is equally fast but drops to 78.63% accuracy and only 0.52 IOU, failing to preserve semantically relevant tokens. Published in Transactions on Machine Learning Research (11/2025)"
        },
        {
            "title": "3.3 Rethinking Query Relevance",
            "content": "Recent studies (Zhang et al., 2024b; Yang et al., 2025b) address visual token redundancy using text-agnostic pruning strategies that retain tokens with high [CLS] attention scores from the vision encoders final layer. However, these methods often fail to incorporate the query explicitly. They are also limited by inherent issues in attention mechanisms, such as the attention sink effect, where disproportionate amount of attention is assigned to the first token in sequence, regardless of its semantic relevance. As result, these methods tend to preserve tokens from visually salient or high-attention regions. Meanwhile, they may discard less prominent areas, such as backgrounds or textures, which can still be crucial to answering the query. This observation raises critical question: Is output-layer [CLS] attention sufficient to capture all queryrelevant visual information? Empirical observations suggest such attention-based strategies often overemphasize dominant foreground objects while neglecting background elements relevant to the query. As shown in Figure 1, attention focuses on the pot and fruit in the foreground but overlooks the actual target, the flower in the background. This results in incorrect outputs. Moreover, both divergence-based and similarity-based approaches also fail to adapt to the user query in this example. Insight 4: CLS Attention Misses Query-Relevant Background Tokens Attention-based relevance measures tend to focus on visually salient foregrounds and can miss query-relevant background tokens, underscoring the need for explicitly query-aware relevance scoring. 3.4 Rethinking Query-Relevant Visual Token Selection Let the query tokens be Hq = {h(q) }, and define their mean embedding as: h(q) µ = 1 ℓ ℓ j=1 h(q) . (3) The relevance score qi of each visual token hi is computed as the cosine similarity between hi and the mean query embedding h(q) µ : ri = i h(q) hih(q) µ µ , for = 1, . . . , n. (4) We construct diagonal matrix = diag(r1, . . . , rn) Rnn to encode the query relevance of each visual token. To encourage diversity among selected tokens, we define similarity matrix S(v) Rnn, where S(v) measures the pairwise similarity between vision tokens hi and hj. We assume S(v) is symmetric and ij positive semidefinite, which approximately holds when using cosine similarity among normalized features. This formulation naturally aligns with modeling token selection as k-DPP (Determinantal Point Process), which promotes subsets that balance individual relevance with collective diversity. We therefore construct the following k-DPP kernel (note that is positive-semidefinite kernel, not loss function) to integrate both query relevance and feature diversity: = Q1/2S(v)Q1/2, (5) where explicitly encodes query alignment and S(v) impliciltly encourages feature diversity. Insight 5: S(v) Promotes Diversity For any subset I, det(LI) equals the squared volume of the parallelotope spanned by the selected token vectors (see Appendix B). If S(v) were the identity matrix, this volume reduces to iI qi, i.e., relevance only. Introducing off-diagonal similarities lowers the determinant when two tokens are similar, thus maximizing det(LI) encourages mutually orthogonal (diverse) token subsets. Published in Transactions on Machine Learning Research (11/2025) k-DPP assigns to each subset [n] of fixed size probability proportional to det(LI), the determinant of the principal submatrix of indexed by I. This determinant reflects the volume spanned by the selected vectors, and is larger when the selected tokens are both informative and mutually distinct. Expanding the determinant yields: det(LI) = det(Q1/2 = det(Q1/2 ! ) S(v) Q1/2 )2 det(S(v) ) = qi det(S(v) ). (6) iI This decomposition highlights the trade-off between query relevance (via det(S(v) qi) and visual diversity (via )). Based on this, we define the final surrogate objective for selecting an informative token subset: = arg max I[n], I=m qi det(S(v) ). iI (7) Insight 6: k-DPP Balances Relevance and Diversity via Geometry The k-DPP determinant can be analogized to the feature-space volume spanned by selected tokens, thus optimizing it jointly maximizes query relevance and inter-token diversity in single objective."
        },
        {
            "title": "4 Method",
            "content": "Motivated by the empirical analysis in Section 3, we introduce Script, as illustrated in Figure 4. Script progressively refines token selection through two main modules: (1) Graph-Structured Pruning (GSP), which removes visually redundant tokens using bipartite similarity graph; and (2) Query-Conditioned Semantic Pruning (QCSP), which consists of two steps: Query-Conditioned Relevance Scoring (QCRS), computing the semantic relevance of each token with respect to the input query, and Diversity-Preserving Selection via DPP, selecting compact, diverse subset of relevant tokens. The outputs from GSP and QCSP are then intersected to obtain the final token subset . 4.1 Graph-Structured Pruning Transformer-based visual encoders often generate dense token sequences with substantial visual redundancy across both local and long-range contexts, as identified in Section 3.2. To mitigate this, we propose an inference-time pruning strategy based on bipartite similarity graphs without relying on model parameters. Bipartite Graph Construction. Given visual token embeddings Hv = [h1, . . . , hn] Rnd, we construct bipartite graph by partitioning tokens into two disjoint node sets Vsrc and Vdst via alternating index assignment (e.g., even indices to Vsrc, odd-indexed to Vdst) (Buchholz, 2024). The bipartite graph is defined as = (Vsrc, Vdst, S(v)), where each node in Vsrc is fully connected to every node in Vdst, and the edge weight S(v) ij S(v) represents the cosine similarity between tokens ti and tj: S(v) ij = hsrc hdst hdst hsrc . (8) In this way, explicitly structures token redundancy, with similarity scores providing insights into both local and global redundancy. Notably, this bipartite graph design reduces computational cost by up to 75% compared to conventional similarity-based pruning methods. As empirically validated in Section 3.2, most redundancy lies in local neighborhoods and is thus well-captured by the bipartite approximation effectively. Overall, Vsrc and Vdst jointly offer structured, low-cost representation of Hv for redundancy modeling. 7 Published in Transactions on Machine Learning Research (11/2025) Figure 4: Overview of Script, three-stage pruning framework: (a) overall architecture; (b) QueryConditioned Semantic Pruning (QCSP); (c) Graph-Structured Pruning (GSP). Together, these modules remove semantically irrelevant and visually redundant tokens through joint selection process. Redundancy Scoring. To identify redundant tokens, we retain all similarity edges above threshold τ , forming pruned subgraph Gτ , where each node is connected to highly similar neighbors. For token ti Vsrc, we define its degree as: d(ti) = ij τ }, where denotes the indicator function, returning 1 if the condition is true and 0 otherwise. Nodes with high degrees in Gτ are identified as structurally redundant, typically indicating that tokens represent repeated visual content in either local or long-range contexts. To further distinguish tokens with similar degrees, we use the average similarity µ(ti) between token ti and its neighbors in Gτ , which captures their typical similarity. For cases where ti is an isolated node in Gτ , we define fallback similarity µ(ti) based on the mean similarity between ti and all tokens in Vdst or Vsrc under the full graph G. The final redundancy score is then defined as: I{S(v) jVdst score(ti) = ( d(ti) exp (γ(µ(ti) τ )) , d(ti) > 0 in Gτ , µ(ti) in G, otherwise, (9) where γ is tunable scaling factor. The score, computed using d(ti) and µ(ti) with exponential weighting, is designed to improve ranking contrast between highly and moderately redundant tokens. Pruning Rate and Token Removal. Based on the computed redundancy scores, we rank all visual tokens in descending order and discard the top most redundant ones, where denotes the pruning ratio. This yields structurally pruned candidate set Hv, which is then combined with the token set semantically aligned with the query, produced in the next stage, to jointly form . 8 Published in Transactions on Machine Learning Research (11/2025) Table 2: Performance comparisons on LLaVA-1.5-7B (Liu et al., 2023) across 10 image understanding benchmarks. The best results in each setting are bolded, and the second-best are underlined. Method Venue GQA MMB MMBCN MME POPE SQAIMG VQAV2 VQAText VizWiz MMVet Acc. Relative Upper Bound, 576 Tokens (100%), 3.817 TFLOPs LLaVA-1.5-7B (Liu et al., 2023) Nips23 61.94 FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed 58.09 58.18 59.24 59.91 59.54 60.82 56.85 56.85 57.87 59.16 58.41 60.27 53.59 56.35 55.47 57.74 53.80 59.28 49.61 54.35 53.32 55.11 51.74 57. 44.08 50.81 50.88 51.10 46.16 54.42 58.10 69.41 86.96 1507.06 82.30 78.09 86.63 87.56 86.49 87. 68.67 68.47 68.22 69.01 68.62 68.79 80.24 77.95 85.20 87.36 86.22 87.32 54.04 51.02 53.52 43.00 55.20 57.55 68.96 67.87 68.37 69.31 68.67 69.49 53.61 48.66 53.18 42.16 53.48 55.85 1440.19 1359.86 1429.75 1412.97 1429.41 1431. 1490.65 1405.29 1445.67 1436.39 1450.07 1493.87 78.50 64.09 Retain 192 Tokens in Average ( 66.7%), 1.253 TFLOPs 72.88 63.40 74.86 59.19 75.44 63.75 75.10 62.54 75.61 63.46 77.62 63.85 Retain 128 Tokens in Average ( 77.8%), 0.833 TFLOPs 71.68 62.46 74.42 58.42 74.62 62.20 74.19 61.86 73.86 62.75 76.55 63.10 Retain 64 Tokens in Average ( 88.9%), 0.415 TFLOPs 58.71 59.62 73.10 55.68 71.59 60.82 72.35 59.28 68.24 60.14 75.08 61.90 Retain 32 Tokens in Average ( 94.5%), 0.208 TFLOPs 56.48 54.64 69.54 53.31 67.51 58.85 69.41 58.93 63.25 59.64 73.25 61.28 Retain 16 Tokens in Average ( 97.3%), 0.103 TFLOPs 52.06 41.41 61.48 45.28 62.48 52.23 63.89 53.09 58.72 54.16 68.77 57.02 1108.35 1215.42 1306.97 1303.26 1181.05 1338.27 1366.33 1288.31 1374.67 1368.28 1295.16 1412.08 898.83 1085.66 1179.56 1235.71 987.32 1258.55 68.26 68.22 68.62 68.57 69.35 68. 68.59 77.54 78.18 83.93 77.27 86.77 59.89 81.10 75.11 75.98 69.00 84.44 75.34 77.80 81.68 86.51 80.92 86.95 43.99 43.27 49.31 35.99 48.64 50.38 50.08 46.39 51.29 39.20 50.68 52.93 32.04 38.46 42.61 31.76 42.93 45. 68.51 66.98 66.98 69.21 68.26 69.12 68.11 68.22 68.96 68.22 69.46 68.65 58.20 50.32 31.82 63. 100% 54.09 54.29 54.89 53.38 56.93 57.81 52.21 53.74 54.12 51.92 56.29 56.45 51.62 51.69 52.72 54.51 53.67 55.20 49.85 49.12 50.76 51.46 51.28 53.10 42.15 39.80 45.77 41.15 41.78 47. 54.97 48.23 53.97 55.33 51.23 53.80 55.51 47.62 54.39 56.21 51.83 53.97 54.74 43.84 54.81 57.55 50.11 54.31 54.22 38.41 55.47 57.26 49.62 53.77 52.16 35.26 54.78 55.04 49.47 54.09 29.59 30.82 30.74 30.96 31.24 31. 28.07 29.35 27.39 28.30 29.12 31.09 27.06 28.29 27.13 27.39 24.92 29.96 24.50 25.92 24.95 25.73 23.38 27.57 16.24 20.49 20.69 19.86 21.84 23.71 61.29 59.28 59.28 60.89 62.08 63.45 60.13 58.35 60.87 60.08 61.21 62. 56.72 56.58 59.32 59.12 57.67 61.49 52.56 54.05 57.23 57.16 55.32 59.93 45.35 49.39 53.05 52.29 50.17 56.81 96.56% 93.41% 93.40% 95.94% 97.82% 100.00% 94.74% 91.93% 95.90% 94.66% 96.43% 98.49% 89.36% 89.14% 93.46% 93.14% 90.86% 96.88% 82.80% 85.15% 90.17% 90.05% 87.16% 94.42% 71.45% 77.82% 83.58% 82.38% 79.04% 89.51% 4.2 Query-Conditioned Semantic Pruning While the GSP module eliminates visually redundant tokens, it does not guarantee semantic alignment with the user query. To address this limitation, we introduce the QCSP module, which comprises QCRS for each token and diversity-preserving selection via DPP based on the relevance scores. Query-Conditioned Relevance Scoring. We begin by computing an average query embedding h(q) µ via mean pooling over Hq. Each visual token hi is then scored by cosine similarity to h(q) µ , yielding raw query relevance vector rraw = [r1 . . . rn] (see Eq. 4). To ensure consistency across samples, we apply min-max normalization to obtain rnorm [0, 1]n. Query-Conditioned Kernel Construction. To support diversity-preserving selection of query-relevant tokens, we construct query-aware DPP kernel that integrates token relevance with visual similarity. Importantly, visual similarity in this context helps refine query-relevant token selection rather than explicitly promote visual diversity, which is addressed by the GSP module. Specifically, let S(v) Rnn be the cosine similarity matrix of ℓ2-normalized visual token embeddings: (cid:28) hi hi hj hj ij = S(v) (cid:29) . , (10) Then, we define query-conditioned kernel Rnn by reweighting the pairwise visual similarity matrix S(v) with normalized token-to-query relevance scores rnorm: = diag(rnorm) S(v) diag(rnorm). Intuitively, captures diversity within token relevance, promoting token selections that are both query-aligned and diverse. Its symmetric and positive semi-definite structure makes it well-suited for k-DPP-based selection 1. 1k is not tunable hyperparameter but direct control of the number of retained tokens. It is conceptually equivalent to the pruning ratio in GSP, with the correspondence = (1 p) n, where is the number of original tokens. Published in Transactions on Machine Learning Research (11/2025) Table 3: Performance comparisons on LLaVA-NeXT-7B (Liu et al., 2024a) across 9 image understanding benchmarks. The best results in each setting are bolded, and the second-best are underlined. Method Venue GQA MMB MMBCN MME POPE SQAIMG VQAV2 VQAText VizWiz MMVet Acc. Relative LLaVA-NeXT-7B (Liu et al., 2024a) CVPR24 62.83 Upper Bound, 2,880 Tokens (100%), 20.825 TFLOPs 65.81 1504. 86.92 57.65 67.59 81.20 FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) FastV (Chen et al., 2024a) TRIM (Song et al., 2025) VisionZip (Yang et al., 2025b) DivPrune (Alvar et al., 2025) SparseVLM (Zhang et al., 2024c) Script (Ours) ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed Retain 640 Tokens in Average ( 77.8%), 4.627 TFLOPs 77.06 78.36 79.13 79.26 77.11 1412.86 1473.62 1408.67 1457.48 1426.83 67.32 66.71 66.36 66.76 67.27 63.14 65.83 64.52 65.77 65.66 79.96 86.92 85.86 86.69 85. 53.88 55.79 56.28 57.31 56.81 58.93 61.13 60.84 60.58 60.32 62.64 65.98 57.76 1481. 87.65 67.43 80.47 Retain 320 Tokens in Average ( 88.9%), 2.314 TFLOPs 52.17 59.94 59.11 59.24 58.75 53.87 63.58 62.89 64.03 64. 44.51 51.01 53.39 55.58 54.86 1178.61 1426.49 1360.61 1418.46 1399.77 72.54 86.17 84.44 84.92 85.16 65.67 66.22 65.71 67.11 66.14 66.52 74.27 76.41 77.24 75.62 61. 65.39 55.82 1452.85 87.22 67.80 78. Retain 160 Tokens in Average ( 94.4%), 1.156 TFLOPs 48.19 57.36 56.28 55.62 53.27 47.96 61.53 59.72 62.97 62.93 39.77 47.79 51.33 53.51 53.45 1083.20 1279.44 1342.84 1359.82 1362.86 68.81 84.58 84.06 81.05 84. 64.07 65.51 65.89 66.28 66.08 61.42 71.04 73.49 74.48 71.83 60.38 55.65 41.11 65. 100% 58.15 54.85 59.90 57.04 59.64 58.92 52.30 51.02 58.53 56.17 56.55 57.24 48.92 45.94 54.32 54.18 53. 53.94 54.12 55.46 55.62 55.58 39.17 37.16 38.79 38.16 39.48 62.22 63.46 63.76 64.01 63.92 95.09% 96.98% 97.44% 97.82% 97.69% 55.71 41. 65.35 99.88% 51.33 53.94 55.27 54.97 55.22 26.58 32.77 36.25 35.72 37.94 54.44 61.02 62.00 62.59 62.44 83.21% 93.27% 94.76% 95.66% 95.43% 55.51 38.12 63.95 97.78% 48.04 52.92 53.68 54.87 54.61 22.04 29.82 34.59 32.38 34. 50.34 58.05 60.05 60.33 60.38 76.93% 88.71% 91.78% 92.21% 92.28% 60.73 64.20 53.67 1423. 87.19 67.44 76.71 55.28 55.19 36. 62.80 95.98% Greedy Token Selection via Kernel Decomposition To select compact yet informative subset of visual tokens, we perform approximate MAP (maximum posteriori) inference under k-DPP defined by the query-conditioned kernel L. This procedure allows us to find tokens that are not only individually relevant to the query but also collectively diverse in semantic content, reducing redundancy while ensuring broad query coverage. The objective is to select subset of tokens such that the corresponding submatrix LS maximizes det( LS), maxS=k det (cid:0) LS (cid:1). The determinant reflects both semantic relevance and diversity. higher value indicates greater diversity in the selected subset. To efficiently approximate the MAP solution2 for DPP, we adopt the Cholesky-based greedy algorithm (Chen et al., 2018). The algorithm starts with an empty set and incrementally adds tokens. At each step, it selects the token that yields the highest gain, where the gain is defined as the increase in the determinant of the = Lii, ui = 0 kernel submatrix after adding the token. The algorithm is initialized as follows: v2 represents the importance of token for selection, and ui is vector Rk that stores the projection of token onto the subspace spanned by the previously selected tokens. At each iteration, the token with the largest v2 is selected: = arg maxiZS v2 . After the selection, for each / (S {j}), we compute: for all {1, . . . , n}, where v2 ei = Lji uj, ui pv2 + ϵ , ui ui + eiej, v2 i e2 , (11) where ei represents the projection of token onto the direction defined by token j, ej is the j-th standard basis vector in Rn, and ϵ is small positive constant (e.g., 106) introduced for numerical stability. The update v2 encourages selecting tokens with small projections ei. smaller ei indicates that the token is less similar, that is, closer to being orthogonal to the already selected tokens, thereby enhancing the diversity of the subset. v2 e2 4.3 Final Token Selection GSP and QCSP each address complementary aspects of token selection. GSP explicitly targets visual redundancy, while QCSP captures query relevance. However, neither is sufficient on its own for robust selection. We therefore take their intersection to ensure that the final subset is both visually compact and semantically aligned with the query. If the intersection contains fewer tokens than the required number, we supplement it with additional tokens from QCSP by iteratively increasing to retrieve more tokens. 2MAP for DPP is an NP-hard problem. 10 Published in Transactions on Machine Learning Research (11/2025) Table 4: Performance comparison of different pruning methods on Video-LLaVA-7B with 64 frames per video. The best results in each setting are bolded, and the second-best are underlined. Method Metric Venue MLVU MVBench LongVideoBench Video-MME Acc. Relative m-avg test val perception relation w/o sub short medium long Upper Bound, All 64 169 Tokens (100%) 65.03 58. 53.64 59.07 63.66 76.46 61.21 53. 62.19 100% Retain 64 64 Tokens ( 62.1%) 60.60 64.32 61.20 64.25 56.21 57.97 56.30 58.27 52.15 53.75 51.57 53.37 55.81 54.82 56.68 57. Retain 64 32 Tokens ( 81.1%) 56.88 62.41 58.51 61.40 48.25 51.34 49.79 52.97 52.64 56.64 53.47 56.35 52.27 53.37 54.13 55.37 Retain 64 16 Tokens ( 90.5%) 48.68 57.56 53.02 57.13 44.79 47.29 42.89 48. 46.61 52.27 47.42 52.91 46.57 51.13 48.87 53.45 61.89 61.31 60.94 62.36 56.00 59.73 59.09 60.35 49.88 56.37 49.58 57.20 73.46 72.49 73.20 74. 63.38 69.69 69.18 72.11 54.92 67.67 53.81 65.97 59.45 59.43 58.78 60.31 55.79 57.82 56.49 58.46 50.30 53.34 49.45 56.08 52.71 51.02 51.12 52. 48.34 49.88 50.38 51.20 45.28 48.11 46.22 49.46 59.56 59.90 59.45 61.08 54.70 58.04 56.82 58.99 48.85 54.72 49.25 55.52 95.77% 96.32% 95.59% 98.22% 87.95% 93.32% 91.37% 95.00% 78.54% 88.98% 79.19% 89.30% LLaVA-Video-7B - 67.75 FastV Chen et al. (2024a) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) ECCV24 CVPR25 ICML25 Purpose FastV Chen et al. (2024a) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) ECCV24 CVPR25 ICML25 Purpose FastV Chen et al. (2024a) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) ECCV24 CVPR25 ICML25 Purpose 63.79 64.01 65.33 66. 58.75 61.44 60.37 62.77 52.58 58.60 51.97 58."
        },
        {
            "title": "5 Experiments",
            "content": "5.1 Experimental Settings We conduct comprehensive experiments to evaluate Scripts performance across diverse MLLMs and benchmarks3. In all tables, the Relative column reports performance relative to the unpruned model. Models and Baselines. We select four representative MLLMs: LLaVA 1.5 7B (Liu et al., 2023), LLaVA NeXT 7B (Liu et al., 2024a), Video LLaVA 7B (Lin et al., 2024), and Intern VL3 (Zhu et al., 2025). These models cover both image and video modalities with diverse architectures, providing comprehensive testbed for evaluating Scripts generalizability. In addition, we compare with five state-of-the-art pruning baselines, including FastV (Chen et al., 2024a), TRIM (Song et al., 2025), SparseVLM (Zhang et al., 2024c), DivPrune (Alvar et al., 2025), and VisionZip (Yang et al., 2025b). Notably, most of these baselines are either query-agnostic or tightly bound to specific model architectures. Some of these architectures are included in our evaluation, enabling fair and direct comparisons. Benchmarks. We evaluate Script across fourteen widely adopted benchmarks spanning both image and video understanding tasks. Image understanding tasks: GQA (Hudson & Manning, 2019), ScienceQA (Lu et al., 2022), VQAv2 (Goyal et al., 2017), TextVQA (Singh et al., 2019), VizWiz (Gurari et al., 2018), MMVet (Yu et al., 2023), MMBench (Liu et al., 2024c), MMBench-CN (Liu et al., 2024c), MME (Fu et al., 2023), and POPE (Li et al., 2023b). Video reasoning tasks: LongVideoBench (Wu et al., 2024), VideoMME (Fu et al., 2025), MLVU (Zhou et al., 2025), and MVBench (Li et al., 2023a). 5.2 Results and Discussions LLaVA-1.5-7B. Table 2 compares visual token pruning methods on LLaVA-1.5-7B across different token budgets. Our proposed method, Script, consistently outperforms baselines such as FastV, DivPrune, and SparseVLM, demonstrating strong adaptability and robustness across pruning levels and diverse tasks. At the highest token budget (192 tokens), Script achieves perfect accuracy (100.00%), outperforming the next-best method, DivPrune, by over 4 percentage points (95.94%). When reducing tokens to an intermediate level (64 tokens), Script retains exceptional accuracy at 96.86%, indicating minimal performance drop despite significant token reduction. This outperforms SparseVLM by around 6 percentage points, underscoring Scripts effective token selection strategy. Even under the most aggressive pruning (16 tokens, corresponding 3See the appendix for additional experiments, model settings, limitations, and broader impact. 11 Published in Transactions on Machine Learning Research (11/2025) to 97.2% pruning), Script still retains high accuracy (89.51%), significantly outperforming all baselines, highlighting its strong ability to preserve essential semantic and visual information, making it suitable for resource-constrained deployment. LLaVA-NeXT-7B. Table 3 presents detailed comparison of visual token pruning methods applied to LLaVA-NeXT-7B across different token budgets. Our method, Script, consistently delivers the best performance across pruning levels, outperforming all baseline methods. Under mild pruning (640 tokens, 77.8% reduction), Script achieves the highest average accuracy (65.1%) and the best relative performance (99.88%), indicating minimal information loss despite significant pruning. Under moderate pruning conditions (320 tokens, 88.9% reduction), Script maintains robust performance, achieving 63.97% accuracy and 97.09% relative performance, outperforming the next-best baseline by 2.12% points. Even at the most aggressive pruning level (160 tokens, 94.4% reduction), Script still maintains superior results, reaching 62.80% accuracy and 95.98% relative performance. These results demonstrate Scripts strong ability to retain essential visual and semantic cues under tight computational budgets. Video-LLaVA-7B. Table 4 reports the performance of SOTA pruning methods on Video-LLaVA-7B across multiple token budgets. Script consistently outperforms all baselines across pruning levels, demonstrating robust performance under growing compression ratios. Under mild pruning (6464 tokens, 62.1% reduction), Script achieves 98.22% average accuracy, demonstrating effective preservation of informative video content essential for accurate multimodal reasoning. Furthermore, under moderate pruning (6432 tokens, 81.1% reduction), Script maintains 95.00% accuracy, outperforming FastV by roughly 7 percentage points, which demonstrates its effectiveness even under tighter token constraints. Even under aggressive pruning (6416 tokens, 90.5% reduction), Script retains strong performance with 89.30% average accuracy, significantly surpassing other baselines at similar compression levels. These results highlight Scripts ability to preserve spatio-temporal semantics, supporting its applicability in real-world video-based multimodal tasks. 5.3 Ablation study GSP QCRS DPP POPE MME GQA SQAIM Acc. Relative Table 5: Ablation on LLava-1.5-7B evaluating the impact of GSP, QCRS, and DPP. To analyze the individual impact of each module, we conduct an ablation study to evaluate the contributions of the GSP, QCRS, and DPP modules. All experiments are conducted under stringent constraints, retaining only 16 tokens on average (97.3% pruning), with compute cost of 0.103 TFLOPs. As shown in Table 5, naively pruning tokens causes sharp drop in relative accuracy (average: 63.73%), underscoring the need for informed token selection strategies. Incorporating GSP alone raises relative accuracy to 66.58%, demonstrating the benefit of using bipartite graph to model redundancy. Integrating QCSP (QCRS + DPP) alone further improves performance, increasing relative accuracy to 91.19%. Notably, the full configuration achieves 92.19% of the original accuracy while retaining only 2.7% of tokens. These results further reflect the distinct yet complementary contributions of each module. 38.69 695.70 49.92 40.86 758.23 52.85 1225.27 45.82 77.20 1111.92 51.11 77.75 83.38 1252.28 53.28 84.44 1258.55 54.42 63.73% 46.78 66.58% 48.87 86.14% 63.23 86.17% 63.25 66.95 91.19% 67.73 92.19% LLaVA-1.5-7B, Retain 576 Tokens (100%) 1507.06 61.94 Retain 16 Tokens in Average ( 97.3%), 0.103 TFLOPs 63.76 63.86 68.67 68.57 68.51 69.12 100% 86.96 69. 73.41 5.4 Case study To analyze how different pruning strategies handle query-relevant vision tokens, we compare four approaches: attention-based (via [CLS]), divergence-based, similarity-based, and DPP-based. Figure 5 visualizes the normalized scores from each method on representative POPE benchmark. The scores reveal how each method prioritizes tokens based on distinct selection criteria. As shown in Figure 5, the four strategies exhibit clearly different token selection behaviors. The attentionbased method emphasizes globally salient regions, such as central objects, but often overlooks contextually relevant backgrounds like human bodies. In contrast, divergence-based and similarity-based methods struggle to prioritize tokens that align with the input query. The similarity-based method, in particular, ignores query information and produces relatively uniform token scores that primarily reflect visual diversity. While such diversity helps preserve overall image content, it fails to capture query-specific relevance. The DPP-based 12 Published in Transactions on Machine Learning Research (11/2025) Input Image Attention-based Divergence-based Similarity-based DPP-based Figure 5: Visualizations of Pruning Preferences. We compute four different pruning scores for sample from the POPE benchmark using LLaVA-1.5-7B, with the query: Is there person in the image? Red indicates high preference, while blue indicates low. method assigns the highest relevance to human faces and moderately high scores to body-related tokens, demonstrating strong alignment with query semantics. However, similar to attention-based approaches, it lacks diversity in the selected tokens. These observations highlight the importance of combining GSP and QCSP for more balanced and effective token pruning. 5.5 Efficiency Analysis 2880 Method # Token LLaVA-NeXT-7B FLOPs (T) Prefill Time (ms/token) Decode Time (ms/token) Table 6: Efficiency comparisons on the POPE benchmark. We report the theoretical FLOPs, actual runtime, KV cache compression rate (%), and the achieved accuracy. We comprehensively evaluate recent token pruning methods on the POPE benchmark, focusing on computation inference latency, memory uscost, As shown in age, and accuracy. Table 6, the uncompressed LLaVANeXT-7B requires 41.7T FLOPs, with 246ms prefill and 29ms decode latency It consumes 1440MB KV per token. cache and 16.7GB peak GPU memory, achieving an F1 score of 86.8. In contrast, Script reduces FLOPs to 4.1T (10 reduction), prefill latency to 35ms (6.8 faster), and decode latency to 22ms. It also lowers KV cache to 160MB and memory to 13.5GB, while maintaining an F1 score of 86.7. Without DPP, Script maintains similar efficiency and achieves slightly lower F1 score of 86.5. Compared to other baselines, Script achieves the lowest prefill/decode latency, the smallest KV cache, and the best overall F1 score. For instance, FastV and PDrop suffer major accuracy loss (F1: 49.5 and 60.8), while VisionZip and DivPrune perform better (F1: 82.3 and 84.7) but at higher computational and memory cost. Overall, Script achieves the best balance between efficiency and accuracy, outperforming all baselines across nearly all metrics. FastV(ECCV24) PDrop(CVPR25) SparseVLM(ICML25) VisionZip(CVPR25) DivPrune(CVPR25) 23 (1.2) 24 (1.2) 25 (1.1) 22 (1.3) 22 (1.3) 4.4 (9.5) 4.5 (9.3) 4.5 (9.3) 4.2 (9.9) 4.2 (9.9) 54 (4.6) 55 (4.5) 71 (3.5) 38 (6.6) 38 (6.6) GPU Memory (GB) 160.3 160.2 161.2 160.0 160.0 Script(Ours) w/o DPP KV Cache (MB) 15.6 15.6 18.6 14.8 13.9 49.5 60.8 76.9 82.3 84.7 22 (1.3) 22 (1.3) 4.1 (10) 4.1 (10) 35 (6.8) 38 (6.6) F1 Score (F1) 320 320 320 320 320 160.0 160.0 86.7 86.5 13.5 14.0 320 1440.0 86.8 41.7 16.7"
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce Script which reduces inference cost by selecting compact yet semantically meaningful subset of visual tokens, conditioned on the user query. Script combines visual redundancy and query relevance perspectives through four stages: (1) graph-structured filtering to remove visually redundant tokens, (2) query-conditioned relevance scoring, (3) diversity-promoting subset selection based on relevance scores using DPP, and (4) intersection-based fusion to retain tokens that are both visually compact and semantically aligned with the query. Its architecture-agnostic, plug-and-play design enables seamless integration into existing MLLMs without requiring retraining or architectural changes. Extensive experiments on fourteen vision-language benchmarks show that Script consistently reduces computational overhead and latency while preserving, or even improving, task performance under aggressive pruning settings. Future work includes extending Script to additional modalities such as audio and video in spatial-temporal contexts, and exploring adaptive pruning schedules that dynamically adjust to input complexity. 13 Published in Transactions on Machine Learning Research (11/2025)"
        },
        {
            "title": "References",
            "content": "Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Bińkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: visual language model for few-shot learning. In Advances in Neural Information Processing Systems, volume 35, pp. 2371623736, 2022. Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, and Yong Zhang. Divprune: Diversity-based visual token pruning for large multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 93929401, 2025. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023. Federico Barbero, Alvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličković, and Razvan Pascanu. Why do llms attend to the first token? arXiv preprint arXiv:2504.02732, 2025. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your ViT but faster. In International Conference on Learning Representations, 2023. Simon Buchholz. Learning partitions from context. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=pRSgf5VdD0. Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Internlm2 technical report. arXiv preprint arXiv:2403.17297, 2024. Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorlarge language model as thi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023. Laming Chen, Guoxin Zhang, and Eric Zhou. Fast greedy map inference for determinantal point process to improve recommendation diversity. Advances in Neural Information Processing Systems, 31, 2018. Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models. In European Conference on Computer Vision, pp. 1935. Springer, 2024a. Yukang Chen, Fuzhao Xue, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, et al. Longvila: Scaling long-context visual language models for long videos. arXiv preprint arXiv:2408.10188, 2024b. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024c. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning for generic visuallinguistic tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2418524198, 2024d. Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. 14 Published in Transactions on Machine Learning Research (11/2025) Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 69046913, 2017. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1437514385, 2024. Zonghao Guo, Ruyi Xu, Yuan Yao, Junbo Cui, Zanlin Ni, Chunjiang Ge, Tat-Seng Chua, Zhiyuan Liu, and Gao Huang. Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images. In European Conference on Computer Vision, pp. 390406. Springer, 2024. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. Wenbo Hu, Zi-Yi Dou, Liunian Li, Amita Kamath, Nanyun Peng, and Kai-Wei Chang. Matryoshka query transformer for large vision-language models. Advances in Neural Information Processing Systems, 37: 5016850188, 2024. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Ahmadreza Jeddi, Negin Baghbanzadeh, Elham Dolatabadi, and Babak Taati. Similarity-aware token pruning: Your vlm but faster. arXiv preprint arXiv:2503.11549, 2025. Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pp. 235251. Springer, 2016. Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github.com/openimages, 2017. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123:3273, 2017. Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024a. Duo Li, Zuhao Yang, Xiaoqin Zhang, Ling Shao, and Shijian Lu. comprehensive study on visual token redundancy for discrete diffusion-based multimodal large language models, 2025a. URL https://arxiv. org/abs/2511.15098. 15 Published in Transactions on Machine Learning Research (11/2025) Duo Li, Zuhao Yang, Xiaoqin Zhang, Ling Shao, and Shijian Lu. Todre: Effective visual token pruning via token diversity and task relevance, 2025b. URL https://arxiv.org/abs/2505.18757. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, Limin Wang, and Yu Qiao. Mvbench: comprehensive multi-modal video understanding benchmark. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2219522206, 2023a. URL https://api.semanticscholar.org/CorpusID:265466214. Yadong Li, Jun Liu, Tao Zhang, Song Chen, Tianpeng Li, Zehuan Li, Lijun Liu, Lingfeng Ming, Guosheng Dong, Da Pan, et al. Baichuan-omni-1.5 technical report. arXiv preprint arXiv:2501.15368, 2025c. Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models. In European Conference on Computer Vision, pp. 323340. Springer, 2024b. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 292305, 2023b. Zongqian Li, Yinhong Liu, Yixuan Su, and Nigel Collier. Prompt compression for large language models: survey. In Luis Chiruzzo, Alan Ritter, and Lu Wang (eds.), Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 71827195, Albuquerque, New Mexico, April 2025d. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025. naacl-long.368/. Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united In Proceedings of the Conference on Empirical visual representation by alignment before projection. Methods in Natural Language Processing, pp. 59715984, 2024. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. Llava-next: Improved reasoning, ocr, and world knowledge, January 2024a. URL https://llava-vl.github.io/ blog/2024-01-30-llava-next/. Ting Liu, Liangtao Shi, Richang Hong, Yue Hu, Quanjun Yin, and Linfeng Zhang. Multi-stage vision token dropping: Towards efficient multimodal large language model. arXiv preprint arXiv:2411.10803, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? In European Conference on Computer Vision, pp. 216233. Springer, 2024c. Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu, Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67(12):220102, 2024d. Zuyan Liu, Yuhao Dong, Jiahui Wang, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Ola: Pushing the frontiers of omni-modal language model with progressive modality alignment. arXiv preprint arXiv:2502.04328, 2025. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 16 Published in Transactions on Machine Learning Research (11/2025) Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, and Rongrong Ji. Feast your eyes: Mixtureof-resolution adaptation for multimodal large language models. arXiv preprint arXiv:2403.03003, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. Lorenzo Papa, Paolo Russo, Irene Amerini, and Luping Zhou. survey on efficient vision transformers: Algorithms, techniques, and performance benchmarking. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46:76827700, 2023. URL https://api.semanticscholar.org/CorpusID:261531260. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PmLR, 2021. Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. Llava-prumerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 83178326, 2019. Dingjie Song, Wenjun Wang, Shunian Chen, Xidong Wang, Michael X. Guan, and Benyou Wang. Less is more: simple yet effective token reduction method for efficient multi-modal LLMs. In Owen Rambow, Leo Wanner, Marianna Apidianaki, Hend Al-Khalifa, Barbara Di Eugenio, and Steven Schockaert (eds.), Proceedings of the 31st International Conference on Computational Linguistics, pp. 76147623, Abu Dhabi, UAE, January 2025. Association for Computational Linguistics. URL https://aclanthology.org/2025. coling-main.508/. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Hongjie Wang, Bhishma Dedhia, and Niraj K. Jha. Zero-tprune: Zero-shot token pruning through leveraging In Proceedings of the IEEE/CVF Conference on of the attention graph in pre-trained transformers. Computer Vision and Pattern Recognition (CVPR), 2024. Zichen Wen, Yifeng Gao, Weijia Li, Conghui He, and Linfeng Zhang. Token pruning in multimodal large language models: Are we solving the right problem? arXiv preprint arXiv:2502.11501, 2025a. Zichen Wen, Yifeng Gao, Shaobo Wang, Junyuan Zhang, Qintong Zhang, Weijia Li, Conghui He, and Linfeng Zhang. Stop looking for important tokens in multimodal language models: Duplication matters more. arXiv preprint arXiv:2502.11494, 2025b. Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:28828 28857, 2024. Long Xing, Qidong Huang, Xiaoyi Dong, Jiajie Lu, Pan Zhang, Yuhang Zang, Yuhang Cao, Conghui He, Jiaqi Wang, Feng Wu, et al. Pyramiddrop: Accelerating your large vision-language models via pyramid visual redundancy reduction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. 17 Published in Transactions on Machine Learning Research (11/2025) An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Senqiao Yang, Yukang Chen, Zhuotao Tian, Chengyao Wang, Jingyao Li, Bei Yu, and Jiaya Jia. Visionzip: Longer is better but not necessary in vision language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025b. Zhongyu Yang, Jun Chen, Dannong Xu, Junjie Fei, Xiaoqian Shen, Liangbing Zhao, Chun-Mei Feng, and Mohamed Elhoseiny. Wikiautogen: Towards multi-modal wikipedia-style article generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1553215541, October 2025c. Zhongyu Yang, Junhao Song, Siyang Song, Wei Pang, and Yingfang Yuan. MERMAID: Multi-perspective self-reflective agents with generative augmentation for emotion recognition. In Christos Christodoulopoulos, Tanmoy Chakraborty, Carolyn Rose, and Violet Peng (eds.), Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 2465024666, Suzhou, China, November 2025d. Association for Computational Linguistics. ISBN 979-8-89176-332-6. doi: 10.18653/v1/2025.emnlp-main. 1252. URL https://aclanthology.org/2025.emnlp-main.1252/. Zuhao Yang, Sudong Wang, Kaichen Zhang, Keming Wu, Sicong Leng, Yifan Zhang, Chengwei Qin, Shijian Lu, Xingxuan Li, and Lidong Bing. Longvt: Incentivizing \"thinking with long videos\" via native tool calling, 2025e. URL https://arxiv.org/abs/2511.20785. Weihao Ye, Qiong Wu, Wenhao Lin, and Yiyi Zhou. Fit and prune: Fast and training-free visual token In Proceedings of the AAAI Conference on Artificial pruning for multi-modal large language models. Intelligence, volume 39, pp. 2212822136, 2025. Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness. arXiv preprint arXiv:2405.17220, 2024. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pretraining. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1197511986, 2023. Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858, 2023. Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, and Lidong Bing. Openmmreasoner: Pushing the frontiers for multimodal reasoning with an open and general recipe, 2025a. URL https://arxiv.org/abs/2511.16334. Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024a. 18 Published in Transactions on Machine Learning Research (11/2025) Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, and Shanghang Zhang. [cls] attention is all you need for training-free visual token pruning: Make vlm inference faster. arXiv preprint arXiv:2412.01818, 2024b. Shaolei Zhang, Qingkai Fang, Zhe Yang, and Yang Feng. Llava-mini: Efficient image and video large multimodal models with one vision token. arXiv preprint arXiv:2501.03895, 2025b. Yuan Zhang, Chun-Kai Fan, Junpeng Ma, Wenzhao Zheng, Tao Huang, Kuan Cheng, Denis Gudovskiy, Tomoyuki Okuno, Yohei Nakata, Kurt Keutzer, et al. Sparsevlm: Visual token sparsification for efficient vision-language model inference. arXiv preprint arXiv:2410.04417, 2024c. Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024d. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. Retrieval-augmented generation for ai-generated content: survey, 2024. URL https://arxiv.org/abs/2402.19473. Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1369113701, 2025. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. MiniGPT-4: Enhancing visionlanguage understanding with advanced large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=1tZbq88f27. Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 19 Published in Transactions on Machine Learning Research (11/2025) Script: Graph-Structured and Query-Conditioned Semantic Token Pruning for Multimodal Large Language Models Appendix"
        },
        {
            "title": "A Notation Overview",
            "content": "To facilitate clarity and improve readability, we provide detailed glossary of all mathematical symbols used throughout this paper, as shown in Table A.1. The table serves as centralized reference point, systematically organizing every notation involved in the core components of the Script framework, including equations, algorithmic modules, and architectural definitions. The glossary covers all key components of the proposed framework, including visual encoding, query representation, relevance scoring, similarity computation, DPPbased token selection, structural pruning, and auxiliary constants. This table is intended to improve the clarity of mathematical derivations, reduce cross-referencing overhead, and support accurate reproduction and further development of the method. Table A.1: Detailed symbol glossary used throughout the Script framework. Definition Symbol Xv fv n, d, ℓ, Hv Hq Hv fφ L(, ) hi h(q) µ qi = diag(q1, . . . , qn) Diagonal matrix Sij = Q1/2SQ1/2 I, = L r, Vsrc, Vdst = (Vsrc, Vdst, S) d2 ui ei Rn τ ϵ , Scalar Matrix (n n) Matrix (n n) Index set Index set Matrix Vectors ( [0, 1]n) Token subsets Graph Scalar Vector ( RT ) Scalar Matrix Integer Threshold (scalar) Constant (106) Operator Operator Operator Type / Dim Image / video input Raw visual input passed into the vision encoder fv. Function Function Integers Matrix Sequence of vectors Matrix (m d) Function (LLM) Scalar loss Vector ( Rd) Vector ( Rd) Scalar Vision encoder extracting patch-level features from Xv. Multimodal projector mapping visual features to token embeddings. #tokens (input, dim, query length, retained) respectively. Sequence of visual tokens, each d-dimensional, before pruning. Tokenized user query of length ℓ. Visual token subset after pruning (m < n). Frozen language model consuming [ Hv; Hq]. Task loss comparing model outputs before and after pruning. i-th visual token embedding in Hv. Mean pooled embedding of the query tokens. Query-conditioned relevance score of token i. Relevance weighting matrix for visual tokens. Cosine similarity between tokens and j. Symmetric similarity matrix among visual tokens. DPP kernel combining relevance and diversity. Token indices selected for retention. Optimal index subset maximizing log-det of DPP kernel. Query-conditioned DPP kernel with temperature scaling. Raw and temperature-scaled relevance scores. Bipartite partitions used in graph-based redundancy pruning. Token similarity graph for structural filtering. Residual variance of token under low-rank projection. Coefficient vector for token in basis space. Normalizer in Cholesky-style update. Basis matrix for low-rank approximation of DPP kernel. Target rank or DPP subset size. Redundancy threshold for similarity pruning. Numerical stability term in projection update. Element-wise (Hadamard) product. Inner product used for cosine similarity. Euclidean norm used in normalization. 20 Published in Transactions on Machine Learning Research (11/2025) Diversity in Determinantal Point Processes: Geometric Intuition and Theoretical"
        },
        {
            "title": "Analysis",
            "content": "B.1 Notation. To facilitate clear and consistent exposition, we first define the key symbols and assumptions used throughout the paper and illustrate each with simple example or explanation. 2 (Euclidean norm): The standard ℓ2 norm for vectors in Rd. Example: (3, 4)2 = 32 + 42 = 5. λi(A) (i-th largest eigenvalue of A): For symmetric matrix Rkk, we order its eigenvalues as λ1(A) λ2(A) λk(A). Since all our kernels will be positive semi-definite (PSD), all λi 0. 1k (all-ones vector): k-dimensional column vector with all entries equal to one, i.e., (1, . . . , 1) Rk. Let = v1, . . . , vn Rd denote collection of feature vectors. We assume that each vi has unit norm: vi2 = 1 for all i. This means that each vector lies on the unit hypersphere in Rd. Example: The vector vi = (1, 0, . . . , 0) has vi2 = 1, and thus is valid member of . We adopt this unit-norm convention to ensure that inner products vj correspond exactly to cosine similarities, simplifying the interpretation of the similarity kernel introduced next. Meanwhile, this normalization remains fixed throughout the paper. Similarity Kernel. Given collection of unit-norm vectors = [v1, . . . , vn] Rd as introduced above, we define similarity kernel matrix Rnn whose (i, j)-entry captures the cosine similarity between vi and vj: Lij = i vj, for all 1 i, n. (B.1) Since all vi lie on the unit hypersphere, we have similar two vectors are, the closer Lij is to 1. We can express the full kernel compactly using matrix notation. Let = [v1 v2 vn] Rdn be the matrix whose columns are the feature vectors. vj [1, 1]. In particular, Lii = vi = 1, and the more Then: = V. This representation immediately implies that is symmetric and positive semi-definite (PSD), because for any Rn, we have: (B.2) xLx = xV = x2 2 0. (B.3) Therefore, satisfies the standard spectral properties of Gram matrices: all eigenvalues of are real and non-negative, and admits an orthonormal eigendecomposition. These properties are critical for the DPP model discussed next. B.2 Determinant as Diversity Objective We now connect the determinant of kernel submatrix with geometric diversity. Specifically, we show that for any subset of unit vectors, the determinant of their Gram matrix equals the square of the volume of the parallelotope they span. Proposition 1 (DeterminantVolume Equivalence). Let = [i1, . . . , ik] [n] be an index subset of size k. Define VS = [vi1, . . . , vik VS Rkk as their Gram matrix. Then, ] Rdk as the submatrix of feature vectors indexed by S, and LS = det LS = det(V VS) = (Volk(VS))2, (B.4) 21 Published in Transactions on Machine Learning Research (11/2025) where Volk(VS) is the k-dimensional volume of the parallelotope spanned by the column vectors of VS. Proof. Let VS = QR be the QR decomposition of VS, where Rdk has orthonormal columns and Rkk is upper triangular. Then, using the fact that QQ = Ik. Taking determinants on both sides gives: LS = VS = RQQR = RR, det LS = det(RR) = (det R)2. Geometrically, the k-dimensional volume satisfies Volk(VS) = det(V VS) = det R, hence det LS = (Volk(VS))2. (B.5) (B.6) (B.7) This equivalence forms the foundation for interpreting det LS as natural diversity objective: higher values to sets of vectors that are more spread out or orthogonal in space. it assigns B.3 Diversity Upper Bound via Hadamard Inequality The determinant of any Gram matrix LS formed from unit-norm vectors is upper bounded by 1. This follows from the classical Hadamard inequality, which gives constraint on the determinant of positive semi-definite matrix in terms of its diagonal entries. Corollary 1 (Hadamard Bound). Let LS be the Gram matrix defined above, formed from unit vectors. Then: det LS 1, det LS = 1 ip viq = 0 for all = and d. (B.8) Proof. Hadamards inequality states that for any positive semi-definite matrix Rkk with diagonal entries aii, det a11a22 akk, with equality if and only if the columns of are orthogonal. In our case, LS = Therefore, VS is PSD and satisfies diag(LS) = (1, 1, . . . , 1) since each vi2 = 1. with equality if and only if the vectors vi1 , . . . , vik then rank(LS) < and det LS = 0 < 1. are mutually orthogonal; this requires d. If > d, det LS 1, (B.9) The determinant det LS quantifies how linearly independent or spread out the vectors in are. The maximum value 1 occurs when the vectors are exactly orthogonal. Therefore, maximizing det LS promotes diversity by encouraging the selection of vectors that are as close to orthogonal as possible. B.4 Determinantal Point Processes and Diversity Maximization We now formally introduce Determinantal Point Processes (DPPs), which are probability distributions over subsets that inherently promote diversity. In the fixed-size or k-DPP setting, each subset of size is assigned probability proportional to the determinant of its associated submatrix LS: PL(S) det LS, for = k. (B.10) 22 Published in Transactions on Machine Learning Research (11/2025) Specifically, we formulate this as: PL(S) = det LS det LT . [n] =k (B.11) where the denominator sums over all k-subsets of [n], ensuring valid probability distribution. This probabilistic model encourages the selection of sets whose vectors are geometrically diverse, since higher determinant values correspond to higher volume (as shown previously). In practice, one often seeks the most probable subset under this model, known as the maximum posteriori (MAP) estimate: SMAP = arg max S=k det LS. (B.12) This optimization problem aims to find the subset of vectors that span the largest volume parallelotope in Rd, or equivalently, the subset exhibiting maximal geometric diversity. The DPP framework thus provides both probabilistic model and concrete objectivedet LSfor achieving diverse subset selection. B.5 Redundancy Metrics and Determinant Bounds While the determinant det LS captures diversity, we can also assess subset quality via redundancy metrics. These measure how similar the vectors in are to each other, either in the worst-case or on average. Two commonly used metrics are: ρmax(S) = max i=jS Lij, ρavg(S) = 2 k(k 1) Lij. i<jS (B.13) vj is the cosine similarity between unit vectors vi and vj. The value ρmax(S) measures the Here, Lij = most redundant pair (i.e., most similar), while ρavg(S) captures the overall similarity level. We now relate these redundancy measures to det LS using spectral bounds. The first bound is derived using Gershgorins circle theorem. Lemma 1 (Gershgorin-Based Lower Bound). For any subset of size k, define ρ(S) := maxi=jS Lij. Then: λmin(LS) 1 (k 1) ρ(S), det LS [1 (k 1) ρ(S)] +. (B.14) Proof. For each row of LS, the Gershgorin radius is ri = theorem, every eigenvalue of LS lies in at least one disk centered at 1 with radius ri, hence j=i Lij (k 1)ρ(S). By Gershgorins circle λmin(LS) 1 max ri 1 (k 1)ρ(S). Since LS is symmetric PSD, its determinant is the product of eigenvalues, so det LS = i=1 λi(LS) (λmin(LS))k [1 (k 1)ρ(S)] +. Next, we upper bound det LS via the arithmetic meangeometric mean inequality. Lemma 2 (AMGM Upper Bound). Let λ1, . . . , λk be the eigenvalues of LS. Then: det LS (cid:19)k (cid:18) tr(LS) = 1. 23 (B.15) Published in Transactions on Machine Learning Research (11/2025) Discussion. This bound follows directly from the arithmetic meangeometric mean inequality: the product of nonnegative eigenvalues is at most the k-th power of their average. Since LS is Gram matrix of unitnorm vectors, all diagonal entries are 1, so tr(LS) = k. Hence, the right-hand side reduces to 1, independent of the actual off-diagonal similarities. Therefore, the AMGM bound is extremely loose: it only states the trivial fact that det LS 1, without reflecting the effect of redundancy or ρavg(S). This motivates the refined spectral bound derived next, which captures how det LS decays with increasing average similarity. Motivation for Refined Bound. The AMGM bound above provides worst-case envelope that does not incorporate any spectral structure of LS beyond its average similarity. However, when all off-diagonal entries of LS are equal to ρavg, the matrix exhibits an extremal spectral configuration that maximizes the determinant under fixed average similarity. By explicitly analyzing this case, we obtain refined upper envelope that better captures how the maximum achievable det LS decays with increasing redundancy. Lemma 3 (Refined Upper Bound via Spectral Construction). Let LS be the Gram matrix of unit vectors with average off-diagonal similarity ρavg(S) [ 1 k1 , 1). Then, det LS (1 + (k 1)ρavg(S)) (1 ρavg(S))k1. (B.16) Proof. Consider the class of symmetric PSD matrices LS where all off-diagonal entries are equal to ρavg, and all diagonal entries are 1: (LS)ij = (1 ρavg if = if = j, which is feasible iff ρavg [ 1 equals 1 + (k 1)ρavg, and the remaining (k 1) eigenvalues are 1 ρavg. Hence, the determinant is: k1 , 1). This matrix has known spectral decomposition: one eigenvalue det LS = (1 + (k 1)ρavg) (1 ρavg)k1. Among all PSD matrices with unit diagonal and fixed average off-diagonal similarity, the functional log det() is concave and permutation-invariant; by symmetry, an optimizer (for maximizing det under this constraint) must be permutation-invariant, i.e., the equicorrelation matrix above. Thus, the displayed value yields an upper bound, establishing the claim. This refined bound is an upper bound for all positive semi-definite matrices with unit diagonal and given average off-diagonal similarity, and it is tight: the equicorrelation matrix attains equality. Among such uniformly redundant matrices, the bound is achieved exactly, making it sharp envelope for assessing diversity degradation as average correlation increases. Compared to Lemma 2, this refined bound is strictly tighter whenever ρavg(S) > 0, as it directly models the spectral behavior of uniformly redundant configurations. The bound thus provides sharper envelope for analyzing diversity degradation in highly correlated subsets. B.6 Global Optimality of the MAP Subset Proposition 2 (MAP subset need not minimize redundancy). arg maxS=k det LS need not minimize either ρmax or ρavg among all size-k subsets. In general, k-subset = Proof. Consider = 2 with = (cid:18)1 (cid:19) , [1, 1]. Then det = 1 c2 is maximized at = 0, where ρmax = 0. For = 1 2 (strictly smaller), yet det = 1 1 = 3 4 (nor ρavg). 2 < 0 4 < 1. Hence maximizing det does not generally minimize ρmax , we have ρmax = 1 Published in Transactions on Machine Learning Research (11/2025) o p L 1 0. 0.6 0.4 0.2 0 0 AMGM Bound: 1 Refined Bound: (1 + 4ρ)(1 ρ) 0.2 0.4 0.6 0.8 ρavg Figure B.1: Upper envelope on det LS vs. average similarity ρavg for = 5. Unlike the trivial AMGM bound (which is constantly 1), the refined envelope (1 + 4ρ)(1 ρ)4 decreases with redundancy, revealing how the maximum attainable determinant shrinks as ρavg grows. B.7 Approximation via Greedy MAP Inference Exact MAP inference for k-DPP is NP-hard, as it requires evaluating (cid:0)n regularized objective (cid:1) determinants. However, the (S) = log det(I + LS) is known to be monotone and submodular when is positive semi-definite, which allows for natural greedy approximation algorithm. The function log det(I + LS) is monotone and submodular when is PSD, as it arises from the entropy of Gaussian variables or from spectral submodular theory. Let bS denote the greedy solution obtained by iteratively selecting vectors that offer the greatest marginal increase in log det(I +LS). Then, by the classical NemhauserWolsey theorem for submodular maximization: (B.17) log det(I + ) (1 1/e) log det(I + LS ), bS where is the exact optimal size-k subset for (S) = log det(I + LS). Exponentiating both sides gives multiplicative guarantee on the regularized determinant: Translating such guarantees into bounds on ρmax or ρavg requires additional structural assumptions on (e.g., incoherence or equicorrelation); we therefore refrain from stating general redundancy guarantees here. det(I + ) (det(I + LS ))11/e. bS (B.18) B.8 Sufficient Condition for Equivalence The counterexample in Proposition 2 shows that, in general, the MAP solution of k-DPP need not minimize redundancy measures such as ρmax or ρavg. Nevertheless, under additional structural assumptions, maximizing geometric diversity and minimizing redundancy do become equivalent. We now state sufficient condition under which this equivalence holds. Proposition 3 (Equivalence under Equicorrelation). Suppose that for each candidate subset of size k, the associated Gram matrix LS has the form (LS)ij = (1 if = j, ρ if = j, 25 Published in Transactions on Machine Learning Research (11/2025) for some ρ [0, 1). Then: arg max S=k det LS = arg min S=k ρmax(S) = arg min S=k ρavg(S). In other words, the MAP subset simultaneously maximizes geometric diversity and minimizes redundancy. Proof. For such an equicorrelation matrix, the eigenvalues are known in closed form: Therefore, λ1 = 1 + (k 1)ρ, λ2 = = λk = 1 ρ. det LS = (cid:0)1 + (k 1)ρ(cid:1)(1 ρ)k1. Since ρmax(S) = ρavg(S) = ρ in this setting, it suffices to analyze the monotonicity of det LS with respect to ρ. Taking logarithms and differentiating, dρ log det LS = 1 1 + (k 1)ρ 1 1 ρ < 0, for ρ (0, 1). Thus det LS decreases strictly with ρ, and maximizing the determinant is equivalent to minimizing ρ. The claim follows. This proposition establishes clean sufficient condition under which DPP-based MAP inference exactly aligns with redundancy minimization: in equicorrelated settings, maximizing diversity volume and minimizing redundancy are equivalent optimization goals. B.9 Summary. This section establishes theoretical link between geometric diversity, measured through the determinant of kernel (Gram) submatrix, and redundancy metrics such as ρmax and ρavg. The analysis has revealed three central insights: 1. Maximizing det LS systematically favors subsets of vectors that are nearly orthogonal to one another, thereby reducing both worst-case and average redundancy. 2. In general, the maximum posteriori (MAP) solution of the k-DPP maximizes geometric diversity (via det LS), but it does not necessarily minimize ρmax or ρavg; rather, large determinants enforce envelope-type constraints on admissible redundancy (Proposition 2). 3. However, under additional structuremost notably the equicorrelation condition formalized in Proposition 3the MAP solution does coincide with redundancy minimization: maximizing det LS is then equivalent to minimizing ρmax and ρavg. 4. When exact MAP inference is computationally infeasible, greedy algorithm on the monotone submodular surrogate (S) = log det(I + LS) provides constant-factor guarantees on the regularized determinant, thereby preserving much of the diversity of the global optimum. Taken together, these results justify the determinant objective as principled and effective diversity metric, one that offers both rigorous theoretical foundations and practical algorithmic strategies for subset selection. This theoretical perspective provides solid basis for applications where balancing diversity and redundancy is essential. 26 Published in Transactions on Machine Learning Research (11/2025) Application to Script. The diversity metric and DPP framework developed here directly ground the Script method proposed in the main text. Script is designed for practical, query-conditioned multimodal setting, yet its core selection principle remains the same: maximize the determinant of positive semidefinite kernel submatrix in order to favor diverse and complementary elements. In Script, the kernel is defined as = diag(r) diag(r), (B.19) where encodes tokentoken similarities (e.g., cosine similarity), and Rn stores query-conditioned 0 relevance scores. Because is symmetric positive semidefinite and diag(r) is diagonal with nonnegative entries, the resulting kernel remains positive semidefinite under the congruence transformation. This ensures that the spectral properties required for DPP inference are preserved, even when relevance weights vary across tokens. Crucially, the token similarity structure in Script satisfies the equicorrelation-type condition analyzed in Proposition 3. Consequently, the MAP subset = arg max S=k det(LS), (B.20) not only maximizes geometric diversity but also simultaneously minimizes redundancy. In other words, Script inherits the best of both worlds: the probabilistic diversity guarantee of DPPs and the redundancy minimization property under its structural assumptions. This explains why Script can aggressively prune tokens while preserving both relevance and diversity. Empirical results presented in Section 5 confirm that maximizing det(LS) with query-conditioned kernel yields subsets that are simultaneously relevant, non-redundant, and diverse, thereby improving both summarization and retrieval performance. 27 Published in Transactions on Machine Learning Research (11/2025)"
        },
        {
            "title": "C Details of experimental setup",
            "content": "C.1 Model Architectures LLaVA-1.5 (Liu et al., 2023) The LLaVA series represents foundational line of open-source visionlanguage models (VLMs), recognized for their simple design, low training cost, and strong performance. The original LLaVA architecture integrates pretrained CLIP (Radford et al., 2021) as the visual encoder and Vicuna (Chiang et al., 2023) as the language model, connected via linear projection layer. This enables the LLM to accept image grid features as input. Through visual instruction tuning, LLaVA gains the ability to handle multimodal tasks. LLaVA-1.5 enhances this framework by replacing the linear connector with multi-layer perceptron (MLP), increasing input image resolution, and utilizing broader and more diverse set of instruction tuning data. These modifications lead to substantial performance improvements. The model processes images at resolution of 336336, resulting in 576 visual tokens per image. LLaVA-NeXT (Liu et al., 2024a) LLaVA-NeXT (also referred to as LLaVA-1.6) builds upon LLaVA-1.5 by introducing dynamic resolution mechanism aimed at improving visual perception. Instead of using fixed resolution, the model selects an optimal aspect ratio based on the original image and increases its resolution by up to 4. Importantly, the visual encoder remains unchanged. To handle the higher-resolution inputs, the image is divided into multiple sub-images of the original size. Each sub-image is encoded independently, and the resulting visual tokens are concatenated and passed to the language model. This approach enhances the models performance in tasks such as visual reasoning, optical character recognition (OCR), and knowledge-intensive questions. For consistency and fair comparison, we fix the resolution to 672672 (4 the original), generating 2,880 visual tokens per image. LLaVA-Video (Zhang et al., 2024d) LLaVA-Video is variant of the LLaVA family designed specifically for video understanding. It introduces the SlowFast frame sampling strategy to balance the number of frames and the density of visual tokens. The model utilizes SigLIP (Zhai et al., 2023) as the visual encoder and processes video frames at 384384 resolution, encoding each frame into 729 visual tokens. To reduce computational load, 22 average pooling operation is applied to the grid features, effectively reducing the number of visual tokens by factor of 4. During evaluation, we uniformly sample 64 frames per video, resulting in total of 10,816 visual tokens. This design allows LLaVA-Video to efficiently model both spatial and temporal aspects of visual input. InternVL3 (Zhu et al., 2025) One of the most advanced open-source MLLMs at present. Building upon its predecessor, InternVL2.5, it retains the ViT-MLP-LLM architecture, integrating Vision Transformer InternVL3 features native multimodal prewith large language model through an MLP connector. training paradigm, jointly acquiring linguistic and multimodal capabilities in single stage. It incorporates Variable Visual Position Encoding to handle extended multimodal contexts and employs advanced training techniques like supervised fine-tuning and mixed preference optimization. InternVL3 demonstrates superior performance across wide range of multimodal tasks, including tool usage, GUI agents, industrial image analysis, and 3D vision perception. C.2 Evaluation Benchmarks C.2.1 General Image Benchmarks VQAv2 (Goyal et al., 2017) An open-ended visual question answering benchmark that evaluates models ability to understand images, natural language, and commonsense knowledge. It contains 265,016 images from the COCO dataset (Lin et al., 2014) and abstract scenes, with each image paired with an average of 5.4 questions. Each question is annotated with 10 ground truth answers and 3 plausible alternatives. We use the test-dev split for evaluation. GQA (Hudson & Manning, 2019) large-scale and widely used VQA benchmark based on real-world images from the Visual Genome dataset (Krishna et al., 2017), specifically designed to test compositional reasoning and fine-grained visual understanding. It provides over 22 million balanced question-answer pairs, with each image accompanied by detailed scene graph explicitly describing objects, attributes, and relationships. For our experiments, we evaluate on the standard test-dev balanced split. 28 Published in Transactions on Machine Learning Research (11/2025) VizWiz (Gurari et al., 2018) real-world VQA benchmark created from images taken by blind users, paired with spoken questions and 10 crowd-annotated answers each. It introduces two main challenges: answering questions and detecting unanswerable ones, due to issues like poor image quality and ambiguous content. We use the test split for evaluation. ScienceQA (Lu et al., 2022) multimodal, multiple-choice QA benchmark covering diverse scientific domains. It includes 21,208 questions categorized across 26 topics, 127 categories, and 379 skills. Nearly half of the questions include image or text context, and the majority are supplemented with grounded lectures and detailed explanations. We evaluate using the test split for evaluation. POPE (Li et al., 2023b) benchmark focused on evaluating object hallucination in MLLMs. Using images from COCO, it formulates binary questions regarding the presence of specific objects in the scene. Precision, recall, and F1 score are used to quantify hallucination. We use the test split for evaluation. MME (Fu et al., 2023) broad benchmark assessing the perceptual and cognitive abilities of multimodal large language models comprises 14 subtasks across perception (e.g., counting, color, position, OCR) and cognition (e.g., commonsense reasoning, translation, code understanding). All binary instruction-answer pairs are manually constructed to avoid data leakage, ensuring rigorous evaluation. MMBench (Liu et al., 2024c) comprehensive benchmark designed to evaluate wide range of multimodal capabilities. It features large and diverse set of questions, surpassing prior benchmarks in scale and coverage. novel CircularEval strategy, powered by ChatGPT, converts open-ended responses into structured formats for consistent scoring. Chinese version, MMBench-CN, is also provided. MM-Vet (Yu et al., 2023) benchmark emphasizing the integration of diverse multimodal skills. It defines six core capabilities: recognition, OCR, knowledge, language generation, spatial reasoning, and mathematics, via 218 carefully designed challenging examples. Evaluation is conducted using ChatGPT to ensure consistency across varied real-world answer formats. HallusionBench (Guan et al., 2024) An image-context reasoning benchmark crafted to expose two frequent failure modes of large visionlanguage models: language hallucination (answers driven by strong linguistic priors that contradict the image) and visual illusion (misleading visual features that produce confident yet wrong responses). Comprising carefully designed examples that remain challenging for GPT4V and LLaVA-1.5, it enables fine-grained diagnosis of how VLMs over-trust language or under-exploit vision, offering insights for building more faithfully grounded models. C.2.2 Text-Oriented Benchmarks TextVQA (Singh et al., 2019) benchmark designed to evaluate models ability to read and reason about text embedded in images. Sourced from the Open Images v3 dataset (Krasin et al., 2017), it includes scenes rich in textual content such as signs and packaging. The benchmark emphasizes integration of OCR with visual and linguistic reasoning. We use the validation split for evaluation. AI2D (Kembhavi et al., 2016) diagram-based question answering benchmark consisting of over 5,000 grade school science diagrams, annotated with more than 150,000 structured labels and ground-truth syntactic parses. It also includes over 15,000 multiple-choice questions aligned with the diagrams, enabling research on visual reasoning and diagram understanding in scientific contexts. We use the test split for evaluation. ChartQA (Masry et al., 2022) large-scale benchmark designed for question answering over charts, focusing on complex reasoning that involves both visual interpretation and logical or arithmetic operations. It includes 9.6K human-written questions and 23.1K questions generated from chart summaries. Unlike prior template-based benchmarks, ChartQA challenges models to perform multi-step reasoning using both the visual content and underlying data tables of charts, highlighting the need for advanced multimodal understanding. We use the test split for evaluation. OCRBench (Liu et al., 2024d) comprehensive evaluation benchmark assessing the OCR capabilities of large multimodal models. It comprises 29 datasets across diverse text-related visual tasks, including text recognition, scene text-centric VQA, document-oriented VQA, key information extraction, and handwritten mathematical expression recognition. Published in Transactions on Machine Learning Research (11/2025) C.2.3 Video Benchmarks MLVU (Zhou et al., 2025) The first large-scale benchmark for long video understanding. It features videos ranging from 3 minutes to 2 hours and spans nine tasks covering holistic, single-detail, and multidetail understanding. Both multiple-choice and open-ended questions are included. We report performance using the M-Avg metric. MVBench (Li et al., 2023a) benchmark tailored for evaluating temporal reasoning in video comprehension. It includes 20 carefully designed tasks requiring dynamic understanding across multiple frames. MVBench introduces static-to-dynamic transformation approach to systematically test temporal understanding. We use the test split for evaluation. LongVideoBench (Wu et al., 2024) large-scale benchmark for evaluating understanding of long-form videos. It consists of 3,763 videos (up to 1 hour long), each accompanied by subtitles and 6,678 humanwritten multiple-choice questions across 17 categories. key feature is the referring reasoning task, where questions target specific video segments. We use the validation split for evaluation. Video-MME (Fu et al., 2025) comprehensive video benchmark featuring 900 expert-curated videos spanning 256 hours across six primary domains and 30 subfields. Videos range from 11 seconds to 1 hour in duration and include video, audio, and subtitles (not used during evaluation). It provides 2,700 expertannotated QA pairs designed to probe complex temporal and multimodal reasoning abilities. C.3 Comparison Methods C.3.1 Text-based Methods FastV (Chen et al., 2024a) The first work to identify inefficiencies in visual attention within MLLMs. Based on this observation, FastV proposes simple, training-free acceleration method: after the second transformer layer, it prunes portion of visual tokens with the lowest visual-text attention scores. This strategy significantly reduces computational cost during inference without retraining. SparseVLM (Zhang et al., 2024c) Inspired by multi-stage pruning methods such as PyramidDrop, SparseVLM introduces more fine-grained strategy that incorporates textual guidance. It observes that not all instruction tokens are equally informative for pruning visual tokens. Therefore, it first selects text tokens highly relevant to the visual content as raters,\" and uses their attention distribution to guide which visual tokens should be preserved or pruned, resulting in improved model efficiency and accuracy. C.3.2 Vision-based Methods TRIM (Song et al., 2025) Pruning based solely on visual input, while ignoring textual context, may lead to suboptimal decisions. TRIM addresses this issue by utilizing CLIP-based similarity. It computes cosine similarities between image tokens (from the visual encoder) and text tokens (from the text encoder), and uses these similarity scores to rank visual tokens by importance. Low-similarity tokens are pruned to accelerate inference without significant performance loss. VisionZip (Yang et al., 2025b) visual-only token pruning method that analyzes self-attention concentration in the visual encoder. VisionZip first selects dominant tokens with high self-attention weights. Then, it applies clustering on the remaining tokens to extract diverse contextual tokens. The union of dominant and contextual tokens is passed to the language model, aiming to preserve both saliency and diversity of visual content. C.3.3 Similarity-based Methods DivPrune (Alvar et al., 2025) DivPrune reformulates token pruning as Maximum Minimum Distance Problem (MMDP), aiming to select the most diverse subset of visual tokens. Rather than relying solely on attention or similarity scores, it explicitly maximizes the minimum pairwise distance among retained tokens, ensuring the selected tokens cover broad semantic space. This diversity-preserving strategy leads to robust performance under extreme token reduction. 30 Published in Transactions on Machine Learning Research (11/2025) Table D.1: Performance comparisons on InternVL3-8B Zhu et al. (2025) across 10 image understanding benchmarks. The best results in each setting are bolded, and the second-best are underlined. Method AI2D TextVQA ChartQA OCRBench HallBench MME MMB-EN MMB-CN Acc. Average InternVL3-8B 85.28 81.51 FastV DivPrune Script FastV DivPrune Script 82.21 80.88 82.87 77.35 76.45 79.88 74.34 64.70 75.97 63.55 55.44 67.65 85. 70.58 57.51 72.20 46.82 42.58 50.78 C.4 Implementation Details Upper Bound, All 1280 Tokens (100%) 2393.22 50.02 Retain 256 Tokens ( 80.0%) 632 477 640 48.45 38.63 48.98 2348,31 2249.17 2334.22 Retain 128 Tokens ( 90.0%) 426 378 42.75 37.57 44.46 2250.31 2166.22 2282.98 83.82 83.36 80.63 83.45 81.09 78.49 82.12 82. 84.22 100.0% 82.04 80.27 81.57 80.20 77.55 80.53 77.98 70.41 78.35 68.44 64.31 70. 92.6% 82.80% 93.03% 80.9% 75.75% 84.29% For image-based benchmarks, we adopt the official implementation of LLaVA4, loading the released checkpoints (e.g., LLaVA-1.5 and LLaVA-NeXT) and following the default preprocessing pipeline for image resizing (336336 or 672672), tokenization, and prompt formatting. All evaluations are conducted in zero-shot setting unless otherwise specified. For video-based benchmarks, we instead use the official implementation of LLaVA-NeXT5, which also supports LLaVA-Video. Videos are processed by uniformly sampling 64 frames, resizing each to 384384, and pooling visual tokens as described in the original LLaVA-Video paper. For evaluation, we further employ the lmms-eval toolkit6, which standardizes metric computation and dataset loading for long-form video understanding. By default, Script in GSP is configured with τ = 0.3 and γ = 5. All experiments are run on NVIDIA H100 GPUs (80GB) with bfloat16 precision. Moreover, to ensure fair comparison across models and pruning strategies, we keep inference batch size and decoding settings (temperature = 0.2, top-k = 1) consistent. Finally, each benchmark is evaluated 3 times, and we report the average as the final score. C.5 Licenses Table C.1 summarizes all benchmarks and software licenses employed in our study. The image understanding benchmarks such as GQA, VQAv2, TextVQA, and VizWiz are released under the CC BY 4.0 license, while others, including ScienceQA, MME, and POPE, adopt the MIT license, and recent large-scale resources such as MMWet, MMBench, and MVBench are distributed under Apache-2.0. For video reasoning, we rely on LongVideoBench and MLVU (Apache2.0) as well as VideoMME (MIT). On the software side, multimodal models like LLaVA and LLaVA-NEXT are covered by the Llama Community License, whereas InternVL3 is open-sourced under Apache-2.0. We have reviewed the terms of each license, ensured that all datasets and models are publicly available, and strictly limited their usage to noncommercial, academic research purposes. No proprietary or closed-access resources are included, which guarantees reproducibility, transparency, and compliance with community standards. Table C.1: License information for the scientific artifacts. Data Sources URL CC BY 4.0 Link Link ScienceQA Link VQAv2 Link TextVQA Link VizWiz Link MMVet Link MMBench Link MMBench-CN Link MME POPE Link LongVideoBench Link Link VideoMME Link MLVU Link MVBench License MIT CC BY 4.0 CC BY 4.0 CC BY 4.0 Apache-2.0 Apache-2.0 Apache-2.0 MIT MIT Apache-2.0 MIT Apache-2.0 Apache-2.0 Software Code URL Link LLaVA Link LLaVA-NEXT Link InternVL3 License Llama Community Licence Llama Community Licence Apache-2. 4https://github.com/haotian-liu/LLaVA 5https://github.com/LLaVA-VL/LLaVA-NeXT 6https://github.com/EvolvingLMMs-Lab/lmms-eval 31 Published in Transactions on Machine Learning Research (11/2025) Table D.2: Performance comparisons on LLaVA-1.5-13B Liu et al. (2024a) across 10 image understanding benchmarks. Method Venue VQAV2 GQA VizWiz SQAIMG VQAText POPE MME MMBEN MMBCN MMVet Acc. Average LLaVA-1.5-13B Liu et al. (2023) Nips23 80.10 63.35 53. 72.85 61.24 86.00 1531.25 68.55 63. 36.22 66.21 100% Upper Bound, 576 Tokens (100%), 3.817 TFLOPs FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed 75.53 76.34 76.83 77.41 77.61 77.87 65.73 73.12 73.75 75.20 73.12 76. 61.13 69.83 68.40 72.00 71.57 75.25 Retain 128 Tokens in Average ( 77.8%), 0.833 TFLOPs 58.13 59.14 57.79 59.21 59.46 59.27 54.46 49.87 52.03 53.35 51.74 52.49 74.32 72.14 73.68 72.28 74.23 73.29 58.76 55.08 58.39 58.40 59.93 58. 75.55 86.38 82.77 86.81 85.02 87.31 1460.56 1426.49 1449.42 1457.97 1487.59 1498.30 Retain 64 Tokens in Average ( 88.9%), 0.415 TFLOPs 51.39 57.85 56.23 57.91 55.91 59.64 53.48 49.23 53.12 54.49 52.17 53.55 73.01 72.00 74.29 71.67 72.09 72. 53.74 52.10 57.41 57.54 57.14 57.86 56.49 86.65 75.67 84.35 77.91 87.19 1246.54 1406.12 1379.66 1454.29 1374.35 1466.98 Retain 32 Tokens in Average ( 94.5%), 0.208 TFLOPs 48.36 55.67 52.71 56.20 54.05 58.75 51.68 48.58 53.09 54.55 51.54 53. 72.37 70.64 72.79 70.89 70.86 71.99 50.73 49.65 55.20 54.76 53.74 55.43 53.99 85.85 66.85 79.12 77.45 87.31 1198.33 1284.57 1257.67 1405.02 1327.37 1421.10 66.01 67.11 67.14 66.13 68.14 67.15 59.12 65.04 64.94 64.13 65.12 65. 54.27 63.13 61.32 61.47 62.88 63.79 62.23 58.14 62.95 60.74 62.36 61.35 55.18 52.57 61.33 59.87 60.23 58.72 53.22 45.45 55.58 57.12 58.91 56.36 32.78 35.13 36.01 34.41 35.92 36.12 26.89 27.98 33.40 29.31 32.94 36. 23.65 26.34 29.43 27.98 28.13 30.77 63.17 63.06 64.01 64.16 64.87 64.83 55.78 60.73 61.91 62.73 61.62 64.20 52.93 57.79 57.81 60.42 59.55 62.41 95.41% 95.24% 96.67% 96.90% 97.97% 97.64% 84.24% 91.72% 93.51% 94.74% 93.07% 96.96% 79.95% 86.28% 87.62% 91.25% 89.94% 94.26% Table D.3: Performance comparisons on LLaVA-Next-13B Liu et al. (2024a) across 8 image understanding benchmarks. The best results in each setting are bolded, and the second-best are underlined. Method Venue VQAV2 GQA VizWiz SQAIMG VQAText POPE MME MMBEN MMBCN MMVet Acc. Average LLaVA-NeXT-13B Liu et al. (2024a) Nips23 FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) FastV Chen et al. (2024a) TRIM Song et al. (2025) VisionZip Yang et al. (2025b) DivPrune Alvar et al. (2025) SparseVLM Zhang et al. (2024c) Script (Ours) ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed ECCV24 COLING25 CVPR25 CVPR25 ICML25 Proposed 82.30 79.34 79.34 79.37 80.34 79.49 81.14 66.98 75.94 76.18 78.13 76.75 79.64 62.67 72.15 72.45 75.64 74.62 77.85 Upper Bound, 576 Tokens (100%), 3.817 TFLOPs 85.20 64. 59.15 63.20 73.15 1539.50 68.55 61. 45.02 67.91 100% Retain 640 Tokens in Average ( 77.8%), 4.627 TFLOPs 60.89 63.19 62.79 63.54 62.74 64.22 56.44 54.31 56.12 56.73 57.55 57. 71.75 71.42 70.83 72.12 72.35 72.08 60.74 57.64 61.91 59.42 62.48 61.40 80.24 87.31 85.83 86.44 85.56 87.31 1536.7 1543.36 1529.22 1531.41 1573.74 1552.61 Retain 320 Tokens in Average ( 88.9%), 2.314 TFLOPs 54.36 61.13 60.37 61.28 60.91 63. 53.33 52.12 54.84 55.10 54.75 55.31 70.11 69.59 70.12 72.39 70.94 71.46 55.34 52.18 60.17 57.46 60.01 58.74 64.10 87.42 82.33 85.32 81.35 87.63 1288.0 1484.6 1417.13 1465.01 1491.46 1501.15 Retain 160 Tokens in Average ( 94.4%), 1.156 TFLOPs 50.79 58.93 57.81 60.30 59.79 62.32 52.03 51.12 52.35 53.15 52.38 53.19 69.11 69.19 69.47 71.44 69.89 71.37 51.74 49.12 58.69 56.43 55.89 56.47 62.95 87.00 76.38 81.29 80.92 88.83 1229.18 1392.30 1393.49 1436.17 1429.44 1476. 65.25 68.75 68.31 67.51 68.85 68.93 59.48 67.53 66.45 65.91 68.10 66.73 56.48 65.75 64.85 65.19 65.19 65.49 59.39 61.21 62.61 62.59 64.10 61.91 54.14 57.34 62.53 61.59 63.45 61.48 53.16 51.65 60.01 60.59 59.17 59. 43.68 42.34 46.98 39.10 41.35 47.13 30.52 33.41 41.21 39.42 39.36 42.32 27.36 27.68 35.39 37.34 36.48 40.34 65.45 66.26 67.13 66.42 67.31 67.87 57.27 63.09 64.50 64.98 65.05 66.16 54.77 60.21 61.70 63.31 62.58 64. 96.38% 97.57% 98.84% 97.81% 99.11% 99.95% 84.34% 92.90% 94.96% 95.68% 95.78% 97.42% 80.66% 88.66% 90.86% 93.22% 92.15% 95.65%"
        },
        {
            "title": "D Additional Experimental Results",
            "content": "D.1 Script for advanced open-source MLLM In addition to LLaVA, we further apply Script to one of the most advanced open-source MLLMs to date, InternVL3. The results are shown in Table D.1. Here, we fix the input resolution to 896896, yielding 1,280 visual tokens. Notably, unlike its performance on the LLaVA series, DivPrune exhibits significant performance drop on InternVL3, as it does not account for the relevance to user instructions during pruning. In contrast, our Script jointly considers both diversity and relevance, consistently achieving the best performance across different reduction ratios. Specifically, even when 90% of the visual tokens are removed, our method retains 83.9% of the original performance, 3% higher than the second-best FastV, demonstrating its effectiveness and adaptability in advanced MLLM architectures. D.2 Script for Large Parameters of Model To evaluate the effectiveness of our proposed method on larger language models, we apply Script to two models equipped with 13B LLMs: LLaVA-1.5-13B and LLaVA-NeXT-13B. The results are presented in 32 Published in Transactions on Machine Learning Research (11/2025) Table D.4: Hyperparameter sensitivity analysis of Scripts on LLaVA-1.5-7B with 64 tokens retained. The hyperparameters include the graph threshold τ (0.1, 0.3, 0.5, 0.7, 0.9), scaling factor γ (1, 10, 50, 100), and kernel choices (S vs. S). Bold is the default setting (Hyperparameter). Hyperparameter Benchamark VQAV2 GQA VizWiz SQAIMG VQAText POPE MME MMBEN MMBCN MMVet Average Relative 61.94 64. Upper Bound, 576 Tokens (100%), 3.817 TFLOPs 58.20 1507.06 78.50 86.96 69.41 58. 59.07 59.28 59.33 61.90 60.92 58.41 60.52 58.77 51.43 61.30 61.90 52.93 52.71 52.48 51.94 61.50 59.11 59.28 61.90 61.80 59.30 59.35 61.90 59.38 61.90 52.77 52.93 52.95 52.90 52.90 55.31 55.20 55.10 54.99 53.82 τ (graph threshold, 0.1, 0.3, 0.5, 0.7, 0.9) 1421.74 1412.08 1409.31 1394.89 1377. 73.98 66.88 68.65 75.08 74.28 68.00 74.72 67.45 74.11 67.34 84.76 86.95 85.16 84.05 85.74 γ (Scaling factor) 86.80 86.95 86.75 86.60 86.90 74.55 66.40 68.65 75.08 74.68 67.32 73.88 67.75 74.35 68.20 Kernel Choices (S vs. S) 1399.22 1412.08 1415.20 1407.75 1412.02 53.90 55.20 55.70 54.75 54.66 59.58 59. 51.75 60.30 61.90 52.93 1399.20 1412.08 85.40 86.95 66.25 75.30 68.65 75.08 54.40 55.20 50. 31.82 63.47 100% 54.44 54.31 54.07 53.98 53.99 53.50 54.31 54.10 54.20 54.30 53.11 54. 28.76 29.96 29.06 28.77 29.41 29.87 29.96 29.20 29.55 30.30 28.77 29.96 60.71 61.49 61.01 60.56 60.45 60.83 61.49 61.25 61.13 61.35 60.47 61. 95.64% 96.88% 96.12% 95.42% 95.25% 95.85% 96.88% 96.51% 96.31% 96.66% 95.28% 96.88% 0.1 0.3 0.5 0.7 0.9 1 5 10 50 100 Table D.2 and Table D.3. The larger language models lead to significant performance improvements and also make MLLMs less sensitive to visual token pruning. Among various pruning strategies, text-attentionbased methods benefit the most from scaling up the language model, indicating that larger LLM brings more accurate attention. Across different types of pruning methods, Script consistently outperforms all other approaches under various reduction ratios. With 77.8% of visual tokens removed, our method retains 97.64% and 99.95% of the original performance on LLaVA-1.5-13B and LLaVA-NeXT-13B, respectively, demonstrating its effectiveness on larger language models. D.3 Hyperparameter sensitivity & guidance. To better understand the robustness of Scripts, we conduct hyperparameter sensitivity analysis under the setting where 64 tokens are retained. Specifically, we vary the graph threshold τ , the scaling factor γ in Eq. 9, and kernel choices (S vs. S). As shown in Table D.4, our method demonstrates remarkable robustness to the choice of threshold: across τ [0.1, 0.9], the average relative performance consistently remains above 95%, indicating minimal sensitivity to this hyperparameter. Moreover, performance varies smoothly and predictably with τ , without instability or abrupt drops, further confirming the stability and reliability of our approach under different settings. In practice, extremely low (0.1) or high (0.9) values lead to slightly worse performance, while moderate settings achieve better trade-off. Among them, τ = 0.3 yields the strongest overall results. For the scaling factor γ, all values produce competitive outcomes, but γ = 5 achieves the most consistent improvements across benchmarks. In terms of kernel design, consistently outperforms S, suggesting that it better captures redundancy patterns among tokens. In the comparison between and S, the former represents the redundancy of tokens informed by visual similarity, while the latter incorporates both visual redundancy and query relevance. Based on these observations, we use τ = 0.3, γ = 5, and as the default configuration. It is worth noting that in Script, the parameter in DPP is not treated as tunable hyperparameter but as direct control of the number of retained tokens. This is conceptually equivalent to the pruning ratio in GSP, with the correspondence: = (1 p) n, (D.1) where is the number of original tokens. In other words, while specifies the proportion of redundant tokens to be discarded, explicitly determines the number of tokens to be preserved by ranking redundancy from low to high. Detailed results under different settings are reported in Section 5 and Appendix D. 33 Published in Transactions on Machine Learning Research (11/2025) Table D.5: Intersection dynamics (GSP QCSP) Benchamark VQAV2 GQA VizWiz SQAIMG VQAText POPE MME MMBEN MMBCN MMVet Average 45.69 45.48 33.29 33.78 21.89 23. 15.75 16.45 10.73 11.16 52.44 48. 49.32 36.75 47.28 33.68 Retain 192 Tokens in Average ( 66.7%), 1.253TFLOPs 48.21 44.53 Retain 128 Tokens in Average ( 77.8%), 0.833 TFLOPs 37.51 31.55 Retain 64 Tokens in Average ( 88.9%), 0.415 TFLOPs 19.27 Retain 32 Tokens in Average ( 94.5%), 0.208 TFLOPs 11.95 Retain 16 Tokens in Average ( 97.3%), 0.103 TFLOPs 6.82 15. 21.32 11.77 19.38 14.85 11.57 26. 20.81 31.50 28.71 23.42 12.62 18. 42.12 15.51 11.72 9.84 54.40 40. 26.25 17.49 10.83 49.08 48.52 36. 35.73 24.58 23.61 18.44 16.42 12. 11.00 D.4 Overlap analysis between GSP and QCSP. GSP and QCSP are two complementary modules that operate from different perspectives and together determine which tokens are retained. Table D.5 quantifies the overlap size between the two modules under different retention budgets. Overall, the overlap ratio decreases steadily as the pruning ratio increases. This trend is intuitive: when fewer tokens are allowed to remain, it becomes more difficult for both modules to consistently select the same tokens, given their distinct selection criteria. For example, retaining 192 tokens leads to an average overlap of 66.7%, while retaining only 16 tokens reduces the overlap to 97.3%, indicating that QCSP dominates token retention under stricter pruning. Another noteworthy observation is that the overlap never reaches 100%, which necessitates the use of QCSP as fallback mechanism to ensure the desired number of tokens is met. As the pruning becomes more aggressive, QCSP contributes proportionally more to the final selection. At the same time, our ablation studies confirm that GSP provides complementary benefits by capturing structural redundancy patterns that QCSP alone may overlook. These results highlight the importance of the cooperative design: GSP guides pruning with global structural view, while QCSP ensures query relevance and satisfies token budget constraints. Their interaction achieves balance between efficiency and accuracy, with QCSP gradually taking on stronger role as the pruning ratio increases. D.5 Statistical Significance Experiments Table D.6: Statistical comparison between our method and other baselines on MME benchmark with ratain 160 tokens. StdDev denotes standard deviation. Reported p-values correspond to the paired one-sided ttest and Wilcoxon signed-rank test. All experiments are set with the significance level of α = 5%. Mean (%) StdDev To assess the robustness and statistical reliability of our method, we therefore conduct 20 independent runs using LLaVA-Next-7B under identical experimental settings with varying random seeds, as shown in Table D.6. All statistical comparisons are made consistently against the Script. To determine whether Script is statistically better than other methods, we accordingly perform paired one-sided ttests and Wilcoxon signed-rank tests. We explicitly evaluate the null hypothesis H0: Script performs the same as the others, against the alternative hypothesis H1: Script performs better. Since the resulting p-values are significantly lower than the significance threshold of α = 0.05, we can confidently reject the null hypothesis H0. This suggests that the observed performance difference is statistically significant. t-test Wilcoxon 9.54 107 9.54 107 9.54 107 9.54 107 9.54 107 1.00 FastV TRIM VisionZip DivPrune SparseVLM Script(Ours) 1.01 1043 8.96 1035 8.49 1036 1.90 1036 7.58 1035 1. 1076.81 1316.74 1349.28 1347.72 1369.62 1487.98 2.62 3.41 2.34 2.04 2.39 0.94 Method 34 Published in Transactions on Machine Learning Research (11/2025)"
        },
        {
            "title": "E Additional Visualization Results",
            "content": "Figure E.1: Visualizations of relevance scores and retained tokens. Each visualization illustrates the spatial attention allocated by models to regions corresponding to various textual instructions, demonstrating the capacity of pre-trained multimodal models to identify and focus on task-specific visual elements. E.1 Case study In this section, we provide additional visualizations that comprehensively illustrate the relevance scores and retained visual tokens as shown in Figures E.1 and E.2. These visualizations further emphasize the strengths of models utilizing language-image pre-training, showcasing their enhanced capability to align textual instructions accurately with the corresponding visual regions. Specifically, the visualizations depict clear and intuitive correspondences: for instance, when instructions involve detecting the presence of specific objects or counting individuals or animals, the relevance maps highlight pertinent regions directly associated with 35 Published in Transactions on Machine Learning Research (11/2025) Figure E.2: Visualizations of relevance scores and retained tokens. Each visualization illustrates the spatial attention allocated by models to regions corresponding to various textual instructions, demonstrating the capacity of pre-trained multimodal models to identify and focus on task-specific visual elements. these queries. Such precise spatial attention enables effective and efficient visual token pruning, significantly reducing redundancy by retaining only informative regions. This targeted retention not only improves computational efficiency but also boosts interpretability by transparently displaying the reasoning processes of multimodal large language models. Overall, these visualizations underscore the robustness and adaptability of pre-trained models in dynamically identifying and concentrating on task-specific visual details essential for accurate multimodal understanding. 36 Published in Transactions on Machine Learning Research (11/2025) Figure E.3: Visualization of structural redundancy under different thresholds. We show patch-level redundancy maps computed from CLIP-ViT features on images from the COCO dataset. Redundancy is defined as the average cosine similarity between each patch and its spatial neighbors. Red regions indicate high structural redundancy, while blue regions are more distinctive. As the threshold increases (from left to right: 0.1 to 0.9), the resulting selection becomes increasingly sparse, focusing on structurally salient regions. E.2 Redundancy Visualization Across Thresholds To better understand how structural redundancy varies across visual patches, we visualize the redundancy score maps under multiple thresholds (τ = 0.1, 0.3, 0.5, 0.7, 0.9), as shown in Fig. E.3. For each image, patch-wise redundancy score is computed by averaging the cosine similarity of each patch with its 8-connected spatial neighbors. Binary masks are then obtained by applying different thresholds τ to these scores. From these visualizations, several detailed observations emerge: 1. Very low thresholds saturate the map. At τ = 0.1, almost every patch is marked redundant, producing mask that is nearly saturated. This indicates that even weak local correlations are enough to pass the threshold, causing both background and structured regions to be flagged. Such masks preserve nearly all information but fail to distinguish between repetitive and unique content, providing little benefit for redundancy reduction. 2. Progressive sparsification with increasing thresholds. As τ increases to 0.3 and 0.5, the maps become less dense and begin to highlight differences between structurally homogeneous regions and structurally diverse ones. Patches in flat or repetitive textures remain marked as redundant, while more irregular patches start to drop out. This progression shows how the threshold acts as filter, gradually shifting the representation from inclusive to selective. 3. Emergence of discriminative structures at mid thresholds. Around τ = 0.5, the contrast between redundant and non-redundant patches becomes clearer. Large homogeneous regions are consistently retained as redundant, but edges, boundaries, and irregular textures are less frequently included. This suggests that mid thresholds achieve balance: they still capture redundancy for compression purposes, yet they allow distinctive features to remain available for recognition or further processing. 4. Strong filtering at high thresholds. At τ = 0.7 and especially τ = 0.9, the redundancy maps become extremely sparse. Only the most distinctive patches, which sharply differ from their neighbors, survive as non-redundant. This makes the mask highly selective, isolating unique structures while discarding most background and repeated patterns. However, such aggressiveness may also risk losing subtle contextual cues that are semantically useful. 5. Spatial clustering of redundancy. Across all thresholds, redundant patches tend not to appear as isolated points but rather form spatially contiguous clusters. This behavior indicates that redundancy is inherently local phenomenon, strongly driven by the short-range similarity between neighboring patches. It also reinforces the idea that large uniform areas will consistently be flagged together, rather than in scattered fragments. 37 Published in Transactions on Machine Learning Research (11/2025) Figure E.4: Error case analysis. Representative failure patterns when pre-trained multimodal models process task-specific queries, comparing query, Graph-Structured Pruning (GSP), Query-Conditioned Semantic Pruning (QCSP), model prediction, and ground truth. (a) Model capability: success when textual guidance suffices (GSP correct, QCSP correct). (b) Textvisual misalignment: errors from limited CLIP encoder alignment (GSP correct, QCSP incorrect). (c) Visual feature deficiency: failures due to inadequate CLIP visual representations, leading to incorrect grounding and relevance (GSP incorrect, QCSP incorrect). Summary of trade-offs. Threshold selection directly governs the density and informativeness of the redundancy maps. Low thresholds preserve nearly all content but provide little compression, while high thresholds isolate only the most distinctive patches but risk losing contextual information. Mid-level thresholds (e.g., τ 0.5) offer favorable compromise, retaining structural diversity while still filtering out redundant clusters. This motivates our pruning strategy, which leverages structural redundancy to maintain discriminative information while reducing redundancy. E.3 Error Analysis The case study in Figures E.1 and E.2 has provided an excellent visualization of success case. To provide more balanced perspective, it is equally important to include and discuss failure cases. These highlight the inherent limitations of current pruning strategies and offer insight into potential directions for improvement. As shown in Figure E.4, we highlight three illustrative types of errors, though in reality these categories are not mutually exclusive and some overlap may exist. (a) Model capability error. This case illustrates the inherent limitations of the underlying LLM. Both GSP and QCSP assign relatively high retention scores to the visual token corresponding to the remote control. However, the final prediction is still incorrect. This suggests that even when 38 Published in Transactions on Machine Learning Research (11/2025) query-relevant tokens are preserved, reasoning failures may occur due to the LLMs limited capacity in associating visual evidence with the query semantics. (b) Textvisual misalignment. Here, QCSP assigns higher scores to tokens related to the chairs under the parasol, while GSP distributes attention in scattered manner. The model incorrectly concludes that person is present. This reflects semantic drift issue, where token relevance scoring does not fully capture the nuanced mapping between textual concepts (person) and visual regions (chair), leading to confusion in visually similar contexts. Such errors highlight the need for stronger cross-modal grounding mechanisms that can better align text queries with the correct visual entities. (c) Compounded pruning error. In this example, both GSP and QCSP assign misleading values to tokens, causing the warning sign to be pruned or overlooked. As result, the system outputs an incorrect response. Unlike case (a), the error here stems from cumulative misjudgments at the pruning stage rather than downstream reasoning. This indicates that pruning errors can directly suppress crucial evidence, creating an information bottleneck that the LLM cannot recover from. In summary, these failure cases expose three distinct but complementary challenges: (i) intrinsic reasoning limits of the base LLM, (ii) misalignment between text semantics and visual grounding, and (iii) error propagation from aggregated pruning strategies. Together, they emphasize that while Script substantially improves efficiency and often preserves accuracy, there remain scenarios where query-specific signals are underutilized or entirely lost. Future work could address these issues by incorporating adaptive confidence calibration, multimodal consistency checks, or dynamic pruning thresholds that adjust based on query difficulty. Limitations and Future Work. One key limitation of our work is that the proposed pruning method requires direct access to encoded visual tokens during inference, which restricts its applicability to open-source Multimodal Large Language Models. By contrast, widely used proprietary systems such as ChatGPT, Gemini, and Claude are closed black boxes where intermediate visual features are inaccessible. Although these models also incur high computational costs for visual reasoning, our method cannot be deployed in such settings. Another limitation is that our evaluation is limited to visionlanguage inputs, assuming queries and context are restricted to text and visual tokens. We do not consider other modalities such as audio, depth, or IMU signals. Multimodal tasks like audio-grounded dialogue, audio-visual question answering, or egocentric video reasoning introduce challenges beyond visionlanguage settings. For instance, audio streams are temporally dense and require alignment between speech content, prosody, and visual context, while depth and IMU data carry fine-grained geometric and motion cues that evolve continuously over time. Efficient pruning in such scenarios would require not only modality-specific encoders but also redundancy metrics that capture temporal correlations (e.g., repeated acoustic frames or redundant motion patterns) rather than purely spatial visual redundancy. While exploring these directions would broaden the scope of our approach, they fall outside the primary focus of this work and are left for future investigation. Moreover, while our method is compatible with advanced open-source MLLMs such as Qwen2.5-VL and InternVL3, we observe that these models are more sensitive to visual token pruning compared to the LLaVA series. Under the same pruning ratio, they exhibit more significant performance degradation. This likely stems from the fact that such architectures already incorporate internal visual token compression techniques, such as pixel unshuffle or token merging, which reduce redundancy prior to pruning. As result, additional pruning may lead to excessive information loss. Adapting pruning strategies to better suit these optimized architectures, for example, through pruning-aware training, modality-specific heuristics, or model-adaptive redundancy metrics, remains an important direction for future research. Finally, our method requires manually selecting the threshold τ . Although our sensitivity analysis (Table D.4) shows relatively stable performance across wide range of values and we provide default recommendations, the threshold remains fixed rather than adaptively determined. Future work could explore automated or learning-based strategies for setting τ , such as validation-driven calibration, distribution-aware rules (e.g., 39 Published in Transactions on Machine Learning Research (11/2025) Otsu or quantile-based selection on similarity scores), or meta-learning schemes that condition τ on image/- query characteristics. Such strategies may further improve robustness under domain shift and heterogeneous token redundancy profiles. We believe addressing this limitation will enhance the practicality and generalizability of pruning in multimodal systems."
        },
        {
            "title": "G Broader Impacts",
            "content": "Multimodal large language models have demonstrated remarkable success across wide range of domains, including education, accessibility, robotics, and content creation. Despite their capabilities, these models often incur high inference costs, particularly when processing high-resolution images or extended video sequences. Such computational demands pose substantial challenges for real-world deployment. In this work, we introduce simple yet effective visual token pruning strategy that enhances inference efficiency without requiring any additional training. By selectively removing redundant visual inputs, our method significantly reduces computational overhead and deployment costs. This improvement facilitates the deployment of MLLMs on resource-constrained platforms, such as mobile devices and edge computing environments, thereby promoting broader accessibility and scalability in practical applications. However, it is important to note that improving computational efficiency does not inherently address the ethical challenges associated with MLLMs. Our method does not mitigate risks related to potential misuse, such as the generation of harmful content or the dissemination of misinformation. As such, continued research and policy development are essential to ensure the responsible and safe use of increasingly efficient multimodal language models."
        }
    ],
    "affiliations": [
        "BCML, Heriot-Watt University"
    ]
}