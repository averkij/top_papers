{
    "paper_title": "OneRec: Unifying Retrieve and Rank with Generative Recommender and Iterative Preference Alignment",
    "authors": [
        "Jiaxin Deng",
        "Shiyao Wang",
        "Kuo Cai",
        "Lejian Ren",
        "Qigen Hu",
        "Weifeng Ding",
        "Qiang Luo",
        "Guorui Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, generative retrieval-based recommendation systems have emerged as a promising paradigm. However, most modern recommender systems adopt a retrieve-and-rank strategy, where the generative model functions only as a selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with a unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the user's historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixture-of-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) a session-wise generation approach. In contrast to traditional next-item prediction, we propose a session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, a recommendation system typically has only one opportunity to display results for each user's browsing request, making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design a reward model to simulate user generation and customize the sampling strategy. Extensive experiments have demonstrated that a limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, achieving a 1.6\\% increase in watch-time, which is a substantial improvement."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 1 5 6 9 8 1 . 2 0 5 2 : r OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Jiaxin Deng KuaiShou Inc. Beijing, China dengjiaxin03@kuaishou.com Lejian Ren KuaiShou Inc. Beijing, China renlejian@kuaishou.com Shiyao Wang KuaiShou Inc. Beijing, China wangshiyao08@kuaishou.com Qigen Hu KuaiShou Inc. Beijing, China huqigen03@kuaishou.com Kuo Cai KuaiShou Inc. Beijing, China caikuo@kuaishou.com Weifeng Ding KuaiShou Inc. Beijing, China dingweifeng@kuaishou.com Qiang Luo KuaiShou Inc. Beijing, China luoqiang@kuaishou.com Guorui Zhou KuaiShou Inc. Beijing, China zhouguorui@kuaishou.com Abstract Recently, generative retrieval-based recommendation systems (GRs) have emerged as promising paradigm by directly generating candidate videos in an autoregressive manner. However, most modern recommender systems adopt retrieve-and-rank strategy, where the generative model functions only as selector during the retrieval stage. In this paper, we propose OneRec, which replaces the cascaded learning framework with unified generative model. To the best of our knowledge, this is the first end-to-end generative model that significantly surpasses current complex and well-designed recommender systems in real-world scenarios. Specifically, OneRec includes: 1) an encoder-decoder structure, which encodes the users historical behavior sequences and gradually decodes the videos that the user may be interested in. We adopt sparse Mixtureof-Experts (MoE) to scale model capacity without proportionally increasing computational FLOPs. 2) session-wise generation approach. In contrast to traditional next-item prediction, we propose session-wise generation, which is more elegant and contextually coherent than point-by-point generation that relies on hand-crafted rules to properly combine the generated results. 3) an Iterative Preference Alignment module combined with Direct Preference Optimization (DPO) to enhance the quality of the generated results. Unlike DPO in NLP, recommendation system typically has only one opportunity to display results for each users browsing request, Equal contribution. Corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX making it impossible to obtain positive and negative samples simultaneously. To address this limitation, We design reward model to simulate user generation and customize the sampling strategy according to the attributes of the recommendation systems online learning. Extensive experiments have demonstrated that limited number of DPO samples can align user interest preferences and significantly improve the quality of generated results. We deployed OneRec in the main scene of Kuaishou, short video recommendation platform with hundreds of millions of daily active users, achieving 1.6% increase in watch-time, which is substantial improvement. CCS Concepts Information systems Computational advertising; Multimedia information systems. Keywords Generative Recommendation, Autoregressive Generation, Semantic Tokenization, Direct Preference Optimization ACM Reference Format: Jiaxin Deng, Shiyao Wang, Kuo Cai, Lejian Ren, Qigen Hu, Weifeng Ding, Qiang Luo, and Guorui Zhou. 2018. OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 10 pages. https: //doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nTo balance efficiency and effectiveness, most modern recommender\nsystems adopt a cascade ranking strategy[6, 26, 34, 43]. As illus-\ntrated in Figure 1(b), a typical cascade ranking system employs a\nthree-stage pipeline: recall [6, 19, 54], pre-ranking [28, 46], and rank-\ning [2, 3, 15, 16, 33, 52, 53]. Each stage is responsible for selecting\nthe top-ùëò items from the received items and passing the results to\nthe next stage, collectively balancing the trade-off between system\nresponse time and sorting accuracy.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Deng et al. proposed method. We also conduct series of ablation experiments to demonstrate the effectiveness of each module in detail. The main contributions of this work are summarized as follows: To overcome the limitations of cascade ranking, we introduce OneRec, single-stage generative recommendation framework. To the best of our knowledge, this is one of the first industrial solutions capable of handling item recommendations with unified generation model, significantly surpassing the traditional multi-stage ranking pipeline. We highlight the necessity of model capacity and contextual information of target items through session-wise generation manner, which enables more accurate predictions and enhances the diversity of generated items. We propose novel self-hard negative samples selection strategy based on personalized reward model. With direct preference optimization, we enhance OneRecs generalization across broader range of user preference. Extensive offline experiments and online A/B testing demonstrates their effectiveness and efficiency."
        },
        {
            "title": "2 Related Work\n2.1 Generative Recommendation\nIn recent years, with the remarkable progress in generative models,\ngenerative recommendation has received increasing attention. Un-\nlike traditional embedding-based retrieval methods which largely\nrely on a two-tower model for calculating the ranking score for each\ncandidate item and utilize an effecient MIPS or ANN [14, 17, 21, 31,\n38] search system for retrieving top-ùëò relevant items. Generative\nRetrieval (GR) [41] method formulates the problem of retrieving\nrelevant documents from the database as a sequence generation\ntask which generate the relevant document tokens sequentially.\nThe document tokens can be the document titles, document IDs\nor pre-trained semantic IDs [42]. GENRE [8] first adopts the trans-\nformer architecture for entity retrieval, generating entity names in\nan autoregressive fashion based on the conditioned context. DSI\n[42] first proposes the concept of assigning structured semantic\nIDs to documents and training encoder-decoder models for gen-\nerative document retrieval. Following this paradigm, TIGER [36]\nintroduces the formulation of generative item retrieval models for\nrecommender systems.",
            "content": "In addition to the generation framework, how to index items has also attracted increasing attention. Recent research focuses on the semantic indexing technique [12, 36, 42], which aims to index items based on content information. Specifically, TIGER [36] and LC-Rec [51] apply residual quantization (RQ-VAE) to textual embeddings derived from item titles and descriptions for tokenization. Recforest [12] utilizes hierarchical k-means clustering on item textual embeddings to obtain cluster indexes as tokens. Furthermore, recent studies such as EAGER [45] explore integrating both semantic and collaborative information into the tokenization process."
        },
        {
            "title": "2.2 Preference Alignment of Language Models\nDuring the post-training [10] phase of Large Language Models\n(LLMs), Reinforcement Learning from Human Feedback (RLHF)\n[32, 39] is a prevalent method in aligning LLMs with human values\nby employing reinforcement learning techniques guided by reward\nmodels that represent human feedback. However, RLHF suffers from",
            "content": "Figure 1: (a) Our proposed unified architecture for end-toend generation. (b) typical cascade ranking system, which includes three stages from the bottom to the top: Retrieval, Pre-ranking, and Ranking. Although efficient in practice, existing methods typically treat each ranker independently, where the effectiveness of each isolated stage serves as the upper bound for the subsequent ranking stage, thereby limiting the performance of the overall ranking system. Despite various efforts [11, 13, 18, 20, 34, 44] to improve overall recommendation performance by enabling interaction among rankers, they still maintain the traditional cascade ranking paradigm. Recently, generative retrieval-based recommendation systems (GRs) [36, 45, 51] have emerged as promising paradigm by directly generating the identifier of candidate item in an autoregressive sequence generation manner. By indexing items with quantized semantic IDs that encode item semantics [24], recommenders can leverage the abundant semantic information within the items. The generative nature of GRs makes them suitable for directly selecting candidate items through beam search decoding and producing more diverse recommendation results. However, current generative models only act as selectors in the retrieval stage, as their recommendation accuracy does not yet match that of well-designed multiple cascade rankers. To address the above challenges, we propose unified end-toend generative framework for single-stage recommendation named OneRec. First, we present an encoder-decoder architecture. Taking inspiration from the scaling laws observed in training large language models, we find that scaling the capacity of recommendation models also consistently improves the performance. So we scale up the model parameters based on the structure of MoE [7, 9, 55], which significantly improves the models ability to characterize user interests. Second, unlike the traditional point-by-point prediction of the next item, we propose session-wise list generation approach that considers the relative content and order of the items within each session. The point-by-point generation method necessitates hand-craft strategies to ensure coherence and diversity in the generated results. In contrast, the session-wise learning process enables the model to autonomously learn the optimal session structure by feeding it preferred data. Last but not least, we explore preference learning by using direct preference optimization (DPO) [35] to further enhance the quality of the generated results. For constructing preference pairs, we take inspiration from hard negative sampling [37] by creating self-hard rejected samples from the beam search results rather than random sampling. We propose an Iterative Preference Alignment (IPA) strategy to rank the sampled responses based on scores provided by the pre-trained reward model (RM), identifying the best-chosen and worst-rejected samples. Our experiments on large-scale industry datasets show the superiority of the OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 2: The overall framework of OneRec, consists of two stages: (i) the session training stage which train OneRec with session-wise data; (ii) the IPA stage which utilizes iterative direct preference optimization with self-hard negatives. instability and inefficiency. Direct Preference Optimization (DPO) [35] is proposed which derives the optimal policy in closed form and enables direct optimization using preference data. Apart from that, several variants have been proposed to further improve the original DPO. For example, IPO [1] bypasses two approximations in DPO with general objective. cDPO [35] alleviates the influence of noisy labels by introducing hyperparameter ùúñ. rDPO [5] designs an unbiased estimate of the original Binary Cross Entropy loss. Other variants including CPO [47], simDPO [5], also enhance or expand DPO in various aspects. However, unlike traditional NLP scenarios where preference data is explicitly annotated through humans, preference learning in recommendation systems faces unique challenge because of the sparsity of user-item interaction data. This challenge results in adapting DPO for recommendation are still largely unexplored. Different from S-DPO which focuses on incorporating multiple negatives in user preference data for LM-based recommenders, we train reward model and based on the scores from reward model we choose personalized preference data for different users."
        },
        {
            "title": "3 Methods\nIn this section, we propose OneRec, an end-to-end framework that\ngenerates target items through a single-stage retrieval manner.\nIn Section 3.1, we first introduce the feature engineering for the\nsingle-stage generative recommendation pipeline in industrial ap-\nplications. Then, in Section 3.2, we formally define the session-\nwise generative tasks and present the architecture of our proposed",
            "content": "OneRec model. Finally, we elaborate on the models capability with personalized reward model for self-hard negative sampling in Section 3.3, and demonstrate how we iteratively improve model performance through direct preference optimization. The overall framework of OneRec is illustrated in Figure 2."
        },
        {
            "title": "3.1 Preliminary\nIn this section, we introduce the construction of the single-stage\ngenerative recommendation pipeline from the perspectives of fea-\nture engineering. For user-side feature, OneRec takes the posi-\ntive historical behavior sequences Hùë¢ = {ùíó‚Ñé\nùëõ } as input,\n1\nwhere ùíó represent the videos that the user has effectively watched\nor interacted with (likes, follows, shares), and ùëõ is the length of\nbehaviour sequence. The output of OneRec is a list of videos, con-\nsisting of a session S = {ùíó1, ùíó2, ..., ùíóùëö }, where ùëö is the number of\nvideos within a session (the detailed definition of ‚Äúsession‚Äù can be\nfound in Section 3.2).",
            "content": ", . . . , ùíó‚Ñé , ùíó‚Ñé 2 For each video ùíóùëñ , we describe them with multi-modal embeddings eùëñ Rùëë which are aligned with the real user-item behaviour distribution [27]. Based on the pretrain multi-modal representation, existing generative recommendation frameworks [25, 36] use RQVAE [49] to encode the embedding into semantic tokens. However, such method is suboptimal due to the unbalanced code distribution which is known as the hourglass phenomenon [23]. We apply multi-level balanced quantitative mechanism to transform the eùëñ with residual K-Means quantization algorithm[27]. At the first level (ùëô = 1), the initial residual is defined as ùíì 1 ùëñ = ùíÜùëñ . At each level Conference acronym XX, June 0305, 2018, Woodstock, NY Deng et al. , . . . , ùíÑùëô ùêæ } with random selection; Algorithm 1: Balanced K-means Clustering Input: Item set V, number of clusters ùêæ 1 Compute ùë§ /ùêæ 2 Initialize centroids Cùëô = {ùíÑùëô 1 3 repeat 4 Initialize unassigned set for each cluster ùëò {1, . . . , ùêæ } do 5 7 8 9 Sort by ascending distance from centroid ùíÑùëô ùëò ; Assign Vùëò [0 : ùë§ 1]; ùíìùëô Vùëò ùíìùëô ; Update centroid ùíÑùëô (cid:205) Remove assigned items Vùëò ; ùëò 1 ùë§ end 10 11 until Assignment convergence; Output: Optimized codebook Cùëô = {ùíÑùëô 1 , . . . , ùíÑùëô ùêæ } ùëô, we have codebook Cùëô = {ùíÑùëô , ..., ùíÑùëô ùêæ }, where ùêæ is the codebook 1 size. The index of the closest centroid node embedding is generate ùëñ = arg minùëò rùëô through ùíîùëô 2 and for next level ùëô +1 the residual ùëñ ùíÑùëô is defined as ùíìùëô+1 . So the corresponding codebook tokens ùíîùëô ùëñ are generated through hierarchical indexing: ùëñ ùíÑùëô = ùíìùëô ùëò 2 ùëñ ùíì 1 ùëñ ùíÑ1 ùíì ùëñ ùíÑ2 ùëò 2 2 ùëò 2 2 , , ùëñ = ùíì 1 ùíì 2 ùíì 3 ùëñ = ùíì ùëñ ùíÑ1 ùíî1 ùëñ ùëñ ùíÑ2 ùíî2 ùëñ ùíî1 ùëñ = arg min ùëò ùíî2 ùëñ = arg min ... ùëò ùíî ùêø ùëñ = arg min ùëò ùíì ùêø ùëñ ùíÑ ùêø ùëò 2 2 where ùêø is the total layers of sematic ID. , . . . , ùíÑùëô To construct balanced codebook Cùëô = {ùíÑùëô 1 ùêæ }, we apply the Balanced K-means as detailed in Algorithm 1 for itemset partitioning. Given the total video set V, this algorithm partitions the set into ùêæ clusters, where each cluster contains exactly ùë§ = /ùêæ videos. During iterative computation, each centroid is sequentially assigned its ùë§ nearest unallocated videos based on Euclidean distance, followed by centroid recalibration using mean vectors of assigned videos. The termination criterion is satisfied when cluster assignments reach convergence."
        },
        {
            "title": "3.2 Session-wise List Generation\nDifferent from traditional point-wise recommendation methods\nthat only predict the next video, session-wise generation aims to\ngenerate a list of high-value sessions based on users‚Äô historical\ninteraction sequences, which enables the recommendation model\nto capture the dependencies between videos in the recommended\nlist. Specifically, a session refers to a batch of short videos returned\nin response to a user‚Äôs request, typically consisting of 5 to 10 videos.\nThe videos within a session generally take into account factors such\nas user interest, coherence, and diversity. We have devised several\ncriteria to identify high-quality sessions, including:",
            "content": "The total duration for which the user watches the session exceeds certain threshold; The user exhibits interaction behaviors, such as liking, collecting, or sharing the videos; This approach ensures that our session-wise model learns from real user engagement patterns and captures more accurate contextual information within the session list. So the objective of our session-wise model can be formalized as: := (Hùë¢ ) (1) , , ùíîùêø 2 ), , (ùíî1 ùëõ, , ùíîùêø ùëö)}. ùëõ )} and = {(ùíî1 1 2 ), , (ùíî1 ùëö, ùíî , , , ùíî2 where Hùë¢ is represented from the remantic IDs: Hùë¢ = {(ùíî1 1 1 ùíîùêø , , ùíîùêø ùëõ, ùíî2 1 ), (ùíî1 , ùíî2 1 ), , ùíî2 2 1 2 ùëö, , ùíîùêø , , ùíîùêø , ùíî2 (ùíî1 2 2 As shown in Figure 2 (a), consistent with the T5 [48] architecture, our model employs transformer-based framework consisting of two main components: an encoder for modeling user historical interactions and decoder for session list generation. Specifically, the encoder leverages the stacked multi-head self-attention and feed-forward layers to process the input sequence Hùë¢ . We denote the encoded historical interaction features as = ùê∏ùëõùëêùëúùëëùëíùëü (Hùë¢ ). The decoder takes the semantic IDs of the target session as input and generates the target in an auto-regressive manner. To train larger model at reasonable economic costs, for the feed-forward neural networks (FNNs) in the decoder, we adopt the MoE architecture [7, 9, 55] commonly used in Transformer-based language models and substitute the ùëô-th FFN with: (cid:16) ùëîùëñ,ùë° FFNùëñ (cid:16) ùëô ùë° (cid:17)(cid:17) ùëô ùë° , + ùëô+1 ùë° = ùëîùëñ,ùë° = ùëÅMoE ùëñ=1 (cid:40)ùë†ùëñ,ùë° , 0, ùë†ùëñ,ùë° Topk({ùë† ùëó,ùë° 1 ùëó ùëÅ }, ùêæMoE), otherwise, (cid:17) ùëá (cid:16) (2) ùë†ùëñ,ùë° = Softmaxùëñ ùëô ùë° ùëô ùëñ , where ùëÅMoE represents the total number of experts, FFNùëñ () is the ùëñth expert FFN, and ùëîùëñ,ùë° denotes the gate value for the ùëñ-th expert. The gate value ùëîùëñ,ùë° is sparse, meaning that only ùêæMoE out of ùëÅMoE gate values are non-zero. This sparsity property ensures computational efficiency within an MoE layer and each token will be assigned to and computed in only ùêæMoE experts. For training, we add start token ùíî [BOS] at the beginning of codes to construct the decoder inputs with: = {ùíî [BOS], ùíî1 1 , ùíî2 , , ùíî 1 , ùíî [BOS], ùíî1 ùêø 1 ùëö, ùíî2 , ùíî [BOS], ùíî1 2 ùêø ùëö, , ùíî ùëö } , ùíî2 2 , , ùíî , ùêø 2 (3) We utilize cross-entropy loss for next-token prediction on the sematic IDs of the target session. The NTP loss LNTP is formulated as: LNTP = ùëö ùêø ùëñ=1 ùëó=1 log ùëÉ (ùíî ùëó+1 ùëñ [ùíî [BOS], ùíî1 1 , ùíî2 1 , , ùíî ùêø 1 , , (4) ùíî [BOS], ùíî1 ùëñ , , ùíî ùëó ùëñ ]; Œò). The number of short videos actually watched by the user within After certain amount of training on session-wise list generation session is greater than or equal to 5; task, we obtain the seed model Mùë° . OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Conference acronym XX, June 0305, 2018, Woodstock, NY Algorithm 2: Iterative Preference Alignment (IPA) Input: Number of responses ùëÅ , pretrained RM ùëÖ(ùíñ, S), seed model Mùë° , DPO ratio ùëüDPO, total epochs ùëá and samples per epoch ùëÅsample 1 for epoch ùë° to ùëá do 2 for sample 1 to ùëÅsample do if rand () < ùëüDPO then 3 4 5 7 8 9 10 11 13 14 15 16 17 19 20 Generate ùëÅ responses via Mùë° : for ùëñ 1 to ùëÅ do Sùëñ ùë¢ Mùë° (Hùë¢ ); ùë¢ ùëÖ(ùíñ, Sùëñ ùëüùëñ ùë¢ ); end Select the best and worst responses: ùë¢ Sarg maxùëñ ùëü ùëñ Sùë§ ùë¢ Sarg minùëñ ùëü ùëñ Sùëô ; Compute NTP and DPO loss: LNTP + ùúÜLDPO; ùë¢ ùë¢ ùë¢ ùë¢ ; else Compute NTP loss: LNTP; end Update parameters: Œò Œò ùõºŒòL; end Update model snapshot: Mùë° +1 Mùë° ; 21 22 end Output: Optimized parameters Œò"
        },
        {
            "title": "3.3 Iterative Preference Alignment with RM\nThe high-quality sessions defined in Section 3.2 provide valuable\ntraining data, enabling the model to learn what constitutes a good\nsession, thereby ensuring the quality of generated videos. Building\non this foundation, we aim to further enhance the model‚Äôs ability\nby Direct Preference Optimization (DPO). In traditional natural\nlanguage processing (NLP) scenarios, preference data is explicitly\nannotated by humans. However, preference learning in recommen-\ndation systems confronts a unique challenge due to the sparsity\nof user-item interaction data, which necessitates a reward model\n(RM). So we introduce a session-wise reward model in Section 3.3.1.\nMoreover, we improve the conventional DPO by proposing an it-\nerative direct preference optimization that enables the model to\nself-improvement described in Section 3.3.2.",
            "content": "3.3.1 Reward Model Training. We use ùëÖ(ùíñ, S) to denote the reward model which selects preference data for different users. Here, the output ùëü represents the reward corresponding to user ùë¢s (usually represented by user behavior) preference on the session = {ùíó1, ùíó2, . . . , ùíóùëö }. In order to equip the RM with the capacity to rank session, we first extract the target-aware representation ùíÜùëñ = ùíóùëñ ùíñ of each item ùíóùëñ in S, where represents the target-aware operation (such as target attention toward user behavior). So we get the target-aware representation ùíâ = {ùíÜ1, ùíÜ2, , ùíÜùëö } for session S. Then the items within session interact with each other through self-attention layers to fuse the necessary information among different items: ùíâùëì = SelfAttention(ùíâùëæ ùëÑ ùë† , ùíâ ùëæ ùêæ ùë† , ùíâùëæ ùëâ ùë† ) (5) Next we utilize different tower to make predictions on multitarget reward and the RM is pre-trained with abundant recommendation data: where ÀÜùëü ùë†ùë§ùë° = Towerùë†ùë§ùë° (cid:0)Sum(cid:0)ùíâùëì (cid:1)(cid:1), ÀÜùëü ùë£ùë°ùëü = Towerùë£ùë°ùëü (cid:0)Sum(cid:0)ùíâùëì (cid:1)(cid:1), ÀÜùëü ùë§ùë°ùëü = Towerùë§ùë°ùëü (cid:0)Sum(cid:0)ùíâùëì (cid:1)(cid:1), ÀÜùëü ùëôùë°ùëü = Towerùëôùë°ùëü (cid:0)Sum(cid:0)ùíâùëì (cid:1)(cid:1), Tower() = Sigmoid(cid:0)MLP() (cid:1) After getting all the estimated rewards ÀÜùëüùë†ùë§ùë° , . . . and the groundtruth labels ùë¶ùë†ùë§ùë° , . . . for each session, we directly minimize the binary cross-entropy loss to train the RM. The loss function LRM is defined as follows: (6) LRM = ùë•ùë°ùëü ùë†ùë§ùë°,... (cid:0)ùë¶ùë•ùë°ùëü log ( ÀÜùëü ùë•ùë°ùëü ) + (1 ùë¶ùë•ùë°ùëü ) log (1 ÀÜùëü ùë•ùë°ùëü )(cid:1) (7) Iterative Preference Alignment. Based on pre-trained RM 3.3.2 ùëÖ(ùíñ, S) and current OneRec Mùë° , we generate ùëÅ different responses for each user by beam search: Sùëõ ùë¢ ùëÄùë° (Hùë¢ ) for all ùë¢ and ùëõ [ùëÅ ], (8) where we use [ùëÅ ] to denote {1, 2, . . . , ùëÅ }. Then we computes the reward ùëüùëõ based on the RM ùëÖ(ùíñ, S): ùë¢ for each of these responses ùë¢ = ùëÖ(ùíñ, Sùëõ ùëüùëõ ùë¢ ) (9) ùë° ùë¢ , Sùëô Next we build the preference pairs ùê∑pairs = (Sùë§ ùë¢, Hùë¢ ) by choosing the winner response (Sùë§ ùë¢ , Hùë¢ ) with the highest reward value and loser response (Sùëô ùë¢, Hùë¢ ) with the lowest reward value. Given the preference pairs, we can now train new model ùëÄùë° +1 which is initialized from model ùëÄùë° , and updated with loss function that combines the DPO loss [35] for learning from the preference pairs. The loss corresponding to each preference pair is as follows: LDPO = LDPO (Sùë§ (cid:32) ùë¢ , Sùëô (cid:33) = log ùúé ùõΩ log ùë¢ Hùë¢ ) ùëÄùë° +1 (Sùë§ ùëÄùë° (Sùë§ ùë¢ Hùë¢ ) ùë¢ Hùë¢ ) ùõΩ log ùëÄùë° +1 (Sùëô ùëÄùë° (Sùëô ùë¢ Hùë¢ ) ùë¢ Hùë¢ ) . (10) As shown in Algorithm 2 and Figure 2 (b), the overall procedure involves training sequence of models ùëÄùë° , . . . , ùëÄùëá . To mitigate the computational burden during beam search inference, we randomly sample only ùëüDPO = 1% data for preference alignment. For each successive model ùëÄùë° +1, it initializes from previous model ùëÄùë° and utilizes the preference data ùê∑pairs generated by the ùëÄùë° for training. ùë° Figure 3: Framework of Online Deployment of OneRec. Conference acronym XX, June 0305, 2018, Woodstock, NY Deng et al. Table 1: Offline performance of our proposed OneRec (green) with pointwise methods (brown) , listwise methods (blue) and preference alignment methods (yellow) . Best results are in bold, sub-optimal results are underlined. Metrics with indicate higher is better, while indicates lower is better. Model SASRec BERT4Rec FDSA Watching-Time Metrics swt vtr Interaction Metrics wtr ltr mean max mean max mean max mean max Pointwise Discriminative Method 0.03750.002 0.08030.005 0.43130.013 0.58010.013 0.03360.002 0.07060.004 0.41920.014 0.56330.013 0.03250.002 0.06830.005 0.41450.015 0.55880. 0.002940.001 0.002810.001 0.002710.001 0.009780.001 0.009320.001 0.009210.001 0.03140.002 0.06040.004 0.03160.002 0.06060.004 0.03130.002 0.06040.003 TIGER-0.1B TIGER-1B 0.08790.007 0.12860.010 0.58260.016 0.66250.017 0.08730.006 0.13680.010 0.58270.015 0.67760.015 0.002770.001 0.002920. 0.006710.001 0.007580.001 0.03160.004 0.05410.007 0.03230.004 0.05790.008 Pointwise Generative Method OneRec-0.1B OneRec-1B 0.09730.010 0.15010.015 0.60010.021 0.69810.021 0.09910.008 0.15290.012 0.60390.020 0.70130.020 0.003260.001 0.003490. 0.008700.001 0.009190.002 0.03490.009 0.06420.015 0.03600.005 0.06600.008 Listwise Generative Method with Preference Alignment Listwise Generative Method 0.10140.010 0.15950.015 0.61270.017 0.71160.016 OneRec-1B+DPO OneRec-1B+IPO 0.09790.003 0.15280.005 0.60000.007 0.70120.007 OneRec-1B+cDPO 0.09930.006 0.15470.008 0.60300.011 0.70300.009 0.10050.006 0.15550.008 0.60710.014 0.70590.011 OneRec-1B+rDPO OneRec-1B+CPO 0.09930.008 0.15380.012 0.60450.021 0.70290.018 OneRec-1B+simPO 0.09950.008 0.15360.013 0.60470.016 0.70220.015 OneRec-1B+S-DPO 0.10210.008 0.15750.013 0.60960.016 0.70700.015 OneRec-1B+IPA 0.03510.004 0.06440.008 0.03500.003 0.06540.004 0.03550.006 0.06520.009 0.03570.004 0.06570.006 0.03570.008 0.06590.014 0.03600.005 0.06590.008 0.03610.004 0.06590.008 0.10250.009 0.19330.017 0.61410.020 0.76460.021 0.003540.001 0.009920.001 0.03970.004 0.12030. 0.003390.001 0.003350.001 0.003390.001 0.003390.001 0.003430.001 0.003490.001 0.003450.001 0.008960.001 0.009050.001 0.009010.001 0.008990.001 0.009110.002 0.009180.001 0.009090."
        },
        {
            "title": "5.1 Experimental Settings\nImplementation Details. Our model is trained using the Adam\n5.1.1\noptimizer with an initial learning rate of 2√ó10‚àí4. We utilize NVIDIA\nA800 GPUs for OneRec optimization. The DPO sample ratio ùëüDPO\nis set to 1% throughout training and we generate ùëÅ = 128 different\nresponses for each user by beam search; The semantic identifier\nclustering process employs ùêæ = 8192 clusters for each codebook\nlayer and the number of codebook layers is set to ùêø = 3; The Mixture-\nof-Experts architecture contains ùëÅMoE = 24 expert with ùêæMoE = 2\nexperts activated per forward pass through top-ùëò selection; For\nsession modeling, we consider ùëö = 5 target session items and adopt\nùëõ = 256 historical behavior as context.",
            "content": "5.1.2 Baseline Methods. We adopt the following representative recommendation models, DPO and its variants to serve as additional baselines for comparison. The baseline methods include: SASRec [22] employs unidirectional Transformer architecture to capture sequential dependencies in user-item interactions for next-item prediction. BERT4Rec [40] leverages bidirectional Transformers with masked language modeling to learn contextual item representations through sequence reconstruction. FDSA [50] implements dual self-attention pathways to jointly model item-level transitions and feature-level transformation patterns in heterogeneous recommendation scenarios. TIGER [36] leverages hierarchical semantic identifiers and generative retrieval techniques for sequential recommendation through auto-regressive sequence generation. OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Conference acronym XX, June 0305, 2018, Woodstock, NY DPO [35] formalizes preference optimization with closed-form reward function derived from human feedback data via implicit reward modeling. IPO [1] proposes theoretically grounded preference optimization framework which bypass the approximations inherent in standard DPO. cDPO [30] introduces robustness-aware variant incorporating label flipping rate parameter ùúñ to account for noisy preference annotations. rDPO [5] develops an unbiased loss estimator using importance sampling to reduce variance in preference optimization. CPO [47] unifies contrastive learning with preference optimization through joint training of sequence likelihood rewards and supervised fine-tuning objectives. simPO [29] conducts preference optimization by employing sequence-level reward margins while eliminating reference model dependencies through normalized probability averaging. S-DPO [4] adapts DPO for recommendation systems through hard negative sampling and multi-item contrastive learning to enhance ranking accuracy. 5.1.3 Evaluation Metric. We evaluate the models performance with several key metrics. Each metric serves distinct purpose in assessing different aspects of the models output and we conduct the evaluation on randomly sampled set of test cases in each iteration. To estimate the probabilities of various interactions for each specific user-session pair, we employ the pre-trained reward model to assess the value of recommended sessions. We calculate the mean reward for different target metrics, including session watch time (swt), view probability (vtr), follow probability (wtr) and like probability (ltr). Among these targets, swt and vtr are watching-time metrics, while wtr and ltr are interaction metrics. Figure 4: The ablation study on DPO sample ratio ùëüDPO. The results indicate that 1% ratio of DPO training leads to significant gains but further increase the sample ratio results in limited improvements."
        },
        {
            "title": "5.2 Offline Performance\nTable 1 presents the comprehensive comparison between OneRec\nand various baselines. For watching-time metric we mainly care\nabout the session watch time (swt) and like probability (ltr) in\ninteraction metrics. Our result reveals three key observations:",
            "content": "First, the proposed session-wise generation approach significantly outperforms traditional dot-product-based methods and point-wise generation methods like TIGER. OneRec1B achieves 1.78% higher maximum swt and 3.36% higher maximum ltr compared to TIGER-1B. This demonstrates the advantage of session-wise modeling in maintaining contextual coherence across recommendations, whereas point-wise methods struggle to balance coherence and diversity in generated outputs. Second, small ratio of DPO training yields substantial gains. With only 1% DPO training ratio (ùëüDPO), OneRec-1B+IPA surpasses the base OneRec-1B by 4.04% in maximum swt and 5.43% in maximum ltr. This suggests limited DPO training can effectively aligns the model with desired generation patterns. Third, the proposed IPA strategy outperforms various existing DPO variants. As shown in Table 1, IPA achieves superior performance compared to alternative DPO implementations. Notably, some DPO baselines underperform even the non-aligned OneRec-1B model, suggesting that iterative mining of self-generated outputs for preference selection proves more effective than other methods."
        },
        {
            "title": "5.3 Ablation Study\n5.3.1 DPO Sample Ratio Ablation. In order to investigate the im-\npact of sample ratio ùëüDPO in DPO training, we varied the DPO sam-\nple ratio from 1% to 5% under controlled conditions. As illustrated\nin Figure 4, ablation results demonstrate that increasing the sample\nratio yields marginal performance improvements across multiple\nevaluation targets. Notably, the performance gains beyond the 1%\nbaseline remain insignificant despite increased computational ex-\npenditure. It worth noting that there exists a linear relationship\nbetween and GPU resource utilization during DPO sample server\ninference: the 5% sample ratio requires 5√ó more GPU resources than\nthe 1% baseline. This scaling characteristic establishes an explicit\ntrade-off between computational efficiency and model performance.\nTherefore, after balancing the best trade-off with computation effi-\nciency and performance, we apply 1% DPO sample ratio for training,\nwhich achieves average 95% of the maximum observed performance\nwhile requiring only 20% of the computational resources needed\nfor higher sample ratio.",
            "content": "5.3.2 Model Scaling Ablation. We evaluate how OneRec performs when the model scale increases. As Figure 6 shows, scaling OneRec from 0.05B to 1B achieves consistent accuracy gains, demonstrating consistent scaling properties. Specifically, compared to OneRec0.05B, OneRec-0.1B achieves significant maximum 14.45% gain in accuracy, and 5.09%, 5.70% and 5.69% additional accuracy gains can be achieved when scaling to 0.2B, 0,5B and 1B."
        },
        {
            "title": "5.4 Prediction Dynamics of OneRec\nAs shown in Figure 5, we present the predicted probability dis-\ntributions of 8192 codes across different layer, where the red star\ndenotes the semantic ID of the item with the highest reward value.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Deng et al. Figure 5: The visualization of the probability distribution of the softmax output for each layer of the semantic ID. The red star represents the sematic ID of item which has the highest reward value. main page and we compare the performance of OneRec and current multi-stage recommender system with 1% main traffic for experiments. We use Total Watch Time to measure the total time that users spend watching videos and Average View Duration calculates the average watch time per video when the user is exposed to requested session by the recommendation system. Online evaluation shows that OneRec has achieved 1.68% improvement in total watch time and 6.56% improvement in average view duration, which indicates that OneRec achieves much better recommendation results and brings considerable revenue increments for the platform. Table 2: The absolute improvement of OneRec compared to the current multi-stage system in the online A/B testing setting. Model Total Watch Time Average View Duration OneRec-0.1B OneRec-1B OneRec-1B+IPA +0.57% +1.21% +1.68% +4.26% +5.01% +6.56%"
        },
        {
            "title": "6 Conclusion\nIn this paper, we focus on the introduction of an industrial solution\nfor single-stage generative recommendation. Our solution estab-\nlishes three key contributions: First, we effectively scale the model\nparameters with high computational efficiency by applying the\nMoE architecture, offering a scalable blueprint for large-scale indus-\ntrial recommendation. Next, we find the necessity of modeling the\ncontextual information of target items in a session-wise generation\nmanner, proving contextual sequence modeling inherently captures\nuser preference dynamics better than isolated point-wise manner.\nFurthermore, we propose an Iterative Preference Alignment (IPA)\nstrategy to improve OneRec‚Äôs generalization across diverse user\npreference patterns. Extensive offline experiments and online A/B\ntesting verify the effectiveness and efficiency of OneRec. Addition-\nally, our analysis of online results reveals that, besides user watch\ntime, our model has limitations in interactive indicators, such as\nlikes. In future research, we aim to enhance the end-to-end genera-\ntive recommendation‚Äôs capability in multi-objective modeling to\nprovide a better user experience.",
            "content": "Figure 6: Scalability of OneRec on model scaling. The results show that OneRec constantly benefits from performance improvement when the parameters are scaled up. Compared to the OneRec baseline, OneRec+IPA exhibits significant confidence shift in prediction distributions, indicating that our proposed preference alignment strategy effectively encourages the base model to produce preferred generation patterns. Furthermore, we observe that the probability distribution in the first layer demonstrates greater divergence (entropy = 6.00) compared to subsequent layers (average entropy = 3.71 in the second layer and entropy = 0.048 in third layer), which exhibit progressively concentrated distributions. This hierarchical uncertainty reduction can be attributed to the autoregressive decoding mechanism: the initial layers predictions inherit higher uncertainty from preceding decoding steps, while later layers benefit from accumulated context that constrains the decision space."
        },
        {
            "title": "5.5 Online A/B Test\nTo evaluate the online performance of OneRec, we conduct strict\nonline A/B tests on Kuaishou‚Äôs video recommendation scenarios of",
            "content": "OneRec: Unifying Retrieve and Rank with Generative Recommender and Preference Alignment Conference acronym XX, June 0305, 2018, Woodstock, NY References [1] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. 2024. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics. PMLR, 44474455. [2] Christopher JC Burges. 2010. From ranknet to lambdarank to lambdamart: An overview. Learning 11, 23-581 (2010), 81. [3] Jianxin Chang, Chenbin Zhang, Zhiyi Fu, Xiaoxue Zang, Lin Guan, Jing Lu, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song, et al. 2023. TWIN: TWo-stage interest network for lifelong user behavior modeling in CTR prediction at kuaishou. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 37853794. [4] Yuxin Chen, Junfei Tan, An Zhang, Zhengyi Yang, Leheng Sheng, Enzhi Zhang, Xiang Wang, and Tat-Seng Chua. 2024. On Softmax Direct Preference Optimization for Recommendation. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=qp5VbGTaM0 [5] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. 2024. Provably Robust DPO: Aligning Language Models with Noisy Feedback. In ICML 2024. [6] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In Proceedings of the 10th ACM conference on recommender systems. 191198. [7] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. 2024. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066 (2024). [8] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2020. Autoregressive entity retrieval. arXiv preprint arXiv:2010.00904 (2020). [9] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. 2022. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning. PMLR, 55475569. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [11] Hongliang Fei, Jingyuan Zhang, Xingxuan Zhou, Junhao Zhao, Xinyang Qi, and Ping Li. 2021. GemNN: gating-enhanced multi-task neural networks with feature interaction learning for CTR prediction. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 2166 2171. [12] Chao Feng, Wuchao Li, Defu Lian, Zheng Liu, and Enhong Chen. 2022. Recommender forest for efficient retrieval. Advances in Neural Information Processing Systems 35 (2022), 3891238924. [13] Luke Gallagher, Ruey-Cheng Chen, Roi Blanco, and Shane Culpepper. 2019. Joint optimization of cascade ranking models. In Proceedings of the twelfth ACM international conference on web search and data mining. 1523. [14] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization. IEEE transactions on pattern analysis and machine intelligence 36, 4 (2013), 744755. [15] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017. DeepFM: factorization-machine based neural network for CTR prediction. arXiv preprint arXiv:1703.04247 (2017). [16] Hidasi. 2015. Session-based Recommendations with Recurrent Neural Networks. arXiv preprint arXiv:1511.06939 (2015). [17] Michael Houle and Michael Nett. 2014. Rank-based similarity search: Reducing the dimensional dependence. IEEE transactions on pattern analysis and machine intelligence 37, 1 (2014), 136150. [18] Jiri Hron, Karl Krauth, Michael Jordan, and Niki Kilbertus. 2021. On component interactions in two-stage recommender systems. Advances in neural information processing systems 34 (2021), 27442757. [19] Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management. 23332338. [20] Xu Huang, Defu Lian, Jin Chen, Liu Zheng, Xing Xie, and Enhong Chen. 2023. Cooperative Retriever and Ranker in Deep Recommenders. In Proceedings of the ACM Web Conference 2023. 11501161. [21] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117128. [22] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM). IEEE, 197206. [23] Zhirui Kuai, Zuxu Chen, Huimu Wang, Mingming Li, Dadong Miao, Wang Binbin, Xusong Chen, Li Kuang, Yuxing Han, Jiaxing Wang, et al. 2024. Breaking the Hourglass Phenomenon of Residual Quantization: Enhancing the Upper Bound of Generative Retrieval. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track. 677685. [24] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. 2022. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1152311532. [25] Han Liu, Yinwei Wei, Xuemeng Song, Weili Guan, Yuan-Fang Li, and Liqiang Nie. 2024. MMGRec: Multimodal Generative Recommendation with Transformer Model. arXiv preprint arXiv:2404.16555 (2024). [26] Shichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. 2017. Cascade ranking for operational e-commerce search. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. 15571565. [27] Xinchen Luo, Jiangxia Cao, Tianyu Sun, Jinkai Yu, Rui Huang, Wei Yuan, Hezheng Lin, Yichen Zheng, Shiyao Wang, Qigen Hu, et al. 2024. QARM: Quantitative Alignment Multi-Modal Recommendation at Kuaishou. arXiv preprint arXiv:2411.11739 (2024). [28] Xu Ma, Pengjie Wang, Hui Zhao, Shaoguo Liu, Chuhan Zhao, Wei Lin, KuangChih Lee, Jian Xu, and Bo Zheng. 2021. Towards better tradeoff between effectiveness and efficiency in pre-ranking: learnable feature selection based approach. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 20362040. [29] Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. SimPO: Simple Preference Optimization with Reference-Free Reward. In Advances in Neural Information Processing Systems (NeurIPS). [30] Eric Mitchell. [n. d.]. note on dpo with noisy preferences and relationship to ipo, 2023. URL https://ericmitchell. ai/cdpo. pdf ([n. d.]). [31] Marius Muja and David Lowe. 2014. Scalable nearest neighbor algorithms for high dimensional data. IEEE transactions on pattern analysis and machine intelligence 36, 11 (2014), 22272240. [32] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 2773027744. [33] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 26852692. [34] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022. Rankflow: Joint optimization of multistage cascade ranking systems as flows. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 814824. [35] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems 36 (2024). [36] Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan Hulikal Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Tran, Jonah Samost, et al. 2023. Recommender systems with generative retrieval. Advances in Neural Information Processing Systems 36 (2023), 1029910315. [37] Wentao Shi, Jiawei Chen, Fuli Feng, Jizhi Zhang, Junkang Wu, Chongming Gao, and Xiangnan He. 2023. On the theories behind hard negative sampling for recommendation. In Proceedings of the ACM Web Conference 2023. 812822. [38] Anshumali Shrivastava and Ping Li. 2014. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). Advances in neural information processing systems 27 (2014). [39] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2020. Learning to summarize with human feedback. Advances in Neural Information Processing Systems 33 (2020), 30083021. [40] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In Proceedings of the 28th ACM international conference on information and knowledge management. 14411450. [41] Yubao Tang, Ruqing Zhang, Jiafeng Guo, and Maarten de Rijke. 2023. Recent advances in generative information retrieval. In Proceedings of the Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region. 294297. [42] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al. 2022. Transformer memory as differentiable search index. Advances in Neural Information Processing Systems 35 (2022), 2183121843. [43] Lidan Wang, Jimmy Lin, and Donald Metzler. 2011. cascade ranking model for efficient ranked retrieval. In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval. 105114. [44] Yunli Wang, Zhiqiang Wang, Jian Yang, Shiyang Wen, Dongying Kong, Han Li, and Kun Gai. 2024. Adaptive Neural Ranking Framework: Toward Maximized Business Goal for Cascade Ranking Systems. In Proceedings of the ACM on Web Conference 2024. 37983809. Conference acronym XX, June 0305, 2018, Woodstock, NY Deng et al. [45] Ye Wang, Jiahao Xun, Minjie Hong, Jieming Zhu, Tao Jin, Wang Lin, Haoyuan Li, Linjun Li, Yan Xia, Zhou Zhao, et al. 2024. EAGER: Two-Stream Generative Recommender with Behavior-Semantic Collaboration. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 32453254. [46] Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. 2020. Cold: Towards the next generation of pre-ranking system. arXiv preprint arXiv:2007.16122 (2020). [47] Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. 2024. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. arXiv preprint arXiv:2401.08417 (2024). [48] Shuyuan Xu, Wenyue Hua, and Yongfeng Zhang. 2024. Openp5: An open-source platform for developing, training, and evaluating llm-based recommender systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 386394. [49] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2021. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing 30 (2021), 495507. [50] Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor Sheng, Jiajie Xu, Deqing Wang, Guanfeng Liu, Xiaofang Zhou, et al. 2019. Feature-level deeper selfattention network for sequential recommendation.. In IJCAI. 43204326. [51] Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, and Ji-Rong Wen. 2024. Adapting large language models by integrating collaborative semantics for recommendation. In 2024 IEEE 40th International Conference on Data Engineering (ICDE). IEEE, 14351448. [52] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33. 59415948. [53] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 10591068. [54] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai. 2018. Learning tree-based deep model for recommender systems. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining. 10791088. [55] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. 2022. Designing effective sparse expert models. arXiv preprint arXiv:2202.08906 2, 3 (2022), 17."
        }
    ],
    "affiliations": [
        "KuaiShou Inc. Beijing, China"
    ]
}