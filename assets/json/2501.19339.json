{
    "paper_title": "PixelWorld: Towards Perceiving Everything as Pixels",
    "authors": [
        "Zhiheng Lyu",
        "Xueguang Ma",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Existing foundation models typically process visual input as pixels and textual input as tokens, a paradigm that contrasts with human perception, where both modalities are processed in a unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for a unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. \"Perceive Everything as Pixels\" (PEAP). We introduce PixelWorld, a novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models' performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models' perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement. Our code, dataset will be released upon acceptance."
        },
        {
            "title": "Start",
            "content": "PixelWorld: Towards Perceiving Everything as Pixels Zhiheng Lyu 1 2 Xueguang Ma 1 Wenhu Chen 1 2 https://tiger-ai-lab.github.io/PixelWorld/ 5 2 0 2 1 ] . [ 1 9 3 3 9 1 . 1 0 5 2 : r Abstract Existing foundation models typically process visual input as pixels and textual input as tokens, paradigm that contrasts with human perception, where both modalities are processed in unified manner. With the rise of embodied and agentic AI, where inputs primarily come from camera pixels, the need for unified perception framework becomes increasingly evident. In this paper, we propose to unify all modalities (text, tables, code, diagrams, images, etc) as pixel inputs, i.e. Perceive Everything as Pixels (PEAP). We introduce PIXELWORLD, novel evaluation suite that unifies all the mentioned modalities into pixel space to gauge the existing models performance. Our findings show that (1) PEAP outperforms baseline with token-based input in multimodal datasets, benefiting from unified input for better disambiguation, (2) significant declines in reasoning and coding capabilities across all models when processing pixel-based input, underscoring the need to enhance foundation models perceptual abilities, (3) larger models can maintain strong performance on non-reasoning tasks under PEAP, while smaller models like Phi-3.5-V suffer significant performance degradation, (4) the attention pattern of PEAP is highly aligned with text token input, (5) PEAP can be accelerated significantly by exploiting the spatial sparsity. We conclude that the existing frontier models are competent in pixel perception, however, there is still headroom for improvement in the future work. 1. Introduction In recent years, large language models (LLMs) (OpenAI, 2025; Team, 2024) have achieved impressive results across 1Department of Computer Science, University of Waterloo 2Vector Institute, Toronto. Correspondence to: Zhiheng Lyu <z63lyu@uwaterloo.ca>, Wenhu Chen <wenhuchen@uwaterloo.ca>. 1 diverse real-world tasks. With advancements in training techniques and model architectures (Brown et al., 2020; Touvron et al., 2023), LLMs process textual data by tokenizing words into subword units and applying causal attention. However, this token-based method differs from how humans and AI agents (e.g., embodied robots (Tellex et al., 2020)) perceive textnamely, through pixel-based visual inputs such as images or screenshots. This mismatch poses key challenges. First, heuristic preprocessing can be domainand modality-sensitive (Dagan et al., 2024). Second, morphologically rich or underrepresented languages are often split into disproportionately long sequences (Liang et al., 2023), leading to inefficiencies. Third, even minor typographical errors can significantly degrade performance (Chai et al., 2024). Moreover, crucial format or layout information is usually lost during tokenization. As result, there is growing interest in vision-language models (VLMs) that directly process text as visual input, potentially improving robustness and preserving spatial or layout cues essential to real-world tasks. This is particularly prominent in areas like embodied agents (Tellex et al., 2020; Driess et al., 2023) and computer-based agents (Zheng et al., 2024; Koh et al., 2024), textual inputs are often captured as pixel-based representations, such as images or video frames obtained via cameras or screenshots. Although existing foundation models demonstrate strong generalization capabilities, most evaluations in the multimodal domain have primarily focused on image-based semantic understanding, with limited emphasis on understanding and reasoning capabilities for text-rich contexts. This discrepancy highlights the misalignment between current evaluation methodologies and the broader demands of real-world multimodal applications. To bridge this gap, we introduce PIXELWORLD, an evaluation suite designed to systematically analyze and compare LLMs performance on both textual and multimodal tasks when text is perceived as pixel-based input. In Section 2, we describe our dataset collection process, categorizing tasks into three major types: Text-Only, Structural, and Multimodal. For the first two categories, we develop an image synthesis pipeline to transform textual inputs into pixel representations, while for multimodal datasets, we leverage OCR pipeline to extract PixelWorld: Towards Perceiving Everything as Pixels Figure 1. PEAP framework: we investigate the possibility of perceive everything as pixels. This framework aligns better with human perception reducing the need for excessive pre-processing. Evaluated on our benchmark PIXELWORLD, PEAP boosts performance on multimodal tasks (e.g., websites, slides, documents) but struggles with complex, text-centric tasks (e.g., reasoning and coding). Larger models achieve better transferability between pixeland token-based performance compared to smaller ones. We also observed that text and images exhibit similar attention patterns, and reduced the overhead of model reasoning through patch pruning by PEAP-Fast. the text (see Table 1). In Section 3, we conduct comprehensive evaluation of models with varying scales on PIXELWORLD. Sections 3.1 and 3.2 show that applying PEAP to pure-text inputs can degrade performance, but this effect is strongly correlated with task difficulty. No significant decline is observed in simpler tasks such as semantic understanding, whereas more complex tasks like knowledge reasoning and code generation experience substantial drops. This aligns with Insight 2 in Figure 1. Furthermore, larger PEAP models align better with text inputs, while smaller models struggle with instruction following, warranting further investigation. This corresponds to Insight 3 in Figure 1. In Section 3.3, we demonstrate that PEAP excels in intrinsically multimodal tasks, such as website rendering, slide comprehension, and document understanding. Traditional OCR-based preprocessing often introduces errors and information loss, whereas PEAP mitigates these issues, supporting Insight 1 in Figure 1. However, smaller models still exhibit limitations in long-context tasks when using PEAP. Building on these observations, we provide deeper discussion in Section 4. In Section 4.1, we visualize the attention patterns of the final layer in Qwen2VL-7B. Despite differences in tokenization (words vs. visual patches), our attention visualizations reveal that PEAP maintains high degree of consistency in the attention mechanism, correspond to the Insight 4 in the Figure 1. This insight highlights the feasibility of using vision encoder as generic universal tokenizer, which could potentially address some of the known issues associated with existing text tokenizers. Furthermore, in Section 4.2, we conduct quantitative analysis of the potential time overhead using SuperGLUE as an example. We observe that PEAP can lead to increased latency, with processing times up to 3 longer due to larger token sizes. To mitigate this, we implemented sparsification algorithm, PEAP-Fast, which removes blank pixel regions, significantly accelerating processing times. This approach also aligns more closely with how humans perceive visual content, as empty patches mostly serve to maintain layout (which is already preserved by position embeddings of non-empty patches) and do not warrant additional attention. Section 4.3 evaluates PEAPs prompt sensitivity, showing that Chain-of-Thought (CoT) prompting boosts performance more effectively than standard methods. In summary, our contributions are as follows: 1. PixelWorld: This is comprehensive evaluation suite designed to evaluate pixel-based models across text, structural, and multimodal tasks, enabling direct comparisons 2 PixelWorld: Towards Perceiving Everything as Pixels between PEAP and token-based approaches. 2. Task Performance Analysis: PEAP can enhance structural and multimodal understanding (e.g., website, document) but degrade performance on complex text-centric tasks (e.g., code generation, reasoning). Notably, larger models exhibit better transferability between pixeland token-based performance, while smaller models struggle with instruction following under PEAP. 3. Efficiency and Attention Analysis: We propose PEAPFast to optimize the inference speed by removing blank pixel regions, reducing computation overhead without accuracy loss. Additionally, we show that PEAP and token-based models exhibit similar attention patterns, suggesting the possibility of adopting vision encoders as universal multimodal tokenizer. 2. Datasets Several representative datasets covering different skill domains are selected, as shown in Table 1. We primarily utilize the prompts provided by the datasets. If no prompts are available, we apply default prompt. By default, we employ Direct Prompting; however, for more complex and mathematical datasets such as MBPP (Austin et al., 2021), MMLU-Pro (Wang et al., 2024b), and MathVerse (Zhang et al., 2025), we adopt Chain-of-Thought (CoT) prompting to enhance performance. All evaluations are conducted in zero-shot manner to mitigate potential performance degradation caused by the sensitivity of instruction-tuned large models to few-shot prompting. To evaluate both Token-based and Pixel-based methods, we require paired Text-input and Image-input prompts. We adopted modality transfer strategies to reduce reliance on the information modality provided by existing datasets, as detailed in Table 1. For datasets categorized as Text-Only and Structured, all data is originally in plain text format, necessitating image synthesis prior to evaluation. For Multimodal datasets, textual content embedded in images is extracted using OCR, or the textual components provided by the original datasets are directly utilized for evaluation. Notably, the MathVerse dataset (Zhang et al., 2025) inherently includes Text-Only modality, offering detailed textual descriptions of image-based information. Image Data Synthesis For text-only and structured datasets, we developed an image data synthesis pipeline to generate diverse image inputs for evaluation. Image widths were adaptively adjusted between 512 and 1024 pixels based on text length, with fixed height of 256 pixels. Font sizes ranged from 15 to 25 points, and padding varied from 5 to 30 pixels. To enhance robustness, we applied various types of noise, including radial, horizontal, vertical, and Multi-Gaussian noise, as well as high-frequency Gaussian noise to simulate distortions commonly introduced by real-world cameras. For structured datasets, such as tables, data was rendered as images using the Python package dataframe image. Example inputs from different tasks are provided in Appendix A. 3. Experiments In this section, we will detail our baseline, metrics and models. The experimental results will be organized by Text Input, Structued Input and Multimodal Input. Baseline We establish the baseline by using the same VLMs with text-only prompts. To ensure fairness, we employ identical prompts and add the instruction Please follow the instruction in the image when applying PEAP. This ensures that the VLMs can correctly process instructions embedded within images. Ideally, the baseline and PEAP should yield equivalent performance. This comparison helps identify areas for improvement in existing VLMs. Metrics For question-answering tasks such as WikiSS-QA, SlidesVQA, and TableBench, we adopt ROUGE-L as our primary metric, as it effectively captures the alignment between generated answers and ground truth by measuring the longest common subsequence. For classification benchmarks, including MMLU-Pro, GLUE, SuperGLUE, ARC, and MathVerse, we use accuracy, which directly reflects the models performance in selecting correct options. For GLUE and SuperGLUE, we follow their standard evaluation protocols, utilizing task-specific metrics such as Matthews correlation, F1 score, and Pearson correlation. For the code generation task MBPP, we evaluate performance using the pass@1 rate, which measures whether the generated code successfully passes all test cases. For the mathematical reasoning dataset GSM8K, we employ exact match accuracy, as these problems require precise numerical answers. For the visualization subtask of TableBench, following the original codebase, we treat it as code generation task and evaluate the correctness of the generated visualizations. Model Selection To validate PIXELWORLD, we selected diverse set of vision-language models (VLMs) with varying scales to ensure the robustness and generalizability of our findings. It also allowed us to analyze the behavior of models across different sizes. We evaluated several widely used vision-language models (VLMs), including Qwen2VL2B (Wang et al., 2024a), Phi-3.5-3.2B (Abdin et al., 2024), Qwen2VL-7B (Wang et al., 2024a), Gemini-Flash (Team, 2024), and GPT-4o (OpenAI, 2025). 3.1. Text Input Figure 2 reports model accuracy on text-only datasets (e.g., ARC, MMLU-Pro, GLUE, GSM8K, SuperGLUE, MBPP). Two major insights emerge: 3 PixelWorld: Towards Perceiving Everything as Pixels Dataset Name Size Task Modality Transfer Split Text-only GLUE (Wang, 2018) SuperGLUE (Sarlin et al., 2020) MMLU-Pro (Wang et al., 2024b) ARC (Clark et al., 2018) GSM8K (Cobbe et al., 2021) MBPP (Austin et al., 2021) 59,879 Natural language understanding 19,294 Natural language understanding 12,032 Domain knowledge and reasoning 3,548 1,319 Math problem solving 757 Science question answering Programming tasks Synthesis Synthesis Synthesis Synthesis Synthesis Synthesis test test test test test test Structured TableBench (Wu et al., 2024) 888 Table data understanding and analysis Synthesis test Multimodal MathVerse (Zhang et al., 2025) MMMU-Pro (Yue et al., 2024) SlidesVQA (Tanaka et al., 2023) Wiki-SS (Ma et al., 2024) Math and visual reasoning 788 1,730 Multimodal reasoning 2,136 Multimodal question answering 3,000 Multimodal retrieval question answering Natural Synthesis OCR OCR test test test train Table 1. Overview of datasets categorized by modality, usage, size, and split. Modality Transfer means the method to adopt the dataset into counterpart modality. For OCR, we adopt the result from the origin datasets. For WikiSS-QA, since the positive document of the test set is not released, we subsample 3,000 training data points randomly to evaluate. Better Transferability in Larger Models Larger language models (e.g., GPT-4o, Gemini-Flash) exhibit better transferability between text and image-based performance, while smaller models struggle with both transferability and instruction following. For instance, on the ARC dataset, GPT-4os performance declines by only 0.59 points when transitioning from text to synthetic images, whereas the smaller Qwen2VL-2B suffers substantial 21.73-point drop (from approximately 68.61 to 46.88). This trend suggests that more capable models preserve their reasoning abilities across modalities, while smaller models face greater difficulty. Additionally, smaller models (e.g., Phi-3.5-vision) not only show weaker overall performance on standard benchmarks but also struggle significantly when instructions are presented as images. Their performance consistently lags behind that of larger models, particularly on tasks like MBPP. This supports Insight 3 in Figure 1. Performance Degradation with More Complex Tasks We observe significant drops on benchmarks requiring advanced reasoning, such as mathematical, coding or domain-specific tasks. For example, when moving from text to image inputs on the MMLU-Pro dataset, GPT-4o exhibits drop of more than 25 points. In contrast, on GLUE and SuperGLUE, the decline remains under 5 points. These findings indicate that while existing large models achieve comparable performance between text and visual modalities on simpler tasks, gap still exists at deeper level in visual-based and textbased understanding, demonstrating room for improvement in modality adaptation training. 3.2. Structured Input Figure 3 summarizes model performance on four TableBench subsets: Fact Checking, Data Analysis, Numerical Reasoning, and Visualization. Reasoning Complexity Impacts Performance Fact Checking and Data Analysis show moderate performance drops, as they rely on semantic understanding. In contrast, Numerical Reasoning and Visualizationrequiring more intricate reasoning and codingexhibit larger declines when switching to synthetic images. Combined with Performance Degradation with More Complex Tasks in Section 3.1, this supports Insight 2 in Figure 1. Smaller Performance Gaps with Structured Data Compared to text-only tasks, structured tasks show smaller performance gaps between text and image inputs. Notably, Qwen2VL-2B even outperforms its text-based results on Fact Checking, suggesting robust visual representations can aid semantic tasks in smaller models. Challenges with Mixed-Modality Inputs The semi formatwhere tables appear as images while questions remain text-basedperforms worse than either fully text-based or fully image-based formats. This suggests that conventional VQA approaches, which process text and images using separate encoders, may be more susceptible to performance bottlenecks. As multimodal scenarios become increasingly prevalent, PEAP is expected to demonstrate superior performance compared to mixed-modality methods. 4 PixelWorld: Towards Perceiving Everything as Pixels Figure 2. The performance of text-only datasets. The comparison is made between text input and synthesized image input. Most models demonstrate comparable performance on language understanding datasets such as SuperGLUE, GLUE, and ARC. However, notable performance disparities emerge between text-based input and synthesized image input on mathematical reasoning tasks (e.g., MMLU-Pro, GSM8K) and programming tasks (e.g., MBPP). Phi-3.5-Vision exhibits consistently poor performance across all vision tasks, primarily due to its insufficient instruction-following capabilities. 3.3. Multimodal Input 4. Discussion Figure 5 presents model performance on multimodal datasets, including text-only and vision-only subsets of Mathverse and VQA tasks like SlidesVQA and WikiSSQA. Results on MMMU-Pro (Figure 4) use reported values from the original paper. Three key observations emerge: Image Inputs Enhance Disambiguation Incorporating images improves performance by reducing ambiguity compared to text-only benchmarks. In SlidesVQA, all models outperform their text-only baselines, while in WikiSS-QA and MMLU-Pro, visual context provides clarifying information, leading to accuracy gains in larger models. Combined with Smaller Performance Gaps with Structured Data in Section 3.2, this supports Insight 1 in Figure 1. Challenges in Complex Reasoning While multimodal inputs aid basic tasks, complex reasoning remains bottleneck. In Mathverse, visual cues help but fail to support multi-step logical deductions. Even Gemini-Flash shows accuracy drops on intricate reasoning tasks. Additionally, WikiSS-QA poses challenges due to its long-context nature. Smaller models struggle with PEAP, and GPT-4o underperforms in token-based tasks, highlighting difficulties in processing extended contextual dependencies. This aligns with Sections 3.1 and 3.2. Larger Models Benefit More from Multimodal Data Larger models gain more from multimodal inputs. On SlidesVQA, Gemini Flash improves by 34.24 points, compared to Qwen2-VL-7Bs 23.55-point boost. This suggests that larger models, with more extensive prior knowledge and advanced architectures, leverage multimodal data more effectively than smaller models. 4.1. Q1: Does PEAP have the same attention? To investigate whether VLMs behaves similarly on textual and image inputs, we visualized the Average Attention of Qwen2-VL-7Bs final layer using heatmap (see Figure 6). Concretely, we examined its responses on SuperGLUE BoolQ example, comparing the models attention maps for text-based versus image-based inference. As shown in Figure 6, the model largely focuses on taskrelevant elements such as the question prompt (will there be sequel ...), the key words in the passage (e.g., film, starring, Alice), and the required answer format (Answer: True/False). This holds true across both textual and visual representations, indicating Qwen2-VL-7B exhibits comparable attention patterns irrespective of input modality. However, we also observe that certain blank patches in the image-based input can receive disproportionately high attention. This suggests that while the visual encoder parallels the text encoder in many respects, it still has redundancy. 4.2. Q2: How to make PEAP more efficient? As trade-off for generalization, image-based inference often requires significantly more computational resources than text-based inference. This is partly due to the additional overhead from the ViT backbone and higher redundancy in image tokens. To estimate the performance gap quantitatively, we conducted experiments on SuperGLUE  (Table 2)  . The results show that inference latency for image-based inputs can exceed text-based methods by 150% to 250%. To reduce redundancy in visual inputs, we propose PEAPFast, which first identifies empty patches via simple 5 PixelWorld: Towards Perceiving Everything as Pixels Figure 3. The performance of the structured dataset. We report all the subsets for the TableBench. In the semi setting, questions were presented as text, while tables were rendered as synthetic images. We observed that for tasks involving reasoning (numerical reasoning) and coding (visualization subset), synthetic images yielded inferior performance compared to text. However, for tasks emphasizing semantic understanding, such as data analysis and fact checking, synthetic images achieved performance comparable to or even surpassing text. Additionally, we found that the semi approach often performed worse than either text or synthetic images individually, providing insights into potential limitations and future directions for leveraging vision-language models (VLMs). Task BoolQ CB COPA MultiRC ReCoRD RTE WiC WSC SuperGLUE Evaluation Results Text PEAP PEAP-Fast 79.69% 82.11% 67.70% 40.77% 93.00% 91.00% 65.90% 61.28% 12.54% 5.94% 82.31% 72.92% 53.29% 55.80% 63.46% 65.38% 80.89% 39.57% 86.00% 60.80% 6.08% 77.26% 55.64% 59.62% Final Score 64.74% 59.40% 58.23% Table 2. Performance of Qwen2VL-7B on SuperGLUE dataset by Text, PEAP and PEAP-Fast. We can observe the comparable performance between PEAP and PEAP-Fast. stantial computational savings while maintaining strong performance, making image-based inference more practical for real-world deployments. Attention heatmap between PEAP and PEAP-Fast are shown in Appendix B. 4.3. Q3: Is PEAP sensitive to the prompting method? Massive experimental results in Section 3 show that the performance gap between image and text inputs still exists, potentially due to domain gaps in datasets or insufficient instruction following in image inputs. To address this, we applied CoT-style prompts to the SuperGLUE dataset to enhance cross-domain instruction following  (Table 4)  . Notably, Qwen2VL-7B showed significant improvements in tasks where image input underperformed compared to text input, such as CB and RTE. Overall, CoT prompts improved image input performance by 2.58%, surpassing the 0.3% Figure 4. The performance of the multimodal dataset (MMMUPro). We adopt the result reported by the origin paper. We can observe that strong models perform better in PEAP. variance-based thresholdif the pixel-value variance in patch is lower than preset threshold, that patch is treated as empty and is pruned from all attention computations. Crucially, we preserve the original positional embeddings for the remaining tokens, ensuring no loss of spatial layout perception. This strategy aligns with how humans naturally focus on salient regions rather than blank spaces, thereby significantly reducing context length without sacrificing structural information. Testing PEAP-Fast on SuperGLUE reveals minor accuracy drop of only 1.17%  (Table 2)  . More importantly, the average overhead decreases from 205.27% to 123.19%, yielding an 82.98% reduction  (Table 3)  . These results demonstrate that removing empty patches offers sub6 PixelWorld: Towards Perceiving Everything as Pixels Figure 5. The performance of the multimodal datasets (except MMMU-Pro). We compare text-only and vision-only subsets in Mathverse, while SlidesVQA and WikiSS-QA are evaluated as VQA tasks. Larger models perform better on text-based tasks with more modalities. GPT-4o tends to generate longer responses in long-context QA, leading to performance degradation on WikiSS-QA. Inference Time (s) Overhead (%) Subset Text PEAP PEAP-Fast PEAP PEAP-Fast 1,381 369 BoolQ 22 8 CB 38 39 COPA MultiRC 3,861 609 ReCoRD 7,016 19,012 68 RTE 69 WiC 11 WSC 117 224 36 Total 8,089 24, 906 15 22 2,550 14,288 92 157 27 18,051 274.80 175.00 -2.56 534.80 171.01 72.06 224.64 227.27 205.27 145.55 87.50 -43.59 318.71 103.72 35.29 127.54 145.45 123. Table 3. Inference Time (s) of Qwen2VL-7B on SuperGLUE dataset with single A100 server by PEAP and PEAP-Fast. We can observe 82.08% overhead reduce on PEAP-Fast method. Overhead is calculated as the percentage increase in time relative to the text method. improvement observed for text input. 5. Related Work Multimodal Large Language Models and Benchmarks Recent progress in multimodal AI has led to the development of models like GPT-4o (OpenAI, 2025), Gemini (Team, 2024), and Claude-3.5 (Anthropic, 2025), which integrate vision-based training to improve instruction-following capabilities. Benchmarks for these models have evolved from task-specific datasets, such as VQA (Agrawal et al., 2016) and DocVQA (Mathew et al., 2021), to more comprehensive evaluations, including MMMU-Pro (Yue et al., 2024), MMBench (Liu et al., 2024), and MegaBench (Chen et al., 2024). However, most current research focuses on the semantic understanding of visual content, with only few benchmarkssuch as MathVerse (Zhang et al., 2025) and MMMU-Pro (Yue et al., 2024)addressing text recognition and comprehension within images. Our work shifts the focus towards evaluating how well large language models understand language through visual input compared to traditional token-based input. Screenshot LMs Recent studies have demonstrated that pretraining on synthetic screenshots can enable visionlanguage models (VLMs) to achieve performance comparable to that of BERT on language modeling tasks (Lee et al., 2022; Rust et al., 2023; Gao et al., 2024). This approach allows models to better capture text structures without relying on OCR-based methods. Furthermore, our analysis highlights performance gap between existing VLMs on vision-based tasks and their text-only counterparts, particularly in the absence of relevant pretraining. Interestingly, in certain scenarios, VLMs perform as well as or even better than text-only models, underscoring the potential of this research direction. In the context of document retrieval, recent advancements (Faysse et al., 2024; Ma et al., 2024) have shown that large-scale pretraining on screenshots can outperform traditional OCR-based methods, further reinforcing the advantages of vision-language pretraining. Language Tokenization Tokenization methods, such as Byte Pair Encoding (BPE) (Shibata et al., 1999; Sennrich et al., 2016), are widely used in language modeling, but recent studies suggest that they may not always be optimal. For instance, MegaByte (Yu et al., 2023) demonstrated that fixed-length tokenization can improve both computational efficiency and cross-modal capabilities. Similarly, BLT (Pagnoni et al., 2024) proposed entropy-based tokenization, while LCM (team et al., 2024) emphasized the benefits of processing higher-level semantic concepts rather than in7 PixelWorld: Towards Perceiving Everything as Pixels Figure 6. Last Layer Attention Heatmap on QWen2VL-7B between token-based (left) and pixel-based (right) inference. Metric Direct CoT Improve (CoT - Direct) Text PEAP Text PEAP Text BoolQ CB COPA MultiRC ReCoRD 12.50% 5.88% RTE WiC WSC 79.88% 81.71% 81.13% 80.73% 1.25% 67.70% 34.78% 81.04% 59.57% 13.34% 93.00% 87.00% 89.00% 83.00% -4.00% 65.73% 62.28% 69.08% 60.41% 3.35% -6.13% 6.37% 82.31% 72.92% 83.03% 77.26% 0.72% 52.82% 54.39% 54.39% 53.92% 1.57% 65.38% 61.54% 57.69% 61.54% -7.69% 4.66% Overall 64.92% 57.56% 65.22% 60.14% 0.30% PEAP -0.98% 24.79% -4.00% -1.87% -1.22% 4.34% -0.47% 0.00% 2.58% Table 4. Comparison of Direct and CoT performance across Text and Image modalities, along with their respective improvements (CoT - Direct), presented as percentages. dividual tokens. Inspired by these approaches, we explore whether adaptive image patches can effectively infer textual meaning. At higher level, we investigate the unification of text and image inputs into shared representation space, enabling reasoning through abstract semantic concepts rather than traditional token-based methods. 6. Conclusion In this work, we introduce PIXELWORLD, an evaluation suite that unifies text, tables, code, and images into pixelbased inputs to bridge the gap between tokenized text processing and human-like visual perception. Our comprehensive experiments with PEAP show that while pixel-based input improves performance on structurally complex and inherently multimodal tasks (e.g., websites and slides) by reducing OCR errors and preserving contextual layout, it underperforms compared to token-based approaches on challenging text-centric tasks, such as advanced knowledge reasoning and coding. Additionally, attention visualizations reveal high transferability between pixel patches and textual tokens, indicating the feasibility of future vision-as-token approaches. However, pixel-based input incurs higher computational overhead, which we mitigate with PEAP-Fast by removing blank pixel regions, significantly speeding up inference. These findings highlight both the promise and trade-offs of perceiving everything as pixels, offering unified framework for multimodal understanding while underscoring the need for further research to optimize efficiency and address performance gaps in complex reasoning and coding tasks. 8 PixelWorld: Towards Perceiving Everything as Pixels"
        },
        {
            "title": "References",
            "content": "Abdin, M., Aneja, J., Awadalla, H., Awadallah, A., Awan, A. A., Bach, N., Bahree, A., Bakhtiari, A., Bao, J., Behl, H., Benhaim, A., Bilenko, M., Bjorck, J., Bubeck, S., Cai, M., Cai, Q., Chaudhary, V., Chen, D., Chen, D., Chen, W., Chen, Y.-C., Chen, Y.-L., Cheng, H., Chopra, P., Dai, X., Dixon, M., Eldan, R., Fragoso, V., Gao, J., Gao, M., Gao, M., Garg, A., Giorno, A. D., Goswami, A., Gunasekar, S., Haider, E., Hao, J., Hewett, R. J., Hu, W., Huynh, J., Iter, D., Jacobs, S. A., Javaheripi, M., Jin, X., Karampatziakis, N., Kauffmann, P., Khademi, M., Kim, D., Kim, Y. J., Kurilenko, L., Lee, J. R., Lee, Y. T., Li, Y., Li, Y., Liang, C., Liden, L., Lin, X., Lin, Z., Liu, C., Liu, L., Liu, M., Liu, W., Liu, X., Luo, C., Madan, P., Mahmoudzadeh, A., Majercak, D., Mazzola, M., Mendes, C. C. T., Mitra, A., Modi, H., Nguyen, A., Norick, B., Patra, B., PerezBecker, D., Portet, T., Pryzant, R., Qin, H., Radmilac, M., Ren, L., de Rosa, G., Rosset, C., Roy, S., Ruwase, O., Saarikivi, O., Saied, A., Salim, A., Santacroce, M., Shah, S., Shang, N., Sharma, H., Shen, Y., Shukla, S., Song, X., Tanaka, M., Tupini, A., Vaddamanu, P., Wang, C., Wang, G., Wang, L., Wang, S., Wang, X., Wang, Y., Ward, R., Wen, W., Witte, P., Wu, H., Wu, X., Wyatt, M., Xiao, B., Xu, C., Xu, J., Xu, W., Xue, J., Yadav, S., Yang, F., Yang, J., Yang, Y., Yang, Z., Yu, D., Yuan, L., Zhang, C., Zhang, C., Zhang, J., Zhang, L. L., Zhang, Y., Zhang, Y., Zhang, Y., and Zhou, X. Phi-3 technical report: highly capable language model locally on your phone, 2024. URL https://arxiv.org/abs/2404.14219. Agrawal, A., Lu, J., Antol, S., Mitchell, M., Zitnick, C. L., Batra, D., and Parikh, D. Vqa: Visual question answering, 2016. URL https://arxiv.org/abs/1505. 00468. Claude 3.5: sonnet of progress, Anthropic. URL https://www.anthropic.com/ 2025. news/claude-3-5-sonnet. Accessed: 2025-0113. Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 18771901, 2020. Chai, Y., Fang, Y., Peng, Q., and Li, X. Tokenization falling short: On subword robustness in large language models. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 15821599, 2024. Chen, J., Liang, T., Siu, S., Wang, Z., Wang, K., Wang, Y., Ni, Y., Zhu, W., Jiang, Z., Lyu, B., Jiang, D., He, X., Liu, Y., Hu, H., Yue, X., and Chen, W. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks, 2024. URL https://arxiv.org/abs/2410.10563. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Dagan, G., Synnaeve, G., and Roziere, B. Getting the most out of your tokenizer for pre-training and domain adaptation. In Forty-first International Conference on Machine Learning, 2024. Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., et al. Palm-e: an embodied multimodal language model. In Proceedings of the 40th International Conference on Machine Learning, pp. 84698488, 2023. Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., and Colombo, P. Colpali: Efficient document retrieval with vision language models, 2024. URL https://arxiv.org/abs/2407.01449. Gao, T., Wang, Z., Bhaskar, A., and Chen, D. Improving language understanding from screenshots, 2024. URL https://arxiv.org/abs/2402.14073. Koh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C., Huang, P.-Y., Neubig, G., Zhou, S., Salakhutdinov, R., and Fried, D. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024. Lee, K., Joshi, M., Turc, I., Hu, H., Liu, F., Eisenschlos, J., Khandelwal, U., Shaw, P., Chang, M.-W., and Toutanova, K. Pix2struct: Screenshot parsing as pretraining for visual language understanding, 2022. URL https://arxiv. org/abs/2210.03347. Liang, D., Gonen, H., Mao, Y., Hou, R., Goyal, N., Ghazvininejad, M., Zettlemoyer, L., and Khabsa, M. Xlmv: Overcoming the vocabulary bottleneck in multilingual masked language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1314213152, 2023. 9 PixelWorld: Towards Perceiving Everything as Pixels Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J., He, C., Liu, Z., Chen, K., and Lin, D. Mmbench: Is your multi-modal model an allaround player?, 2024. URL https://arxiv.org/ abs/2307.06281. Ma, X., Lin, S.-C., Li, M., Chen, W., and Lin, J. Unifying multimodal retrieval via document screenshot embedding. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 6492 6505, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024. emnlp-main.373. URL https://aclanthology. org/2024.emnlp-main.373/. Mathew, M., Karatzas, D., and Jawahar, C. V. Docvqa: dataset for vqa on document images, 2021. URL https: //arxiv.org/abs/2007.00398. OpenAI. Hello gpt-4o, 2025. URL https://openai. com/index/hello-gpt-4o/. Accessed: 2025-0113. Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J., Zettlemoyer, L., et al. Byte latent transformer: Patches scale better than tokens. arXiv preprint arXiv:2412.09871, 2024. Rust, P., Lotz, J. F., Bugliarello, E., Salesky, E., de Lhoneux, M., and Elliott, D. Language modelling with pixels. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=FkSp8VW8RjH. Sarlin, P.-E., DeTone, D., Malisiewicz, T., and Rabinovich, A. Superglue: Learning feature matching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 49384947, 2020. Sennrich, R., Haddow, B., and Birch, A. Neural machine In Erk, translation of rare words with subword units. K. and Smith, N. A. (eds.), Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 17151725, Berlin, Germany, August 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1162. URL https://aclanthology.org/P16-1162/. Shibata, Y., Kida, T., Fukamachi, S., Takeda, M., Shinohara, A., Shinohara, T., and Arikawa, S. Byte pair encoding: text compression scheme that accelerates pattern matching. 1999. Tanaka, R., Nishida, K., Nishida, K., Hasegawa, T., Saito, I., and Saito, K. Slidevqa: dataset for document visual question answering on multiple images, 2023. URL https://arxiv.org/abs/2301.04883. Team, G. Gemini: family of highly capable multimodal models, 2024. URL https://arxiv.org/abs/ 2312.11805. team, L., Barrault, L., Duquenne, P.-A., Elbayad, M., Kozhevnikov, A., Alastruey, B., Andrews, P., Coria, M., Couairon, G., Costa-juss`a, M. R., Dale, D., Elsahar, H., Heffernan, K., Janeiro, J. M., Tran, T., Ropers, C., Sanchez, E., Roman, R. S., Mourachko, A., Saleem, S., and Schwenk, H. Large concept models: Language modeling in sentence representation space, 2024. URL https://arxiv.org/abs/2412.08821. Tellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 3(1):2555, 2020. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Wang, A. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024b. Wu, X., Yang, J., Chai, L., Zhang, G., Liu, J., Du, X., Liang, D., Shu, D., Cheng, X., Sun, T., et al. Tablebench: comprehensive and complex benchmark for table question answering. arXiv preprint arXiv:2408.09174, 2024. Yu, L., Simig, D., Flaherty, C., Aghajanyan, A., Zettlemoyer, L., and Lewis, M. Megabyte: Predicting million-byte sequences with multiscale transformers. Advances in Neural Information Processing Systems, 36:7880878823, 2023. Yue, X., Zheng, T., Ni, Y., Wang, Y., Zhang, K., Tong, S., Sun, Y., Yu, B., Zhang, G., Sun, H., Su, Y., Chen, 10 PixelWorld: Towards Perceiving Everything as Pixels W., and Neubig, G. Mmmu-pro: more robust multidiscipline multimodal understanding benchmark, 2024. URL https://arxiv.org/abs/2409.02813. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Qiao, Y., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? In European Conference on Computer Vision, pp. 169186. Springer, 2025. Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v (ision) is generalist web agent, if grounded. In Forty-first International Conference on Machine Learning, 2024. 11 PixelWorld: Towards Perceiving Everything as Pixels Figure 7. An example input of GSM8K dataset, using Direct Prompt. A. Example Input Figure 7 and Figure 8 gives two examples about the vision input. B. Attention Heatmap before and after ImageFast Method Figure 9 presents heatmap comparison between PEAP and PEAP-Fast. PEAP-Fast effectively reduces redundant patches while preserving attention on key regions. 12 PixelWorld: Towards Perceiving Everything as Pixels Figure 8. An example input of TableBench dataset, using Direct Prompt. Figure 9. Last Layer Attention Heatmap on Qwen2VL-7B between PEAP (left) and PEAP-Fast (right)."
        }
    ],
    "affiliations": [
        "Department of Computer Science, University of Waterloo",
        "Vector Institute, Toronto"
    ]
}