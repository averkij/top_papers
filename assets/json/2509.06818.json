{
    "paper_title": "UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward",
    "authors": [
        "Yufeng Cheng",
        "Wenxu Wu",
        "Shaojin Wu",
        "Mengqi Huang",
        "Fei Ding",
        "Qian He"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in image customization exhibit a wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, a significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, a Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With \"multi-to-multi matching\" paradigm, UMO reformulates multi-identity generation as a global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop a scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose a new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting a new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 8 1 8 6 0 . 9 0 5 2 : r UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward Yufeng Cheng Wenxu Wu Shaojin Wu Mengqi Huang Fei Ding Qian He UXO Team, Intelligent Creation Lab, ByteDance"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in image customization exhibit wide range of application prospects due to stronger customization capabilities. However, since we humans are more sensitive to faces, significant challenge remains in preserving consistent identity while avoiding identity confusion with multi-reference images, limiting the identity scalability of customization models. To address this, we present UMO, Unified Multi-identity Optimization framework, designed to maintain high-fidelity identity preservation and alleviate identity confusion with scalability. With multi-tomulti matching paradigm, UMO reformulates multi-identity generation as global assignment optimization problem and unleashes multi-identity consistency for existing image customization methods generally through reinforcement learning on diffusion models. To facilitate the training of UMO, we develop scalable customization dataset with multi-reference images, consisting of both synthesised and real parts. Additionally, we propose new metric to measure identity confusion. Extensive experiments demonstrate that UMO not only improves identity consistency significantly, but also reduces identity confusion on several image customization methods, setting new state-of-the-art among open-source methods along the dimension of identity preserving. Code and model: https://github.com/bytedance/UMO Date: September 9, 2025 Project Page: https://bytedance.github.io/UMO Correspondence: Yufeng Cheng at chengyufeng.cb1@bytedance.com"
        },
        {
            "title": "Introduction",
            "content": "Image customization, which aims to create images that simultaneously adhere to the semantic content of textual prompts and the visual appearance of reference images, has garnered significant research attention in recent years. Among various subjects, the customization of human identities (i.e., ID) has attracted particular interest due to its broad range of real-world applications, such as personalized film production and virtual avatar creation. Different from other subjects, humans are exceptionally sensitive to identity customization, i.e., even subtle discrepancies in appearance can lead to noticeable loss of identity fidelity, thereby making human ID customization significantly more challenging. This challenge is further amplified when multiple identities need to be customized simultaneously, as it requires the model not only to preserve the unique characteristics of each individual ID, but also to maintain clear distinctions among them within the generated images. * Corresponding author Project lead. 1 Figure 1 Showcase of our UMO model in different scenarios. The detailed prompts are listed in Table 9. Existing multi-identity customization methods primarily focus on constructing improved multi-identity paired data to enhance the consistency of multiple identities. For example, DreamO [18] and OmniGen [33] build large-scale training datasets for identity customization, including multi-identity paired data derived from either video sources or in-context image generation. Meanwhile, some recent approaches aim to mitigate confusion between multiple identities by employing identity masks to explicitly constrain the location or Figure 2 Our UMO unleashes multi-identity consistency and alleviates identity confusion. Existing image customization methods suffer low facial fidelity and severe identity confusion, while UMO can tackle these problems with results in blue boxes. position of each identity. For instance, MS-Diffusion [29] introduces layout-guided mechanism to explicitly control the generation location of each identity. RealCustom++ [13, 17] further proposes to explicitly separate the influence masks of each identity in order to disentangle their respective generations. In summary, existing methods mainly adopt one-to-one mapping paradigm, in which focuses on learning direct correspondence between each identity in the reference image and the corresponding generated one. In this study, we argue that the existing one-to-one mapping paradigm fails to comprehensively address both intra-ID variability and inter-ID distinction, leading to increased identity confusion and reduced identity similarity as the number of identities grows. On the one hand, intra-ID variability refers to the inherent variability within single identity, where the same individual may present different attributes (e.g., poses, expressions, etc.) between the reference image and the generated output. On the other hand, inter-ID distinction underscores the importance of not only accurately capturing the distinctive characteristics of the target identity during generation, but also explicitly suppressing the features associated with other identities, thereby ensuring clear separation and minimizing identity confusion in multi-identity scenarios. As the number of identities increases, the risk arises that the distinctions between different identities may become less salient, potentially approaching the degree of variability observed within single identity. Therefore, by focusing exclusively on the one-to-one mapping between each identity and its corresponding reference, existing methods overlook the increasing overlap between intra-ID variability and inter-ID distinction as the number of identities grows. This limitation fundamentally restricts their scalability, as they are unable to effectively preserve clear identity boundaries in large-scale multi-identity scenarios. To address the challenge, we propose novel multi-to-multi matching paradigm, whose key idea is to reformulate multi-identity generation as global assignment optimization problem, aiming to maximize the 3 overall matching quality between multiple identities and multiple references. Therefore, each generated identity can be paired with the most suitable reference, which maximizes inter-ID distinction while minimizing the impact of intra-ID variability, thereby enabling accurate and scalable multi-identity generation. Technically, to ensure that our method remains concise and readily applicable to various customized models to improve identity consistency, we propose Unified Multi-identity Optimization (UMO) framework, which operationalizes the multi-to-multi matching paradigm through novel Reference Reward Feedback Learning (ReReFL). Specifically, UMO begins by defining lightweight and reliable single-identity reference reward based on the cosine distance between identity embeddings. This is then scaled to multi-identity context by formulating bipartite graph between multiple references and generated identities, and optimizing the global matching score to achieve the optimal assignment. Moreover, to support the effective training of UMO, we develop scalable customization dataset with multiple reference images for each identity, along with new metric (ID-Conf) designed to precisely evaluate the extent of multi-identity confusion. Our main contributions can be summarized as follows: Concept: We highlight that existing customization methods fails to address intra-ID variability and inter-ID distinction, as their one-to-one mapping paradigm leads to decreased identity consistency with the scale of the identities. For the first time, we propose novel multi-to-multi matching paradigm that maximizes overall matching quality between multiple identities and references. Technique: We propose UMO, novel Reference Reward Feedback Learning framework featuring scalable multi-identity reference reward optimization, which can be easily integrated into various customization models. Metric: We propose ID-Conf to accurately evaluate the extent of multi-identity confusion. For reference identity, ID-Conf is defined as the relative margin between the two most similar generated candidates, given the observation that confusion occurs with several indistinct generated faces. Performance: We conduct extensive experiments on XVerseBench [4] and OmniContext [31], including multiidentity scenarios. Our UMO significantly boosts identity similarity and mitigates confusion simultaneously on various customized models, leading to the highest ID-Sim and ID-Conf scores. This demonstrates its strong and general promoting effect on identity consistency, showcasing its capability to deliver state-of-the-art (SOTA) results with high fidelity identities as shown in Figure 1."
        },
        {
            "title": "2.1 Multi-subject Driven Generation",
            "content": "Text-to-image models [1, 7, 16, 22, 24, 26] experienced explosive growth in recent years. Despite the increasing text-instruction following capability of those models, they still struggle to fulfill common requirement: generating images for given subjects with reference images. Early works [8, 25] extended subject-driven generation from text-to-image (T2I) models through parameterefficient fine-tuning on each given subject. Subsequent works [30, 38] achieved general single-subject injection by modifying and training components such as the cross-attention module. Building on the general single-subject feature injection, numerous extensions have been proposed. series of works, such as MIP-adapter [14], MSDiffusion [29], and UNO [32] extend the method to general multi-subject driven generation by leveraging the flexible context window lengths of attention mechanisms. Another improvement aims at ID preservation. InstanceID [28], Pulid [9] achieved higher ID preservation by adapting widely used face recognition models as encoders and training supervisions. Other works directly train an image generation model from scratch to natively support both textual input and reference subject as input [5, 6, 33]. Some closed-source commercial model [37] may support the greater potential in fidelity of those methods, but havent matched by current open-source and academic works."
        },
        {
            "title": "2.2 Reinforcement Learning for Diffusion Model",
            "content": "For LLM, reinforcement learning has been wildely used to align with human preferences like truthfulness, helpfulness, etc. [20]. Since the distribution formulation difference between the autoregressive model and 4 Figure 3 Illustration of the training framework of UMO. UMOs training process follows ReReFL in Algorithm 1 with Multi-Identity Matching Reward. diffusion model, the success of reinforcement learning in language models cant directly transfer to image generation models. Prior works [2, 27, 35, 36] have explored several reinforcement learning algorithms that can be used to improve diffusion models and align with external rewards. But most of them are focusing on text alignment and aesthetics, few works for identity similarity improvement with reinforcement learning."
        },
        {
            "title": "3.1 Preliminary",
            "content": "Diffusion models [10] have shown great capability in text-to-image generation, which sampling desired data distribution through iterative denoising. To align the models with human preferences, RLHF has been widely adapted, e.g., ReFL [35] with objective Lreward = EyiY (ϕ(r(yi, gθ(yi)))) (1) where = {y1, y2, . . . , yn} is prompt set, ϕ is reward-to-loss map function, is reward model and gθ represents the generated image of diffusion model with parameters θ corresponding to prompt yi."
        },
        {
            "title": "3.2 Data Curation Pipeline",
            "content": "In this paper, we aim to scale the number of identities in multi-identity preservation. But most public datasets, like X2I-subject [33] and Openstory [39], have few samples with identitiesx larger than two, limiting our methods. To address that, we build data pipeline inspired by MovieGen [23], which uses the frame with multiple identities as query and recalls each identity from frames with other clips in the same long video. Besides, we explored synthetic data pipeline suggested by previous works such as UNO [32] to construct multi-identity preservation dataset. However, the identity similarity of synthetic data is relatively low. We applied strict face similarity filtering and retained only data in highly imaginative and partially stylized scenes, using them as complement. 5 Algorithm 1 Reference Reward Feedback Learning (ReReFL) for diffusion models Require: Image customization diffusion models with pretrained parameters θ0, pretrain loss function ϕ, pretrain loss weight λ; reward function R; the number of noise scheduler time steps , time step range for ReReFL finetuning [Ts, Te]; dataset = {(yi, I0i, 1 ) = 1, 2, . . . } where yi is prompt, I0i ri is target image and 1 , 2 , . . . , ri ri ri , . . . , ) do ri , . . . , , 2 Ldiff ϕθi(yi, 1 ri ri ri U(Ts, Te) // Pick random time step xT (0, I) for τ = T, . . . , + 1 do ; I0i ) // Calculate pretrained loss with reference images , 2 ri are reference images. 1: for (yi, I0i, 1 ri 2: , . . . , ri , 2 ri 3: 4: 5: 6: 7: 8: xτ 1 no_grad(vθi(yi, 1 ri , 2 ri , . . . , ri , xτ )) , 2 ri end for , . . . , xt1 vθi(yi, 1 ri ri ˆI0i xt1 // Predict original image by noise scheduler LReReFL R( ˆI0i; yi, 1 , 2 ri ri λLdiff + LReReFL θi+1 θi // Update parameters with , . . . , ri , xt) 9: 10: 11: 12: 13: end for ) // Calculate ReReFL loss with negative reward (a) SIR scores of UNO [32]. (b) SIR scores of OmniGen2 [31]. Figure 4 Single identity reward (SIR) scores of UNO [32] and OmniGen2 [31] with different generation seeds along denoising steps. The scores become stable after step 5 and 10 respectively. And the results with highest and lowest reward scores indicating its discriminatory ability. The combination of diverse data sources and the carefully designed extraction pipeline ensures the resulting multi-person identity preservation dataset features larger number of distinct individuals and great variations beyond the identity features."
        },
        {
            "title": "3.3 UMO Training Framework",
            "content": "3.3.1 Reinforcement Learning on Image Customization trivial solution to scale multi-identity preservation is finetuning existing image customization methods with data constructed above which only achieves minor improvement as shown in Table 4 partly due to its relatively small proportion in diffusion models objective. To steer models to align to human sensitivity towards faces, we extend ReFL to customization. Specifically, we propose Reference Reward Feedback Learning (ReReFL), as illustrated in the red dashed box of Figure 3, 6 which directly backpropagate reward signals with reference to the inference, as Algorithm 1 shown."
        },
        {
            "title": "3.3.2 Reward with Single Identity Reference",
            "content": "Effective reward is crucial to the improvement brought by RLHF. In the easiest case with only one reference identity, we introduce Single Identity Reward (SIR), the cosine distance between identity embeddings to ensure high degree of identity fidelity RSIR = cos(ψ( ˆI0), ψ(I 1 where ψ represents tiny network to recognition face and get face embedding. As shown in Figure 4, single identity reward varies drastically during former steps while getting relatively stable during latter steps. The generation result with the highest reward score preserves identity better than the one with the lowest reward score, indicating that it could serve as reliable reward function in ReReFL. )) (2)"
        },
        {
            "title": "3.3.3 Reward with Multi-Identity Reference",
            "content": "When scaling SIR to more complicated case with multi-reference, new challenge arises, that is improving identity fidelity while enlarging inter-ID distinction to alleviate confusion at the same time. As shown in Figure 2, when identity confusion occurs, the customization models usually ignore some references, or generate person with face from one identity while cloth from another identity. We suggest that the keypoint lies in finding the corresponding face for each reference identity. Inspired by DETR [3] and Multi-Object Tracking [41], we formulate the problem as the assignment problem under multi-to-multi matching paradigm. Specifically, let us denote by the number of reference identities, the number of detected faces in ˆI0, i.e., ψ( ˆI0) RN where represents the dimension of face embedding. Each face is vertex in bipartite graph with one part ˆF containing all faces detected in ˆI0 and the other part containing all faces from reference identities, as illustrated in the black box of Figure 3. The edges are weighted by SIR of two vertices, i.e., eFj , ˆFk )). To find maximum weight matching, we search assignment ˆσ in all the potential ones Sn of reference faces to faces in ˆI0 with the lowest cost: = cos(ψ( ˆI0)j, ψ(I ˆσ = arg min σSn (cid:88) i=1 Lmatch(Fi, ˆFσ(i)) = arg max σSn (cid:88) i=1 eFi, ˆFσ(i) (3) where Lmatch(Fi, ˆFσ(i)) = eFi, ˆFσ(i) is pair-wise matching cost between reference identity Fi and generated one with index σ(i). This optimal assignment is computed efficiently with the Hungarian algorithm [15]. With the optimal assignment ˆσ, shown as solid lines in the black box of Figure 3, we find the association between reference identities and generated identities. To improve multi-identity fidelity and alleviate confusion together, we define Multi-Identity Matching Reward (MIMR) as: RMIMR = 1 (cid:88) (cid:88) j=1 k=1 (λ11{k=ˆσ(j)} + λ21{k=ˆσ(j)})eFj , ˆFk (4) where λ1 > 0, λ2 < 0, adjusting gradient orientation according to ˆσ."
        },
        {
            "title": "4.1 Experiments Setting",
            "content": "4.1.1 Implementation Details To demonstrate our UMOs generalist, the experiments are conducted based on two kinds of SOTA methods as the pretrained models: UNO [32], an image customization method that supports multi-reference images based on in-context learning [12, 34] on DiT [21]. Figure 5 Qualitative comparison with different methods on XVerseBench [4]. OmniGen2 [31], unified model capable of understanding and diverse generation tasks, including image editing and in-context generation. For the training hyperparameters, we set pretrain loss weight λ = 1 in Algorithm 1 and λ1 = 1, λ2 = 1 in Equation (4). The inference-related parameters in Algorithm 1 are set according to the suggestions of the pretrained models. Specifically, for UNO, we set the number of noise scheduler time steps = 25, time step range [TS, Te] = [1, 10] since the reward scores have become stable as shown in Figure 4a, while we set = 50 and [TS, Te] = [1, 20] for OmniGen2 according to Figure 4b. We train these models with learning rate of 5 106 and total batch size of 8 on 8 NVIDIA A100 GPUs. We use LoRA [11] with rank of 512 during the training process. The remaining hyperparameters follow their own original settings. 4.1.2 Comparative Methods UMO is unified multi-identity optimization framework to improving identity fidelity and reducing confusion. We compare it with the two pretrained models UNO [32] and OmniGen2 [31] as baselines. Except two baselines, we compare UMO with some leading methods which support multi-reference images, including MS-Diffusion [29], MIP-Adapter [14], OmniGen [33], DreamO [18] and XVerse [4]. 4.1.3 Evaluation Benchmark and Metrics We evaluate these methods on XVerseBench [4] and OmniContext [31], which cover scenarios of both single reference and multi-reference. Since OmniContext only employs GPT-4.1 [19] to assess the generated outputs, we supplement the evaluation with the ID-Sim score in XVerseBench. To measure the severity of multi-identity confusion, we propose new metric ID-Conf, which is defined as the relative margin between the two most similar generated candidate faces for reference identity, based on the observation the confusion occurs with several indistinct generated faces. Given several reference identities = {F1, F2, . . . , Fn} and detected faces ˆF = { ˆF1, ˆF2, . . . , ˆFm} from the generated result, we define the 8 Figure 6 Qualitative comparison with different methods on OmniContext [31]. ID-Conf metric as below: j[1] j[2] := arg max 1jm cos (cid:0)Ψ (Fi), Ψ ( ˆFj)(cid:1) := arg max 1jm,j=j[1] cos (cid:0)Ψ (Fi), Ψ ( ˆFj)(cid:1) ID-Conf = 1 (cid:88) (cid:18) clip 1 1in cos (cid:0)Ψ (Fi), Ψ ( ˆFj[2] cos (cid:0)Ψ (Fi), Ψ ( ˆFj[1] (5) (cid:19) )(cid:1) )(cid:1) , 0, 1 where Ψ is the model used in XVerseBench to get face embeddings. larger value of the ID-Conf metric indicates lower severity of identity confusion."
        },
        {
            "title": "4.3 Qualitative Analysis",
            "content": "We compare with various SOTA methods on XVerseBench [4] to verify the effectiveness of UMO as shown in Figure 5. From the top row to the bottom one, the identity promoting effect of UMO is scalable from single identity to multi-identity scenarios and has generalization on both UNO [32] and OmniGen2 [31]. Specifically, UMO improves identity fidelity with more similar generated faces with the reference on all scenarios and alleviates confusion as the last three rows shown with more distinct generated faces with each other. For example, UNO itself gets two similar girls, both different with the reference one, in the third row while the two reference identities can be easily discriminated in the result of UMO. Also, the customization ability of general subjects is boosted or retained as the second and the fourth rows shown. As contrast, MS-Diffusion [29], UNO [32] and OmniGen2 [31] all suffer low identity fidelity. Although DreamO [18] shows moderate identity similarity, confusion issue occurs when the number of reference increases. Results of these models show the limited identity scalability of one-to-one mapping paradigm, while UMO indicates the superiority of 9 Method ID-Sim IP-Sim AVG MS-Diffusion [29] MIP-Adapter [14] OmniGen [33] DreamO [18] XVerse [4] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 44.12 39.59 76.51 75.48 79.48 47.91 80.89 62.41 91.57 76.48 71.97 78.46 70.84 76.86 80.40 77. 74.08 79.74 60.30 55.78 77.49 73.16 78.17 64.16 78.99 68.25 85.66 Table 1 Quantitative results on task type Single-Subject from XVerseBench. : We evaluate MS-Diffusion with boxes set as in MS-Bench [29]. We highlight the best and the second-best values for each metric. Method ID-Sim ID-Conf IP-Sim AVG MS-Diffusion [29] DreamO [18] XVerse [4] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 38.98 50.24 66. 31.82 69.09 40.81 71.59 66.40 68.67 72.44 61.06 78.06 62.02 77. 70.98 64.63 71.48 67.00 68.57 67.15 73.80 58.79 61.18 70.17 53.29 71. 56.66 74.38 Table 2 Quantitative results on task type Multi-Subject from XVerseBench. : We align ID-Conf score with the value range of the other metrics. Method Overall ID-Sim ID-Conf AVG MS-Diffusion [29] DreamO [18] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 4.72 6.25 4.71 5.34 7.18 7. 2.32 4.44 1.91 4.62 3.51 7.07 6.59 6.23 4.91 6. 6.35 7.60 4.54 5.64 3.84 5.52 5.68 7. Table 3 Quantitative results on OmniContext. The Overall score is the geometric mean of Prompt Following (PF) and Subject Consistency (SC) scores. multi-to-multi matching paradigm. Further comparison on OmniContext [31] demonstrates the positive impact on mitigating identity confusion as shown in Figure 6, where DreamO and OmniGen2 both suffer confusion issue, e.g., reference identities missing in the first row of OmniGen2 and the third row of DreamO, mismatching characteristics of each identity like mismatching hair in the first row of DreamO and wrongly assigned clothes in the second row of OmniGen2."
        },
        {
            "title": "4.4 User Study",
            "content": "We further conduct user study questionnaire to show the superiority of UMO. Questionnaires are distributed to both domain experts and non-experts, who rank the results from each method along with several dimension, 10 Figure 7 Radar charts of user evaluation of methods on different dimensions. i.e., identity consistency, prompt following, aesthetic and overall performance. As shown in Figure 7, UMO achieves the best preference, demonstrating the effectiveness of multi-to-multi matching paradigm and showcasing its capability to deliver state-of-the-art results. Also, compared to the two baselines, i.e., UNO [32] and OmniGen2 [31], UMO gets significant improvement across all evaluated dimensions."
        },
        {
            "title": "4.5 Ablation Study",
            "content": "We conduct ablation study with UMO trained on UNO [32] on XVerseBench [4] as shown in Table 4 and Figure 8 to demonstrate the effect of ReReFL and MIMR. We also conduct further ablation study with UMO trained on OmniGen2 [31] on OmniContext [31] in Table 5. 4.5.1 Effect of ReReFL As shown in the first two columns of Figure 8 and the first two rows in Table 4, finetuning UNO with the same data as UMO (i.e., raw SFT) leads to minor improvement especially in ID-Sim and ID-Conf scores, while optimizing UNO with ReReFL demonstrates significant improvement. The comparison indicates the necessity to utilize reinforcement learning with reward focusing on facial region to unleash potential in identity consistency. Instead, vanilla SFT would suppress attention of facial feature due to its small proportion. Similar comparison on OmniGen2 in Table 5 shows that the effect of ReReFL is general. Method ID-Sim ID-Conf IP-Sim AVG UNO [32] SFT ReReFL w/ SIR UMO (Ours) 31.82 33.94 65.16 69.09 61.06 62.88 65.28 78.06 67.00 65.17 67. 68.57 53.29 54.00 65.90 71.91 Table 4 Ablation study with UNO as the pretrained model on task type Multi-Subject from XVerseBench. 11 Method Overall ID-Sim ID-Conf AVG OmniGen2 [31] SFT ReReFL w/ SIR UMO (Ours) 7.23 7.24 7.14 7. 2.86 3.38 6.44 6.61 6.67 6.80 7.32 9.04 5.59 5.81 6.97 7. Table 5 Ablation study with OmniGen2 as the pretrained model on task type MULTI from OmniContext. Figure 8 Visualization of ablation study. Zoom in for details. 4.5.2 Effect of MIMR The last two rows of Table 4 demonstrate that training with SIR instead of MIMR has significant drop in both ID-Sim and ID-Conf in multi-subject scenario. As the last two columns of Figure 8 shown, although SIR enhances identity similarity as well, it suffers severe confusion problem, e.g., result of training with SIR in the first row of Figure 8 has two similar identities with the reference girl missing, and the result in the last row suffers mismatching characteristics of each identity, i.e., the hair color of the generated woman. The comparison proves the effectiveness of MIMR through assigning correct facial supervisions to boost identity consistency and mitigate confusion. Similar observation in Table 5 indicates the generalization of the effect of MIMR."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we present UMO, Unified Multi-identity Optimization framework to improve identity consistency and alleviate confusion in multi-reference scenarios, which is based on the multi-to-multi matching paradigm through novel Reference Reward Feedback Learning algorithm with scalable Multi-Identity Reference Reward that reformulating multi-identity generation as global assignment optimization problem. Additionally, we develop scalable customization dataset along with new metric to evaluate the extent of multi-identity confusion. Extensive experiments demonstrate that UMO significantly enhances the identity preserving ability with less confusion and greater identity scalability on various customized models, setting new state-of-the-art among open-source methods along the dimension of identity preserving."
        },
        {
            "title": "References",
            "content": "[1] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. [2] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with In The Twelfth International Conference on Learning Representations, 2024. URL reinforcement learning. https://openreview.net/forum?id=YCWjhGrJFD. [3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. [4] Bowen Chen, Mengyi Zhao, Haomiao Sun, Li Chen, Xu Wang, Kang Du, and Xinglong Wu. Xverse: Consistent multi-subject control of identity and semantic attributes via dit modulation. arXiv preprint arXiv:2506.21416, 2025. [5] Xi Chen, Zhifei Zhang, He Zhang, Yuqian Zhou, Soo Ye Kim, Qing Liu, Yijun Li, Jianming Zhang, Nanxuan Zhao, Yilin Wang, Hui Ding, Zhe Lin, and Hengshuang. Unireal: Universal image generation and editing via learning real-world dynamics. arXiv preprint arXiv:2412.07774, 2024. [6] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [8] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. [9] Zinan Guo, Yanze Wu, Chen Zhuowei, Peng Zhang, Qian He, et al. Pulid: Pure and lightning id customization via contrastive alignment. Advances in neural information processing systems, 37:3677736804, 2024. [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [11] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [12] Lianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and Jingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775, 2024. [13] Mengqi Huang, Zhendong Mao, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom: Narrowing real text word for real-time open-domain text-to-image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74767485, 2024. [14] Qihan Huang, Siming Fu, Jinlong Liu, Hao Jiang, Yipeng Yu, and Jie Song. Resolving multi-condition confusion for finetuning-free personalized image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 37073714, 2025. [15] Harold Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2): 8397, 1955. [16] Black Forest Labs. Flux: Official inference repository for flux.1 models, 2024. URL https://github.com/ black-forest-labs/flux. Accessed: 2025-02-07. [17] Zhendong Mao, Mengqi Huang, Fei Ding, Mingcong Liu, Qian He, and Yongdong Zhang. Realcustom++: Representing images as real-word for real-time customization. arXiv preprint arXiv:2408.09744, 2024. [18] Chong Mou, Yanze Wu, Wenxu Wu, Zinan Guo, Pengze Zhang, Yufeng Cheng, Yiming Luo, Fei Ding, Shiwen Zhang, Xinghui Li, et al. Dreamo: unified framework for image customization. arXiv preprint arXiv:2504.16915, 2025. 13 [19] OpenAI. Introducing gpt-4.1 in the api, 2025. URL https://openai.com/index/gpt-4-1. Accessed: 2025-04-14. [20] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. [21] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [22] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [23] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [24] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [25] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2250022510, 2023. [26] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. [27] Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82288238, 2024. [28] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. [29] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. [30] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1594315953, 2023. [31] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. [32] Shaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization: Unlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160, 2025. [33] Shitao Xiao, Yueze Wang, Junjie Zhou, Huaying Yuan, Xingrun Xing, Ruiran Yan, Chaofan Li, Shuting Wang, Tiejun Huang, and Zheng Liu. Omnigen: Unified image generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1329413304, 2025. [34] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580, 2023. [35] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. [36] Zeyue Xue, Jie Wu, Yu Gao, Fangyuan Kong, Lingting Zhu, Mengzhao Chen, Zhiheng Liu, Wei Liu, Qiushan Guo, Weilin Huang, et al. Dancegrpo: Unleashing grpo on visual generation. arXiv preprint arXiv:2505.07818, 2025. 14 [37] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation. arXiv preprint arXiv:2504.02782, 2025. [38] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [39] Zilyu Ye, Jinxiu Liu, JinJin Cao, Zhiyang Chen, Ziwei Xuan, Mingyuan Zhou, Qi Liu, and Guo-Jun Qi. Openstory: In Proceedings of the IEEE/CVF large-scale open-domain dataset for subject-driven visual storytelling. Conference on Computer Vision and Pattern Recognition, pages 79537962, 2024. [40] Yang Yue, Zhiqi Chen, Rui Lu, Andrew Zhao, Zhaokai Wang, Shiji Song, and Gao Huang. Does reinforcement learning really incentivize reasoning capacity in llms beyond the base model? arXiv preprint arXiv:2504.13837, 2025. [41] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. Bytetrack: Multi-object tracking by associating every detection box. In European conference on computer vision, pages 121. Springer, 2022. 15 UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward"
        },
        {
            "title": "Appendix",
            "content": "We report detailed comparison on each task type from OmniContext [31] as shown in Table 6, Table 7 and Table 8. In all SINGLE, MULTI and SCENE task types from OmniContext, UMO significantly boosts the ID-Sim and ID-Conf on both pretrained models, i.e., UNO [32] and OmniGen2 [31], leading over previous methods, e.g., MS-Diffusion [29] and DreamO [18]. The comprehensive evaluation demonstrate the effectiveness and generalization of UMO training framework to improve identity consistency and mitigate confusion. Method Overall ID-Sim ID-Conf AVG MS-Diffusion [29] DreamO [18] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 5.83 7.65 6.72 6.77 7.82 7.78 2.89 5.09 2.11 5. 4.75 7.95 6.05 5.83 4.48 7.03 7.08 6.72 4.92 6. 4.44 6.33 6.55 7.48 Table 6 Quantitative results on task type SINGLE from OmniContext. Method Overall ID-Sim ID-Conf AVG MS-Diffusion [29] DreamO [18] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 4.75 7.05 4.48 5. 7.23 7.14 2.18 4.21 1.75 4.46 2.86 6.61 6.97 7. 5.23 7.20 6.67 9.04 4.63 6.13 3.82 5.67 5. 7.60 Table 7 Quantitative results on task type MULTI from OmniContext. Method Overall ID-Sim ID-Conf AVG MS-Diffusion [29] DreamO [18] UNO [32] UMO (Ours) OmniGen2 [31] UMO (Ours) 3.95 4.52 3.59 4.38 6.71 6. 1.90 4.03 1.87 4.22 2.91 6.65 6.75 5.74 5.03 5. 5.31 7.03 4.20 4.76 3.50 4.73 4.98 6. Table 8 Quantitative results on task type SCENE from OmniContext."
        },
        {
            "title": "G More Qualitative Results",
            "content": "We show more qualitative results on XVerseBench [4] in Figure 9 and Figure 10, and OmniContext [31] in Figure 11 and Figure 12. UMO improves identity similarity without confusion on both single identity and multi-identity scenarios, showing its general and scalable effectiveness. Single Identity: In Figure 9, UNO [32] itself generates customization results with low identity fidelity. As comparison, UMO gets much more similar generated identities. In Figure 11, although OmniGen2 [31] gets moderate fidelity, UMO based on it still achieves remarkable improvement without degradation of subject similarity (e.g., clothes, etc) or prompt following. The observation in single identity scenario demonstrates the extraordinary potential of UMO to enhance identity consistency across several existing models. Multi-Identity: In Figure 11, UNO [32] suffers low similarity of facial features and identity confusion, e.g., the two generated identities in the last row have almost the same facial features which is the average facial features of the two reference ones. By contrast, UMO shows its superiority with higher fidelity and without identity confusion. In Figure 12, the results of OmniGen2 [31] show moderate identity similarity, while UMO still boosts it without degradation. The observation in the scenario of multi-identity shows the impressive promoting ability of UMO to improve identity fidelity and alleviate confusion on existing image customization methods."
        },
        {
            "title": "H Discussion",
            "content": "Although we build UMO to maintain high-fidelity identity preservation and alleviate identity confusion with scalability to multi-identity, stably scaling to more identities is still restricted due to the dramatic decrease of the pretrained models reference ability when the number of reference images or identities increases, which demonstrates similar view with [40]. Scenarios Single Identity Single Identity + Subject Prompts (1) The man on the beach. (2) The girl holds board written \"UMO\" (3) The man is skateboarding on the street. (4) Transform the style of image into Sketch style (1) The man holding the can. (2) The elf wearing the T-shirt in the second image, with \"UXO Team\" written on it. (3) The woman in the first image is running with the dog in the second image. Two Identities (1) The man and the woman are sitting in classroom (2) The man in the first image and the man in the second image shake hands and look straight ahead. (3) Half-body portrait of the woman in the first image and the old man in the second image, retro comic style Two Identities + Subject (1) The woman is holding the toy, while the man standing beside. (2) The man is holding hair dryer and drying the womans hair. More Identities (1) The three people are playing cards. (2) Portrait of the three people, oil painting style. Table 9 The detailed prompts used in Figure 1. 17 Figure 9 Qualitative results on task type Single-Subject from XVerseBench [4]. 18 Figure 10 Qualitative results on task type Multi-Subject from XVerseBench [4]. Figure 11 Qualitative results on task type SINGLE Character from OmniContext [31]. 20 Figure 12 Qualitative results on task type MULTI Character from OmniContext [31]."
        }
    ],
    "affiliations": [
        "UXO Team, Intelligent Creation Lab, ByteDance"
    ]
}