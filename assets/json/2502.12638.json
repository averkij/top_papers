{
    "paper_title": "NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation",
    "authors": [
        "Zhiyuan Liu",
        "Yanchen Luo",
        "Han Huang",
        "Enzhi Zhang",
        "Sihang Li",
        "Junfeng Fang",
        "Yaorui Shi",
        "Xiang Wang",
        "Kenji Kawaguchi",
        "Tat-Seng Chua"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose a foundation model -- NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecule's 3D conformers with a 3D diffusion model. We enhance NExT-Mol's performance by scaling up the LM's model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves a 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and a 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . - [ 1 8 3 6 2 1 . 2 0 5 2 : r Published as conference paper at ICLR NEXT-MOL: 3D DIFFUSION MEETS 1D LANGUAGE MODELING FOR 3D MOLECULE GENERATION Zhiyuan Liu1, Yanchen Luo2, Han Huang3, Enzhi Zhang4, Sihang Li2, Junfeng Fang2, Yaorui Shi2, Xiang Wang2, Kenji Kawaguchi1, Tat-Seng Chua1 1 National University of Singapore, 3 Chinese University of Hong Kong, zhiyuan@nus.edu.sg, luoyanchen@mail.ustc.edu.cn 2 University of Science and Technology of China, 4 Hokkaido University"
        },
        {
            "title": "ABSTRACT",
            "content": "3D molecule generation is crucial for drug discovery and material design. While prior efforts focus on 3D diffusion models for their benefits in modeling continuous 3D conformers, they overlook the advantages of 1D SELFIES-based Language Models (LMs), which can generate 100% valid molecules and leverage the billion-scale 1D molecule datasets. To combine these advantages for 3D molecule generation, we propose foundation model NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation. NExT-Mol uses an extensively pretrained molecule LM for 1D molecule generation, and subsequently predicts the generated molecules 3D conformers with 3D diffusion model. We enhance NExT-Mols performance by scaling up the LMs model size, refining the diffusion neural architecture, and applying 1D to 3D transfer learning. Notably, our 1D molecule LM significantly outperforms baselines in distributional similarity while ensuring validity, and our 3D diffusion model achieves leading performances in conformer prediction. Given these improvements in 1D and 3D modeling, NExT-Mol achieves 26% relative improvement in 3D FCD for de novo 3D generation on GEOM-DRUGS, and 13% average relative gain for conditional 3D generation on QM9-2014. Our codes and pretrained checkpoints are available at https://github.com/acharkq/NExT-Mol."
        },
        {
            "title": "INTRODUCTION",
            "content": "Molecule discovery is crucial for designing new drugs and materials. To efficiently navigate the astronomical chemical space of molecules, generative deep learning methods have been extensively explored. While promising progress has been made in generating 2D molecular graphs (Jin et al., 2018; Vignac et al., 2023a), recent research has shifted toward 3D molecule generation due to its broader application scope. For example, understanding the 3D molecular geometry is crucial for structure-based drug design (Zhang et al., 2023), prediction of molecular quantum chemical properties (Zhou et al., 2023), and molecular dynamic simulation (Hansson et al., 2002). 3D molecule generation aims to predict 3D molecular conformers along with their 2D graphs (Hoogeboom et al., 2022). These generated 3D molecules are typically evaluated based on their molecular validity and stability, ensuring adherence to the chemical valency rules. Recent advancements in 3D diffusion models (Vignac et al., 2023b; Hua et al., 2023; Huang et al., 2024) have improved these metrics by better modeling continuous 3D conformers, yet they still occasionally generate invalid molecules. This validity issue hinders distribution learning of valid molecular structures, like pharmacophoric functional groups. For improvement, we draw inspiration from 1D molecule generation (Fang et al., 2024b; Polykovskiy et al., 2020) studies, which reliably ensure 100% validity. By representing 2D molecular graphs as linear strings of SELFIES (Krenn et al., 2020), these approaches typically leverage 1D language models (LMs) for 2D molecule generation. Due to SELFIES inherent robustness, the generated molecules are guaranteed to be 100% valid. Inspired by these studies, natural solution for improving 3D molecule generation is to incorporate 1D SELFIES-based LM into 3D diffusion model (Jing et al., 2022), thus leveraging the chemical Equal contribution. Correspondence to Xiang Wang. xiangwang1223@gmail.com. 1 Published as conference paper at ICLR Figure 1: Overview of our NExT-Mol foundation model for 3D molecule generation. NExT-Mol consists of three key components: (1) MoLlama, large LM for generating 1D molecule sequences; (2) DMT, diffusion model to predict 3D conformers from the 1D sequences; and (3) NExT-Mol leverages transfer learning to enhance DMTs 3D prediction with MoLlamas 1D representations. validity of 1D representations while improving 3D conformer prediction. To our best knowledge, few prior research has thoroughly explored this incorporation for 3D molecule generation. To bridge the research gap above, we explore two-step solution for 3D molecule generation: initially generating 1D molecule (a subset of 3D molecule) using an LM and subsequently predicting its 3D conformer with diffusion model. Here we focus on three key strategies scaling up 1D molecular LMs, refining the architecture of 3D diffusion models, and utilizing transfer learning between 1D and 3D modeling to resolve the following three challenges faced by prior studies: The Development of An Effective 1D Molecular LM. This can be done by training an autoregressive transformer LM (Vaswani et al., 2017) on large SELFIES corpus. However, existing studies have the following limitations: some use non-autoregressive pretraining, rendering them unsuitable for de novo generation (Fang et al., 2024b; Irwin et al., 2022; Born & Manica, 2023; Yuksel et al., 2023); some do not have 100% validity (Bagal et al., 2021); and others are constrained by small model sizes and employ non-transformer architectures, limiting their scalability (Polykovskiy et al., 2020; Eckmann et al., 2022; Arus-Pous et al., 2019; Jin et al., 2018). The Design of Powerful 3D Diffusion Model. This is to accurately generate the 3D conformers for the 1D molecules generated by the 1D molecular LM in the earlier step. Existing works can be improved by adopting scalable architectures (Jing et al., 2022; Corso et al., 2024; Xu et al., 2022; Ganea et al., 2021) or leveraging the full information of 2D molecular graphs (Wang et al., 2024). Transfer Learning between 1D Molecule Sequences and 3D Conformers. It has the potential to offer significant improvement to 3D conformer prediction, given the greater availability of 1D sequences compared to high-accuracy 3D conformers, which are typically derived by expensive physics-based computations. For example, ZINC22 (Tingle et al., 2023) now includes over 54.9 billion 1D sequences and GEOM (Axelrod & Gomez-Bombarelli, 2022) holds only 37 million 3D conformers. Although this 1D to 3D transfer learning was successfully applied to 3D protein structure prediction (Lin et al., 2023; Wu et al., 2022), similar methods remain mostly unexplored for small molecules, indicating significant research opportunity. To address the challenges above, we propose foundation model NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation, as illustrated in Figure 1. NExT-Mol consists of three key components: (1) To achieve effective autoregressive 1D molecule generation, we pretrain Molecular Llama LM (MoLlama) (Touvron et al., 2023; Zhang et al., 2024) on large collection of 1.8B SELFIES sequences. This extensive pretraining empowers MoLlama to effectively capture the desired 1D/2D molecular patterns (e.g., scaffolds and fragments) in downstream datasets, laying strong foundation for the subsequent 3D conformer prediction. (2) To achieve high-accuracy 3D conformer prediction, we introduce novel diffusion model Diffusion Molecular Transformer (DMT). DMT combines the power of scalable neural architecture (Wang et al., 2024) and retains the full information of 2D molecular graphs by incorporating the Relational Multi-Head Self-Attention (Huang et al., 2024) that extends the standard self-attention by incorporating pair inPublished as conference paper at ICLR 2025 formation describing atomic interactions. We show that DMT achieves leading performance for 3D conformer prediction, surpassing prior works by 1.1% COV-R on GEOM-DRUGS. Further, it accurately reveals the 3D structures of MoLlama-generated 1D molecules, providing 26% relative gain in 3D FCD and significant improvements in geometric similarity and stability on GEOM-DRUGS. (3) We show that transfer learning between 1D molecular sequences and 3D conformers improves conformer prediction by 1.3% COV-R on GEOM-DRUGS. This improvement is driven by transfering MoLlamas pretrained 1D representations, which encode rich molecular knowledge, to DMT for better molecular representation. The 1D-to-3D modality gap in transfer learning is bridged by our proposed cross-modal projector and the corresponding training strategy (Liu et al., 2024a). Collectively, our NExT-Mol foundation model is versatile multi-task learner, and demonstrates leading performances for de novo 3D molecule generation, conditional 3D molecule generation, and 3D conformer prediction on the GEOM-DRUGS, GEOM-QM9 (Axelrod & Gomez-Bombarelli, 2022) and QM9-2014 (Ramakrishnan et al., 2014) datasets. The strong performance highlights NExT-Mols effectiveness and its potential impact as foundation model in the field. We further present extensive ablation studies to demonstrate the significance of each component of NExT-Mol."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "A complete molecule includes atoms, bonds, and the 3D coordinates of atoms (i.e., 3D conformer). However, due to the expensive computation for obtaining high-accuracy 3D conformers (Axelrod & Gomez-Bombarelli, 2022), many studies focus on generating atoms and bonds without 3D conformers, representing molecules as 1D sequences or 2D graphs. Here we begin by reviewing 1D and 2D molecule generation, then discuss 3D molecule generation and 3D conformer prediction. 1D and 2D Molecule Generation aims to generate the atoms and bonds of molecule. 1D generation works are mostly based on LMs. However, they usually apply non-autoregressive pretraining such as span-prediction (Irwin et al., 2022; Fang et al., 2024b; Born & Manica, 2023), making them unsuitable for de novo generation. Other works use non-transformer architecture (Arus-Pous et al., 2019; Polykovskiy et al., 2020; Flam-Shepherd et al., 2022; Gomez-Bombarelli et al., 2018; Eckmann et al., 2022; Popova et al., 2018), which are unsuitable for scale-up (Vaswani et al., 2017). 2D molecule generation works typically decompose molecular graphs as functional fragments (or atoms), and train models to recurrently generate or edit these fragments (Jin et al., 2018; Xie et al., 2021; Luo et al., 2021; Shi et al., 2020; Sun et al., 2022; Liu et al., 2018; You et al., 2018; Popova et al., 2019; Jin et al., 2019). However, due to their non-transformer architectures and domainspecialized training methods, these 2D generation models also face challenges with scalability and transfer learning. We refer readers to (Du et al., 2022) for comprehensive survey in this area. 3D Molecule Generation is dominated by diffusion models (Hoogeboom et al., 2022; Bao et al., 2023; Huang et al., 2023a; 2024; 2023b; Vignac et al., 2023b; Hua et al., 2023). While autoregressive methods have been explored (Gebauer et al., 2019; 2022; Luo & Ji, 2022; Simm et al., 2020), they underperform diffusion models, potentially due to their inability to model bonds and the error accumulation when autoregressively generating 3D coordinates. Diffusion models typically employ 3D equivariant neural networks (Satorras et al., 2021) to denoise the variables of atoms, bonds, and 3D coordinates within single diffusion process. However, they predict molecules without validity constraints and are limited by insufficient 3D data. To address these issues, we aim to integrate the two advantages of 1D SELFIES sequences 100% validity and the more abundant dataset (Sterling & Irwin, 2015; Tingle et al., 2023) into 3D molecule generation for improvement. 3D Conformer Prediction is to predict the 3D conformer given the atoms and bonds of molecule (Xu et al., 2022; Ganea et al., 2021; Zhou et al., 2023; Jing et al., 2022; Corso et al., 2024). The current state-of-the-art approach scales up diffusion model using general-purpose transformer architecture (Wang et al., 2024), but it overlooks the chemical bond information and uses lossy representation of molecular structures. We address these issues by introducing the DMT architecture that maintains scalability and retains the full information of 2D molecular graphs. 3 3D DIFFUSION MEETS 1D LM FOR 3D MOLECULE GENERATION NExT-Mol for 3D Molecule Generation. NExT-Mol is foundation model that generates 3D molecules with two-step method: initially generating the 1D molecule sequence (a subset of 3D 3 Published as conference paper at ICLR 2025 Figure 2: Overview of DMTs neural architecture. (a) DMT is diffusion model learning to denoise random Gaussian perturbations ϵ applied on the 3D coordinates of atoms. (b) DMT relies on the RMHA module to iteratively update atom representations and pair representations E. molecule) using the MoLlama LM and subsequently predicting its 3D conformer using the DMT diffusion model. Here we begin by introducing the MoLlama for 1D molecule generation and then proceed to DMT. Finally, we detail the transfer learning method to incorporate MoLlamas 1D representation to enhance DMTs 3D conformer prediction. Appendix includes implementation details. 3. 1D MOLECULE GENERATION WITH MOLECULAR LLAMA LM Data Preparation. Following (Irwin et al., 2022), we collect 1.8 billion molecules from the ZINC15 database (Sterling & Irwin, 2015), significantly more than the 100 million molecules used in previous studies (Irwin et al., 2022; Fang et al., 2024b). We preprocess the molecules to transform them into SELFIES and perform data filtering to avoid overlap with the downstream datasets. The resulting dataset contains 90 billion SELFIES tokens. Pretraining MoLlama. Our MoLlama is 960M parameter LM with the popular decoder-only Llama-2 (Touvron et al., 2023) architecture. We pretrain it from scratch for 1D molecule generation with the next-token prediction objective. The pretraining takes 555K global steps, processing 145 billion tokens, which amounts to approximately 1.6 passes through the pretraining dataset. Randomized SELFIES Augmentation. We use randomized SELFIES as data augmentations during fine-tuning MoLlama for 1D molecule generation. molecule can have multiple valid SELFIES, because they are generated by traversing the 2D molecular graph in different orders. Randomized SELFIES are generated by traversing in random orders. This approach improves sample diversity and mitigates overfitting compared to using the canonical traversal order (Arus-Pous et al., 2019). The intuition is that the atoms in molecule are inherently unordered, therefore an ideal LM should generate different orderings of the same molecule with equal likelihood. 3.2 3D CONFORMER PREDICTION WITH DIFFUSION MOLECULAR TRANSFORMER Here we elaborate on the three key components of our proposed DMT: (1) the diffusion process governing the training and inference; (2) the neural architecture; and (3) the rotation augmentation. Diffusion Process. molecule = (x, h, e) is represented by its 3D coordinates RN 3, atom features RN d1 (e.g., atom types), and pair features RN d2 (e.g., chemical bonds), where is the number of atoms and d1 and d2 are the feature dimensions. For 3D conformer prediction, we use continuous-time diffusion model (Kingma et al., 2021) that denoises molecules 3D coordinates based on its atom and pair features. As Figure 2a shows, in the forward diffusion process, noises are gradually applied to the original 3D coordinates x(0) = such that q(x(t)x(0)) = (x(t); α(t)x(0), (1 α(t))I), where (0, 1] is the diffusions time-step, and α(t) is hyperparameter controlling the noise scale at the step. Based on the reparameterization trick (Ho et al., 2020), we can sample x(t) = 1 α(t)ϵ(t), where ϵ(t) (0, I). Given the perturbed coordinates x(t), DMT is trained to predict the noise ϵ(t) by minimizing the 2, where G(t) = (x(t), h, e). After training, DMT can be MSE loss = ϵ(t) DMT(G(t), t)2 employed for 3D conformer prediction through ancestral sampling (Ho et al., 2020). α(t)x(0) + Neural Architecture. As Figure 2b illustrates, DMT adopts Relational Multi-Head Self-Attention (RMHA) (Huang et al., 2024) and adaptive layernorm (adaLN) (Perez et al., 2018; Peebles & Xie, 2023). adaLN replaces the learnable scale and shift parameters in standard layernorm (Ba, 2016) Published as conference paper at ICLR 2025 Figure 3: Transfer learning between MoLlamas 1D representations and DMTs 3D prediction. (a) cross-modal projector bridges the gap between MoLlama and DMT. Grey atoms have no corresponding SELFIES tokens, and are replaced by learnable token. (b) Transfer learnings three training stages. Snowflake denotes frozen parameters while flame denotes trainable ones. with adaptive ones that are generated from the condition embedding C, which combines the timestep and optionally desired chemical property. For simplicity, we omit adaLNs in discussion below. The philosophy behind DMTs neural architecture generally follows the bitter lesson recently revealed by MCF (Wang et al., 2024) that large scalable models outperform domain-specific inductive biases. Notably, MCF shows that it is unnecessary to have an architecture of built-in 3D equivariance for conformer prediction. However, MCF is limited to employing lossy representation of 2D molecular structures and overlooks bond information, by relying on the top-k eigenvectors of the graph Laplacian (Maskey et al., 2022) to represent 2D molecular graphs. To address this issue, DMT retains the full information of 2D molecular graphs in its atom representation RN and pair representation RN d, and then applies RMHA to learn and distinguish the 2D graph structures. Specifically, the atom representations are initialized by concatenating the atom features and the perturbed 3D coordinates x(t), the pair representations are initialized by concatenating the pair features and the distances between each atom pair. and are then iteratively refined by RMHA. The single-head RMHA is defined below with the multi-head version in Appendix C.2: [Q; K; V] = [Wq; Wk; Wv]H, (1) [QE; VE] = tanh([Weq; Wev]E), ai,j = softmaxj( (QE i,j Qi)K ), (3) Oi = (cid:88) j=1 ai,j(VE i,j Vj), (2) (4) where denotes element-wise product; softmaxj denotes softmax along the dimension; linear projectors Wq, Wk, and Wv generate queries, keys, and values for atom representations, Weq and Wev generate queries and values for pair representations; Oi is RMHAs output for the i-th atom; and QE i,j Rd are the query and value for the atom pair representation (i, j). i,j, VE ij and key VE RMHA uses the pair-level query QE ij of to modify the atom-level query Qi and value Vj through element-wise multiplication (), enabling RMHA to fully incorporate pair representations. Specifically, the pair affects attention scores via (QE , and affects the aggregated attention values via VE ij Vj. In this way, the output is adaptively informed by the structural and interaction information in E. After RMHA, Oi is passed to an MLP to update the atom representation Hi, and the linear combination of Oi and Oj is used to update the pair representation Ei,j. As Figure 2b illustrates, residual connections and adaLNs are included for improved performance. ij Qi)K Random Rotation Augmentation. Following AlphaFold3 (Abramson et al., 2024), we apply the same random rotation augmentation on both the input 3D coordinates (x(t)) and the target 3D coordinates (ϵ(t)) to help DMT obtain equivariance to rotated inputs by learning. While (Wang et al., 2024) report decreased performance given random rotations, DMT benefits from it, potentially due to the improved neural architecture. 3.3 MOLLAMA REPRESENTATIONS IMPROVE DMTS 3D CONFORMER PREDICTION We explore the transfer learning between molecular 1D sequences and 3D conformers. As Figure 3 illustrates, we leverage MoLlamas pretrained representation to improve DMTs 3D conformer prediction. This is achieved by our cross-modal projector and the corresponding training strategy. 5 Published as conference paper at ICLR 2025 Cross-Modal Projector. Following Liu et al. (2023a), DMT uses projector to leverage MoLlama for atom representation, addressing two challenges: (1) MoLlama uses causal self-attention, where each token only perceives preceding tokens, limiting the representation quality; and (2) SELFIES tokens do not map directly to individual atoms. Mitigating the first issue, we feed MoLlamas SELFIES representations into single-layer bi-directional self-attention (Vaswani et al., 2017), expanding the receptive field of SELFIES tokens. Further, we program the SELFIES-to-atom mapping using the SELFIES and RDKit software. For atoms corresponding to multiple SELFIES tokens, we obtain its representation by mean pooling; for hydrogen atoms without corresponding SELFIES tokens, we use learnable token as replacement. The output of the SELFIES-to-atom mapping is then fed into an MLP and concatenated with DMTs original atom representations for 3D conformer prediction. Training Strategy. As Figure 3b illustrates, to save computation, we fine-tune pretrained DMT to incorporate MoLlama representations, instead of training new DMT from scratch using MoLlama representations. Throughout the process, MoLlama uses LoRA tuning (Hu et al., 2021) to save memory. The training strategy consists of three stages. In the first stage, we train standalone DMT without MoLlama until convergence. In the second stage, we attach MoLlama and the cross-modal projector to the pretrained DMT, keeping the DMT parameters frozen, and train for 10 epochs to warmup the random parameters in the projector and LoRA. This step prevents the gradients from the random parameters from distorting the pretrained DMT parameters (Kumar et al., 2022). In the final stage, we fine-tune the entire integrated model until convergence. When incorporating MoLlama representations into DMT, we find that canonical SELFIES performs better than randomized SELFIES. This may be because bridging the gap between 1D MoLlama and 3D DMT is challenging, and using the fixed canonical representations leads to faster convergence."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "In this section, we evaluate NExT-Mols performance on de novo 3D molecule generation and conditional 3D molecule generation. Further, we report results of 3D conformer prediction, the critical second step in our two-step generation process. Finally, we present ablation studies to demonstrate the effectiveness of each component of NExT-Mol. 4.1 EXPERIMENTAL SETTINGS Table 1: Datasets for each task. Datasets. As Table 1 shows, we evaluate on the popular GEOM-DRUGS (Axelrod & Gomez-Bombarelli, 2022), GEOM-QM9 (Axelrod & Gomez-Bombarelli, 2022), and QM92014 (Ramakrishnan et al., 2014) datasets. Among them, we focus on GEOM-DRUGS, which is the most pharmaceutically relevant and largest one. Due to different tasks incorporating different dataset splits, we separately fine-tune NExT-Mol for each task without sharing weights. De novo 3D Mol Gen Conditional 3D Mol Gen QM9-2014 3D Conformer Pred GEOM-DRUGS, GEOM-QM GEOM-DRUGS, QM9-2014 Dataset Task Baselines. For de novo and conditional 3D molecule genration, we use baselines of CDGS (Huang et al., 2023a), JODO (Huang et al., 2024), MiDi (Vignac et al., 2023b), G-SchNet (Gebauer et al., 2019), G-SphereNet (Luo & Ji, 2022), EDM (Hoogeboom et al., 2022), MDM (Huang et al., 2023b), GeoLDM (Xu et al., 2023), EEGSDE (Bao et al., 2023), EQGAT-diff (Le et al., 2024), MolGPT (Bagal et al., 2021), and MolGen (Fang et al., 2024b). For 3D conformer prediction, we use baselines of OMEGA (Hawkins, 2017), GeoMol (Ganea et al., 2021), GeoDiff (Xu et al., 2022), Torsional Diffusion (Jing et al., 2022), Particle Guidance (Corso et al., 2024), and MCF (Wang et al., 2024). More details on experimental settings are in Appendix D. NExT-Mol. Throughout the section, NExT-Mol fine-tunes the pretrained 960M MoLlama for 1D molecule generation. We have trained two versions of DMT: DMT-B of 55 million parameters and DMT-L of 150 million. For the de novo and conditional 3D generation molecule tasks (cf. Section 4.2 and Section 4.3), NExT-Mol uses DMT-B. DMT uses 100 sampling steps by default. 4.2 De Novo 3D MOLECULE GENERATION Experimental Setting. Generating complete 3D molecule involves generating the 2D molecular graph and the corresponding 3D conformer. Therefore, we evaluate both the predicted 2D molecu6 Published as conference paper at ICLR 2025 Table 2: Performances for de novo 3D molecule generation. * denotes our reproduced results using their source codes. Other baseline results are borrowed from (Huang et al., 2024). 2D-Metric evaluates the directly predicted 2D molecular graphs, whereas the 3D-Metric evaluates the predicted 3D coordinates or the 2D molecular graphs reconstructed from the 3D coordinates. (a) Performances on the GEOM-DRUGS dataset. 2D-Metric FCD AtomStable MolStable V&C V&U V&U&N SNN Frag Scaf Train MolGPT* MolGen* CDGS JODO MiDi* EQGAT-diff* NExT-Mol, ours 3D-Metric Train EDM JODO MiDi* EQGAT-diff* NExT-Mol, ours 0.251 0.888 0.655 22.051 2.523 7.054 6.310 0. FCD 13.73 31.29 19.99 23.14 25.89 14.69 1.000 0.979 1.000 0.991 1.000 0.968 0.999 1.000 1.000 0.977 0.995 0.706 0.981 0.818 0.998 0.999 1.000 1.000 0.957 0.955 1.000 0.993 0.285 0.285 0.874 0.905 0.633 0.654 0.959 0.993 1.000 0.999 0.000 0.918 0.759 0.285 0.902 0.652 0.702 0. 0.585 0.999 0.520 0.991 0.513 0.993 0.262 0.789 0.417 0.993 0.392 0.951 0.368 0.986 0.529 0.999 0.584 0.539 0.549 0.022 0.483 0.196 0.147 0.552 AtomStable Bond length Bond angle Dihedral angle 0.861 0.831 0.845 0.750 0.846 0.848 1.56E-04 4.29E-01 8.49E-02 1.17E-01 1.23E-01 2.05E-02 1.81E-04 4.96E-01 1.15E-02 9.57E-02 5.29E-02 8.18E-03 1.56E-04 1.46E-02 6.68E-04 4.46E-03 2.17E-03 2.31E-04 (b) Performances on the QM9-2014 dataset. 2D-Metric FCD AtomStable MolStable V&C V&U V&U&N SNN Frag Scaf 0.063 Train 0.461 MolGPT* 0.085 MolGen* 0.798 CDGS 0.138 JODO 0.187 MiDi* EQGAT-diff* 2.157 NExT-Mol, ours 0.070 0.999 0.982 1.000 0.997 0.999 0.998 1.000 1.000 0.988 0.976 0.988 0.951 0.988 0.976 0.972 0.989 0.989 0.989 0.977 0.937 1.000 0.955 0.951 0.936 0.990 0.960 0.980 0.954 1.000 0.996 1.000 0. 0.000 0.763 0.479 0.860* 0.780* 0.769 0.695 0.802 0.490 0.992 0.523 0.958 0.500 0.988 0.493 0.973 0.522 0.986 0.501 0.979 0.479 0.949 0.530 0.992 0.946 0.923 0.934 0.784 0.934 0.882 0.707 0.945 3D-Metric FCD AtomStable Bond length Bond angle Dihedral angle 0.877 Train 2.386 G-SchNet 6.659 G-SphereNet 1.285 EDM 4.861 MDM 0.885 JODO 1.100 MiDi* 1.519 EQGAT-diff* NExT-Mol, ours 0.879 0.994 0.957 0.672 0.986 0.992 0.992 0.983 0.988 0.993 5.44E-04 3.62E-01 1.51E-01 1.30E-01 2.74E-01 1.48E-01 8.96E-01 4.09E-01 1.15E-01 4.65E-04 7.27E-02 3.54E-01 1.82E-02 6.60E-02 1.21E-02 2.08E-02 1.91E-02 7.32E1.78E-04 4.20E-03 1.29E-02 6.64E-04 2.39E-02 6.29E-04 8.14E-04 1.14E-03 1.95E-04 Table 3: Performance of conditional 3D molecule generation on the QM9-2014 dataset. We report MAE between the desired properties and the predicted properties of the generated samples. Baseline results are from (Huang et al., 2024). We bold the best performance. µ (D) α (Bohr3) Cv mol K(cid:1) εHOMO (meV) εLUMO (meV) ε (meV) (cid:0) cal Method L-Bound EDM EEGSDE GeoLDM JODO 0.043 1.123 0.777 1.108 0.628 0.09 2.78 2.50 2.37 1.42 NExT-Mol, ours relative improv. 0.507 19.3% 1.16 18.3% 0.040 1.065 0.941 1.025 0. 0.512 11.9% 39 371 302 340 226 205 9.3% 36 601 447 522 256 235 8.2% 65 671 487 587 297 11.3% lar graphs (i.e., 2D-Metric), and the predicted 3D coordinates (i.e., 3D-Metric), following (Hoogeboom et al., 2022; Huang et al., 2024). 2D-Metrics can be roughly grouped into three types: (1) stability and validity: atom stability, molecule stability, and validity & completeness (V&C); (2) 7 Published as conference paper at ICLR 2025 Table 4: 3D conformer prediction results. Baseline results are from (Jing et al., 2022; Corso et al., 2024; Wang et al., 2024). * denotes reproduction using their codes. -RRecall and -PPrecision. (a) Performances on the GEOM-DRUGS dataset. TD w/ PG denotes torsional diffusion with particle guidance. Method Model Size Mean Median Mean Median Mean Median Mean Median COV-R (%) AMR-R COV-P (%) AMR-P Model size 100M OMEGA GeoMol GeoDiff Torsional Diffusion TD w/ PG TD w/ PG* MCF-S MCF-B DMT-B, ours DMT-B, PC samp. Model size > 100M MCF-L DMT-L, ours - 0.3M 1.6M 1.6M 1.6M 1.6M 13M 64M 55M 55M 242M 150M 53.4 44.6 42.1 72.7 77.0 73.8 79.4 84.0 85.4 85.5 84.7 85. 54.6 41.4 37.8 80.0 82.6 79.3 87.5 91.5 92.2 91.2 92.2 92.3 0.841 0.875 0.835 0.582 0.543 0.566 0.512 0.427 0.401 0.396 0.762 0.834 0.809 0.565 0.520 0.539 0.492 0.402 0.375 0.370 0.390 0.375 0.247 0. 40.5 43.0 24.9 55.2 68.9 65.2 57.4 64.0 65.2 67.6 66.8 67.9 33.3 36.4 14.5 56.9 78.1 70.8 57.6 66.2 67.8 71.5 71.3 72.5 0.946 0.928 1.136 0.778 0.656 0.680 0.761 0.667 0.642 0.623 0.854 0.841 1.090 0.729 0.594 0.615 0.715 0.605 0.577 0. 0.618 0.598 0.530 0.527 (b) Performances on the GEOM-QM9 dataset. COV-R (%) AMR-R COV-P (%) AMR-P Method Model size Mean Median Mean Median Mean Median Mean Median OMEGA GeoMol GeoDiff Torsoinal Diffusion MCF-B DMT-B, ours - 0.3M 1.6M 1.6M 64M 55M 85.5 91.5 76.5 92.8 95.0 95. 100.0 100.0 100.0 100.0 100.0 100.0 0.177 0.225 0.297 0.178 0.103 0.090 0.126 0.193 0.229 0.147 0.044 0.036 82.9 86.7 50.0 92.7 93.7 93.8 100.0 100.0 33.5 100.0 100.0 100.0 0.224 0.270 0.524 0.221 0.119 0. 0.186 0.241 0.510 0.195 0.055 0.049 diversity: validity & uniqueness (V&U), and validity & uniqueness & novelty (V&U&N); and (3) distribution similarity between the generated molecules and the test set: similarity to nearest neighbor (SNN), fragment similarity (Frag), scaffold similarity (Scaf), and Frechet ChemNet Distance (FCD) (Polykovskiy et al., 2020). For 3D-Metrics, we follow (Hoogeboom et al., 2022) to evaluate the predicted 3D molecules by assessing atom stability, and FCD of the 2D molecular graphs reconstructed from predicted 3D coordinates. Additionally, 3D-Metrics includes the maximum mean discrepancy (MMD) (Gretton et al., 2012) for bond lengths, bond angles, and dihedral angles to evaluate geometric similarity to the test set. Training set performance is also reported for reference. The experimental results are presented in Table 2. We can observe that: Obs. 1: NExT-Mol Demonstrates Leading Performances for 3D Molecule Generation. It achieves the best performance across all metrics on GEOM-DRUGS, and achieves the best performance in 13 out of 14 metrics on QM9-2014. Although CDGS shows higher novelty score on QM9-2014, it significantly underperforms NExT-Mol for other metrics. This observation shows that NExT-Mol is highly effective at generating chemically valid and diverse 3D molecular structures. Its strong performance on both large (i.e., GEOM-DRUGS) and small (i.e., QM9-2014) molecules highlights its robustness and potential as foundation model for various tasks. Obs. 2: NExT-Mol is Powerful in Capturing 1D/2D Molecular Characteristics, including SNN, Frag, Scaf, and FCD. Notably, it improves the FCD from 0.655 to 0.334 on GEOM-DRUGS, acheving 49% relative improvement. This good performance is attributed to MoLlamas extensive pretraining, which lays strong foundation for the subsequent 3D conformer prediction. 4.3 CONDITIONAL 3D MOLECULE GENERATION WITH QUANTUM CHEMICAL PROPERTIES Adatping NExT-Mol for Conditional Generation. We employ NExT-Mol for conditional 3D molecule generation targeting quantum chemistry properties. To adapt NExT-Mol to incorporate numerical conditions, the desired property values are encoded into vector embeddings using MLPs. These embeddings are prepended to the SELFIES sequences during MoLlama fine-tuning, serving 8 Published as conference paper at ICLR 2025 Table 5: Incorporating MoLlamas 1D representations to improve DMTs 3D conformer prediction. COV-R (%) AMR-R COV-P (%) AMR-P Dataset Method Mean Median Mean Median Mean Median Mean Median GEOMDRUGS DMT-B +MoLlama DMT-L +MoLLama 85.4 86.1 85.8 87.1 92.2 92.1 92.3 93.0 0.401 0. 0.375 0.360 0.375 0.367 0.346 0.334 65.2 66.2 67.9 68.1 67.8 68. 72.5 71.8 0.642 0.626 0.598 0.595 0.577 0.566 0.527 0.525 Table 6: 3D conformer prediction performance on GEOM-DRUGSs test subsets, split by scaffold frequency in the training set. 68 low-quality samples are filtered following (Jing et al., 2022). Test subset #Mol Method AMR-R AMR-P unseen scaffold 348 scaf. freq. 1 scaf. freq. 10 285 DMT-B +MoLlama DMT-B +MoLlama DMT-B +MoLlama 0.450 0. 0.364 0.359 0.348 0.347 0.785 0.755 0.549 0.548 0.515 0.513 (a) Case 1. to R: GT, DMT, DMT+MoLlama. (b) Case 2. to R: GT, DMT, DMT+MoLlama. Figure 4: Visualization of 3D conformers. We select the predicted conformers with the least RMSD to the ground truth (GT). as soft-prompt to condition its output (Li & Liang, 2021), and are also fed into the DMT through the condition MLP module (cf. Figure 2). See Appendix D.4 for details of this methodology. Remark. Quantum chemical properties (e.g., HOMO-LUMO gap) often vary across molecules different 3D conformers. As result, the 1D molecules generated by MoLlama alone cannot achieve errors lower than the average across molecules different conformers. To address this, we condition DMT on the desired property value when predicting the 3D conformer, enabling DMT to find the conformer that best matches the target property. Experimental Settings. Following (Hoogeboom et al., 2022; Huang et al., 2024), we focus on six properties of heat capacity Cv, dipole moment µ, polarizability α, highest occupied molecular orbital energy ϵHOMO, lowest unoccupied molecular orbital energy ϵLUMO, and HOMO-LUMO gap ϵ. For evaluation, we report the mean absolute error (MAE) between the desired property values and the predicted values of the generated molecules, using property classifier network ϕc (Hoogeboom et al., 2022). QM9-2014s training set is split into two halves: Da and Db, each containing 50k molecules. ϕc is trained on Da and NExT-Mol is trained on Db. We report ϕcs performance on Db as the performances lower-bound (L-Bound). Table 3 shows the results. Obs. 3: NExT-Mol Outperforms Baselines for Conditional 3D Molecule Generation. NExTMols improvements are consistent and significant, with an average relative gain of 13% on MAE, demonstrating its ability to effectively capture quantum chemical properties. This good performance is partially attributed to DMT, which finds the 3D conformer that best aligns the desired property. 4.4 3D MOLECULAR CONFORMER PREDICTION Experimental Setting. Our setting follows (Jing et al., 2022). Evaluation metrics include Average Minimum RMSD (AMR), which measures the distance between predicted conformer and ground truth, and Coverage (COV), which measures the proportion of predicted conformers that are sufficiently close to ground truth. Due to 2D molecule can have multiple ground truth and predicted conformers, we report both precision (comparing prediction to its most similar ground truth) and recall (comparing ground truth to its most similar prediction) for AMR and Coverage. For DMT-B, we report performance with the predictor-corrector sampler (PC samp.; Song et al. (2021)), comparing with the particle guidance sampler (TD w/ PG). Obs. 4: DMT Demonstrates Leading Performance for 3D Conformer Prediction. Table 4 compares DMT and baselines for 3D conformer prediction. We can observe that DMT-B outperforms MCF-B, and DMT-L surpasses MCF-L, even though DMT-L is only 60% of the size of MCF-L. This improvement demonstrates that DMT can better utilize 2D molecular graph structures than MCF. Further, DMT-L improves upon DMT-B, demonstrating DMTs scalability. Both the improvements 9 Published as conference paper at ICLR Table 7: Enhancing 3D molecule generation with MoLlama representations on GEOM-DRUGS. Method 3D Pred. FCD AtomStable Bond length Bond angle Dihedral angle NExT-Mol DMT-B +MoLLama 14.69 14.32 0.848 0.852 2.05E-02 1.48E-02 8.18E-03 8.08E-03 2.31E-04 1.81E-04 Table 8: Ablating randomized SELFIES augmentations for 1D molecule generation on QM9-2014. 2D metrics FCD AtomStable MolStable V&C V&U V&U&N SNN Frag Scaf 0.070 MoLlama w/o randomized aug. 0.074 1.000 1.000 0.989 0. 1.000 0.967 1.000 0.948 0.802 0.395 0.530 0.992 0.945 0.491 0.989 0.939 above are attributed to DMTs meticulously designed architecture, combining the power of scalability while effectively leveraging the full information of 2D molecular graphs. Obs. 5: MoLlamas 1D Representation Improves DMTs 3D Conformer Prediction. As Table 5 shows, MoLlama enhances DMT on GEOM-DRUGS. Table 12 shows integrating MoLlama into DMT also improves performance on GEOM-QM9. This observation demonstrates the potential to leverage the abundant 1D molecule sequences to improve 3D generation and design tasks, mitigating their data scarcity issue. Further, this observation highlights MoLlamas value to generate expressive molecule representations for 3D tasks, beyond its 1D molecule generation ability. Although MoLlama is pretrained only on 1D molecules, we hypothesize that large-scale pretraining helps it develop chemical heuristics useful for 3D prediction. 4.5 ANALYSIS AND ABLATION STUDIES MoLlamas 1D Representation Improves 3D Prediction for Unseen Scaffolds. Scaffold split is widely used to evaluate molecular models generalization ability to unseen structures (Hu et al., 2020). We divide GEOM-DRUGSs test set into subsets based on the test molecules scaffold frequency in the training set. As Table 6 shows, DMT-Bs performance drops significantly for molecules with unseen scaffolds: AMR-R and AMR-P increase by 0.086 and 0.236, respectively, compared to molecules with scaffold frequency 1. However, incorporating MoLlama mitigates this issue, reducing AMR-R and AMR-P by 0.028 and 0.030, respectively. This improvement stems from MoLlamas exposure to diverse scaffolds during pretraining on large molecular dataset, enabling better generalization for transfer learning. Figure 4 highlights cases where MoLlama significantly enhances conformer prediction, particularly by improving torsion angle predictions. Enhancing 3D Molecule Generation with MoLlama Representations. NExT-Mol uses DMT-B without MoLlama for conformer prediction by default for de novo 3D molecule generation. Here we show that enhancing DMT-B with MoLlama further improves its performance on 3D-metrics. Table 7 shows significant gains in geometric measures (i.e., bond lengths, angles, and dihedral angles), highlighting MoLlamas ability to enhance DMTs 3D geometry prediction. Random SELFIES Augmentation. As Table 8 shows, using randomized SELFIES augmentation significantly improves the novelty (i.e., V&U&N) of the generated samples. It also improves other metrics, like SNN and FCD, highlighting its importance for 1D molecule generation."
        },
        {
            "title": "5 CONCLUSION AND FUTURE WORKS",
            "content": "In this work, we presented NExT-Mol, foundation model for 3D molecule generation that integrated the strengths of 1D SELFIES-based LMs and 3D diffusion models. NExT-Mol demonstrated leading performances in de novo 3D molecule generation, 3D conformer prediction, and conditional 3D molecule generation. These good performances are attributed to our focus on incorporating chemical inductive biases without compromising model scalability, and they highlight NExT-Mols promising potential as foundation model in the field. Additionally, NExT-Mol showed that transfer learning between 1D molecule sequences and 3D conformers can significantly improve 3D conformer prediction performance, underscoring the value of leveraging the abundant 1D molecular data to enhance 3D prediction tasks. Looking ahead, we plan to extend NExT-Mol to process multiple molecular inputs, aiming to tackle structure-based molecule design and modeling interactions between small molecules and proteins or RNAs, with real-world applications in drug discovery. 10 Published as conference paper at ICLR"
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "Our research advances 3D molecule generation with the NExT-Mol model, aiming to enhance generative deep learning methods for molecular design. This work is primarily technical and foundational, with applications in drug discovery and materials science. We have carefully considered potential societal impacts and do not foresee any direct, immediate, or negative consequences. We are committed to the ethical dissemination of our findings and encourage their responsible use."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "All the results in this work are reproducible. We provide all the necessary code to replicate our results in an anonymous GitHub repository https://anonymous.4open.science/r/NExT-Mol. The repository includes environment configurations, run scripts, and other relevant materials. We discuss the experimental settings for various tasks in Section 4, including details on parameters such as sampling steps. Additionally, detailed experimental settings are provided in Appendix D."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "This research is supported by the National Natural Science Foundation of China (92270114). This material is based upon work supported by the Air Force Office of Scientific Research under award number FA2386-24-1-4011, and this research is partially supported by the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207). This research is supported by NExT Research Center."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Abramson, Jonas Adler, Jack Dunger, Richard Evans, Tim Green, Alexander Pritzel, Olaf Ronneberger, Lindsay Willmore, Andrew Ballard, Joshua Bambrick, et al. Accurate structure prediction of biomolecular interactions with alphafold 3. Nature, pp. 13, 2024. Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta-2: Towards chemical foundation models. CoRR, abs/2209.01712, 2022. doi: 10. 48550/ARXIV.2209.01712. URL https://doi.org/10.48550/arXiv.2209.01712. Josep Arus-Pous, Simon Johansson, Oleksii Prykhodko, Esben Jannik Bjerrum, Christian Tyrchan, Jean-Louis Reymond, Hongming Chen, and Ola Engkvist. Randomized SMILES strings improve the quality of molecular generative models. J. Cheminformatics, 11(1):71:171:13, 2019. Simon Axelrod and Rafael Gomez-Bombarelli. Geom, energy-annotated molecular conformations for property prediction and molecular generation. Scientific Data, 9(1):185, 2022. Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Viraj Bagal, Rishal Aggarwal, PK Vinod, and Deva Priyakumar. Molgpt: molecular generation using transformer-decoder model. Journal of Chemical Information and Modeling, 62(9):2064 2076, 2021. Fan Bao, Min Zhao, Zhongkai Hao, Peiyao Li, Chongxuan Li, and Jun Zhu. Equivariant energyguided SDE for inverse molecular design. In ICLR. OpenReview.net, 2023. Jannis Born and Matteo Manica. Regression transformer enables concurrent sequence regression and generation for molecular language modelling. Nat. Mac. Intell., 5(4):432444, 2023. Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id= F72ximsx7C1. 11 Published as conference paper at ICLR 2025 Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, et al. Sciassess: Benchmarking llm proficiency in scientific literature analysis. arXiv preprint arXiv:2403.01976, 2024. Gabriele Corso, Yilun Xu, Valentin De Bortoli, Regina Barzilay, and Tommi S. Jaakkola. Particle guidance: non-i.i.d. diverse sampling with diffusion models. In ICLR. OpenReview.net, 2024. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. URL https:// openreview.net/forum?id=mZn2Xyh9Ec. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pp. 41714186. Association for Computational Linguistics, 2019. Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, pp. 87808794, 2021. Yuanqi Du, Tianfan Fu, Jimeng Sun, and Shengchao Liu. Molgensurvey: systematic survey in machine learning models for molecule design. arXiv preprint arXiv:2203.14500, 2022. Peter Eckmann, Kunyang Sun, Bo Zhao, Mudong Feng, Michael K. Gilson, and Rose Yu. LIMO: latent inceptionism for targeted molecule generation. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 57775792. PMLR, 2022. Junfeng Fang, Shuai Zhang, Chang Wu, Zhengyi Yang, Zhiyuan Liu, Sihang Li, Kun Wang, Wenjie Du, and Xiang Wang. Moltc: Towards molecular relational modeling in language models. In ACL (Findings), pp. 19431958. Association for Computational Linguistics, 2024a. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Jie Shi, Xiang Wang, Xiangnan He, and Tat-Seng Chua. Alphaedit: Null-space constrained model editing for language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=HvSytvg3Jh. Yin Fang, Ningyu Zhang, Zhuo Chen, Lingbing Guo, Xiaohui Fan, and Huajun Chen. Domainagnostic molecular generation with self-feedback. In ICLR. OpenReview.net, 2024b. Daniel Flam-Shepherd, Kevin Zhu, and Alan Aspuru-Guzik. Language models can learn complex molecular distributions. Nature Communications, 13(1):3293, 2022. Gordon Flynn. Substituent constants for correlation analysis in chemistry and biology. by corwin hansch and albert leo., 1980. Octavian Ganea, Lagnajit Pattanaik, Connor W. Coley, Regina Barzilay, Klavs F. Jensen, William H. Green Jr., and Tommi S. Jaakkola. Geomol: Torsional geometric generation of molecular 3d conformer ensembles. In NeurIPS, pp. 1375713769, 2021. Niklas Gebauer, Michael Gastegger, and Kristof Schutt. Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules. Advances in neural information processing systems, 32, 2019. Niklas WA Gebauer, Michael Gastegger, Stefaan SP Hessmann, Klaus-Robert Muller, and Kristof Schutt. Inverse design of 3d molecular structures with conditional generative neural networks. Nature communications, 13(1):973, 2022. Rafael Gomez-Bombarelli, Jennifer Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamın Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy Hirzel, Ryan Adams, and Alan Aspuru-Guzik. Automatic chemical design using data-driven continuous representation of molecules. ACS central science, 4(2):268276, 2018. Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Scholkopf, and Alexander Smola. kernel two-sample test. The Journal of Machine Learning Research, 13(1):723773, 2012. 12 Published as conference paper at ICLR 2025 Tomas Hansson, Chris Oostenbrink, and WilfredF van Gunsteren. Molecular dynamics simulations. Current opinion in structural biology, 12(2):190196, 2002. Paul C. D. Hawkins. Conformation generation: The state of the art. J. Chem. Inf. Model., 57(8): 17471756, 2017. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In ICML, volume 162 of Proceedings of Machine Learning Research, pp. 88678887. PMLR, 2022. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In KDD, pp. 594604. ACM, 2022. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. CoRR, abs/2106.09685, 2021. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In ICLR, 2020. Chenqing Hua, Sitao Luan, Minkai Xu, Zhitao Ying, Jie Fu, Stefano Ermon, and Doina Precup. Mudiff: Unified diffusion for complete molecule generation. In LoG, volume 231 of Proceedings of Machine Learning Research, pp. 33. PMLR, 2023. Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Conditional diffusion based on discrete graph structures for molecular graph generation. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 43024311, 2023a. Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Learning joint 2-d and 3-d graph diffusion models for complete molecule generation. IEEE Transactions on Neural Networks and Learning Systems, 2024. Lei Huang, Hengtong Zhang, Tingyang Xu, and Ka-Chun Wong. MDM: molecular diffusion model for 3d molecule generation. In AAAI, pp. 51055112. AAAI Press, 2023b. Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: pretrained transformer for computational chemistry. Mach. Learn. Sci. Technol., 3(1):15022, 2022. Wengong Jin, Regina Barzilay, and Tommi S. Jaakkola. Junction tree variational autoencoder for molecular graph generation. In ICML, volume 80 of Proceedings of Machine Learning Research, pp. 23282337. PMLR, 2018. Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi S. Jaakkola. Learning multimodal graphto-graph translation for molecule optimization. In ICLR (Poster). OpenReview.net, 2019. Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi S. Jaakkola. Torsional diffusion for molecular conformer generation. In NeurIPS, 2022. Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances in neural information processing systems, 34:2169621707, 2021. Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017. Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Selfreferencing embedded strings (SELFIES): 100% robust molecular string representation. Mach. Learn. Sci. Technol., 1(4):45024, 2020. Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Finetuning can distort pretrained features and underperform out-of-distribution. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum? id=UYneFzXSJWh. 13 Published as conference paper at ICLR 2025 Tuan Le, Frank Noe, and Djork-Arne Clevert. Representation learning on biomolecular structures In LoG, volume 198 of Proceedings of Machine Learning using equivariant graph attention. Research, pp. 30. PMLR, 2022. Tuan Le, Julian Cremer, Frank Noe, Djork-Arne Clevert, and Kristof T. Schutt. Navigating the design space of equivariant diffusion-based generative models for de novo 3d molecule generation. In ICLR. OpenReview.net, 2024. Sihang Li, Xiang Wang, An Zhang, Yingxin Wu, Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning. In ICML, pp. 1305213065, 2022. Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji Kawaguchi, Tat-Seng Chua, and Qi Tian. 3d-molm: Towards 3d molecule-text interpretation in language models. In ICLR, 2024. URL https://openreview.net/forum?id=xI4yNlkaqh. Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP (1), pp. 45824597. Association for Computational Linguistics, 2021. Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024a. Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph variational autoencoders for molecule design. Advances in neural information processing systems, 31, 2018. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pretraining molecular graph representation with 3d geometry. In ICLR. OpenReview.net, 2022. Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Molca: Molecular graph-language modeling with cross-modal projector and In EMNLP, 2023a. URL https://openreview.net/forum?id= uni-modal adapter. 14WRhMNq7H. Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Rethinking tokenizer and decoder in masked graph modeling for molecules. In NeurIPS, 2023b. URL https://openreview.net/forum?id=fWLf8DV0fI. Zhiyuan Liu, Yaorui Shi, An Zhang, Sihang Li, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. Reactxt: Understanding molecular reaction-ship via reaction-contextualized molecule-text pretraining. In Findings of the Association for Computational Linguistics: ACL 2024. Association for Computational Linguistics, 2024b. URL https://openreview.net/ forum?id=V-ejDfLiwe. Zhiyuan Liu, An Zhang, Hao Fei, Enzhi Zhang, Xiang Wang, Kenji Kawaguchi, and Tat-Seng Chua. Prott3: Protein-to-text generation for text-based protein understanding. In ACL. Association for Computational Linguistics, 2024c. URL https://openreview.net/forum?id= ZmIjOPil2b. Chengqiang Lu, Qi Liu, Chao Wang, Zhenya Huang, Peize Lin, and Lixin He. Molecular propIn The Thirty-Third erty prediction: multilevel quantum interactions modeling perspective. AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pp. 10521060. AAAI Press, 2019. doi: 10.1609/AAAI.V33I01.33011052. URL https://doi.org/10.1609/aaai.v33i01.33011052. Yanchen Luo, Sihang Li, Zhiyuan Liu, Jiancan Wu, Zhengyi Yang, Xiangnan He, Xiang Wang, and Qi Tian. Text-guided diffusion model for 3d molecule generation. 2023. 14 Published as conference paper at ICLR 2025 Youzhi Luo and Shuiwang Ji. An autoregressive flow model for 3d molecular geometry generation from scratch. In International conference on learning representations (ICLR), 2022. Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: discrete flow model for molecular graph generation. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 71927203. PMLR, 2021. Sohir Maskey, Ali Parviz, Maximilian Thiessen, Hannes Stark, Ylli Sadikaj, and Haggai Maron. Generalized laplacian positional encoding for graph representation learning. arXiv preprint arXiv:2210.15956, 2022. Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. OpenEye, Cadence Molecular Sciences. OMEGA 5.0.1.3. OpenEye, Cadence Molecular Sciences, Santa Fe, NM. http://www.eyesopen.com. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual reasoning with general conditioning layer. In AAAI, pp. 39423951. AAAI Press, 2018. Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, Artur Kadurin, Simon Johansson, Hongming Chen, Sergey Nikolenko, Alan AspuruGuzik, and Alex Zhavoronkov. Molecular Sets (MOSES): Benchmarking Platform for Molecular Generation Models. Frontiers in Pharmacology, 2020. Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. Science advances, 4(7):eaap7885, 2018. Mariya Popova, Mykhailo Shvets, Junier Oliva, and Olexandr Isayev. Molecularrnn: Generating realistic molecular graphs with optimized properties. arXiv preprint arXiv:1905.13372, 2019. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1140:67, 2020. Raghunathan Ramakrishnan, Pavlo Dral, Matthias Rupp, and Anatole Von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific data, 1(1):17, 2014. Zachary Rollins, Alan Cheng, and Essam Metwally. Molprop: Molecular property prediction with multimodal language and graph fusion. Journal of Cheminformatics, 16(1):56, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Vıctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. (n) equivariant graph neural networks. In International conference on machine learning, pp. 93239332. PMLR, 2021. Kristof Schutt, Huziel Sauceda, P-J Kindermans, Alexandre Tkatchenko, and K-R Muller. Schneta deep learning architecture for molecules and materials. The Journal of Chemical Physics, 148(24), 2018. Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: flow-based autoregressive model for molecular graph generation. In ICLR. OpenReview.net, 2020. Yaorui Shi, An Zhang, Enzhi Zhang, Zhiyuan Liu, and Xiang Wang. ReLM: Leveraging language models for enhanced chemical reaction prediction. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id= gJZqSRfV21. Published as conference paper at ICLR 2025 Yaorui Shi, Sihang Li, Taiyan Zhang, Xi Fang, Jiankun Wang, Zhiyuan Liu, Guojiang Zhao, Zhengdan Zhu, Zhifeng Gao, Renxin Zhong, et al. Intelligent system for automated molecular patent infringement assessment. arXiv preprint arXiv:2412.07819, 2024. Gregor Simm, Robert Pinsler, and Jose Miguel Hernandez-Lobato. Reinforcement learning for molecular design guided by quantum mechanics. In International Conference on Machine Learning, pp. 89598969. PMLR, 2020. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR. OpenReview.net, 2021. Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 2047920502. PMLR, 2022. URL https://proceedings.mlr.press/v162/stark22a.html. Teague Sterling and John J. Irwin. ZINC 15 - ligand discovery for everyone. J. Chem. Inf. Model., 55(11):23242337, 2015. Mengying Sun, Jing Xing, Han Meng, Huijun Wang, Bin Chen, and Jiayu Zhou. Molsearch: Searchbased multi-objective molecular generation and property optimization. In KDD, pp. 47244732. ACM, 2022. Nathaniel Thomas, Tess E. Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick Riley. Tensor field networks: Rotationand translation-equivariant neural networks for 3d point clouds. CoRR, abs/1802.08219, 2018. Benjamin Tingle, Khanh Tang, Mar Castanon, John Gutierrez, Munkhzul Khurelbaatar, Chinzorig Dandarchuluun, Yurii Moroz, and John Irwin. Zinc-22 free multi-billion-scale database of tangible compounds for ligand discovery. Journal of chemical information and modeling, 63 (4):11661176, 2023. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. CoRR, abs/2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 59986008, 2017. Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pascal Frossard. Digress: Discrete denoising diffusion for graph generation. In ICLR. OpenReview.net, 2023a. Clement Vignac, Nagham Osman, Laura Toni, and Pascal Frossard. Midi: Mixed graph and 3d In ECML/PKDD (2), volume 14170 of Lecture denoising diffusion for molecule generation. Notes in Computer Science, pp. 560576. Springer, 2023b. Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nat. Mach. Intell., 4(3):279 doi: 10.1038/S42256-022-00447-X. URL https://doi.org/10.1038/ 287, 2022. s42256-022-00447-x. 16 Published as conference paper at ICLR Yuyang Wang, Ahmed A. A. Elhag, Navdeep Jaitly, Joshua M. Susskind, and Miguel Angel Bautista. Swallowing the bitter pill: Simplified scalable conformer generation. In ICML. OpenReview.net, 2024. David Weininger. Smiles, chemical language and information system. 1. introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci., 28(1):3136, 1988. Ruidong Wu, Fan Ding, Rui Wang, Rui Shen, Xiwen Zhang, Shitong Luo, Chenpeng Su, Zuofan Wu, Qi Xie, Bonnie Berger, et al. High-resolution de novo structure prediction from primary sequence. BioRxiv, pp. 202207, 2022. Zhenqin Wu, Bharath Ramsundar, Evan Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh Pappu, Karl Leswing, and Vijay Pande. Moleculenet: benchmark for molecular machine learning. Chemical science, 9(2):513530, 2018. Yutong Xie, Chence Shi, Hao Zhou, Yuwei Yang, Weinan Zhang, Yong Yu, and Lei Li. MARS: markov molecular sampling for multi-objective drug discovery. In ICLR. OpenReview.net, 2021. Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2019. Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian Tang. Geodiff: geometric diffusion model for molecular conformation generation. In ICLR. OpenReview.net, 2022. Minkai Xu, Alexander S. Powers, Ron O. Dror, Stefano Ermon, and Jure Leskovec. Geometric latent diffusion models for 3d molecule generation. In ICML, volume 202 of Proceedings of Machine Learning Research, pp. 3859238610. PMLR, 2023. Kevin Yang, Kyle Swanson, Wengong Jin, Connor W. Coley, Philipp Eiden, Hua Gao, Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, Andrew Palmer, Volker Settels, Tommi S. Jaakkola, Klavs F. Jensen, and Regina Barzilay. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model., 59(8):33703388, 2019. doi: 10.1021/ ACS.JCIM.9B00237. URL https://doi.org/10.1021/acs.jcim.9b00237. Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. Advances in neural information processing systems, 31, 2018. Atakan Yuksel, Erva Ulusoy, Atabey Unlu, Gamze Deniz, and Tunca Dogan. Selformer: Molecular representation learning via SELFIES language models. CoRR, abs/2304.04662, 2023. Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model. CoRR, abs/2401.02385, 2024. Zaixi Zhang, Yaosen Min, Shuxin Zheng, and Qi Liu. Molecule generation for target protein binding with structural motifs. In ICLR. OpenReview.net, 2023. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Xiaofan Zheng and Yoichi Tomiura. bert-based pretraining model for extracting molecular structural information from smiles sequence. Journal of Cheminformatics, 16(1):71, 2024. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: universal 3d molecular representation learning framework. In ICLR. OpenReview.net, 2023. 17 Published as conference paper at ICLR"
        },
        {
            "title": "A LIMITATIONS AND MORE FUTURE WORKS",
            "content": "NExT-Mol has several limitations that have not been addressed due to our limited computational resources and other technical challenges. We outline these limitations below: Explore Generalization of 3D Conformer Prediction to Unseen Scaffolds. Table 6 shows that DMT-Bs performance drop significantly on test molecules with unseen scaffolds in the training set. While our proposed transfer learning using MoLlamas pretrained 1D representations can mitigate this issue, there is still room for improvement. Future work could explore advanced generalization techniques and the integration of chemical inductive biases to enhance performance on unseen scaffolds. Additionally, developing more comprehensive evaluation benchmark with stricter scaffold split would provide deeper insights into model generalization. We leave these for future research. Explore Randomized SELFIES Data Augmentation in Pretraining. Although randomized SELFIES augmentation shows promising results when fine-tuning MoLlama for 1D molecule generation, we do not use this augmentation technique during pretraining due to our limited computational resources. We believe applying this technique in pretraining could lead to different outcomes. We leave this exploration for future work. Explore Pretrained Molecular Large LM with Bi-directional Self-Attention. MoLlama uses causal self-attention, where each token can only attend to previous tokens. While this approach is good fit for 1D molecule generation, it constrains MoLlamas potential for molecule representation learning. To mitigate this issue, we have attached bi-directional self-attention layer after MoLlama (cf. Figure 3). However, more natural solution would be to use molecular LM with built-in bidirectional self-attention. Due to resource constraints, we do not pursue this, and existing works are often limited in scale (Irwin et al., 2022; Zheng & Tomiura, 2024). We hope this work draws more attention to this area and encourages the development of more foundation models for biochemistry. Explore NExT-Mol for Struture-based Molecule Generation. We do not explore NExT-Mol for structure-based molecule generation (Zhang et al., 2023) due to the limited scope of this work. However, NExT-Mol could be extended for this task by conditioning the generation process on the structural embeddings of target pockets, potentially using techniques like cross-attention, adaptive layer normalization, or soft-prompting (Li & Liang, 2021). We leave this for future works. Limited Exploration on Diffusion Guidance. Our DMT model utilizes i.i.d. sampling, without exploring advanced sampling method like classifier guidance (Dhariwal & Nichol, 2021) and particle guidance (Corso et al., 2024). However, particle guidance demonstrates that well-tuned guidance method can improve the conformer prediction by 10% precision. This is because the 3D molecular conformational space is large, and guidance method with appropriate chemical inductive bias can improve the sampling efficiency. We leave this exploration as future work. Computational Cost when Incorporating MoLlama for 3D Conformer Prediction. Incorporating MoLlama, large LM with 960M parameters, increases training time. For example, training DMT-B alone (55M parameters) takes 52 seconds per epoch on an A100 GPU, while DMT-B with MoLlama takes 210 seconds. We mitigated this problem by using pretrained DMT-B, instead of training it from scratch, to reduce the training epochs when incorporating MoLlama. Yet, we will need improvement when transferring 1D representations from large LM. Quadratic Memory Complexity of DMTs Pair Representation. This pair representation incurs an additional O(N 2) GPU memory cost than the standard transformer, compared to the standard transformers O(N ) memory complexity when using FlashAttention, where is the node number of molecular graphs. While we encountered no memory issues on the GEOM-DRUGS dataset (molecules with hundreds of nodes), this could be bottleneck for molecules with thousands of nodes. Potential solutions include smaller batch sizes and model parallelism. More Future Works. In future, we plan to build multi-modal foundation models (Liu et al., 2024c; Li et al., 2024) with NExT-Mol as the essential backbone for 3D molecule generation (Luo et al., 2023), in order to support tasks like chemical reaction prediction (Liu et al., 2024b; Shi et al., 2023) and drug-drug interaction prediction (Fang et al., 2024a). This is to support the broad application of LLMs for scientific discovery (Cai et al., 2024; Shi et al., 2024). We will also explore model editing techniques (Fang et al., 2025) to locate the influential parameters and update its knowledge according to the application requirement. 18 Published as conference paper at ICLR"
        },
        {
            "title": "B MORE EXPERIMENTAL RESULTS",
            "content": "B.1 ABLATION STUDY Ablating MoLlama Pretraining. As Table 9 shows, pretraining significantly improves MoLlamas performances on the 1D distribution similarity metrics of SNN, Scaf and FCD, but slightly decreases novelty score (V&U&N). This may be because the model without pretraining prefers more random sampling, increasing the novelty but reducing the similarity to the desired molecule distribution. Pretraining does not significantly influence stability and validity measures, because they are mostly guaranteed by the SELFIES representations. Table 9: Ablation study for the MoLlama pretraining for 1D molecule generation on the GEOMDRUGS dataset. Method FCD AtomStable MolStable V&C V&U V&U&N SNN Frag Scaf 0.334 MoLlama w/o pretraining 0.586 1.000 1.000 0.999 0.995 1.000 0.999 1.000 0.999 0.945 0.974 0.529 0.999 0.552 0.495 0.999 0. Table 10: Ablating random rotation augmentation for 3D conformer prediction on GEOM-QM9. COV-R (%) AMR-R COV-P (%) AMR-P Method Mean Median Mean Median Mean Median Mean Median DMT-B w/o rand rot aug. 95.2 95.2 100.0 100.0 0.090 0.095 0.036 0. 93.8 93.3 100.0 100.0 0.108 0.113 0.049 0.053 Random Rotation Augmentation. Table 10 shows that DMT benefits from random rotation augmentations. Unlike MCF (Wang et al., 2024), which relies on fixed canonical rotations, this is key improvement because real data may be out-of-distribution and do not follow canonical rotations. B.2 MOLECULE PROPERTY PREDICTION RESULTS FOR MOLLAMA Experimental Settings. To evaluate MoLlamas capabilities beyond 1D molecule generation, we apply it to molecular property prediction tasks (Liu et al., 2023b; Li et al., 2022), highlighting the quality of its molecular representations. Following the setup in (Rollins et al., 2024), we fine-tune MoLlama on four MoleculeNet (Wu et al., 2018) datasets: FreeSolv, ESOL, Lipo, and QM7. We adopt the same experimental settings and dataset splits as (Rollins et al., 2024), reporting mean performance and standard deviation over 10 random seeds. Following Section 3.3, we attach single-layer bi-directional self-attention layer after MoLlama to improve its encoding ability. After that, we apply linear layer on the mean embedding of all molecule tokens for property prediction. For each run, MoLlama is trained for 100 epochs, with test performance selected based on the validation dataset. We use fixed learning rate of 1e-4 with the AdamW optimizer, and fine-tune MoLlama using LoRA (Hu et al., 2021) (LoRA = 8 and α = 32) applied to all linear layers of the model. Following Section 3.1, we apply the random SELFIES augmentation during training. During inference, we use the average prediction values of 20 differently augmented SELFIES as the final prediction. Observation. As shown in Table 11, MoLlama significantly outperforms baseline methods, achieving relative improvements of 6.5%, 4.7%, 3.5%, and 16.9% on the FreeSolv, ESOL, Lipo, and QM7 datasets, respectively. Notably, our baselines include LM-based, GNN-based, and pretrained GNNbased methods, and MoLlamas better performance demonstrates its advantages derived from the extensive pretraining. B.3 3D MOLECULAR CONFORMER PREDICTION Table 12 presents the results of integrating MoLlamas pretrained 1D representations into DMT-B for 3D conformer prediction, using the same experimental setup as Table 5. The results demonstrate that MoLlamas pretrained representations can enhance DMT-Bs performance. Published as conference paper at ICLR 2025 Table 11: Molecule property regression results on four MoleculeNet datasets (Wu et al., 2018). Baseline results are from (Rollins et al., 2024). Lower is better. Method FreeSolv (RMSE) ESOL (RMSE) Lipo (RMSE) QM7 (MAE) Supervised Learning Methods RF (Wang et al., 2022) SVM (Wang et al., 2022) Supervised GNN-based Methods GCN (Kipf & Welling, 2017) GATv2 (Brody et al., 2022) GIN (Xu et al., 2019) SchNet (Schutt et al., 2018) 3D Infomax (Stark et al., 2022) MGCN (Lu et al., 2019) D-MPNN (Yang et al., 2019) Pretrained GNN-based Methods Pretrain-GNN (Hu et al., 2020) MolCLR (Wang et al., 2022) LM-based Methods ChemBERTa-2 (Ahmad et al., 2022) MolPROP (Rollins et al., 2024) MoLlama, ours 2.030.22 3.140.00 2.870.14 3.140.00 2.760.18 3.220.76 2.230.26 3.350.01 2.180.91 2.830.12 2.200.20 1.070.19 1.500. 0.880.04 0.820.00 122.74.2 156.90.0 1.430.05 1.410.00 1.450.02 1.050.06 0.950.04 1.270.15 0.980.26 0.850.08 0.890.00 0.850.07 0.910.10 0.740.01 1.110.04 0.650.05 122.92.2 113.30.0 124.80.7 74.26.0 - 77.64.7 105.813.2 1.220.02 1.110. 0.740.00 0.650.08 110.26.4 87.22.0 2.0470.00 1.700.09 1.590.04 0.8890.00 0.7770.02 0.7400.01 0.7980.00 0.7330.02 0.6270.01 172.80.00 151.810.0 63.51. Table 12: Incorporating MoLlamas 1D representations to improve DMTs 3D conformer prediction. COV-R (%) AMR-R COV-P (%) AMR-P Dataset GEOMQM9 Method Mean Median Mean Median Mean Median Mean Median DMT-B +MoLlama 95.2 95.6 100.0 100. 0.090 0.083 0.036 0.036 93.8 94.2 100.0 100.0 0.108 0.097 0.049 0. B.4 INFLUENCE OF HYPERPARAMETERS More Results on the Predictor-Corrector Sampler. Table 4 reports DMT-Bs performance with the predictor-corrector sampler with the hyperparameter of snr=0.3. To provide more comprehensive analysis, Table 13 presents results for additional hyperparameter settings. Different Noise Schedules at Inference Time. We test DMT-Bs robustness to different noise schedulers at inference, using two representative options: the linear (Ho et al., 2020) and polynomial (Hoogeboom et al., 2022) schedulers. The original noise scheduler, based on the cosine function, follows (Nichol & Dhariwal, 2021). In this study, we use the existing DMT-B checkpoint without retraining the model with these new schedulers, so the results are suboptimal. Figure 5: Effect of sampling steps on AMR for 3D conformer prediction using DMT-B. Observation. As shown in Table 14, the polynomial scheduler achieves performance close to the cosine scheduler, likely because their curve shapes are similar. However, the linear scheduler results in significant performance drop, suggesting that retraining DMT-B with the linear scheduler is necessary to achieve better results. Sampling Steps. We evaluate 3D conformer prediction performance given different sampling steps. Observation. As shown in Figure 5, we observe an improving trend in AMR for both recall and precision as the sampling steps increase from 5 to 100. The most significant improvements occur 20 Published as conference paper at ICLR 2025 Table 13: Performances of 3D conformer prediction on the GEOM-DRUGS dataset. COV-R (%) AMR-R COV-P (%) AMR-P Model Mean Median Mean Median Mean Median Mean Median DMT-B, PC samp., snr=0.2 85.3 DMT-B, PC samp., snr=0.3 85.5 DMT-B, PC samp., snr=0.4 73.8 91.5 91.2 79. 0.398 0.396 0.535 0.372 0.370 0.501 66.5 67.6 68.0 69.2 71.5 72.1 0.633 0.623 0.621 0.560 0.546 0. Table 14: DMT-Bs 3D conformer prediction performances on the GEOM-DRUGS dataset when using different noise schedulers at inference time. COV-R (%) AMR-R COV-P (%) AMR-P Noise schedule Mean Median Mean Median Mean Median Mean Median linear cosine, original polynomial 62.7 85.4 84.9 62.7 92.2 91.7 0.648 0.401 0.454 0.634 0.375 0.421 60.3 65.2 64. 60.6 67.8 66.2 0.726 0.642 0.685 0.624 0.577 0.619 Figure 6: Comparison of conformer generation time on the test set of the GEOM-Drugs dataset using various methods. between 5 and 20 steps, with diminishing returns beyond 50 steps. This indicates that our model can half the inference cost by trading off small amount of performance. The Influence of Batch Size to 3D Conformer Prediction. We evaluate the performance of DMTB with different batch sizes. The original batch size of 256 was chosen to maximize GPU utilization. To assess the impact of batch size, we tested two variations: (1) reducing the batch size to 128, and (2) increasing it to 512 using gradient accumulation. Observation. As shown in Table 15, the performance with 512 batch size is slightly worse than the original model. This is likely due to underfitting caused by fewer training steps. We keep the number of training epochs the same as the original experiment (256 batch size), therefore the larger batch size results in fewer gradient updates, leading to reduced model performance. Other than this observation, using the 128 batch size does not lead to significant difference than the original model. B.5 COMPUTATIONAL TIME COMPARISON We conducted time comparison between our model and representative baselines for conformer generation on the test set of the GEOM-Drugs dataset, which includes 1000 molecules. The baselines include the OpenEye Omega (OpenEye, Cadence Molecular Sciences), TD w/ PG (Corso et al., 2024), and xTB1. The results are shown in Figure 6. 1https://xtb-docs.readthedocs.io/en/latest/ 21 Published as conference paper at ICLR Table 15: DMT-Bs 3D conformer prediction performances on the GEOM-DRUGS dataset when using different batch sizes. COV-R (%) AMR-R COV-P (%) AMR-P Batch size Mean Median Mean Median Mean Median Mean Median 128 256, original 512 85.5 85.4 85.1 92.4 92.2 92.0 0.395 0.401 0.410 0.366 0.375 0. 65.1 65.2 64.9 68.0 67.8 67.7 0.644 0.642 0.645 0.575 0.577 0.582 Table 16: 3D Molecule stability performances. * denotes our reproduced results. (a) GEOM-DRGUS dataset. (b) QM9-2014 dataset. 3D-Metric MolStable 3D-Metric MolStable Train EDM JODO MiDi* EQGAT NExT-Mol, ours 0.028 0.002 0.010 0.003 0.025 0.027 Train G-SchNet G-SphereNet EDM MDM JODO MiDi* EQGAT NExT-Mol, ours 0.953 0.681 0.134 0.817 0.896 0.934 0.842 0.889 0.946 These experiments were performed on platform with an 8-core Intel Xeon Processor@2.90GHz CPU and an NVIDIA A100 GPU and the time is measured in minutes and seconds. Please note that the Omega and xTB are run on the CPU only, while DMT and Mollama are run on the GPU. So the results may vary depending on the hardware. B.6 3D MOLECULAR STABILITY PERFORMANCE We do not report the 3D molecule stability metric (Hoogeboom et al., 2022) in the main part of this work, because this metric presents significant limitation on the GEOM-DRUGS dataset, showing only 2.8% for the ground truth training set. We present the results here for backup purposes. B.7 MORE VISUALIZATIONS Visualization of Random Samples. Visualizations of complete molecules sampled from NExT-Mol on GEOM-Drugs and QM9 are shown in Figure 8 and Figure 9, respectively. These samples are randomly selected to illustrate the diversity and effectiveness of our model. The visualization includes 1D SELFIES sequences, 2D molecular graphs, and 3D conformers highlighting the spatial arrangement of atoms within the molecules. Notably, in the complex GEOM-Drugs dataset, NExT-Mol demonstrates its robustness by consistently generating molecules without disconnected components and effectively preserving the stable geometric planes of aromatic ring structures. These visualizations not only demonstrate the fidelity of the molecules generated by NExT-Mol with 1D SELFIES sequences along with 3D spatial coordinates, but also emphasize the ability of our model to produce stable and chemically valid conformers accommodating wide range of molecular weights. Visualization of 3D Conformer Prediction. To gain more insights on how transfer learning using MoLlamas 1D representations can improve 3D conformer prediction, we present more visu22 Published as conference paper at ICLR (a) Ground truth. (b) DMT-Bs prediction (RMSD = 0.90). (c) DMT-B + MoLlamas prediction (RMSD = 0.05). (d) Ground truth. (e) DMT-Bs prediction (RMSD = 0.87). (f) DMT-B + MoLlamas prediction (RMSD = 0.06). (g) Ground truth. (h) DMT-Bs prediction (RMSD = 0.84). (i) DMT-B + MoLlamas prediction (RMSD = 0.07). (j) Ground truth. (k) DMT-Bs prediction (RMSD = 0.86). (l) DMT-B + MoLlamas prediction (RMSD = 0.07). Figure 7: Visualization of 3D conformers. From left-to-right, we have the ground truth conformer, the conformer predicted by DMT-B, and the conformer predicted by DMT-B+MoLlama. For each model, we select the predicted conformer with the least RMSD to the ground truth. alizations of 3D conformer prediction in Figure 7. The samples are selected from the test set of GEOM-DRUGS with unseen scaffolds in the training set."
        },
        {
            "title": "C FURTHER DETAILS ON METHODOLOGY",
            "content": "C.1 1D MOLECULE GENERATION WITH MOLECULAR LLAMA LM Data Preparation. Following (Irwin et al., 2022), we collect 1.8 billion molecules from the ZINC15 database (Sterling & Irwin, 2015), significantly more than the 100 million molecules used in previous studies (Irwin et al., 2022; Fang et al., 2024b). We keep only molecules with molecular 23 Published as conference paper at ICLR 2025 Figure 8: Visualization of random samples generated by NExT-Mol trained on GEOM-DRUGS. Table 17: Hyperparameter for pretraining MoLlama. hidden size intermediate size max position embeddings num attention heads num hidden layers num key value heads query groups 2048 5632 512 32 22 4 4 hidden act batch size warmup steps min lr init lr weight decay grad clip silu 512 2000 4.00E-05 4.00E-04 1.00E-01 1.0 weight500 Daltons and LogP5 (Flynn, 1980), and transform them into SELFIES (Krenn et al., 2020) sequences. After canonicalizing the SELFIES and removing hydrogen atoms, the dataset contains 90 billion tokens. We further filter the molecules in the valid and test sets of the GEOMQM9 and GEOM-DRUGS datasets (Axelrod & Gomez-Bombarelli, 2022) and randomly sampled 1% of the remaining data as the validation set. Randomized SELFIES Augmentation Details. In order to generate randomized SELFIES, we first generate the randomized SMILES (Weininger, 1988), and transform the SMILES into SELFIES. We follow (Arus-Pous et al., 2019) for the implementation details of random SMILES, and use restricted random sampling of SMILES. Similarly, we also generate canonical SELFIES by transforming canonical SMILES. Pretraining Details. We train MoLlama from scratch for 1D molecule generation using next-token prediction objective. The code and hyperparameters are based on (Zhang et al., 2024), utilizing Flash-Attention (Dao, 2024) and FSDP (Zhao et al., 2023) for faster training. We use max context length of 512, concatenating multiple SELFIES sequences into the same context, with any overflow 24 Published as conference paper at ICLR 2025 Figure 9: Visualization of random samples generated by NExT-Mol trained on QM9-2014. trimmed and used in the next context. We use the AdamW optimizer and scheduler with linear warmup and cosine decay. The key parameters are included in Table 17. We train the model for 555k global steps. The training was done on 4 NVIDIA A100-40G GPUs and took approximately two weeks. The training log is shown in Figure 10. On the Advantages of Acheving 100% Validity beyond Validity Itself. We employ the 1D SELFIES representation for LM training. Here we elaborate on the other advantages beyond 100% validity, which are also crucial for real-world applications: Improving validity could improve other 2D metrics, like SNN, Frag, and Scaf. These metrics measure the distributional similarity of 2D molecular structures of valid molecules. If model still generate invalid molecules, it is likely the model does not capture the true target distribution, which contain only valid molecules. 100% validity helps the model learn from and sample from the valid molecular structures, which is essential for molecule generation tasks. This is demonstrated by our improved FCD, SNN, Frag, and Scaf metrics in Table 2. Improving validity could improve 3D geometry learning. The improved validity also leads to better learning of 3D molecular geometry, because it grounds 3D structure prediction on valid 2D structures. Other joint 2D and 3D prediction methods (Huang et al., 2024; Vignac et al., 2023b) can easily encounter invalid 2D structures when sampling 3D structures, therefore leads to worse 3D structure prediction. This is demonstrated by NExT-Mols significant improvements in geometry similarity metrics (e.g., bond angle and bond length) in Table 2. 25 Published as conference paper at ICLR 2025 Figure 10: Visualization of MoLlamas training and validation PPL log during pretraining. C.2 3D CONFORMER PREDICTION WITH DIFFUSION MOLECULAR TRANSFORMER Diffusion Process. Here we elaborate on the details of our diffusion process. Following (Nichol & Dhariwal, 2021; Huang et al., 2024), we use the cosine scheduler controlling the noise scale for the diffusion process: αt = (t) (0) , (t) = cos (cid:18) + 1 + π 2 (cid:19) , (5) where (0, 1] is the time step, and is hyperparameter empirically set to 0.008, following (Nichol & Dhariwal, 2021). Our pseudo codes for training and sampling are shown in Algorithm 1 and Algorithm 3 below. Following (Ho et al., 2020), we have the following hyperparameters used in the pseudo-codes for training and sampling: α(t) = α(t)/α(t1), σ(t) = (cid:112) 1 α(t). (6) Algorithm 1 Training 1: U(0, 1] 2: G(0) = (x(0), h, e) Training Set 3: x(0) x(0) x(0) 4: x(0) x(0)R, where SO(3) is randomly sampled 5: ϵ(t) (0I) 6: x(t) = α(t)x(0) + 7: G(t) (x(t), h, e) 8: Minimize loss = ϵ(t) DMT(G(t), t)2 2 1 α(t)ϵ(t) {Sample time step} {Sample 3D molecule} {Centering molecule coordinates} {Random rotation augmentation} {Forward diffusion} RMHA. Here we define the multi-head version of RMHA. Similar to the single-head version, we first generate the queries, keys, and values for atom representation H, and generate the queries and values for pair representation E: [Q; K; V] = [Wq; Wk; Wv]H, (7) [QE; VE] = tanh([Weq; Wev]E), (8) 26 Published as conference paper at ICLR 2025 Algorithm 2 Sampling 3D Conformers Require: time steps {ti}M 1: x(t1) (0, I) 2: for 1 to do ti1, ti 3: 4: G(t) (x(t), h, e) 5: (0, I) if < else = (cid:18) x(t) 1α(t) 1 α(t) DMT(G(t), t) 6: x(s) = 1 αt 7: end for 8: return x(M ) Algorithm 3 Sampling RNA 3D Conformers Require: time steps {ti}M 1: x(t1) (0, I) 2: for 1 to do ti1, ti 3: 4: R(t) (x(t), h, e) 5: (0, I) if < else = 0 (cid:18) x(t) 1α(t) 1 α(t) RTrans(R(t), t) 6: x(s) = 1 αt 7: end for 8: return x(M ) i=1, 2D molecular graph G2D (h, e) {Set the initial noise conformer} {Set time step} + σ(t)z {Update conformer} (cid:19) i=1, RNAs primary and secondary structure (s, e) {Set the initial noise conformer} {Set time step} + σ(t)z {Update conformer} (cid:19) Subsequently, we define the Relational-Attention (R-Attention) module, which is the combination of Equation 3 and Equation 4: = R-Attention(Q, K, V, QE, VE), where Oi = (cid:88) j=1 ai,j(VE i,j Vj), ai,j = softmaxj( (QE i,j Qi)K ). After this, the muli-head version of RMHA can be written as: RMHA(Q, K, V, QE, VE) = Concat(O1, ..., Oh)Wo (9) (10) (11) (12) where Of = R-Attention(Wqf Q, Wkf K, Wvf V, Weqf QE, Wevf VE), (13) where is the number of head; [1, h]; Wo is the linear projector combining outputs of different heads; and Wqf , Wkf , and Wvf are linear projectors for the -th head of atom representations; and Weqf and Weqf are linear projectors for the -th head of the pair representation. C.3 MOLLAMA REPRESENTATIONS IMPROVE DMTS 3D CONFORMER PREDICTION Details of SELFIES-to-Atom Mapping. The mapping process is not straightforward with existing software, so we have to manually code significant portion. For details on the full implementation, please refer to our code. In brief, the SELFIES software provides mapping between SELFIES and SMILES tokens, and RDKit gives the atom order when generating SMILES. We manually convert this atom order into mapping between SMILES and atom indices, then combine the SELFIESto-SMILES and SMILES-to-atom mappings into the SELFIES-to-atom mapping. Additionally, we handle missing hydrogen atoms in both SMILES and SELFIES during the mapping process. Rationale behind Transfer Learning between 1D Molecule Sequences and 3D Conformers. The final goal of this transfer learning is to leverage the billion-scale 1D/2D molecule dataset to improve 27 Published as conference paper at ICLR 2025 the 3D conformer prediction performance, which is constrained by limited 3D data. For clarity, we decompose the rationale into the following chain of arguments: 3D conformers are theoretically governed by 2D molecular graphs under quantum mechanics (QM). 3D molecular properties and structures are fundamentally rooted in QM. Using (approximated) QM-based methods, like DFT, we can accurately predict 3D conformers from 2D molecular graphs, though at high computational cost. This establishes the critical role of 2D representations in determining 3D structures. 3D conformer prediction relies on high quality 2D molecule representations. Deep learning models predict 3D conformers from 2D graphs, and their performance is heavily influenced by the quality of 2D molecular representations. Transfer learning can enhance 2D molecular representations, as demonstrated by prior works (Hu et al., 2020; Liu et al., 2022; Hou et al., 2022). 1D molecular representations can be converted to 2D molecular representations, and contribute to 3D prediction. 1D molecule sequences encode the same information as 2D molecular graphs, and the 1D to 2D transformation can be achieved by deterministic toolkit, like RDkit. Leveraging RDkit and our proposed cross-modal projector (cf. Section 3.3), we can transform 1D molecular representations to 2D molecular representations, and therefore contribute to the 3D prediction. We have demonstrated this improvement in Table 5, where using the pretrained 1D representations improve 3D conformer prediction. 1D pretraining scales more effectively than 2D. Given the billion-scale 1D/2D molecule dataset, we mostly prioritize the scalability when selecting the pretraining method. After literature review, we find that 1D LM-based pretraining methods, like Llama (Touvron et al., 2023) and BERT (Devlin et al., 2019), are extensively demonstrated for scalability and effectiveness. Therefore, we opt to 1D pretraining instead of 2D pretraining."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 BASELINES Here we present brief introduction for the baselines used in our experiments. We categorize baselines by their benchmarks. De Novo and Conditional 3D Molecule Generation. G-SchNet (Gebauer et al., 2019): G-SchNet autoregressively generates 3D molecules by considering molecular symmetries through the SchNet (Schutt et al., 2018) G-SphereNet (Luo & Ji, 2022): G-SphereNet autoregressively generates 3D molecules, in which each step determines the atom type, bond length, angle, and torsion angles. EDM (Hoogeboom et al., 2022): EDM pioneers the diffusion methods for 3D molecue generation. It constructs diffusion model with the EGNN (Satorras et al., 2021) architecture and the VDM diffusion process (Kingma et al., 2021). MDM (Huang et al., 2023b): MDM is diffusion model for 3D molecule generation. Through specialized edge construction module, it leverages both global interatomic interactions and local interatomic interactions for 3D modeling. CDGS (Huang et al., 2023a) and JODO (Huang et al., 2024): CDGS is diffusion model for 2D molecular graph generation. It models discrete vairables (e.g., atom types and bond types) using one-hot encoding and applies continous diffusion for generative modeling. JODO extends CDGS by studying joint 2D and 3D molecule generation. It features RMHA module for enhanced relational molecular graph modeling. MiDi (Vignac et al., 2023b): MiDi is joint 2D and 3D diffusion model for 3D molecule generation. It leverages two diffusion processes of discrete diffusion (Vignac et al., 2023a) and continous diffusion (Hoogeboom et al., 2022) for the corresponding data types in molecule. EQGAT-diff (Le et al., 2024): EQGAT-diff modified the EQGAT (Le et al., 2022) architecture for joint 2D and 3D molecular generation. EQGAT is based on the Tensor Field Networks (Thomas et al., 2018) to achieve 3D rotational and translational equivariance. 28 Published as conference paper at ICLR 2025 GeoLDM (Xu et al., 2023): GeoLDM explores the idea of latent diffusion model (Rombach et al., 2022) for 3D molecule generation. EEGSDE (Bao et al., 2023): EEGSDE explores conditional 3D molecule generation with diffusion guidance by an energy function. MolGPT (Bagal et al., 2021): MolGPT is decoder-only molecule LM pretrained on 1D SMILES sequences. MolGen (Fang et al., 2024b): MolGen is an encoder-decoder molecular LM pretrained on 1D SELFIES sequences. Following (Raffel et al., 2020), it is pretrained and evaluated using spancorruption objective. 3D Conformer Prediction. OMEGA (Hawkins, 2017): OpenEye OMEGA is commercial software that employs combination of fragment-based methods and torsional sampling, guided by empirical force fields or customized energy functions, to predict 3D conformers. GeoMol (Ganea et al., 2021): GeoMol is an SE(3)-invariant model for 3D conformer prediction. In the first step, it predicts the bond angles and bond lengths for all the neighbors of each nonterminal atom. Next, it assembles the local structures together by predicting their torsion angles. GeoDiff (Xu et al., 2022): GeoDiff is diffusion model that leverages roto-translational equivariant GNN for 3D conformer prediction. Torsional Diffusion (Jing et al., 2022): Torsional diffusion is diffusion model defined on the dihedral angles of 3D molecules. It samples seed conformers using RDkit, and applies diffusion only on the dihedral angles of molecular bonds, while fixing the bond lengths and bond angles. Particle Guidance (Corso et al., 2024): Particle guidance is diffusion guidance method designed to improve the sampling diversity compared to the vanilla i.i.d. sampling. It modifies torsional diffusions sampling process for 3D conformer prediction, without changing its training process. MCF (Wang et al., 2024): MCF explores the power of scaling law for 3D conformer prediction. Instead of following prior works and leveraging neural architecture with built-in 3D equivariance, it scales up general-purpose transformer, and demonstrates strong performances. D.2 DMT CONFIGURATIONS Hyperparameter. Table 18 shows the key hyperparameters used for training the DMT-B and DMTL models. Other hyperparameters, like batch size and training epochs, are separately listed for each task in the following sections. Features. We use the same atom features and pair features as (Jing et al., 2022). For the GEOMDRUGS dataset, the atom feature has 74 dimensions; for the QM9-2014 and GEOM-QM9 datasets, the atom feature has 44 dimensions. The bond feature has 4 dimensions. D.3 TASK: De Novo MOLECULE GENERATION For De Novo molecule generation, we separately train NExT-Mol for the GEOM-DRUGS and the QM9-2014 datasets. This process involve training both the MoLlama and DMT of NExT-Mol. MoLlama Settings. For QM9-2014, we use batch size of 512 and train for 100 epochs, while for GEOM-DRUGS, we use batch size of 256 and train for 20 epochs. For sampling, we employ sampling temperature of 1.0 and, beam size of 1, and we sample 10,000 molecules for evaluation. We use the AdamW optimizer and learning rate scheduler with linear warmup and cosine decay. The optimizer hyperparameters are as follows: init lr=1e-4, min lr=1e-5, warmup lr=1e-6, warmup steps=1000, and weight decay=0.05. DMT Settings. We use dropout rate of 0.1 for QM9-2014 and 0.05 for GEOM-DRUGS. Following (Huang et al., 2024), we select only the conformer with the lowest energy for training on the GEOM-DRUGS dataset. For both datasets, we train DMT-B for 1000 epochs. The batch size for QM9-2014 is 2048 and the batch size for GEOM-DRUGS is 256. 29 Published as conference paper at ICLR 2025 Table 18: Hyperparameters of the DMT-B and DMT-L models. layers atom hidden size atom intermediate size pair hidden size pair intermediate size heads total params optimizer init lr min lr warmup lr warmup steps weight decay DMT-B DMT-L 10 512 2048 128 512 8 55M 12 768 3072 192 768 8 150M AdamW 1.00E-04 1.00E-05 1.00E-06 1000 0.05 Details on the Evaluation Metrics. We use the MMD distance when computing the distributional similarity of bond lengths, bond angles, and dihedral angles. Note that, we do not perform Kekulization and Sanitization when computing molecule and atom stability for 2D and 3D molecules. We use canonicalized SMILES for both the generated molecules and the training dataset when computing novelty and uniqueness of molecules. All the baselines are consistently evaluated under the same setting above. D.4 TASK: CONDITIONAL MOLECULE GENERATION Details for Adapting NExT-Mol for Conditional Generation. For conditional molecule generation on the QM9-2014 dataset, we modify the NExT-Mol architecture to incorporate propertyspecific information into both the MoLlama language model and the DMT conformer prediction model. This approach allows us to generate molecules with desired properties in both 1D sequence and 3D structure spaces. Condioning MoLlama. We implement condition MLP to encode property information into soft prompt. This MLP consists of two linear layers with GELU activation function in between. It transforms single property value into 4-token sequence embedding, each token having the same dimensionality as the models hidden size. The resulting soft prompt is prepended to the input sequence embeddings of SELFIES before being fed into the language model. We adjust the attention mask accordingly to ensure the model attends to these conditional tokens. Condioning DMT. We use an MLP to process the property value, followed by linear projection to match the time embedding dimension. This processed condition is then added to the time embedding, allowing the diffusion process to be guided by the desired property throughout the denoising steps. MoLlama Setting. For conditional molecule generation, we train MoLlama with batch size of 256 for 100 epochs on the QM9-2014 dataset. We use sampling temperature of 1.0, beam size of 5, and we sample 10,000 molecules for evaluation of each desired property. DMT Setting. For the DMT-B model, we train with batch size of 512 for 1000 epochs on the QM9-2014 dataset. We employ dropout rate of 0 with 100 sampling steps for evaluation. The optimizer and learning rate schedule are consistent with the de novo generation task, using AdamW with linear warmup followed by cosine decay. We train the conditional generation model for six different quantum properties using the same optimization strategy as in the de novo generation task. Each model is trained on 4 NVIDIA A100-80GB GPUs. D.5 TASK: 3D CONFORMER PREDICTION Training Details. We elaborate the training details for each of the three training stages in Section 3.3. 30 Published as conference paper at ICLR 2025 Stage 1: DMT Training. For GEOM-QM9, we train the DMT-B model for 2000 epochs with batch size of 2048. For GEOM-DRUGS, we train both the DMT-B and DMT-L models for 3000 epochs with batch size 256. Note that, for each epoch, we randomly sample 3D conformer for each molecule, but not enumerate all the 3D conformers of that molecule. The resulting models (i.e., DMT-B and DMT-L) are used directly for evaluation in Table 4. Stage 2: Projector Warmup. For both datasets, we train only the LoRA weights of MoLlama, and the cross-modal projector for 10 epochs. The pretrained weights of DMT and MoLlama are frozen throughout the process. Stage 3: Integrated Fine-tuning. For both datasets, we train the integrated model for 500 epochs. We train the LoRA weight of MoLlama, the cross-modal pojector, and the DMT model. The pretrained weights of MoLlama are frozen throughout the process. Evaluation. Following (Wang et al., 2024; Jing et al., 2022), we use the dataset split of 243473/30433/1000 for GEOM-DRUGS and 106586/13323/1000 for GEOM-QM9, provided by (Ganea et al., 2021). For molecule with ground truth conformers, we generate 2K conformers as predictions. Evaluation Metrics. Let {C }l[1,L] be the predicted conformers and let {Ck}k[1,K] be the ground truth conformers. The evaluation metrics AMR-R (AMR-Recall) and COV-R (COV-Recall) can be formally defined as follows: COV-R := AMR-R := 1 1 {l [1..L] : [1..K], RMSD(Ck, ) < δ}, (cid:88) l[1..L] min k[1..K] RMSD(Ck, ), (14) (15) where δ is threshold that is set to 0.75 for GEOM-DRUGS and set to 0.5 for GEOM-QM9, following (Wang et al., 2024; Jing et al., 2022). AMR-P (AMR-Precision) and COV-P (COV-Precision) can be similarly defined by swapping the ground truth conformers and predicted conformers."
        }
    ],
    "affiliations": [
        "Chinese University of Hong Kong",
        "Hokkaido University",
        "National University of Singapore",
        "University of Science and Technology of China"
    ]
}