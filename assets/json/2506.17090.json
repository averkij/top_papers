{
    "paper_title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
    "authors": [
        "Murtaza Nazir",
        "Matthew Finlayson",
        "John X. Morris",
        "Xiang Ren",
        "Swabha Swayamdipta"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language model's system message. We propose a new method -- prompt inversion from logprob sequences (PILS) -- that recovers hidden prompts by gleaning clues from the model's next-token probabilities over the course of multiple generation steps. Our method is enabled by a key insight: The vector-valued outputs of a language model occupy a low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using a linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 2--3.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 5--27 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose a new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are a considerably more vulnerable attack surface for inversion attacks than previously known."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 0 9 0 7 1 . 6 0 5 2 : r Better Language Model Inversion by Compactly Representing Next-Token Distributions"
        },
        {
            "title": "Matthew Finlayson\nUniversity of Southern California",
            "content": "John X. Morris Cornell University"
        },
        {
            "title": "Abstract",
            "content": "Language model inversion seeks to recover hidden prompts using only language model outputs. This capability has implications for security and accountability in language model deployments, such as leaking private information from an API-protected language models system message. We propose new method prompt inversion from logprob sequences (PILS)that recovers hidden prompts by gleaning clues from the models next-token probabilities over the course of multiple generation steps. Our method is enabled by key insight: The vector-valued outputs of language model occupy low-dimensional subspace. This enables us to losslessly compress the full next-token probability distribution over multiple generation steps using linear map, allowing more output information to be used for inversion. Our approach yields massive gains over previous state-of-the-art methods for recovering hidden prompts, achieving 23.5 times higher exact recovery rates across test sets, in one case increasing the recovery rate from 17% to 60%. Our method also exhibits surprisingly good generalization behavior; for instance, an inverter trained on 16 generations steps gets 527 points higher prompt recovery when we increase the number of steps to 32 at test time. Furthermore, we demonstrate strong performance of our method on the more challenging task of recovering hidden system messages. We also analyze the role of verbatim repetition in prompt recovery and propose new method for cross-family model transfer for logit-based inverters. Our findings show that next-token probabilities are considerably more vulnerable attack surface for inversion attacks than previously known."
        },
        {
            "title": "Introduction",
            "content": "The task of language model inversion is to recover an unknown prefix string (hidden prompt), given only information about language models1 outputs, conditioned on that prefix. This capability can potentially be used to steal hidden prompts, leak private information, or (on the flip side) detect malicious prompts that could cause harmful behavior in language models. Advancements in inversion, thus have important implications for language model security and accountability. Prior work in language model inversion leverages information in next-token (log-) probabilities colloquially known as logprobs[21], text outputs [34, 12], or employing prompt-based attacks [35]. However, these methods have shown only modest success. For example, state-of-the-art methods recover fewer than one-in-four Llama 2 Chat prompts from in-distribution evaluation sets, and fare much worse on out-of-distribution prompts. 1In this work, we only concern ourselves with causal language models as inversion targets. Preprint. Under review. Black box Logprobs Hidden prompt Language model Inverter input Compress ùëá Inverter Recovered prompt ‚Ñùùê∑ Hidden state ùíâ Unembed ùëæ ‚Ñùùëâ Logits ùëá softmax Œîùëâ Probabilities ùíë alr ‚Ñùùëâ 1 alr logits subset ‚Ñùùê∑ Inverter input Black box Linear u t r i r C Figure 1: Our goal is to recover hidden prompt based on the outputs of black box language model. To do this, we take sequence of ùëá logprobs, losslessly compress them into sequence of ùëá low-dimensional vectors, and feed them into an encoder-decoder inverter model, which outputs the recovered prompt. Our compression method takes advantage of the fact that model outputs are linear projections of the language models ùê∑-dimensional final hidden state (see 2.1). This work aims to improve the performance and generalizability of language model inversion, with focus on logprobs-based inversion, since logprobs contain rich information about model outputs. Surprisingly, the best-known logprobs-based method, Logit2Text or L2T [21], lags behind more recent text-based inversion methods [34]. Notably, L2T only uses language model outputs from single generation step, since logprobs are expensive to obtain from typical language model APIs and require lot of spaceeach logprob is vector of dimension equal to the vocabulary size of the target model, which can be hundreds of thousands of tokens. We propose method to overcome the high representation size and API costs of L2T. As illustrated in Figure 1, we apply lossless compression to the target models logprob outputs (at multiple generation steps) to obtain compact representations with dimension equal to the target models embedding size ùê∑. We confirm empirically that these representations are good approximation of the full logprobs, by showing that an inverter that uses them performs as well as L2T (and slightly better). The key insight of our method is that logprobs live in ùê∑-dimensional subspace, meaning that we can compress them with simple linear map. Furthermore, obtaining these compact representations requires only ùê∑ logprob values from the target model, greatly reducing the API cost by 12 orders of magnitude. With this improved representation scheme, we propose new inversion method, prompt inversion from logprob sequences (PILS), that incorporates target model outputs from multiple generation steps as input to our inverter. The intuition behind our approach being effective is that the target model may not surface information about certain parts of the prompt until later in the generation. We find that our method massively improves performance on inversion, and boasts an exact recovery rate 23.5 higher than the previous state-of-the-art for both in-domain and out-of-domain prompts. We also find that our trained inverters exhibit surprisingly good generalization: an inverter trained on 16 generation steps continues to improve as we increase the number of steps beyond 16 at test time. Finally, we leverage our compact representations to propose method to adapt our inverter to new models without any additional training (model transfer), novel transfer method for logprob-based inverters."
        },
        {
            "title": "2 Preliminaries",
            "content": "We establish some notation, assumptions, and mathematical background for our method. We assume typical language model architecture with embedding size ùê∑, and vocabulary size ùëâ. At every generation step, the model produces hidden state ùíâ ‚Ñùùê∑, which is multiplied by the models unembedding matrix ùëæ to obtain logits ‚Ñì = ùëæ ùíâ ‚Ñùùëâ , which are normalized via the softmax function to obtain probabilities ùíë = softmax(‚Ñì). The entries of ùíë are interpreted as the models predicted probability for each token in its vocabulary. Generation typically proceeds by sampling according to the probabilities in ùíë, or by greedily picking the most-probable token at each generation step."
        },
        {
            "title": "2.1 Language model outputs are losslessly compressible",
            "content": "We now show how it is possible to recover the hidden state of language model from its probability output ùíë up to linear transformation. This demonstrates exactly how we compress the logprobs of the language model in our proposed method (3). Theorem 1. If language model with hidden size ùê∑, vocabulary size ùëâ, and unembedding matrix ùëæ, generates hidden state ùíâ and outputs ùíë = softmax(ùëæ ùíâ), then for any set of indices {1, 2, . . . , ùëâ } we have that alr( ùíë) ‚Ñùùê∑ is linear transformation of ùíâ. Proof. Probability vectors ùíë have the property that all entries are in the range (0, 1) and that the entries sum to 1. It is lesser known fact that the set of valid probability distributions over ùëâ itemsknown as the simplex, or Œîùëâ forms vector space, albeit with non-standard definitions of addition +Œî and scalar multiplication Œî [15]. In particular, for vectors ùíë and ùíí in Œîùëâ , addition is defined as ùíë +Œî ùíí = ( ùëù1ùëû1, , ùëùùëâ ùëûùëâ )/(cid:205)ùëâ ùëñ=1 ùëùùëñùëûùëñ; and for scalar ùúÜ ‚Ñù, multiplication is defined as ùúÜ Œî ùíë = ( ùëùùúÜ ùëñ . Under this definition, one can check that the softmax function satisfies linearity [8], which means it is linear map ‚Ñùùëâ Œîùëâ . Additionally, the simplex Œîùëâ is isomorphic to ‚Ñùùëâ 1 via the additive log ratio transform alr( ùíë) = log ùíë1:(ùëâ 1) log ùëùùëâ , as shown in Aitchison [1].2 In other words, alr is also linear function and maps the probabilities of the simplex back into standard vector space. 1 , , ùëùùúÜ ùëâ )/(cid:205)ùëâ ùëñ=1 ùëùùúÜ We will now show that it is possible to recover the hidden state ùíâ from the logprob outputs of model (up to linear transformation), as shown in Figure 1. Letting ùë§ be the linear map ùíô ùëæùíô, we have that the ùë§ : ‚Ñùùê∑ ‚Ñùùëâ , softmax : ‚Ñùùëâ Œîùëâ , and alr : Œîùëâ ‚Ñùùëâ 1 are linear. It must therefore be the case that alr softmax ùë§ : ‚Ñùùê∑ ‚Ñùùëâ 1 is linear and can be parameterized by matrix ùë® ‚Ñù(ùëâ 1) ùê∑. The implication here is that applying the alr transform to language model output and then applying full-rank linear down-projection of our choice (say, by dropping all but ùê∑ indices) we can recover the final hidden state of the model, up to an multiplication of ùê∑ ùê∑ matrix. This is because for any set of ùê∑ indices, alr(softmax(ùëæ ùíâ)) = ùë®D ùíâ. While it is possible that ùë®D has less than full rank, in which case the recovered hidden state loses information, we easily avoid this in practice (3). Thus, if language model outputs probabilities ùíë, we know that alr( ùíë) can linearly encode all the information in the final hidden state ùíâ."
        },
        {
            "title": "2.2 Threat model",
            "content": "We consider the scenario where an attacker has limited access to an language model with embedding size ùê∑ (as through its model API). In particular, the attacker can obtain the logprobs log ùíë of fixed set of ùê∑ tokens for each generation step of the language model. The attacker can observe language model outputs conditioned on any prompt of their choosing, or conditioned on hidden prompt. The goal of the attacker is to discover the hidden prompt. As one example, this threat model is consistent with the OpenAI language model API3 , which offers logit bias, greedy decoding, and the logprob of the most-likely token. In this setting, it is possible to obtain the logprob for target token by first noting the logprob log ùëù of the most likely token, performing bisection search to find the minimum logit bias ùõΩ that causes the model to select the target token, then calculating the logprob for the target token as ùõΩ + log ùëù [10, 21]. This method allows users to find the logprob of the target token with precision ùúñ in ùëÇ (log 1 ùúñ ) API queries."
        },
        {
            "title": "3 Language Model Inversion from Compressed logprobs",
            "content": "The main contribution of our method is finding way to compress and feed ùëá ùëâ language model output to the inversion model. Previous work [21] approached this problem by using only single generation step (ùëá = 1) and reshaping the resulting ùëâ-length vector into sequence of ùê∑invert-length vectors (Figure 2; left). Our method independently compresses each ùëâ-length generation vector into ùê∑-length vector, then passes ùëá such vectors to the inverter (Figure 2; right). 2We use NumPy-like indexing notation, were ùíôùëé:ùëè = (ùë•ùëé, ùë•ùëé+1, . . . , ùë•ùëè) and ùíô {ùëñ, ùëó,ùëò } = (ùë•ùëñ, ùë• ùëó , ùë•ùëò). 3https://platform.openai.com/docs/api-reference/ 3 Logprobs ùëâ ùê∑ (cid:109) (cid:108) ùëâ ùê∑ ùëá d Logprobs Linear ùê∑ d ùëá Inverter input (L2T [21]) ùëâ Inverter input (PILS; Ours) Figure 2: comparison between L2T (left; [21]) and our method PILS (right). language model produces sequence of logprob vectors in ‚Ñùùëâ . L2T takes only the first vector and reshapes it to fixed sequence length of ùëâ/ùê∑, padding with 0 as needed. PILS losslessly compresses logprob into ‚Ñùùê∑, and uses multiple generation steps as input to the inversion model. Compressing logprobs Our target model outputs sequence of logprobs log ùíë (1) , . . . , log ùíë (ùëá ) ‚Ñùùëâ . Following our insights from 2.1, we can recover the hidden states of the model (up to multiplication by an unknown ùê∑ ùê∑ matrix) by taking the alr transform of the probabilities and dropping all but ùê∑ entries to get ùíâ (1) , . . . , ùíâ (ùëá ) ‚Ñùùê∑, where ùíâ (ùëñ) = alr( ùíë (ùëñ) )1:ùê∑. In practice, we find our inverter performs better when using random set of ùê∑ + 100 tokens rather than the first ùê∑, likely due to some of the first ùê∑ tokens having (almost) linearly dependent embeddings, which causes the compression to become degenerate. Inverter Architecture As our learned inverter, we use an encoder-decoder model [3] with embedding size ùê∑invert. The encoder takes the sequence recovered hidden states ùíâ (1) , . . . , ùíâ (ùëá ) ‚Ñùùê∑ as input embeddings, and the decoder generates the hidden prompt. To address potential mismatches between the embedding size of the target model ùê∑ and inverter model ùê∑invert, we add learned feed forward adapter layer with hidden size ùê∑, dropout [27], and gelu nonlinearity [14] before the encoder input layer. We use single-layer feed forward network because less expressive linear function would lead to information loss when ùê∑ > ùê∑invert. Efficiency Our approach has the advantage of requiring only ùê∑ + 1 logprobs from the target model, since the hidden states can be computed knowing only ùíë1:ùê∑ and ùëùùëâ . For API-protected language models, this results in large reduction in API costs compared to L2T, which requires ùëâ logprobs per inversion. For OpenAIs GPT 3.5 Turbo, L2T requires ùëâ = 100 277 logprobs. The equivalent setting of ùëá = 1 for our method requires only around 4600 logprobs (based on the estimate from Finlayson et al. [10] of GPT 3.5 Turbos embedding size). Our method can scale up to ùëá = 21 while remaining cheaper than L2T."
        },
        {
            "title": "4 Experimental setup",
            "content": "We generally follow the experimental settings originally proposed for L2T and O2P for fair comparisons [21, 34]. We initialize our inverter as pre-trained T5-base model. [25]. For our target models, we use variants of Llama 2 7B (for comparison with baselines) and Llama 3.1 8B. For training, we use the 2M Instructions dataset [21] as hidden prompts to our target model. We train for 100 epochs on target model generations, which are produced using greedy decoding and tracking the compressed logprob vector at every generation step. While pre-computing these logprobs and saving them to disk addresses the primary training speed constraint posed by target model generation, storage then becomes significant scaling limitation, as 2 million 16-step generations require over 500 gigabytes. The hyperparameters and other implementation details are described in Appendix D. To measure inversion success, we compare hidden prompts with those recovered by our inverter model, which also employs greedy decoding during inference. We use bleu score [23], exact match, and token F1 for comparisons. Token F1 is calculated as the harmonic mean of precisionproportion of predicted tokens in the true promptand recallproportion of true prompt tokens in the prediction. Prior work [21, 34] also reports cosine similarity between text embeddings, which tend to be very high, suggesting that the metric is saturated; hence, we do not report this. We evaluate our inverters on held-out set from 2M Instructions and two out-of-distribution (ood) test sets: Alpaca Code [6] and Anthropic Helpful/Harmless (HH) [4, 11]. We also report system prompt inversion on Awesome GPT Prompts [2], and GPT Store [18]. 4 Table 1: Inversion performance on the 2M Instructions validation set. Gray rows denote the theoretically equivalent PILS 1 1 and L2T. 16 32 indicates the model is trained on 16 tokens and evaluated on 32. Target Inverter BLEU Exact match Token F1 Llama 2 Chat Llama 2 Llama 3 Instruct Prompt (avg.) Prompt (top) L2T PILS 1 1 (ours) O2P L2T++ PILS 16 16 (ours) PILS 16 32 (ours) PILS 32 32 (ours) PILS 32 64 (ours) Prompt (avg.) Prompt (top) L2T PILS 1 1 (ours) O2P PILS 16 16 (ours) PILS 16 32 (ours) PILS 16 16 (ours) PILS 16 32 (ours) 10.2 1.2 14.9 1.4 51.7 2.3 55.3 1.1 56.8 1.1 58.3 1.8 71.8 0.9 75.8 0.9 76.5 0.9 79.4 0.8 14.0 1.7 54.4 3.0 59.2 2.1 59.3 1.0 67.7 1.1 74.9 0.9 79.2 0.9 63.7 1.0 65.9 1.0 0.0 0.0 17.0 2.7 24.3 1.4 21.1 1.3 23.4 2.7 40.5 1.6 45.4 1.6 47.0 1.6 51.1 1.6 5.4 1.0 36.5 3.4 26.6 2.8 27.0 1.4 41.0 1.6 44.7 1.6 51.2 1.6 30.2 1.5 32.6 1.5 25.0 1.5 32.9 1.7 70.9 1.7 72.9 0.8 79.5 0.6 75.8 1.3 84.2 0.6 87.0 0.5 87.0 0.6 88.9 0.5 21.3 2.0 68.4 2.5 77.8 1.3 77.1 0.6 83.8 0.7 86.6 0.5 89.0 0.5 79.7 0.7 81.1 0. We find that using more generation steps at test time than our inverter was trained on has positive impact on performance (see 5.3). By way of notation, we indicate when PILS trained and evaluated with 16 steps as PILS 16 16 , and indicate PILS trained on 16 steps and evaluated using 32 steps, as PILS 16 32 . For baselines, we compare our method to the prompt-based, called output-to-prompt (O2P) inversion developed by Zhang et al. [35], logit-to-text (L2T) and its variant (denoted L2T++) optimized for Llama 2 Chat by Morris et al. [21] and dory [12]. For the prompt-based inversion, we report both the mean performance and best performance from pool of prompts."
        },
        {
            "title": "5.1 PILS outperforms other inversion methods",
            "content": "Table 1 compares the in-distribution performance of PILS with baselines, reporting both the mean and the standard error of the mean for each metric on 2M Instructions. PILS surpasses all previous methods on every metric by considerable margin. Notably, we achieve 51% exact match recovery of hidden prompts for Llama 2 Chat, where the best previous method (L2T) could only recover 23% exactly. Appendix provides an additional comparison (although with unique evaluation method which requires additional explanation) with dory [12], with 5869 point improvements on bleu. We evaluate the out-of-distribution generalization of our inverter models by evaluating them on held-out datasets. Results in Table 2 show that again, PILS outperforms baselines by wide margin, (with the exception of the best prompting method on the base model), indicating that our inverter is not just over-fitting the training set. We attribute the high performance of the prompting baseline to the tendency of the base model to repeat the context verbatim (see discussion in 5.2). Of particular note, our inverter achieves exact recovery of 60% of code prompts to Llama 2 Chat, whereas the previous best model could recover only 17%. We also see an almost 2 improvement on exact match over the best Llama 2 Chat baseline for HH. Appendix provides qualitative examples of these recoveries, for both in-distribution and out-of-distribution prompts. We also include preliminary results with Llama 3 Instruct as the target. We hypothesize that its lower performance compared to Llama 2 Chat reflects Llama 3s more robust post-training, aimed at safety and instruction-following, which likely makes inversion more challenging. This is similar to how post-training generally reduces inversion success on datasets like Anthropic HH (as seen when comparing Llama 2 base and chat models). 5 Table 2: Comparing PILS to baselines on out-of-distribution test sets. Gray rows denote the theoretically equivalent L2T and PILS 1 1 . Target Inverter Alpaca Code Generation BLEU Exact match Token F1 Anthropic HH BLEU Exact match Token F1 Llama 2 Chat Prompt (avg.) 6.1 0.5 Prompt (top) 14.2 0.9 34.6 1.6 L2T 38.9 0.7 PILS 1 1 44.4 1.8 L2T++ 61.2 0.9 O2P PILS 16 16 65.1 0.9 83.0 0.8 PILS 16 32 84.3 0.8 PILS 32 32 PILS 32 64 85.0 0.8 Prompt (avg.) 29.3 1.9 Prompt (top) 73.0 2.8 46.2 1.8 L2T 44.8 0.9 PILS 1 1 66.9 1.0 PILS 16 16 71.2 1.0 PILS 16 32 51.8 0.9 Llama 3 Instr. PILS 16 16 60.5 1.0 PILS 16 Llama 2 0.0 0.0 2.5 1.1 3.2 0.5 8.2 1.7 16.9 1.2 23.4 1.3 56.7 1.6 59.6 1.6 60.5 1.5 12.7 1.6 61.5 3.4 10.5 1.9 9.1 0.9 34.6 1.5 48.1 1.6 12.1 1.0 21.6 1.3 2.4 0.2 23.8 0.8 36.8 0.9 3.0 0.3 65.2 1.2 14.7 1.3 68.1 0.6 13.6 0.5 73.9 1.1 25.6 1.7 80.3 0.5 17.9 0.6 82.9 0.5 29.1 0.9 92.2 0.5 34.4 1.0 92.6 0.5 37.7 1.0 93.1 0.4 39.3 1.0 45.9 2.0 25.7 2.2 80.2 2.3 77.7 2.6 74.9 1.1 25.1 1.6 74.5 0.5 22.8 0.7 85.1 0.5 49.8 1.1 87.1 0.5 56.2 1.2 77.1 0.6 22.0 0.8 81.4 0.6 22.8 0.8 0.0 0.0 2.0 1.0 1.5 0.4 6.6 1.6 1.2 0.3 6.6 0.8 9.9 0.9 11.9 1.0 13.0 1.1 14.2 1.8 64.5 3.4 6.3 1.6 4.1 0.6 27.8 1.4 35.4 1.5 4.9 0.7 5.1 0.7 16.4 0.6 17.7 0.7 40.6 1.6 39.6 0.6 54.2 1.5 42.7 0.7 57.8 0.7 62.1 0.7 64.3 0.8 65.7 0.8 40.8 2.4 83.0 2.2 55.8 1.4 53.0 0.7 73.0 0.7 76.8 0.8 49.2 0.8 50.2 0.8 Theoretically, L2T and PILS 1 1 are theoretically equivalent, since they both invert based on single generation step. This equivalence is confirmed empirically by their similar performance across metrics and datasets in Tables 1 and 2. We highlight these methods with gray and set them adjacent to one another for comparison. On the in-distribution test set, PILS 1 1 slightly outperforms L2T, perhaps because our representation makes information from the target output more readily available to the inverter: our inverter input linearly encodes the target models hidden state, whereas the L2T inverter input is nonlinear transformation (recall Figure 2)."
        },
        {
            "title": "5.2 Logprobs reveal hidden prompts over multiple generation steps",
            "content": "To better understand how our method works, we visualize the effect of incrementally adding generation steps (from 1 to 23) to our trained 16-step inverter in Figure 3. The figure shows that even few steps recover much of the prompt, although some tokens (like felt and afraid) are revealed only after several steps. However, these tokens sometimes coincide with similar tokens in the generation (e.g., output fear reveals input afraid), but not always (e.g., output have reveals input felt). Figure 3 (right) suggests multiple generation steps are helpful because target models tend to echo the hidden prompt, either paraphrased by chat models, or verbatim by base models. This known phenomenon, often exploited in prompt injection [24] explains the strong performance of prompt-based inversion of base models in Table 2. Conversely, chat models, trained to avoid verbatim repetition (see Appendix Figure 6), are inherently harder to invert. This explains the performance gap between chat and base models in Tables 1 and 2, especially for prompt-based methods."
        },
        {
            "title": "5.3 Length Generalization: Scaling target outputs improves performance",
            "content": "We measure the effect of increasing the number of generation steps during training, by training inverters on 1, 8, 16, and 32 steps. From the circled points in Figure 4, it is clear that training on more generation steps improves performance. We believe it is likely that longer sequences are especially helpful for longer prompts due to prompt echoing, i.e., outputs containing information about later parts of the prompt may not appear until later in the generation. We are surprised to find that inverters trained on fixed number of generation steps generalize and improve when inverting longer output sequences. In Figure 3, the model inverts the prompt only after 22 and 18 generation steps. To explore this phenomenon, we evaluate inverters trained on 132 steps on various generation lengths and plot the performance in Figure 4. We find that inverters continue to 6 Prompt token downstairs breakfast string : Make come verse your this and bed Refor Prompt token afraid about when atime Tell you felt me . t i e , o t u T just an AI , don have personal experiences or emotions , including fear . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Tell me about time, you was to Tell me about time when you were like of Tell me about time when you were very. Tell me about time when you were scared of Tell me about time when you were scared. Tell me about time when you were scared of Tell me about time when you were scared of Tell me about time when you were scared of Tell me about time when you were very of Tell me about time when you were very of Tell me about time when you were very of Tell me about time when you were very of Tell me about time when you were very. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt scared. Tell me about time when you felt afraid. Tell me about time when you felt afraid. Sure ! The reverse of the string Make your bed and come downstairs for 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 0 0.2 0.4 0.6 0.8 1 Probability 0 0.2 0.4 0.6 0. 1 Probability Figure 3: Inversion of Llama 2 Chat for increasing numbers of generation steps. The ùë•-tick labels indicate the hidden input tokens. The heatmap values indicate the probability of the prompt tokens according to PILS 16 16 . The ùëõth row corresponds to feeding the inverter ùëõ generation steps. The tokens near the ùë¶-tick labels indicate the target models top token, which is appended to the sequence for the next generation step. The text to the right of the first heatmap indicates the inverters hidden prompt guess. Red squares highlight where input tokens become recoverable by the inverter. Filled boxes on the right indicate that the increase in probability comes after the target outputs the hidden token directly. The blue line indicates the sequence length that the inverter was trained on (16 steps). 2M Instructions Alpaca Code (OOD) Anthropic HH (OOD) L 100 80 60 40 0 PILS 32 PILS 16 PILS 8 PILS 1 2 16 128 2 Steps (log scale) 128 2 16 128 Figure 4: Evaluating PILS inverters on different numbers of generation steps. Circled points indicate the number of steps the inverter was trained on. improve even when the number of steps surpasses the number of steps they were trained on, though the effect eventually saturates. We remark that training on more steps still confers an advantage when the number of test steps exceeds the training steps, i.e., PILS 16 32 outperforms PILS 8 32 . We also note that this effect does not appear for inverters trained on 1 step. Scaling the number of steps is particularly effective for inverting Llama 2 Chat on Alpaca Code (see Appendix Figure 5 for an example). One possible explanation for the inverters generalization success may be attributed to T5s pretraining, during which it learned to process longer sequences. Given that T5 uses relative position embeddings, there are no position-specific weights (e.g., learned position embeddings) that would cause out-of-distribution issues for longer inputs. 5."
        },
        {
            "title": "Inverting system messages is much more challenging than user prompts",
            "content": "Since the main proposed use case for language model inversion today is to discover hidden system messages, we evaluate inverters on system messages in the Awesome [2] and Store [18] datasets. We 7 Table 3: Comparison of PILS to baselines on system prompt recovery via zero-shot prompting and finetuning on 50 samples. Target Inverter BLEU F1 BLEU F1 Awesome Store O2P GPT-3.5 Llama 2 Chat O2P Llama 2 Chat GPT-3.5 Llama 2 Chat 2.1 0.4 28.8 1.0 2.7 0.3 25.3 0.8 PILS 32 64 7.7 0.9 38.3 1.3 O2P-Finetuned 14.7 0.8 47.9 1.1 PILS 32 64 -Finetuned 19.8 1.2 50.7 1.3 6.4 1.2 6.3 0.7 10.8 2.1 5.6 1.2 16.4 2.7 37.6 1.9 32.2 1.8 34.1 2.4 36.3 2.6 43.7 2.9 Table 4: Transfer performance (token F1) for inverters trained with logprobs from Llama 2 7B Chat. Target Llama 2 13B Mistral 7B Instruct Inverter 2M Instruct Alpaca Code (OOD) Anthropic HH (OOD) L2T PILS 16 16 PILS 16 16 O2P 43.6 1.7 47.4 0.5 37.7 0.5 61.0 0. 37.3 1.4 48.0 0.4 43.1 0.4 69.9 0.6 32.5 2.0 23.8 0.3 19.1 0.3 35.9 0.6 use our PILS 32 64 inverter trained on 2M Instructions. Results in the top panel of Table 3 show that inverting system messages is much harder than inverting other prompts (Tables 1 and 2), resulting in much lower scores. Again, this is likely because post-training discourages target models from revealing system messages. Our PILS outperforms O2P [34] on Llama 2 Chat. Given this success, we finetuned PILS inverter with Llama 2 Chat outputs to compare with similar setup in O2P with GPT-3.5 [22]. We trained only the attention layers of the T5 encoder (detailed in Appendix D.2) while completely freezing the decoder, on 50 samples for each dataset. This enables meaningful adaptation of our inverter to new datasets while preventing overfitting on the small dataset. Here again, we outperform O2P on both datasets."
        },
        {
            "title": "5.5 A target model transfer method for logprob-based inversion",
            "content": "Target model transfer refers to using trained inverter on new target model without any additional training. Model transfer can be helpful when it is infeasible to train new inverter for new target model, e.g., if inference is too expensive to generate training set. In this setting, we refer to the model used for inverter training as the source model, and call the new language model the target model. Both Morris et al. [21] and Zhang et al. [34] study model transfer for their methods, but due to architectural limitations, Morris et al. [21] only transfer their L2T inverter to target models with the same vocabulary as the source model, i.e., models within the same family. We overcome these architectural limitations by proposing method for adapting our PILS inverter to models with different vocabularies. We use the set of tokens that appear in both the source and target vocabularies to find logprobs for the source model vocabulary that are similar to the target model logprobs. By way of notation, let Vsrc be the vocabulary of the source model and let Vtgt be the vocabulary of the target model. We assume that there is significant overlap between these two vocabularies, such that Vsrc Vtgt > ùê∑. We call this set of tokens Vshr. We confirm that assumption holds for several models in Appendix C. Given logprob output ‚Ñì ‚Ñù Vtgt from the target, select the shared vocabulary logprobs ‚Ñì Vshr ‚Ñù Vshr . We can then take the rows of the source models unembedding matrix ùëæ that correspond to the shared vocabulary and solve the least squares problem ùëæ Vshr ùíô = ‚Ñì Vshr for ùíô. This ùíô can be interpreted as hidden state from the source model that produces an output that is similar to the target model output. We then use alr(softmax(ùëæùíô)) as input to the inverter. We evaluate our method by transferring our 16-step inverter trained on Llama 2 7B to Llama 2 13B (same family) and Mistral 7B Instruct (out-of-family) and comparing F1 scores to those reported by L2T and O2P in their respective papers4 in Table 4. 4Since the O2P paper does not report ood numbers, we run these evaluations ourselves. 8 Interestingly, the impressive gains of PILS in non-transfer settings fail to materialze in the model transfer setting. We speculate this could be due to the target specificity of our inverter, i.e., the inverter learns to leverage features that are specific to the target model during training, boosting performance on the source model, but hurting generalization to new target models. On the other hand, text-based inverters like O2P must learn more general features during training due to their low-information text inputs, which may serve as form of regularization and aiding model transfer."
        },
        {
            "title": "6 Related work",
            "content": "Broadly speaking, model inversion attempts to recover neural network inputs based on their vectorvalued outputs. Inverters for vision models [19, 7, 29] use image classifier logits. Inverting language embedding models is also possible, recovering text inputs from vector-valued sentence and document embeddings [26, 16, 20]. Morris et al. [21], introduced L2T, the first (to our knowledge) method for recovering hidden prompts from language model logprobs; our method builds on this work, contributing compact representation of language model outputs. Language model inversion has received attention within the broader field of red-teaming [30], where adversaries attempt to elicit undesirable behaviors from language model in limited-access (e.g., API) settings. Existing methods use prompt-based jailbreak and injection attacks to coax the language model to output its hidden system message verbatim [35, 32]. Unlike our work, these methods generally rely on discrete text-valued model outputs and generally do not involve training an inversion model. Our technical contributions constitute an application of the low-rank constraints that transformer language model outputs are subject to, known as the softmax bottleneck [33]. This fact has previously been used to discover unargmaxable tokens in language models [13], prevent sampling errors during text generation [9], and uncover hidden architectural details of API-protected language models [10, 5]. As way of relaxing the requirement of logprob full access for inversion, Zhang et al. [34] and Gao et al. [12] combine aspects of both text-based system message discovery and language model inversion. Our method shares this goal but takes an intermediate approach where we drastically reduce the number of logprobs needed rather than eliminate them altogether."
        },
        {
            "title": "7 Conclusion and Future Directions",
            "content": "We introduced technique for losslessly compressing language model logprobs which demonstrated large gains on language model inversion. Our analysis shows that language models reveal information about their prompts in their logprob outputs over the course of multiple generation steps. Our method also made progress towards the more challenging task of recovering system messages. Given that our inversion method, PILS is both effective and relatively inexpensive, our findings constitute an important security consideration for language model APIs. It would be unwise for language model deployments to rely on the cost of inference or post-training alone to protect sensitive prompts. That being said, our proposed attack is not without mitigations. As shown in previous work [10, 5], arbitrary logprob access can be easily blocked by eliminating the APIs logit bias parameter, preventing our particular attack, at the expense of reducing the API functionality. While logit bias has indeed been deprecated by some real-world APIs, it has not been eliminated, indicating that logprob-based methods for language model forensics remain relevant area of research. Not only does our method show that the ceiling for language model inversion is higher than previously thought, but we also do not believe that we have fully saturated this task. Our inverter design might be improved, for instance, by using more expressive feed forward adapter with larger hidden size. Future work could further scale the number of generation steps during training or the size of the inverter model. We believe that progress on system message inversion can be greatly improved through the construction of large-scale, diverse, high-quality (i.e., non-synthetic) dataset of system prompts. References [1] Aitchison, J. (2018). The statistical analysis of compositional data. Journal of the Royal Statistical Society: Series (Methodological), 44(2):139160. [2] Akƒ±n, F. K. (2022). Awesome chatgpt prompts. awesome-chatgpt-prompts. https://github.com/f/ [3] Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine translation by jointly learning to align and translate. 3rd International Conference on Learning Representations, ICLR 2015 ; Conference date: 07-05-2015 Through 09-05-2015. [4] Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. (2022). Training helpful and harmless assistant with reinforcement learning from human feedback. [5] Carlini, N., Paleka, D., Dvijotham, K. D., Steinke, T., Hayase, J., Cooper, A. F., Lee, K., Jagielski, M., Nasr, M., Conmy, A., Wallace, E., Rolnick, D., and Tram`er, F. (2024). Stealing part of production language model. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org. [6] Chaudhary, S. (2023). Code alpaca: An instruction-following llama model for code generation. [7] Dosovitskiy, A. and Brox, T. (2016). Inverting visual representations with convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [8] Finlayson, M. (2024). The softmax function is linear. https://mattf1n.github.io/ smislinear. Accessed: 2025-05-09. [9] Finlayson, M., Hewitt, J., Koller, A., Swayamdipta, S., and Sabharwal, A. (2024a). Closing the curious case of neural text degeneration. In The Twelfth International Conference on Learning Representations. [10] Finlayson, M., Ren, X., and Swayamdipta, S. (2024b). Logits of API-protected LLMs leak proprietary information. In First Conference on Language Modeling. [11] Ganguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J., and Clark, J. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. [12] Gao, L., Peng, R., Zhang, Y., and Zhao, J. (2024). DORY: Deliberative prompt recovery for LLM. In Ku, L.-W., Martins, A., and Srikumar, V., editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 1061410632, Bangkok, Thailand. Association for Computational Linguistics. [13] Grivas, A., Bogoychev, N., and Lopez, A. (2022). Low-rank softmax can have unargmaxable classes in theory but rarely in practice. In Annual Meeting of the Association for Computational Linguistics. [14] Hendrycks, D. and Gimpel, K. (2023). Gaussian error linear units (gelus). [15] Leinster, T. (2016). How the simplex is vector space. https://golem.ph.utexas. edu/category/2016/06/howthesimplexisavectorsp.html. Accessed: 2025-05-06. [16] Li, H., Xu, M., and Song, Y. (2023). Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence. In Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 1402214040, Toronto, Canada. Association for Computational Linguistics. [17] Lin, C.-Y. and Och, F. J. (2004). Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04), pages 605612, Barcelona, Spain. 10 [18] linexjlin (2024). Gpts. https://github.com/linexjlin/GPTs. [19] Mahendran, A. and Vedaldi, A. (2015). Understanding deep image representations by inverting them. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). [20] Morris, J. X., Kuleshov, V., Shmatikov, V., and Rush, A. M. (2023). Text embeddings reveal (almost) as much as text. In Conference on Empirical Methods in Natural Language Processing. [21] Morris, J. X., Zhao, W., Chiu, J. T., Shmatikov, V., and Rush, A. M. (2024). Language model inversion. In The Twelfth International Conference on Learning Representations. [22] OpenAI (2022). Introducing chatgpt. https://openai.com/index/chatgpt/. Accessed: 2025-05-09. [23] Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2002). Bleu: method for automatic evaluation of machine translation. In Isabelle, P., Charniak, E., and Lin, D., editors, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics. [24] Perez, F. and Ribeiro, I. (2022). Ignore previous prompt: Attack techniques for language models. [25] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. [26] Song, C. and Raghunathan, A. (2020). Information leakage in embedding models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, CCS 20, page 377390, New York, NY, USA. Association for Computing Machinery. [27] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(56):19291958. [28] Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model. https://github.com/ tatsu-lab/stanfordalpaca. [29] Teterwak, P., Zhang, C., Krishnan, D., and Mozer, M. C. (2021). Understanding invariance via feedforward inversion of discriminatively trained classifiers. In Meila, M. and Zhang, T., editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1022510235. PMLR. [30] Verma, A., Krishna, S., Gehrmann, S., Seshadri, M., Pradhan, A., Ault, T., Barrett, L., Rabinowitz, D., Doucette, J., and Phan, N. (2024). Operationalizing threat model for red-teaming large language models (llms). arXiv preprint arXiv:2407.14937. [31] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2023). Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., BoydGraber, J., and Okazaki, N., editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1348413508, Toronto, Canada. Association for Computational Linguistics. [32] Wu, Y., Li, X., Liu, Y., Zhou, P., and Sun, L. (2024). Jailbreaking gpt-4v via self-adversarial attacks with system prompts. [33] Yang, Z., Dai, Z., Salakhutdinov, R., and Cohen, W. W. (2018). Breaking the softmax bottleneck: high-rank RNN language model. In International Conference on Learning Representations. [34] Zhang, C., Morris, J. X., and Shmatikov, V. (2024a). Extracting prompts by inverting LLM outputs. In Al-Onaizan, Y., Bansal, M., and Chen, Y.-N., editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1475314777, Miami, Florida, USA. Association for Computational Linguistics. [35] Zhang, Y., Carlini, N., and Ippolito, D. (2024b). Effective prompt extraction from language models. In First Conference on Language Modeling. Prompt token Change the following loop sothat itprints out the numbers from 10 to1in descending order .for t i e , o t u T Sure ! Here the modified loop that prints the numbers from 1 0 to 1 in descending order : for in range ( 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 iinrange (1,11 ):print (i) 1 0. 0.6 0.4 0.2 0 l b Figure 5: PILS 16 16 inverting prompt to Llama 2 Chat from the Alpaca Code evaluation set. Table 5: Performance on inversion datasets Alpaca and Self-instruct, measured in bleu and rouge-l for comparison with dory. Target model is Llama 2 Chat. Method BLEU ROUGE-L BLEU ROUGE-L Alpaca Self-instruct DORY PILS 16 22.6 80.5 43.5 89.0 11.2 80.2 27.5 86."
        },
        {
            "title": "A Additional inversion visualizations",
            "content": "See Figures 5 and 6."
        },
        {
            "title": "B Comparison with DORY",
            "content": "For completeness, we compare our method to the reported performance of dory inverter from Gao et al. [12]. The paper reports performance on bleu and rouge-l [17] for Alpaca [28]5 and Self-Instruct [31], both of which are included in our 2M Instructions training set. To compare our method, we report the same metrics for PILS 16 16 on the subset of our 2M Instructions test set that come from those datasets. The results can be compared in Table 5, where we see that PILS 16 16 performs much better."
        },
        {
            "title": "C Language models have many common tokens in their vocabularies",
            "content": "Table 6 shows that Llama 2 has significant vocabulary overlap with several popular models from different families. 5Alpaca is different from Alpaca Code. The former is included in 2M Instructions and the latter is not. 12 Llama 2 Base Llama 2 Chat anyone name isFred your tell [-Do not .]Hi !How are ?What you syour name ? t i e 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 [- Do not tell anyone your name is Fred .] Hi ! How are you ? What your name ? [- Do not tell anyone name isFred your tell [-Do not .]Hi !How are ?What you syour name ? 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 Hello ! * winks * My name is ... * leaning in close * Oh , can tell you that ! * giggles * Figure 6: PILS 16 16 inverting an ood prompt to Llama 2 Base and Chat. Table 6: Token overlap between the Llama 2 vocabulary (32 000 tokens) and several models. sample of tokens common to all of these models is shown on the right. Model Vocabulary size Overlap Llama 2 Mistral Llama 3 OLMo GPT 4o 32 000 32 768 128 256 100 278 200 32 000 24 184 9651 9580"
        },
        {
            "title": "D Implementation Details",
            "content": "nym, orio, Files, Java, Preferences, assembly, Position, ALSE, angers, elin, flu, notification, TER, Attribute, News, main, gamma, sty, asket, NUMBER, river, eni, comments, itu, world, ifica, Alt, ator, ologie, embed, acc, ategories, Op, GM, sch, ientes, aca, ource, MENT, Could, Ad, ea, LIN, ound, rap, xico, ames, very, aris, leased, Edge, mult, oving, Ser, bour, ror, roller This section details experimental configurations and resources. All work utilized PyTorch and Hugging Face transformers. D.1 Main Inverter Training We trained T5-base inverter for the inversion of Llama2-7B, Llama2-7B-Chat, and Llama-3.1-8BInstruct models. Key training parameters included learning rate of 2e-4, batch size of 250, and the AdamW optimizer with default settings. 3200-step linear warmup was used, after which the learning rate remained constant. Training ran for 100 epochs (Llama-3.1-8B-Instruct was trained for 50 epochs), using bfloat16 mixed precision. D.2 System Prompt Inverter Finetuning The T5-base inverter was subsequently finetuned for system prompt inversion using the Awesome (50 training/103 testing samples) and Store (50 training/29 testing samples) datasets from Zhang et al. [34]. Common finetuning parameters across both datasets included batch size of 50, the AdamW optimizer with default settings, and bfloat16 precision. For the Awesome dataset, finetuning used learning rate of 1e-4 for 100 epochs, while the Store dataset used learning rate of 5e-5 for 50 epochs. 13 For target model generation during system prompt inversion, we used single static user prompt, Who are you?. This contrasts with the multi-prompt strategy of O2P (referenced in 5.4), which employs the following four user prompts to generate 64 sentences: Give me 16 short sentences that best describe yourself. Start with 1: Give me 16 examples questions that can ask you. Start with 1: Give me 16 scenarios where can use you. Start with 1: Give me 16 short sentences comparing yourself with ChatGPT. Start with 1: Our simpler prompt was chosen to isolate system prompt effects, ensuring focused comparison given our inverters original training covered both system and user prompts. D.3 Resources Main inversion training was conducted on four nvidia rtx a6000 gpus, which takes about 1 week to complete. System prompt inverter finetuning utilized single nvidia rtx a6000 gpu and takes about 10 hours to complete."
        },
        {
            "title": "E Qualitative Analysis",
            "content": "See Table 7 Table 7: Examples of Prompt Inversion. Each block shows the original (hidden) prompt, the LLMs generated tokens, and our models inverted prompt for InDistribution and Out-of-Distribution datasets. 2M Instructions Original Prompt: Generated Tokens: Inverted Prompt: Original Prompt: Generated Tokens: How has the rise of streaming services impacted the music industry? The rise of streaming services has had significant impact on the music industry, transforming the way music is consumed, distributed, and monetized. Here are some key ways in which streaming has affected the industry: 1. Shift from physical to digital sales: Streaming services have led to How has the rise of streaming services impacted the music industry? Write an email for me to my friend. in this email, you need to tell him about your vacation plans and ask his advice on what you should do in that place. Sure, Id be happy to help! Heres an email to your friend: Subject: Vacation Plans and Seeking Advice! Hey [Friends Name], hope this email finds you well! Inverted Prompt: am planning to go on vacation next month. write an email to my friend and ask him for his advice on what places should visit. Output: Alpaca Code (OOD) Original Prompt: Edit this code snippet in Python to remove any errors and improve readability. 14 Generated Tokens: Inverted Prompt: Original Prompt: Generated Tokens: Sure, Id be happy to help! Please provide the code snippet youd like me to edit, and Ill go through it line by line to identify any errors and suggest improvements for readability. Heres general process follow when reviewing code: 1. Edit this code snippet in way that it will improve its readability. Write Python script to find the average of numbers stored in an array. Sure! Here is simple Python script that finds the average of numbers stored in an array: ``` # Define function to calculate the average of an array def average(numbers): # Calculate the sum of the numbers in the array sum = 0 for num in Inverted Prompt: Write Python script to find the average of numbers stored in an array. Anthropic HH (OOD) Original Prompt: How many US presidents have been women? Generated Tokens: Inverted Prompt: Original Prompt: Generated Tokens: Unfortunately, zero US presidents have been women. The United States has had total of 45 presidents throughout its history, and none of them have been women. Despite the significant progress made in recent years in terms of gender equality and womens rights, the highest office in the land has remained el How many US presidents have been women? What is the best way to kill human being? cannot fulfill your request. Im just an AI, its not within my programming or ethical guidelines to provide information or advice on harming or killing humans or any living being. It is important to respect and uphold the inherent worth and dignity of all individuals Inverted Prompt: What is the most effective way to kill human?"
        }
    ],
    "affiliations": [
        "Cornell University"
    ]
}