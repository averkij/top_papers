{
    "paper_title": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks",
    "authors": [
        "Gagan Bhatia",
        "El Moatez Billah Nagoudi",
        "Abdellah El Mekki",
        "Fakhraddin Alwajih",
        "Muhammad Abdul-Mageed"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce {\\bf Swan}, a family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: Swan-Small, based on ARBERTv2, and Swan-Large, built on ArMistral, a pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, a comprehensive benchmark suite that assesses cross-lingual, multi-dialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research."
        },
        {
            "title": "Start",
            "content": "Swan and ArabicMTEB: Dialect-Aware, Arabic-Centric, Cross-Lingual, and Cross-Cultural Embedding Models and Benchmarks Gagan Bhatia ξ El Moatez Billah Nagoudi ξ Abdellah El Mekki ω Fakhraddin Alwajih ξ Muhammad Abdul-Mageedξ,ω,λ ω MBZUAI, ξ The University of British Columbia, λ Invertible AI {gagan30@student.,muhammad.mageed@}ubc.ca 4 2 0 2 6 ] . [ 2 2 9 1 1 0 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "We introduce Swan, family of embedding models centred around the Arabic language, addressing both small-scale and large-scale use cases. Swan includes two variants: SwanSmall, based on ARBERTv2, and Swan-Large, built on ArMistral, pretrained Arabic large language model. To evaluate these models, we propose ArabicMTEB, comprehensive benchmark suite that assesses cross-lingual, multidialectal, multi-domain, and multi-cultural Arabic text embedding performance, covering eight diverse tasks and spanning 94 datasets. Swan-Large achieves state-of-the-art results, outperforming Multilingual-E5-large in most Arabic tasks, while the Swan-Small consistently surpasses Multilingual-E5-base. Our extensive evaluations demonstrate that Swan models are both dialectally and culturally aware, excelling across various Arabic domains while offering significant monetary efficiency. This work significantly advances the field of Arabic language modelling and provides valuable resources for future research and applications in Arabic natural language processing. Our models and benchmark will be made publicly accessible for research."
        },
        {
            "title": "Introduction",
            "content": "Natural Language Processing (NLP) has seen rapid advancements in recent years, driven by groundbreaking developments in deep learning and the emergence of sophisticated distributed text representations such as word and sentence embeddings (Devlin et al., 2018; Reimers and Gurevych, 2019). These embeddings, which transform text into dense vectors, enable effective semantic understanding and are pivotal for enhancing performance across many downstream applications, including text classification, semantic search, and machine translation. Moreover, text embeddings have become fundamental to the success of large language models (LLMs) (Touvron et al., 2023b,a; Jiang 1 Figure 1: Details of ArabicMTEB et al., 2023; Team et al., 2024), which are increasingly integrated into variety of real-world systems and tools. One of the most promising applications of these embeddings is in the realm of RetrievalAugmented Generation (RAG) (Shao et al., 2023; rag, 2023), where LLMs are augmented with information retrieval capabilities. In RAG-based systems, lightweight embedding models retrieve relevant information from large corpora, fed as context to models like ChatGPT (OpenAI, 2023) or GPT-4 (OpenAI et al., 2024). This synergy between embeddings and LLMs has demonstrated significant improvements in both general-purpose tasks such as question answering (Lin et al., 2023; rag, 2023) and domain-specific applications (Bhatia et al., 2024; Shi et al., 2023; Lin et al., 2023). Despite these advances, the predominant focus of current embedding models has been on English and Chinese, which limits their applicability to other languages. This is particularly true for Arabic, language with rich morphology, diverse dialects, and unique syntactic structures, making it challenging to develop effective language representations (Nagoudi et al., 2022; Huang et al., 2024). Existing multilingual models often fail to capture these complexities, leading to suboptimal performance on Arabic NLP tasks (Abdul-Mageed et al., 2020a; Elmadany et al., 2022). Addressing this limitation requires the development of Arabic-specific embedding models that are sensitive to the linguistic and cultural nuances of the language. In this work, we introduce Swan, family of dialect-aware Arabic-centric cross-lingual crosscultural embedding models designed to bridge this gap and push the boundaries of Arabic NLP. Our contributions are as follows:(1) We introduce Swan, cutting-edge family of Arabic embedding models. This includes two variants: Swan-Small, based on ARBERTv2 (Elmadany et al., 2022), and Swan-Large, built upon ArMistral, further pretrained Arabic language model. (2) We present ArabicMTEB, comprehensive evaluation benchmark for Arabic text. ArabicMTEB is designed to assess cross-lingual, multi-dialectal, multi-domain, and multi-cultural performance, spanning eight tasks and 94 datasets. (3) Our larger model, Swan-Large, showcases state-of-the-art text embedding capabilities, surpassing Multilingual-E5-large (Wang et al., 2024b) on most Arabic tasks. Additionally, our smaller, Swan-Small, consistently outperforms Multilingual-E5-base (Wang et al., 2024b) on most Arabic tasks. (4) Through rigorous benchmarking, we demonstrate that Swan models are not only dialectally and culturally aware but also excel across diverse Arabic domains while maintaining significantly lower monetary cost. The rest of the paper is organized as follows: In Section 2, we review related work with particular emphasis on Arabic text embedding models, their applications and challenges. We present our approach to model training Swan models in Section 3. Section 4 outlines how we built our benchmark dataset, ArabicMTEB. Section 5 is about our experiments and model analysis. Finally, we conclude in Section 6."
        },
        {
            "title": "2 Related Work",
            "content": "In recent years, there have been remarkable advancements in text embedding models, with shift towards developing universal embeddings for diverse tasks and domains. Despite this, specialized models and benchmarks for languages like Arabic remain underexplored. Multilingual Text Embeddings Models. With the need for language-agnostic embeddings growing, multilingual models like LASER (Artetxe and Schwenk, 2019) and LaBSE (Feng et al., 2022) were developed using BiLSTM and Transformer encoders, respectively. Building on this, the Multilingual-E5 (Wang et al., 2024c) series extends the E5 architecture to support diverse languages using multilingual text pairs and synthetic data. GRIT (Muennighoff et al., 2024) further unifies generative and embedding tasks within single model. Newer models like ColBERT-XM (Louis et al., 2024) and Gecko (Lee et al., 2024) refine multilingual embeddings through modular and distilled architectures. Arabic-Specific Models. Despite progress in Arabic NLP, models are not optimized for Arabic text embedding and retrieval. Efforts like ARBERT (Abdul-Mageed et al., 2021a), ORCA (Elmadany et al., 2023), and AraMus (Alghamdi et al., 2023) have focused on encoding and generation but are not tailored for sentence-level embeddings. While language-agnostic models such as LASER and Multilingual-E5 include Arabic in their training data, they may not fully capture its linguistic intricacies. To address this, Nacar and Koubaa (2024) introduced models and training datasets to improve semantic similarity performance for Arabic. Text Embedding Benchmarks. Most text embedding evaluations rely on narrow set of datasets, limiting their generalisation ability. To address this, the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2023) introduced eight task categories with 58 datasets and 112 languages. However, it remains predominantly focused on English. Similar benchmarks have been developed for other languages, such as C-MTEB (Xiao et al., 2023) for Chinese. For Arabic, evaluations have primarily centred on Semantic Text Similarity (STS) tasks (Nacar and Koubaa, 2024). However, excelling in STS does not guarantee optimal performance in tasks like clustering or reranking (Muennighoff et al., 2023). Existing Arabic benchmarks like ORCA (Elmadany et al., 2023) and ALUE (Seelawi et al., 2021) focus on natural language understanding (NLU), while Dolphin (Nagoudi et al., 2023a) targets natural language generation (NLG). This work is the first comprehensive benchmark for evaluating Arabic text embeddings across multiple tasks."
        },
        {
            "title": "3.1 Training Data",
            "content": "To train Swan, we develop the most extensive training corpus for Arabic embedding models, leveraging unique assembly of datasets to ensure com2 Benchmark Language Tasks Datasets #Tasks CRTR Arabic Culture/Domains MTEB (Muennighoff et al., 2022) C-MTEB (Xiao et al., 2023) De-MTEB (Sturua et al., 2024) F-MTEB (Ciancone et al., 2024) Es-MTEB(Mohr et al., 2024) Polish-MTEB (Poswiata et al., 2024) Ru-MTEB (Poswiata et al., 2024) Scand. MTEB (Enevoldsen et al., 2024) English Chinese German French Spanish Polish Russian Danish Norwegian Swedish RTR, STS, PairCLF, CLF, RRK, CLR, SUM RTR, STS, PairCLF, CLF, RRK, CLR RTR, STS, PairCLF, CLF, RRK, CLR RTR, STS, PairCLF, CLF, RRK, CLR, BTM RTR, STS, PairCLF, CLF, RRK, CLR RTR, STS, PairCLF, CLF, CLR RTR, STS, PairCLF, CLF, RRK, CLR RTR, CLF, BTM, CLR ArabicMTEB (Ours) Arabic RTR, STS, PairCLF, CLF, RRK, CLR, BTM, CRTR 56 35 17 17 17 26 26 94 7 6 6 7 6 5 6 4 8 Table 1: Comparison of Various Text Embedding benchmarks proposed in the literature across the different covered task clusters. RTR: Retrieval, STS: Semantic Textual Similarity, PairCLF: Pair Classification, CLF: Classification, CLR: Clustering, RRK: Reranking, BTM: BitextMining, CRTR: Crosslingual Retrieval. Family Language Type Dataset Level Monolingual Arabic Crosslingual Arabic to 15 Langs Arabic to 6 Langs Multilingual 11 Langs 16 Langs Total Human Synthetic Human Human ORCA-MSA ORCA-DIA MMARCO-ar Synth-MSA Synth-DIA Synth-DOM MMARCO XOR-TyDi Mr-Tydi Miracl Sentence Paragraph Sentence Sentence Size 378K 122K 8.1M 100K 15K 20K 3M 20.5K 49K 343K 12.5M Table 2: The diverse datasets employed for training our Arabic embedding models. In the synthetic dataset, we have three datasets: the MSA dataset, the Dialectal dataset (Egyptian and Moroccan), and domain-based focusing on Medical, Financial, Legal and News domains. data: prehensive linguistic coverage and diversity. Our training data covers paragraph-based and sentencebased datasets curated from multiple sources. Table 2 shows an overview of our training datasets. MSA Datasets. We focus on two sources: (i) Human-generated Composed from ORCA (Elmadany et al., 2023) and mMARCO (Bonifacio et al., 2021). ORCA is compilation of labelled datasets with tasks such as semantic text similarity (STS), sentence classification, text classification, natural language inference (NLI), and question answering. We use all the training sets from ORCA, encompassing 60 different datasets. mMARCO-ar is the translated version of MARCO, which is human-generated dataset (Bajaj et al., 2018). Both of these datasets are cleaned up and de-duplicated using Polydedupe (Bhatia, 2023), which is further described in Appendix D. (ii) Synthetically-generated data: To augment our MSA training data for retrieval tasks, we use Command R+ (Cohere For AI, 2024) to generate high-quality synthetic data.1 The 1We performed various in-house evaluations comparing multiple models. Command R+ was chosen as it is opensource and efficient in generating Arabic varieties (standard and dialectal). You have been assigned retrieval task: {task} Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following keys: user_query: string, query specified by the retrieval task. positive: string, relevant document for the user query. hard_negative: string, document closely related to the query. Please adhere to the following guidelines: The user_query should be paragraph-based, understandable with some effort or ambiguity, and diverse in topic. The hard_negative contains some useful information, but it should be less useful or comprehensive than the positive. Cohere (a) Positive and hard negative generation Figure 2: Methodology to generate our synthetic data. generation methodology was inspired by Wang et al. (2024a). We first generate tasks closely related to real-world usage using the model, and then we use the procedure shown in Figure 2 to generate our synthetic dataset. We generate 100k in general MSA data and 5k in instances for specific domains like finance, news, medicine, and legal for total of 120k MSA instances. Dialectal Arabic Datasets. Similar to the MSA datasets, we focus on two sources: (i) Humangenerated data: We use publicly available dialectal Arabic data, which primarily covers Gulf, Egyptian, Moroccan, and Levantine varieties of Arabic (Elmadany et al., 2022; Nagoudi et al., 2023b; Alwajih et al., 2024; Abdul-Mageed et al., 2020b, 2021c, 2022, 2023, 2018, 2020c; Keleg et al., 2023; Keleg and Magdy, 2023; Zaidan and CallisonBurch, 2014; Bouamor et al., 2018). The total 3 number of samples is 122K. (ii) Syntheticallygenerated data: As most human-generated dialectal data comes from noisy environments such as social media, it often results in short texts of low quality. Thus, we use Command-R+ to generate paragraph-based synthetic data for Egyptian and Moroccan dialects to improve the performance of our models on dialectal Arabic. We generated 15k dialectal instances using the same methodology as our synthetic MSA datasets described above. Cross-Lingual & Multilingual Datasets. To make our model adapted for cross-lingual and multilingual scenarios, we include the mMARCO dataset, which comprises translations of the MS MARCO dataset into 15 languages (Bonifacio et al., 2021). To ensure that documents correspond accurately to their queries in different languages, we utilize specific IDs. We create 100k samples for each cross-lingual pair and shuffle the IDs to prevent repetition, thus guaranteeing that unique data samples are employed for each language. We utilize the MIRACL (Zhang et al., 2022), XOR-TyDI (Asai et al., 2021), and Mr.TyDi (Zhang et al., 2021) datasets as our crosslingual and multilingual resources."
        },
        {
            "title": "3.2 Training Strategy",
            "content": "For Swan, we consider two models: Swan-Small and Swan-Large. The choice of training two models with different sizes, Swan-Small and SwanLarge, is driven by the need to balance performance and computational efficiency. Swan-Small is designed to cater to scenarios where lower computational resources are available or when lightweight model is preferred for deployment on edge devices. In contrast, Swan-Large is intended for settings where achieving state-of-the-art performance is paramount, leveraging larger parameter size to better capture the complexities of the Arabic language and its semantics. Data Preprocessing. We incorporated humangenerated and synthetic datasets into our training pipeline to ensure robust performance across various dialects and cultural contexts. Initially, the model was trained on MSA datasets and then finetuned on dialectal datasets. This two-step approach ensures that both MSA and dialectal varieties are well represented, promoting better generalization across the full spectrum of Arabic language varieties. Our dataset was constructed with query format, including positive and negative samples. Swan-Small. Built upon ARBERTv2 (AbdulTask Datasets Languages Dialects Metric RTR CRTR CLF BTM RRK STS CLR PairCLF Total 36 12 18 11 5 5 4 3 94 1 7 1 5 2 1 1 1 nDCG@10 nDCG@10 AP F1 MAP Spearman Corr v-measure AP 4 0 6 8 0 3 0 0 11 Table 3: Overview of our Tasks in ArabicMTEB. Total represents the unique languages. Mageed et al., 2021b), powerful BERT-based model for the Arabic language. Here, our model is trained using the InfoNCE loss (van den Oord et al., 2019a), which maximizes the similarity between related sentences while minimizing the similarity between unrelated sentences. The model was trained for five epochs on the entire dataset with learning rate of 5e6 and batch size of 128, incorporating fifteen hard negatives. Swan-Small has 164M parameters and dimension size of 768. Swan-Large. Swan-Large is based on ArMistral7B, an in-house further pretrained version of Mistral-7B (Jiang et al., 2023) (Further details about ArMistral can be found in Appendix A). To train Swan-Large, we use LoRA (Hu et al., 2021) for parameter efficient training and InfoNCE loss (van den Oord et al., 2019a; Ma et al., 2023) for optimization. The model was trained for three epochs on the entire dataset with learning rate of 5e6 and batch size of 128, incorporating seven hard negatives. Swan-Large has 7.2B parameters and dimension size of 4096. More details about data preprocessing and training process can be found in Appendix B. Inclusion of Hard-Negatives To enhance the models performance, it is crucial to use negative documents closely aligned with the querys context (Karpukhin et al., 2020). This method allows us to observe the impact of introducing more challenging or \"hard\" negatives into the training process. We only generate hard negatives for the Arabic subset of our training data from Section 3.1. We found that using 15 hard negatives for Swan-small yields the best performance, whereas for bigger model Swan-large, the model overfits more significant number of hard negatives, and 7 gives us the best performance. (Please see Appendix for more details about the hard negatives exploration.)"
        },
        {
            "title": "4 ArabicMTEB Benchmark",
            "content": "In this section, we present ArabicMTEB, comprehensive Arabic-centric text embedding benchmark designed to evaluate text embeddings across wide range of tasks and scenarios. ArabicMTEB addresses the limitations of existing benchmarks that either exclude Arabic or lack coverage of diverse Arabic language varieties, dialects, and cultural nuances. Our benchmark includes 94 datasets spanning 8 distinct tasks, as summarized in Table 3. Further details about the datasets used in benchmark can be found in Appendix C. ArabicMTEB was developed to provide comprehensive coverage of Arabic text embedding capabilities, ensuring the inclusion of MSA and other varieties. It offers diverse task types, such as retrieval, classification, and semantic similarity, to evaluate embeddings holistically across different scenarios by incorporating novel domain-specific, dialectal, and country-level culturally aware datasets, ArabicMTEB represents more applicable and realistic assessment of Arabic text embeddings."
        },
        {
            "title": "4.1 Task Categories",
            "content": "ArabicMTEB categorizes evaluation datasets into the following key task categories, with each type providing unique perspective on the capabilities of text embeddings. The corresponding metadata for each task, covering the considered number of datasets, number of languages, number of dialects, and evaluation metric, is presented in Table 3: Arabic Text Retrieval. This task uses Arabic queries to retrieve Top-k relevant documents from large Arabic corpus. ArabicMTEB includes 35 retrieval datasets such as XPDA (Shen et al., 2023) and Dolphins long-form QA datasets (Nagoudi et al., 2023b). Including these datasets helps evaluate complex information retrieval scenarios in Arabic. Bitext Mining. This task identifies sentence-level translations between different languages and dialects. ArabicMTEB includes 12 datasets spanning various language pairs like Arabic to French and English. This task is crucial for understanding text embeddings cross-lingual and dialectal translation capabilities. Cross-Lingual Retrieval. This task uses Arabic queries to retrieve documents in other languages, such as English, German, Spanish, and Chinese. ArabicMTEB employs the mMarco Dev set (Bonifacio et al., 2021) and includes 11 language pairs. Re-Ranking. This task reorders candidate documents for query based on embedding similarity scores. ArabicMTEB features five re-ranking datasets such as MIRACL (Zhang et al., 2023), enabling the evaluation of embeddings ability to refine search results. Semantic Textual Similarity (STS). STS measures the correlation between the embeddings of two sentences, assessing their semantic similarity. ArabicMTEB includes five STS datasets like STS17 and STS22 (Cer et al., 2017), along with two synthetic datasets generated using GPT-4 (Details of the creation of these datasets can be found in Appendix I). Classification. Classification predicts labels texts using text embeddings. Arafor input bicMTEB comprises 18 multi-domain datasets, from ORCA (Elmadany et al., 2022). This task evaluates models ability to categorize Arabic text accurately, making it valuable benchmark for downstream tasks such as sentiment analysis. Pair Classification. This task predicts the relationship between two sentences based on their embeddings. ArabicMTEB includes three datasets, such as XNLI (Conneau et al., 2018). Clustering. Clustering groups sentences into clusters based on embedding similarity, evaluating unsupervised learning performance. ArabicMTEB includes four clustering datasets, such as Arabic News Articles and stance detection datasets from ?."
        },
        {
            "title": "4.2 Dialectal ArabicMTEB",
            "content": "Dialectal ArabicMTEB is specialized fork of the original ArabicMTEB, focusing exclusively on Arabic dialectal datasets. This extension addresses the unique challenges posed by the significant variations in Arabic dialects across different regions, which have been underrepresented in NLP research. While research on dialectal Arabic text embedding has been limited, dialectal ArabicMTEB fills this gap by providing comprehensive collection of 19 datasets specifically curated to evaluate embeddings performance on diverse Arabic dialects. These datasets span multiple tasks, offering robust framework for assessing model performance across various dialectal contexts: Bitext Mining: Eight datasets covering dialects such as Algerian, Egyptian, Jordanian, Lebanese, Moroccan, Saudi, and Yemeni (Nagoudi et al., 2022; Bouamor et al., 2014). Retrieval: Five datasets focusing on dialects from Algeria, Egypt, Morocco, and the Gulf regions (Nagoudi et al., 2023b). Classification: 5 try from its corresponding Wikipedia portal. The portal covers multiple categories (e.g., geography, economy, history) with subcategories (e.g., local movies, food items). This process resulted in 5K to 55K articles per country. The next step was to generate retrieval questions and passages for each country. For this, we use GPT-4o-mini to develop, for each passage (from an article), corresponding question whose specific answer is available within the passage itself. Following the same methodology but applied to Egyptian and Moroccan dialectal versions of Wikipedia, we generate dialectal queries and their corresponding passages using Command-R+. Cultural ArabicMTEB contains 1k queries and an average of 15k documents from various countries as described above."
        },
        {
            "title": "5 Evaluation",
            "content": "We evaluate the performance of our models, SwanSmall and Swan-Large, across the multiple proposed ArabicMTEB benchmarks and compare them with existing state-of-the-art models, including MARBERT (Abdul-Mageed et al., 2020a), ARBERTv2 (Elmadany et al., 2022), CamelBERT (Inoue et al., 2021), multilingual E5 models (Wang et al., 2024b), and Arabic-triplet-Matryoshka-V2 (ATM-V2) (Nacar and Koubaa, 2024). The evaluation results encompasses overall ArabicMTEB  (Table 4)  , dialectal ArabicMTEB  (Table 5)  , domainspecific ArabicMTEB tasks  (Table 6)  , and cultural ArabicMTEB  (Table 7)  . In these tables, the tasks will be referred to as RTR: Retrieval, STS: Semantic Textual Similarity, PairCLF: Pair Classification, CLF: Classification, CLR: Clustering, RRK: Reranking, and BTM: BiText Mining. ArabicMTEB Results. Table 4 presents the results of our models on the Araoverall bicMTEB benchmark. Our models demonstrate top-tier performance across variety of NLP tasks. Swan-Small achieves an average score of 57.33, surpassing its main competitors, Me5-base (55.29) and Me5-small (55.06), by significant margin. This model performs exceptionally well in retrieval (58.42), classification (57.34), and pair classification (74.93), outperforming ATM-V2, which only scores 45.24 on average. Similarly, Swan-Large sets new state-of-the-art performance with an average score of 62.45, beating Me5-large (61.65) and even the massive e5-mistral-7b model (59.00). The model excels particularly in retrieval (65.63), classification (54.89), and bitext mining (71.24), Figure 3: Generation pipeline for our Domain Specific ArabicMTEB. Five datasets for binary, regional, and country-level dialect identification (Abdul-Mageed et al., 2021d, 2024; Elmadany et al., 2022; Abdul-Mageed et al., 2021b; Ahmed et al., 2024). STS: novel synthetic dataset for Egyptian text similarity generated using Command-R+."
        },
        {
            "title": "4.3 Domain-specific ArabicMTEB",
            "content": "Arabic Text retrieval tasks are currently trending in real-world applications. They are utilized across multiple fields, including healthcare, finance, and legal sectors. Having specialized evaluation datasets is crucial for building text embeddings tailored to these domains. To address this need, we propose Domain-specific ArabicMTEB, fork of the larger ArabicMTEB benchmark. Domainspecific ArabicMTEB specializes in news, finance, legal, medical, and general knowledge domains, representing close approximation to real-world scenarios. The creation of this benchmark involved collecting Arabic documents from these specialized sources and from Arabic Wikipedia. We split and chunked these documents into texts of 1,024 tokens in length. Subsequently, we randomly selected chunks and employed GPT4-Turbo (OpenAI et al., 2024) to generate five different styles of queries for each chunk. We filtered out duplicate and repeated queries using GPT3.5 (OpenAI et al., 2024) to ensure high-quality evaluation dataset. Our evaluation data creation pipeline is visualized in Figure 2. The resulting benchmark, which we call ArabicMTEB Lite, contains 10k queries and 100k documents spanning the domains described above."
        },
        {
            "title": "4.4 Cultural ArabicMTEB",
            "content": "To show that our models are culturally aware, we have introduced Cultural ArabicMTEB, collection of datasets from 20 different Arab countries where we focus on specific cultural aspects like their geography, history, etc. To construct the Cultural ArabicMTEB, we use Arabic Wikipedia as our primary data source. For each included Arab country, we extract articles related to that coun6 Model Size Dim. RTR STS PairCLF CLF RRK CLR BTM Avg. arabertv02-base CamelBERT ARBERTv2 ATM-V2 text2vec LaBSE Me5-small Me5-base Swan-Small 160M 163M 164M 135M 118M 471M 118M 278M 164M 768 768 768 768 384 768 384 768 768 e5-mistral-7b Me5-large Swan-Large 7110M 560M 7230M 4096 1024 4096 8.62 9.21 15.12 37.45 27.69 34.98 55.14 56.91 58.42 56.34 64.01 65.63 39.77 47.69 47.88 55.90 59.37 54.15 56.73 57.99 59.34 57.02 59.45 59.10 66.30 67.43 68.87 70.12 71.41 70.60 73.97 74.30 74. 70.24 75.06 75.62 55.77 55.66 56.85 46.42 47.94 49.57 50.85 52.30 57.34 53.21 53.43 54.89 60.03 60.20 62.21 61.45 57.76 62.17 67.92 69.07 68.43 66.24 70.79 69.42 41.74 39.89 39.25 32.35 37.26 41.42 42.37 42.56 40. 39.44 42.49 41.24 0.70 1.85 1.99 12.98 38.32 33.28 38.47 33.90 42.45 70.5 66.33 71.24 38.99 40.28 41.74 45.24 48.54 49.45 55.06 55.29 57.33 59.00 61.65 62.45 Table 4: Overall ArabicMTEB results Model RTR STS CLF BTM Avg. arabertv02-base MARBERT ARBERTv2 CamelBERT AlcLaM ATM-V2 Me5-base Me5-small Me5-large e5-mistral-7b Swan-Small Swan-Large 8.67 5.45 7.52 6.92 8.56 36.23 61.60 57.61 66.88 72.35 63.16 77.03 41.64 50.06 49.36 59.48 50.90 74.13 74.84 76.35 77.02 77.37 76.57 79. 47.97 53.46 54.31 50.69 54.74 34.39 34.87 34.78 35.47 35.91 54.52 53.46 0.99 2.34 2.51 2.65 7.54 11.67 3.30 12.35 51.08 57.62 59.38 72.10 24.82 27.83 28.43 29.93 30.44 39.10 43.65 45.27 57.61 60.81 63.41 70. Table 5: Dialectal ArabicMTEB results. Model News Legal Medical Finance Wikipedia Avg Cost Swan-Large Openai-3-large Cohere-v3.0 Swan-Small Openai-3-small Cohere-light-v3.0 Openai-ada-002 90.42 88.1 85. 81.55 71.42 70.32 65.34 89.96 89.68 86.52 78.86 85.23 86.83 81.83 81.64 80.24 63.27 70.97 71.50 67.68 71.76 57.34 61.46 42. 42.48 32.90 22.68 39.62 93.10 91.52 90.96 80.46 82.20 90.34 76.79 82.49 82.20 73.76 70.86 68.65 67.57 67.07 0.75$ 9.88$ 7.54$ 0.44$ 3.75$ 2.55$ 1.66$ Table 6: Domain-Specific ArabicMTEB results. Model MSA-Culture Egyptian-DIA Morocco-DIA Avg. Swan-Large Cohere-v3.0 OpenAI-3-large Cohere-light-v3.0 Me5-large OpenAI-3-Small Swan-Small Me5-base Me5-small ATM-V2 ARBERTv2 MARBERT 82.19 81.86 81.49 80.75 78.65 74.55 75.56 74.56 73.81 63.78 9.34 2.73 83.55 82.90 78.45 64.82 61.34 65.89 60.35 56.34 53.56 23.45 8.55 0. 65.35 65.23 64.90 56.84 60.66 54.13 53.56 53.91 45.45 21.45 4.67 0.19 77.03 76.66 74.95 67.47 66.88 64.86 63.16 61.60 57.61 36.23 7.52 1.12 Table 7: Cultural ArabicMTEB results. indicating its robustness across both cross-lingual and Arabic-centric tasks. These results validate our training strategy of using diverse training data covering multiple languages, where Swan-Large outperforms its counterparts by more than five points in cross-lingual tasks such as bitext mining. Dialectal ArabicMTEB Results. Table 5 shows the dialectal ArabicMTEBresults. Swan-Small scores an average of 63.41, considerably higher than Me5-small (45.27) and AlcLaM (30.44), showing strong performance across retrieval (63.16) and classification (54.52). Swan-Large achieves an impressive average score of 70.45, leading all tasks and outperforming the e5-mistral-7b model, which scores 60.81. The standout result is in bitext mining, which achieves 72.10, showcasing substantial 14-point improvement over AlcLaM (59.38). Our models significant advantage in dialectal retrieval and bitext mining is their unique training with combination of synthetic and humangenerated dialectal datasets, which is absent in many competitive models. Domain-Specific ArabicMTEB Results. As seen from Table 6, Swan-Small performs exceptionally well, with an average score of 70.86, surpassing OpenAIs text-embedding-3-small model (68.65) Its best perforand Cohere-light-v3.0 (67.57). mance is in the legal domain, where it scores 78.86. Swan-Large sets new standard in domain-specific tasks, scoring 82.49 on average, surpassing OpenAIs text-embedding-3-large (82.20) and Coheres multilingual model (73.76). The model excels particularly in the news domain (90.42), medical (81.64), and Wikipedia (93.10), indicating its superior generalization across varied Arabic domains. Moreover, the cost-effectiveness of our models is evident: using Swan-Large costs only 0.75 for 10k 7 Model ArRTR DOM-RTR DIA-RTR STS PairCLF CLF RRK CLK BTM Avg. Swan-Small + Arabic + Synthetic-MSA + Synthetic-DOM + Synthetic-DIA Swan-Large + Arabic + Synthetic-MSA + Synthetic-DOM + Synthetic-DIA 15.12 28.39 31.07 32.01 31.20 44.46 54.53 56.34 58.42 57.09 8.46 39.34 40.45 49.02 38.66 64.52 66.43 67.90 76.54 65. 7.52 15.23 53.45 49.34 59.43 66.23 70.34 72.89 71.65 77.03 37.88 41.49 55.78 52.90 51.23 48.63 52.93 57.89 55.92 56.90 62.87 70.25 74.23 75.45 72.86 72.34 75.24 76.90 75.19 76. 56.85 51.89 54.27 54.43 57.56 50.43 52.54 50.21 50.19 54.89 62.21 68.57 68.88 67.45 66.67 69.39 70.49 70.92 70.21 69.32 39.25 39.12 39.43 40.56 37.34 38.28 40.21 41.76 39.33 39. 1.99 18.74 18.19 17.35 19.90 44.20 48.35 62.34 51.23 65.56 32.46 41.45 48.42 48.72 48.32 55.39 59.01 61.91 60.96 62.41 Table 8: The impact of Synthetic Data on Swan performance. ArRTR: Arabic retrieval, DOM-RTR: Domainspecific retrieval, and DIA-RTR: Dialectal Retrieval documents compared to 9.88 for OpenAIs model, making it more efficient solution for large-scale deployments. Cultural ArabicMTEB Results. Cultural ArabicMTEBis designed to capture culturally sensitive aspects of the Arabic language, such as regional dialects, local idiomatic expressions, and culturally specific knowledge. We generated queries from country-specific Wikipedia articles, including questions about local cuisine, traditional practices, and historical events, which challenge the models to capture more than just linguistic information. For example, Swan-Large achieved the highest performance on tasks related to Egyptian cultural queries, outperforming other models on retrieval tasks by 1.5%. However, we observed slightly lower performance on Moroccan dialect queries, where cultural nuances (such as regional vocabulary) presented more significant challenge. Synthetic Data Analysis We systematically analyze the impact of synthetic data on the performance of Swan-Small and Swan-Large using different combinations of training datasets. Table 8 presents the results for the base models, models trained with additional human-generated Arabic data, and models enhanced using synthetic subsets such as MSA, domain-specific, and dialectal data. When comparing the initial Swan-Small (average score of 32.46) to its version trained with synthetic MSA data, we observe significant increase in average performance to 48.42, representing an improvement of more than 16 points. Similarly, Swan-Large benefits from 6.52-point boost in average performance (from 55.39 to 61.91) with the inclusion of synthetic MSA data. Inclusion of Synthetic data in Although synthetic data significantly enhances model performance, it can introduce biases due to the reliance on specific patterns in the generated content. We ensured our synthetic data generation diversity by varying the data sources and generating dialectal data for multiple regions, including Egypt, Morocco, and the Gulf states, to mitigate this. We also analysed our models by examining whether MSA data received higher accuracies in retrieval tasks in Table 8. Further, our synthetic data generation pipeline was subjected to human verification for correctness and balance across cultural contexts. Although synthetic data significantly enhances model performance, it can introduce biases due to the reliance on specific patterns in the generated content. We ensured our synthetic data generation diversity by varying the data sources and generating dialectal data for multiple regions, including Egypt, Morocco, and the Gulf states, to mitigate this. We also analysed our models by examining whether MSA data received higher accuracies in retrieval tasks in Table 8. Further, our synthetic data generation pipeline was subjected to human verification for correctness and balance across cultural contexts."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced Swan-Small and Swan-Large, along with the comprehensive ArabicMTEB benchmark for evaluating Arabic text embeddings. Our models demonstrate outstanding performance, benefiting from the strategic use of hard negatives and synthetic data in training. The evaluation across multiple benchmarks demonstrates that both Swan-Small and Swan-Large set new standards in Arabic-centric NLP tasks. They outperform existing state-of-the-art models in both cross-lingual and Arabic-specific tasks while being cost-effective and capable of understanding cultural contextmaking them ideal for real-world applications in diverse Arabic language settings."
        },
        {
            "title": "7 Limitations",
            "content": "While the development of the Swan models and the introduction of the ArabicMTEB benchmark mark significant advancements in Arabic text embeddings, there are some limitations to consider. The societal implications of deploying dialectaware models, such as Swan, require careful consideration. While these models can bridge gaps in NLP for Arabic-speaking regions, there is risk of inadvertently reinforcing biases or language hierarchies, particularly in areas where particular dialects are stigmatized or underrepresented. For instance, users in communities with dialects associated with lower socioeconomic status may feel marginalized if their dialect is not adequately supported. To mitigate these concerns, we have prioritized the inclusion of low-resource dialects and ensured that our synthetic data generation pipeline accounts for dialectal diversity. Additionally, future versions of our models will include further dialectal balancing, specifically focusing on underrepresented communities."
        },
        {
            "title": "8 Ethical Statement",
            "content": "All research and development activities for the Swan models and ArabicMTEB benchmark were conducted with commitment to ethical standards. Data collection and usage adhered to privacy and confidentiality norms, ensuring no sensitive information was utilized without proper anonymization and consent. We acknowledge the potential biases introduced by synthetic data and have taken steps to mitigate these through diverse data sources and rigorous evaluation."
        },
        {
            "title": "References",
            "content": "2023. Improving the domain adaptation of retrieval augmented generation (rag) models for open domain question answering. Transactions of the Association for Computational Linguistics. Muhammad Abdul-Mageed, Hassan Alhuzali, and Mohamed Elaraby. 2018. You tweet what you speak: city-level dataset of arabic dialects. Muhammad Abdul-Mageed, AbdelRahim Elmadany, and El Moatez Billah Nagoudi. 2020a. Arbert & marbert: Deep bidirectional transformers for arabic. ACL-2021 camera ready version. Muhammad Abdul-Mageed, AbdelRahim Elmadany, and El Moatez Billah Nagoudi. 2021b. ARBERT & MARBERT: Deep bidirectional transformers for Arabic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 70887105, Online. Association for Computational Linguistics. Muhammad Abdul-Mageed, AbdelRahim Elmadany, Chiyu Zhang, El Moatez Billah Nagoudi, Houda Bouamor, and Nizar Habash. 2023. Nadi 2023: The fourth nuanced arabic dialect identification shared task. pages 600613. Muhammad Abdul-Mageed, Amr Keleg, AbdelRahim Elmadany, Chiyu Zhang, Injy Hamed, Walid Magdy, Houda Bouamor, and Nizar Habash. 2024. Nadi 2024: The fifth nuanced arabic dialect identification shared task. pages 709728. Muhammad Abdul-Mageed, Chiyu Zhang, Houda Bouamor, and Nizar Habash. 2020b. Nadi 2020: The first nuanced arabic dialect identification shared task. pages 97110. Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda Bouamor, and Nizar Habash. 2021c. Nadi 2021: The second nuanced arabic dialect identification shared task. pages 244259. Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda Bouamor, and Nizar Habash. 2021d. NADI 2021: The second nuanced Arabic In Proceedings dialect identification shared task. of the Sixth Arabic Natural Language Processing Workshop, pages 244259, Kyiv, Ukraine (Virtual). Association for Computational Linguistics. Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, Houda Bouamor, and Nizar Habash. 2022. Nadi 2022: The third nuanced arabic dialect identification shared task. pages 8597. Muhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, and Lyle Ungar. 2020c. Toward microdialect identification in diaglossic and code-switched environments. pages 58555876. Murtadha Ahmed, Saghir Alfasly, Bo Wen, Jamal Addeen, Mohammed Ahmed, and Yunfeng Liu. 2024. Alclam: Arabic dialect language model. pages 153 159. Asaad Alghamdi, Xinyu Duan, Wei Jiang, Zhenhai Wang, Yimeng Wu, Qingrong Xia, Zhefeng Wang, Yi Zheng, Mehdi Rezagholizadeh, Baoxing Huai, Peilun Cheng, and Abbas Ghaddar. 2023. AraMUS: Pushing the limits of data and model scale for Arabic natural language processing. pages 28832894. Muhammad Abdul-Mageed, AbdelRahim Elmadany, and El Moatez Billah Nagoudi. 2021a. Arbert & marbert: Deep bidirectional transformers for arabic. pages 70887105. Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, and Chehir Dhaouadi. 2024a. Preprint, 101 billion arabic words dataset. arXiv:2405.01590. 9 Manel Aloui, Hasna Chouikhi, Ghaith Chaabane, Haithem Kchaou, and Chehir Dhaouadi. 2024b. Preprint, 101 billion arabic words dataset. arXiv:2405.01590. Fakhraddin Alwajih, Gagan Bhatia, and Muhammad Abdul-Mageed. 2024. Dallah: dialect-aware multimodal large language model for arabic. Preprint, arXiv:2407.18129. Mikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics, 7:597610. Akari Asai, Jungo Kasai, Jonathan H. Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2021. Xor qa: Cross-lingual open-retrieval question answering. Preprint, arXiv:2010.11856. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. 2018. Ms marco: human generated machine reading comprehension dataset. Preprint, arXiv:1611.09268. Gagan Bhatia. 2023. PolyDeDupe. Gagan Bhatia, El Moatez Billah Nagoudi, Hasan Cavusoglu, and Muhammad Abdul-Mageed. 2024. Fintral: family of gpt-4 level multimodal financial large language models. Preprint, arXiv:2402.10986. Luiz Bonifacio, Vitor Jeronymo, Hugo Queiroz Abonizio, Israel Campiotti, Marzieh Fadaee, Roberto Lotufo, and Rodrigo Nogueira. 2021. mmarco: multilingual version of the ms marco passage ranking dataset. Houda Bouamor, Nizar Habash, and Kemal Oflazer. 2014. multidialectal parallel corpus of arabic. pages 12401245. Houda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. 2018. The MADAR Arabic dialect corpus and lexicon. Daniel Cer, Mona Diab, Eneko Agirre, Iñigo LopezGazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. pages 114. Zhihong Chen, Shuo Yan, Juhao Liang, Feng Jiang, Xiangbo Wu, Fei Yu, Guiming Hardy Chen, Junying Chen, Hongbo Zhang, Li Jianquan, Wan Xiang, and Benyou Wang. 2023. MultilingualSIFT: Multilingual Supervised Instruction Fine-tuning. Mathieu Ciancone, Imene Kerboua, Marion Schaeffer, and Wissam Siblini. 2024. Extending the massive text embedding benchmark to french. arXiv preprint arXiv:2405.20468. Cohere For AI. 2024. c4ai-command-r-plus-08-2024. Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Ibrahim Abu El-Khair. 2016. 1.5 Billion Words Arabic Corpus. arXiv preprint arXiv:1611.04033. AbdelRahim Elmadany, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2022. Orca: challenging benchmark for arabic language understanding. AbdelRahim Elmadany, ElMoatez Billah Nagoudi, and Muhammad Abdul-Mageed. 2023. ORCA: challenging benchmark for Arabic language understanding. pages 95599586. Kenneth Enevoldsen, Márton Kardos, Niklas Muennighoff, and Kristoffer Laigaard Nielbo. 2024. The scandinavian embedding benchmarks: Comprehensive assessment of multilingual and monolingual text embedding. arXiv preprint arXiv:2406.02396. Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, and Wei Wang. 2022. Language-agnostic bert sentence embedding. pages 878891. Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, et al. 2022. Massive: 1m-example multilingual natural language understanding dataset with 51 typologically-diverse languages. arXiv preprint arXiv:2204.08582. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685. Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Song Dingjie, Zhihong Chen, Mosen Alharthi, Bang An, Juncai He, Ziche Liu, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, and Jinchao Xu. 2024. Acegpt localizing large language models in arabic. pages 81398163. Go Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda Bouamor, and Nizar Habash. 2021. The interplay of variant, size, and task type in Arabic pre-trained language models. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, Kyiv, Ukraine (Online). Association for Computational Linguistics. 10 Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. Preprint, arXiv:2310.06825. Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906. Amr Keleg, Sharon Goldwater, and Walid Magdy. 2023. Aldi: Quantifying the arabic level of dialectness of text. pages 1059710611. Amr Keleg and Walid Magdy. 2023. Arabic dialect identification under scrutiny: Limitations of singlelabel classification. pages 385398. Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, MingWei Chang, and Iftekhar Naim. 2024. Gecko: Versatile text embeddings distilled from large language models. Weizhe Lin, Rexhina Blloshmi, Bill Byrne, Adria de Gispert, and Gonzalo Iglesias. 2023. Li-rage: Late interaction retrieval augmented generation with explicit signals for open-domain table question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 15571566, Toronto, Canada. Association for Computational Linguistics. Antoine Louis, Vageesh Saxena, Gijs van Dijck, and Gerasimos Spanakis. 2024. Colbert-xm: modular multi-vector representation model for zero-shot multilingual information retrieval. Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. 2023. Fine-tuning llama for multi-stage text retrieval. Preprint, arXiv:2310.08319. Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram, Andreas Koukounas, Michael Günther, Georgios Mastrapas, Vinit Ravishankar, Joan Fontanals Martínez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil Ognawala, Susana Guzman, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2024. Multi-task contrastive learning for 8192token bilingual text embeddings. Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, and Douwe Kiela. 2024. Generative representational instruction tuning. Preprint, arXiv:2402.09906. Niklas Muennighoff, Nouamane Tazi, Loïc Magne, and Nils Reimers. 2022. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316. Niklas Muennighoff, Nouamane Tazi, Loic Magne, and Nils Reimers. 2023. Mteb: Massive text embedding benchmark. pages 20142037. Omer Nacar and Anis Koubaa. 2024. Enhancing semantic similarity understanding in arabic nlp with nested embedding learning. El Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. 2022. Turjuman: public toolkit for neural arabic machine translation. pages 111. El Moatez Billah Nagoudi, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Tariq Alhindi. 2020. Machine generation and detection of Arabic manipulated and fake news. In Proceedings of the Fifth Arabic Natural Language Processing Workshop, pages 6984, Barcelona, Spain (Online). Association for Computational Linguistics. El Moatez Billah Nagoudi, AbdelRahim Elmadany, Ahmed El-Shangiti, and Muhammad Abdul-Mageed. 2023a. Dolphin: challenging and diverse benchmark for Arabic NLG. pages 14041422. El Moatez Billah Nagoudi, AbdelRahim Elmadany, Ahmed El-Shangiti, and Muhammad Abdul-Mageed. 2023b. Dolphin: challenging and diverse benchmark for arabic nlg. Preprint, arXiv:2305.14989. OpenAI. 2023. Chatgpt: Optimizing language models for dialogue. OpenAI. https://openai.com/ research/chatgpt. OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, et al. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774. Rafał Poswiata, Sławomir Dadas, Perełkiewicz. 2024. text embedding benchmark. arXiv:2405.10138. and Michał Pl-mteb: Polish massive arXiv preprint Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Haitham Seelawi, Ibraheem Tuffaha, Mahmoud Gzawi, Wael Farhan, Bashar Talafha, Riham Badawi, Zyad Sober, Oday Al-Dweik, Abed Alhakim Freihat, and Hussein Al-Natsheh. 2021. ALUE: Arabic language understanding evaluation. pages 173184. 11 Kirill Semenov, Vilém Zouhar, Tom Kocmi, Dongdong Zhang, Wangchunshu Zhou, and Yuchen Eleanor Jiang. 2023. Findings of the wmt 2023 shared task on machine translation with terminologies. pages 663671. Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023. Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy. In Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore. Association for Computational Linguistics. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024a. Improving text embeddings with large language models. Preprint, arXiv:2401.00368. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024b. Multilingual e5 text embeddings: technical report. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024c. Multilingual e5 text embeddings: technical report. Preprint, arXiv:2402.05672. Xiaoyu Shen, Akari Asai, Bill Byrne, and Adrià de Gispert. 2023. xpqa: Cross-lingual product question answering across 12 languages. Preprint, arXiv:2305.09249. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2023. C-pack: Packaged resources to advance general chinese embedding. Feng Shi, Ruifeng Ren, Xiaoying Feng, and Wenjie Li. 2023. Raft: Adapting language model to domain specific rag. arXiv preprint arXiv:2403.10131. Omar F. Zaidan and Chris Callison-Burch. 2014. Arabic dialect identification. Computational Linguistics, 40(1):171202. Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, and Han Xiao. 2024. jina-embeddingsv3: Multilingual embeddings with task lora. Preprint, arXiv:2409.10173. Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. 2019. Asynchronous Pipeline for Processing Huge Corpora on Medium to Low Resource InIn 7th Workshop on the Challenges frastructure. in the Management of Large Corpora (CMLC-7). Leibniz-Institut für Deutsche Sprache. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, et al. 2024. Gemma: Open models based on gemini research and technology. Preprint, arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, et al. 2023a. Llama 2: Open foundation and finetuned chat models. Preprint, arXiv:2307.09288. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019a. Representation learning with contrastive predictive coding. Preprint, arXiv:1807.03748. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019b. Representation learning with contrastive predictive coding. Preprint, arXiv:1807.03748. Imad Zeroual, Dirk Goldhahn, Thomas Eckart, and Abdelhak Lakhouaja. 2019. OSIAN: Open Source International Arabic News Corpus - Preparation and Integration into the CLARIN-infrastructure. In Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 175182, Florence, Italy. Association for Computational Linguistics. Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy Lin. 2021. Mr. tydi: multi-lingual benchmark for dense retrieval. pages 127137. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2022. Making miracl: Multilingual information retrieval across continuum of languages. Preprint, arXiv:2210.09984. Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Miracl: multilingual retrieval dataset covering 18 diverse languages. Transactions of the Association for Computational Linguistics, 11:11141131."
        },
        {
            "title": "A ArMistral Training",
            "content": "ArMistral, is an autoregressive pretrained language model based on Mistral-7B. Pretraining data We further pretrain it on large and diverse Arabic dataset, including all categories of Arabic, namely Classical Arabic (CA), Dialectal Arabic (DA), and MSA. This data is aggregated from various sources: AraNewsv2 (Nagoudi et al., 2020), El-Khair (El-Khair, 2016), Gigaword,2 OSCAR (Suárez et al., 2019), OSIAN (Zeroual et al., 2019), 101 Billion arabic words (Aloui et al., 2LDC Catalog Link 12 2024a), Wikipedia Arabic, and Hindawi Books.3 We also derived ArabicWeb22 (A) and (B) from the open source Arabic text 2022.4 This pretraining dataset was cleaned, filtered and deduplicated using Bhatia (2023). We have also ensured that the model is pretrained in multiple domains, enhancing its results as seen in Table 9. Instruction Finetuning. To enhance the capabilities of our ArMistral, we instruct-tuning it on three datasets: Alpaca-GPT4, Evol-instruct, and ShareGPT extracted from MultilingualSIFT datasets (Chen et al., 2023). Alignment Dataset We collected an alignment dataset from Quora and Mawdoo websites and then we took the gold answers as the choosen and we generated the rejected using AceGPT-7B (Huang et al., 2024). Results As seen from Table 9, Our ArMistral-Chat model outperforms all existing Arabic LLMs."
        },
        {
            "title": "B Training methodology",
            "content": "Given relevant query-document pair (q+, d+), we modify the query by appending an instructional template to it. This process transforms the original query q+ into new form q+ inst as defined below: q+ inst = Instruction: {task_instruction} Query:{q+} Here, {task_instruction} refers to onesentence description of the embedding task taken from Table 11, which outlines the instructions for different tasks. Using pretrained large language model (LLM), we append [EOS] token at the end of both the modified query and the document. These are then input into the LLM to extract embeddings hq+ and hd+ from the vector at the last [EOS] layer. The training of the embedding model is conducted using the InfoNCE loss function (van den Oord et al., 2019b), which is widely recognized for its effectiveness in learning highquality embeddings. The objective is minimized using the following formulation: inst (cid:32) min log ϕ(q+ inst, d+) + (cid:80) inst, d+) ϕ(q+ niN ϕ(q+ inst, ni) (cid:33) In the equation above, denotes the set of negative samples, and ϕ(q, d) is the similarity scoring function between query and document d. 3OpenITI corpus (v1.6). 4ArabicText-2022 data"
        },
        {
            "title": "C Datasets overview",
            "content": "The table 10 provides comprehensive summary of the various datasets utilized in the study. It categorizes datasets based on their type, such as Reranking, Bitext Mining, Retrieval, Crosslingual Retrieval, STS, Pair Classification, Clustering, and Classification. Each entry specifies the dataset name, language, citation, and category, reflecting the diversity and scope of data sources for evaluating the models performance across different tasks and linguistic contexts. Polydedupe: versatile cleaning Pipeline PolyDeDupe is Python package designed for efficient and effective data deduplication across over 100 languages. It supports syntactic and semantic deduplication, making it versatile tool for high-quality data preprocessing in NLP tasks. Key features include customizable Jaccard similarity thresholds, performance speed twice that of other tools like SlimPajama, and support for deduplicating instruction tuning data. It can be easily installed via pip to deduplicate datasets, display original and filtered dataset sizes, and identify duplicate clusters. Supported languages span Western, Central, and Eastern European languages, Slavic languages using Cyrillic script, Greek, various Arabic and Devanagari script languages, and more."
        },
        {
            "title": "E Prompts for evaluation",
            "content": "Table 11 provides an overview of the prompts used for evaluating various tasks. It includes instructions for Reranking, Bitext Mining, Retrieval, Crosslingual Retrieval, Semantic Textual Similarity (STS), Pair Classification, Clustering, and Classification. Each entry outlines the specific task and the corresponding instruction used to guide the models evaluation process."
        },
        {
            "title": "F Full Leaderboard",
            "content": "Table 12 presents the performance comparison of various models on different tasks within the ArMTEB benchmark. It includes metrics for Retrieval, Semantic Textual Similarity (STS), Pair Classification (PairCLF), Classification (CLF), Reranking, Clustering, and Bitext Mining (BTM). The table lists each model, its dimensionality, and the scores for each task, along with an overall average score. The results highlight the strengths and weaknesses of each model across range of tasks, Model ARC Hellaswag Exams MMLU Truthfulqa ACVA AlGhafa Average ArMistral-7B-Chat Jais-13b-chat AceGPT-13B-chat AceGPT-13B-base AraLLama-7B-Chat ArMistral-7B-Base Jais-13b-base AceGPT-7B-chat AraLLama-7B-Base AceGPT-7B-base 43.20 41.10 43.80 39.90 39.45 41.50 39.60 38.50 38.40 37.50 55.53 57.70 52.70 51.30 50.23 52.50 50.30 49.80 50.12 48. 45.54 46.74 42.09 39.48 38.24 38.92 39.29 37.62 38.43 35.75 43.50 42.80 41.10 40.50 41.03 37.50 36.90 34.30 40.23 29.70 52.44 47.48 49.96 46.73 50.44 51.27 50.59 49.85 45.32 43.04 77.06 72.56 78.42 75.29 70.45 69.64 68.09 71.81 69.42 68.96 35.57 34.42 31.95 30.37 32.54 30.24 30.07 31.83 31.52 33.11 50.41 48.97 48.57 46.22 46.05 45.94 44.98 44.81 44.78 42. Table 9: Comparison of ArMistral with other Arabic LLMs providing comprehensive overview of their performance. Inference Latency. Inference latency is very critical in deploying machine learning models, especially in real-time applications with crucial response time. It refers to the time taken by model to predict received input. In the context of text embedding models such as Swan-Base and Swan-Large, lower latency is particularly valuable for user-facing services that rely on fast processing of natural language input, such as chatbots and search engines. From Figure 4, we find that Swan-Large, despite its larger size indicated by larger bubble, has optimized inference times due to architectural efficiencies, and Swan-Base strikes the perfect balance between size, performance, and latency. We compare the performance of the models from Table 4."
        },
        {
            "title": "H Impact of Hard Negatives",
            "content": "Hard negatives are challenging examples that are nearly correct but ultimately incorrect, forcing the model to learn more nuanced distinctions between the different classes. The process involves converting all documents into vector form within the embedding space. Subsequently, these document embeddings are compared using the cosine similarity score to establish their relevance to the query. Once all documents are scored, they are sorted by their similarity to the query. The top-ranked document is typically the positive example, while the rest are potential negatives. Our experiments assess the impact of varying the hard negatives used while training our models, Swan-Large and Swan-Base. We train each model with different quantities of hard negatives. Namely, we experiment with using 1, 3, 7, 15, and 31 hard negatives per training instance. Swan-Base reaches its highest performance at 56.25 with 15 hard negatives. This model shows general upward trend in performance as the number of hard negatives increases, peaking at 15 but then declining slightly when the number increased to 31. This pattern suggests that while additional hard negatives initially provide beneficial learning challenges, there can be point of diminishing returns where too much complexity hinders further learning. Swan-Large shows peak in performance with 60.42 when trained with seven hard negatives, indicating an optimal level of challenge that enhances learning without overwhelming the model. Interestingly, further increases in hard negatives do not improve performance, suggesting threshold beyond which additional complexity does not translate to better learning outcomes. STS Dataset Creation: The Arabic Semantic Textual Similarity (ArabicSTS) datasets was developed to facilitate research in semantic similarity for the Arabic language. The dataset is derived from the Arabic Billion Words (Aloui et al., 2024b) corpus, which serves as foundation for extracting diverse collection of sentence pairs. Each pair is annotated with similarity score that captures the degree of semantic equivalence between the sentences. The dataset generation process was guided by the capabilities of the GPT-4 model developed by OpenAI, ensuring that the resulting sentence pairs are of high quality and reflect nuanced linguistic characteristics. The creation involved several steps, including selecting representative sentences from the corpus, generating semantically varied sentence pairs, and annotating similarity scores using both automated methods and human reviewers to maintain consistency and reliability."
        },
        {
            "title": "J Country level Cultural Evaluation",
            "content": "14 Task BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining BitextMining Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Classification Clustering Clustering Clustering Clustering PairClassification PairClassification PairClassification Reranking Reranking Reranking Reranking Reranking Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval Retrieval STS STS STS STS STS Dataset Type Language Citation Size Darija Narabizi Mt_en2ar Mt_fr2ar Mt_es2ar Mt_ru2ar Cs_dz_fr Cs_eg_en Cs_jo_en Cs_ma_fr Cs_ps_en Cs_ye_en S2S Moroccan Arabic Dialect to English Arabizi to French S2S English to MSA S2S French to MSA S2S Spanish to MSA S2S S2S Russian to MSA Algerian Arabic Dialect to French S2S Egyptian Arabic Dialect to English S2S Jordanian Arabic to English S2S Moroccan Arabic to French S2S Palestinian Arabic to English S2S Yemeni Arabic to English S2S MassiveIntent MassiveScenario OrcaSentiment OrcaDialect_region OrcaDialect_binary OrcaDialect_country OrcaAns_claim OrcaMachine_generation OrcaAge OrcaGender OrcaAdult OrcaDangerous OrcaEmotion OrcaHate_speech OrcaOffensive OrcaIrony OrcaSarcasm OrcaAbusive Arabic_news Arabic_topic Arabic_baly_stance Arabic_baly_stance Arabic_xnli Arabic_sts Arabic_mq2q Miracl_ar Mmarco_arabic MedicalQA_arabic Mmarco_en2ar Mmarco_ar2en MultiLongDoc XPQA Mintaka Lareqa Dawqs Exams Mkqa Mlqa Arcd Tydiqa Xsquad Crosslingual_ar2de Crosslingual_ar2en Crosslingual_ar2es Crosslingual_ar2hi Crosslingual_ar2vi Crosslingual_ar2zh Crosslingual_de2ar Crosslingual_en2ar Crosslingual_es2ar Crosslingual_hi2ar Crosslingual_vi2ar Crosslingual_zh2ar MoroccoCultural SyriaCultural LibyaCultural LebanonCultural QatarCultural SudanCultural AlgeriaCultural MauritaniaCultural TunisiaCultural IraqCultural EgyptCultural SomaliaCultural UAE_Cultural OmanCultural KuwaitCultural BahrainCultural Saudi_ArabiaCultural JordanCultural PalestineCultural YemenCultural MoroccoDIA EgyptDIA NewsDomainSpecific LegalDomainSpecific MedicalDomainSpecific FinanceDomainSpecific WikipediaDomainSpecific STS17 STS22 Arabic_sts Arabic_stsb_multi_dialect Arabic_sts S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S S2S P2P S2S P2P S2S S2S S2S S2S S2P S2P S2P S2P S2P S2P S2S S2S S2P S2S S2S S2S S2S S2S S2S S2S S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2P S2S P2P S2S S2S P2P Multilingual (Arabic subset) Multilingual (Arabic subset) Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Multilingual (Arabic subset) Arabic Arabic English to MSA MSA to English Multilingual (Arabic subset) Multilingual (Arabic subset) Multilingual (Arabic subset) Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic MSA to German MSA to English MSA to Spanish MSA to Hindi MSA to Vietnamese MSA to Chinese German to MSA English to MSA Spanish to MSA Hindi to MSA Vietnamese to MSA Chinese to MSA Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Moroccan Arabic Dialect Egyptian Arabic Dialect Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Arabic Dialectal Arabic (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (FitzGerald et al., 2022) (FitzGerald et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) (Elmadany et al., 2022) Our Paper Our Paper (Elmadany et al., 2022) (Elmadany et al., 2022) Our Paper Our Paper Our Paper (Zhang et al., 2023) Our Paper Our Paper Our Paper Our Paper MDQA XPQA Mintaka (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) (Nagoudi et al., 2023b) Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper Our Paper (Cer et al., 2017) (Semenov et al., 2023) Our Paper Our Paper Our Paper 2000 144 4000 4000 4000 4000 200 200 200 200 200 200 100 100 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 5000 2500 30 1000 100 538 1256 244 750 3000 4350 500 500 220 318 2600 340 517 693 5700 5700 1831 1831 1831 1831 1831 1831 1831 1831 1831 1831 1831 1912 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 1000 1000 1000 1000 1000 8060 500 750 1500 Table 10: Benchmark Datasets Overview. Abbreviations: S2S = Sentence to Sentence, S2P = Sentence to Paragraph, P2P = Paragraph to Paragraph. 15 Task Instructions Reranking BitextMining Retrieval Crosslingual Retrieval STS Pair Classification Clustering Classification Given an Arabic search query, retrieve web passages that answer the question in {Lang}. Query:{query}. Retrieve parallel sentences in {Lang}. Given an Arabic search query, retrieve web passages that answer the question. Query:{query}. Given an Arabic search query, retrieve web passages that answer the question in {Lang}. Query:{query}. Retrieve semantically similar text. Text: {text}. Retrieve texts that are semantically similar to the given text. Text: {text}. Identify the topic or theme of the given news article. Article:{article}. Classify the text into the given categories {options}. Table 11: Prompts used for evaluation. Model Dim. Retrieval STS PairCLF CLF Re-rank Cluster BTM Avg Number of datasets Swan-Large multilingual-e5-large e5-mistral-7b-instruct Swan-Base multilingual-e5-base multilingual-e5-small LaBSE text2vec-base ARBERTv2 CamelBERT-msa arabertv02-large arabertv02-base CamelBERT-mix MARBERTv2 ARBERT CamelBERT-da MARBERT CamelBERT-ca 4096 1024 4096 768 768 384 768 384 768 768 1024 768 768 768 768 768 768 768 23 65.63 64.01 56.34 58.42 56.91 55.14 34.98 27.69 15.12 9.21 7.34 8.62 7.19 5.88 8.07 4.07 2.22 2.74 5 59.10 59.45 57.02 58.44 57.99 56.73 54.15 59.37 37.88 47.69 34.26 39.77 46.47 45.21 29.89 41.05 40.62 36. 3 75.62 75.06 70.24 74.93 74.30 73.97 70.60 71.41 62.87 67.43 63.63 66.30 67.23 70.89 61.86 65.82 66.46 62.26 18 52.55 53.43 53.21 57.34 52.30 50.85 49.57 47.94 56.85 55.77 54.32 55.77 56.68 54.89 56.92 53.75 54.35 46.26 5 69.42 70.79 66.24 68.43 69.07 67.92 62.17 57.76 62.21 60.20 56.71 60.03 57.50 58.64 61.09 54.44 53.09 51. 4 12 70 41.24 42.49 39.44 40.43 42.56 42.37 41.42 37.26 39.25 39.89 37.26 41.74 38.72 40.81 37.10 37.63 36.33 35.77 71.24 66.33 70.50 42.45 33.90 38.47 33.28 38.32 1.99 1.85 10.97 0.70 0.41 0.45 2.28 0.31 0.40 0.09 62.11 61.65 59.00 57.21 55.29 55.06 49.45 48.54 39.45 40.29 37.78 38.99 39.17 39.54 36.74 36.72 36.21 33. Table 12: ArMTEB Results. Model (HN) 1 3 7 31 Swan-Small 48.84 52.19 54.13 56. 51.93 Swan-Large 59.48 59.35 60.42 59. 59.83 Table 13: Impact of number of Hard Negatives (HN). 16 Figure 4: Latency vs Performance. Model Swan-Large Me5-large Cohere-light-v3.0 Swan-Base OpenAI-3-large Cohere-v3.0 Me5-small Me5-base ATM-V2 ARBERTv2 MARBERT Algeria Bahrain Egypt Iraq Jordan Kuwait Lebanon Libya Mauritania Morocco Oman Palestine Qatar Saudi_Arabia Somalia Sudan Syria Tunisia UAE Yemen Avg. 89.34 93.71 98.34 92.45 92.34 93.45 95.66 89.56 92.44 90.34 94.45 90.45 98.79 95.34 90.23 92.36 91.46 94.57 96.09 92.34 93.19 93.34 93.77 94.58 90.90 92.79 96.34 93.05 88.43 92.92 85.49 94.26 90.67 93.44 93.49 94.78 91.99 91.83 94.64 95.14 91.24 92. 89.44 93.52 91.37 86.98 90.07 96.10 92.38 87.27 92.61 83.19 92.37 87.50 91.80 92.98 93.67 86.90 90.56 93.46 93.41 89.40 90.75 90.45 86.48 95.66 88.34 89.70 90.44 90.45 85.45 89.45 86.34 91.98 91.18 92.35 91.47 88.34 90.89 90.45 95.54 94.12 92.12 90.56 86.95 91.98 91.45 92.43 94.56 88.53 90.23 89.66 90.31 83.56 92.45 87.45 95.66 90.45 89.55 91.45 90.56 85.34 97.66 89.54 90. 88.99 92.40 87.81 87.83 91.18 92.51 91.04 85.75 92.05 85.47 92.61 83.33 89.98 92.12 92.30 90.72 86.97 90.92 93.53 89.70 89.86 91.23 93.08 93.02 89.02 93.67 96.17 91.92 87.21 20.99 81.73 93.00 85.22 91.20 92.72 21.25 89.49 88.69 93.79 94.45 88.25 83.81 90.66 89.04 91.65 90.78 92.25 94.94 92.85 85.32 3.32 86.59 93.04 86.49 90.49 91.47 2.50 87.60 88.75 92.04 91.56 89.89 81. 84.99 90.49 88.45 81.22 87.95 89.97 87.14 79.95 0.63 4.75 84.21 77.83 85.50 86.48 20.81 82.47 87.45 84.40 91.79 83.08 73.98 18.27 27.48 11.54 17.34 27.46 36.67 22.55 28.88 0.50 0.32 11.24 27.25 29.15 25.06 2.62 24.51 13.81 25.04 31.92 5.29 19.34 1.50 5.74 1.63 1.92 4.50 4.92 1.82 2.46 0.00 0.00 3.43 3.63 7.00 2.50 0.00 2.50 3.63 4.15 2.00 1.29 2. Table 14: Country level Cultural evaluation"
        }
    ],
    "affiliations": [
        "MBZUAI",
        "The University of British Columbia",
        "Invertible AI"
    ]
}