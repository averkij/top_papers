{
    "paper_title": "LawThinker: A Deep Research Legal Agent in Dynamic Environments",
    "authors": [
        "Xinyu Yang",
        "Chenlong Deng",
        "Tongyu Wen",
        "Binyu Xie",
        "Zhicheng Dou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. A DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with a memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves a 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent ."
        },
        {
            "title": "Start",
            "content": "LawThinker: Deep Research Legal Agent in Dynamic Environments Chenlong Deng dengchenlong@ruc.edu.cn Renmin University of China Beijing, China Xinyu Yang yxygsai@ruc.edu.cn Renmin University of China Beijing, China Tongyu Wen wentongyu@ruc.edu.cn Renmin University of China Beijing, China 6 2 0 2 2 1 ] A . [ 1 6 5 0 2 1 . 2 0 6 2 : r Binyu Xie xiebinyu0929@ruc.edu.cn Renmin University of China Beijing, China Zhicheng Dou dou@ruc.edu.cn Renmin University of China Beijing, China Abstract Legal reasoning requires not only correct outcomes but also procedurally compliant reasoning processes. However, existing methods lack mechanisms to verify intermediate reasoning steps, allowing errors such as inapplicable statute citations to propagate undetected through the reasoning chain. To address this, we propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. The core idea is to enforce verification as an atomic operation after every knowledge exploration step. DeepVerifier module examines each retrieval result along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, with memory module for cross-round knowledge reuse in long-horizon tasks. Experiments on the dynamic benchmark J1-EVAL show that LawThinker achieves 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong improvements on process-oriented metrics. Evaluations on three static benchmarks further confirm its generalization capability. The code is available at https://github.com/yxy-919/LawThinker-agent. CCS Concepts Information systems Information retrieval; Language models. Keywords Legal Agent, Information Retrieval, Large Language Models ACM Reference Format: Xinyu Yang, Chenlong Deng, Tongyu Wen, Binyu Xie, and Zhicheng Dou. 2018. LawThinker: Deep Research Legal Agent in Dynamic Environments. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 15 pages. https://doi.org/XXXXXXX.XXXXXXX Zhicheng Dou is the corresponding author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: An example of error propagation from an incorrect statute citation. While both methods reach the same affirmative outcome, direct reasoning cites an inapplicable article, while LawThinker mitigates this issue by exploring with explicit verification, underscoring the necessity of process-level compliance in legal reasoning beyond answer correctness."
        },
        {
            "title": "1 Introduction\nRecent advances in Large Reasoning Models (LRMs) have demon-\nstrated strong multi-step problem-solving capabilities in domains\nsuch as mathematics [2, 38, 42, 51], code generation [12, 19, 23,\n24, 36], and scientific reasoning [13, 52]. These successes have\nprompted growing interest in applying LRMs to legal reasoning\ntasks. However, legal reasoning differs from other domains in a\ncritical way: a legally valid conclusion must be supported not only\nby a correct outcome, but also by a reasoning process that is accurate\nand procedurally compliant. Citing an inapplicable statute or omit-\nting a required reasoning step renders the entire analysis legally\ndefective, even if the final answer happens to be correct. The chal-\nlenge is further amplified in real-world judicial scenarios, which\nare inherently dynamic and interactive. Legal consultations involve\nmulti-turn dialogues where users progressively raise follow-up\nquestions, document drafting requires iterative information gather-\ning across multiple exchanges, and courtroom proceedings demand",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. strict adherence to multi-stage procedural workflows. These dynamic settings impose strong demands on the agents ability to maintain accurate, procedurally grounded reasoning over long interaction horizons. Figure 1 illustrates this challenge with concrete example. When asked whether paid child support can be reclaimed after discovering non-paternity, direct reasoning approach selects Article 985 of the Civil Code based on surface-level keyword matching with unjust enrichment, and then constructs an apparently coherent explanation on this incorrect legal basis. Although the final answer happens to be affirmative, the cited provision is inapplicable, and the correct legal ground should be Article 122. This case reveals that errors introduced at the statute selection stage are not corrected by current legal reasoning systems, but instead absorbed into subsequent reasoning steps. As result, the final conclusion may appear well-reasoned yet lack valid legal foundation. More critically, such errors are difficult to detect through outcome evaluation alone, since the final answer may coincidentally be correct. This observation highlights the need for explicit verification mechanisms that can examine the accuracy and relevance of each intermediate step, rather than relying solely on end-to-end outcome correctness. Beyond error propagation, legal reasoning imposes stricter requirement that is often overlooked by existing systems. In judicial practice, every conclusion must be supported by accurate statutory provisions and grounded in verified case facts, and the reasoning process must follow legally prescribed procedures. judgment built on an inapplicable statute may be overturned on appeal, regardless of whether its outcome is correct. This demand for process-level legitimacy is precisely where current methods fall short. Existing approaches either rely solely on parametric knowledge with frequent hallucinations [3, 9, 57], or incorporate external retrieval without verifying the accuracy and relevance of the retrieved information [34, 49, 66]. Even methods that introduce step-level verification focus on whether intermediate steps lead to correct outcome, rather than whether the reasoning process conforms to legally prescribed procedures. These existing methods fail to jointly ensure knowledge accuracy, fact-law relevance, and procedural compliance throughout the reasoning trajectory. To address these limitations, we propose LawThinker, an autonomous legal research agent designed for dynamic judicial environments. LawThinker adopts an Explore-Verify-Memorize strategy that integrates iterative knowledge exploration with explicit verification throughout the reasoning process. When encountering knowledge gaps, the agent autonomously retrieves relevant legal provisions, cases, and procedural rules from external knowledge bases. Each retrieval is immediately followed by DeepVerifier module, which examines the accuracy of the retrieved content, its relevance to the case facts, and whether its use conforms to legal procedures. The structured verification results are fed back into the reasoning chain, allowing the agent to correct errors before they propagate to subsequent steps. To support long-horizon interactive tasks, memory module persistently stores validated legal knowledge and key case context, enabling the agent to reuse verified information across multiple reasoning turns. We further design suite of 15 legal tools spanning the three dimensions of exploration, verification, and memorization, providing the agent with comprehensive capabilities tailored for real-world judicial scenarios. We conduct extensive experiments on both dynamic and static legal benchmarks to validate the effectiveness of LawThinker. On J1-EVAL, dynamic benchmark covering six judicial scenario types, LawThinker achieves 24% overall improvement over direct reasoning baselines and an 11% gain over workflow-based methods. The improvement is particularly significant on process-oriented metrics such as format-following and procedural-following scores, confirming that step-level verification directly strengthens procedural compliance. Our analysis also reveals that workflow-based methods, despite accessing external knowledge, can exhibit lower processoriented scores than direct reasoning, indicating that unverified retrieval may actively harm procedural correctness. In courtroom simulation scenarios, LawThinker achieves the highest stage completion rates across all civil and criminal trial phases, demonstrating its ability to maintain procedurally grounded reasoning over long interaction horizons. Further evaluations on three static benchmarks, LawBench, LexEval, and UniLaw-R1-Eval, show an average accuracy improvement of approximately 6% over direct reasoning, confirming that the Explore-Verify-Memorize strategy generalizes beyond dynamic settings to conventional legal reasoning tasks. In summary, our contributions are as follows: (1) We propose LawThinker, an autonomous legal research agent that adopts an Explore-Verify-Memorize strategy for dynamic judicial environments. In particular, we design DeepVerifier module that verifies each exploration step along three dimensions of knowledge accuracy, fact-law relevance, and procedural compliance, preventing error accumulation throughout reasoning. (2) We design 15 legal tools spanning exploration, verification, and memorization, enabling the agent to navigate the legal knowledge space, validate intermediate reasoning steps, and reuse verified information across long-horizon tasks. (3) Extensive experiments on dynamic benchmark covering six judicial scenarios and three static benchmarks demonstrate that LawThinker significantly outperforms existing methods on both outcome and process-oriented metrics, with particularly strong gains in procedural compliance and courtroom stage completion."
        },
        {
            "title": "2 Related Work\n2.1 LLM Reasoning and Verification\nRecent advances in Large Language Models (LLMs) have substan-\ntially enhanced reasoning capabilities in the legal domain [7, 30,\n31, 67]. Beyond general chain-of-thought (CoT) [46], several stud-\nies [11, 21, 48, 64] have introduced legal syllogism reasoning to\nimprove performance on legal tasks. Specialized frameworks have\nalso been proposed to address the inherent complexity of judicial\npractice [43, 45, 58, 59, 65]. For instance, ADAPT [10] introduces an\nAsk-Discriminate-Predict reasoning framework inspired by human\njudicial decision-making. Most recently, legal reasoning research\nhas shifted toward agentic paradigms [40, 44, 66]. GLARE [55] pro-\nposes an agentic legal reasoning framework that dynamically inter-\nacts with external modules to retrieve and apply legal knowledge\nduring decision-making. This transition toward agentic reasoning\nhas emerged as a leading paradigm in recent research, where mod-\nels no longer reason over static prompts alone, but actively navigate\nthe legal knowledge space to reach final judgments [33, 47].",
            "content": "LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY Figure 2: Overview of our autonomous legal research agent LawThinker, which adopts an Explore-Verify-Memorize strategy, integrating iterative exploration with explicit verification during reasoning and interacting closely with memory module. Despite these advances, ensuring the accuracy and procedural compliance of the reasoning process itself remains significant challenge. LegalReasoner [35] introduces step-wise reasoning for legal judgment prediction by identifying dispute points and employs process verifier to validate each reasoning step. However, the verification primarily focuses on whether intermediate steps contribute to correct final conclusion, rather than whether the reasoning process adheres to legally prescribed procedures. Recent benchmarks [20, 25, 27] reveal that evaluating only final answers is insufficient, and that the quality and validity of intermediate reasoning steps are critical to reliable legal decision-making."
        },
        {
            "title": "3 Methodology\n3.1 Task Formulation\nWe consider interactive legal tasks situated in dynamic judicial\nenvironments. A legal agent assumes a designated judicial role,\nsuch as a legal trainee, lawyer, or judge, and engages in multi-\nround dialogues with other participants in the environment. At\neach dialogue round ùë°, given the dialogue history ùêªùë° and a set of\navailable tools T , the agent is required to generate a reasoning\nchain ùëÖùë° along with a corresponding response ùëüùë° . The interaction\ncontinues until the task objective is fulfilled, which may involve\nresolving a legal inquiry, producing a formal legal document, or\ncompleting a judicial procedure.",
            "content": "The agent does not receive all relevant information at once. Instead, it must incrementally gather knowledge and context across dialogue rounds, and the reasoning at round ùë° depends on information accumulated from all previous rounds. This requires the agent to maintain accurate and verified knowledge over long interaction horizons, as errors introduced in early rounds can propagate and compromise the entire subsequent reasoning trajectory. To meet this requirement, we decompose the reasoning chain ùëÖùë° into sequence of steps {ùë†1, ùë†2, ..., ùë†ùëõ }. At each step ùë†ùëñ , the agent may invoke an exploration tool and obtain result ùëíùëñ . verification module then takes (ùë†ùëñ, ùëíùëñ, ùêªùë° ) as input and produces structured assessment ùë£ùëñ = (ùëéùëñ, relùëñ, ùëùùëñ ), representing knowledge accuracy, factlaw relevance, and procedural compliance respectively. Based on ùë£ùëñ , the agent decides whether to accept the current result, revise its reasoning, or reformulate the query and re-explore. This step-level verification mechanism is the foundation of our framework, and we describe its design in detail in the following sections."
        },
        {
            "title": "3.2 Overview of the LawThinker Framework\nWe propose LawThinker, a legal research agent framework that\nadopts an Explore-Verify-Memorize strategy. The central design\nprinciple is to treat knowledge exploration and verification as an\nenforced atomic operation at the system level. Rather than rely-\ning on the model to spontaneously reflect on its own reasoning,",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. Figure 3: Overview of tools used in exploration, verification, and memorization. Details of these tools can be found in Table 3. LawThinker guarantees that every exploration step is immediately followed by dedicated verification module. This design prevents errors introduced during knowledge retrieval from propagating into subsequent reasoning steps. An overview of the framework is illustrated in Figure 2. Given legal task, the agent begins its reasoning process and invokes exploration tools when it encounters knowledge gaps. The system controller intercepts the result of each exploration and activates DeepVerifier module, which examines the retrieved information and returns structured assessment to the main reasoning chain. Based on this feedback, the agent determines whether to proceed with the current information, revise its analysis, or reformulate the query and re-explore. This explore-verify cycle may iterate multiple times within single dialogue round until sufficient verified knowledge has been gathered, at which point the agent generates the final response. The technical design of the DeepVerifier is detailed in Section 3.4. The framework is built on three critical design decisions. First, the verification step is enforced by the system controller rather than left to the models discretion. The DeepVerifier is activated after every exploration, ensuring that no retrieved information enters the reasoning chain without examination. Second, verification operates at the granularity of individual reasoning steps. We define step as the reasoning segment from the initiation of reasoning to the invocation of single exploration tool. Verifying at this fine granularity allows the system to detect and correct errors before they compound across steps. Third, the framework incorporates memory module that persistently stores validated legal knowledge and key case context. Both the main reasoning agent and the DeepVerifier can write to memory, ensuring that stored information has passed through verification. The agent retrieves this information in subsequent rounds to avoid redundant exploration. The memory mechanism is described in Section 3.5."
        },
        {
            "title": "3.3 Legal Knowledge Exploration\nLegal knowledge is not a flat collection of documents but a multi-\nsource, densely connected space. Statutes map to specific charges,\nrelated statutes share semantic or structural proximity, and prece-\ndents provide empirical references for statutory application. A sin-\ngle retrieval method is insufficient to navigate this interconnected\nspace. To support comprehensive knowledge acquisition, we con-\nstruct a legal knowledge base consisting of a law article corpus\n(55,347 provisions covering Civil Law, Criminal Law, and judicial\ninterpretations) [62], a charge corpus (346 standardized criminal\ncharges), a case corpus (criminal and civil judgments from China\nJudgments Online), and a law-charge mapping dictionary that ex-\nplicitly links charges to their relevant articles.",
            "content": "Based on these resources, we design seven exploration tools organized into three categories. The design is motivated by the observation that legal knowledge forms densely connected space rather than flat document collection. single retrieval query may locate relevant statute, but legal reasoning often requires traversing from statutes to related provisions, from provisions to applicable charges, and from charges to precedents. The tools of three categories are designed to support these traversal patterns. The first category supports statute and charge exploration. It enables the agent to retrieve relevant provisions via dense retrieval (e.g., BGE [50]), discover related articles through semantic and structural similarity, and expand candidate charges by identifying confusing or closely related offenses [55]. The second category supports precedent exploration. The agent retrieves similar cases via the SAILER [26] retriever to align its reasoning with established judicial practice. The third category provides task-guided support LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY for structured tasks, including template retrieval and writing plan generation for document drafting, as well as procedure retrieval for courtroom simulation. Detailed descriptions of all tools are provided in Figure 3 and Appendix A. The invocation of these tools is fully autonomous. The agent decides which tools to call and what queries to construct based on the knowledge gaps it perceives during reasoning, without predefined trigger rules or fixed tool sequences."
        },
        {
            "title": "3.4 DeepVerifier: Hybrid Step-Level Verification\nA natural approach to verifying intermediate reasoning is to let the\nmodel reflect on its own output [32, 35]. However, self-reflection is\nlimited in two ways, and the DeepVerifier is designed to address\neach of them through specific mechanisms. First, self-reflection\ncannot access external ground truth. When the model generates a\nfabricated statute, it has no means to detect this fabrication because\nthe error originates from its own generation. The DeepVerifier ad-\ndresses this limitation through grounded verification tools such as\nLaw Article Content Check, which queries authoritative databases\nto retrieve the full text of a cited provision. This provides a hard\nfactual constraint that is independent of the model‚Äôs parametric\nknowledge. Second, self-reflection operates within the same rea-\nsoning context that produced the error. The model tends to treat its\nprevious outputs as established premises and is unlikely to question\nassumptions already absorbed into the chain.",
            "content": "The DeepVerifier mitigates this by operating under dedicated verification prompt with distinct role definition. Although the DeepVerifier shares the same base model, the separated prompt context prevents it from inheriting the reasoning assumptions of the main agent. It evaluates each exploration result from an independent, critical perspective rather than continuing the generative flow. Given the current reasoning step ùë†ùëñ , its associated exploration result ùëíùëñ , and the dialogue history ùêªùë° , the DeepVerifier selects and invokes verification tools along three dimensions that correspond to the core requirements of legal reasoning. (1) Knowledge Accuracy. Legal reasoning requires that cited statutes are authentic and complete. LLMs frequently hallucinate statutory content, such as fabricating provisions or mismatching article numbers with their actual text [17, 57]. To address this, Law Article Content Check directly queries the law article database to retrieve the authoritative full text of cited provision, providing hard factual constraint that is independent of the models reasoning capability. When initial retrieval results are unsatisfactory, Search Query Rewrite refines the original query to improve subsequent exploration quality, ensuring that the agent works with accurate and relevant legal knowledge. (2) Fact-Law Relevance. correct statute citation is insufficient if the provision does not apply to the case at hand. Fact-Law Relevance Check analyzes whether cited provisions are applicable to the given case by examining the correspondence between case facts and the four constitutive elements of crime [31], including subject, object, subjective aspect, and objective aspect. Charge-Law Consistency Check further verifies whether predicted charges are legally supported by the corresponding statutes. Together, these two tools ensure that the mapping between legal knowledge and case facts is logically sound. (3) Procedural Compliance. Legal tasks often impose strict requirements on the format and sequence of outputs. Procedure Check examines whether the agent follows the correct procedural sequence in courtroom simulation and interacts appropriately with different participants at each stage. Document Format Check analyzes the structural completeness of generated legal documents and provides revision suggestions. These two tools ensure that the agents outputs conform to legally prescribed norms, covering both judicial procedures and document standards. The six tools combine two verification strategies. Some tools, such as Law Article Content Check, perform grounded verification by checking information against authoritative databases. Others, such as Fact-Law Relevance Check, perform analytical verification through specialized LLM reasoning with well-defined evaluation criteria. This hybrid design allows the system to eliminate factual errors through hard constraints while handling complex legal judgments that require deeper reasoning. After all checks complete, the structured results are fed back into the main reasoning chain, and the agent autonomously decides whether to proceed, revise its analysis, or initiate new exploration."
        },
        {
            "title": "3.5 Memory Mechanism\nAs discussed above, information in dynamic legal tasks is acquired\nprogressively across dialogue rounds, and the agent must retain\nverified knowledge for reuse in later rounds. LawThinker addresses\nthis through a memory module that maintains two categories of\ncontent. Legal Knowledge Memory stores validated legal informa-\ntion, including law articles, related charges, precedents, and legal\ninterpretations that have been explored and verified during the\nreasoning process. Case Context Memory stores task-specific in-\nformation, including dialogue history, parties‚Äô identities, disputed\nissues, collected evidence, and trial workflow progress. The sep-\naration of these two categories reflects their distinct roles: legal\nknowledge provides the normative basis for reasoning, while case\ncontext grounds the reasoning in the specific scenario.",
            "content": "Interaction with the memory module is implemented through two dedicated tools, memory_store and memory_fetch. The agent autonomously decides what to store and when to retrieve based on its reasoning needs. Both the main reasoning agent and the DeepVerifier can write to memory. The main agent stores task-specific context such as newly identified disputed issues or evidence. The DeepVerifier stores legal knowledge that has been validated during the verification process, such as confirmed statute content or verified fact-law mappings. This dual-channel mechanism ensures that all stored legal knowledge has been examined before entering memory, so that subsequent retrieval does not reintroduce unverified information into the reasoning chain."
        },
        {
            "title": "3.6 Legal Agent in Various Scenarios\nThe Explore-Verify-Memorize strategy provides a unified reasoning\nframework, but different judicial scenarios vary significantly in\ntask complexity, interaction depth, and procedural constraints. The\nframework adapts to these differences through varying tool combi-\nnations and verification priorities. We describe three representative\nlevels in order of increasing difficulty.",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. Level I: Knowledge Questioning and Legal Consultation. At this level, the legal agent acts as legal trainee and responds to progressive questions from the general public regarding legal knowledge or specific cases. The primary challenge is knowledge accuracy. Users often ask about specific provisions or legal concepts, and any fabricated or misquoted content directly undermines the consultation quality. The agent therefore relies heavily on exploration tools for statute and case retrieval, with verification focused on confirming the authenticity of cited provisions and their relevance to the query. Level II: Complaint and Defense Drafting. At this level, the legal agent acts as lawyer and engages in multi-turn interactions with litigants to draft formal legal documents. The primary challenge shifts to information completeness and structural compliance. complaint or defense document must contain specific required components, such as party information, claims, evidence, and legal reasoning, organized in prescribed format. The agent collects these components across multiple dialogue turns and progressively assembles complete document. Verification at this level focuses on document format checks to ensure that no required section is missing and the overall structure meets professional standards. Level III: Civil and Criminal Courts. At the highest level, the agent acts as judge and interacts with multiple parties to conduct complete courtroom process, ultimately delivering legally valid judgment. The primary challenge is procedural compliance over long interaction horizon. civil trial consists of multiple sequential stages, including preparation, investigation, debate, and mediation, each with mandatory actions that must be executed in the correct order. The agent must track which stages have been completed, ensure that no mandatory step is skipped, and maintain consistency between evolving case facts and the applied legal grounds. Verification at this level relies on procedure checks to monitor stage completion and prevent procedural violations."
        },
        {
            "title": "4 Experiments\n4.1 Experimental Setup",
            "content": "Dataset. To evaluate the performance of LawThinker in dynamic and interactive judicial scenarios, we conduct comprehensive evaluation on J1-EVAL [20], which comprises 508 distinct real-world judicial environments spanning six scenario types, organized into three hierarchical levels according to task complexity and interaction depth. The benchmark covers topics involving diverse entities, multiple types of civil disputes, and wide range of criminal offense categories, enabling an effective assessment of LawThinkers adaptability across different legal contexts. Metrics. For evaluation metrics, we adopt the same measures as those used in J1-EVAL. Given the varying complexity across judicial environments, it adopts fine-grained, scenario-specific evaluations, combining rule-based and LLM-based approaches. For LevelI, which includes both binary questions and open-ended queries, we report the average score across these two question types. For Level-II, the Format-Following Score (FOR) assesses adherence to document structure, focusing on the ordering and presence of required components, while the Document Score (DOC) evaluates the accuracy and completeness of the content, such as the information of plaintiff and defendant, claims, and supporting evidence. For Level-III, the Procedural-Following Score (PFS) measures the completeness of different stages in court proceedings; the Civil Judgment Score (JUD) evaluates the accuracy of judicial decisions; Crime Accuracy (CRI) assesses the correctness of predicted charges; VER measures the accuracy of fines and sentences using log-distance; and Law Accuracy (LAW) evaluates the completeness and correctness of cited legal provisions. Overall, these metrics can be broadly categorized into outcome-oriented and process-oriented evaluations, enabling comprehensive assessment of both final results and intermediate reasoning processes. Baselines. We compare our method against three categories of baseline approaches. (1) Direct Reasoning: We employ different types of models to drive the legal agent, including both generalpurpose multilingual LLMs and legal-specific LLMs: Qwen2.5-7BInstruct [54], Qwen3-8B/14B/32B [53], Ministral-8B-Instruct-2410 [5], GLM-4-9B-Chat [16], Gemma3-12B-IT [37]; ChatLaw2-7B [8], and LawLLM-7B [60, 61]. (2) Workflow-based Methods: ReAct [56] interleaves reasoning with task-specific actions, Plan-and-Solve [41] first decomposes the overall task into sequence of manageable subtasks and then solves them according to predefined plan, and Plan-and-Execute [39] formulates multi-step plan that is executed sequentially, with the model revisiting and refining the plan upon task completion. (3) Autonomous Tool Usage within Reasoning: Search-o1 [29] augments reasoning by dynamically retrieving external knowledge when the model encounters uncertain or ambiguous information. For fair comparison, we provide all baseline methods with access to exploration tools. Implementation Details. We use Qwen3-32B to drive the environment, where it plays the role of non-critical agents such as general public, plaintiffs and defendants. To avoid unbounded interaction loops due to model limitations while ensuring sufficient conversational coverage, we impose an upper bound on the number of interaction rounds for each scenario: 15 for knowledge questioning, 10 for legal consultation, 30 for complaint drafting, 30 for defence drafting, 60 for civil court, and 50 for criminal court. For LLM-based metric evaluation, we adopt GPT-4o [1]. All experiments are conducted on two NVIDIA A800-80GB GPUs."
        },
        {
            "title": "4.2 Main Result",
            "content": "Overall Performance. The main results are presented in Table 1, demonstrating the superiority of LawThinker in dynamic judicial environments. More detailed metrics are reported in Appendix B. (1) Direct Reasoning exhibits consistently limited performance across all tasks, with particularly poor results in more complex scenarios. This is largely due to severe hallucination issues in legal reasoning, such as fabricating legal provisions. Since direct reasoning relies solely on the models internal knowledge, its overall accuracy remains below 50%. In Level-I, users progressively query specific legal knowledge or case details, placing high demands on comprehensive legal understanding. Models fail to explore sufficiently broad legal knowledge and lack mechanisms to verify intermediate reasoning steps, leading to unreliable conclusions. Level-II and Level-III further require procedural awareness, Methods Direct Reasoning Multilingual LLMs Qwen2.5-7B Ministral-8B Qwen3-8B GLM4-9B Gemma3-12B Qwen3-14B Qwen3-32B Legal-specific LLMs ChatLaw2-7B LawLLM-7B LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY Table 1: Performance comparisons on J1-EVAL. KQ: Knowledge Questioning, LC: Legal Consultation, CD: Complaint Drafting, DD: Defence Drafting, CI: Civil Court, CR: Criminal Court. Best results are in bold and - denotes failure to complete the task. Level-I Level-II Level-III KQ LC CD DD CI CR Avg. Avg. FOR DOC FOR DOC PFS JUD REA LAW PFS CRI VER REA LAW 52.9 49.1 52.8 55.6 51.2 57.7 59.6 52.9 48.7 40.1 32.1 40.3 34.2 33.5 43.3 47.4 36.5 35.3 73.1 50.5 7.5 52.5 57.2 69.3 71.4 30.3 74. 76.9 65.3 57.9 67.0 77.0 77.5 66.4 24.0 69.9 67.5 68.5 62.8 26.5 40.1 12.2 57.0 11.8 32.6 30.8 15.8 26.5 41.2 42.7 40. 50.4 52.8 40.7 54.8 51.6 59.0 51.0 18.9 43.6 54.5 59.0 55.4 23.4 15.1 12.9 20.8 42.7 6.8 42.5 3.7 3.4 39.2 38.0 40. 9.1 5.7 9.9 5.9 17.7 4.1 16.7 5.6 - 21.6 23.4 23.8 20.0 10.1 23.1 16.5 32.8 5.9 35.7 7.8 - 38.3 38.7 42. 8.5 4.3 14.7 4.7 14.8 3.9 17.3 - - 20.8 19.4 20.5 22.4 5.7 22.0 31.4 33.2 30.7 33.7 3.7 2.2 24.3 23.8 24. 81.2 11.6 49.3 66.7 72.5 55.1 69.6 24.6 13.0 75.4 79.7 75.4 68.1 16.2 46.5 54.4 61.3 49.6 57.8 20.9 7.9 62.1 65.3 57. 54.8 7.4 35.1 40.3 44.5 44.8 55.5 11.9 10.0 58.8 57.7 58.3 23.9 2.7 25.9 17.5 18.5 30.8 33.9 2.2 1.6 34.5 36.6 34. Workflow-based Methods (with Qwen3-32B) ReAct Plan-and-Solve Plan-and-Execute 61.3 60.9 61.9 46.8 46.4 47.9 50.1 53.3 53.1 Autonomous Tool Usage within Reasoning (with Qwen3-32B) 42.7 63.5 Search-o1 81.4 64.1 LawThinker (ours) 53.3 87. 48.4 52.4 70.3 75.6 54.9 60.3 40.6 50.3 20.5 28.5 36.3 44. 21.2 23.2 28.9 41.5 84.1 84.1 67.1 69.7 60.9 63.9 39.5 40. Table 2: Accuracy evaluation on different legal benchmarks. Method LawBench LexEval Unilaw-R1-Eval Avg.(%) Direct Reasoning ReAct Plan-and-Solve Plan-and-Execute Search-o1 LawThinker 52.0 56.2 57.9 57.0 55.3 62.0 67.1 68.6 66.6 68.1 69.0 71. 48.3 51.3 50.3 52.0 49.7 53.7 55.8 58.7 58.3 59.0 58.0 62.3 yet models struggle to follow legally grounded procedures, yielding low scores on process-oriented metrics. This limitation is particularly evident in legal-specific LLMs such as ChatLaw and LawLLM, as weaker interaction abilities and limited long-horizon contextual reasoning hinder their performance in dynamic settings. (2) Workflow-based Methods outperform direct reasoning by incorporating external knowledge, but their gains are constrained by the absence of explicit verification mechanisms, which often introduce noise. Approaches such as ReAct interleave reasoning with actions to acquire richer legal information, partially mitigating hallucinations and achieving over 13% improvement in overall accuracy compared to direct reasoning. However, without systematic checking, these methods exhibit degraded procedural compliance. Notably, in the Complaint Drafting scenario, their performance can even underperform direct reasoning, highlighting the necessity of verifying intermediate reasoning steps rather than simply increasing access to external information. Figure 4: Ablation study across six legal scenarios. (3) LawThinker consistently outperforms all baselines across scenarios, validating the effectiveness of the Explore-VerifyMemorize strategy. Overall, LawThinker achieves 24% improvement over direct reasoning and an 11% gain over workflow-based methods, with particularly strong performance on process-oriented metrics such as FOR and PFS, highlighting the value of step-level verification for improving process accuracy and procedural compliance. Search-o1 performs an in-depth analysis of retrieved documents, but its reasoning is mainly based on self-reflection, with limited support from verified external knowledge. We observe that Defence Drafting is more challenging than Complaint Drafting, as it requires comprehensively gathering relevant information and constructing more complex counterarguments in response to the opposing partys claims. In Level-III, LawThinker more effectively Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. Figure 5: Performance with outcome-oriented and processoriented metrics across models and reasoning paradigms. Figure 6: Court-stage completion rates across different methods. Left: Civil Court. Right: Criminal Court. completes each stage of court proceedings and attains higher accuracy in judgment generation, legal reasoning, and statute citation. Overall, LawThinker demonstrates robust performance across judicial environments ranging from inquiry to adjudication, offering promising evidence toward the development of reliable and comprehensive legal intelligence. Generalization to Static Scenarios. To assess the generalization ability of LawThinker in static legal scenarios, we evaluate it on three Chinese legal domain benchmarks: LexEval [28], LawBench [15], and UniLaw-R1-Eval [4]. Detailed descriptions of these benchmarks are provided in Appendix C, and evaluation is conducted using the original metrics defined by each benchmark. We conduct comparative experiments between LawThinker and other reasoning methods based on the Qwen3-32B model. As shown in Table 2, LawThinker consistently achieves the best performance across all benchmarks, delivering an average accuracy improvement of approximately 6% over direct reasoning baselines. These results further validate the effectiveness of the proposed ExploreVerify-Memorize strategy for legal tasks. By autonomously invoking specialized legal tools, the model is able to acquire comprehensive legal knowledge, while the DeepVerifier module ensures the accuracy of each reasoning step and adherence to legal procedures. Overall, our approach aligns well with the requirements of legal practice, where both static and dynamic scenarios demand high degree of rigor and reliability in the reasoning process."
        },
        {
            "title": "4.3 Ablation Study\nTo examine the contributions of exploration, verification, and mem-\norization in LawThinker, we conduct ablation studies across six\nscenarios. The results are shown in Figure 4.",
            "content": "(1) DeepVerifier is critical to overall performance. Removing DeepVerifier leads to consistent performance drops across all scenarios, indicating that verifying intermediate reasoning steps is essential for legal tasks. This is particularly pronounced in knowledgeintensive scenarios such as KQ and LC, as well as in CR, where strict procedural compliance is required. These results show that explicit verification effectively reduces hallucinations and reasoning errors. (2) The memory module substantially benefits long-horizon tasks. In document drafting and court simulation scenarios, the agent must retain information gathered across multiple turns to generate final documents or judgments. Consequently, removing the memory module causes more severe degradation in these tasks. In contrast, KQ relies mainly on information from the current turn and is therefore less sensitive to historical memory. (3) The Explore-Verify-Memorize strategy yields the largest gains. When exploration, verification, or memorization is absent, the model fails to acquire accurate external knowledge and cannot support long-horizon reasoning, leading to performance degradation across all scenarios compared to LawThinker. This confirms that the synergy of exploration, verification, and memorization is crucial for robust legal reasoning."
        },
        {
            "title": "4.4 Quantitative Analysis\nTo comprehensively evaluate the performance of LawThinker, we\nconduct multiple quantitative experiments, analyzing both outcome\naccuracy and procedural compliance.",
            "content": "Comprehensive analysis of outcome and process performance. In legal reasoning tasks, the correctness of final outcomes is as critical as the compliance of the reasoning process. We evaluate the performance of different models and methods along these two dimensions, as illustrated in Figure 5. LawThinker consistently demonstrates strong performance in both OO and PO, indicating that it effectively improves not only decision accuracy but also procedural compliance throughout the reasoning chain. Within the same reasoning paradigm, larger models generally achieve higher OO and PO scores. However, even large-scale models exhibit constrained PO performance in the absence of explicit verification. Although workflow-based methods often attain higher OO scores by incorporating external knowledge, their PO scores can fall below those of direct reasoning, suggesting that introducing unverified external information may enhance outcome accuracy while undermining procedural correctness in complex legal settings. Analysis of court process completeness. To examine procedural completeness across methods in civil and criminal court settings, we compute stage completion rates and visualize them using heatmaps, as shown in Figure 6. LawThinker achieves the highest scores across all four civil stages and both criminal stages, reflecting its strong ability to adhere to judicial procedures. In contrast, most LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY other methods score below 25% in the Preparation and Investigation stages, highlighting lack of explicit control in fine-grained processes such as information gathering and evidence verification. Notably, all methods exceed 50% completion in the Debate stage, indicating that argumentative exchanges around disputed issues are relatively easier to generate. Moreover, most approaches perform better in criminal Investigation than in civil cases, likely due to the higher degree of standardization in criminal procedures, which reduces the likelihood of omissions. Interestingly, workflow-based methods such as ReAct underperform direct reasoning, underscoring that exploration without verification is insufficient to ensure procedural compliance. Overall, these results emphasize the critical role of deep verification in improving judicial process completeness, and LawThinker demonstrates clear and consistent advantages across both civil and criminal court scenarios."
        },
        {
            "title": "5 Conclusion\nIn this work, we propose LawThinker, an autonomous legal research\nagent designed for dynamic judicial environments. LawThinker\nadopts an Explore-Verify-Memorize strategy that integrates itera-\ntive knowledge exploration with explicit verification. We further\ndesign 15 specialized tools spanning exploration, verification, and\nmemorization. Extensive experiments on a dynamic legal bench-\nmark and three static benchmarks demonstrate that LawThinker\nsignificantly outperforms existing methods, achieving both accu-\nrate outcomes and legally compliant reasoning processes.",
            "content": "References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. arXiv preprint arXiv:2402.00157 (2024). [3] Andrew Blair-Stanek and Benjamin Van Durme. 2025. Llms provide unstable answers to legal questions. In Proceedings of the Twentieth International Conference on Artificial Intelligence and Law. 425429. [4] Hua Cai, Shuang Zhao, Liang Zhang, Xuli Shen, Qing Xu, Weilin Shen, Zihao Wen, and Tianke Ban. 2025. Unilaw-r1: large language model for legal reasoning with reinforcement learning and iterative inference. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing. 1812818142. [5] Devendra Singh Chaplot. 2023. Albert q. jiang, alexandre sablayrolles, arthur mensch, chris bamford, devendra singh chaplot, diego de las casas, florian bressand, gianna lengyel, guillaume lample, lucile saulnier, l√©lio renard lavaud, marie-anne lachaux, pierre stock, teven le scao, thibaut lavril, thomas wang, timoth√©e lacroix, william el sayed. arXiv preprint arXiv:2310.06825 3 (2023). [6] Junjie Chen, Haitao Li, Minghao Qin, Yujia Zhou, Yanxue Ren, Wuyue Wang, Yiqun Liu, Yueyue Wu, and Qingyao Ai. 2025. Simulating Dispute Mediation with LLM-Based Agents for Legal Research. arXiv preprint arXiv:2509.06586 (2025). [7] Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567 (2025). [8] Jiaxi Cui, Munan Ning, Zongjian Li, Bohua Chen, Yang Yan, Hao Li, Bin Ling, Yonghong Tian, and Li Yuan. 2023. Chatlaw: multi-agent collaborative legal assistant with knowledge graph enhanced mixture-of-experts large language model. arXiv preprint arXiv:2306.16092 (2023). [9] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel Ho. 2024. Large legal fictions: Profiling legal hallucinations in large language models. Journal of Legal Analysis 16, 1 (2024), 6493. [10] Chenlong Deng, Kelong Mao, Yuyao Zhang, and Zhicheng Dou. 2024. Enabling discriminative reasoning in llms for legal judgment prediction. arXiv preprint arXiv:2407.01964 (2024). [11] Wentao Deng, Jiahuan Pei, Keyi Kong, Zhe Chen, Furu Wei, Yujun Li, Zhaochun Ren, Zhumin Chen, and Pengjie Ren. 2023. Syllogistic reasoning for legal judgment analysis. In Proceedings of the 2023 conference on empirical methods in natural language processing. 1399714009. [12] Hao Ding, Ziwei Fan, Ingo Guehring, Gaurav Gupta, Wooseok Ha, Jun Huan, Linbo Liu, Behrooz Omidvar-Tehrani, Shiqi Wang, and Hao Zhou. 2024. Reasoning and planning with large language models in code development. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 64806490. [13] Run-Ze Fan, Zengzhi Wang, and Pengfei Liu. 2025. Megascience: Pushing the frontiers of post-training datasets for science reasoning. arXiv preprint arXiv:2507.16812 (2025). [14] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. 2025. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407 (2025). [15] Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Shen, et al. 2024. Lawbench: Benchmarking legal knowledge of large language models. In Proceedings of the 2024 conference on empirical methods in natural language processing. 79337962. [16] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, et al. 2024. Chatglm: family of large language models from glm-130b to glm-4 all tools. arXiv preprint arXiv:2406.12793 (2024). [17] Xue Guo, Yuting Huang, Bin Wei, Kun Kuang, Yiquan Wu, Leilei Gan, Xianshan Huang, and Xianglin Dong. 2025. Specialized or general AI? comparative evaluation of LLMs performance in legal tasks. Artificial Intelligence and Law (2025), 137. [18] Jennifer Haase and Sebastian Pokutta. 2025. Beyond Static Responses: MultiAgent LLM Systems as New Paradigm for Social Science Research. arXiv preprint arXiv:2506.01839 (2025). [19] Kevin Halim, Sin Teo, Ruitao Feng, Zhenpeng Chen, Yang Gu, Chong Wang, and Yang Liu. 2025. Study on Thinking Patterns of Large Reasoning Models in Code Generation. arXiv preprint arXiv:2509.13758 (2025). [20] Zheng Jia, Shengbin Yue, Wei Chen, Siyuan Wang, Yidong Liu, Yun Song, and Zhongyu Wei. 2025. Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments. arXiv preprint arXiv:2507.04037 (2025). [21] Cong Jiang and Xiaolei Yang. 2023. Legal syllogism prompting: Teaching large language models for legal judgment prediction. In Proceedings of the nineteenth international conference on artificial intelligence and law. 417421. [22] Cong Jiang and Xiaolei Yang. 2025. Agentsbench: multi-agent llm simulation framework for legal judgment prediction. Systems 13, 8 (2025), 641. [23] Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim. 2024. survey on large language models for code generation. ACM Transactions on Software Engineering and Methodology (2024). [24] Xue Jiang, Yihong Dong, Lecheng Wang, Zheng Fang, Qiwei Shang, Ge Li, Zhi Jin, and Wenpin Jiao. 2024. Self-planning code generation with large language models. ACM Transactions on Software Engineering and Methodology 33, 7 (2024), 130. [25] Huihao Jing, Wenbin Hu, Hongyu Luo, Jianhui Yang, Wei Fan, Haoran Li, and Yangqiu Song. 2025. MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning. arXiv preprint arXiv:2509.24922 (2025). [26] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Yueyue Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2023. SAILER: structure-aware pre-trained language model for legal case retrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval. 10351044. [27] Haitao Li, Junjie Chen, Jingli Yang, Qingyao Ai, Wei Jia, Youfeng Liu, Kai Lin, Yueyue Wu, Guozhi Yuan, Yiran Hu, et al. 2025. Legalagentbench: Evaluating llm agents in legal domain. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 23222344. [28] Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. 2024. Lexeval: comprehensive chinese legal benchmark for evaluating large language models. Advances in Neural Information Processing Systems 37 (2024), 2506125094. [29] Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025. Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366 (2025). [30] Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, et al. 2025. From system 1 to system 2: survey of reasoning large language models. arXiv preprint arXiv:2502.17419 (2025). [31] Yue Liu, Jiaying Wu, Yufei He, Ruihan Gong, Jun Xia, Liang Li, Hongcheng Gao, Hongyu Chen, Baolong Bi, Jiaheng Zhang, et al. 2025. Efficient inference for large reasoning models: survey. arXiv preprint arXiv:2503.23077 (2025). [32] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems 36 (2023), 4653446594. [33] Catherine Gage OGrady and Casey OGrady. 2024. Agentic workflows in the practice of lawai agents as ethics counsel. Arizona Legal Studies Discussion Paper (2024), 2503. Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. prediction. Information Processing & Management 63, 1 (2026), 104319. [60] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, Xuanjing Huang, and Zhongyu Wei. 2023. DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services. arXiv:2309.11325 [cs.CL] [61] Shengbin Yue, Shujun Liu, Yuxuan Zhou, Chenchen Shen, Siyuan Wang, Yao Xiao, Bingxuan Li, Yun Song, Xiaoyu Shen, Wei Chen, et al. 2024. LawLLM: Intelligent Legal System with Legal Reasoning and Verifiable Retrieval. In International Conference on Database Systems for Advanced Applications. Springer, 304321. [62] Kaiyuan Zhang, Jiaqi Li, Yueyue Wu, Haitao Li, Cheng Luo, Shaokun Zou, Yujia Zhou, Weihang Su, Qingyao Ai, and Yiqun Liu. 2025. Chinese Court Simulation with LLM-Based Agent System. arXiv preprint arXiv:2508.17322 (2025). [63] Kaiyuan Zhang, Zhaoxi Li, and Xuancheng Li. [n. d.]. Sim-Court: Simulation of Court using LLM-based Agents. ([n. d.]). [64] Kepu Zhang, Weijie Yu, Zhongxiang Sun, and Jun Xu. 2025. Syler: framework for explicit syllogistic legal reasoning in large language models. In Proceedings of the 34th ACM International Conference on Information and Knowledge Management. 41174127. [65] Yunong Zhang, Xiao Wei, and Hang Yu. 2024. HD-LJP: hierarchical dependencybased legal judgment prediction framework for multi-task learning. KnowledgeBased Systems 299 (2024), 112033. [66] Yujin Zhou, Chuxue Cao, Jinluan Yang, Lijun Wu, Conghui He, Sirui Han, and Yike Guo. 2026. LRAS: Advanced Legal Reasoning with Agentic Search. arXiv preprint arXiv:2601.07296 (2026). [67] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, et al. 2024. survey on efficient inference for large language models. arXiv preprint arXiv:2404.14294 (2024). [68] Haibin Zhu and MengChu Zhou. 2008. Role-based multi-agent systems. In Personalized information retrieval and access: concepts, methods and practices. IGI Global Scientific Publishing, 254285. [34] Xiao Peng and Liang Chen. 2024. Athena: Retrieval-augmented legal judgment prediction with large language models. arXiv preprint arXiv:2410.11195 (2024). [35] Weijie Shi, Han Zhu, Jiaming Ji, Mengze Li, Jipeng Zhang, Ruiyuan Zhang, Jia Zhu, Jiajie Xu, Sirui Han, and Yike Guo. 2025. LegalReasoner: Step-wised VerificationCorrection for Legal Judgment Reasoning. arXiv preprint arXiv:2506.07443 (2025). [36] Zhihong Sun, Chen Lyu, Bolun Li, Yao Wan, Hongyu Zhang, Ge Li, and Zhi Jin. 2024. Enhancing code generation performance of smaller models by distilling the reasoning ability of llms. arXiv preprint arXiv:2403.13271 (2024). [37] Gemma Team. 2025. Gemma 3. (2025). https://goo.gle/Gemma3Report [38] Qwen Team. 2024. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face (2024). [39] Oguzhan Topsakal and Tahir Cetin Akinci. 2023. Creating large language model applications utilizing langchain: primer on developing llm apps fast. In International conference on applied engineering and natural sciences, Vol. 1. 10501056. [40] Laura Underkuffler. 1998. Agentic and Conscientic Decisions in Law: Death and Other Cases. Notre Dame L. Rev. 74 (1998), 1713. [41] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chainof-thought reasoning by large language models. arXiv preprint arXiv:2305.04091 (2023). [42] Peng-Yuan Wang, Tian-Shuo Liu, Chenyang Wang, Ziniu Li, Yidi Wang, Shu Yan, Chengxing Jia, Xu-Hui Liu, Xinwei Chen, Jiacheng Xu, et al. 2025. survey on large language models for mathematical reasoning. Comput. Surveys (2025). [43] Xuran Wang, Xinguang Zhang, Vanessa Hoo, Zhouhang Shao, and Xuguang Zhang. 2024. LegalReasoner: multi-stage framework for legal judgment prediction via large language models and knowledge integration. IEEE Access (2024). [44] Ziqi Wang and Boqin Yuan. 2025. L-MARS: Legal multi-agent workflow with orchestrated reasoning and agentic search. arXiv preprint arXiv:2509.00761 (2025). [45] Bin Wei, Yaoyao Yu, Leilei Gan, and Fei Wu. 2025. An LLMs-based neuro-symbolic legal judgment prediction framework for civil cases. Artificial Intelligence and Law (2025), 135. [46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems 35 (2022), 2482424837. [47] Tianxin Wei, Ting-Wei Li, Zhining Liu, Xuying Ning, Ze Yang, Jiaru Zou, Zhichen Zeng, Ruizhong Qiu, Xiao Lin, Dongqi Fu, et al. 2026. Agentic Reasoning for Large Language Models. arXiv preprint arXiv:2601.12538 (2026). [48] Jerzy Wr√≥blewski. 1974. Legal syllogism and rationality of judicial decision. Rechtstheorie 5 (1974), 33. [49] Yiquan Wu, Siying Zhou, Yifei Liu, Weiming Lu, Xiaozhong Liu, Yating Zhang, Changlong Sun, Fei Wu, and Kun Kuang. 2023. Precedent-enhanced legal judgment prediction with llm and domain-model collaboration. arXiv preprint arXiv:2310.09241 (2023). [50] Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack: Packed resources for general chinese embeddings. In Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval. 641649. [51] Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, et al. 2025. Towards large reasoning models: survey of reinforced reasoning with large language models. arXiv preprint arXiv:2501.09686 (2025). [52] Yibo Yan, Shen Wang, Jiahao Huo, Jingheng Ye, Zhendong Chu, Xuming Hu, Philip Yu, Carla Gomes, Bart Selman, and Qingsong Wen. 2025. Position: Multimodal large language models can significantly advance scientific reasoning. arXiv preprint arXiv:2502.02871 (2025). [53] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388 (2025). [54] An Yang, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoyan Huang, Jiandong Jiang, Jianhong Tu, Jianwei Zhang, Jingren Zhou, et al. 2025. Qwen2. 5-1m technical report. arXiv preprint arXiv:2501.15383 (2025). [55] Xinyu Yang, Chenlong Deng, and Zhicheng Dou. 2025. GLARE: Agentic Reasoning for Legal Judgment Prediction. arXiv preprint arXiv:2508.16383 (2025). [56] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations. [57] Yaoyao Yu, Leilei Gan, Yinghao Hu, Bin Wei, Kun Kuang, and Fei Wu. 2025. Evaluating test-time scaling llms for legal reasoning: Openai o1, deepseek-r1, and beyond. arXiv preprint arXiv:2503.16040 (2025). [58] Weikang Yuan, Junjie Cao, Zhuoren Jiang, Yangyang Kang, Jun Lin, Kaisong Song, Tianqianjin Lin, Pengwei Yan, Changlong Sun, and Xiaozhong Liu. 2024. Can large language models grasp legal theories? enhance legal reasoning with insights from multi-agent collaboration. In Findings of the association for computational linguistics: EMNLP 2024. 75777597. [59] Weikang Yuan, Kaisong Song, Zhuoren Jiang, Junjie Cao, Yujie Zhang, Chengyuan Liu, Jun Lin, Ji Zhang, Kun Kuang, and Xiaozhong Liu. 2026. multi-agent framework with legal event logic graph for multi-defendant legal judgment LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY law article prediction. These tasks are organized into five categories: single-label classification (SLC), multi-label classification (MLC), regression, information extraction, and text generation, providing diverse and structured assessment framework. UniLaw-R1-Eval is constructed from the open-source JEC-QA dataset and proprietary data derived from the Chinese National Judicial Examination spanning 2015 to 2021, comprising 800 samples in total. The benchmark is divided into knowledge-based and case-based subsets. Since our focus is on knowledge-intensive legal reasoning, we select tasks 1-1, 1-2, 3-1, 3-2, and 3-4 from LexEval, 1-1, 1-2, 3-1, 3-2, and 3-6 from LawBench, and dominant domains from UniLaw-R1Eval, including criminal procedure law, labor law, commercial law, constitutional law, and jurisprudence. Prompts in LawThinker We provide detailed prompts in Tables 7, 8, 9, and 10. Detailed Description of Legal Tools To support long-horizon legal reasoning and sustained interaction across diverse judicial scenarios, the legal agent is equipped with suite of specialized tools spanning exploration, verification, and memorization. These tools enable the agent to actively retrieve and expand relevant legal knowledge, examine the validity and consistency of intermediate reasoning steps, and preserve contextual information across multi-turn interactions. Table 3 provides an overview of the tool taxonomy, including their core functions, inputs, and outputs. Additional overall performance We provide additional results of the overall performance in J1-EVAL. In Level I, Binary Accuracy (BIN) measures correctness on binary questions that require yes-or-no answer, while Non-Binary Accuracy (NBIN) evaluates the accuracy and completeness of responses to open-ended questions. In Level II, the quality of generated legal documents is assessed based on key components. For complaint documents, the evaluation covers information about the plaintiff (PLA) and the defendant (DEF), claims (CLA), evidence (EVI), as well as facts and reasoning (F&R). Similarly, defence documents are evaluated based on respondent information (RES), defence arguments (DEF), and supporting evidence (EVI). In Level III, completion of court stage requires the execution of all mandatory procedural actions. Specifically, civil trials are organized into five sequential stages: preparation, investigation, debate, mitigation, and decision. In contrast, criminal trials adopt simplified structure with three stages: preparation, investigation, and decision. Accordingly, court completeness is evaluated using stage-level completion (STA), action-level completion within each stage (ACT), and binary indicator (UNI). In the criminal court setting, judgment accuracy is further assessed by evaluating predicted fines (FINE) and sentences (SEN) using normalized logarithmic deviation. Together, these fine-grained metrics provide detailed assessment of both judgment accuracy and procedural compliance. As shown in Tables 4 and 5, our method demonstrates increasingly pronounced advantages in more complex court simulation scenarios. These results indicate that the proposed Explore-VerifyMemorize strategy effectively verifies intermediate reasoning steps, thereby ensuring procedural compliance and improving overall performance in judicial decision-making. Descriptions of Static Benchmarks LexEval adopts the Legal Cognitive Ability Taxonomy (LexCog) to systematically organize wide range of legal tasks. The taxonomy defines six core legal abilities, including Memorization, Understanding, Logical Inference, Discrimination, Generation, and Ethics, and covers 23 tasks with total of 14,150 questions. LexEval integrates data from existing legal benchmarks, real-world legal examination datasets, and newly annotated samples curated by legal experts, enabling comprehensive evaluation of LLMs legal cognitive capabilities. LawBench is designed to simulate judicial cognition along three dimensions: legal knowledge memorization, comprehension, and application. It evaluates large language models across 20 tasks, including statute recitation, legal knowledge question answering, and Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. Table 3: Overview of exploration, verification, and memorization tools used by the legal agent. Tool Name Description Function Inputs Outputs Exploration Tools Law Article Retrieval Law Article Recommendation Charge Expansion Case Retrieval Template Retrieval Writing Plan Generation Procedure Retrieval Checking Tools Law Article Content Check Fact & Law Relevance Check Charge & Law Consistency Check Search Query Rewrite Document Format Check Procedure Check Memory Tools Memory Storage Retrieve relevant law articles. Recommend similar law articles. Expand related criminal charges. Retrieve similar cases. Retrieve document templates. Generate document writing plan. Retrieve court procedures. law_retrieval law_recommendation charge_expansion case_retrieval template_retrieval plan_generation procedure_retrieval Query; top-ùëò Law article Charges; top-ùëò Case type; query Template type Document type Court type; stage Law articles Similar laws Expanded charges Similar cases Document template Writing plan Procedural steps Verify law article content. Check law applicability to facts. Check charge-law consistency. law_check fact_law_relevance_check crime_law_consistency_check Charge; law Law name; Case facts; law Verified content Relevance result Consistency result Rewrite law retrieval queries. Check document format. law_query_rewrite document_format_check Check procedural compliance. procedure_check Store knowledge or context. storeMemory Query; context Document document Court type type; Refined query Format feedback Procedural guidance type; Memory content Memory type Storage status Retrieved memory Memory Fetch Retrieve stored memory. fetchMemory Table 4: Fine-grained evaluation at Level and Level II in J1-EVAL. Methods Direct Reasoning Multilingual LLMs Qwen2.5-7B Ministral-8B Qwen3-8B GLM4-9B Gemma3-12B Qwen3-14B Qwen3-32B Legal-specific LLMs ChatLaw2-7B LawLLM-7B Workflow-based Methods ReAct-Qwen3-32B Plan-and-Solve-Qwen3-32B Plan-and-Execute-Qwen3-32B Level-I Level-II KQ LC CD DD BIN NBIN BIN NBIN PLA DEF CLA EVI F&R RES DEF EVI 56.6 56.2 53.3 64.0 60.0 58.3 60.7 57.8 55.0 57.6 59.0 60.2 49.3 41.9 52.4 47.1 42.4 57.0 58.5 47.9 42.3 65.0 62.8 63. 44.1 48.5 45.8 43.4 48.0 50.9 51.1 51.5 51.5 55.0 45.9 53.0 54.4 58.0 36.0 15.7 34.7 25.0 19.0 35.7 43.7 21.5 19. 38.6 46.8 42.8 43.2 46.8 91.2 82.7 68.3 80.9 92.2 91.2 70.6 32.6 84.9 76.6 80.1 71.3 91.8 75.4 68.1 72.5 84.2 90.6 79. 28.6 82.6 79.1 78.9 72.3 76.9 77.8 63.9 80.0 89.6 89.2 77.6 27.5 75.4 75.2 77.2 72.3 48.1 36.3 29.0 37.2 50.9 54.6 52. 11.1 44.0 50.1 49.5 44.2 76.8 54.0 60.1 64.4 68.1 62.0 51.6 20.2 62.6 56.6 57.1 54.1 67.2 75.4 57.3 72.9 68.9 76.3 66. 20.6 59.6 72.6 77.2 73.6 62.2 54.9 50.5 64.0 72.4 75.8 69.6 26.9 54.3 63.5 70.9 66.7 21.7 28.0 14.1 27.5 13.4 24.9 17. 9.2 17.0 27.4 29.0 26.0 81.9 84.5 84.9 86.3 81.0 88.0 45.3 50. 58.2 69.0 72.6 82.7 63.5 71.5 27.4 26.7 Autonomous Tool Usage within Reasoning Search-o1-Qwen3-32B LawThinker-Qwen3-32B(ours) 63.3 64. 63.8 63.4 LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY Methods Direct Reasoning Multilingual LLMs Qwen2.5-7B Ministral-8B Qwen3-8B GLM4-9B Gemma3-12B Qwen3-14B Qwen3-32B Legal-specific LLMs ChatLaw2-7B LawLLM-7B Table 5: Fine-grained evaluation at Level III in J1-EVAL. Level-III CI STA ACT UNI STA ACT 13.4 13.2 9.4 19.4 36.0 5.1 37.6 2.2 3.2 33.3 30.9 33.1 33.3 17.0 16.3 22.2 49.4 8.4 47.4 5.2 3. 45.1 45.0 47.9 46.3 55.9 48.4 21.5 43.0 41.9 64.5 11.8 62.4 29.0 4.3 67.7 72.0 73.1 71.0 72. 8.0 2.9 21.0 31.2 29.0 21.0 30.4 - 1.4 16.7 13.8 16.7 17.4 33.3 36.9 8.5 23.0 31.7 37.5 40.4 37.1 7.4 2. 31.9 33.7 31.7 40.4 49.7 CR UNI 87.0 14.5 59.4 75.4 91.3 60.9 76.8 33.3 15. 79.7 88.4 78.3 91.3 91.3 SEN FINE 73.6 20.0 54.2 61.2 70.4 51.3 58.2 25.5 12. 62.5 66.5 58.9 70.0 70.1 62.7 12.4 38.9 47.7 52.1 47.9 57.5 16.3 3.5 61.7 64.2 56.8 64.1 69. Workflow-based Methods ReAct-Qwen3-32B Plan-and-Solve-Qwen3-32B Plan-and-Execute-Qwen3-32B Autonomous Tool Usage within Reasoning Search-o1-Qwen3-32B LawThinker-Qwen3-32B(ours) 34.9 44.6 Figure 7: LawThinker Instruction. LawThinker Instruction You are legal-reasoning assistant that may call domain-specific legal tools whenever necessary. Available tools memory_fetch retrieve stored knowledge or context. law_retrieval retrieve the topk most relevant statutes given naturallanguage query. law_recommendation return statutes similar to the one provided. charge_expansion expand list of charges with related ones. case_retrieval retrieve similar civil/criminal cases. template_retrieval fetch document template (e.g. complaint document, defence document). writing_plan_generation generate writing plan for the given document type. procedure_retrieval retrieve the civil/criminal court procedure (stage=0 for the full procedure). Tool-calling format Whenever tool is needed, output <tool_call>{\"name\": ..., \"arguments\": ...}</tool_call>. The system will respond with <tool_call_result> ... </tool_call_result>. Example: examples Conference acronym XX, June 0305, 2018, Woodstock, NY Trovato et al. Exploration-Phase Examples Figure 8: Exploration-Phase Examples. For multi-turn QA, we recommend memory_fetch, law_retrieval, and law_recommendation. <tool_call>{\"name\":\"law_retrieval\", \"arguments\":{\"query\":\"Must recusal request state reasons?\", \"topk\":5}}</tool_call> <tool_call_result>...statute snippets...</tool_call_result> ... For document generation, we recommend template_retrieval, plan_generation, and memory_fetch. <tool_call>{\"name\":\"template_retrieval\", \"arguments\":{\"template_type\":\"complaint document\"}}</tool_call> <tool_call_result>...template text...</tool_call_result> <tool_call>{\"name\":\"writing_plan_generation\", \"arguments\":{\"document_type\":\"complaint document\"}}</tool_call> <tool_call_result>...plan steps...</tool_call_result> <tool_call>{\"name\":\"memory_fetch\", \"arguments\":{\"memory_type\":\"knowledge\"}}</tool_call> <tool_call_result>...knowledge memory...</tool_call_result> ... For court simulation, we recommend procedure_retrieval, law_retrieval, law_recommendation, charge_expansion, and case_retrieval. Before issuing judgment you must gather sufficient case facts and legal grounds. <tool_call>{\"name\":\"procedure_retrieval\", \"arguments\":{\"court_type\":\"civil court\",\"stage\":0}}</tool_call> <tool_call_result>...five stages returned...</tool_call_result> <tool_call>{\"name\":\"law_retrieval\", \"arguments\":{\"query\":\"case summary\", \"topk\":5}}</tool_call> <tool_call_result>...candidate statutes...</tool_call_result> <tool_call>{\"name\":\"law_recommendation\", \"arguments\":{\"law\":\"Criminal Law Art.201\"}}</tool_call> <tool_call_result>...related articles...</tool_call_result> <tool_call>{\"name\":\"charge_expansion\", \"arguments\":{\"charges\":[\"charge\"]}}</tool_call> <tool_call_result>...alternative charges...</tool_call_result> <tool_call>{\"name\":\"case_retrieval\", \"arguments\":{\"type\":\"civil\",\"query\":\"key facts\"}}</tool_call> <tool_call_result>...precedents...</tool_call_result> ... LawThinker: Deep Research Legal Agent in Dynamic Environments Conference acronym XX, June 0305, 2018, Woodstock, NY DeepVerifier Prompt Figure 9: DeepVerifier Prompt. You are deep-analysis legal assistant. Given (i) the users last query or response, (ii) the current reasoning trace, and (iii) the result of the exploration step, decide whether the retrieved information is correct and relevant; decide whether more exploration is needed; and store any key facts or statutes. Tool principles 1. Choose the appropriate checking tool to verify the accuracy and relevance of retrieved knowledge. 2. Summarize key information and store it with memory_store. retrieve stored knowledge / context store key knowledge / context verify law article content Available tools memory_fetch memory_store law_article_check fact_law_relevance_check charge_law_consistency_check search_query_rewrite document_format_check procedure_check Check procedural compliance rewrite law retrieval queries check document format check law applicability to facts check charge-law consistency Tool-calling format <tool_call>{\"name\": ..., \"arguments\": ...}</tool_call> <tool_call_result> ... </tool_call_result>. Example: examples Figure 10: Checking-Phase Examples. Checking-Phase Examples Multi-turn QA <tool_call>{\"name\":\"fact_law_relevance_check\", \"arguments\":{\"fact\":\"...\",\"law\":\"Art.201\"}}</tool_call> <tool_call_result> ... </tool_call_result>. <tool_call>{\"name\":\"law_query_rewrite\", \"arguments\":{\"query\":\"original\",\"context\":\"case background\"}}</tool_call> <tool_call_result> ... </tool_call_result>. Document Generation <tool_call>{\"name\":\"document_format_check\", \"arguments\":{\"document_type\":\"complaint document\",\"document\":\"<draft text>\"}} </tool_call> <tool_call_result> ... </tool_call_result>. Court Simulation <tool_call>{\"name\":\"procedure_check\", \"arguments\":{\"court_type\":\"civil court\"}}</tool_call> <tool_call_result> ... </tool_call_result>. <tool_call>{\"name\":\"law_check\",\"arguments\":{\"law_name\":\"Art.201\"}}</tool_call> <tool_call_result> ... </tool_call_result>."
        }
    ],
    "affiliations": [
        "Renmin University of China"
    ]
}