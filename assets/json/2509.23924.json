{
    "paper_title": "Taming Masked Diffusion Language Models via Consistency Trajectory Reinforcement Learning with Fewer Decoding Step",
    "authors": [
        "Jingyi Yang",
        "Guanxu Chen",
        "Xuhao Hu",
        "Jing Shao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Masked diffusion language models (MDLMs) have recently emerged as a promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. A naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such a naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMs, so why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits a training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skip-step optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJ-GRPO, hold significant promise for effectively and efficiently taming MDLMs. Code: https://github.com/yjyddq/EOSER-ASS-RL."
        },
        {
            "title": "Start",
            "content": "TAMING MASKED DIFFUSION LANGUAGE MODELS VIA CONSISTENCY TRAJECTORY REINFORCEMENT LEARNING WITH FEWER DECODING STEP Jingyi Yang1, 2 Guanxu Chen2, 3 Xuhao Hu1, 2 1Fudan University 3Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory Jing Shao2 5 2 0 2 8 ] . [ 1 4 2 9 3 2 . 9 0 5 2 : r yangjingyi946@gmail.com, shaojing@pjlab.org.cn Code: https://github.com/yjyddq/EOSER-ASS-RL"
        },
        {
            "title": "ABSTRACT",
            "content": "Masked diffusion language models (MDLMs) have recently emerged as promising alternative to autoregressive (AR) language models, offering properties such as parallel decoding, flexible generation orders, and the potential for fewer inference steps. Despite these advantages, decoding strategies and reinforcement learning (RL) algorithms tailored for MDLMs remain underexplored. naive approach is to directly transfer techniques well-established for AR models to MDLMs. However, this raises an immediate question: Is such naive transfer truly optimal? For example, 1) Block-wise and semi-AR decoding strategies are not employed during the training of MDLMsso why do they outperform full diffusion-style decoding during inference? 2) Applying RL algorithms designed for AR models directly to MDLMs exhibits training-inference inconsistency, since MDLM decoding are non-causal (parallel). This results in inconsistencies between the rollout trajectory and the optimization trajectory. To address these challenges, we propose EOS Early Rejection (EOSER) and Ascending Step-Size (ASS) decoding scheduler, which unlock the potential of MDLMs to perform full diffusion-style decoding, achieving competitive performance with fewer decoding steps. Additionally, we introduce Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO) for taming MDLMs, which emphasizes the consistency between rollout trajectory and optimization trajectory, and reduces the optimization errors caused by skipstep optimization. We conduct extensive experiments on reasoning tasks, such as mathematical and planning benchmarks, using LLaDA-8B-Instruct. The results demonstrate that the proposed EOSER and ASS mechanisms, together with CJGRPO, hold significant promise for effectively and efficiently taming MDLMs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Autoregressive (AR) large language models Brown et al. (2020); Bai et al. (2023); Touvron et al. (2023); Grattafiori et al. (2024); Guo et al. (2025); Yang et al. (2025) have demonstrated compelling scaling laws in general capabilities, alignment, and reasoning. Their success suggests that scaling up the model size, training corpora and computational resources can yield models capable of solving broad spectrum of tasks. However, beyond sheer scale, there is no evidence indicating that autoregressive modeling is the inevitable or optimal foundational paradigm for language modeling. Diffusion language models (DLMs) Khanna et al. (2025); DeepMind (2025); Song et al. (2025) have recently emerged as promising competitor, exhibiting parallel decoding (non-causal, high throughput), flexible orders, and potential for fewer inference steps. Built upon prior theoretical and empirical work Austin et al. (2021); Lou et al. (2023); Nie et al. (2024), masked diffusion language models (MDLMs) have stood out from counterparts (i.e., DLMs) and established dominant position. Corresponding Author."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (a) and (b) illustrate the vertical and front view of Step-Position Heatmap of Token Probability, showing the frequency of <EOS> appearing at each position per step. (c) depicts the performance comparison of semi-AR and full-diffusion decoding strategies. (d) shows the average confidence of tokens as denoising progresses. representative train-from-scratch MDLMLLaDA Nie et al. (2025) demonstrates competitive performance compared to similarly sized AR models. Beyond the promising results, two problems regarding MDLMs remain underexplored: (1) Decoding Strategy, as shown in the (d) of Fig. 1, why does semi-AR (i.e., block-wise) decoding outperform full diffusion (without block constraint) decoding without specifically training for this strategy? The (a), (b) and (c) of Fig. 1 may unveil the reason behind this behavior. In particular, (c.1) and (c.2) indicate two key characteristics: 1) The average tokens confidence is lower in the initial denoising steps but increases sharply as denoising progresses. 2) The average confidence of <EOS> is significantly higher than that of <non-EOS> . These characteristics explain the phenomena observed in (a) and (b). The block constraint avoids decoding the <EOS> at the end of sequence in early steps, whereas full diffusion decoding lacks such constraint. Consequently, when employing full diffusion decoding, few semantically informative tokens are decoded in the initial low-confidence phase, potentially causing derailmentthe model deviates from the correct denoising trajectory. We refer to this phenomenon the <EOS> Trap! of full diffusion decoding. Furthermore, characteristic 1) suggests that uniform step-size decoding scheduler may be suboptimal. An intuitive approach is to decode cautiously in the early low-confidence phase and more aggressively in the later steps. (2) Online Reinforcement Learning, several recent studies attempt to replicate the success of online reinforcement learning (RL) in AR models to MDLMs. For example, d1 Zhao et al. (2025) directly applies GRPO Shao et al. (2024); Guo et al. (2025) to LLaDA Nie et al. (2025), which overlooks the inconsistency between rollout trajectory and optimization trajectory. While LLaDOU Huang et al. (2025) recognizes this issue and introduces an unmasking policy module to assist low confidence remasking, then combines it with RL to improve performance, this module requires training from scratch and often needs customized training for different model sizes and architectures, which is neither elegant nor scalable. Moreover, even when trajectory consistency considered, hardware memory confronts significant bottleneck in online reinforcement learning. As maintaining consistency between rollout and optimization trajectories necessitates storing intermediate states throughout the denoising process, memory consumption drastically increases and becomes prohibitive as the number of denoising steps grows. To this end, we propose approaches from the three perspectives, including decoding strategy, denoising step-size scheduler and reinforcement learning algorithm to alleviate the above problems. In summary, our contributions are three-fold: EOS Early Rejection (EOSER): We introduce EOS early rejection (EOSER) for full diffusion decoding, which excessively suppress the confidence of <EOS> in early steps and gradually restores it as denoising progresses, thereby alleviating the <EOS> Trap! . Experimental results show that EOSER significantly improves the performance of full diffusion decoding."
        },
        {
            "title": "Preprint",
            "content": "Ascending Step-Size (ASS) Decoding Scheduler: Based on the observation that average token confidence evolves from low to sharply increasing during denoising, we propose an ascending step-size decoding scheduler instead of uniform step-size ones. This scheduler decodes fewer tokens cautiously in early steps and more tokens aggressively in later steps, substantially reducing the number of decoding steps (e.g., from 2 to log2 L). Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO): We propose Consistency Trajectory GRPO (CJ-GRPO) to align the denoising trajectories of rollout and optimization. It considers the non-causal nature of full diffusion decoding and effectively reduces the optimization errors caused by skip-step optimization. When combined with the EOSER and ASS scheduler, CJ-GRPO exhibits the potential to achieve considerable performance with fewer decoding steps."
        },
        {
            "title": "2.1 MASKED DIFFUSION LANGUAGE MODELS",
            "content": "Recently, utilizing diffusion models to construct large language models (DLMs) Nie et al. (2025); Ye et al. (2025) has emerged as promising trend. Many works Nie et al. (2025); Ye et al. (2025); Austin et al. (2021); Wu et al. (2023); Sahoo et al. (2024); Lou et al. (2023); Zheng et al. (2024); Gong et al. (2024); Ou et al. (2024); Nie et al. (2024); Arriola et al. (2025) are obsessed with the parallel decoding, flexible order, and potential fewer inference steps of DLMs, considering them strong competitor to autoregressive models. D3PM Austin et al. (2021) initiates the discrete probability diffusion model, proposing four commonly used corruption approaches to damage sentences in the forward process: uniform, absorbing state (mask), discrete Gaussian, and token embedding distance. Among these, masked diffusion language models (MDLMs) have stood out and established dominant position among the counterparts. Recent research has scale MDLMs to the billion-parameter regime, achieving performance comparable to similarly sized autoregressive LLMs (e.g., SMDM Gong et al. (2024), LLaDA Nie et al. (2025), Dream Ye et al. (2025), DiffuLLaMA Nie et al. (2024)), significantly scaling their capabilities. Other notable works include Diffusion-LM Li et al. (2022) and simple guidance Schiff et al. (2024) explore controllable text generation for DLMs. Block Diffusion Arriola et al. (2025) introduces semi-autoregressive training-inference paradigm, combining intra-block diffusion, inter-block autoregression to enable variable-length generation."
        },
        {
            "title": "2.2 ONLINE REINFORCEMENT LEARNING FOR DIFFUSION LANGUAGE MODELS",
            "content": "Reinforcement learning (RL) has been extensively explored to align autoregressive (AR) LLMs on human preference and enhancing their reasoning capabilities Schulman et al. (2017); Rafailov et al. (2023); Shao et al. (2024). More recently, MDLMs Ye et al. (2024); Huang et al. (2025); Zhu et al. (2025); Zhao et al. (2025) have shown promise in reasoning tasks (e.g., math, game). For instance, DoT Ye et al. (2024) demonstrates that DLMs are effective in producing chain-of-thought reasoning and addressing reasoning tasks. d1 Zhao et al. (2025) directly adopts the representative online reinforcement learning algorithm (i.e., GRPO Shao et al. (2024); Guo et al. (2025)) to adapt MDLMs, while LLaDOU Huang et al. (2025) explores online reinforcement learning algorithms more suitable for DLMs/MDLMs. So far, most policy gradient-based RL algorithms Shao et al. (2024); Guo et al. (2025); Zeng et al. (2025); Yu et al. (2025); Hu et al. (2025) have been tailored for autoregressive LLMs, which perform with causal mask and token-by-token generation. These algorithms are seamlessly compatible with sequential modeling and decision making. However, the non-causal attention nature of MDLMs prevents RL algorithms designed for AR models from being directly applied to MDLMs."
        },
        {
            "title": "3.1 MASKED DIFFUSION LARGE LANGUAGE MODELS",
            "content": "Similar to typical diffusion models Ho et al. (2020); Song et al. (2020), MDLMs (Austin et al., 2021; Lou et al., 2023; Shi et al., 2024) model distribution pθ(x0) through discrete forward and reverse process defined by q(xt+τ xt) and pθ(xtxt+τ ), respectively. The forward process gradually corrupts"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: (a) EOS Early Rejection. (b) Ascending Step-Size Scheduler for fewer inference steps. sequence x0 by replacing tokens with <MASK> according to time index [0, 1]. At = 0, the sequence x0 is clean and unmasked. The intermediate state xt is partially masked and each token is masked with the probability or unmasked with 1 at time tick t. The reverse process recovers the original sequence x0 from the fully masked state x1. During the pre-training, the model learns mask predictor pθ(xt) that takes xt as input and predicts all <MASK> . The training objective minimizes the negative evidence lower bound, which upper-bounds the negative log-likelihood of the model distribution: Lθ = EtU (0,1], x0pdata, xtqt0(xtx0) I[xi = <MASK>] log pθ(xi 0 xt) Epdata(x0) [log pθ(x0)] Lθ, 1 xt (cid:88) (1) (2) , where xt is the sequence length of x, means the k-th position, denotes the indicator function that ensures the loss is computed only for masked tokens. Instruction tuning follows similar procedure: the prompt p0 remains unchanged, while the response r0 is partially masked. The model takes the prompt p0 and masked response rt as input and is trained to predict the clean response r0: θ = EtU (0,1], p0pdata, rtqt0(rtp0,r0) 1 rt (cid:88) i=1 I[ri = <MASK>] log pθ(ri 0 p0, rt) , (3) where rt is the generation length of response. In practice, the continuous time variable (0, 1] is discretized into the step with interval = 1 , such that = t, {0, 1, , 1}. The denoising step is defined as = s, {1, 0, , 1}, implying xs = xSs = xSSt = x1t. For ease of description, we will use in the remainder of this paper."
        },
        {
            "title": "3.2 EOS EARLY REJECTION",
            "content": "As illustrated in Fig. 1 (Sec. 1), the performance of full diffusion-style decoding significantly lags behind that of semi-autoregressive (block-wise) decoding. This issue arises because LLaDA Nie et al. (2025) replaces <PAD> with <EOS> during pre-training, resulting the training sequences include large number of <EOS> , and enabling the model to learn such bias from data. Consequently, during the early steps of full diffusion decodingwhen the average token confidence is lowthe model exhibits disproportionately high confidence in <EOS> compared to <non-EOS> , leading it prematurely generate <EOS> at the end of sequence in early steps. In contrast, commonly used block-wise decoding avoids the <EOS> Trap! by isolating later blocks. However, it introduces several limitations: (1) The block boundaries restricts decoding flexibility. Tokens in later blocks that may have high confidence and should be decoded earlier are forcibly delayed, potentially constraining the optimal decoding trajectory. (2) Block length may be sensitive hyperparameter that requires careful tuning. In comparison, full diffusion-style decoding is free from such constraints. To alleviate the <EOS> Trap! associated with full diffusion-style decoding, we introduce attenuation coefficient γ for <EOS> confidence that varies with the denoising steps s: γ = γmin + (γmax γmin) 1 , ˆpθ(xi sxs1) = (cid:40) sxs1), γ pθ(xi pθ(xi sxs1), if decode(xi s) = EOS , otherwise (4) (5) here, γmin and γmax represent the lower and upper bound, respectively, is the current step, and is the total number of steps. The attenuation coefficient γ is used for early rejection mechanism. This"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: (a) The causality gurarantee the consistency between AR LLMs rollout and optimization. (b) Two inconsistency trajectories RL training for MDLMs, we refer to 1 as one-step xS and 2 as one-step x0 xS. (c) Consistency Trajectory Group Relative Policy Optimization. mechanism suppresses the occurrence of <EOS> in the early steps by multiplying the confidence corresponding to early <EOS> by the attenuation coefficient γ, as shown in Eqn. 5 and Fig. 2 (a), while restoring its probability in the later steps to allow for sentence termination. Empirically, we set γmax = 1.0 and γmin [0.4, 0.6] for uniform step-size decoding scheduler."
        },
        {
            "title": "3.3 ASCENDING STEP-SIZE SCHEDULER FOR FEWER STEPS INFERENCE",
            "content": "As shown in Fig. 1 and discussed in Sec. 1, the average token confidence remains low in early denoising steps and increases sharply in later steps. This observation suggests that cautious decoding strategydenoising fewer tokensis preferable initially, while more aggressive approachdenoising more tokenscan be adopted later. To leverage this phenomenon, we propose an ascending step-size decoding scheduler that adapts to the confidence trend, significantly reducing the number of denoising steps required. As illustrated in Fig. 2 (b), we replace the fixed step-size of with power-of-2 schedule to determine the number of tokens decoded at each step s. Specifically, at decoding step {0, , s, , 1}, the number of tokens decoded is 2s, and the cumulative number of tokens decoded up to step is 2s+1 1. The current and cumulative token counts are thus defined as: cur tokens = {1, , 2s, , 2S1}, cum tokens = {1, , 2s+1 1, , 2S 1}. Since the sum of the first terms of geometric progression with common ratio of 2 is 2S 1, but considering that the target generation length is typically set to power of 2, we adjust the final step to decode one additional token. Thus the ascending step-size scheduler is defined as follows: Ascending Step-Size Scheduler(s) = (cid:26)2s + 1, 2s, if = 1 otherwise . (6) To avoid the <EOS> Trap! when using Ascending Step-Size (ASS) scheduler with full diffusion decoding, we integrate the EOSER. Given that the step-sizes are non-uniform, we adjust the γ to account for the progressive increase in power-of-2. Specifically, the γASS is defined as follows: γASS = γmin + (γmax γmin) γmin + (γmax γmin) , 2s + 1 2S 2s 2S , if = 1 . otherwise (7) Empirically, we set γmax = 1.0 and γmin = 0.01. The ASS scheduler can also be extended to block-wise decoding. For example, multiple power-of-2 step groups can be combined into single block. An example is as follows: Blocks = { 20, 21, 22, 23, 24, (cid:124) (cid:125) (cid:123)(cid:122) block 1, block length=25 1 , 2s, 2s+1, 2s+2, 2s+3, 2s+4 (cid:124) (cid:125) (cid:123)(cid:122) block n, block length=2s (25 1) , }. (8)"
        },
        {
            "title": "Preprint",
            "content": "The ASS decoding significantly reduces the inference steps (time) of MDLMs. Several studies Nie et al. (2025); Zhao et al. (2025) demonstrate that the empirically optimal uniform steps for MDLMs are often half the generation length 2 to log2 L, thereby lowering the time complexity from O(L) to O(log2 L). This exhibits significantly potential for accelerating inference. 2 . The ASS decoding scheduler reduce this from L"
        },
        {
            "title": "3.4 CONSISTENCY TRAJECTORY GROUP RELATIVE POLICY OPTIMIZATION",
            "content": "GRPO Shao et al. (2024); Guo et al. (2025) is widely used and cost-effective online reinforcement learning algorithm for training autoregressive (AR) models. However, directly applying GRPO to masked diffusion language models (MDLMs) may lead to fundamental mismatch. Unlike AR decoding, Semi-AR and full diffusion-style decoding in MDLM lacks the causality guarantee (i.e., causal attention). As depicted in (a) of Fig. 3, in AR models, the predicted confidence of each token depends only on previously decoded tokens, and token confidences obtained during rollout are completely consistent with the confidences computed on final prompt-completion pairs. In contrast, MDLMs employ bidirectional attention, leading to non-causal (diffusion-style) rollout trajectories. The predicted token confidences during rollout which are estimated in the presence of <MASK> , differ from the confidences computed on final prompt-completion pair (p0, r0 in Eqn.3). This discrepancy arises because the models predictions at intermediate steps are conditioned on partially masked sequences, unlike the autoregressive paradigm, as illustrated in Fig. 3 (a). To address the issue posed by non-causal rollout trajectories in MDLMs, there are two compromise inconsistency trajectory GRPO algorithm (we refer to them as iCJ-GRPO) in Fig.3 (b). 1 involves calculating confidence from prompt-completion pairs with slightly perturbed prompts (as in d1 Zhao et al. (2025)). This method can be seen as one-step denoising optimization from starting point near the final state xS, or 2 directly computes confidence from fully masked responses r1(x0), and optimizing to reach the final answers r0(xS), equivalent to performing the one-step denoising optimization from initial state x0 to the final state xS. Both of them may bring significant optimization errors of trajectory and may degrade performance. To ensure consistency between rollout and optimization trajectories, we propose the Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO), as illustrated in Fig. 3 (c). During the rollout phase, we record intermediate states at each denoising step in two queues: one for storing token confidences πθ(xg s1), and the other for storing the position posg of the token decoded at each step, which exhibit the following relationship: pθ(xg sxg s1) = πθ(xg (9) sxg where pθ(xg s1) represents token confidences (probabilities) corresponding to the decoding positions posg at step 1, and denotes the group ID in GRPO. These intermediate states are then utilized to guide the optimization trajectory. The loss between adjacent steps 1 is as follow: sxg sxg Ag + βKL(πθπθref ). Lθ,s1 = pθ(xg pθold(xg s1) s1) (cid:88)"
        },
        {
            "title": "1\nG",
            "content": "(10) s1) posg s, g=1 The sample-wise trajectory loss and the corresponding batched trajectory loss are: (cid:88) (cid:88) Lb θ ="
        },
        {
            "title": "1\nS",
            "content": "s=1 Lθ,s1, Lθ ="
        },
        {
            "title": "1\nB",
            "content": "Lb θ. b=1 (11) The detailed optimization pipeline is depicted in Algo. 1. However, limitation of consistency trajectory GRPO algorithm is that storing intermediate states increases memory overhead proportionally with the number of denoising steps S. To address this issue, we naturally consider employing ASS scheduler during rollout. This not only reduces the time complexity from O(L) to O(log2 L), but also the space complexity from O(L) to O(log2 L). Furthermore, models trained in this manner exhibit improved fewer-step generation capability. The combination of CJ-GRPO and ASS scheduler brings multiple advantages simultaneously, embodying the Kill three birds with one stone."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "Following previous work d1 Zhao et al. (2025), we evaluate LLaDA Nie et al. (2025) after reinforcement learning on mathematical, planning, and coding tasks to assess how our proposed decoding strategies and reinforcement learning algorithm improve reasoning capabilities."
        },
        {
            "title": "Preprint",
            "content": "0 with and mask tokens Sample batch of question-answer pair (q, a) for in 1, ..., do Initialize xg for in 1, ..., do Decoding xg Enqueue Q(xg Algorithm 1 CJ-GRPO training with ASS and EOSER Input Unmasking policy πθ, training dataset D, reward verifier R(x, a), and queue of denoising trajectory Q(xs, poss) for recording intermediate states and unmasking positions. 1: while πθ not converged or max epochs unreached do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: Compute the advantages µ denotes the number of GRPO iterations Compute denoising trajectory loss πθ(xg s1) with ASS & EOSER s1, posg s) end for Ag = rg mean(r1:G) std(r1:G) for in 1, ..., µ do denotes the number of groups per question denotes the number of denoising steps Compute the rewards end for rg = R(xg s1) = πθold (xg s1) posg S, a) s1, posg s) sxg s1) = πθ(xg s1) posg s, pθold (xg xg pθ (xg s1) (cid:80)G s1) Ag + βKL(πθπθref ) xg (xg g=1 for in 1, ..., do Dequeue Q(xg sxg pθ(xg Lθ,s1 = 1 16: pθold see Eqn. 10 see Eqn. 11 (cid:80)S end for Lb θ = 1 Update θ of πθ with θLθ = 1 17: 18: 19: end for 20: 21: end while Output Unmask policy πθ. s=1 Lθ,s1, calculate the gradient Lθ = 1 (cid:80)B b=1 Lb θ (cid:80)B b=1 θLb θ 4."
        },
        {
            "title": "IMPLEMENTATION DETAILS",
            "content": "Datasets and Benchmarks We evaluate our method on following domains, (1) Mathematics: The mathematical problems which can be verified the correctness between models response and ground truth answer with robust regular expressions. We utilize GSM8K Cobbe et al. (2021), highquality dataset of linguistically diverse grade school math word problems, and MATH500 Lightman et al. (2023), curated subset of 500 problems filtered from the MATH Hendrycks et al. (2021). (2) Planning: Involving two categories of tasks: 4x4 Sudoku puzzles, which require constraint satisfaction and systematic elimination to fill grid with numbers, and Countdown with 3 numbers, combinatorial arithmetic game where models must reach target number using basic arithmetic operations on given set of numbers. Model and Training We utilize LLaDA-8B-Instruct Nie et al. (2025) as the base model. To ensure fair comparison, we share the same configuration as diffu-GRPO Zhao et al. (2025). For both diffu-GRPO and our CJ-GRPO, the group size for group-relative advantage estimation is 6, the KLdivergence coefficient β is 0.04. For supervised fine-tuning (SFT), we train on the s1k Muennighoff et al. (2025) dataset for 20 epochs iteration, and the maximum sequence length of 4096. The importance sampling parameter ϵ is set to 0.5, and the temperature of Gumbel noise sampling Zheng et al. (2024) during rollout is set to 1.0. The number of GRPO iterations µ is set to 12 for math tasks and 8 for planning tasks. We set the γmax = 1.0 for uniform and ASS decoding. For uniform decoding, γmin [0.4, 0.6], while for ASS scheduler, γmin = 0.01. All baseline methods (SFT, diffu-GRPO) and CJ-GRPO are trained with batch size of 48 and 2 gradient accumulation steps. Evaluation and metric We compare our methods against SFT and diffu-GRPO Zhao et al. (2025) across all benchmarks. Evaluation is performed using 0-shot prompting and greedy decoding. The generation lengths are set to 128 and 256, with corresponding block lengths of 64 and 128 (i.e., half the generation length). The number of denoising steps is set to 7/8, 32/64, and 64/128 for the respective configurations."
        },
        {
            "title": "4.2 MAIN RESULTS",
            "content": "We report the performance of baseline LLaDA-8B-Instruct Nie et al. (2025) equipped with different decoding strategies training-free comparisonin the upper part of Tab. 1 and Tab. 2. The lower 7 EOSER* SFT"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Performance Comparison on Countdown, GSM8K, MATH500 and Sudoku: We report results under different decoding strategies and training algorithms, with generation lengths of 128 and 256 across denoising steps S= 2 with uniform step-size. The notation + indicates combination of decoding strategies and algorithms, * denotes training-free, and gray cell represents our methods. Bold font indicates the optimal while underline is the suboptimal performance. 4 and Countdown GSM8K MATH500 Sudoku Generation Length 128 256 128 256 256 128 256 Methods / Steps 32 64 128 32 64 64 32 64 64 128 32 64 128 Semi-AR* 26.56 30.08 30. 19.92 65.58 70.20 63.31 76.65 26. 26.60 30.00 31.20 Full Diffusion* 35.94 5. 0.00 0.00 56.94 51.86 34.87 22. 22.20 20.40 19.60 15.60 6.88 5. 11.62 8.98 44.14 46.88 41.80 39. 60.73 63.15 58.45 61.33 22.20 24. 22.40 24.20 13.18 13.92 9.42 0. 2.69 2.64 0.83 1.42 23.83 30. 18.36 24.22 61.49 68.39 61.87 77. 23.80 27.40 23.00 31.40 6.54 8. 11.82 4.44 diffu-GRPO 35.16 28.52 24. 28.52 64.59 71.72 65.58 79.91 28. 31.20 30.40 33.80 24.41 22.66 19. 18.85 CJ-GRPO + Semi-AR 58.20 61.33 63.52 67. 77.48 78.39 80.41 84.29 28.80 33. 33.40 38.00 27.83 54.10 47.90 57. CJ-GRPO + EOSER 66.02 70.80 69.34 75.59 62. 67.10 71.72 77.75 23.60 23.20 26. 28.40 85.25 85.69 85.96 85.37 Table 2: Performance of fewer decoding steps on Countdown, GSM8K, MATH500 and Sudoku: We report results under various decoding strategies and training algorithms when the decoding steps are set to log2 L. For ascending step-size scheduler, the steps are set to log2 L, whereas for uniform step-size ones, we fix the steps at 8 to ensure divisibility with both 128 and 256. Countdown GSM8K MATH500 Sudoku Generation Length 256 128 256 128 256 256 Methods / Steps 7/8 8 7/8 7/8 8 7/8 8 Semi-AR* + Uniform (S = 8) Full Diffusion* + Uniform (S = 8) EOSER* + Uniform (S = 8) Semi-AR* + ASS (S = log2 L) Full Diffusion* + ASS (S = log2 L) EOSER* + ASS (S = log2 L) 18.75 11.33 5.86 2. 19.14 4.69 0.00 42.15 26.61 17. 13.20 44.12 12.74 17.00 9.20 22. 43.44 29.42 16.20 14.20 0.00 0. 35.10 9.40 12.80 14.00 46.55 17. 19.20 12.20 29.30 51.17 48.14 50. 20.00 19.60 6.05 0.00 0.05 4. 3.22 3.17 SFT + Semi-AR + Uniform (S = 8) 28.91 22.66 25. 12.28 12.00 6.40 8.64 diffu-GRPO + Semi-AR + Uniform (S = 8) 30. 26.56 32.83 20.32 12.00 18.40 21. 0.59 0.44 0.20 1.03 0.00 18. 0.73 7.03 CJ-GRPO + Semi-AR + ASS (S = log2 L) CJ-GRPO + EOSER + ASS (S = log2 L) 3.12 1.95 38. 5.53 16.40 19.20 24.95 19.04 53. 59.38 55.57 53.22 21.20 22.20 43. 50.32 Figure 4: The Step-Position Heatmap of EOSER. sections of the same tables show the results on various post-training recipes across four tasks, evaluated in zero-shot setting where each model is trained specifically for each task. Training-free Setting Comparison Among Decoding Strategies. We compare the performance of three decoding strategies under the training-free setting: semi-autoregressive (AR), full diffusion and EOSER. For semi-AR decoding, the block length is set to half the generation length, 2 , based on empirical studies that show this configuration yields better performance. In contrast, for full diffusion and EOSER, the block length is equal to the generation length, L. The results are illustrated in the upper part of Tab. 1 and the Fig. 4. Our findings are as follows: (1) Full diffusion decoding performs significantly worse than the other two strategies across all tasks. In the planning tasks, EOSER outperforms semi-AR, particularly on the Countdown task. However, in mathematics questions, EOSER is inferior to semi-AR. We provide detailed analysis of this in Sec. 4.4. (2) EOSER effectively mitigates the <EOS> Trap! (i.e., <EOS> are not decoded per step, leading to large number of <EOS> being ejected in the sequence) and significantly improve the performance of full diffusion. Fig. 4 indicates that the arrival of <EOS> is delayed both spatially (in position) and temporally (in step). CJ-GRPO Combined with Semi-AR and EOSER Compared to SFT and diffu-GRPO. We evaluate the performance of CJ-GRPO combined with semi-AR and EOSER decoding separately, as shown in Tab. 1. We can observe that CJ-GRPO+Semi-AR outperforms both SFT and diffuGRPO across all tasks and 16 settings. Specifically, for planning tasks, Countdown and Sudoku, CJ-GRPO+Semi-AR achieves performance levels that are twice as high as those of diffu-GRPO in several configurations. Moreover, CJ-GRPO+EOSER significantly surpasses SFT, diffu-GRPO, and even CJ-GRPO+Semi-AR on planning tasks. The improvements are particularly notable on Sudoku, where CJ-GRPO+EOSER achieves performance that is approximately twice that of CJ-GRPO+Semi-"
        },
        {
            "title": "Preprint",
            "content": "AR and four times that of diffu-GRPO. Interestingly, on the mathematics tasks, CJ-GRPO+EOSER underperforms compared to diffu-GRPO and is significantly lower than CJ-GRPO+Semi-AR. This finding is consistent with the results observed in the training-free settings. We provide detailed analysis of these findings in Sec. 4.4. Fewer Decoding Steps Potential of Ascending Step-Size (ASS) Scheduler and CJ-GRPO Combined with ASS. We evaluate the performance of SFT, diffu-GRPO, and CJ-GRPO using fewer decoding steps, specifically log2 L, as depicted in Tab. 2. Under the training-free settings, the combination of EOSER+ASS demonstrates optimal performance across almost all benchmarks on 7 settings, outperforming various decoding methods and step-size schedulers. This result indicates that EOSER+ASS holds significant potential for reducing the number of decoding steps. Next, we train the model using CJ-GRPO combined with EOSER and Semi-AR (note that Semi-AR is also compatible with ASS scheduler as mentioned in Sec. 3.3). We found that even with only 7/8 (log2 L) steps compared to original 32/64/128 ( 2 ), EOSER+ASS achieves considerable performance, significantly surpassing other methods using either uniform step-size or the ASS scheduler in all settings. Regarding planning tasks, CJ-GRPO+EOSER+ASS even surpasses the original diffu-GRPO which using 2 denoising steps. Please refer to the Appendix for more discussion."
        },
        {
            "title": "4.3 ABLATION STUDY",
            "content": "S to xS 1 One-step Countdown GSM8K MATH500 Table 3: Ablation on denoising trajectory optimizations. Ablation on Consistency v.s. Inconsistency in Trajectory Optimization. We conduct an ablation study to evaluate the two trajectory optimization methods introduced in Sec. 3.4 to clarify the effect of consistent versus inconsistent trajectory optimization. Experiments are conducted with generation length 128 (L), block length 64 ( 2 ), and 32 denoising steps ( to xS (prompt masking as used in d1 Zhao et al. (2025)), and 2 as one-step from x0 to xS, and ours as consistency trajectory optimization over [x0, , xs, , xS]. Results show that optimizing over each intermediate denoising step leads to significantly better performance. Skipping intermediate steps and implementing 2 introduces substantial optimization errors of the actual rollout trajectory which leads to optimization biases to some extent. Moreover, implementing 2 is equivalent to optimize from point near the final solution xS, represents compromised alternative and yields the worst performance. 4 ). We refer to 1 as one-step from Consistency Trajectory [x0, , xs, , xS] 2 One-step x0 to xS Sudoku 68.76 25.00 35. 64.59 28.20 24.41 45.31 25.68 27. 58.20 28.80 77."
        },
        {
            "title": "4.4 ANALYSIS",
            "content": "Analyzing of Characteristics of MDLMs on Reasoning Tasks. As noted in Sec. 4.2, training-free and training-based (CJ-GRPO) EOSER decoding significantly outperforms Semi-AR decoding on planning tasks, yet slightly underperforms on mathematical tasks. We attribute this discrepancy to the inherent pattern differences between two types of reasoning tasks, namely planning and math. Specifically, Countdown exhibits clear parallel reasoning pattern, while math have sequential pattern. For example, Countdown requires inferring missing operators sandwiched between given operands and target values, while Sudoku demands reasoning over both row and column constraints (2D grid), or thinking forward and backward from the unfilled space (1D sequence) simultaneously. In contrast, math problems in GSM8k and MATH500 typically follow sequential reasoning process, where each step builds directly on the previous one. This pattern makes semi-AR decoding more suitable for mathematics. These observations suggest the value of future work that hybridizes diffusion-style (parallel) and autoregressive (sequential) reasoning to better adapt MDLMs to tasks with diverse reasoning patterns."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This paper introduces reinforcement learning algorithm for MDLMS combined with full diffusionstyle decoding strategy and variable step-size scheduler. The goal is to tame MDLMs via reinforcement learning with fewer decoding steps. We propose three key components: (1) EOS Early Rejection (EOSER), which alleviates the <EOS> Trap! in full diffusion-style decoding by suppressing early"
        },
        {
            "title": "Preprint",
            "content": "<EOS> generation, (2) Ascending Step-Size (ASS) Decoding Scheduler, which reduces decoding steps while maintaining performance by adapting to the change trend of token confidencelow in early steps and sharply increasing later, and (3) Consistency Trajectory Group Relative Policy Optimization (CJ-GRPO), which aligns rollout and optimization trajectories to ensure the consistency. Experimental results on reasoning tasks demonstrate that CJ-GRPO is particularly effective and efficient for taming MDLMs. When combined with EOSER decoding and ASS scheduler, it achieves comparable performance to 2 steps using only log2 steps."
        },
        {
            "title": "6 ETHICS STATEMENT",
            "content": "Our research focuses on reinforcement learning and decoding strategies for diffusion large language models. We do not involve human subjects in our study. The datasets used in our research are publicly available and do not contain any sensitive or private information. We have ensured that our research practices comply with all relevant legal regulations. We have no conflicts of interest or sponsorship to disclose. Our research aims to improve the performance and efficiency of diffusion large language models, and we believe that our methods and findings are ethical and do not raise any concerns regarding discrimination, bias, fairness, privacy, security, or research integrity."
        },
        {
            "title": "7 REPRODUCIBILITY STATEMENT",
            "content": "Ensuring the reproducibility of our work is priority. In the experimental section of our paper, we provide detailed descriptions of the parameter settings and experimental configurations for the reinforcement learning and decoding strategies applied to masked diffusion large language modelsLLaDA-8B-Instruct. To further support reproducibility, we have open-sourced our codebase to the community. This codebase includes all necessary scripts and instructions for reproducing our results. Additionally, any datasets used in our experiments are publicly accessible. We believe that these efforts will enable other researchers to replicate our findings and build upon our work."
        },
        {
            "title": "REFERENCES",
            "content": "Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. Advances in neural information processing systems, 34:1798117993, 2021. Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609, 2023. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Google DeepMind. Gemini diffusion: our state-of-the-art experimental text diffusion model. URL https://deepmind.google/models/gemini-diffusion/, 2025. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, et al. Scaling diffusion language models via adaptation from autoregressive models. arXiv preprint arXiv:2410.17891, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024."
        },
        {
            "title": "Preprint",
            "content": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025. Zemin Huang, Zhiyang Chen, Zijun Wang, Tiancheng Li, and Guo-Jun Qi. Reinforcing the diffusion chain of lateral thought with diffusion language models. arXiv preprint arXiv:2505.10446, 2025. Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, et al. Mercury: Ultra-fast language models based on diffusion. arXiv preprint arXiv:2506.17298, 2025. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in neural information processing systems, 35: 43284343, 2022. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling. arXiv preprint arXiv:2501.19393, 2025. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, JiRong Wen, and Chongxuan Li. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. arXiv preprint arXiv:2406.03736, 2024. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Subham Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin Chiu, Alexander Rush, and Volodymyr Kuleshov. Simple and effective masked diffusion language models. Advances in Neural Information Processing Systems, 37:130136130184, 2024. Yair Schiff, Subham Sekhar Sahoo, Hao Phung, Guanghan Wang, Sam Boshar, Hugo Dalla-torre, Bernardo de Almeida, Alexander Rush, Thomas Pierrot, and Volodymyr Kuleshov. Simple guidance mechanisms for discrete diffusion models. arXiv preprint arXiv:2412.10193, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017."
        },
        {
            "title": "Preprint",
            "content": "Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis Titsias. Simplified and generalized masked diffusion for discrete data. Advances in neural information processing systems, 37: 103131103167, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, et al. Seed diffusion: large-scale diffusion language model with high-speed inference. arXiv preprint arXiv:2508.02193, 2025. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Tong Wu, Zhihao Fan, Xiao Liu, Hai-Tao Zheng, Yeyun Gong, Jian Jiao, Juntao Li, Jian Guo, Nan Duan, Weizhu Chen, et al. Ar-diffusion: Auto-regressive diffusion model for text generation. Advances in Neural Information Processing Systems, 36:3995739974, 2023. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Jiacheng Ye, Shansan Gong, Liheng Chen, Lin Zheng, Jiahui Gao, Han Shi, Chuan Wu, Xin Jiang, Zhenguo Li, Wei Bi, et al. Diffusion of thought: Chain-of-thought reasoning in diffusion language models. Advances in Neural Information Processing Systems, 37:105345105374, 2024. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025. Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerlzoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv preprint arXiv:2503.18892, 2025. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. d1: Scaling reasoning in diffusion large language models via reinforcement learning. arXiv preprint arXiv:2504.12216, 2025. Kaiwen Zheng, Yongxin Chen, Hanzi Mao, Ming-Yu Liu, Jun Zhu, and Qinsheng Zhang. Masked diffusion models are secretly time-agnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, et al. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. arXiv preprint arXiv:2505.19223, 2025."
        },
        {
            "title": "Roadmap",
            "content": "In the Appendix, we present the supplementray experiments and discussions in App. A, the limitations in App. B, and the use of Large Language Models (LLMs) statement in App. C."
        },
        {
            "title": "A SUPPLEMENTARY EXPERIMENTS AND DISCUSSIONS",
            "content": "Table 4: Ablation study on the denoising steps: Countdown. We report the performance changes as denoising steps varies from 1 to 256, 2s, across decoding strategies. * denotes training-free. Gen Len = 256 Denoising Steps Semi-AR* (Block Len = 64) Full-Diffusion* EOSER* Countdown 1 - 2 - 0. 0.00 0.00 0.00 4 8 32 64 128 256 0.00 0. 8.98 4.69 0.00 23.83 16.02 15. 27.34 21.09 0.78 0.00 0.00 0. 0.00 22.27 21.48 23.83 38.67 41. 39.45 Table 5: Ablation study on the denoising steps: GSM8K. We report the performance changes as denoising steps varies from 1 to 256, 2s, across decoding strategies. * denotes training-free. Gen Len = 256 Denoising Steps Semi-AR* (Block Len = 64) Full-Diffusion* EOSER* GSM8K 1 - 3.11 3. 2 - 4 8 16 64 128 256 16.76 26.61 30. 48.14 71.27 77.71 79.98 3.49 3. 12.74 36.85 42.46 34.34 34.87 22. 16.22 23.28 29.42 38.59 47.76 52. 58.45 61.33 Table 6: Ablation study on the denoising steps: MATH500. We report the performance changes as denoising steps varies from 1 to 256, 2s, across decoding strategies. * denotes training-free. Gen Len = 256 Denoising Steps Semi-AR* (Block Len = 64) Full-Diffusion* EOSER* MATH500 1 - - 6.00 6.00 6.00 9.60 8 16 32 64 128 6.40 6.40 13.20 18.60 22.40 27. 33.20 37.80 9.20 16.40 19.80 21. 17.80 17.20 12.20 14.20 18.20 20. 22.40 24.20 22.67 Table 7: Ablation study on the denoising steps: Sudoku. We report the performance changes as denoising steps varies from 1 to 256, 2s, across decoding strategies. * denotes training-free. Gen Len = 256 Denoising Steps Semi-AR* (Block Len = 64) Full-Diffusion* EOSER* Sudoku 1 - 2 - 0.00 0.00 0.00 0. 4 8 16 32 64 256 0.05 0.00 0.15 0.59 0. 0.20 2.44 8.20 0.10 5.22 8. 2.10 10.84 3.96 2.64 5.42 0. 2.69 5.66 0.44 1.42 Ablation on Denoising Steps. As depicted in Tab. 4, 5, 6, 7. We conduct ablation studies on the number steps using different decoding strategies across four benchmarks. We observe common"
        },
        {
            "title": "Preprint",
            "content": "trend: for Semi-AR and EOSER, the best performance is often achieved when the number of denoising steps is half the generation length 2 . Regarding the full diffusion-style decoding strategy, the optimal number of steps is usually smaller than 2 , additionally, while for EOSER (also full diffusion-style), although its optimal number of step is 2 , the performance difference across denoising steps such as 64, 128, and 256 is relatively small, however, for Semi-AR decoding strategy, block length may be sensitive hyperparameter, which may result in large performance variance obtained. This indicates that full diffusion-style decoding has greater potential for fast generation (fewer decoding steps). Specifically, when the generation length is and the number of denoising step 2 , two tokens are generated at each decoding step. This does not significantly increase the token throughput compared to the next-token prediction of AR models (one-by-one). Moreover, the presence of fixed-length <MASK> means that the generation speed of MDLMs with more decoding steps (e.g. uniform stepsize scheduler) may not surpass that of AR models with the same software and hardware infrastructure. Our findings indicate that exploring few-step generation for full diffusion-style decoding in MDLMs is meaningful. Semi-AR, while effective, is not the ultimate solution. Finally, this exploration may truly reveal the speed advantages of MDLMs compared to AR LLMs. Currently, the performance improvements are not significant, and the speed advantages are not fully realized."
        },
        {
            "title": "B LIMITATIONS",
            "content": "In Sec.3.4, we discussed limitation faced by CJ-GRPO: as the number of steps increases, so does the amount of memory required, because storing intermediate states increases memory overhead proportionally with the number of denoising steps S. This constraint greatly affects the training speed. To mitigate this issue, we already have proposed the Ascending Step Size (ASS) scheduler. In planning tasks, the combination of CJ-GRPO with EOSER and ASS, which generates fewer steps (log2 L), has proven to be even more effective than the previous uniform step-size method that relies on more denoising steps ( 2 ). This not only reduces the time complexity from O(L) to O(log2 L), but also the space complexity from O(L) to O(log2 L). However, when it comes to mathematical tasks, this approach still falls short compared to diffu GRPO+Semi-AR and CJ-GRPO+Semi-AR. Given these observations, we believe that exploring better fewer-step full diffusion-style decoding strategies in the future holds great promise. Such methods could potentially resolve the memory issues associated with CJ-GRPO and enhance its mathematical reasoning capabilities. This area of research is well worth exploring. THE USE OF LARGE LANGUAGE MODELS (LLMS) STATEMENT In the process of drafting this paper, we utilized large language models (LLMs) as general-purpose assist tool. Specifically, the LLM was employed to enhance the clarity and coherence of the content, refine the logical structure of the arguments, and improve the grammatical accuracy of the text. The LLM provided suggestions and revisions that helped in polishing the overall quality of the manuscript. However, it is important to note that the LLM did not play significant role in the research ideation or the core writing of the scientific content. The ideas, methodologies, and conclusions presented in this paper are the original work of the authors. We take full responsibility for the content of this paper, including any text that has been refined using the LLM."
        },
        {
            "title": "D SHOWCASE ON SEQUENTIAL AND PARALLEL REASONING TASKS",
            "content": "Our analysis of the models reasoning characteristics (patterns) in mathematical and planning tasks is primarily intuitive and draws loose analogies to human cognitive processes, without rigorous formalization. Therefore, the following cases and interpretations should be considered preliminary and illustrative rather than definitive. Sequential Reasoning Pattern: GSM8K Question: Emmett does 12 jumping jacks, 8 pushups, and 20 situps. What percentage of his exercises were pushups? Response: <reasoning> To determine the percentage of Emmetts exercises that were pushups, we first must follow these steps: 1. Calculate the total number of exercises Emmett did. 2. Identify the number of pushups Emmett did. 3. Determine the ratio of pushups to the total number of exercises. 4. Convert the ratio to percentage. 1. Given: - Jumping jacks: 12 - Pushups: 8 - Situps: 20 The total number of exercises is: 2. The number of pushups is 8. 3. The ratio of pushups to the total number of exercises is: 12 + 8 + 20 = 40 8 40 8 = 0.2 0.2 100 = 20 20% 4. Simplify the ratio: 5. Convert the ratio to percentage: </reasoning> <answer> </answer> Mathematical reasoning tasks typically exhibit sequential structure, where each step logically follows the previous one. For an example of GSM8K, which involving the calculation of pushups percentage. The denominator is obtained by summing the given numerators, after which the final percentage is derived through division. The purple font represents the sequential reasoning pattern among different sub-steps."
        },
        {
            "title": "Preprint",
            "content": "Sequential Reasoning Pattern: MATH500 Question: You are math expert. You will be given question to solve. Solve it step by step. Wrap the final answer in boxed{}. Container holds 4 red balls and 6 green balls; containers and each hold 6 red balls and 4 green balls. container is selected at random and then ball is randomly selected from that container. What is the probability that the ball selected is green? Express your answer as common fraction. Response: <reasoning> 1. Calculate the sum of probabilities of selecting each container: - Probability of selecting each container: 1 3 2. Calculate the probability of selecting green ball from each container: - Probability of selecting green ball from container A: - Probability of selecting green ball from container B: 6 10 = 3 5 - Probability of selecting green ball from container C: 4 10 = 2 5 4 10 = 2 3. Calculate the overall probability of selecting green ball: - Probability of green ball: 1 3 3 5 + 1 3 2 5 + 1 3 2 5 = 1 3 7 5 = 7 15 </reasoning> <answer> </answer> 7 15 For instance, in MATH500 problem requiring the calculation of ratio, one must first determine the numerator and denominator. Given individual numerators, the denominator is obtained by summing them. Calculating the proportions in each container separately. Then, since there are three containersA, B, and Cthe final result is derived by combining their respective probabilities. The purple font represents the sequential reasoning pattern among different sub-steps."
        },
        {
            "title": "Preprint",
            "content": "Parallel Reasoning Pattern: Countdown Question: Using only the numbers [49, 55, 53], create an arithmetic expression that evaluates to exactly 51. You must use all numbers from the list, and each number must be used exactly once. You may use the operations +, -, *, and / as needed. After reasoning, provide only your final expression inside <answer></answer> tags without including an equals sign or the target number. For example, if the numbers are [2, 3, 4] and the target is 5, valid answer is: <answer>n2*4-3n</answer>. Response: <reasoning> To create an arithmetic expression that evaluates to 51 using the numbers 49, 55, and 53, we need to consider the operations +, -, *, and /. Lets start by trying different combinations of these numbers: 1.49+55-53 = 51 2.49+53-55 = 47 3.55+53-49 = After trying different combinations, we find that the expression works: 1.49 + 55 53 = 51 So, the expression 49 + 55 - 53 evaluates to 51. </reasoning> <answer> 49 + 55 53 </answer> The Countdown game is naturally amenable to parallel reasoning, as inferring an operator between two operands does not require strictly sequential steps. For example, the model need not determine operators between earlier operand pairs before handling subsequent ones. In the case illustrated (with operands highlighted in blue and operators in bold), the model does not attempt pairwise combinations first, instead, it directly applies the + and operation to three numbersadding the first two and subtracting the thirdand successfully concludes the correct answer."
        },
        {
            "title": "Preprint",
            "content": "Parallel Reasoning Pattern: Sudoku Question: Please solve the following 4x4 Sudoku puzzle. The puzzle is provided as 16-character string reading left-to-right, top-to-bottom, where 0 represents empty cells. Rules: - Fill empty cells with digits 1-4 - Each row must contain digits 1-4 exactly once - Each column must contain digits 1-4 exactly once - Each 2x2 box must contain digits 1-4 exactly once Important: Your solution must be COMPLETE 16-character string with only the digits 1-4, representing your final solved grid. Respond in this exact format: <reasoning> Your step-by-step solving process </reasoning> <answer> [16-character solution string with no spaces or separators] </answer> Solve the following Sudoku puzzle: 0000100420013142. Response: <reasoning> To solve the given 4x4 Sudoku puzzle, we need to fill the empty cells with digits 1-4 such that each row, column, and 2x2 box contains all the digits exactly once. Here is the step-by-step process: 1. Identify Empty Cells: Locate all empty cells represented by 0 in the puzzle. 2. Check Rows: Ensure each row contains the digits 1-4 exactly once. 3. Check Columns: Ensure each column contains the digits 1-4 exactly once. 4. Check 2x2 Boxes: Ensure each 2x2 box contains the digits 1-4 exactly once. 5. Fill Empty Cells: Fill the empty cells with digits 1-4 while maintaining the above constraints. 6. Verify the Solution: After filling the puzzle, verify that each row, column, and 2x2 box contains digits 1-4 exactly once. By following these steps, we can solve the given 4x4 Sudoku puzzle. </reasoning> <answer> 4213132424313142 </answer> Sudoku requires reasoning over both row and column constraints (in 2D grids), or simultaneous forward and backward thinking along sequences (in 1D formulations). This task is not well-suited to purely sequential reasoning but instead benefits from parallelor bidirectionalreasoning. Accordingly, the model does not proceed by filling numbers step-by-step; rather, it first enumerates the relevant constraints (shown in blue), then deduces the answer directly. In this context, parallel reasoning can be viewed as an implicit process."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
    ]
}