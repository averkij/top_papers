{
    "paper_title": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation",
    "authors": [
        "Linyan Huang",
        "Haonan Lin",
        "Yanning Zhou",
        "Kaiwen Xiao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as a critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, a novel framework that decouples these objectives through two dedicated components: a Personalization Adapter for stylistic manipulation and a Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page: https://flexip-tech.github.io/flexip/)."
        },
        {
            "title": "Start",
            "content": "FlexIP: Dynamic Control of Preservation and Personality for Customized Image Generation Linyan Huang Haonan Lin Yanning Zhou Kaiwen Xiao"
        },
        {
            "title": "Tencent AIPD",
            "content": "5 2 0 2 0 1 ] . [ 1 5 0 4 7 0 . 4 0 5 2 : r Figure 1. Top: FlexIP showcases versatility and precision in personalized image generation. Given single reference image (left column), it vividly captures identity details while creatively following diverse text prompts, resulting in coherent yet highly varied edits. Bottom: FlexIPs dynamic weight gating mechanism smoothly transitions between strong identity preservation and diverse personalization, significantly outperforming IP-Adapter, which suffers from abrupt identity shifts and rigid control. This reflects superior flexibility and user-friendly controllability. Equal contribution."
        },
        {
            "title": "Abstract",
            "content": "With the rapid advancement of 2D generative models, preserving subject identity while enabling diverse editing has emerged as critical research focus. Existing methods typically face inherent trade-offs between identity preservation and personalized manipulation. We introduce FlexIP, novel framework that decouples these objectives through two dedicated components: Personalization Adapter for stylistic manipulation and Preservation Adapter for identity maintenance. By explicitly injecting both control mechanisms into the generative model, our framework enables flexible parameterized control during inference through dynamic tuning of the weight adapter. Experimental results demonstrate that our approach breaks through the performance limitations of conventional methods, achieving superior identity preservation while supporting more diverse personalized generation capabilities (Project Page). 1. Introduction The swift progress of 2D diffusion models [40, 62] has propelled ongoing advancements in image synthesis [35] and editing technologies [4]. These models demonstrate remarkable abilities to generate high-quality and diverse visual content from textual or visual input, showing immense potential in artistic creation and advertising design. Current research in subject-driven image generation primarily follows two paradigms: inference-time fine-tuning and zero-shot image-based customization. The fine-tuning approach [11, 43, 44] learns pseudo-words as compact subject representations, requiring per-subject optimization. While this achieves high-fidelity reconstruction, it inherently sacrifices editing flexibility due to overfitting on narrow feature manifolds. In contrast, zero-shot methods [8, 30, 64] leverage cross-modal alignment modules trained without subject-specific fine-tuning, achieving greater editing flexibility but often compromising identity integrity. Fundamentally, existing methods treat identity preservation and editing personalization as inherently conflicting objectives, forcing models to make implicit trade-offs. We identify critical limitation in existing zero-shot methods: they often adopt alignment modules similar to the Q-former [1, 22] from vision-language models (VLMs) to align image-text modalities. While effective in visual understanding for text generation, such modules become insufficient for image generation tasks, as they require capturing broader and more complex visual knowledge. This image-text misalignment results in identity preservation and editorial fidelity not working harmoniously together. Therefore, more explicit decomposition of visual and textual information is necessaryassigning images to handle identity preservation and texts to guide personalization instructions. Separating these information flows enables each modality to specialize, fostering stronger complementarity and achieving superior cross-modal alignment. To address these issues, we propose FlexIP, the first framework to explicitly decouple identity preservation and personalized editing into independently controllable dimenInspired by the principle of low coupling, high sions. cohesion, we introduce dual-adapter architecture, enabling each adapter to focus clearly and independently on its specific taskidentity preservation or personalized editingthus maximizing their complementary strengths. Specifically, the preservation adapter captures essential identity details by retrieving both high-level semantic concepts and low-level spatial details through cross-attention layers. Intuitively, this approach resembles recognizing person not just by general descriptors (e.g., age or contour) but also by detailed visual cues (e.g., facial features or hairstyle), thereby robustly preserving their identity even under diverse edits. On the other hand, the personalization adapter interacts with the text instructions and high-level semantic concepts. The text instructions provide editing flexibility, while the high-level semantic concepts ensure identity preservation. By separating identity and personalization feature flows, our design eliminates feature competition found in traditional single-path approaches, enabling explicit decoupling of what to preserve and how to edit. As illustrated in Fig. 1 bottom, by changing preservation scale, existing methods produce abrupt transitions between identity preservation and personalization, making precise control challenging. Motivated by this, we aim to achieve an explicit control between identity preservation and personalization, and thus introduce dynamic weight gating mechanism that interpolates between two complementary adapters during inference. Users can continuously adjust adapter contributions, flexibly balancing preservation and personalization (Fig. 1 bottom). Our empirical analysis reveals critical dependency between training data modality and adapter efficacy: video-frame training pairs inherently capture temporal deformations (e.g., pose variations, lighting changes), enabling flexible feature disentanglement, whereas static image pairs tend to induce copy-paste artifacts due to overfitting on rigid spatial correlations. To mitigate this, we implement modality-aware weighting strategy: preservation adapter dominance (higher preservation weight) for image-trained scenarios, enforcing strict identity consistency through feature locking in cross-attention maps. Personalization adapter dominance (higher personalization style) for video-trained scenarios, leveraging temporal coherence to guide structurally coherent deformations. The adapters govern distinct aspects of the generation process: This dynamic weight gating mechanism transforms the traditionally binary preservation-edit trade-off into continuous parametric control surface. This enables appli2 cations ranging from nuanced, identity-consistent retouching to radical yet coherent subject transmutation. Our contributions are threefold: First, we introduce FlexIP, novel plug-and-play framework that decouples identity preservation and personalized editing into independently controllable dimensions, addressing the inherent trade-offs in existing methods. Second, we propose dualadapter architecture comprising preservation adapter and personalization adapter, which respectively handle identitycritical features and editing flexibility, thereby eliminating feature competition and enhancing edit fidelity. Third, we develop dynamic weight gating mechanism that allows for continuous modulation between identity preservation and personalization. Our extensive experiments demonstrate that FlexIP significantly improves identity preservation accuracy while maintaining high levels of editing flexibility, outperforming state-of-the-art methods. 2. Related Work 2.1. Subject-driven Image Generation Recent advances in customized image generation primarily follow two paradigms: tuning-based and tuning-free methods. Methods like Textual Inversion [11], DreamBooth [43], and DreamTuner [16] learn target concepts by fine-tuning pretrained text-to-image model with specialized token or prompt. While these approaches [16, 20] achieve strong identity preservation through direct parameter optimization, they suffer from prohibitive computational overhead from per-subject optimization, reduced editability due to overfitting on narrow concept distributions and inherent latency in serving novel concepts. To address these limitations, recent works [5, 8, 21, 47, 63] employ pretrained visual encoders to bypass test-time fine-tuning. BLIPDiffusion [21] aligns image-text pairs via BLIP-2s [23] crossmodal attention for zero-shot adaptation but struggles with disentangling subject identity from contextual attributes. IP-Adapter [59] and InstantID [52] inject identity features via cross-attention modulation, though their static fusion mechanisms lead to implicit entanglement of preservation and stylization objectives. MSDiffusion [54] constrains edits via spatial attention maps, sacrificing free-form stylization for geometric consistency. CustomContrast [6] use contrastive learning to decouple subject intrinsic attributes from irrelevant attributes. But it still conduct the implict trade-off between preservation and personalization, which hinders the further improvement of the model. DisEnvisioner [13] extract the subject-essential features while filtering out irrelevant information but inherits the copy-paste artifact from static image training pairs. In this paper, we enable explicit control over the trade-off between identity preservation and stylistic personalization, allows users to continuously balancing feature rigidity and editability. Figure 2. Comparison with other methods on two indicators, image preservation and text fidelity, demonstrates that our approach surpasses previous methods in both aspects. 2.2. Preservation-Personalization Trade-off core challenge in personalized image generation is balancing identity preservation against editing flexibility, typically measured through text fidelity (alignment with textual instructions). As illustrated in Fig. 2, existing methods [13, 42, 49, 55, 59, 63] show an inherent compromise: methods optimized for high identity preservation (high CLIP-I scores) generally exhibit reduced text fidelity, while those achieving greater editing freedom frequently sacrifice identity consistency. This trade-off arises due to conflicting optimization goals: strong identity preservation demands strict adherence to reference features, constraining editability, whereas flexible edits encourage semantic diversity at the risk of drifting from the original identity. Thus, we ask: Can method simultaneously achieve robust identity preservation and faithful textual controllability for personalization? To address this critical question, our proposed framework, FlexIP, explicitly decouples identity preservation from personalization. By introducing independent adapters controlled through dynamic weight gating mechanism, FlexIP navigates this trade-off more effectively. This design allows continuous, precise balancing of feature rigidity (preservation) and editability (personalization), which is detailed in the following. 3. Method In this section, we begin by providing foundational overview of text-to-image diffusion models, including their core mechanisms and relevance to our work. Building on this basis, we present comprehensive exposition of the proposed FlexIP framework. Specifically, we first elucidate the key observations and challenges that motivated its development, followed by systematic breakdown of its architecture and operational workflow, detailing its innovative methodology for enabling subject preservation and person3 alization using pre-trained text-to-image diffusion model. 3.2. Preservation Adapter 3.1. Preliminaries Diffusion Models. Diffusion models [15, 48] are generative models consisting of two core processes: (i) forward (diffusion) process that gradually adds Gaussian noise to data, and (ii) denoising process, guided by conditions such as text prompts, to reconstruct images from noise. The training objective of the noise prediction network ϵθ optimizes: Lsimple = Ex0,ϵN (0,I),c,t ϵ ϵθ(xt, c, t)2 , where x0 is clean data, denotes conditioning signals, and xt represents the noisy state at timestep t. Classifier-free guidance [14] enhances conditional generation by training the model with randomly dropped conditioning signals. During inference, predictions blend conditional and unconditional outputs: ˆϵθ(xt, c, t) = ϵθ(xt, c, t) + (1 w) ϵθ(xt, t), where > 1 controls the strength of conditioning. Our work is built upon Latent Diffusion Model [40], which is conditioned on text embeddings from CLIP text encoder. Resampler with Unified Input Representations. The Resampler serves as bridge, connecting input queriesdesigned to capture refined identity informationwith retrieval embeddings that store rich, albeit sparse, visual details: Z(R,X) = Resampler (cid:16) Z(X), Z(D)(cid:17) , (1) where Z(X) denotes the input query, and Z(D) denotes retrieval embedding. The input queries originate from three types of embeddings, all mapped into Rd: Learnable Query Embeddings: Z(L) RNLd, CLIP [CLS] Embeddings: Z(C) RNC d, CLIP Text Embeddings: Z(T ) RNT d. The retrieval embeddings are derived from DINO Patch Embeddings, which are also mapped into the shared latent space Z(D) RNDd, capturing detailed visual information from reference images. By leveraging several perceiver cross-attention (PSA) layers [17], Resampler ensures that input queries effectively extract identity-relevant features from these retrieval embeddings: = softmax (cid:18) Q(Z(X))K(Z(X) Z(D)) (cid:19) , PSAout = AV(Z(X) Z(D)) where denotes concatenation. In this way, the output Z(R,X) integrates rich, low-level visual details from DINO embeddings with the querys high-level semantic context. This refined embedding serves as effective identity conditioning for the subsequent diffusion generation steps. 4 The first step in ensuring identity preservation is determining which queries and which features should be used to retrieve subject-specific attributes. That is, what kind of queries can effectively extract identity-rich information? Learnable queries for adapatability. To generalize across different subjects, an intuitive approach is to learn representations directly from the data distribution. Unlike static embeddings, learnable queries Z(L) provide trainable subject representation that dynamically adapts to diverse subjects. These queries form flexible latent space, capable of encoding subject-specific details while remaining generalizable across different styles and conditions. [CLS] embedding for global identity representation. Moreover, CLIP [CLS] embeddings Z(C) serve as pretrained holistic identity descriptor, which encapsulate highlevel semantics such as structure, style in compact form, offering stability and robustness in identity preservation. Why do these two complete each other? Preserving both fine-grained and global identity attributes is often treated as trivial challenge. However, as shown in Appendix 1.1, we found that learnable queries specialize in capturing fine-grained variations but lack strong global coherence, while CLIP [CLS] embeddings provide global identity consistency but may miss subtle subject details. Therefore, instead of relying on single embedding to learn both, we adopt divide and conquer strategy that integrating both for retrieving fine-grained adaptability and global robustness simultaneously from DINO patch embeddings (as shown in Fig. 3 left bottom), ensuring that identity preservation remains stable even during edits. Formally, we independently resample the learnable queries Z(L) and CLIP [CLS] embedding Z(C) through cross-attention with DINO patch embeddings Z(D): Z(R,L) = ResamplerL Z(R,C) = ResamplerC (cid:16) Z(L), Z(D)(cid:17) Z(C), Z(D)(cid:17) (cid:16) , , (2) = Z(R,L) Z(R,C), where denotes concatenation. And serves as identity preservation, which integrates fine-grained local details (via learnable queries) and global semantics (via CLIP [CLS]). 3.3. Personalization Adapter Considering personalization, Stable Diffusion already condition UNet latents on textual embeddings through crossattention. However, this conditioning provides only general semantic guidance and lacks explicit grounding in the subjects specific visual identity. Consequently, relying solely on the original textual embeddings can cause misalignment between the intended edits and the subjects appearance. Figure 3. The overall pipeline of FlexIP. It introduces three key improvements to the model: the preservation adapter, the personalization adapter, and dynamic weight gating. First, the preservation adapter combines high-level and low-level features to ensure preservation. The personalization adapter interacts with text and visual CLS tokens to absorb meaningful visual cues, grounding textual modifications within coherent visual context. Finally, dynamic weight gating navigates the trade-off between personalization and preservation more effectively through independent adapters controlled by dynamic weight gating mechanism. We address this limitation by introducing an additional personalization adapter, where textual embeddings explicitly attend to the CLIP [CLS] embedding. This additional resampling step enables text embeddings to absorb meaningful visual cues, grounding textual modifications within coherent visual context. As result, the textual instructions become more identity-aware, guiding edits that are both accurate and consistent with the subjects appearance. Formally, the personalization adapter functions as: = Z(R,T ) = ResamplerT Z(T ), Z(C)(cid:17) (cid:16) , (3) where Z(T ) RNT are text embeddings (queries), and Z(C) R1d are CLIP [CLS] embeddings (key-value pairs). Through this, textual guidance is no longer isolated; instead, it becomes visually contextualized, resulting in more precise, flexible and identity-consistent edits. 3.4. Dynamic Weight Gating To address the inherent trade-off between preservation capability and stylized freedom in existing methods, we propose novel dynamic weight gating (DWG) mechanism for joint training on image and video datasets. Empirical analysis reveals that image data enhances preservation quality but induces copy-paste artifacts [8] and weakens instruction adherence, while video data promotes temporal diversity but compromises preservation strength. Our framework leverages the complementary strengths of both modalities by dynamically adjusting the contributions of two specialized adapters. Preservation adapter optimized to maintain high-fidelity details and instruction consistency from image data. Personalization adapter designed to inject temporal diversity and stylized freedom from video data. The DWG mechanism adaptively reweights and based on the input data type. Let Dimg Dvid denote training sample from either the image Dimg or video Dvid dataset. The feature representation h(x) is computed as gated fusion: h(x) = γ(x) + (1 γ(x)) S, (4) where γ(x) is data-dependent gating weight given by: γ(x) = (cid:40) α, 1 β, if Dimg, if Dvid, (5) here, α [0, 1] and β [0, 1] are parameters initialized to prioritize for images (α 1) and for videos (β 1). This formulation ensures: image-centric training amplifies to maximize preservation, ensuring that the essential features of the image are retained. In contrast, video-centric training suppresses to enhance the stylization capabilities of S, allowing for more dynamic and expressive transformations that are suited to video data. This adaptive mechanism enables the model to dynamically balance preservation and Method mRank Personalization Preservation Image Quality User Study (%) CLIP-T CLIP-I DINO-I CLIP-IQA Aesthetic Flex ID-Pres BLIP-Diffusion [21] ELITE [55] MoMA [49] SSR-Encoder [63] IP-Adapter [59] IP-Adapter-Instruct [42] λ-Eclipse [33] DisEnVisoner [13] FlexIP (Ours) 8.8 6.2 5.8 5.2 4.2 4.8 4.4 4. 1.2 0.166 0.269 0.265 0.277 0.209 0.234 0.296 0.255 0.284 0.681 0.793 0.830 0.802 0.855 0.833 0.747 0.851 0.873 0.374 0.657 0.656 0.581 0.728 0.701 0.467 0. 0.739 0.486 0.522 0.546 0.568 0.581 0.584 0.589 0.470 0.598 5.234 5.437 5.437 5.578 5.594 5.459 5.597 5.658 6.039 9.43 6.67 4.33 12.5 5. 61.4 7.26 3.28 2.23 6.97 3.52 76.8 Table 1. Comparison of different methods, reorganized by controllability (CLIP-T, Image Reward), similarity (CLIP-I, DINO-I), and image quality (CLIP-IQA, Aesthetic). Flex denotes the models controllability, allowing for adjustable and dynamic modifications. ID-Pres represents the models ability to preserve the identity of the reference image. Bold text indicates the best result, while underlined text denotes the second-best result. stylization without relying on manual heuristics, effectively leveraging the strengths of both data modalities. By transforming the traditionally binary preservation-edit trade-off into continuous parametric control surface, this approach could facilitate wide range of applications. 4. Experiments 4.1. Training Dataset Training ideally requires image pairs showing the same subject in varied scenes or viewpoints, but such data are typically unavailable. Previous methods [59, 63] rely on simple augmentations that fail to represent realistic pose and viewpoint variations. We follow previous works [7, 8] by utilizing multi-view and video datasets, which naturally provide multiple frames of the same subject. Our dataset includes 1.23M varied samples and 11M invariant images, covering facial images, natural scenes, virtual try-on, human actions, saliency, and multi-view objects. To balance diversity and generalization, we resample video data to maintain 1:1 ratio between invariant and varied data, avoiding redundancy. For more details on the dataset construction, please refer to the supplementary materials. Moreover, previous works often use simplistic and unilimiting the form textual prompts across video frames, models ability to follow nuanced instructions. To improve textual conditioning and editing flexibility, we use Qwen2VL [58] to generate high-quality, distinct captions for each frame. This approach enhances the diversity and semantic relevance of textual guidance, improving the models ability to follow detailed editing instructions. 4.2. Evaluation Dataset and Metrics We collect evaluation data from DreamBench+ [34] and MSBench [54], comprising 187 unique subjects. Each image is tested using its set of 9 prompts, with 10 generation runs per prompt. This procedure results in 16,830 customized images used for comprehensive evaluation. We assess our model using several metrics. For identity preservation, we calculate similarity scores using DINOI [61], CLIP-I [36], after applying segmentation [26, 39] to remove background interference. For personalization, CLIP-T measures the semantic alignment between generated images and prompts in the CLIP text-image embedding space. Moreover, image quality is assessed using CLIPIQA [50] and CLIP-Aesthetic scores [46]. Additionally, we compute the mean ranking (mRank) of all metrics for each method to provide an overview of its overall performance. 4.3. Comparisons 4.3.1. Quantitative comparison In this experiment, we compared various methods in terms of personalization, preservation, image quality, and user study. The results are shown in the Tab. 1, where FlexIP outperformed all other methods across all evaluation metrics, particularly in mRank, personalization (CLIP-T), preservation (CLIP-I and DINO-I), image quality (CLIP-IQA and Aesthetic). In terms of personalization, FlexIP scored 0.284 on CLIP-T, which is slightly lower than λ-Eclipse. However, λ-Eclipse achieves this at the expense of subject preservation abilities. For preservation, FlexIP achieved high scores of 0.873 and 0.739 on CLIP-I and DINO-I, respectively, demonstrating its advantage in maintaining image details and semantic consistency. In image quality evaluation, FlexIP scored 0.598 on CLIP-IQA and 6.039 on Aesthetic, indicating superior quality and aesthetics of the 6 Figure 4. Qualitative comparison with other methods. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating wide range of diverse and personalized outputs. generated images. To provide more human-aligned evaluation for personalization, we adopt the MLM-Filter [53] to assess personalization. Unlike traditional methods like CLIP-Twhich rely on global contrastive features and often miss finegrained object detailsThe MLM-Filter utilizes advanced MLLM capabilities [3, 24, 51] to capture subtle object properties and semantic nuances, enabling precise, contextaware evaluations aligned with human judgment. Table 2 demonstrates that FlexIP excels across three complementary dimensionsimage-text matching (I-T Match), object detail satisfaction (Detail), and semantic understanding (Semantic). This highlights FlexIPs ability to effectively capture subtle visual nuances and accurately integrate meaningful auxiliary information, closely aligning with human preferences and expectations. To better demonstrate the effective of our methods, we evaluate the user satisfaction of different methods in practical applications, specifically focusing on flexibility (Flex) and identity preservation (ID-Pres). In this study, total of 33 samples were utilized for evaluation purposes. During each evaluation session, participants were presented with collection of images generated by various methods. group of 60 evaluators was then asked to make selections based on two criteria: the image that best aligns with the textual semantics and the image that best preserves the subject. As show in Tab. 1, FlexIP excelled in both metrics."
        },
        {
            "title": "Method",
            "content": "I-T Match Detail"
        },
        {
            "title": "Semantic",
            "content": "λ-Eclipse DisEnVisioner SSR-Encoder IP-Adapter MoMA"
        },
        {
            "title": "FlexIP",
            "content": "83.9 66.6 83.1 40.2 78.4 88.3 57.2 56.9 56.1 58.0 56.5 59.8 38.8 38.6 38.5 37.7 38.3 40. Table 2. The evaluation metrics among different methods. Among these dimensions, I-T Match stands for image-text matching, Detail represents object detail satisfaction, and Semantic refers to semantic understanding. FlexIP surpasses previous methods across all three complementary indicators. 4.3.2. Qualitative comparison To further evaluate FlexIPs capabilities, we present qualitative comparisons with five state-of-the-art methods across three distinct images per subject. As illustrated in Fig. 4, FlexIP generates images with significantly enhanced fidelity, editability, and identity consistency compared to existing approaches. Fig. 4 highlights FlexIPs ability to maintain subject preservation and personalization across reference images under identical textual instructions, confirming the effectiveness of the explicit trade-off in the model. Figure 5. The effectiveness of the dynamic weight gating mechanism. Figure 6. Comparison with other methods on style transfer tasks. two objectives during inference by adjusting the relative weights of the preservation and personalization adapters. The proposed gating mechanism disentangles the optimization pathways of the two adapters during training, thereby mitigating the suboptimal performance caused by implicit trade-offs in joint optimization scenarios. Qualitative results in Fig. 5 demonstrate that increasing the weight of the preservation adapter (e.g., γ(x) 1) prioritizes high-fidelity retention of the identity of the input subject, with minimal deviation in structural and textural details. In contrast, increasing the weight of the personalization adapter (e.g., γ(x) 0) improves editability, allowing greater stylistic transformations while maintaining semantic coherence. Critically, the linear interpolation between these weights enables users to smoothly traverse the preservation-editability spectrum at inference time, capability absent in static fusion approaches. Furthermore, we extended the model to the task of zeroshot style transfer, emphasizing instruction following and detailed image information extraction. As demonstrated in Fig. 6, our approach outperforms other methods in this task. This success is attributed to our dual adapters ability to extract detailed information and maintain balanced integration of detail extraction and instruction editing. 5. Conclusion 4.4. Ablation Study To validate the efficacy of the dynamic weight gating mechanism in explicitly balancing identity preservation and personalized editability, we conduct comprehensive ablation study. As illustrated in Fig. 5, our framework enables fine-grained control over the trade-off between these FlexIP is novel framework for flexible subject attribute editing in image synthesis, effectively balancing identity preservation and personalized editing. By decoupling these objectives into independently controllable dimensions, FlexIP overcomes the limitations of existing methods. Its dual-adapter architecture ensures the maintenance 8 of identity integrity by utilizing high-level semantic concepts and low-level spatial details. The dynamic weight gating mechanism allows users to control the trade-off between identity preservation and stylistic personalization, transforming the binary preservation-edit trade-off into continuous parametric control surface which offer robust and flexible solution for subject-driven image generation."
        },
        {
            "title": "References",
            "content": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: visual language model for few-shot learning. Advances in neural information processing systems, 35:2371623736, 2022. 2 [2] Sizhe An, Hongyi Xu, Yichun Shi, Guoxian Song, Umit Ogras, and Linjie Luo. Panohead: Geometry-aware 3d fullIn CVPR, pages 2095020959, head synthesis in 360deg. 2023. 12 [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 7 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In CVPR, pages 1839218402, 2023. 2 [5] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793, 2023. 3 [6] Nan Chen, Mengqi Huang, Zhuowei Chen, Yang Zheng, Lei Zhang, and Zhendong Mao. Customcontrast: multilevel contrastive perspective for subject-driven text-to-image customization. arXiv preprint arXiv:2409.05606, 2024. 3 [7] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. NeurIPS, 37:8401084032, 2024. [8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level image customization. In CVPR, pages 65936602, 2024. 2, 3, 5, 6 [9] Seunghwan Choi, Sunghyun Park, Minsoo Lee, and Jaegul Choo. try-on via Viton-hd: High-resolution virtual misalignment-aware normalization. In CVPR, pages 14131 14140, 2021. 12 [10] Prafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat GANs on image synthesis. In NeurIPS, 2021. 13 [11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. An image is worth one word: Personalizing text-toimage generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 3 [12] Jianzhu Guo, Xiangyu Zhu, Zhen Lei, and Stan Li. Face synthesis for eyeglass-robust face recognition. arXiv preprint arXiv:1806.01196, 2018. 12 [13] Jing He, LI Haodong, Guibao Shen, CAI Yingjie, Weichao Qiu, Ying-Cong Chen, et al. Disenvisioner: Disentangled and enriched visual prompt for customized image generation. In ICLR, 2024. 3, [14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 4, 13 [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 4 [16] Miao Hua, Jiawei Liu, Fei Ding, Wei Liu, Jie Wu, and Qian He. Dreamtuner: Single image is enough for subject-driven generation. arXiv preprint arXiv:2312.13691, 2023. 3 [17] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International conference on machine learning, pages 46514664. PMLR, 2021. 4 [18] Xuan Ju, Xian Liu, Xintao Wang, Yuxuan Bian, Ying Shan, and Qiang Xu. Brushnet: plug-and-play image inpainting In ECCV, model with decomposed dual-branch diffusion. pages 150168. Springer, 2024. 12 [19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In ICCV, pages 40154026, 2023. 12 [20] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In CVPR, pages 19311941, 2023. [21] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pretrained subject representation for controllable text-to-image generation and editing. NeurIPS, 36:3014630166, 2023. 3, 6 [22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 2 [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational conference on machine learning, pages 19730 19742. PMLR, 2023. 3 [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 7 [25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. arXiv preprint arXiv:2202.09778, 2022. [26] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 6 9 [27] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. In ICCV, 2015. Deep learning face attributes in the wild. 12 [28] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. 12 [29] Davide Morelli, Matteo Fincato, Marcella Cornia, Federico Landi, Fabio Cesari, and Rita Cucchiara. Dress code: HighIn CVPR, pages resolution multi-category virtual try-on. 22312235, 2022. [30] Jisu Nam, Heesu Kim, DongJae Lee, Siyoon Jin, Seungryong Kim, and Seunggyu Chang. Dreammatcher: appearance matching self-attention for semantically-consistent text-toIn Proceedings of the IEEE/CVF image personalization. Conference on Computer Vision and Pattern Recognition, pages 81008110, 2024. 2 [31] Kepan Nan, Rui Xie, Penghao Zhou, Tiehan Fan, Zhenheng Yang, Zhijie Chen, Xiang Li, Jian Yang, and Ying Tai. Openvid-1m: large-scale high-quality dataset for text-tovideo generation. In ICLR, 2025. 12 [32] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 12 [33] Maitreya Patel, Sangmin Jung, Chitta Baral, and Yezhou lambda-eclipse: Multi-concept personalized text-toYang. image diffusion models by leveraging clip latent space. arXiv preprint arXiv:2402.05195, 2024. 6 [34] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned bencharXiv preprint mark for personalized image generation. arXiv:2406.16855, 2024. 6 [35] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models for high-resolution image synthesis. In ICLR, 2024. 2 [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6, 12, [37] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 12 [38] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In ICCV, pages 10901 10911, 2021. 12 [39] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, et al. Grounded sam: Assembling open-world models for diverse visual tasks. arXiv preprint arXiv:2401.14159, 2024. 6 [40] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 2, 4, 13 [41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [42] Ciara Rowles, Shimon Vainer, Dante De Nigris, Slava Elizarov, Konstantin Kutsy, and Simon Donne. Ipadapterinstruct: Resolving ambiguity in image-based conditioning using instruct prompts. arXiv preprint arXiv:2408.03209, 2024. 3, 6 [43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 2, 3 [44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. In CVPR, pages 65276536, 2024. 2 [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 13 [46] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 6 [47] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image generation without testtime finetuning. In CVPR, pages 85438552, 2024. 3 [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 4, 12 [49] Kunpeng Song, Yizhe Zhu, Bingchen Liu, Qing Yan, Ahmed Elgammal, and Xiao Yang. Moma: Multimodal llm adapter for fast personalized image generation. In ECCV, pages 117 132. Springer, 2024. 3, [50] Jianyi Wang, Kelvin CK Chan, and Chen Change Loy. Exploring clip for assessing the look and feel of images. In Proceedings of the AAAI conference on artificial intelligence, pages 25552563, 2023. 6 [51] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 7 10 [52] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, Anthony Chen, Huaxia Li, Xu Tang, and Yao Hu. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [53] Weizhi Wang, Khalil Mrini, Linjie Yang, Sateesh Kumar, Yu Tian, Xifeng Yan, and Heng Wang. Finetuned multimodal language models are high-quality image-text data filters. 2024. 7 [54] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. In ICLR, 2025. 3, 6 [55] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint arXiv:2302.13848, 2023. 3, 6 [56] Liangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: high-quality dataset and benchmark In CVPR, pages 657666, for video face super-resolution. 2022. [57] Zhangyang Xiong, Chenghong Li, Kenkun Liu, Hongjie Liao, Jianqiao Hu, Junyi Zhu, Shuliang Ning, Lingteng Qiu, Chongjie Wang, Shijie Wang, et al. Mvhumannet: largescale dataset of multi-view daily dressing human captures. In CVPR, pages 1980119811, 2024. 12 [58] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. 6 [59] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 6 [60] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: largescale dataset of multi-view images. In CVPR, pages 9150 9161, 2023. 12 [61] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 6 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 2 [63] Yuxuan Zhang, Yiren Song, Jiaming Liu, Rui Wang, Jinpeng Yu, Hao Tang, Huaxia Li, Xu Tang, Yao Hu, Han Pan, et al. Ssr-encoder: Encoding selective subject representation for subject-driven generation. In CVPR, pages 80698078, 2024. 3, [64] Yanbing Zhang, Mengping Yang, Qin Zhou, and Zhe Wang. Attention calibration for disentangled text-to-image personIn Proceedings of the IEEE/CVF Conference alization. on Computer Vision and Pattern Recognition, pages 4764 4774, 2024. 2 [65] Hao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. Celebvhq: large-scale video facial attributes dataset. In ECCV, pages 650667. Springer, 2022. 12 11 A. More Analysis Figure 7. Visualization of attention maps across different modules.In the image, the white areas of the attention map indicate activation valuesthe whiter the color, the higher the activation value. It is evident that the two preservation modules function differently: the learnable query module concentrates more on the subjects details, while the CLIP CLS Embeds focus more on the subjects global aspects. Consequently, high-level and low-level information complement each other. For the personalization module, the text embeds pay more attention to the surrounding environment and some identity preservation details. This observation supports our decision to decouple preservation and personalization. As shown in Fig. 7, we found that learnable queries specialize in capturing fine-grained variations but lack strong global coherence, while CLIP CLS embeddings provide global identity consistency but may miss subtle subject details. Therefore, instead of relying on single embedding to learn both, we adopt divide and conquer strategy that integrating both for retrieving fine-grained adaptability and global robustness simultaneously from DINO patch embeddings B. More Experimental Details B.1. Implementation Details FlexIP is built on Stable Diffusion v1.5, utilizing OpenCLIP ViT-H/14 [36] as the high-level image encoder and DinoV2-L [32] as the low-level image encoder. The model is trained on 8 GPUs with 32GB of memory for 140,000 steps at resolution of 512512, with batch size of 16 per GPU, learning rate of 1e-4, and weight decay of 0.01. After 100,000 steps, it undergoes fine-tuning with higherquality images for an additional 40,000 steps. During training, classifier-free guidance is applied with 5% probabil12 Table 3. Data Information used for training. Quality specifically refers to the image resolution. Type Dataset Instances Quality Invariant Datasets (11.1M)"
        },
        {
            "title": "Image",
            "content": "SAM [19] BrushData [18] 9.0M 2.1M"
        },
        {
            "title": "High\nMedium",
            "content": "Variant Datasets (1.23M) MVImageNet [60] MVHumanNet [57] co3d [38] PanoHead [2] CelebA [27] MeGlass [12] VITON-HD [9] DressCode [29] SAM2 [37] CelebV-HQ [65] VFHQ [56] Pexel OpenVid1M [31]"
        },
        {
            "title": "High\nLow\nMedium\nHigh\nLow\nHigh\nMedium",
            "content": "51000 35666 15204 181038 633885 High Medium Medium High High Multi-View Video ity of dropping text, images, or both. For inference, DDIM sampling with 50 steps and guidance scale of 7.5 is used. As shown in Table 3, we include various types of datasets used for training. C. Background Diffusion Models. Diffusion models comprise family of generative models characterized by two fundamental pro- (i) diffusion process (forward process), which cesses: gradually corrupts data through fixed -step Markov chain by adding Gaussian noise, and (ii) denoising process that iteratively recovers data from noise via learnable model. For conditional variants like text-to-image models, the denoising process is guided by auxiliary inputs such as text prompts. The training objective for the noise prediction network ϵθ optimizes simplified variational bound: Lsimple = Ex0,ϵN (0,I),c,t ϵ ϵθ(xt, c, t)2 , (6) where x0 denotes clean data, represents conditioning signals, {1, . . . , } indexes the diffusion timestep, and xt = αtx0 + σtϵ describes the noisy state at step with αt, σt being predefined noise scheduling coefficients. During inference, initial noise xT (0, I) is progressively denoised through iterations. Accelerated sampling is typically achieved via deterministic ODE solvers like DDIM [48], PNDM [25], or adaptive-step methods like DPM-Solver [28]. For conditional diffusion models, classifier guidance [10] balances image fidelity and sample diversity by leveraging gradients from an independently trained classifier. To circumvent the requirement for separate classifier, classifier-free guidance [14] is widely adopted. This method jointly trains conditional and unconditional denoising paths by randomly omitting the condition with probability pdrop during training. At inference, the noise prediction is interpolated between the conditional and unconditional outputs: ˆϵθ(xt, c, t) = ϵθ(xt, c, t) + (1 w) ϵθ(xt, t), (7) where > 1 (termed the guidance scale) amplifies alignment with the condition c. For text-to-image diffusion models, this mechanism critically strengthens the semantic correspondence between generated images and text prompts. In our work, we implement the FlexIP atop the opensource Stable Diffusion (SD) framework [40]. SD operates as latent diffusion model conditioned on text embeddings from frozen CLIP text encoder [36]. Its backbone comprises time-conditional U-Net [41] with cross-attention layers that fuse text features into the diffusion process. Unlike pixel-space models (e.g., Imagen [45]), SD achieves computational efficiency by performing diffusion in the latent space of pretrained variational autoencoder, reducing dimensionality by factor of 464 compared to raw pixels. 13 14 Figure 8. Qualitative comparison with other methods in animal domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating wide range of diverse and personalized outputs. Figure 9. Qualitative comparison with other methods in human domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating wide range of diverse and personalized outputs. Figure 10. Qualitative comparison with other methods in object domain. Our approach surpasses alternative methods in its exceptional ability to preserve identity while generating wide range of diverse and personalized outputs."
        }
    ],
    "affiliations": []
}