{
    "paper_title": "Resa: Transparent Reasoning Models via SAEs",
    "authors": [
        "Shangshang Wang",
        "Julian Asilis",
        "√ñmer Faruk Akg√ºl",
        "Enes Burak Bilgin",
        "Ollie Liu",
        "Deqing Fu",
        "Willie Neiswanger"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, a family of 1.5B reasoning models trained via a novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from a source model, and then uses the trained SAE to guide a standard supervised fine-tuning process to elicit such abilities in a target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterpart's reasoning performance while reducing training costs by >2000x to roughly \\$1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around \\$1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on a larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 7 6 9 9 0 . 6 0 5 2 : r Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, √ñmer Faruk Akg√ºl, Enes Burak Bilgin, Ollie Liu, Deqing Fu, and Willie Neiswanger"
        },
        {
            "title": "University of Southern California",
            "content": "How cost-effectively can we elicit strong reasoning in language models by leveraging their underlying representations? We answer this question with Resa, family of 1.5B reasoning models trained via novel and efficient sparse autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to capture reasoning abilities from source model, and then uses the trained SAE to guide standard supervised fine-tuning process to elicit such abilities in target model, all using verified question-answer data without any reasoning traces. Notably, when applied to certain base models before further RL post-training, SAE-Tuning retains >97% of its RL-trained counterparts reasoning performance while reducing training costs by >2000x to roughly $1 and training time by >450x to around 20 minutes. Furthermore, when applied to lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only around $1 additional cost. Surprisingly, the reasoning abilities extracted via SAEs are potentially both generalizable and modular. Generality means abilities extracted from one dataset still elevate performance on larger and overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math can be attached to the R1-Distill model at test time, without any retraining, and yield comparable gains. Extensive ablations validate these findings and all artifacts are fully open-sourced. Notion Blog: https://shangshangwang.notion.site/resa Code Repository: https://github.com/shangshang-wang/Resa Training Logs: https://wandb.ai/upup-ashton-wang-usc/Resa Model Weights & Checkpoints: https://huggingface.co/Resa-Yi 1. Introduction Reasoning language models have demonstrated increasing performance in domains like math, coding, and science (Wang and Neiswanger, 2025, Xu et al., 2025). Despite the impressive reasoning performance elicited by reinforcement learning (RL) or supervised fine-tuning (SFT) (Chu et al., 2025), these methods often operate as black box. In other words, while they improve reasoning, how they alter the models internal representations to do so is largely opaque. Furthermore, RL-based workflows are notoriously resourceintensive, requiring vast computational power and long training time to converge. On the other hand, SFT hinges on the availability of high-quality Chain-of-Thought (CoT) reasoning traces, which are costly to curate (Muennighoff et al., 2025). This leaves critical gap in the field: The need for three-birds-one-stone method that can elicit strong reasoning abilities in way that is not only effective but also computationally efficient and transparent. In this paper, we bridge this gap with Resa, family of 1.5B reasoning models trained via sparse autoencoders (SAEs), using novel SAE-Tuning procedure. SAEs are unsupervised models designed to deconstruct models dense internal activations into sparse dictionary of more interpretable latent features (Anthropic, 2023, 2024). Our key insight is that within this dictionary, certain features must correspond to the fundamental building blocks of reasoning. By instilling latent reasoning features captured by an SAE back into model via tuning procedure, we can effectively and efficiently elicit the models reasoning abilities. Corresponding author(s): Shangshang Wang shangshangwang.github.io; Willie Neiswanger neiswang@usc.edu Resa: Transparent Reasoning Models via SAEs Figure 1: Comparison between Example Resa Models and Baselines The Tina models correspond to the best checkpoints in Wang et al. (2025a). Resa-STILL and Resa-DeepScaleR correspond to Resa-STILL-v5 and Resa-DeepScaleR-v3 in Table 4, respectively. For these Resa models, the required SAEs are trained from scratch (as shown in Section 3.2) and both computational and time costs are total costs for training SAEs and models. Reasoning performance denotes the average zero-shot Pass@1 score across AIME24/25, AMC23, MATH500, GPQA Diamond, and Minerva benchmarks. Specifically, SAE-Tuning involves two key stages: First, we use an SAE to probe the internal activations of source model, identifying and extracting dictionary of latent features that correspond to its reasoning processes. Second, we freeze this feature-rich SAE and insert it into target model to guide SFT process to elicit reasoning abilities in the target model. SAE-Tuning also distinguishes itself from existing methods by using SFT on minimal verified CoT-free question-answer data type. By verified, we mean that the answer correctness is ensured via methods like human annotation or language-model-based verification (Guha et al., 2025), while CoT-free signifies that our SAE-Tuning procedure functions without needing explicit step-by-step reasoning traces. Crucially, our control experiments in Table 4 demonstrate that performing standard SFT on this same CoT-free data without an SAE fails to elicit any meaningful reasoning, highlighting the vital role of the SAE. We summarize our core contributions as follows: Efficient Reasoning Ability Elicitation Purely using verified CoT-free data, we demonstrate that SAETuning can be applied in an end-to-end manner to certain base models with trained-from-scratch SAE to elicit reasoning abilities on par with those achieved via costly RL. This leads to substantial gains with peak training cost reductions of over 2000x (to approximately $1) and time reductions of over 450x (to under 20 minutes) compared to RL-based workflows, while maintaining comparable performance. Generalizable and Modular Reasoning Ability We establish the generality and modularity of the extracted reasoning abilities such that these abilities generalize across out-of-distribution datasets and can be attached to models within the same family at test time without additional training, functioning as portable reasoning adapter. Transparent Reasoning Feature Extraction We provide transparent view into the models reasoning abilities. Specifically, we propose novel prompt-only method to extract and quantify latent reasoning features using SAEs and show that the layer-wise distribution of these reasoning features correlates with reasoning performance of Resa models, offering data-driven path to optimizing SAE-Tuning. 2 Resa: Transparent Reasoning Models via SAEs 2. Resa: Transparent Reasoning Models via SAEs Resa is family of 1.5B transparent reasoning models derived from the Tina models (Wang et al., 2025a) and R1-Distill1 (DeepSeek-AI, 2025) via SAEs. The transparent designation reflects our methodologys focus on interpretability. We use an SAE to explicitly isolate and extract implicit reasoning abilities (i.e., latent reasoning features) from source model, and use that trained SAE to controllably instill those features into target model to elicit reasoning abilities. The entire procedure relies on novel SAE-Tuning procedure, which uses only verified CoT-free question-answer data. Figure 2: Two-Stage Pipeline of SAE-Tuning The procedure begins with SAE training (Left), where an SAE is trained to capture reasoning features from source model with trigger dataset. During SAE-guided SFT (Right), the trained SAE is then frozen and inserted into target model. An elicitation dataset is used to guide SFT process to elicit the reasoning abilities in the target model. Notably, the trigger and elicitation datasets are usually the same CoT-free data. See Section 2.1 for description of each component. 2.1. Sparse Autoencoder Tuning SAE-Tuning is an efficient two-stage training procedure designed to transfer implicit reasoning abilities from source model to target model; this full procedure is summarized in Figure 2. The two stages consist of: Stage I: SAE Training (Reasoning Ability Extraction) The first stage involves training an SAE to reconstruct the activations from specific layer of source model. We feed trigger dataset, comprising only verified CoT-free question-answer pairs, into the source model and capture the resulting activations at chosen SAE hookpoint. The SAE is then trained on these activations, learning features that represent the source models internal reasoning while processing the trigger data. Our central insight is that subset of these features corresponds to the source models implicit reasoning abilities. Stage II: SAE-Guided SFT (Reasoning Ability Elicitation) Once the SAE has been trained, we shift to training target model. The trained SAE is actively integrated into the target models architecture (at certain layer) and kept with frozen weights during standard SFT process. By exposing the target model to the feature representations captured by the SAE, the SFT process is guided to develop internal pathways that elicit reasoning abilities, effectively reconstructing such abilities extracted from the source model. This entire stage uses an elicitation dataset, which is typically identical to the trigger dataset. 1deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 3 Resa: Transparent Reasoning Models via SAEs The SAE-Tuning procedure is configured by five key components. Table 1 details the configurations used to create each of our main Resa models in this paper. Mai Mode Source Model Target Model Trig ger Data Elicitation Data SAE Tr aining Mode Resa-STILL-v1 (in Table 3) Tina-STILL Resa-STILL-v2 (in Table 4) - Resa-STILL-v3 (in Table 4) Resa-STILL-v4 (in Table 4) Resa-STILL-v5 (in Table 4) Tina-STILL R1-Distill R1-Distill Resa-DeepScaleR-v1 (in Table 3) Tina-DeepScaleR Resa-DeepScaleR-v2 (in Table 4) Tina-DeepScaleR Resa-DeepScaleR-v3 (in Table 4) R1-Distill Resa-DeepScaleR-v4 (in Table 5) Tina-STILL R1-Distill R1-Distill R1-Distill R1-Distill R1-Distill R1-Distill R1-Distill R1-Distill R1-Distill STILL - STILL STILL STILL STILL STILL STILL STILL STILL DeepScaleR DeepScaleR DeepScaleR DeepScaleR Fine-tuned Pre-trained Trained-from-Scratch Fine-tuned Trained-from-Scratch Fine-tuned Trained-from-Scratch DeepScaleR DeepScaleR Trained-from-Scratch STILL DeepScaleR Fine-tuned Table 1: Configurations of Main Resa Models Each row corresponds to Resa model and outlines the components used in SAE-Tuning. The bolded rows represent key configuration where reasoning abilities are extracted from the base R1-Distill model itself and then instilled back into the same model. In the following, we describe these components: Source Model The model from which reasoning-related features are extracted. The SAE is trained on the intermediate activations of one of its layers. Target Model The model for which we aim to elicit the reasoning abilities. In this paper, the target model is usually R1-Distill, which also serves as the base model of the source models, e.g., Tina models are more specialized models on R1-Distill. Trigger Dataset The CoT-free dataset used to trigger reasoning-related features in SAEs during SAE training. It is constructed from standard open-source question-answer dataset by simply formatting each entry into specific template. For given question and its final answer, we have an input sequence: Problem: [Question] <think> [Answer] </think> <answer> Answer: [Answer] </answer> Crucially, while this template uses <think> and </think> tokens, the dataset remains CoT-free as no intermediate reasoning steps are provided between the tokens, only the definitive answer is present. The inclusion of this structure is hypothesized to activate the source models latent reasoning abilities, allowing the SAE to capture the corresponding features. detailed analysis of the role and importance of these thinking tokens is provided in Section 5. Elicitation Dataset The CoT-free dataset used for the SAE-guided SFT of the target model to elicit reasoning abilities. In our experiments, it is usually the same as the trigger dataset. SAE Training Mode This defines how Stage (i.e., SAE training) is carried out. We explore three distinct modes. (1) Pre-trained: We use the pre-trained SAE on R1-Distill from EleutherAI.2 This mode bypasses Stage entirely as there is no need to train the SAE. (2) Fine-tuned: The default pre-trained SAE is further fine-tuned on activations from the source model using the trigger dataset. (3) Trained-from-Scratch: An SAE is trained from random initialization, exclusively on the activations produced by the source model with the trigger dataset. 2EleutherAI/sae-DeepSeek-R1-Distill-Qwen-1.5B-65k Resa: Transparent Reasoning Models via SAEs In the following, We now formalize the two primary stages of the SAE-Tuning procedure. Stage I: SAE Training Given source model M0 , we denote the SAE to be trained as s‚Ñì, which is hooked at ‚Ñì-th layer (i.e., the multilayer perceptron output) of the source model M0. Suppose the source model has layers with hidden dimension d. Given input x0 and output y, we denote the activation after the ‚Ñìth layer by x‚Ñì. We view the ‚Ñìth transformer block as function h‚Ñì and we then have x‚Ñì = h‚Ñì(x‚Ñì1), 1 ‚Ñì L, = softmax(xL). (1) (2) The SAE s‚Ñì trains an encoder enc Rmd for d, decoder dec Rdm with unit norm columns, and biases benc Rm, bdec Rd. For activation x‚Ñì, the SAE reconstructs activation x‚Ñì as = Top-k(W enc(x‚Ñì bdec) + benc), x‚Ñì = decz + bdec = wi fi, (3) (4) where Top-k means that we only the top features in the vector (Gao et al., 2024). This is simple and standard practice when training SAEs. During training, the SAE minimizes the reconstruction error 2. ‚Ñí = x‚Ñì x‚Ñì Stage II: SAE-Guided SFT For the trained SAE s‚Ñì with hookpoint ‚Ñì, we freeze its weights and insert it3 immediately after layer ‚Ñì of target model M. Note that this operation requires the source and target models to have the same underlying model architecture and size such that the SAE can be inserted directly. We denote the intermediate activation after i-th layer, before and after SAE insertion, as xi , respectively. Given the SAE s‚Ñì with hookpoint ‚Ñì, we have xi = xi, ‚Ñì 1 and the reconstructed activation x‚Ñì = SAE(x‚Ñì) propagates through the remaining layers to produce and xi (5) xi = hi( xi1), ‚Ñì + 1 L, = softmax( xL). (6) (7) We then add low-rank adapters of rank in each multilayer perceptron and attention sublayer of every layer of the target model we are adapting. We provide insights on why we choose low-rank adapters in Section 4. and Bi Rrd2. We train only Concretely, for each frozen weight matrix Wi Rd1 the low-rank adapters Œò = {Ai} {Bi}. The training objective is the KL divergence between the next token probability distribution with and without the SAE inserted: d2, we add Ai Rd1 argmin Œò ùíüKL( y, y). (8) The core intuition behind this loss function is to force the target model to produce internal representations at ‚Ñì-th layer that are compatible with the frozen SAE with rich reasoning features. Since the SAE was trained to reconstruct the reasoning-focused activations of the source model, this objective pushes the target models activations to become explicitly similar to the source models internal reasoning structure. In essence, by fine-tuning the target model to accommodate the SAE with minimal disruption to its output, we are instilling the reasoning patterns embodied by the SAEs learned features. Crucially, after this SAE-guided SFT is complete, the SAE is entirely removed from the model at test time. This leaves an enhanced target model with the elicited reasoning abilities instilled directly into its own parameters, ready for standard inference and evaluation. 3In this version, we only consider single-layer SAE insertion, i.e., insert at most one SAE at time. 5 Resa: Transparent Reasoning Models via SAEs In the following, we offer two more perspectives to build intuition for the SAE-Tuning procedure. Perspective I: Knowledge Distillation The SAE acts as knowledge bottleneck. In Stage I, it is forced to learn compressed and essential representation of the source models reasoning processes. In Stage II, it becomes teacher. The KL divergence objective distills this knowledge into the target (student) models LoRA parameters, effectively teaching the student model to replicate the teachers reasoning behavior while being guided by the explicit reasoning features captured by the SAE. We provide more detailed discussion of this perspective in Section 3.2. Perspective II: Alternating Optimization We are optimizing two distinct sets of parameters for two different goals, one after the other. 1) Optimizing SAE: We hold the model constant and train the SAE parameters to best capture the models latent reasoning features. 2) Optimizing Model Adapters: We hold the model and the SAE constant and train the low-rank adapters to best integrate the SAEs reasoning feature representation into the model. Combining these two goals allows the SAE to act as bridge between the two models. The first optimization step builds this bridge by learning compressed blueprint of the source models reasoning process. The second step then fine-tunes the target model to align with and cross this bridge, ensuring it inherits the structural properties of the source models reasoning without needing to learn them from scratch. 2.2. Practical Implementation Setup In the following, we show our practical setup for implementing SAE-Tuning. Particularly, we demonstrate that this implementation is efficient and only requires minimal setup and computational cost. Expe im ental Task Tr aining Cost Est. Evaluation Cost Est. Total Cost Est. Main: Resa-STILL-v1 (in Table 3) Main: Resa-STILL-v2 (in Table 4) Main: Resa-STILL-v3 (in Table 4) Main: Resa-STILL-v4 (in Table 4) Main: Resa-STILL-v5 (in Table 4) Main: Resa-DeepScaleR-v1 (in Table 3) Main: Resa-DeepScaleR-v2 (in Table 4) Main: Resa-DeepScaleR-v3 (in Table 4) Main: Resa-DeepScaleR-v4 (in Table 5) Ablation: Algorithm (in Table 3) Ablation: Source Model (in Table 4) Ablation: SAE Training Mode (in Table 4) Hypothesis: Generality (in Table 5) Hypothesis: Modularity (in Table 5) Hypothesis: Transparency (in Table 6) Total: All Tasks Total: Main Tasks Total: Best Resa Model $ $1 $1 $1 $1 $1 $ $1 $1 $2 $14 $3 $ $102 $26 $163 $9 $1 $ $4 $4 $4 $4 $6 $ $6 $6 $10 $60 $12 $ $8 $104 $273 $44 $4 $ $5 $5 $5 $5 $7 $ $7 $7 $12 $74 $15 $ $120 $130 $436 $53 $5 Table 2: Computational Cost Breakdown We provide detailed cost breakdown of all experiments in this paper. Notice that the training cost estimate includes the costs for training both models and SAEs. 6 Resa: Transparent Reasoning Models via SAEs Training Setup Our experiments are designed to be both effective and reproducible. The SAE-Tuning training code is built on the combination of OpenR1 (Hugging Face, 2025) and Sparsify.4 The default configuration for SAE-Tuning is as follows. The primary datasets are STILL5 (RUCAIBox STILL Team, 2025) and DeepScaleR6 (Luo et al., 2025). The source model varies across experiments, from specialized fine-tuned models (i.e., Tina models) to their base R1-Distill model (DeepSeek-AI, 2025). By default, we choose the best Tina checkpoint trained on specific dataset as the source model. The target model is by default the R1-Distill. For SAE, unless stated otherwise, we follow the fine-tuned training mode. The SAE is hooked to the output of the multilayer perceptron submodule after the 12th layer (out of 28) of the source model. This choice is based on the heuristic that middle layers in transformer-based language model are often crucial for understanding and reasoning. We provide detailed discussion on layer selection in Section 5. The full hyperparameter is provided in Appendix A. Evaluation Setup All evaluations reported herein utilize the lighteval framework (Fourrier et al., 2023) integrated with the vLLM (Kwon et al., 2023) inference engine for efficiency. We maintain fixed hardware configuration (two GPUs) and apply standardized set of vLLM inference parameters across all evaluated models. All scores are zero-shot Pass@1 performance. Particularly, we evaluate the reasoning abilities of models across diverse suite of six reasoning benchmarks, primarily focused on mathematical and scientific reasoning: AIME24/25 (Art of Problem Solving, 2024), AMC23 (Art of Problem Solving, 2023), MATH500 (Hendrycks et al., 2021, Lightman et al., 2023), Minerva (Lewkowycz et al., 2022), and GPQA Diamond (short as GPQA in this rest of the paper) (Rein et al., 2024). Overall Budget primary motivation for developing SAE-Tuning is to democratize research into reasoning models by establishing low-cost and high-efficiency paradigm via SAEs. We deliberately constrain our setup to minimal hardware footprint, using just 2 NVIDIA L40S or NVIDIA RTX 6000 Ada GPUs for all training and evaluation tasks. This setup is readily accessible on major cloud platforms, with an approximate cost of $1 USD per GPU hour at the time of our experiments. As detailed in Table 2, this approach demonstrates high cost-efficiency. We believe this setup provides valuable testbed for the broader research community. 3. Efficient Reasoning Ability Elicitation In this section, we empirically validate the effectiveness of SAE-Tuning. We first demonstrate that it can successfully replicate the performance of models fully trained with RL. Building on this, we show its primary practical utility: that SAE-Tuning remains effective with the base R1-Distill model as source model, thus bypassing the need for expensive further RL. In addition, we establish the methods self-sufficiency with trained-from-scratch SAE, which eliminates the dependence on pre-trained SAEs. 3.1. Proof of Concept: Reasoning Ability Replication We establish proof of concept by answering: Can SAE-Tuning extract and transfer reasoning ability from source model post-trained for reasoning via RL? Therefore, we use the Tina models (Wang et al., 2025a), which were trained with RL, as our source models. The goal is to see if our Resa models can match the Tina models performance. Table 3 summarizes our proof of concept results. On the STILL dataset, our Resa-STILL-v1 (47.28% avg) 4https://github.com/EleutherAI/sparsify 5RUC-AIBOX/STILL-3-Preview-RL-Data 6agentica-org/DeepScaleR-Preview-Dataset Resa: Transparent Reasoning Models via SAEs Mode Name AIM E2 4 AIME25 AM C23 MATH500 GPQA Mi nerva Avg. DeepSeek-R1-Distilled-Qwen-1.5B STILL-3-1.5B-preview Tina-STILL (CoT-free RL) Resa-STILL-v1 (CoT-free SAE-Tuning) DeepScaleR-1.5B-Preview Tina-DeepScaleR (CoT-free RL) Resa-DeepScaleR-v1 (CoT-free SAE-Tuning) 23.33 26. 36.67 33.33 36.67 43.33 36.67 16. 26.67 30.00 33.33 26.67 26.67 23. 62.50 67.50 77.50 75.00 77.50 67. 85.00 82.60 86.40 84.60 83.80 87. 86.20 83.00 31.82 30.15 41.18 34. 33.33 29.41 31.82 37.88 32.35 27. 26.84 28.79 31.99 28.68 33.33 44. 48.16 47.28 48.74 48.38 48.95 lgorithm blation AIM E2 4 AIME25 AM C23 MATH500 GPQA Mi nerva Avg. STILL-CoT-free-SFT DeepScaleR-CoT-based-SFT 20.00 10.00 16. 6.67 60.00 57.50 81.20 68.60 29. 20.22 26.36 36.36 39.00 33.22 Table 3: Reasoning Ability Replication SAE-Tuning successfully replicates the performance of RL-trained source models. Resa models (trained with SAE-Tuning on CoT-free data) achieve performance on par with or exceeding their Tina counterparts. More detailed results are shown in Appendix B.1. recovers 98.2% of the performance of the RL-trained Tina-STILL (48.16% avg). On the DeepScaleR dataset, our Resa-DeepScaleR-v1 (48.95% avg) not only replicates but slightly surpasses the performance of its corresponding Tina-DeepScaleR source model (48.38% avg). The algorithm ablation clearly demonstrates the necessity of our method using SAEs: standard SFT on the same CoT-free data (i.e., STILL-CoT-free-SFT) achieves mere 39.00% average, falling far short of both the Tina models and our Resa counterparts. This shows that simply training on the final answers is insufficient and the SAE-guided SFT is the critical ingredient for eliciting reasoning abilities. Furthermore, standard SFT on CoT-based dataset7 (i.e., DeepScaleR-CoTbased-SFT) also performs worse (33.22% avg), suggesting that naive CoT-based training is not an effective strategy for improving reasoning and underscores the novelty of our CoT-free approach via SAE-Tuning. 3.2. Practical Utility: End-to-End Reasoning Ability Elicitation Having proven that SAE-Tuning can replicate reasoning ability, we now study: Can one bypass the need for specialized source model and the need for an existing SAE altogether? Specifically, this section investigates if we can simplify SAE-Tuning to an end-to-end procedure such that one can elicit reasoning abilities directly from the base R1-Distill model, without the need for pre-trained SAE for this model. SAE Simplification The SAE training mode ablation in Table 4 shows that training an SAE from scratch on the trigger dataset (i.e, Resa-STILL-Trained-from-Scratch-SAE, 47.36% avg) is just as effective as fine-tuning generic pre-trained SAE on the same dataset (i.e., Resa-STILL-Finetuned-SAE, 47.28% avg). Both outperform using default, pre-trained SAE (44.99% avg). The key insight is that the SAEs performance for reasoning ability elicitation hinges on its exposure to the specific reasoning features encapsulated in the data (i.e., the trigger dataset), while the general knowledge from its initial pre-training is less critical in SAE-Tuning. This result aligns with the knowledge distillation perspective of SAE-Tuning at the end of Section 2.1. In that view, the SAE is teacher guiding the student (i.e., the target model). static, pre-trained SAE is an ineffective teacher because it is ignorant of the curriculumthe reasoning patterns in the trigger dataset. In contrast, both training from scratch and fine-tuning are effective because they ensure the teacher first learns the specific material it is meant to teach. Overall, the finding simplifies the pipeline by eliminating the need for pre-trained SAE, which yields substantial compute savings by avoiding the costly pre-training on 7The DeepScaleR dataset originally contains CoT reasoning traces which are only used in this ablation study. Resa: Transparent Reasoning Models via SAEs large corpora like SmolLM2 (Allal et al., 2025) and RedPajama (Weber et al., 2024). Model Name STILL-3-1.5B-preview Tina-STILL AIM E24 AIM E25 AMC23 MATH500 GPQA Mi nerva Avg. 26.67 36.67 26.67 30.00 67.50 77. 86.40 84.60 34.34 33.33 27.57 26. 44.86 48.16 SA Tr in ing Mode bl ati on AIM E24 AIM E25 AMC23 MATH500 GPQA Mi nerva Avg. Resa-STILL-Finetuned-SAE (i.e., Resa-STILL-v1) Resa-STILL-Pretrained-SAE (i.e., Resa-STILL-v2) Resa-STILL-Trained-from-Scratch-SAE (i.e., Resa-STILL-v3) 33.33 23.33 33.33 33.33 23. 33.33 75.00 72.50 70.00 83.80 85. 83.00 29.41 30.51 30.15 28.79 34. 34.34 47.28 44.99 47.36 Source Mode bl ation ( ine -t un ed SAE) AIM E24 AIM E25 AMC23 MATH500 GPQA Mi nerva Avg. Resa-STILL-Tina-0-step (i.e., Resa-STILL-v4) Resa-STILL-Tina-1-step Resa-STILL-Tina-10-step Resa-STILL-Tina-50-step Resa-STILL-Tina-100-step Resa-STILL-Tina-500-step Resa-STILL-Best-Tina-2000-step (i.e., Resa-STILL-v1) Resa-STILL-Tina-3000-step 23.33 40.00 23.33 33. 43.33 36.67 33.33 26.67 20.00 26. 23.33 26.67 23.33 26.67 33.33 23. 77.50 70.00 75.00 72.50 82.50 80. 75.00 72.50 84.60 83.20 82.20 82. 85.60 83.80 83.80 85.40 27.57 31. 29.41 27.57 28.68 31.62 29.41 27. 35.35 36.36 33.33 38.89 33.33 31. 28.79 34.85 44.73 47.98 44.43 46. 49.46 48.43 47.28 45.11 Source Mode bl ation ( Tr aine dfrom -Sc atc SAE ) AIM E24 AIM E25 AMC23 MATH500 GPQA Mi nerva Avg. Resa-STILL-Tina-0-step (i.e., Resa-STILL-v5) Resa-STILL-Tina-1-step Resa-STILL-Tina-10-step Resa-STILL-Tina-50-step Resa-STILL-Tina-100-step Resa-STILL-Tina-500-step Resa-STILL-Best-Tina-2000-step (i.e., Resa-STILL-v3) Resa-STILL-Tina-3000-step Resa-DeepScaleR-Best-Tina-1000-step (i.e., Resa-DeepScaleR-v2) Resa-DeepScaleR-Tina-0-step (i.e., Resa-DeepScaleR-v3) 33.33 33.33 33. 43.33 33.33 36.67 33.33 30.00 40. 33.33 26.67 33.33 16.67 23.33 23. 20.00 33.33 20.00 30.00 23.33 70. 72.50 67.50 77.50 90.00 67.50 70. 77.50 75.00 80.00 87.00 82.20 86. 83.40 82.60 84.20 83.00 86.20 84. 86.00 29.41 29.41 30.51 29.41 28. 30.88 30.15 31.62 30.15 30.51 41. 35.35 37.37 38.89 35.35 35.35 34. 37.88 33.33 31.31 48.06 47.69 45. 49.31 48.88 45.77 47.36 47.20 48. 47.41 Table 4: End-to-End Reasoning Ability Elicitation (SAE Training Mode Ablation) Training an SAE from scratch is comparably as effective as fine-tuning pre-trained SAE. (Source Model Ablations) The key results are Resa-STILL-v5 and Resa-DeepScaleR-v3 models which use the base model as its own source and match the reasoning performance of the RL-trained models. More detailed results are shown in Appendix B.2. Source Model Simplification Based on the above SAE simplification, we then simplify the source model used for SAE training, using models ranging from the base R1-Distill model (i.e., Tina-0-step) to well-trained Tina checkpoints. We notice that the source of reasoning features is nuanced such that there is non-monotonic relationship between the source models training progression and the resulting Resa models performance. The best reasoning features for extraction are not always found in the final, most-trained source model checkpoint. Specifically, it shows that one can get optimal performance with light RL training, i.e., ResaSTILL-Tina-100-step (with fine-tuned SAE, 49.46%) and Resa-STILL-Tina-50-step (with train-from-scratch SAE, 49.31%). Another important finding is that by training an SAE from scratch and using the base model as the source, our method achieves competitive average score of 48.06% (i.e., Resa-STILL-v5). This performance is nearly identical to the fully RL-trained Tina-STILL model (48.16%), demonstrating that our simplified, end-to-end SAE-Tuning procedure has potential to replace the RL fine-tuning stage with no meaningful loss in reasoning performance. This also confirms that the necessary reasoning features are already latent within the base model and can be elicited with high efficiency. Overall, this presents practitioners with trade-off: using lightly RL-trained source yields peak performance, while using the 9 Resa: Transparent Reasoning Models via SAEs base model enables maximally efficient, end-to-end workflow that still delivers competitive results. 4. Hypothesis: Generalizable and Modular Reasoning Ability We now test central hypothesis of our work: that the reasoning ability captured by SAE-Tuning is not brittle, dataset-specific artifact but rather generalizable and modular skill. We formulate this as testable claim: Reasoning abilities extracted via SAEs can be transferred across both data distributions and models. To validate this, we conduct two sets of experiments: First, we test out-of-distribution generalization by applying reasoning extracted from one dataset to another. Second, we test cross-model transfer by applying reasoning extracted from one model to another within the same family. Outof-Distribution Cover age Data AIME 24 AIME 25 AMC2 3 MAT 50 0 PQA Min erva Avg. DeepScaleR-1.5B-Preview Tina-DeepScaleR Resa-STILL2DeepScaleR (i.e., Resa-DeepScaleR-v4) 36.67 43.33 33.33 26.67 26. 30.00 77.50 67.50 80.00 87.80 86. 84.00 31.82 37.88 29.41 31.99 28. 35.86 48.74 48.38 48.77 Outof-Distribution In ter ecti on Data AIME 24 AIME 25 AMC2 3 MAT 50 0 PQA Min erva Avg. Open-RS Tina-Open-S1 Resa-STILL2Open-S1 II-Thought-1.5B-Preview Tina-II-Thought Resa-STILL2II-Thought Tina-OpenR Resa-STILL2OpenR1 26.67 43.33 36.67 30.00 40. 40.00 36.67 33.33 20.00 20.00 23. 23.33 20.00 23.33 26.67 30.00 72. 80.00 85.00 72.50 80.00 75.00 75. 77.50 83.60 84.00 84.60 86.80 86. 83.20 86.80 86.80 28.68 28.68 30. 30.88 33.84 31.25 30.51 27.21 35. 35.35 31.82 31.90 26.84 38.89 39. 41.92 44.47 48.56 48.72 45.90 47. 48.61 49.26 49.46 Reas oning-as-an-A dapt er AIME 24 AIME 25 AMC2 3 MAT 50 0 PQA Min erva Avg. Resa-STILL-Qwen-Math-Adapter Resa-STILL-Qwen-Adapter 36.67 30.00 20.00 30.00 82. 72.50 83.40 85.60 31.25 31.25 33. 35.86 47.86 47.54 Table 5: Generality and Modularity of Reasoning Ability (Top & middle) The results demonstrate OOD generalization across datasets. Resa-STILL2X models are trained by extracting reasoning from the STILL dataset and applying it to new elicitation dataset X. (Bottom) The results demonstrate cross-model transfer. reasoning adapter trained on Qwen-Math or Qwen is transferred to R1-Distill at inference time. More detailed results are shown in Appendix B.3. Out-of-Distribution Generalization To assess out-of-distribution (OOD) generalization, we use single dataset, STILL, to train the SAE on the source model (the trigger step). We then use that trained SAE to guide SFT process of the target model on completely different dataset (the elicit step). We test this on datasets that have varying degrees of overlap with STILL. Specifically, DeepScaleR fully covers the STILL dataset (which we refer as the coverage dataset) while Open-S1 (Dang and Ngo, 2025), II-Thought (Internet, 2025), and OpenR1 (Hugging Face, 2025) have underlying overlapped sources with STILL (which we coin as the intersection datasets). As shown in Table 5, the Resa-STILL2X models, where reasoning ability from STILL is transferred to new dataset X, consistently achieve performance on par with models trained end-to-end via RL on that new dataset. For example, Resa-STILL2DeepScaleR scores 48.77%, almost identical to Tina-DeepScaleR (48.38%) which was trained entirely on DeepScaleR. This pattern holds across all tested datasets. This robust performance demonstrates that the reasoning features extracted from the STILL dataset are not overfitted to its specific data distribution. They represent more general reasoning process that can be effectively applied to new distributions, showcasing OOD resilience. Modular Reasoning-as-an-Adapter Recall from Section 2.1 that during SAE-guided SFT, the parameters we 10 Resa: Transparent Reasoning Models via SAEs train are all from low-rank adapters. Therefore, we explore if the extracted reasoning ability can similarly be treated as modular adapter that can be plugged into other model. Specifically, we perform SAE-Tuning on models like Qwen-Math8 (Yang et al., 2024) and Qwen9 (Qwen et al., 2025) to produce set of adapters. Then, at test time, we attach such adapters to R1-Distill in the same family, without any further training. The models in this family share an architecture but differ in their foundational knowledge: R1-Distill has the most general knowledge, Qwen-Math is specialized with math data, and Qwen is the most basic. This tests whether our extracted reasoning abilities can be separated from the foundational knowledge of the model it was trained on. As shown in the final rows of Table 5, the adapter trained on Qwen-Math or Qwen and attached to R1-Distill achieves an average score of 47.86% or 47.54%, respectively. This performance is competitive with models where the entire SAE-Tuning process was performed directly on R1-Distill (e.g., Resa-STILL-v1, 47.28% avg). This result provides evidence that: Strong Reasoning Model Abstract Reasoning Ability + Foundational Knowledge. Our SAE-Tuning procedure aims to isolate the Abstract Reasoning Ability component into portable adapter and the final performance is then direct combination of this adapter with model that possesses sufficient Foundational Knowledge. This opens up possibilities for creating highly capable and efficient models by composing reasoning abilities and foundational knowledge. 5. Hypothesis: Transparent Reasoning Feature Extraction core claim of SAE-Tuning is that it provides transparent approach to reasoning. Having demonstrated that it works, we now investigate how it works. We notice that the performance varies not only depending on the source model but also depending on the specific layer chosen of the source model for SAE training. This moves us beyond heuristics for SAE layer selection to hypothesis: The suitability of model layer for reasoning is predictable and correlated with the presence of quantifiable reasoning features. To test this hypothesis, we introduce novel prompt-only reasoning feature extraction method and use it to establish the underlying correlation between these features and reasoning performance. Figure 3: Reasoning Feature Extraction (Left) This shows the layer-wise feature counts of the base R1-Distill model. (Middle) This shows the layer-wise feature counts of the Tina-STILL model. (Right) This shows the reasoning performance of the trained Resa models with different layer-wise SAEs when Tina-STILL is the source model. Prompt-Only Reasoning Feature Extraction We propose novel method to explicitly identify and quantify reasoning features and test if their distribution predicts the final performance of Resa model. We 8Qwen/Qwen2.5-Math-1.5B 9Qwen/Qwen2.5-1.5B 11 Resa: Transparent Reasoning Models via SAEs hypothesize that features specifically involved in reasoning should activate primarily when the model is prompted to think. Specifically, we pass the standard DeepSeek-R1 system prompt containing <think> and </think> tokens through model equipped with trained SAEs inserted after each layer indexed from 2 to 27. We cut off the first and final layer from the total 28 layers since these two layers are mainly used for embedding and next token prediction, respectively. We then define reasoning features as those SAE features that are exclusively and simultaneously activated at the <think> and </think> tokens and not by other parts of the prompt. Applying this method to the base R1-Distill model revealed an interesting pattern that the layer-wise count of these reasoning features exhibits tri-modal distribution around layer indices 3, 12, and 20 as shown in Figure 3. Model Name AIME24 AIME25 AM C23 MATH500 GPQA Minerva Avg. Feature Cnts. Resa-STILL-2nd-Layer Resa-STILL-3rd-Layer Resa-STILL-4th-Layer Resa-STILL-5th-Layer Resa-STILL-6th-Layer Resa-STILL-7th-Layer Resa-STILL-8th-Layer Resa-STILL-9th-Layer Resa-STILL-10th-Layer Resa-STILL-11th-Layer Resa-STILL-12th-Layer Resa-STILL-13th-Layer Resa-STILL-14th-Layer Resa-STILL-15th-Layer Resa-STILL-16th-Layer Resa-STILL-17th-Layer Resa-STILL-18th-Layer Resa-STILL-19th-Layer Resa-STILL-20th-Layer Resa-STILL-21st-Layer Resa-STILL-22nd-Layer Resa-STILL-23rd-Layer Resa-STILL-24th-Layer Resa-STILL-25th-Layer Resa-STILL-26th-Layer Resa-STILL-27th-Layer 26.67 26.67 33.33 40.00 33.33 26. 20.00 36.67 36.67 26.67 30.00 33. 33.33 36.67 30.00 43.33 43.33 33. 33.33 30.00 33.33 30.00 36.67 40. 36.67 30.00 30.00 36.67 20.00 23. 23.33 26.67 23.33 20.00 23.33 36. 26.67 33.33 20.00 23.33 20.00 16. 20.00 30.00 23.33 33.33 23.33 20. 20.00 30.00 20.00 36.67 80.00 70. 80.00 70.00 72.50 77.50 82.50 80. 67.50 77.50 77.50 75.00 77.50 80. 80.00 77.50 77.50 72.50 75.00 75. 75.00 80.00 77.50 70.00 65.00 67. 83.20 83.40 83.80 83.20 83.40 81. 85.20 84.20 84.80 83.80 84.60 83. 83.60 84.80 85.00 83.80 84.00 84. 85.00 84.60 82.20 82.00 83.80 84. 85.60 83.00 29.78 28.31 28.68 26. 31.62 27.94 29.78 27.57 29.78 31. 31.99 29.41 28.68 30.15 30.15 30. 33.82 29.04 29.04 27.57 31.99 29. 30.51 28.68 30.88 27.94 37.37 33. 36.87 44.95 35.35 35.86 33.33 34. 37.37 32.83 34.85 28.79 29.80 38. 34.85 36.36 37.88 36.87 29.29 36. 34.34 35.35 29.80 37.88 36.87 40. 47.84 46.48 47.11 48.05 46.59 46. 45.69 47.13 46.58 48.12 47.60 47. 45.48 48.97 46.67 48.09 49.42 47. 45.83 47.90 46.70 46.07 46.38 48. 45.84 47.59 1 4 3 2 3 0 3 3 1 3 3 3 0 0 5 3 0 1 2 1 2 2 Table 6: SAE Hookpoints Ablation Performance evaluation of Resa-STILL models where SAE-Tuning is applied to each layer indexed from 2 to 27 individually. Feature Cnts. is the number of identified reasoning features in the corresponding layer of the Tina-STILL model. More detailed results are shown in Appendix B.4. Feature Counts v.s. Reasoning Performance Correlation To test the hypothesis that this feature count distribution can predict reasoning performance, we conducted large-scale study that we created 26 different Resa-STILL models, with each one generated by applying SAE-Tuning to different layer of Tina-STILL, from layer 2 to 27. The results in Table 6 confirm that the choice of SAE hookpoint is critical. The average reasoning score fluctuates significantly, ranging from low of 45.48% (Layer 14) to high of 49.42% (Layer 12 Resa: Transparent Reasoning Models via SAEs 18). Also, naive interpretation, assuming more reasoning features equals better performance, is proven false. For instance, Layer 18 yields the top performance (49.42%) but has 0 identified reasoning features, while Layer 19 has the most features (5) but achieves lower score (47.66%). This validates our earlier finding that the source of reasoning features is nuanced. Such result indicates more complex relationship between reasoning features and reasoning abilities exists. The key insight to discover such relationship comes from analyzing the overall distributions rather than single points. Just as the feature counts across layers form tri-modal distribution, so does the final reasoning performance. Therefore, we fit 3-component Gaussian Mixture Model (3-GMM) to both distributions: (1) the priori reasoning feature counts from the base model, and (2) the final reasoning scores from our 26 Resa models. The GMM analysis reveals an interesting and close structural alignment between the two GMM distributions. The means of the three Gaussian components for the feature count distribution are located near layers 4.9, 14.5, and 22.7. The reasoning performance distributions components cluster around nearly identical means at layers 5.6, 15.1, and 23.0. This similarity extends to the component weights, which represent the proportion of layers belonging to each cluster. The feature distributions weights (41%, 37%, 22%) are closely mirrored by the reasoning performance distributions weights (39%, 37%, 24%). The overall spread of the two distributions, as measured by entropy, are nearly identical (3.194 for feature counts vs. 3.202 for reasoning performance). This suggests that while single layers feature count may not be good predictor, the overall structure of how reasoning is organized into three distinct layer-clusters within the model is robust predictor of how performance will be distributed. In practice, one can therefore analyze the source models feature distribution to strategically identify layer-clusters likely to yield high-performing models, providing data-driven and transparent method for optimizing the SAE-Tuning process. 6. Related Work Reinforcement Learning for Reasoning Ability Elicitation The structure of reasoning tasks lends itself well to RL approaches, primarily because the final outputs correctness provides clear and verifiable reward signal. This feedback loop helps the model develop more robust reasoning strategies (Shao et al., 2024, DeepSeek-AI, 2025). Recently, growing body of work suggests that RL primarily elicits and amplifies reasoning capabilities already embedded within pretrained models, rather than installing them from scratch. Training dynamics analysis supports this elicitation hypothesis, showing that post-training largely surfaces latent abilities (Zhao et al., 2025). The elicitation hypothesis is substantiated by several findings. For example, significant reasoning gains are achievable through minimal parameter updates that merely teach the model new output format (Wang et al., 2025a), and even through one-shot RL with data selection (Wang et al., 2025b). More surprisingly, studies have shown that RL can surface reasoning skills even with spurious or incorrect rewards (Shao et al., 2025), indicating that the primary mechanism is the surfacing of useful, pre-existing representations. We in this paper show that one can perform such elicitation in much more efficient way by bypassing RL. Sparse Autoencoders Recent advances in SAEs have enabled new approaches for analyzing and steering neural network computations. Building on the original SAE architecture proposed by Cunningham et al. (2023), subsequent work from Anthropic (2023) demonstrated how these sparse bottleneck networks can decompose transformer activations into human-interpretable features. The scaling properties of SAEs were systematically studied in Anthropic (2024), establishing practical guidelines for training SAEs across model sizes. Recent innovations have improved SAE training stability and feature quality: Li et al. (2024) introduced gradient stabilization techniques, while Rajamanoharan et al. (2024) developed methods for automatic dead feature resuscitation. Concurrent work by Belrose et al. (2024) and Marks and Tegmark (2024) has 13 Resa: Transparent Reasoning Models via SAEs focused on optimizing SAE computational efficiency and integration with modern transformer architectures. Karvonen et al. (2025) developed comprehensive evaluation on components of SAE training. In the SAE-Tuning procedure, we leverage SAEs specifically to isolate and extract the latent features that underpin reasoning abilities of the model. Model Steering The use of SAEs for model steering builds on earlier work in activation editing (Alain and Bengio, 2018). Todd et al. (2023) first demonstrated that SAE features could be used for controlled behavior modification, while Liu et al. (2024b) later developed more precise steering vectors through feature subspace analysis. Recent work by Mo et al. (2024) and Liu et al. (2024a) has formalized SAE-based steering as form of concept-based reinforcement learning, with Tan et al. (2024) demonstrating improved safety properties through SAE-mediated interventions and Gan et al. (2025) showing SAEs transferability of steering across modalities. Alternative to SAEs, activation differences (Li et al., 2023) and recursive feature machines (Beaglehole et al., 2025) are also widely used for steering, and Wu et al. (2025) evaluated the concept steering abilities of these methods. Besides steering, Chen et al. (2025) demonstrates the feasibility of adapting models to pretrained SAEs. Our proposed procedure, SAE-Tuning, beyond steering and adapting, fully leverages sparse autoencoders to identify, extract, and elicit latent reasoning abilities. 7. Conclusion In this work, we confronted the challenge of eliciting reasoning abilities from language models in threebirds-one-stone way that is effective, efficient, and transparent. We moved beyond the prevailing paradigms of resource-intensive RL and quality-sensitive CoT-based SFT. Specifically, we introduced SAE-Tuning, novel procedure that leverages SAEs to identify, extract, and elicit latent reasoning abilities using only CoT-free data. Our extensive experiments validated this approach on three key fronts. First, we demonstrated that SAE-Tuning is performant and practical method, capable of not only replicating the performance of RL-trained models but, more importantly, of eliciting equivalent reasoning abilities directly from certain base models. Second, we established the surprising generality of these extracted abilities, demonstrating both their robustness to out-of-distribution data and also their modularity as portable reasoning adapters. Finally, we provided new layer of transparency, showing that reasoning features are distributed in predictable pattern across model layers and that this structure correlates with the reasoning performance. 8. Acknowledgment We want to express our gratitude to the broader open-source community. This research was made possible by leveraging numerous publicly available resources, including training and evaluation framework, open datasets, accessible pre-trained language models, and the insights shared through technical reports. The computational resources required for the experiments described herein were partially provided by the Center for Advanced Research Computing (CARC) at the University of Southern California (USC). We are grateful for the support which enabled the training and evaluation of our models. J.A. was supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE-1842487. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. 14 Resa: Transparent Reasoning Models via SAEs"
        },
        {
            "title": "References",
            "content": "Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes, 2018. URL https://arxiv.org/abs/1610.01644. Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Mart√≠n Bl√°zquez, Guilherme Penedo, Lewis Tunstall, Andr√©s Marafioti, Hynek Kydl√≠ƒçek, Agust√≠n Piqueres Lajar√≠n, Vaibhav Srivastav, Joshua Lochner, Caleb Fahlgren, Xuan-Son Nguyen, Cl√©mentine Fourrier, Ben Burtenshaw, Hugo Larcher, Haojun Zhao, Cyril Zakka, Mathieu Morlon, Colin Raffel, Leandro von Werra, and Thomas Wolf. Smollm2: When smol goes big data-centric training of small language model, 2025. URL https://arxiv.org/abs/2502.02737. Anthropic. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread, 2023. Anthropic. Scaling monosemanticity: Extracting interpretable features from Claude 3 Sonnet. Transformer Circuits Thread, 2024. Art of Problem Solving. Amc problems and solutions, 2023. URL https://artofproblemsolving.com/ wiki/index.php/AMC_12_Problems_and_Solutions. Art of Problem Solving. Aime problems and solutions, February 2024. URL https:// artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions. Daniel Beaglehole, Adityanarayanan Radhakrishnan, Enric Boix-Adser√†, and Mikhail Belkin. Toward universal steering and monitoring of ai models, 2025. URL https://arxiv.org/abs/2502.03708. Nora Belrose, Zach Furman, and Logan Smith. Efficient sparse autoencoders for language models. arXiv preprint arXiv:2410.13928, 2024. Matthew Chen, Joshua Engels, and Max Tegmark. Low-rank adapting models for sparse autoencoders, 2025. URL https://arxiv.org/abs/2501.19406. Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc V. Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training, 2025. URL https://arxiv.org/abs/2501.17161. Hoagy Cunningham, Adam Ewart, Lukas Riggs, Trenton Bricken, and Lee Templeton. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600, 2023. Quy-Anh Dang and Chris Ngo. Reinforcement learning for reasoning in small LLMs: What works and what doesnt, 2025. URL https://arxiv.org/abs/2503.16219. DeepSeek-AI. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Cl√©mentine Fourrier, Nathan Habib, Hynek Kydl√≠ƒçek, Thomas Wolf, and Lewis Tunstall. Lighteval: lightweight framework for llm evaluation, 2023. URL https://github.com/huggingface/ lighteval. Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, and Willie Neiswanger. Textual steering vectors can improve visual understanding in multimodal large language models, 2025. URL https://arxiv.org/abs/2505.14071. Resa: Transparent Reasoning Models via SAEs Leo Gao, Tom Dupr√© la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders, 2024. URL https://arxiv.org/abs/ 2406.04093. Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal, Marianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https: //arxiv.org/abs/2103.03874. Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github. com/huggingface/open-r1. Intelligent Internet. Ii-thought : large-scale, high-quality reasoning dataset, 2025. Adam Karvonen, Can Rager, Johnny Lin, Curt Tigges, Joseph Bloom, David Chanin, Yeu-Tong Lau, Eoin Farrell, Callum McDougall, Kola Ayonrinde, Demian Till, Matthew Wearden, Arthur Conmy, Samuel Marks, and Neel Nanda. Saebench: comprehensive benchmark for sparse autoencoders in language model interpretability, 2025. URL https://arxiv.org/abs/2503.09532. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of Symposium on Operating Systems Principles (SOSP), 2023. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Proceedings of Advances in Neural Information Processing Systems (NeurIPS), volume 35, pages 38433857, 2022. Kenneth Li, Oam Patel, Fernanda Vi√©gas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from language model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=aLLuYpn83y. Yihan Li, Kevin Yang, Dan Klein, and Jacob Steinhardt. Sparse autoencoders enable scalable and reliable concept-based reward modeling. arXiv preprint arXiv:2406.04093, 2024. Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In Proceedings of International Conference on Learning Representations (ICLR), 2023. Ziming Liu, Yilun Shi, Chujie Zheng, Yixin Chen, and Hao Liu. Principled steering of language models via sparse concept activation vectors. arXiv preprint arXiv:2411.08790, 2024a. 16 Resa: Transparent Reasoning Models via SAEs Ziming Liu, Yilun Shi, Chujie Zheng, Yixin Chen, and Hao Liu. Steering large language models with sparse concept activations. arXiv preprint arXiv:2411.02193, 2024b. Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. DeepScaleR: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Samuel Marks and Max Tegmark. Sparse autoencoders improve large language models. arXiv preprint arXiv:2410.20526, 2024. Anson Mo, Senthooran Rajamanoharan, and Neel Nanda. SAE-based concept erasure in language models. arXiv preprint arXiv:2411.07122, 2024. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand√®s, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Senthooran Rajamanoharan, Deven Misener, and Neel Nanda. Sparse autoencoders for adaptive regularization. arXiv preprint arXiv:2408.05147, 2024. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. GPQA: graduate-level google-proof Q&A benchmark. In Proceedings of Conference on Language Modeling (COLM), 2024. RUCAIBox STILL Team. STILL-3-1.5B-preview: Enhancing slow thinking abilities of small models through reinforcement learning. 2025. URL https://github.com/RUCAIBox/Slow_Thinking_with_LLMs. Rulin Shao, Shuyue Stella Li, Rui Xin, Scott Geng, Yiping Wang, Sewoong Oh, Simon Shaolei Du, Nathan Lambert, Sewon Min, Ranjay Krishna, Yulia Tsvetkov, Hannaneh Hajishirzi, Pang Wei Koh, and Luke Zettlemoyer. Spurious rewards: Rethinking training signals in rlvr. https://rethink-rlvr.notion.site/SpuriousRewards-Rethinking-Training-Signals-in-RLVR-1f4df34dac1880948858f95aeb88872f, 2025. Notion Blog. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. DeepSeekMath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/2402.03300. Jerry Tan, Yihan Li, and Jacob Steinhardt. Improving language model safety using sparse feature activation. arXiv preprint arXiv:2501.11036, 2024. Ethan Todd, Kevin Li, Nora Belrose, Nora Bjorck, and David Bau. Steering Llama 2 via contrastive activation addition. arXiv preprint arXiv:2308.10248, 2023. Shangshang Wang and Willie Neiswanger. LLM reasoning: Curated insights, 2025. URL https: //shangshangwang.notion.site/llm-reasoning. 17 Resa: Transparent Reasoning Models via SAEs Shangshang Wang, Julian Asilis, √ñmer Faruk Akg√ºl, Enes Burak Bilgin, Ollie Liu, and Willie Neiswanger. Tina: Tiny reasoning models via lora, 2025a. URL https://arxiv.org/abs/2504.15777. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025b. URL https://arxiv. org/abs/2504.20571. Maurice Weber, Daniel Y. Fu, Quentin Anthony, Yonatan Oren, Shane Adams, Anton Alexandrov, Xiaozhong Lyu, Huu Nguyen, Xiaozhe Yao, Virginia Adams, Ben Athiwaratkun, Rahul Chalamala, Kezhen Chen, Max Ryabinin, Tri Dao, Percy Liang, Christopher R√©, Irina Rish, and Ce Zhang. Redpajama: an open dataset for training large language models. NeurIPS Datasets and Benchmarks Track, 2024. Zhengxuan Wu, Aryaman Arora, Atticus Geiger, Zheng Wang, Jing Huang, Dan Jurafsky, Christopher D. Manning, and Christopher Potts. Axbench: Steering llms? even simple baselines outperform sparse autoencoders, 2025. URL https://arxiv.org/abs/2501.17148. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. Rosie Zhao, Alexandru Meterez, Sham Kakade, Cengiz Pehlevan, Samy Jelassi, and Eran Malach. Echo chamber: Rl post-training amplifies behaviors learned in pretraining, 2025. URL https://arxiv.org/ abs/2504.07912. 18 Resa: Transparent Reasoning Models via SAEs"
        },
        {
            "title": "Appendix",
            "content": "A. Full Hyperparameter We show our default choice of hyperparameter in Table 7. The differences between main and ablation experiments largely lie in the hyperparameter we ablate over, which means the most of following hyperparameter is held constant across all experiments. SAE-Tuning Stage I: SAE Training Number of features Dead feature threshold Expansion factor Top-k value Decoder normalization"
        },
        {
            "title": "Optimizer\nEpochs\nBatch Size\nLearning Rate\nLearning Rate Scheduler",
            "content": "SAE-Tuning Stage II: SAE-Guided SFT LoRA Modules LoRA Rank LoRA Œ± LoRA Dropout"
        },
        {
            "title": "Optimizer\nOptimizer Momentum\nEpochs\nBatch Size\nLearning Rate\nLearning Rate Scheduler",
            "content": "65536 1e6 64 32 True Signum 1 16 2.5e-4 Constant query, key, value, dense 32 128 0.05 AdamW = 0.9, 0.999 Œ≤1 , Œ≤ 2 1 1e-6 Cosine with Min LR Table 7: Default Hyperparameter Settings of SAE-Tuning (Top) The default setting of SAE training. (Bottom) The default setting of SAE-Guided SFT. 19 Resa: Transparent Reasoning Models via SAEs B. Additional Experiment Results B.1. Full Results of Table In the following tables, we present the full performance evaluation results of models in Table 3. Chec kpoint Steps AIME24 AIME25 AMC 23 MATH500 GPQA Mine rva Avg. 1000 1500 2000 20.00 33.33 33.33 30.00 33.33 23. 33.33 23.33 75.00 75.00 75.00 77. 82.60 82.80 83.80 84.20 29.78 30. 29.41 26.47 33.33 30.81 28.79 33. 45.67 46.03 47.28 45.81 Table 8: Performance of Resa-STILL-v1 Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 500 1000 1500 2000 2500 3500 4000 4500 5000 30.00 16. 23.33 33.33 36.67 20.00 23.33 33. 36.67 30.00 30.00 10.00 20.00 23. 23.33 26.67 23.33 13.33 13.33 23. 67.50 67.50 70.00 70.00 85.00 65. 70.00 72.50 70.00 67.50 84.00 83. 84.40 86.00 83.00 83.80 81.80 83. 83.80 84.40 28.68 30.15 27.57 29. 32.35 30.51 30.15 29.41 27.94 28. 32.32 37.88 37.88 36.87 33.33 35. 36.36 36.36 31.31 32.32 45.42 40. 43.86 46.43 48.95 43.64 44.16 44. 43.84 44.37 Table 9: Performance of Resa-DeepScaleR-v1 Each epoch contains 2630 Steps. 20 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 500 1000 1500 20.00 23.33 13. 16.67 16.67 10.00 60.00 62.50 60. 81.20 77.20 74.00 29.78 26.47 28. 26.36 26.26 27.78 39.00 38.74 35. Table 10: Performance of STILL-CoT-free-SFT Each epoch contains 936 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 3000 5000 6000 7000 10.00 16.67 10. 10.00 10.00 0.00 10.00 6.67 6. 6.67 6.67 0.00 6.67 0.00 57. 52.50 37.50 35.00 32.50 40.00 42. 68.60 67.40 64.20 61.80 64.40 64. 60.20 20.22 21.69 25.37 22.79 23. 23.53 20.22 36.36 28.28 32.32 27. 29.29 28.79 25.76 33.22 32.20 29. 27.34 26.62 27.16 26.45 Table 11: Performance of DeepScaleR-CoT-based-SFT Each epoch contains 2520 Steps. Resa: Transparent Reasoning Models via SAEs B.2. Full Results of Table 4 In the following tables, we present the full performance evaluation results of models in Table 4. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 23.33 20.00 23.33 23. 23.33 20.00 23.33 26.67 72.50 75. 67.50 67.50 85.40 81.40 83.60 83. 30.51 29.78 29.78 25.74 34.85 30. 35.86 34.34 44.99 42.75 43.90 43. Table 12: Performance of Resa-STILL-Pretrained-SAE Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 30.00 33.33 33.33 30.00 20.00 33. 26.67 23.33 67.50 70.00 72.50 70. 84.40 83.00 81.60 85.60 28.31 30. 30.51 32.35 34.85 34.34 29.29 33. 44.18 47.36 45.65 45.77 Table 13: Performance of Resa-STILL-Trained-from-Scratch-SAE Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 23.33 30. 33.33 23.33 23.33 16.67 16.67 20. 77.50 77.50 65.00 77.50 82.20 80. 83.40 84.60 28.31 31.25 27.21 27. 31.82 29.80 34.85 35.35 44.42 44. 43.41 44.73 Table 14: Performance of Resa-STILL-Tina-0-step (Fine-tuned SAE) Each epoch contains 1448 Steps. 22 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 26.67 40. 26.67 36.67 20.00 26.67 26.67 23. 67.50 70.00 65.00 65.00 84.00 83. 83.40 84.00 29.04 31.62 27.94 30. 35.35 36.36 36.36 36.36 43.76 47. 44.34 46.04 Table 15: Performance of Resa-STILL-Tina-1-step (Fine-tuned SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 30.00 23.33 23.33 20. 16.67 23.33 23.33 20.00 67.50 75. 62.50 70.00 83.80 82.20 84.20 84. 31.62 29.41 27.57 27.94 32.32 33. 34.85 35.86 43.65 44.43 42.63 43. Table 16: Performance of Resa-STILL-Tina-10-step (Fine-tuned SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 26.67 20.00 30.00 33.33 23.33 26. 33.33 26.67 70.00 65.00 67.50 72. 83.00 84.60 82.80 82.60 28.68 30. 30.15 27.57 38.89 34.85 36.36 38. 45.09 43.60 46.69 46.93 Table 17: Performance of Resa-STILL-Tina-50-step (Fine-tuned SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 43.33 40. 30.00 23.33 23.33 23.33 16.67 23. 82.50 72.50 70.00 72.50 85.60 84. 85.40 84.40 28.68 25.74 33.09 30. 33.33 32.32 33.33 36.36 49.46 46. 44.75 45.01 Table 18: Performance of Resa-STILL-Tina-100-step (Fine-tuned SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 36.67 30.00 36.67 26. 26.67 23.33 26.67 20.00 80.00 82. 70.00 72.50 83.80 84.20 85.60 82. 31.62 31.62 31.99 28.31 31.82 36. 38.89 35.86 48.43 48.00 48.30 44. Table 19: Performance of Resa-STILL-Tina-500-step (Fine-tuned SAE) Each epoch contains 1448 Steps. 23 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 30.00 26.67 26.67 23. 20.00 23.33 26.67 26.67 75.00 72. 72.50 67.50 83.00 85.40 83.60 82. 26.84 27.94 27.57 30.51 32.32 34. 30.81 36.36 44.53 45.11 44.64 44. Table 20: Performance of Resa-STILL-Tina-3000-step (Fine-tuned SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 33.33 23.33 36.67 43.33 26.67 23. 23.33 23.33 70.00 70.00 72.50 65. 87.00 84.00 83.40 84.20 29.41 29. 28.68 27.94 41.92 34.85 36.87 29. 48.06 44.22 46.91 45.60 Table 21: Performance of Resa-STILL-Tina-0-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 30.00 26. 33.33 33.33 23.33 23.33 23.33 33. 75.00 72.50 65.00 72.50 83.80 85. 82.00 82.20 25.74 29.78 29.78 29. 34.85 38.38 34.85 35.35 45.45 45. 44.72 47.69 Table 22: Performance of Resa-STILL-Tina-1-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 26.67 33.33 33.33 26. 20.00 16.67 23.33 30.00 67.50 67. 67.50 62.50 85.80 86.20 84.40 82. 28.31 30.51 29.41 26.84 36.87 37. 32.32 34.85 44.19 45.26 45.05 43. Table 23: Performance of Resa-STILL-Tina-10-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 36.67 23.33 30.00 43.33 23.33 33. 30.00 23.33 80.00 72.50 70.00 77. 84.20 84.00 83.20 83.40 28.68 27. 32.72 29.41 35.86 35.86 37.37 38. 48.12 46.04 47.22 49.31 Table 24: Performance of Resa-STILL-Tina-50-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 33.33 46.67 36.67 30.00 23.33 20. 20.00 23.33 90.00 62.50 75.00 65. 82.60 83.00 83.20 83.20 28.68 28. 30.51 29.04 35.35 31.31 38.38 35. 48.88 45.30 47.29 44.32 Table 25: Performance of Resa-STILL-Tina-100-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 23.33 30. 36.67 20.00 20.00 20.00 20.00 23. 75.00 70.00 67.50 67.50 84.60 83. 84.20 82.80 30.51 30.51 30.88 28. 33.84 32.32 35.35 35.35 44.55 44. 45.77 42.94 Table 26: Performance of Resa-STILL-Tina-500-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 16.67 30.00 33.33 20. 23.33 20.00 26.67 26.67 65.00 77. 65.00 67.50 83.60 86.20 85.20 82. 30.88 31.62 31.62 28.68 34.85 37. 35.35 32.83 42.39 47.20 46.20 43. Table 27: Performance of Resa-STILL-Tina-3000-step (Trained-from-Scratch SAE) Each epoch contains 1448 Steps. 25 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 3000 3500 30.00 33. 30.00 23.33 26.67 30.00 23.33 23. 16.67 20.00 16.67 23.33 70.00 80. 77.50 75.00 72.50 75.00 86.40 86. 83.60 82.00 83.20 85.80 27.94 30. 28.31 29.78 31.62 27.94 32.83 31. 31.82 36.36 33.33 30.80 45.08 47. 44.65 44.41 44.00 45.48 Table 28: Performance of Resa-DeepScaleR-Best-Tina-1000-step (Trained-from-Scratch SAE) Each epoch contains 1914 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 3000 26.67 36.67 23.33 33.33 40.00 40. 16.67 30.00 20.00 23.33 20.00 30. 75.00 77.50 82.50 67.50 72.50 75. 84.60 83.80 82.40 83.00 84.40 84. 28.68 29.41 28.68 29.04 26.47 30. 31.80 31.26 34.36 32.43 35.35 33. 43.90 48.11 45.21 44.77 46.45 48. Table 29: Performance of Resa-DeepScaleR-Tina-0-step (Trained-from-Scratch SAE) Each epoch contains 1914 Steps. 26 Resa: Transparent Reasoning Models via SAEs B.3. Full Results of Table 5 In the following tables, we present the full performance evaluation results of models in Table 5. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 500 1000 1500 2000 2500 3500 4000 4500 5000 33.33 33. 26.67 26.67 33.33 30.00 26.67 30. 36.67 36.67 30.00 23.33 13.33 23. 16.67 26.67 13.33 23.33 20.00 20. 80.00 70.00 67.50 70.00 70.00 57. 77.50 60.00 75.00 72.50 84.00 84. 83.80 83.20 82.40 83.20 83.20 84. 83.80 84.00 29.41 29.41 31.25 28. 27.57 28.68 28.31 26.84 27.94 26. 35.86 32.83 36.87 34.34 35.35 37. 34.85 37.37 37.37 31.82 48.77 45. 43.24 44.37 44.22 43.90 43.98 43. 46.80 45.31 Table 30: Performance of Resa-STILL2DeepScaleR Each epoch contains 1914 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 500 1500 2000 23.33 36.67 33.33 30. 20.00 23.33 26.67 26.67 72.50 85. 72.50 75.00 84.00 84.60 83.40 84. 30.51 30.88 29.41 30.88 32.83 31. 39.90 37.88 43.86 48.72 47.54 47. Table 31: Performance of Resa-STILL2Open-S1 Each epoch contains 1063 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 3000 5000 30.00 40.00 26.67 26.67 26. 20.00 23.33 23.33 13.33 20.00 75. 75.00 75.00 67.50 72.50 84.60 83. 85.20 85.80 85.60 27.21 31.25 28. 27.94 29.41 34.34 38.89 36.36 41. 37.37 45.19 48.61 45.81 43.78 45. Table 32: Performance of Resa-STILL2II-Thought Each epoch contains 2664 Steps. 27 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 3000 4000 5000 6000 7000 9000 33.33 33.33 30.00 33.33 33. 20.00 23.33 33.33 30.00 16.67 30. 30.00 23.33 23.33 20.00 20.00 23. 23.33 72.50 77.50 72.50 65.00 67. 72.50 67.50 72.50 70.00 84.00 86. 84.60 84.20 84.40 84.20 81.40 80. 83.60 31.99 27.21 29.04 29.04 28. 28.68 30.88 26.10 27.94 40.91 41. 33.84 34.34 32.32 31.31 37.88 35. 30.81 46.57 49.46 46.66 44.88 44. 42.78 43.50 45.25 44.28 Table 33: Performance of Resa-STILL2OpenR1 Each epoch contains 4911 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 36.67 36. 40.00 26.67 20.00 16.67 20.00 16. 82.50 67.50 72.50 72.50 83.40 86. 84.60 84.60 31.25 31.25 30.15 26. 33.33 34.85 32.32 35.86 47.86 45. 46.60 43.86 Table 34: Performance of Resa-STILL-Qwen-Math-Adapter Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 30.00 20.00 26.67 30. 30.00 20.00 30.00 16.67 72.50 72. 67.50 70.00 85.60 83.00 84.60 83. 31.25 30.15 25.74 29.78 35.86 35. 32.83 34.34 47.54 43.50 44.56 44. Table 35: Performance of Resa-STILL-Qwen-Adapter Each epoch contains 1448 Steps. 28 Resa: Transparent Reasoning Models via SAEs B.4. Full Results of Table 6 In the following tables, we present the full performance evaluation results of models in Table 6. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 26.67 26. 26.67 16.67 20.00 16.67 30.00 23. 70.00 70.00 80.00 67.50 85.00 82. 83.20 86.00 29.41 26.84 29.78 32. 32.32 31.31 37.37 34.34 43.90 42. 47.84 43.37 Table 36: Performance of Resa-STILL-2nd-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 20.00 26.67 33.33 26. 20.00 36.67 23.33 16.67 82.50 70. 70.00 72.50 84.60 83.40 83.40 84. 24.26 28.31 28.68 31.25 32.32 33. 33.84 30.30 43.95 46.48 45.43 43. Table 37: Performance of Resa-STILL-3rd-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 26.67 33.33 36.67 40.00 16.67 20. 23.33 20.00 67.50 80.00 70.00 72. 84.20 83.80 83.20 83.80 28.31 28. 27.57 30.88 40.91 36.87 34.85 31. 44.04 47.11 45.94 46.42 Table 38: Performance of Resa-STILL-4th-Layer Each epoch contains 1448 Steps. Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 26.67 30.00 40.00 33.33 20.00 16. 23.33 26.67 75.00 72.50 70.00 70. 80.80 82.80 83.20 83.60 29.41 30. 26.84 29.41 32.32 30.81 44.95 31. 44.03 43.88 48.05 45.72 Table 39: Performance of Resa-STILL-5th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 20.00 30. 33.33 30.00 20.00 20.00 23.33 16. 75.00 80.00 72.50 72.50 85.60 83. 83.40 82.60 27.57 30.88 31.62 27. 39.90 31.31 35.35 38.38 44.68 45. 46.59 44.68 Table 40: Performance of Resa-STILL-6th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 33.33 26.67 23.33 30. 20.00 26.67 20.00 33.33 72.50 77. 70.00 67.50 83.40 81.60 82.60 83. 31.62 27.94 29.78 24.63 30.30 35. 40.91 36.36 45.19 46.04 44.44 45. Table 41: Performance of Resa-STILL-7th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 20.00 16.67 30.00 16.67 23.33 23. 20.00 20.00 82.50 70.00 70.00 72. 85.20 82.60 83.20 81.00 29.78 27. 28.31 29.04 33.33 34.34 37.88 28. 45.69 42.36 44.90 41.33 Table 42: Performance of Resa-STILL-8th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 20.00 36. 36.67 26.67 36.67 20.00 20.00 26. 77.50 75.00 80.00 67.50 84.20 83. 84.20 83.80 25.37 28.31 27.57 27. 37.88 34.34 34.34 38.89 46.94 46. 47.13 45.18 Table 43: Performance of Resa-STILL-9th-Layer Each epoch contains 1448 Steps. 30 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 26.67 36. 23.33 36.67 23.33 23.33 23.33 23. 75.00 67.50 75.00 70.00 84.60 84. 84.20 83.20 27.21 29.78 29.41 29. 33.84 37.37 25.25 36.36 45.11 46. 43.42 46.50 Table 44: Performance of Resa-STILL-10th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 26.67 26.67 30.00 20. 36.67 33.33 23.33 26.67 77.50 75. 67.50 72.50 83.80 84.60 84.60 83. 31.25 30.51 31.99 33.09 32.83 34. 45.45 33.84 48.12 47.41 47.15 44. Table 45: Performance of Resa-STILL-11th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 36.67 20.00 30.00 30.00 23.33 23. 30.00 26.67 70.00 72.50 72.50 77. 82.20 84.60 82.60 84.60 29.04 26. 30.51 31.99 34.34 40.40 32.83 34. 45.93 44.61 46.41 47.60 Table 46: Performance of Resa-STILL-12th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 20.00 33. 33.33 30.00 33.33 23.33 33.33 23. 75.00 75.00 75.00 77.50 82.60 82. 83.80 84.20 29.78 30.88 29.41 26. 33.33 30.81 28.79 33.33 45.67 46. 47.28 45.81 Table 47: Performance of Resa-STILL-13th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 33.33 26.67 23.33 33. 20.00 26.67 16.67 23.33 77.50 70. 72.50 65.00 83.60 84.80 83.80 82. 28.68 26.84 28.31 30.88 29.80 30. 34.34 32.32 45.48 44.30 43.16 44. Table 48: Performance of Resa-STILL-14th-Layer Each epoch contains 1448 Steps. 31 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 36.67 26.67 36.67 30. 23.33 26.67 20.00 16.67 80.00 72. 72.50 67.50 84.80 84.40 82.00 83. 30.15 30.51 29.04 32.72 38.89 31. 36.36 36.87 48.97 45.43 46.10 44. Table 49: Performance of Resa-STILL-15th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 23.33 30.00 30.00 30.00 16.67 20. 20.00 20.00 72.50 65.00 80.00 70. 85.40 83.40 85.00 82.80 30.15 29. 30.15 30.15 38.89 32.83 34.85 38. 44.49 43.50 46.67 45.22 Table 50: Performance of Resa-STILL-16th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 33.33 33. 36.67 43.33 30.00 13.33 23.33 16. 67.50 67.50 65.00 77.50 85.40 83. 83.20 83.80 30.88 30.88 31.25 30. 36.36 35.86 34.85 36.36 47.25 43. 45.72 48.09 Table 51: Performance of Resa-STILL-17th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 43.33 26.67 23.33 43. 20.00 20.00 23.33 23.33 77.50 75. 75.00 72.50 84.00 83.00 83.40 84. 33.82 29.41 33.82 29.04 37.88 35. 38.38 31.82 49.42 44.99 46.21 47. Table 52: Performance of Resa-STILL-18th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 40.00 30.00 33.33 26.67 13.33 23. 30.00 23.33 75.00 72.50 72.50 75. 84.20 83.20 84.20 84.80 27.94 30. 29.04 28.68 38.38 35.35 36.87 35. 46.48 45.76 47.66 45.64 Table 53: Performance of Resa-STILL-19th-Layer Each epoch contains 1448 Steps. Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 26.67 33.33 20.00 40.00 33.33 23. 30.00 20.00 70.00 75.00 65.00 67. 83.60 85.00 83.60 82.80 27.21 29. 28.31 27.94 31.82 29.29 27.27 33. 45.44 45.83 42.36 45.35 Table 54: Performance of Resa-STILL-20th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 30.00 33. 40.00 23.33 33.33 16.67 20.00 30. 75.00 72.50 62.50 70.00 84.60 84. 81.80 84.00 27.57 30.51 29.41 30. 36.87 30.81 34.85 33.84 47.90 44. 44.76 45.34 Table 55: Performance of Resa-STILL-21st-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 33.33 30.00 23.33 23. 23.33 26.67 16.67 20.00 75.00 70. 70.00 72.50 82.20 83.20 83.80 85. 31.99 31.62 31.25 29.04 34.34 35. 34.85 39.39 46.70 46.23 43.32 45. Table 56: Performance of Resa-STILL-22nd-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 30.00 30.00 23.33 23.33 20.00 20. 23.33 23.33 72.50 80.00 60.00 67. 83.80 82.00 84.80 85.00 28.31 29. 27.57 29.78 32.32 35.35 37.37 43. 44.49 46.07 42.73 45.40 Table 57: Performance of Resa-STILL-23rd-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 36.67 20. 26.67 33.33 20.00 16.67 33.33 16. 77.50 65.00 67.50 75.00 83.80 85. 85.40 82.80 30.51 29.41 30.15 29. 29.80 34.85 33.33 33.33 46.38 41. 46.06 45.15 Table 58: Performance of Resa-STILL-24th-Layer Each epoch contains 1448 Steps. 33 Resa: Transparent Reasoning Models via SAEs Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 2500 40.00 30. 23.33 30.00 30.00 26.67 16.67 26. 70.00 67.50 75.00 67.50 84.20 83. 82.80 84.60 28.68 29.41 30.88 27. 37.88 35.86 36.36 34.34 48.46 45. 44.17 45.11 Table 59: Performance of Resa-STILL-25th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 2000 2500 36.67 26.67 26.67 30. 20.00 16.67 23.33 23.33 65.00 75. 67.50 70.00 85.60 83.60 84.40 83. 30.88 30.15 27.21 31.62 36.87 33. 35.86 36.87 45.84 44.24 44.16 45. Table 60: Performance of Resa-STILL-26th-Layer Each epoch contains 1448 Steps. Checkpoin Steps AI E24 AIM E25 AMC2 3 MAT H50 0 GPQA Mi nerva Avg. 1000 1500 2000 16.67 33.33 36.67 30.00 26.67 23. 30.00 36.67 65.00 72.50 62.50 67. 82.80 82.80 83.00 83.00 30.51 31. 27.94 27.94 34.85 36.87 33.84 40. 42.75 46.74 45.66 47.59 Table 61: Performance of Resa-STILL-27th-Layer Each epoch contains 1448 Steps."
        }
    ],
    "affiliations": []
}