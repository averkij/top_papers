{
    "paper_title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "authors": [
        "Aaron Serianni",
        "Tyler Zhu",
        "Olga Russakovsky",
        "Vikram V. Ramaswamy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Computer vision models have been shown to exhibit and amplify biases across a wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of a model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within a model's internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels."
        },
        {
            "title": "Start",
            "content": "Attention IoU: Examining Biases in CelebA using Attention Maps"
        },
        {
            "title": "Olga Russakovsky",
            "content": "Vikram V. Ramaswamy Princeton University {serianni, tylerzhu, olgarus, vr23}@princeton.edu 5 2 0 2 6 2 ] . [ 2 6 4 8 9 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Computer vision models have been shown to exhibit and amplify biases across wide array of datasets and tasks. Existing methods for quantifying bias in classification models primarily focus on dataset distribution and model performance on subgroups, overlooking the internal workings of model. We introduce the Attention-IoU (Attention Intersection over Union) metric and related scores, which use attention maps to reveal biases within models internal representations and identify image features potentially causing the biases. First, we validate Attention-IoU on the synthetic Waterbirds dataset, showing that the metric accurately measures model bias. We then analyze the CelebA dataset, finding that Attention-IoU uncovers correlations beyond accuracy disparities. Through an investigation of individual attributes through the protected attribute of Male, we examine the distinct ways biases are represented in CelebA. Lastly, by subsampling the training set to change attribute correlations, we demonstrate that Attention-IoU reveals potential confounding variables not present in dataset labels. Our code is available at https://github.com/ aaronserianni/attention-iou. 1. Introduction Biases in computer vision models can lead to failures in model performance and unequal behavior for different groups. These biases are often caused by spurious correlations, where model relies on an attribute that is associated with, but not causally related to, the target. model dependent on such spurious correlations might then perform poorly on out-of-distribution test data or exhibit low accuracy for groups for which the correlation does not hold. For example, models have been shown to be biased towards low-level features such as texture and image spectra [17, 18, 83], and high-level attributes including background and contextual objects [66]. This becomes more concerning for tasks involving people, since these correlations can cause models to discriminate against race, age, societally protected groups such as gender, Average Image Male Blond Hair Wavy Hair Figure 1. We use attention maps to understand which image regions model relies on for the target classification task. Our proposed Attention-IoU framework provides insights into how models represents biases between correlated attributes. For example, consider the spatially related attributes of blond and wavy hair in the CelebA dataset [44], which have similar label correlations to the Male label. They are attended to differently by the model, with blond hair appearing closer to Male in both average attention map (top row) and the Attention-IoU mask score (bottom row). Thus, Attention-IoU reveals that blond hair, when compared to wavy hair, has spurious correlation with Male that is not present in the dataset labels. ethnicity, and income [6, 12, 23, 63, 90, 91]. Past works have extensively investigated biases and spurious correlations through the lens of dataset labeling and model accuracy. For example, fairness metrics reveal disparities in model accuracy between groups or individuals (see [8, 46, 51, 74] for surveys). Others have created tools to surface biases by analyzing and categorizing objects, gender, skin tone, geographical labels, among others, sometimes in combination with model predictions and unsupervised techniques [4, 35, 76]. Many studies have also explored methods to mitigate the effects of spurious correlations in datasets [23, 27, 50, 80, 91]. These approaches to the discovery and measurement of spurious correlations using dataset inputs and model outputs have revealed many biases exhibited by computer vision models. However, they are often only able to find biases at coarse level, restricted to the binary labels present within the dataset. For example, while these metrics excel at identifying when the classification of persons attributes might depend on gender, they are unable to highlight the specific features of the persons gender presentation that the model uses to make prediction. In the absence of finegrained labels, interpretability methods hold the potential to reveal representations of correlations within model, and how they might affect the models output. In this paper, we propose Attention-IoU, generalized intersection-over-union metric that uses attention maps to measure biases in image classification models. We specifically aim to quantify spurious correlations for when model relies on regions of images that are not directly relevant to the target classification tasks. For example, within the CelebA dataset [36, 44] of faces, blond hair is correlated with person being labeled not male. As such, model trained to identify the blond hair attribute may use gendered aspects faces in addition to using hair features to compute its output. Thus, the model may attend to regions such as the eyes, nose, and mouth as well as hair  (Fig. 1)  . As part of Attention-IoU, we present two scores: the mask score, where an attention map is compared to ground-truth feature mask; and the heatmap score, where the attention maps for two different attributes are compared with each other. We first validate Attention-IoU on the synthetic Waterbirds [59] dataset, showing that it accurately reflects the bias within the dataset. We then examine CelebA [44], as the dataset is widely-used benchmark for fairness methods, spanning dataset bias identification to model debiasing, with the Male attribute used as the sensitive attribute. Through this analysis of CelebA, we demonstrate that Attention-IoU can identify specific ways in which the protected Male attribute might influence other attributes. We show that attributes can be unevenly influenced by the classifiers representation of the protected Male attribute, and that certain attributes have biases beyond simple correlations in dataset labels. These insights reveal different ways in which computer vision models might be biased, allowing the community to develop better debiasing techniques. 2. Related Work Bias in computer vision. Computer vision models and datasets have been extensively shown to exhibit biases across wide range of tasks [15, 23, 24, 47, 71, 72, 77, 79]. Models can even amplify disparities from the datasets on which they are trained [7, 62, 90, 91]. When biased datasets and models involve people and society, there are significant fairness and societal implications, as models often perform anomalously regarding protected classes including race, gender, and age [5, 6, 45, 49, 81]. Past works about identifying biases in computer vision focus either on quantifying bias in the dataset, the output of the trained model, or combination thereof [6, 21, 49]. Bias is often quantified by analyzing the distribution of attributes within dataset, and identifying which attributes have unequal distributions or are underrepresented compared to real-world demographics [6, 63]. For unannotated attributes, this can be revealed through the use of both image generative models to balance the distribution [2, 13, 41], and vision language models for the fine-grained identification of attributes and unlabeled biases [29, 33, 40]. Other approaches find correlations between labeled attributes and features in the images themselves, such as co-occurring objects [66], stereotypical and offensive portrayals [5], or low-level features like pose and color [47]. In trained model, bias identification is primarily restricted to looking at the models outputs, often including calculating the accuracy and error rates for various labeled groups within the dataset [11, 14, 20, 74], or, if groups are unlabeled, using unsupervised techniques to find them [35, 39, 48, 68]. Interpretability methods and metrics. Interpretability for machine learning aims to explain the external behavior of models and give insight into their internal mechanisms. Instance or local explanations are the most common interpretability technique for computer vision, describing how the model behaves locally around features in specific input. The output of this technique is an attention or saliency heatmap, highlighting the areas of the image most responsible for the models output [16, 19, 52, 57, 60, 64, 65, 73, 78, 87, 92]. Class activation maps (CAM) [92] and its derivatives, including GradCAM [60], are the most common methods for creating attention maps. Attention maps are frequently used qualitatively to evaluate debiasing methods [26, 67, 70, 86], or highlight biases in models [60], as Wolfe et al. perform through average heatmaps [82]. Krishnakumar et al. and Lee et al. both use attention maps as part of bias visualization systems by highlighting the maps of individual pertinent images [35, 37]. Beyond quantitative evaluations, Bang et al. present method to directly identify model bias using aggregated explanation alignment metrics, focusing on the bias between different model instances [3]. Some debiasing methods also use attention maps directly, by creating loss functions that integrate attention maps [38, 56, 66], or highlighting pertinent image regions through thresholding of the map [1, 31, 53]. Specifically, Singh et al. use loss function to minimize element-wise overlap between the attention maps of an attribute and its co-occurring context, but do not use the maps to evaluate the biases themselves [66]. 3. Method Existing bias metrics for computer vision classification models focus on how the models perform with respect to 2 Background Bias Object Bias Depiction Bias 3.1. GradCAM Preliminaries tribute instead of the target attribute. This lets us quantify bias by comparing models attention map for the target attribute to either attention maps of confounding attributes or ground-truth feature maps. Figure 2. Attention maps for landbird on water background in the Waterbirds dataset [59], illustrating possible forms of model bias for incorrect classifications. (left) attending to the whole background; (center) attending to ship instead of the bird; (right) only attending to part of the bird, its wing in flight. certain groups within dataset [20, 91]. This can involve investigating the distribution of groups in dataset, differences in accuracy and error rates between groups, or some combination thereof [74]. These common approaches often only consider the final predictions of models, but in line with other works [2, 13, 35, 37], we aim to understand why these biases might occur. Consider, for example, dataset where we try to distinguish between waterbirds and landbirds [59]. Here, the birds are correlated with the backgrounds, and most images of waterbirds picture water background, while most images of landbirds picture land background. Moreover, assume that model trained on this dataset struggles to recognize waterbirds pictured on land backgrounds. Metrics that consider the difference in performance between different groups would correctly identify this model as biased. However, we argue that there are multiple forms that this bias could take: The model could be using the entire background to identify the bird, and thus, is (incorrectly) using cues from the water background when landbirds are pictured on water (Fig. 2 left). The model could be using specific cues from the background. For example, suppose land backgrounds always contain tree, while water backgrounds always contain boat. The model could use these cues (rather than the entire background) to classify the image (Fig. 2 center). Landbirds pictured on water backgrounds could be depicted differently to those pictured on land backgrounds. For example, maybe these birds are pictured mid-flight, making them smaller and thus harder for model to classify. In this case, the model might be (correctly) using cues from the bird, but the cues learned are not generalized to landbirds on water backgrounds (Fig. 2 right). In order to better understand these differences, we turn to attention maps as mechanism for revealing which image features are important for the models decision-making. The key insight for our bias identification method is the following: if model learns spurious correlation between target attribute and confounding attribute in the dataset, it will learn to use features helpful for the confounding atWe use Gradient-weighted Class Activation Mapping (GradCAM) target atto obtain attention maps for tributes [60]. Given an input image and target attribute a, GradCAM computes the gradient of the class output ya with respect to the output of convolutional layer, usually the final layer, to obtain activation maps of the attribute. simple gradient-weighted linear combination of the layers feature activation maps produces the attribute-specific attention map GradCAMa(x). GradCAM was developed for models trained with categorical cross entropy loss, and thus, in its standard implementation, only able create attention maps for positive predictions for model trained with binary cross entropy loss. For our metric, we instead take the gradient of the absolute value of the class output, ya, so that image features that contribute positively to either prediction is attended to in the attention map. For further explanation, see Appendix A. 3.2. Attention Map Metrics Now that we have maps corresponding to attention maps and ground-truth feature masks, all we need is way to compare the maps. The metric should be able to compare two real-valued attention maps with each other, as well as an attention map with binary ground-truth feature mask. Two commonly-used metrics for evaluating attention maps, the pointing game [89] and intersection-over-union (IoU), both fail this requirement as they require binary mask for one of their inputs. Furthermore, as image attributes can vary drastically in pixel area, such as hair vs. eye color, the metric should be size invariant and remain constant if the two maps scale proportionally with each other. Based on these constraints, we propose generalized IoU metric, which we refer to as Attention-IoU, that works on weighted dense-pixel maps and is size and scale invariant. Given two maps M1, M2 Rhw, which can be either attention maps or feature masks, denote their L1 normalized maps as (cid:99)Mi = Mi/Mi1, which are akin to probability density functions. The metric is defined as BA-IoU(M1, M2) = (cid:99)M1, (cid:99)M2F (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:99)M1+(cid:99)M2 2 = (cid:80) i,j((cid:99)M1)ij ((cid:99)M2)ij (cid:16) (cid:99)M1+(cid:99)M2 (cid:80) 2 (cid:17)2 ij ij (1) where A, BF is the Frobenius inner product, i.e., the sum of the element-wise matrix product, and A2 is the Frobenius norm, i.e., the sum of squared entries of the matrix. The L1 normalization ((cid:99)M1 = M1/M11) inside the products makes BA-IoU scale invariant to values of the maps. The numerator of our metric calculates weighted intersection between the two maps. If one is binary, then this reports the overall mass focused on relevant mask areas, and when both are continuous, then this simply weights the mass by the corresponding pixel-wise probability. The denominator of our metric is union of the two maps. We average both maps so that the resulting matrix still has values in [0, 1]. For full proofs of the invariants, see Appendix B. This metric has desirable properties similar to IoU; for example, if M1 = M2, BA-IoU(M1, M2) is 1, and if the maps are completely disjoint then BA-IoU is 0. Since Attention-IoU allows for continuous scores, if M1 and M2 overlap, then as the weight in their intersection increases (and decreases, respectively), so does BA-IoU. 3.3. Bias Scores Using Attention-IoU, we define two methods to score biases in model for given target attribute. The first one, the heatmap score, compares the attention map for the target attribute with the attention map of chosen protected attribute between each input image. Given set of images {xi}n i=1, the scores formulation is Attention-IoUHeatmap(t, p) = (2) 1 (cid:88) i=1 BA-IoU(GradCAMt(xi), GradCAMp(xi)). The mask score is computed between the targets attention map and chosen ground-truth feature mask maskf (x), corresponding to the specific input image. As the size of the attention map is the size of the final convolution layer, whereas the feature mask is the size of the input image, the feature mask is downsampled with bilinear interpolation: Attention-IoUMask(t, ) = (3) 1 (cid:88) i=1 BA-IoU(GradCAMt(xi), interp(maskf (xi))). Advantages of Attention-IoU. Attention-IoU has several advantages over existing bias detection methods. First, since the metric is based on attention maps, it highlights specific regions of the sensitive attribute that most contribute to the target attribute prediction. Thus, we are able to identify bias at more fine-grained level than other bias metrics. Next, by visualizing the scores separately for different types of images, we can infer if the bias is different for the different sets. For example, this allows us to understand if the features of the sensitive attribute are used solely when the attribute takes on particular value. Finally, the metric allows us to unearth potential confounding variables; i.e., when the bias is due to more than the simple proportion of labels within the training dataset. One limitation to this attention-based approach is that attention maps only convey spatial information about what the model is attending to in an image. Information regarding shape, color, or texture is not included in an attention map. Thus, if target and confounding attribute are co-located, but the model is attending to different image features within the region containing both attributes, our metric will still indicate high correlation between the two attributes. Despite this limitation, in the next two sections we show how Attention-IoU can be used to closely examine dataset. 4. Validating the metric To start, we test the proposed metric on Waterbirds [59]. This simple synthetic dataset is constructed by combining cropped bird images from the CUB dataset [75] with backgrounds from the Places dataset [93]. In the dataset birds are labeled as either waterbird or landbird, and backgrounds are similarly labeled as land or water. The dataset can be constructed with different levels of correlation between the bird and the background, introducing single axis of bias within the dataset. Moreover, masks of the bird and background are clearly available within this dataset, which can be used to compute Attention-IoU. Experimental setup. Following prior work, we place specified percentage (between 50%-100%) of the waterbirds on water background, with the remaining 0%-50% of the waterbirds are placed on land background, and similarly for landbirds and land backgrounds. The validation and test sets are unbiased with bird being 50% likely to align with its background. We followed Sagawa et al. [59] in using the official train-test split of the CUB dataset, composed of 5,994 training images and 5,794 testing images, and randomly choosing 20% of the training images to form the validation set. The test set was used to compute the overall accuracy, per-group accuracy, and Attention-IoU. We used ResNet-18 [22] pretrained on ImageNet [58] as our model, trained on Waterbirds using categorical crossentropy loss and an Adam optimizer [34] (learning rate 0.001, weight decay 0.0001). Input images are rescaled to be 224 224, and augmented using random crops and horizontal flips during training. Models were trained for 10 epochs, with batch size of 64. We report averages and standard deviations over 20 individually trained models. Results. We compare the heatmap generated with the ground-truth masks for the bird. In Fig. 3, we show the average bird mask, as well as the average heatmaps generated by GradCAM across all images in the test set for models trained at different levels of bias. As the bias increases, models rely more on cues from the background. This is reflected in the heatmaps, which highlight regions other than 4 Bird Mask 70% bias 90% bias 95% bias 100% bias Figure 3. Average bird mask and average heatmaps for Waterbirds at increasing levels of bias. We see that the model attends less on the bird as the bias increases, as indicated by its mask. Figure 4. Evaluation of mask score using GradCAM on Waterbirds test set. The X-axis represents the Attention-IoU mask score for the ground-truth masks of the bird and background. We note the dataset bias and the worst group accuracy (WGA) along the Y-axis. As the bias increases, the worst group accuracy decreases and the model attends less to the bird and more to the background. the bird mask. We verify that Attention-IoU captures this effect in Fig. 4, which shows the mask scores across varying training set bias for both bird and background masks. We also report the worst group accuracy (WGA) of models for each. As expected, the worst group accuracy decreases from 0.81 0.02 to 0.21 0.10 as bias increases from 50% to 100%. The decrease from 0.72 0.02 to 0.42 0.03 in mask score almost exactly mirrors the proportional decrease in WGA, validating that the metric accurately measures model bias. Due to the simple nature of Waterbirds, the bias in the dataset is directly represented in the training distribution, and Attention-IoU captures this perfectly. 5. Analyzing CelebA In this section, we analyze the CelebA dataset [44] using Attention-IoU. CelebA is widely used dataset for variety of tasks, including evaluating debiasing methods. CelebA contains 2,022,599 images of celebrity faces, each labeled with 40 binary attributes, including both attributes localized to specific face regions (e.g., Big Nose, Mouth Slightly Open, Blond Hair) and attributes that are more global (e.g., Male1, Heavy Makeup). We 5 use Attention-IoU to understand more about the attributes in the dataset, and how they might influence each other. Background. The CelebA dataset is one of the most widely used benchmarks for studying facial recognition, debiasing, and generative modeling [44]. Studies using CelebA have significantly advanced their respective fields. In generative modeling for example, CelebA is common real world testbed, such as StarGAN for facial attribute transfer and in CoCosNetv2 for image translation [10, 84, 94]. The recent explosion of text-to-image models that can be personalized and controlled for realistic synthesis has caused resurgence of facial recognition models for controllable editing [43]. Finally, many techniques for bias mitigation are validated on CelebA, from reweighting by using committees or biased models, to re-sampling or using pseudolabels [32, 48, 54, 61, 85]. commonly studied setting is Blond Hair as the target attribute and Male as the protected attribute, as popularized in the evaluation of group DRO paper by Sagawa et al. [59] and used by many subsequent works [28, 33, 61]. Several followups of the original dataset have also been developed for further study, such as the CelebA-HQ subset of 30,000 images of 10241024 resolution [30], as well as the CelebAMask-HQ dataset which additionally annotates the images with semantic masks of 19 facial component categories at 512 512 resolution [36]. The high resolution datasets are especially useful for testing high quality superresolution and inpainting [9, 88]. Despite its popularity, CelebA has many flaws which have been noted in previous works. Several attributes (e.g., Big Lips, Heavy Makeup, etc.) have been shown to be inconsistently labelled [54, 55]. Ramaswamy et al. also find that 13 of the attributes exhibit extreme class imbalance for gender expression [55]. Others find issues of hidden (unlabeled) biases, which bias discovery works aim to target, such as hair length and visible hair area [2, 40]. These issues in CelebA directly lead to biased models and generations. We aim to shed light on these different biases, to better understand how they occur and propagate into trained models. 5.1. Comparing to ground-truth masks We start by evaluating heatmaps using ground-truth masks, for attributes that are localized and have associated masks. Experiment Setup. Since we require ground-truth segmentation masks, we use CelebAMask-HQ [36], subset of CelebA in which each image has high-quality segmentation mask of different facial features, including hair, nose, 1We acknowledge that these binary feature labels in CelebA, especially the Male label, forces peoples presentations to fit into binaries. The Male label inherently assumes that an individuals gender presentation is tied to their gender identity. It is not clear what standards the creators of CelebA use in their definition of the Male label and other feature labels. However, for our goal of creating and evaluating bias metrics, we follow existing literature in our use of CelebA labels. 5.2. Comparison with the Male heatmap In line with prior works, which investigate the impact of bias due to the protected Male attribute, we next examine the correlation between the heatmaps of different attributes and the heatmap for the Male attribute. The experimental setup remains the same as Sec. 5.1. We compute Attention-IoU for all 40 attributes with Male (Fig. 6 left). We measure the correlation between the attribute and the Male label using the absolute value of Matthews correlation coefficient (MCC), which is tailored for comparing two binary variables. The heatmap score ranges from 0.63 0.02 for Black Hair to 0.94 0.01 for Wearing Lipstick. Male is 1 because its attention map is being compared with itself. There is clear positive trend between the heatmap score and predicted label MCC. Some attributes are outliers to this trend, such as Mustache and Eyeglasses having higher heatmap scores, and Wavy Hair having lower heatmap score. We also report the mask score for selected attributes (Fig. 6 right). The mask score for Male demonstrates that the models attend most strongly to the eye, eyebrow, and mouth region of the face, and slightly less to the nose and hair regions. We notice that this is most closely replicated by Wearing Lipstick, validating the high heatmap score. This per-region score computation also allows us to understand how features of different attributes differ: for example, the main difference between Blond Hair and Wavy Hair appears to be in how much the models attend to regions around the eyes and nose. We now analyze in detail five attributes representative of those with distinct properties: Wearing Lipstick: This attribute is strongly correlated with Male, in both MCC and heatmap score. Eyeglasses and Mustache: These are outliers to the heatmap score trend, having significantly higher heatmap scores compared to other attributes with similar MCCs. Blond Hair and Wavy Hair: This pair of attributes relate to the same regions within the image (hair) with similar MCCs, but have very different heatmap scores. Wearing Lipstick. Wearing Lipstick has the highest absolute correlation with Male out of all 40 attributes, with an MCC of 0.88 0.03. Furthermore, this correlation is predictive in both directions. One would expect that the attention map for Wearing Lipstick would highlight the mouth region. However, the mask score shows that the models attend to the eyes, eyebrows, nose, and hair regions, in addition to the mouth. In fact, the mask score distribution for Wearing Lipstick is closely similar to that of Male, only with slightly higher mouth mask score. This close similarity between Wearing Lipstick and Male is reflected in the heatmap score, the highest of any attribute. Figure 5. Evaluation of mask score using GradCAM on CelebA test set with attribute-specific feature masks, compared to worst group accuracy with Male. mask score of 1 indicates perfect agreement between the attention map and feature mask, and 0 indicates perfect disagreement. Groups are considered based on ground-truth labels for the different combinations of target attribute and Male. If the number of images in group is less than 1% of the test set, the group was excluded from consideration. skin, hats, and jewelry. We group like features together, e.g., {left brow, right brow} and {upper lip, lower lip, mouth}. Large non-localized feature masks (background, skin, and cloth) are excluded from our analyses. We choose 70%-15%-15% train-validation-test split for training on CelebAMask-HQ. To train classifiers for the attributes, we use ResNet-50 model [22] pretrained on ImageNet [58]. We replaced the final layer with two fully-connected layers with hidden layer size of 2,048 and dropout layer between them in order to improve accuracy, following Ramaswamy et al. for their CelebA ResNet classifier [55]. We used binary cross-entropy loss, weighted proportionally to positive examples of each attribute, with batch size of 32. Other hyperparameters remain the same as Sec. 4. Results. We choose subset of 17 CelebA attributes that have directly corresponding feature masks, and calculate the respective mask score for each attribute  (Fig. 5)  . Unlike Waterbirds, there is not strong correlation between worst group accuracy (WGA) and the mask score. This is not surprising, since dataset bias is not immediately correlated to singular attributes labeling. Instead, an attributes WGA and bias is dependent on the features in the image and the distribution of its label with the labels of other attributes. For example, Wearing Lipstick has moderately high WGA, but relatively low mask score. We hypothesize that this effect is due to the attributes very strong correlation with Male, causing the model to attend away from the mouth and towards features relevant for Male. Other attributes, like Eyeglasses, have both high mask score and WGA, because they are highly distinguishable. Figure 6. Comparison of attributes with the Male attribute heatmap. (Left) We compare Attention-IoU with the absolute value of the Matthews correlation coefficient between the predictions of the attribute and Male, noticing strong positive trend. Some attributes are outliers to this trend, including Eyeglasses and Mustache, which lie above this trend, and Wavy Hair, which lies below. (Right) We measure the mask score for selection of attributes. We notice that the heatmap for Male attends most strongly to the eye, eyebrows, and mouth region, which is closely mimicked by Wearing Lipstick. We can also compare attributes like Blond Hair and Wavy Hair, and find that the main difference between their heatmaps is in the eye region. Eyeglasses. Eyeglasses is moderately correlated with Male, having an MCC of 0.260.02, suggesting that Male is unlikely to influence the prediction of Eyeglasses much (or vice versa). As shown by the Eyeglasses mask score, the models attend strongly to the eyes, eyebrows, and nose regions. The score for the eyeglasses mask is low, because the score is averaged over all images in the test set, most of which do not contain eyeglasses as mask. However, the eyeglasses mask score for Eyeglasses is still the highest for any attribute, suggesting that when Eyeglasses is present, the models attend highly to that region. Surprisingly for an attribute with low MCC, the heatmap score for Eyeglasses is high at 0.860.01. We posit that this might be due to one of the weaknesses within Attention-IoU: its unable to detect when features are colocalized: we notice in Fig. 6 (right) the heatmap attends highly to eyes and eyebrows, similar to that in Male. To verify this, we train two separate sets of models, one with just images for which Eyeglasses are present, and another for which Eyeglasses are absent. We hypothesize that if the Male and Eyeglasses classifiers are using the same features, Male would continue to attend to the eye region, since these features would continue to be useful. However, when Eyeglasses are present, Male attends primarily to the mouth, not the eyes, because eyeglasses are obscuring the features relevant to Male  (Fig. 7)  . Thus, the high heatmap score Eyeglasses is not due to an underlying bias with Male in the model, but instead caused by co-localized features relevant to both attributes. Mustache is moderately correlated with Mustache. Male, with predicted label MCC of 0.51 0.04. Eyes Mask No Eyeglasses Eyeglasses Eyeglasses Mask Figure 7. Average heatmaps for Male with average masks. We train models to predict Male when Eyeglasses are absent (center-left) and present (center-right). There is stark difference in the heatmaps, suggesting that the features used by the model for predicting Eyeglasses are distinct from those used to predict Male, despite them being co-localized in the original models. Mustaches mask score distribution reflects that of Male, with slightly more attention to the hair and mouth regions. This is reflected by high heatmap score of 0.90 0.02. We choose this attribute since this attribute represents one-way correlation: images where Mustache are labeled as present are almost often labeled Male, whereas images where Mustache are labeled as absent are roughly evenly split among being labeled Male and not Male. We investigate how Attention-IoU changes based on the ground-truth values of these attributes  (Fig. 8)  . The score is extremely high (0.94 0.02) among images labeled not Male. When Male is false, the Mustache and Male attention maps closely align, indicating that the model is heavily relying on Male to classify Mustache. However, when the image is labeled as Male, the score is lower (0.84 0.5 and 0.82 0.03 for Mustache true and false respectively), the models attend less to Male regions in orMale heatmap not Male, no Mustache Male, no Mustache Male, Mustache Figure 8. Average heatmaps for Mustache. We visualize average heatmaps for Mustache for images where Mustache and Male are labeled false (center-left), where Mustache is labeled false and Male is labeled true (center-right) and where Mustache and Male are labeled true (far right), and compare to the Male heatmap (far left). When Male is labeled as false, Mustache and Male attention maps closely align but do not when Male is labeled true. der to classify Mustache. Mustache demonstrates that even though two attributes may be one-way predictive in the dataset (and thus have lower MCC), the models still attend strongly to any correlation between the attributes, which is indicated through Attention-IoU. Blond Hair and Wavy Hair. Both Blond Hair and Wavy Hair have similar predicted label MCCs of 0.34 0.02 and 0.37 0.05 respectively. Despite both referring to the hair feature, Blond Hair and Wavy Hair exhibit distinct attention maps. Relative to the Male mask score, for Wavy Hair the models attend to more to the hair region, and significantly less to the eyes, nose, and mouth. This increase for hair is larger regarding Blond Hair, which also has smaller decrease in the eye region. Overall, Blond Hair has higher heatmap score of 0.72 0.02, while Wavy Hair is lower at 0.65 0.03. We investigate difference further, positing two potential hypotheses: first, the Wavy Hair has significantly lower APN of 0.80 0.03, compared to 0.96 0.01 for Blond Hair. This could be due to labeling inconsistencies for Wavy Hair [42, 55] resulting in the heatmaps being less useful for this attribute, since GradCAM uses the predicted label to generate heatmaps. Another hypothesis for this difference could be that one of these attributes are not directly related with the Male attribute, instead, the attribute and Male are both correlated with an (unlabeled) confounder attribute, resulting in this correlation. To test this hypothesis, we modified the training distribution for Blond Hair and Wavy Hair by training models on subsampled training set  (Fig. 9)  . To do so, we varied the ground-truth MCC from 0.5 to 0.1 between the target attribute and Male by varying proportion of the 4 subgroups within the training set, keeping the overall number constant (details in Appendix C). For Blond Hair we find that there is no statistically significant change in heatmap score, with Kendall τ value of 0.007. HowBlond Hair τ : 0.073 Wavy Hair τ 0.778 Figure 9. Varying the correlation in the training dataset. To understand if the correlations are indeed responsible for the mask scores in their entirety, we subsample the dataset to vary the ground-truth MCC between Blond Hair and Wavy Hair and Male. We find that changing the ground-truth MCC for Blond Hair (left) does not change the heatmap score, while changing the MCC for Wavy Hair (right) results in strong change in the heatmap score (orange/square indicates the original results). This suggests that there might be hidden confounder present between Blond Hair and Male, which leads to the large heatmap score. This is unlike Wavy Hair, which is much more dependent on ground-truth correlations within the dataset. ever, Wavy Hair demonstrates strong correlation between MCC and heatmap score (τ = 0.785), with the model bias decreasing as train set bias decreases. This indicates that there might be an unlabeled confounder present in Blond Hair: there is an innate quality to the features distinct from dataset labels that create bias within the model for Blond Hair, rather than the simple proportion of attributes to one another in the dataset as in Wavy Hair. 6. Conclusion insights into the CelebA Attention-IoU yields several In particular, we identify specific ways in dataset [44]. which different attributes are influenced by the Male label: attributes can be biased more or less based on labels of the sensitive attribute and can be biased in ways beyond the correlation of labels within the dataset. These insights allow us to better understand how debiasing techniques might perform on this dataset. For example, methods that attempt to rebalance the dataset or improve group accuracies for Blond Hair [59, 61] might struggle since the bias is not due to the presence of blond hair, but hidden confounder. In conclusion, we propose Attention-IoU, metric for identifying and explaining spurious correlations through attention maps. We demonstrate the metrics effectiveness through validations on the Waterbirds [59] and CelebA [44] datasets. Within CelebA, we show that the metric and the mask and heatmap scores reveals aspects beyond dataset labels and model accuracies, recontextualizing prior analyses of CelebA. Future investigations of the proposed methods on other datasets and tasks can provide further insights into the nature of biases within computer vision models."
        },
        {
            "title": "Acknowledgements",
            "content": "We acknowledge support from the Princeton SEAS Innovation Grant to VVR, and from the Princeton Universitys Office of Undergraduate Research Undergraduate Fund for Academic Conferences through the Hewlett Foundation Fund to AS. This material is based upon work supported by the National Science Foundation under grant No. 2145198 to OR. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. The experiments presented in this work were performed on computational resources managed and supported by Princeton Research Computing, consortium of groups including the Princeton Institute for Computational Science and Engineering (PICSciE) and Research Computing at Princeton University."
        },
        {
            "title": "References",
            "content": "[1] Saeid Asgari, Aliasghar Khani, Fereshte Khani, Ali Gholami, Linh Tran, Ali Mahdavi Amiri, and Ghassan Hamarneh. MaskTune: Mitigating Spurious Correlations by Forcing to Explore. In Advances in Neural Information Processing Systems, pages 2328423296. 2022. 2 [2] Guha Balakrishnan, Yuanjun Xiong, Wei Xia, and Pietro Perona. Towards Causal Benchmarking of Bias in Face Analysis Algorithms. In Deep Learning-Based Face Analytics, pages 327359. Springer International Publishing, Cham, 2021. 2, 3, 5 [3] Hyemin Bang, Angie Boggust, and Arvind Satyanarayan. Explanation Alignment: Quantifying the Correctness of Model Reasoning At Scale. In ECCV Workshop on Explainable Computer Vision, 2024. 2 [4] R. K. E. Bellamy, K. Dey, M. Hind, S. C. Hoffman, S. Houde, K. Kannan, P. Lohia, J. Martino, S. Mehta, A. Mojsilovic, S. Nagar, K. Natesan Ramamurthy, J. Richards, D. Saha, P. Sattigeri, M. Singh, K. R. Varshney, and Y. Zhang. AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias. IBM Journal of Research and Development, 63:4:14:15, 2019. Conference Name: IBM Journal of Research and Development. 1 [5] Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, and Aylin Caliskan. Easily Accessible Text-to-Image Generation Amplifies Demographic In 2023 ACM Conference on Stereotypes at Large Scale. Fairness, Accountability, and Transparency, pages 1493 1504, 2023. 2 [6] Joy Buolamwini and Timnit Gebru. Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification. In Proceedings of the 1st Conference on Fairness, Accountability and Transparency, pages 7791. 2018. 1, 2 [7] Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183186, 2017. [8] Simon Caton and Christian Haas. Fairness in Machine Learning: Survey. ACM Computing Surveys, 56(7):138, 2024. 1 [9] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning Continuous Image Representation with Local Implicit Image Function. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 86248634, 2021. 5 [10] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 5 [11] Alexandra Chouldechova. Fair Prediction with Disparate Impact: Study of Bias in Recidivism Prediction Instruments. Big Data, 5(2):153163, 2017. 2 [12] Terrance de Vries, Ishan Misra, Changhan Wang, and Laurens van der Maaten. Does Object Recognition Work for In Proceedings of the IEEE/CVF Conference Everyone? on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019. 1 [13] Remi Denton, Ben Hutchinson, Margaret Mitchell, Timnit Gebru, and Andrew Zaldivar. Image Counterfactual Sensitivity Analysis for Detecting Unintended Bias, 2020. arXiv:1906.06439. 2, 3 [14] Lucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Measuring and Mitigating Unintended In Proceedings of the 2018 Bias in Text Classification. AAAI/ACM Conference on AI, Ethics, and Society, pages 67 73, 2018. [15] Simone Fabbrizzi, Symeon Papadopoulos, Eirini Ntoutsi, and Ioannis Kompatsiaris. survey on bias in visual datasets. Computer Vision and Image Understanding, 223: 103552, 2022. 2 [16] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal perturbations and smooth masks. In ICCV, 2019. 2 [17] Paul Gavrikov and Janis Keuper. Can Biases in ImageNet Models Explain Generalization? In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2218422194, 2024. 1 [18] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness. In International Conference on Learning Representations, 2019. 1 [19] Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual explanations. In ICML, 2019. 2 [20] Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. Equality of Opportunity in Supervised Learning. In Advances in Neural Information Processing Systems. 2016. 2, [21] Sophia Harrison, Eleonora Gualdoni, and Gemma Boleda. Run Like Girl! Sport-Related Gender Bias in Language and Vision. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1409314103, 2023. 2 [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceed9 ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 4, 6 [23] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women Also Snowboard: In Computer ViOvercoming Bias in Captioning Models. sion ECCV 2018, pages 793811. Springer International Publishing, Cham, 2018. 1, 2 [24] Yusuke Hirota, Yuta Nakashima, and Noa Garcia. Gender and Racial Bias in Visual Question Answering Datasets. In 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 12801292, 2022. 2 [25] Derek Hoiem, Yodsawalai Chodpathumwan, and Qieyun In Computer Dai. Diagnosing Error in Object Detectors. Vision ECCV 2012, pages 340353, 2012. 14, 15 [26] Linzhi Huang, Mei Wang, Jiahao Liang, Weihong Deng, Hongzhi Shi, Dongchao Wen, Yingjie Zhang, and Jian Zhao. Gradient Attention Balance Network: Mitigating Face Recognition Racial Bias via Gradient Attention. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3847, 2023. 2 [27] Pavel Izmailov, Polina Kirichenko, Nate Gruver, and Andrew Gordon Wilson. On feature learning in the presence In Proceedings of the 36th Interof spurious correlations. national Conference on Neural Information Processing Systems, pages 3851638532, 2024. 1 [28] Taeuk Jang and Xiaoqian Wang. Difficulty-Based Sampling for Debiased Contrastive Representation Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2403924048, 2023. 5 [29] Krish Kabra, Kathleen M. Lewis, and Guha Balakrishnan. GELDA: generative language annotation framework to reveal visual biases in datasets, 2023. arXiv:2311.18064. 2 [30] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive Growing of GANs for Improved Quality, Stability, and Variation, 2018. arXiv:1710.10196. 5 [31] Eungyeup Kim, Jihyeon Lee, and Jaegul Choo. BiaSwap: Removing Dataset Bias with Bias-Tailored Swapping Augmentation. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1497214981, 2021. 2 [32] Nayeong Kim, Sehyun Hwang, Sungsoo Ahn, Jaesik Park, and Suha Kwak. Learning debiased classifier with biased In Proceedings of the 36th International Concommittee. ference on Neural Information Processing Systems, pages 1840318415, 2024. [33] Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, and Jinwoo Shin. Discovering and Mitigating Visual Biases through Keyword Explanation, 2024. arXiv:2301.11104. 2, 5 [34] Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization, 2017. arXiv:1412.6980. 4 [35] Arvind Krishnakumar, Viraj Prabhu, Sruthi Sudhakar, and Judy Hoffman. UDIS: Unsupervised Discovery of Bias in Deep Visual Recognition Models. In British Machine Vision Conference (BMVC), 2021. 1, 2, 3 [36] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGAN: Towards Diverse and Interactive Facial Image Manipulation, 2020. arXiv:1907.11922. 2, 5 [37] Seongmin Lee, Judy Hoffman, Zijie J. Wang, and Duen Horng Chau. VIsCUIT: Visual Auditor for Bias in In 2022 IEEE/CVF Conference on CNN Image Classifier. Computer Vision and Pattern Recognition (CVPR), pages 2144321451, 2022. 2, 3 [38] Kunpeng Li, Ziyan Wu, Kuan-Chuan Peng, Jan Ernst, and Yun Fu. Tell Me Where to Look: Guided Attention Inference Network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 92159223, 2018. 2 [39] Zhiheng Li, Anthony Hoogs, and Chenliang Xu. Discover and Mitigate Unknown Biases with Debiasing Alternate Networks. In Computer Vision ECCV 2022, pages 270288, 2022. 2 [40] Zhiheng Li, Anthony Hoogs, and Chenliang Xu. Discover and Mitigate Unknown Biases with Debiasing Alternate Networks, 2022. arXiv:2207.10077. 2, 5 [41] Hao Liang, Pietro Perona, and Guha Balakrishnan. Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach Using Synthetic Faces and Human Evaluation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 49774987, 2023. [42] Bryson Lingenfelter, Sara R. Davis, and Emily M. Hand. Quantitative Analysis of Labeling Issues in the CelebA Dataset. In Advances in Visual Computing, pages 129141. Springer International Publishing, Cham, 2022. 8 [43] Ming Liu, Yukang Ding, Min Xia, Xiao Liu, Errui Ding, Wangmeng Zuo, and Shilei Wen. STGAN: Unified Selective Transfer Network for Arbitrary Image Attribute Editing. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 36683677, 2019. 5 [44] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep Learning Face Attributes in the Wild. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 37303738, 2015. 1, 2, 5, 8 [45] Alexandra Sasha Luccioni, Christopher Akiki, Margaret Mitchell, and Yacine Jernite. Stable Bias: Analyzing Societal Representations in Diffusion Models, 2023. arXiv:2303.11408. 2 [46] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys, 54(6):135, 2022. 1 [47] Nicole Meister, Dora Zhao, Angelina Wang, Vikram V. Ramaswamy, Ruth Fong, and Olga Russakovsky. Gender Artifacts in Visual Datasets. In 2023 IEEE International Conference on Computer Vision (ICCV), pages 48374848, 2023. 2 [48] Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin. Learning from Failure: De-biasing Classifier from Biased Classifier. In Advances in Neural Information Processing Systems, pages 2067320684. 2020. 2, 5 [49] Jaspar Pahl, Ines Rieger, Anna Moller, Thomas Wittenberg, and Ute Schmid. Female, white, 27? Bias Evaluation on In Data and Algorithms for Affect Recognition in Faces. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency, pages 973987, 2022. 10 [50] Otavio Parraga, Martin D. More, Christian M. Oliveira, Nathan S. Gavenski, Lucas S. Kupssinsku, Adilson Medronha, Luis V. Moura, Gabriel S. Simoes, and Rodrigo C. Barros. Fairness in Deep Learning: Survey on Vision and Language Research. ACM Computing Surveys, page 3637549, 2023. 1 [51] Dana Pessach and Erez Shmueli. Review on Fairness in Machine Learning. ACM Computing Surveys, 55(3):144, 2023. 1 [52] Vitali Petsiuk, Abir Das, and Kate Saenko. RISE: Randomized input sampling for explanation of black-box models. In Proceedings of the British Machine Vision Conference (BMVC), 2018. 2 [53] Vipin Pillai, Soroush Abbasi Koohpayegani, Ashley Ouligian, Dennis Fong, and Hamed Pirsiavash. Consistent Explanations by Contrastive Learning. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1020310212, 2022. 2 [54] Maan Qraitem, Kate Saenko, and Bryan A. Plummer. Bias Mimicking: Simple Sampling Approach for Bias Mitigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20311 20320, 2023. [55] Vikram V. Ramaswamy, Sunnie S. Y. Kim, and Olga Russakovsky. Fair Attribute Classification Through Latent Space In Proceedings of the IEEE/CVF Conference De-Biasing. on Computer Vision and Pattern Recognition (CVPR), pages 93019310, 2021. 5, 6, 8 [56] Sukrut Rao, Moritz Bohle, Amin Parchami-Araghi, and Bernt Schiele. Studying How to Efficiently and Effectively In 2023 IEEE/CVF InGuide Models with Explanations. ternational Conference on Computer Vision (ICCV), pages 19221933, 2023. 2 [57] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why Should Trust You?: Explaining the Predictions of Any Classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 11351144, 2016. 2 [58] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition ChalInternational Journal of Computer Vision, 115(3): lenge. 211252, 2015. 4, 6 [59] Shiori Sagawa, Pang Wei Koh, Tatsunori B. Hashimoto, and Percy Liang. Distributionally Robust Neural Networks. In International Conference on Learning Representations, 2020. 2, 3, 4, 5, 8 [60] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-CAM: Visual Explanations from Deep Networks International Journal of via Gradient-based Localization. Computer Vision, 128(2):336359, 2020. 2, [61] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Unsupervised Learning of Debiased Representations with PseudoAttributes. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1672116730, 2022. 5, 8 [62] Preethi Seshadri, Sameer Singh, and Yanai Elazar. The Bias Amplification Paradox in Text-to-Image Generation. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 63676384, 2024. 2 [63] Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D. Sculley. No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World, 2017. arXiv:1711.08536. 1, 2 [64] Vivswan Shitole, Fuxin Li, Minsuk Kahng, Prasad Tadepalli, and Alan Fern. One explanation is not enough: Structured attention graphs for image classification. In Advances in Neural Information Processing Systems, pages 1135211363. 2021. 2 [65] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, 2014. arXiv:1312.6034. 2 [66] Krishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Dont Judge an Object by Its Context: Learning to Overcome Contextual Bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 1, [67] Sahil Singla, Besmira Nushi, Shital Shah, Ece Kamar, and Eric Horvitz. Understanding Failures of Deep Networks via Robust Feature Extraction. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1284812857, 2021. 2 [68] Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher Re. No subclass left behind: Finegrained robustness in coarse-grained classification problems. Advances in Neural Information Processing Systems, 33: 1933919352, 2020. 2 [69] Mingxing Tan and Quoc Le. EfficientNetV2: Smaller ModIn Proceedings of the 38th Interels and Faster Training. national Conference on Machine Learning, pages 10096 10106. 2021. 15 [70] Enzo Tartaglione, Carlo Alberto Barbano, and Marco Grangetto. EnD: Entangling and Disentangling deep representations for bias correction. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1350313512, 2021. 2 [71] Antonio Torralba and Alexei A. Efros. Unbiased look at dataset bias. In CVPR 2011, pages 15211528, 2011. 2 [72] Emiel van Miltenburg. Stereotyping and Bias in the Flickr30K Dataset, 2016. arXiv:1605.06083. 2 [73] Simon Vandenhende, Dhruv Mahajan, Filip Radenovic, and Deepti Ghadiyaram. Making heads or tails: Towards semantically consistent visual counterfactuals. In ECCV, 2022. 2 [74] Sahil Verma and Julia Rubin. Fairness definitions explained. In Proceedings of the International Workshop on Software Fairness, pages 17, 2018. 1, 2, 3 [75] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical Re- [88] Yanhong Zeng, Jianlong Fu, Hongyang Chao, and Baining Guo. Learning Pyramid-Context Encoder Network for HighIn 2019 IEEE/CVF Conference Quality Image Inpainting. on Computer Vision and Pattern Recognition (CVPR), pages 14861494, 2019. 5 [89] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan Brandt, Xiaohui Shen, and Stan Sclaroff. Top-Down Neural Attention by Excitation Backprop. International Journal of Computer Vision, 126(10):10841102, 2018. 3 [90] Dora Zhao, Angelina Wang, and Olga Russakovsky. Understanding and Evaluating Racial Biases in Image Captioning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1483014840, 2021. 1, 2 [91] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 29792989, 2017. 1, 2, 3 [92] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning Deep Features for Discriminative Localization. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 29212929, 2016. 2 [93] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: 10 Million Image Database for Scene Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(6):14521464, 2018. 4 [94] Xingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, and Fang Wen. CoCosNet v2: Full-Resolution Correspondence Learning for Image Translation. In 2021 Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1146511475, 2021. port CNS-TR-2011-001, California Institute of Technology, 2011. 4 [76] Angelina Wang, Alexander Liu, Ryan Zhang, Anat Kleiman, Leslie Kim, Dora Zhao, Iroha Shirai, Arvind Narayanan, and Olga Russakovsky. REVISE: Tool for Measuring and MitInternational Journal of igating Bias in Visual Datasets. Computer Vision, 130(7):17901810, 2022. 1 [77] Jialu Wang, Yang Liu, and Xin Wang. Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in In Proceedings of the 2021 Conference on Image Search. Empirical Methods in Natural Language Processing, pages 19952008, 2021. 2 [78] Pei Wang and Nuno Vasconcelos. SCOUT: Self-Aware Discriminant Counterfactual Explanations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 89788987, 2020. 2 [79] Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced Datasets Are Not Enough: Estimating and Mitigating Gender Bias in Deep Image RepIn 2019 IEEE International Conference on resentations. Computer Vision (ICCV), pages 53105319, 2019. 2 [80] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and Olga Russakovsky. Towards Fairness in Visual Recognition: Effective Strategies for Bias Mitigation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8916 8925, 2020. 1 [81] Robert Wolfe and Aylin Caliskan. American == White in In Proceedings of Multimodal Language-and-Image AI. the 2022 AAAI/ACM Conference on AI, Ethics, and Society, pages 800812, 2022. [82] Robert Wolfe, Yiwei Yang, Bill Howe, and Aylin Caliskan. Contrastive Language-Vision AI Models Pretrained on WebScraped Multimodal Data Exhibit Sexual Objectification Bias. In Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, pages 11741185, 2023. 2 [83] Kai Yuanqing Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or Signal: The Role of Image Backgrounds in Object Recognition. In International Conference on Learning Representations, 2021. 1 [84] Rui Xu, Xintao Wang, Kai Chen, Bolei Zhou, and Chen Change Loy. Positional Encoding as Spatial Inductive Bias in GANs. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13564 13573, 2021. 5 [85] Tian Xu, Jennifer White, Sinan Kalkan, and Hatice Gunes. Investigating Bias and Fairness in Facial Expression Recognition. In Computer Vision ECCV 2020 Workshops, pages 506523, 2020. 5 [86] Yufei Xu, Qiming ZHANG, Jing Zhang, and Dacheng Tao. Vitae: Vision transformer advanced by exploring intrinsic inductive bias. In Advances in Neural Information Processing Systems, pages 2852228535. 2021. 2 [87] Matthew D. Zeiler and Rob Fergus. Visualizing and Understanding Convolutional Networks. In Computer Vision ECCV 2014, pages 818833, 2014. 12 Attention IoU: Examining Biases in CelebA using Attention Maps"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Gradients for GradCAM In Sec. 3.1, to compute GradCAM for image features that contribute positively, we describe taking the gradient of the absolute value of the class output ya for binary crossentropy loss, while taking gradient of class output ya directly for categorical cross-entropy loss. When using model that is trained using binary crossentropy loss, computing the gradient w.r.t. the absolute value of the logit (before the sigmoid) is equivalent to computing the gradient w.r.t. to the predicted class for categorical cross-entropy loss with two heads (one each for the positive and negative class). Concretely, let be the value of the logit; the probability that this model assigns to the positive class is σ(s) = 1 1+es , and the probability assigned to the negative class is 1 σ(s) = es 1+es = σ(s). The model prediction is arg max(σ(s), σ(s)) = arg max(s, s). Thus, taking the gradient with respect to the absolute value of the logits allows us to find positive contributions to the predicted binary class. B. Proofs of Invariants In Sec. 3.2, we introduce the Attention-IoU metric, BA-IoU, which is invariant to scale and size for pixel maps. First, we confirm that if the two input maps are identical, M1 = M2 = Rhw, the Attention-IoU metric is 1: with α = 2, 5 5 box in the center of the map will be resized to be 10 10 box, with the same spacial location within the map. Note that the L1 normalized maps are (cid:99)Mα = Mα Mα 1 = Mα α2Mi , (7) as each pixel in the original map appears α2 times in the resized map. Furthermore, the Frobenius inner product of the two resized maps is Mα 1 , Mα 2 = αh (cid:88) αw (cid:88) i=1 j=1 (Mα 1 )ij (Mα 2 )ij (cid:88) = α2 (cid:88) (M1)ij (M2)ij j= i=1 = α2M1, M2F (8) (9) (10) BA-IoU(M, M) = (cid:99)M, (cid:99)MF (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:99)M+(cid:99)M 2 (4) and, for the norm, = (cid:99)M, (cid:99)MF (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)(cid:99)M (cid:13) = (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)(cid:99)M (cid:13) (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13)(cid:99)M (cid:13) = 1. (5) We next prove that BA-IoU is scale invariant. Given two maps M1, M2 Rhw, suppose the maps are multiplied by the scalars a1, a2 R+ respectively. Then their L1 normalized maps are (cid:100)aiMi = aiMi aiMi1 = aiMi aiMi1 = (cid:99)Mi (6) So BA-IoU(a1M1, a2M2) = BA-IoU(M1, M2). For the proof of size invariance, we assume for simplicity that the maps are resized by positive integer scalar α using nearest neighbor interpolation. Again, consider two maps M1, M2 Rhw. Let Mα 2 Rαhαw be the rescaling of the two maps by the constant α. For example, 1 , Mα 13 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:99)Mα 1 + (cid:99)Mα 2 2 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) = 1 αh (cid:88) αw (cid:88) i=1 j=1 (cid:18) (Mα Mα 1 )ij 1 + (Mα Mα 2 )ij 2 1 (cid:19)2 = 1 4α αh (cid:88) αw (cid:88) i=1 j=1 (cid:18) (Mα 1 )ij M1 + (Mα 2 )ij M21 = 1 4α2 (cid:88) (cid:88) i=1 j=1 (cid:18) (M1)ij M11 + (M2)ij M21 = 1 α2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:99)M1 + (cid:99)M2 2 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) . (11) (cid:19)2 (12) (cid:19)2 (13) (14) Thus, combining the two parts together, BA-IoU(Mα 1 , Mα 2 ) = = = = (cid:99)Mα (cid:13) (cid:99)Mα (cid:13) (cid:13) 1 , (cid:99)Mα 2 (cid:13) 2 1 +(cid:99)Mα (cid:13) 2 (cid:13) 2 1 α4M11M21 2 Mα 1 , Mα (cid:13) 2 (cid:13) (cid:13) M1, M2F 1 α2 (cid:99)M1+(cid:99)M2 2 1 α2 (cid:13) (cid:13) (cid:13) 1 M11M21 (cid:13) (cid:13) (cid:13) 1 α2 (cid:99)M1+(cid:99)M2 2 (cid:13) 2 (cid:13) (cid:13) (cid:99)M1, (cid:99)M2F (cid:13) (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) (cid:99)M1+(cid:99)M2 = BA-IoU(M1, M2). (15) (16) (17) (18) (19) Although in the proof Mα 1 and Mα 2 are larger matrices than M1 and M2, the same argument applies if M1 and M2 are zero-padded to have same dimensions as the resized maps. C. Subsampling Training Details Here we provide experimental details for varying training set correlations in Sec. 5.2. Given target Matthews correlation coefficient between the specified attribute and Male, we find subgroup sizes that achieve the target MCC (as MCC is dependent entirely on the sizes of the 4 subgroups) using SciPys optimize.minimize with the trust region method2  (Fig. 10)  . We bound the sizes of the subsampled subgroups to the size of the original groups, and aim to minimize the distance to the original group sizes by the L2 norm. To reduce fluctuations between the subsampled sizes, we initialize the optimizer with the adjacent subgroup sizes, with the original subgroups sizes in the training set as the starting point. Lastly, after running the optimization once for all MCCs, we rerun the optimization process with the additional bound of the smallest subsampled training set, so that all the subsampled training sets are of the same size. As the subsampling was an ablation study, the heatmap scores reported in Fig. 9 were run on the validation set. D. Additional CelebA Results Model Evaluation. The average precision weighted for all 40 attributes in CelebA, averaged across the 20 trained models with the experimental setup detailed in Sec. 5.1, is 0.902 0.025. For reference, the normalized average precision (APN) [25] for the Male attribute is 0.994 0.003, the second highest after Eyeglasses (0.998 0.001). In Fig. 11 we show average heatmaps for select attributes. 2https : / / docs . scipy . org / doc / scipy / reference / optimize.minimize-trustconstr.html 14 Figure 10. Training set subgroup sizes under subsampling. Here we report subgroup sizes of the training set of varying MCCs for Blond Hair and Wavy Hair with Male, under our optimization scheme, to compute the results in Sec. 5.2 and Fig. 9. Subgroup sizes are bounded to the smallest subsampled training set size. The legend shows the four different subgroups groups, with the first value indicating the target label and the second Male. Male Wearing Lipstick Eyeglasses Mustache Blond Hair Wavy Hair Figure 11. Average heatmaps for CelebA attributes. We visualize average heatmaps for the selected attributes investigated in Sec. 5.2. Figure 12. Evaluation of mask score using GradCAM on CelebA test set with attribute-specific feature masks, compared to average precision. To compare per-attribute AP between attributes, we adopt Hoiem et al.s normalized average precision (APN) metric [25]. Figure 13. EfficientNetV2 mask score on Waterbirds. The top bars indicate Attention-IoU mask scores for EfficientNetV2-S models, while the bottom bars are corresponding ResNet-50 scores from Fig. 3. WGA is for the EfficientNet model. As with ResNet, the EfficientNet models attend less to the bird and more to the background, mirroring the decrease in WGA. CelebA Normalized Average Precision. As comparison to Fig. 5, which shows CelebA mask score against worst group accuracy, in Fig. 12 we show the mask score of the same 17 attributes to their normalized average precision (APN). Compared with worst group accuracy, there is no correlation for normalized average precision with respect to the mask score. Unlike worst group accuracy, to calculate normalized average precision one does not need to assume the correlated attribute. E. Evaluating with EfficientNet To demonstrate the effectiveness of Attention-IoU on architectures other than ResNet, we also evaluated the metric using the EfficientNetV2-S architecture [69] on both the Waterbirds and CelebA datasets. Aside from the change in architecture, and averaging over 10 trained models instead of 20, the experimental setup remained the same. For Waterbirds, the EfficientNet models show very similar pattern to ResNet in attending less to the bird and more to the background as dataset bias increases  (Fig. 14)  . The EfficientNet heatmap scores for CelebA also show strong positive trend with MCC like ResNet  (Fig. 13)  . The 5 highlighted attributes maintain their relative positions, with some changes owing to different architectures and pretraining weights. Figure 14. EfficientNetV2 heatmap scores on CelebA attributes. Orange/circle indicates results with EfficientNetV2-S models, and light blue/triangle are ResNet-50 results from Fig. 5. We observe very similar trend in EfficientNetV2 to that of ResNet-50. Highlighted attributes maintain their relative position, with some movement owing to different architectures and pretraining weights."
        }
    ],
    "affiliations": [
        "Princeton University"
    ]
}