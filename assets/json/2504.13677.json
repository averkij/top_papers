{
    "paper_title": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results",
    "authors": [
        "Andrea Santilli",
        "Adam Golinski",
        "Michael Kirchhof",
        "Federico Danieli",
        "Arno Blaas",
        "Miao Xiong",
        "Luca Zappella",
        "Sinead Williamson"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functions -- from lexical-based and embedding-based metrics to LLM-as-a-judge approaches -- across 4 datasets x 4 models x 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LLM-as-a-judge approaches as among the least length-biased choices and hence a potential solution to mitigate these biases."
        },
        {
            "title": "Start",
            "content": "Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results Andrea Santilli* Sapienza University of Rome Adam Goli nski* Apple"
        },
        {
            "title": "Arno Blaas\nApple",
            "content": "Miao Xiong National University of Singapore"
        },
        {
            "title": "Luca Zappella\nApple",
            "content": "Sinead A. Williamson Apple 5 2 0 2 8 1 ] . [ 1 7 7 6 3 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Uncertainty Quantification (UQ) in Language Models (LMs) is crucial for improving their safety and reliability. Evaluations often use performance metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). In this paper, we show that commonly used correctness functions bias UQ evaluations by inflating the performance of certain UQ methods. We evaluate 7 correctness functionsfrom lexical-based and embedding-based metrics to LM-as-a-judge approachesacross 4 datasets 4 models 6 UQ methods. Our analysis reveals that length biases in the errors of these correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge approaches as among the least length-biased choices and hence potential solution to mitigate these biases."
        },
        {
            "title": "Introduction",
            "content": "Language Models (LMs) excel at natural language generation but often produce factually incorrect outputs, or hallucinations (Guerreiro et al., 2023; Huang et al., 2025). These hallucinations are typically associated with high uncertainty about the correct output (Xiao and Wang, 2021), leading to the emergence of Uncertainty Quantification (UQ) methods as compelling approach to detect errors (Farquhar et al., 2024; Baan et al., 2023). Since often no ground truth measure for uncertainty exists, evaluation in benchmarks is typically performed using UQ performance metrics like AUROC, which measure how well UQ methods distinguish correct from incorrect answers: if model is well calibrated, low uncertainty should correlate with correctness. Consequently, accurately determining correctness in given generative task is crucial as *Equal contribution Work done during an internship at Apple. 1 any inaccuracies can severely compromise UQ performance evaluation. Since the UQ performance metric relies on both the UQ method under consideration and the correctness function, errors in either can distort the final evaluation. If the correctness function is unreliable, it can mislabel answers, leading to further inaccurate uncertainty assessments. Further, confounding factorsexternal variables that simultaneously affect both scorescan introduce biases, distorting the evaluation. In this paper, we critically analyze the impact of the correctness function on UQ performance metrics. Specifically, we test 7 different correctness functions, including lexical-based metrics (e.g., ROUGE metrics), embedding-based metrics (e.g., BERTScore (Zhang et al., 2020a)), and LM-asa-judge approaches (Zheng et al., 2024) across 4 datasets 4 models 6 UQ methods. Our findings reveal two key issues: (i) the correctness function choice significantly impacts UQ results, raising concerns about UQ benchmarks reliability, and (ii) widely used lexical-based and embeddingbased correctness functions (Farquhar et al., 2024; Fadeeva et al., 2023) introduce systematic biases that distort the performance of certain UQ methods. human evaluation of 450 LM samples, with four annotations per sample, reveals that this bias stems from confoundedness, that is the mutual dependence of certain UQ methods and the errors of certain correctness functions on the output length. Building on this, we identify correctness functions that mitigate bias by avoiding such confounding, finding LM-as-a-judge approaches best suited for UQ evaluation and most aligned with human judgment. Overall, this study highlights widespread pitfalls in UQ evaluation and charts path toward more reliable evaluation protocol."
        },
        {
            "title": "2 Evaluating uncertainty",
            "content": "Correctness function Used in UQ eval protocol Threshold Here, we review common UQ methods, correctness functions and UQ performance metrics. 2.1 UQ methods Given an input to an LM, which generates an output sequence ˆy, UQ method estimates measure of the models uncertainty about ˆy, denoted as ˆg(ˆy, x). These methods can be broadly categorized into three types: (i) single-sample, (ii) multisample, and (iii) learned. One simple single-sample approach is negative sequence probability, ˆg(ˆy, x) = ˆp(ˆyx) = (cid:81)L i=1 ˆp(ˆyiˆy<i, x), (1) where is the length of the generated answer and ˆp the output probabilities assigned by the model. Note that Eq. 1 increases with L. Multiple-sample approaches derive uncertainty scores by sampling multiple responses for the same input and measuring metric (e.g., variance) across samples. Notably, several UQ methods in this second group, like Naïve Entropy (Farquhar et al., 2024), use Eq. 1 to compute the probability of sequence of generated tokens. Lastly, learned methods train binary classifier via supervised learning on correctnesslabeled dataset (Kadavath et al., 2022). We refer readers to App. for more details on each family. ROUGE-1 (F1) ROUGE-L (F1) SQuAD (F1) BERTScore (F1) SentenceBERT Aichberger et al. (2025) Fadeeva et al. (2023); Kuhn et al. (2023) Duan et al. (2024); Chen et al. (2024a) Qiu and Miikkulainen (2024) Aichberger et al. (2025) Farquhar et al. (2024) 0.1 1.0 0.5 0.5 0.1 1.0 0.1 1.0 0.3 Fadeeva et al. (2023) Chen et al. (2024a) AlignScore Vashurin et al. (2025) LM-as-a-judge (Prompt) Farquhar et al. (2024) N/A 0. 0.5 N/A Table 1: Correctness functions used in UQ evals. LM-as-a-judge Correctness functions evaluating correctness by using another LM to judge the accuracy of ˆy against y. Functions in this category include AlignScore (Zha et al., 2023), specifically training the evaluating LM for this task, and prompt-based variants of LM-as-a-judge (Zheng et al., 2024). Table 1 summarizes common correctness functions in recent UQ papers. AUROC requires binary labels, so certain threshold is typically applied to binarize continuous correctness scores. Some correctness functions are inherently binary (e.g., LMas-a-judge), and some UQ performance metrics do not require binarization (Fadeeva et al., 2023). This variety in UQ eval protocols raises questions about which combination to trust. App. offers broader view on each metric."
        },
        {
            "title": "2.3 UQ performance metrics",
            "content": "Correctness functions ˆh(ˆy, x, y) compare generated answer ˆy to reference answer to estimate correctness score, and can be categorized as lexicalbased, embedding-based, or LM-as-a-judge. Lexical-based Correctness functions such as SQuAD (Rajpurkar et al., 2016) and ROUGE variants (Lin, 2004), are based on lexical overlap between ˆy and y. While limitations of these metrics have been studied in areas like summarization and Question Answering (QA) (Guo and Vosoughi, 2023; Chen et al., 2019; Cohan and Goharian, 2016; Fabbri et al., 2021; Reiter and Belz, 2009), their impact on UQ evaluation remains largely unexplored. Embedding-based Correctness functions assessing similarity by encoding both ˆy and using language model, typically BERT-based. Examples include BERTScore (Zhang et al., 2020b) and SentenceBERT cosine similarity (Reimers and Gurevych, 2019). The utility of UQ methods is typically assessed using UQ performance metric that quantifies how well uncertainty estimates (2.1) correlate with correctness. Among the various UQ performance metrics available in the literature (Malinin and Gales, 2020; Fadeeva et al., 2023), we focus on the Area Under the Receiver Operating Characteristic curve (AUROC) due to its widespread use in UQ benchmarks (Farquhar et al., 2024; Chen et al., 2024a). Let ˆgi ˆg( ˆyi, xi) be the uncertainty score assigned by some UQ method to the i-th data sample, and let hi be binary label denoting (ground truth) correctness of that data sample (hi = 1 if correct, hi = 0 if incorrect). AUROC can be written as AUROC = (ˆgi < ˆgj hi = 1, hj = 0) , (2) i.e., the probability that randomly chosen correct data sample receives lower uncertainty score than randomly chosen incorrect data sample. 2 Figure 1: (cid:92)AU ROC of various UQ methods across correctness functions averaged over models and datasets. The ranking of UQ methods (top row) changes across correctness functions, raising questions about which one to trust."
        },
        {
            "title": "3 Experiments",
            "content": "In this section, we evaluate several UQ methods following evaluation protocols in line with previous related works (Lin et al., 2024; Fadeeva et al., 2023; Farquhar et al., 2024) while varying just the correctness function. We consider generative QA tasks, as they are standard in UQ literature and their singleanswer format simplifies correctness evaluation relative to more open-ended tasks like summarization. Experimental setup. We evaluate the performance of 6 UQ methods across 4 datasets, 4 models, and 7 correctness functions, in line with Farquhar et al. (2024). See App. for details of UQ methods; App. for correctness functions; and App. for models, datasets, and prompt details. 3."
        },
        {
            "title": "Impact of the correctness function on UQ",
            "content": "Figure 1 illustrates the performance of different UQ methods when varying only the correctness function. For each method, we report average (cid:92)AUROC across datasets and models, focusing on the most commonly used correctness functions from Table 1. Figure 1 reveals that changing the correctness function affects not only the magnitude of the AUROC but also the ranking of UQ methods, validating our error analysis in 2.3. This, however, raises the question of which metric to trust and what is the source of disagreement."
        },
        {
            "title": "3.2 Evaluating correctness functions for UQ",
            "content": "The previous section showed how correctness functions choices impact benchmarking conclusions, but it is unclear which function yields reliable UQ results. To investigate, we evaluated several correctness functions against human annotations (four annotators per sample for 450 samples, see Figure 2: Cohen Kappa agreement rates between annotators and correctness functions. Per dataset: Fig. 5."
        },
        {
            "title": "2.4 Error analysis in UQ performance metrics",
            "content": "In practice, we estimate hi, using correctness function ˆhi ˆh(ˆyi, xi, yi) from 2.2. This means that we are actually evaluating (cid:92)AUROC = (cid:16) (cid:17) ˆgi < ˆgj ˆhi = 1, ˆhj = 0 . (3) Eq. 3 highlights key challenge: the measured performance is merely an estimate of the true AUROC and is subject to correctness function errors, which can interact with and distort the final outcome. Specifically, two scenarios may arise: (i) If errors in ˆh are independent of ˆg, the expected ranking of UQ methods based on (cid:92)AUROC will align with that of AUROCi.e., no estimator is systemically advantaged. (ii) If errors in ˆh correlate with ˆg, the estimated performance of ˆg will be systematically biased, either inflating or deflating results depending on the correlations direction. App. provides detailed analysis of these two scenarios. In practice, in 3 we find that both UQ methods and correctness functions can exhibit biases over the answer length L, falling into scenario (ii). Understanding the impact of these biases is crucial to ensuring fair and reliable comparisons in UQ. 3 Figure 3: Spearmans rank correlations coefficients. App. E). The Cohens Kappa (Cohen, 1960; Artstein and Poesio, 2008) values in Figure 2 show that LM-as-a-judge approaches (prompt-based and AlignScore) align best with human labelers, followed by ROUGE-L (Recall) with = 1. Some approaches show no agreement at all because the chosen threshold is unsuitable for our combination of tasks and models. Revisiting Figure 1, we see that these three correctness functions show more stable orderings, with some variability in AUROC magnitudesconsistent with expectations for small errors that are mostly uncorrelated with UQ methods, as per case (i) in 2.4. Conversely, most previously-used lexicaland embedding-based correctness functions poorly reflect human judgment."
        },
        {
            "title": "3.3 Spurious interaction between correctness",
            "content": "functions and UQ methods We concluded that LM-as-a-judge approaches achieve higher agreement with humans than other correctness functions. However, this alone does not explain the shift in UQ method rankings observed in 3.1. If errors were random, no systematic effect would emerge, falling into case (i) of our error analysis (2.4). However, this is not the case: the relative performance of negative sequence probability, perplexity, and probes varies dramatically. As discussed in App. C, this is indicative of spurious correlation between UQ methods and errors in the correctness functions. Many UQ methods are biased by length. Many UQ methods explicitly or implicitly depend on the length of response. In particular, negative sequence probability assigns higher uncertainty to longer responses, as each term in Eq. 1 is < 1. Other UQ methods based on sequence probability, such as entropy and perplexity, might be affected too. To investigate this relationship, we compute Spearman correlation between the scores from var4 (a) ROUGE-L (F1) (b) AlignScore Figure 4: Correctness function vs response length. The color indicates human correctness judgments. Results for other correctness functions in the Appendix, Fig. 6. ious UQ methods and the length of generated answers (measured in tokens and characters). In Figure 3, we see that multiple estimators exhibit significant positive or negative correlations with length. Many correctness functions are biased by length Many correctness functions are also known to exhibit length bias (Guo and Vosoughi, 2023). We demonstrate that this issue also affects QA. In Figure 4, we analyze the relationship between response length and correctness function output, showing correctness values for responses where all annotators agree on the label. We observe that with ROUGE-L (F1), the score is highly dependent on response length, favoring shorter sentences and making threshold selection challenging. In contrast, AlignScore is length-independent and clearly separates correct and incorrect samples. App. presents similar findings for other correctness functions. Spurious interaction. Mutual correlation between UQ methods and correctness functions on answer length can systematically inflate or deflate UQ performance metrics (App. C). This effect is evident in Figure 1, where length-based baselines (token and character lengthblue bars) perform competitively on lexicaland embedding-based correctness functions but rank last under LM-as-a-judge metrics. This may also explain discrepancies in prior works, such as the inflated ranking of negative sequence probability in Fadeeva et al. (2023). We recommend using LM-as-a-judge where possible, as its lower error is less likely to impact UQ performance metrics. While ROUGE-L (Recall) is inherently independent of the generated answers length, it offers lower correlation with human judgment, offering higher likelihood of additional confounding variables. For example, it is vulnerable to exploitation by models that produce multiple offtarget answers alongside the correct one."
        },
        {
            "title": "4 Conclusion",
            "content": "The choice of correctness function heavily influences evaluation of UQ performance metrics, often introducing biases that distort results and compromise benchmark integrity. Our study exposes these biases, particularly in lexical and embeddingbased correctness functions, and shows that LM-asa-judge provides more reliable assessments. This work underscores the critical role of metric selection in ensuring accurate UQ evaluation."
        },
        {
            "title": "Limitations",
            "content": "In this work, we have critically examined the role of the correctness function in the evaluation of UQ methods using UQ performance metrics. While our analysis sheds light on biases introduced by correctness functions, certain limitations remain. Our analysis is focused on the context of QA, as it is standard task in UQ literature and provides well-defined single-answer questions, making the definition of correctness function easier compared to open-ended tasks like machine translation and summarization where even objective human judgment of correctness is difficult. However, previous work suggests that the length bias of errors in correctness functions is not unique to the QA setting (Guo and Vosoughi, 2023), suggesting that UQ performance metrics will face similar issues in such tasks. Our recommendation is to use LM-as-a-judge as potential correctness function. While using another LM to judge correctness has demonstrated advantages (Zheng et al., 2024), it also comes with known limitations (Wang et al., 2024; Chen et al., 2024b). The reliability of the correctness assessment may vary depending on the choice of the judging LM and the prompt formulation. More concerningly, if the same LM is used as both the correctness function and as part of the UQ method, we are likely to have correlations between the LM-as-ajudges errors and the UQ method, which could inflate the UQ methods performancealthough this is mitigated by the relatively low frequency of such errors. Additionally, while our analysis on QA is based on widely used QA datasets, we do not know whether the same LM judge and prompts would generalize effectively to other tasks and datasets. Ideally, an LM judge should be rigorously evaluated against human annotators before being employed in new tasks and datasets (Bavaresco et al., 2024). Furthermore, LM-as-a-judge introduces significant computational overhead compared to traditional correctness functions, making it less practical for resource-constrained applications. Finally, our study identifies response length as confounding factor in UQ benchmarking, but other latent variables may also influence UQ methods and correctness functions in subtle ways, as discussed in App. C. deeper understanding of these biases is crucial for refining UQ evaluation protocols and ensuring more reliable assessments of model uncertainty."
        },
        {
            "title": "Acknowledgements",
            "content": "We would like to thank Xavier Suau, Miguel Sarabia, Pau Rodríguez, and Eugene Ndiaye for their feedback on an earlier draft of this manuscript."
        },
        {
            "title": "References",
            "content": "Lukas Aichberger, Kajetan Schweighofer, Mykyta Ielanskyi, and Sepp Hochreiter. 2025. Improving uncertainty estimation through semantically diverse language generation. In The Thirteenth International Conference on Learning Representations. Ron Artstein and Massimo Poesio. 2008. Survey article: Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555596. Joris Baan, Nico Daheim, Evgenia Ilia, Dennis Ulmer, Haau-Sing Li, Raquel Fernández, Barbara Plank, Rico Sennrich, Chrysoula Zerva, and Wilker Aziz. 2023. Uncertainty in natural language generation: From theory to applications. arXiv preprint arXiv:2307.15703. Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. 1983. maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179 190. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya Surikuchi, Ece Takmaz, and Alberto Testoni. 2024. Llms instead of human judges? large scale empirical study across 20 nlp evaluation tasks. Preprint, arXiv:2406.18403. Anthony Chen, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Evaluating question answering evaluation. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 119124, Hong Kong, China. Association for Computational Linguistics. 5 Chao Chen, Kai Liu, Ze Chen, Yi Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and Jieping Ye. 2024a. INSIDE: LLMs internal states retain the power of hallucination detection. In The Twelfth International Conference on Learning Representations. Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024b. Humans or LLMs as the judge? study on judgement bias. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 83018327, Miami, Florida, USA. Association for Computational Linguistics. Arman Cohan and Nazli Goharian. 2016. Revisiting summarization evaluation for scientific articles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pages 806813, Portorož, Slovenia. European Language Resources Association (ELRA). Jacob Cohen. 1960. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):3746. Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, and Kaidi Xu. 2024. Shifting attention to relevance: Towards the predictive uncertainty quantification of In Proceedings free-form large language models. of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50505063, Bangkok, Thailand. Association for Computational Linguistics. Alexander Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409. Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun, Artem Vazhentsev, Sergey Petrakov, Kirill Fedyanin, Daniil Vasilev, Elizaveta Goncharova, Alexander Panchenko, Maxim Panov, Timothy Baldwin, and Artem Shelmanov. 2023. LM-polygraph: Uncertainty estimation for language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. multilingual translation models. Transactions of the Association for Computational Linguistics, 11:1500 1517. Xiaobo Guo and Soroush Vosoughi. 2023. Length does matter: Summary length can bias summarization metrics. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1586915879, Singapore. Association for Computational Linguistics. Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and Ting Liu. 2025. survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. ACM Trans. Inf. Syst., 43(2). Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825. Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language models (mostly) know what they know. arXiv. Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, and Andrew Gordon Wilson. 2024. Large language models must be taught to know what they dont know. arXiv preprint arXiv:2406.08391. Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630. Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. ICLR. Marina Fomicheva, Shuo Sun, Lisa Yankovskaya, Frédéric Blain, Francisco Guzmán, Mark Fishel, Nikolaos Aletras, Vishrav Chaudhary, and Lucia Specia. 2020. Unsupervised quality estimation for neural machine translation. Transactions of the Association for Computational Linguistics, 8:539555. Nuno Guerreiro, Duarte Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch, Pierre Colombo, and André FT Martins. 2023. Hallucinations in large Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. 6 Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024. Generating with confidence: Uncertainty quantification for black-box large language models. TMLR. Andrey Malinin and Mark Gales. 2020. Uncertainty estimation in autoregressive structured prediction. arXiv. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36:7915579172. Xin Qiu and Risto Miikkulainen. 2024. Semantic density: Uncertainty quantification for large language models through confidence measurement in semantic space. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23832392, Austin, Texas. Association for Computational Linguistics. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Ehud Reiter and Anja Belz. 2009. An investigation into the validity of some metrics for automatically evaluating natural language generation systems. Computational Linguistics, 35(4):529558. Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, and Artem Shelmanov. 2025. Benchmarking uncertainty quantification methods for large language models with lm-polygraph. Preprint, arXiv:2406.15627. Leandro Von Werra, Lewis Tunstall, Abhishek Thakur, Sasha Luccioni, Tristan Thrush, Aleksandra Piktus, Felix Marty, Nazneen Rajani, Victor Mustar, and Helen Ngo. 2022. Evaluate & evaluation on the hub: Better best practices for data and model measurements. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 128136, Abu Dhabi, UAE. Association for Computational Linguistics. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, and Zhifang Sui. 2024. Large language models are not fair evaluators. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94409450, Bangkok, Thailand. Association for Computational Linguistics. Jason Wei, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. Measuring short-form factuality in large language models. Preprint, arXiv:2411.04368. Yijun Xiao and William Yang Wang. 2021. On hallucination and predictive uncertainty in conditional language generation. Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating factual consistency with unified alignment function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1132811348, Toronto, Canada. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020a. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020b. Bertscore: EvalIn International uating text generation with bert. Conference on Learning Representations. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc."
        },
        {
            "title": "A Details of UQ methods",
            "content": "There are several methods for generating uncertainty estimates that help assess the in-correctness of LM outputs. These approaches can be broadly classified into three main categories: 1) Singlesample methods: methods that require single forward pass from the model and that generally use directly the logits and probability distributions over the vocabulary space provided as output from 7 the model; 2) Multiple-sample methods: methods that, given prompt x, sample multiple possible outputs for the same prompt and compute an uncertainty score based on these outputs; 3) Learned methods: usually probes or small networks directly trained to predict the accuracy of the model given the prompt and the answer. We denote with the sequence of tokens corresponding to the prompt. This usually includes the instruction prompt (e.g., \"Answer the following question\") together with the question and additional context. The generated tokens are indicated as ˆyi. Additionally, superscript ˆy(s) is used for multiplesample methods to indicate the s-th sample (out of SUQ samples) sampled for given prompt. ˆp() denotes the probability assigned by the model. Single-sample methods. Single-sample methods estimate the uncertainty score using the logits that the models output. These logits are usually computed on the greedy decoded output or on lowtemperature sample decoded from the model given the prompt x. Negative Sequence Probability. Sequence probability computes the cumulative probability of the sequence. This can be used as an uncertainty score by flipping the sign and considering ˆp(ˆyx). When evaluated on the greedy decoding samples, this method is sometimes referred to in the literature as Maximum Sequence Probability (MSP) (Fadeeva et al., 2023; Vashurin et al., 2025). Aichberger et al. (2025) investigate the difference between the performance of MSP estimated using the greedy decoding and estimated using Beam Search decoding algorithm, which yields sequences with higher likelihood. ˆp(ˆyx) = (cid:89) i=1 ˆp(ˆyiˆy<i, x). (4) Perplexity. Perplexity computes the uncertainty score via the exponential of the mean token likelihood (Bahl et al., 1983). Compared to sequence probability, perplexity normalizes the underlying probability by the number of the generated tokens, (cid:32) exp"
        },
        {
            "title": "1\nL",
            "content": "L (cid:88) i=1 (cid:33) log ˆp(ˆyiˆy<i, x) . (5) Mean Token Entropy. Mean token entropy (Fomicheva et al., 2020; Malinin and Gales, 2020) computes the mean of the per-token entropies over the vocabulary distribution, HT (ˆy, x) = 1 (cid:88) i= [ˆp(ˆyiˆy<i, x)] . (6) Multiple-Sample methods. Multiple-sample methods compute an uncertainty score by sampling SUQ times for single prompt. Since it is accessing (more of) the full probability distribution, this class of methods should provide better uncertainty scores than single-sample methods, albeit at the expense of an increased computational cost at inference time. The exact number of samples SUQ is hyperparameter that usually depends on the specific UQ method. Naive Entropy. Naive Entropy computes the entropy over the different generated samples. The sequence probability of each generation is computed using the chain rule of probability, like in the Sequence Probability method, SUQ (cid:88) s=0 ˆp(ˆy(s)x) log ˆp(ˆy(s)x). (7) Learned methods. Learned methods leverage the models internal activations or its entire architecture to train additional networks or classifiers that predict the correctness of the answer. Probes are the most common form of learned method. The most prominent variety of probe is P(IK), also known as P(I Know) (Kadavath et al., 2022), which finetunes the entire model to predicts binary score whether the model can answer the question correctly or not. This is accomplished by attaching classifier to the embedding of the final token in the last layer. The training set is collected by labeling some generations from the model with the task correctness function. In this paper, we follow the implementation of (Farquhar et al., 2024; Kapoor et al., 2024) that does not train the full model but just logistic regression classifier on top of the representation of the final answer token as in (Chen et al., 2024a). We trained probes using two different correctness functions: LM-as-a-judge (using Qwen/Qwen2.5-72B-Instruct), and ROUGEL (Recall) with 0.5 threshold. When reporting results, we specify the correctness function used to label the dataset and train the probe in round bracketsfor example, Probe(LM-as-a-judge), where LM-as-a-judge denotes the judging model. Probes are trained until convergence on each training dataset with L-BFGS and tolerance value of 0.0001 and maximum number of optimization iterations of 10000."
        },
        {
            "title": "B Details of Correctness functions",
            "content": "In this section, we describe in detail the correctness functions used in our experiments. Many of these metrics return continuous score, which is binarized for calculating AUROC; we detail the thresholds used in this binarization below. B.1 Lexical-based Lexical-based metrics assess similarity by measuring lexical overlap between the generated sentence and the ground truth. These metrics are among the most widely used due to their low computational cost and long-standing history in QA evaluation. It is important to note that these metrics were originally used to evaluate QA in trained systems, where the output distribution of generated sequences has been aligned with the expected distribution of ground-truth answers in the dataset. However, in common LM zero-shot evaluation settings, this alignment is no longer guaranteed. Consequently, these metrics may fail to accurately assess correctness, requiring careful consideration when applying them. While techniques like incorporating few-shot examples, as demonstrated by Farquhar et al. (2024), can mitigate this issue to some extent, this does not fully address the fundamental limitations of lexical-based metrics. ROUGE-L. ROUGE-L measures the longest common subsequence (LCS) between the generated response and the reference answer, allowing for non-contiguous matches (Lin, 2004). In UQ the F1-score (ROUGE-L (F1)) of this metric is typically used, balancing precision and recall (Fadeeva et al., 2023; Kuhn et al., 2023; Duan et al., 2024; Chen et al., 2024a; Qiu and Miikkulainen, 2024; Aichberger et al., 2025). ROUGE-L (Precision) measures the ratio of the longest common subsequence (LCS) length to the number of unigrams in the generated answer. ROUGE-L (Recall) measures the ratio of the LCS length to the number of unigrams in the reference answer. ROUGE-L (F1) is the harmonic mean of ROUGE-L precision and recall. It is important to note that ROUGE-L recall is not affected by the length of the generated answer, whereas precision and F1 metrics are influenced by it. In the experiments of this paper, we consider ROUGE-L (F1) and ROUGE-L (Recall) variants, with both metrics computed using the Python package rouge_scorer. Both ROUGE-L variants return continuous scores; where binary score is used, we consider thresholds {0.1, 0.3, 0.5} for ROUGE-L (F1), and = 1.0 for ROUGE-L (Recall). ROUGE-1. ROUGE-1 measures the unigrams overlap between the generated response and the ground truth (Lin, 2004). ROUGE-1 captures similarity based on single-tokens overlap. This metric has been widely used in QA evaluations and in UQ benchmarks in Aichberger et al. (2025). As in Aichberger et al. (2025), we use the F1 variant (ROUGE-1 (F1)). In the experiments of this paper, the metric has been computed using the Python package rouge_scorer. ROUGE-1 (F1) returns continuous scores; where binary score is used, we use threshold = 0.1. SQuAD. This metric has been introduced in Rajpurkar et al. (2016) to measure the performance of systems trained on the homonymous dataset. The metric computes the F1 score based on word overlap between the prediction and ground truth, treating them as unordered bags of tokens, selecting the highest F1 among multiple references per question, and averaging across all questions. This metric has been used to evaluate correctness for UQ benchmarks in Farquhar et al. (2024). To compute the metric we used the implementation from Von Werra et al. (2022). SQuAD returns continuous scores; where binary score is used, we use threshold = 0.3. B.2 Embedding-based Embedding-based metrics assess similarity by encoding both the ground truth and generated text using neural model, typically BERT-based. The goal is to measure semantic similarity rather than surface-level overlap. BERTScore. BERTScore (Zhang et al., 2020b) evaluates generated answers by embedding both the generated text and the ground truth using BERT pretrained model. It then computes the pairwise cosine similarity between tokens. For each token in the generated text, the highest similarity score with any token in the reference text is selected. Finally, precision, recall, and F1-score are calculated, with the F1-score commonly used in UQ to balance precision and recall. This metric has been used to evaluate correctness for UQ benchmarks in Fadeeva et al. (2023). In the experiments of this paper, the metric has been computed using the Python et al. (2024) with Qwen/Qwen2.5-72B-Instruct as the judging model. LM-as-a-judge returns binary scores, so no thresholds are used. package bert_score as in Fadeeva et al. (2023). BERTScore returns continuous scores; where binary score is used, we use threshold = 0.8, which we empirically found to yield the highest agreement with human raters in Figure 2. SentenceBERT Similarity. SentenceBERT model (Reimers and Gurevych, 2019) is used to encode both the generated answer and the ground truth answer. Specifically, following Chen et al. (2024a), nli-roberta-large1 has been used. The cosine similarity is then calculated between the ground truth and generated answer embeddings. This metric has been used to evaluate correctness for UQ benchmarks in Chen et al. (2024a). SentenceBERT returns continuous scores; where binary score is used, we use threshold {0.4, 0.9}. B.3 LM-as-a-judge methods LM-as-a-judge metrics evaluate correctness by using another LM to judge the accuracy of generated answer against the reference answer from the dataset. The evaluating LM may be specifically trained for this task or not. AlignScore. AlignScore is metric designed to evaluate the factual consistency of generated text with respect to ground truth answer (Zha et al., 2023). It employs RoBERTa model trained to assess the alignment between two text pieces, determining how well the generated content corresponds to the source information. The training process integrates data from several NLP tasksnatural language inference, question answering, paraphrasing, fact verification, information retrieval, semantic similarity, and summarizationresulting in model trained specifically to evaluate correctness. This metric has been used to evaluate correctness for UQ benchmarks in Vashurin et al. (2025). AlignScore returns continuous scores; where binary score is used, we use threshold = 0.5. LM-as-a-judge (Prompt). LM-as-a-judge (Zheng et al., 2024) encompasses set of approaches that rely on large language model to provide human-like assessment of generated content by comparing it against reference answer. Generally, different prompting strategies can be applied to guide the evaluation process. In our experiments, we used the same prompt as Farquhar 1https://huggingface.co/sentence-transformers/nliroberta-large"
        },
        {
            "title": "C Impact of Correlated and Uncorrelated\nErrors in the Correctness function on\nAUROC Estimation",
            "content": "C.1 Case 1: Independent Errors Analysis ˆhi ˆgi hi. In this setting, we have In this section, we analyze the impact of correlated or uncorrelated errors in the correctness function on AUROC estimation. Let ˆgi ˆg(ˆyi, xi) be the uncertainty (UQ) score assigned to the answer ˆyi given the question xi. We use hi h(ˆyi, xi) {0, 1} to denote the ground-truth correctness of ˆyi, i.e., hi = 1 if the answer is correct and 0 otherwise. The estimated correctness under some correctness function ˆh is ˆhi ˆh(ˆyi, xi, yi) {0, 1}, possibly using reference answer yi. We define: TPR = (cid:0)h = 1 ˆh = 1(cid:1), FPR = 1 TPR, TNR = (cid:0)h = 0 ˆh = 0(cid:1), FNR = 1TNR. The true AUROC of ˆg, based on ground-truth labels, is AUROC(ˆg) = (cid:0)ˆgi < ˆgj hi = 1, hj = 0(cid:1). When correctness is measured by ˆh, we obtain (cid:92)AUROC(ˆg) = (cid:0)ˆgi < ˆgj ˆhi = 1, ˆhj = 0(cid:1). We additionally assume (ˆgi = ˆgj) = 0 for = j, implying AUROC(ˆg) = 1 AUROC(ˆg). Expanding (cid:92)AUROC. We rewrite (cid:92)AUROC(ˆg) by conditioning on both the true labels (hi, hj) and the estimated labels (ˆhi, ˆhj): (cid:92)AUROC(ˆg) (cid:88) = (cid:0)gi < gj ˆhi = 1, ˆhj = 0, hi = a, hj = b(cid:1) a,b{0,1} (cid:0)hi = ˆhi = 1(cid:1) (cid:0)hj = ˆhj = 0(cid:1) (cid:92)AUROC(ˆg) (cid:88) = (gi < gjhi = a, hj = b) a,b{0,1} (hi = aˆhi = 1) (hj = bˆhj = 0) = (gi < gjhi = 0, hj = 0) FPR TNR + (gi < gjhi = 1, hj = 1) TPR FNR + (gi < gjhi = 1, hj = 0) TPR TNR + (gi < gjhi = 0, hj = 1) FPR FNR = 0.5 FPR TNR + 0.5 TPR FNR + (gi < gjhi = 1, hj = 0) TPR TNR + (gi < gjhi = 0, hj = 1) FPR FNR = 0.5 FPR TNR + 0.5 TPR FNR + AUROC(ˆg) TPR TNR + (1 AUROC(ˆg)) FPR FNR. (8) All terms {TPR, TNR, FPR, FNR} in Eq. 8 are constant properties of the correctness function ˆh, and do not depend on the UQ method ˆg. Hence (cid:92)AUROC(ˆg) becomes noisy version of the true AUROC(ˆg), biased toward 0.5. Implications. In this uncorrelated setting, the bias introduced by ˆh does not depend on the UQ metric ˆg. Consequently, while the estimated AUROC values will be inaccurate, the ranking of UQ methods by (cid:92)AUROC(ˆg) will, in expectation, match the ranking by AUROC(ˆg), provided TPRTNR > FPR FNR. In practice, finite-sample effects can lead to variance, but with appropriate sample sizes, comparisons based on (cid:92)AUROC(ˆg) remain valid. C.2 Case 2: Correlated errors Analysis ˆhi ˆgi hi. In this case, (gi < gjhi = a, hj = b, ˆhi = 1, ˆhj = 0) = (gi < gjhi = a, hj = b). We discuss below two cases: (i) when ˆhs errors are independent of ˆg, and (ii) when they are correlated. In particular, if the correctness functions errors are negatively correlated with our UQ method (i.e., the more confident the UQ method is on task, the more likely it is to be erroneously marked as correct), then we have (gi < gjhi = a, hj = b, ˆhi = 1, ˆhj = 0) > (gi < gjhi = a, hj = b), for all values of and b, with the magnitude of the difference increasing with the magnitude of the correlation. This implies that (cid:92)AUROC(ˆg) > AUROC(ˆg). Similarly, if the errors are positively correlated with UQ metric, then we have (cid:92)AUROC(ˆg) < AUROC(ˆg). This indicates that we will over-estimate the true AUROC if we have negatively correlated errors, and under-estimate if we have positively correlated errors. This is problematic because it introduces errors in AUROC that do depend on the UQ method under consideration, leading to potential reordering of metrics. Sources of Correlation. Since, in general, the UQ method does not depend on the output of the correctness function or vice versa, any correlation between the UQ method, and errors in the correctness function, must be due to information in ˆy and/or that a) introduces systemic errors in the correctness function, and b) is used by the UQ method. In this paper, we look at length as such confounding variable, but it is not the only possible option. For example, the use of less frequently occurring words in ˆy might lead to both an increase in uncertainty scores due to unfamiliar language, and an increase in the probability of erroneously marking an answer as incorrect due to reduced lexical overlap with the reference answer. We leave the exploration of additional confounders as future work."
        },
        {
            "title": "D Experimental Details",
            "content": "The datasets considered are TriviaQA (Joshi et al., 2017), SQuAD (Rajpurkar et al., 2016), NQ-Open (Lee et al., 2019), and SimpleQA (Wei et al., 2024). The models considered are Falcon-7B (Penedo et al., 2023), Qwen2.5-7B (Qwen et al., 2025), and two versions of Mistral-7B (Jiang et al., 2023). Our evaluation setup closely follows the methodology proposed by Farquhar et al. (2024). To obtain model responses, we employed the same prompt as in Farquhar et al. (2024) for the long-form setting, instructing the model as follows: Answer the following question in single brief but complete sentence. Similarly, Responses are sampled using greedy decoding. for the LM-as-a-judge evaluation, we adhered to the same prompt of Farquhar et al. (2024) and used the model Qwen/Qwen2.5-72B-Instruct as the judging model. For our experiments, we employed the following models from the Hugging Face mistralai/Mistral-7B-Instruct-v0.1, Hub mistralai/Mistral-7B-Instruct-v0.3, Qwen/Qwen2.5-7B-Instruct, and tiiuae/falcon-7b-instruct. The datasets used in our evaluation consist primarily of closed-book QA datasets, with the exception of SQuAD which is an open-book dataset. Specifically, for SQuAD we incorporated the available context as part of the prompt."
        },
        {
            "title": "E Details on Human Annotation process",
            "content": "We used an internal crowdsourcing platform to gather annotations. The raters were fluent English speakers and were compensated at or above the minimum wage. We randomly sampled 450 data points from TriviaQA (Joshi et al., 2017), NQ-Open (Lee et al., 2019), and SimpleQA (Wei et al., 2024), ensuring that each dataset contained at least 150 data points, of answers from Qwen2.5-7B-Instruct sampled with greedy decoding. We excluded SQuAD from the human annotation process to avoid incorporating the additional context into the annotation prompt, thereby streamlining and accelerating the annotation process. We then tasked human annotators to evaluate the correctness, collecting four annotations per data point. Below, we present the annotation guidelines provided to each annotator. Each dataset included in the guidelines two manually labeled examples. In Figure 2 and Figure 5, we present the Cohens Kappa agreement rates among human annotators. The first figure reports agreement computed across all 450 data points, while the second breaks down the agreement rates for each individual dataset (150 data points each). For clearer visualization, Figure 4 and Figure 6 display uniformly sampled subset of 150 data points from the full set of 450. These points represent correctness values for responses where all annotators agreed on the label."
        },
        {
            "title": "Annotation Guidelines",
            "content": "You will be provided with three pieces of text: 12 (a) TriviaQA (b) SimpleQA (c) NQ Figure 5: Cohen Kappa agreement rates between human annotators and correctness functions. Question: question previously proposed to system or user. You are not asked to answer this question; it has already been answered (see Proposed Answer). Reference Answer: The authoritative or \"gold standard\" answer provided for the Question. Proposed Answer: The response given by user for the Question that needs to be evaluated. Your task is to determine whether the Proposed Answer is equivalent to the Reference Answer in the context of the Question."
        },
        {
            "title": "Evaluation Criteria",
            "content": "Focus on Equivalence: Assess whether the Proposed Answer conveys the same meaning as the Reference Answer, regardless of additional details or alternative phrasings. Ignore Personal Knowledge: Do not rely on your own knowledge or conduct external research. Base your judgment solely on the given text. Exact Matching is Not Required: The Proposed Answer does not need to be identical to the Reference Answer, but it must preserve the core meaning. Context Matters: Ensure that the Proposed Answer is relevant to the Question and correctly aligns with the Reference Answers meaning."
        },
        {
            "title": "Rating Scale",
            "content": "Choose one of the following ratings for each evaluation: Equivalent: The Proposed Answer conveys the same meaning as the Reference Answer. Not Equivalent: The Proposed Answer does not convey the same meaning, either due to missing essential information, contradictions, or incorrect interpretation."
        },
        {
            "title": "Additional Notes",
            "content": "More Detail vs. Different Information: Extra information is acceptable as long as the meaning remains the same. However, if the additional details introduce contradictions, the Proposed Answer should be marked Not Equivalent. Paraphrasing is Allowed: The wording of the Proposed Answer does not need to match exactly, but the core meaning must remain intact. Avoid Assumptions: Do not infer additional meaning beyond what is explicitly stated."
        },
        {
            "title": "Examples",
            "content": "Example 1: Equivalent <Follows one annotated equivalent example from the dataset> Example 2: Not Equivalent <Follows one annotated not equivalent example from the dataset>"
        },
        {
            "title": "F Additional Results",
            "content": "We present here supplementary results that were excluded from the main paper. Figure 5 presents the Cohens Kappa agree13 ment rate with human annotators, broken down by dataset. LM-as-a-judge approaches demonstrate stronger alignment with human judgments, whereas lexical-based and embedding-based correctness functions are highly sensitive to the selection of an appropriate threshold. Figure 6 illustrates the score assigned by correctness functions as function of the generated answers length. Among the evaluated approaches, LM-as-a-judge methods (AlignScore and LM-asa-judge Qwen/Qwen2.5-72B-Instruct) appear to be the only robust ones that remain invariant to length while effectively distinguishing between correct and incorrect samples without requiring threshold tuning. Figure 6: Correctness function vs response length. Color indicates human correctness judgments."
        }
    ],
    "affiliations": [
        "Apple",
        "National University of Singapore",
        "Sapienza University of Rome"
    ]
}