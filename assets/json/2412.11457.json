{
    "paper_title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
    "authors": [
        "Ruijie Lu",
        "Yixin Chen",
        "Junfeng Ni",
        "Baoxiong Jia",
        "Yu Liu",
        "Diwen Wan",
        "Gang Zeng",
        "Siyuan Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to a single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the model's comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the model's capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise a structure-guided timestep sampling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 6 1 ] . [ 1 7 5 4 1 1 . 2 1 4 2 : r MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes Ruijie Lu1,2 , Yixin Chen2 , Junfeng Ni2,3, Baoxiong Jia2, Yu Liu2,3, Diwen Wan1, Gang Zeng1, Siyuan Huang2 Equal contribution Work done as an intern at BIGAI 1 State Key Laboratory of General Artificial Intelligence, Peking University 2 State Key Laboratory of General Artificial Intelligence, BIGAI 3 Tsinghua University Figure 1. Novel view synthesis and cross-view image matching. The first row shows that MOVIS generalizes to different datasets on novel view synthesis (NVS). We also show visualizations of cross-view consistency compared with Zero-1-to-3 [31] and ground truth by applying image-matching. MOVIS can match significantly greater number of points, closely aligned with the ground truth."
        },
        {
            "title": "Abstract",
            "content": "Repurposing pre-trained diffusion models has been proven to be effective for NVS. However, these methods are mostly limited to single object; directly applying such methods to compositional multi-object scenarios yields inferior results, especially incorrect object placement and inconsistent shape and appearance under novel views. How to enhance and systematically evaluate the cross-view consistency of such models remains under-explored. To address this issue, we propose MOVIS to enhance the structural awareness of the view-conditioned diffusion model for multi-object NVS in terms of model inputs, auxiliary tasks, and training strategy. First, we inject structure-aware features, including depth and object mask, into the denoising U-Net to enhance the models comprehension of object instances and their spatial relationships. Second, we introduce an auxiliary task requiring the model to simultaneously predict novel view object masks, further improving the models capability in differentiating and placing objects. Finally, we conduct an in-depth analysis of the diffusion sampling process and carefully devise structure-guided timestep sam1 pling scheduler during training, which balances the learning of global object placement and fine-grained detail recovery. To systematically evaluate the plausibility of synthesized images, we propose to assess cross-view consistency and novel view object placement alongside existing image-level NVS metrics. Extensive experiments on challenging synthetic and realistic datasets demonstrate that our method exhibits strong generalization capabilities and produces consistent novel view synthesis, highlighting its potential to guide future 3D-aware multi-object NVS tasks. 1. Introduction Novel view synthesis (NVS) from single image is imperative for various applications, including AR/VR, interior designs, robotics, etc. This is highly challenging as it requires understanding complex spatial structures from single 2D perspective observation while being able to extrapolate consistent and plausible content for unobserved areas. The substantial demands for comprehensive knowledge of the 3D world render it difficult task, even for humans with rich priors of the 3D environments. Recently, significant progress has been made in the realm of single-object image-to-3D generation [29, 30, 33, 46, 47, 51] empowered by the advances in 2D diffusion models [16, 44]. Among them, one prominent line of research [4, 17, 27, 28, 30, 32, 42, 50, 60] has achieved compelling results by building on insights from Zero-1to-3 [31]: repurposing pre-trained diffusion model as novel view synthesizer by fine-tuning on large 3D object datasets can provide promising 3D-aware prior for imageto-3D tasks. However, these methods are mostly restricted to the single-object level. It remains unclear if this paradigm can be effectively extended to the multi-object level to facilitate more complex tasks like reconstructing an indoor scene. In Fig. 1, we visualize cross-view matching results of directly applying the aforementioned novel view synthesizers [31] in multi-object scenarios, which showcases weak consistency with input views. Specifically, we believe that the lack of structural awareness is the primary reason for the disappearance, distortion, incorrect position, and orientation of objects under novel views. While several works [45, 52] have explored training on mixed real-world scene datasets, the complexity introduced by multiple objects, such as spatial placement, per-instance geometry and appearance, and occlusion relationship, makes incorporating such awareness non-trivial. Inspired by the discussion above, our paper seeks to address the question: How to enhance the structural awareness of current diffusion-based novel view synthesizers? We begin by identifying the key challenges in extending singleobject methods for multi-object NVS tasks. multi-object image possesses more complicated structural information than single-object one. The model must first grasp the hierarchical structure within, which includes both high-level global object placement, e.g., position and orientation, and low-level ones like per-object geometry and appearance. High-level structural information significantly reduces the ambiguity in object composition while low-level details are essential for accurately capturing the characteristics of each object instance. Subsequently, the model needs to retain this hierarchical information captured from the input view while synthesizing novel-view images to ensure cross-view structural consistency. These capabilities are less critical in single-object level NVS tasks due to the reduced ambiguity in one-to-one mapping but are crucial for effective multiobject NVS models. Building on these insights, our technical designs are threefold. We first propose injecting structure-aware features, i.e., depth and object mask, from the input view as additional inputs to provide information on both high-level global placement and fine-grained local details. Secondly, we utilize the prediction of novel view object masks as an auxiliary task during training for the model to differentiate object instances, laying solid foundation for fine-grained geometry and appearance recovery. Finally, through an indepth analysis of the models inference process, we highlight the importance of revising the noise timestep sampling schedule, which influences the learning focus in the training process. To be specific, larger timesteps emphasize global placement learning, while smaller timesteps focus on local fine-grained object geometry and appearance recovery. To endow the view-conditioned diffusion model with both capabilities, we propose structure-guided timestep sampling scheduler that prioritizes larger timesteps in the initial stage, gradually decreasing over time to balance these two conflicting inductive biases. This design is fundamental to our proposed models effectiveness in addressing the complexity of multi-object level NVS tasks. To systematically assess the plausibility of synthesized novel view images, we additionally evaluate novel-view object mask and cross-view structural consistency apart from the existing NVS metrics. Specifically, we employ imagematching techniques [25, 58] to compare the input-view image with both the ground-truth and synthesized novel-view images. Cross-view structural consistency evaluates how closely the matching results align, providing measure of the accuracy in recovering object placement, shape, and appearance. On the other hand, the object mask, as measured by Intersection over Union (IoU), assesses the precision of object placement. Extensive experiments demonstrate that our method excels at multi-object level NVS in indoor scenes, achieving consistent object placement, shape, and appearance. Notably, it exhibits strong generalization capabilities for generating novel views on unseen datasets, including both synthetic ones 3D-FRONT [13], Room-Texture [35] and Objaverse [10], as well as the realworld SUNRGB-D [49]. In summary, our main contributions are: 1. We introduce structure-aware features as model inputs and incorporate novel view mask prediction as an auxiliary task during training. This enhances the models understanding of hierarchical structures in multi-object scenarios, leading to improved NVS performance. 2. We present novel noise timestep sampling scheduler designed to balance the learning of global object placement and fine-grained detail recovery, which is critical for addressing the increased complexity in multi-object scenarios. 3. We introduce additional metrics to systematically evaluate the novel view structural consistency. Through extensive experiments, our model demonstrates superiority in consistent object placement, geometry, and appearance recovery, showcasing strong generalization capability to unseen datasets. 2. Related Work Single object NVS with generative models Synthesizing novel view images for single objects given singleview image is an extremely ill-posed problem that requires strong priors. With great advances achieved in diffusion models [16, 44], research efforts [36, 51, 63] seek to distill priors [19, 41] learned from Text-to-Image (T2I) diffusion models via image captioning like [26]. However, this presents huge gap between the image and semantics due to the ambiguity of the text, hindering the 3D consistency of these methods. On the other hand, view-conditioned diffusion models like Zero-1-to-3 [31] explore an Imageto-Image (I2I) generation paradigm that teaches the diffusion model to control viewpoints to synthesize plausible images under novel views, providing more consistent 3D-aware prior. Subsequent work focuses on accelerating the generation speed [30, 50], enhancing the view consistency [4, 17, 28, 32, 60], or accelerating the training process [20]. However, all these methods deal with single and complete object novel view synthesis tasks since they usually fine-tune their model on Objaverse [9, 10], an extensive single-object level dataset, contrary to real images which normally consist of multiple or incomplete objects. The lack of specific model designs for compositional scenes also leads to significant inconsistencies when directly applying them to the multi-object scenarios, as shown in Fig. 1. first break down multi-object composition into several components via segmentation models like SAM [23], and then complete every single object with amodal [40, 64, 68] or inpainting [34, 44] techniques. The object instances are lifted to 3D via image-to-3D models [31, 50, 55, 62] and finally composited into whole utilizing spatial-aware optimization, 3D bounding box detection [1, 38] or carefully estimating the metric depth [21, 66]. However, this divideand-conquer paradigm is limited by the user-specified spatial relations from language prompts [6] and relies heavily on the cascaded modules of detection [1, 21, 23], completion [34, 44] and 3D-aware object-level novel view synthesis (NVS) [30, 55] to provide priors for reconstruction. Unlike any of the above, our method aims to build an end-toend image-conditioned novel view synthesis model that can directly cope with the increased complexity in multi-object compositions, especially in the multiple-object setting. Scene-level NVS with sparse view input Early efforts [18, 56, 67] attempted to directly perform scene-level NVS tasks by extracting image features from input-view images and inferring the underlying 3D representation [37]. With the development of Gaussian Splatting [22], recent works [2, 7] attempt to switch the underlying representation to Gaussian Splatting for efficiency. However, they mainly deal with synthesizing views near input ones with Inlimited generative capabilities to the unseen region. spired by the great success of diffusion models [44] and the object-level 3D-aware novel-view synthesizer [31, 55], several recent works have also attempted to perform scenelevel NVS tasks by directly conditioning the generative models on single-view scene image or monocular dynamic scene video [54]. ZeroNVS [45] proposes to train view-conditioned diffusion model on mixture of realworld datasets, MegaScenes [52] further scales up the training dataset with Internet-level data pairs for stronger generalization capabilities. However, all these works mainly deal with small view-change and simple scenarios in terms of object number, with few adaptations to tackle the multiobject complexity. In this work, we systematically examine the cross-view consistency of NVS by proposing new metrics, and explore the critical designs required to enhance the structural consistency of the view-conditioned diffusion models in the multi-object scenarios. 3. Method Multi-object 3D reconstruction with single object priors Following the advance in 3D-aware single object generative prior [31, 46], line of research work [5, 6, 12, 57] focuses on extending their application to compositional multi-object scenarios. The core idea is to decompose object compositions into individual objects, thereby fully leveraging the powerful generative priors of single-object models. They In this section, we address the challenge of enhancing the structural awareness of diffusion-based novel view synthesizers for better cross-view consistency in multi-object scenarios. We begin with brief introduction to diffusion models and view-conditioned diffusion models (Sec. 3.1). Next, we detail the key architectural designs of MOVIS, including how we incorporate structural-aware features as 3 Figure 2. Overview of MOVIS. Our model performs NVS from the input image and relative camera change. We introduce structureaware features as additional inputs and employ mask prediction as an auxiliary task (Sec. 3.2). The model is trained with structure-guided timestep sampling scheduler (Sec. 3.3) to balance the learning of global object placement and local detail recovery. input to improve the models understanding of hierarchical structure information (Sec. 3.2) and how we introduce novel view mask prediction as an auxiliary task, instructing the model to differentiate the object instances with correct object placement (Sec. 3.2). Finally, we provide an in-depth analysis of the inference process and adopt structure-guided timestep sampling scheduler (Sec. 3.3) to balance the learning of global object placement and local fine-grained object geometry and appearance recovery. We provide an overview of our model in Fig. 2. 3.1. Diffusion models as novel view synthesizers RH where ˆx0 RH Diffusion models have been recently repurposed as novel By training on posed image pairs view synthesizer. 3 denotes the input view (x0, ˆx0) { } 3 denotes the target view, viewimage and x0 conditioned diffusion models [31, 59] use the input image ˆx0 and camera pose transformation as conditions to predict the target view image x0. Concretely, the learning objective of view-conditioned diffusion models is: features to form new pose-aware feature map, taking the place of the origin CLIP [43] feature embedding. Moreover, input view image x0 will be concatenated with the noisy image as the input of the denoising U-Net. As discussed in Sec. 2, single-image-based NVS is extremely challenging, current methods inherit natural image priors from largescale pre-training [44] and fine-tune diffusion models on large-scale 3D object datasets like Objaverse [10] to learn the transformation between objects in the input and novel views given the relative camera pose. Despite their ability to generalize to in-the-wild objects, these view-conditioned diffusion models struggle with multi-object scenarios like multi-furniture indoor scenes due to the scarcity of similar data and increased complexity arising from intricate object compositions. Our method builds on the insight of repurposing the diffusion model as novel view synthesizer while emphasizing the inherent properties of multi-object scenarios in both model design and training strategy to facilitate multi-object NVS. E[ ϵθ(αtx0 + σtϵ, t, C(ˆx0, R, )) 2 2], ϵ (1) where R, represent the relative camera pose transformation between the target view x0 and the input view ˆx0. C(ˆx0, R, ) is the view-conditioned feature, combining the relative camera pose transformation with encoded image 3.2. MOVIS Our proposed method extends view-conditioned diffusion models to multi-object level, as illustrated in Fig. 2. The model leverages pre-trained Stable Diffusion [44] and concatenates the 2D structural information from the input view with noisy target image as input. Additionally, it in4 tegrates pre-trained image encoder [39] to capture semantic information, which is injected into the network through cross-attention alongside the relative camera pose. Moreover, it predicts novel view mask simultaneously as an auxiliary task to aid global object placement learning. Structure-Aware Feature Amalgamation To synthesize plausible images under novel viewpoints, the model must first grasp the compositional structural information from the input view, laying solid foundation for generation. To address the innate complexity in multi-object scenarios due to the intricate object relationship, we propose to leverage structure-aware features to facilitate models comprehension. Specifically, we use depth maps and object masks as proxies for image-level structural information. Object masks provide rough concept of object placement and shape as well as distinguishing distinct object instances, while depth maps encode the rough relative position and shape of the visible objects. Together with input-view images, these conditions provide both global structural information like object placement and local fine-grained details like object shape. Concretely speaking, we normalize the image rendered with object instance IDs of the input view to create continuous object mask image (cid:99)M. We then replicate the depth map (cid:98)D and object mask image (cid:99)M into three channels to simulate RGB images. These two structuralaware feature images, along with the input image ˆx0, are passed into VAE to obtain latent features, which will be later concatenated with the noisy target view image xt as input to the denoising U-Net. Note that both object mask and depth can be obtained with off-the-shelf detectors during the inference stage, such as SAM [23] and Marigold [21]. After introducing these additional conditions, the learning objective of MOVIS becomes: E[ ϵθ(αtx0 + σtϵ, t, CSA(ˆx0, R, T, (cid:98)D, (cid:99)M)) We use CSA( ) as shorthand for the structure-aware viewconditioned feature throughout the paper. 2 2]. (2) ϵ Auxiliary Novel View Mask Prediction Task Input-view depth maps and mask images are intended to help the model indirectly understand the structure of multi-object compositions by incorporating additional structure-aware information into the input. To encourage the model to better grasp overall structure, particularly its ability to generate it, we propose leveraging structural information (i.e., mask image) prediction under the target view as an auxiliary task, providing more direct supervision. Our approach draws inspiration from classifier guidance [11], where classifier pϕ(y xt, t) guides the denoising process of image xt to meet the criterion via incorporating the gradient xt, t)) during the inference process as an auxxtlog pϕ((y iliary guidance. Similarly, to improve the models ability to learn compositional structure, particularly in synthesizing novel view plausible object placement (position and orientation), we introduce an auxiliary task during training: pre- )) undicting object mask images Mt der target view. This prediction is conditioned on the noisy target-view image xt, timestep and input-view structureaware feature CSA( ), derived from the final layer of the denoising U-Net. We jointly train the mask predictor and denoising U-Net following: xt, t, CSA( p(Mt E[ ϵθ(αtx0+σtϵ, t, CSA( )) where we use Mtgt to denote the ground-truth target-view image, and we use the weight γ = 0.1 to balance the diffusion loss and mask prediction loss. Mtgt Mt ϵ 2 2+γ 2 2], (3) 3.3. Structure-Guided Sampling Scheduler Inspired by previous works [3, 20] that identify the importance of different scheduling strategies, we provide an indepth analysis of the inference process of multi-object novel view synthesis, where we adopt DDIM [48] sampler: 1x 0 + (cid:113) xt 1 = αt αt 1 σ2 + σtϵt. (4) F)/αt. We use as shortαt 1 where 0 = (xt (0, I). We examine hand for ϵθ(xt, t, CSA( )) and ϵt 0 (as in Eq. (4)) and the predicted mask imthe predicted age Mt at various timesteps during the inference process as they offer direct visualizations for analysis. These visualized results are presented in Fig. 3. In Fig. 3, we observe that blurry image, which indicates the approximate placement of each object, is quickly restored in the early stages (i.e., larger t) of the inference process. This suggests that global structural information is prioritized for the model to learn during this stage. Accurate object placements are crucial for synthesizing reasonable novel view images, as incorrect placement predictions indicate fundamental misunderstanding of the compositional structure. This underscores the importance of training the model with larger during the initial training periods, which is even more important for multi-object NVS scenarios considering the increased compositional complexity compared with single object. Conversely, mask with clear boundary is not predicted until later stage of the sampling process (i.e., smaller t). This is because accurate mask prediction depends heavily on relatively noiseless image. Therefore, to capture fine-grained geometry and appearance details of objects, it is essential to train the model with smaller during later training periods. Recognizing the importance of timestep in balancing the learning of global placement information and local finegrained details, we propose to adjust the original timestep sampling process to: (1, 1000) (µ(s), σ), (5) Figure 3. Visualization of inference. The early stage of the denoising process focuses on restoring global object placements, while the prediction of object masks requires relatively noiseless image to recover fine-grained geometry. This motivates us to seek balanced timestep sampling scheduler during training. The model trained w/ shift yields better mask prediction and thus recovers an image with more details and sharp object boundary. The w/o shift here refers to not shifting the µ value. Ts µlocal) where µ(s) = µlocal + (µglobal and denotes the model training iteration, Ts denotes the total number of training steps, σ = 200 is constant variance. We sample the timestep from Gaussian distribution with mean µ(s) following linear decay from large value µglobal = 1000 to small value µlocal = 500. This approach allows the model to initially learn correct global object placement information and gradually turn its focus to refining detailed object geometry in later training stages. In practice, we include warmup period with 4000 training steps sampling with fixed µ(s) = µglobal. After the warmup, we use the linear decay schedule over 2000 steps, and then stabilize the learning for fine-grained details after 6000 steps where we use µ(s) = µlocal. 4. Experiments 4.1. Experiment Setup We focus on multi-object composite NVS tasks in indoor scenes, with an emphasis on foreground objects, examining novel view structural plausibility regarding object placement, geometry, appearance, and cross-view consistency 6 with input view. This choice stems from the recent advancements in object segmentation [23], while we leave the background modeling for future work. Datasets. To facilitate the training and evaluation of our proposed method, we curate scalable synthetic dataset Compositional 3D-FUTURE (C3DFS), comprising 100k composites for training and 5k for testing. Each composite is created by composing pre-filtered furniture items from 3D-FUTURE [14] using heuristic strategy to avoid collision and penetration. Beyond C3DFS, we emphasize testing the generalization capability by benchmarking our method on Room-Texture [35] and Objaverse [10]. We also evaluate our model on diverse indoor scenes from both the synthetic dataset 3D-FRONT [13] and the real-world dataset SUNRGB-D [49]. Refer to supplementary materials for more details. Baselines. We compare our method against two recent novel view synthesis methods including Zero-1-to-3 [31] and ZeroNVS [45]. The original Zero-1-to-3 is trained on extensive object-level datasets. Therefore, we also re-train Zero-1-to-3 on our synthetic dataset C3DFS, denoted as Zero-1-to-3. ZeroNVS is trained on mixture of realFigure 4. Qualitative results of NVS and cross-view matching. Our method generates plausible novel-view images across various datasets, surpassing baselines regarding object placement, shape, and appearance. In cross-view matching, points of the same color indicate correspondences between the input and target views. We achieve higher number of matched points with more precise locations. Table 1. Quantitative results of multi-object NVS, Object Placement, and Cross-view Consistency. We evaluate on C3DFS test set, along with generalization experiments on Room-Texture [35] and Objaverse [10]. indicates re-training on C3DFS. Dataset Method Novel View Synthesis Placement Cross-view Consistency PSNR() SSIM() LPIPS() IoU() Hit Rate() Dist() C3DFS Room-Texture Objaverse ZeroNVS Zero-1-to-3 Zero-1-to-3 Ours ZeroNVS Zero-1-to-3 Zero-1-to-3 Ours ZeroNVS Zero-1-to-3 Zero-1-to-3 Ours 10.704 14.255 14.811 17. 8.217 9.860 8.342 10.014 10.557 15.850 15.433 17.749 0.533 0.771 0.794 0.825 0.647 0.712 0.657 0.718 0.513 0.810 0.815 0.840 0.481 0.302 0.283 0. 0.487 0.406 0.452 0.366 0.486 0.259 0.273 0.169 21.6 33.7 34.4 58.1 8.2 13.9 13.5 24.2 17.3 34.7 29.7 51.3 1.2 5.8 1.6 37. 1.2 2.9 0.5 6.1 2.3 10.7 1.7 50.0 130.3 86.9 120.3 44.8 140.3 104.1 157.4 78.1 126.9 80.7 126.7 47.2 world images with background, so we use images with backgrounds as its input if possible for fair comparison. Metrics. We utilize PSNR, SSIM, and LPIPS as metrics for evaluating the quality of Novel View Synthesis. To assess global object Placement, we compute the foreground-background IoU with ground-truth masks. Fi7 nally, we propose metrics to evaluate Cross-view Consistency with image-matching. More specifically, we first apply MASt3R [25] to acquire the image matching between the input-view image and target-view image for both ground truth and model predictions. With the ground-truth matching as references, we compute each methods Hit Rate and the nearest matching distance (Dist.). Hit Rate measures the proportion of predicted matches that align with the ground truth matches. Dist. quantifies the distance between the predicted matching and ground-truth matching in the target view. Please refer to supplementary materials for more details about the metrics. 4.2. Results and Discussions Fig. 4 presents qualitative results of multi-object NVS and cross-view matching visualization on different datasets, with quantitative results in Tab. 1. We summarize the following key observations: 1. Our method realizes the highest PSNR and generates high-quality images under novel views, closely aligned with the ground truth images, especially regarding novel-view object placement (position and orientation), shape, and appearance. In contrast, the baseline models struggle to accurately capture the compositional structure under novel views. For example, in the first row, the red bed is incorrectly oriented in Zero-1-to-3 and is either missing or distorted in other baselines. 2. From the visualized cross-view matching results and the metrics in Tab. 1, it is evident that our method significantly outperforms the baseline approaches in Crossview Consistency. It achieves much higher IoU and Hit Rate while exhibiting considerably lower matching distance. The visualized results are consistent with the metrics, further validating our methods accuracy in capturing cross-view structural consistency, which cannot be reflected by existing NVS metrics. 3. Our model exhibits strong generalization capabilities on unseen datasets, e.g., Room-Texture and Objaverse. We demonstrate more qualitative results, including on 3D-FRONT and SUNRGB-D in supplementary materials. We showcase potential applications, including object removal and reconstruction in supplementary materials. Further discussion about limitation and failure cases are presented in supplementary materials. 4.3. Ablation Study To verify the efficacy of each component, we perform an ablation study on our key technical designs, including the depth input (w/o depth), mask prediction auxiliary task (w/o mask), and the scheduler (w/o sch. learns with uniform (1, 1000) ). Results in Tab. 2 show that the sampler auxiliary mask prediction task and the timestep sampler are the most critical components, significantly affecting all the Figure 5. Qualitative comparison for ablation study. Excluding mask predictions or the scheduler reduces the models ability to learn object placement, as shown by the brown cabinet example. Table 2. Ablation results on C3DFS."
        },
        {
            "title": "Method",
            "content": "w/o depth w/o mask w/o sch. Ours"
        },
        {
            "title": "Placement",
            "content": "PSNR() SSIM() LPIPS() IoU() 17.080 16.914 16.166 17.432 0.819 0.818 0.808 0. 0.178 0.187 0.212 0.171 57.2 54.7 49.1 58.1 metrics and the realistic object recovery as demonstrated by the misoriented brown cabinet in the example from Fig. 5. Without the scheduler, the model produces less accurate object positions, evident both qualitatively and quantitatively. Furthermore, removing depth or mask predictions weakens the models understanding of spatial relationships and object existence. This also shows incorporating structureaware features as inputs, though seemingly intuitive, offers the most straightforward approach to enhancing the models structural awareness, particularly given recent advancements in monocular predictors [21, 23]. We present more comprehensive discussion on the scheduler strategy and ablations in supplementary materials. 5. Conclusion We extend diffusion-based novel view synthesizers to handle multi-object compositions in indoor scenes. Our proposed model generalize well across diverse datasets with more accurate object placement, shape, and appearance, showing stronger cross-view consistency with input view. The core of our approach lies in integrating structure-aware features as additional inputs, an auxiliary mask prediction task, and structure-guided timestep sampling scheduler. These components enhance the models awareness of compositional structure while balancing the learning of global object placement and fine-grained local shape and appearance. Given the prevalence of multi-object compositions in real-world scenes, we believe that our model designs and comprehensive evaluations can offer valuable insights for advancing NVS models in more complex environments."
        },
        {
            "title": "References",
            "content": "[1] Garrick Brazil, Abhinav Kumar, Julian Straub, Nikhila Ravi, Justin Johnson, and Georgia Gkioxari. Omni3d: large benchmark and model for 3d object detection in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1315413164, 2023. 3 [2] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In arXiv, 2023. 3 [3] Ting Chen. On the importance of noise scheduling for diffusion models, 2023. 5 [4] Yabo Chen, Jiemin Fang, Yuyang Huang, Taoran Yi, Xiaopeng Zhang, Lingxi Xie, Xinggang Wang, Wenrui Dai, Hongkai Xiong, and Qi Tian. Cascade-zero123: One image to highly consistent 3d with self-prompted nearby views. arXiv preprint arXiv:2312.04424, 2023. 2, 3 [5] Yixin Chen, Junfeng Ni, Nan Jiang, Yaowei Zhang, Yixin Zhu, and Siyuan Huang. Single-view 3d scene reconstrucIn International tion with high-fidelity shape and texture. Conference on 3D Vision (3DV), 2024. 3, [6] Yongwei Chen, Tengfei Wang, Tong Wu, Xingang Pan, Kui Jia, and Ziwei Liu. Comboverse: Compositional 3d assets creation using spatially-aware diffusion guidance. arXiv preprint arXiv:2403.12409, 2024. 3 [7] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv preprint arXiv:2403.14627, 2024. 3 [8] Blender Online Community. Blender - 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. 13 [9] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 3 [10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 13142 13153, 2023. 2, 3, 4, 6, 7, 14, 15 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34:87808794, 2021. 5 [12] Andreea Dogaru, Mert Özer, and Bernhard Egger. Generalizable 3d scene reconstruction via divide and conquer from single view. arXiv preprint arXiv:2404.03421, 2024. 3 [13] Huan Fu, Bowen Cai, Lin Gao, Ling-Xiao Zhang, Jiaming Wang, Cao Li, Qixun Zeng, Chengyue Sun, Rongfei Jia, Binqiang Zhao, et al. 3d-front: 3d furnished rooms with layouts and semantics. In International Conference on Computer Vision (ICCV), pages 1093310942, 2021. 2, 6, [14] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. International Journal of Computer Vision (IJCV), 129:33133337, 2021. 6, 13, 14, 16 [15] Ming Gui, Johannes Fischer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, and Björn Ommer. Depthfm: Fast monocular depth estimation with flow matching. arXiv preprint arXiv:2403.13788, 2024. 13, 14 [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:68406851, 2020. 2, 3 [17] Zehuan Huang, Hao Wen, Junting Dong, Yaohui Wang, Yangguang Li, Xinyuan Chen, Yan-Pei Cao, Ding Liang, Yu Qiao, Bo Dai, et al. Epidiff: Enhancing multi-view synthesis via localized epipolar-constrained diffusion. arXiv preprint arXiv:2312.06725, 2023. 2, 3 [18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on diet: Semantically consistent few-shot view synthesis. In International Conference on Computer Vision (ICCV), pages 58855894, 2021. [19] Ajay Jain, Ben Mildenhall, Jonathan Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 867876, 2022. 3 [20] Yifan Jiang, Hao Tang, Jen-Hao Rick Chang, Liangchen Song, Zhangyang Wang, and Liangliang Cao. Efficient3dim: Learning generalizable single-image novel-view synthesizer in one day. arXiv preprint arXiv:2310.03015, 2023. 3, 5, 12 [21] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3, 5, 8 [22] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 2023. 3 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In International Conference on Computer Vision (ICCV), pages 40154026, 2023. 3, 5, 6, 8, 13, 14 [24] Xin Kong, Shikun Liu, Xiaoyang Lyu, Marwan Taher, Xiaojuan Qi, and Andrew Davison. Eschernet: generative model for scalable view synthesis. arXiv preprint arXiv:2402.03908, 2024. 12, [25] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. GroundarXiv preprint ing image matching in 3d with mast3r. arXiv:2406.09756, 2024. 2, 8, 14 [26] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational Conference on Machine Learning (ICML), pages 1973019742. PMLR, 2023. 3 [27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution 9 text-to-3d content creation. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 300309, 2023. [28] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu, Yachao Zhang, and Xiu Li. Consistent123: One image to highly consistent 3d asset using case-aware diffusion priors. arXiv preprint arXiv:2309.17261, 2023. 2, 3 [29] Minghua Liu, Ruoxi Shi, Linghao Chen, Zhuoyang Zhang, Chao Xu, Xinyue Wei, Hansheng Chen, Chong Zeng, Jiayuan Gu, and Hao Su. One-2-3-45++: Fast single image to 3d objects with consistent multi-view generation and 3d diffusion. arXiv preprint arXiv:2311.07885, 2023. 2 [30] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexiang Xu, Hao Su, et al. One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization. arXiv preprint arXiv:2306.16928, 2023. 2, 3 [31] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In International Conference on Computer Vision (ICCV), 2023. 1, 2, 3, 4, 6, 12 [32] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang. Syncdreamer: Generating multiview-consistent images from single-view image. arXiv preprint arXiv:2309.03453, 2023. 2, 3, 16 [33] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: Single image to 3d using cross-domain diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 99709980, 2024. 2 [34] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting In Conferusing denoising diffusion probabilistic models. ence on Computer Vision and Pattern Recognition (CVPR), pages 1146111471, 2022. 3 [35] Rundong Luo, Hong-Xing Yu, and Jiajun Wu. Unsupervised discovery of object-centric neural fields. arXiv preprint arXiv:2402.07376, 2024. 2, 6, 7, 15, 16 [36] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and Andrea Vedaldi. Realfusion: 360deg reconstruction of any In Conference on Computer object from single image. Vision and Pattern Recognition (CVPR), pages 84468455, 2023. [37] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view In European Conference on Computer Vision synthesis. (ECCV), 2020. 3 [38] Yinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Jun Zhang. Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from single image. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 5564, 2020. 3 [39] Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5, 12 [40] Ege Ozguroglu, Ruoshi Liu, Dídac Surís, Dian Chen, Achal pix2gestalt: Dave, Pavel Tokmakov, and Carl Vondrick. In ConferAmodal segmentation by synthesizing wholes. ence on Computer Vision and Pattern Recognition (CVPR), pages 39313940, 2024. 3, 15 [41] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3 [42] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843, 2023. [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language superIn International Conference on Machine Learning vision. (ICML), pages 87488763. PMLR, 2021. 4 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 10684 10695, 2022. 2, 3, 4 [45] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree view synthesis from single real image. arXiv preprint arXiv:2310.17994, 2023. 2, 3, 6 [46] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. arXiv preprint arXiv:2310.15110, 2023. 2, 3, 16 [47] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. arXiv:2308.16512, 2023. 2, 16 [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations (ICLR), 2020. [49] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. Sun rgb-d: rgb-d scene understanding benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 567576, 2015. 3, 6, 15, 16 [50] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 2, 3 [51] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d creation from single image with diffusion prior. arXiv preprint arXiv:2303.14184, 2023. 2, 3 [52] Joseph Tung, Gene Chou, Ruojin Cai, Guandao Yang, Kai Zhang, Gordon Wetzstein, Bharath Hariharan, and Noah Snavely. Megascenes: Scene-level view synthesis at scale. arXiv preprint arXiv:2406.11819, 2024. 2, 3 10 [66] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1037110381, 2024. [67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 45784587, 2021. 3 [68] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zisserman. Amodal ground truth and completion in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2800328013, 2024. 3, 15 [69] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua Self-supervised scene deLin, and Chen Change Loy. occlusion. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 37843792, 2020. [70] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr Dollár. Semantic amodal segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1464 1472, 2017. 15 [53] Basile Van Hoorick, Pavel Tokmakov, Simon Stent, Jie Li, and Carl Vondrick. Tracking through containers and occluders in the wild. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1380213812, 2023. 15 [54] Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, and Carl Vondrick. Generative camera dolly: ExarXiv treme monocular dynamic novel view synthesis. preprint arXiv:2405.14868, 2024. 3 [55] Peng Wang and Yichun Shi. multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 3, 16 Imagedream: Image-prompt arXiv preprint [56] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo MartinIbrnet: Brualla, Noah Snavely, and Thomas Funkhouser. Learning multi-view image-based rendering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3 [57] Qi Wang, Ruijie Lu, Xudong Xu, Jingbo Wang, Michael Yu Wang, Bo Dai, Gang Zeng, and Dan Xu. Roomtex: Texturing compositional indoor scenes via iterative inpainting. In European Conference on Computer Vision (ECCV), pages 465482. Springer, 2025. 3 [58] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2069720709, 2024. 2, 15 [59] Daniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. arXiv preprint arXiv:2210.04628, 2022. 4 [60] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong Zhang, CL Chen, and Lei Zhang. Consistent123: Improve consistency for one image to 3d object synthesis. arXiv preprint arXiv:2310.08092, 2023. 2, [61] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 1613316142, 2023. 12 [62] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. arXiv preprint arXiv:2405.20343, 2024. 3 [63] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to 3d object with 360deg views. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 44794489, 2023. 3 [64] Katherine Xu, Lingzhi Zhang, and Jianbo Shi. Amodal completion via progressive mixed context diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 90999109, 2024. 3, 15 [65] Jiayu Yang, Ziang Cheng, Yunfei Duan, Pan Ji, and Hongdong Li. Consistnet: Enforcing 3d consistency for multiview images diffusion. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 70797088, 2024. 16 11 MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Model details A.1. DINO patch feature and camera view embedding The original image encoder of Stable Diffusion is CLIP, which excels at aligning images with text. Other image encoders like DINO-v2 [39] or ConvNeXtv2 [61] may provide denser image features that may benefit generation tasks as mentioned by previous works [20, 24]. Therefore, we opt to use the DINO feature instead of the original CLIP feature in our network following [20]. To inject the DINO patch feature into our network, we encode the input view image using DINO-v2 [39] norm patchtokens, whose shape dimension is (b, 16, 16, 1024). We will simply flatten it into (b, 256, 1024) to apply cross-attention, and means batch size here. As for the camera view embedding, we choose to embed it using 6 degrees of freedom (6DoF) representation. To be specific, let Ei be the extrinsic matrix under the input view and Ej be the extrinsic matrix under the output 1 view, we represent relative camera pose change as Ej. We will also flatten it into 16 dimensions to concatenate it to the image feature. Afterwards, we will replicate the 16-dimension embedding 256 times to concatenate the embedding to every channel of the DINO feature map. projection layer will later be employed to project the feature map into (b, 256, 768) to match the dimension of the CLIP encoder, which was originally used by Stable Diffusion so that we can fine-tune the pre-trained checkpoint. It is worth noting that we also tried other novel view synthesizers camera embedding like Zero-1-to-3 [31] using 3DoF spherical coordinates in early experiments, but we found that it does not make much of difference. A.2. Depth and mask condition In this section, we will explain how input view depth and mask are incorporated as additional conditioning inputs. For depth maps, regions with infinite depth values are assigned value equal to twice the maximum finite depth value in the rest of the image. After this adjustment, we apply normalization technique to scale the depth values 1, 1], enabling the use of the same VAE to the range of [ architecture as for images. For mask images, we assign unique values to different object instances in the input view. For instance, if there are four objects in the multi-object composite, they will be labeled as 1, 2, 3, and 4, respectively, while the background will be assigned value of 0. The same normalization technique used for depth maps is applied to these mask images. These mask images, like all other inputs, are processed by the VAE, with all images set to resolution of 256 256. A.3. Supervision for auxiliary mask prediction task To implement the auxiliary mask prediction task, we encode the output view mask images into the same latent space as the input view mask images. Object instances viewed from different angles will be assigned the same value, which is ensured during the curation of our compositional dataset. Supervision is directly applied to the latent mask features extracted from the final layer of the denoising U-Net. Only the input view mask images are required during inference, simplifying the process while preserving consistency across views. A.4. Timestep scheduler Figure S.6. Illustration of different timestep sampling strategies. Table S.3. Ablation on different strategies. Incorporating sampling strategies significantly improves the model performance, while the linear decline (LDC) achieves the best. Dataset Method Novel View Synthesis PSNR() SSIM() LPIPS() C3DFS w/o sch. KMS LIND LDC 16.166 17.148 17.279 17.432 0.808 0.823 0.824 0.825 0.212 0.175 0.172 0.171 Though we finally employed linearly declining strategy, we experimented with several alternatives. Specifically, we tested linearly declining the mean of the Gaussian 12 they are not available, as well as for all real-world images. B.2. Datasets C3DFS We use the furniture models from the 3DFUTURE dataset [14] to create our synthetic multi-object compositional data. The 3D-FUTURE dataset contains 9,992 detailed 3D furniture models with high-resolution textures and labels. Following previous work [5], we categorize the furniture into seven groups: bed, bookshelf, cabinet, chair, nightstand, sofa, and table. To ensure unbiased evaluation, we further split the furniture into distinct training and test sets, ensuring that none of the test set items are seen during training. After filtering the furniture, we first determine the number of pieces to include in each composite, which is randomly selected to be between 3 and 6. Next, we establish probability distribution based on the different types of furniture items and sample each piece according to this distribution. To prevent collisions and penetration between furniture items, we employ heuristic strategy. Specifically, for each furniture item to be added, we apply random scale adjustment within the range of [0.95, 1.05], as the inherent scale of the furniture models accurately reflects real-world sizes. We also rotate each model by random angle to introduce additional variability. Once these adjustments are complete, we begin placing the furniture items in the scene. The first item is positioned at the center of the scene at coordinates (0, 0, 0). Subsequent objects are added one by one, initially placed at the same central location. Since this results in inevitable collisions, we randomly sample direction and gradually move the newly added item along this vector until there is no intersection between the bounding boxes of the objects. By following these steps, we generate substantial number of multiple furniture items composites, ultimately creating training set of 100,000 composites and test set of 5,000 to evaluate the capabilities of our network. After placing all the furniture items, we render multiview images to facilitate training, using Blender [8] as our renderer due to its high-quality output. We first normalize each composite along its longest axis. To simulate realworld camera poses and capture meaningful multi-object compositions, we employ the following method for sampling camera views. Cameras are randomly sampled using spherical coordinates, with radius range of [1.3, 1.7] and an elevation angle range of [2, 40]. There are no constraints on the azimuth angle, allowing the camera to rotate freely around multiple objects. The chosen ranges for the radius and elevation angles are empirical. In addition to determining the camera positions, we establish \"look-at\" point to compute the camera pose. This point is randomly selected on spherical shell with radius range of [0.01, 0.2]. To enhance the models compositional structural awareFigure S.7. Comparison of different strategies. The predicted images and mask images under novel views using different strategies are visualized. We can observe that images predicted by the KMS strategy possess weird and blurry color while LDC strategy seems to be slightly better than LIND. distribution (LDC), linearly increasing the mean after sudden drop (LIND), and keeping the mean constant (KMS). These strategies are illustrated in Fig. S.6. The metrics on the test set of our C3DFS are provided in Tab. S.3, with some visual comparisons in Fig. S.7. w/o sch. in Tab. S.3 refers to applying uniform sampler, same as the one in the main paper. From the results, we observe that LDC achieves slightly better performance than LIND and KMS, largely outperforming w/o sch. However, we observed significant visual artifacts such as weird colors and extremely blurry mask images when combining the auxiliary mask prediction task with the KMS sampling strategy, as demonstrated in Fig. S.7. For example, the bed in the second example possesses unclear object boundaries and distorted texture. We believe this is due to KMS focusing primarily on denoising at larger timesteps, which provides limited guidance for recovering mask images and refining fine-grained geometry and appearance. Consequently, without dedicated period for denoising smaller timesteps, the per-object shape and appearance appear distorted and unrealistic. B. Experiment Details B.1. Implementation Details We solely utilize the data from C3DFS as the training source for our model. The training process takes around 2 days on 8 NVIDIA A100 (80G) GPUs, employing batch size of 172 per GPU. The exact training steps are 8,000 steps. During the inference process, we apply 50 DDIM steps and set the guidance scale to 3.0. We use DepthFM [15] and SAM [23] to extract the depth maps and object masks when Table S.4. Availability of conditions in different datasets. C3DFS Room-Texture Objaverse SUNRGB-D 3D-FRONT depth mask ness, we also render depth maps and instance masks (both occluded and unoccluded) from 12 different viewpoints. The unoccluded instance mask ensures that if one object is blocked by another, the complete amodal mask of the occluded object is still provided, regardless of any obstructions. Although these unoccluded instance masks are not currently necessary for our network, we render them for potential future use. Objaverse To evaluate our networks generalization capability, we create small dataset comprising 300 composites sourced from Objaverse [10]. Specifically, we utilize the provided LVIS annotations to select categories that are commonly found in indoor environments, such as beds, chairs, sofas, dressers, tables, and others. Since the meshes from Objaverse vary in scale, we rescale each object based on reference object scales from the 3D-FUTURE dataset [14]. The composition and rendering processes follow the same strategy employed in C3DFS. Algorithm 1 Hit Rate Computation 1: // Obtain image-matching pairs using MASt3R and save in list 2: Pairsgt = MASt3R(GT) 3: Pairsours = MASt3R(Ours) 4: // Each element in the list is four-element tuple = (x0, y0, x1, y1) 5: // (x0, y0) refers to the point in the input view image and (x1, y1) the point in output view image 6: Hits = 0 7: For pgt in Pairsgt // pi 8: // p[:2] refers to the first two element in the tuple 9: ours is the i-th element of Pairsours and p[2:] the last two = arg min (L2(pgt[:2], pi ours[:2])) IF L2(pgt[:2], pi ours[2:]) < 20 L2(pgt[2:], pi ours[:2]) < 20 and // Successfully hit one, delete it from gt pairs 10: 11: 12: 13: and ours pairs Hits Hits + 1 POP(Pairsours, pi 14: 15: return Hits/len(Pairsgt) ours) Inference Details Since our model requires input-view depth map and mask images as additional inputs, we need to 14 Algorithm 2 Nearest Matching Distance Computation 1: // The notations are the same as the one in Algorithm 1 2: Pairsgt = MASt3R(GT) 3: Pairsours = MASt3R(Ours) 4: Dist = EmptyList() 5: For pgt in Pairsgt = arg min 6: IF L2(pgt[:2], pi (L2(pgt[:2], pi ours[:2]) < 20 ours[:2])) 7: 8: 9: 10: return Mean(Dist) Append(Dist, L2(pgt[2:], pi POP(Pairsours, pi ours) ours[2:])) use DepthFM [15] and SAM [23] to extract the depth maps and object masks when they are not available, as well as for all real-world images. We show whether all the used datasets have provided depth maps and mask images in Tab. S.4. means they do not provide such conditions while means they do provide such conditions. B.3. Metrics Intersection over Union (IoU) Since all baseline methods do not possess the concept of every object instance, we compute foreground-background IoU for comparison. This metric can provide rough concept of the overall placement alignment with ground truth images. We extract the foreground object mask by converting the generated image to grayscale (IL). Given that the generated image has white background, we compute the foreground mask as = IL < βth, where βth is threshold that is fixed as 250. Cross-view Matching As outlined in the main paper, we introduce two metrics to systematically evaluate cross-view consistency with the input view: Hit Rate and Nearest Matching Distance. Since direct assessment of cross-view consistency is not feasible by merely evaluating the success matches between each methods predicted novel view images and the input view image, we opt to how far the predicted matches deviate from the ground-truth matches. We first compute ground-truth matching points and every models matching points using MASt3R [25] upon the input view image and the output view image (ground truth or predicted). Each matching pair is represented as fourelement tuple (x0, y0, x1, y1), where (x0, y0) corresponds gt, gt, y1 gt, y0 to the point on the input-view image, and (x1, y1) corresponds to the point on the output-view image. For each ground-truth matching pair (x0 gt), we find the nearest predicted matching pair in each models results, denoted as (x0, y0, x1, y1), based on the Euclidean distance between points in the input view image. If both is and L2 L2 smaller than fixed threshold 20, the match is considered successful hit. The Hit Rate is then calculated as the ratio of successful hits to the total number of ground-truth matches. gt), (x0, y0) gt), (x1, y1) gt, y1 gt, y0 (x1 (x0 gt, is within the threshold. gt), (x0, y0) gt), (x1, y1) For Nearest Matching Distance, we examine whether (x0 For L2 those passing this check, we compute the mean distance as the Nearest Matching DisL2 tance, averaging over all successful hits. detailed pseudocode explanation can be found in Algorithm 1 and Algorithm 2. gt, y1 (x1 B.4. Results We show more visualized results of our own methods along with ground truth on C3DFS in Fig. S.15, on Objaverse [10] in Fig. S.16, and on Room-Texture [35] in Fig. S.17. More visualized comparisons with baselines on RoomTexture [35], SUNRGB-D [49] and 3D-FRONT [13] are shown in Fig. S.8. more complete ablation study on other datasets including Objaverse and Room-Texture is shown in Tab. S.5. Some continuous rotation examples on SUNRGBD and 3D-FRONT are shown in Fig. S.9, and more crossview matching results without ground-truth pairs as reference are shown in Fig. S.10. B.5. Applications Object Removal Since we can predict mask images under novel views, we can support simple image editing tasks like novel view object removal by simply setting threshold value in the mask image and mask out corresponding pixels to achieve object removal. An example is shown in Fig. S.13. Reconstruction The capability to synthesize novel view images that are consistent with the input view image the model possesses 3D-awareness, demonstrates that which can assist downstream tasks such as reconstruction. We leverage an off-the-shelf reconstruction method DUSt3R [58] using the input-view image and novel view images predicted by our method. Two visualized examples are shown in Fig. S.14. B.6. Mutual Occlusion In multi-object compositions, mutual occlusion between objects is very common. Although we did not specifically design the method to make the model aware of mutual occlusion, the model has learned some understanding 15 of these occlusion relationships. series of research efforts [40, 53, 64, 6870] specifically focus on addressing mutual occlusion relationships by predicting the amodal masks or synthesizing amodal appearance, but these models typically do not consider scenarios involving camera view change. Moreover, there may not be well-established metric to measure how well the model understands mutual occlusion from novel viewpoints. We provide simple experiment and discussion in this section to illustrate models comprehension of mutual occlusion. First, in the context of novel view synthesis, the comprehension of occlusion relationships can be divided into two parts. The first is the ability to synthesize parts that were occluded in the input view. The second is the ability to synthesize new occlusion relationships under the novel view. We show several examples of synthesizing occluded parts and synthesizing new occlusions in Fig. S.12. We believe this capability is learned in data-driven way since the multi-object composites are physically plausible regarding these occlusion relationships. Secondly, we now propose new metric to evaluate the capability of understanding mutual occlusion under this setting. We first use visible mask and amodal mask in the input-view image to determine how heavily an object is occluded: 1. If an objects visible mask is exactly its full mask, there exists no occlusion. 2. If an objects visible mask is more than 70% of its full mask, the object is occluded. 3. If an objects visible mask is less than 70% of its full mask, the object is heavily occluded. Afterward, we segment the predicted view image with ground truth per-object visible mask. We calculate the specific regions PSNR, SSIM, and LPIPS metrics as shown in Tab. S.6. It can reflect how well our model and baseline models are at synthesizing novel view plausible images that are originally occluded under the input view. There are 10903 fully visible objects, 6058 occluded objects, and 2215 heavily occluded objects. This experiment is conducted on our own C3DFS. C. Failure Cases and Limitations Failure Cases We showcase two failure cases in Fig. S.11. We can observe that delicate structure and texture like colorful pillows on the sofa or slim legs of chairs are hard for our model to learn. Though object placement is approximately accurate, more fine-grained consistency is not quite ideal in these cases. We believe training with higher resolution and incorporating epipolar constraints will mitigate this problem in the future. Limitations We identify two limitations of our work. Firstly, though we achieve stronger cross-view consistency Figure S.8. Visualized comparison on Room-Texture [35], SUNRGB-D [49], and 3D-FRONT [14]. Table S.5. Ablation study on various datasets. Dataset Method Novel View Synthesis Placement Cross-view Consistency PSNR() SSIM() LPIPS() IoU() Hit Rate() Dist() C3DFS Room-Texture Objaverse w/o depth w/o mask w/o sch. Ours w/o depth w/o mask w/o sch. Ours w/o depth w/o mask w/o sch. Ours 17.080 16.914 16.166 17.432 9.829 9.576 9.173 10.014 17.457 17.176 16.642 17.749 0.819 0.818 0.808 0.825 0.705 0.699 0.689 0. 0.835 0.834 0.825 0.840 0.178 0.187 0.212 0.171 0.365 0.384 0.392 0.366 0.178 0.187 0.210 0.169 57.2 54.7 49.1 58.1 25.7 24.2 22.4 24. 50.5 47.3 43.2 51.3 39.2 25.4 11.9 37.0 5.5 2.7 2.3 6.1 23.0 11.1 6.3 50.0 45.2 50.4 48.6 44.8 75.3 92.2 88.6 78. 52.6 57.1 55.0 47.2 with the input view image, our model does not guarantee the multi-view consistency between our synthesized images. It is plausible to synthesize any results in areas with ambiguity, leading to potential multi-view inconsistency in our model. We believe incorporating multi-view awareness techniques [24, 32, 46, 47, 55, 65] can mitigate this prob16 Figure S.9. Continuous rotation examples on SUNRGB-D and 3D-FRONT. We rotate the camera around the multi-object composites, successfully synthesizing plausible novel-view images across wide range of camera pose variations. This first five examples are from SUNRGB-D, and the last three examples are from 3D-FRONT. Table S.6. Evaluation on objects with varying extents of occlusion. Method Visible Occluded Heavily Occluded PSNR() SSIM() LPIPS() PSNR() SSIM() LPIPS() PSNR() SSIM() LPIPS() Ours Zero-1-to-3 Zero-1-to-3 11.45 9.46 9.68 0.56 0.54 0.55 0.13 0.16 0. 11.33 9.33 9.54 0.55 0.52 0.52 0.14 0.17 0.15 10.57 9.00 9.26 0.55 0.53 0.53 0.14 0.16 0. Figure S.10. Visualized cross-view matching results. Since we do not have ground truth image for 3D-FRONT and SUNRGB-D, we only visualize cross-view matching results using our predicted images. But we can still observe strong cross-view consistency from the accurate matching results. Figure S.12. Occlusion Synthesis Capability. Our proposed method can synthesize new occlusion relationship under novel views as shown in the highlighted area of sofa or cabinet in (a). Our method can also hallucinate occluded parts as shown in the highlighted area of chairs in (b). D. Potential Negative Impact The use of diffusion models to generate compositional assets can raise ethical concerns, especially if used to create realistic yet fake environments. This could be exploited for misinformation or deceptive purposes, potentially leading to trust issues and societal harm. Additionally, hallucinations from diffusion generation models can produce misleading or false information within generated images. This is particularly concerning in applications where accuracy and fidelity to the real world are critical. Figure S.11. Failure Cases. It is hard for our model to learn extremely fine-grained consistency on objects with delicate structure and texture. lem. Secondly, we do not model background texture in our framework due to difficulty of realistically mimicking realworld background texture, making it less convenient to directly apply our method to in-the-wild images. We believe training on more realistic data with background in the future can make our model more convenient to use. 18 Figure S.13. Object Removal Example. We can remove an object under novel views by setting threshold to the predicted mask image and delete corresponding pixels. Figure S.14. Reconstruction results using DUSt3R. We rotate our camera around the multi-object composite and use the predicted images along with the input-view image for reconstruction. 19 Figure S.15. More visualized results on C3DFS dataset. Figure S.16. More visualized results on Objaverse dataset. 21 Figure S.17. More visualized results on Room-Texture dataset."
        }
    ],
    "affiliations": [
        "State Key Laboratory of General Artificial Intelligence, BIGAI",
        "State Key Laboratory of General Artificial Intelligence, Peking University",
        "Tsinghua University"
    ]
}