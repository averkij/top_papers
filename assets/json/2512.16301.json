{
    "paper_title": "Adaptation of Agentic AI",
    "authors": [
        "Pengcheng Jiang",
        "Jiacheng Lin",
        "Zhiyi Shi",
        "Zifeng Wang",
        "Luxi He",
        "Yichen Wu",
        "Ming Zhong",
        "Peiyang Song",
        "Qizheng Zhang",
        "Heng Wang",
        "Xueqiang Xu",
        "Hanwen Xu",
        "Pengrui Han",
        "Dylan Zhang",
        "Jiashuo Sun",
        "Chaoqi Yang",
        "Kun Qian",
        "Tian Wang",
        "Changran Hu",
        "Manling Li",
        "Quanzheng Li",
        "Hao Peng",
        "Sheng Wang",
        "Jingbo Shang",
        "Chao Zhang",
        "Jiaxuan You",
        "Liyuan Liu",
        "Pan Lu",
        "Yu Zhang",
        "Heng Ji",
        "Yejin Choi",
        "Dawn Song",
        "Jimeng Sun",
        "Jiawei Han"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems."
        },
        {
            "title": "Start",
            "content": "Pengcheng Jiang1, Jiacheng Lin1, Zhiyi Shi1,4, Zifeng Wang1, Luxi He3, Yichen Wu4, Ming Zhong1, Peiyang Song6,7, Qizheng Zhang2, Heng Wang1, Xueqiang Xu1, Hanwen Xu5, Pengrui Han1, Dylan Zhang1, Jiashuo Sun1, Chaoqi Yang1, Kun Qian12, Tian Wang12, Changran Hu7, Manling Li10, Quanzheng Li4, Hao Peng1, Sheng Wang5, Jingbo Shang8, Chao Zhang9, Jiaxuan You1, Liyuan Liu1, Pan Lu2, Yu Zhang11, Heng Ji1, Yejin Choi2, Dawn Song7, Jimeng Sun1, Jiawei Han1 1 UIUC"
        },
        {
            "title": "Stanford",
            "content": ""
        },
        {
            "title": "Princeton",
            "content": "4 Harvard 5 UW"
        },
        {
            "title": "Caltech",
            "content": ""
        },
        {
            "title": "UC Berkeley",
            "content": "8 UCSD"
        },
        {
            "title": "Georgia Tech",
            "content": "10 Northwestern 11 TAMU 12Unity Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into toolexecutionsignaled and agent-outputsignaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems. Github Repository: https://github.com/pat-jj/Awesome-Adaptation-of-Agentic-AI 5 2 0 D 8 1 ] . [ 1 1 0 3 6 1 . 2 1 5 2 : r Figure 1 Overview of adaptations in agentic AI. Agent: the foundation models serving as orchestration and reasoning modules; Tool: callable components other than the agent model that operate independently, e.g., APIs, ML models, subagents, or memory. We categorize these adaptations into two: agent adaptation (A1 & A2): adapting agent models, and tool adaptation (T1 & T2): adapting tools for agents. See more details in 3."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Background (2) Agentic AI Systems (2.1) Adaptation (2.2) Mathematical Notations (3.1) Overview (3) Adaptation Paradigms of Agentic AI (3.2) Prompt Engineering (2.2.1) Fine-Tuning (2.2.2) Illustrative Examples (3.3) A1: Tool Execution Result as Signal (4.1) Earlier Works: SFT & Off-Policy Methods (4.1.1) Agent Adaptation (4) A2: Agent Output as Signal (4.2) T1: Agent-Agnostic Tool Adaptation (5.1) Tool Adaptation (5) RLVR-Based Methods (4.1.2) Adaptation w/o Tools (4.2.1) Adaptation w/ Tools (4.2.2) Foundational Systems and Architectures (5.1.1) Categories and Training Methods (5.1.2) Earlier Methods (5.2.1) T2: Agent-Supervised Tool Adaptation (5.2) Subagent-as-Tool (5.2.2) A1 & A2 (6.2) Agentic Memory and Others (5.2.3) i g n a d Comparison (6) T1 & T2 (6.3) Applications (7) Opportunities (8) Strategic Recommendations (6.5) Deep Research (7.1) Software Development (7.2) Computer Use (7.3) Drug Discovery & Development (7.4) ... Co-Adaptation (8.1) Continual Adaptation (8.2) Safe Adaptation (8.3) Efficient Adaptation (8.4) Figure 2 The structure of this paper."
        },
        {
            "title": "1 Introduction",
            "content": "The rapid progress of foundation models, such as large language models (LLMs), has catalyzed the rise of agentic AI systems: autonomous AI systems capable of perceiving their environment, invoking external tools, managing memory, and executing multi-step plans toward completing complex tasks [14]. Agentic AI demonstrates remarkable potential in applications ranging from scientific discovery [5, 6] to software development and clinical research [79]. However, current agentic AI systems still struggle with challenges such as unreliable tool use, limited long-horizon planning, domain-specific reasoning gaps, robustness issues in real-world environments, and poor generalization to unexplored environments where the agent lacks prior interaction experience [1013]. These limitations reveal that even highly capable foundation models often require additional adaptation to specialize for particular tasks or real-world scenarios. This motivates the need for adaptation in agentic AI systems, whereby the components of an agentic system are modified or optimized so that the agent achieves higher task performance, improved reliability, and better generalization across diverse scenarios. Building on this motivation, we conduct comprehensive survey on the adaptation in agentic AI systems, aiming to systematically analyze how components in agentic AI systems are modified to overcome current limitations. Compared with existing surveys on modern AI agents [1, 1418], this paper centers specifically on adaptation in agentic AI. To structure this rapidly expanding literature, we introduce unified framework that organizes adaptation in agentic AI into four core paradigms spanning both agent adaptation and tool adaptation, as shown in Figure 1. This framework clarifies the underlying design space, highlights the trade-offs between different"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "adaptation strategies, and provides practical guidance for choosing or transitioning between paradigms based on supervision signals, task requirements, and system-level constraints. In our framework, we conclude adaptation strategies for agentic AI into two dimensions according to which component is optimized (3). The first dimension, which we term Agent Adaptation, focuses on modifying the agents internal parameters, representations, or behavioral policies to better align with task requirements. This includes both traditional fine-tuning approaches [19] and modern reinforcement learning methods that leverage environment feedback [20, 21]. The second dimension, Tool Adaptation, shifts the optimization target from the agent to its external tools, e.g., retrievers, planners, memory modules, and specialized models, enabling frozen agents to benefit from an adaptive operational environment [22, 11, 23]. Within these two broad paradigms, we further identify four distinct adaptation strategies, forming comprehensive taxonomy that organizes the rapidly evolving landscape of agentic AI research: A1: Tool Execution Signaled Agent Adaptation (3.2.1, 4.1): The agent is optimized using verifiable outcomes produced by external tools it invokes. This paradigm captures settings where correctness signals arise directly from tool execution, such as code sandbox results, retrieval relevance scores, or API call outcomes. A2: Agent Output Signaled Agent Adaptation (3.2.2, 4.2): The agent is optimized using evaluations of its own outputs, e.g., final answers, plans, or reasoning traces, possibly after incorporating tool results. This paradigm includes both tool-free outcome-based learning and tool-augmented adaptation driven by answer correctness or preference scores. T1: Agent-Agnostic Tool Adaptation (3.2.3, 5.1): Tools are trained independently of the frozen agent. These tools include retrievers, domain-specific models, and other pretrained components that can be used as plug-and-play modules orchestrated by the frozen agent. T2: Agent-Supervised Tool Adaptation (3.2.4, 5.2): The agent remains fixed while its tools are adapted using signals derived from the agents outputs. This paradigm includes reward-driven retriever tuning, adaptive rerankers, search subagents, and memory-update modules trained to better support the frozen agent. It is worth noting that these four strategies are not mutually exclusive: state-of-the-art systems increasingly combine multiple adaptation paradigms to achieve optimal performance [2426]. For instance, deep research system might employ T1-style retrieval tools (pre-trained dense retrievers), T2-style adaptive search agents (trained via frozen LLM feedback), and A1-style reasoning agents (fine-tuned with execution feedback) in cascaded architecture [6]. In 6, we further emphasize that the choice among these paradigms involves fundamental trade-offs along several dimensions. (1) Cost and flexibility: Agent adaptation (A1/A2) typically requires substantial computational resources for training billion-parameter models but offers maximal flexibility, while tool adaptation (T1/T2) optimizes external components at lower cost but may be constrained by the frozen agents capabilities [27, 22]. (2) Generalization: T1 tools trained on broad data distributions often generalize well across agents and tasks [23, 28], whereas A1 methods may overfit to specific environments unless carefully regularized [20]. (3) Modularity: T2 approaches enable independent tool upgrades without agent retraining [29, 22], facilitating continuous system improvement, while A1/A2 methods may suffer from catastrophic forgetting when adapting to new tasks. Scope and contributions. This paper provides the first comprehensive taxonomy of adaptation strategies for agentic AI, systematically organizing recent advances across agent adaptation (A1, A2) and tool adaptation (T1, T2). We offer several key contributions: unified conceptual framework that clarifies the associations and distinctions between adaptation paradigms and their underlying principles (Figure 2). Detailed technical surveys of representative methods within each category, documenting their training objectives, architectural choices, and empirical performance across diverse benchmarks. Systematic comparison of adaptation strategies along dimensions of cost, flexibility, generalization capability, and modularity."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Demonstrating how adaptation strategies are tailored to domain applications, spanning deep research, software development, computer use, and drug discovery (7). Identification of open challenges and future research directions, including unified agent-tool co-adaptation frameworks, theoretical understanding of adaptation dynamics, and standardized evaluation protocols (8). Organization. The remainder of this paper is organized as follows. Section 2 provides foundational concepts, introducing the core components of agentic AI systems and the two primary forms of adaptation (prompt engineering and fine-tuning). Section 3 presents an overview of adaptation paradigms under our proposed framework, formalizing the four paradigms (A1, A2, T1, T2) and illustrating them with concrete examples. Sections 4 and 5 present our main taxonomy, systematically reviewing agent adaptation (A1, A2) and tool adaptation (T1, T2) methods respectively. Section 6 compares these paradigms along key dimensions. Section 7 examines real-world applications across multiple domains. Finally, Section 8 discusses open challenges and future research directions. Throughout, we emphasize the complementary nature of agent and tool adaptation, arguing that the most effective agentic systems will strategically combine both paradigms to achieve robust, efficient, and generalizable performance across diverse tasks and environments."
        },
        {
            "title": "2 Background",
            "content": "In this section, we provide the background to facilitate better understanding of the concepts discussed throughout this survey. Specifically, we first introduce the fundamental components of Agentic AI Systems (2.1). We then discuss different forms of Adaptation (2.2), which enable agentic AI systems to better adjust their behaviors and capabilities to specific tasks or application scenarios."
        },
        {
            "title": "2.1 Agentic AI Systems",
            "content": "Agentic AI systems refer to autonomous artificial intelligence systems capable of perceiving, reasoning, acting, and continuously improving through interaction with their environment. Such systems are designed to perform complex, open-ended tasks that require adaptive decision-making, contextual understanding, and iterative problem solving. In this survey, we primarily focus on single-agent systems, which provide controlled yet expressive framework to study how an individual agent perceives, plans, and acts within an environment. Single-agent settings serve as the foundational building blocks of more complex multi-agent systems, in which multiple agents coordinate, cooperate, or compete to achieve shared or opposing goals. Comprehensive overviews of AI agent architectures and their extensions to multi-agent scenarios can be found in recent surveys such as [1, 2, 30]. At the core of an agentic AI system lies foundation model, typically implemented as large language model (LLM) or multimodal model that functions as the agents reasoning and control center. This foundation model provides the fundamental abilities for understanding, reasoning, planning, and interaction. Complementing this core are several additional components that extend the agents autonomy and enable it to operate effectively in complex and dynamic environments: Planning Module: Decomposes complex goals into actionable steps and organizes their sequential or hierarchical execution. Depending on the degree of feedback integration, planning can be conducted in two main modes. Static planning methods, such as Chain-of-Thought [31] and Tree-of-Thought [32], enable structured reasoning through single-path or multi-path task decomposition. In contrast, dynamic planning approaches, such as ReAct [33] and Reflexion [34], incorporate feedback from the environment or past actions, allowing the agent to iteratively refine its plans and improve performance in long-horizon or partially observable scenarios. Tool Use: Enables the agent to interact with external resources and computational systems, extending its capabilities beyond the limitations of its internal knowledge. Typical tools include web search engines, APIs, code execution environments, Model Context Protocols (MCPs), and browser automation frameworks [35, 3]. Effective tool use involves selecting appropriate tools, constructing task-specific inputs, invoking external functions, and integrating their outputs into the agents reasoning and decision-making process, thereby enhancing performance in real-world and computationally intensive scenarios."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Memory Module: Allows the agent to retain, retrieve, and utilize past information for context-aware reasoning and long-term consistency. Memory is typically divided into short-term memory, which stores contextual information generated during the current task, and long-term memory, which persists across sessions to accumulate reusable knowledge and experience [1, 36]. To access relevant information from long-term memory, many systems employ retrieval-augmented generation (RAG) mechanisms that retrieve and integrate stored knowledge into the agents reasoning process. Designing an effective memory module involves challenges such as how to structure stored information, when and what to retain, how to retrieve relevant knowledge efficiently, and how to seamlessly integrate it into ongoing reasoning and decision-making."
        },
        {
            "title": "2.2 Adaptation",
            "content": "Adaptation is crucial aspect of agentic AI systems, enabling them to operate effectively across diverse and complex tasks. It allows an agent to adjust its behaviors, decision strategies, and internal representations to better align with the requirements of specific domain, task, or operational environment. Without such adaptive mechanisms, agents may struggle to generalize beyond their initial design or handle dynamic, real-world conditions. These mechanisms can be broadly categorized into prompt-based adaptation (2.2.1) and fine-tuning-based adaptation (2.2.2)."
        },
        {
            "title": "2.2.1 Prompt Engineering",
            "content": "Prompt engineering serves as lightweight form of adaptation that guides the behavior of an agentic AI system without modifying its underlying model parameters. Instead of retraining the core model, the agents behavior is shaped by carefully crafted input prompts that define goals, constraints, and contextual instructions. Through prompt design, an agent can be steered toward specific reasoning patterns, task formulations, or action strategies, enabling rapid adaptation across diverse tasks and environments. prompt refers to the input context provided to the agents core model, typically consisting of instructions, examples, or task descriptions that specify the desired behavior. By modifying or composing prompts, an agent can be adapted to new goals or environments without any additional model training, making this approach highly efficient and easily transferable across tasks. Such prompt-based adaptation has been widely adopted in recent agentic systems, such as CAMEL [37], AutoGen [38], MetaGen [39] and ChatDev [40]. For comprehensive overview of prompt engineering techniques and their design principles, we refer readers to the survey by Sahoo et al. [41]."
        },
        {
            "title": "2.2.2 Fine-Tuning",
            "content": "In contrast to prompt engineering, fine-tuning achieves adaptation by updating the internal parameters of the core model. Through exposure to task-specific data, fine-tuning enables the model to internalize new knowledge, reasoning patterns, or behavioral tendencies that better align with the target domain or task objectives. Fine-tuning can be performed at different granularities depending on data availability, computational cost, and the desired degree of adaptation. Full fine-tuning updates all model parameters using labeled data, providing maximal flexibility but often requiring substantial resources. Alternatively, parameter-efficient fine-tuning (PEFT) methods, such as low-rank adaptation (LoRA) [19], update only small subset of parameters. These approaches offer practical balance between efficiency and performance, enabling large agentic systems to be specialized for particular tasks or environments without extensive retraining. For comprehensive overview of PEFT methods, we refer readers to the survey by Han et al. [42]. Fine-tuning for adapting agents encompasses several major training paradigms. Supervised Fine-Tuning (SFT) [43] performs imitation learning on curated demonstrations. Preference-based methods, such as Direct Preference Optimization (DPO) [44] and its extensions [45], align the model with human or automated preference signals. Reinforcement-learning-based approaches, including algorithms such as Proximal Policy Optimization (PPO) [46] and Group Relative Policy Optimization (GRPO) [47], further adapt agents by optimizing their behavior 1While memory is fundamental component of an agent, this survey classifies adaptive memory systems under the Tool Adaptation paradigm (specifically T2, discussed in 5.2.3). We frame them as external, optimizable tools, such as retrievers or reflective databases, that are \"tuned\" using the frozen agents outputs as supervision."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Figure 3 Illustration of Four Adaptation Paradigms (A1, A2, T1, and T2). In all the panels, letters highlighted in Red denote the components directly being optimized during adaptation. The red arrows show the sources of adaptation signals. The dotted black lines separate the cases of supervised fine-tuning (SFT) and reinforcement learning (RL). through interaction with evaluative environments. These families of methods form the core techniques for adapting foundation-model-based agents to specialized tasks and deployment settings. For more comprehensive review of these approaches, see the survey by Zhang et al. [48]. We next introduce general framework that categorizes existing agentic AI adaptation approaches. This framework, presented in the following section, forms the conceptual foundation for the rest of this survey."
        },
        {
            "title": "3 Overview of Adaptation Paradigms of Agentic AI",
            "content": "In this section, we provide an overview of the adaptation paradigms that form the analytical basis of this paper. Our objective is to establish unified framework for categorizing existing studies on agentic AI systems according to what is adapted (the agent or the tool) and how the adaptation signal is obtained. We summarize these perspectives into four canonical paradigms, which together capture the major directions of adaptation explored in recent literature. To facilitate clear understanding of these paradigms, this section proceeds in three parts. We first introduce the"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "mathematical notations that are used throughout this paper (3.1). We then provide formal expressions for the four paradigms (3.2). Finally, we present illustrative examples that help clarify how each paradigm operates and how they differ in adaptation mechanisms (3.3)."
        },
        {
            "title": "3.1 Mathematical Notations",
            "content": "To ensure consistency in the subsequent formalization, here, we introduce the key mathematical notations used throughout this paper. We organize the notations into three conceptual categories that together define the adaptation process of agentic AI systems: the adaptation targets, the adaptation data sources, and the adaptation objectives. Adaptation Targets. This category specifies the entities that undergo adaptation within an agentic AI system. Agent (A): The foundation model that serves as the core reasoning and decision-making component of the system, parameterized by θ. Adaptation of the agent can occur through parameter updates, prompt refinement, or other modifications to its internal policy. Tool (T ): The set of external callable components that extend the agents capabilities beyond its internal parameters. Tools can include retrievers, planners, executors, simulators, or other computational modules. In this paper, we also categorize the memory module within , since memory can be viewed as dynamic and updatable database that interacts with and learns from the agents outputs. Typically, the retrieval process for accessing stored information is performed through dedicated retriever or search tool, which allows the agent to query and integrate relevant past knowledge into its reasoning process. Adaptation Data Sources. This category describes the sources from which the adaptation signals are obtained. Offline Data (D): Offline data that serve as alignment references or supervision sources for improving either the agent or the tool. These data may include human-labeled demonstrations, synthetic trajectories, or logs of prior interactions. Environment (E ): The external environment in which the agent or tool interacts and receives feedback. It provides online experience signals that reflect task performance or execution quality. Adaptation Objectives. Having defined the adaptation targets and data sources, we next describe the objective that guides the adaptation process, which quantifies performance or alignment quality. Objective Function O(): The objective function optimized during adaptation, which evaluates how effectively the agenttool system performs according to the designated evaluation protocol. For example, the objective for offline data may correspond to supervised or imitation learning losses such as supervised fine-tuning (SFT) or behavior cloning. When adaptation relies on interactions with the environment , the objective is typically defined by outcome-based metrics such as task success rate. These notations provide unified foundation for expressing how adaptation operates at both the agent and tool levels, which we formalize in the subsequent subsection."
        },
        {
            "title": "3.2 Four Adaptation Paradigms of Agentic AI",
            "content": "Building upon the mathematical notations introduced earlier, we now present the four adaptation paradigms proposed in this paper, which together form unified framework for classifying existing approaches to agentic AI adaptation. In this framework, adaptation is first categorized by the optimization target, namely the agent or the tool. For agent adaptation, we further differentiate paradigms based on the type of optimization signal used, which may originate from tool-execution feedback (A1) or from evaluations of the agents own final output (A2). For tool adaptation, the distinction instead concerns whether the adaptation process involves the agent, where tools may be optimized independently of any agent (T1) or adapted under the supervision of fixed agent (T2). Taken together, these considerations give rise to four paradigms, A1, A2, T1, and T2, which collectively characterize the principal modes of adaptation explored in agentic AI research."
        },
        {
            "title": "3.2.1 A1: Tool Execution Signaled Agent Adaptation",
            "content": "This paradigm focuses on improving the agent through feedback signals derived from the execution results of external tools . It captures scenarios where the agent interacts with tools in verifiable manner, allowing the tool outcomes to serve as measurable basis for optimization. AgentTool Interaction Process. The agent receives an input (e.g., user query or task description) and generates structured tool call or action = A(x), which may include the tool name, arguments, and calling context. The tool set then executes this call to produce result = (a). The pair (a, y) represents single agenttool interaction, and the overall process can be summarized as T y. This pipeline captures how the agent leverages tools to complete tasks. For simplicity and without loss of generality, we describe the interaction using single tool invocation; multi-turn tool use follows as direct extension of the formulation. Optimization Objective. Given this interaction process, the general optimization goal is to adjust the agent to generate high-quality tool call action such that the tool-executed outcomes achieve better performance. Formally, (A1) = arg max Otool(A, ), (1) where Otool measures the quality or correctness of the outputs obtained from invoking , such as tool execution success rate or retrieval scores. This optimization can be instantiated in two primary forms: (1) by imitating collected successful tool-call trajectories, or (2) by generating actions interactively and using the resulting tool feedback to optimize via Reinforcement Learning. Supervised Fine-Tuning (SFT). When explicit target actions are available, the agent learns to imitate successful tool-using behaviors from recorded trajectories without performing online interaction. Let Dsucc = {(x, a)} denote dataset of input and reference action that is known to lead to correct or desirable tool outcome (y). The supervised objective is formulated as: = arg min (cid:2)ℓ(A(x), a)(cid:3) arg max (cid:2) log pA(ax)(cid:3), E(x,a)Dsucc E(x,a) (2) where ℓ denotes the cross-entropy loss used for next-token prediction in language models. Reinforcement Learning (RL). Alternatively, the agent can acquire adaptation signals through interactions with the environment, where it executes tool calls and receives evaluative feedback from the resulting outcomes. The process follows: T y, with reward = Otool(y). Here, the agent generates an action or tool call based on input x, the tool executes to produce result y, and the evaluation function Otool assigns scalar feedback indicating task success or quality. The optimization objective can be expressed as: J(A) = xD0, aA(x), y=T (a)[Otool(y)], (3) where D0 denotes the input distribution."
        },
        {
            "title": "3.2.2 A2: Agent Output Signaled Agent Adaptation",
            "content": "Unlike the A1 paradigm, where the adaptation signal is derived from tool-execution outcomes, the A2 paradigm obtains its optimization signal from the agents own final output. For simplicity and without loss of generality, the following description focuses on single-turn interaction; multi-turn tool use, where the agent invokes multiple tool calls and integrates multiple intermediate results, extends naturally from the formulation."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "AgentTool Interaction Process. In the A2 paradigm, the agent first generates tool call from the input x, the tool executes this call and returns an executed result y, and the agent then integrates and to produce the final output o: where = A(x, a, y). This formulation naturally includes the special case where the agent produces directly without calling any tools. o, x Optimization Objective. The goal of A2 adaptation is to optimize the agent such that its final output aligns with correctness, quality, or alignment criteria. Formally: (A2) = arg max Oagent(A, ), (4) where Oagent evaluates the final output generated by the agent. Similarly, A2 paradigm optimization also includes two main forms: Supervised Fine-Tuning (SFT). Let Dans = {(x, y, a, o)} denote dataset consisting of the input x, optional intermediate tool outputs y, the reference tool call a, and the corresponding target final output o. key characteristic of the A2 paradigm is that its adaptation signal comes solely from the final agent output. However, supervising only the final output o, i.e., optimizing = arg min E(x,y,a,o)Dans (cid:104) ℓ(cid:0)A(x, a, y), o(cid:1)(cid:105) arg max (cid:104) E(x,y,a,o) log pA(ox, a, y) (cid:105) , (5) is insufficient for learning tool-use behavior: the agent could improve its final-answer likelihood without ever invoking tools, since the supervision provides no incentive to produce the correct tool call. Therefore, for A2-style SFT to effectively support tool-using agents, it must combine final-output supervision with tool-call supervision, effectively integrating A2-style supervision with the A1-style imitation of tool-use trajectories. The supervised objective then becomes: = arg min ℓ(cid:0)A(x), a(cid:1) + ℓ(cid:0)A(x, a, y), o(cid:1)(cid:105) E(x,y,a,o)Dans (cid:104) , which is equivalently written as: arg max (cid:104) E(x,y,a,o) log pA(ax) + log pA(ox, a, y) (cid:105) . (6) Here, ℓ denotes the cross-entropy loss used in next-token prediction for LLMs. The first term teaches the agent to make correct tool calls (A1-style imitation), while the second term supervises the agents final answer generation (A2-style imitation). This formulation naturally includes the special case where no tools are invoked, in which case and are empty. Reinforcement Learning (RL). When explicit target outputs are unavailable, the agent learns from feedback assigned to its final response. The interaction follows: The optimization objective becomes: y o, with reward = Oagent(o). xD0, aA(x), y=T (a), o=A(x,a,y) where D0 is the distribution of task inputs. Here, the agent receives rewards based solely on the quality of its final output, irrespective of how many intermediate tool calls were invoked. J(A) = (cid:2)Oagent(o)(cid:3),"
        },
        {
            "title": "3.2.3 T1: Agent-Agnostic Tool Adaptation",
            "content": "In the T1 paradigm, the agent is kept fixed, and adaptation is applied to only the external tool set . This setting arises naturally when the agent is powerful and robust closed-source API (such as GPT, Claude, or Gemini) that usually cannot be fine-tuned, or when the goal is to enhance the fixed agent by training specialized tools to complement the frozen agent, such as retrievers, rerankers, planners, simulators, or additional foundation models. In this sense, tool in T1 primarily refers to trainable model, regardless of whether it is traditional machine-learning model or large-scale foundation model."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Optimization Objective. The goal of T1 is to optimize the tool in an agent-agnostic manner: (T1) = arg max Otool(T ), where Otool(T ) evaluates the quality of tool-produced results, often through metrics such as retrieval accuracy, ranking quality, simulation fidelity, or downstream task success. Since the agent is fixed and only is trainable, T1 reduces to standard model training under various learning paradigms, such as supervised learning, contrastive learning, or reinforcement learning."
        },
        {
            "title": "3.2.4 T2: Agent-Supervised Tool Adaptation",
            "content": "In the T2 paradigm, tool adaptation is guided by the frozen agent A. Unlike T1, where tools are trained independently of any agent, T2 explicitly aims to adapt or construct tool that complements the fixed agent and enhances its overall capability. This setting reflects practical motivation: when the main agent is powerful closed-source foundation model, it is often preferable to train auxiliary tools around it rather than modifying the agent itself. AgentTool Interaction Process. Without loss of generality, we describe the interaction using single-turn example, noting that multi-turn processes can be extended naturally from this. The agent receives input and produces tool call = A(x). The tool executes this call to return result = (a), and the agent integrates (x, a, y) or (x, y) to produce the final output o: T o. Optimization Objective. The tool is optimized to improve the performance of the fixed agenttool system: where Oagent evaluates how effectively the agent performs when equipped with tool . This objective emphasizes that T2 adapts the tool specifically to the needs of the given agent. Tool adaptation in T2 generally takes two forms: (T2) = arg max Oagent(A, ), Supervised Learning. In the supervised setting, the frozen agent provides signals that indicate how the tool should improve. The core idea is to adjust the tool so that its future outputs (a) become more helpful for the agents downstream reasoning. This can be instantiated in several ways. For example: Quality-Weighted Training. The agents final output induces quality score = ω(o) that reflects the desirability or correctness of the agents behavior. The tool is trained by weighting each trajectory according to this score: = arg min (cid:104) E(a,y,o) w(o) ℓ(cid:0)T (a), y(cid:1)(cid:105) , where ℓ is task-specific loss encouraging the tools output to improve. If w(o) takes binary values {0, 1}, this reduces to data-selection scheme where only trajectories associated with desirable agent outputs are used to train the tool. Output-Consistency Training. The agents final output induces an implicit supervision target τ = ϕ(a, y, o), which prescribes how the tool output should change to better support the agent. The tool is updated by: = arg min (cid:104) E(a,y,o) ℓ(cid:0)T (a), τ(cid:1)(cid:105) , where the mapping ϕ(a, y, o) extracts learning target from the relationship between and o. This encourages the tool to produce outputs that more effectively align with the agents downstream reasoning. Reinforcement Learning (RL). The tool is updated using scalar reward based on the final quality of the agents output. Let = Oagent(o) denote the reward assigned to the final output = A(x, a, y). The RL objective becomes: J(T ) = xD0, a=A(x), y=T (a), o=A(x,a,y) (cid:2)Oagent(o)(cid:3), where D0 is the distribution of task inputs."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Memory as Special Case of T2. In this paper, the memory module storing long-term memory is treated as tool within the T2 paradigm. Although memory serves conceptual role distinct from conventional executable tools, its update mechanism aligns precisely with the agent-driven tool adaptation view. During interaction, the frozen agent produces final output that reflects its reasoning over both the input and retrieved memory contents. This final output is then used to update the memory module through fixed or learnable write function: Update(M, o), where denotes the memory store. This process corresponds directly to T2: the agent remains fixed, the adaptation signal originates from the agents own output, and the memory module is optimized to better support future agent reasoning. Thus, adaptive memory systems naturally fall under the T2 paradigm: the tool being optimized is the memory module, and the supervision signal arises entirely from the behavior of fixed agent interacting with and benefiting from that memory. 3."
        },
        {
            "title": "Illustrative Examples",
            "content": "To make the above adaptation paradigms more concrete, this section provides illustrative examples drawn from two representative application settings: retrieval-augmented generation (RAG) and code-execution-based tasks. These two settings are chosen because they highlight the central role of tool use in agentic AI systems, while exhibiting distinct toolagent interaction patterns and evaluation protocols. For each application, we present pair of examples that correspond to the A1 and A2 paradigms. The paired examples share the same form of tool-call action, i.e., document retrieval or executing code, respectively. This allows us to clearly contrast tool-feedbackbased agent adaptation (A1) with agent-outputbased agent adaptation (A2). Finally, we provide T2 example in the RAG setting. By examining these examples side by side, readers can develop an intuitive understanding of how these paradigms differ in objectives, update signals, and learning dynamics."
        },
        {
            "title": "3.3.1 Agent Adaptation Examples Across Two Applications",
            "content": "We now illustrate the A1 and A2 paradigms through two representative tool-use settings: retrieval-augmented generation (RAG) and code-execution-based question answering. For each application, we begin by describing the underlying problem setup, followed by pair of examples that instantiate A1 and A2 under the same form of tool-call action. This allows clean comparison between adaptation driven by tool-execution feedback (A1) and adaptation driven by agent final outputs (A2). Retrieval-Augmented Generation (RAG) Setting. In the RAG setting, the agent receives query and performs retrieval action to obtain relevant documents from database. Formally, the agent produces retrieval query a, the retriever returns set of documents y, and the agent synthesizes these documents together with the original query to generate final answer o. A1 example. DeepRetrieval [21] optimizes the agent using feedback signals computed directly from retrieval quality. After generating retrieval query a, the retriever returns documents y, and metrics such as recall or nDCG are computed from and used as the reward for updating the agent. Since the adaptation signal depends solely on the tool-execution outcome, this represents the A1 paradigm. A2 example. Search-R1 [49] follows the full RAG pipeline, where the agent first retrieves documents and then integrates them into its context to produce final answer o. The adaptation signal is computed from the correctness or quality of this final answer by calculating exact matching accuracy. Because the optimization is guided by the agents final output rather than the retrieval result alone, this falls under the A2 paradigm. Code-Execution-Based Task Setting. In code-execution-based tasks, the agent receives problem description and produces executable code as the tool-call action. The sandbox executes the code and returns an execution result y, which the agent may optionally use to generate final answer o."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "A1 example: DeepSeek-R1 (code) [24]. During reinforcement learning, DeepSeek-R1 generates code that is executed inside sandbox. The execution output, such as test-case pass rate or numerical correctness, is used directly as the reward for policy optimization. Since adaptation is based entirely on the tools execution result, this example fits the A1 paradigm. A2 example: ReTool [50] also generates executable code, but the sandbox result is fed back into the agent as additional context. The agent then produces final answer o, whose correctness determines the reward. Because the adaptation signal depends only on the final output of the agent after integrating tool feedback, this corresponds to the A2 paradigm."
        },
        {
            "title": "3.3.2 Tool Adaptation Examples in the RAG Setting",
            "content": "In many practical systems, the central agent is instantiated as powerful closed-source API model (such as GPT-, Claude-, or Gemini-style models) that already exhibits strong performance and robustness across wide range of tasks. Training from an open-source model to match this level of robustness is extremely challenging: the data quality must be carefully curated, scaling laws suggest that much larger models and much more data are required for competitive performance, and such models demand substantial training infrastructure. As result, convenient and often more feasible strategy is to treat the closed-source API model as fixed agent and instead adapt auxiliary tools around it. In the RAG setting, this motivates tool adaptation for components such as retrievers, which can be optimized to complement the fixed agent. (1) Classic Dense Retrievers. Under the T1 paradigm, tools are trained independently of any T1 examples. specific agent and can be plugged into frozen LLM agents without further co-adaptation. canonical example is standard dense retriever, such as bi-encoder trained with contrastive learning to map queries and documents into shared embedding space for vector similarity search. Once trained, such retriever can be used as standalone retrieval tool: given tool call action (a retrieval query), the dense retriever returns ranked document set = (a) optimized for recall or semantic relevance. closed-source agent (e.g., GPT-, Claude-, or Geministyle) can then consume to perform downstream reasoning and answer synthesis, despite the agent itself never participating in the retrievers training. (2) Learned Subagents as Agent-Agnostic Tools. Beyond classic dense retrievers, once agent adaptation has produced strong retrieval-oriented models, these learned models can themselves be reused as tools under the T1 paradigm. For example, model trained in the DeepRetrieval [21] style can be deployed purely as high-quality subagent that rewrites queries for improved retrieval over specific document databases. Given tool call action (a retrieval query), the subagent returns reformulated query or curated document set = (a) with enhanced recall or relevance, which is then consumed by fixed closed-source agent that performs the final reasoning and answer synthesis. T2 examples: Under the T2 paradigm, the tool is adapted using supervision signals derived directly from fixed agents final outputs. In the RAG setting, both S3 [27] and AgentFlow [51] provide representative examples. The tool (a learnable search subagent) is updated based on the fixed agents output signal so that its behavior becomes increasingly aligned with what the fixed agent needs for successful downstream reasoning. Concretely, in S3 [27], given question x, the learnable subagent takes as input and internally generates retrieval query = (x). This query is executed on static search engine to retrieve document set y, which is then inserted into the frozen agents context. The fixed agent consumes (x, y) and produces final answer = A(x, y). An evaluation function Oagent(o) (e.g., answer correctness) assigns scalar reward, which is then used to update the tool so that its future retrieval behaviors yield document sets that more effectively support the agents downstream reasoning. Thus, S3 directly realizes the T2 objective with the agent-output signaled tool adaptation specifically for the fixed agents downstream performance. AgentFlow [51] further extends this idea by training more expressive planning-oriented subagent capable of multi-tool decision-making, applying T2 supervision signal to align richer planning policy with the fixed agents final-output preferences. With these illustrative examples in place, we next move on to systematically review the literature associated with each of the four adaptation paradigms."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Figure 4 Development timeline of A1 methods (agent adaptation with tool-execution result as signal)."
        },
        {
            "title": "4 Agent Adaptation",
            "content": "Agent adaptation refers to the mechanisms through which agents refine their behavior and decision-making capabilities based on feedback from their interactions with tools, environments, or their own outputs. This process is pivotal for enhancing the autonomy, reasoning, and generalization abilities of agents across diverse tasks. Broadly, agent adaptation can be categorized into two paradigms: A1, which leverages tool execution results as feedback signals, and A2, which focuses on evaluating the agents own outputs. Formally, let denote an agent, parameterized by its internal configuration or policy (which includes prompt templates or model weights), and let represent the set of tools accessible to the agent. The agents performance under given configuration is evaluated by an objective function O(), which provides feedback based on either tool performance or agent output quality. Accordingly, the two adaptation paradigms can be formalized as optimization objectives: (A1) = arg max Otool(A, ), (A2) = arg max Oagent(A, ), where Otool quantifies the correctness or utility of outcomes derived from tool execution, such as successful code compilation or retrieval precision, and Oagent measures the quality of the agents generated outputs, including reasoning validity, factual accuracy, or alignment with human preferences. Here, denotes the optimized agent configuration that maximizes the corresponding feedback objective."
        },
        {
            "title": "4.1 A1: Tool Execution Result as Signal",
            "content": "Tool execution result as signal refers to class of adaptation mechanisms where an agent leverages the actual outcomes of external tool invocations (such as execution correctness, functional success, or numerical improvement)"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "as feedback to refine its behavior. Here, the tool or environment serves as an objective source of feedback, forming verifiable signal that can drive both supervised and reinforcement-based learning. Table 1 A1 Methods (Tool Execution Signaled): Earlier Methods (SFT & DPO) and Recent RLVR-based Methods Time Method Venue Task(s) Tool(s) Agent Backbone Tuning Links SFT & Off-Policy Methods GPT-J"
        },
        {
            "title": "SFT",
            "content": "P 2023."
        },
        {
            "title": "Toolformer",
            "content": "NeurIPS23 QA, Math 2023."
        },
        {
            "title": "TRICE",
            "content": "NAACL24 Math Reasoning, QA"
        },
        {
            "title": "2023.05 Gorilla",
            "content": "2023.06 ToolAlpaca 2023.07 ToolLLM 2024.01 NExT 2024.02 2024.02 2024.03 CodeAct RetPO CYCLE arXiv ICLR24 NeurIPS24 Tool-Calling, API Retrieval Multi-Turn Tool-Use Tool-Calling, API Planning, Multi-Tool Reasoning Program Repair Coding ICML24 ICML24 NAACL25 IR OOPSLA24 Coding Calculator, QA system, Search Engine, Translation System, Calendar Calculator, WikiSearch, Atlas QA Model, NLLB Translator APIs Code Executor Code Executor Retriever Code Executor 2024.05 AutoTools WWW25 Tool-Calling APIs 2024.06 2024.10 2024.10 TP-LLaMA ToolFlow LeReT NeurIPS24 Tool-Calling NAACL25 Tool-Calling ICLR25 IR APIs APIs Dense Retriever RLVR Methods 2024.05 LeDex NeurIPS24 Coding Code Executor 2024.08 DeepSeekProver-V1.5 RLEF 2024.10 2025.01 DeepSeekICLR ICML25 Nature Formal Theorem Proving Coding Coding Lean 4 Prover Code Executor Code Executor R1-Zero (Code) 2025.02 DeepRetrieval COLM25 Web Search, IR, Text2SQL Search Engine, Retrievers, SQL exec. Code Executor 2025.03 2025.03 Code-R1 ReZero arXiv Coding Web Search, IR Web Search 2025.03 Rec-R1 TMLR25 Recommendation Optimization 2025.04 SQL-R NeurIPS25 Text2SQL Search Engine Recommendation System SQL Engine 14 ChatGLM, Alpaca, Vicuna SFT, Contrastive Learning"
        },
        {
            "title": "LLaMA",
            "content": "Simulated APIs Vicuna"
        },
        {
            "title": "SFT",
            "content": "SFT Real-World APIs LLaMA, Vicuna SFT P P PaLM2 LLaMA2, Mistral LLaMA2-7B CodeGen, StarCoder GPT4, LLaMA3, Mistral LLaMA2 LLaMA3.1 LLaMA3, Gemma SFT SFT SFT, DPO SFT SFT SFT, DPO SFT DPO-like (IPO) StarCoder & CodeLlaMA DeepSeekProver-V1.5-RL LLaMA3.1 DeepSeek-V3Base Qwen2.5, LLaMA3.2 Qwen2.5 LLaMA3. Qwen2.5, LLaMA3.2 Qwen2.5, OmniSQL SFT, PPO SFT, GRPO PPO GRPO PPO, GRPO GRPO GRPO GRPO SFT, GRPO Continued on next page"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 1 Continued from previous page Task(s) Tool(s) Agent Backbone Tuning Links Time Method"
        },
        {
            "title": "Prover",
            "content": "2025.05 2025."
        },
        {
            "title": "2025.04 DeepSeek-\nProver-V2\nTool-N1\nR1-Code-\nInterpreter\nRouter-R1",
            "content": "2025.06 Venue arXiv arXiv arXiv arXiv"
        },
        {
            "title": "Formal Theorem\nProving",
            "content": "Formal Theorem Proving Tool-Calling Coding NeurIPS25 Multi-Round 2025.07 LeanabellProver-V"
        },
        {
            "title": "2025.08 Goedel-",
            "content": "2025.08 Prover-V2 FTRL 2025.09 Tool-R1 2025.09 WebGenAgent arXiv arXiv arXiv arXiv arXiv 2025.10 2025.10 AlphaProof ToolExpander arXiv Nature Routing Formal Theorem Proving Formal Theorem Proving Multi-Step Tool-Use Tool-Augmented Reasoning, QA Website Generation Tool-Calling Formal Theorem Proving 2025.10 olmOCR arXiv Document OCR 2025.11 Orion arXiv IR"
        },
        {
            "title": "4.1.1 Earlier Works: SFT & Off-Policy Methods",
            "content": "Qwen2.5 Lean 4 Compiler, Numina Lean Server Lean 4 Compiler DeepSeek-V3 Tool APIs Code Execution Sandbox LLM Routing Pool Lean 4 Verifier"
        },
        {
            "title": "Lean Compiler",
            "content": "Qwen2.5 Qwen2.5 Qwen2.5, LLaMA3.2 Kimina, DeepSeek-V2 Qwen"
        },
        {
            "title": "Simulated APIs",
            "content": "Qwen3 SFT, GHPO SFT, GRPO"
        },
        {
            "title": "PPO",
            "content": "P SFT, DAPO SFT, GRPO Code Execution, Multimedia Tools VLM, GUI Agent, Code Executor Tool APIs Lean Solver Synthetic Document Verifier Retrievers Qwen2."
        },
        {
            "title": "GRPO",
            "content": "GRPO Qwen2.5-Code, Qwen3 SFT, Step-GRPO Qwen2.5 Transformer (3B Enc-Dec) Qwen2.5-VL SFT, GRPO SFT, AlphaZero, TTRL SFT, GRPO LFM2 GRPO Early A1-type methods typically focus on SFT or DPO, which aim to teach agents using pre-collected data. These methods begin by collecting set of model responses or trajectories involving tool usage, and then use this data for supervised fine-tuning or DPO. These A1 methods share common foundation in leveraging objective environment-grounded outcomes, but differ in the form, source, and utilization of their feedback signals. The evolution of these imitation-based A1-type methods primarily centers on the transformation in how training signal is obtained and utilized. The earliest representative, Toolformer [4] (NeurIPS 2023), introduced the idea of using tool outcomes as selfsupervised learning signals. The model automatically inserts candidate API calls into text, executes them, and measures whether the returned result improves token prediction likelihood. call is retained if it significantly reduces perplexity, formalized as τf , where the reduction quantifies self-supervised tool execution result signal. This implicit signal anchors learning in the correctness of tool usage, enabling the model to autonomously discover when external APIs improve performance. L+ However, since the training is based on self-supervised feedback, Toolformer remains limited in precision when applied to real executable environments. This limitation motivated series of subsequent approaches that sought to introduce more reliable, externally grounded learning signals. Building upon this insight, the evolution of A1-type methods can be viewed through three progressively grounded paradigms of learning: Alignment with golden answers: supervision comes from correct responses or expert trajectories. Alignment with golden formats: correctness is defined structurally or syntactically."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Alignment with direct tool execution: learn from verifiable outcomes produced by executing tools, allowing supervision to emerge from actual tool behavior rather than predefined labels. Alignment with golden answers. Early A1-type approaches focused on aligning models with correct final outputs, typically defined by task-specific ground truths or verified expert solutions. TRICE [52] (NAACL 2024) is two-stage framework designed to teach LLMs when and how to use tools selectively. The first stage uses supervised fine-tuning to provide the model with preliminary ability to imitate tool-use behavior. The core of the method lies in the second stage, Reinforcement Learning with Execution Feedback (RLEF). In this stage, the agent is trained using reward signal derived directly from tool execution. The system collects set of candidate responses for given task, some of which involve tool calls. reward strategy then scores each response by comparing its execution result against the ground-truth answer. The model is then reinforced, using ranking loss, to align its preferences with the high-reward responses, effectively learning from the successful or failed outcomes of tool execution to mitigate excessive reliance on tools and improve accuracy. This design provides clear instance of learning grounded in correctness alignment, where rewards are explicitly tied to whether the tool-executed outcome matches the golden answer. ToolAlpaca [53] represents one of the earliest closed-loop implementations of A1-type adaptation, where the model refines its tool-use capability directly through iterative interaction with executable environments. The model first generates tool-call candidate, for example an API invocation, which is then executed within the environment. The system records the runtime outcome of each execution, including returned values, errors, or completion states, to determine the correctness of the models action. This design establishes an automated self-improvement loop consisting of four key stages: generate, execute, evaluate, and finetune. Through repeated cycles, ToolAlpaca progressively aligns its internal representation with the actual semantics and behavior of the tools it uses. This closed-loop process embodies the essence of the A1 paradigm, where tool execution results themselves serve as the primary adaptation signal. ToolAlpaca demonstrates the ability to generalize across unseen tools and to adapt its calling strategy according to contextual requirements. By grounding updates in correctness relative to observed outcomes, it implicitly aligns its learning with the golden answer paradigm. TP-LLaMA [54] (NeurIPS 2024) is an inference trajectory optimization framework designed to improve toolaugmented LLMs by learning from errors. The authors observe that prior models like ToolLLaMA [10] are trained via SFT exclusively on successful expert trajectories from the ToolBench dataset, which ignores valuable information contained in failed exploration paths of the decision trees. To address this, their method consists of two stages: first, the model undergoes standard SFT on successful trajectories, similar to the baseline. Second, the framework leverages the \"failed paths\" by constructing novel preference dataset called ToolPreference. This dataset is built using step-wise method: for any decision node along successful path, the \"preferred\" output (yw) is the experts correct next step, while the dispreferred output (yl) is any corresponding failed branch originating from that same node. The model is then trained on these preference pairs using DPO. This approach explicitly uses the execution feedback from failed attempts as training signal, enabling the model to learn from failure and significantly enhancing its decision-making, generalization, and reasoning efficiency. TP-LLaMA therefore transforms failure signals into preference-aligned supervision, reinforcing learning according to correctness with respect to expert trajectories. Alignment with golden formats. complementary branch of A1-type evolution shifted from output-based correctness to format-based structural correctness, emphasizing syntactic and logical validity of tool calls rather than explicit task answers. Gorilla [55] (NeurIPS 2024) is retrieval-augmented LLaMA-based language model fine-tuned to generate correct API calls across large and changing set of machine learning APIs. During training, it leverages self-instructed instructionAPI pairs and, optionally, document retriever to adapt to test-time documentation changes. crucial component of Gorillas evaluation and feedback loop is the use of Abstract Syntax Trees (ASTs). Both the modelgenerated API calls and reference API calls are converted into ASTs, and correctness is determined by checking whether the reference API forms subtree of the generated AST. Compared to direct text matching, AST-based evaluation is more robust: differences in parameter order or optional arguments do not lead to false negatives, as ASTs focus on the logical structure of the API call. In the context of A1-type adaptation, this serves as form of"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "tool execution result signal, providing the model with structured feedback on whether its API call was functionally correct, which can then be used to guide learning and improve tool-use performance. Gorilla therefore exemplifies the golden-format paradigm, where correctness is defined by adherence to canonical structural representations rather than output values. ToolFlow [56] (NAACL 2025) is data synthesis pipeline designed to enhance the tool-calling capabilities of LLMs through the generation of natural and coherent dialogues. Traditional SFT approaches rely on synthetically generated tool-call data. However, previous methods often suffer from low diversity and limited coherence because tools are sampled randomly and dialogues are synthesized as single-turn interactions, ignoring multi-turn dependencies. ToolFlow addresses these limitations by introducing two key strategies: Graph-based Sampling and Planned Generation. The Graph-based Sampling strategy constructs tool graph based on parameter and return-value similarities between tools. Nodes represent tools, and edges indicate their relevance, allowing the selection of tool subsets that are likely to interact effectively. This facilitates the generation of complex user requirements involving multiple interconnected tools. The Planned Generation strategy enables the LLM to first create high-level dialogue plan that organizes user requests across multiple turns, including both tool-call tasks and non-tool interactions. This ensures logical consistency and natural flow throughout the dialogue, resulting in more realistic training data. Overall, ToolFlow provides systematic approach for generating high-quality multi-turn tool-call dialogues that closely reflect real-world interaction scenarios, and exemplifies structural alignment through graph-based format consistency. Alignment with direct tool execution. The most advanced stage of A1 evolution centers on learning directly from verifiable environment signals, where tool execution outcomes themselves become the supervision source. CodeAct [57] (ICML 2024) represented paradigm in which LLMs learn tool use through direct interaction with executable code environments. Instead of producing textual or JSON-based commands, the model generates executable code actions that are run within sandboxed environment. The environment returns explicit execution feedbacksuch as success or failure signals and resulting outputswhich are then used as supervision to refine the model. This process grounds the learning objective in the verifiable outcomes of tool execution, rather than in human-annotated correctness or model-predicted preferences. Through this execution-based feedback loop, CodeAct effectively aligns model behavior with the underlying causal mechanisms of tools. NExT [58] (ICML 2024) is method designed to teach LLMs to reason about code execution, specifically for program repair tasks. The approach utilizes an iterative self-training loop based on SampleFilterTrain process. The core of this method lies in its filtering step, which uses tools execution result as the primary training signal. An external tool, takes the agents generated code output and validates it by running set of unit tests. The binary pass-or-fail diagnostic from this execution serves as reward signal. Only the candidate solutions (both rationale and code) that successfully pass all unit tests are deemed correct and are collected into new synthetic dataset, which is then used to finetune the agent for the next iteration. This iterative refinement, guided strictly by the tools verification, progressively improves the models ability to generate accurate fixes and high-quality, execution-aware rationales. ToolLLM [10] and AutoTools [59] (WWW 2025) push this paradigm toward autonomous tool learning. Specifically, AutoTools consists of two core stages: tool encapsulation and tool programming. In the tool encapsulation stage, the LLM automatically parses raw API documentation and transforms each tool into callable function, including structured docstrings, argument specifications, and usage examples. Syntax and runtime correctness are verified through an integration verification procedure, which checks not only individual functions but also inputoutput dependencies among related tools. Verified functions are aggregated into function library for subsequent use. In the tool programming stage, the LLM directly generates executable programs that sequentially call these functions, resolve intermediate outputs, and ultimately solve user queries. This design allows the model to flexibly integrate multiple tools using unified programming language, instead of relying on specialized tokens or handcrafted formats. To further enhance model expertise, AutoTools introduces AutoTools-Learning, multi-task learning approach that trains the LLM on three synthetic tasks: (1) documentation understanding, (2) relevance learning for selecting appropriate tools, and (3) function learning for generating correct multi-step tool programs. The training dataset comprises 34k high-quality examples synthesized from public sources and existing benchmarks, providing"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "fine-grained supervision for both tool understanding and programmatic reasoning. Through this combination of automated tool encapsulation, programmatic integration, and multi-task training, AutoTools represents significant step towards fully autonomous A1-type adaptation. Execution outcomes of each tool call serve as direct feedback for iterative self-improvement, enabling LLMs to handle complex, multi-step tool-use tasks without explicit human intervention. LeReT [60] (ICLR 2025) is reinforcement learning framework for improving multi-hop retrieval in LLM pipelines. query-generating LLM πr produces diverse search queries using few-shot prompt ensembles, and retrieved documents are scored with reward function R. Queries are converted into preference pairs (yi, yj), where the higher-reward query is preferred. The model is then fine-tuned using preference-based reinforcement learning, specifically Identity Policy Optimization (IPO). IPO is method for directly optimizing model to reflect human or reward-based preferences. Instead of modeling the probability of preferred output as in DPO, IPO explicitly enforces that the models implicit reward difference between preferred and dispreferred outputs matches target margin. Formally, it minimizes the squared deviation of the reward difference from fixed margin: (cid:104) LIPO = E(x,yw,yl )Dp (rϕ(x, yw) rϕ(x, yl) 0.5τ1)2(cid:105) , πϕ(yx) where rϕ(x, y) = log πref(yx) , yw and yl denote the preferred and dispreferred queries, is the context, and τ is margin hyperparameter controlling the target difference between rewards. Intuitively, IPO encourages the model to assign higher scores to better outputs and ensures the difference reaches predefined magnitude, providing direct and stable training signal. LeReT can be applied iteratively, using the fine-tuned model to generate better exploration data in subsequent iterations. This method improves both retrieval accuracy and downstream generation quality, and can adapt to arbitrary off-the-shelf retrievers without modifying the generator model. As such, it represents the culmination of environment-grounded A1 learning, where reward optimization is explicitly tied to real-world, verifiable signals. Similarly, RetPO [61] (NAACL 2025) trains query reformulation model to translate LLM-generated queries into retriever-optimal queries. The training signal is ingenious in its simplicity: GPT-4 generates multiple candidate rewrites, each is evaluated by running it through an off-the-shelf retriever (e.g., BM25), and the retrieval performance (measured by how well the retrieved documents support the correct answer) serves as the reward. smaller, open-source LM is then trained via DPO to produce high-reward rewrites. Overall, the evolution of A1-type methods reflects gradual shift from implicit, self-supervised feedback to explicit, execution-grounded learning signals. Early methods such as Toolformer demonstrated the feasibility of using tool execution as self-supervised signal, but their reliance on internal likelihood metrics limited their external validity. Subsequent approaches strengthened the supervision source by grounding model optimization in correctness relative to golden answers or expert trajectories, allowing models to learn more directly from verified outcomes. The next wave of methods, such as Gorilla and ToolFlow, advanced this idea by emphasizing structural and syntactic alignment, ensuring that generated tool calls conform to canonical formats even under distributional shifts. Finally, environment-grounded methods fully integrated external feedback loops, enabling models to learn directly from verifiable execution signals and real-world rewards. This progression illustrates clear trend toward deeper coupling between model reasoning and environment interaction, where feedback transitions from probabilistic imitation to causally grounded supervision. Such grounding not only enhances tool reliability and adaptability but also marks key step toward autonomous, self-improving LLM agents capable of operating robustly in open, dynamic environments. Despite these advances, these methods rely on SFT or DPO, where model updates are derived from pre-collected trajectories or candidate responses. While effective for exploiting existing data, these approaches are inherently constrained in exploration and may fail to fully capture the dynamic nature of interactive environments."
        },
        {
            "title": "4.1.2 RLVR-Based Methods",
            "content": "Reinforcement learning with verifiable reward (RLVR) marks pivotal stage in the evolution of A1-type adaptation, where models learn directly from online interaction with tools and environments. Unlike SFT or DPO approaches that rely on pre-collected trajectories or candidate responses, RLVR-based methods enable LLMs to iteratively explore, execute, and refine their actions based on immediate environment feedback. This paradigm allows adaptation to be"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "dynamic, context-aware, and tightly coupled with the specific execution environment, spanning diverse domains such as web search, code generation, multi-tool reasoning, and downstream applications. Web search and information retrieval tools optimized query generation and retrieval using environment-derived rewards. DeepRetrieval [21] (COLM 2025) marked key turning point in A1-type adaptation, introducing RLVR to train LLMs as search agents that learn directly from retrieval outcomes. It formalizes query reformulation as an MDP where the user query is the state, the rewritten query is the action, and retrieval metricssuch as Recall@K, NDCG, or SQL execution accuracyserve as the reward. The policy is optimized via KL-regularized PPO: ˆπ = arg max π q,qπ (cid:2)r(q, q) β log π(qq) πref(qq) (cid:3), where r(q, q) = rretrieval(q, q) + rformat(q) jointly captures retrieval effectiveness and syntactic validity. This unified formulation enables the same framework to adapt seamlessly across literature search, QA-style retrieval, local corpus search with dense or BM25 retrievers, and text-to-SQL database querying. Empirically, DeepRetrieval achieved roughly threefold improvement in recall (65.1% vs. previous state-of-the-art 24.7%) on literature search tasks using real-world search engines, while maintaining impressive performance across other retrieval and SQL domains. These results established reinforcement learning on environment rewards as general, scalable, and cost-efficient paradigm for retrieval-based agent adaptation. ReZero [62], successor to DeepRetrieval, extends this idea with GRPO-based reinforcement learning that rewards adaptive retries after failed searches. By introducing retry-aware reward shaping, it improves agent persistence and robustness in dynamic or partially observable web environments, further validating the effectiveness of DeepRetrievals reinforcement-driven approach. Orion [63] further extends DeepRetrieval by moving from single-step reformulation to multi-turn adaptive search. Using GRPO with turn-level rewards based on normalized similarity and rank, Orion trains models to iteratively refine, pivot, or backtrack through structured think-search cycles. This yields strong multi-hop retrieval performance with compact 350M1.2B models, showing that effective multi-step search strategies can be learned without large controllers. Code-based tools provided deterministic or sandboxed execution environments for reasoning and task completion. LeDex [64] (NeurIPS 2024) applied reinforcement learning using PPO-based algorithm with novel reward function that considers both the correctness of the refined code (via unit test results and CodeBLEU) and the quality of the explanation (via semantic similarity). RLEF [20] (ICML 2025) framed code synthesis as multi-turn interactive task, where the LLM generates solution, receives automatic feedback from executing the code on public test cases, and updates its subsequent generations accordingly. The process is formalized as partially observable Markov Decision Process (MDP), with actions corresponding to token-level code generation and rewards determined by the success of private test cases, optimized using PPO. Code-R1 [65] built reliable, scalable, and sandboxed reward pipeline to minimize reward false positives caused by faulty tests, unsolvable prompts, or mismatched execution environments. The key finding is that reward quality matters more than data quantity clean, verified datasets and secure sandboxed execution are essential for effective code RL training. R1-Code-Interpreter [66] introduced general framework for training LLMs to effectively use Code Interpreter through multi-stage reinforcement learning. Unlike prior works limited to math or retrieval tasks, it identifies key challenge that task heterogeneity causes sparse and unstable rewards during RL. To address this, the authors propose multi-stage curriculum learning approach that prioritizes samples based on their improvement potential. Tool-R1 [67] proposed sample-efficient reinforcement learning framework that performs multi-step reasoning through executable Python code. It introduces dynamic sample queue to cache and reuse high-quality trajectories and employs outcome-driven rewards based on code execution success and LLM-judged correctness. Formal theorem proving [6876] has emerged as canonical domain for RLVR under the A1 paradigm, as proof assistants provide ground-truth, tool-executionsignaled feedback at every step. In this setting, the agent proposes one or more tactics (i.e., proof steps), formal proof checker (the tool) deterministically verifies their validity, and the resulting validated proof-state transition is returned to the agent. This verification outcomee.g., whether tactic is accepted, whether it advances the proof state, or whether complete proof is achievedserves directly as verifiable reward signal for policy optimization. Compared to code-execution RLVR, where unit tests may be sparse or incomplete, theorem proving offers step-wise semantic verification with minimal ambiguity, enabling denser rewards and substantially easing long-horizon credit assignment. Recent systems such as AlphaProof [77] (Nature 2025), DeepSeek-Prover-V2 [78] (ICLR 2025), Kimina-Prover [72], and Leanabell-Prover-V2 [79] leverage this"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "verifier feedback to train multi-step proof search policies via reinforcement learning, while complementary line of work augments the native proof checker feedback with auxiliary guidance signals to prioritize trajectories, shape exploration, or stabilize optimization on top of verifier-grounded rewards [8084]. While RLVR is well suited for learning proof strategies under fixed prover snapshot, formal theorem proving also highlights broader adaptation challenge: formal libraries (e.g., MATHLIB [85]) and large, actively evolving formalization projects [86] built by the Lean community grow continuously, expanding the available premise space. Addressing this non-stationarity often requires complementary continual or low-resource adaptation mechanisms beyond pure RLVR, which we discuss in Continual Adaptation (8.2). Multi-tool reasoning systems incorporated multiple tools in sequential or compositional pipelines, which used environment feedback to guide action selection. Router-R1 [87] (NeurIPS 2025) trained policy language model to coordinate multiple large language models through RL framework that formulates multi-round routing and aggregation as sequential decision process. During training, the policy LLM learns to alternate between internal reasoning and external model selection, dynamically invoking different LLMs from routing pool to solve complex tasks. FTRL [88] proposed an automated strategy for constructing tool-use training environments through multi-stage pipeline, enabling the creation of diverse and comprehensive training settings without external toolsets. Building on these environments, FTRL introduces feedback-driven training framework that improves models tool-use capabilities by leveraging verifiable reward function, which balances tool invocation accuracy and task completion using only environmental feedback. Nemotron-Research-Tool-N1 (Tool-N1) [25] is series of LLMs trained with R1-style reinforcement learning to enhance tool-calling capabilities in multi-tool reasoning scenarios. At each action step, the model produces explicit reasoning enclosed in <think> tags, followed by structured tool calls in <tool_call> tags, effectively separating internal reasoning from external tool execution. WebGen-Agent [89] introduced novel framework for interactive website code generation and integrated visual-language model for assessing website appearance via screenshots and GUI-agent for testing functional correctness. To enhance smaller open-source models, the authors propose Step-GRPO with Screenshot and GUI-agent Feedback, step-level reinforcement learning method that uses appearance and functionality scores as dense rewards. ToolExpander [90] enhanced GRPO-based RL for single-turn tool tasks, specifically targeting small-scale, resource-constrained LLMs. It introduced Dynamic Multi-Round Hard Sampling to replace difficult samples with high-quality few-shot examples during training, reducing the proportion of hard samples and improving learning stability. Additionally, the SelfExemplifying Thinking mechanism allows the model to autonomously generate and analyze few-shot examples, receiving small extra reward to encourage self-guided learning. More tasks A1-type training is also applied to many other downstream tasks achieving promising results. RecR1 [91] (TMLR 2025) is reinforcement learning framework that directly optimizes LLMs for recommendation tasks using feedback from downstream recommendation systems, rather than relying on imitation of other models or synthetic supervised data. By casting LLM generation as policy and using recommendation metrics (e.g., NDCG, Recall) as reward signals, Rec-R1 enables closed-loop adaptation of LLM outputs, aligning generation with actual recommendation performance. SQL-R1 [92] is recent work that addresses the Natural Language to SQL (NL2SQL) task by leveraging reinforcement learning to enhance reasoning capabilities in complex database scenarios. SQL-R1 introduces tailored RL reward function comprising format, execution, result, and length rewards to guide the model toward generating SQL queries that accurately reflect user intent. olmOCR 2 [93] is state-of-the-art open-source OCR system for converting digitized print documents into naturally ordered plain text. This reinforcement learning-based method is key departure from the projects first version, olmOCR 1 [94], which was trained using supervised fine-tuning to mimic the static outputs of teacher model. Critically, this new approach moves beyond traditional edit distance metrics, which often fail to capture practical correctness or handle layout ambiguities. Instead, the model is trained using diverse set of binary unit tests as the reward signal, covering text presence/absence, reading order, table accuracy, and math formula rendering. In summary, RLVR-based A1 methods represent substantial evolution, directly engaging with interactive environments to iteratively improve performance. RLVR-based A1 methods leverage environment-derived reward signals to guide policy optimization, often integrating advanced techniques such as KL-regularized PPO, GRPO, and dynamic sampling. The unifying principle is that models learn through trial-and-error, receiving immediate feedback from the environment to refine both reasoning and tool-use strategies. Despite their effectiveness, RLVR-based approaches"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Figure 5 Development timeline of A2 methods (agent adaptation with agent output as signal). generally require careful reward design, computational resources for interactive training, and mechanisms to stabilize learning."
        },
        {
            "title": "4.2 A2: Agent Output as Signal",
            "content": "Different from A1-type adaptation, which leverages feedback obtained from tool executions or external environments, the A2 paradigm focuses on using evaluations of the agents own outputs as the optimization signal. In this setting, the learning or adjustment process is driven by assessing the quality of the agents generated outputs. Such evaluations may come from human judgments, automated metrics, or environment-based rewards, and are used to update or refine the agent policy. Under this paradigm, adaptation can occur in two primary settings: Agent Adaptation w/o Tools: The agent relies on the evaluation of its reasoning or problem-solving outputs without involving external tools. This direction mainly focuses on improving intrinsic reasoning abilities, such as mathematical reasoning, coding, or logical inference, by optimizing the model based on evaluations of its generated solutions. Agent Adaptation w/ Tools: The agents generated outputs are assessed in conjunction with tool interactions, providing feedback on how effectively the agent plans, selects, and executes tool usage. This line of adaptation aims to enhance the agents capability in coordinating and utilizing tools, using evaluation signals derived from task outcomes that depend on tool-mediated actions."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 2 A2 Methods: Tool Adaptation w/ Agent Supervision Time Method Venue Task(s) Tool(s) Agent Backbone Tuning Links w/o Tools GPT3.5, GPT4, CODEX GPT3.5, GPT4o"
        },
        {
            "title": "Prompt\nEngineering\nPrompt\nEngineering",
            "content": "2023.03 Self-Refine NeurIPS23 Dialogue, Math, 2024."
        },
        {
            "title": "Nature",
            "content": "Coding Code Optimization, Molecule Optimization, etc. 2024."
        },
        {
            "title": "RISE",
            "content": "NeurIPS24 Math 2024."
        },
        {
            "title": "SCoRe",
            "content": "ICLR"
        },
        {
            "title": "Nature",
            "content": "Zero (Math) 2025.01 Kimi k1.5 EHRMind 2025.05 arXiv arXiv Math, Coding, QA Math Math, Coding EHR-based Reasoning 2025.05 metaTextGrad NeurIPS25 QA, Math, Word Sorting 2025.06 Magistral 2025.10 GRACE 2025.10 KnowRL 2025. Empower arXiv arXiv arXiv arXiv Math, Coding Embedding Tasks Knowledge Calibration Coding w/ Tools 2023.10 FireAct arXiv QA Search API 2023.10 Self-RAG ICLR24 QA, Fact Verification Retriever LLaMA2, LLaMA3, Mistral Gemini1.0 Pro, Gemini1.5 Flash DeepSeek-V3 Kimi k1.5 LLaMA3 Qwen3-235BA22B, Claude-3.5Sonnet Magistral Qwen2.5, Qwen3, LLaMA3.2 LLaMA3.1, Qwen2.5 Gemma3 GPT3.5, LLaMA2, CodeLLaMA LLaMA2 2024. RPG EMNLP24 QA, Reasoning 2024.06 Re-ReST 2025.01 Agent-R EMNLP24 QA, VQA, Sequential Decision, Coding Various Tasks arXiv 2025.02 RAS arXiv 2025.03 R1-Searcher arXiv QA QA 2025.03 Search-R1 COLM25 QA 2025.03 ReSearch NeurIPS25 QA 2025.04 ReTool arXiv Math 22 Search Engine, Retriever Tool APIs LLaMA2, GPT3.5 Various Models Monte Carlo Tree Search Retriever Retriever Qwen2.5, LLaMA3.2 LLaMA2, LLaMA3.2 LLaMA3.1, Qwen2.5 Qwen2.5 Search Engine, Retriever Search Engine, Retriever Code Interpreter Qwen2.5 Qwen2.5 P P"
        },
        {
            "title": "GRPO",
            "content": "GRPO SFT, GRPO Prompt Engineering PPO, GRPO GRPO REINFORCE++ SFT SFT SFT SFT"
        },
        {
            "title": "DPO",
            "content": "SFT SFT REINFORCE++ PPO, GRPO GRPO PPO Continued on next page"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 2 Continued from previous page Time Method Venue Task(s) Tool(s) Agent Backbone Tuning"
        },
        {
            "title": "2025.04 DeepResearcher",
            "content": "arXiv QA, Reasoning, Deep Research"
        },
        {
            "title": "ToolRL",
            "content": "2025.04 2025.05 AutoRefine ZeroSearch 2025."
        },
        {
            "title": "Tool Calling",
            "content": "arXiv NeurIPS25 QA QA arXiv 2025."
        },
        {
            "title": "StepSearch",
            "content": "EMNLP25 QA 2025.06 SelfChallenging arXiv"
        },
        {
            "title": "2025.06 MMSearch-R1",
            "content": "arXiv Multi-Turn FunctionCalling, Calculation QA, VQA 2025.07 DynaSearcher arXiv QA 2025.07 2025.08 Agent CodePRM Lightning ACL25 arXiv Coding Text2SQL, Math 2025.08 MedResearcherR1 arXiv Medical QA 2025.09 VerlTool arXiv 2025.10 A2FM arXiv Math, QA, SQL, Visual, Web Search, Coding Web Navigation, Math, QA 2025.10 TT-SI arXiv Tool Calling Web Search API, Web Browser Tool APIs Retriever Search Engine, Web Search Search Engine, Retriever Code Interpreter, Web Browser Image Search, Web Browser, Retriever Document Search, KG Search Code Executor SQL Executor, Retriever, Calculator Medical Retriever, Web Search API, Document Reader Code Interpreter, Search Engine, SQL Executor, Vision Tools Search Engine, Crawl, Code Executor Tool APIs Qwen2."
        },
        {
            "title": "GRPO",
            "content": "Various Models Qwen2.5 Qwen2.5, LLaMA3.2 Qwen2.5 LLaMA3.1 GRPO GRPO REINFORCE, GPRO, PPO, SFT StePPO REINFORCE, SFT Qwen2. REINFORCE, SFT Qwen2.5, LLaMA3.1 GRPO Qwen2.5-Coder LLaMA3.2 SFT LightningRL Links P MedResearcherR1 SFT, GRPO Qwen2.5, Qwen3 GRPO Qwen2.5 APO,GRPO Qwen2.5 Test-Time Fine-Tuning P"
        },
        {
            "title": "4.2.1 Agent Adaptation w/o Tools",
            "content": "A major breakthrough in the paradigm of output-based agent adaptation emerged with the introduction of the DeepSeek-R1 framework [24] (Nature 2025), which demonstrated that reinforcement learning with verifiable reward (RLVR) can effectively enhance the reasoning capabilities of large agents. In this framework, strong base model serves as the core reasoning engine, while reinforcement learning encourages the generation of outputs that are logically consistent and verifiably correct. The training process focuses primarily on reasoning-intensive domains such as mathematics and code generation, where the quality of outputs can be automatically evaluated through deterministic correctness signals. This approach not only improved model reasoning but also revealed scalable pathway for further enhancing agent intelligence beyond supervised fine-tuning. Concurrently, Kimi-1.5 [95] advanced this paradigm by scaling reinforcement learning for multi-modal agents and introducing simplified yet effective policy optimization strategies. By leveraging large-scale reasoning data and efficient reward modeling, Kimi-1.5 achieved strong performance across range of reasoning benchmarks,"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "matching or surpassing prior state-of-the-art models. Together, these works sparked new wave of research into the so-called R1 paradigm, where reinforcement learning is used to refine the reasoning process of agentic systems based on verifiable output evaluations. Following this development, number of subsequent studies have extended the paradigm to various applications and reasoning settings. Following the emergence of the R1 paradigm, series of subsequent works further extended the idea of optimizing agent reasoning through evaluations of model outputs, without relying on external tools. These studies explored diverse learning signals, objectives, and task domains, collectively enriching the landscape of output-based adaptation for reasoning enhancement. Empower [96] proposed self-supervised fine-tuning framework for assistive language models, where the optimization objective centers on maximizing human empowerment rather than explicit correctness. By using only offline text data, the method aligns agents to assist human users in multi-turn coding tasks, encouraging context-sensitive and cooperative behavior without the need for additional feedback or verifiable rewards. KnowRL [97] introduced reinforcement-based approach to strengthen self-knowledge calibration. Instead of focusing on task-specific correctness, KnowRL trains agents to assess their own confidence and feasibility in producing reliable answers. Through internally generated rewards derived from self-assessment, the model enhances its awareness of what it knows and what it does not, improving reliability and consistency across reasoning domains. GRACE [98] reimagines contrastive learning as form of reward-guided optimization, transforming contrastive objectives into policy signals that encourage explicit, interpretable reasoning. By treating positivenegative sample distinctions as reward feedback, GRACE bridges generative reasoning and representation learning, yielding improved embedding alignment and more transparent rationales. related study, Rec-R1 [91], applies reinforcement optimization to product re-ranking tasks, demonstrating that reinforcement signals derived from task-specific output evaluations can improve discriminative performance while preserving general reasoning ability. EHRMind [99] extends the reinforcement-with-verifiable-reward framework to clinical reasoning scenarios. Targeting electronic health record (EHR) interpretation, the study highlights the limitations of RLVR alone, noting that domain-specific reasoning often requires prior knowledge alignment through SFT. EHRMind combines lightweight SFT warm-up phase with subsequent RLVR optimization, effectively stabilizing training and improving interpretability in medical tasks such as clinical calculation, patient-trial matching, and disease diagnosis. This finding underscores broader insight: while RL-based adaptation enhances reasoning quality, SFT remains critical foundation for domain grounding and stable agent adaptation. Before the emergence of the R1 paradigm, several studies had already explored output-based adaptation strategies that optimize reasoning quality through self-generated feedback, without modifying model parameters via traditional supervised signals. These early attempts laid the conceptual foundation for reinforcement-driven reasoning improvement later seen in R1-style frameworks. Self-Refine [100] (NeurIPS 2023) introduced an iterative refinement framework in which the same language model acts as both generator and critic. The model first produces an initial response and then evaluates and revises it based on self-generated textual feedback. This process, inspired by human-style revision, improves output quality across diverse domains such as dialogue, mathematics, and code generation. Notably, Self-Refine requires no supervised data, auxiliary models, or reinforcement learning, which shows that structured self-feedback alone can lead to measurable gains in reasoning accuracy and output preference. Building upon this direction, SCoRe [100] (ICLR 2025) proposed reinforcement learning approach for enabling language models to self-correct using entirely self-generated data. Unlike conventional SFT, which struggles to teach effective correction behavior, SCoRe employs multi-turn online reinforcement learning to encourage models to iteratively refine their reasoning under their own distribution of responses. By combining reward regularization and self-generated correction traces, the method significantly improves self-correction ability on mathematical and reasoning benchmarks, demonstrating that reinforcement learning can effectively operationalize self-reflection into stable learning signal. Backpropagating language-model feedback. TextGrad [101] (Nature 2025) introduced general framework for agent self-improvement through textual gradient descent (TGD). Instead of relying on numerical gradients or"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "verifiable environment rewards, TextGrad propagates language-model feedback in the form of natural-language critiques that describe how to improve the models outputs. These feedback messages act as textual gradients, allowing optimization across black-box LLM systems without requiring access to their internal parameters. This method formalizes self-refinement as differentiable-like process and generalizes earlier outcome-based approaches such as DeepSeek-R1. Empirically, TextGrad improves GPT-4os zero-shot code accuracy on LEETCODE-HARD (from 26% to 36%), raises MMLU-Physics performance from 91.2% to 95.1%, and enhances the multi-tool agent CHAMELEON by 7.7%. Conceptually, TextGrad exemplifies the A2 paradigm, extending reinforcement-style adaptation from scalar rewards to structured linguistic feedback. It unifies prompt tuning, reasoning refinement, and compound-agent optimization under single abstraction of backpropagating language feedback, marking step toward interpretable and parameter-agnostic agent adaptation. Building on this, metaTextGrad [102] (NeurIPS 2025) applies the A2 paradigm recursively to the optimizer itself, using validation feedback to automatically refine the optimizers prompts and structure for better task alignment."
        },
        {
            "title": "4.2.2 Agent Adaptation w/ Tools",
            "content": "Following the rise of the R1 paradigm, the idea of using agent outputs as optimization signals has expanded beyond pure reasoning tasks to encompass tool-using agents. These studies extend output-based RL to settings where agents must decide when and how to invoke external tools, integrating real-time feedback from retrieval systems, APIs, or executable environments. This marks shift from reasoning-centric refinement to tool-grounded adaptation, where outcome feedback from the external world provides rich and verifiable learning signals. Retrieval-based tool learning. Different from earlier work like Self-RAG [103] (ICLR 2024) and its successors [104106], which introduces an distillation-SFT-based paradigm to teach models to use retrieval tools for search, recent major line of work investigates how RL can improve the use of retrieval tools for question answering. R1-Searcher [107], Search-R1 [49] (COLM 2025), ReSearch [108] (NeurIPS 2025), and their successors [109 115] all extend the R1 paradigm by enabling LLMs to autonomously generate and refine search queries during multi-turn reasoning. R1-Searcher [107] proposes two-stage RL framework that incentivizes the use of external search APIs, enhancing factual accuracy and reducing hallucinations in open-domain QA. Trained with multi-turn RL, the model learns to balance reasoning and retrieval, achieving up to 24% improvement over strong RAG baselines. Similarly, Search-R1 [49] formulates search invocation as reinforcement optimization problem, where retrieved evidence and final correctness jointly form the outcome-based reward. ReSearch [108] trains LLMs to reason with search via reinforcement learning, without any supervised data on reasoning steps. It integrates search queries and retrieved results directly into the reasoning chain using tags such as <think>, <search>, and <result> and optimizes the model with GRPO to decide when and how to search. Trained on multi-hop QA tasks, ReSearch yields 922% absolute gains over iterative RAG baselines and exhibits emergent reflection and self-correction behaviors during RL training These works demonstrate that reasoning-oriented RL can naturally extend to retrieval-augmented contexts, allowing LLMs to internalize when external information is needed. Codeand execution-based tool learning. parallel direction focuses on code-based environments where tools provide executable feedback. CodePRM [116] (ACL 2025) introduces process reward model that scores reasoning steps based on code execution results, forming GenerateVerifyRefine pipeline that dynamically corrects reasoning errors during inference. ReTool [50] advances this idea by integrating real-time code execution into RL rollouts, teaching models when and how to invoke computational tools such as interpreters to optimize mathematical and symbolic reasoning. General multi-tool and agentic learning. Beyond retrieval, several studies generalize this principle to agents interacting with diverse APIs and environments. Test-Time Self-Improvement (TT-SI) [117] introduces on-thefly self-improvement, where the agent identifies uncertain test cases and generates new training data for them, performing fine-tuning directly at inference time. Agent Lightning [118] provides flexible RL framework that decouples agent execution from training, allowing reinforcement optimization to handle complex, multi-agent, and multi-tool workflows with minimal code modification. Re-ReST [119] extends self-training with reflection, using environment feedback such as unit test results to refine low-quality trajectories, yielding large gains on HotpotQA and AlfWorld. In similar spirit, Self-Challenging Agents [120] introduce self-generated curriculum: the model first generates novel tool-use tasks as challenger, then solves them via RL as an executor, achieving"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "more than twofold improvement on multi-turn tool-use benchmarks. Agent-R [121] further formalizes iterative self-reflection via model-guided critique construction, continuously correcting failed trajectories using Monte Carlo Tree Search (MCTS) rollouts, which improves performance by 5.6% across interactive environments. Finally, A2FM [122] unifies reasoning and acting within cost-regularized RL framework, dynamically choosing between internal reasoning, tool invocation, or direct answering, thereby improving both efficiency and accuracy in hybrid reasoning benchmarks. Addressing system-level scalability in similar vein, VerlTool [123] introduces unified infrastructure for agentic RL. By decoupling the RL workflow from modular tool server and implementing asynchronous rollouts, it eliminates synchronization bottlenecks, enabling the efficient training of multi-modal agents across disparate tasks ranging from visual reasoning to software engineering."
        },
        {
            "title": "5 Tool Adaptation",
            "content": "Tool adaptation represents another emerging trend for enhancing AI agents performance on specific tasks, marking conceptual shift from optimizing the agent itself to optimizing its ecosystem. Instead of modifying the agents parameters through fine-tuning or reinforcement learning, this paradigm targets the external componentsthe tools that mediate perception, computation, and interaction. These tools may encompass pre-trained models, retrievers, planners, or executors that the agent can invoke through language or code. Consequently, tool adaptation focuses on improving the agents operational environment rather than its internal cognition. Methods in this category typically (1) employ pre-trained machine learning modelsdeveloped via environment feedback or data imitationas plug-and-play components, ranging from simple classifiers to complex LLM-based sub-agents as discussed in 4; or (2) leverage the agents outputs as supervision or reinforcement signals to train, align, or refine the tool itself. This perspective re-frames intelligent systems as co-adaptive ecosystems, where frozen agents and adaptive tools evolve symbiotically toward higher task efficiency, modularity, and generalization. Formally, let denote an agent, parameterized by its internal configuration or policy (which include prompt templates or model weights) and denote tool or set of tools that can be trained or optimized based on task feedback. The adaptation process can be characterized by two complementary paradigms: (T1) = arg max Otool(T ), (T2) = arg max Oagent(A, ), where Otool quantifies task-specific or environment-driven improvements that are independent of the agent, such as retrieval accuracy or planning efficiency, while Oagent incorporates agent-derived supervision, where the agents outputs provide learning signals to refine or align the tool. Here, denotes the optimized tool configuration that maximizes the respective objective, illustrating how tool adaptation complements agent-level optimization within the broader agenttool ecosystem."
        },
        {
            "title": "5.1 T1: Agent-Agnostic Tool Adaptation",
            "content": "The foundational architecture for tool-augmented systems is to use pre-trained models as plug-and-play tools for frozen agents. The agent orchestrates tool usage through prompting alone, never updating its parameters, while leveraging tools that were trained independently on diverse data sources before deployment."
        },
        {
            "title": "5.1.1 Foundational Systems and Architectures",
            "content": "Early systems exemplifying the tool-adaptation paradigm established the architectural foundations for how frozen agents can effectively orchestrate or invoke external models. These pioneering works demonstrate distinct mechanisms functional, prompt-based, code-based, and graph-basedthat together shaped the design space for modern toolaugmented AI systems. Operator-Learning Tools: Neural Operators [124] (JMLR). Before large-scale LLM-based orchestration emerged, Neural Operators represented seminal example of agent-agnostic tool learning: models trained to approximate mappings between infinite-dimensional function spaces, serving as differentiable surrogates for complex simulators. Unlike conventional neural networks tied to discrete grids, Neural Operators are discretization-invariantthey learn"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "the underlying operator itself, not its finite discretizationand can generalize across resolutions and geometries. The Fourier Neural Operator (FNO) achieves O(J log J) inference via spectral convolution and outperforms classical solvers on NavierStokes, Darcy flow, and elasticity equations by orders of magnitude in speed. Conceptually, FNO and its variants (Graph-, Low-rank-, Multipole-NO) mark the first wave of frozen tools that agents can query repeatedly for reasoning, planning, or control without retraining. In modern agentic pipelines, they are used as plug-in surrogatesfast, differentiable black-box functions invoked within decision or inference loops. HuggingGPT [125] (NeurIPS 2023) pioneered the orchestration paradigm by enabling ChatGPT to command 1000+ machine learning models from HuggingFace Hub without any fine-tuning. The frozen LLM executes four-stage workflow: task planning (decomposing user requests), model selection (choosing from tool descriptions), task execution (invoking models), and response generation (synthesizing outputs). This architecture demonstrates that language serves as universal interfacetool descriptions in natural language suffice for the frozen agent to coordinate complex multimodal workflows. On composite cross-modal tasks, HuggingGPT enables GPT-3.5 to achieve performance comparable to GPT-4V by orchestrating specialized vision, speech, and language models. The primary limitation lies in latency from sequential LLM calls and token length constraints for tool descriptions. ViperGPT [126] (ICCV 2023) introduced code generation as the orchestration mechanism. The frozen GPT-3 Codex generates Python code that composes vision modelsGLIP for detection, SAM for segmentation, MiDaS for depth estimationinto executable programs. This code-based approach achieves state-of-the-art zero-shot performance on GQA visual reasoning, outperforming end-to-end models by 1015% on compositional tasks. The key insight: Python functions provide more flexible tool composition than fixed API calls. Each tool exposes simple functions like find(image, object_name) or compute_depth(image), which Codex chains programmatically without learning tool-specific interfaces. SciToolAgent [127] (Nature Computational Science 2025) scales tool orchestration to scientific domains through graph-based organization. The frozen GPT-4o accesses 500+ biology, chemistry, and materials science tools via SciToolKGa knowledge graph encoding tool metadata, dependencies, and safety constraints. Graph-based retrieval for tool selection achieves 94% accuracy on scientific query benchmarks, representing 1520% improvement over GPT-4o without tool access. The system successfully automates protein engineering workflows chaining ESMFold for structure prediction, BLAST for sequence alignment, and custom analysis tools. This architecture demonstrates that structured knowledge graphs address scalability challenges inherent in prompt-based descriptions. These foundational systems illustrate the dominant integration patterns. HuggingGPT exemplifies prompt-based orchestration, where the agent parses tool calls from text. ViperGPT uses code generation, exposing tools as Python functions. SciToolAgent demonstrates knowledge graph retrieval, using RAG to select from structured tool graphs. fourth common pattern is multimodal bridging, which converts non-textual modalities into text representations; for example, Visual ChatGPTs [128] prompt manager serializes vision operations as text API calls. The usability of these patterns depends on clear interface design, such as programmatic function signatures (e.g., find_object(image: PIL.Image, ...)), structured JSON schemas, or simple natural language descriptions. Execution modes are similarly varied, ranging from direct API calls and code generation to HTTP requests and command-line invocations. Model Context Protocol (MCP) and Code Execution Environments [129]. As large-scale agent ecosystems began to connect thousands of heterogeneous tools, the Model Context Protocol (MCP) emerged as an open standard for unifying how agents interface with external systems. Rather than embedding long tool definitions and intermediate results directly into the models context, MCP provides universal API layer that enables frozen agents to discover, invoke, and coordinate tools across domains using consistent schema. Building upon this infrastructure, Anthropics Code Execution with MCP paradigm introduced an execution-centric design in which the agent writes executable code to interact with MCP servers instead of performing token-level tool calls. This approach allows agents to load only the necessary tool definitions, filter or aggregate data within sandboxed environment, and pass compact results back to the model, reducing context usage by over 98% while maintaining full compositionality. Conceptually, MCP represents scalable T1-style tool adaptation infrastructure that decouples execution from inference, while the code-execution mode bridges toward T2-style optimization by dynamically improving efficiency under frozen agents. Together with systems such as HuggingGPT, ViperGPT, and SciToolAgent, it exemplifies the architectural evolution from static tool invocation to protocol-driven, programmable,"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "and context-efficient orchestration."
        },
        {
            "title": "5.1.2 Categories and Training Methodologies",
            "content": "Tool adaptation encompasses wide range of pre-trained model categories, each contributing unique functional capabilities to frozen agents. The following examples highlight representative tool families and the training methodologies that enable their plug-and-play deployment across multimodal and domain-specific settings. Vision models dominate T1 deployments as plug-and-play tools. CLIP [130], trained contrastively on 400M image-text pairs, provides zero-shot classification and semantic understanding through frozen encoders. SAM [131], trained on 11M images with 1B masks via human-in-the-loop data engines, enables promptable segmentation with point, box, or mask inputs. SAM-CLIP [132] merges these capabilities through multi-task distillation with frozen teachers, achieving +6.8% mIoU on Pascal VOC for zero-shot semantic segmentation while retaining both parent models strengths. These vision tools require no task-specific fine-tuning - frozen agents invoke them directly via APIs for image understanding, segmentation, and classification tasks. Speech and audio tools leverage massive pre-training for robust performance. Whisper [133], trained on 680K hours of multilingual audio with weak supervision, provides speech recognition, translation, and language identification as frozen API for multimodal agents. The encoder-decoder Transformer architecture enables zero-shot transcription across languages and domains, demonstrating remarkable robustness to accents, noise, and technical terminology. Agents simply pass audio inputs to the frozen Whisper model and process text outputs without any model adaptation. Code execution tools encompass models that learn to compose and execute functions through code. CodeAct [57] demonstrates that representing tool use in executable Python rather than static JSON improves compositional reasoning, achieving over 20% higher success rates on API-Bank benchmarks. The dynamic nature of code allows agents to flexibly construct, parameterize, and combine tools without predefined schemas. Search and retrieval tools comprise pre-trained dense retrievers such as DPR [134], ColBERT [135], Contriever [136], and e5 [137], often deployed as frozen components within retrieval-augmented generation pipelines. These bi-encoder models, trained on passage ranking tasks, enable semantic search over large corpora. Scientific tools extend capabilities to specialized fields. AlphaFold2 [138] and ESMFold [139] provide protein structure prediction from sequences. Materials science models like CGCNN [140] predict crystal properties. Molecule representation learning approaches [141147] have been developed for molecular property prediction, whereas some encoderdecoder frameworks [148, 149] aim to predict transcriptional profiles elicited by chemical perturbations. These tools represent years of domain-specific model development, deployed as-is for frozen agents tackling scientific queries. Beyond these static models, adaptive agents introduced in 4 (such as DeepRetrieval [21] for search query rewriting and Code-R1 [65] for code generation) illustrate how trained reasoning agents themselves can function as dynamic tools. Once frozen, they extend the tool ecosystem by reformulating queries, generating executable code, or performing reasoning-driven actions, thereby bridging the gap between pre-trained models and the environment or offline data."
        },
        {
            "title": "5.2 T2: Agent-Supervised Tool Adaptation",
            "content": "The T2 paradigm represents profound conceptual inversion in how we approach adaptation in agentic systems. Rather than asking how can we modify the agent to better use its tools? (the A1/A2 question), T2 asks: how can we modify the tools to better serve fixed agent? This seemingly simple reversal has far-reaching implications. It reframes the expensive, monolithic foundation model as stable source of supervision rather than the target of optimization, and reconceptualizes the agents ecosystem of tools as dynamic, adaptive periphery that can be continuously refined. This inversion is not merely technical choice but reflects deeper understanding of the economics and modularity of modern AI systems. Training or fine-tuning billion-parameter foundation models is computationally prohibitive and risks catastrophic forgetting. In contrast, the peripheral toolsretrievers, planners, memory modulesare typically orders of magnitude smaller and can be trained with dramatically less data and computation. The T2 paradigm exploits this asymmetry, achieving what we term symbiotic adaptation: the frozen"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Figure 6 Development timeline of T2 methods (agent-supervised tool adaptation, classic memory-related methods are not included in this figure due to space limitation). agent provides high-quality supervision signals derived from its vast pre-trained knowledge, while the tools learn to translate, filter, and present information in exactly the form the agent finds most useful. The evolution of T2 methods from 2023 to 2025 reveals clear intellectual progression: from using internal proxy signals (perplexity, preferences) to train passive retrieval tools, to using verifiable outcome signals (task success, accuracy gains) to train active, multi-turn agentic tools. This progression mirrors broader maturation in the fields understanding of what makes an effective training signal and what kinds of tools are worth building. Table 3 T2 Methods: Tool Adaptation w/ Agent Supervision Time Method Venue Task(s) Tool Backbone Agent Backbone Tuning Links 2023. REPLUG NAACL24 QA Contriever Earlier Methods 2023.03 UPRISE EMNLP23 Zero-shot NLU (QA, NLI, etc.) GPT-Neo-2.7B 2023.05 ToolkenGPT NeurIPS23 Numerical Reasoning, QA, Plan Generation Token Embedding 29 GPT3-175B, PaLM, Codex, LLaMA-13B BLOOM-7.1B, OPT-66B, GPT-3-175B GPT-J 6B, OPT-6.7B, OPT-13B Proxy-Tuning, LSR Contrastive Learning Proxy-Tuning Continued on next page"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 3 Continued from previous page Time Method Venue Task(s) Tool Backbone Agent Backbone Tuning"
        },
        {
            "title": "2023.05 AAR",
            "content": "LLM-R ACL23 E5-base RA-DIT ICLR24 EACL ANCE, Contriever Zero-Shot Generalization (MMLU, PopQA) Zero-shot NLU (QA, NLI, Paraphrase, etc.) KnowledgeIntensive Tasks (MMLU, NQ, TQA, ELI5, HotpotQA, etc.) BGM T5-XXL-11B QA Proxy-Tuning COLM24 QA, Math, Code LLaMA2-7B DeBERTa-v3QA Bboxbase (0.1B), Adapter DeBERTa-v3large (0.3B) GPT-3.5-Turbo EMNLP24 Coding DRAGON+ ICML24 ACL EVOR 2023.06 2023.10 2024.01 2024.01 2024.02 2024.02 2024.02 ARL ACL24 QA LLaMA2-7B 2024.03 BLADE AAAI QA BLOOMZ-1b7 2024.05 Medadapter EMNLP24 Medical QA, 2024.06 CoBB NLI, RQE EMNLP24 QA, Math BERT-BaseUncased Mistral-7b-instv2 2024.10 Matryoshka NeurIPS25 Math, Planning, 2025.06 Pilot Sysformer arXiv Reasoning QA LLaMA3-8B, Qwen2.5-7B Small Transformer 2025.05 s3 EMNLP25 QA Qwen2.5-7B RLVR Methods 2025.08 R-Zero arXiv Math, Reasoning Qwen3-4B, Qwen3-8B, OctoThinker-3B, OctoThinker-8B 30 Links P Flan-T5-Small, InstructGPT"
        },
        {
            "title": "Contrastive\nLearning",
            "content": "GPT-Neo-2.7B, LLaMA-13B, GPT-3.5-Turbo LLaMA-65B"
        },
        {
            "title": "Contrastive\nLearning",
            "content": "SFT, LSR PaLM2-S LLaMA2-70B GPT-3.5-Turbo, Mixtral-8x7B SFT, PPO Proxy-Tuning Contrastive Learning GPT-3.5-Turbo, CodeLLaMA GPT-3.5-Turbo ChatGPT, ChatGLM, Baichuan, Qwen GPT-3.5-Turbo GPT-3.5-Turbo, Claude-3-Haiku, Phi-3-mini-4kinst, Gemma-1.1-7Bit, Mistral-7B-instv2 GPT-4o-Mini, GPT-3.5-Turbo LLaMA-2-7B, LLaMA-3.1-8B, Mistral-7B, Phi-3.5-mini, Zephyr-7B-beta Qwen2.5-7B, Qwen2.5-14B, Claude-3-Haiku Qwen3-4B, Qwen3-8B, OctoThinker-3B, OctoThinker-8B Prompt Engineering Contrastive Learning SFT, BPO SFT, BPO SFT, ORPO DPO, IDPO Supervised Learning PPO GRPO Continued on next page Time Method"
        },
        {
            "title": "2025.08 Memento",
            "content": "Venue arXiv"
        },
        {
            "title": "2025.09 Mem-α",
            "content": "arXiv arXiv"
        },
        {
            "title": "2025.10 AgentFlow",
            "content": "arXiv"
        },
        {
            "title": "2025.10 AutoGraph-\nR1",
            "content": "2025.10 MAE 2025.10 Advisor Models arXiv arXiv arXiv"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 3 Continued from previous page Task(s) Tool Backbone Agent Backbone Tuning Long-Horizon Reasoning, Web Research, QA, Academic Reasoning QA Q-function (two-layer MLPs) GPT-4.1 Soft Q-Learning Qwen3-32B Qwen2.5-7B"
        },
        {
            "title": "PPO",
            "content": "Qwen3-4B Test-Time Learning, Long-Range Understanding Web Search, Planning, Reasoning, Math KG Construction KG Constructor Qwen2.5-7B (Qwen2.53B/7B) Qwen2.5-3B Math, Coding, QA Math, Reasoning Qwen2.5-7B, Qwen3-8B Qwen3-4B, Qwen3-32B, GPT-4.1-Mini"
        },
        {
            "title": "GRPO",
            "content": "Qwen2.5-7B Flow-GRPO"
        },
        {
            "title": "GRPO",
            "content": "Frozen RAG Generator (Qwen2.5-7B) Qwen2.5-3B REINFORCE++ GPT-4o-Mini, GPT-5, Claude4-Sonnet, GPT-4.1-Mini Qwen-7B GRPO GRPO P Links P 2025.10 QAgent arXiv QA Qwen2.5-3B"
        },
        {
            "title": "5.2.1 Earlier Methods: From Proxy Signals to Structured Preferences",
            "content": "The earliest T2 methods emerged from the retrieval-augmented generation (RAG) community, where researchers sought to optimize dense retrievers for compatibility with large language models. These foundational works established the core principle that frozen LMs internal computations could serve as supervision, but they reveal, in hindsight, the limitations of relying on proxy metrics that may not align with downstream task objectives. REPLUG [11] (NAACL 2024) introduced general framework for adapting frozen language models through black-box supervision, using perplexity reduction as training signal for the retriever. The core intuition is that if conditioning the LM on retrieved document lowers its perplexity for given query, the document likely provides informative context. Formally, the retriever is optimized to align its retrieval distribution with the distribution induced by the LMs perplexity-based preferences: LREPLUG = DKL(Pretriever(dq) PLM-perplexity(dq)) , where PLM-perplexity(dq) reflects how strongly each document reduces the LMs perplexity when conditioned on query q. This design enabled retrieval adaptation without parameter access to the LM, establishing the foundation for family of agent-supervised methods that optimize external modules based solely on frozen-agent feedback. BLADE [150] (AAAI 2025) further extended this paradigm by replacing traditional retrievers with domain-specific models that synthesize auxiliary knowledge: it couples frozen general LLM with small, domain-specific LM optimized via Bayesian Prompted Optimization (BPO). The small LM learns to generate domain-relevant knowledge and soft prompts that improve the black-box LLMs responses, extending REPLUGs black-box adaptation principle from retrieval to generative, domain-specialized tool co-adaptation. BBox-Adapter [151] (ICML 2024) reframed adaptation as an energy-based modeling problem, introducing ranking-based noise-contrastive estimation loss and an online update framework to align outputs from black-box APIs like GPT-3.5 without access to internal probabilities. These methods collectively shifted the focus from likelihood-based alignment to utility-driven adaptation, paving the way for reinforcement-learning-based search and reasoning frameworks. proxy-tuning [152] (COLM 2024) extends black-box, agent-supervised adaptation to decoding time: small tuned expert and its"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "untuned anti-expert provide logit offsets that steer frozen large LM without modifying its weights, effectively training lightweight steering tool under frozen agent (T2). EVOR [153] (EMNLP 2024) further extends to the domain of code generation, formulating retrieval and knowledge evolution as co-adaptive process driven by execution feedback from frozen LLM. Preference Learning: Toward Task Alignment. While black-box adaptation methods such as REPLUG and BBox-Adapter relied on indirect proxy signals like perplexity or ranking scores, subsequent studies [28, 154157] moved toward explicit preference-based supervision that better reflects task utility. AAR [28] (ACL 2023) introduced augmentation-aware retrieval, where frozen source LM (Flan-T5-250M) constructs preference pairs by comparing documents that most improve its own likelihood against human-annotated references. The retriever is then trained to reproduce these preferences via contrastive loss, yielding signal that directly encodes the LMs notion of helpful context. Remarkably, these LM-derived preferences transfer effectively across architectures and scales, improving even 175B-parameter models. RA-DIT [154] (ICLR 2023) formalized this idea by defining document utility as the log-probability gain for producing correct answers: Utility(d, q, a) = log PLM(aq, d) log PLM(aq), training retrievers to prefer documents that yield higher expected gains. This preference-based approach aligns retrieval more directly with downstream reasoning objectives while still operating through single forward pass of the frozen LM. Together, these works mark conceptual shift from proxy-based to task-aligned supervision, paving the way for reinforcement-style feedback and multi-turn optimization in later frameworks. Multi-Stage Architectures: Distilling Complex Preferences The sophistication of training signals reached new level with LLM-R [23] (EACL 2024), which introduced multi-stage distillation pipeline. Rather than training the retriever directly on the frozen LMs outputs, LLM-R first trains an intermediate cross-encoder reward model to capture the frozen LMs nuanced preferences over in-context examples. This reward model, which can afford to be slow and expressive because its only used during training, is then distilled into fast bi-encoder retriever. This architecture embodies key design principle: the complexity of the training signal need not be constrained by inference-time efficiency requirements. By decoupling the preference modeling from the final retrieval tool, LLM-R achieves both high-quality supervision and fast deployment. UPRISE [158] (EMNLP 2023) extends this paradigm beyond documents to prompts, training prompt retriever using the frozen LLMs task performance across diverse tasks. By training on multiple tasks simultaneously, UPRISE learns generalizable meta-skill: selecting prompts that improve LLM performance in zero-shot settings. The cross-task transfer (+8.5% on reading comprehension, +14.6% on paraphrase detection) suggests that T2-trained tools can internalize abstract principles of what helps an LM rather than memorizing task-specific heuristics. By 2024, the field had reached critical realization: optimizing retrieval in isolation is insufficient. Even perfect retriever, measured by traditional IR metrics like NDCG or MRR, may produce results that are poorly suited for LLM reasoning. This insight catalyzed wave of research focused on training bridge tools: rerankers, query reformulators, and document selectors that explicitly align retrieval outputs with LLM preferences. The Architecture of Preference Translation BGM [159] crystallizes this new paradigm. The core observation is stark: there exists systematic preference gap between what traditional retrievers optimize for (surface-level relevance, lexical overlap) and what LLMs find useful for reasoning (contextual coherence, inferential support). BGM addresses this by training T5-XXL bridge model that sits between frozen retriever and frozen generator (PaLM2-S), transforming the retrievers output into an LLM-friendly context. The training methodology is sophisticated: Stage 1 uses supervised learning on synthetically generated preference data (documents that improve LLM task performance vs. those that dont), while Stage 2 employs reinforcement learning where the frozen LLMs final task success provides the ultimate reward signal. This two-stage approach allows the bridge model to first learn coarse-grained preferences efficiently, then fine-tune its policy for actual downstream impact. The results are impressive: on HotpotQA, the bridged system achieves 35.6% exact-match accuracy compared to 25.8% with the best prior retrievera relative improvement of 38%. What makes BGM architecturally significant is its demonstration that specialized adaptation layers can be more effective than end-to-end fine-tuning. Rather"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "than trying to make single retriever simultaneously satisfy IR metrics and LLM preferences (which may be in tension), BGM decomposes the problem: the retriever handles broad recall, while the bridge model handles preference alignment. This modularity is recurring theme in the most successful T2 systems. Synthesis: The Multi-Tool Ecosystem Collectively, these advances reveal new architectural pattern: cascaded tool adaptation. Rather than single monolithic retriever, state-of-the-art T2 systems now employ pipeline of specialized tools: query reformulators, retrievers, selectors, each trained using different aspects of the frozen LLMs behavior as supervision. This decomposition offers multiple advantages: Separation of concerns: Each tool can optimize specific sub-problem (recall vs. precision, speed vs. quality). Composability: Tools can be mixed and matched; new reranker can be trained without retraining the retriever. Efficiency: Expensive operations (LLM inference) are deferred to the end of the pipeline, after cheaper tools have filtered the space. Yet this raises fundamental question: if tools can learn from tools, which learn from frozen LLMs, how deep can this hierarchy go before compounding errors overwhelm the benefits? This question remains open, though empirical results suggest that 2-3 stages of tool adaptation (e.g., query reformulator retriever reranker) strikes good balance."
        },
        {
            "title": "5.2.2 Subagent-as-Tool",
            "content": "The year 2025 marked paradigm shift in T2 research: the transition from training reactive tools (retrievers that respond to queries) to training proactive sub-agents (autonomous systems that actively explore, plan, orchestrate, and refine their operations over multiple turns while serving frozen primary agents). This shift was enabled by breakthroughs in reinforcement learning with verifiable rewards (RLVR) and represents the full maturation of the T2 vision: specialized sub-agents trained using frozen agent outputs as supervision signals, creating symbiotic ecosystem where lightweight agentic tools co-evolve with frozen reasoning cores. Critically, this agentic transformation applies not only to information retrievaltraining searchers that iteratively gather evidencebut also to meta-cognitive processes such as workflow orchestration and memory management, where sub-agents learn to coordinate frozen specialist modules and manage multi-step reasoning trajectories. We organize this evolution into three families of subagents: (i) agentic searchers, (ii) memory-construction subagents, (iii) meta-cognitive planners and orchestrators, and (iv) self-evolving subagents. Agentic Searcher Breakthrough s3 [27] (EMNLP 2025) demonstrated that training agentic tools could be radically more data-efficient than training agentic LLMs. The system trains lightweight 7B searcher that performs multi-turn iterative search: generate queries, retrieve documents, select evidence, decide whether to search again or feed context to the frozen generator. The frozen generator (agent, Qwen2.5-14B or Claude) never updates, but provides the ultimate training signal through metric called Gain Beyond RAG (GBR): GBR = Accuracy (Gfrozen(q, Ds3), a) Accuracy (Gfrozen(q, Dnaive), a) where Ds3 are documents retrieved by the trained searcher and Dnaive are documents from naive top-k retrieval. This reward directly measures the value added by the search tool, focusing training on examples where naive retrieval fails. The efficiency gains are staggering: s3 achieves 58.9% average generation accuracy with only 2.4k training samples70 less data than Search-R1 (an A2-style agent requiring 170k examples) and 33 faster wall-clock training time. Moreover, s3 trained on general QA achieves 76.6% accuracy on specialized medical QA versus 71.8% for Search-R1, suggesting that T2-trained tools learn more generalizable search skills than agents trained end-to-end. The theoretical explanation for this efficiency advantage is illuminating: in A2-style agent training, the model must simultaneously learn (1) domain knowledge, (2) tool use skills, and (3) task-specific reasoning, leading to complex, high-dimensional optimization landscape; in T2, the frozen generator already"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "possesses domain knowledge and reasoning ability, so the tool need only learn the procedural skill of effective search. Similar to this idea, DynamicRAG [160] (NeurIPS 2025) agentifies reranking: instead of static reorderings, an RL policy adapts how many and which documents to pass based on query hardness and retrieval noise, balancing quality and context cost. The training combines imitation learning on expert trajectories (to bootstrap reasonable behavior) with policy gradient RL where the generators output provides the reward signal. The learned policy exhibits emergent adaptive behavior: for simple queries with high-quality initial retrieval, it presents fewer documents; for complex queries with noisy retrieval, it retrieves more broadly and reranks more aggressively. QAgent [161] further clarifies how to robustly train such search subagents. Its Stage 1 trains 3B search agent end-to-end, rewarding it based on whether its own generated answer is correct, but this encourages reward hacking, where the agent prefers shallow, easily copyable evidence over genuinely informative documents. Its Stage 2 corrects this by switching to evaluation from stronger frozen generator: RStage2 = [Gfrozen(q, Dagent) = acorrect] , rewarding the searcher only when the frozen model can answer correctly using its retrieved documents. This decoupling forces the subagent to optimize for retrieval quality rather than its own myopic behavior, reinforcing core T2 principle: the frozen generator should not only consume tools outputs but also supervise its learning. Learning to Construct Memory as Subagent Extending beyond search, complementary line of work treats long-term memory construction itself as T2-style subagent problem. Mem-α [162] formulates memory management as RL over an explicit memory API, training lightweight Qwen3-4B controller to operate threepart external memory (core summary, semantic facts, episodic events) for frozen backend generator. Only the memory-writing policy is optimized; the generator and retriever for downstream QA remain frozen. Rewards derive from verifiable outcomesquestion-answering accuracy over long horizons, tool-call correctness, effective compression, and semantic validity of memory entriesso the subagent learns to construct compact yet sufficient memories that maximize the frozen models utility. Empirically, Mem-α significantly outperforms prior memory baselines and generalizes from 30k-token training sequences to contexts exceeding 400k tokens, exemplifying memory-construction subagent that adaptively curates the information diet for fixed reasoning core. AutoGraph-R1 [163] applies this symbiotic principle to the construction of structured Knowledge Graphs (KGs). Rather than relying on static extraction heuristics, it optimizes an LLM-based constructor subagent to generate KGs from raw text. The supervision signal is derived directly from the frozen agents performance on downstream reasoning tasks (GraphRAG [164]) using the generated graph. This allows the constructor to learn policy that prioritizes functional utility, creating connectivity and paths that specifically facilitate the host agents retrieval and reasoning, over intrinsic metrics like triple density. Meta-cognitive and Control Subagents Stepping further up the stack, recent work trains subagents that shape how frozen models think (planning, steering, and budgeting computation) rather than what they retrieve or store. AI-SearchPlanner [26] introduces multi-objective optimization that balances effectiveness with efficiency. The system trains planner tool (Qwen2.5-7B) that generates multi-step search strategies for frozen generator, optimizing = [Routcome + λ Rprocess α Cost] , where Routcome measures final task success, Rprocess evaluates the rationality of the search plan (critiqued by the frozen generator), and Cost penalizes excessive planning. By leveraging both outcome and process rewards [165], the frozen model acts as executor and teacher, enabling the planner to internalize not only what works but why it works. Varying λ traces Pareto frontier between cost and quality, yielding planners tailored to different deployment budgets. Advisor Models [166] generalize this idea to instance-wise natural-language steering. small advisor model learns, via GRPO, to prepend context-specific advice that nudges frozen foundation model toward preferred behaviors (style, safety, reasoning depth) without touching its weights. Within our taxonomy, such advisors function as trainable control interfaces or parametric memories that encode environmentand user-specific latents."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Bridging from advising to driving, Matryoshka Pilot [167] (NeurIPS 2025) formalizes controllergenerator loop where small white-box LLM controls larger black-box LLM by emitting intermediate decomposition steps, plans, and summaries. Treating the black-box model as an environment, M-Pilot collects trajectory-level success signals and optimizes the controller with Iterative DPO. This yields 37% gains across reasoning, planning, and personalization benchmarks, and transfers plug-and-play across multiple black-box backends, further reinforcing the view of control subagents as portable T2 tools. Learning to Orchestrate Frozen Specialists Orchestration-focused subagents push the T2 vision to its logical conclusion: training dedicated policy to coordinate multiple frozen specialists. AgentFlow [51] decomposes an agent into modulesa planner, tool executor, verifier, and solution generatorimplemented mostly as frozen Qwen2.5-7B-Instruct models. Only the planner is trained. Using Flow-GRPO, single trajectory-level reward (correct vs. incorrect, judged by GPT-4o) is broadcast to all decisions in each rollout, with group-normalized advantages enabling effective credit assignment despite sparse rewards. Empirically, 7B AgentFlow planner achieves 57.3% on search-intensive tasks (+14.9% over AutoGen), 51.5% on mathematical reasoning (+14.5% over ToRL), and 33.1% on GAIAoutperforming GPT-4 (200B parameters) on several setups. This shows that learned orchestration of frozen specialists can rival or surpass monolithic models. Self-Evolving (Sub)Agent more advanced branch of the subagent-as-tool paradigm allows the tools themselves to co-evolve through self-generated tasks and rewards. R-Zero [168] instantiates two rolesa Solver and Challengerfrom the same base LLM. When the Solver is frozen, its successes, failures, and uncertainty (via self-consistency) define rewards that train the Challenger to propose tasks near the Solvers capability frontier. Alternating these phases creates bidirectional loop, yet each step still follows the T2 principle of optimizing lightweight subagent under signals from stronger or temporarily fixed core. Multi-Agent Evolve (MAE) [169] extends this design into triadic architecture with Proposer, Solver, and Judge. The Proposer and Judge operate as adaptive T2 subagents: the Judge learns to evaluate trajectories produced by the system, and the Proposer learns to generate diverse, high-quality, Solver-challenging tasks. Rather than tuning the main Solver, MAE improves performance by training these peripheral subagents to shape data, rewards, and curricula. Together, R-Zero and MAE illustrate second generation of subagent-as-tool methodsself-evolving ecosystems that autonomously construct the learning conditions for otherwise frozen reasoning cores. Synthesis: The Maturation of T2 Across these lines of work, the subagent-as-tool paradigm extends T2 from what to retrieve, to what to remember, to how to plan, steer, and orchestrate, and finally to how to self-evolve. Agentic searchers (s3, DynamicRAG, QAgent) optimize information acquisition for frozen generators; memory-construction subagents (Mem-α) curate long-horizon state; meta-cognitive controllers and orchestrators (AI-SearchPlanner, Advisor Models, Matryoshka Pilot, AgentFlow) decide how tools and specialists are deployed; and self-evolving frameworks (R-Zero, Multi-Agent Evolve) autonomously generate curricula and reward signals that continually refine these ecosystems. The consistent lesson is that decoupling tool training from generator training, while enabling tools to adapt to one another, yields systems that are more data-efficient, modular, generalizable, and robust than monolithic alternatives. In this mature T2 view, intelligence emerges not from scaling single model, but from the learned coordination and co-evolution of specialized, frozen components through lightweight, adaptive subagents."
        },
        {
            "title": "5.2.3 Agentic Memory and Others",
            "content": "An agents memory system itself can also be framed as an adaptive tool. Instead of modifying the agents core parameters, T2 methods can train or tune the memory modulehow it writes, retrieves, reflects, and forgetsusing the frozen agents downstream task performance or outputs as the supervisory signal. Survey on agent memory [48] categorizes wide array of mechanisms that can be optimized as T2 tools. These works explore memory as foundational component for agentic behavior, spanning short-term buffers, long-term experiential databases, and structured knowledge."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Dynamic Memory Stores Many foundational memory architectures function as T2 tools, where the frozen agents output (e.g., \"write\" command or new information) dynamically \"tunes\" the external memory store. This includes systems that manage short-term (in-context) and long-term (retrieval-based) storage, demonstrating how agents can manage context, retrieve past interactions, and maintain coherence over time by updating peripheral memory tool [170175]. Experiential and Reflective Memory significant line of T2-aligned research focuses on memory modules that learn from experience. These tools enable frozen agent to store, reflect on, and learn from its own output (e.g., entire trajectories), often using verbal reinforcement or self-correction. This allows the agent to build curriculum of skills and avoid repeating past failures by tuning the memory tool without updating the core LLMs weights [34, 176180]. Structured Memory (Graphs, Trees, and Databases) To move beyond linear text, some T2 memory tools structure information in sophisticated forms, such as knowledge graphs, trees, or symbolic databases. The frozen agents outputs are used as signals to tune this structured toolfor example, by adding new nodes, updating relationships, or writing to database. These structured representations can be more efficiently queried and reasoned over by the frozen agent, effectively externalizing complex memory management into specialized, adaptive tool [181184]. Episodic Memory as Trainable Module Memento [22] demonstrates that an agents memory system can be optimized as an external tool without any modification to the LLM planner. The system combines frozen GPT-4.1 high-level planner with trainable episodic case memory module. The memory stores past problem-solving trajectories, and the tool being trained is neural Q-function that learns case retrieval policy: which past cases to present to the frozen planner when facing new problem. The training signal is remarkably simple: binary task success or failure. This sparse, trajectory-level reward is broadcast to all case-selection decisions in that trajectory, and soft Q-learning algorithm updates the retrieval policy. Crucially, the frozen LLM never sees the Q-values or policy internals; it simply receives retrieved cases as context and generates its plan. Memento achieves top-tier performance: 87.88% on GAIA validation (ranked 1st), 79.40% on GAIA test (3rd place), and 95.0% on SimpleQA. Ablations show that case-based memory adds 4.79.6% absolute improvement on out-of-distribution tasks. This is remarkable because only the memory is trained; the same frozen LLM that performed worse without memory now excels simply because its information diet has been optimized. Test-Time Memory Curation Another prominent example of T2 memory adaptation at inference-time is Dynamic Cheatsheet (DC) [185], lightweight framework that provides persistent, evolving memory for black-box LMs. The system operates without modifying their underlying parameters and requires no gradient-based updates. The framework consists of two core modules: Solution Generator and Memory Curator. This Memory Curator is the adaptive T2 tool. It operates without access to ground-truth labels ; instead, it has to assess the correctness and efficiency of the solutions by itself after they are produced by the frozen generator. Based on this self-assessment, the curator updates the memory by storing concise, transferable snippets such as reusable strategies, code snippets, and general problem-solving insights , rather than full, uncurated transcripts. ReasoningBank [186] extends this test-time curation concept by creating memory framework that explicitly distills generalizable reasoning strategies from both successful and self-judged failed experiences. Unlike methods that store raw trajectories or only successful routines, ReasoningBank analyzes failures to extract crucial preventative lessons. This curated bank of reasoning strategies is then retrieved to guide the agent in future tasks. The framework also introduces memory-aware test-time scaling, which uses the curated memory to guide scaled exploration, creating synergy between memory and test-time scaling where the diverse experiences from scaling help forge stronger, more generalizable memories. This approach was shown to be effective on complex benchmarks like WebArena [187] and SWE-Bench [188]. Adapting the Embedding Space An approach for tool scalability is ToolkenGPT [29], which represents tools as learnable token embeddings within the frozen LLMs vocabulary. The entire LLaMA-13B/33B model remains"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "frozen; only small embedding matrix Wτ RTd (where is the number of tools and is the embedding dimension) is trained. These toolkens are concatenated with the standard vocabulary, and the frozen LLM learns to predict them like any other token. Training uses supervised learning on parallel sequences where ground-truth tool calls are replaced with toolken placeholders. The loss is masked so that only the toolken predictions (and subsequent argument tokens) contribute gradients to Wτ. This is remarkably parameter-efficient: adding 234 tools requires training only 234 4096 1M parameters (for LLaMAs 4096-dimensional embeddings), compared to the 13B+ parameters of the full model. ToolkenGPT achieves 73% one-hop accuracy on FuncQA (vs. 57% for ReAct), 75% supervised accuracy on 234-relation KAMEL, and 68% success on VirtualHome with 58 action/object tools. Crucially, new tools can be added by simply expanding Wτ and continuing trainingno full model retraining required. The conceptual contribution of ToolkenGPT is showing that adaptation can occur at the interface layer (the embedding space) rather than the parameter layer (the LLM weights), offering middle ground between fully frozen T1 systems and fully fine-tuned A1/A2 systems. Beyond the paradigms above, diverse set of recent approaches further extends the tool adaptation framework. These methods introduce new training objectives, modalities, and architectural innovations that broaden the scope of tool adaptation. UniMuR [189] trains unified multimodal embeddings aligned with frozen LLM semantic representations, yielding 6.5% R@1 improvement on MMDialog. DIFO [190] adapts frozen CLIP through task-specific prompt learning via mutual information maximization for source-free domain adaptation. V2L Tokenizer [191] trains encoder-decoder structures mapping images to frozen LLM token space, using the frozen vocabulary as quantization codebook to enable low-level vision tasks with frozen text LLMs. Sysformer [192] trains small transformer that adapts the system-prompt embeddings based on each user prompt while keeping the LLM frozen. Supervision comes entirely from the frozen models own likelihoods over refusal and compliance targets, augmented by reconstruction and optional classifier losses. Common patterns emerge across T2 methods: lightweight training of small modules (millions of parameters) while keeping LLMs frozen (billions of parameters), semantic exploitation of rich representations (hidden states, token spaces, vocabularies), modality bridging between vision/retrieval/tools and frozen text LLMs, efficiency gains (14842,630 speedups), and strong generalization to zero-shot or unseen settings."
        },
        {
            "title": "6 Comparison of Adaptation Paradigms",
            "content": "This section provides comprehensive comparison of the four adaptation paradigms: (A1) Agent Adaptation with Tool Execution Signal, (A2) Agent Adaptation with Agent Output Signal, (T1) Agent-Agnostic Tool Adaptation, and (T2) Agent-Supervised Tool Adaptation. We first establish conceptual framework for comparison, then analyze the agent-centric (A1/A2) and tool-centric (T1/T2) paradigms in depth, with special focus on the emergent subagent-as-tool and graduation concepts, and conclude with quantitative synthesis of critical trade-offs."
        },
        {
            "title": "6.1 A Framework for Comparison",
            "content": "We compare the four paradigms along four main axes. Cost and Flexibility: We use cost to refer to compute and engineering effort required for adaptation, and flexibility to mean how easily the systems behavior can be reconfigured. A1/A2 provide high parametric flexibility (the entire agent policy can change), whereas T1/T2 provide high system-level flexibility (capabilities can be added, swapped, or composed via tools) but remain bounded by the frozen agents intrinsic reasoning power. Data Efficiency: Beyond raw compute, the amount of training data required differs dramatically across paradigms. Recent evidence suggests that T2 methods can match or surpass A2-style end-to-end agent training with orders of magnitude less data, by only training small subagents around frozen backbone."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 4 High-level qualitative comparison of the four adaptation paradigms. Flex. denotes the dominant form of flexibility: parametric (within single agent policy) vs. system-level (via modular tools and orchestration). Paradigm ID Locus of Adaptation Supervision Signal Cost & Flexibility Modularity & Evolution A1 Core Agent Policy Agent, Tool Signal A2 Core Agent Policy Agent, Output Signal Tool, Agent-Agnostic T1 External Tool Tool, Agent-Supervised T2 External Tool High Cost, High Param. Flex. Monolithic, Risk of Overfitting Tool Execution High Cost, High Param. Flex. Monolithic, Risk of Forgetting Agent Output Low Cost, High System Flex. Agent-Independent Frozen Agent Output Low Cost, High System Flex. High (Plug-and-Play) High (Symbiotic, No Forgetting) Generalization Capability: This axis captures how well an adaptation strategy transfers to new tasks, agents, or environments. T1 tools trained on broad data distributions generalize across different agents and tasks, while T2 tools often inherit cross-domain robustness from the frozen foundation models supervising them. A1/A2, especially on-policy variants, risk overfitting to specific environments without explicit regularization. Modularity and System Evolution: This axis focuses on engineering implications: how easily system can be extended or maintained over time. Tool-centric paradigms (T1/T2) enable modular evolution and hot-swapping of components; agent-centric paradigms (A1/A2) tend to be monolithic and may suffer from catastrophic forgetting when adapted repeatedly. In prose, the picture is as follows. Agent adaptation (A1/A2) is expensive but gives fine-grained control over the agent itself: one can rewrite the entire policy, alter reasoning style, alignment, and domain knowledge in single model. This is high parametric flexibility, but each change typically requires retraining large model and may unintentionally affect other behaviors. Tool adaptation (T1/T2), in contrast, does not touch the core agent; instead, it achieves system-level flexibility by letting us attach specialized tools, each tuned independently. We can freely augment separate capabilities (e.g., retrieval, planning, memory, code search), orchestrate them, and retire or replace tools without destabilizing the base agent. The trade-off is that these tools cannot push the system beyond what the frozen agent can understand and use. Data efficiency strongly favors tool-centric adaptation. T2 methods like s3 [27] reach competitive or superior performance to A2-style agents such as Search-R1 while using roughly 70 fewer labeled examples, because the T2 subagent only learns narrow procedural skill (e.g., search policy) rather than relearning general reasoning [27, 108]. Generalization exhibits similar pattern: T1 tools, by construction, are agent-agnostic and often robust across tasks and agents; T2 tools inherit the inductive biases of strong frozen agents and thus often transfer better across domains than aggressively fine-tuned A1/A2 agents, which may overfit and forget. Finally, modularity is where T1/T2 shine. Adding new retrieval strategy, memory writer, or planner simply means training (or swapping in) another tool, leaving the core agent untouched. In A1/A2, the only way to change behavior is to re-adapt the monolithic agent, risking interference with prior skills. In large-scale multi-tool systems, this difference in evolution ergonomics is often more decisive than raw performance."
        },
        {
            "title": "6.2 Agent Adaptation Paradigms: A1 and A2",
            "content": "We now analyze the two agent-centric paradigms, which both modify the agents core parameters but differ fundamentally in their training signals and optimization objectives."
        },
        {
            "title": "6.2.1 A1: Optimizing Tool Mechanics via Causal Feedback",
            "content": "The defining feature of A1 on-policy methods is reliance on causal, immediate, and fine-grained reward signals. The supervision source is the verifiable outcome of tool execution itself, not downstream task metric. For example, DeepRetrieval [21] formalizes query reformulation as an MDP where reward is directly derived from retrieval metrics like Recall@K or NDCG, and RLEF [20] frames code synthesis with rewards from test-case execution. This stands in contrast to A2 signals that only evaluate the final answer. Conceptually, A1 on-policy RL optimizes tool-use mechanicsteaching the agent how to wield tools correctly, grounding behavior in environment physics (this syntax executes, this query retrieves). This direct engagement"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "with ground-truth feedback drives strong performance in domains with verifiable, deterministic outcomes. Quantitative evidence. Mechanistic optimization under A1 achieves state-of-the-art performance in specialized domains: Retrieval: DeepRetrieval achieves roughly 3 improvement in recall (65.1% vs. 24.7%) on literature search [21]. Code reasoning: R1-Code-Interpreter reaches 72.4% accuracy on 37 test tasks through multi-stage RL [66]. However, learning through trial-and-error introduces significant challenges: KL-regularized PPO or GRPO, curriculum learning, and dynamic sampling for stable convergence. it requires careful reward design,"
        },
        {
            "title": "6.2.2 A2: Optimizing Tool Strategy via Holistic Rewards",
            "content": "A2 methods instead use holistic, sparse, and high-level rewards based on agent output qualitytypically final answer correctnesswhich depends on tool usage but does not directly supervise individual tool calls. ReSearch [108], trained on multi-hop QA, optimizes when and how to search. The reward asks not was this particular search good? but did the entire process of thinking, searching, and reasoning lead to the correct answer? Thus A2 optimizes tool-use strategy or coordination. Rather than learning search mechanics (assuming T1 retriever handles that), it learns the cognitive policy for when to search, what to search for, and how to integrate results. This strategic focus explains why ReSearch reports emergent reflection and self-correction behaviors during RL training [108]. Quantitative evidence. Strategic optimization under A2 proves highly effective for complex, multi-step reasoning: Retrieval-augmented QA: ReSearch yields 922% absolute gains over strong iterative RAG baselines [108]. Factual accuracy: R1-Searcher reports up to 24% improvement over strong RAG baselines, demonstrating enhanced factual accuracy and reduced hallucination through learned retrieval policy [107]. In terms of flexibility, A2 offers the richest parametric flexibility: the agent can change its entire global strategy for orchestrating tools and reasoning, but each such change requires expensive retraining, and the resulting policy is baked into single large model. A1 & A2: Signal Source as Reliability Axis. Beyond taxonomic categorization, the distinction between A1 and A2 fundamentally determines the granularity and scope of the adaptation signal. Tool-execution signals (A1) are grounded, causal, and process-oriented. The feedback is produced by an environment or tool whose semantics are independent of the agents internal beliefs (e.g., code execution, retrieval metrics, formal proof checkers). This grounding enables learning that is tightly coupled to intermediate correctness and tool mastery, but often comes with higher interaction cost and environment dependence. Agent-output signals (A2) are holistic, flexible, and outcome-oriented. Rewards are assigned to the agents final outputs, derived from either verifiable ground truths (e.g., gold answers, math solutions) or subjective preferences (e.g., reward models). While this allows for end-to-end task optimization, relying solely on terminal signals can make the agent vulnerable to shortcut learning (getting the right answer for the wrong reason) and sparse feedback issues compared to the dense signals of A1."
        },
        {
            "title": "6.3 Tool Adaptation Paradigms: T1 and T2",
            "content": "We now pivot to tool-centric paradigms, which shift optimization from the expensive agent to cheaper external tools. These paradigms sacrifice some parametric flexibility (the agent policy stays fixed) but gain system-level flexibility: we can grow, specialize, and rewire the tool ecosystem without touching the main agent."
        },
        {
            "title": "6.3.1 T1: The “Graduated Agent” as Subagent-as-Tool",
            "content": "T1 is defined by agent-agnostic, pre-trained, plug-and-play components. critical concept within T1 is the subagent-as-tool, which reveals rich development lifecycle."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "At one extreme, we have static, foundational tools like SAM [131] or AlphaFold2 [138], trained once on massive datasets and deployed as fixed APIs. They primarily encapsulate learned representations or simulators and can be called by any agent. At the other extreme are dynamic, graduated tools: adaptive agents from 6.2 can be trained under A1 or A2 and then frozen and reused as T1 tools. This Graduation Lifecycle (A1 T1) proceeds as: 1. Train (A1/A2): Use on-policy RL or outcome-based RL to train an agent for specific task (e.g., DeepRetrieval as search-query rewriter, Code-R1 as code generator). 2. Freeze: Once the agent reaches expert performance, freeze its parameters. 3. Deploy (T1): The frozen expert becomes T1 subagent-as-tool callable by any higher-level agent. Concrete examples already follow this pattern. DeepRetrieval is trained via on-policy A1 RL as query reformulation agent [21], but once frozen it can be used as an interchangeable T1 retrieval-augmentation tool in many different pipelines. Similarly, SWE-Grep [193] is trained as specialized RL subagent for fast, multi-turn, highly parallel code context retrieval, and then exposed as tool that software-engineering agents (e.g., SWE-Agent or Cursor-style IDE agents) can call for high-quality repository search. In both cases, the graduated subagent encapsulates learned policy (not just representation) and slots into new systems without retraining. From the flexibility perspective, T1 offers high system-level flexibility: one can assemble different T1 tools into various configurations, or replace one tool (e.g., swap retriever) without touching the agent. The cost of adding capability is proportional to the size of the corresponding tool, not the backbone agent. The trade-off is that the tools are not tailored to any particular agent; the agent must adapt its prompts or orchestration logic to whatever interface the tool exposes."
        },
        {
            "title": "6.3.2 T2: The “Symbiotic Inversion” and Subagent Federation",
            "content": "T2 represents conceptual inversion. Rather than asking how can we modify the agent to better use tools? (A1/A2), T2 asks: how can we modify tools to better serve fixed agent? This reframes the expensive foundation model from optimization target to stable supervision source. This creates symbiotic adaptation: the frozen host agent (e.g., GPT, Claude) provides high-level reasoning and reward signals, while adaptive symbiote subagents (e.g., lightweight 7B models) learn to translate, filter, and present information in exactly the form the agent finds most useful. The core benefit is decoupling skill from knowledge. traditional A2 agent like Search-R1 must learn (1) domain knowledge, (2) tool-use skills, and (3) task reasoning simultaneouslya complex optimization landscape. In T2, the frozen generator already possesses (1) and (3); the T2 subagent needs only learn procedural skill. T2 subagent families also demonstrate powerful architectural strategy: unbundling the agents monolithic cognitive loop (PerceivePlanActReflect) into specialized, independently trainable submodules: Optimizing Perception (Agentic Searchers): Systems like s3, DynamicRAG, and QAgent train search subagents to decide what to query, where to search, and when to stop [27]. Optimizing Reflection (Memory Construction): Subagents such as Mem-α learn memory-writing policies via RL, rewarded based on whether stored experiences improve future performance for the frozen generator. Optimizing Planning (Meta-Cognitive Planners): Subagents like AI-Search Planner and AgentFlow decide how tools and specialists are deployed. AgentFlow [51] trains only lightweight planner that orchestrates frozen specialists using trajectory-level rewards, achieving 33.1% on GAIA and surpassing 200B-parameter GPT-4. T2 thus achieves high system-level flexibility: new T2 subagents can be trained and attached incrementally (e.g., better planner, domain-specific searcher, new memory module), without retraining the host agent. Compared to T1, T2 trades some agent-agnosticity for tighter compatibility: tools are specialized for given frozen agent, leading to higher data efficiency and better end-to-end performance under the same backbone."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Table 5 Quantitative comparison of flagship adaptation methods. Method Paradigm Training Signal Key Result (Quantitative) Key Insight (Qualitative) DeepRetrieval [21] A1 A2 ReSearch [108] T2 s3 [27] T2 AgentFlow [51] Retrieval Metrics Final Answer Correctness GBR from Frozen Generator Final Answer Correctness 3 Recall (65.1% vs. 24.7%) Causal RL optimizes tool mechanics. 922% gains over RAG 58.9% Acc. w/ 2.4k samples 33.1% on GAIA (beats GPT-4) Holistic RL optimizes tool strategy. Much more data-efficient than A2. Learned orchestration of specialists."
        },
        {
            "title": "6.4 Synthesis: The Showdown on Data Efficiency and Modularity (A2 vs. T2)",
            "content": "The most direct comparison arises between A2 and T2. Both aim to produce sophisticated tool-using systems, but they place the learning burden in different places. A2 adapts the agent, letting it internalize tool-use strategies; T2 adapts the tools, letting them learn to support fixed agent. Empirically, the contrast is stark. Comparing Search-R1 (A2) and s3 (T2), two state-of-the-art methods for retrieval-augmented generation: A2 approach (Search-R1): Trains the entire agent, requiring roughly 170k examples to co-adapt internal knowledge, reasoning, and tool-use policy [108]. T2 approach (s3): Trains only lightweight 7B searcher subagent using frozen-generator feedback (GBR), achieving comparable performance (58.9% average accuracy) with only 2.4k training samples [27]. This corresponds to about 70 reduction in data requirements and roughly 33 faster wall-clock training for the T2 varianta phase change rather than marginal improvement. Moreover, s3 generalizes better: on specialized medical QA, T2-trained s3 reaches 76.6% accuracy vs. A2-trained Search-R1s 71.8%, suggesting that s3 learned more transferable search skills while Search-R1 overfit to its training distribution [27]. The underlying reason is the symbiotic inversion discussed above. A2s optimization landscape is high-dimensional and entangled: the agent must simultaneously adjust its knowledge, reasoning style, and tool-use policy. T2 dramatically simplifies the learning problem by assuming the backbone already solves (most of) knowledge and reasoning, and only learning narrow procedural skill in small subagent. From an engineering perspective, T2 also wins on modularity. To add new tool or update Search-R1 (A2), one must retrain the monolithic agent, potentially inducing catastrophic forgetting. In T2 architecture, new tools can be trained and hot-swapped without touching the host agent, enabling continuous evolution of the peripheral ecosystem while the core remains stable."
        },
        {
            "title": "6.5 Strategic Recommendations",
            "content": "Choosing the appropriate adaptation strategy requires balancing computational cost, data efficiency, and the need for system modularity. We organize these considerations into strategic framework based on whether the priority lies in internal parametric adjustment or external ecosystem evolution. We detail the specific applicability, strengths, and limitations of each paradigm below: A1: Best suited for local, mechanistic mastery of verifiable tools in stable domains (e.g., retrieval, code execution, SQL). By optimizing directly on executable outcomes, A1 develops strong low-level competence and causal grounding. Pros: precise control over tool behavior; robust alignment to verifiable signals. Cons: high computational cost, brittle generalization, and limited transferability across tasks. A2: Appropriate for system-level orchestration within single agent, enabling holistic reasoning and multi-tool coordination. A2 internalizes when, how, and why to invoke tools, yielding deeply integrated reasoning patterns. Pros: rich cross-tool strategies; unified end-to-end policies for complex workflows."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Cons: expensive monolithic retraining and susceptibility to catastrophic forgetting when scaling across domains. T1: Ideal for horizontal scalability and reusability. T1 spans both static foundational models (e.g., SAM, AlphaFold2) and graduated subagentsA1/A2-trained experts frozen and redeployed as reusable modules (e.g., DeepRetrieval, SWE-Grep [193]). These subagents-as-tools encapsulate learned procedural expertise while remaining decoupled from any specific host agent. Pros: plug-and-play modularity and broad compositional flexibility across ecosystems. Cons: agent-agnostic training may under-optimize tools for any particular host agents reasoning style. T2: Represents the symbiotic inversion: rather than adapting the agent to use tools better, T2 trains lightweight tools and subagents under frozen-agent supervision to better serve fixed backbone (e.g., s3-style searchers, planners, advisors, and memory builders). The host agent provides high-level reasoning and reward signals, while T2 subagents learn narrow procedural skills that can be added, replaced, or composed without touching the backbone. Pros: dramatic data-efficiency for new skills, mitigation of catastrophic forgetting via modular updates, and sustained compatibility with evolving or swapped host agents. Cons: subagent capability is bounded by the supervising agents quality; multi-subagent pipelines introduce added orchestration complexity and potential error compounding. Taken together, these four paradigms define coherent 2 2 landscape (Figure 7) along two conceptual axes: (i) the localtosystemic spectrum (y-axis), from low-level control of specific tools (A1/T1) to holistic orchestration of multi-tool reasoning (A2/T2); and (ii) the monolithictomodular spectrum (x-axis), from end-to-end retraining of single agent (A1/A2) to compositional adaptation via distributed subagents and tools (T1/T2). Viewed through this lens, A1 and A2 occupy the agent-centric half of the landscape: they directly reshape the policy parameters of the core agent, offering rich parametric flexibility but incurring heavy costs in compute, data, and stability. T1 and T2, by contrast, occupy the tool-centric half : they shift learning outward into modular ecosystem, enabling incremental evolution, specialization, and compositional reuse. The two axes interact nonlinearly: A1 T1 reflects the graduation path (frozen experts becoming reusable subagents), while A2 T2 embodies the federation path (frozen backbones supervising growing constellation of adaptive specialists). In practice, mature agentic architectures increasingly inhabit the upper-right quadrant (T2): high modularity and high orchestration, where foundation agents serve as stable cognitive centers and peripheral subagents continuously evolve to extend their capabilities. Figure 7 The 2 2 adaptation landscape. The x-axis captures monolithic-to-modular evolution, while the y-axis represents local-to-systemic orchestration. A1/A2 inhabit the agentcentric half, whereas T1/T2 embody modular and system-level flexibility. Dotted arrows show that A1/A2-trained agents can graduate as tools for T1. This synthesis also clarifies the emerging division of labor in agentic AI research. A1/A2 remain indispensable for generating novel reasoning competencies or re-aligning models internal cognition - tasks that require touching the agents core. T1/T2, however, dominate system construction: they enable continual growth, fine-grained specialization, and safe parallel experimentation. The prevailing design trend thus points toward hybrid systems: frozen foundation models at the center, surrounded by living ecology of T1/T2 subagents trained for specific procedural roles, with occasional A1/A2 updates marking evolutionary leaps in the agents internal reasoning."
        },
        {
            "title": "7 Applications",
            "content": "The rapid advancement of agentic AI systems has led to their adoption across growing range of scientific and engineering domains. In this section, we organize representative applications by discipline. Specifically, we categorize these applications into the following major areas: General Science, such as Deep Research (7.1); Computer Science, where agents augment or automate processes in Software Development (7.2) and Computer Use (7.3); and Biomedicine, where agents accelerate research in drug discovery and development (7.4)."
        },
        {
            "title": "7.1 Deep Research",
            "content": "Deep research systems represent the emerging class of AI-powered applications designed to automate end-toend scientific investigation by integrating large language models (LLMs), advanced retrieval, and autonomous reasoning [6]. OpenAIs DeepResearch [194] is prominent example, featuring multi-step reasoning workflow that conducts iterative search, validation, and synthesis. Similar paradigms have been adopted in recently announced systems such as Claudes deep-search capabilities [195] and Googles Gemini-based research agents [196]. The defining distinction of deep research systems, compared to general-purpose AI agents, is their dual adaptation in both agent reasoning and scientific tool integration. Agent adaptation Deep research systems require sophisticated agentic workflows capable of decomposing complex scientific questions into structured research plans. This includes: (1) adapting underlying LLMs toward long-context reasoning, hypothesis refinement, and multi-step self-critique, (2) orchestrating multiple agents to collaborate hierarchically, e.g., for literature review, data interpretation, and conclusion synthesis, and (3) maintaining persistent memory and knowledge tracking across long investigative trajectories. These adaptations enable agents not only to respond to queries but to behave as autonomous researchers navigating the breadth of scientific knowledge. Tool adaptation Reliable research requires grounded evidence. To address hallucination and improve informativeness, deep research agents must incorporate diverse tools that provide direct access to external knowledge, including: (1) structured retrieval interfaces to literature databases (e.g., PubMed, arXiv), (2) web navigation tools for interacting with scientific resources, (3) and modular computational utilities for data analysis and visualization. Recent advances further enhance tool adaptation through learning-based retrieval modules, such as DeepRetrieval [21] and s3 [27], which boost accuracy in real-time information gathering, especially when operating atop proprietary models that cannot be fine-tuned. Toward domain-specialized deep research While current systems are primarily built on generic corpora and may struggle with nuanced expert-level inquiries, the paradigm naturally extends to specialized scientific fields. Future development will increasingly involve integrating: domain knowledge bases and ontologies, validated bioinformatics and biomedical computation tools, field-specific safety, reliability, and evaluation protocols. Such advancements are expected to transform deep research systems from broad knowledge navigators into expert collaborators for vertical domains like medicine, materials science, and drug development, guiding researchers from problem conception to actionable discoveries."
        },
        {
            "title": "7.2 Software Development",
            "content": "AI-assisted software development represents one of the most technically demanding and economically significant domains for agentic AI systems. Unlike conventional code completion systems, software development agents are designed to autonomously navigate multi-stage engineering workflows, including requirement interpretation, code generation, debugging, testing, and deployment, within real development environments. Modern systems, including Cursor [197], Claude Code [198], and OpenAIs CodeX [199], exemplify this shift from passive code assistants toward interactive, full-cycle programming agents capable of understanding project context and performing tool-mediated reasoning. To evaluate these capabilities, the SWE-Bench benchmark [200] has emerged as representative testing suite that measures an agents ability to autonomously fix real-world software bugs in open-source repositories by reading, editing, and validating code through continuous integration workflows."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "Figure 8 Applications of Adaptation in Agentic AI. Other notable research efforts have also explored the development of autonomous software engineering agents. SWEAgent [201] introduces an agent-computer interface (ACI) that enables language model agents to autonomously perform end-to-end software engineering tasks, including repository navigation, code modification, and test execution. OpenHands [202], an open-source platform for AI software developers, extends this paradigm by providing sandboxed execution environment and modular evaluation framework for developing and benchmarking generalpurpose coding agents. Building effective software agents in this setting requires both agent adaptation, which strengthens reasoning, planning, and self-verification across complex development pipelines, and tool adaptation, which integrates and evolves the surrounding development ecosystem such as compilers, debuggers, and test frameworks. Agent adaptation Agent adaptation in software development focuses on enhancing model reasoning and autonomy across complex multi-stage engineering workflows. Recent systems train agents directly from interaction trajectories within real or simulated development environments. Tool adaptation Tool adaptation in this domain involves evolving the software ecosystem itself to improve the reliability, responsiveness, and contextual integration of tools that agents depend on for code execution, testing, and evaluation. Instead of merely wrapping existing IDE functionalities, modern systems are increasingly training tools for agents to optimize for usability and feedback efficiency. representative example is Cursors TabRL framework [203], which applies reinforcement learning to refine the editors tab completion behavior based on real-world user interactions, aligning the tools interface dynamics with agent and developer preferences. more advanced example is SWE-Grep [193], specialized sub-agent trained using reinforcement learning for fast, multi-turn, and highly parallel context retrieval. By delegating code search to this T2-style tool, the main agents context window is conserved and protected from irrelevant context pollution, allowing it to focus on higher-level reasoning. More broadly, this category of tool adaptation includes the automated creation or refinement of compilers, debuggers, and linters that provide structured feedback loops for agents."
        },
        {
            "title": "7.3 Computer Use",
            "content": "Computer-use agents represent an emerging class of multimodal AI systems capable of autonomously operating computers and software environments through direct interaction with graphical user interfaces (GUIs). Rather than relying on predefined APIs or task-specific integrations, these agents perceive screens as visual input, reason about interface elements such as buttons, menus, and text fields, and execute actions using virtual keyboard and mouseclosely mirroring human computer operation. recent example is OpenAIs Computer-Using Agent (CUA) [204], which combines vision-based perception with reinforcement learning to navigate complex digital environments. This paradigm signifies step toward generalized digital intelligence, where agents can perform diverse tasks, such as information retrieval, document editing, and software automation, directly within existing human-designed computing ecosystems. Representative benchmarks for this paradigm include OSWorld [205], WebArena [206], VisualWebArena [207],"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "AppWorld [208], WebVoyager [209], and τ-bench [210] which evaluate an agents ability to perceive, reason, and act across diverse digital environments ranging from full operating systems to real-world web interfaces. Achieving reliable and efficient performance in computer-use scenarios requires adaptation from the agent and tool levels. Agent adaptation To handle complex computer-use tasks, agent adaptation plays crucial role in equipping models with new knowledge and operational skills beyond those learned from general-purpose pretraining. Such adaptation often involves exposing the agent to realistic or synthesized trajectories of GUI-based interactions, enabling it to acquire procedural competence in perceiving interface states, reasoning over visual elements, and executing multi-step actions. representative example is OpenCUA [211], which shows how large-scale, GUIcentric data can significantly improve an agents computer-use abilities. By collecting human demonstrations across diverse operating systems and applications, and converting them into stateaction trajectories with reflective reasoning, OpenCUA provides agents with realistic exposure to interface dynamics. Another approach to agent adaptation is explored in AgentTrek [212], which takes different approach to agent adaptation by synthesizing training trajectories from web tutorials instead of relying on human demonstrations. It converts tutorial text into step-by-step goals and has VLM agent execute them in real environments, keeping only correct trajectories through automatic evaluation. This scalable data generation helps agents acquire new skills and interface patterns at low cost, showing that synthesized trajectories can effectively support GUI-agent adaptation. Tool adaptation Complementary to agent-level adaptation, tool adaptation aims to enhance the tools and interfaces that agents rely on, enabling them to become more adaptive, context-aware, and synergistic with the agents reasoning process. Instead of modifying model parameters, these approaches update or expand the tools experience pool, memory, or contextual representations to better support long-horizon interaction and dynamic task requirements. representative example is Agentic Context Engineering (ACE) [213], which treats evolving contexts as structured playbooks that accumulate, refine, and organize strategies for tool use. By continuously curating and updating contextual knowledge through execution feedback, ACE effectively adapts the operational layer of toolsreducing rollout latency and improving alignment with the agents decision-making. Such approaches highlight broader trend: as agents become more capable, the tools they employ must likewise evolve, incorporating persistent memory and adaptive control mechanisms to ensure seamless collaboration in open computer-use environments."
        },
        {
            "title": "7.4 Drug Discovery and Development",
            "content": "LLM-empowered AI agents are rapidly transforming biomedical research and, by extension, the entire drug discovery and development pipeline [5]. Modern systems increasingly integrate both agent adaptation (e.g., fine-tuning LLMs and designing agentic workflows) and tool adaptation (e.g., incorporating domain-specific databases, scientific software, and retrieval components) [8]. These two forms of adaptation are fundamentally complementary: agent adaptation improves reasoning and procedural reliability, whereas tool adaptation equips agents with practical scientific capabilities. Below, we describe representative advances that emphasize one aspect more than the other, while acknowledging that most systems combine both. Agent adaptation for drug discovery GeneAgent adapts LLM agents to gene analysis tasks (e.g., gene set enrichment analysis), integrating structured workflows such as generation, self-verification, and iterative refinement to reduce hallucinations [214]. DSWizard focuses on transparent and reproducible biomedical data science, guiding the agent to construct analysis plans before execution and enabling human oversight and modification [215]. Further, multi-agent systems have emerged where heterogeneous agents collaborate in drug discovery workflows. For instance, virtual teams can simulate interdisciplinary research meetings to design novel therapeutic molecules such as nanobodies [216]. Agent adaptation for drug development Clinical research is central component of drug development, and agents are increasingly tailored to literature analysis, patient recruitment, and trial design. TrialMind adapts LLM agents for biomedical evidence retrieval by integrating medical guidelines and structured access to clinical-trial and publication databases to support search, screening, and data extraction [7]. LEADS builds on this by further training the model with curated literature corpora to improve agent-driven evidence discovery [217]. Similarly,"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "TrialGPT operationalizes guideline-based patient-to-trial matching through reason-and-retrieve workflow [12]. For upstream design tasks, TrialGenie leverages multi-agent collaboration to parse historical trial documents and generate analytical code for real-world datasets [218]. Tool adaptation Tool adaptation has progressed equally rapidly, driven by the need to support diverse scientific tasks. Tools such as SyntheMol and related frameworks integrate ML-based molecular property predictors as reward functions to steer generative models toward biologically desirable compounds [219, 220]. ToolUniverse focuses on scalable scientific tool creation: discovery module constructs tools from natural language specifications, and an optimizer iteratively refines them before incorporation into shared library for custom agent assembly [221]. Biomni takes complementary approach by manually mining biomedical literature to curate high-quality tool repository, which can be dynamically injected into agent workflows [222]. Meanwhile, STELLA proposes self-evolving paradigm where an expanding Template Library captures reasoning strategies and Tool Ocean continuously grows as tool-creation agent autonomously discovers and integrates new bioinformatics utilities [223]."
        },
        {
            "title": "8 Opportunities",
            "content": "This paper has systematically categorized the landscape of agentic AI adaptation into four key paradigms: (A1) Agent Adaptation with Tool Execution Signal, (A2) Agent Adaptation with Agent Output Signal, (T1) Agent-Agnostic Tool Adaptation, and (T2) Agent-Supervised Tool Adaptation. These paradigms provide crucial framework for organizing and understanding current methods. However, their true value lies in illuminating the path forward. The separation of agent and tool adaptation, while analytically useful, is largely construct of the fields nascent stage; the future of capable, robust, and efficient agentic AI will almost certainly be defined by their synthesis. This section functions as forward-looking roadmap, identifying some critical and interdependent opportunities for future research that emerge directly from our taxonomy. These opportunities represent the next frontier of agentic AI, moving from static, monolithic adaptation toward dynamic, co-adaptive, and federated systems."
        },
        {
            "title": "8.1 Co-Adaptation",
            "content": "The taxonomy presented in this paper (A1/A2 vs. T1/T2) is necessary simplification, organizing the field by its dominant locus of optimization: either the agent or its tools. The most significant and challenging opportunity for the next decade of research is to dissolve this boundary and develop unified agent-tool co-adaptation frameworks. Such framework implies complex, bi-level optimization problem, formally maxA,T O(A, ), where the agents policy (A) and the tools internal parameters (T ) are adapted simultaneously within the same learning loop. This represents fundamental departure from current paradigms, which almost universally rely on freezing one component to provide stable learning target for the other (e.g., Afrozen in T2, or Tfrozen in A1/A2). This is not an entirely new problem, and researchers can draw deep conceptual inspiration from several established fields: Co-evolutionary Algorithms. Classic work in evolutionary computation has long studied how two or more interacting populationssuch as hosts vs. parasites or predators vs. preyapply reciprocal selection pressures that drive arms races, emergent structure, and increasingly sophisticated strategies. Hillis [224] introduced the seminal hostparasite model, showing that co-evolving adversarial test cases can significantly improve solution robustness. Subsequent work on competitive co-evolution explored dynamics such as disengagement, cycling, and evolutionary complexification [225]. Other lines of research developed cooperative multi-population architectures in which sub-components co-adapt to form joint solutions [226]. Comprehensive surveys [227] situate these approaches within broader taxonomy of competitive and cooperative CEAs. In our setting, we can view the agent and its tool as two interdependent populations evolving on shared fitness landscape, allowing reciprocal adaptation, arms-race dynamics, and emergent specialization to arise naturally from their interaction. Multi-Agent Systems. complementary line of work emerges from multi-agent reinforcement learning, where each agent learns in non-stationary environment induced by other concurrently learning agents. Foundational surveys [228230] describe how decentralized learners must cope with shifting policies, partial observability,"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "and strategic coupling, challenges that closely mirror those of agenttool co-adaptation. Classic problems such as equilibrium selection, credit assignment, and coordination under changing partner behaviors have led to techniques including opponent modeling, joint-policy search, centralized training with decentralized execution (CTDE), and communication-based coordination. In our context, viewing and as two-agent partially cooperative system highlights the need for algorithms that stabilize learning under mutual adaptation, prevent non-stationarity-induced divergence, and support the emergence of complementary capabilities rather than competitive oscillations. primary technical barrier to effective co-adaptation is the intractable credit assignment problem. When an agentic system fails at complex task, the source of the failure is inherently ambiguous. Consider system in which an A2-style planner invokes T2-style search subagent (e.g., an S3-like searcher). If the final answer is incorrect, which component is responsible? Nascent research is beginning to address fragments of this joint-optimization challenge. MATPO (Multi-Agent Tool-Integrated Policy Optimization) [231] proposes principled credit assignment mechanism for jointly training planner and worker agents. However, in its current form, these agents correspond to distinct prompt roles instantiated within single LLM, rather than heterogeneous models. Other work studies joint refinement of agent prompts and tool specifications [232]. The true opportunity lies in extending these ideas to distributed, heterogeneous systems in which the agent and tools are distinct learning entities. This may require importing architectures from multi-agent RL (such as centralized-criticdecentralized-actor methods [233]) to enable principled credit allocation over an interconnected agenttool graph. deeper difficulty involves the StabilityPlasticity Dilemma. Coadaptation aims for and to become mutually optimized, yet in joint-learning framework, is adapting to that is itself changing. As established in the study of complex adaptive systems, such non-stationarity can induce chaotic or unstable dynamics [234]. The system may enter Red Queen regime in which and continually adjust to each others most recent changes without increasing overall performance, or may even collapse into degenerate policies. Conversely, premature convergence may cause the system to lock in brittle, suboptimal agenttool interface, losing the plasticity required for generalization. key research direction is the development of pacemaker mechanisms that regulate the relative learning rates of agents and tools, or the use of evolutionary game-theoretic analyses to guarantee convergence toward stable symbiotic equilibria [234]. These mechanisms will be essential for enabling reliable, scalable co-adaptation in next-generation agentic AI systems."
        },
        {
            "title": "8.2 Continual Adaptation",
            "content": "Figure 9 An illustrative example of coadaptation. While our discussion has so far centered on agent adaptation mechanisms such as A1 and A2, these methods still assume fixed task distribution and are typically instantiated on single downstream task at time. In contrast, real-world deployments involve non-stationary task distributions, where tasks, tools, and user needs evolve over time, making isolated, one-off adaptations prone to Catastrophic Forgetting (CF). This calls for Self-Evolving Agents that continuously update their behaviors, tools, and memories in open and dynamic environments. Continual Learning (CL) [235238] provides natural foundation for this goal, as it studies how models learn from non-stationary task streams while retaining prior knowledge. We therefore revisit CL techniques that can serve as concrete mechanisms for Self-Evolving Agents and organize them into the following two categories. Parameter-update Mechanisms. (Dynamic A1/A2 Paradigm). To align with the A1/A2 paradigm, we group continual learning methods that adapt models through explicit parameter updates. Regularization-based CL approaches such as EWC [239], LwF [240], and VR-MCL [241] estimate which parameters are important for previous tasks"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "and selectively protect them, so that adaptation to new tasks is primarily absorbed by parameters deemed less critical for past performance. Orthogonal-update methods [242, 243] instead modify gradients so that updates lie in directions that are intended to interfere less with previously learned solutions. complementary line of work introduces parameter-efficient update mechanisms, such as low-rank adapters [244, 245], Mixture-of-Experts routing [246, 247], and model-merging schemes [248]. These methods offer concrete inspirations for dynamic A1/A2-style adaptation. External-memory Mechanisms (Evolving T2 Adaptation). Classic replay-style approaches maintain memory buffer of past examples and study how to select [249, 250], utilize [251, 252], and compress them [253, 254] so that small set of stored items can approximate the full training history. Dual-memory systems [255] further separate fast, high-capacity but unstable episodic buffers from slower, more compact long-term memories. For Self-Evolving Agents, these ideas directly inspire how to curate, compress, and stage interaction logs, tool traces, and user feedback into different memory tiers. In foundation model settings, prompts often act as lightweight external memory, because the backbone is typically kept fixed and adaptation occurs primarily through prompt changes [256258]. As result, the overall paradigm naturally aligns with our notion of T2 adaptation. Notably, this challenge of continual adaptation becomes especially pronounced in domains with strong executionbased supervision signals, such as those enabled by reinforcement learning with verifiable rewards (RLVR). As discussed in 4.1.2, environments like formal theorem proving provide reliable, tool-executionsignaled feedback for learning multi-step behaviors. At the same time, these domains often evolve structurally over time for example, through expanding formal math libraries and large, actively maintained formalization projects making them representative testbeds for continual agent adaptation. Instead of repeatedly retraining the core agent, many prover agent systems adapt to expanding libraries by updating premise retrieval indices, tactic databases, or proof-state memories, allowing agents to exploit newly introduced lemmas without rewriting the entire policy [73, 259]. Such low-resource adaptation complements RLVR-style training by isolating long-term knowledge growth from short-term policy optimization, and again aligns with our notion of T2 adaptation. Figure 10 An illustrative example of continual adaptation. Taken together, these two lines of work highlight complementary trade-offs for building Self-Evolving Agents. Within the dynamic A1/A2 paradigm, recent results [260] show that not all parameter-update schemes forget equally: RL with reverse-KL objective and on-policy data can achieve comparable or better performance than SFT while exhibiting substantially less forgetting, suggesting that on-policy data streams can act as an intrinsic CL mechanism for continual agent adaptation. Yet such methods still rewrite shared set of parameters, so forgetting and interference are mitigated rather than structurally removed. The evolving T2 paradigm tackles CF at the architectural level by freezing the core agent and encapsulating new capabilities in external, independently trained tools or subagents, which avoids the clobbering of monolithic parameter space. Looking ahead, promising direction is to systematically integrate these two perspectives, using CL-aware parameter updates where they are most effective while shifting as much long-term adaptation as possible into T2-style modular tools and external memories."
        },
        {
            "title": "8.3 Safe Adaptation",
            "content": "The transition from static foundation models to adaptive agentic systems marks fundamental inflection point in AI safety. While traditional safety paradigms focus on the alignment of frozen weights, adaptation mechanisms, specifically on-policy optimization (A1) and outcome-driven tool tuning (T2), introduce dynamic threat vectors characterized by autonomous risk-taking and adversarial co-evolution [261]. We categorize these emerging risks into"
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "two primary failure modes: Unsafe Exploration, arising from stochastic trial-and-error, and Parasitic Adaptation, arising from exploitative optimization loops. Security Risk I: Unsafe Exploration. Unsafe exploration represents the primary bottleneck for the A1 paradigm. When agents employ on-policy RL to master tools [262, 21], they must deviate from known safe trajectories to probe the state-action space. In high-stakes or partially observable environments, this decoupling of competence from safety leads to catastrophic, often irreversible outcomes [263, 264]. The Reward-Safety Gap: In frameworks like RLEF [262] or DeepRetrieval [21], rewards are typically sparse and binary (e.g., task completion). This creates feedback vacuum for intermediate actions, encouraging agents to maximize efficacy regardless of collateral damage (e.g., deleting system files to free space) [265]. Irreversibility in Tool Use: Unlike simulated games, agentic environments such as Bash terminals or cloud infrastructure possess irreversible state transitions. An agent learning via trial-and-error may trigger API calls or data deletions that cannot be undone by resetting the episode [266, 267]. Erosion of Guardrails (Case Study: DeepSeek-R1): Empirical analysis of DeepSeek-R1 [24] reveals that aggressive RL optimization for reasoning can erode safety guardrails established during SFT. The models ability to construct complex Chain-of-Thought justifications allows it to reason its way around refusal mechanisms, increasing susceptibility to jailbreaks and malicious compliance compared to non-adapted baselines [268, 269]. Security Risk II: Parasitic Adaptation. Parasitic adaptation refers to the emergence of exploitative relationships where the agent or tool maximizes its reward function at the expense of the systems intent, mirroring biological host-parasite co-evolution [270]. Type A: Specification Gaming (The Agent as Parasite): In A2 paradigms, agents exploit imperfect proxy rewards (Goodharts Law) [271]. As reasoning capabilities scale, agents become adept at hacking the evaluation process. For example, modifying game logs to falsify wins or overwriting reward functions in the file system rather than solving the task [265, 272]. Type B: Adversarial Tooling (The Tool as Parasite): In T2 ecosystems utilizing protocols like MCP [129], tools can evolve to exploit the agent. compromised or parasitic tool may return prompt-injected data that hijacks the agents reasoning (the Confused Deputy problem), forcing the agent to exfiltrate sensitive data under the guise of standard tool use [273, 274]. Type C: Sycophancy Loops: Co-adaptation can lead to degenerate equilibria where tools learn to confirm an agents hallucinations to maximize acceptance scores, or where agents and red-teaming tools engage in Red Queen dynamics, overfitting to each others artifacts without achieving general robustness [275]. Mitigation Strategies. Addressing these risks requires moving beyond scalar rewards toward robust specification. straightforward yet effective mitigation for Security Risk 1 is to introduce safety-check layer before the agents input reaches the tool (Figure 11). This gate ensures that any anomalous or unsafe behaviors are intercepted and filtered out prior to execution. More sophisticated solutions include: Constrained Policy Optimization [276278] and safety shields project agent actions onto verified safe sets to prevent catastrophic exploration. Verifiable Rewards [279, 58] replace opaque preference models with programmatic outcome verification (e.g., unit tests, proofs) to reduce sycophancy. Specification Self-Correction [280] allows agents to dynamically critique and refine reward functions at inference time to detect gaming. Finally, Proof-of-Use [281] Figure 11 An illustrative example of safe adaptation."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "frameworks enforce causal links between retrieved evidence and generated answers, preventing tool-use hallucination."
        },
        {
            "title": "8.4 Efficient Adaptation",
            "content": "Efficient adaptation in agentic AI presents several important opportunities. Current systems typically rely on largescale GPU clusters for fine-tuning or policy refinement, which limits accessibility and personalization. Shifting adaptation toward resource-constrained settings could enable agents to learn efficiently on mobile devices, edge hardware, or other low-power environments. This would not only broaden the applicability of agentic AI but also allow continual adaptation directly on user devices while preserving privacy. Moreover, learning close to the interactions that generate the training signal could reduce latency between experience and model update, effectively blurring the distinction between inference and training. Within this context, we identify several concrete opportunities: Parameter-Efficient Adaptation: Techniques such as Low-Rank Adaptation (LoRA) [19] and its extensions [282 288] allow large models to adapt to new tasks by updating only small subset of weights, significantly reducing memory and computational requirements. Recent work LoRA Without Regrets [289] empirically demonstrates that LoRA can be effectively applied in reinforcement learning settings. They provide evidence that LoRA performs equivalently to full fine-tuning even at small ranks in RL tasks, indicating that RL often requires very low parameter capacity. This observation suggests that models can be fine-tuned on resource-constrained devices while maintaining strong RL performance. Figure 12 shows an illustrative example of this. Quantized Adaptation: FlashRL [290] introduces framework for accelerating reinforcement learning by performing rollout generation in lower numerical precision, such as INT8 or FP8, while preserving downstream performance. The key innovation lies in mitigating the rollout-training mismatch caused by quantization through truncated importance sampling (TIS), which stabilizes gradient estimation when the agents policy generating rollouts is quantized, but the training engine remains in higher precision. Empirical results demonstrate that FlashRL can achieve significant speedups in RL training without sacrificing final task performance. This work provides compelling empirical evidence that quantization, technique widely employed in supervised model inference, can be effectively extended to agentic reinforcement learning. By enabling rollout generation at reduced numerical precision without degrading downstream performance, FlashRL demonstrates the potential for RL training across large-scale and resource-constrained environments. On-Device & Personalized Adaptation: On-device adaptation [291 297, 118] focuses on enabling agents to learn and update directly on user devices under tight computational and memory constraints. This capability has become increasingly important as modern agents operate across heterogeneous hardware, operating systems, interface designs, and interaction contexts, all of which introduce substantial variation in user behavior, application semantics, and device-specific execution patterns. key component of on-device adaptation is personalization, which allows agents to reflect individual preferences [298, 299], maintain persistent memory [300], and adjust behavior over time [301, 302]. Recent progress in GUI agents further stresses the importance of this direction: modern GUI agents [303307, 305, 308, 309] rely on strong user-specific multimodal reasoning. promising strategy for agentic personalization is tool adaptation, where each device maintains lightweight tool module aligned with user-specific habits and interaction patterns. Instead of modifying the full model or even parameter-efficient adapters, adapting small tool module focuses directly on modeling personal preferences, recording relevant user history, and shaping dynamic behavioral adjustments. Since the tool module is fully decoupled from the base model, it can update locally without compromising global capabilities. Frequent and incremental updates of tool modules strengthen preference alignment, reinforce long-term memory, and support continual Figure 12 An illustrative example of efficient adaptation."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "behavioral adaptation. Overall, these opportunities suggest pathway toward agentic AI that is more efficient and scalable. By combining parameter-efficient fine-tuning, quantization, and on-device adaptation, future agents can evolve continuously in close alignment with user needs and environmental constraints."
        },
        {
            "title": "9 Conclusion",
            "content": "The transition from static foundation models to autonomous agentic systems marks fundamental shift in artificial intelligence, moving from passive response generation to active and multi-step problem solving. As these systems are deployed in increasingly complex and open-ended environments, the ability to adapt to refine behavior, master new tools, and align with specific tasks has become the primary driver of reliability and performance. In this paper, we have provided comprehensive roadmap of this landscape, introducing unified taxonomy that organizes adaptation strategies into four distinct paradigms based on the locus of optimization and the source of the supervision signal. Our framework reveals that the design space of agentic adaptation is defined by the tension between monolithic and modular evolution. The agent centric paradigms, A1 (Tool Execution Signaled) and A2 (Agent Output Signaled), offer high parametric flexibility, allowing models to internalize tool mechanics and complex reasoning strategies through direct environmental feedback or holistic outcome evaluation. However, these approaches often incur high computational costs and risk catastrophic forgetting. Conversely, the tool centric paradigms, T1 (Agent Agnostic) and T2 (Agent Supervised), shift the burden of adaptation to the peripheral ecosystem. By treating tools and even other graduated agents as modular and optimizing components, these paradigms enable system level flexibility and significant data efficiency. critical insight emerging from our analysis is the symbiotic inversion represented by the T2 paradigm. Rather than treating the foundation model as the object of optimization, T2 reframes it as stable source of supervision, training lightweight subagents such as searchers, planners, and memory curators to serve the frozen core. This architectural shift not only decouples skill acquisition from general reasoning but also paves the way for federated agentic systems that can evolve continuously without destabilizing the backbone model. Looking forward, the advancement of agentic AI depends on the strategic integration of these paradigms rather than their isolation. Future systems will likely leverage hybrid architecture, combining the reasoning depth of agent centric adaptation with the modular efficiency of tool centric adaptation to achieve robustness and scalability. Realizing this potential requires addressing the fundamental challenges of continual adaptation to maintain performance in dynamic streams, safe adaptation to mitigate risks such as reward hacking, and efficient adaptation to enable deployment in resource constrained environments. Ultimately, the next generation of intelligent systems will be defined not by single monolithic model, but by the principled orchestration of stable reasoning cores supported by specialized and adaptive tools."
        },
        {
            "title": "References",
            "content": "[1] Jinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu, Siwei Liu, Zihao Li, et al. comprehensive survey of self-evolving ai agents: new paradigm bridging foundation models and lifelong agentic systems. arXiv preprint arXiv:2508.07407, 2025. [2] Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, et al. Large language model agent: survey on methodology, applications and challenges. arXiv preprint arXiv:2503.21460, 2025. [3] Weikai Xu, Chengrui Huang, Shen Gao, and Shuo Shang. Llm-based agents for tool learning: survey: W. xu et al. Data Science and Engineering, pages 131, 2025. [4] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[5] Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, and Marinka Zitnik. Empowering biomedical discovery with ai agents. Cell, 187(22): 61256151, 2024. [6] Renjun Xu and Jingwen Peng. comprehensive survey of deep research: Systems, methodologies, and applications. arXiv preprint arXiv:2506.12594, 2025. [7] Zifeng Wang, Lang Cao, Benjamin Danek, Qiao Jin, Zhiyong Lu, and Jimeng Sun. Accelerating clinical evidence synthesis with large language models. npj Digital Medicine, 8(1):509, 2025. [8] Zifeng Wang, Hanyin Wang, Benjamin Danek, Ying Li, Christina Mack, Luk Arbuckle, Devyani Biswal, Hoifung Poon, Yajuan Wang, Pranav Rajpurkar, et al. perspective for adapting generalist ai to specialized medical ai applications and their challenges. NPJ Digital Medicine, 8(1):429, 2025. [9] Alex Gu, Naman Jain, Wen-Ding Li, Manish Shetty, Yijia Shao, Ziyang Li, Diyi Yang, Kevin Ellis, Koushik Sen, and Armando Solar-Lezama. Challenges and paths towards ai for software engineering, 2025. URL https: //arxiv.org/abs/2503.22625. [10] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. In ICLR, 2024. [11] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83648377, 2024. [12] Qiao Jin, Zifeng Wang, Charalampos Floudas, Fangyuan Chen, Changlin Gong, Dara Bracken-Clarke, Elisabetta Xue, Yifan Yang, Jimeng Sun, and Zhiyong Lu. Matching patients to clinical trials with large language models. Nature communications, 15(1):9074, 2024. [13] Peiyang Song, Pengrui Han, and Noah Goodman. survey on large language model reasoning failures. In 2nd AI for Math Workshop@ ICML 2025, 2025. [14] Huan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong Liu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. survey of self-evolving agents: On path to artificial super intelligence. arXiv preprint arXiv:2507.21046, 2025. [15] Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, and Kees Joost Batenburg. Agentic large language models, survey. arXiv preprint arXiv:2503.23037, 2025. [16] Zhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei Huang, Dacheng Tao, and Jingren Zhou. survey on self-evolution of large language models. arXiv preprint arXiv:2404.14387, 2024. [17] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6): 186345, 2024. [18] Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025. [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. [20] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning, 2025. URL https://arxiv.org/ abs/2410.02089. [21] Pengcheng Jiang, Jiacheng Lin, Lang Cao, R. Tian, S. Kang, Z. Wang, Jimeng Sun, and Jiawei Han. Deepretrieval: Hacking real search engines and retrievers with large language models via reinforcement learning. In The Second Conference on Language Modeling, 2025. [22] Huichi Zhou, Yihang Chen, Siyuan Guo, Xue Yan, Kin Hei Lee, Zihan Wang, Ka Yiu Lee, Guchun Zhang, Kun Shao, Linyi Yang, et al. Memento: Fine-tuning llm agents without fine-tuning llms. arXiv preprint arXiv:2508.16153, 2025. [23] Liang Wang, Nan Yang, and Furu Wei. Learning to retrieve in-context examples for large language models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 17521767, 2024."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[24] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Peiyi Wang, Qihao Zhu, Runxin Xu, Ruoyu Zhang, Shirong Ma, Xiao Bi, et al. Deepseek-r1 incentivizes reasoning in llms through reinforcement learning. Nature, 645(8081): 633638, 2025. [25] Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. [26] Lang Mei, Zhihan Yang, and Chong Chen. Ai-searchplanner: Modular agentic search via pareto-optimal multi-objective reinforcement learning. arXiv preprint arXiv:2508.20368, 2025. [27] Pengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng Xiao, Zifeng Wang, Jimeng Sun, and Jiawei Han. s3: You dont need that much data to train search agent via rl. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, 2025. [28] Zichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu. Augmentation-adapted retriever improves generalization of language models as generic plug-in. In The 61st Annual Meeting Of The Association For Computational Linguistics, 2023. [29] Shibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. Advances in neural information processing systems, 36:4587045894, 2023. [30] Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, et al. Advances and challenges in foundation agents: From brain-inspired intelligence to evolutionary, collaborative, and safe systems. arXiv preprint arXiv:2504.01990, 2025. [31] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [32] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in neural information processing systems, 36:1180911822, 2023. [33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. [34] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. [35] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. Tool learning with foundation models. In ACM Computing Surveys, 2024. [36] Yaxiong Wu, Sheng Liang, Chen Zhang, Yichao Wang, Yongyue Zhang, Huifeng Guo, Ruiming Tang, and Yong Liu. From human memory to ai memory: survey on memory mechanisms in the era of llms. arXiv preprint arXiv:2504.15965, 2025. [37] Guohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large language model society. Advances in Neural Information Processing Systems, 36: 5199152008, 2023. [38] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. [39] Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for multi-agent collaborative framework. In The Twelfth International Conference on Learning Representations, 2023. [40] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1517415186, 2024."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[41] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. systematic survey of prompt engineering in large language models: Techniques and applications. arXiv preprint arXiv:2402.07927, 2024. [42] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient fine-tuning for large ISSN 2835-8856. URL models: comprehensive survey. Transactions on Machine Learning Research, 2024. https://openreview.net/forum?id=lIsCS8b6zj. [43] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=gEZrGCozdqR. [44] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [45] Wenyi Xiao, Zechuan Wang, Leilei Gan, Shuai Zhao, Zongrui Li, Ruirui Lei, Wanggui He, Luu Anh Tuan, Long Chen, Hao Jiang, et al. comprehensive survey of direct preference optimization: Datasets, theories, variants, and applications. arXiv preprint arXiv:2410.15595, 2024. [46] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [48] Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model-based agents. ACM Transactions on Information Systems, 43(6):147, 2025. [49] Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. In The Second Conference on Language Modeling, 2025. [50] Jiazhan Feng, Shijue Huang, Xingwei Qu, Ge Zhang, Yujia Qin, Baoquan Zhong, Chengquan Jiang, Jinxin Chi, and Wanjun Zhong. Retool: Reinforcement learning for strategic tool use in llms. arXiv preprint arXiv:2504.11536, 2025. [51] Zhuofeng Li, Haoxiang Zhang, Seungju Han, Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, and Pan Lu. In-the-flow agentic system optimization for effective planning and tool use. arXiv preprint arXiv:2510.05592, 2025. [52] Shuofei Qiao, Honghao Gui, Chengfei Lv, Qianghuai Jia, Huajun Chen, and Ningyu Zhang. Making language models better tool learners with execution feedback. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35503568, 2024. [53] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301, 2023. [54] Sijia Chen, Yibo Wang, Yi-Feng Wu, Qingguo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, and Lijun Zhang. Advancing tool-augmented large language models: Integrating insights from errors in inference trees. Advances in Neural Information Processing Systems, 37:106555106581, 2024. [55] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. Advances in Neural Information Processing Systems, 37:126544126565, 2024. [56] Zezhong Wang, Xingshan Zeng, Weiwen Liu, Liangyou Li, Yasheng Wang, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. Toolflow: Boosting llm tool-calling through natural and coherent dialogue synthesis. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 42464263, 2025. [57] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. Executable code actions elicit better llm agents. In International Conference on Machine Learning, pages 5020850232. PMLR, 2024. [58] Ansong Ni, Miltiadis Allamanis, Arman Cohan, Yinlin Deng, Kensen Shi, Charles Sutton, and Pengcheng Yin. Next: teaching large language models to reason about code execution. In Proceedings of the 41st International Conference on Machine Learning, pages 3792937956, 2024."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[59] Zhengliang Shi, Shen Gao, Lingyong Yan, Yue Feng, Xiuyi Chen, Zhumin Chen, Dawei Yin, Suzan Verberne, and Zhaochun Ren. Tool learning in the wild: Empowering language models as automatic tool agents. In Proceedings of the ACM on Web Conference 2025, pages 22222237, 2025. [60] Sheryl Hsu, Omar Khattab, Chelsea Finn, and Archit Sharma. Grounding by trying: Llms with reinforcement learning-enhanced retrieval. In The Thirteenth International Conference on Learning Representations, 2024. [61] Chanwoong Yoon, Gangwoo Kim, Byeongguk Jeon, Sungdong Kim, Yohan Jo, and Jaewoo Kang. Ask optimal questions: Aligning large language models with retrievers preference in conversation. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 58995921, 2025. [62] Alan Dao and Thinh Le. Rezero: Enhancing llm search ability by trying one-more-time. arXiv preprint arXiv:2504.11001, 2025. [63] Supriti Vijay, Aman Priyanshu, Anu Vellore, Baturay Saglam, and Amin Karbasi. Think before you retrieve: Learning test-time adaptive search with small language models. arXiv preprint arXiv:2511.07581, 2025. [64] Nan Jiang, Xiaopeng Li, Shiqi Wang, Qiang Zhou, Soneya Hossain, Baishakhi Ray, Varun Kumar, Xiaofei Ma, and Anoop Deoras. Ledex: Training llms to better self-debug and explain code. Advances in Neural Information Processing Systems, 37:3551735543, 2024. [65] Jiawei Liu and Lingming Zhang. Code-r1: Reproducing r1 for code with reliable rewards. https://github.com/ ganler/code-r1, 2025. [66] Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, and Chuchu Fan. R1code-interpreter: Training llms to reason with code via supervised and reinforcement learning. arXiv preprint arXiv:2505.21668, 2025. [67] Yabo Zhang, Yihan Zeng, Qingyun Li, Zhen Hu, Kavin Han, and Wangmeng Zuo. Tool-r1: Sample-efficient reinforcement learning for agentic tool use. arXiv preprint arXiv:2509.12867, 2025. [68] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393, 2020. [69] Kaiyu Yang, Aidan Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Leandojo: Theorem proving with retrieval-augmented lanRyan Prenger, and Animashree Anandkumar. guage models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 2157321612. Curran AssoURL https://proceedings.neurips.cc/paper_files/paper/2023/file/ ciates, 4441469427094f8873d0fecb0c4e1cee-Paper-Datasets_and_Benchmarks.pdf. Inc., 2023. [70] Yong Lin, Shange Tang, Bohan Lyu, Ziran Yang, Jui-Hui Chung, Haoyu Zhao, Lai Jiang, Yihan Geng, Jiawei Ge, Jingruo Sun, et al. Goedel-prover-v2: Scaling formal theorem proving with scaffolded data synthesis and self-correction. arXiv preprint arXiv:2508.03613, 2025. [71] Huajian Xin, ZZ Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, et al. Deepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In The Thirteenth International Conference on Learning Representations, 2024. [72] Haiming Wang, Mert Unsal, Xiaohan Lin, Mantas Baksys, Junqi Liu, Marco Dos Santos, Flood Sung, Marina Vinyes, Zhenzhe Ying, Zekai Zhu, et al. Kimina-prover preview: Towards large formal reasoning models with reinforcement learning. arXiv preprint arXiv:2504.11354, 2025. [73] Luoxin Chen, Jinming Gu, Liankai Huang, Wenhao Huang, Zhicheng Jiang, Allan Jie, Xiaoran Jin, Xing Jin, Chenggang Li, Kaijing Ma, et al. Seed-prover: Deep and broad reasoning for automated theorem proving. arXiv preprint arXiv:2507.23726, 2025. [74] Peiyang Song, Kaiyu Yang, and Anima Anandkumar. Lean copilot: Large language models as copilots for theorem proving in lean, 2025. URL https://arxiv.org/abs/2404.12534. [75] George Tsoukalas, Jasper Lee, John Jennings, Jimmy Xin, Michelle Ding, Michael Jennings, Amitayush Thakur, and Swarat Chaudhuri. Putnambench: multilingual competition-mathematics benchmark for formal theorem-proving. In AI for Math Workshop@ ICML 2024, 2024. [76] Kaiyu Yang, Gabriel Poesia, Jingxuan He, Wenda Li, Kristin Lauter, Swarat Chaudhuri, and Dawn Song. Formal mathematical reasoning: new frontier in ai, 2024. URL https://arxiv.org/abs/2412.16075."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[77] Thomas Hubert, Rishi Mehta, Laurent Sartran, Miklós Horváth, Goran Žužic, Eric Wieser, Aja Huang, Julian Schrittwieser, Yannick Schroecker, Hussain Masoom, et al. Olympiad-level formal mathematical reasoning with reinforcement learning. Nature, pages 13, 2025. [78] ZZ Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, et al. Deepseek-prover-v2: Advancing formal mathematical reasoning via reinforcement learning for subgoal decomposition. arXiv preprint arXiv:2504.21801, 2025. [79] Xingguang Ji, Yahui Liu, Qi Wang, Jingyuan Zhang, Yang Yue, Rui Shi, Chenxi Sun, Fuzheng Zhang, Guorui Zhou, and Kun Gai. Leanabell-prover-v2: Verifier-integrated reasoning for formal theorem proving via reinforcement learning. arXiv preprint arXiv:2507.08649, 2025. [80] Ran Xin, Zeyu Zheng, Yanchen Nie, Kun Yuan, and Xia Xiao. Scaling up multi-turn off-policy rl and multi-agent tree search for llm step-provers. arXiv preprint arXiv:2509.06493, 2025. [81] Suozhi Huang, Peiyang Song, Robert Joseph George, and Anima Anandkumar. Leanprogress: Guiding search for neural theorem proving via proof progress prediction. arXiv preprint arXiv:2502.17925, 2025. [82] Tudor Achim, Alex Best, Alberto Bietti, Kevin Der, Mathïs Fédérico, Sergei Gukov, Daniel Halpern-Leistner, Kirsten Henningsgard, Yury Kudryashov, Alexander Meiburg, Martin Michelsen, Riley Patterson, Eric Rodriguez, Laura Scharff, Vikram Shanker, Vladmir Sicca, Hari Sowrirajan, Aidan Swope, Matyas Tamas, Vlad Tenev, Jonathan Thomm, Harold Williams, and Lawrence Wu. Aristotle: Imo-level automated theorem proving, 2025. URL https: //arxiv.org/abs/2510.01346. [83] Alex Sanchez-Stern, Abhishek Varghese, Zhanna Kaufman, Dylan Zhang, Talia Ringer, and Yuriy Brun. Qedcartographer: Automating formal verification using reward-free reinforcement learning. In Proceedings of the IEEE/ACM 47th International Conference on Software Engineering, pages 307320, 2025. [84] Zijian Wu, Suozhi Huang, Zhejian Zhou, Huaiyuan Ying, Jiayu Wang, Dahua Lin, and Kai Chen. Internlm2. 5stepprover: Advancing automated theorem proving via expert iteration on large-scale lean problems. arXiv preprint arXiv:2410.15700, 2024. [85] The mathlib Community. The lean mathematical library. In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs, POPL 20, page 367381. ACM, January 2020. doi: 10.1145/3372885. 3373824. URL http://dx.doi.org/10.1145/3372885.3373824. [86] W. T. Gowers, Ben Green, Freddie Manners, and Terence Tao. On conjecture of marton, 2023. URL https: //arxiv.org/abs/2311.05762. [87] Haozhen Zhang, Tao Feng, and Jiaxuan You. Router-r1: Teaching llms multi-round routing and aggregation via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [88] Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang, et al. Feedback-driven tool-use improvements in large language models via automated build environments. arXiv preprint arXiv:2508.08791, 2025. [89] Zimu Lu, Houxing Ren, Yunqiao Yang, Ke Wang, Zhuofan Zong, Junting Pan, Mingjie Zhan, and Hongsheng Li. Webgen-agent: Enhancing interactive website generation with multi-level feedback and step-level reinforcement learning. arXiv preprint arXiv:2509.22644, 2025. [90] Fu Chen, Peng Wang, Xiyin Li, Wen Li, Shichi Lei, and Dongdong Xiang. Toolexpander: Extending the frontiers of tool-using reinforcement learning to weak llms. arXiv preprint arXiv:2510.07737, 2025. [91] Jiacheng Lin, Tian Wang, and Kun Qian. Rec-r1: Bridging generative large language models and user-centric recommendation systems via reinforcement learning. Transactions on Machine Learning Research, 2025. ISSN 2835-8856. URL https://openreview.net/forum?id=YBRU9MV2vE. [92] Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, and Jian Guo. Sql-r1: Training natural language to sql reasoning model by reinforcement learning. arXiv preprint arXiv:2504.08600, 2025. [93] Jake Poznanski, Luca Soldaini, and Kyle Lo. olmocr 2: Unit test rewards for document ocr. arXiv preprint arXiv:2510.19817, 2025. [94] Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[95] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [96] Evan Ellis, Vivek Myers, Jens Tuyls, Sergey Levine, Anca Dragan, and Benjamin Eysenbach. Training llm agents to empower humans. arXiv preprint arXiv:2510.13709, 2025. [97] Sahil Kale and Devendra Singh Dhami. Knowrl: Teaching language models to know what they know. arXiv preprint arXiv:2510.11407, 2025. [98] Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, and Jiawei Han. Grace: Generative representation learning via contrastive policy optimization. arXiv preprint arXiv:2510.04506, 2025. [99] Jiacheng Lin, Zhenbang Wu, and Jimeng Sun. Training llms for ehr-based reasoning tasks via reinforcement learning. arXiv preprint arXiv:2505.24105, 2025. [100] Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi Su, John Co-Reyes, Avi Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, Lei Zhang, Kay McKinney, Disha Shrivastava, Cosmin Paduraru, George Tucker, Doina Precup, Feryal Behbahani, and Aleksandra Faust. Training language models to self-correct via In The Thirteenth International Conference on Learning Representations, 2025. URL reinforcement learning. https://openreview.net/forum?id=CjwERcAU7w. [101] Mert Yuksekgonul, Federico Bianchi, Joseph Boen, Sheng Liu, Pan Lu, Zhi Huang, Carlos Guestrin, and James Zou. Optimizing generative ai by backpropagating language model feedback. Nature, 639(8055):609616, 2025. [102] Guowei Xu, Mert Yuksekgonul, Carlos Guestrin, and James Zou. metatextgrad: Automatically optimizing language model optimizers. arXiv preprint arXiv:2505.18524, 2025. [103] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations, 2024. [104] Yuanjie Lyu, Zihan Niu, Zheyong Xie, Chao Zhang, Tong Xu, Yang Wang, and Enhong Chen. Retrieve-plan-generation: An iterative planning and answering framework for knowledge-intensive llm generation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 46834702, 2024. [105] Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, and Jiawei Han. Ras: Retrievaland-structuring for knowledge-intensive llm generation. arXiv preprint arXiv: 2502.10996, 2025. [106] Xinyan Guan, Jiali Zeng, Fandong Meng, Chunlei Xin, Yaojie Lu, Hongyu Lin, Xianpei Han, Le Sun, and Jie Zhou. Deeprag: Thinking to retrieve step by step for large language models. arXiv preprint arXiv:2502.01142, 2025. [107] Huatong Song, Jinhao Jiang, Yingqian Min, Jie Chen, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-Rong Wen. R1-searcher: Incentivizing the search capability in llms via reinforcement learning. arXiv preprint arXiv:2503.05592, 2025. [108] Mingyang Chen, Linzhuang Sun, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, Jeff Pan, Wen Zhang, Huajun Chen, et al. Learning to reason with search for llms via reinforcement learning. arXiv preprint arXiv:2503.19470, 2025. [109] Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. [110] Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. [111] Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, and Yichao Wu. Stepsearch: Igniting llms search ability via step-wise proximal policy optimization. arXiv preprint arXiv:2505.15107, 2025. [112] Chuzhan Hao, Wenfeng Feng, Yuewei Zhang, and Hao Wang. Dynasearcher: Dynamic knowledge graph augmented search agent via multi-reward reinforcement learning. arXiv preprint arXiv:2507.17365, 2025. [113] Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, et al. Medresearcher-r1: Expert-level medical deep researcher via knowledge-informed trajectory synthesis framework. arXiv preprint arXiv:2508.14880, 2025. [114] Yaorui Shi, Shihan Li, Chang Wu, Zhiyuan Liu, Junfeng Fang, Hengxing Cai, An Zhang, and Xiang Wang. Search and refine during think: Autonomous retrieval-augmented reasoning of llms. arXiv e-prints, pages arXiv2505, 2025."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[115] Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, and Ziwei Liu. Mmsearch-r1: Incentivizing lmms to search. arXiv preprint arXiv:2506.20670, 2025. [116] Qingyao Li, Xinyi Dai, Xiangyang Li, Weinan Zhang, Yasheng Wang, Ruiming Tang, and Yong Yu. Codeprm: Execution feedback-enhanced process reward model for code generation. In Findings of the Association for Computational Linguistics: ACL 2025, pages 81698182, 2025. [117] Emre Can Acikgoz, Cheng Qian, Heng Ji, Dilek Hakkani-Tür, and Gokhan Tur. Self-improving llm agents at test-time. arXiv preprint arXiv:2510.07841, 2025. [118] Xufang Luo, Yuge Zhang, Zhiyuan He, Zilong Wang, Siyun Zhao, Dongsheng Li, Luna Qiu, and Yuqing Yang. Agent lightning: Train any ai agents with reinforcement learning. arXiv preprint arXiv:2508.03680, 2025. [119] Zi-Yi Dou, Cheng-Fu Yang, Xueqing Wu, Kai-Wei Chang, and Nanyun Peng. Re-rest: Reflection-reinforced selftraining for language agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1539415411, 2024. [120] Yifei Zhou, Sergey Levine, Jason Weston, Xian Li, and Sainbayar Sukhbaatar. Self-challenging language model agents. arXiv preprint arXiv:2506.01716, 2025. [121] Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. Agent-r: Training language model agents to reflect via iterative self-training. arXiv preprint arXiv:2501.11425, 2025. [122] Qianben Chen, Jingyi Cao, Jiayu Zhang, Tianrui Qin, Xiaowan Li, King Zhu, Dingfeng Shi, He Zhu, Minghao Liu, Xiaobo Liang, et al. A2fm: An adaptive agent foundation model for tool-aware hybrid reasoning. arXiv e-prints, pages arXiv2510, 2025. [123] Dongfu Jiang, Yi Lu, Zhuofeng Li, Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, et al. Verltool: Towards holistic agentic reinforcement learning with tool use. arXiv preprint arXiv:2509.01055, 2025. [124] Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function spaces with applications to pdes. Journal of Machine Learning Research, 24(89):197, 2023. [125] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information Processing Systems, 36:3815438180, 2023. [126] Dídac Surís, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for reasoning. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1188811898, 2023. [127] Keyan Ding, Jing Yu, Junjie Huang, Yuchen Yang, Qiang Zhang, and Huajun Chen. Scitoolagent: knowledge-graphdriven scientific agent for multitool integration. Nature Computational Science, pages 111, 2025. [128] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. [129] Anthropic. Code execution with mcp: Building more efficient ai agents, November 2025. URL https://www. anthropic.com/engineering/code-execution-with-mcp. Published Nov 04 2025. [130] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [131] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. [132] Haoxiang Wang, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Mehrdad Farajtabar, Sachin Mehta, Mohammad Rastegari, Oncel Tuzel, and Hadi Pouransari. Sam-clip: Merging vision foundation models towards semantic and spatial understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36353647, 2024. [133] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. In International conference on machine learning, pages 2849228518. PMLR, 2023. [134] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP (1), pages 67696781, 2020."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[135] Omar Khattab and Matei Zaharia. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, pages 3948, 2020. [136] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. [137] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint arXiv:2212.03533, 2022. [138] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [139] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with language model. Science, 379(6637):11231130, 2023. [140] Tian Xie and Jeffrey Grossman. Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties. Physical review letters, 120(14):145301, 2018. [141] Bohao Xu, Yingzhou Lu, Chenhao Li, Ling Yue, Xiao Wang, Nan Hao, Tianfan Fu, and Jim Chen. Smiles-mamba: Chemical mamba foundation models for drug admet prediction. arXiv preprint arXiv:2408.05696, 2024. [142] Wengong Jin, Connor Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic reaction outcomes with weisfeiler-lehman network. arXiv preprint arXiv:1709.04555, 2017. [143] Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graph translation for molecular optimization. arXiv preprint arXiv:1812.01070, 2018. [144] Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu, and Yuedong Yang. Predicting retrosynthetic reactions using self-corrected transformer neural networks. Journal of Chemical Information and Modeling, 60(1):4755, 2019. [145] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data, 2020. [146] Yin Fang, Qiang Zhang, Ningyu Zhang, Zhuo Chen, Xiang Zhuang, Xin Shao, Xiaohui Fan, and Huajun Chen. Knowledge graph-enhanced molecular contrastive learning with functional prompt. Nature Machine Intelligence, pages 112, 2023. [147] Pengcheng Jiang, Cao Xiao, Tianfan Fu, Jimeng Sun, and Jiawei Han. Bi-level contrastive learning for knowledgeenhanced molecule representations. In Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence, 2025. [148] Xiaoning Qi, Lianhe Zhao, Chenyu Tian, Yueyue Li, Zhen-Lin Chen, Peipei Huo, Runsheng Chen, Xiaodong Liu, Baoping Wan, Shengyong Yang, et al. Predicting transcriptional responses to novel chemical perturbations using deep generative model for drug discovery. Nature Communications, 15(1):9256, 2024. [149] Xiaochu Tong, Ning Qu, Xiangtai Kong, Shengkun Ni, Jingyi Zhou, Kun Wang, Lehan Zhang, Yiming Wen, Jiangshan Shi, Sulin Zhang, et al. Deep representation learning of chemical-induced transcriptional profile for phenotype-based drug discovery. Nature Communications, 15(1):5378, 2024. [150] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, and Yiqun Liu. Blade: Enhancing black-box large language models with small domain-specific models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 2442224430, 2025. [151] Haotian Sun, Yuchen Zhuang, Wei Wei, Chao Zhang, and Bo Dai. Bbox-adapter: Lightweight adapting for black-box large language models. arXiv preprint arXiv:2402.08219, 2024. [152] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah Smith. Tuning language models by proxy. In First Conference on Language Modeling, 2024. [153] Hongjin Su, Shuyang Jiang, Yuhang Lai, Haoyuan Wu, Boao Shi, Che Liu, Qian Liu, and Tao Yu. Evor: Evolving retrieval for code generation. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 25382554, 2024. [154] Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Richard James, Pedro Rodriguez, Jacob Kahn, Gergely Szilvasy, Mike Lewis, et al. Ra-dit: Retrieval-augmented dual instruction tuning. In The Twelfth International Conference on Learning Representations, 2023."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[155] Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Haotian Sun, Hang Wu, Carl Yang, and May Wang. Medadapter: Efficient test-time adaptation of large language models towards medical reasoning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing, volume 2024, page 22294, 2024. [156] Jaehyung Kim, Dongyoung Kim, and Yiming Yang. Learning to correct for qa reasoning with black-box llms. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 89168937, 2024. [157] Lingxi Zhang, Yue Yu, Kuan Wang, and Chao Zhang. Arl2: Aligning retrievers with black-box large language models via self-guided adaptive relevance labeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 37083719, 2024. [158] Daixuan Cheng, Shaohan Huang, Junyu Bi, Yuefeng Zhan, Jianfeng Liu, Yujing Wang, Hao Sun, Furu Wei, Weiwei Deng, and Qi Zhang. Uprise: Universal prompt retrieval for improving zero-shot evaluation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1231812337, 2023. [159] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Bridging the preference gap between retrievers and llms. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1043810451, 2024. [160] Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, and Jiawei Han. Dynamicrag: Leveraging outputs of large language model as feedback for dynamic reranking in retrieval-augmented generation. arXiv preprint arXiv:2505.07233, 2025. [161] Yi Jiang, Lei Shen, Lujie Niu, Sendong Zhao, Wenbo Su, and Bo Zheng. Qagent: modular search agent with interactive query understanding. arXiv preprint arXiv:2510.08383, 2025. [162] Yu Wang, Ryuichi Takanobu, Zhiqi Liang, Yuzhen Mao, Yuanzhe Hu, Julian McAuley, and Xiaojian Wu. Mem- {alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911, 2025. [163] Hong Ting Tsang, Jiaxin Bai, Haoyu Huang, Qiao Xiao, Tianshi Zheng, Baixuan Xu, Shujie Liu, and Yangqiu Song. Autograph-r1: End-to-end reinforcement learning for knowledge graph construction. arXiv preprint arXiv:2510.15339, 2025. [164] Yilin Xiao, Junnan Dong, Chuang Zhou, Su Dong, Qian-wen Zhang, Di Yin, Xing Sun, and Xiao Huang. Graphragbench: Challenging domain-specific reasoning for evaluating graph retrieval-augmented generation. arXiv preprint arXiv:2506.02404, 2025. [165] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [166] Parth Asawa, Alan Zhu, Matei Zaharia, Alexandros Dimakis, and Joseph Gonzalez. How to train your advisor: Steering black-box llms with advisor models. arXiv preprint arXiv:2510.02453, 2025. [167] ChangHao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, and Bo Dai. Matryoshka pilot: Learning to drive black-box llms with llms. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. [168] Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, and Dong Yu. R-zero: Self-evolving reasoning llm from zero data. arXiv preprint arXiv:2508.05004, 2025. [169] Yixing Chen, Yiding Wang, Siqi Zhu, Haofei Yu, Tao Feng, Muhan Zhang, Mostofa Patwary, and Jiaxuan You. Multi-agent evolve: Llm self-improve through co-evolution. arXiv preprint arXiv:2510.23595, 2025. [170] Joon Sung Park, Joseph OBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael Bernstein. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th annual acm symposium on user interface software and technology, pages 122, 2023. [171] Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir Patil, Ion Stoica, and Joseph Gonzalez. Memgpt: Towards llms as operating systems. arXiv preprint arXiv:2310.08560, 2023. [172] Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1972419731, 2024. [173] Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, and Yunsheng Wu. Memochat: Tuning llms to use memos for consistent long-range open-domain conversation. arXiv preprint arXiv:2308.08239, 2023."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[174] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Schütze. Ret-llm: Towards general read-write memory for large language models. arXiv preprint arXiv:2305.14322, 2023. [175] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343, 10, 2023. [176] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023. [177] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 1963219642, 2024. [178] Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao Feng, Le Xue, Rithesh RN, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, et al. Retroformer: Retrospective large language agents with policy gradient optimization. In The Twelfth International Conference on Learning Representations, 2023. [179] Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations, 2024. [180] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. Think-in-memory: Recalling and post-thinking enable llms with long-term memory. arXiv preprint arXiv:2311.08719, 2023. [181] Alireza Rezazadeh, Zichao Li, Wei Wei, and Yujia Bao. From isolated conversations to hierarchical schemas: Dynamic tree memory representation for llms. arXiv preprint arXiv:2410.14052, 2024. [182] Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Andrey Kravchenko, Mikhail Burtsev, and Evgeny Burnaev. Arigraph: Learning knowledge graph world models with episodic memory for llm agents. arXiv preprint arXiv:2407.04363, 2024. [183] Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. Chatdb: Augmenting llms with databases as their symbolic memory. arXiv preprint arXiv:2306.03901, 2023. [184] Xiaoxia Cheng, Zeqi Tan, Wei Xue, and Weiming Lu. Information re-organization improves reasoning in large language models. Advances in Neural Information Processing Systems, 37:130214130236, 2024. [185] Mirac Suzgun, Mert Yuksekgonul, Federico Bianchi, Dan Jurafsky, and James Zou. Dynamic cheatsheet: Test-time learning with adaptive memory. arXiv preprint arXiv:2504.07952, 2025. [186] Siru Ouyang, Jun Yan, Hsu, Yanfei Chen, Ke Jiang, Zifeng Wang, Rujun Han, Long Le, Samira Daruki, Xiangru Tang, et al. Reasoningbank: Scaling agent self-evolving with reasoning memory. arXiv preprint arXiv:2509.25140, 2025. [187] Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023. [188] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [189] Ziyang Wang, Heba Elfardy, Markus Dreyer, Kevin Small, and Mohit Bansal. Unified embeddings for multimodal retrieval via frozen llms. In Findings of the Association for Computational Linguistics: EACL 2024, pages 15371547, 2024. [190] Song Tang, Wenxin Su, Mao Ye, and Xiatian Zhu. Source-free domain adaptation with frozen multimodal foundation model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2371123720, 2024. [191] Lei Zhu, Fangyun Wei, and Yanye Lu. Beyond text: Frozen large language models in visual signal comprehension. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2704727057, 2024. [192] Kartik Sharma, Yiqiao Jin, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, and Srijan Kumar. Sysformer: Safeguarding frozen large language models with adaptive system prompts. arXiv preprint arXiv:2506.15751, 2025. [193] Ben Pan, Carlo Baronio, Albert Tam, Pietro Marsella, Mokshit Jain, Swyx, and Silas Alberti. Introducing swe-grep and swe-grep-mini: Rl for multi-turn, fast context retrieval. https://cognition.ai/blog/swe-grep, October 2025. Accessed: 2025-10-29."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[194] OpenAI. Introducing deep research. https://openai.com/index/introducing-deep-research/, 2025. [195] Anthropic. Claude takes research to new places. https://www.anthropic.com/news/research, 2025. [196] Dave Citron. Deep research is now available on gemini 2.5 pro experimental. https://blog.google/products/ gemini/deep-research-gemini-2-5-pro-experimental/, 2025. [197] Cursor. Cursor - the ai code editor. https://www.cursor.com/, 2025. Accessed: 2025-10-29. [198] Anthropic. Claude code: Deep coding at terminal velocity. https://www.anthropic.com/claude-code, 2025. Accessed: 2025-10-29. [199] OpenAI. Codex. https://openai.com/codex/, 2025. Accessed: 2025-10-29. [200] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=VTF8yNQM66. [201] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [202] Xingyao Wang, Boxuan Li, Yufan Song, Frank F. Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, Hoang H. Tran, Fuqiang Li, Ren Ma, Mingzhang Zheng, Bill Qian, Yanjun Shao, Niklas Muennighoff, Yizhe Zhang, Binyuan Hui, Junyang Lin, Robert Brennan, Hao Peng, Heng Ji, and Graham Neubig. In The Thirteenth International Openhands: An open platform for AI software developers as generalist agents. Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=OJd3ayDDoF. [203] Jacob Jackson, Phillip Kravtsov, and Shomil Jain. Improving cursor tab with online reinforcement learning. https: //cursor.com/blog/tab-rl, September 2025. Accessed: 2025-10-29. [204] OpenAI. Computer-using agent. https://openai.com/index/computer-using-agent/, January 2025. Accessed: 2025-10-28. [205] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. [206] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=oKn9c6ytLx. [207] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 881905, 2024. [208] Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: controllable world of apps and people for benchmarking interactive coding agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1602216076, 2024. [209] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. Webvoyager: Building an end-to-end web agent with large multimodal models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 68646890, 2024. [210] Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. tau-bench: benchmark for tool-agent-user interaction in real-world domains. In The Thirteenth International Conference on Learning Representations, 2025. [211] Xinyuan Wang, Bowen Wang, Dunjie Lu, Junlin Yang, Tianbao Xie, Junli Wang, Jiaqi Deng, Xiaole Guo, Yiheng Xu, Chen Henry Wu, Zhennan Shen, Zhuokai Li, Ryan Li, Xiaochuan Li, Junda Chen, Zheng Boyuan, LI PEIHANG, Fangyu Lei, Ruisheng Cao, Yeqiao Fu, Dongchan Shin, Martin Shin, Hu Jiarui, Yuyan Wang, Jixuan Chen, Yuxiao Ye, Danyang Zhang, Yipu Wang, Heng Wang, Diyi Yang, Victor Zhong, Y.Charles, Zhilin Yang, and Tao Yu. OpenCUA: Open foundations for computer-use agents. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. URL https://openreview.net/forum?id=6iRZvJiC9Q."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[212] Yiheng Xu, Dunjie Lu, Zhennan Shen, Junli Wang, Zekun Wang, Yuchen Mao, Caiming Xiong, and Tao Yu. Agenttrek: In The Thirteenth International Conference on Agent trajectory synthesis via guiding replay with web tutorials. Learning Representations, 2025. URL https://openreview.net/forum?id=EEgYUccwsV. [213] Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, et al. Agentic context engineering: Evolving contexts for self-improving language models. arXiv preprint arXiv:2510.04618, 2025. [214] Zhizheng Wang, Qiao Jin, Chih-Hsuan Wei, Shubo Tian, Po-Ting Lai, Qingqing Zhu, Chi-Ping Day, Christina Ross, Robert Leaman, and Zhiyong Lu. Geneagent: self-verification language agent for gene-set analysis using domain databases. Nature Methods, pages 19, 2025. [215] Zifeng Wang, Benjamin Danek, Ziwei Yang, Zheng Chen, and Jimeng Sun. Making large language models reliable data science programming copilot for biomedical research. Nature Biomedical Engineering, 2025. [216] Kyle Swanson, Wesley Wu, Nash Bulaong, John Pak, and James Zou. The virtual lab of ai agents designs new sars-cov-2 nanobodies. Nature, pages 13, 2025. [217] Zifeng Wang, Lang Cao, Qiao Jin, Joey Chan, Nicholas Wan, Behdad Afzali, Hyun-Jin Cho, Chang-In Choi, Mehdi Emamverdi, Manjot Gill, et al. foundation model for human-ai collaboration in medical literature mining. Nature Communications, 16(1):8361, 2025. [218] Haoyang Li, Weishen Pan, Suraj Rajendran, Chengxi Zang, and Fei Wang. Trialgenie: Empowering clinical trial design with agentic intelligence and real world data. medRxiv, pages 202504, 2025. [219] Kyle Swanson, Gary Liu, Denise Catacutan, Autumn Arnold, James Zou, and Jonathan Stokes. Generative ai for designing and validating easily synthesizable and structurally novel antibiotics. Nature machine intelligence, 6(3): 338353, 2024. [220] Aarti Krishnan, Jacqueline Valeri, Wengong Jin, Nina Donghia, Leif Sieben, Andreas Luttens, Yu Zhang, Seyed Majed Modaresi, Andrew Hennes, Jenna Fromer, et al. generative deep learning approach to de novo antibiotic design. Cell, 2025. [221] Shanghua Gao, Richard Zhu, Pengwei Sui, Zhenglun Kong, Sufian Aldogom, Yepeng Huang, Ayush Noori, Reza Shamji, Krishna Parvataneni, Theodoros Tsiligkaridis, et al. Democratizing ai scientists using tooluniverse. arXiv preprint arXiv:2509.23426, 2025. [222] Kexin Huang, Serena Zhang, Hanchen Wang, Yuanhao Qu, Yingzhou Lu, Yusuf Roohani, Ryan Li, Lin Qiu, Gavin Li, Junze Zhang, et al. Biomni: general-purpose biomedical ai agent. biorxiv, 2025. [223] Ruofan Jin, Zaixi Zhang, Mengdi Wang, and Le Cong. Stella: Self-evolving llm agent for biomedical research. arXiv preprint arXiv:2507.02004, 2025. [224] W. Daniel Hillis. Co-evolving parasites improve simulated evolution as an optimization procedure. In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, Artificial Life II, volume X, pages 313324. Addison-Wesley, 1990. [225] Christopher D. Rosin and Risto Miikkulainen. Competitive coevolution through evolutionary complexification. Journal of Artificial Intelligence Research, 21:63100, 2004. [226] Mitchell A. Potter and Kenneth A. De Jong. Cooperative coevolution: An architecture for evolving co-adapted subcomponents. Evolutionary Computation, 8(1):129, 2000. [227] Louis Sushil and collaborating authors. comprehensive survey of coevolutionary algorithms. IEEE Transactions on Evolutionary Computation, 2008. Survey of competitive, cooperative, and multi-population CEA frameworks. [228] Liviu Panait and Sean Luke. Cooperative multi-agent learning: The state of the art. Autonomous agents and multi-agent systems, 11(3):387434, 2005. [229] Zepeng Ning and Lihua Xie. survey on multi-agent reinforcement learning and its application. Journal of Automation and Intelligence, 3(2):7391, 2024. [230] Mert Cemri, Melissa Pan, Shuyi Yang, Lakshya Agrawal, Bhavya Chopra, Rishabh Tiwari, Kurt Keutzer, Aditya Parameswaran, Dan Klein, Kannan Ramchandran, et al. Why do multi-agent llm systems fail? arXiv preprint arXiv:2503.13657, 2025. [231] Liang Zhou, Rohan Patel, and Seong Kim. Multi-agent tool-integrated policy optimization. arXiv preprint arXiv:2510.04678, 2025. URL https://arxiv.org/pdf/2510.04678. Accessed: 2025-11-17."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[232] Jiajun Wang, Shurui Liu, and Tianyi Zhang. joint optimization framework for enhancing efficiency of tool In Findings of the Association for Computational Linguistics: ACL 2025, 2025. URL utilization in llm agents. https://aclanthology.org/2025.findings-acl.1149.pdf. Accessed: 2025-11-17. [233] Rui Huang, Ashok Kumar, and Sandip Sen. design framework for scalable and adaptive multi-agent coordination in dynamic environments. IEEE Transactions on Systems, Man, and Cybernetics, 2023. URL https://ieeexplore. ieee.org/iel8/6287639/10820123/10965637. Accessed: 2025-11-17. [234] Joon Lee, Robert Martens, and Yifan Du. From chaos to symbiosis: Exploring adaptive co-evolution strategies for hybrid intelligent systems. Complexity, 2024. URL https://pmc.ncbi.nlm.nih.gov/articles/PMC12465495/. Accessed: 2025-11-17. [235] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. comprehensive survey of continual learning: Theory, method and application. IEEE transactions on pattern analysis and machine intelligence, 46(8):53625383, 2024. [236] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. continual learning survey: Defying forgetting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):33663385, 2021. [237] Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, and Hao Wang. Continual learning of large language models: comprehensive survey. ACM Computing Surveys, 2024. [238] Jiacheng Lin, Zhongruo Wang, Kun Qian, Tian Wang, Arvind Srinivasan, Hansi Zeng, Ruochen Jiao, Xie Zhou, Jiri Gesi, Dakuo Wang, et al. Sft doesnt always hurt general capabilities: Revisiting domain-specific fine-tuning in llms. arXiv preprint arXiv:2509.20758, 2025. [239] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13):35213526, 2017. [240] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence, 40(12):29352947, 2017. [241] Yichen Wu, Long-Kai Huang, Renzhen Wang, Deyu Meng, and Ying Wei. Meta continual learning revisited: Implicitly enhancing online hessian approximation via variance reduction. In The Twelfth international conference on learning representations, volume 2, 2024. [242] Shipeng Wang, Xiaorong Li, Jian Sun, and Zongben Xu. Training networks in null space of feature covariance for continual learning. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 184193, 2021. [243] Mehrdad Farajtabar, Navid Azizan, Alex Mott, and Ang Li. Orthogonal gradient descent for continual learning. In International conference on artificial intelligence and statistics, pages 37623773. PMLR, 2020. [244] Yichen Wu, Hongming Piao, Long-Kai Huang, Renzhen Wang, Wanhua Li, Hanspeter Pfister, Deyu Meng, Kede Ma, and Ying Wei. Sd-lora: Scalable decoupled low-rank adaptation for class incremental learning. In ICLR, 2025. [245] Yan-Shuo Liang and Wu-Jun Li. Inflora: Interference-free low-rank adaptation for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2363823647, 2024. [246] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, and Jun Zhu. Hierarchical decomposition of prompt-based continual learning: Rethinking obscured sub-optimality. Advances in Neural Information Processing Systems, 36:6905469076, 2023. [247] Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, et al. Medrek: Retrieval-based editing for medical llms with key-aware prompts. In Socially Responsible and Trustworthy Foundation Models at NeurIPS 2025, 2025. [248] Daniel Marczak, Bartłomiej Twardowski, Tomasz Trzcinski, and Sebastian Cygert. Magmax: Leveraging model merging for seamless continual learning. In European Conference on Computer Vision, pages 379395. Springer, 2024. [249] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio. Gradient based sample selection for online continual learning. Advances in neural information processing systems, 32, 2019. [250] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learning with memory of diverse samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 82188227, 2021."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[251] Quanziang Wang, Renzhen Wang, Yichen Wu, Xixi Jia, and Deyu Meng. Cba: Improving online continual learning via continual bias adaptor. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1908219092, 2023. [252] Yichen Wu, Hong Wang, Peilin Zhao, Yefeng Zheng, Ying Wei, and Long-Kai Huang. Mitigating catastrophic forgetting in online continual learning by modeling previous task interrelations via pareto optimization. In Forty-first international conference on machine learning, 2024. [253] Liyuan Wang, Xingxing Zhang, Kuo Yang, Longhui Yu, Chongxuan Li, Lanqing HONG, Shifeng Zhang, Zhenguo Li, Yi Zhong, and Jun Zhu. Memory replay with data compression for continual learning. In International Conference on Learning Representations, 2022. [254] Quang Pham, Chenghao Liu, and Steven Hoi. Dualnet: Continual learning, fast and slow. Advances in Neural Information Processing Systems, 34:1613116144, 2021. [255] Elahe Arani, Fahad Sarfraz, and Bahram Zonooz. Learning fast, learning slow: general continual learning method based on complementary learning system. In International Conference on Learning Representations, 2022. [256] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 139149, 2022. [257] Zhiyi Shi, Binjie Wang, Chongjie Si, Yichen Wu, Junsik Kim, and Hanspeter Pfister. Dualedit: Dual editing for knowledge updating in vision-language models. arXiv preprint arXiv:2506.13638, 2025. [258] Hongming Piao, Yichen Wu, Dapeng Wu, and Ying Wei. Federated continual learning via prompt-based dual knowledge transfer. In Forty-first International Conference on Machine Learning, 2024. [259] Adarsh Kumarappan, Mo Tiwari, Peiyang Song, Robert Joseph George, Chaowei Xiao, and Anima Anandkumar. Leanagent: Lifelong learning for formal theorem proving. arXiv preprint arXiv:2410.06209, 2024. [260] Howard Chen, Noam Razin, Karthik Narasimhan, and Danqi Chen. Retaining by doing: The role of on-policy data in mitigating forgetting. arXiv preprint arXiv:2510.18874, 2025. [261] Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement learning. arXiv preprint arXiv:1904.12901, 2019. [262] Jonas Gehring, Kunhao Zheng, Jade Copet, Vegard Mella, Quentin Carbonneaux, Taco Cohen, and Gabriel Synnaeve. Rlef: Grounding code llms in execution feedback with reinforcement learning. arXiv preprint arXiv:2410.02089, 2024. [263] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016. [264] Javier Garcıa and Fernando Fernández. comprehensive survey on safe reinforcement learning. Journal of Machine Learning Research, 16(1):14371480, 2015. [265] Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Jan Leike, and Shane Legg. Specification gaming: the flip side of ai ingenuity. DeepMind Blog, 2020. [266] John Yang, Carlos Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering. Advances in Neural Information Processing Systems, 37:5052850652, 2024. [267] Shangding Gu, Long Yang, Yali Du, Guang Chen, Florian Walter, Jun Wang, and Alois Knoll. review of safe reinforcement learning: Methods, theories and applications. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [268] Kaiwen Zhou, Chengzhi Liu, Xuandong Zhao, Shreedhar Jangam, Jayanth Srinivasa, Gaowen Liu, Dawn Song, and Xin Eric Wang. The hidden risks of large reasoning models: safety assessment of r1. arXiv preprint arXiv:2502.12659, 2025. [269] Paul Kassianik and Amin Karbasi. Evaluating security risk in deepseek and other frontier reasoning models. Cisco Blogs, Cisco Systems, 31, 2025. [270] Daniel Hillis. Co-evolving parasites improve simulated evolution as an optimization procedure. Physica D: Nonlinear Phenomena, 42(1-3):228234, 1990. [271] David Manheim and Scott Garrabrant. Categorizing variants of goodharts law. arXiv preprint arXiv:1803.04585, 2018. [272] Alexander Bondarenko, Denis Volk, Dmitrii Volkov, and Jeffrey Ladish. Demonstrating specification gaming in reasoning models. arXiv preprint arXiv:2502.13295, 2025."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[273] Shuli Zhao, Qinsheng Hou, Zihan Zhan, Yanhao Wang, Yuchong Xie, Yu Guo, Libo Chen, Shenghong Li, and Zhi Xue. Mind your server: systematic study of parasitic toolchain attacks on the mcp ecosystem. arXiv preprint arXiv:2509.06572, 2025. [274] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz. Not what youve signed up for: Compromising real-world llm-integrated applications with indirect prompt injection. In Proceedings of the 16th ACM workshop on artificial intelligence and security, pages 7990, 2023. [275] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail? Advances in Neural Information Processing Systems, 36:8007980110, 2023. [276] Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization."
        },
        {
            "title": "In International",
            "content": "conference on machine learning, pages 2231. PMLR, 2017. [277] Krishnan Srinivasan, Benjamin Eysenbach, Sehoon Ha, Jie Tan, and Chelsea Finn. Learning to be safe: Deep rl with safety critic. arXiv preprint arXiv:2010.14603, 2020. [278] Nathan Hunt, Nathan Fulton, Sara Magliacane, Trong Nghia Hoang, Subhro Das, and Armando Solar-Lezama. Verifiably safe exploration for end-to-end reinforcement learning. In Proceedings of the 24th International Conference on Hybrid Systems: Computation and Control, pages 111, 2021. [279] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Wu, and Zhifang Sui. Math-shepherd: label-free step-by-step verifier for llms in mathematical reasoning. arXiv preprint arXiv:2312.08935, 3, 2023. [280] Víctor Gallego. Specification self-correction: Mitigating in-context reward hacking through test-time refinement. arXiv preprint arXiv:2507.18742, 2025. [281] SHengjie Ma, Chenlong Deng, Jiaxin Mao, Jiadeng Huang, Teng Wang, Junjie Wu, Changwang Zhang, et al. Pou: Proof-of-use to counter tool-call hacking in deepresearch agents. arXiv preprint arXiv:2510.10931, 2025. [282] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. Advances in neural information processing systems, 36:1008810115, 2023. [283] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. [284] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Nikos Karampatziakis, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adalora: Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512, 2023. [285] Chongjie Si, Zhiyi Shi, Shifan Zhang, Xiaokang Yang, Hanspeter Pfister, and Wei Shen. Unleashing the power of task-specific directions in parameter efficient fine-tuning. In The Thirteenth International Conference on Learning Representations, 2024. [286] Zhiyi Shi, Junsik Kim, Wanhua Li, Yicong Li, and Hanspeter Pfister. Mora: Lora guided multi-modal disease diagnosis with missing modality. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 273282. Springer, 2024. [287] Chongjie Si, Xuehui Wang, Xue Yang, Zhengqin Xu, Qingyun Li, Jifeng Dai, Yu Qiao, Xiaokang Yang, and Wei Shen. Flora: Low-rank core space for n-dimension. arXiv preprint arXiv:2405.14739, 10, 2024. [288] Chongjie Si, Zhiyi Shi, Xuehui Wang, Yichen Xiao, Xiaokang Yang, and Wei Shen. Generalized tensor-based parameter-efficient fine-tuning via lie group transformations. arXiv preprint arXiv:2504.00851, 2025. [289] John Schulman and Thinking Machines Lab. Lora without regret. Thinking Machines Lab: Connectionism, 2025. doi: 10.64434/tml.20250929. https://thinkingmachines.ai/blog/lora/. [290] Liyuan Liu, Feng Yao, Dinghuai Zhang, Chengyu Dong, Jingbo Shang, and Jianfeng Gao. Flashrl: 8bit rollouts, full power rl, August 2025. URL https://fengyao.notion.site/flash-rl. [291] Dan Peng, Zhihui Fu, and Jun Wang. Pocketllm: Enabling on-device fine-tuning for personalized llms. arXiv preprint arXiv:2407.01031, 2024. [292] Liang Li, Xingke Yang, Wen Wu, Hao Wang, Tomoaki Ohtsuki, Xin Fu, Miao Pan, and Xuemin Shen. Mobillm: Enabling llm fine-tuning on the mobile device via server assisted side tuning. arXiv preprint arXiv:2502.20421, 2025. [293] Xiaopei Chen, Liang Li, Fei Ji, and Wen Wu. Memory-efficient split federated learning for llm fine-tuning on heterogeneous mobile devices. arXiv preprint arXiv:2506.02940, 2025."
        },
        {
            "title": "Adaptation of Agentic AI",
            "content": "[294] Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, and Shangguang Wang. {FwdLLM}: Efficient federated finetuning of large language models with perturbed inferences. In 2024 USENIX Annual Technical Conference (USENIX ATC 24), pages 579596, 2024. [295] Vinay Venkatesh, Vamsidhar Kamanuru, Lav Kumar, and Nikita Kothari. Edge-fit: Federated instruction tuning of quantized llms for privacy-preserving smart home environments. arXiv preprint arXiv:2510.03284, 2025. [296] Guangji Bai, Yijiang Li, Zilinghan Li, Liang Zhao, and Kibaek Kim. Fedspallm: Federated pruning of large language In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for models. Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 83618373, 2025. [297] Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, and Kun Shao. Distrl: An asynchronous distributed reinforcement learning framework for on-device control agents. arXiv preprint arXiv:2410.14803, 2024. [298] Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie Huang, Zhengyang Wang, Yifan Gao, Xiaoman Pan, Lian Xiong, Jingguo Liu, Philip S. Yu, and Xian Li. Personaagent: When large language model agents meet personalization at test time. In First Workshop on Multi-Turn Interactions in Large Language Models, 2025. URL https://openreview.net/forum?id=fgCOkyJG3f. [299] Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, and Vishvak Murahari. Personagym: Evaluating persona agents and llms. arXiv preprint arXiv:2407.18416, 2024. [300] Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large language model-based human-agent collaboration for complex task solving. arXiv preprint arXiv:2402.12914, 2024. [301] Rafael Mendoza, Isabella Cruz, Richard Liu, Aarav Deshmukh, David Williams, Jesscia Peng, and Rohan Iyer. Adaptive self-supervised learning strategies for dynamic on-device llm personalization. arXiv preprint arXiv:2409.16973, 2024. [302] Logan Cross, Violet Xiang, Agam Bhatia, Daniel LK Yamins, and Nick Haber. Hypothetical minds: Scaffolding theory of mind for multi-agent tasks with large language models. arXiv preprint arXiv:2407.07086, 2024. [303] Run Luo, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, and Xiaobo Xia. Gui-r1: generalist r1-style visionlanguage action model for gui agents. arXiv preprint arXiv:2504.10458, 2025. [304] Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Junyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao, Junjie Cao, Zhengxi Lu, et al. Mobile-agent-v3: Fundamental agents for gui automation. arXiv preprint arXiv:2508.15144, 2025. [305] Yuhang Liu, Zeyu Liu, Shuanghe Zhu, Pengxiang Li, Congkai Xie, Jiasheng Wang, Xueyu Hu, Xiaotian Han, Jianbo Yuan, Xinyao Wang, et al. Infigui-g1: Advancing gui grounding with adaptive exploration policy optimization. arXiv preprint arXiv:2508.05731, 2025. [306] Yuqi Zhou, Sunhao Dai, Shuai Wang, Kaiwen Zhou, Qinglin Jia, and Jun Xu. Gui-g1: Understanding r1-zero-like training for visual grounding in gui agents. arXiv preprint arXiv:2505.15810, 2025. [307] Yucheng Shi, Wenhao Yu, Zaitang Li, Yonglin Wang, Hongming Zhang, Ninghao Liu, Haitao Mi, and Dong Yu. Mobilegui-rl: Advancing mobile gui agent through reinforcement learning in online environment. arXiv preprint arXiv:2507.05720, 2025. [308] Liujian Tang, Shaokang Dong, Yijia Huang, Minqi Xiang, Hongtao Ruan, Bin Wang, Shuo Li, Zhiheng Xi, Zhihui Cao, Hailiang Pang, et al. Magicgui: foundational mobile gui agent with scalable data pipeline and reinforcement fine-tuning. arXiv preprint arXiv:2508.03700, 2025. [309] Xinbin Yuan, Jian Zhang, Kaixin Li, Zhuoxuan Cai, Lujian Yao, Jie Chen, Enguang Wang, Qibin Hou, Jinwei Chen, Peng-Tao Jiang, et al. Enhancing visual grounding for gui agents via self-evolutionary reinforcement learning. arXiv preprint arXiv:2505.12370, 2025."
        }
    ],
    "affiliations": [
        "Harvard",
        "Northwestern",
        "TAMU",
        "UCSD",
        "UIUC",
        "UW",
        "Unity"
    ]
}