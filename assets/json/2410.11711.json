{
    "paper_title": "Zero-shot Model-based Reinforcement Learning using Large Language Models",
    "authors": [
        "Abdelhakim Benechehab",
        "Youssef Attia El Hili",
        "Ambroise Odonnat",
        "Oussama Zekri",
        "Albert Thomas",
        "Giuseppe Paolo",
        "Maurizio Filippone",
        "Ievgen Redko",
        "Balázs Kégl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs' deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-of-concept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . s [ 1 1 1 7 1 1 . 0 1 4 2 : r Preprint. Under review. ZERO-SHOT MODEL-BASED REINFORCEMENT LEARNING USING LARGE LANGUAGE MODELS Abdelhakim Benechehab* Huawei Noahs Ark Lab, EURECOM Youssef Attia El Hili Huawei Noahs Ark Lab Ambroise Odonnat Huawei Noahs Ark Lab, Inria Oussama Zekri ENS Paris-Saclay Albert Thomas Huawei Noahs Ark Lab Giuseppe Paolo Huawei Noahs Ark Lab Maurizio Filippone KAUST Ievgen Redko Huawei Noahs Ark Lab Balazs Kegl Huawei Noahs Ark Lab"
        },
        {
            "title": "ABSTRACT",
            "content": "The emerging zero-shot capabilities of Large Language Models (LLMs) have led to their applications in areas extending well beyond natural language processing tasks. In reinforcement learning, while LLMs have been extensively used in text-based environments, their integration with continuous state spaces remains understudied. In this paper, we investigate how pre-trained LLMs can be leveraged to predict in context the dynamics of continuous Markov decision processes. We identify handling multivariate data and incorporating the control signal as key challenges that limit the potential of LLMs deployment in this setup and propose Disentangled In-Context Learning (DICL) to address them. We present proof-ofconcept applications in two reinforcement learning settings: model-based policy evaluation and data-augmented off-policy reinforcement learning, supported by theoretical analysis of the proposed methods. Our experiments further demonstrate that our approach produces well-calibrated uncertainty estimates. We release the code at https://github.com/abenechehab/dicl."
        },
        {
            "title": "INTRODUCTION",
            "content": "The rise of large language models (LLMs) has significantly impacted the field of Natural Language Processing (NLP). LLMs (Brown et al., 2020; Hugo Touvron & the Llama 2 team., 2023; Dubey & the Llama 3 team, 2024), which are based on the transformer architecture (Vaswani et al., 2017), have redefined tasks such as machine translation (Brown et al., 2020), sentiment analysis (Zhang et al., 2023b), and question answering (Roberts et al., 2020; Pourkamali & Sharifi, 2024) by enabling machines to understand and generate human-like text with remarkable fluency. One of the most intriguing aspects of LLMs is their emerging capabilities, particularly in-context learning (ICL) (von Oswald et al., 2023). Through ICL, an LLM can learn to perform new task simply by being provided examples of the task within its input context, without any gradient-based optimization. This phenomenon has been observed not only in text generation but also in tasks such as image classification (Abdelhamed et al., 2024; Zheng et al., 2024) and even solving logic puzzles (Giadikiaroglou et al., 2024), which is unexpected in the context of the standard statistical learning theory. To our knowledge, ICL capabilities of pre-trained LLMs have been only scarcely explored in reinforcement learning (Wang et al., 2023) despite the demonstrated success of the former in understanding the behavior of deterministic and chaotic dynamical systems (Liu et al., 2024c). In this paper, we show how ICL with pre-trained LLMs can improve the sample efficiency of Reinforcement Learning (RL), with two proof-of-concepts in policy evaluation and data-augmented off-policy RL. Following the dynamical system perspective on ICL introduced in Li et al. (2023) and experimentally studied in Liu et al. (2024c), we use the observed trajectories of given agent *Correspondence to abdelhakim.benechehab1@huawei.com Work done while at Huawei Noahs Ark Lab Department of Data Science, EURECOM Univ. Rennes 2, CNRS, IRISA Statistics Program, KAUST 1 Preprint. Under review. to predict its future state and reward in commonly used realistic RL environments. To achieve this, we solve two crucial challenges related to considering continuous state-space Markov Decision Processes (MDP): 1) incorporating the action information into the LLMs context and 2) handling the interdependence between the state-actions dimensions, as prior approaches were known to treat multivariate datas covariates independently. Our approach leads to several novel insights and contributions, which we summarize as follows: 1. Methodological. We develop novel approach to integrate state dimension interdependence and action information into in-context trajectories. This approach, termed Disentangled In-Context Learning (DICL), leads to new methodology for applying ICL in RL environments with continuous state spaces. We validate our proposed approach on tasks involving proprioceptive control. 2. Theoretical. We theoretically analyze the policy evaluation algorithm resulting from multibranch rollouts with the LLM-based dynamics model, leading to novel return bound. 3. Experimental. We show how the LLMs MDP modeling ability can benefit two RL applications: policy evaluation and data-augmented offline RL. Furthermore, we show that the LLM is calibrated uncertainty estimator, desirable property for MBRL algorithms. Organization of the paper. The paper is structured as follows: Section 2 introduces the main concepts from the literature used in our work (while more detailed related work is differed to Appendix B). We then start our analysis in Section 3.1, by analyzing LLMs attention matrices. DICL is presented in Section 3.3, while Section 4 contains different applications of the proposed method in RL, along with the corresponding theoretical analysis."
        },
        {
            "title": "2 BACKGROUND KNOWLEDGE",
            "content": "(cid:2) (cid:80) Reinforcement Learning (RL). The standard framework of RL is the infinite-horizon Markov decision process (MDP) = S, A, P, r, µ0, γ where represents the state space, the action space, : the (possibly stochastic) transition dynamics, : the reward function, µ0 the initial state distribution, and γ [0, 1] the discount factor. The goal of RL is to find, for each state S, distribution π(s) over the action space A, called the policy, that maximizes the expected sum of discounted rewards η(π) := Es0µ0,atπ, st>0P t[(cid:80) t=0 γtr(st, at)]. Under policy π, we define the state value function at as the expected sum of discounted rewards, starting from the state s, and following the policy π afterwards until termination: π(s) = Eatπ,st>0P Model-based RL (MBRL). MBRL algorithms address the supervised learning problem of estimating the dynamics of the environment ˆP (and sometimes also the reward function ˆr) from data collected when interacting with the real system. The models loss function is typically the log-likelihood L(D; ˆP ) = 1 t+1si t) or Mean Squared Error (MSE) for deterN ministic models. The learned model can subsequently be used for policy search under the MDP (cid:99)M = S, A, ˆP , r, µ0, γ. This MDP shares the state and action spaces S, A, reward function r, with the true environment M, but learns the transition probability ˆP from the dataset D. The policy ˆπ = arg maxπ ˆη(π) learned on (cid:99)M is not guaranteed to be optimal under the true MDP due to distribution shift and model bias. t=0 γtr(st, at) s0 = s(cid:3). i=1 log ˆP (si t, ai (cid:80)N Large Language Models (LLMs). Within the field of Natural Language Processing, Large Language Models (LLMs) have emerged as powerful tool for understanding and generating humanlike text. An LLM is typically defined as neural network model, often based on the transformer architecture (Vaswani et al., 2017), that is trained on vast corpus of sequences, = {U1, U2, . . . , Ui, . . . , UN }, where each sequence Ui = (u1, u2, . . . , uj, . . . , uni) consists of tokens uj from vocabulary V. Decoder-only LLMs (Radford et al., 2019; Dubey & the Llama 3 team, 2024) typically encode an autoregressive distribution, where the probability of each token is conditioned only on the previous tokens in the sequence, expressed as pθ(Ui) = (cid:81)ni j=1 pθ(uju0:j1). The parameters θ are learned by maximizing the probability of the entire dataset, pθ(U ) = (cid:81)N i=1 pθ(Ui). Every LLM has an associated tokenizer, which breaks an input string into sequence of tokens, each belonging to V. 2 Preprint. Under review. (a) Rectangular pulse. Figure 1: LLM can perceive time patterns. The LLM is fed with 3 time series presenting distinct (c) The fthigh patterns. dimension of HalfCheetah under an expert policy. Tokens belonging to constant slots (or peaks) attend to all the similar ones that precede them, yet they focus more on the first occurrence of that phenomena. (b) Rectangular signal with constant sub-parts. In-Context Learning (ICL). In order to use trajectories as inputs in ICL, we use the tokenization of time series proposed in Gruver et al. (2023b) and Jin et al. (2024). The latter works use subset of the LLM sub-vocabulary Vnum representing digits as summarized in Algorithm 1. Given an univarite time series, we first rescale it into specific range (Liu et al., 2024b; Zekri et al., 2024a; Requeima et al., 2024) then encode the rescaled time series with digits. The logits corresponding to tokens in Vnum can be further used to predict the next timestamp transition rule for Markovian systems (Liu et al., 2024c). Algorithm 1 ICLθ (Liu et al., 2024b; Gruver et al., 2023b) Input: Time series (xi)it, LLM pθ, sub-vocabulary Vnum ˆxt = x1 1. Tokenize time series 2. logits pθ(ˆxt) 3. {P (Xi+1xi, . . . , x0)}it softmax(logits(Vnum)) Return: {P (Xi+1xi, . . . , x0)}it 1 . . . xk 1, . . . 1x"
        },
        {
            "title": "3 ZERO-SHOT DYNAMICS LEARNING USING LARGE LANGUAGE MODELS",
            "content": "3.1 MOTIVATION Before considering the multivariate trajectories of agents collected in RL environments, we first want to verify whether pre-trained LLM model is sensitive to the primitive univariate signals akin to those encountered in them. For this, we investigate the attention mechanism of the Llama3 8B model (Dubey & the Llama 3 team, 2024) when we feed it with different signals, including the periodic fthigh dimension from the HalfCheetah system (Brockman et al., 2016). By averaging the attention matrices over the 32 heads for each of the 32 layers of the multi-head attention in Llama3, we observed distinct patterns that provide insight into the models focus and behavior (Fig. 1 shows selected attention layers for each signal). The attention matrices exhibit diagonal pattern, indicative of strong self-correlation among timestamps, and subtriangular structure due to the causal masked attention in decoder-only transformers. Further examination of the attention matrices reveals more intricate finding. Tokens within repeating patterns (e.g., signal peaks, constant parts) not only attend to past tokens within the same cycle but also to those from previous occurrences of the same pattern, demonstrating form of in-context learning. The ability to detect and exploit repeating patterns within such signals is especially valuable in RL, where state transitions and action outcomes often exhibit cyclical or recurring dynamics, particularly in continuous control tasks. However, applying this insight to RL presents two critical challenges related to 1) the integration of actions into the forecasting process, and 2) handling of the multivariate nature of RL problems. We now address these challenges by building on the insights from the analysis presented above. 3 Preprint. Under review."
        },
        {
            "title": "3.2 PROBLEM SETUP",
            "content": "Given an initial trajectory = (s0, a0, r1, s1, a1, r2, s2, . . . , rT 1, sT 1) of length , with st S, at = π(st) A1, where the policy π is fixed for the whole trajectory, and rt R, we want to predict future transitions: given (sT 1, aT 1) predict the next state and reward (sT , rT ) and subsequent transitions autoregressively. For simplicity we first omit the actions and the reward, focusing instead on the multivariate sequence τ π = (s0, s1, . . . , sT ) where we assume that the state dimensions are independent. Later, we show how to relax the assumptions of omitting actions and rewards, as well as state independence, which is crucial for applications in RL. The joint probability density function of τ π can be written as: (cid:40)P(τ π) = µ0(s0) (cid:81)T t=1 π(stst1) where π(stst1) = (cid:82) aA π(ast1)P (stst1, a) da . (1) (sj sj t1, . . . , sj 0)}tT = ICLθ(τ π,j) Using the decoder-only nature of the in-context learner defined in Section 2, we can apply Algorithm 1 to each dimension of the state vector to infer the transition rule of each visited state in τ π conditioned on its relative history: for all {1, . . . , ds}, { ˆP π,j θ 1, sj where θ are the fixed parameters of the LLM used as an in-context learner, and its context length. Assuming complete observability of the MDP state, the Markovian property unveils an equivalence between the learned transition rules and the corresponding Markovian ones: ˆPθ(stst1, . . . , s1, s0) = ˆPθ(stst1). This approach, that we name vICL (for vanilla ICL), thus applies Algorithm 1 on each dimension of the state individually, assuming their independence. Furthermore, the action information is integrated-out (as depicted in Eq. (1)), which in theory, limits the application scope of this method to quantities that only depend on policy through the expectation over actions (e.g., the value function π(s)). We address these limitations in the next section. (2) 3.3 STATE AND ACTION DIMENSION INTERDEPENDENCE In this section we address the two limitations of vICL discussed in Section 3.2 by introducing Disentangled InContext Learning (DICL), method that relaxes the assumption of state feature independence and reintroduces the action by employing strategies that aim to map the state-action vector to latent space where the features are independent. We can then apply vICL, which operates under the assumption of feature independence, to the latent representation. An added benefit of using such latent space is that it can potentially reduce the dimensionality, leading to speed-up of the overall approach. While sophisticated approaches2 like disentangled autoencoders could be considered for DICL, in this work we employ Principal Component Analysis (PCA). In fact, the absence of pre-trained models for this type of representation learning requires training from scratch on potentially large dataset. This goes against our goal of leveraging the pre-trained knowledge of LLMs and ICL. Instead, we find that PCA, which generates new linearly uncorrelated features and can reduce dimensionality, strikes good balance between simplicity, tractability, and performance (Fig. 2 and Fig. 3). Nonetheless, DICL is agnostic to this aspect and any approach that can disentangle features can be used in place of PCA. Figure 2: The covariance matrix computed from the D4RL expert dataset in the Halfcheetah environment indicates linear correlations between stateaction features. We perform PCA before applying ICL to achieve potentially lower-dimensional space with linearly uncorrelated features. 1In practice, states and actions are real valued vectors spanning space of dimensions respectively ds and da: = Rds , = Rda 2A more detailed discussion of alternative approaches to PCA is provided in Appendix C. 4 Preprint. Under review. (a) Multi-step error. (b) Predicted trajectories. (c) Time. Figure 3: PCA-based methods achieve smaller multi-step error in less computational time. We compare DICL-(s) and DICL-(s, a) using number of components equal to half the number of features, with the vanilla approach vICL and an MLP baseline. The baseline is trained for next state prediction using the transitions present in the context of the LLM. In the rest of the paper, we refer to the routine that applies PCA to feature space of states and actions and then runs Algorithm 1 in the projection space of principal components as DICL-(s, a). In some cases, integrating the action may not be necessary, such as when one is only interested in estimating the value function π(s). To address this, we introduce DICL-(s) which denotes the combination of PCA and ICL applied solely to the states. 3.4 AN ILLUSTRATIVE EXAMPLE In this section, we aim to challenge our approach against the HalfCheetah system from the MuJoCo Gym environment suite (Brockman et al., 2016; Todorov et al., 2012). All our experiments are conducted using the Llama 3 series of models (Llama 3 8B, Llama 3.2 1B, Llama 3.2 3B) (Dubey & the Llama 3 team, 2024). Fig. 3a shows the average MSE over prediction horizon of {1, . . . , 20} steps for each state dimension. Fig. 3b shows predicted trajectories for selected state dimensions of the HalfCheetah system (the details of the experiment, the metrics and the remaining state dimensions are differed to Appendix F). We first observe that the LLM-based dynamics forecasters exhibit burn-in phase ( 70 steps in Fig. 3b) that is necessary for the LLM to gather enough context. For multi-step prediction, Fig. 3a, showing the average MSE over prediction horizons and trajectories, demonstrates that both versions of DICL improve over the vanilla approach and the MLP baseline, trained on the context data, in almost all state dimensions. Indeed, we hypothesize that this improvement is especially brought by the projection in linearly uncorrelated space that PCA enables. Furthermore, we also leveraged the dimensionality reduction feature by selecting number of components equal to half the number of the original features. This results in significant decrease in the computational time of the method without loss of performance, as showcased by Fig. 3c. We now move to integrating our methodology in RL-related applications."
        },
        {
            "title": "4 USE-CASES IN REINFORCEMENT LEARNING",
            "content": "As explored in the preceding sections, LLMs can be used as accurate dynamics learners for proprioceptive control through in-context learning. We now state our main contributions in terms of the integration of DICL into MBRL. First, we generalize the return bound of Model-Based Policy Optimization (MBPO) (Janner et al., 2019) to the more general case of multiple branches and use it to analyze our method. Next, we leverage the LLM to augment the replay buffer of an off-policy RL algorithm, leading to more sample-efficient algorithm. In second application, we apply our method to predict the reward signal, resulting in hybrid model-based policy evaluation technique. Finally, we show that the LLM provides calibrated uncertainty estimates and conclude with discussion of our results. 5 Preprint. Under review."
        },
        {
            "title": "4.1 THEORETICAL ANALYSIS: RETURN BOUND UNDER MULTI-BRANCH ROLLOUTS",
            "content": "When using dynamics model in MBRL, one ideally seeks monotonic improvement guarantees, ensuring that the optimal policy under the model is also optimal under the true dynamics, up to some bound. Such guarantees generally depend on system parameters (e.g., the discount factor γ), the prediction horizon k, and the model generalization error εm. As established in Janner et al. (2019) and Frauenknecht et al. (2024), the framework for deriving these theoretical guarantees is the one of branched model-based rollouts. branched rollout return ηbranch[π] of policy π is defined in Janner et al. (2019) as the return of rollout which begins under the true dynamics and at some point in time switches to rolling out under learned dynamics ˆP for steps. For our LLM-based dynamics learner, we are interested in studying more general branching scheme that will be later used to analyze the results of our data-augmented off-policy algorithm. We begin by defining the multi-branch rollout return. Definition 4.1 (Multi-branch rollout return). The multibranch rollout return ηllm p,k,T [π] of policy π is defined as the expected return over rollouts with the following dynamics: 1. for < , where is the minimal context length, the rollout follows the true dynamics . 2. for , with probability p, the rollout switches to the LLM-based dynamics ˆPllm for steps, otherwise the rollout continues with the true dynamics . Figure 4: Multi-branch return. The rollout following the true dynamics is shown in blue. The branched rollouts following LLM-based dynamics ˆPllm are in purple. Branched rollouts can overlap and the corresponding return is the expectation over the overlapping branches. These different rollout realizations, referred to as branches, can overlap, meaning that multiple LLM-based dynamics can run in parallel if multiple branchings from the true dynamics occur within the k-step window (see Fig. 4). With this definition, we now state our main theoretical result, consisting of return bound between the true return and the multi-branch rollout return. Theorem 4.2 (Multi-branch return bound). Let be the minimal length of the in-context trajectories, [0, 1] the probability that given state is branching point. We assume that the reward is bounded and that the expected total variation between the LLM-based model and the true dynamics under policy π is bounded at each timestep by maxtT EsP t,aπ[DTV(P (.s, a) ˆPllm(.s, a))] εllm(T ). Then under multi-branched rollout scheme with branch length of k, the return is bounded as follows: η(π) ηllm p,k,T (π) γT 1 γ rmaxk2 εllm(T ) , (3) where rmax = maxsS,aA r(s, a). Theorem 4.2 generalizes the single-branch return presented in Janner et al. (2019), incorporating an additional factor of the prediction horizon due to the presence of multiple branches, and directly accounting for the impact of the amount of LLM training data through the branching factor p. Additionally, the bound is inversely proportional to the minimal context length , both through the power in the discount factor γT and the error term εllm(T ). Indeed, the term εllm(T ) corresponds to the generalization error of in-context learning. Several works in the literature studied it and showed that it typically decreases in O(T 1/2) with the length of the context trajectories (Zekri et al., 2024b; Zhang et al., 2023c; Li et al., 2023). Preprint. Under review."
        },
        {
            "title": "4.2 DATA-AUGMENTED OFF-POLICY REINFORCEMENT LEARNING",
            "content": "In this section, we show how DICL can be used for data augmentation in off-policy model-free RL algorithms such as Soft Actor-Critic (SAC) (Haarnoja et al., 2018). The idea is to augment the replay buffer of the off-policy algorithm with transitions generated by DICL, using trajectories already collected by previous policies. The goal is to improve sample-efficiency and accelerate the learning curve, particularly in the early stages of learning as the LLM can generate accurate transitions from small trajectory. We name this application of our approach DICL-SAC. Algorithm 2 DICL-SAC LLM replay buffer Rllm, and context size Tmax 2: Initialize policy πϕ, critic Qψ, replay buffer R, and New transition (st, at, rt, st+1) from πθ Add (st, at, rt, st+1) to Store auxiliary action at πθ(.st) if Generate LLM data then 1: Inputs: LLM-based dynamics learner (e.g. DICL-(s)), batch size b, LLM data proportion α, minimal context length , and maximal context length Tmax As defined in Corrado & Hanna (2023), data-augmented off-policy RL involves perturbing previously observed transitions to generate further new transitions, without interaction with the environment. The generated transitions should ideally be diverse and feasible under to enhance the MDP dynamics sample efficiency while ensuring that the optimal policy remains learnable. Algorithm 2 (DICL-SAC) integrates multiple components to demonstrate novel proof-ofconcept for improving the sample efficiency of SAC using DICL for data augmentation. Let = (s0, a0, r0, . . . , sTmax, aTmax, rTmax) collected be with fixed policy πϕ, sampled from the transitions being real stored in replay buffer R. We transitions synthetic generate (st, at, rt, ˆst+1)T tTmax , where ˆst+1 is the next state generated by the LLM model applied on the trajectory of the states only, at is an action sampled from the data collection policy πϕ(.st), and is the minimal context length. These transitions are then stored in separate replay buffer Rllm. At given update frequency, DICL-SAC performs gradient updates using data sampled from and α% gradient updates using data sampled from Rllm. Other hyperparameters of our method include the LLM-based method (vICL or DICL-(s)), how often we generate new LLM data and the maximal context length Tmax (see Appendix for the full list of hyperparameters). 3: for = 1, . . . , interactions do 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for Sample trajectory = (s0, . . . , sTmax) from {ˆsi+1}0iTmax DICL-(s) (T ) Add {(si, ai, ri, ˆsi+1)}T iTmax to Rllm Sample batch of size from Sample batch Bllm of size α from Rllm Update θ and ψ on Bllm end if if update SAC then trajectory end if real It is possible to use the LLM as transition model with DICL-(s, a), generating the next state based on the previous state and an action (potentially sampled from different policy), this approach would make data generation very expensive due to the high inference cost of the LLM. Indeed, thanks to Eq. (2), our data augmentation strategy, applies the LLM only to the states, allowing us to generate Tmax transitions with ds calls to the LLM, one call for each dimension of the state. However, using the LLM as transition model, accounting for actions, would require (Tmax ) ds calls as we would need one call for each action we modify. This is also why we assume fixed policy in the context, enabling the LLM to learn πϕ using only the states. Fig. 5 compares the return curves obtained by DICL-SAC against SAC in three control environments from the Gym library (Brockman et al., 2016). As anticipated with our data augmentation approach, we observe that our algorithm improves the sample efficiency of SAC at the beginning of training. This improvement is moderate but significant in the Pendulum and HalfCheetah environments, while the return curves tend to be noisier in the Hopper environment. Furthermore, as the proportion of LLM data α increases, the performance of the algorithm decreases (particularly in HalfCheetah), as predicted by Theorem 4.2. Indeed, larger proportion of LLM data correlates with higher probability of branching p, as more branching points will be sampled throughout the training. Regarding the other parameters of our bound in Theorem 4.2, we set = 1, meaning all LLM-generated transitions are added to Rllm, and = 1 to minimize LLM inference time. Preprint. Under review. Figure 5: Data-augmented off-policy RL. In the early stages of training DICL-SAC improves the sample efficiency of SAC on three Gym control environments. 4.3 POLICY EVALUATION In this section we show how DICL can be used for policy evaluation. System engineers are often presented with several policies to test on their systems. On the one hand, off-policy evaluation (e.g., Uehara et al. (2022)) involves using historical data collected from different policy to estimate the performance of target policy without disrupting the system. However, this approach is prone to issues such as distributional shift and high variance. On the other hand, online evaluation provides direct and unbiased comparison under real conditions. System engineers often prefer online evaluation for set of pre-selected policies because it offers real-time feedback and ensures that deployment decisions are based on live data, closely reflecting the systems true performance in production. However, online evaluations can be time-consuming and may temporarily impact system performance. To address this, we propose hybrid approach using LLM dynamics predictions obtained through ICL to reduce the time required for online evaluation: the initial phase of policy evaluation is conducted as standard online test, while the remainder is completed offline using the dynamics predictions enabled by the LLMs ICL capabilities. Figure 6: Policy evaluation with DICL. Relative error on the predicted value over = 500 steps, with context length of = 500. Fig. 6 illustrates the relative error in value obtained by predicting the trajectory of rewards for steps, given context length of = 500. When 500, we complete the remaining steps of the 1000-step episode using the actual rewards. For the two versions of DICL, the reward vector is concatenated to the feature space prior to applying PCA. In the Hopper environment, it is evident that predicting the reward trajectory alone is challenging task for the vanilla method vICL. On the contrary, both DICL-(s) and DICL-(s, a) effectively capture some of the dependencies of the reward signal on the states and actions, providing more robust method for policy evaluation, and matching the MLP baseline that has been trained on dataset of transitions sampled from the same policy. However, in HalfCheetah we observe that the vanilla method largely improves upon both the baseline and DICL. We suspect that this is due to the fact that the reward signal is strongly correlated rootx dimension in HalfCheetah, which proved to be harder to predict by our approach, as with the can be seen in Fig. 3a. Note that the experimental setup that we follow here is closely related to the concept of Model-based Value Expansion (Feinberg et al., 2018; Buckman et al., 2018), where we use the dynamics model to improve the value estimates through an n-step expansion in an Actor Critic algorithm. 8 Preprint. Under review."
        },
        {
            "title": "4.4 CALIBRATION OF THE LLM UNCERTAINTY ESTIMATES",
            "content": "An intriguing property observed in Fig. 3b is the confidence interval around the predictions. As detailed in Algorithm 1, one can extract full probability distribution for the next prediction given the context, enabling uncertainty estimation in the LLMs predictions. Notably, this uncertainty is pronounced at the beginning when context is limited, around peaks, and in regions where the average prediction exhibits large errors. We explore this phenomenon further in the next section by evaluating the calibration of the LLMs uncertainty estimates. Calibration is known to be an important property of dynamics model when used in reinforcement learning (Malik et al., 2019). In this section, we aim to investigate whether the uncertainty estimates derived from the LLMs logits are well-calibrated. We achieve this by evaluating the quantile calibration of the probability distributions obtained for each LLM-based method. Quantile calibration. For regression problem with variable = R, and model that outputs cumulative distribution function (CDF) Fi over yi (where indexes data points), quantile calibration implies that yi (groundtruth) should fall within p%-confidence interval p% of the time: (cid:80)N i=1 I{yi 1 i (p)} for all [0, 1] as (4) where 1 [0, 1], and the number of samples. : [0, 1] denotes the quantile function (p) = inf{y : Fi(y)} for all are well-calibrated LLMs forecasters. Fig. 7 shows the reliability diagram for the bf oot dimension of the HalfCheetah system. The overall conclusion is that, regardless of the LLM-based sub-routine used to predict the next state, the uncertainty estimates derived from the LLMs logits are well-calibrated in terms Ideally, forecasters of quantile calibration. should align with the diagonal in Fig. 7, which the LLM approach nearly achieves. Furthermore, when comparing with naive baseline (the details are differed to Appendix G), the LLM-forecaster matches the baseline when its already calibrated, and improves over it when its not. To quantify forecasters calibration with point statistic, we compute the Kolmogorov-Smirnov goodness-of-fit test (Eq. (10)) that is shown in the legend of Fig. 7."
        },
        {
            "title": "5 CONCLUSION",
            "content": "Figure 7: Quantile calibration reliability diagram. We show that the LLM uncertainty estimates are well-calibrated. Vertical lines show the Kolmogorov-Smirnov statistic for each fit. In this paper, we ask how we can leverage the emerging capabilities of Large Language Models to benefit model-based reinforcement learning. We build on previous work that successfully conceptualized in-context learning for univariate time series prediction, and provide systematic methodology to apply ICL to an MDPs dynamics learning problem. Our methodology, based on projection of the data in linearly uncorrelated representation space, proved to be efficient in capturing the dynamics of typical proprioceptive control environments, in addition to being more computationally efficient through dimensionality reduction. To derive practical applications of our findings, we tackled two RL use-cases: data-augmented offpolicy RL, where our algorithm DICL-SAC improves the sample efficiency of SAC, and benefits from theoretical guarantee under the framework of model-based multi-branch rollouts. Our second application, consisted in predicting the trajectory of rewards in order to perform hybrid online and model-based policy evaluation. Finally, we showed that the LLM-based dynamics model also provides well-calibrated uncertainty estimates. 9 Preprint. Under review."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "In order to ensure reproducibility we release the code at https://github.com/abenechehab/dicl. The implementation details and hyperparameters are listed in Appendix D."
        },
        {
            "title": "REFERENCES",
            "content": "Abdelrahman Abdelhamed, Mahmoud Afifi, and Alec Go. What do you see? enhancing zero-shot image classification with multimodal large language models. arXiv preprint arXiv:2405.15668, 2024. Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? Investigations with linear models, May 2023. URL http: //arxiv.org/abs/2211.15661. arXiv:2211.15661 [cs]. Arthur Argenson and Gabriel Dulac-Arnold. Model-based offline planning. In International Conference on Learning Representations, 2021. Abdelhakim Benechehab, Albert Thomas, and Balazs Kegl. Deep autoregressive density nets vs neural ensembles for model-based offline reinforcement learning. February 2023. URL https: //openreview.net/forum?id=gvOSQjGTtxj. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI gym, 2016. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners, July 2020. URL http://arxiv.org/abs/2005.14165. arXiv:2005.14165 [cs]. Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. SampleIn Proceedings of efficient reinforcement learning with stochastic ensemble value expansion. the 32nd International Conference on Neural Information Processing Systems, NIPS18, pp. 82348244, Red Hook, NY, USA, 2018. Curran Associates Inc. Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Yue Chen, Guolong Liu, Gaoqi Liang, Junhua Zhao, Jinyue Yan, and Yun Li. Survey on large language model-enhanced reinforcement learning: Concept, taxonomy, and methods, 2024. URL https://arxiv.org/abs/2404.00282. Thomas Carta, Clement Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive environments with online reinforcement learning, 2023. URL https://arxiv.org/abs/2302.02662. Chang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Transdreamer: Reinforcement learning with transformer world models, 2022. URL https://arxiv.org/abs/2202.09481. Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling, 2021. URL https://arxiv.org/abs/2106.01345. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in handful of trials using probabilistic dynamics models. In Advances in Neural Information Processing Systems 31, pp. 47544765. Curran Associates, Inc., 2018. Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matthew Botvinick, Jane X. Wang, and Eric Schulz. Meta-in-context learning in large language models, May 2023. URL http://arxiv.org/ abs/2305.12907. arXiv:2305.12907 [cs]. Nicholas Corrado and Josiah Hanna. Understanding when dynamics-invariant data augmentations benefit model-free reinforcement learning updates. arXiv preprint arXiv:2310.17786, 2023. 10 Preprint. Under review. Abhimanyu Das, Weihao Kong, Rajat Sen, and Yichen Zhou. decoder-only foundation model for time-series forecasting, April 2024. URL http://arxiv.org/abs/2310.10688. arXiv:2310.10688 [cs]. Marc Peter Deisenroth and Carl Edward Rasmussen. PILCO: model-based and data-efficient approach to policy search. In Proceedings of the International Conference on Machine Learning, 2011. Andreas Draeger, Sebastian Engell, and Horst Ranke. Model predictive control using neural networks. IEEE Control Systems, 15:6166, 1995. ISSN 1066033X. doi: 10.1109/37.466261. Abhimanyu Dubey and the Llama 3 team. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I. Jordan, Joseph E. Gonzalez, and Sergey Levine. Model-based value estimation for efficient model-free reinforcement learning, 2018. URL https://arxiv.org/abs/1803.00101. Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, and Ji-Rong Wen. Large Language Model-based Human-Agent Collaboration for Complex Task Solving, February 2024. URL http://arxiv.org/abs/2402.12914. arXiv:2402.12914 [cs]. Bernd Frauenknecht, Artur Eisele, Devdutt Subhasish, Friedrich Solowjow, and Sebastian Trimpe. Trust the model where it trusts itself model-based actor-critic with uncertainty-aware rollout adaption, 2024. URL https://arxiv.org/abs/2405.19014. Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 20522062. PMLR, 0915 Jun 2019. URL https://proceedings.mlr. press/v97/fujimoto19a.html. Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving PILCO with Bayesian neural network dynamics models. In Data-Efficient Machine Learning workshop, International Conference on Machine Learning, 2016. Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What Can Transformers Learn In-Context? Case Study of Simple Function Classes, August 2023. URL http://arxiv. org/abs/2208.01066. arXiv:2208.01066 [cs]. Panagiotis Giadikiaroglou, Maria Lymperaiou, Giorgos Filandrianos, and Giorgos Stamou. Puzzle solving using reasoning of large language models: survey. arXiv preprint arXiv:2402.11291, 2024. Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large Language Models Are Zero-Shot Time Series Forecasters, October 2023a. URL http://arxiv.org/abs/2310. 07820. arXiv:2310.07820 [cs]. Nate Gruver, Marc Anton Finzi, Shikai Qiu, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasters. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https://openreview.net/forum?id=md68e8iZK1. David Ha and Jurgen Schmidhuber. Recurrent world models facilitate policy evolution. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 24502462. Curran Associates, Inc., 2018. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with Stochastic Actor. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 18611870. PMLR, 1015 Jul 2018. Preprint. Under review. Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. Learning latent dynamics for planning from pixels. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 25552565, 2019. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. TabLLM: Few-shot Classification of Tabular Data with Large Language Models, March 2023. URL http://arxiv.org/abs/2210.10723. arXiv:2210.10723 [cs]. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with In International Conference on Learning Representations, constrained variational framework. 2017. URL https://openreview.net/forum?id=Sy2fzU9gl. Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, Jeff Braga, Dipam Chakraborty, Kinal Mehta, and Joao G.M. Araujo. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. Journal of Machine Learning Research, 23(274):118, 2022. URL http://jmlr.org/papers/v23/21-1342.html. Louis Martin Hugo Touvron and the Llama 2 team. Llama 2: Open foundation and fine-tuned chat models, 2023. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Michael Janner, Qiyang Li, and Sergey Levine. Offline Reinforcement Learning as One Big Sequence Modeling Problem, November 2021. URL http://arxiv.org/abs/2106. 02039. arXiv:2106.02039 [cs]. Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series forecasting by reprogramming large language models, 2024. Shyam Sundar Kannan, Vishnunandan L. N. Venkatesh, and Byung-Cheol Min. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models, March 2024. URL http://arxiv.org/abs/2309.10062. arXiv:2309.10062 [cs]. Balazs Kegl, Gabriel Hurtado, and Albert Thomas. Model-based micro-data reinforcement learning: what are the crucial model properties and which model to choose? In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id= p5uylG94S68. Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: ModelIn H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balbased offline reinforcement learning. can, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 2181021823. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/f7efa4f864ae9b88d43527f4b14f750f-Paper.pdf. Hyunjik Kim and Andriy Mnih. Disentangling by factorising, 2019. URL https://arxiv. org/abs/1802.05983. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http: //arxiv.org/abs/1412.6980. Diederik Kingma and Max Welling. Auto-encoding variational bayes, 2022. URL https: //arxiv.org/abs/1312.6114. 12 Preprint. Under review. Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. Reward design with language models, 2023. URL https://arxiv.org/abs/2303.00001. Byung-Jun Lee, Jongmin Lee, and Kee-Eung Kim. Representation balancing offline model-based reinforcement learning. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=QpNz8r_Ri2Y. Sergey Levine and Vladlen Koltun. Guided policy search. In Sanjoy Dasgupta and David McAllester (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 19, Atlanta, Georgia, USA, 1719 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/levine13.html. Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. Transformers as algorithms: Generalization and stability in in-context learning. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 1956519594. PMLR, 2329 Jul 2023. URL https://proceedings.mlr.press/v202/li23l.html. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as Policies: Language Model Programs for Embodied Control, May 2023. URL http://arxiv.org/abs/2209.07753. arXiv:2209.07753 [cs]. Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. Learning to Model the World with Language, May 2024. URL http://arxiv.org/abs/ 2308.01399. arXiv:2308.01399 [cs]. Ruizhen Liu, Zhicong Chen, and Dazhi Zhong. Dromo: Distributionally robust offline model-based policy optimization. 2021. Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu, Zongqing Lu, and Jiaya Jia. RL-GPT: Integrating Reinforcement Learning and Code-as-policy, February 2024a. URL http://arxiv.org/abs/2402.19299. arXiv:2402.19299 [cs]. Toni J. B. Liu, Nicolas Boulle, Raphael Sarfati, and Christopher J. Earls. LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law, 2024b. Toni J. B. Liu, Nicolas Boulle, Raphael Sarfati, and Christopher J. Earls. LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law, February 2024c. URL http://arxiv.org/abs/2402.00795. arXiv:2402.00795 [cs]. Zuxin Liu, Jesse Zhang, Kavosh Asadi, Yao Liu, Ding Zhao, Shoham Sabach, and Rasool Fakoor. TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models, October 2023. URL http://arxiv.org/abs/2310.05905. arXiv:2310.05905 [cs]. Runyu Ma, Jelle Luijkx, Zlatan Ajanovic, and Jens Kober. ExploRLLM: Guiding Exploration in Reinforcement Learning with Large Language Models, March 2024. URL http://arxiv. org/abs/2403.09583. arXiv:2403.09583 [cs]. Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Ermon. Calibrated Model-Based Deep Reinforcement Learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 43144323. PMLR, 0915 Jun 2019. URL https://proceedings.mlr.press/v97/malik19a.html. Vincent Micheli, Eloi Alonso, and Francois Fleuret. Transformers are Sample-Efficient World Models. September 2022. URL https://openreview.net/forum?id=vhFu1Acb0xb. Eduardo Pignatelli, Johan Ferret, and Tim Rocktaschel. Assessing the Zero-Shot Capabilities of LLMs for Action Evaluation in RL. Preprint. Under review. Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, Jan Achterhold, Joerg Stueckler, Michal Rolinek, and Georg Martius. Sample-efficient cross-entropy method for real-time planning. In Conference on Robot Learning 2020, 2020. URL https://corlconf.github.io/ corl2020/paper_217/. Rudra P. K. Poudel, Harit Pandya, Chao Zhang, and Roberto Cipolla. LanGWM: Language Grounded World Model, November 2023. URL https://arxiv.org/abs/2311. 17593v1. Nooshin Pourkamali and Shler Ebrahim Sharifi. Machine translation with large language models: Prompt engineering for persian, english, and russian directions. arXiv preprint arXiv:2401.08429, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Machel Reid, Yutaro Yamada, and Shixiang Shane Gu. Can Wikipedia Help Offline Reinforcement Learning?, July 2022. URL http://arxiv.org/abs/2201.12122. arXiv:2201.12122 [cs]. James Requeima, John Bronskill, Dami Choi, Richard E. Turner, and David Duvenaud. LLM Processes: Numerical Predictive Distributions Conditioned on Natural Language, May 2024. URL http://arxiv.org/abs/2405.12856. arXiv:2405.12856 [cs, stat]. Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of language model? In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (eds.), Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 54185426, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.437. URL https://aclanthology.org/2020. emnlp-main.437. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms, 2017. URL https://arxiv.org/abs/1707.06347. Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S. Du, and Huazhe Xu. Unleashing the Power of Pretrained Language Models for Offline Reinforcement Learning, November 2023. URL http: //arxiv.org/abs/2310.20587. arXiv:2310.20587 [cs]. Richard S. Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM ISSN 0163-5719. doi: 10.1145/122344.122377. URL SIGART Bulletin, 2:160163, 7 1991. https://dl.acm.org/doi/10.1145/122344.122377. Richard Sutton, Csaba Szepesvari, Alborz Geramifard, and Michael Bowling. Dyna-style planning with linear function approximation and prioritized sweeping. Moore and Atkeson, 1992. Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 50265033, 2012. doi: 10.1109/IROS.2012.6386109. Masatoshi Uehara, Chengchun Shi, and Nathan Kallus. review of off-policy evaluation in reinforcement learning, 2022. URL https://arxiv.org/abs/2212.06355. Robert Vacareanu, Vlad-Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From Words to Numbers: Your Large Language Model Is Secretly Capable Regressor When Given In-Context Examples, September 2024. URL http://arxiv.org/abs/2404.07544. arXiv:2404.07544 [cs]. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS17, pp. 60006010, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964. Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, May 2023. URL http://arxiv.org/abs/2212.07677. arXiv:2212.07677 [cs]. Preprint. Under review. Yen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt Robot to Walk with Large Language Models, November 2023. URL http://arxiv.org/abs/2309.09969. arXiv:2309.09969 [cs, eess]. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ 2020.emnlp-demos.6. Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, and Tom M. Mitchell. Read and reap the rewards: Learning to play atari with the help of instruction manuals, 2024. URL https: //arxiv.org/abs/2302.04449. Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An Explanation of In-context Learning as Implicit Bayesian Inference, July 2022. URL http://arxiv.org/abs/2111. 02080. arXiv:2111.02080 [cs]. Hao Xue and Flora D. Salim. PromptCast: New Prompt-based Learning Paradigm for Time Series Forecasting, December 2023. URL http://arxiv.org/abs/2210.08964. arXiv:2210.08964 [cs, math, stat]. Sherry Yang, Ofir Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation Models for Decision Making: Problems, Methods, and Opportunities, March 2023. URL http: //arxiv.org/abs/2303.04129. arXiv:2303.04129 [cs]. Yu Yang and Pan Xu. Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer, August 2024. URL http://arxiv.org/abs/2408.01402. arXiv:2408.01402 [cs]. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1412914142. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf. Combo: Conservative offline model-based policy optimization. Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2895428967. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/2021/file/ f29a179746902e331572c483c45e5086-Paper.pdf. Oussama Zekri, Abdelhakim Benechehab, and Ievgen Redko. Can LLMs predict the convergence of Stochastic Gradient Descent? June 2024a. URL https://openreview.net/forum? id=FraikHzMu9. Oussama Zekri, Ambroise Odonnat, Abdelhakim Benechehab, Linus Bleistein, Nicolas Boulle, and Ievgen Redko. Large language models as markov chains, 2024b. URL https://arxiv.org/ abs/2410.02724. Xianyuan Zhan, Xiangyu Zhu, and Haoran Xu. Model-based offline planning with trajectory pruning. 2021. Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, and Zhaoran Wang. How Can LLM Guide RL? Value-Based Approach, February 2024. URL http://arxiv.org/abs/2402.16181. arXiv:2402.16181 [cs]. 15 Preprint. Under review. Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang. STORM: Efficient Stochastic Transformer based World Models for Reinforcement Learning, October 2023a. URL https: //arxiv.org/abs/2310.09615v1. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. Sentiment analysis in the era of large language models: reality check, 2023b. URL https://arxiv.org/abs/ 2305.15005. Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization, 2023c. URL https://arxiv.org/abs/2305.19420. Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, and Ram Nevatia. Large language models are good prompt learners for low-shot image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2845328462, 2024. 16 Preprint. Under review."
        },
        {
            "title": "Appendix",
            "content": "Outline. In Appendix A, we prove our main theoretical result (Theorem 4.2). We provide an extended related work in Appendix B. Additional materials about the state and action dimensions interdependence are given in Appendix C. The implementation details and hyperparameters of our methods are given in Appendix D. Finally, we provide additional experiments about multi-step errors (Appendix F), calibration (Appendix G), and the impact of the data collecting policy on the prediction error (Appendix E)."
        },
        {
            "title": "TABLE OF CONTENTS",
            "content": "A Theoretical analysis A.1 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Related Work State and action dimensions interdependence - additional materials C.1 Principal Component Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . . . . C.2 Independent Component Analysis (ICA) . . . . . . . . . . . . . . . . . . . . . . . C.3 AutoEncoder-based approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.4 Sensitivity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Algorithms D.1 Soft-Actor Critic D.2 DICL-SAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . What is the impact of the policy on the prediction error? Multi-step prediction errors Calibration 18 18 20 21 22 22 23 23 24 24 25 26 17 Preprint. Under review."
        },
        {
            "title": "A THEORETICAL ANALYSIS",
            "content": "A.1 PROOF OF THEOREM 4.2 We start by formally defining the LLM multi-branch return ηllm p,k,T . To do so, we first denote At the random event of starting k-step LLM branch at timestep and we denote Xt the associated indicator random variable Xt = 1[At]. We assume that the (Xt)tT are independent. We then define the random event Ak that at least one of the preceding timesteps has been branched, meaning that the given timestep belongs to at least one LLM branch among the possible branches: Ak = (cid:83)k1 i=0 Ati. The LLM multi-branch return can then be written as follows: ηllm p,k,T (π) = 1 (cid:88) γtEstP t,atπ (cid:2)r(st, at)(cid:3) t=0 (cid:124) (cid:123)(cid:122) Burn-in phase to gather minimal context size (cid:125) γtEXtib(p),1ik (cid:20) 1[Ak ] + (cid:88) t=T 1 i=1 Xti (cid:80)k (cid:88) i=1 XtiE st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) (5) (cid:123)(cid:122) average reward among the branches spanning timestep (cid:125) ]EstP t,atπ (cid:2)r(st, at)(cid:3) + 1[ Ak (cid:124) (cid:125) (cid:123)(cid:122) When no branch is spanning timestep (cid:124) (cid:21) , where = (.P t1) with 0 = µ0 the initial state distribution and ˆP Before continuing, we first need to establish the following lemma. Lemma A.1. (Multi-step Error Bound, Lemma B.2 in Frauenknecht et al. (2024) and Janner et al. (2019).) Let and be two transition functions. Define the multi-step error at time step t, starting from any initial state distribution µ0, as: llm(.P ti). t,llm = ˆP εt := DTV(P t(µ0) t(µ0)) with 0 = 0 = µ0. Let the one-step error at time step 1 be defined as: ξt := EsP t1(µ0) (cid:104) (cid:105) DTV(P (s) (s)) , and ξ0 = ε0 = 0. Then, the multi-step error satisfies the following bound: εt (cid:88) i=0 ξi. Proof. Let > 0. We start with the definition of the total variation distance: (cid:90) = εt = DTV(P t(µ0) t(µ0)) (cid:12) (cid:12) (cid:12)P t(sµ0) t(sµ0) (cid:12) ds (cid:12) (cid:12) (cid:12) (cid:90) (cid:12) (cid:12) (cid:12) (cid:90) sS sS = (cid:90) (cid:90) (cid:12) (cid:12) (ss)P t1(sµ0) (ss) t1(sµ0) ds (cid:12) (cid:12) (cid:12) (cid:12) ds ds (cid:12) (cid:12) (cid:12) ds ds (cid:12) sS (cid:12) (cid:12)P (ss)P t1(sµ0) (ss) t1(sµ0) (cid:12) (cid:12) (cid:12)P (ss)P t1(sµ0) (ss) t1(sµ0) (cid:12) ds sS (cid:90) sS sS (cid:90) sS 1 2 1 2 1 2 1 2 = 18 Preprint. Under review. = 1 2 1 (cid:90) = (cid:90) (cid:90) (cid:12) (cid:12)P (ss)P t1(sµ0) (ss)P t1(sµ0) (cid:12) (cid:90) sS sS (cid:12) + (ss)P t1(sµ0) (ss) t1(sµ0) (cid:12) ds ds (cid:12) (cid:90) (cid:12) (cid:12) (cid:12)P (ss) (ss) (cid:12) ds ds (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P t1(sµ0) t1(sµ0) (cid:12) ds ds (cid:12) (cid:12) (cid:21) t1(sµ0) (ss) sS (cid:90) sS (cid:90) + sS (cid:12) (cid:12)P (ss) (ss) (cid:12) (cid:19) (cid:12) (cid:12) (cid:12)P t1(sµ0) t1(sµ0) (cid:12) (cid:12) (cid:12) ds t1(sµ0) ds (ss) ds (cid:12) (cid:12) ds (cid:12) sS (cid:90) 1 2 (cid:20) 1 2 (cid:90) sS (cid:18)(cid:90) sS sS (cid:104) DTV(P (µ0) (s)) (cid:105) + DTV(P t1(µ0) t1(µ0)) + sS 1 2 = EsP t1(µ0) = ξt + εt1 Given that ξ0 = ε0 = 0, by induction we have: εt (cid:88) i= ξi. We now restate and prove Theorem 4.2: Theorem A.2 (Multi-branch return bound). Let be the minimal length of the in-context trajectories, [0, 1] the probability that given state is branching point. We assume that the reward is bounded and that the expected total variation between the LLM-based model and the true dynamics under policy π is bounded at each timestep by maxtT EsP t,aπ[DTV(P (.s, a) ˆPllm(.s, a))] εllm(T ). Then under multi-branched rollout scheme with branch length of k, the return is bounded as follows: η(π) ηllm p,k,T (π) 2 γT 1 γ rmaxk2 εllm(T ) , (6) where rmax = maxsS,aA r(s, a). Proof. Step 1: Expressing the bound in terms of horizon-dependent errors. η(π) ηllm p,k,T (π) = (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) t=T γtEstP t,atπ (cid:2)r(st, at)(cid:3) EXtib(p),1ik (cid:20) 1[Ak ] 1[ Ak ]EstP t,atπ (cid:2)r(st, at)(cid:3) (cid:88) i=1 XtiE st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) (cid:88) t=T γt (cid:12) (cid:12) (cid:12) (cid:12) EXtib(p),1ik (cid:20) 1[Ak ]EstP t,atπ (cid:2)r(st, at)(cid:3) + 1[ Ak ]EstP t,atπ (cid:21) (cid:2)r(st, at)(cid:3) EXtib(p),1ik (cid:20) 1[Ak ] 1[ Ak ]EstP t,atπ (cid:2)r(st, at)(cid:3) (cid:88) i=1 XtiE st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) (cid:80)k 1 i=1 Xti (cid:21)(cid:12) (cid:12) (cid:12) (cid:12) (cid:80)k 1 i=1 Xti (cid:21)(cid:12) (cid:12) (cid:12) (cid:12) 19 Preprint. Under review. (cid:88) t=T γt (cid:12) (cid:12) (cid:12) (cid:12) EXtib(p),1ik (cid:18) (cid:20) 1[Ak ] EstP t,atπ (cid:2)r(st, at)(cid:3) (cid:88) i=1 XtiE st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) (cid:19)(cid:21)(cid:12) (cid:12) (cid:12) (cid:12) (cid:80)k 1 i=1 Xti (cid:20) 1[Ak ] (cid:88) t=T γt (cid:12) (cid:12) (cid:12) (cid:12) EXtib(p),1ik 1 i=1 Xti (cid:80)k (cid:88) (cid:18) Xti i=1 EstP t,atπ (cid:2)r(st, at)(cid:3) st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) (cid:19)(cid:21)(cid:12) (cid:12) (cid:12) (cid:12) We then expand the integrals in the terms EstP t,atπ express it in terms of horizon-dependent multi-step model errors: (cid:2)r(st, at)(cid:3) st ˆP t,llm,atπ (cid:2)r(st, at)(cid:3) and (7) (8) EstP t,atπ (cid:2)r(st, at)(cid:3) (cid:2)r(st, at)(cid:3) t,llm,atπ st ˆP r(s, a)(cid:0)P t(s, a) ˆP t,llm(s, a)(cid:1) da ds (cid:90) (cid:90) = sS rmax aA (cid:90) (cid:90) sS (cid:90) aA (cid:90) sS (cid:90) aA rmax rmax (cid:0)P t(s) ˆP t,llm(s)(cid:1) ds sS 2rmaxDTV(P ˆP t,llm) (cid:0)P t(s, a) ˆP t,llm(s, a)(cid:1) da ds (cid:0)P t(s) ˆP t,llm(s)(cid:1)π(as) da ds Step 2: Simplifying the bound. By applying Lemma A.1 we can bound the multi-step errors using the bound on one-step errors: DTV(P ˆP t,llm) εllm(T ) εllm(T ) Therefore, the bound becomes: η(π) ηllm p,k,T (π) 2rmax εllm(T ) = 2rmax εllm(T ) 2rmax εllm(T ) γt (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) EXtib(p),1ik (cid:34) 1[Ak ] 1 i=1 Xti (cid:80)k (cid:88) i=1 Xti (cid:35)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) γt (cid:12) (cid:12)EXtib(p),1ik (cid:2)1[Ak ](cid:3)(cid:12) (cid:12) γtkp (cid:88) t=T (cid:88) t=T (cid:88) t=T = 2 γT 1 γ rmaxk2 εllm(T ) (9)"
        },
        {
            "title": "B RELATED WORK",
            "content": "Model-based reinforcement learning (MBRL). MBRL has been effectively used in iterated batch RL by alternating between model learning and planning (Deisenroth & Rasmussen, 2011; Hafner et al., 2021; Gal et al., 2016; Levine & Koltun, 2013; Chua et al., 2018; Janner et al., 2019; 20 Preprint. Under review. Kegl et al., 2021), and in the offline (pure batch) RL where we do one step of model learning followed by policy learning (Yu et al., 2020; Kidambi et al., 2020; Lee et al., 2021; Argenson & Dulac-Arnold, 2021; Zhan et al., 2021; Yu et al., 2021; Liu et al., 2021; Benechehab et al., 2023). Planning is used either at decision time via model-predictive control (MPC) (Draeger et al., 1995; Chua et al., 2018; Hafner et al., 2019; Pinneri et al., 2020; Kegl et al., 2021), or in the background where model-free agent is learned on imagined model rollouts (Dyna; Janner et al. (2019); Sutton (1991); Sutton et al. (1992); Ha & Schmidhuber (2018)), or both. For example, model-based policy optimization (MBPO) (Janner et al., 2019) trains an ensemble of feed-forward models and generates imaginary rollouts to train soft actor-critic agent. LLMs in RL. LLMs have been integrated into reinforcement learning (RL) (Cao et al., 2024; Yang et al., 2023), playing key roles in enhancing decision-making (Kannan et al., 2024; Pignatelli et al.; Zhang et al., 2024; Feng et al., 2024), reward design (Kwon et al., 2023; Wu et al., 2024; Carta et al., 2023; Liu et al., 2023), and information processing (Poudel et al., 2023; Lin et al., 2024). The use of LLMs as world models is particularly relevant to our work. More generally, the Transformer architecture (Vaswani et al., 2017) has been used in offline RL (Decision Transformer Chen et al. (2021); Trajectory Transformer Janner et al. (2021)). Pre-trained LLMs have been used to initialize decision transformers and fine-tune them for offline RL tasks (Shi et al., 2023; Reid et al., 2022; Yang & Xu, 2024). As world models, Dreamer-like architectures based on Transformers have been proposed (Micheli et al., 2022; Zhang et al., 2023a; Chen et al., 2022), demonstrating efficiency for long-memory tasks such as Atari games. In text-based environments, LLMs have found multiple applications (Lin et al., 2024; Feng et al., 2024; Zhang et al., 2024; Ma et al., 2024), including using code-generating LLMs to generate policies in zero-shot fashion (Liang et al., 2023; Liu et al., 2024a). The closest work to ours is Wang et al. (2023), where system prompt consisting of multiple pieces of information about the control environment (e.g., description of the state and action spaces, nature of the controller, historical observations, and actions) is fed to the LLM. Unlike our approach, which focuses on predicting the dynamics of RL environments, Wang et al. (2023) aim to directly learn low-level control policy from the LLM, incorporating extra information in the prompt. Furthermore, Wang et al. (2023) found that only GPT-4 was usable within their framework, while we provide proof-of-concept using smaller open LLMs such as Llama 3.2 1B. ICL on Numerical Data. In-context learning for regression tasks has been theoretically analyzed in several works, providing insights based on the Transformer architecture (Li et al., 2023; von Oswald et al., 2023; Akyurek et al., 2023; Garg et al., 2023; Xie et al., 2022). Regarding time series forecasting, LLMTime (Gruver et al., 2023a) successfully leverages ICL for zero-shot extrapolation of one-dimensional time series data. Similarly, Das et al. (2024) introduce foundational model for one-dimensional zero-shot time series forecasting, while Xue & Salim (2023) combine numerical data and text in question-answer format. ICL can also be used to approximate continuous density from the LLM logits. For example, Liu et al. (2024c) develop Hierarchical softmax algorithm to infer the transition rules of uni-dimensional Markovian dynamical systems. Building on this work, Zekri et al. (2024a) provide an application that predicts the parameter value trajectories in the Stochastic Gradient Descent algorithm. More relevant to our work, Requeima et al. (2024) presented LLMProcesses, method aimed at extracting multi-dimensional distributions from LLMs. Other practical applications of ICL on numerical data include few-shot classification on tabular data (Hegselmann et al., 2023), regression (Vacareanu et al., 2024), and meta ICL (Coda-Forno et al., 2023). STATE AND ACTION DIMENSIONS INTERDEPENDENCE - ADDITIONAL MATERIALS C.1 PRINCIPAL COMPONENT ANALYSIS (PCA) Principal Component Analysis. PCA is dimensionality reduction technique that transforms the original variables into new set of variables, the principal components, which are linearly uncorrelated. The principal components can be ordered such that the first few retain most of the variation present in all of the original variables. Formally, given data matrix with Preprint. Under review. observations and variables, PCA diagonalizes the covariance matrix = 1 n1 XT to find the eigenvectors, which represent the directions of the principal components: PCA: = XW, where are the eigenvectors of C. In our case, the data represents dataset of states and actions given data collecting policy πD, while the variables represent the state (eventually also the action) dimensions. Ablation on the number of components. Fig. 8 shows an ablation study on the number of components used in the DICL-(s, a) method. Surprisingly, we observe sharp decline in the average multi-step error (see Appendix for detailed definition) given only 4 components among 23 in the HalfCheetah system. The error then slightly increases for an intermediate number of components, before going down again when the full variance is recovered. This finding strengthens the position of PCA as our Disentangling algorithm of choice in DICL. C.2 INDEPENDENT COMPONENT ANALYSIS (ICA) ICA is statistical and computational technique used to separate multivariate signal into additive, statistically independent components. Unlike PCA, which decorrelates the data, ICA aims to find linear transformation that makes the components as independent as possible. Given data matrix X, ICA assumes that the data is generated as linear mixtures of independent components: = AS, where is an unknown mixing matrix and is the matrix of independent components with independent rows. The goal of ICA is to estimate an unmixing matrix such that = WX is good approximation of the independent components S. The implications of ICA on independence are profound: while PCA only guarantees uncorrelated components, ICA goes step further by optimizing for statistical independence, often measured by non-Gaussianity (kurtosis or negentropy). Figure 8: Ablation study on the number of principal components in the DICL- (s, a) method. Fig. 9 shows the estimated mixing matrix when running ICA on the D4RL-expert dataset on the Hopper environment. Under the assumptions of ICA, notably the statistical independence of the source signals, their linear mixing and the invertibility of the original (unknown) mixing matrix, the original sources are successfully recovered if each line of the estimated mixing matrix is mostly dominated by single value, meaning that its close to an identity matrix up to permutation with scalIn the case of our states and actions data, its not ing. clear that this is the case from Fig. 9. Similarly to PCA, we can transform the in-context multi-dimensional signal using ICA, and apply the ICL procedure to the recovered independent sources. We plan on exploring this method in future follow-up work. C.3 AUTOENCODER-BASED APPROACH Figure 9: ICA estimated mixing matrix. Variational Autoencoders (VAEs) (Kingma & Welling, 2022) offer powerful framework for learning representations. disentangled representation is one where each dimension of the latent space captures distinct and interpretable factor of variation in the data. By combining an encoder network that maps inputs to probabilistic latent space with decoder network that reconstructs the data, VAEs employ the reparameterization trick to enable backpropagation through the sampling process. The key to disentanglement lies in the KL-divergence term of the VAE loss function, which regularizes the latent distribution to be close to standard normal distribution. Variants such as βVAE (Higgins et al., 2017) further emphasize this regularization by scaling the KL-divergence term, thereby encouraging the model to learn more disentangled representation at the potential cost of reconstruction quality. Beyond simple VAEs, there exist previous work in the literature that specifPreprint. Under review. ically aim at learning factorized posterior distribution in the latent space (Kim & Mnih, 2019). Although this direction looks promising, it strikes different concerns about the learnability of these models in the low data regime considered in our paper. C.4 SENSITIVITY ANALYSIS The preceding analysis examines state dimensions as features within representation space, disregarding their temporal nature and our ultimate objective of predicting the next state. In practice, our interest lies in capturing the dependencies that most significantly influence the next state through the dynamics function of the MDP. To achieve this, we use Sensitivity Analysis (SA) to investigate how variations in the input of the dynamics function impact its output. Sensitivity Analysis. Sensitivity analysis is systematic approach to evaluate how the uncertainty in the output of model can be attributed to different sources of uncertainty in the models inputs. The One-at-a-Time (OAT) method is technique used to understand the impact of individual input variables on the output of model. In the context of transition function of MDP, the OAT method involves systematically varying one current state or action dimension at time, while keeping all others fixed, and observing the resulting changes in the output dimensions: (st+1)k , where (st)i, (at)j (st)i and (st+1)k denote the i-th dimension of the state, the jth dimension of the action, and the k-th dimension of the next state, respectively. and (st+1)k (at)j Figure 10: Sensitivity matrix. In practice, we measure the sensitivity by applying perturbation (of scale 10%) to each input dimension separately, reporting the absolute change that occurs in each dimension of the output. Precisely, for deterministic transition function , input state dimension i, and output dimension k, we measure (s + ϵ, a)k (s, a)k where ϵi = 0.1 scale(i) and 0 elsewhere. The sensitivity matrix in Fig. 10 demonstrates that most of the next state dimensions are mostly affected by their respective previous values (the diagonal shape in the state dimensions square). In addition to that, actions only directly affect some state dimensions, specifically velocities, which is expected from the nature of the physics simulation underlying those systems. This finding suggests that the vICL method might give good results in practice for the considered RL environments, and makes us hope that the DICL-(s) approach is enough to capture the state dimensions dependencies, especially for single-step prediction. Remark C.1. This sensitivity analysis is specific to the single-step transition function. In practice, such conclusions might change when looking at larger time scale of the simulation."
        },
        {
            "title": "D ALGORITHMS",
            "content": "D.1 SOFT-ACTOR CRITIC Soft Actor-Critic (SAC) (Haarnoja et al., 2018) is an off-policy algorithm that incorporates the maximum entropy framework, which encourages exploration by seeking to maximize the entropy of the policy in addition to the expected return. SAC uses deep neural network to approximate the policy (actor) and the value functions (critics), employing two Q-value functions to mitigate positive bias in the policy improvement step typical of off-policy algorithms. This approach helps in learning more stable and effective policies for complex environments, making SAC particularly suitable for tasks with high-dimensional, continuous action spaces. We use the implementation provided in CleanRL (Huang et al., 2022) for SAC. In all environments, we keep the default hyperparameters provided with the library, except for the update frequency. In fact, the default update frequency in SAC is 1 step, meaning that the policy that interacts with the environment gets updated after every interaction. In our LLM-based framework, this is problematic because it implies that the state visitation distribution of the in-context trajectories will be moving 23 Preprint. Under review. from one timestamp to another. To solve this issue, we fix the update frequency of SAC to the maximal number of steps of an episode of given environment. With that in mind, we specify in Table 1 the complete list of hyperparameters used for every considered environment. Table 1: SAC hyperparameters. Environment HalfCheetah Hopper Pendulum Update frequency Learning starts Batch size Total timesteps Gamma γ policy learning rate 1000 5000 128 1e6 0.99 3e 4 1000 5000 128 1e6 0.99 3e 4 200 1000 64 1e4 0.99 3e 4 D.2 DICL-SAC For our algorithm, we integrate an LLM inference interface (typically the Transformers library from Huggingface (Wolf et al., 2020)) with CleanRL (Huang et al., 2022). Table 2 shows all DICL-SAC hyperparameter choices for the considered environments. Table 2: DICL-SAC hyperparameters. Environment HalfCheetah Hopper Pendulum Update frequency Learning starts LLM Learning starts LLM Learning frequency Batch size LLM Batch size (α%) Total timesteps Gamma γ Max context length Min context length LLM sampling method LLM dynamics learner 1000 5000 10000 256 128 7(5%), 13(10%), 32(25%) 1e6 0.99 500 1 mode vICL 1000 5000 10000 256 128 7(5%), 13(10%), 32(25%) 1e6 0.99 500 1 mode vICL 200 1000 2000 16 64 4(5%), 7(10%), 16(25%) 1e4 0.99 198 1 mode vICL Balancing gradient updates. To ensure that DICL-SAC performs equally important gradient updates on the LLM generated data, we used gradient updates balancing mechanism. Indeed, since the default reduction method of loss functions is averaging, the batch with the smallest batch 1 . To address this, we multiply size gets assigned higher weight when doing gradient descent: the loss corresponding to the LLM generated batch Bllm with correcting coefficient Bllm ensuring equal weighting across all samples. We now show the full training curves on the HalfCheetah and Hopper environments  (Fig. 11)  . The return curves show smoothed average training curves 95% Gaussian confidence intervals for 5 seeds in HalfCheetah and Hopper, and 10 seeds for Pendulum. WHAT IS THE IMPACT OF THE POLICY ON THE PREDICTION ERROR? In this experiment, We investigate how policy impacts the accuracy and calibration of our LLMbased dynamics models. To do so, we train three model-free algorithms (PPO (Schulman et al., 2017), SAC (Haarnoja et al., 2018), and TD3 (Fujimoto et al., 2019)) on the HalfCheetah environment, selecting different checkpoints throughout training to capture diverse policies. We then analyze the correlation between policy characteristics, specifically state coverage (defined as the maximum distance between any two states encountered by the policy) and entropy, with the Mean Squared Error and Kolmogorov-Smirnov (KS) statistic. Our findings indicate that the state coverage correlates with both MSE and KS, possibly because policies that explore wide range of states generate trajectories that are more difficult to learn. Regarding the entropy, we can see that it also correlates with MSE, but interestingly, it does not appear to impact the calibration. 24 Preprint. Under review. Figure 11: Data-augmented off-policy RL. Full training curves. Figure 12: Correlation plots between state coverage and entropy of policies with MSE and KS metrics under the vICL dynamics learner. MULTI-STEP PREDICTION ERRORS The average multi-step error. In Fig. 3a, we compute the average Mean Squared Error over prediction horizons for = 1, . . . , 20, and 5 trajectories sampled uniformly from the D4RL expert dataset. For visualization purposes, we first rescale all the dimensions (using pipeline composed of MinMaxScaler and StandardScaler) so that the respective MSEs are on the same scale. The MLP baseline. For the LP baseline, we instantiate an MLP with: 4 layers, 128 neurons each, and ReLU activations. We then format the in-context trajectory as dataset of {(st, at, st+1)} on which we train the MLP for 150 epochs using early stopping and the Adam optimizer (Kingma & Ba, 2015). We now extend Fig. 3 to show the multi-step generated trajectories for all the dimensions of the HalfCheetah system in Fig. 13. 25 Preprint. Under review. Figure 13: Halfcheetah"
        },
        {
            "title": "G CALIBRATION",
            "content": "The naive baseline. In the calibration plots Figs. 7 and 14, we compare the LLM-based dynamics models with (naive) baseline that estimates Gaussian distribution using the in-context moments (mean and variance). KOLMOGOROV-SMIRNOV STATISTIC (KS): This metric is computed using the quantiles (under the model distribution) of the ground truth values. Hypothetically, these quantiles are uniform if the error in predicting the ground truth is random variable distributed according to Gaussian with the predicted standard deviation, property we characterize as calibration. To assess this, we compute the Kolmogorov-Smirnov (KS) statistics. Formally, starting from the model cumulative distribution 26 Preprint. Under review. function (CDF) Fθ(st+1st, at), we define the empirical CDF of the quantiles of ground truth values (cid:12) (cid:12) (cid:8)(st,at,st+1)DF θ (st+1st,at)x(cid:9)(cid:12) (cid:12) by Fθ,j(x) = for [0, 1]. We denote by (x) the CDF of the uniform distribution over the interval [0, 1], and we define the KS statistics as the largest absolute difference between the two CDFs across the dataset D: KS(D; θ; {1, . . . , ds}) = max i{1,...,N } (cid:12) (cid:12)Fθ,j(F (cid:12) θ (si,t+1si,t, ai,t)) (F θ (si,t+1si,t, ai,t)) (10) (cid:12) (cid:12) (cid:12) The KS score ranges between zero and one, with lower values indicating better calibration. Figure 14: Halfcheetah"
        }
    ],
    "affiliations": [
        "ENS Paris-Saclay",
        "EURECOM",
        "Huawei Noahs Ark Lab",
        "Inria",
        "KAUST"
    ]
}